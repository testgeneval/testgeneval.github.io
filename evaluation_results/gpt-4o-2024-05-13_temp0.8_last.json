{"raw_generations": {"sample_0": ["def test_propagate_with_unsupported_operation(UncertClass):\n    uncert = UncertClass([1, 2, 3])\n    with pytest.raises(ValueError, match='unsupported operation'):\n        uncert.propagate(np.mean, None, None, 0)\n", "def test_getitem(UncertClass):\n    # Test slicing functionality\n    data = np.arange(10)\n    uncert = UncertClass(data)\n    sliced_uncert = uncert[2:5]\n    assert isinstance(sliced_uncert, UncertClass)\n    assert_array_equal(sliced_uncert.array, data[2:5])\n    assert sliced_uncert.unit == uncert.unit\n\n    # Test slicing with unit\n    data_with_unit = np.arange(10) * u.adu\n    uncert_with_unit = UncertClass(data_with_unit)\n    sliced_uncert_with_unit = uncert_with_unit[2:5]\n    assert isinstance(sliced_uncert_with_unit, UncertClass)\n    assert_array_equal(sliced_uncert_with_unit.array, data_with_unit.value[2:5])\n    assert sliced_uncert_with_unit.unit == u.adu\n", "def test_setstate_fake_uncertainty(UncertClass):\n    # Test the __setstate__ and __getstate__ methods for pickling support\n    uncert = UncertClass([1, 2, 3], unit=u.adu)\n    state = uncert.__getstate__()\n\n    # Ensure state contains expected components\n    assert len(state) == 3\n    assert_array_equal(state[0], [1, 2, 3])\n    assert state[1] == u.adu\n    assert state[2] is None\n\n    # Create a new instance and set its state\n    new_uncert = UncertClass()\n    new_uncert.__setstate__(state)\n\n    # Ensure the new instance has the same properties\n    assert_array_equal(new_uncert.array, [1, 2, 3])\n    assert new_uncert.unit == u.adu\n    with pytest.raises(MissingDataAssociationException):\n        new_uncert.parent_nddata\n", "def test_propagate_uncertainties(NDClass, UncertClass):\n    # Test the propagation of uncertainties with different operations\n    data1 = np.array([1, 2, 3])\n    data2 = np.array([4, 5, 6])\n    ndd1 = NDClass(data1, unit='adu', uncertainty=UncertClass(data1 * 0.1))\n    ndd2 = NDClass(data2, unit='adu', uncertainty=UncertClass(data2 * 0.2))\n\n    # Test addition\n    result_add = ndd1.data + ndd2.data\n    propagated_add = ndd1.uncertainty.propagate(np.add, ndd2, result_add, 0)\n    expected_add_uncert = np.sqrt((data1 * 0.1) ** 2 + (data2 * 0.2) ** 2)\n    assert_allclose(propagated_add.array, expected_add_uncert)\n\n    # Test subtraction\n    result_sub = ndd1.data - ndd2.data\n    propagated_sub = ndd1.uncertainty.propagate(np.subtract, ndd2, result_sub, 0)\n    expected_sub_uncert = np.sqrt((data1 * 0.1) ** 2 + (data2 * 0.2) ** 2)\n    assert_allclose(propagated_sub.array, expected_sub_uncert)\n\n    # Test multiplication\n    result_mul = ndd1.data * ndd2.data\n    propagated_mul = ndd1.uncertainty.propagate(np.multiply, ndd2, result_mul, 0)\n    expected_mul_uncert = np.sqrt((data1 * 0.1) ** 2 * data2 ** 2 + (data2 * 0.2) ** 2 * data1 ** 2)\n    assert_allclose(propagated_mul.array, expected_mul_uncert)\n\n    # Test division\n    result_div = ndd1.data / ndd2.data\n    propagated_div = ndd1.uncertainty.propagate(np.divide, ndd2, result_div, 0)\n    expected_div_uncert = np.sqrt((data1", "def test_init_with_none(UncertClass):\n    # Initializing uncertainty with None should work and array should be None\n    fake_uncert = UncertClass(None)\n    assert fake_uncert.array is None\n    assert fake_uncert.unit is None\n    # Initialize with None and a unit\n    fake_uncert = UncertClass(None, unit=u.adu)\n    assert fake_uncert.array is None\n    assert fake_uncert.unit == u.adu\n"], "sample_3": ["def test_write_read_roundtrip_with_units():\n    \"\"\"\n    Write a table with units and see that it round-trips on readback.\n    \"\"\"\n    t = Table()\n    t['a'] = Column([1, 2, 3], unit=u.m)\n    t['b'] = Column([4.5, 5.5, 6.5], unit=u.s)\n    t['c'] = Column([7.8, 8.8, 9.8], unit=u.kg)\n\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    \n    assert t2.colnames == t.colnames\n    for name in t.colnames:\n        assert t[name].unit == t2[name].unit\n        assert np.all(t[name] == t2[name])\n", "def test_write_read_roundtrip_with_units():\n    \"\"\"\n    Test round-trip of a table with Quantity columns with units.\n    \"\"\"\n    t = Table()\n    t['length'] = [1, 2, 3] * u.m\n    t['mass'] = [4, 5, 6] * u.kg\n    t['time'] = [7, 8, 9] * u.s\n\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n\n    assert t2.colnames == t.colnames\n    for name in t.colnames:\n        assert np.all(t[name] == t2[name])\n        assert t[name].unit == t2[name].unit\n", "def test_ecsv_round_trip_with_units():\n    \"\"\"Test round-trip of a table with various units.\"\"\"\n    t = Table()\n    t['distance'] = [1, 2, 3] * u.m\n    t['speed'] = [10, 20, 30] * u.km / u.h\n    t['force'] = [100, 200, 300] * u.N\n    t['area'] = [1, 4, 9] * u.m**2\n\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n\n    assert t.colnames == t2.colnames\n    for name in t.colnames:\n        assert t[name].unit == t2[name].unit\n        assert np.all(t[name] == t2[name])\n", "def test_process_lines():\n    \"\"\"\n    Test the process_lines method of EcsvHeader to ensure it correctly processes lines.\n    \"\"\"\n    lines = [\n        \"# Comment line 1\",\n        \"# Comment line 2\",\n        \"Non-comment line\",\n        \"# Comment line 3\",\n        \"\",\n        \"# Comment line 4\"\n    ]\n    header = EcsvHeader()\n    header.comment = '# '\n    processed_lines = list(header.process_lines(lines))\n\n    assert processed_lines == [\n        \"Comment line 1\",\n        \"Comment line 2\",\n        \"Comment line 3\",\n        \"Comment line 4\"\n    ]\n", "def test_write_read_with_units():\n    \"\"\"\n    Write a table with various units and ensure that it round-trips on readback.\n    \"\"\"\n    t = Table()\n    t['a'] = [1, 2, 3] * u.m\n    t['b'] = [4.1, 5.2, 6.3] * u.kg\n    t['c'] = [7.4, 8.5, 9.6] * u.s\n\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n\n    assert t2['a'].unit == u.m\n    assert t2['b'].unit == u.kg\n    assert t2['c'].unit == u.s\n\n    assert np.all(t2['a'] == t['a'])\n    assert np.all(t2['b'] == t['b'])\n    assert np.all(t2['c'] == t['c'])\n"], "sample_1": ["def test_separability_matrix_single_input_multiple_outputs():\n    transform = models.Shift(1) & models.Shift(2)\n    result = separability_matrix(transform)\n    expected = np.array([[1], [1]], dtype=np.bool_)\n    assert_allclose(result, expected)\n", "def test_compute_n_outputs():\n    # Test with Model instances\n    assert _compute_n_outputs(sh1, scl1) == 2\n    assert _compute_n_outputs(rot, p2) == 4\n\n    # Test with ndarray instances\n    left = np.array([[1, 0], [0, 1]])\n    right = np.array([[1, 1], [1, 0]])\n    assert _compute_n_outputs(left, right) == 4\n\n    # Test with mixed Model and ndarray\n    assert _compute_n_outputs(sh1, right) == 3\n    assert _compute_n_outputs(left, scl1) == 4\n", "def test_separability_matrix_single_input_multi_output():\n    single_input_model = models.Shift(1) & models.Shift(2) & models.Shift(3)\n    separable_matrix = separability_matrix(single_input_model)\n    expected_matrix = np.ones((3, 1), dtype=bool)\n    assert_allclose(separable_matrix, expected_matrix)\n\n    separable = is_separable(single_input_model)\n    expected_separable = np.array([False, False, False])\n    assert_allclose(separable, expected_separable)\n", "def test_compute_n_outputs():\n    # Test with model inputs\n    n_outputs = _compute_n_outputs(sh1, scl1)\n    assert n_outputs == 2\n\n    n_outputs = _compute_n_outputs(rot, p2)\n    assert n_outputs == 4\n\n    # Test with ndarray inputs\n    n_outputs = _compute_n_outputs(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]]))\n    assert n_outputs == 4\n\n    # Test with mixed inputs\n    n_outputs = _compute_n_outputs(sh1, np.array([[1, 2], [3, 4]]))\n    assert n_outputs == 3\n\n    n_outputs = _compute_n_outputs(np.array([[1, 2], [3, 4]]), sh1)\n    assert n_outputs == 3\n", "def test_separability_matrix():\n    # Test separability_matrix function directly with various compound models\n    for name, (compound_model, expected) in compound_models.items():\n        assert_allclose(separability_matrix(compound_model), expected[1])\n\n    # Additional test cases\n    result = separability_matrix(sh1 & sh2 | rot)\n    expected = np.array([[True, True], [True, True]])\n    assert_allclose(result, expected)\n\n    result = separability_matrix(sh1 & sh2 | Mapping([0, 1, 0, 1]) | p2 & p22)\n    expected = np.array([[True, True], [True, True]])\n    assert_allclose(result, expected)\n"], "sample_2": ["def test_ccddata_invalid_uncertainty_type_raises_error():\n    ccd_data = create_ccd_data()\n    with pytest.raises(ValueError, match=r\"unsupported uncertainty type\"):\n        _uncertainty_unit_equivalent_to_parent(\"InvalidUncertaintyType\", u.adu, u.adu)\n", "def test_initialize_with_all_parameters():\n    data = np.ones((5, 5))\n    uncertainty = StdDevUncertainty(np.ones((5, 5)) * 0.1)\n    mask = data > 0.5\n    flags = np.zeros((5, 5))\n    wcs = WCS(naxis=2)\n    meta = {'EXPTIME': 30.0}\n    unit = 'adu'\n\n    ccd = CCDData(data=data, uncertainty=uncertainty, mask=mask, flags=flags,\n                  wcs=wcs, meta=meta, unit=unit)\n\n    np.testing.assert_array_equal(ccd.data, data)\n    np.testing.assert_array_equal(ccd.uncertainty.array, uncertainty.array)\n    np.testing.assert_array_equal(ccd.mask, mask)\n    np.testing.assert_array_equal(ccd.flags, flags)\n    assert ccd.wcs.wcs == wcs.wcs\n    assert ccd.meta == meta\n    assert ccd.unit == u.Unit(unit)\n", "def test_initialize_with_invalid_wcs_type():\n    data = np.ones((5, 5))\n    invalid_wcs = \"not_a_wcs_instance\"\n    with pytest.raises(TypeError, match=\"the wcs must be a WCS instance.\"):\n        CCDData(data, unit=\"adu\", wcs=invalid_wcs)\n", "def test_fits_ccddata_writer_with_custom_keywords(tmpdir):\n    # Test that custom keywords in the header are preserved when writing and reading CCDData.\n    ccd_data = create_ccd_data()\n    custom_meta = {'TELESCOP': 'Hubble', 'INSTRUME': 'ACS'}\n    ccd_data.header = custom_meta\n    filename = tmpdir.join('test_custom_keywords.fits').strpath\n    ccd_data.write(filename)\n\n    ccd_read = CCDData.read(filename, unit=ccd_data.unit)\n    assert ccd_read.header['TELESCOP'] == 'Hubble'\n    assert ccd_read.header['INSTRUME'] == 'ACS'\n", "def test_ccddata_init_with_invalid_wcs():\n    invalid_wcs = \"this is not a valid WCS\"\n    with pytest.raises(TypeError):\n        CCDData(np.zeros([2, 2]), unit='adu', wcs=invalid_wcs)\n"], "sample_5": ["def test_parameter_to_float():\n    \"\"\"Test the _tofloat function to ensure it properly converts various types to float.\"\"\"\n    \n    # Test with a list of numbers\n    assert np.array_equal(_tofloat([1, 2, 3]), np.array([1.0, 2.0, 3.0]))\n    \n    # Test with a Quantity\n    qty = Quantity(10, unit=u.m)\n    assert _tofloat(qty) == qty\n    \n    # Test with a numpy array\n    np_array = np.array([1, 2, 3])\n    assert np.array_equal(_tofloat(np_array), np.array([1.0, 2.0, 3.0]))\n    \n    # Test with a single number\n    assert _tofloat(5) == 5.0\n    \n    # Test with a float numpy number\n    assert _tofloat(np.float32(2.5)) == 2.5\n    \n    # Test with a boolean should raise an error\n    with pytest.raises(InputParameterError):\n        _tofloat(True)\n    \n    # Test with a string should raise an error\n    with pytest.raises(InputParameterError):\n        _tofloat(\"string\")\n", "def test_parameter_unit_conversion(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n        \n    m = model['class'](**model['parameters'])\n\n    for param_name, param_value in model['parameters'].items():\n        if isinstance(param_value, u.Quantity) and param_value.unit.is_equivalent(u.m):\n            param = getattr(m, param_name)\n            param.unit = u.cm\n            assert param.unit == u.cm\n            assert param.value == param_value.to(u.cm).value\n\n    for param_name, param_value in model['parameters'].items():\n        if isinstance(param_value, u.Quantity) and param_value.unit.is_equivalent(u.m):\n            param = getattr(m, param_name)\n            with pytest.raises(ValueError):\n                param.unit = u.s  # setting incompatible unit should raise ValueError\n", "def test_parameter_value_setter():\n    param = Parameter(name='test', default=10.0)\n    param.value = 20.0\n    assert param.value == 20.0\n\n    with pytest.raises(TypeError):\n        param.value = 30 * u.m\n\n    param_with_unit = Parameter(name='test', default=10.0 * u.m, unit=u.m)\n    param_with_unit.value = 20.0\n    assert param_with_unit.value == 20.0\n    assert param_with_unit.quantity == 20.0 * u.m\n\n    with pytest.raises(ValueError):\n        param_with_unit.value = 30 * u.s\n\n    param_with_unit.quantity = 50 * u.m\n    assert param_with_unit.value == 50.0\n    assert param_with_unit.quantity == 50.0 * u.m\n", "def test_parameter_initialization_with_quantity():\n    param = Parameter(name='test_param', default=5 * u.m, unit=u.m)\n    assert param.value == 5\n    assert param.unit == u.m\n\n    with pytest.raises(ParameterDefinitionError):\n        Parameter(name='test_param', default=5 * u.s, unit=u.m)\n\n    param = Parameter(name='test_param', default=5 * u.m)\n    assert param.value == 5\n    assert param.unit == u.m\n\n    param = Parameter(name='test_param', default=5 * u.m, unit=u.cm)\n    assert param.value == 500  # default value in cm\n    assert param.unit == u.cm\n", "def test_parameter_initialization():\n    # Test Parameter initialization with various configurations\n    param1 = Parameter(name='param1', description='test parameter', default=5.0)\n    assert param1.name == 'param1'\n    assert param1.description == 'test parameter'\n    assert param1.default == 5.0\n    assert param1.value == 5.0\n\n    param2 = Parameter(name='param2', unit=u.m, default=5.0 * u.m)\n    assert param2.unit == u.m\n    assert param2.default == 5.0\n    assert param2.value == 5.0\n\n    with pytest.raises(ParameterDefinitionError):\n        Parameter(name='param3', unit=u.s, default=5.0 * u.m)\n\n    param4 = Parameter(name='param4', fixed=True)\n    assert param4.fixed is True\n\n    param5 = Parameter(name='param5', tied=lambda model: model.param4.value * 2)\n    assert callable(param5.tied)\n\n    param6 = Parameter(name='param6', bounds=(0, 10))\n    assert param6.bounds == (0, 10)\n"], "sample_6": ["def test_wrap_at():\n    a = Angle([350, 10, -10, 180] * u.deg)\n\n    # Test wrapping with default wrap_angle (360 degrees)\n    wrapped_a = a.wrap_at(360 * u.deg)\n    npt.assert_almost_equal(wrapped_a.degree, [350, 10, 350, 180])\n\n    # Test wrapping with 180 degrees wrap_angle\n    wrapped_a = a.wrap_at(180 * u.deg)\n    npt.assert_almost_equal(wrapped_a.degree, [-10, 10, -10, 180])\n\n    # Test wrapping with 90 degrees wrap_angle\n    wrapped_a = a.wrap_at(90 * u.deg)\n    npt.assert_almost_equal(wrapped_a.degree, [-10, 10, -10, -180])\n\n    # Test inplace wrapping\n    a.wrap_at(180 * u.deg, inplace=True)\n    npt.assert_almost_equal(a.degree, [-10, 10, -10, 180])\n\n    # Test invalid wrap angle\n    with pytest.raises(ValueError):\n        a.wrap_at('invalid')\n", "def test_angle_wrap_at():\n    \"\"\"\n    Test the wrap_at method of Angle class.\n    \"\"\"\n    a = Angle([350, 10, 370, -10] * u.deg)\n    wrapped_a = a.wrap_at(360 * u.deg)\n    npt.assert_almost_equal(wrapped_a.degree, [350, 10, 10, 350])\n\n    wrapped_a = a.wrap_at('180d')\n    npt.assert_almost_equal(wrapped_a.degree, [-10, 10, 10, -10])\n\n    a.wrap_at(180 * u.deg, inplace=True)\n    npt.assert_almost_equal(a.degree, [-10, 10, 10, -10])\n\n    a = Angle([-200, 190, 370, -370] * u.deg)\n    wrapped_a = a.wrap_at(180 * u.deg)\n    npt.assert_almost_equal(wrapped_a.degree, [160, -170, 10, -10])\n\n    a.wrap_at(180 * u.deg, inplace=True)\n    npt.assert_almost_equal(a.degree, [160, -170, 10, -10])\n", "def test_angle_to_string():\n    \"\"\"\n    Test the to_string method of Angle class with various configurations.\n    \"\"\"\n    a = Angle(10.684, unit=u.deg)\n\n    assert a.to_string() == '10d41m02.4s'\n    assert a.to_string(decimal=True) == '10.684'\n    assert a.to_string(unit=u.rad, decimal=True, precision=4) == '0.1864'\n    assert a.to_string(sep=':', precision=2) == '10:41:02.40'\n    assert a.to_string(sep='dms', fields=2) == '10d41m'\n    assert a.to_string(alwayssign=True) == '+10d41m02.4s'\n    assert a.to_string(pad=True) == '10d41m02.4s'\n    assert a.to_string(format='unicode') == '10\u00b041\u20322.4\u2033'\n    assert a.to_string(format='latex') == '$10^{\\\\circ}41^{\\\\prime}2.4^{\\\\prime\\\\prime}$'\n    assert a.to_string(format='latex_inline') == '$10^{\\\\circ}41^{\\\\prime}2.4^{\\\\prime\\\\prime}$'\n\n    a = Angle([-20.0, 150.0, 350.0], unit=u.deg)\n    result = a.to_string(decimal=True)\n    expected = np.array(['-20.', '150.', '350.'])\n    npt.assert_array_equal(result, expected)\n", "def test_longitude_wrap_at():\n    \"\"\"\n    Test wrapping of Longitude objects at specified angles.\n    \"\"\"\n    lon = Longitude([0, 90, 180, 270, 360, -90] * u.deg)\n    \n    # Wrap at 180 degrees\n    wrapped_lon = lon.wrap_at(180 * u.deg)\n    npt.assert_almost_equal(wrapped_lon.degree, [0, 90, 180, -90, 0, -90])\n\n    # Wrap at 360 degrees\n    wrapped_lon = lon.wrap_at(360 * u.deg)\n    npt.assert_almost_equal(wrapped_lon.degree, [0, 90, 180, 270, 360, 270])\n\n    # Wrap at 540 degrees\n    wrapped_lon = lon.wrap_at(540 * u.deg)\n    npt.assert_almost_equal(wrapped_lon.degree, [0, 90, 180, 270, 360, 450])\n\n    # In-place wrapping\n    lon.wrap_at(180 * u.deg, inplace=True)\n    npt.assert_almost_equal(lon.degree, [0, 90, 180, -90, 0, -90])\n", "def test_wrap_angle():\n    \"\"\"\n    Test the wrap_at method of the Angle class.\n    \"\"\"\n    a = Angle([10, -20, 350] * u.deg)\n\n    # Test wrapping at 360 degrees\n    wrapped_a = a.wrap_at(360 * u.deg)\n    npt.assert_almost_equal(wrapped_a.degree, [10, 340, 350])\n\n    # Test wrapping at 180 degrees\n    wrapped_a.wrap_at(180 * u.deg, inplace=True)\n    npt.assert_almost_equal(wrapped_a.degree, [10, -20, -10])\n\n    # Test wrapping with other units\n    a = Angle([1, -0.5, 1.5] * u.hourangle)\n    wrapped_a = a.wrap_at(24 * u.hour)\n    npt.assert_almost_equal(wrapped_a.hourangle, [1, 23.5, 1.5])\n\n    wrapped_a.wrap_at(12 * u.hour, inplace=True)\n    npt.assert_almost_equal(wrapped_a.hourangle, [1, -0.5, -10.5])\n"], "sample_7": ["def test_repr_html_output(Column):\n    \"\"\"Test that the _repr_html_ method produces the correct HTML output for Column and MaskedColumn\"\"\"\n    c = Column([1, 2, 3], name='a')\n    html_output = c._repr_html_()\n    expected_output = \"&lt;Column name='a' dtype='int64' length=3&gt;<br>1<br>2<br>3\"\n    assert html_output == expected_output\n\n    if Column is table.MaskedColumn:\n        c = Column([1, 2, 3], name='a', mask=[False, True, False])\n        html_output = c._repr_html_()\n        expected_output = \"&lt;MaskedColumn name='a' dtype='int64' length=3&gt;<br>1<br>--<br>3\"\n        assert html_output == expected_output\n", "def test_column_info_dict_representation():\n    \"\"\"Test the _represent_as_dict method in ColumnInfo class to ensure it correctly represents column attributes.\"\"\"\n\n    # Create a Column with various attributes\n    col = table.Column(data=[1, 2, 3], name='a', description='desc', unit='m', format=\"%i\", meta={'key': 'value'})\n    info_dict = col.info._represent_as_dict()\n\n    # Check if all attributes are correctly represented in the dict\n    assert info_dict['name'] == 'a'\n    assert info_dict['description'] == 'desc'\n    assert info_dict['unit'] == 'm'\n    assert info_dict['format'] == '%i'\n    assert info_dict['meta'] == {'key': 'value'}\n\n    # Test with structured columns\n    dtype = np.dtype([('field1', 'f8'), ('field2', 'i4')])\n    structured_col = table.Column(data=np.zeros(3, dtype=dtype), name='structured', unit=StructuredUnit({'field1': 'm', 'field2': 's'}, dtype))\n    structured_info_dict = structured_col.info._represent_as_dict()\n\n    assert 'data' in structured_info_dict\n    assert 'shape' not in structured_info_dict\n    assert 'unit' in structured_info_dict\n    assert structured_info_dict['unit'] == StructuredUnit({'field1': 'm', 'field2': 's'}, dtype)\n", "def test_col_copy_with_indices():\n    \"\"\"\n    Test `col_copy` function to ensure it correctly copies a column with indices.\n    \"\"\"\n    from astropy.table import Table, Column, MaskedColumn\n    from astropy.table import index\n\n    # Create a table with a column\n    tbl = Table()\n    col = Column([1, 2, 3, 4, 5], name='a')\n    tbl.add_column(col)\n    \n    # Add an index to the column\n    tbl.add_index('a')\n    assert len(tbl.indices) == 1\n    \n    # Copy the column with indices\n    col_copy_with_idx = col_copy(tbl['a'])\n    assert col_copy_with_idx.name == 'a'\n    assert np.all(col_copy_with_idx == [1, 2, 3, 4, 5])\n    assert hasattr(col_copy_with_idx, 'info')\n    assert col_copy_with_idx.info.indices == tbl['a'].info.indices\n    \n    # Copy the column without indices\n    col_copy_no_idx = col_copy(tbl['a'], copy_indices=False)\n    assert col_copy_no_idx.name == 'a'\n    assert np.all(col_copy_no_idx == [1, 2, 3, 4, 5])\n    assert hasattr(col_copy_no_idx, 'info')\n    assert col_copy_no_idx.info.indices == []\n", "def test_col_copy_with_indices():\n    \"\"\"\n    Test the col_copy function to ensure it correctly copies columns with and without indices.\n    \"\"\"\n    from astropy.table import Table, Column\n\n    # Create a table with a column and add an index\n    t = Table()\n    t['a'] = Column([1, 2, 3, 4, 5])\n    t.add_index('a')\n\n    # Copy the column without indices\n    col_no_indices = col_copy(t['a'], copy_indices=False)\n    assert isinstance(col_no_indices, Column)\n    assert col_no_indices.indices == []\n\n    # Copy the column with indices\n    col_with_indices = col_copy(t['a'], copy_indices=True)\n    assert isinstance(col_with_indices, Column)\n    assert len(col_with_indices.indices) == 1\n    assert col_with_indices.indices[0].colnames == ['a']\n\n    # Verify that modifying the copy does not affect the original column\n    col_with_indices[0] = 10\n    assert t['a'][0] == 1  # Original should remain unchanged\n", "def test_column_copy_with_info():\n    \"\"\"\n    Test that copying a column with info attributes retains the correct information.\n    \"\"\"\n    col = table.Column([1, 2, 3], name='a', unit='m', format='%d', description='test column', meta={'key': 'value'})\n    col.info.indices = [0, 1, 2]\n    col_copy = col.copy()\n\n    assert col_copy.name == col.name\n    assert col_copy.unit == col.unit\n    assert col_copy.format == col.format\n    assert col_copy.description == col.description\n    assert col_copy.meta == col.meta\n    assert col_copy.info.indices == col.info.indices\n    assert col_copy.info.unit == col.info.unit\n    assert col_copy.info.description == col.info.description\n    assert col_copy.info.meta == col.info.meta\n"], "sample_8": ["    def test_datetime_as_string(self):\n        # Create a masked array of datetime64\n        dt_array = np.array(['2020-01-01', '2021-01-01'], dtype='datetime64')\n        mask = np.array([False, True])\n        masked_dt_array = Masked(dt_array, mask=mask)\n\n        result = datetime_as_string(masked_dt_array)\n        expected_data = np.datetime_as_string(dt_array)\n        expected_mask = mask\n\n        assert_array_equal(result[0], expected_data)\n        assert_array_equal(result[1], expected_mask)\n        assert result[2] is None\n", "    def test_get_data_and_masks(self):\n        from astropy.utils.masked import Masked\n        a = Masked(np.array([1, 2, 3]), mask=[False, True, False])\n        b = Masked(np.array([4, 5, 6]), mask=[True, False, True])\n        data, masks = _get_data_and_masks(a, b)\n        expected_data = (np.array([1, 2, 3]), np.array([4, 5, 6]))\n        expected_masks = (np.array([False, True, False]), np.array([True, False, True]))\n        assert np.array_equal(data[0], expected_data[0])\n        assert np.array_equal(data[1], expected_data[1])\n        assert np.array_equal(masks[0], expected_masks[0])\n        assert np.array_equal(masks[1], expected_masks[1])\n", "    def test_dispatched_functions(self, func, params):\n        data = np.array(params[0])\n        mask = np.array([False] * len(data))\n        masked_data = Masked(data, mask)\n\n        result = func(masked_data)\n\n        if func == np.nan_to_num and 'copy' in result.__code__.co_varnames:\n            assert_array_equal(result[0], np.nan_to_num(data))\n            assert_array_equal(result[1], mask)\n        else:\n            assert_array_equal(result[0], func(data))\n            assert_array_equal(result[1], mask)\n", "    def test_datetime_as_string(self):\n        from astropy.utils.masked.function_helpers import datetime_as_string\n        dtypes = ['datetime64[s]', 'datetime64[ms]', 'datetime64[us]']\n        for dtype in dtypes:\n            arr = np.array(['2005-02-25', '2005-02-26'], dtype=dtype)\n            mask = [False, True]\n            masked_arr = Masked(arr, mask)\n            result = datetime_as_string(masked_arr)\n            expected_data = np.datetime_as_string(arr)\n            expected_mask = np.array(mask, copy=True)\n            assert_array_equal(result[0], expected_data)\n            assert_array_equal(result[1], expected_mask)\n", "    def test_dispatched_function(self, func, input_data, expected_data, expected_mask):\n        result = func(input_data)\n        assert isinstance(result, Masked)\n        assert_array_equal(result.unmasked, expected_data)\n        assert_array_equal(result.mask, expected_mask)\n"], "sample_9": ["def test_htmlinputter_custom_parser():\n    \"\"\"\n    Test HTMLInputter with custom parser.\n    \"\"\"\n\n    table_in = ['<table>',\n                '<tr><th>A</th><th>B</th></tr>',\n                '<tr><td>1</td><td>2</td></tr>',\n                '<tr><td>3</td><td>4</td></tr>',\n                '</table>']\n\n    inputter = html.HTMLInputter()\n    inputter.html = {'parser': 'html.parser'}\n\n    expected = ['<tr><th>A</th><th>B</th></tr>',\n                '<tr><td>1</td><td>2</td></tr>',\n                '<tr><td>3</td><td>4</td></tr>']\n                \n    assert [str(x) for x in inputter.process_lines(table_in)] == expected\n", "def test_htmlheader_set_cols_from_names():\n    \"\"\"\n    Test to ensure that _set_cols_from_names method of HTMLHeader\n    correctly sets columns from header names, handling multicolumns.\n    \"\"\"\n\n    header = html.HTMLHeader()\n    \n    # Test single column names\n    header.names = [\"A\", \"B\", \"C\"]\n    header._set_cols_from_names()\n    assert len(header.cols) == 3\n    assert all(isinstance(col, core.Column) for col in header.cols)\n    assert header.names == [\"A\", \"B\", \"C\"]\n\n    # Test multicolumn names\n    header.names = [(\"A\", 2), \"B\"]\n    header._set_cols_from_names()\n    assert len(header.cols) == 3\n    assert header.cols[0].name == \"A\"\n    assert header.cols[0].colspan == 2\n    assert header.cols[1].name == \"\"\n    assert header.cols[2].name == \"B\"\n    assert header.names == [\"A\", \"\", \"B\"]\n", "def test_htmlheader_set_cols_from_names():\n    \"\"\"\n    Test to ensure that HTMLHeader correctly sets columns from header names,\n    handling multicolumns appropriately.\n    \"\"\"\n\n    header = html.HTMLHeader()\n\n    # Simple header names\n    header.names = ['A', 'B', 'C']\n    header._set_cols_from_names()\n    assert len(header.cols) == 3\n    assert header.names == ['A', 'B', 'C']\n    assert all(isinstance(col, core.Column) for col in header.cols)\n    assert all(col.name in ['A', 'B', 'C'] for col in header.cols)\n\n    # Multicolumn header names\n    header.names = [('X', 2), 'Y']\n    header._set_cols_from_names()\n    assert len(header.cols) == 3\n    assert header.names == ['X', '', 'Y']\n    assert isinstance(header.cols[0], core.Column) and header.cols[0].colspan == 2\n    assert isinstance(header.cols[1], core.Column) and header.cols[1].name == ''\n    assert isinstance(header.cols[2], core.Column) and header.cols[2].name == 'Y'\n\n    # Mixed multicolumn and single column names\n    header.names = [('A', 3), 'B', ('C', 2)]\n    header._set_cols_from_names()\n    assert len(header.cols) == 6\n    assert header.names == ['A', '', '', 'B', 'C', '']\n    assert isinstance(header.cols[0], core.Column) and header.cols[0].colspan == 3\n    assert isinstance(header.cols[1], core.Column) and header.cols[1].name == ''\n    assert isinstance(header.cols[2], core.Column) and header.cols[2].name == ''\n    assert isinstance(header.cols[3], core.Column) and header.cols[3].name == 'B'\n    assert isinstance(header.cols[4], core.Column) and header.cols[4].colspan == 2\n    assert isinstance(header.cols[5], core.Column) and header.cols[5].name == ''\n", "def test_htmlheader_set_cols_from_names():\n    \"\"\"\n    Test to ensure that HTMLHeader correctly sets columns from header names,\n    including handling multicolumns appropriately.\n    \"\"\"\n    header = html.HTMLHeader()\n    header.names = ['A', ('B', 2), 'C']\n\n    # Manually calling the method to set columns from names\n    header._set_cols_from_names()\n\n    # Check that the columns are correctly set\n    assert len(header.cols) == 4\n    assert header.cols[0].name == 'A'\n    assert header.cols[1].name == 'B'\n    assert header.cols[1].colspan == 2\n    assert header.cols[2].name == ''\n    assert header.cols[3].name == 'C'\n    assert header.names == ['A', 'B', '', 'C']\n", "def test_htmlheader_set_cols_from_names():\n    \"\"\"\n    Test to ensure that HTMLHeader correctly sets columns from names,\n    handling multicolumns appropriately.\n    \"\"\"\n\n    header = html.HTMLHeader()\n    \n    # Test single column names\n    header.names = ['A', 'B', 'C']\n    header._set_cols_from_names()\n    assert [col.name for col in header.cols] == ['A', 'B', 'C']\n\n    # Test multicolumn names\n    header.names = [('A', '2'), 'B', ('C', '3')]\n    header._set_cols_from_names()\n    assert [col.name for col in header.cols] == ['A', '', 'B', 'C', '', '']\n    assert header.cols[0].colspan == 2\n    assert header.cols[3].colspan == 3\n\n    # Test multicolumn names with mixed valid and dummy columns\n    header.names = [('A', '2'), 'B', ('C', '2'), 'D']\n    header._set_cols_from_names()\n    assert [col.name for col in header.cols] == ['A', '', 'B', 'C', '', 'D']\n    assert header.cols[0].colspan == 2\n    assert header.cols[3].colspan == 2\n"], "sample_10": ["def test_convert_data_to_col():\n    t = table.Table([[1, 2, 3], [4, 5, 6]], names=['a', 'b'])\n    data = [10, 20, 30]\n    col = t._convert_data_to_col(data, name='c')\n    assert isinstance(col, table.Column)\n    assert np.all(col == data)\n    assert col.info.name == 'c'\n\n    # Test converting a masked array\n    mdata = np.ma.array(data, mask=[False, True, False])\n    mcol = t._convert_data_to_col(mdata, name='d')\n    assert isinstance(mcol, table.MaskedColumn)\n    assert np.all(mcol == data)\n    assert np.all(mcol.mask == [False, True, False])\n    assert mcol.info.name == 'd'\n\n    # Test converting a Quantity\n    qdata = [1, 2, 3] * u.m\n    qcol = t._convert_data_to_col(qdata, name='e')\n    assert isinstance(qcol, u.Quantity)\n    assert np.all(qcol.value == [1, 2, 3])\n    assert qcol.unit == u.m\n    assert qcol.info.name == 'e'\n\n    # Test converting a scalar\n    scalar = 100\n    scol = t._convert_data_to_col(scalar, name='f')\n    assert isinstance(scol, table.Column)\n    assert np.all(scol == [100, 100, 100])\n    assert scol.info.name == 'f'\n\n    # Test converting an object array\n    obj_array = np.array([1, 'string', 3], dtype=object)\n    ocol = t._convert_data_to_col(obj_array, name='g')\n    assert isinstance(ocol, table.Column)\n    assert np.all(ocol == obj_array)\n    assert ocol.info.name == 'g'\n", "    def _setup(self, table_types):\n        self._table_type = table_types.Table\n        self._column_type = table_types.Column\n", "    def test_remove_indices(self, table_types):\n        \"\"\"Test removing indices from a column\"\"\"\n        self._setup(table_types)\n        t = table_types.Table([self.a, self.b])\n        t.add_index('a')\n        t.add_index('b')\n        assert len(t.indices) == 2\n\n        # Remove index from column 'a'\n        t.remove_indices('a')\n        assert len(t.indices) == 1\n        assert t.indices[0].columns[0].info.name == 'b'\n\n        # Attempt to remove non-existing index should not raise error\n        t.remove_indices('a')\n        assert len(t.indices) == 1\n\n        # Remove index from column 'b' (remaining index)\n        t.remove_indices('b')\n        assert len(t.indices) == 0\n", "def test_column_renaming_with_pprint_exclude_include():\n    \"\"\"Test renaming columns that are in pprint_exclude_names and pprint_include_names\"\"\"\n    t = table.Table([[1, 2], [3, 4]], names=['a', 'b'])\n    t.pprint_exclude_names.add('a')\n    t.pprint_include_names.add('b')\n    assert 'a' in t.pprint_exclude_names()\n    assert 'b' in t.pprint_include_names()\n    \n    t.rename_column('a', 'c')\n    t.rename_column('b', 'd')\n    \n    assert 'a' not in t.pprint_exclude_names()\n    assert 'c' in t.pprint_exclude_names()\n    assert 'b' not in t.pprint_include_names()\n    assert 'd' in t.pprint_include_names()\n", "    def test_insert_row_at_start(self, table_types):\n        \"\"\"Insert a row at the start of the table.\"\"\"\n        self._setup(table_types)\n        t = table_types.Table([self.a, self.b, self.c])\n        t.insert_row(0, [10, 20, '30'])\n        assert len(t) == 4\n        assert np.all(t['a'] == np.array([10, 1, 2, 3]))\n        assert np.allclose(t['b'], np.array([20.0, 4.0, 5.1, 6.2]))\n        assert np.all(t['c'] == np.array(['30', '7', '8', '9']))\n"], "sample_12": ["def test_angle_wrap_at_in_place_edge_cases():\n    \"\"\"\n    Test wrap_at method in place for edge cases, especially around the wrap angle.\n    \"\"\"\n    a = Angle([360, -360, 720, -720, 1080, -1080, 0, 1, -1, 359, -359] * u.deg)\n    a.wrap_at(360 * u.deg, inplace=True)\n    assert np.all(a.degree == [0, 0, 0, 0, 0, 0, 0, 1, 359, 359, 1])\n\n    a = Angle([360, -360, 720, -720, 1080, -1080, 0, 1, -1, 179, -179] * u.deg)\n    a.wrap_at(180 * u.deg, inplace=True)\n    assert np.all(a.degree == [0, 0, 0, 0, 0, 0, 0, 1, -1, 179, -179])\n\n    a = Angle([0, 180, -180, 360, -360, 540, -540, 720, -720, 900, -900] * u.deg)\n    a.wrap_at(180 * u.deg, inplace=True)\n    assert np.all(a.degree == [0, 180, -180, 0, 0, -180, 180, 0, 0, 180, -180])\n", "def test_angle_to_string_format():\n    \"\"\"Test the to_string method with various formats for Angle class\"\"\"\n    a = Angle(10.2345, u.deg)\n\n    # Test default format\n    assert a.to_string() == \"10d14m04.2s\"\n\n    # Test decimal format\n    assert a.to_string(decimal=True) == \"10.2345\"\n\n    # Test different units\n    assert a.to_string(unit=u.hour) == \"0h40m56.28s\"\n    assert a.to_string(unit=u.radian) == \"0.178567rad\"\n\n    # Test with different separators\n    assert a.to_string(sep=':') == \"10:14:04.2\"\n    assert a.to_string(unit=u.hour, sep=['h', 'm', 's']) == \"0h40m56.28s\"\n\n    # Test with precision\n    assert a.to_string(precision=1) == \"10d14m04.2s\"\n    assert a.to_string(unit=u.hour, precision=3) == \"0h40m56.280s\"\n\n    # Test with alwayssign\n    assert a.to_string(alwayssign=True) == \"+10d14m04.2s\"\n    assert a.to_string(alwayssign=True, unit=u.hour) == \"+0h40m56.28s\"\n\n    # Test with pad\n    assert a.to_string(pad=True) == \"10d14m04.200s\"\n    assert a.to_string(unit=u.hour, pad=True) == \"00h40m56.280s\"\n\n    # Test with fields\n    assert a.to_string(fields=2) == \"10d14m\"\n    assert a.to_string(unit=u.hour, fields=1) == \"0h\"\n\n    # Test with format 'latex'\n    assert a.to_string(format='latex') == \"10\\\\d14\\\\m04.2\\\\s\"\n    assert a.to_string(unit=u.hour, format='latex') == \"0\\\\h40\\\\m56.28\\\\s\"\n\n    # Test with format 'unicode'\n    assert a.to_string(format='unicode') == \"10\u00b014\u203204.2\u2033\"\n    assert a.to_string(unit=u.hour, format='unicode') == \"0\u02b040\u1d5056.28\u02e2\"\n\n    # Test with format 'latex_inline'\n    assert a.to_string(format='latex_inline') == \"10\\\\d", "def test_angle_wrap_at_edge_cases():\n    \"\"\"\n    Test edge cases for the wrap_at method.\n    \"\"\"\n    a = Angle([-360, -180, 0, 180, 360] * u.deg)\n\n    # Test wrap at 0 degrees\n    assert np.all(a.wrap_at(0 * u.deg).degree == np.array([0., -180., 0., 180., 0.]))\n\n    # Test wrap at -180 degrees\n    assert np.all(a.wrap_at(-180 * u.deg).degree == np.array([0., -180., 0., -180., 0.]))\n\n    # Test wrap at 180 degrees\n    assert np.all(a.wrap_at(180 * u.deg).degree == np.array([0., -180., 0., 180., 0.]))\n\n    # Test wrap at 360 degrees\n    assert np.all(a.wrap_at(360 * u.deg).degree == np.array([0., -180., 0., 180., 0.]))\n\n    # Test wrap with a very large angle\n    assert np.all(a.wrap_at(1000 * u.deg).degree == np.array([-360., -540., 0., 180., 360.]))\n\n    # Test wrap with a very small angle\n    assert np.all(a.wrap_at(-1000 * u.deg).degree == np.array([-360., -540., 0., 180., 360.]))\n", "def test_angle_with_unit_conversion():\n    \"\"\"\n    Test for creating Angle with various units and converting between them.\n    \"\"\"\n    # Create Angle in degrees and convert to radians\n    angle_deg = Angle(180, unit=u.deg)\n    angle_rad = angle_deg.to(u.rad)\n    assert_allclose(angle_rad.value, np.pi)\n    assert angle_rad.unit == u.rad\n\n    # Create Angle in radians and convert to degrees\n    angle_rad = Angle(np.pi, unit=u.rad)\n    angle_deg = angle_rad.to(u.deg)\n    assert_allclose(angle_deg.value, 180)\n    assert angle_deg.unit == u.deg\n\n    # Create Angle in hours and convert to degrees\n    angle_hour = Angle(12, unit=u.hour)\n    angle_deg = angle_hour.to(u.deg)\n    assert_allclose(angle_deg.value, 180)\n    assert angle_deg.unit == u.deg\n\n    # Create Angle in degrees and convert to hours\n    angle_deg = Angle(180, unit=u.deg)\n    angle_hour = angle_deg.to(u.hour)\n    assert_allclose(angle_hour.value, 12)\n    assert angle_hour.unit == u.hourangle\n\n    # Test invalid unit conversion\n    with pytest.raises(u.UnitsError):\n        angle_deg.to(u.m)\n\n    # Test valid conversion with Quantity\n    angle_rad = Angle(Angle(3.14159 * u.rad))\n    assert_allclose(angle_rad.value, 3.14159)\n    assert angle_rad.unit == u.rad\n", "def test_angle_to_string_formats():\n    \"\"\"\n    Tests the `to_string` method for various formats and units\n    \"\"\"\n    angle = Angle('10d20m30.12345678s')\n\n    # Default format (degrees with sexagesimal notation)\n    assert angle.to_string() == '10d20m30.12345678s'\n\n    # Decimal degrees\n    assert angle.to_string(decimal=True) == '10.341701'\n\n    # Hours with sexagesimal notation\n    assert angle.to_string(unit=u.hourangle) == '0h41m22.7803044s'\n\n    # Decimal hours\n    assert angle.to_string(unit=u.hourangle, decimal=True) == '0.689661'\n\n    # Radians with decimal notation\n    assert angle.to_string(unit=u.radian, decimal=True) == '0.180468'\n\n    # Radians with default precision\n    assert angle.to_string(unit=u.radian) == '0.1804678rad'\n\n    # Custom separator\n    assert angle.to_string(sep=':') == '10:20:30.12345678'\n\n    # Custom separator with hours\n    assert angle.to_string(unit=u.hourangle, sep=':') == '0:41:22.7803044'\n\n    # Padding with sexagesimal notation\n    assert angle.to_string(pad=True) == '10d20m30.12345678s'\n    assert angle.to_string(unit=u.hourangle, pad=True) == '00h41m22.7803044s'\n    \n    # Precision control\n    assert angle.to_string(precision=2) == '10d20m30.12s'\n    assert angle.to_string(unit=u.hourangle, precision=2) == '0h41m22.78s'\n    assert angle.to_string(unit=u.radian, precision=2) == '0.18rad'\n    \n    # Latex format\n    assert angle.to_string(format='latex') == '10^\\circ20{}^\\prime30.12345678{}^{\\prime\\prime}'\n    assert angle.to_string(unit=u.hourangle, format='latex') == '0^{\\mathrm{h}}41^{\\mathrm{m}}22.7803044^{\\mathrm{s}}'\n    assert angle.to_string(unit=u.radian, format='latex') == '0.1804678rad'\n    \n    # Unicode format\n    assert angle.to_string(format='unicode')"], "sample_13": ["def test_angle_arithmetic_operations():\n    \"\"\"\n    Test arithmetic operations between Angle objects and with scalars\n    \"\"\"\n    a1 = Angle(30, unit=u.deg)\n    a2 = Angle(45, unit=u.deg)\n    \n    # Test addition\n    a3 = a1 + a2\n    assert isinstance(a3, Angle)\n    assert_allclose(a3.degree, 75)\n    \n    # Test subtraction\n    a4 = a1 - a2\n    assert isinstance(a4, Angle)\n    assert_allclose(a4.degree, -15)\n    \n    # Test multiplication by scalar\n    a5 = a1 * 2\n    assert isinstance(a5, Angle)\n    assert_allclose(a5.degree, 60)\n    \n    # Test division by scalar\n    a6 = a1 / 2\n    assert isinstance(a6, Angle)\n    assert_allclose(a6.degree, 15)\n    \n    # Test negation\n    a7 = -a1\n    assert isinstance(a7, Angle)\n    assert_allclose(a7.degree, -30)\n    \n    # Test addition with Quantity\n    a8 = a1 + 15 * u.deg\n    assert isinstance(a8, Angle)\n    assert_allclose(a8.degree, 45)\n    \n    # Test subtraction with Quantity\n    a9 = a1 - 15 * u.deg\n    assert isinstance(a9, Angle)\n    assert_allclose(a9.degree, 15)\n", "def test_angle_properties():\n    \"\"\"\n    Tests the properties of Angle class such as hour, hms, dms, and signed_dms\n    \"\"\"\n    angle = Angle(30, unit=u.degree)\n\n    # Test hour property\n    assert_allclose(angle.hour, 2)\n\n    # Test hms property\n    hms = angle.hms\n    assert isinstance(hms, tuple)\n    assert len(hms) == 3\n    assert hms.h == 2\n    assert hms.m == 0\n    assert_allclose(hms.s, 0)\n\n    # Test dms property\n    dms = angle.dms\n    assert isinstance(dms, tuple)\n    assert len(dms) == 3\n    assert dms.d == 30\n    assert dms.m == 0\n    assert_allclose(dms.s, 0)\n\n    # Test signed_dms property\n    signed_dms = angle.signed_dms\n    assert isinstance(signed_dms, tuple)\n    assert len(signed_dms) == 4\n    assert signed_dms.sign == 1\n    assert signed_dms.d == 30\n    assert signed_dms.m == 0\n    assert_allclose(signed_dms.s, 0)\n\n    # Test negative angle for signed_dms\n    neg_angle = Angle(-45.5, unit=u.degree)\n    signed_dms_neg = neg_angle.signed_dms\n    assert signed_dms_neg.sign == -1\n    assert signed_dms_neg.d == 45\n    assert signed_dms_neg.m == 30\n    assert_allclose(signed_dms_neg.s, 0)\n", "def test_angle_to_string_with_separators():\n    \"\"\"\n    Test the to_string method with various separators for both hour and degree angles.\n    \"\"\"\n    # Test hour angles\n    angle = Angle(\"3h36m29.7888000120s\")\n    assert angle.to_string(unit=u.hour, sep=':', precision=4) == '3:36:29.7888'\n    assert angle.to_string(unit=u.hour, sep=('h', 'm', 's'), precision=2) == '3h36m29.79s'\n    assert angle.to_string(unit=u.hour, sep='-', precision=0) == '4-00-00'\n    \n    # Test degree angles\n    angle = Angle(\"10d20m30.12345678s\")\n    assert angle.to_string(unit=u.degree, sep=':', precision=4) == '10:20:30.1235'\n    assert angle.to_string(unit=u.degree, sep=('d', 'm', 's'), precision=2) == '10d20m30.12s'\n    assert angle.to_string(unit=u.degree, sep='-', precision=0) == '10-21-00'\n\n    # Test radian angles\n    angle = Angle(np.pi / 6, unit=u.radian)\n    assert angle.to_string(unit=u.radian, decimal=True, precision=4) == '0.5236'\n    assert angle.to_string(unit=u.radian, decimal=True, precision=8) == '0.52359878'\n", "def test_angle_to_string_format():\n    \"\"\"\n    Test the to_string method with different formats including latex and unicode.\n    \"\"\"\n    angle = Angle(\"54.12412\", unit=u.degree)\n\n    # Test 'latex' format\n    assert angle.to_string(format='latex') == r'$54^\\circ07{}^\\prime26.832{}^{\\prime\\prime}$'\n    assert angle.to_string(unit=u.hourangle, format='latex') == r'$3^{\\mathrm{h}}36^{\\mathrm{m}}29.7888^{\\mathrm{s}}$'\n\n    # Test 'unicode' format\n    assert angle.to_string(format='unicode') == '54\u00b007\u203226.832\u2033'\n    assert angle.to_string(unit=u.hourangle, format='unicode') == '3\u02b036\u1d5029.7888\u02e2'\n\n    # Test with separators for latex and unicode\n    assert angle.to_string(unit=u.hourangle, sep=[\"h\", \"m\", \"s\"], format='latex') == r'$3^{\\mathrm{h}}36^{\\mathrm{m}}29.7888^{\\mathrm{s}}$'\n    assert angle.to_string(unit=u.hourangle, sep=[\"h\", \"m\", \"s\"], format='unicode') == '3\u02b036\u1d5029.7888\u02e2'\n", "def test_angle_arithmetic_with_constants():\n    \"\"\"\n    Test arithmetic operations between Angle objects and constants.\n    \"\"\"\n    a1 = Angle(45, unit=u.deg)\n    \n    # Test addition with a scalar\n    result = a1 + 15\n    assert isinstance(result, Angle)\n    assert result.degree == 60\n\n    # Test subtraction with a scalar\n    result = a1 - 15\n    assert isinstance(result, Angle)\n    assert result.degree == 30\n\n    # Test multiplication with a scalar\n    result = a1 * 2\n    assert isinstance(result, Angle)\n    assert result.degree == 90\n\n    # Test division by a scalar\n    result = a1 / 2\n    assert isinstance(result, Angle)\n    assert result.degree == 22.5\n\n    # Test in-place addition\n    a1 += 15\n    assert a1.degree == 60\n\n    # Test in-place subtraction\n    a1 -= 15\n    assert a1.degree == 45\n\n    # Test in-place multiplication\n    a1 *= 2\n    assert a1.degree == 90\n\n    # Test in-place division\n    a1 /= 2\n    assert a1.degree == 45\n"], "sample_14": ["def test_wrap_at_with_quantities():\n    \"\"\"\n    Test wrap_at method with Quantity objects as input.\n    \"\"\"\n    angle = Angle([10, 20, 30, 40, 350, 360, 370] * u.deg)\n    wrap_angle = 180 * u.deg\n    wrapped_angle = angle.wrap_at(wrap_angle)\n    expected_angle = Angle([10, 20, 30, 40, -10, 0, 10] * u.deg)\n    assert_allclose(wrapped_angle.degree, expected_angle.degree)\n\n    wrap_angle = 2 * np.pi * u.rad\n    wrapped_angle = angle.wrap_at(wrap_angle)\n    expected_angle = Angle([10, 20, 30, 40, 350, 0, 10] * u.deg)\n    assert_allclose(wrapped_angle.degree, expected_angle.degree)\n", "def test_angle_to_string_with_format_options():\n    \"\"\"\n    Test the to_string method with various format options.\n    \"\"\"\n\n    # Create an angle\n    angle = Angle('54.12412d')\n\n    # Test latex format\n    latex_str = angle.to_string(format='latex')\n    expected_latex = r'$54^\\circ07{}^\\prime26.832{}^{\\prime\\prime}$'\n    assert latex_str == expected_latex\n\n    # Test latex_inline format\n    latex_inline_str = angle.to_string(format='latex_inline')\n    expected_latex_inline = r'$54^\\circ07{}^\\prime26.832{}^{\\prime\\prime}$'\n    assert latex_inline_str == expected_latex_inline\n\n    # Test unicode format\n    unicode_str = angle.to_string(format='unicode')\n    expected_unicode = '54\u00b007\u203226.832\u2033'\n    assert unicode_str == expected_unicode\n\n    # Test invalid format\n    with pytest.raises(ValueError, match=\"Unknown format 'invalid_format'\"):\n        angle.to_string(format='invalid_format')\n", "def test_angle_to_string_format():\n    \"\"\"Test the to_string method with various formats\"\"\"\n    \n    angle = Angle(1, unit=u.deg)\n    \n    # Test default format\n    assert angle.to_string() == '1d00m00s'\n    \n    # Test 'latex' format\n    assert angle.to_string(format='latex') == '1^\\circ00{}^\\prime00{}^{\\prime\\prime}'\n    \n    # Test 'latex_inline' format\n    assert angle.to_string(format='latex_inline') == '1^\\circ00{}^\\prime00{}^{\\prime\\prime}'\n    \n    # Test 'unicode' format\n    assert angle.to_string(format='unicode') == '1\u00b000\u203200\u2033'\n    \n    # Test 'decimal' format\n    assert angle.to_string(decimal=True) == '1'\n    \n    # Test custom separator\n    assert angle.to_string(sep=':') == '1:00:00'\n    \n    # Test custom precision\n    assert angle.to_string(precision=4) == '1d00m00.0000s'\n    \n    # Test alwayssign\n    assert angle.to_string(alwayssign=True) == '+1d00m00s'\n    \n    # Test pad\n    assert angle.to_string(pad=True) == '01d00m00s'\n    \n    # Test fields\n    assert angle.to_string(fields=2) == '1d00m'\n    assert angle.to_string(fields=1) == '1d'\n    \n    # Test invalid format\n    with pytest.raises(ValueError, match=\"Unknown format 'invalid'\"):\n        angle.to_string(format='invalid')\n    \n    # Test different units\n    assert angle.to_string(unit=u.hourangle) == '0h04m00s'\n    assert angle.to_string(unit=u.radian, decimal=True) == '0.017453292519943295'\n    \n    # Test invalid separator for decimal\n    with pytest.raises(ValueError, match=\"'radian' can not be represented in sexagesimal notation\"):\n        angle.to_string(unit=u.radian, sep=':')\n", "def test_angle_wrap_at_zero():\n    \"\"\"Test that wrapping at 0 degrees works correctly.\"\"\"\n    a = Angle([-10, 0, 10, 180, 350, 370] * u.deg)\n    wrapped_angle = a.wrap_at(0 * u.deg)\n    expected_values = np.array([350., 0., 10., 180., 350., 10.])\n    assert np.allclose(wrapped_angle.degree, expected_values, atol=1e-10)\n", "def test_latitude_wrap_at():\n    \"\"\"\n    Test the wrap_at method for Latitude to ensure it wraps correctly.\n    Since Latitude is constrained between -90 and 90 degrees, it should not wrap.\n    \"\"\"\n    lat = Latitude(['80d', '-85d', '90d', '-90d'])\n    wrapped_lat = lat.wrap_at(180 * u.deg)\n    assert np.all(wrapped_lat.degree == lat.degree)\n    \n    with pytest.raises(ValueError):\n        lat.wrap_at(100 * u.deg)\n"], "sample_16": ["    def setup_method(self):\n        self.dtype = np.dtype([(\"a\", \"f8\"), (\"b\", \"f8\")])\n        self.data = np.array([(1.0, 2.0), (3.0, 4.0), (5.0, 6.0)], self.dtype)\n        self.unit = u.StructuredUnit((u.m, u.s), (\"a\", \"b\"))\n        self.q_data = self.data << self.unit\n", "    def setup_method(self):\n        self.q = np.arange(9.0).reshape(3, 3) * u.m\n", "    def test_special_functions(self):\n        # Test np.sqrt\n        result = np.sqrt(self.q)\n        expected = np.sqrt(self.q.value) * u.m**0.5\n        assert_array_equal(result, expected)\n\n        # Test np.square\n        result = np.square(self.q)\n        expected = np.square(self.q.value) * u.m**2\n        assert_array_equal(result, expected)\n\n        # Test np.sin\n        result = np.sin(self.q.to(u.rad))\n        expected = np.sin(self.q.to_value(u.rad)) * u.dimensionless_unscaled\n        assert_array_equal(result, expected)\n\n        # Test np.cos\n        result = np.cos(self.q.to(u.rad))\n        expected = np.cos(self.q.to_value(u.rad)) * u.dimensionless_unscaled\n        assert_array_equal(result, expected)\n", "    def test_fftshift_with_unit(self):\n        q = np.arange(8.0) * u.s\n        out = np.fft.fftshift(q)\n        expected = np.fft.fftshift(q.value) * u.s\n        assert_array_equal(out, expected)\n", "    def test_piecewise(self):\n        x = np.linspace(-2.5, 2.5, 6) * u.m\n        condlist = [x < 0, x >= 0]\n        funclist = [-1 * u.s, 1 * u.day]\n        out = np.piecewise(x, condlist, funclist)\n        expected = np.piecewise(x.value, [x.value < 0, x.value >= 0], [-1, 24 * 3600]) * u.s\n        assert np.all(out == expected)\n\n        condlist = [x < 1 * u.m, x >= 0]\n        funclist = [-1 * u.s, 1 * u.day, lambda x: 1 * u.hour]\n        out2 = np.piecewise(x, condlist, funclist)\n        expected2 = np.piecewise(x.value, [x.value < 1, x.value >= 0], [-1, 24 * 3600, 3600]) * u.s\n        assert np.all(out2 == expected2)\n\n        condlist = [x < 1 * u.m, x >= 0]\n        funclist = [0, 1 * u.percent, lambda x: 1 * u.one]\n        out3 = np.piecewise(x, condlist, funclist)\n        expected3 = np.piecewise(x.value, [x.value < 1, x.value >= 0], [0, 0.01, 1]) * u.one\n        assert np.all(out3 == expected3)\n\n        with pytest.raises(TypeError):  # no Quantity in condlist.\n            np.piecewise(x, [x], [0.0])\n\n        with pytest.raises(TypeError):  # no Quantity in condlist.\n            np.piecewise(x.value, [x], [0.0])\n"], "sample_17": ["    def setup_class(cls):\n        cls.pv_dtype = np.dtype([(\"p\", \"f8\"), (\"v\", \"f8\")])\n        cls.pv_t_dtype = np.dtype(\n            [(\"pv\", np.dtype([(\"pp\", \"f8\"), (\"vv\", \"f8\")])), (\"t\", \"f8\")]\n        )\n\n        cls.pv = np.array([(1.0, 0.25), (2.0, 0.5), (3.0, 0.75)], cls.pv_dtype)\n        cls.pv_t = np.array(\n            [((4.0, 2.5), 0.0), ((5.0, 5.0), 1.0), ((6.0, 7.5), 2.0)], cls.pv_t_dtype\n        )\n\n        cls.pv_unit = u.StructuredUnit((u.km, u.km / u.s), (\"p\", \"v\"))\n        cls.pv_t_unit = u.StructuredUnit((cls.pv_unit, u.s), (\"pv\", \"t\"))\n\n        cls.q_pv = cls.pv << cls.pv_unit\n        cls.q_pv_t = cls.pv_t << cls.pv_t_unit\n", "    def test_unwrap(self):\n        q = np.array([0.0, 3.5, 7.0, 10.5]) * u.radian\n        out = np.unwrap(q)\n        expected = np.unwrap(q.value) * u.radian\n        assert np.all(out == expected)\n", "    def setup_method(self):\n        self.q_int = np.arange(3) * u.m\n        self.q_float = np.arange(3.0) * u.m\n", "    def test_common_type(self):\n        a = np.array([1, 2, 3], dtype='f4')\n        b = np.array([1.0, 2.0, 3.0]) * u.m\n        result = np.common_type(a, b)\n        expected = np.result_type(a, b)\n        assert result == expected\n", "    def setup_method(self):\n        self.dtype = np.dtype([(\"field1\", np.float64), (\"field2\", np.int32)])\n        self.data = np.array([(1.5, 2), (3.5, 4)], dtype=self.dtype) * u.m\n        self.expected_unstructured = np.array([[1.5, 2], [3.5, 4]]) * u.m\n"], "sample_18": ["    def test_addition_with_scalar(self):\n        q1 = self.q + 10 * u.m / u.s\n        assert_no_info(q1)\n        ", "    def test_quantity_with_different_units(self):\n        q1 = u.Quantity(np.arange(1.0, 5.0), \"m\")\n        q2 = u.Quantity(np.arange(1.0, 5.0), \"cm\")\n\n        q_add = q1 + q2\n        assert_no_info(q_add)\n        assert q_add.unit.is_equivalent(u.m)\n\n        q_sub = q1 - q2\n        assert_no_info(q_sub)\n        assert q_sub.unit.is_equivalent(u.m)\n\n        q_mul = q1 * q2\n        assert_no_info(q_mul)\n        assert q_mul.unit.is_equivalent(u.m * u.cm)\n\n        q_div = q1 / q2\n        assert_no_info(q_div)\n        assert q_div.unit.is_equivalent(u.dimensionless_unscaled)\n", "    def test_flat_iterator(self):\n        q = self.q.copy()\n        flat_iter = q.flat\n        for val in flat_iter:\n            assert_no_info(val)\n        assert_info_equal(flat_iter.base, q)\n", "    def test_flat_iteration(self):\n        q_iter = self.q.flat\n        for q in q_iter:\n            assert_no_info(q)\n        for i in range(len(self.q.flat)):\n            assert_no_info(self.q.flat[i])\n        q_copy = self.q.copy()\n        q_copy.flat[0] = 10 * u.m / u.s\n        assert q_copy.flat[0] == 10 * u.m / u.s\n", "    def test_flat_iter(self):\n        q = self.q\n        flat_iter = q.flat\n        assert isinstance(flat_iter, u.QuantityIterator)\n        for i, q_elem in enumerate(flat_iter):\n            assert q_elem == q[i]\n            assert_no_info(q_elem)\n"], "sample_19": ["def test_parse_keysel():\n    \"\"\"\n    Test the _parse_keysel function for different keysel inputs\n    to ensure correct flag output.\n    \"\"\"\n    from astropy.wcs.wcs import _parse_keysel\n\n    # Test with keysel as None\n    keysel = None\n    expected_flag = -1\n    assert _parse_keysel(keysel) == expected_flag\n\n    # Test with keysel containing 'image'\n    keysel = ['image']\n    expected_flag = _wcs.WCSHDR_IMGHEAD\n    assert _parse_keysel(keysel) == expected_flag\n\n    # Test with keysel containing 'binary'\n    keysel = ['binary']\n    expected_flag = _wcs.WCSHDR_BIMGARR\n    assert _parse_keysel(keysel) == expected_flag\n\n    # Test with keysel containing 'pixel'\n    keysel = ['pixel']\n    expected_flag = _wcs.WCSHDR_PIXLIST\n    assert _parse_keysel(keysel) == expected_flag\n\n    # Test with keysel containing multiple elements\n    keysel = ['image', 'binary', 'pixel']\n    expected_flag = _wcs.WCSHDR_IMGHEAD | _wcs.WCSHDR_BIMGARR | _wcs.WCSHDR_PIXLIST\n    assert _parse_keysel(keysel) == expected_flag\n\n    # Test with keysel containing an invalid element\n    keysel = ['invalid']\n    with pytest.raises(ValueError, match=\"keysel must be a list of 'image', 'binary' and/or 'pixel'\"):\n        _parse_keysel(keysel)\n", "def test_invalid_tabular_parameters():\n    \"\"\"\n    Test for InvalidTabularParametersError exception.\n    \"\"\"\n    hdr = {\n        \"CTYPE1\": \"RA---TAN\",\n        \"CTYPE2\": \"DEC--TAN\",\n        \"CRPIX1\": 1,\n        \"CRPIX2\": 1,\n        \"CRVAL1\": 40.0,\n        \"CRVAL2\": 0.0,\n        \"CDELT1\": -0.1,\n        \"CDELT2\": 0.1,\n        \"NAXIS\": 2,\n        \"NAXIS1\": 1024,\n        \"NAXIS2\": 1024\n    }\n    with pytest.raises(wcs.InvalidTabularParametersError):\n        w = wcs.WCS(hdr)\n        w.all_pix2world([[1000, 1000]], 0)\n", "def test_wcs_to_from_header():\n    \"\"\"\n    Test that creating a WCS object from a header generated by WCS.to_header()\n    retains all the necessary WCS information.\n    \"\"\"\n    # Create a WCS object with various parameters\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.crval = [202.5, 47.2]\n    w.wcs.crpix = [512, 512]\n    w.wcs.cdelt = [-0.066667, 0.066667]\n    w.wcs.cunit = [\"deg\", \"deg\"]\n    w.wcs.set()\n\n    # Generate a FITS header from the WCS object\n    header = w.to_header()\n\n    # Create a new WCS object from the generated header\n    new_w = wcs.WCS(header)\n\n    # Check that the new WCS object retains the original WCS information\n    assert new_w.naxis == w.naxis\n    assert_array_equal(new_w.wcs.ctype, w.wcs.ctype)\n    assert_array_equal(new_w.wcs.crval, w.wcs.crval)\n    assert_array_equal(new_w.wcs.crpix, w.wcs.crpix)\n    assert_array_equal(new_w.wcs.cdelt, w.wcs.cdelt)\n    assert_array_equal(new_w.wcs.cunit, w.wcs.cunit)\n\n    # Check that transformations produce the same results\n    original_pix = np.array([[100, 100], [200, 200], [300, 300]])\n    original_world = w.wcs_pix2world(original_pix, 0)\n    new_world = new_w.wcs_pix2world(original_pix, 0)\n    assert_allclose(original_world, new_world, atol=1e-8, rtol=1e-10)\n", "def test_crpix_not_at_crval():\n    \"\"\"\n    Test to ensure that CRPIX not being at CRVAL does not cause issues in transformations.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.crval = [30, 40]\n    w.wcs.crpix = [100, 200]\n    w.wcs.cdelt = [-0.1, 0.1]\n    w.wcs.set()\n\n    # Generate some pixel coordinates\n    pixel_coords = np.array([[100, 200], [150, 250], [200, 300]])\n    \n    # Perform the forward transformation\n    world_coords = w.wcs_pix2world(pixel_coords, 1)\n    \n    # Perform the inverse transformation\n    pixel_coords_back = w.wcs_world2pix(world_coords, 1)\n    \n    # Check if the returned pixel coordinates match the original ones\n    assert_allclose(pixel_coords, pixel_coords_back, atol=1e-6)\n", "def test_wcs_copy():\n    \"\"\"\n    Test that a shallow copy of a WCS object retains the same WCS settings\n    and that a deep copy results in independent WCS objects.\n    \"\"\"\n    header = get_pkg_data_contents(\"data/sip.fits\", encoding=\"binary\")\n    with pytest.warns(wcs.FITSFixedWarning):\n        original_wcs = wcs.WCS(header)\n\n    # Perform shallow copy\n    shallow_copy = original_wcs.copy()\n    assert shallow_copy.wcs.compare(original_wcs.wcs)\n\n    # Perform deep copy\n    deep_copy = original_wcs.deepcopy()\n    assert deep_copy.wcs.compare(original_wcs.wcs)\n\n    # Modify the deep copy and ensure it does not affect the original\n    deep_copy.wcs.crval = [45.0, 45.0]\n    assert not deep_copy.wcs.compare(original_wcs.wcs)\n    assert original_wcs.wcs.crval != [45.0, 45.0]\n"], "sample_20": ["def test_is_fits():\n    class DummyFile:\n            self.content = content\n            self.position = 0\n        \n            return self.position\n        \n            data = self.content[self.position:self.position + size]\n            self.position += size\n            return data\n        \n            self.position = pos\n\n    # Test with file content matching FITS_SIGNATURE\n    file_content = b\"SIMPLE  =                    T\" + b\" \" * 10\n    dummy_file = DummyFile(file_content)\n    assert is_fits(None, None, dummy_file)\n    \n    # Test with file content not matching FITS_SIGNATURE\n    file_content = b\"NOT_A_FITS_FILE\" + b\" \" * 14\n    dummy_file = DummyFile(file_content)\n    assert not is_fits(None, None, dummy_file)\n    \n    # Test with filepath ending in .fits\n    assert is_fits(None, \"example.fits\", None)\n    \n    # Test with filepath ending in .fit.gz\n    assert is_fits(None, \"example.fit.gz\", None)\n    \n    # Test with filepath ending in .fts\n    assert is_fits(None, \"example.fts\", None)\n    \n    # Test with filepath not ending in a FITS extension\n    assert not is_fits(None, \"example.txt\", None)\n    \n    # Test with HDUList as input\n    assert is_fits(None, None, None, HDUList())\n    \n    # Test with TableHDU as input\n    assert is_fits(None, None, None, TableHDU())\n    \n    # Test with BinTableHDU as input\n    assert is_fits(None, None, None, BinTableHDU())\n    \n    # Test with GroupsHDU as input\n    assert is_fits(None, None, None, GroupsHDU())\n", "def test_read_table_fits_with_hdu_list(tmp_path):\n    \"\"\"\n    Test reading a Table from an HDUList input with multiple HDUs, ensuring the first table HDU is read by default.\n    \"\"\"\n    # Create HDUList with a PrimaryHDU and two BinTableHDUs\n    primary_hdu = PrimaryHDU()\n    data1 = np.array([(1, 2.5), (3, 4.5)], dtype=[(\"a\", int), (\"b\", float)])\n    data2 = np.array([(5, 6.5), (7, 8.5)], dtype=[(\"x\", int), (\"y\", float)])\n    table_hdu1 = BinTableHDU(data1, name=\"table1\")\n    table_hdu2 = BinTableHDU(data2, name=\"table2\")\n    hdulist = HDUList([primary_hdu, table_hdu1, table_hdu2])\n\n    # Write the HDUList to a file\n    filename = tmp_path / \"test_hdu_list.fits\"\n    hdulist.writeto(filename, overwrite=True)\n\n    # Read the table from the file\n    with pytest.warns(\n        AstropyUserWarning,\n        match=r\"hdu= was not specified but multiple tables are present, reading in first available table \\(hdu=1\\)\",\n    ):\n        table = Table.read(filename)\n\n    # Verify that the table data matches the expected first table HDU\n    assert equal_data(table, data1)\n", "def test_read_table_fits_with_astropy_native(tmp_path):\n    \"\"\"\n    Test reading a FITS file with the `astropy_native=True` option to ensure\n    that columns are converted to their appropriate astropy core objects.\n    \"\"\"\n    filename = tmp_path / \"test_astropy_native.fits\"\n\n    # Create a table with a mixin column (Time) and write to FITS file\n    times = Time(['2000-01-01T00:00:00', '2000-01-01T01:00:00'])\n    t1 = Table([times], names=['time'])\n    t1.write(filename, overwrite=True)\n\n    # Read the table back with astropy_native=True\n    t2 = Table.read(filename, astropy_native=True)\n\n    # Check that the time column is a Time object\n    assert isinstance(t2['time'], Time)\n    assert np.all(t2['time'] == times)\n\n    # Check other metadata\n    assert t1.meta == t2.meta\n", "def test_read_table_fits_with_pathlib_path(tmp_path):\n    \"\"\"Test reading a FITS table using a pathlib.Path object as input.\"\"\"\n    filename = tmp_path / \"test_pathlib_read.fits\"\n    t1 = Table(self.data)\n    t1.write(filename, overwrite=True)\n    \n    # Convert to pathlib.Path and read\n    t2 = Table.read(filename)\n    assert equal_data(t1, t2)\n", "def test_invalid_units(self, tmp_path):\n    \"\"\"\n    Test that invalid units in a FITS file raise the appropriate warning.\n    \"\"\"\n    filename = tmp_path / \"test_invalid_units.fits\"\n    t = Table([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n    t[\"a\"].unit = \"invalid_unit\"\n    t[\"b\"].unit = \"m/s\"\n    \n    with pytest.warns(u.UnitsWarning, match=\"did not parse as fits unit\"):\n        t.write(filename, overwrite=True)\n\n    with pytest.warns(u.UnitsWarning, match=\"did not parse as fits unit\"):\n        t2 = Table.read(filename)\n\n    assert t2[\"a\"].unit is None  # invalid unit should not be assigned\n    assert t2[\"b\"].unit == \"m/s\"  # valid unit should be assigned correctly\n"], "sample_21": ["def test_invalid_qdp_file():\n    invalid_qdp = \"\"\"\n    ! Invalid QDP file example\n    This is not a valid QDP line\n    READ SERR 2\n    ! Another invalid line without proper format\n    \"\"\"\n    \n    with pytest.raises(ValueError, match=\"Unrecognized QDP line: This is not a valid QDP line\"):\n        _read_table_qdp(invalid_qdp, names=[\"MJD\", \"Rate\"], table_id=0)\n", "def test_interpret_err_lines():\n    # Testing _interpret_err_lines with various inputs\n\n    # Case 1: No error specifications and default column names\n    err_specs = None\n    ncols = 3\n    colnames = _interpret_err_lines(err_specs, ncols)\n    assert colnames == [\"col1\", \"col2\", \"col3\"]\n\n    # Case 2: Error specifications with given column names\n    err_specs = {'serr': [2], 'terr': [1]}\n    names = [\"MJD\", \"Rate\"]\n    ncols = 5\n    colnames = _interpret_err_lines(err_specs, ncols, names=names)\n    assert colnames == [\"MJD\", \"MJD_perr\", \"MJD_nerr\", \"Rate\", \"Rate_err\"]\n\n    # Case 3: Inconsistent number of columns should raise ValueError\n    with pytest.raises(ValueError, match=\"Inconsistent number of input colnames\"):\n        _interpret_err_lines(err_specs, 6, names=names)\n\n    # Case 4: Only symmetric errors with default column names\n    err_specs = {'serr': [1, 2]}\n    ncols = 4\n    colnames = _interpret_err_lines(err_specs, ncols)\n    assert colnames == [\"col1\", \"col1_err\", \"col2\", \"col2_err\"]\n\n    # Case 5: Only two-sided errors with default column names\n    err_specs = {'terr': [1]}\n    ncols = 3\n    colnames = _interpret_err_lines(err_specs, ncols)\n    assert colnames == [\"col1\", \"col1_perr\", \"col1_nerr\"]\n\n    # Case 6: Mixed errors without column names\n    err_specs = {'serr': [3], 'terr': [1, 2]}\n    ncols = 8\n    colnames = _interpret_err_lines(err_specs, ncols)\n    assert colnames == [\"col1\", \"col1_perr\", \"col1_nerr\", \"col2\", \"col2_perr\", \"col2_nerr\", \"col3\", \"col3_err\"]\n", "def test_interpret_err_lines():\n    # Test with no error specifications\n    colnames = _interpret_err_lines(None, 4, names=[\"a\", \"b\", \"c\", \"d\"])\n    assert colnames == [\"a\", \"b\", \"c\", \"d\"]\n\n    # Test with symmetric and two-sided error specifications\n    err_specs = {'serr': [2], 'terr': [1]}\n    colnames = _interpret_err_lines(err_specs, 5, names=[\"a\", \"b\", \"c\"])\n    assert colnames == [\"a\", \"a_perr\", \"a_nerr\", \"b\", \"b_err\"]\n\n    # Test with inconsistent number of input colnames\n    with pytest.raises(ValueError, match=\"Inconsistent number of input colnames\"):\n        _interpret_err_lines(err_specs, 6, names=[\"a\", \"b\", \"c\"])\n", "def test_interpret_err_lines():\n    colnames = [\"col1\", \"col2\", \"col3\", \"col1_err\", \"col2_perr\", \"col2_nerr\"]\n    err_specs = {\"serr\": [4], \"terr\": [2]}\n    ncols = 6\n    names = [\"col1\", \"col2\", \"col3\"]\n\n    interpreted_cols = _interpret_err_lines(err_specs, ncols, names)\n    expected_cols = [\"col1\", \"col2\", \"col2_perr\", \"col2_nerr\", \"col3\", \"col1_err\"]\n\n    assert interpreted_cols == expected_cols\n\n    # Test with no error specifications\n    interpreted_cols_no_errs = _interpret_err_lines(None, 3, names)\n    expected_cols_no_errs = [\"col1\", \"col2\", \"col3\"]\n\n    assert interpreted_cols_no_errs == expected_cols_no_errs\n\n    # Test with inconsistent number of input colnames\n    with pytest.raises(ValueError, match=\"Inconsistent number of input colnames\"):\n        _interpret_err_lines(err_specs, 5, names)\n\n    # Test with no names provided\n    interpreted_cols_no_names = _interpret_err_lines(err_specs, ncols)\n    expected_cols_no_names = [\"col1\", \"col2\", \"col2_perr\", \"col2_nerr\", \"col3\", \"col1_err\"]\n\n    assert interpreted_cols_no_names == expected_cols_no_names\n", "def test_interpret_err_lines():\n    # Test with no error specifications\n    colnames = _interpret_err_lines(None, 2, names=[\"MJD\", \"Rate\"])\n    assert colnames == [\"MJD\", \"Rate\"]\n\n    # Test with symmetric and two-sided errors\n    err_specs = {\"terr\": [1], \"serr\": [2]}\n    colnames = _interpret_err_lines(err_specs, 5, names=[\"MJD\", \"Rate\"])\n    assert colnames == [\"MJD\", \"MJD_perr\", \"MJD_nerr\", \"Rate\", \"Rate_err\"]\n\n    # Test with inconsistent number of columns\n    with pytest.raises(ValueError, match=\"Inconsistent number of input colnames\"):\n        _interpret_err_lines(err_specs, 6, names=[\"MJD\", \"Rate\"])\n"], "sample_22": ["def test_matrix_product():\n    # Test matrix product with two simple 2D matrices\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    expected_result = np.matmul(m1, m2)\n    assert_array_equal(matrix_product(m1, m2), expected_result)\n\n    # Test matrix product with three 2D matrices\n    m3 = np.array([[9, 10], [11, 12]])\n    expected_result = np.matmul(np.matmul(m1, m2), m3)\n    assert_array_equal(matrix_product(m1, m2, m3), expected_result)\n\n    # Test matrix product with stack of 2D matrices\n    m1_stack = np.array([m1, m1])\n    m2_stack = np.array([m2, m2])\n    expected_result_stack = np.array([np.matmul(m1, m2), np.matmul(m1, m2)])\n    assert_array_equal(matrix_product(m1_stack, m2_stack), expected_result_stack)\n\n    # Test the deprecation warning\n    with pytest.warns(AstropyDeprecationWarning, match=\"matrix_product is deprecated\"):\n        matrix_product(m1, m2)\n", "def test_matrix_product():\n    with pytest.warns(AstropyDeprecationWarning):\n        m1 = rotation_matrix(30 * u.deg, \"x\")\n        m2 = rotation_matrix(45 * u.deg, \"y\")\n        m3 = rotation_matrix(60 * u.deg, \"z\")\n\n        expected = m1 @ m2 @ m3\n        result = matrix_product(m1, m2, m3)\n\n        assert_allclose(result, expected, atol=1e-12)\n\n    # Test with a stack of matrices\n    stack_matrices = np.stack((m1, m2, m3))\n    expected_stack = np.stack([m1 @ m2, m2 @ m3, m3 @ m1])\n    result_stack = matrix_product(stack_matrices, stack_matrices.swapaxes(-2, -1))\n    assert_allclose(result_stack, expected_stack, atol=1e-12)\n", "def test_matrix_product():\n    \"\"\"Test the matrix_product function.\"\"\"\n    m1 = rotation_matrix(45 * u.deg, \"x\")\n    m2 = rotation_matrix(45 * u.deg, \"y\")\n    m3 = rotation_matrix(45 * u.deg, \"z\")\n\n    product = matrix_product(m1, m2, m3)\n    expected_product = np.matmul(np.matmul(m1, m2), m3)\n    assert_allclose(product, expected_product, atol=1e-12)\n\n    # Test with a stack of matrices\n    stack_matrices = np.stack([m1, m2, m3])\n    stack_product = matrix_product(stack_matrices[0], stack_matrices[1], stack_matrices[2])\n    expected_stack_product = np.matmul(np.matmul(stack_matrices[0], stack_matrices[1]), stack_matrices[2])\n    assert_allclose(stack_product, expected_stack_product, atol=1e-12)\n\n    # Test with deprecated warning\n    with pytest.warns(AstropyDeprecationWarning):\n        matrix_product(m1, m2)\n", "def test_matrix_product():\n    # Test simple matrix multiplication\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    expected_result = np.matmul(m1, m2)\n    assert_allclose(matrix_product(m1, m2), expected_result)\n\n    # Test matrix multiplication with more than two matrices\n    m3 = np.array([[9, 10], [11, 12]])\n    expected_result = np.matmul(np.matmul(m1, m2), m3)\n    assert_allclose(matrix_product(m1, m2, m3), expected_result)\n\n    # Test matrix multiplication with higher dimensional arrays\n    m4 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    m5 = np.array([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n    expected_result = np.matmul(m4, m5)\n    assert_allclose(matrix_product(m4, m5), expected_result)\n\n    # Test deprecated warning\n    with pytest.warns(AstropyDeprecationWarning):\n        matrix_product(m1, m2)\n", "def test_matrix_product():\n    \"\"\"Test the deprecated matrix_product function.\"\"\"\n    with pytest.warns(AstropyDeprecationWarning, match=r\".*`matrix_product` is deprecated.*\"):\n        m1 = rotation_matrix(45 * u.deg, \"x\")\n        m2 = rotation_matrix(45 * u.deg, \"y\")\n        m3 = rotation_matrix(45 * u.deg, \"z\")\n        \n        # Compute the product using the deprecated function\n        product_deprecated = matrix_product(m1, m2, m3)\n        \n        # Compute the product manually\n        product_manual = np.matmul(np.matmul(m1, m2), m3)\n        \n        # Check if they are equal\n        assert_allclose(product_deprecated, product_manual, atol=1e-12)\n"], "sample_23": ["def test_angle_wrap_at_inplace():\n    \"\"\"\n    Test wrapping angles in place with various wrap angles.\n    \"\"\"\n    angles = [\n        ([-20, 150, 350, 360], 180, [-20.0, 150.0, -10.0, 0.0]),\n        ([-20, 150, 350, 360], 360, [340.0, 150.0, 350.0, 0.0]),\n        ([-20, 150, 350, 360], 0, [-20.0, 150.0, -10.0, 0.0]),\n        ([-20, 150, 350, 360], -180, [-20.0, 150.0, -10.0, 0.0]),\n        ([-20, 150, 350, 360], -360, [-20.0, 150.0, -10.0, 0.0])\n    ]\n\n    for angle_values, wrap_angle, expected in angles:\n        a = Angle(angle_values * u.deg)\n        a.wrap_at(wrap_angle * u.deg, inplace=True)\n        assert np.all(a.degree == np.array(expected))\n", "def test_angle_to_string_with_format():\n    \"\"\"\n    Test the to_string method with different formats\n    \"\"\"\n\n    angle = Angle(\"54.12412\", unit=u.degree)\n\n    # Test latex format\n    latex_str = angle.to_string(format='latex')\n    assert latex_str == '54d07m26.832s'\n\n    # Test latex_inline format\n    latex_inline_str = angle.to_string(format='latex_inline')\n    assert latex_inline_str == '54d07m26.832s'\n\n    # Test unicode format\n    unicode_str = angle.to_string(format='unicode')\n    assert unicode_str == '54\u00b007\u203226.832\u2033'\n\n    # Test generic format\n    generic_str = angle.to_string(format=None)\n    assert generic_str == '54d07m26.832s'\n", "def test_is_within_bounds_edge_cases():\n    \"\"\"\n    Tests `is_within_bounds` method for edge cases\n    \"\"\"\n    # Angle exactly at the lower bound\n    a = Angle(\"0d\")\n    assert a.is_within_bounds(\"0d\", \"360d\") is True\n\n    # Angle exactly at the upper bound\n    a = Angle(\"360d\")\n    assert a.is_within_bounds(\"0d\", \"360d\") is False  # upper bound is exclusive\n\n    # Angle just below the lower bound\n    a = Angle(\"-0.0001d\")\n    assert a.is_within_bounds(\"0d\", \"360d\") is False\n\n    # Angle just below the upper bound\n    a = Angle(\"359.9999d\")\n    assert a.is_within_bounds(\"0d\", \"360d\") is True\n\n    # Angle exactly at negative lower bound\n    a = Angle(\"-180d\")\n    assert a.is_within_bounds(\"-180d\", \"180d\") is True\n\n    # Angle exactly at positive upper bound\n    a = Angle(\"180d\")\n    assert a.is_within_bounds(\"-180d\", \"180d\") is False  # upper bound is exclusive\n", "def test_longitude_wrap_angle():\n    \"\"\"\n    Test setting wrap_angle for Longitude and ensuring correct wrapping.\n    \"\"\"\n\n    # Initialize Longitude with default wrap_angle 360d\n    lon = Longitude([0, 90, 180, 270, 360] * u.deg)\n    assert np.all(lon.degree == [0, 90, 180, 270, 0])\n\n    # Change wrap_angle to 180d and check if values are wrapped correctly\n    lon.wrap_angle = 180 * u.deg\n    assert np.all(lon.degree == [0, 90, -180, -90, 0])\n\n    # Initialize with custom wrap_angle directly\n    lon = Longitude([0, 90, 180, 270, 360] * u.deg, wrap_angle=180 * u.deg)\n    assert np.all(lon.degree == [0, 90, -180, -90, 0])\n\n    # Check a case where wrap_angle is different from 360 and 180\n    lon.wrap_angle = 90 * u.deg\n    assert np.all(lon.degree == [0, -270, -180, -90, 0])\n\n    # Check for automatic Angle conversion of wrap_angle\n    lon.wrap_angle = \"180d\"\n    assert np.all(lon.degree == [0, 90, -180, -90, 0])\n\n    # Check wrap_angle attribute is an instance of Angle\n    assert isinstance(lon.wrap_angle, Angle)\n", "def test_latitude_wrap_at():\n    \"\"\"\n    Test Latitude wrapping to ensure it remains within valid bounds.\n    \"\"\"\n    lat = Latitude([85, -85, 90, -90, 95, -95] * u.deg)\n    assert np.all(lat.wrap_at(90 * u.deg).degree == np.array([85, -85, 90, -90, -85, 85]))\n\n    lat = Latitude([np.pi / 2, -np.pi / 2, 2 * np.pi / 2, -2 * np.pi / 2] * u.rad)\n    wrapped_lat = lat.wrap_at(np.pi / 2 * u.rad)\n    expected = [np.pi / 2, -np.pi / 2, np.pi / 2, -np.pi / 2]\n    assert np.allclose(wrapped_lat.rad, expected)\n\n    with pytest.raises(ValueError):\n        lat.wrap_at(180 * u.deg)\n"], "sample_24": ["    def test_invariant_a_helper(self):\n        self.check(np.copy)\n        self.check(np.asfarray)\n        self.check(np.real_if_close)\n        self.check(np.sort_complex)\n        self.check(np.resize)\n", "    def test_dot(self):\n        out = np.dot(self.ma, self.mb.T)\n        expected = np.dot(self.a, self.b)\n        expected_mask = np.dot(self.mask_a, self.mask_b.T)\n        assert_array_equal(out.unmasked, expected)\n        assert_array_equal(out.mask, expected_mask)\n", "    def test_norm(self):\n        result = np.linalg.norm(self.ma)\n        expected = np.linalg.norm(self.a)\n        assert_array_equal(result.unmasked, expected)\n        assert_array_equal(result.mask, self.mask_a)\n", "    def test_function_helper_registration(self):\n            pass\n\n        @function_helper\n            pass\n\n        assert helper_func in FUNCTION_HELPERS.values()\n        assert FUNCTION_HELPERS[helper_func] is helper_func\n\n        @function_helper(helps=dummy_func)\n            pass\n\n        assert another_helper_func in FUNCTION_HELPERS.values()\n        assert FUNCTION_HELPERS[dummy_func] is another_helper_func\n", "    def test_linalg_functions(self, function, input, expected, check_mask):\n        if isinstance(input, tuple):\n            input = tuple(Masked(np.array(arr), mask=np.array([False] * arr.size)).reshape(arr.shape) for arr in input)\n        else:\n            input = Masked(np.array(input), mask=np.array([False] * input.size)).reshape(input.shape)\n\n        result = function(*input) if isinstance(input, tuple) else function(input)\n        assert isinstance(result, Masked)\n        assert_array_equal(result.unmasked, expected)\n        if check_mask:\n            assert_array_equal(result.mask, np.zeros_like(expected, dtype=bool))\n"], "sample_25": ["def test_invalid_rvkc_image_parsing():\n    \"\"\"\n    Test that an invalid RVKC image correctly triggers parsing errors.\n    \"\"\"\n\n    # This card has an invalid value that should not be parsed as a RVKC\n    invalid_card_image = \"DP1     = 'AXIS.1: not_a_number' / Invalid number\"\n    c = fits.Card.fromstring(invalid_card_image)\n    assert c.keyword == \"DP1\"\n    assert c.value == \"AXIS.1: not_a_number\"\n    assert c.comment == \"Invalid number\"\n    assert c.field_specifier is None\n\n    # Now verify the card and ensure it raises an error due to the invalid value\n    with pytest.raises(fits.VerifyError, match=r\"Unparsable card \\(DP1\\)\"):\n        c.verify(\"exception\")\n\n    # Attempting to set a numeric value should now work correctly\n    c.value = \"AXIS.1: 2.0\"\n    assert c.keyword == \"DP1.AXIS.1\"\n    assert c.value == 2.0\n    assert c.field_specifier == \"AXIS.1\"\n    assert c.comment == \"Invalid number\"\n", "def test_card_format_image():\n    \"\"\"Test formatting of card image with various keyword, value, and comment combinations.\"\"\"\n    \n    # Case 1: Keyword, value, and comment\n    c = fits.Card(\"TESTKEY\", \"TestValue\", \"This is a comment\")\n    assert str(c) == _pad(\"TESTKEY = 'TestValue' / This is a comment\")\n\n    # Case 2: Keyword and value only\n    c = fits.Card(\"TESTKEY\", \"TestValue\")\n    assert str(c) == _pad(\"TESTKEY = 'TestValue'\")\n\n    # Case 3: Keyword and comment only\n    c = fits.Card(\"TESTKEY\", comment=\"This is a comment\")\n    assert str(c) == _pad(\"TESTKEY =                    / This is a comment\")\n\n    # Case 4: Keyword only\n    c = fits.Card(\"TESTKEY\")\n    assert str(c) == _pad(\"TESTKEY =\")\n\n    # Case 5: Long string value requiring CONTINUE cards\n    long_value = \"A long string value that exceeds the standard 80 character limit \" \\\n                 \"and requires CONTINUE cards to be properly stored in the FITS header.\"\n    c = fits.Card(\"LONGSTRG\", long_value)\n    expected_output = \"LONGSTRG= 'A long string value that exceeds the standard 80 character limit&'\\n\" \\\n                      \"CONTINUE  ' and requires CONTINUE cards to be properly stored in the FITS&'\\n\" \\\n                      \"CONTINUE  ' header.'\"\n    assert str(c) == expected_output\n    \n    # Case 6: Long comment requiring truncation\n    long_comment = \"This is a very long comment that will exceed the limit for a single card and thus \" \\\n                   \"needs to be truncated when formatted as a FITS card.\"\n    c = fits.Card(\"TESTKEY\", \"TestValue\", long_comment)\n    expected_output = \"TESTKEY = 'TestValue' / This is a very long comment that will exceed the limit for a\"\n    with pytest.warns(fits.verify.VerifyWarning):\n        assert str(c) == _pad(expected_output)\n", "def test_card_float_with_leading_zeros():\n    \"\"\"Test Card constructor with float value containing leading zeros\"\"\"\n\n    c = fits.Card(\"FLOATVAL\", 0.001234)\n    assert str(c) == _pad(\"FLOATVAL=              0.001234\")\n\n    c = fits.Card.fromstring(\"FLOATVAL= 0000000.1234E+01\")\n    assert c.value == 1.234\n    with pytest.warns(fits.verify.VerifyWarning):\n        assert str(c) == _pad(\"FLOATVAL=                1.234E+00\")\n", "def test_card_string_with_special_characters():\n    \"\"\"\n    Test Card constructor and fromstring method with special characters in string value,\n    ensuring that they are correctly handled.\n    \"\"\"\n\n    special_string = \"String with special chars !@# $%^&*()_+\"\n    c = fits.Card(\"SPECIAL\", special_string)\n    assert str(c) == _pad(\"SPECIAL = 'String with special chars !@# $%^&*()_+'\")\n    assert c.keyword == \"SPECIAL\"\n    assert c.value == special_string\n\n    c = fits.Card.fromstring(\"SPECIAL = 'String with special chars !@# $%^&*()_+'\")\n    assert c.keyword == \"SPECIAL\"\n    assert c.value == special_string\n", "def test_card_init_with_invalid_keyword():\n    \"\"\"Test that initializing a Card with an invalid keyword raises a ValueError.\"\"\"\n\n    with pytest.raises(ValueError, match=\"Illegal keyword name: '123ABC'\"):\n        fits.Card(\"123ABC\", 1)\n\n    with pytest.raises(ValueError, match=\"Keyword name 'None' is not a string.\"):\n        fits.Card(None, 1)\n\n    with pytest.raises(ValueError, match=\"Illegal keyword name: 'FOO BAR'\"):\n        fits.Card(\"FOO BAR\", 1)\n"], "sample_26": ["def test_section_shape_property():\n    \"\"\"\n    Test that the shape property of the Section class returns the correct shape.\n    \"\"\"\n\n    data = np.arange(24).reshape((2, 3, 4))\n    hdu = fits.PrimaryHDU(data=data)\n    section = hdu.section\n\n    assert section.shape == (2, 3, 4)\n", "    def test_fits_primary_hdu_initialization(self):\n        # Test the initialization of a PrimaryHDU object with various combinations of parameters\n        header = fits.Header([(\"SIMPLE\", True), (\"BITPIX\", 8), (\"NAXIS\", 0)])\n        \n        # Minimal initialization, data and header provided\n        data = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.int16)\n        primary_hdu = fits.PrimaryHDU(data=data, header=header)\n        assert np.array_equal(primary_hdu.data, data)\n        assert primary_hdu.header[\"BITPIX\"] == 16\n        \n        # Initialization with do_not_scale_image_data set to True\n        primary_hdu = fits.PrimaryHDU(data=data, header=header, do_not_scale_image_data=True)\n        assert primary_hdu._do_not_scale_image_data is True\n        \n        # Initialization with ignore_blank set to True\n        primary_hdu = fits.PrimaryHDU(data=data, header=header, ignore_blank=True)\n        assert primary_hdu._blank is None\n        \n        # Initialization with uint set to False\n        primary_hdu = fits.PrimaryHDU(data=data, header=header, uint=False)\n        assert primary_hdu._uint is False\n        \n        # Initialization with scale_back set to True\n        primary_hdu = fits.PrimaryHDU(data=data, header=header, scale_back=True)\n        assert primary_hdu._scale_back is True\n        \n        # Initialization with name and ver\n        primary_hdu = fits.PrimaryHDU(data=data, header=header, name=\"TEST_NAME\", ver=2)\n        assert primary_hdu.name == \"TEST_NAME\"\n        assert primary_hdu.ver == 2\n", "def test_image_hdu_scale_method():\n    \"\"\"\n    Test the scale method of the ImageHDU class.\n    \"\"\"\n    # Create an ImageHDU with int16 data\n    data = np.arange(100, dtype=np.int16).reshape(10, 10)\n    hdu = fits.ImageHDU(data=data)\n\n    # Scale to float32\n    hdu.scale(type='float32')\n    assert hdu.data.dtype == np.float32\n    assert hdu.header['BITPIX'] == -32\n    assert 'BSCALE' not in hdu.header\n    assert 'BZERO' not in hdu.header\n\n    # Scale to uint8 with specified bscale and bzero\n    hdu.scale(type='uint8', bscale=0.5, bzero=10)\n    assert hdu.data.dtype == np.uint8\n    assert hdu.header['BITPIX'] == 8\n    assert hdu.header['BSCALE'] == 0.5\n    assert hdu.header['BZERO'] == 10\n\n    # Check the scaled data\n    scaled_data = (data - 10) * 2\n    np.testing.assert_array_equal(hdu.data, scaled_data)\n\n    # Test scaling with 'minmax' option\n    data = np.arange(100, dtype=np.int16).reshape(10, 10)\n    hdu = fits.ImageHDU(data=data)\n    hdu.scale(type='uint8', option='minmax')\n    assert hdu.data.dtype == np.uint8\n    assert hdu.header['BITPIX'] == 8\n\n    # Test scaling back\n    data = np.arange(100, dtype=np.int16).reshape(10, 10)\n    hdu = fits.ImageHDU(data=data, scale_back=True)\n    hdu.data[0, 0] = 500  # Modify data\n    hdu.scale(type='uint8')\n    hdu.scale(type='int16', bscale=1, bzero=0)\n    assert hdu.data[0, 0] == 500\n", "    def test_invalid_bitpix(self):\n        \"\"\"\n        Test for invalid BITPIX values in the header to ensure proper handling\n        and error-raising.\n        \"\"\"\n\n        # Create a header with invalid BITPIX value\n        header = fits.Header()\n        header['BITPIX'] = 128  # Invalid value, valid values are typically 8, 16, 32, -32, -64\n        data = np.zeros((10, 10))\n\n        with pytest.raises(ValueError, match=\"Invalid BITPIX value: 128\"):\n            fits.PrimaryHDU(data=data, header=header)\n\n        # Modify the header in an existing HDU to have invalid BITPIX\n        hdu = fits.PrimaryHDU(data=data)\n        hdu.header['BITPIX'] = 128\n\n        with pytest.raises(ValueError, match=\"Invalid BITPIX value: 128\"):\n            hdu.writeto(self.temp('test_invalid_bitpix.fits'))\n", "    def test_imagehdu_constructor_with_data(self):\n        \"\"\"\n        Test the ImageHDU constructor when initialized with data and header\n        \"\"\"\n        data = np.ones((10, 10), dtype=np.float32)\n        header = fits.Header([('TEST', 'VALUE')])\n        hdu = fits.ImageHDU(data=data, header=header, name='TEST_IMAGE')\n\n        assert np.array_equal(hdu.data, data)\n        assert hdu.header['TEST'] == 'VALUE'\n        assert hdu.header['EXTNAME'] == 'TEST_IMAGE'\n        assert hdu.data.shape == (10, 10)\n"], "sample_28": ["def test_invalid_character_in_keyword():\n    \"\"\"\n    Test that Card constructor correctly raises an error when an invalid character\n    is present in the keyword.\n    \"\"\"\n\n    # Test invalid character '+'\n    with pytest.raises(ValueError, match=r\"Illegal keyword name: 'KEY+WORD'\"):\n        fits.Card(\"KEY+WORD\", \"value\")\n\n    # Test invalid character ':'\n    with pytest.raises(ValueError, match=r\"Illegal keyword name: 'KEY:WORD'\"):\n        fits.Card(\"KEY:WORD\", \"value\")\n\n    # Test invalid character ' '\n    with pytest.raises(ValueError, match=r\"Illegal keyword name: 'KEY WORD'\"):\n        fits.Card(\"KEY WORD\", \"value\")\n\n    # Test invalid character '?'\n    with pytest.raises(ValueError, match=r\"Illegal keyword name: 'KEY?WORD'\"):\n        fits.Card(\"KEY?WORD\", \"value\")\n\n    # Test invalid character '('\n    with pytest.raises(ValueError, match=r\"Illegal keyword name: 'KEY(W)ORD'\"):\n        fits.Card(\"KEY(W)ORD\", \"value\")\n", "def test_card_invalid_keyword_name():\n    \"\"\"\n    Ensure that invalid keyword names raise ValueError and are handled correctly.\n    \"\"\"\n    with pytest.raises(ValueError, match=r\"Illegal keyword name\"):\n        fits.Card(\"INVALID-KEY\", 100)\n\n    with pytest.raises(ValueError, match=r\"Illegal keyword name\"):\n        fits.Card(\"!@#$%^\", 200)\n    \n    with pytest.raises(ValueError, match=r\"Illegal keyword name\"):\n        fits.Card(\"123*ABC\", 300)\n    \n    with pytest.raises(ValueError, match=r\"Illegal keyword name\"):\n        fits.Card(\"abcdefghijklmno\", \"Value exceeding 8 chars keyword\")\n", "def test_card_keyword_update():\n    \"\"\"Test updating the keyword of a Card through the Header.\"\"\"\n\n    header = fits.Header([(\"A\", 1), (\"B\", 2)])\n    assert header[0] == 1\n    assert header[1] == 2\n    assert header.keys() == [\"A\", \"B\"]\n\n    # Update keyword\n    header.rename_keyword(\"A\", \"C\")\n    assert header[0] == 1\n    assert header[1] == 2\n    assert header.keys() == [\"C\", \"B\"]\n\n    # Ensure the original keyword is gone\n    with pytest.raises(KeyError):\n        _ = header[\"A\"]\n\n    # Ensure the new keyword works\n    assert header[\"C\"] == 1\n    assert header.comments[\"C\"] == \"\"\n\n    # Update keyword again and add a comment\n    header.rename_keyword(\"C\", \"D\")\n    header.set(\"D\", 1, \"Updated comment\")\n    assert header.keys() == [\"D\", \"B\"]\n    assert header[\"D\"] == 1\n    assert header.comments[\"D\"] == \"Updated comment\"\n", "def test_card_keyword_modification_exception():\n    \"\"\"Test that modifying an existing Card's keyword raises an exception\"\"\"\n    c = fits.Card(\"KEY1\", \"value1\", \"comment1\")\n    with pytest.raises(AttributeError, match=\"Once set, the Card keyword may not be modified\"):\n        c.keyword = \"NEWKEY\"\n", "def test_card_keyword_set_after_initialization():\n    \"\"\"\n    Test setting the keyword for a Card after initialization\n    to ensure that it raises the appropriate exception.\n    \"\"\"\n\n    c = fits.Card(\"INITIAL\", 42)\n    assert c.keyword == \"INITIAL\"\n\n    with pytest.raises(AttributeError, match=\"Once set, the Card keyword may not be modified\"):\n        c.keyword = \"NEWKEY\"\n"], "sample_29": ["    def test_latex_names_option(self, write, latex_names, tmp_path):\n        \"\"\"Test the 'latex_names' option in write_latex function\"\"\"\n        fp = tmp_path / f\"test_latex_names_option_{latex_names}.tex\"\n        write(fp, format=\"latex\", latex_names=latex_names)\n        tbl = QTable.read(fp)\n        if latex_names:\n            for column_name in tbl.colnames[2:]:\n                assert column_name in _FORMAT_TABLE.values()\n        else:\n            for column_name in tbl.colnames[2:]:\n                assert column_name in _FORMAT_TABLE.keys() or column_name not in _FORMAT_TABLE.values()\n", "def test_write_latex_with_kwargs(self, write, tmp_path):\n    \"\"\"Test writing with additional keyword arguments.\"\"\"\n    fp = tmp_path / \"test_write_latex_with_kwargs.tex\"\n    write(fp, format=\"latex\", overwrite=True, latexdict={\"preamble\": \"\\\\usepackage{amsmath}\"})\n    tbl = QTable.read(fp)\n    assert tbl.meta['preamble'] == \"\\\\usepackage{amsmath}\"\n", "    def test_write_latex_overwrite(self, write, tmp_path, overwrite_flag):\n        \"\"\"Test writing LaTeX file with different overwrite flags\"\"\"\n        fp = tmp_path / \"test_write_latex_overwrite.tex\"\n        write(fp, format=\"latex\", overwrite=overwrite_flag)\n        # Write again to check overwrite behavior\n        if overwrite_flag:\n            write(fp, format=\"latex\", overwrite=overwrite_flag)  # Should not raise an error\n        else:\n            with pytest.raises(OSError, match=\"overwrite=True\"):\n                write(fp, format=\"latex\", overwrite=overwrite_flag)\n", "    def test_write_latex_with_kwargs(self, write, tmp_path):\n        \"\"\"Test write_latex with additional kwargs\"\"\"\n        fp = tmp_path / \"test_write_latex_with_kwargs.tex\"\n        write(fp, format=\"latex\", overwrite=True, caption=\"Cosmology Parameters\", latexdict={'preamble': '\\\\usepackage{color}'})\n        tbl = QTable.read(fp)\n        # Ensure the table was written with the correct format and additional kwargs\n        assert tbl.meta['caption'] == \"Cosmology Parameters\"\n", "def test_write_latex_without_latex_names(self, write, tmp_path):\n    \"\"\"Test writing LaTeX without LaTeX names for parameters\"\"\"\n    fp = tmp_path / \"test_write_latex_without_latex_names.tex\"\n    write(fp, format=\"latex\", latex_names=False)\n    tbl = QTable.read(fp)\n    # asserts each column name is the original parameter name\n    for column_name in tbl.colnames[2:]:\n        assert column_name not in _FORMAT_TABLE.values()\n"], "sample_30": ["def test_check_astroyear():\n    # Test valid astronomical years\n    assert tree.check_astroyear(\"J2000\", \"test_field\")\n    assert tree.check_astroyear(\"B1950\", \"test_field\")\n    assert tree.check_astroyear(\"2000\", \"test_field\")\n    assert tree.check_astroyear(\"2000.5\", \"test_field\")\n\n    # Test invalid astronomical years\n    with pytest.warns(VOWarning):\n        assert not tree.check_astroyear(\"X2000\", \"test_field\")\n    with pytest.warns(VOWarning):\n        assert not tree.check_astroyear(\"2000X\", \"test_field\")\n    with pytest.warns(VOWarning):\n        assert not tree.check_astroyear(\"2000.5X\", \"test_field\")\n", "def test_check_astroyear():\n    assert tree.check_astroyear(\"J2000\", \"epoch\")\n    assert tree.check_astroyear(\"B1950.0\", \"epoch\")\n    assert not tree.check_astroyear(\"2000AD\", \"epoch\")\n    assert not tree.check_astroyear(\"202X\", \"epoch\")\n", "def test_lookup_by_attr_factory():\n    # Create a mock element with the required attributes for testing\n    class MockElement:\n            self.ID = ID\n            self.name = name\n\n    # Create a mock class with an iterator method that yields MockElements\n    class MockClass:\n            self._elements = elements\n\n            return iter(self._elements)\n\n    elements = [MockElement(ID=\"elem1\"), MockElement(ID=\"elem2\"), MockElement(ID=\"elem3\")]\n    mock_obj = MockClass(elements)\n\n    # Create the lookup function using the factory\n    lookup_by_id = _lookup_by_attr_factory(\"ID\", True, \"iter_elements\", \"MockElement\", \"Lookup by ID\")\n\n    # Attach the lookup function to the mock class instance\n    mock_obj.lookup_by_id = lookup_by_id.__get__(mock_obj)\n\n    # Test the lookup function\n    result = mock_obj.lookup_by_id(\"elem2\")\n    assert result.ID == \"elem2\"\n\n    with pytest.raises(KeyError):\n        mock_obj.lookup_by_id(\"nonexistent_id\")\n", "def test_check_astroyear():\n    assert check_astroyear(\"J2000\", \"test_field\")\n    assert check_astroyear(\"B1950\", \"test_field\")\n    assert check_astroyear(\"2000.5\", \"test_field\")\n    assert not check_astroyear(\"InvalidYear\", \"test_field\")\n    assert not check_astroyear(\"2022-01-01\", \"test_field\")\n", "def test_lookup_by_attr_factory():\n    class DummyClass:\n            self.elements = [\n                tree.Field(tree.VOTableFile(), ID=\"elem1\", name=\"Element 1\"),\n                tree.Field(tree.VOTableFile(), ID=\"elem2\", name=\"Element 2\"),\n                tree.Field(tree.VOTableFile(), ID=\"elem3\", name=\"Element 3\")\n            ]\n\n            yield from self.elements\n\n    dummy = DummyClass()\n    lookup_by_id = tree._lookup_by_attr_factory(\"ID\", True, \"iter_elements\", \"FIELD\", \"Lookup by ID\")\n    lookup_by_name = tree._lookup_by_attr_factory(\"name\", True, \"iter_elements\", \"FIELD\", \"Lookup by name\")\n\n    assert lookup_by_id(dummy, \"elem1\").ID == \"elem1\"\n    assert lookup_by_name(dummy, \"Element 3\").name == \"Element 3\"\n\n    with pytest.raises(KeyError):\n        lookup_by_id(dummy, \"nonexistent\")\n\n    with pytest.raises(KeyError):\n        lookup_by_name(dummy, \"Nonexistent Element\")\n"], "sample_31": ["    def test_write_latex_latex_names(self, write, tmp_path, format, latex_names):\n        \"\"\"Test the effect of `latex_names` parameter on column names\"\"\"\n        fp = tmp_path / \"test_write_latex_latex_names.tex\"\n        write(fp, format=format, latex_names=latex_names)\n        tbl = QTable.read(fp)\n        for column_name in tbl.colnames[2:]:\n            if latex_names:\n                assert column_name in _FORMAT_TABLE.values()\n            else:\n                assert column_name not in _FORMAT_TABLE.values()\n", "    def test_write_latex_latex_names_flag(self, write, tmp_path, format, latex_names):\n        \"\"\"Test writing LaTeX with and without LaTeX names for parameters.\"\"\"\n        fp = tmp_path / f\"test_write_latex_latex_names_{latex_names}.tex\"\n        write(fp, format=format, latex_names=latex_names)\n        tbl = QTable.read(fp)\n\n        # Check column names based on latex_names flag\n        if latex_names:\n            for column_name in tbl.colnames[2:]:\n                assert column_name in _FORMAT_TABLE.values()\n        else:\n            # Assuming original names match the keys of _FORMAT_TABLE\n            for column_name in tbl.colnames[2:]:\n                assert column_name in _FORMAT_TABLE.keys()\n", "    def test_write_latex_latex_names_option(self, write, tmp_path, format, latex_names):\n        \"\"\"Test the latex_names option for LaTeX file writing\"\"\"\n        fp = tmp_path / f\"test_write_latex_latex_names_{latex_names}.tex\"\n        write(fp, format=format, latex_names=latex_names)\n        tbl = QTable.read(fp)\n        # Check if the columns are correctly renamed based on latex_names option\n        if latex_names:\n            for column_name in tbl.colnames[2:]:\n                assert column_name in _FORMAT_TABLE.values()\n        else:\n            for column_name in tbl.colnames[2:]:\n                assert column_name not in _FORMAT_TABLE.values()\n", "    def test_write_latex_latex_names_option(self, write, tmp_path, format, latex_names):\n        \"\"\"Test the latex_names option for column renaming\"\"\"\n        fp = tmp_path / \"test_write_latex_latex_names_option.tex\"\n        write(fp, format=format, latex_names=latex_names)\n        tbl = QTable.read(fp)\n        if latex_names:\n            for column_name in tbl.colnames[2:]:\n                assert column_name in _FORMAT_TABLE.values()\n        else:\n            for column_name in tbl.colnames[2:]:\n                assert column_name not in _FORMAT_TABLE.values()\n", "    def test_write_latex_latex_names_toggle(self, write, tmp_path, format, latex_names):\n        \"\"\"Test toggling the latex_names parameter.\"\"\"\n        fp = tmp_path / \"test_write_latex_latex_names_toggle.tex\"\n        write(fp, format=format, latex_names=latex_names)\n        tbl = QTable.read(fp)\n        if latex_names:\n            for column_name in tbl.colnames[2:]:\n                assert column_name in _FORMAT_TABLE.values()\n        else:\n            for column_name in tbl.colnames[2:]:\n                assert column_name not in _FORMAT_TABLE.values()\n"], "sample_32": ["def test_de_density_scale(self, cosmo):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    z = np.array([0.0, 0.5, 1.0, 1.5, 2.0])\n    expected = (1 + z) ** (3 * (1 + cosmo.w0 - cosmo.wz)) * np.exp(3 * cosmo.wz * z)\n    result = cosmo.de_density_scale(z)\n    assert np.allclose(result, expected)\n\n    # Test scalar input\n    z_scalar = 1.0\n    expected_scalar = (1 + z_scalar) ** (3 * (1 + cosmo.w0 - cosmo.wz)) * np.exp(3 * cosmo.wz * z_scalar)\n    result_scalar = cosmo.de_density_scale(z_scalar)\n    assert np.allclose(result_scalar, expected_scalar)\n", "def test_de_density_scale(self, cosmo):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    z = np.array([0.0, 0.5, 1.0, 1.5, 2.0])\n    expected = (z + 1.0) ** (3.0 * (1.0 + cosmo.w0 - cosmo.wz)) * np.exp(3.0 * cosmo.wz * z)\n    assert np.allclose(cosmo.de_density_scale(z), expected)\n\n    z = 0.5\n    expected = (z + 1.0) ** (3.0 * (1.0 + cosmo.w0 - cosmo.wz)) * np.exp(3.0 * cosmo.wz * z)\n    assert np.isclose(cosmo.de_density_scale(z), expected)\n\n    z = 1.0\n    expected = (z + 1.0) ** (3.0 * (1.0 + cosmo.w0 - cosmo.wz)) * np.exp(3.0 * cosmo.wz * z)\n    assert np.isclose(cosmo.de_density_scale(z), expected)\n", "def test_de_density_scale(self, cosmo):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    z = np.array([0.0, 0.5, 1.0, 1.5, 2.0])\n    expected_scale = (1 + z) ** (3 * (1 + cosmo.w0 - cosmo.wz)) * np.exp(3 * cosmo.wz * z)\n    assert np.allclose(cosmo.de_density_scale(z), expected_scale)\n    \n    z_scalar = 1.0\n    expected_scale_scalar = (1 + z_scalar) ** (3 * (1 + cosmo.w0 - cosmo.wz)) * np.exp(3 * cosmo.wz * z_scalar)\n    assert np.isclose(cosmo.de_density_scale(z_scalar), expected_scale_scalar)\n", "    def test_de_density_scale(self, cosmo, z, expected):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n        result = cosmo.de_density_scale(z)\n        assert np.allclose(result, expected, rtol=1e-5), f\"Failed for z={z}\"\n", "def test_de_density_scale(self, cosmo):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    z = np.array([0.0, 0.5, 1.0, 1.5, 2.0])\n    expected = (1 + z) ** (3 * (1 + cosmo.w0 - cosmo.wz)) * np.exp(3 * cosmo.wz * z)\n    \n    assert np.allclose(cosmo.de_density_scale(z), expected)\n"], "sample_33": ["def test_indent():\n    text = \"This is a line of text.\\nThis is another line.\"\n    expected_output = \"    This is a line of text.\\n    This is another line.\"\n    assert misc.indent(text, shift=1, width=4) == expected_output\n\n    text_with_newline = \"This is a line of text.\\nThis is another line.\\n\"\n    expected_output_with_newline = \"    This is a line of text.\\n    This is another line.\\n\"\n    assert misc.indent(text_with_newline, shift=1, width=4) == expected_output_with_newline\n\n    text_with_multiple_lines = \"Line 1\\nLine 2\\nLine 3\"\n    expected_output_multiple_lines = \"  Line 1\\n  Line 2\\n  Line 3\"\n    assert misc.indent(text_with_multiple_lines, shift=1, width=2) == expected_output_multiple_lines\n", "def test_indent():\n    text = \"Hello\\nWorld\"\n    expected = \"    Hello\\n    World\"\n    assert misc.indent(text) == expected\n    \n    text_with_trailing_newline = \"Hello\\nWorld\\n\"\n    expected_with_trailing_newline = \"    Hello\\n    World\\n\"\n    assert misc.indent(text_with_trailing_newline) == expected_with_trailing_newline\n    \n    text_with_shift = \"Hello\\nWorld\"\n    expected_with_shift = \"        Hello\\n        World\"\n    assert misc.indent(text_with_shift, shift=2) == expected_with_shift\n", "def test_indent():\n    text = \"line1\\nline2\\nline3\"\n    expected_output = \"    line1\\n    line2\\n    line3\"\n    assert misc.indent(text) == expected_output\n\n    text_with_newline = \"line1\\nline2\\nline3\\n\"\n    expected_output_with_newline = \"    line1\\n    line2\\n    line3\\n\"\n    assert misc.indent(text_with_newline) == expected_output_with_newline\n\n    custom_indent = 2\n    expected_custom_output = \"        line1\\n        line2\\n        line3\"\n    assert misc.indent(text, shift=custom_indent) == expected_custom_output\n\n    custom_width = 2\n    expected_custom_width_output = \"  line1\\n  line2\\n  line3\"\n    assert misc.indent(text, width=custom_width) == expected_custom_width_output\n", "def test_indent():\n    input_string = \"This is a test string.\\nWith multiple lines.\\nAnd another line.\"\n    expected_output = \"    This is a test string.\\n    With multiple lines.\\n    And another line.\"\n    assert misc.indent(input_string) == expected_output\n\n    input_string_with_trailing_newline = \"This is a test string.\\nWith multiple lines.\\nAnd another line.\\n\"\n    expected_output_with_trailing_newline = \"    This is a test string.\\n    With multiple lines.\\n    And another line.\\n\"\n    assert misc.indent(input_string_with_trailing_newline) == expected_output_with_trailing_newline\n\n    assert misc.indent(input_string, shift=2) == \"        This is a test string.\\n        With multiple lines.\\n        And another line.\"\n    assert misc.indent(input_string, width=2) == \"  This is a test string.\\n  With multiple lines.\\n  And another line.\"\n    assert misc.indent(input_string, shift=2, width=2) == \"    This is a test string.\\n    With multiple lines.\\n    And another line.\"\n", "def test_indent():\n    text = \"Hello\\nWorld\"\n    expected = \"    Hello\\n    World\"\n    assert misc.indent(text) == expected\n\n    text_with_empty_line = \"Hello\\n\\nWorld\\n\"\n    expected_with_empty_line = \"    Hello\\n\\n    World\\n\"\n    assert misc.indent(text_with_empty_line) == expected_with_empty_line\n\n    text_with_custom_shift = \"Hello\\nWorld\"\n    expected_with_custom_shift = \"        Hello\\n        World\"\n    assert misc.indent(text_with_custom_shift, shift=2) == expected_with_custom_shift\n\n    text_with_custom_width = \"Hello\\nWorld\"\n    expected_with_custom_width = \"  Hello\\n  World\"\n    assert misc.indent(text_with_custom_width, width=2) == expected_with_custom_width\n"], "sample_34": ["def test_unit_registry():\n    # Test initializing an empty registry\n    registry = u._UnitRegistry()\n    assert len(registry.all_units) == 0\n    assert len(registry.registry) == 0\n    assert len(registry.equivalencies) == 0\n\n    # Test adding units to the registry\n    registry.add_enabled_units([u.m, u.s])\n    assert u.m in registry.all_units\n    assert u.s in registry.all_units\n    assert 'm' in registry.registry\n    assert 's' in registry.registry\n\n    # Test adding equivalencies to the registry\n    registry.add_enabled_equivalencies(u.spectral())\n    assert len(registry.equivalencies) > 0\n    assert any(equiv[0] == u.AA for equiv in registry.equivalencies)\n\n    # Test resetting units and equivalencies\n    registry._reset_units()\n    assert len(registry.all_units) == 0\n    assert len(registry.registry) == 0\n\n    registry._reset_equivalencies()\n    assert len(registry.equivalencies) == 0\n\n    # Test setting enabled units\n    registry.set_enabled_units([u.kg])\n    assert u.kg in registry.all_units\n    assert 'kg' in registry.registry\n\n    # Test setting enabled equivalencies\n    registry.set_enabled_equivalencies(u.spectral())\n    assert len(registry.equivalencies) > 0\n    assert any(equiv[0] == u.AA for equiv in registry.equivalencies)\n", "def test_unit_registry():\n    # Test creating a UnitRegistry and adding units\n    registry = u._UnitRegistry()\n    registry.add_enabled_units([u.m, u.s, u.kg])\n    \n    # Check that the units were added correctly\n    assert 'm' in registry.registry\n    assert 's' in registry.registry\n    assert 'kg' in registry.registry\n    assert u.m in registry.all_units\n    assert u.s in registry.all_units\n    assert u.kg in registry.all_units\n    assert u.m in registry.non_prefix_units\n    assert u.s in registry.non_prefix_units\n    assert u.kg in registry.non_prefix_units\n    \n    # Test getting units with physical type\n    assert registry.get_units_with_physical_type(u.m) == {u.m}\n    assert registry.get_units_with_physical_type(u.kg) == {u.kg}\n    assert registry.get_units_with_physical_type(u.s) == {u.s}\n    \n    # Test setting enabled units\n    registry.set_enabled_units([u.cm, u.min])\n    assert 'cm' in registry.registry\n    assert 'min' in registry.registry\n    assert 'm' not in registry.registry\n    assert 's' not in registry.registry\n    assert 'kg' not in registry.registry\n    \n    # Test equivalencies\n    dimensionless_angles = u.dimensionless_angles()\n    registry.set_enabled_equivalencies(dimensionless_angles)\n    assert registry.equivalencies == dimensionless_angles\n", "def test_irreducible_unit_pickle():\n    # Test pickling and unpickling of an IrreducibleUnit\n    original_unit = u.def_unit('irreducible_unit', namespace=locals())\n    pkl = pickle.dumps(original_unit)\n    unpickled_unit = pickle.loads(pkl)\n    \n    assert unpickled_unit == original_unit\n    assert unpickled_unit.represents is unpickled_unit\n\n    # Ensure it is correctly registered in the registry after unpickling\n    assert 'irreducible_unit' in u.get_current_unit_registry().registry\n", "def test_unit_addition_with_string():\n    \"\"\"Test addition of unit with a string unit representation.\"\"\"\n    u1 = u.m\n    us = '5s'\n    with pytest.raises(TypeError):\n        u1 + us\n    with pytest.raises(TypeError):\n        us + u1\n", "def test_set_enabled_units():\n    \"\"\"\n    Test `set_enabled_units` functionality.\n    \"\"\"\n    from ... import units as u\n\n    # Define some custom units\n    foo = u.def_unit('foo', 42 * u.m)\n    bar = u.def_unit('bar', 1 * u.s)\n\n    # Enable custom units\n    with u.set_enabled_units([foo, bar]):\n        assert 'foo' in u.get_current_unit_registry().registry\n        assert 'bar' in u.get_current_unit_registry().registry\n        assert u.m.is_equivalent(foo)\n        assert u.s.is_equivalent(bar)\n\n    # Ensure custom units are no longer enabled\n    assert 'foo' not in u.get_current_unit_registry().registry\n    assert 'bar' not in u.get_current_unit_registry().registry\n"], "sample_35": ["def test_resolve_name():\n    # Test basic resolution\n    assert introspection.resolve_name('astropy.utils.introspection.resolve_name') is introspection.resolve_name\n    \n    # Test resolution with additional parts\n    assert introspection.resolve_name('astropy', 'utils', 'introspection', 'resolve_name') is introspection.resolve_name\n    \n    # Test resolution of a module\n    import astropy\n    assert introspection.resolve_name('astropy') is astropy\n    \n    # Test resolution failure\n    with pytest.raises(ImportError):\n        introspection.resolve_name('non.existent.module')\n", "def test_resolve_name():\n    # Test resolving a function within the same module\n    resolved = introspection.resolve_name('astropy.utils.introspection.resolve_name')\n    assert resolved == introspection.resolve_name\n\n    # Test resolving a submodule and function within the same module\n    resolved = introspection.resolve_name('astropy.utils.introspection', 'resolve_name')\n    assert resolved == introspection.resolve_name\n\n    # Test resolving a built-in module and function\n    resolved = introspection.resolve_name('math.sqrt')\n    import math\n    assert resolved == math.sqrt\n\n    # Test resolving a module itself\n    resolved = introspection.resolve_name('math')\n    assert resolved == math\n\n    # Test resolving a non-existing module\n    with pytest.raises(ImportError):\n        introspection.resolve_name('nonexistent.module')\n\n    # Test resolving a non-existing attribute in an existing module\n    with pytest.raises(ImportError):\n        introspection.resolve_name('math.nonexistent')\n", "def test_resolve_name():\n    from astropy.utils.introspection import resolve_name\n    import math\n\n    # Test resolving a standard library function\n    assert resolve_name('math.sqrt') == math.sqrt\n\n    # Test resolving a nested module function\n    assert resolve_name('astropy.utils.introspection.resolve_name') == resolve_name\n\n    # Test resolving with additional parts\n    assert resolve_name('astropy.utils', 'introspection', 'resolve_name') == resolve_name\n\n    # Test non-existent module\n    with pytest.raises(ImportError):\n        resolve_name('non.existent.module')\n\n    # Test non-existent attribute\n    with pytest.raises(ImportError):\n        resolve_name('math.non_existent_function')\n", "def test_isinstancemethod():\n    class MetaClass(type):\n\n    class MyClass(metaclass=MetaClass):\n\n        @classmethod\n\n        @staticmethod\n\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.another_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n\n    with pytest.raises(AttributeError):\n        isinstancemethod(MyClass, \"non_existent_method\")\n", "def test_resolve_name():\n    from astropy.utils.introspection import resolve_name\n    import astropy\n\n    # Test resolving a function within the module\n    assert resolve_name('astropy.utils.introspection.resolve_name') == resolve_name\n\n    # Test resolving a module\n    assert resolve_name('astropy') == astropy\n\n    # Test resolving with additional parts\n    assert resolve_name('astropy', 'utils', 'introspection', 'resolve_name') == resolve_name\n\n    # Test ImportError for non-existent module\n    with pytest.raises(ImportError):\n        resolve_name('non.existent.module')\n\n    # Test ImportError for non-existent attribute within a valid module\n    with pytest.raises(ImportError):\n        resolve_name('astropy.non_existent_attribute')\n"], "sample_36": ["def test_biweight_location_with_M():\n    data = np.array([1, 2, 3, 4, 5])\n    M = 3\n    cbl = biweight_location(data, M=M)\n    assert_allclose(cbl, 3)\n\n    M_array = np.array([1, 2, 3, 4, 5])\n    with pytest.raises(ValueError):\n        biweight_location(data, M=M_array)\n", "def test_biweight_midcorrelation_with_nan():\n    \"\"\"Test biweight_midcorrelation with arrays containing NaN values.\"\"\"\n    x = [0, 1, 2, np.nan, 4]\n    y = [2, 1, 0, 3, np.nan]\n\n    with pytest.raises(ValueError):\n        biweight_midcorrelation(x, y)\n", "def test_biweight_scale_axis():\n    \"\"\"Test a 2D array with the axis keyword for biweight_scale.\"\"\"\n    with NumpyRNGContext(12345):\n        ny = 100\n        nx = 200\n        data = normal(5, 2, (ny, nx))\n\n        bw = biweight_scale(data, axis=0)\n        bwi = []\n        for i in range(nx):\n            bwi.append(biweight_scale(data[:, i]))\n        bwi = np.array(bwi)\n        assert_allclose(bw, bwi)\n\n        bw = biweight_scale(data, axis=1)\n        bwi = []\n        for i in range(ny):\n            bwi.append(biweight_scale(data[i, :]))\n        bwi = np.array(bwi)\n        assert_allclose(bw, bwi)\n", "def test_biweight_scale_axis():\n    \"\"\"Test a 2D array with the axis keyword for biweight_scale.\"\"\"\n    with NumpyRNGContext(12345):\n        ny = 100\n        nx = 200\n        data = normal(5, 2, (ny, nx))\n\n        bw = biweight_scale(data, axis=0)\n        bwi = []\n        for i in range(nx):\n            bwi.append(biweight_scale(data[:, i]))\n        bwi = np.array(bwi)\n        assert_allclose(bw, bwi)\n\n        bw = biweight_scale(data, axis=1)\n        bwi = []\n        for i in range(ny):\n            bwi.append(biweight_scale(data[i, :]))\n        bwi = np.array(bwi)\n        assert_allclose(bw, bwi)\n", "def test_biweight_location_median():\n    \"\"\"\n    Test biweight_location with a specific initial guess for the location.\n    \"\"\"\n    data = [1, 2, 3, 4, 5, 100]\n    M = 3  # initial guess for the location\n    cbl = biweight_location(data, M=M)\n    expected_result = 3.0161290322580645  # verified with external robust statistics tools\n    assert_allclose(cbl, expected_result, rtol=1e-5)\n"], "sample_37": ["def test_wcs_copy():\n    \"\"\"\n    Test that the WCS copy method creates an identical WCS object.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n    w_copy = w.copy()\n\n    # Compare key attributes\n    assert w.naxis == w_copy.naxis\n    assert np.all(w.wcs.crval == w_copy.wcs.crval)\n    assert np.all(w.wcs.cdelt == w_copy.wcs.cdelt)\n    assert np.all(w.wcs.crpix == w_copy.wcs.crpix)\n    assert np.all(w.wcs.ctype == w_copy.wcs.ctype)\n    assert w.sip == w_copy.sip\n\n    # Compare transformations\n    world = w.wcs_pix2world([[97, 97]], 1)\n    world_copy = w_copy.wcs_pix2world([[97, 97]], 1)\n    assert_array_almost_equal(world, world_copy)\n\n    pix = w.wcs_world2pix([[285.0, -66.25]], 1)\n    pix_copy = w_copy.wcs_world2pix([[285.0, -66.25]], 1)\n    assert_array_almost_equal(pix, pix_copy)\n", "def test_no_convergence_error():\n    \"\"\"\n    Test that the NoConvergence exception is raised correctly and contains the appropriate attributes.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crval = [0, 0]\n    w.wcs.crpix = [10, 10]\n\n    with pytest.raises(wcs.NoConvergence) as excinfo:\n        w.all_world2pix([[180, 90], [181, 91]], 1, maxiter=2, tolerance=1e-10)\n\n    e = excinfo.value\n    assert isinstance(e.best_solution, np.ndarray)\n    assert isinstance(e.accuracy, np.ndarray)\n    assert isinstance(e.niter, int)\n    assert e.niter == 2\n    assert e.divergent is None or isinstance(e.divergent, np.ndarray)\n    assert e.slow_conv is None or isinstance(e.slow_conv, np.ndarray)\n    assert e.best_solution.shape == (2, 2)\n", "def test_no_sip_coefficients():\n    \"\"\"\n    Test that when creating a WCS object with no SIP coefficients, it does not apply SIP transformations.\n    \"\"\"\n    header = {\n        'CTYPE1': 'RA---TAN',\n        'CTYPE2': 'DEC--TAN',\n        'CUNIT1': 'deg',\n        'CUNIT2': 'deg',\n        'CRPIX1': 1,\n        'CRPIX2': 1,\n        'CRVAL1': 40.,\n        'CRVAL2': 0.,\n        'CDELT1': -0.1,\n        'CDELT2': 0.1\n    }\n    w = wcs.WCS(header)\n    assert w.sip is None\n\n    # Check that pix2foc and foc2pix return the same pixel coordinates without SIP applied\n    x, y = 200, 200\n    x_foc, y_foc = w.sip_pix2foc(x, y, 0)\n    assert_array_almost_equal_nulp(x, x_foc, 10)\n    assert_array_almost_equal_nulp(y, y_foc, 10)\n\n    x_pix, y_pix = w.sip_foc2pix(x_foc, y_foc, 0)\n    assert_array_almost_equal_nulp(x, x_pix, 10)\n    assert_array_almost_equal_nulp(y, y_pix, 10)\n", "def test_wcs_sub():\n    \"\"\"\n    Test the sub method of WCS class to ensure it correctly extracts specific coordinate axes.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n\n    # Test sub method for celestial axes\n    w_sub = w.sub([wcs.WCSSUB_CELESTIAL])\n    assert w_sub.naxis == 2\n    assert w_sub.wcs.naxis == 2\n    assert w_sub.wcs.ctype[0] == 'RA---TAN-SIP'\n    assert w_sub.wcs.ctype[1] == 'DEC--TAN-SIP'\n\n    # Test sub method for individual axes\n    w_sub_1 = w.sub([1])\n    assert w_sub_1.naxis == 1\n    assert w_sub_1.wcs.naxis == 1\n\n    w_sub_2 = w.sub([2])\n    assert w_sub_2.naxis == 1\n    assert w_sub_2.wcs.naxis == 1\n\n    # Test sub method for invalid axes\n    with pytest.raises(IndexError):\n        w.sub([4])\n", "def test_read_sip_kw():\n    \"\"\"\n    Test the _read_sip_kw function to ensure it correctly parses SIP keywords from the header.\n    \"\"\"\n    header = get_pkg_data_contents('data/sip.hdr', encoding='binary')\n    header = fits.Header.fromstring(header)\n\n    w = wcs.WCS(header)\n    \n    sip = w._read_sip_kw(header)\n    \n    assert sip is not None\n    assert sip.crpix == (512.0, 512.0)\n    assert sip.a[0, 0] == 0\n    assert sip.b[0, 0] == 0\n    assert sip.ap[0, 0] == 0\n    assert sip.bp[0, 0] == 0\n"], "sample_38": ["def test_invalid_ctype():\n    \"\"\"\n    Test for handling invalid CTYPE in the WCS header.\n    \"\"\"\n    header = get_pkg_data_contents(\n        'data/invalid_ctype.hdr', encoding='binary')\n    \n    with pytest.raises(wcs.WcsError) as exc:\n        w = wcs.WCS(header)\n    \n    assert \"Unrecognized coordinate axis\" in str(exc.value)\n", "def test_wcs_sub():\n    # Test the sub method for WCS objects\n    hdr_name = get_pkg_data_filename('data/sip.fits')\n    header = fits.Header.fromfile(hdr_name)\n    w = wcs.WCS(header)\n\n    # Create a sub-WCS with only the celestial axes\n    sub_wcs = w.sub([1, 2])\n    assert sub_wcs.naxis == 2\n    assert (sub_wcs.wcs.ctype == ['RA---TAN-SIP', 'DEC--TAN-SIP']).all()\n\n    # Check that the sub-WCS still performs transformations correctly\n    pix_coords = np.array([[100, 100], [200, 200]])\n    world_coords = sub_wcs.all_pix2world(pix_coords, 1)\n    assert world_coords.shape == (2, 2)\n\n    # Check that the reverse transformation works as well\n    pix_coords2 = sub_wcs.all_world2pix(world_coords, 1)\n    assert_allclose(pix_coords, pix_coords2, rtol=1e-5)\n\n    # Create a sub-WCS with a non-existent axis\n    sub_wcs = w.sub([1, 3])\n    assert sub_wcs.naxis == 2\n    assert (sub_wcs.wcs.ctype == ['RA---TAN-SIP', '']).all()\n", "def test_header_with_invalid_sip():\n    \"\"\"\n    Test for handling header with invalid SIP distortion coefficients.\n    \"\"\"\n    hdr = \"\"\"\n    WCSAXES =                    2 / Number of coordinate axes\n    CRPIX1  =                  0.0 / Pixel coordinate of reference point\n    CRPIX2  =                  0.0 / Pixel coordinate of reference point\n    CDELT1  =                  1.0 / Coordinate increment at reference point\n    CDELT2  =                  1.0 / Coordinate increment at reference point\n    CRVAL1  =                  0.0 / Coordinate value at reference point\n    CRVAL2  =                  0.0 / Coordinate value at reference point\n    CTYPE1  = 'RA---TAN-SIP'  / TAN projection + SIP distortion\n    CTYPE2  = 'DEC--TAN-SIP'  / TAN projection + SIP distortion\n    A_ORDER =                  2 / Polynomial order, axis 1\n    A_0_0   =                0.0 / SIP distortion coefficient\n    A_0_1   =                0.0 / SIP distortion coefficient\n    A_1_0   =                0.0 / SIP distortion coefficient\n    A_1_1   =                0.0 / SIP distortion coefficient\n    A_2_2   =                0.0 / Invalid SIP coefficient (out of bounds)\n    B_ORDER =                  2 / Polynomial order, axis 2\n    B_0_0   =                0.0 / SIP distortion coefficient\n    B_0_1   =                0.0 / SIP distortion coefficient\n    B_1_0   =                0.0 / SIP distortion coefficient\n    B_1_1   =                0.0 / SIP distortion coefficient\n    B_2_2   =                0.0 / Invalid SIP coefficient (out of bounds)\n    \"\"\"\n    header = fits.Header.fromstring(hdr, sep='\\n')\n    \n    with pytest.raises(ValueError) as excinfo:\n        w = wcs.WCS(header)\n    \n    assert \"Invalid SIP\" in str(excinfo.value)\n", "def test_wcs_copy_methods():\n    \"\"\"\n    Test the WCS copy methods: copy, deepcopy, and __copy__.\n    \"\"\"\n    header = get_pkg_data_contents('data/sip.fits', encoding='binary')\n    w = wcs.WCS(header)\n\n    # Test shallow copy\n    w_copy = w.copy()\n    assert w.wcs.compare(w_copy.wcs)\n    assert w.sip.a_order == w_copy.sip.a_order\n    assert w.sip.b_order == w_copy.sip.b_order\n    assert np.array_equal(w.sip.a, w_copy.sip.a)\n    assert np.array_equal(w.sip.b, w_copy.sip.b)\n\n    # Test deep copy\n    w_deepcopy = w.deepcopy()\n    assert w.wcs.compare(w_deepcopy.wcs)\n    assert w.sip.a_order == w_deepcopy.sip.a_order\n    assert w.sip.b_order == w_deepcopy.sip.b_order\n    assert np.array_equal(w.sip.a, w_deepcopy.sip.a)\n    assert np.array_equal(w.sip.b, w_deepcopy.sip.b)\n\n    # Test __copy__\n    w__copy__ = w.__copy__()\n    assert w.wcs.compare(w__copy__.wcs)\n    assert w.sip.a_order == w__copy__.sip.a_order\n    assert w.sip.b_order == w__copy__.sip.b_order\n    assert np.array_equal(w.sip.a, w__copy__.sip.a)\n    assert np.array_equal(w.sip.b, w__copy__.sip.b)\n", "def test_wcs_copy():\n    \"\"\"\n    Test the copy and deepcopy methods of WCS class to ensure they produce\n    accurate copies of WCS objects with all attributes preserved.\n    \"\"\"\n    header = get_pkg_data_contents('data/sip.fits', encoding='binary')\n    original_wcs = wcs.WCS(header)\n\n    shallow_copy = original_wcs.copy()\n    deep_copy = original_wcs.deepcopy()\n\n    # Ensure shallow copy is a new instance but shares the same underlying data\n    assert shallow_copy is not original_wcs\n    assert shallow_copy.wcs is original_wcs.wcs\n    assert shallow_copy.sip is original_wcs.sip\n\n    # Ensure deep copy is a new instance with its own copies of data\n    assert deep_copy is not original_wcs\n    assert deep_copy.wcs is not original_wcs.wcs\n    assert deep_copy.sip is not original_wcs.sip\n\n    # Check that attributes are the same\n    assert np.array_equal(deep_copy.wcs.crpix, original_wcs.wcs.crpix)\n    assert np.array_equal(deep_copy.sip.a, original_wcs.sip.a)\n    assert np.array_equal(deep_copy.sip.b, original_wcs.sip.b)\n    assert np.array_equal(deep_copy.sip.ap, original_wcs.sip.ap)\n    assert np.array_equal(deep_copy.sip.bp, original_wcs.sip.bp)\n    assert deep_copy.wcs.naxis == original_wcs.wcs.naxis\n"], "sample_39": ["def test_calc_footprint_with_header():\n    \"\"\"\n    Test calc_footprint with provided header\n    \"\"\"\n    hdr = {\n        'CRPIX1': 0.5,\n        'CRPIX2': 0.5,\n        'CD1_1': -0.066667,\n        'CD1_2': 0.0,\n        'CD2_1': 0.0,\n        'CD2_2': 0.066667,\n        'CTYPE1': 'RA---TAN',\n        'CTYPE2': 'DEC--TAN',\n        'CRVAL1': 0.0,\n        'CRVAL2': 0.0,\n        'NAXIS1': 1000,\n        'NAXIS2': 2000\n    }\n    w = wcs.WCS(hdr)\n    footprint = w.calc_footprint(center=False)\n    expected_footprint = np.array([\n        [0.066667, -0.0333335],\n        [0.066667, 0.0333335],\n        [-0.066667, 0.0333335],\n        [-0.066667, -0.0333335]\n    ])\n    assert_allclose(footprint, expected_footprint)\n", "def test_wcs_sub():\n    \"\"\"\n    Test the sub method to ensure it correctly reduces the number of axes.\n    \"\"\"\n    hdr = get_pkg_data_contents('data/3d_cd.hdr', encoding='binary')\n    w = wcs.WCS(hdr)\n\n    assert w.naxis == 3\n    assert w.wcs.naxis == 3\n\n    # Subset with only celestial axes\n    w2 = w.sub([wcs.WCSSUB_CELESTIAL])\n    assert w2.naxis == 2\n    assert w2.wcs.naxis == 2\n    assert w2.wcs.ctype[0] == 'RA---TAN'\n    assert w2.wcs.ctype[1] == 'DEC--TAN'\n\n    # Subset with only spectral axes\n    w3 = w.sub([wcs.WCSSUB_SPECTRAL])\n    assert w3.naxis == 1\n    assert w3.wcs.naxis == 1\n    assert w3.wcs.ctype[0] == 'FREQ'\n\n    # Subset with celestial and spectral axes\n    w4 = w.sub([wcs.WCSSUB_CELESTIAL, wcs.WCSSUB_SPECTRAL])\n    assert w4.naxis == 3\n    assert w4.wcs.naxis == 3\n    assert w4.wcs.ctype[0] == 'RA---TAN'\n    assert w4.wcs.ctype[1] == 'DEC--TAN'\n    assert w4.wcs.ctype[2] == 'FREQ'\n", "def test_all_world2pix_no_convergence():\n    \"\"\"\n    Test all_world2pix to ensure it raises NoConvergence exception\n    when points fail to converge.\n    \"\"\"\n    header = get_pkg_data_contents('data/sip.fits', encoding='binary')\n    w = wcs.WCS(header)\n\n    # Deliberately use values that will not converge\n    world = np.array([[202.393, 47.178], [1000.0, 1000.0]])\n\n    with pytest.raises(wcs.NoConvergence) as e:\n        w.all_world2pix(world, 1, tolerance=1e-12, maxiter=2, quiet=False)\n\n    assert 'failed to converge' in str(e.value)\n    assert e.value.slow_conv is not None\n    assert e.value.divergent is None\n    assert e.value.best_solution is not None\n    assert e.value.accuracy is not None\n\n    with pytest.warns(AstropyUserWarning, match=\"may be inaccurate\"):\n        result = w.all_world2pix(world, 1, tolerance=1e-12, maxiter=2, quiet=True)\n    \n    assert result.shape == (2, 2)\n", "def test_wcs_copy():\n    \"\"\"\n    Test the copy and deepcopy functionality of WCS objects.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n\n    w_copy = w.copy()\n    w_deepcopy = w.deepcopy()\n\n    assert w.wcs.compare(w_copy.wcs)\n    assert w.wcs.compare(w_deepcopy.wcs)\n\n    # Modify the original WCS object and ensure the copies are not affected\n    w.wcs.crval[0] += 1\n    assert w.wcs.crval[0] != w_copy.wcs.crval[0]\n    assert w.wcs.crval[0] != w_deepcopy.wcs.crval[0]\n\n    # Modify the shallow copy and ensure the original and deepcopy are not affected\n    w_copy.wcs.crval[1] += 1\n    assert w.wcs.crval[1] != w_copy.wcs.crval[1]\n    assert w_deepcopy.wcs.crval[1] != w_copy.wcs.crval[1]\n\n    # Modify the deepcopy and ensure the original and shallow copy are not affected\n    w_deepcopy.wcs.crval[0] -= 2\n    assert w.wcs.crval[0] != w_deepcopy.wcs.crval[0]\n    assert w_copy.wcs.crval[0] != w_deepcopy.wcs.crval[0]\n", "def test_wcs_copy():\n    \"\"\"Test the copy and deepcopy methods of the WCS class.\"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n\n    w_copy = w.copy()\n    assert w.wcs.compare(w_copy.wcs)\n    assert w.sip.a_order == w_copy.sip.a_order\n    assert w.sip.b_order == w_copy.sip.b_order\n    assert w._naxis == w_copy._naxis\n\n    w_deepcopy = w.deepcopy()\n    assert w.wcs.compare(w_deepcopy.wcs)\n    assert w.sip.a_order == w_deepcopy.sip.a_order\n    assert w.sip.b_order == w_deepcopy.sip.b_order\n    assert w._naxis == w_deepcopy._naxis\n\n    # Modify the original and make sure the copies don't change\n    w.wcs.crval[0] += 1\n    assert not w.wcs.compare(w_copy.wcs)\n    assert not w.wcs.compare(w_deepcopy.wcs)\n    assert w_copy.wcs.crval[0] != w.wcs.crval[0]\n    assert w_deepcopy.wcs.crval[0] != w.wcs.crval[0]\n"], "sample_40": ["def test_dimensionless_angles_raised_power():\n    # Ensure that angles raised to a power still convert to dimensionless\n    rad1 = u.dimensionless_angles()\n    assert (1 * u.radian ** 2).to_value(1, equivalencies=rad1) == 1.\n    assert (1 * u.radian ** 3).to_value(1, equivalencies=rad1) == 1.\n    assert (1 * u.deg ** 2).to_value(1, equivalencies=rad1) == u.deg.to(u.radian) ** 2\n    assert (1 * u.deg ** 3).to_value(1, equivalencies=rad1) == u.deg.to(u.radian) ** 3\n", "def test_logarithmic_coverage():\n    # Additional tests for logarithmic equivalency coverage\n    q_mag = 20 * u.mag\n    q_dex = q_mag.to(u.dex, equivalencies=u.logarithmic())\n    q_db = q_mag.to(u.dB, equivalencies=u.logarithmic())\n    \n    # Check conversion to dex and dB\n    assert_allclose(q_dex.value, -2.0)\n    assert_allclose(q_db.value, -20.0)\n    \n    # Check round-trip conversions\n    assert_allclose(q_dex.to_value(u.mag, equivalencies=u.logarithmic()), 20.0)\n    assert_allclose(q_db.to_value(u.mag, equivalencies=u.logarithmic()), 20.0)\n    \n    # Check invalid conversions\n    with pytest.raises(u.UnitsError):\n        q_mag.to(u.m, equivalencies=u.logarithmic())\n", "def test_invalid_pixel_scale():\n    with pytest.raises(u.UnitsError):\n        u.pixel_scale(0.4 * u.m / u.s)\n", "def test_mass_energy_density():\n    mass_density = 1 * (u.kg / u.m**3)\n    energy_density = mass_density.to(u.J / u.m**3, equivalencies=u.mass_energy())\n    assert_allclose(energy_density.value, (1 * _si.c.value**2).to(u.J / u.m**3).value)\n    \n    energy_density_back = energy_density.to(u.kg / u.m**3, equivalencies=u.mass_energy())\n    assert_allclose(energy_density_back.value, mass_density.value)\n", "def test_mass_energy_density():\n    # Testing mass-energy equivalency for energy density\n    mass_density = 1 * u.kg / u.m**3\n    energy_density = mass_density.to(u.J / u.m**3, equivalencies=u.mass_energy())\n    expected_energy_density = 1 * u.J / u.m**3 * _si.c.value**2\n    assert np.allclose(energy_density.value, expected_energy_density.value, rtol=1e-7)\n    \n    # Convert back to mass density\n    converted_mass_density = energy_density.to(u.kg / u.m**3, equivalencies=u.mass_energy())\n    assert np.allclose(converted_mass_density.value, mass_density.value, rtol=1e-7)\n"], "sample_41": ["def test_unit_registry_initialization():\n    # Test initializing _UnitRegistry with different inputs\n    reg1 = utils._UnitRegistry()\n    assert len(reg1.all_units) == 0\n    assert len(reg1.equivalencies) == 0\n\n    # Test initializing _UnitRegistry with another registry\n    reg2 = utils._UnitRegistry(init=[u.m, u.kg], equivalencies=[(u.m, u.km, lambda x: x, lambda x: x)])\n    reg3 = utils._UnitRegistry(init=reg2)\n    assert len(reg3.all_units) == len(reg2.all_units)\n    assert reg3.equivalencies == reg2.equivalencies\n\n    # Test initializing _UnitRegistry with a list of units\n    reg4 = utils._UnitRegistry(init=[u.s, u.A, u.K])\n    assert len(reg4.all_units) == 3\n    assert u.s in reg4.all_units\n    assert u.A in reg4.all_units\n    assert u.K in reg4.all_units\n    assert len(reg4.equivalencies) == 0\n\n    # Test initializing _UnitRegistry with a list of equivalencies\n    reg5 = utils._UnitRegistry(equivalencies=[(u.m, u.s, lambda x: x, lambda x: x)])\n    assert len(reg5.equivalencies) == 1\n    assert reg5.equivalencies[0][0] == u.m\n    assert reg5.equivalencies[0][1] == u.s\n\n    # Ensure registry and units are correctly copied\n    reg6 = utils._UnitRegistry(init=[u.m])\n    reg6_copy = utils._UnitRegistry(init=reg6)\n    assert len(reg6_copy.all_units) == len(reg6.all_units)\n    assert reg6_copy.registry == reg6.registry\n", "def test_unit_inequality():\n    \"\"\"\n    Ensure that inequality comparisons between units of different types work.\n    \"\"\"\n    assert u.m != u.s\n    assert u.kg != u.A\n    assert u.cm != u.L\n", "def test_flatten_units_collection():\n    from astropy.units import UnitBase, Unit, CompositeUnit\n    \n    # Define some mock units for the test\n    class MockUnit(UnitBase):\n            self.name = name\n        \n            return self.name\n\n    unit1 = MockUnit(\"unit1\")\n    unit2 = MockUnit(\"unit2\")\n    unit3 = MockUnit(\"unit3\")\n\n    # Define test cases\n    test_cases = [\n        ([unit1, unit2], {unit1, unit2}),\n        ([unit1, [unit2, unit3]], {unit1, unit2, unit3}),\n        ([{\"a\": unit1, \"b\": unit2}], {unit1, unit2}),\n        ([unit1, {\"a\": unit2}, [unit3]], {unit1, unit2, unit3}),\n        (unit1, {unit1}),\n        ({\"a\": unit1, \"b\": [unit2, unit3]}, {unit1, unit2, unit3}),\n    ]\n\n    # Test _flatten_units_collection function\n    for inputs, expected in test_cases:\n        assert _flatten_units_collection(inputs) == expected\n", "def test_add_enabled_units_conflict():\n    registry = u.get_current_unit_registry()\n    with pytest.raises(ValueError, match=\"Object with name 'm' already exists in namespace.\"):\n        u.add_enabled_units([u.Unit('m'), u.Unit('m')])\n", "def test_unit_registry_management():\n    \"\"\"\n    Test the management of unit registries, including adding, setting, and getting enabled units and equivalencies.\n    \"\"\"\n\n    # Initial unit registry state\n    initial_registry = u.get_current_unit_registry()\n    initial_units = initial_registry.all_units\n    initial_equivalencies = initial_registry.equivalencies\n\n    # Define some test units\n    foo = u.def_unit('foo', u.m, namespace=locals())\n    bar = u.def_unit('bar', u.s, namespace=locals())\n\n    # Add units to registry\n    with u.add_enabled_units([foo, bar]):\n        current_registry = u.get_current_unit_registry()\n        assert 'foo' in current_registry.registry\n        assert 'bar' in current_registry.registry\n        assert foo in current_registry.all_units\n        assert bar in current_registry.all_units\n\n    # Ensure units are removed outside context\n    current_registry = u.get_current_unit_registry()\n    assert 'foo' not in current_registry.registry\n    assert 'bar' not in current_registry.registry\n    assert foo not in current_registry.all_units\n    assert bar not in current_registry.all_units\n\n    # Test setting equivalencies\n    eq1 = [(u.m, u.cm, lambda x: x * 100, lambda x: x / 100)]\n    with u.set_enabled_equivalencies(eq1):\n        current_registry = u.get_current_unit_registry()\n        assert current_registry.equivalencies == eq1\n\n    # Ensure equivalencies are reset outside context\n    current_registry = u.get_current_unit_registry()\n    assert current_registry.equivalencies == initial_equivalencies\n\n    # Reset to initial state\n    initial_registry.set_enabled_units(initial_units)\n    initial_registry.set_enabled_equivalencies(initial_equivalencies)\n"], "sample_42": ["def test_mass_energy_density():\n    # Test equivalency for mass energy density\n    mass_density = 1 * u.kg / u.m**3\n    energy_density = mass_density.to(u.J / u.m**3, equivalencies=u.mass_energy())\n    assert_allclose(energy_density.value, (mass_density.value * _si.c.value ** 2))\n    \n    energy_density_back = energy_density.to(u.kg / u.m**3, equivalencies=u.mass_energy())\n    assert_allclose(energy_density_back.value, mass_density.value)\n", "def test_molar_mass_amu_edge_cases():\n    # Test very small values\n    x_small = 1e-10 * (u.g/u.mol)\n    y_small = 1e-10 * u.u\n    assert_allclose(x_small.to_value(u.u, u.molar_mass_amu()), y_small.value)\n    assert_allclose(y_small.to_value(u.g/u.mol, u.molar_mass_amu()), x_small.value)\n\n    # Test very large values\n    x_large = 1e10 * (u.g/u.mol)\n    y_large = 1e10 * u.u\n    assert_allclose(x_large.to_value(u.u, u.molar_mass_amu()), y_large.value)\n    assert_allclose(y_large.to_value(u.g/u.mol, u.molar_mass_amu()), x_large.value)\n\n    # Test zero value\n    x_zero = 0 * (u.g/u.mol)\n    y_zero = 0 * u.u\n    assert_allclose(x_zero.to_value(u.u, u.molar_mass_amu()), y_zero.value)\n    assert_allclose(y_zero.to_value(u.g/u.mol, u.molar_mass_amu()), x_zero.value)\n\n    # Test negative value (should raise an error)\n    x_negative = -1 * (u.g/u.mol)\n    with pytest.raises(ValueError):\n        x_negative.to(u.u, u.molar_mass_amu())\n", "def test_mass_energy_density():\n    # This test ensures that the conversion between mass density and energy density works correctly\n    mass_density = 1 * u.kg / u.m**3\n    energy_density = mass_density.to(u.J / u.m**3, equivalencies=u.mass_energy())\n    expected_energy_density = 1 * (u.kg * _si.c.value**2) / u.m**3\n    assert_quantity_allclose(energy_density, expected_energy_density)\n\n    # Convert back from energy density to mass density\n    converted_mass_density = energy_density.to(u.kg / u.m**3, equivalencies=u.mass_energy())\n    assert_quantity_allclose(converted_mass_density, mass_density)\n", "def test_temperature_conversion():\n    # Tests for direct conversion between Kelvin and Celsius\n    assert_quantity_allclose(0 * u.K, -273.15 * u.deg_C, atol=1e-10, equivalencies=u.temperature())\n    assert_quantity_allclose(273.15 * u.K, 0 * u.deg_C, atol=1e-10, equivalencies=u.temperature())\n    assert_quantity_allclose(373.15 * u.K, 100 * u.deg_C, atol=1e-10, equivalencies=u.temperature())\n    \n    # Tests for direct conversion between Kelvin and Fahrenheit\n    assert_quantity_allclose(0 * u.K, -459.67 * u.deg_F, atol=1e-10, equivalencies=u.temperature())\n    assert_quantity_allclose(273.15 * u.K, 32 * u.deg_F, atol=1e-10, equivalencies=u.temperature())\n    assert_quantity_allclose(373.15 * u.K, 212 * u.deg_F, atol=1e-10, equivalencies=u.temperature())\n\n    # Tests for direct conversion between Celsius and Fahrenheit\n    assert_quantity_allclose(-273.15 * u.deg_C, -459.67 * u.deg_F, atol=1e-10, equivalencies=u.temperature())\n    assert_quantity_allclose(0 * u.deg_C, 32 * u.deg_F, atol=1e-10, equivalencies=u.temperature())\n    assert_quantity_allclose(100 * u.deg_C, 212 * u.deg_F, atol=1e-10, equivalencies=u.temperature())\n", "def test_with_H0():\n    from astropy.cosmology import Planck15\n    H0 = Planck15.H0\n\n    # Test conversion from physical units to little-h units\n    distance = 100 * u.Mpc\n    little_h_distance = distance.to(u.littleh * u.Mpc, equivalencies=u.with_H0(H0))\n    assert_quantity_allclose(little_h_distance, 100 * H0.value / 100)\n\n    # Test conversion from little-h units to physical units\n    little_h_distance = 100 * u.littleh * u.Mpc\n    physical_distance = little_h_distance.to(u.Mpc, equivalencies=u.with_H0(H0))\n    assert_quantity_allclose(physical_distance, 100 * 100 / H0.value)\n\n    # Ensure it raises UnitsError for invalid conversions\n    with pytest.raises(u.UnitsError):\n        (100 * u.Mpc).to(u.littleh * u.Mpc, equivalencies=None)\n    with pytest.raises(u.UnitsError):\n        (100 * u.littleh * u.Mpc).to(u.Mpc, equivalencies=None)\n"], "sample_43": ["def test_events_with_gamma():\n    rng = np.random.RandomState(42)\n    t = rng.normal(size=100)\n    edges = bayesian_blocks(t, fitness='events', gamma=0.1)\n    \n    assert len(edges) > 2  # Ensure that there are multiple bins\n", "def test_events_fitness():\n    rng = np.random.RandomState(0)\n    t = rng.exponential(1, 100)\n\n    # Test without specifying x (event counts)\n    bins1 = bayesian_blocks(t, fitness='events')\n    assert len(bins1) > 1\n\n    # Test with specifying x as event counts\n    x = np.ones_like(t)\n    bins2 = bayesian_blocks(t, x, fitness='events')\n    assert_allclose(bins1, bins2)\n\n    # Test with explicit ncp_prior\n    bins3 = bayesian_blocks(t, fitness='events', ncp_prior=4)\n    assert len(bins3) > 1\n", "def test_events_fitness_with_gamma():\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n    gamma = 0.1\n\n    bins = bayesian_blocks(t, fitness='events', gamma=gamma)\n    \n    # Check if the number of bins is reasonable based on the gamma value\n    assert len(bins) > 1\n", "def test_custom_fitness_function():\n    class CustomFitness(FitnessFunc):\n            return N_k * np.log(T_k)\n\n    rng = np.random.RandomState(0)\n    t = np.concatenate([rng.rand(100), 1 + rng.rand(200)])\n    \n    bins = bayesian_blocks(t, fitness=CustomFitness, p0=0.01)\n    \n    assert len(bins) >= 2  # Expecting at least one change point\n", "def test_events_fitness():\n    rng = np.random.RandomState(0)\n    t = np.concatenate([rng.rand(100), 1 + rng.rand(200)])\n    x = np.ones_like(t)\n\n    bins = bayesian_blocks(t, x, fitness='events')\n\n    assert (len(bins) == 3)\n    assert_allclose(bins[1], 1, rtol=0.02)\n"], "sample_44": ["    def test_init_physical_unit(self):\n        \"\"\"Test initialization with different physical units.\"\"\"\n        class TestUnit(FunctionUnitBase):\n            _default_function_unit = u.mag\n            _quantity_class = u.Magnitude\n\n                return x\n\n                return x\n\n        # Test with dimensionless unit\n        tu1 = TestUnit()\n        assert tu1.physical_unit == dimensionless_unscaled\n\n        # Test with valid physical unit\n        tu2 = TestUnit(physical_unit=u.m)\n        assert tu2.physical_unit == u.m\n\n        # Test with invalid physical unit\n        with pytest.raises(ValueError):\n            TestUnit(physical_unit=u.mag)\n", "    def test_init_with_invalid_physical_unit(self):\n        class TestUnit(FunctionUnitBase):\n            @property\n                return u.mag\n\n            @property\n                return FunctionQuantity\n\n                return x\n\n                return x\n\n        with pytest.raises(ValueError, match=\"is not a physical unit\"):\n            TestUnit(physical_unit='not_a_unit')\n", "    def test_init_with_invalid_physical_unit(self):\n        \"\"\"Test initialization of FunctionUnitBase with invalid physical unit.\"\"\"\n        class TestUnit(FunctionUnitBase):\n            @property\n                return u.Unit(\"mag\")\n\n            @property\n                return FunctionQuantity\n\n                return x\n\n                return x\n\n        with pytest.raises(ValueError):\n            TestUnit(u.mag)\n", "    def test_function_unit_base_init_with_valid_units(self):\n        class TestUnit(FunctionUnitBase):\n            @property\n                return u.mag\n\n            @property\n                return FunctionQuantity\n\n                return x\n\n                return x\n\n        unit = TestUnit(physical_unit=u.Jy, function_unit=u.mag)\n        assert unit.physical_unit == u.Jy\n        assert unit.function_unit == u.mag\n", "def test_log_unit_initialization_with_non_equivalent_function_unit():\n    \"\"\"Test initialization of log unit with a non-equivalent function unit.\"\"\"\n    with pytest.raises(ValueError):\n        u.MagUnit(u.Jy, function_unit=u.dB)\n"], "sample_45": ["    def test_trunc_microsecond_func(self):\n        \"\"\"\n        Truncate to microsecond should raise an error as it is not supported.\n        \"\"\"\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        msg = \"Cannot truncate DateTimeField to microsecond.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            list(DTModel.objects.annotate(truncated=Trunc('start_datetime', 'microsecond')))\n\n        msg = \"Cannot truncate DateField to microsecond.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            list(DTModel.objects.annotate(truncated=Trunc('start_date', 'microsecond')))\n\n        msg = \"Cannot truncate TimeField to microsecond.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            list(DTModel.objects.annotate(truncated=Trunc('start_time', 'microsecond')))\n", "    def test_extract_and_trunc_combined(self):\n        start_datetime = datetime(2020, 11, 30, 10, 45, 30, 123)\n        end_datetime = datetime(2021, 1, 1, 5, 15, 45, 456)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        qs = DTModel.objects.annotate(\n            extracted_year=Extract('start_datetime', 'year'),\n            truncated_month=TruncMonth('start_datetime', output_field=DateTimeField()),\n        ).order_by('start_datetime')\n\n        self.assertQuerysetEqual(\n            qs,\n            [\n                (start_datetime, start_datetime.year, truncate_to(start_datetime, 'month')),\n                (end_datetime, end_datetime.year, truncate_to(end_datetime, 'month')),\n            ],\n            lambda m: (m.start_datetime, m.extracted_year, m.truncated_month)\n        )\n", "    def test_trunc_func_edge_cases(self):\n        # Test truncation at the end of the month\n        start_datetime = datetime(2015, 6, 30, 23, 59, 59, 999999)\n        end_datetime = datetime(2015, 7, 1, 0, 0, 0, 1)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=TruncMonth('start_datetime')).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime, 'month')),\n                (end_datetime, truncate_to(end_datetime, 'month')),\n            ],\n            lambda m: (m.start_datetime, m.extracted)\n        )\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=TruncMonth('start_date')).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.date(), 'month')),\n                (end_datetime, truncate_to(end_datetime.date(), 'month')),\n            ],\n            lambda m: (m.start_datetime, m.extracted)\n        )\n        self.assertEqual(DTModel.objects.filter(start_datetime=TruncMonth('start_datetime')).count(), 1)\n\n        with self.assertRaisesMessage(ValueError, \"Cannot truncate TimeField 'start_time' to DateTimeField\"):\n            list(DTModel.objects.annotate(truncated=TruncMonth('start_time')))\n\n        with self.assertRaisesMessage(ValueError, \"Cannot truncate TimeField 'start_time' to DateTimeField\"):\n            list(DTModel.objects.annotate(truncated=TruncMonth('start_time', output_field=TimeField())))\n\n        # Test truncation at the leap day\n        start_datetime = datetime(2016, 2, 29, 12, 0, 0, 0)\n        end_datetime = datetime(2016, 3, 1, 0, 0, 0, 0)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end", "    def test_trunc_func_with_different_timezones(self):\n        \"\"\"\n        Test truncation to different granularities with different timezones,\n        ensuring the result is correctly truncated and timezone-aware.\n        \"\"\"\n        start_datetime_utc = datetime(2017, 3, 10, 12, 45, 30, 123)\n        end_datetime_utc = datetime(2017, 7, 14, 18, 30, 45, 456)\n        start_datetime_utc = timezone.make_aware(start_datetime_utc, timezone.utc)\n        end_datetime_utc = timezone.make_aware(end_datetime_utc, timezone.utc)\n        self.create_model(start_datetime_utc, end_datetime_utc)\n\n        ny_tz = pytz.timezone('America/New_York')\n        tokyo_tz = pytz.timezone('Asia/Tokyo')\n\n            self.assertQuerysetEqual(\n                DTModel.objects.annotate(\n                    truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=tzinfo)\n                ).order_by('start_datetime'),\n                [\n                    (start_datetime_utc, truncate_to(start_datetime_utc.astimezone(tzinfo), kind, tzinfo)),\n                    (end_datetime_utc, truncate_to(end_datetime_utc.astimezone(tzinfo), kind, tzinfo))\n                ],\n                lambda m: (m.start_datetime, m.truncated)\n            )\n            self.assertEqual(expected_start, truncate_to(start_datetime_utc.astimezone(tzinfo), kind, tzinfo))\n            self.assertEqual(expected_end, truncate_to(end_datetime_utc.astimezone(tzinfo), kind, tzinfo))\n\n        test_trunc_with_tz('year', ny_tz, truncate_to(start_datetime_utc.astimezone(ny_tz), 'year', ny_tz), truncate_to(end_datetime_utc.astimezone(ny_tz), 'year', ny_tz))\n        test_trunc_with_tz('month', tokyo_tz, truncate_to(start_datetime_utc.astimezone(tokyo_tz), 'month', tokyo_tz), truncate_to(end_datetime_utc.astimezone(tokyo_tz), 'month', tokyo_tz))\n        test_trunc_with_tz('day', ny_tz, truncate_to(start_datetime_utc.astimezone(ny_tz), 'day", "    def test_now_func(self):\n        \"\"\"\n        Test the Now function to ensure it returns the current timestamp.\n        \"\"\"\n        # Create a model instance with the current datetime for comparison\n        now = timezone.now()\n        if settings.USE_TZ:\n            now = timezone.make_aware(now, is_dst=False)\n        self.create_model(now, now)\n\n        # Annotate the queryset with the Now function\n        qs = DTModel.objects.annotate(current_timestamp=Now()).order_by('start_datetime')\n        model = qs.get()\n\n        # Check that the current timestamp is close to 'now'\n        self.assertAlmostEqual(model.current_timestamp, now, delta=timedelta(seconds=5))\n\n        with self.assertRaisesMessage(ValueError, \"'name' isn't a DateField, TimeField, or DateTimeField.\"):\n            list(DTModel.objects.annotate(current_timestamp=Now('name')))\n\n        self.assertIsInstance(model.current_timestamp, datetime)\n"], "sample_46": ["    def setUpTestData(cls):\n        cls.objs = [\n            NullableUUIDModel.objects.create(field=uuid.uuid4()),\n            NullableUUIDModel.objects.create(field='550e8400e29b41d4a716446655440000'),\n            NullableUUIDModel.objects.create(field=None),\n        ]\n", "    def setUpTestData(cls):\n        cls.objs = [\n            NullableUUIDModel.objects.create(field=uuid.uuid4()),\n            NullableUUIDModel.objects.create(field='550e8400e29b41d4a716446655440000'),\n            NullableUUIDModel.objects.create(field=None),\n        ]\n", "    def setUpTestData(cls):\n        cls.date_objs = [\n            NullableUUIDModel.objects.create(field=uuid.uuid4()),\n            NullableUUIDModel.objects.create(field='550e8400e29b41d4a716446655440000'),\n            NullableUUIDModel.objects.create(field=None),\n        ]\n", "    def setUpTestData(cls):\n        cls.objs = [\n            NullableUUIDModel.objects.create(field=uuid.uuid4()),\n            NullableUUIDModel.objects.create(field='550e8400e29b41d4a716446655440000'),\n            NullableUUIDModel.objects.create(field=None),\n        ]\n", "    def setUpTestData(cls):\n        cls.year_test_model = NullableUUIDModel.objects.create(field=uuid.uuid4())\n        cls.year = cls.year_test_model.created_at.year\n"], "sample_47": ["    def test_get_post_parameters_with_sensitive_data(self):\n        request = RequestFactory().post('/test', {\n            'username': 'user',\n            'password': 'pass1234',\n            'token': 'abcd1234',\n        })\n        request.sensitive_post_parameters = ['password', 'token']\n        filter = SafeExceptionReporterFilter()\n\n        post_params = filter.get_post_parameters(request)\n        self.assertEqual(post_params['username'], 'user')\n        self.assertEqual(post_params['password'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(post_params['token'], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_with_callable(self):\n            return \"This should not be displayed\"\n        cleansed = cleanse_setting('CALLABLE_SETTING', callable_setting)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed), repr(callable_setting))\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n        self.rf = RequestFactory()\n", "    def test_get_safe_settings(self):\n        safe_settings = get_safe_settings()\n        self.assertNotIn('super_secret_key', safe_settings.values())\n        self.assertNotIn('my_api_key', safe_settings.values())\n        self.assertIn('normal_value', safe_settings.values())\n        self.assertNotIn('inner_secret', safe_settings['NESTED_SETTING'].values())\n        self.assertIn('inner_value', safe_settings['NESTED_SETTING'].values())\n", "    def test_get_cleansed_multivaluedict(self):\n        filter = SafeExceptionReporterFilter()\n        request = self.rf.post('/test/', {'sensitive_key': 'value1', 'normal_key': 'value2'})\n        request.sensitive_post_parameters = ['sensitive_key']\n        cleaned_dict = filter.get_cleansed_multivaluedict(request, request.POST)\n        self.assertEqual(cleaned_dict['sensitive_key'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleaned_dict['normal_key'], 'value2')\n"], "sample_48": ["    def test_variance_and_stddev(self):\n        variance_sample = Book.objects.aggregate(var_sample=Variance('rating', sample=True))\n        self.assertEqual(variance_sample, {'var_sample': Approximate(Decimal('0.4167'), places=4)})\n\n        variance_population = Book.objects.aggregate(var_pop=Variance('rating', sample=False))\n        self.assertEqual(variance_population, {'var_pop': Approximate(Decimal('0.3472'), places=4)})\n\n        stddev_sample = Book.objects.aggregate(stddev_sample=StdDev('rating', sample=True))\n        self.assertEqual(stddev_sample, {'stddev_sample': Approximate(Decimal('0.6455'), places=4)})\n\n        stddev_population = Book.objects.aggregate(stddev_pop=StdDev('rating', sample=False))\n        self.assertEqual(stddev_population, {'stddev_pop': Approximate(Decimal('0.5893'), places=4)})\n", "    def test_distinct_not_allowed(self):\n        msg = \"Max does not allow distinct.\"\n        with self.assertRaisesMessage(TypeError, msg):\n            Author.objects.aggregate(Max('age', distinct=True))\n", "    def test_stddev_variance_aggregate(self):\n        # Test standard deviation and variance for both sample and population\n        # StdDev\n        self.assertEqual(\n            Author.objects.aggregate(stddev_sample_age=StdDev(\"age\", sample=True)),\n            {\"stddev_sample_age\": Approximate(10.84, places=2)}\n        )\n        self.assertEqual(\n            Author.objects.aggregate(stddev_population_age=StdDev(\"age\", sample=False)),\n            {\"stddev_population_age\": Approximate(10.20, places=2)}\n        )\n        \n        # Variance\n        self.assertEqual(\n            Author.objects.aggregate(variance_sample_age=Variance(\"age\", sample=True)),\n            {\"variance_sample_age\": Approximate(117.27, places=2)}\n        )\n        self.assertEqual(\n            Author.objects.aggregate(variance_population_age=Variance(\"age\", sample=False)),\n            {\"variance_population_age\": Approximate(104.20, places=2)}\n        )\n", "    def test_stddev_variance(self):\n        # Test StdDev for sample and population\n        vals_sample = Book.objects.aggregate(stddev_sample=StdDev(\"price\", sample=True))\n        vals_population = Book.objects.aggregate(stddev_population=StdDev(\"price\", sample=False))\n        self.assertIsInstance(vals_sample['stddev_sample'], Decimal)\n        self.assertIsInstance(vals_population['stddev_population'], Decimal)\n        \n        # Test Variance for sample and population\n        vals_sample = Book.objects.aggregate(variance_sample=Variance(\"price\", sample=True))\n        vals_population = Book.objects.aggregate(variance_population=Variance(\"price\", sample=False))\n        self.assertIsInstance(vals_sample['variance_sample'], Decimal)\n        self.assertIsInstance(vals_population['variance_population'], Decimal)\n\n        # Check for correct calculation by comparing sample and population results\n        self.assertLess(vals_sample['variance_sample'], vals_population['variance_population'])\n        self.assertLess(vals_sample['stddev_sample'], vals_population['stddev_population'])\n", "    def test_distinct_aggregate(self):\n        # Test distinct aggregate with Count\n        vals = Author.objects.aggregate(distinct_count=Count('age', distinct=True))\n        self.assertEqual(vals, {\"distinct_count\": 8})\n\n        # Test distinct aggregate with Sum\n        publishers = Publisher.objects.aggregate(distinct_sum=Sum('num_awards', distinct=True))\n        self.assertEqual(publishers, {\"distinct_sum\": 20})\n\n        # Test distinct aggregate with Avg\n        books = Book.objects.filter(rating__lt=5).aggregate(distinct_avg=Avg('price', distinct=True))\n        self.assertEqual(books, {\"distinct_avg\": Approximate(Decimal('38.70'), places=2)})\n\n        # Test distinct aggregate with Min\n        min_age = Author.objects.aggregate(distinct_min=Min('age', distinct=True))\n        self.assertEqual(min_age, {\"distinct_min\": 25})\n\n        # Test distinct aggregate with Max\n        max_rating = Book.objects.aggregate(distinct_max=Max('rating', distinct=True))\n        self.assertEqual(max_rating, {\"distinct_max\": 5.0})\n"], "sample_49": ["    def test_render_js(self):\n        # Test rendering of JavaScript media\n        media = Media(js=['/path/to/js1', '/path/to/js2'])\n        self.assertEqual(\n            media.render_js(),\n            [\n                '<script type=\"text/javascript\" src=\"/path/to/js1\"></script>',\n                '<script type=\"text/javascript\" src=\"/path/to/js2\"></script>',\n            ]\n        )\n", "    def test_media_render_css(self):\n        # Test rendering of CSS media.\n        media = Media(css={'screen': ['screen1.css', 'screen2.css'], 'print': ['print.css']})\n        self.assertEqual(\n            media.render_css(),\n            [\n                '<link href=\"http://media.example.com/static/screen1.css\" type=\"text/css\" media=\"screen\" rel=\"stylesheet\">',\n                '<link href=\"http://media.example.com/static/screen2.css\" type=\"text/css\" media=\"screen\" rel=\"stylesheet\">',\n                '<link href=\"http://media.example.com/static/print.css\" type=\"text/css\" media=\"print\" rel=\"stylesheet\">',\n            ]\n        )\n", "    def test_widget_initialization(self):\n        \"\"\"Test the initialization and basic properties of various widgets.\"\"\"\n\n        # TextInput\n        text_input = TextInput()\n        self.assertEqual(text_input.input_type, 'text')\n        self.assertEqual(text_input.template_name, 'django/forms/widgets/text.html')\n        self.assertFalse(text_input.is_hidden)\n\n        # NumberInput\n        number_input = NumberInput()\n        self.assertEqual(number_input.input_type, 'number')\n        self.assertEqual(number_input.template_name, 'django/forms/widgets/number.html')\n\n        # EmailInput\n        email_input = EmailInput()\n        self.assertEqual(email_input.input_type, 'email')\n        self.assertEqual(email_input.template_name, 'django/forms/widgets/email.html')\n\n        # URLInput\n        url_input = URLInput()\n        self.assertEqual(url_input.input_type, 'url')\n        self.assertEqual(url_input.template_name, 'django/forms/widgets/url.html')\n\n        # PasswordInput without render_value\n        password_input = PasswordInput(render_value=False)\n        self.assertEqual(password_input.input_type, 'password')\n        self.assertEqual(password_input.template_name, 'django/forms/widgets/password.html')\n        self.assertFalse(password_input.render_value)\n\n        # PasswordInput with render_value\n        password_input = PasswordInput(render_value=True)\n        self.assertTrue(password_input.render_value)\n\n        # HiddenInput\n        hidden_input = HiddenInput()\n        self.assertEqual(hidden_input.input_type, 'hidden')\n        self.assertEqual(hidden_input.template_name, 'django/forms/widgets/hidden.html')\n        self.assertTrue(hidden_input.is_hidden)\n\n        # MultipleHiddenInput\n        multiple_hidden_input = MultipleHiddenInput()\n        self.assertEqual(multiple_hidden_input.input_type, 'hidden')\n        self.assertEqual(multiple_hidden_input.template_name, 'django/forms/widgets/multiple_hidden.html')\n\n        # Textarea\n        textarea = Textarea()\n        self.assertEqual(textarea.template_name, 'django/forms/widgets/textarea.html')\n        self.assertEqual(textarea.attrs['cols'], '40')\n        self.assertEqual(textarea.attrs['rows'], '10')\n\n        # FileInput\n        file_input = FileInput()\n        self.assertEqual(file_input.input_type, 'file')\n        self.assertEqual(file_input.template_name, 'django/forms/widgets/file.html')\n        self.assertTrue(file_input.needs_multipart_form)\n", "    def test_input_widget_subclass(self):\n        class CustomTextInput(TextInput):\n            input_type = 'custom_text'\n            template_name = 'django/forms/widgets/custom_text.html'\n\n        widget = CustomTextInput(attrs={'placeholder': 'Enter text', 'class': 'custom-class'})\n        self.assertEqual(widget.input_type, 'custom_text')\n        self.assertEqual(widget.template_name, 'django/forms/widgets/custom_text.html')\n        self.assertEqual(widget.attrs['placeholder'], 'Enter text')\n        self.assertEqual(widget.attrs['class'], 'custom-class')\n        \n        context = widget.get_context('custom_name', 'custom_value', {'id': 'custom_id'})\n        self.assertEqual(context['widget']['type'], 'custom_text')\n        self.assertEqual(context['widget']['name'], 'custom_name')\n        self.assertEqual(context['widget']['value'], 'custom_value')\n        self.assertEqual(context['widget']['attrs']['id'], 'custom_id')\n        self.assertEqual(context['widget']['attrs']['placeholder'], 'Enter text')\n        self.assertEqual(context['widget']['attrs']['class'], 'custom-class')\n\n        html = widget.render('custom_name', 'custom_value', {'id': 'custom_id'})\n        self.assertIn('custom_name', html)\n        self.assertIn('custom_value', html)\n        self.assertIn('custom_id', html)\n        self.assertIn('custom-class', html)\n        self.assertIn('Enter text', html)\n", "    def test_widget_rendering(self):\n        class MyWidget(TextInput):\n            input_type = 'text'\n            template_name = 'django/forms/widgets/text.html'\n\n        widget = MyWidget(attrs={'class': 'my-class'})\n        rendered = widget.render(name='my-widget', value='some value', attrs={'id': 'my-widget-id'})\n        expected_html = '<input type=\"text\" name=\"my-widget\" value=\"some value\" class=\"my-class\" id=\"my-widget-id\">'\n        self.assertInHTML(expected_html, rendered)\n\n        class MyPasswordWidget(PasswordInput):\n            input_type = 'password'\n            template_name = 'django/forms/widgets/password.html'\n\n        password_widget = MyPasswordWidget(attrs={'class': 'password-class'}, render_value=True)\n        rendered_pw = password_widget.render(name='password-widget', value='secret', attrs={'id': 'password-widget-id'})\n        expected_pw_html = '<input type=\"password\" name=\"password-widget\" value=\"secret\" class=\"password-class\" id=\"password-widget-id\">'\n        self.assertInHTML(expected_pw_html, rendered_pw)\n\n        password_widget_no_value = MyPasswordWidget(attrs={'class': 'password-class'}, render_value=False)\n        rendered_pw_no_value = password_widget_no_value.render(name='password-widget', value='secret', attrs={'id': 'password-widget-id'})\n        expected_pw_no_value_html = '<input type=\"password\" name=\"password-widget\" class=\"password-class\" id=\"password-widget-id\">'\n        self.assertInHTML(expected_pw_no_value_html, rendered_pw_no_value)\n"], "sample_50": ["    def test_no_optional_params(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n            }), (\n                ['psql', 'dbname'],\n                None,\n            )\n        )\n", "    def test_missing_optional_params(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n            }), (\n                ['psql', 'dbname'],\n                None,\n            )\n        )\n", "    def test_no_user_no_host(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'port': '444',\n            }), (\n                ['psql', '-p', '444', 'dbname'],\n                None,\n            )\n        )\n", "    def test_no_host_no_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                'somepassword',\n            )\n        )\n", "    def test_no_host_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                'somepassword',\n            )\n        )\n"], "sample_51": ["    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('P1DT1H', timedelta(days=1, hours=1)),\n            ('P1DT1H1M', timedelta(days=1, hours=1, minutes=1)),\n            ('P1DT1H1M1S', timedelta(days=1, hours=1, minutes=1, seconds=1)),\n            ('P1.5D', timedelta(days=1.5)),\n            ('PT1H30M', timedelta(hours=1, minutes=30)),\n            ('PT1H0.5M', timedelta(hours=1, minutes=0.5)),\n            ('P1DT0.5H', timedelta(days=1, hours=0.5)),\n            ('-P1D', timedelta(days=-1)),\n            ('-PT1H', timedelta(hours=-1)),\n            ('-P1DT1H', timedelta(days=-1, hours=-1)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "def test_iso8601_duration(self):\n    test_values = (\n        ('P4DT15H30M1S', timedelta(days=4, hours=15, minutes=30, seconds=1)),\n        ('P4DT15H30M0.1S', timedelta(days=4, hours=15, minutes=30, milliseconds=100)),\n        ('PT15H30M', timedelta(hours=15, minutes=30)),\n        ('P4D', timedelta(days=4)),\n        ('PT0.000001S', timedelta(microseconds=1)),\n        ('-P4DT15H30M1S', timedelta(days=-4, hours=-15, minutes=-30, seconds=-1)),\n        ('-PT15H30M', timedelta(hours=-15, minutes=-30)),\n        ('P0D', timedelta(0)),\n        ('PT0S', timedelta(0)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_iso8601_duration(self):\n    test_values = (\n        ('P1D', timedelta(days=1)),\n        ('P2.5D', timedelta(days=2, hours=12)),\n        ('PT1H', timedelta(hours=1)),\n        ('PT2H30M', timedelta(hours=2, minutes=30)),\n        ('PT0.5H', timedelta(minutes=30)),\n        ('PT30M', timedelta(minutes=30)),\n        ('PT1M', timedelta(minutes=1)),\n        ('PT1M30S', timedelta(minutes=1, seconds=30)),\n        ('PT30S', timedelta(seconds=30)),\n        ('PT0.5S', timedelta(milliseconds=500)),\n        ('P1DT12H', timedelta(days=1, hours=12)),\n        ('-P1D', timedelta(days=-1)),\n        ('-PT1H', timedelta(hours=-1)),\n        ('-PT2H30M', timedelta(hours=-2, minutes=-30)),\n        ('-PT0.5H', timedelta(minutes=-30)),\n        ('-PT30M', timedelta(minutes=-30)),\n        ('-PT1M30S', timedelta(minutes=-1, seconds=-30)),\n        ('-PT30S', timedelta(seconds=-30)),\n        ('-PT0.5S', timedelta(milliseconds=-500)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration_format(self):\n        test_values = (\n            ('P3D', timedelta(days=3)),\n            ('PT10H', timedelta(hours=10)),\n            ('P1DT12H', timedelta(days=1, hours=12)),\n            ('PT1H30M', timedelta(hours=1, minutes=30)),\n            ('PT0.5S', timedelta(seconds=0.5)),\n            ('P1DT1H1M1S', timedelta(days=1, hours=1, minutes=1, seconds=1)),\n            ('P1.5D', timedelta(days=1, seconds=43200)),  # 1.5 days = 1 day and 12 hours\n            ('PT1H30M15.5S', timedelta(hours=1, minutes=30, seconds=15.5)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P4DT15H30M10S', timedelta(days=4, hours=15, minutes=30, seconds=10)),\n            ('P3DT12H', timedelta(days=3, hours=12)),\n            ('PT15M', timedelta(minutes=15)),\n            ('P1D', timedelta(days=1)),\n            ('-P1DT12H', timedelta(days=-1, hours=-12)),\n            ('PT30S', timedelta(seconds=30)),\n            ('PT0S', timedelta(0)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_52": ["    def test_modelmultiplechoicefield_basics(self):\n        f = forms.ModelMultipleChoiceField(Category.objects.all())\n        self.assertEqual(list(f.choices), [\n            ('', '---------'),\n            (self.c1.pk, 'Entertainment'),\n            (self.c2.pk, 'A test'),\n            (self.c3.pk, 'Third'),\n        ])\n        with self.assertRaises(ValidationError):\n            f.clean('')\n        with self.assertRaises(ValidationError):\n            f.clean(None)\n        with self.assertRaises(ValidationError):\n            f.clean([0])\n\n        # Invalid types that require TypeError to be caught.\n        with self.assertRaises(ValidationError):\n            f.clean([['fail']])\n        with self.assertRaises(ValidationError):\n            f.clean([{'foo': 'bar'}])\n\n        self.assertEqual([obj.name for obj in f.clean([self.c2.id])], ['A test'])\n        self.assertEqual([obj.name for obj in f.clean([self.c3.id])], ['Third'])\n\n        # Add a Category object *after* the ModelMultipleChoiceField has already been\n        # instantiated. This proves clean() checks the database during clean()\n        # rather than caching it at instantiation time.\n        c4 = Category.objects.create(name='Fourth', url='4th')\n        self.assertEqual([obj.name for obj in f.clean([c4.id])], ['Fourth'])\n\n        # Delete a Category object *after* the ModelMultipleChoiceField has already been\n        # instantiated. This proves clean() checks the database during clean()\n        # rather than caching it at instantiation time.\n        Category.objects.get(url='4th').delete()\n        msg = \"['Select a valid choice. That choice is not one of the available choices.']\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean([c4.id])\n", "def test_model_multiple_choice_field_basics(self):\n    f = forms.ModelMultipleChoiceField(Category.objects.all())\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n    self.assertEqual(f.clean([]), [])\n    with self.assertRaises(ValidationError):\n        f.clean('')\n    with self.assertRaises(ValidationError):\n        f.clean([0])\n    self.assertEqual([obj.pk for obj in f.clean([self.c2.pk, self.c3.pk])], [self.c2.pk, self.c3.pk])\n    \n    # Add a Category object *after* the ModelMultipleChoiceField has already been instantiated.\n    c4 = Category.objects.create(name='Fourth', slug='4th', url='4th')\n    self.assertEqual([obj.pk for obj in f.clean([c4.pk])], [c4.pk])\n    \n    # Delete a Category object *after* the ModelMultipleChoiceField has already been instantiated.\n    Category.objects.get(url='4th').delete()\n    msg = \"['Select a valid choice. 4 is not one of the available choices.']\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean([c4.pk])\n", "def test_modelmultiplechoicefield_clean(self):\n    f = forms.ModelMultipleChoiceField(Category.objects.all(), required=False)\n    self.assertEqual(f.clean([]), [])\n    self.assertEqual(f.clean([self.c1.pk, self.c2.pk]), [self.c1, self.c2])\n    \n    with self.assertRaises(ValidationError):\n        f.clean(None)\n    \n    with self.assertRaises(ValidationError):\n        f.clean('invalid')\n    \n    with self.assertRaises(ValidationError):\n        f.clean([self.c1.pk, 999])\n    \n    # Ensure that the field works with instances directly\n    self.assertEqual(f.clean([self.c1, self.c2]), [self.c1, self.c2])\n    \n    # Change queryset after field creation\n    f.queryset = Category.objects.exclude(name='Third')\n    with self.assertRaises(ValidationError):\n        f.clean([self.c3.pk])\n    \n    self.assertEqual(f.clean([self.c1.pk, self.c2.pk]), [self.c1, self.c2])\n", "    def test_modelchoicefield_with_limit_choices_to(self):\n        restricted_author = Author.objects.create(name='Restricted Author')\n        unrestricted_author = Author.objects.create(name='Unrestricted Author')\n\n        class AuthorForm(forms.ModelForm):\n            class Meta:\n                model = Book\n                fields = ['author']\n\n        form = AuthorForm()\n        # Initially, both authors should be available.\n        self.assertEqual(list(form.fields['author'].queryset), [restricted_author, unrestricted_author])\n\n        # Apply limit_choices_to to restrict authors.\n        form.fields['author'].limit_choices_to = {'name__icontains': 'Restricted'}\n        form.fields['author'].queryset = form.fields['author'].queryset.filter(form.fields['author'].get_limit_choices_to())\n\n        # Now only the restricted author should be available.\n        self.assertEqual(list(form.fields['author'].queryset), [restricted_author])\n", "def test_modelform_initial_data(self):\n    class CategoryForm(forms.ModelForm):\n        class Meta:\n            model = Category\n            fields = ['name', 'slug', 'url']\n\n    initial_data = {'name': 'Initial Name', 'slug': 'initial-slug', 'url': 'initial-url'}\n    form = CategoryForm(initial=initial_data)\n    self.assertEqual(form.initial['name'], 'Initial Name')\n    self.assertEqual(form.initial['slug'], 'initial-slug')\n    self.assertEqual(form.initial['url'], 'initial-url')\n\n    # Test with instance data\n    category_instance = Category.objects.create(name='Instance Name', slug='instance-slug', url='instance-url')\n    form = CategoryForm(instance=category_instance)\n    self.assertEqual(form.initial['name'], 'Instance Name')\n    self.assertEqual(form.initial['slug'], 'instance-slug')\n    self.assertEqual(form.initial['url'], 'instance-url')\n"], "sample_54": ["    def test_file_with_custom_filename(self):\n        with tempfile.NamedTemporaryFile(suffix='.txt') as tmp:\n            response = FileResponse(tmp, filename='custom_name.txt')\n            self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.txt\"')\n            response.close()\n", "    def test_file_response_with_filename_override(self):\n        with tempfile.NamedTemporaryFile(delete=False) as tmp:\n            tmp.write(b'binary content')\n            tmp_filename = tmp.name\n        try:\n            response = FileResponse(open(tmp_filename, 'rb'), filename='override_name.txt')\n            self.assertEqual(response['Content-Length'], str(os.path.getsize(tmp_filename)))\n            self.assertEqual(response['Content-Type'], 'application/octet-stream')\n            self.assertEqual(response['Content-Disposition'], 'inline; filename=\"override_name.txt\"')\n            response.close()\n        finally:\n            os.remove(tmp_filename)\n", "def test_set_headers_with_custom_filename(self):\n        with tempfile.NamedTemporaryFile() as tmp:\n            response = FileResponse(tmp, filename='custom_name.txt')\n            self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.txt\"')\n            self.assertEqual(response['Content-Type'], 'application/octet-stream')\n", "def test_file_with_custom_filename(self):\n    with tempfile.NamedTemporaryFile(suffix='.txt') as tmp:\n        response = FileResponse(tmp, filename='custom_filename.txt')\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_filename.txt\"')\n", "def test_file_response_with_custom_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename=\"custom_name.bin\")\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.bin\"')\n    self.assertEqual(list(response), [b'binary content'])\n"], "sample_53": ["    def test_media_css(self):\n        media = Media(css={'all': ['styles.css']})\n        rendered_css = media.render_css()\n        expected_css = '<link href=\"/static/styles.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">'\n        self.assertIn(expected_css, list(rendered_css))\n", "    def test_merge_media_lists(self):\n        css1 = {'all': ['style1.css']}\n        css2 = {'all': ['style2.css']}\n        js1 = ['script1.js']\n        js2 = ['script2.js']\n        media1 = Media(css=css1, js=js1)\n        media2 = Media(css=css2, js=js2)\n        combined_media = media1 + media2\n\n        self.assertEqual(combined_media._css, {'all': [['style1.css'], ['style2.css']]})\n        self.assertEqual(combined_media._js, ['script1.js', 'script2.js'])\n", "    def test_widget_format_value(self):\n        from django.forms.widgets import TextInput, NumberInput, EmailInput, URLInput, PasswordInput, HiddenInput, CheckboxInput\n        widgets = [\n            TextInput(),\n            NumberInput(),\n            EmailInput(),\n            URLInput(),\n            PasswordInput(),\n            HiddenInput(),\n            CheckboxInput()\n        ]\n\n        for widget in widgets:\n            self.assertEqual(widget.format_value(None), None)\n            self.assertEqual(widget.format_value(''), None)\n            self.assertEqual(widget.format_value('test'), 'test')\n            self.assertEqual(widget.format_value(123), '123')\n            if isinstance(widget, CheckboxInput):\n                self.assertEqual(widget.format_value(True), None)\n                self.assertEqual(widget.format_value(False), None)\n", "    def test_media_merge_with_duplicate_files(self):\n        css_files_1 = {'all': ['style1.css', 'style2.css']}\n        css_files_2 = {'all': ['style2.css', 'style3.css']}\n        js_files_1 = ['script1.js', 'script2.js']\n        js_files_2 = ['script2.js', 'script3.js']\n        media1 = Media(css=css_files_1, js=js_files_1)\n        media2 = Media(css=css_files_2, js=js_files_2)\n\n        with self.assertRaises(CyclicDependencyError):\n            media_combined = media1 + media2\n", "def test_media_rendering(self):\n    media = Media(css={'all': ['styles.css']}, js=['scripts.js'])\n    rendered_css = media.render_css()\n    rendered_js = media.render_js()\n    self.assertIn('<link href=\"/static/styles.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">', list(rendered_css))\n    self.assertIn('<script type=\"text/javascript\" src=\"/static/scripts.js\"></script>', list(rendered_js))\n"], "sample_55": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.section = Section.objects.create(name='Test section')\n        cls.article1 = Article.objects.create(\n            title='Article 1', content='<p>Content 1</p>', date=datetime.datetime(2020, 1, 1, 10, 0), section=cls.section\n        )\n        cls.article2 = Article.objects.create(\n            title='Article 2', content='<p>Content 2</p>', date=datetime.datetime(2020, 1, 2, 10, 0), section=cls.section\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.category1 = Category.objects.create(name='Category 1')\n        cls.category2 = Category.objects.create(name='Category 2')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def test_inline_formset_data(self):\n        \"\"\"\n        Test that inline_formset_data() returns the correct JSON string for\n        the inline formset's data attributes.\n        \"\"\"\n        inline = InlineAdminFormSet(\n            inline=InlineAdminForm,\n            formset=None,\n            fieldsets=None,\n            prepopulated_fields=None,\n            readonly_fields=None,\n            model_admin=None,\n        )\n        inline.opts.verbose_name = \"example\"\n        inline.formset.prefix = \"formset-prefix\"\n        expected_data = {\n            'name': '#formset-prefix',\n            'options': {\n                'prefix': 'formset-prefix',\n                'addText': gettext('Add another %(verbose_name)s') % {'verbose_name': 'Example'},\n                'deleteText': gettext('Remove'),\n            }\n        }\n        self.assertEqual(json.loads(inline.inline_formset_data()), expected_data)\n"], "sample_58": ["    def test_order_fields(self):\n        # Test that the fields are ordered correctly when order_fields() is called\n        class OrderTestForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = OrderTestForm()\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n\n        # Reorder fields\n        form.order_fields(['field3', 'field1'])\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field2'])\n\n        # Ignore unknown fields\n        form.order_fields(['field2', 'unknown_field'])\n        self.assertEqual(list(form.fields), ['field2', 'field3', 'field1'])\n", "    def test_empty_permitted_required_attribute_error(self):\n        # Ensure empty_permitted and use_required_attribute cannot both be True\n        with self.assertRaises(ValueError):\n            Person(empty_permitted=True, use_required_attribute=True)\n", "def test_callable_initial_value(self):\n    # Test with callable initial value for a CharField\n    class CallableInitialForm(Form):\n        name = CharField(initial=lambda: 'default_name')\n\n    form = CallableInitialForm()\n    self.assertEqual(form['name'].value(), 'default_name')\n\n    # Test with callable initial value for a DateField\n    class CallableInitialDateForm(Form):\n        birthdate = DateField(initial=lambda: datetime.date(2000, 1, 1))\n\n    form = CallableInitialDateForm()\n    self.assertEqual(form['birthdate'].value(), datetime.date(2000, 1, 1))\n\n    # Test with callable initial value for a ChoiceField\n    class CallableInitialChoiceForm(Form):\n        language = ChoiceField(choices=[('P', 'Python'), ('J', 'Java')], initial=lambda: 'P')\n\n    form = CallableInitialChoiceForm()\n    self.assertEqual(form['language'].value(), 'P')\n", "def test_order_fields(self):\n    class ReorderedForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field4 = CharField()\n\n    form = ReorderedForm()\n    form.order_fields(['field3', 'field1'])\n    self.assertEqual(list(form.fields), ['field3', 'field1', 'field2', 'field4'])\n\n    form.order_fields(['field4', 'field2', 'field1'])\n    self.assertEqual(list(form.fields), ['field4', 'field2', 'field1', 'field3'])\n\n    form.order_fields(None)\n    self.assertEqual(list(form.fields), ['field1', 'field2', 'field3', 'field4'])\n\n    form.order_fields(['field2'])\n    self.assertEqual(list(form.fields), ['field2', 'field1', 'field3', 'field4'])\n\n    # Test with unknown field\n    form.order_fields(['unknown_field', 'field1'])\n    self.assertEqual(list(form.fields), ['field1', 'field2', 'field3', 'field4'])\n", "    def test_form_order_fields(self):\n        class OrderedForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n\n        form = OrderedForm()\n        self.assertEqual(list(form.fields.keys()), ['field1', 'field2', 'field3', 'field4'])\n\n        form.order_fields(['field4', 'field1', 'field3'])\n        self.assertEqual(list(form.fields.keys()), ['field4', 'field1', 'field3', 'field2'])\n\n        form.order_fields(['field2', 'field3'])\n        self.assertEqual(list(form.fields.keys()), ['field2', 'field3', 'field4', 'field1'])\n\n        form.order_fields(None)\n        self.assertEqual(list(form.fields.keys()), ['field2', 'field3', 'field4', 'field1'])\n"], "sample_56": ["    def test_invalid_ordering(self):\n        class SongAdmin(admin.ModelAdmin):\n            ordering = ('nonexistent',)\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'ordering[0]' refers to 'nonexistent', which is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E033',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_duplicate_field_in_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = (\n                (None, {\n                    'fields': ('title', 'title')\n                }),\n            )\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets' contains duplicate field(s).\",\n                obj=MyModelAdmin,\n                id='admin.E012'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_invalid_autocomplete_field(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = ['nonexistent']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields[0]' refers to 'nonexistent', which is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E037',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_invalid_autocomplete_fields(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = 'title'\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                obj=SongAdmin,\n                id='admin.E036',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_ordering_with_invalid_field(self):\n        class SongAdmin(admin.ModelAdmin):\n            ordering = ['nonexistent_field']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'ordering[0]' refers to 'nonexistent_field', which is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E033',\n            )\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_57": ["    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context('password', None, {})\n        self.assertEqual(context['summary'], [{'label': str(_(\"No password set.\"))}])\n", "    def test_readonly_field_has_changed(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertFalse(field.has_changed('initial', 'data'))\n", "    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context('password', None, {})\n        self.assertIn('summary', context)\n        self.assertEqual(context['summary'][0]['label'], _(\"No password set.\"))\n", "    def test_get_context_no_password_set(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(name='password', value=None, attrs={})\n        self.assertIn('summary', context)\n        self.assertEqual(context['summary'][0]['label'], _(\"No password set.\"))\n", "    def test_user_permissions_queryset(self):\n        user = User.objects.get(username='testclient')\n        form = UserChangeForm(instance=user)\n        user_permissions_field = form.fields.get('user_permissions')\n        if user_permissions_field:\n            self.assertTrue(\n                user_permissions_field.queryset.query.select_related,\n                msg=\"user_permissions queryset should use select_related on 'content_type'.\"\n            )\n        else:\n            self.fail(\"user_permissions field is not found in the form.\")\n"], "sample_59": ["    def test_model_eq_hash(self):\n        \"\"\"\n        Test for the __eq__ and __hash__ methods of the Model class.\n        \"\"\"\n        d1 = Department.objects.create(id=1, name=\"HR\")\n        d2 = Department.objects.create(id=2, name=\"Finance\")\n        d1_copy = Department.objects.get(pk=d1.pk)\n        \n        # Testing equality\n        self.assertEqual(d1, d1_copy)\n        self.assertNotEqual(d1, d2)\n\n        # Testing hash\n        with self.assertRaises(TypeError):\n            hash(Department(name=\"Unsaved\"))\n\n        self.assertEqual(hash(d1), hash(d1_copy))\n        self.assertNotEqual(hash(d1), hash(d2))\n", "    def test_deferred_fields(self):\n        \"\"\"\n        Test that deferred fields are correctly set and retrieved.\n        \"\"\"\n        a = Article.objects.create(\n            headline=\"Deferred headline\", pub_date=datetime.datetime.now(), article_text=\"Deferred text\"\n        )\n        a_deferred = Article.objects.only('headline').get(pk=a.pk)\n        \n        # Ensure the non-deferred field is correct\n        self.assertEqual(a_deferred.headline, \"Deferred headline\")\n        \n        # Ensure the deferred field is indeed deferred\n        self.assertIn('article_text', a_deferred.get_deferred_fields())\n        \n        # Access the deferred field to trigger its loading\n        self.assertEqual(a_deferred.article_text, \"Deferred text\")\n        \n        # After accessing, the field should not be deferred anymore\n        self.assertNotIn('article_text', a_deferred.get_deferred_fields())\n", "    def test_model_save_update_fields(self):\n        # Testing save() method with the 'update_fields' parameter.\n        a = Article.objects.create(\n            headline=\"Old headline\", pub_date=datetime.datetime.now(), article_text=\"Old text\"\n        )\n        a.headline = \"New headline\"\n        a.article_text = \"New text\"\n        a.save(update_fields=['headline'])\n        updated_a = Article.objects.get(pk=a.pk)\n        self.assertEqual(updated_a.headline, \"New headline\")\n        self.assertEqual(updated_a.article_text, \"Old text\")\n", "    def test_proxy_model_with_fields(self):\n        \"\"\"\n        Proxy models should not contain fields.\n        \"\"\"\n        class NonProxyModel(models.Model):\n            field = models.CharField(max_length=255)\n\n        class ProxyModel(NonProxyModel):\n            class Meta:\n                proxy = True\n\n        with self.assertRaisesMessage(\n            checks.Error,\n            \"Proxy model 'ProxyModel' contains model fields.\"\n        ):\n            ProxyModel.check()\n", "    def test_model_str(self):\n        \"\"\"\n        Test the __str__ method of the Model class.\n        \"\"\"\n        article = Article.objects.create(\n            headline=\"Test Article\", pub_date=datetime.datetime.now()\n        )\n        self.assertEqual(str(article), f\"Article object ({article.pk})\")\n"], "sample_60": ["    def setUp(self):\n        self.site = AdminSite()\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        self.e = Episode.objects.create(name='Test Episode')\n        self.url = reverse('admin:generic_inline_admin_episode_change', args=(self.e.pk,))\n", "    def setUp(self):\n        self.site = AdminSite()\n\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                app_label = 'test_app'\n\n        self.TestModel = TestModel\n\n        class TestModelAdmin(BaseModelAdmin):\n            model = TestModel\n            admin_site = self.site\n\n        self.model_admin = TestModelAdmin(TestModel, self.site)\n        self.factory = RequestFactory()\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.admin_site = AdminSite()\n        self.model = Episode\n        self.model_admin = ModelAdmin(self.model, self.admin_site)\n        self.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n"], "sample_61": ["    def test_ascii_validator(self):\n        valid_usernames = ['user_123', 'john.doe', 'jane+doe', 'doe@domain', 'user-name']\n        invalid_usernames = ['user!', 'us#er', 'us$er', 'us%er', 'us^er', 'us&er', 'us*er', 'us( )er']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'john_doe', 'mary.jane', 'user+name', 'user-name']\n        invalid_usernames = [\n            'Ren\u00e9', '\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30', \"o'connell\", '\u0623\u062d\u0645\u062f',\n            \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\",\n            \"en\\u2013dash\", 'trailingnewline\\u000A',\n        ]\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'john_doe123', 'username+plus', 'email@example.com']\n        invalid_usernames = [\n            'invalid space', 'unicode\u00e9', 'name*with*asterisk', 'colon:name', 'semicolon;name',\n        ]\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'renee', 'bigbird123', 'john_doe', 'jane.doe', 'user+1', 'user@name']\n        invalid_usernames = [\n            'joe!', 'ren\u00e9e', 'big bird', 'john#doe', 'jane%doe', 'user^name', 'user/name', \n            'user\\\\name', 'user|name', 'user<name', 'user>name', 'user,name', 'user;name',\n            'user:name', 'user\"name', 'user\\'name', 'user`name', 'user~name', 'user=name',\n            'user?name', 'user!name', 'user*name', 'user(name', 'user)name', 'user[name',\n            'user]name', 'user{name', 'user}name', 'user_name\\n', 'user_name\\t', 'user_name\\r',\n            'user_name\\f', 'user_name\\v', 'user_name\\b'\n        ]\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'john_doe123', 'jane.doe', 'user@name', 'user+name', 'user-name']\n        invalid_usernames = [\n            'user name', 'user!name', 'user#name', 'user$name', 'user%name', 'user^name',\n            'user&name', 'user*name', 'user(name', 'user)name', 'user=name', 'user{n}ame',\n            'user[n]ame', 'user|name', 'user\\\\name', 'user:name', 'user;\"name', 'user\\'name',\n            'user<name', 'user>name', 'user,name', 'user?name', 'user/name', 'user~name'\n        ]\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n"], "sample_62": ["def test_unregister_model(self):\n    \"Ensure that models can be unregistered.\"\n    self.site.register(Person)\n    self.assertTrue(self.site.is_registered(Person))\n    self.site.unregister(Person)\n    self.assertFalse(self.site.is_registered(Person))\n", "def test_unregister_model(self):\n        \"Ensure a model can be unregistered successfully.\"\n        self.site.register(Person)\n        self.assertTrue(self.site.is_registered(Person))\n        self.site.unregister(Person)\n        self.assertFalse(self.site.is_registered(Person))\n", "    def test_unregister_model(self):\n        \"Unregistering a model should remove it from the registry.\"\n        self.site.register(Person)\n        self.assertTrue(self.site.is_registered(Person))\n        self.site.unregister(Person)\n        self.assertFalse(self.site.is_registered(Person))\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def test_unregister_model(self):\n        \"Unregistering a registered model should remove it from the registry.\"\n        self.site.register(Person)\n        self.assertTrue(self.site.is_registered(Person))\n        self.site.unregister(Person)\n        self.assertFalse(self.site.is_registered(Person))\n"], "sample_63": ["    def test_builtin_context_processors(self):\n        engine = Engine()\n        builtins = [import_string(p) for p in _builtin_context_processors]\n        self.assertEqual(engine.template_context_processors[:len(builtins)], tuple(builtins))\n", "    def setUp(self):\n        self.engine = Engine(dirs=[TEMPLATE_DIR])\n", "    def test_custom_library_import(self):\n        libraries = {'custom': 'path.to.custom.library'}\n        engine = Engine(libraries=libraries)\n        self.assertIn('custom', engine.template_libraries)\n        self.assertEqual(engine.template_libraries['custom'].__name__, 'path.to.custom.library')\n", "    def test_import_library(self):\n        engine = Engine(libraries={'custom_tags': 'myapp.templatetags.custom_tags'})\n        self.assertIn('custom_tags', engine.template_libraries)\n        self.assertEqual(engine.template_libraries['custom_tags'].__name__, 'myapp.templatetags.custom_tags')\n", "    def test_get_template_libraries(self):\n        libraries = {'custom_tags': 'myapp.templatetags.custom_tags'}\n        engine = Engine(libraries=libraries)\n        self.assertIn('custom_tags', engine.template_libraries)\n        self.assertEqual(engine.template_libraries['custom_tags'].__name__, 'myapp.templatetags.custom_tags')\n"], "sample_64": ["    def test_file_response(self):\n        with open(__file__, 'rb') as f:\n            response = FileResponse(f)\n            self.assertTrue(response.streaming)\n            self.assertEqual(response.status_code, 200)\n            self.assertIn('Content-Type', response)\n            self.assertIn('Content-Length', response)\n            self.assertNotIn('Content-Disposition', response)\n", "    def test_file_response(self):\n        # Test FileResponse with a simple file-like object.\n        filename = os.path.join(os.path.dirname(__file__), 'abc.txt')\n        with open(filename, 'w') as f:\n            f.write(\"Hello, world!\")\n\n        with open(filename, 'rb') as f:\n            response = FileResponse(f)\n            content = b''.join(response)\n            self.assertEqual(content, b\"Hello, world!\")\n\n        # Test setting headers correctly.\n        with open(filename, 'rb') as f:\n            response = FileResponse(f, as_attachment=True, filename='download.txt')\n            self.assertIn('Content-Disposition', response)\n            self.assertIn('filename=\"download.txt\"', response['Content-Disposition'])\n\n        os.remove(filename)\n", "    def test_charset_default(self):\n        response = HttpResponse()\n        self.assertEqual(response.charset, settings.DEFAULT_CHARSET)\n    ", "    def test_file_response_with_filelike_object(self):\n        content = b\"Hello, world!\"\n        filelike = io.BytesIO(content)\n        response = FileResponse(filelike)\n        \n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(list(response.streaming_content), [content])\n        ", "    def test_gone_response(self):\n        response = HttpResponseGone()\n        self.assertEqual(response.status_code, 410)\n        # Standard HttpResponse init args can be used\n        response = HttpResponseGone(\n            content='The resource is no longer available',\n            content_type='text/html',\n        )\n        self.assertContains(response, 'The resource is no longer available', status_code=410)\n"], "sample_65": ["    def setUp(self):\n        self.factory = RequestFactory()\n", "    def test_get_plural_function(self):\n        view = JavaScriptCatalog()\n        view.translation = DjangoTranslation('ru', domain='djangojs')\n        plural = view.get_plural()\n        self.assertEqual(plural, '(n%10==1 && n%100!=11) ? 0 : (n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20)) ? 1 : 2')\n", "    def test_plural_forms(self):\n        \"\"\"\n        The JavaScriptCatalog view returns the correct plural forms for the\n        selected language.\n        \"\"\"\n        languages = {\n            'en': 'count == 1 ? 0 : 1',  # English\n            'fr': 'count > 1 ? 1 : 0',    # French\n            'ru': 'count % 10 == 1 && count % 100 != 11 ? 0 : count % 10 >= 2 && count % 10 <= 4 && (count % 100 < 10 || count % 100 >= 20) ? 1 : 2',  # Russian\n        }\n        for lang_code, expected_plural in languages.items():\n            with override(lang_code):\n                response = self.client.get(reverse('javascript-catalog'))\n                self.assertContains(response, f'django.pluralidx = function(n) {{ var v={expected_plural};')\n", "    def test_jsi18n_with_empty_catalog(self):\n        \"\"\"\n        The JavaScriptCatalog should return an empty catalog if no translations are found.\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='en-us'), override('fi'):\n            response = self.client.get('/jsi18n/')\n            self.assertContains(response, 'django.catalog = {};', status_code=200)\n", "    def test_get_plural_with_no_plural_string(self):\n        \"\"\"\n        Test that get_plural returns None when there is no plural string available.\n        \"\"\"\n        view = JavaScriptCatalog()\n        view.translation = gettext.NullTranslations()\n        self.assertIsNone(view.get_plural())\n"], "sample_67": ["    def test_modelform_factory_with_meta_inheritance(self):\n        \"\"\"\n        Ensure that when a form is created using modelform_factory, any custom Meta\n        attributes (such as widgets) are inherited correctly.\n        \"\"\"\n        class CustomMetaForm(forms.ModelForm):\n            class Meta:\n                model = Article\n                fields = '__all__'\n                widgets = {\n                    'headline': forms.Textarea(attrs={'cols': 80, 'rows': 20}),\n                }\n\n        form_class = modelform_factory(Article, form=CustomMetaForm)\n        form = form_class()\n        self.assertIsInstance(form.fields['headline'].widget, forms.Textarea)\n        self.assertEqual(form.fields['headline'].widget.attrs['cols'], 80)\n        self.assertEqual(form.fields['headline'].widget.attrs['rows'], 20)\n", "    def test_construct_instance_all_fields(self):\n        \"\"\"Test construct_instance to ensure all fields are set correctly.\"\"\"\n        class TestModel(models.Model):\n            char_field = models.CharField(max_length=100)\n            int_field = models.IntegerField()\n            bool_field = models.BooleanField(default=False)\n            file_field = models.FileField(upload_to='tests/', blank=True, null=True)\n\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = TestModel\n                fields = '__all__'\n\n        form_data = {'char_field': 'Test', 'int_field': 42, 'bool_field': True}\n        form_files = {'file_field': SimpleUploadedFile('test.txt', b'file content')}\n        form = TestForm(data=form_data, files=form_files)\n        self.assertTrue(form.is_valid())\n        instance = construct_instance(form, TestModel(), fields=['char_field', 'int_field', 'bool_field', 'file_field'])\n        self.assertEqual(instance.char_field, 'Test')\n        self.assertEqual(instance.int_field, 42)\n        self.assertEqual(instance.bool_field, True)\n        self.assertEqual(instance.file_field.name, 'tests/test.txt')\n", "    def setUpTestData(cls):\n        cls.writer1 = Writer.objects.create(name='Author 1')\n        cls.writer2 = Writer.objects.create(name='Author 2')\n        cls.book1 = Book.objects.create(title='Book 1', author=cls.writer1)\n        cls.book2 = Book.objects.create(title='Book 2', author=cls.writer2)\n", "    def test_apply_limit_choices_to(self):\n        class LimitedChoicesForm(forms.ModelForm):\n            class Meta:\n                model = Inventory\n                fields = ['parent']\n\n        parent_inventory = Inventory.objects.create(barcode=86, name='Parent')\n        child_inventory1 = Inventory.objects.create(barcode=87, name='Child1', parent=parent_inventory)\n        child_inventory2 = Inventory.objects.create(barcode=88, name='Child2', parent=parent_inventory)\n\n        field = LimitedChoicesForm().fields['parent']\n        apply_limit_choices_to_to_formfield(field)\n        \n        self.assertQuerysetEqual(\n            field.queryset.order_by('name'),\n            Inventory.objects.filter(id__in=[parent_inventory.id, child_inventory1.id, child_inventory2.id]).order_by('name')\n        )\n", "    def test_fields_and_exclude_as_string(self):\n        \"\"\"\n        Test that fields and exclude parameters in Meta class raise TypeError\n        if they are strings, and not '__all__'.\n        \"\"\"\n        with self.assertRaisesMessage(TypeError, \"TestForm.Meta.fields cannot be a string. Did you mean to type: ('field',)?\"):\n            class TestForm(forms.ModelForm):\n                class Meta:\n                    model = Category\n                    fields = 'name'  # Incorrect usage\n\n        with self.assertRaisesMessage(TypeError, \"TestForm.Meta.exclude cannot be a string. Did you mean to type: ('field',)?\"):\n            class TestForm(forms.ModelForm):\n                class Meta:\n                    model = Category\n                    exclude = 'name'  # Incorrect usage\n"], "sample_66": ["    def test_get_signed_cookie(self):\n        \"\"\"\n        Test retrieval of signed cookies.\n        \"\"\"\n        request = HttpRequest()\n        request.COOKIES['test_cookie'] = signing.get_cookie_signer(salt='test_salt').sign('cookie_value')\n        \n        self.assertEqual(request.get_signed_cookie('test_cookie', salt='test_salt'), 'cookie_value')\n        \n        with self.assertRaises(signing.BadSignature):\n            # Tampered cookie should raise BadSignature\n            request.COOKIES['test_cookie'] = signing.get_cookie_signer(salt='test_salt').sign('cookie_value') + 'tampered'\n            request.get_signed_cookie('test_cookie', salt='test_salt')\n\n        # Test with default value\n        self.assertEqual(request.get_signed_cookie('nonexistent_cookie', default='default_value'), 'default_value')\n\n        # Test without default value and missing cookie should raise KeyError\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('nonexistent_cookie')\n\n        # Test with expired cookie\n        expired_cookie = signing.get_cookie_signer(salt='test_salt').sign('cookie_value')\n        request.COOKIES['test_cookie'] = expired_cookie\n        with self.assertRaises(signing.SignatureExpired):\n            request.get_signed_cookie('test_cookie', salt='test_salt', max_age=-1)\n", "    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES['signed_cookie'] = signing.get_cookie_signer(salt='signed_cookie').sign('cookie_value')\n        self.assertEqual(request.get_signed_cookie('signed_cookie', salt='signed_cookie'), 'cookie_value')\n        \n        # Test cookie with bad signature\n        request.COOKIES['bad_cookie'] = 'bad_value'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('bad_cookie', salt='signed_cookie')\n        \n        # Test default value\n        self.assertEqual(request.get_signed_cookie('nonexistent_cookie', default='default_value'), 'default_value')\n        \n        # Test raising error when default is not provided\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('nonexistent_cookie')\n", "    def test_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES['my_cookie'] = signing.get_cookie_signer(salt='my_cookie').sign('cookie_value')\n        self.assertEqual(request.get_signed_cookie('my_cookie', salt='my_cookie'), 'cookie_value')\n\n        # Test with an invalid signature\n        request.COOKIES['my_cookie'] = 'invalid_signature'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('my_cookie', salt='my_cookie')\n\n        # Test with a default value when signature fails\n        self.assertEqual(request.get_signed_cookie('non_existent_cookie', default='default_value', salt=''), 'default_value')\n", "    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES = {\n            'signed_cookie': signing.get_cookie_signer(salt='salt').sign('cookie_value'),\n            'unsigned_cookie': 'cookie_value'\n        }\n\n        # Test retrieving a signed cookie successfully\n        self.assertEqual(request.get_signed_cookie('signed_cookie', salt='salt'), 'cookie_value')\n\n        # Test retrieving an unsigned cookie with default provided\n        self.assertEqual(request.get_signed_cookie('unsigned_cookie', default='default_value'), 'default_value')\n\n        # Test retrieving an unsigned cookie without default should raise an error\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('unsigned_cookie')\n\n        # Test retrieving a non-existent cookie with default provided\n        self.assertEqual(request.get_signed_cookie('non_existent_cookie', default='default_value'), 'default_value')\n\n        # Test retrieving a non-existent cookie without default should raise an error\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('non_existent_cookie')\n", "    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES = {\n            'valid_signed_cookie': signing.get_cookie_signer(salt='').sign('cookie_value'),\n            'expired_signed_cookie': signing.get_cookie_signer(salt='').sign('cookie_value')\n        }\n\n        # Test valid signed cookie\n        self.assertEqual(request.get_signed_cookie('valid_signed_cookie'), 'cookie_value')\n\n        # Test signed cookie with bad signature\n        request.COOKIES['invalid_signed_cookie'] = 'bad_signature_value'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('invalid_signed_cookie')\n\n        # Test signed cookie with a default value\n        self.assertEqual(request.get_signed_cookie('non_existent_cookie', default='default_value'), 'default_value')\n\n        # Test signed cookie with a default value and bad signature\n        self.assertEqual(request.get_signed_cookie('invalid_signed_cookie', default='default_value'), 'default_value')\n\n        # Test missing signed cookie raises KeyError when default is RAISE_ERROR\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('non_existent_cookie')\n"], "sample_69": ["    def test_ensure_echo_on(self, mocked_stdin, mocked_termios):\n        # Test when termios is not available\n        with mock.patch.object(autoreload, 'termios', None):\n            autoreload.ensure_echo_on()\n            self.assertFalse(mocked_termios.tcgetattr.called)\n\n        # Test when stdin is not a tty\n        mocked_stdin.isatty.return_value = False\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_termios.tcgetattr.called)\n\n        # Test when stdin is a tty and echo is off\n        mocked_stdin.isatty.return_value = True\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n\n        # Test when stdin is a tty and echo is on\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, mocked_termios.ECHO, 0, 0]\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertFalse(mocked_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_with_termios(self, mocked_stdin, mocked_termios):\n        mocked_stdin.isatty.return_value = True\n        attr_list = [0, 0, 0, 0b1111]  # ECHO is the 4th bit\n        mocked_termios.tcgetattr.return_value = attr_list\n\n        autoreload.ensure_echo_on()\n\n        mocked_stdin.isatty.assert_called_once()\n        mocked_termios.tcgetattr.assert_called_once_with(mocked_stdin)\n        mocked_termios.tcsetattr.assert_called_once_with(mocked_stdin, mocked_termios.TCSANOW, attr_list)\n", "    def test_ensure_echo_on(self, mocked_stdin, mocked_termios):\n        # Mock sys.stdin.isatty to return True\n        mocked_stdin.isatty.return_value = True\n\n        # Mock termios.tcgetattr to return a list with ECHO disabled\n        attrs = [0, 0, 0, 0]\n        attrs[3] = 0  # ECHO is disabled\n        mocked_termios.tcgetattr.return_value = attrs\n\n        # Call ensure_echo_on\n        autoreload.ensure_echo_on()\n\n        # Verify ECHO was enabled\n        self.assertTrue(attrs[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called_once_with(mocked_stdin, mocked_termios.TCSANOW, attrs)\n", "    def test_ensure_echo_on_enabled(self, mocked_tcsetattr, mocked_tcgetattr, mocked_stdin):\n        mocked_stdin.isatty.return_value = True\n        mocked_tcgetattr.return_value = [None, None, None, 0b1111 | termios.ECHO]  # ECHO is already enabled\n\n        autoreload.ensure_echo_on()\n\n        mocked_tcgetattr.assert_called_once_with(mocked_stdin)\n        mocked_tcsetattr.assert_not_called()\n", "    def test_ensure_echo_on(self, mocked_signal, mocked_stdin, mocked_termios):\n        attr_list = [0, 0, 0, 0, 0]  # Mock termios attribute list\n        mocked_termios.tcgetattr.return_value = attr_list\n        mocked_stdin.isatty.return_value = True\n        attr_list[3] = 0  # ECHO is off\n\n        autoreload.ensure_echo_on()\n\n        self.assertTrue(attr_list[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called_with(mocked_stdin, mocked_termios.TCSANOW, attr_list)\n        mocked_signal.signal.assert_called_with(mocked_signal.SIGTTOU, mocked_signal.SIG_IGN)\n"], "sample_70": ["    def test_protect_error_message(self):\n        \"\"\"\n        Ensure that the ProtectedError's error message is formatted correctly\n        and the protected objects are stored correctly.\n        \"\"\"\n        r = R.objects.create()\n        a = A.objects.create(protect=r)\n        expected_message = (\n            \"Cannot delete some instances of model 'R' because they are \"\n            \"referenced through a protected foreign key: 'A.protect'\"\n        )\n        try:\n            r.delete()\n        except IntegrityError as e:\n            self.assertIsInstance(e, ProtectedError)\n            self.assertEqual(e.protected_objects, [a])\n            self.assertEqual(str(e), expected_message)\n", "    def test_protected_error_message(self):\n        \"\"\"\n        Test that the ProtectedError exception contains the correct message and protected_objects.\n        \"\"\"\n        a = create_a('protect_error')\n        expected_msg = (\n            \"Cannot delete some instances of model 'R' because they are \"\n            \"referenced through a protected foreign key: 'A.protect'\"\n        )\n        try:\n            a.protect.delete()\n        except ProtectedError as e:\n            self.assertEqual(str(e), expected_msg)\n            self.assertEqual(e.protected_objects, [a.protect])\n", "    def test_protect_error_message(self):\n        a = create_a('protect_message')\n        expected_protected_objects = list(a.protect.all())\n        try:\n            a.protect.delete()\n        except IntegrityError as e:\n            self.assertIsInstance(e, ProtectedError)\n            self.assertEqual(e.protected_objects, expected_protected_objects)\n            self.assertIn(\"Cannot delete some instances of model 'R'\", str(e))\n            self.assertIn(\"because they are referenced through a protected foreign key: 'A.protect'\", str(e))\n", "    def test_cascade_with_intermediary(self):\n        \"\"\"\n        Test the deletion of an intermediary model that has a cascading delete\n        effect on related models.\n        \"\"\"\n        r = R.objects.create()\n        s1 = S.objects.create(r=r)\n        s2 = S.objects.create(r=r)\n        t1 = T.objects.create(s=s1)\n        t2 = T.objects.create(s=s2)\n        t3 = T.objects.create(s=s1)\n        r.delete()\n        self.assertFalse(S.objects.filter(r=r).exists())\n        self.assertFalse(T.objects.filter(s__r=r).exists())\n", "    def test_collect_method(self):\n        \"\"\"\n        Ensure the `collect` method correctly collects related objects and respects \n        the keep_parents and reverse_dependency flags.\n        \"\"\"\n        parent = Parent.objects.create()\n        child1 = Child.objects.create(parent=parent)\n        child2 = Child.objects.create(parent=parent)\n        \n        collector = Collector(using='default')\n        collector.collect([parent], keep_parents=True)\n        \n        # Parent should be collected but not its children since keep_parents=True\n        self.assertIn(parent, collector.data[Parent])\n        self.assertNotIn(child1, collector.data[Child])\n        self.assertNotIn(child2, collector.data[Child])\n        \n        collector = Collector(using='default')\n        collector.collect([parent], reverse_dependency=True)\n        \n        # Parent should be collected before its children since reverse_dependency=True\n        self.assertIn(parent, collector.data[Parent])\n        self.assertIn(child1, collector.data[Child])\n        self.assertIn(child2, collector.data[Child])\n"], "sample_71": ["    def test_edge_cases(self):\n        # Test very small Decimal values near zero with different decimal positions\n        self.assertEqual(nformat(Decimal('0.00000000000001'), '.', decimal_pos=16), '0.0000000000000100')\n        self.assertEqual(nformat(Decimal('0.00000000000001'), '.', decimal_pos=17), '0.00000000000001000')\n        self.assertEqual(nformat(Decimal('0.00000000000001'), '.', decimal_pos=18), '0.000000000000010000')\n        \n        # Test very large Decimal values with different grouping\n        self.assertEqual(\n            nformat(Decimal('123456789012345678901234567890'), '.', grouping=3, thousand_sep=',', force_grouping=True),\n            '123,456,789,012,345,678,901,234,567,890'\n        )\n        self.assertEqual(\n            nformat(Decimal('123456789012345678901234567890'), '.', grouping=(4, 3), thousand_sep=',', force_grouping=True),\n            '1,234,567,890,1234,5678,9012,3456,7890'\n        )\n        \n        # Test Decimal with negative exponent\n        self.assertEqual(nformat(Decimal('1e-12'), '.', decimal_pos=12), '0.000000000001')\n        self.assertEqual(nformat(Decimal('1e-13'), '.', decimal_pos=13), '0.0000000000001')\n        \n        # Test a string that looks like scientific notation but isn't valid\n        self.assertEqual(nformat('1e2', '.'), '1e2')\n        self.assertEqual(nformat('3.14e', '.'), '3.14e')\n", "def test_edge_cases(self):\n    # Test zero\n    self.assertEqual(nformat(0, '.'), '0')\n    self.assertEqual(nformat(Decimal('0.00'), '.'), '0.00')\n    self.assertEqual(nformat(Decimal('0'), '.', decimal_pos=2), '0.00')\n\n    # Test negative zero\n    self.assertEqual(nformat(Decimal('-0.00'), '.'), '0.00')\n    self.assertEqual(nformat(Decimal('-0.00'), '.', decimal_pos=2), '0.00')\n\n    # Test a large negative number with grouping\n    self.assertEqual(nformat(-9876543210, '.', grouping=3, thousand_sep=',', force_grouping=True), '-9,876,543,210')\n\n    # Test number as string with high decimal precision\n    self.assertEqual(nformat('1234.56789012345678901234567890', '.', decimal_pos=25), '1234.5678901234567890123456789')\n\n    # Test large number with scientific notation without forcing grouping\n    self.assertEqual(nformat(Decimal('1e10'), '.', grouping=3, thousand_sep=','), '10000000000')\n\n    # Test empty string input\n    self.assertEqual(nformat('', '.'), '')\n\n    # Test non-number string input\n    self.assertEqual(nformat('abc', '.'), 'abc')\n", "    def test_format_with_non_integer_grouping(self):\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n        self.assertEqual(nformat(123456789, '.', grouping=(4, 3, 0), thousand_sep=','), '123,45,6789')\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=',', force_grouping=True), '12,34,56,789')\n        self.assertEqual(nformat(123456789, '.', grouping=(4, 3, 0), thousand_sep=',', force_grouping=True), '123,45,6789')\n", "    def test_large_decimal_subclass(self):\n        class LargeEuroDecimal(Decimal):\n            \"\"\"\n            Wrapper for Decimal which prefixes each amount with the \u20ac symbol\n            and handles very large values.\n            \"\"\"\n                amount = super().__format__(specifier, **kwargs)\n                return '\u20ac {}'.format(amount)\n\n        large_price = LargeEuroDecimal('1e100')\n        self.assertEqual(nformat(large_price, '.', thousand_sep=',', grouping=3, force_grouping=True), '\u20ac 1,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000')\n", "    def test_format_numbers_with_various_grouping(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=3, thousand_sep=','), '1,234,567,890')\n        self.assertEqual(nformat(1234567890, '.', grouping=[3, 2], thousand_sep=','), '12,34,567,890')\n        self.assertEqual(nformat(1234567890, '.', grouping=[4, 3, 2], thousand_sep=','), '1,23,4567,890')\n        self.assertEqual(nformat(1234567890, '.', grouping=[1, 2, 3], thousand_sep=','), '1,2,345,678,90')\n"], "sample_72": ["    def test_serialize_custom_class(self):\n        class CustomClass:\n                self.value = value\n\n                return (\n                    '%s.%s' % (self.__class__.__module__, self.__class__.__name__),\n                    [self.value],\n                    {}\n                )\n\n        instance = CustomClass('test_value')\n        self.assertSerializedResultEqual(\n            instance,\n            (\"migrations.test_writer.CustomClass('test_value')\", {'import migrations.test_writer'})\n        )\n        self.assertSerializedEqual(instance)\n", "    def test_serialize_custom_class_with_deconstruct(self):\n        class CustomClass:\n                return (\n                    '%s.%s' % (self.__class__.__module__, self.__class__.__name__),\n                    [1, 2, 3],\n                    {'arg1': 'value1'}\n                )\n\n        custom_obj = CustomClass()\n        string, imports = MigrationWriter.serialize(custom_obj)\n        self.assertEqual(\n            string,\n            \"migrations.test_writer.CustomClass(1, 2, 3, arg1='value1')\"\n        )\n        self.assertEqual(imports, {'import migrations.test_writer'})\n", "    def test_serialize_custom_class(self):\n        \"\"\"\n        Test serialization of a custom class with a custom serializer.\n        \"\"\"\n        class CustomClass:\n                self.value = value\n\n        class CustomClassSerializer(BaseSerializer):\n                return 'CustomClass(%r)' % self.value.value, {\"import migrations.test_writer\"}\n\n        # Register custom serializer\n        Serializer.register(CustomClass, CustomClassSerializer)\n        \n        value = CustomClass('test')\n        string, imports = MigrationWriter.serialize(value)\n        self.assertEqual(string, \"CustomClass('test')\")\n        self.assertEqual(imports, {'import migrations.test_writer'})\n        \n        # Unregister custom serializer to clean up\n        Serializer.unregister(CustomClass)\n", "    def test_serialize_custom_class(self):\n        class CustomClass:\n                return (\n                    '%s.%s' % (self.__class__.__module__, self.__class__.__name__),\n                    [],\n                    {},\n                )\n\n        custom_instance = CustomClass()\n        self.assertSerializedEqual(custom_instance)\n        self.assertSerializedResultEqual(\n            custom_instance,\n            (\"migrations.test_writer.CustomClass()\", {'import migrations.test_writer'})\n        )\n", "    def test_serialize_custom_class_with_deconstruct(self):\n        class CustomDeconstructible:\n                self.param1 = param1\n                self.param2 = param2\n\n                path = f\"{self.__class__.__module__}.{self.__class__.__name__}\"\n                return (path, [self.param1, self.param2], {})\n\n        custom_instance = CustomDeconstructible(10, \"test\")\n        self.assertSerializedResultEqual(\n            custom_instance,\n            (\"migrations.test_writer.CustomDeconstructible(10, 'test')\", {'import migrations.test_writer'})\n        )\n        self.assertSerializedEqual(custom_instance)\n"], "sample_73": ["    def setUp(self):\n        self.storage = ManifestFilesMixin(location=tempfile.mkdtemp(), base_url='/static/')\n        self.test_file_name = 'test.txt'\n        self.test_file_content = 'This is a test file'\n        self.file_path = os.path.join(self.storage.location, self.test_file_name)\n        with open(self.file_path, 'w') as f:\n            f.write(self.test_file_content)\n", "    def test_stored_name(self):\n        name = \"cached/styles.css\"\n        hashed_name = \"cached/styles.5e0040571e1a.css\"\n        # Ensure the cached name is stored correctly\n        cache_key = storage.staticfiles_storage.hash_key(name)\n        self.assertEqual(storage.staticfiles_storage.stored_name(name), hashed_name)\n\n        # Check stored name with non-existent file in manifest_strict mode\n        missing_file_name = \"cached/missing.css\"\n        configured_storage = storage.staticfiles_storage\n        with self.assertRaisesMessage(ValueError, f\"Missing staticfiles manifest entry for '{missing_file_name}'\"):\n            configured_storage.stored_name(missing_file_name)\n\n        # Check stored name with non-existent file in non-strict mode\n        configured_storage.manifest_strict = False\n        err_msg = f\"The file '{missing_file_name}' could not be found with {configured_storage._wrapped}.\"\n        with self.assertRaisesMessage(ValueError, err_msg):\n            configured_storage.stored_name(missing_file_name)\n\n        # Save and check stored name with newly added file\n        content = StringIO()\n        content.write('Found')\n        configured_storage.save(missing_file_name, content)\n        self.assertEqual(configured_storage.stored_name(missing_file_name), configured_storage.hashed_name(missing_file_name))\n", "    def test_hashed_name_with_empty_content(self):\n        \"\"\"\n        Test the behavior of `hashed_name` when content is None.\n        \"\"\"\n        name = \"test/file.txt\"\n        # When content is None, hashed_name should return the original name\n        hashed_name = storage.staticfiles_storage.hashed_name(name, content=None)\n        self.assertEqual(hashed_name, name)\n", "    def test_hashed_name_generation(self):\n        \"\"\"\n        Test if the hashed_name method correctly generates hashed filenames\n        with and without provided content and filename.\n        \"\"\"\n        storage_class = storage.staticfiles_storage\n        test_file_name = \"test/file.txt\"\n\n        # Create test content\n        content = ContentFile(b\"Test content for hashing\")\n        content_hash = storage_class.file_hash(test_file_name, content)\n\n        # Test hashed_name with content\n        hashed_name_with_content = storage_class.hashed_name(test_file_name, content=content)\n        self.assertIn(content_hash, hashed_name_with_content)\n\n        # Save content to storage for testing hashed_name without content\n        saved_name = storage_class._save(test_file_name, content)\n\n        # Test hashed_name without content\n        hashed_name_no_content = storage_class.hashed_name(saved_name)\n        self.assertIn(content_hash, hashed_name_no_content)\n\n        # Test hashed_name with different filename but same content\n        different_file_name = \"test/other_file.txt\"\n        hashed_name_different_filename = storage_class.hashed_name(different_file_name, content=content, filename=saved_name)\n        self.assertIn(content_hash, hashed_name_different_filename)\n", "    def setUp(self):\n        self.old_storage = storage.staticfiles_storage\n        self.old_storage_class = settings.STATICFILES_STORAGE\n        settings.STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.StaticFilesStorage'\n        self.storage = ConfiguredStorage()\n        self.storage._setup()\n"], "sample_75": ["    def setUpTestData(cls):\n        cls.book = Book.objects.create(title='Test Book')\n        cls.author = Author.objects.create(name='Test Author', first_book=cls.book)\n        cls.bio = Bio.objects.create(author=cls.author)\n        cls.favorite_author = FavoriteAuthors.objects.create(author=cls.author, likes_author=cls.author)\n", "    def setUp(self):\n        self.book = Book.objects.create(title='Poems')\n        self.author = Author.objects.create(name='Jane', first_book=self.book)\n", "    def test_related_name_is_valid(self):\n        # Testing related name that is a valid identifier\n        with self.assertRaises(checks.Error):\n            field = RelatedField(related_name='invalid name')\n            field._check_related_name_is_valid()\n\n        # Testing related name that is a keyword\n        with self.assertRaises(checks.Error):\n            field = RelatedField(related_name='for')\n            field._check_related_name_is_valid()\n\n        # Testing related name that ends with '+'\n        field = RelatedField(related_name='validname+')\n        errors = field._check_related_name_is_valid()\n        self.assertEqual(errors, [])\n", "    def setUpTestData(cls):\n        cls.book = Book.objects.create(title='Unique Poems')\n        cls.author1 = Author.objects.create(name='Jane', first_book=cls.book)\n        cls.author2 = Author.objects.create(name='Tom', first_book=cls.book)\n        cls.author3 = Author.objects.create(name='Robert', first_book=cls.book)\n        cls.author_address1 = AuthorAddress.objects.create(author=cls.author1, address='UniqueStreet 1')\n        cls.author_address2 = AuthorAddress.objects.create(author=cls.author2, address='UniqueStreet 2')\n        cls.favorite1 = FavoriteAuthors.objects.create(author=cls.author1, likes_author=cls.author2)\n        cls.favorite2 = FavoriteAuthors.objects.create(author=cls.author2, likes_author=cls.author3)\n        cls.favorite3 = FavoriteAuthors.objects.create(author=cls.author3, likes_author=cls.author1)\n", "    def setUp(self):\n        self.book1 = Book.objects.create(title='Book1')\n        self.book2 = Book.objects.create(title='Book2')\n        self.author1 = Author.objects.create(name='Author1', first_book=self.book1)\n        self.author2 = Author.objects.create(name='Author2', first_book=self.book2)\n        self.book1.authors.add(self.author1)\n        self.book2.authors.add(self.author2)\n"], "sample_74": ["    def test_nohost_noport(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_no_user(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'host': 'somehost',\n                'port': '444',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-h', 'somehost', '-p', '444', 'dbname'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_empty_params(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql', ''],\n                {},\n            )\n        )\n", "    def test_no_user(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'host': 'somehost',\n                'port': '444',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-h', 'somehost', '-p', '444', 'dbname'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_no_host_no_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n"], "sample_76": ["def test_language_settings_consistent(self):\n    with self.settings(LANGUAGES=[('en', 'English')], LANGUAGE_CODE='en'):\n        self.assertEqual(check_language_settings_consistent(None), [])\n    with self.settings(LANGUAGES=[('fr', 'French')], LANGUAGE_CODE='es'):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(\n                'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.',\n                id='translation.E004',\n            )\n        ])\n    with self.settings(LANGUAGES=[('es', 'Spanish')], LANGUAGE_CODE='en-us'):\n        self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_language_code_not_in_languages(self):\n        with self.settings(LANGUAGE_CODE='fr-CA', LANGUAGES=[('en', 'English'), ('es', 'Spanish')]):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(\n                    'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.',\n                    id='translation.E004',\n                ),\n            ])\n", "def test_language_code_not_in_languages(self):\n    msg = 'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.'\n    with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'English'), ('es', 'Spanish')]):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(msg, id='translation.E004'),\n        ])\n", "    def test_language_code_not_in_languages(self):\n        with self.settings(LANGUAGE_CODE='de', LANGUAGES=[('en', 'English'), ('fr', 'French')]):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004')\n            ])\n\n        with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('en', 'English'), ('fr', 'French')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_language_code_not_in_languages(self):\n    valid_languages = [(tag, tag) for tag in self.valid_tags if tag != 'en']\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=valid_languages):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(\n                'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.',\n                id='translation.E004',\n            )\n        ])\n"], "sample_77": ["    def test_avoid_wrapping(self):\n        tests = (\n            ('Hello World', 'Hello\\xa0World'),\n            ('No wrapping here', 'No\\xa0wrapping\\xa0here'),\n            ('Django is awesome', 'Django\\xa0is\\xa0awesome'),\n            ('Python 3.8', 'Python\\xa03.8'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"hello world\", \"hello\\xa0world\"),\n            (\"a b c\", \"a\\xa0b\\xa0c\"),\n            (\"keep spaces intact\", \"keep\\xa0spaces\\xa0intact\"),\n            (\"multiple   spaces\", \"multiple\\xa0\\xa0\\xa0spaces\"),\n            (\"trailing space \", \"trailing\\xa0space\\xa0\"),\n            (\" leading space\", \"\\xa0leading\\xa0space\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        tests = (\n            ('Avoid wrapping this text', 'Avoid\\xa0wrapping\\xa0this\\xa0text'),\n            ('Already no wrapping', 'Already\\xa0no\\xa0wrapping'),\n            ('Preserve multiple spaces', 'Preserve\\xa0multiple\\xa0spaces'),\n            ('Leading and trailing spaces ', 'Leading\\xa0and\\xa0trailing\\xa0spaces\\xa0'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        tests = (\n            ('hello world', 'hello\\xa0world'),\n            ('a b c d', 'a\\xa0b\\xa0c\\xa0d'),\n            ('no spaces', 'no\\xa0spaces'),\n            ('multiple   spaces', 'multiple\\xa0\\xa0\\xa0spaces'),\n            (lazystr('lazy string test'), 'lazy\\xa0string\\xa0test'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            ('This is a test', 'This\\xa0is\\xa0a\\xa0test'),\n            ('Avoid wrapping text', 'Avoid\\xa0wrapping\\xa0text'),\n            ('Django utils', 'Django\\xa0utils'),\n            ('Non-breaking spaces', 'Non-breaking\\xa0spaces'),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n"], "sample_78": ["def test_command_parser_error_message(self):\n        \"\"\"CommandParser should raise CommandError with a custom error message.\"\"\"\n        parser = CommandParser(missing_args_message=\"Custom missing args message.\")\n        with self.assertRaises(CommandError) as cm:\n            parser.parse_args([])\n        self.assertEqual(str(cm.exception), \"Error: Custom missing args message.\")\n", "    def test_create_parser_missing_args(self):\n        \"\"\"\n        CommandParser should raise a CommandError with a custom missing_args_message\n        if no positional arguments are provided.\n        \"\"\"\n        class TestCommand(BaseCommand):\n            missing_args_message = \"You must provide at least one argument.\"\n\n                parser.add_argument('args', nargs='*')\n\n                pass\n\n        command = TestCommand()\n        parser = command.create_parser('prog_name', 'subcommand')\n\n        with self.assertRaisesMessage(CommandError, \"You must provide at least one argument.\"):\n            parser.parse_args([])\n", "    def test_output_wrapper(self):\n        \"\"\"Test the OutputWrapper class for proper wrapping and styling.\"\"\"\n        out = StringIO()\n        wrapper = OutputWrapper(out)\n        wrapper.write(\"Test message\")\n        self.assertEqual(out.getvalue(), \"Test message\\n\")\n\n        out = StringIO()\n        wrapper = OutputWrapper(out)\n        wrapper.write(\"Test message\", ending=\"!!!\")\n        self.assertEqual(out.getvalue(), \"Test message!!!\")\n\n        out = StringIO()\n        wrapper = OutputWrapper(out)\n        wrapper.write(\"Test message\", style_func=str.upper)\n        self.assertEqual(out.getvalue(), \"TEST MESSAGE\\n\")\n", "    def test_command_parser_error_message(self):\n        \"\"\"\n        Ensure CommandParser provides a better error message for missing arguments.\n        \"\"\"\n        parser = CommandParser(missing_args_message=\"Enter required arguments.\")\n        with self.assertRaises(CommandError) as cm:\n            parser.parse_args([])\n        self.assertEqual(str(cm.exception), \"Error: Enter required arguments.\")\n", "    def test_command_error_handling(self):\n        \"\"\"\n        Ensure CommandError is correctly handled and raises SystemExit \n        with the appropriate message.\n        \"\"\"\n        class TestCommand(BaseCommand):\n                raise CommandError('This is a test error.')\n\n        with self.assertRaises(SystemExit) as cm:\n            management.call_command(TestCommand())\n\n        self.assertEqual(cm.exception.code, 1)\n        self.assertIn('This is a test error.', cm.exception.args[0])\n"], "sample_79": ["    def test_addslashes(self):\n        self.assertEqual(addslashes('Quotes \" and \\' must be escaped'), 'Quotes \\\\\" and \\\\\\' must be escaped')\n        self.assertEqual(addslashes('Backslash \\\\ must be doubled'), 'Backslash \\\\\\\\ must be doubled')\n", "    def test_basic_list(self):\n        output = self.engine.render_to_string('t', {'value': ['Item 1', 'Item 2']})\n        self.assertHTMLEqual(output, \"<li>Item 1</li>\\n<li>Item 2</li>\")\n", "    def test_filesizeformat(self):\n        self.assertEqual(filesizeformat(0), '0 bytes')\n        self.assertEqual(filesizeformat(1), '1 byte')\n        self.assertEqual(filesizeformat(1023), '1023 bytes')\n        self.assertEqual(filesizeformat(1024), '1 KB')\n        self.assertEqual(filesizeformat(1048576), '1 MB')\n        self.assertEqual(filesizeformat(1073741824), '1 GB')\n        self.assertEqual(filesizeformat(1099511627776), '1 TB')\n        self.assertEqual(filesizeformat(1125899906842624), '1 PB')\n        self.assertEqual(filesizeformat(-1024), '-1 KB')\n        self.assertEqual(filesizeformat('invalid'), '0 bytes')\n", "    def test_filesizeformat(self):\n        self.assertEqual(filesizeformat(0), \"0 bytes\")\n        self.assertEqual(filesizeformat(1023), \"1023 bytes\")\n        self.assertEqual(filesizeformat(1024), \"1.0 KB\")\n        self.assertEqual(filesizeformat(1048576), \"1.0 MB\")\n        self.assertEqual(filesizeformat(1073741824), \"1.0 GB\")\n        self.assertEqual(filesizeformat(1099511627776), \"1.0 TB\")\n        self.assertEqual(filesizeformat(1125899906842624), \"1.0 PB\")\n        self.assertEqual(filesizeformat(-1024), \"-1.0 KB\")\n        self.assertEqual(filesizeformat(\"invalid\"), \"0 bytes\")\n", "    def test_filesizeformat(self):\n        self.assertEqual(filesizeformat(1023), '1023 bytes')\n        self.assertEqual(filesizeformat(1024), '1.0 KB')\n        self.assertEqual(filesizeformat(1048576), '1.0 MB')\n        self.assertEqual(filesizeformat(1073741824), '1.0 GB')\n        self.assertEqual(filesizeformat(1099511627776), '1.0 TB')\n        self.assertEqual(filesizeformat(1125899906842624), '1.0 PB')\n        self.assertEqual(filesizeformat(-1024), '-1.0 KB')\n        self.assertEqual(filesizeformat('invalid'), '0 bytes')\n"], "sample_82": ["def test_render_partial_date(self):\n    \"\"\"\n    Rendering a partial date (e.g., missing day) should still render the partial date.\n    \"\"\"\n    self.check_html(self.widget, 'mydate', '2010-04-', html=(\n        \"\"\"\n        <select name=\"mydate_month\" id=\"id_mydate_month\">\n            <option value=\"\">---</option>\n            <option value=\"1\">January</option>\n            <option value=\"2\">February</option>\n            <option value=\"3\">March</option>\n            <option value=\"4\" selected>April</option>\n            <option value=\"5\">May</option>\n            <option value=\"6\">June</option>\n            <option value=\"7\">July</option>\n            <option value=\"8\">August</option>\n            <option value=\"9\">September</option>\n            <option value=\"10\">October</option>\n            <option value=\"11\">November</option>\n            <option value=\"12\">December</option>\n        </select>\n\n        <select name=\"mydate_day\" id=\"id_mydate_day\">\n            <option selected value=\"\">---</option>\n            <option value=\"1\">1</option>\n            <option value=\"2\">2</option>\n            <option value=\"3\">3</option>\n            <option value=\"4\">4</option>\n            <option value=\"5\">5</option>\n            <option value=\"6\">6</option>\n            <option value=\"7\">7</option>\n            <option value=\"8\">8</option>\n            <option value=\"9\">9</option>\n            <option value=\"10\">10</option>\n            <option value=\"11\">11</option>\n            <option value=\"12\">12</option>\n            <option value=\"13\">13</option>\n            <option value=\"14\">14</option>\n            <option value=\"15\">15</option>\n            <option value=\"16\">16</option>\n            <option value=\"17\">17</option>\n            <option value=\"18\">18</option>\n            <option value=\"19\">19</option>\n            <option value=\"20\">20</option>\n            <option value=\"21\">21</option>\n            <option value=\"22\">22</option>\n            <option value=\"23\">23</option>\n            <option value=\"24\">24</option>\n            <option value=\"25", "    def test_render_with_custom_attrs(self):\n        widget = SelectDateWidget(attrs={'class': 'custom-class'})\n        self.check_html(widget, 'mydate', '2012-05-10', html=(\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\" class=\"custom-class\">\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\" selected>May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\" class=\"custom-class\">\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\" selected>10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"19\">19</option>\n                <option value=\"20\">20</option>\n                <option value=\"21\">21</option>\n                <option value=\"22\">22</option>\n                <option value=\"23\">23</option>\n                <option value=\"24\">24</option>\n               ", "    def test_selectdate_default_years(self):\n        \"\"\"\n        Test the default years range if not provided during initialization.\n        \"\"\"\n        widget = SelectDateWidget()\n        current_year = datetime.date.today().year\n        expected_years = list(range(current_year, current_year + 10))\n        self.assertEqual(list(widget.years), expected_years)\n", "    def test_value_from_datadict_invalid(self):\n        widget = SelectDateWidget(years=('2000', '2001'))\n        data = {'field_year': '2000', 'field_month': '13', 'field_day': '1'}\n        self.assertEqual(widget.value_from_datadict(data, {}, 'field'), '2000-13-1')\n\n        data = {'field_year': '2000', 'field_month': '02', 'field_day': '30'}\n        self.assertEqual(widget.value_from_datadict(data, {}, 'field'), '2000-02-30')\n\n        data = {'field_year': '1000', 'field_month': '12', 'field_day': '31'}\n        self.assertEqual(widget.value_from_datadict(data, {}, 'field'), '1000-12-31')\n", "def test_invalid_empty_label(self):\n        \"\"\"\n        Ensure that passing an invalid empty_label raises a ValueError.\n        \"\"\"\n        with self.assertRaises(ValueError):\n            SelectDateWidget(years=('2014',), empty_label=('too', 'few'))\n        \n        with self.assertRaises(ValueError):\n            SelectDateWidget(years=('2014',), empty_label=('too', 'many', 'values', 'here'))\n"], "sample_81": ["    def test_resolve_match(self):\n            pass\n\n        pattern = RoutePattern('test/')\n        url_pattern = URLPattern(pattern, my_view, name='test')\n        match = url_pattern.resolve('test/')\n\n        self.assertIsNotNone(match)\n        self.assertEqual(match.func, my_view)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {})\n        self.assertEqual(match.url_name, 'test')\n        self.assertEqual(match.route, 'test/')\n", "    def test_resolve(self):\n        from django.http import HttpResponse\n            return HttpResponse('Hello, world!')\n        \n        pattern = RoutePattern('hello/')\n        url_pattern = URLPattern(pattern, my_view)\n        \n        resolver_match = url_pattern.resolve('hello/')\n        self.assertIsNotNone(resolver_match)\n        self.assertEqual(resolver_match.func, my_view)\n        self.assertEqual(resolver_match.args, ())\n        self.assertEqual(resolver_match.kwargs, {})\n        self.assertEqual(resolver_match.url_name, None)\n        self.assertEqual(resolver_match.route, 'hello/')\n", "    def setUp(self):\n        self.pattern = RegexPattern(r'^test/$')\n        self.urlconf = 'test_urlconf'\n        self.resolver = URLResolver(self.pattern, self.urlconf)\n", "    def test_pattern_startswith_slash_warning(self):\n        pattern = RegexPattern('^/start_with_slash/')\n        warnings = pattern._check_pattern_startswith_slash()\n        self.assertEqual(len(warnings), 1)\n        self.assertEqual(warnings[0].id, 'urls.W002')\n        self.assertIn(\"Your URL pattern\", warnings[0].msg)\n", "    def test_repr(self):\n        resolver = URLResolver(RegexPattern(r'^/'), 'test_urlconf', app_name='test_app', namespace='test_ns')\n        self.assertEqual(\n            repr(resolver),\n            \"<URLResolver 'test_urlconf' (test_app:test_ns) '^/'>\"\n        )\n"], "sample_83": ["    def test_import_library_valid(self):\n        with self.assertRaises(InvalidTemplateLibrary):\n            import_library('non_existent_module')\n", "    def test_import_library_valid(self):\n        # Assuming there is a valid template tag module named 'valid_module' with a register attribute.\n        with self.settings(INSTALLED_APPS=['valid_module']):\n            from valid_module import register  # Placeholder for actual import\n            self.assertIs(import_library('valid_module'), register)\n", "    def test_import_library_success(self):\n        with self.modify_settings(INSTALLED_APPS={'append': 'django.contrib.admin'}):\n            library = import_library('django.contrib.admin.templatetags.admin_list')\n            self.assertIsInstance(library, Library)\n", "    def test_tag_invalid(self):\n        msg = \"Unsupported arguments to Library.tag: (None, '')\"\n        with self.assertRaisesMessage(ValueError, msg):\n            self.library.tag(None, '')\n", "    def test_import_library_valid(self):\n        module_name = 'django.template.defaulttags'\n        library = import_library(module_name)\n        self.assertIsInstance(library, Library)\n"], "sample_85": ["    def test_check_related_name_is_valid(self):\n        field = models.ForeignKey(\n            'self', on_delete=models.CASCADE, related_name='related_name'\n        )\n        self.assertEqual(field._check_related_name_is_valid(), [])\n\n        field = models.ForeignKey(\n            'self', on_delete=models.CASCADE, related_name='invalid related_name'\n        )\n        errors = field._check_related_name_is_valid()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'fields.E306')\n", "    def test_related_name_is_valid(self):\n        class TestModel(models.Model):\n            field = models.ForeignKey('self', on_delete=models.CASCADE, related_name='test_related_name')\n\n        field = TestModel._meta.get_field('field')\n        errors = field.check()\n        self.assertEqual(errors, [])\n", "    def test_related_name_validity(self):\n        class ModelA(models.Model):\n            pass\n\n        class ModelB(models.Model):\n            a = models.ForeignKey(ModelA, on_delete=models.CASCADE, related_name='valid_related_name')\n\n        field = ModelB._meta.get_field('a')\n        errors = field.check()\n        self.assertEqual(errors, [])\n", "    def test_foreign_key_init(self):\n        # Test initialization of ForeignKey with valid parameters\n        fk = models.ForeignKey(User, on_delete=models.CASCADE)\n        self.assertEqual(fk.remote_field.model, User)\n        self.assertEqual(fk.remote_field.on_delete, models.CASCADE)\n", "    def test_related_name_validation(self):\n        class RelatedModel(models.Model):\n            pass\n\n        class TestModel(models.Model):\n            valid_related_name = models.ForeignKey(\n                RelatedModel,\n                on_delete=models.CASCADE,\n                related_name='valid_related_name'\n            )\n            invalid_related_name = models.ForeignKey(\n                RelatedModel,\n                on_delete=models.CASCADE,\n                related_name='invalid related name'\n            )\n\n        field = TestModel._meta.get_field('invalid_related_name')\n        errors = field._check_related_name_is_valid()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'fields.E306')\n"], "sample_84": ["    def test_limited_parse_qsl(self):\n        qs = 'a=1&b=2&c=3'\n        self.assertEqual(limited_parse_qsl(qs), [('a', '1'), ('b', '2'), ('c', '3')])\n", "    def test_basic_parsing(self):\n        self.assertEqual(limited_parse_qsl('a=1&b=2&c=3'), [('a', '1'), ('b', '2'), ('c', '3')])\n", "    def test_urlquote(self):\n        self.assertEqual(urlquote('test quote'), 'test%20quote')\n        self.assertEqual(urlquote(''), '')\n        self.assertEqual(urlquote('test/quote', safe='/'), 'test/quote')\n", "    def test_valid_date(self):\n        self.assertEqual(\n            parse_http_date_safe('Sun, 06 Nov 1994 08:49:37 GMT'),\n            calendar.timegm(datetime(1994, 11, 6, 8, 49, 37).utctimetuple())\n        )\n", "    def test_parse_qsl_no_fields_limit(self):\n        qs = 'a=1&b=2&c=3'\n        self.assertEqual(limited_parse_qsl(qs), [('a', '1'), ('b', '2'), ('c', '3')])\n"], "sample_88": ["    def test_attach_mimebase(self):\n        \"\"\"Test attaching an existing MIMEBase object to the email.\"\"\"\n        mime_base = MIMEBase('application', 'octet-stream')\n        mime_base.set_payload(b'Some binary content')\n        Encoders.encode_base64(mime_base)\n        mime_base.add_header('Content-Disposition', 'attachment', filename='file.bin')\n\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        email.attach(mime_base)\n        message = email.message()\n\n        # Verify that the attachment is correctly added\n        attachments = [part for part in message.walk() if part.get_content_disposition() == 'attachment']\n        self.assertEqual(len(attachments), 1)\n        self.assertEqual(attachments[0].get_filename(), 'file.bin')\n        self.assertEqual(attachments[0].get_payload(decode=True), b'Some binary content')\n        self.assertEqual(attachments[0].get_content_type(), 'application/octet-stream')\n", "    def test_sanitize_address_invalid_characters(self):\n        \"\"\"\n        Test that sanitize_address raises a ValueError for addresses with invalid characters.\n        \"\"\"\n        invalid_addresses = [\n            'user@domain.com\\x00',  # Null character\n            'user@domain..com',     # Double dot in domain\n            'user@-domain.com',     # Domain starts with hyphen\n            'user@domain-.com',     # Domain ends with hyphen\n            'user@domain.com-',     # Ends with hyphen\n            'user@domain..com',     # Double dot\n            'user@domain.com.',     # Ends with dot\n            'user@.domain.com',     # Starts with dot\n            'user@do_main.com',     # Underscore in domain\n            'user@domain.com,',     # Comma in domain\n            'user@domain com',      # Space in domain\n        ]\n        for address in invalid_addresses:\n            with self.subTest(address=address):\n                with self.assertRaises(ValueError):\n                    sanitize_address(address, encoding='utf-8')\n", "    def test_attach_non_ascii_text(self):\n        \"\"\"\n        Attaching non-ASCII text files correctly handles encoding.\n        \"\"\"\n        email = EmailMessage('subject', 'body', 'from@example.com', ['to@example.com'])\n        email.attach('file.txt', 'some non-ascii text: \u00e0\u00e1\u00e4\u00e7\u00e8\u00e9\u00ea')\n        sent_num = email.send()\n        self.assertEqual(sent_num, 1)\n        filename, content, mimetype = self.get_decoded_attachments(email)[0]\n        self.assertEqual(filename, 'file.txt')\n        self.assertEqual(content, 'some non-ascii text: \u00e0\u00e1\u00e4\u00e7\u00e8\u00e9\u00ea'.encode('utf-8'))\n        self.assertEqual(mimetype, 'text/plain')\n", "    def test_unicode_attachment_filename_with_long_lines(self):\n        \"\"\"\n        Ensure that email with non-ASCII attachment filenames and long lines in body\n        are correctly handled.\n        \"\"\"\n        headers = {\"Date\": \"Fri, 09 Nov 2001 01:08:47 -0000\", \"Message-ID\": \"foo\"}\n        subject, from_email, to = 'hello', 'from@example.com', 'to@example.com'\n        content = 'This is the message.' + ' A' * 2000  # Create a long line in the body\n        msg = EmailMessage(subject, content, from_email, [to], headers=headers)\n        msg.attach(\"an attachment with unicode name - une pi\u00e8ce jointe.pdf\", b\"%PDF-1.4.%...\", mimetype=\"application/pdf\")\n        msg_bytes = msg.message().as_bytes()\n        message = message_from_bytes(msg_bytes)\n        payload = message.get_payload()\n\n        self.assertEqual(payload[1].get_filename(), '=?utf-8?b?dW5lIHBpw6hzZSBqb2ludGUucGRm?=')  # Ensure filename is correctly encoded\n        self.assertIn('=?utf-8?b?VGhpcyBpcyB0aGUgbWVzc2FnZS4gQSBBIEEgQSBBIEEgQSBBIEEgQSBBIEEgQSBBIA==?=', message.as_string())\n        self.assertEqual(message['Content-Transfer-Encoding'], 'quoted-printable')\n", "    def test_invalid_email_address(self):\n        \"\"\"Test sending email with invalid address format should raise ValueError.\"\"\"\n        invalid_addresses = [\n            'plainaddress',\n            '@missingusername.com',\n            'username@.com',\n            'username@missingdomain',\n            'username@domain.c',\n            'username@domain..com'\n        ]\n        for addr in invalid_addresses:\n            with self.assertRaises(ValueError):\n                EmailMessage('Subject', 'Content', 'from@example.com', [addr]).send()\n"], "sample_87": ["    def test_ensure_echo_on_disabled(self, mocked_isatty, mocked_termios):\n        # Mock termios.tcgetattr to return a list with ECHO disabled.\n        attr_list = [0, 0, 0, 0, 0, 0]\n        attr_list[3] = 0  # Disable ECHO\n        mocked_termios.tcgetattr.return_value = attr_list\n\n        autoreload.ensure_echo_on()\n        \n        # ECHO should be enabled.\n        self.assertTrue(attr_list[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called_once_with(sys.stdin, mocked_termios.TCSANOW, attr_list)\n", "    def test_echo_on_enabled(self, mocked_isatty, mocked_termios):\n        attrs = [0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attrs\n\n        autoreload.ensure_echo_on()\n\n        mocked_termios.tcgetattr.assert_called_once_with(sys.stdin)\n        mocked_termios.tcsetattr.assert_not_called()  # Because ECHO is already enabled\n", "    def test_ensure_echo_on(self, mocked_stdin, mocked_termios):\n        attr_list = [0, 0, 0, 0]\n        mocked_stdin.isatty.return_value = True\n        mocked_termios.tcgetattr.return_value = attr_list\n\n        # ECHO is not set\n        mocked_termios.ECHO = 0b1000  # Assuming ECHO is the 4th bit\n        autoreload.ensure_echo_on()\n        self.assertEqual(attr_list[3], 0b1000)\n        mocked_termios.tcsetattr.assert_called_once_with(mocked_stdin, mocked_termios.TCSANOW, attr_list)\n\n        # ECHO is already set\n        attr_list[3] |= mocked_termios.ECHO\n        mocked_termios.tcsetattr.reset_mock()\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_enabled(self, mocked_stdin, mocked_termios):\n        # Mocking termios and sys.stdin to simulate a scenario where echo is disabled.\n        mocked_stdin.isatty.return_value = True\n        mocked_termios.tcgetattr.return_value = [None, None, None, 0]\n        autoreload.ensure_echo_on()\n        mocked_termios.tcgetattr.assert_called_once_with(mocked_stdin)\n        mocked_termios.tcsetattr.assert_not_called()\n", "    def test_ensure_echo_on(self, mocked_tcsetattr, mocked_tcgetattr, mocked_isatty):\n        mocked_tcgetattr.return_value = [None, None, None, 0]\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_tcsetattr.called)\n        \n        mocked_tcgetattr.return_value = [None, None, None, 0, None, None, None, None]\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_tcsetattr.called)\n\n        mocked_tcgetattr.return_value = [None, None, None, 0, None, None, None, None, None]\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_tcsetattr.called)\n\n        mocked_tcgetattr.return_value = [None, None, None, 0, None, None, None, None, None, None]\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_tcsetattr.called)\n\n        mocked_tcgetattr.return_value = [None, None, None, 0, None, None, None, None, None, None, None]\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_tcsetattr.called)\n\n        mocked_tcgetattr.return_value = [None, None, None, 0, None, None, None, None, None, None, None, None]\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_tcsetattr.called)\n        \n        mocked_tcgetattr.return_value = [None, None, None, 0, None, None, None, None, None, None, None, None, None]\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_tcsetattr.called)\n\n        mocked_tcgetattr.return_value = [None, None, None, 0, None, None, None, None, None, None, None, None, None, None]\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_tcsetattr.called)\n\n        mocked_tcgetattr.return_value = [None, None, None, 0, None, None, None, None, None, None, None, None, None,"], "sample_89": ["    def test_ensure_echo_on_with_echo_disabled(self, mocked_isatty, mocked_termios):\n        attr_list = [0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attr_list\n        autoreload.ensure_echo_on()\n        self.assertTrue(attr_list[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called_once_with(\n            sys.stdin, mocked_termios.TCSANOW, attr_list\n        )\n", "    def test_ensure_echo_on_with_no_termios(self):\n        with mock.patch('django.utils.autoreload.termios', None):\n            # Ensure that ensure_echo_on doesn't raise any exceptions.\n            autoreload.ensure_echo_on()\n", "    def test_ensure_echo_on(self, mock_signal, mock_tcsetattr, mock_tcgetattr, mock_isatty, mock_termios):\n        # Mock termios attributes\n        mock_tcgetattr.return_value = [None, None, None, 0b11111111]\n\n        autoreload.ensure_echo_on()\n\n        # Check that tcsetattr is called if ECHO is not set\n        self.assertEqual(mock_tcsetattr.call_count, 1)\n        self.assertEqual(mock_signal.call_count, 1)\n", "    def test_ensure_echo_on_enabled(self, mocked_isatty, mocked_termios):\n        # Setup termios to simulate echo being off\n        mocked_termios.tcgetattr.return_value = [None, None, None, 0, None, None, None, 0]\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_without_termios(self):\n        with mock.patch('django.utils.autoreload.termios', None):\n            with mock.patch('django.utils.autoreload.sys.stdin.isatty', return_value=True):\n                autoreload.ensure_echo_on()  # No exception\n"], "sample_90": ["    def test_construct_instance_with_all_fields(self):\n        \"\"\"\n        Test constructing an instance with all fields provided.\n        \"\"\"\n        class AuthorForm(forms.ModelForm):\n            class Meta:\n                model = Author\n                fields = '__all__'\n\n        publication = Publication.objects.create(title=\"Sample Pub\", date_published=datetime.date(2023, 1, 1))\n        author = Author.objects.create(publication=publication, full_name='John Doe')\n\n        form = AuthorForm(data={'publication': publication.pk, 'full_name': 'Jane Doe'}, instance=author)\n        self.assertTrue(form.is_valid())\n        instance = construct_instance(form, form.instance, fields=['publication', 'full_name'])\n        self.assertEqual(instance.full_name, 'Jane Doe')\n        self.assertEqual(instance.publication, publication)\n", "    def test_modelform_options_initialization(self):\n        class Meta:\n            model = Category\n            fields = ['name', 'url']\n            exclude = ['slug']\n            widgets = {'name': forms.TextInput()}\n            localized_fields = ['name']\n            labels = {'name': 'Category Name'}\n            help_texts = {'name': 'Enter the category name.'}\n            error_messages = {'name': {'required': 'Name is required.'}}\n            field_classes = {'name': forms.CharField}\n        \n        options = ModelFormOptions(Meta)\n        \n        self.assertEqual(options.model, Category)\n        self.assertEqual(options.fields, ['name', 'url'])\n        self.assertEqual(options.exclude, ['slug'])\n        self.assertEqual(options.widgets, {'name': forms.TextInput()})\n        self.assertEqual(options.localized_fields, ['name'])\n        self.assertEqual(options.labels, {'name': 'Category Name'})\n        self.assertEqual(options.help_texts, {'name': 'Enter the category name.'})\n        self.assertEqual(options.error_messages, {'name': {'required': 'Name is required.'}})\n        self.assertEqual(options.field_classes, {'name': forms.CharField})\n", "    def test_modelform_metaclass_fields(self):\n        \"\"\"\n        Ensure that ModelFormMetaclass correctly processes 'fields' and 'exclude'\n        attributes, and raises appropriate errors when they are misconfigured.\n        \"\"\"\n        class TestModelForm(forms.ModelForm):\n            class Meta:\n                model = Category\n                fields = ['name']\n\n        form = TestModelForm()\n        self.assertEqual(list(form.fields), ['name'])\n\n        class TestModelFormExclude(forms.ModelForm):\n            class Meta:\n                model = Category\n                exclude = ['name']\n\n        form = TestModelFormExclude()\n        self.assertEqual(list(form.fields), ['slug', 'url'])\n\n        class TestModelFormAllFields(forms.ModelForm):\n            class Meta:\n                model = Category\n                fields = '__all__'\n\n        form = TestModelFormAllFields()\n        self.assertEqual(list(form.fields), ['name', 'slug', 'url'])\n\n        with self.assertRaisesMessage(ImproperlyConfigured, \"Creating a ModelForm without either the 'fields' attribute or the 'exclude' attribute is prohibited; form MissingAttributesForm needs updating.\"):\n            class MissingAttributesForm(forms.ModelForm):\n                class Meta:\n                    model = Category\n\n        with self.assertRaisesMessage(TypeError, \"TestModelFormStringFields.Meta.fields cannot be a string. Did you mean to type: ('name',)?\"):\n            class TestModelFormStringFields(forms.ModelForm):\n                class Meta:\n                    model = Category\n                    fields = 'name'\n\n        with self.assertRaisesMessage(TypeError, \"TestModelFormStringExclude.Meta.exclude cannot be a string. Did you mean to type: ('name',)?\"):\n            class TestModelFormStringExclude(forms.ModelForm):\n                class Meta:\n                    model = Category\n                    exclude = 'name'\n", "    def test_model_to_dict_include_fields(self):\n        \"\"\"\n        model_to_dict should include only specified fields if fields parameter\n        is provided.\n        \"\"\"\n        book = Book.objects.create(title=\"Test Book\", author=\"John Doe\")\n        data = model_to_dict(book, fields=['title'])\n        self.assertEqual(data, {'title': 'Test Book'})\n", "    def test_modelform_options_initialization(self):\n        class MetaOptions:\n            model = Category\n            fields = ['name', 'slug']\n            exclude = ['url']\n            widgets = {'name': forms.Textarea}\n            localized_fields = ['name']\n            labels = {'name': 'Category Name'}\n            help_texts = {'slug': 'Enter a unique slug'}\n            error_messages = {'slug': {'required': 'Slug is required'}}\n            field_classes = {'name': forms.CharField}\n\n        opts = ModelFormOptions(MetaOptions)\n\n        self.assertEqual(opts.model, Category)\n        self.assertEqual(opts.fields, ['name', 'slug'])\n        self.assertEqual(opts.exclude, ['url'])\n        self.assertEqual(opts.widgets, {'name': forms.Textarea})\n        self.assertEqual(opts.localized_fields, ['name'])\n        self.assertEqual(opts.labels, {'name': 'Category Name'})\n        self.assertEqual(opts.help_texts, {'slug': 'Enter a unique slug'})\n        self.assertEqual(opts.error_messages, {'slug': {'required': 'Slug is required'}})\n        self.assertEqual(opts.field_classes, {'name': forms.CharField})\n"], "sample_91": ["    def test_permission_denied(self):\n        \"\"\"\n        The 403 error handler should render the custom template and provide exception details.\n        \"\"\"\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception(\"Testing 403 error.\"))\n        self.assertContains(response, 'This is a test template for a 403 error', status_code=403)\n        self.assertContains(response, 'exception: Testing 403 error.', status_code=403)\n", "    def test_permission_denied(self):\n        \"\"\"\n        The permission_denied view raises a 403 status and uses the custom template if provided.\n        \"\"\"\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception('Custom permission denied message'))\n        self.assertContains(response, \"test template for a 403 error\", status_code=403)\n        self.assertContains(response, \"exception: Custom permission denied message\", status_code=403)\n\n        # Test default template rendering when the custom template is not available.\n        with override_settings(TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'OPTIONS': {\n                'loaders': [\n                    ('django.template.loaders.locmem.Loader', {}),\n                ],\n            },\n        }]):\n            response = permission_denied(request, Exception('Default permission denied message'))\n            self.assertContains(response, \"<h1>403 Forbidden</h1>\", status_code=403)\n", "    def test_permission_denied(self):\n        \"\"\"\n        The permission_denied view raises a 403 status\n        \"\"\"\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception(\"Permission Denied\"))\n        self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n", "    def test_permission_denied(self):\n        \"\"\"\n        The permission_denied view raises a 403 status and uses the custom template\n        if available.\n        \"\"\"\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception('Access Denied'))\n        self.assertContains(response, 'test template for a 403 error', status_code=403)\n        self.assertContains(response, 'exception: Access Denied', status_code=403)\n", "    def test_permission_denied(self):\n        \"\"\"\n        The 403 page should use the custom template if provided.\n        \"\"\"\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception(\"Permission Denied\"))\n        self.assertContains(response, \"test template for a 403 error\", status_code=403)\n        self.assertContains(response, \"exception: Permission Denied\", status_code=403)\n"], "sample_92": ["    def setUpTestData(cls):\n        cls.user_credentials = {'remote_user': 'remote_test_user'}\n        cls.user = User.objects.create_user(\n            username='remote_test_user',\n            email='remote_test_user@example.com',\n            password='test_password'\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(username='test', email='test@example.com', password='test')\n        cls.inactive_user = User.objects.create_user(username='inactive_test', email='inactive_test@example.com', password='test', is_active=False)\n    ", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            email='remoteuser@example.com', **cls.user_credentials\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username='test', email='test@example.com', password='test', is_active=False\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username='testremote', email='testremote@example.com', is_active=False, password='test'\n        )\n"], "sample_93": ["    def test_combined_expression_sql(self):\n        combined_expression = CombinedExpression(F('rating'), '+', F('pages'))\n        sql, params = combined_expression.as_sql(compiler=None, connection=connection)\n        expected_sql = '(\"rating\" + \"pages\")'\n        self.assertEqual(sql, expected_sql)\n        self.assertEqual(params, [])\n\n        combined_expression = CombinedExpression(Value(10), '*', F('pages'))\n        sql, params = combined_expression.as_sql(compiler=None, connection=connection)\n        expected_sql = '(10 * \"pages\")'\n        self.assertEqual(sql, expected_sql)\n        self.assertEqual(params, [])\n", "    def test_combined_expression(self):\n        # Test CombinedExpression with addition\n        combined_expr = CombinedExpression(F('age'), CombinedExpression.ADD, F('friends__age'))\n        author = Author.objects.annotate(combined_age=combined_expr).get(name='Adrian Holovaty')\n        self.assertEqual(author.combined_age, 66)  # 34 + 32\n\n        # Test CombinedExpression with subtraction\n        combined_expr = CombinedExpression(F('age'), CombinedExpression.SUB, F('friends__age'))\n        author = Author.objects.annotate(combined_age=combined_expr).get(name='Adrian Holovaty')\n        self.assertEqual(author.combined_age, 2)  # 34 - 32\n\n        # Test CombinedExpression with multiplication\n        combined_expr = CombinedExpression(F('age'), CombinedExpression.MUL, F('friends__age'))\n        author = Author.objects.annotate(combined_age=combined_expr).get(name='Adrian Holovaty')\n        self.assertEqual(author.combined_age, 1088)  # 34 * 32\n\n        # Test CombinedExpression with division\n        combined_expr = CombinedExpression(F('age'), CombinedExpression.DIV, F('friends__age'))\n        author = Author.objects.annotate(combined_age=combined_expr).get(name='Adrian Holovaty')\n        self.assertAlmostEqual(author.combined_age, 1.0625, places=4)  # 34 / 32\n", "def test_combined_expression_sqlite_numeric_mixin(self):\n    combined_expr = CombinedExpression(F('rating'), CombinedExpression.ADD, Value(2.0, output_field=FloatField()), output_field=DecimalField())\n    with connection.cursor() as cursor:\n        compiler = cursor.db.compiler('default')\n        sql, params = combined_expr.as_sql(compiler, connection)\n        self.assertIn('CAST', sql)\n        self.assertIn('AS NUMERIC', sql)\n        self.assertEqual(params, [])\n\n    combined_expr = CombinedExpression(F('rating'), CombinedExpression.ADD, Value(2.0, output_field=FloatField()), output_field=FloatField())\n    with connection.cursor() as cursor:\n        compiler = cursor.db.compiler('default')\n        sql, params = combined_expr.as_sql(compiler, connection)\n        self.assertNotIn('CAST', sql)\n        self.assertNotIn('AS NUMERIC', sql)\n        self.assertEqual(params, [])\n", "    def test_combined_expression(self):\n        authors = Author.objects.annotate(\n            combined_age=F('age') + F('friends__age')\n        ).order_by('name')\n        self.assertQuerysetEqual(\n            authors, [\n                ('Adrian Holovaty', 66),\n                ('Brad Dayley', None),\n                ('Jacob Kaplan-Moss', 64),\n                ('James Bennett', 63),\n                ('Jeffrey Forcier', 66),\n                ('Paul Bissex', 60),\n                ('Peter Norvig', 103),\n                ('Stuart Russell', 103),\n                ('Wesley J. Chun', Approximate(84.66, places=1)),\n            ],\n            lambda a: (a.name, a.combined_age)\n        )\n", "    def test_combined_expression_as_sqlite(self):\n        expression = CombinedExpression(Value(3, output_field=IntegerField()), CombinedExpression.ADD, Value(4, output_field=IntegerField()))\n        compiler = Author.objects.all().query.get_compiler(connection=connection)\n        sql, params = expression.as_sqlite(compiler, connection)\n        self.assertEqual(sql, 'CAST(%s + %s AS NUMERIC)')\n        self.assertEqual(params, [3, 4])\n"], "sample_94": ["    def test_missing_required_fields(self):\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, 'You must use --email with --noinput.'):\n            call_command(\n                'createsuperuser',\n                interactive=False,\n                username='joe',\n                stdout=new_io,\n            )\n", "    def test_blank_username_and_email_non_interactive(self):\n        new_io = StringIO()\n\n            with self.assertRaisesMessage(CommandError, 'Username cannot be blank.'):\n                call_command(\n                    'createsuperuser',\n                    username='',\n                    email='',\n                    interactive=False,\n                    stdin=MockTTY(),\n                    stdout=new_io,\n                    stderr=new_io,\n                )\n\n        test(self)\n", "    def test_create_superuser_interactive(self):\n        \"\"\"Test creating a superuser interactively.\"\"\"\n        new_io = StringIO()\n        call_command(\n            'createsuperuser',\n            interactive=True,\n            stdin=MockTTY(),\n            stdout=new_io,\n            stderr=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        user = User.objects.get(username='testuser')\n        self.assertEqual(user.email, 'testuser@example.com')\n        self.assertTrue(user.check_password('testpass'))\n", "    def setUp(self):\n        self.stdout = StringIO()\n        self.stderr = StringIO()\n", "    def test_handle_validation_error(self):\n        \"\"\"\n        Tests that a ValidationError in handle() method correctly raises a CommandError.\n        \"\"\"\n        # Inject a validation error into the User model's save method.\n        with mock.patch.object(User._default_manager, 'create_superuser', side_effect=exceptions.ValidationError(['Test validation error'])):\n            new_io = StringIO()\n            with self.assertRaisesMessage(CommandError, 'Test validation error'):\n                call_command(\n                    'createsuperuser',\n                    interactive=True,\n                    stdin=MockTTY(),\n                    stdout=new_io,\n                    stderr=new_io,\n                )\n"], "sample_98": ["    def test_broken_pipe_error_handling(self):\n        \"\"\"\n        Ensure that the server correctly identifies and logs broken pipe errors.\n        \"\"\"\n        # Create a socket to simulate a client connection\n        client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        server_address = (self.server_thread.host, self.server_thread.port)\n        client_socket.connect(server_address)\n        \n        # Send an incomplete request to cause a broken pipe error\n        client_socket.sendall(b\"GET /example_view/ HTTP/1.1\\r\\nHost: localhost\\r\\n\")\n        client_socket.close()  # Closing the socket should trigger a broken pipe on the server\n\n        # Check the log for broken pipe error message\n        with self.assertLogs('django.server', level='INFO') as log:\n            self.urlopen('/example_view/')\n        self.assertIn('Broken pipe from', log.output[0])\n", "    def test_handle_broken_pipe_error(self):\n        \"\"\"\n        Test that the server handles BrokenPipeError correctly.\n        \"\"\"\n        with self.assertLogs('django.server', level='INFO') as cm:\n            conn = HTTPConnection(\n                WSGIServerErrorHandlingTests.server_thread.host,\n                WSGIServerErrorHandlingTests.server_thread.port,\n                timeout=1,\n            )\n            try:\n                conn.request('POST', '/method_view/', b'{}', headers={\"Connection\": \"keep-alive\"})\n                conn.sock.shutdown(socket.SHUT_RDWR)  # Simulate a client dropping the connection\n                conn.close()\n            except BrokenPipeError:\n                pass  # Expected error\n\n        self.assertIn('Broken pipe from', cm.output[0])\n", "    def test_broken_pipe_error_logging(self):\n        \"\"\"\n        Test that broken pipe errors are logged correctly by the server.\n        \"\"\"\n        with self.assertLogs('django.server', level='INFO') as cm:\n            conn = HTTPConnection(\n                LiveServerErrorHandling.server_thread.host,\n                LiveServerErrorHandling.server_thread.port,\n                timeout=1,\n            )\n            try:\n                conn.request('GET', '/broken_pipe_view/')\n                response = conn.getresponse()\n                # Forcefully close the connection to simulate broken pipe\n                conn.close()\n                self.assertEqual(response.status, 200)\n            except (BrokenPipeError, ConnectionResetError):\n                pass\n            self.assertTrue(any(\"Broken pipe from\" in message for message in cm.output))\n", "    def test_broken_pipe_error_handling(self):\n        \"\"\"\n        Test that the server logs a broken pipe error and continues to function.\n        \"\"\"\n        # Set up a connection and simulate a broken pipe\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port, timeout=1)\n        try:\n            conn.request('GET', '/example_view/')\n            # Close the connection abruptly to simulate a broken pipe\n            conn.sock.shutdown(socket.SHUT_RDWR)\n            conn.sock.close()\n        except OSError:\n            # Expected error due to shutdown\n            pass\n\n        # Verify the server is still functioning after the broken pipe\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port, timeout=1)\n        try:\n            conn.request('GET', '/example_view/')\n            response = conn.getresponse()\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.read(), b'example view')\n        finally:\n            conn.close()\n", "    def test_broken_pipe_error_logging(self):\n        \"\"\"\n        Test that a broken pipe error is correctly logged.\n        \"\"\"\n        with self.assertLogs('django.server', level='INFO') as cm:\n            conn = HTTPConnection(\n                LiveServerViews.server_thread.host,\n                LiveServerViews.server_thread.port,\n                timeout=1,\n            )\n            try:\n                conn.request('GET', '/example_view/')\n                conn.sock.shutdown(socket.SHUT_RDWR)\n            except (OSError, socket.error):\n                pass\n            finally:\n                conn.close()\n            self.assertIn('Broken pipe from', cm.output[0])\n"], "sample_96": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_list_or_tuple(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 'name'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_readonly_fields_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n"], "sample_99": ["    def test_extract_invalid_component(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        msg = \"Tried to Extract from an invalid type.\"\n        with self.assertRaisesMessage(AssertionError, msg):\n            list(DTModel.objects.annotate(extracted=Extract('duration', 'invalid_component')))\n", "    def test_extract_func_edge_cases(self):\n        start_datetime = datetime(2020, 2, 29, 23, 59, 59, 999999)  # Leap year date\n        end_datetime = datetime(2021, 12, 31, 23, 59, 59, 999999)  # Last second of the year\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n        \n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=Extract('start_datetime', 'day')).order_by('start_datetime'),\n            [(start_datetime, 29), (end_datetime, 31)],\n            lambda m: (m.start_datetime, m.extracted)\n        )\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=Extract('start_datetime', 'month')).order_by('start_datetime'),\n            [(start_datetime, 2), (end_datetime, 12)],\n            lambda m: (m.start_datetime, m.extracted)\n        )\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=Extract('start_datetime', 'year')).order_by('start_datetime'),\n            [(start_datetime, 2020), (end_datetime, 2021)],\n            lambda m: (m.start_datetime, m.extracted)\n        )\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=Extract('start_datetime', 'second')).order_by('start_datetime'),\n            [(start_datetime, 59), (end_datetime, 59)],\n            lambda m: (m.start_datetime, m.extracted)\n        )\n", "    def test_extract_mixed_timezones(self):\n        \"\"\"\n        Test Extract function with start_datetime in different time zones.\n        \"\"\"\n        start_datetime_utc = datetime(2015, 6, 15, 14, 30, 50, 321, tzinfo=pytz.UTC)\n        start_datetime_pst = datetime(2015, 6, 15, 14, 30, 50, 321, tzinfo=pytz.timezone('US/Pacific'))\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123, tzinfo=pytz.UTC)\n\n        self.create_model(start_datetime_utc, end_datetime)\n        self.create_model(start_datetime_pst, end_datetime)\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=Extract('start_datetime', 'hour')).order_by('start_datetime'),\n            [\n                (start_datetime_pst, start_datetime_pst.hour),\n                (start_datetime_utc, start_datetime_utc.hour),\n            ],\n            lambda m: (m.start_datetime, m.extracted)\n        )\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=Extract('start_datetime', 'day')).order_by('start_datetime'),\n            [\n                (start_datetime_pst, start_datetime_pst.day),\n                (start_datetime_utc, start_datetime_utc.day),\n            ],\n            lambda m: (m.start_datetime, m.extracted)\n        )\n", "    def test_now_func(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        current_timestamp = timezone.now()\n        if settings.USE_TZ:\n            current_timestamp = timezone.make_aware(current_timestamp)\n\n        qs = DTModel.objects.annotate(current_timestamp=Now()).order_by('start_datetime')\n        self.assertQuerysetEqual(\n            qs,\n            [\n                (start_datetime, current_timestamp),\n                (end_datetime, current_timestamp)\n            ],\n            lambda m: (m.start_datetime, m.current_timestamp)\n        )\n", "    def test_trunc_time_func_boundaries(self):\n        start_datetime = datetime(2016, 10, 30, 23, 30, 50, 321)\n        end_datetime = datetime(2016, 10, 30, 23, 59, 59, 999)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=TruncTime('start_datetime')).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime, 'time')),\n                (end_datetime, truncate_to(end_datetime, 'time')),\n            ],\n            lambda m: (m.start_datetime, m.extracted)\n        )\n        self.assertEqual(DTModel.objects.filter(start_datetime__time=TruncTime('start_datetime')).count(), 2)\n\n        with self.assertRaisesMessage(ValueError, \"Cannot truncate DateField 'start_date' to TimeField\"):\n            list(DTModel.objects.annotate(truncated=TruncTime('start_date')))\n\n        with self.assertRaisesMessage(ValueError, \"Cannot truncate DateField 'start_date' to TimeField\"):\n            list(DTModel.objects.annotate(truncated=TruncTime('start_date', output_field=DateField())))\n"], "sample_100": ["    def test_echo_on_when_disabled(self, mocked_stdin, mocked_termios):\n        mocked_stdin.isatty.return_value = True\n        attr_list = [0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attr_list\n        autoreload.ensure_echo_on()\n        self.assertTrue(attr_list[3] & mocked_termios.ECHO)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_with_termios(self, mocked_stdin, mocked_termios):\n        mocked_stdin.isatty.return_value = True\n        mocked_termios.ECHO = 0x0008  # Example value for the ECHO flag\n        attrs = [0, 0, 0, 0]  # Initial attributes list\n        mocked_termios.tcgetattr.return_value = attrs\n\n        autoreload.ensure_echo_on()\n\n        mocked_termios.tcgetattr.assert_called_once_with(mocked_stdin)\n        self.assertTrue(attrs[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called_once_with(mocked_stdin, mocked_termios.TCSANOW, attrs)\n", "    def test_ensure_echo_on_with_tty(self, mock_stdin, mock_termios):\n        mock_stdin.isatty.return_value = True\n        attrs = [0, 0, 0, 0]\n        mock_termios.tcgetattr.return_value = attrs\n\n        autoreload.ensure_echo_on()\n\n        mock_termios.tcgetattr.assert_called_once_with(mock_stdin)\n        mock_termios.tcsetattr.assert_not_called()\n", "    def test_ensure_echo_on(self, mock_signal, mock_stdin, mock_termios):\n        mock_stdin.isatty.return_value = True\n        mock_termios.tcgetattr.return_value = [None, None, None, 0]\n        \n        autoreload.ensure_echo_on()\n        \n        mock_termios.tcgetattr.assert_called_once_with(mock_stdin)\n        self.assertEqual(mock_termios.tcsetattr.call_count, 0)\n\n        # Test echo is off\n        mock_termios.tcgetattr.return_value = [None, None, None, 0 & ~mock_termios.ECHO]\n        \n        autoreload.ensure_echo_on()\n\n        mock_termios.tcsetattr.assert_called_once_with(\n            mock_stdin, mock_termios.TCSANOW, [None, None, None, mock_termios.ECHO]\n        )\n", "    def test_ensure_echo_on_enabled(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        mock_tcgetattr.return_value = [None, None, None, 0 | termios.ECHO]\n        autoreload.ensure_echo_on()\n        self.assertFalse(mock_tcsetattr.called)\n"], "sample_102": ["    def test_union_with_distinct_and_values(self):\n        ReservedName.objects.create(name='a', order=2)\n        ReservedName.objects.create(name='b', order=3)\n        qs1 = ReservedName.objects.values('name', 'order')\n        qs2 = ReservedName.objects.values('name', 'order')\n        union_qs = qs1.union(qs2)\n        self.assertEqual(union_qs.count(), 2)\n        distinct_qs = union_qs.distinct()\n        self.assertEqual(distinct_qs.count(), 2)\n        results = list(distinct_qs)\n        self.assertDictEqual(results[0], {'name': 'a', 'order': 2})\n        self.assertDictEqual(results[1], {'name': 'b', 'order': 3})\n", "    def test_annotate_combined_queryset(self):\n        qs1 = Number.objects.filter(num__lt=5).annotate(double_num=F('num') * 2)\n        qs2 = Number.objects.filter(num__gte=5).annotate(double_num=F('num') * 2)\n        combined_qs = qs1.union(qs2)\n        results = combined_qs.values_list('num', 'double_num').order_by('num')\n        expected_results = [(i, i * 2) for i in range(10)]\n        self.assertCountEqual(results, expected_results)\n", "    def test_union_with_annotations(self):\n        qs1 = Number.objects.filter(num__lt=5).annotate(diff=F('num') - F('other_num'))\n        qs2 = Number.objects.filter(num__gte=5).annotate(diff=F('num') - F('other_num'))\n        combined_qs = qs1.union(qs2).order_by('num')\n        expected_results = [(i, i - (10 - i)) for i in range(10)]\n        self.assertQuerysetEqual(combined_qs, expected_results, transform=lambda x: (x.num, x.diff), ordered=True)\n", "    def test_union_with_prefetch_related(self):\n        # Creating related objects for prefetch_related test\n        reserved1 = ReservedName.objects.create(name='reserved1', order=1)\n        reserved2 = ReservedName.objects.create(name='reserved2', order=2)\n        Number.objects.bulk_create([\n            Number(num=1, other_num=9, reserved_name=reserved1),\n            Number(num=2, other_num=8, reserved_name=reserved2)\n        ])\n\n        # Union of two querysets with prefetch_related\n        qs1 = Number.objects.filter(num=1).prefetch_related('reserved_name')\n        qs2 = Number.objects.filter(num=2).prefetch_related('reserved_name')\n        union_qs = qs1.union(qs2)\n\n        # Fetching and asserting the union queryset results\n        results = list(union_qs)\n        self.assertEqual(len(results), 2)\n        self.assertTrue(all(hasattr(num, 'reserved_name') for num in results))\n        self.assertEqual(results[0].reserved_name.name, 'reserved1')\n        self.assertEqual(results[1].reserved_name.name, 'reserved2')\n", "def test_annotate_union(self):\n    qs1 = Number.objects.filter(num__lte=1).annotate(\n        annotation=Value(1, IntegerField()),\n    ).values('num', 'annotation')\n    qs2 = Number.objects.filter(num__gte=8).annotate(\n        annotation=Value(2, IntegerField()),\n    ).values('num', 'annotation')\n    union_qs = qs1.union(qs2).order_by('num')\n    expected_results = [\n        {'num': 0, 'annotation': 1},\n        {'num': 1, 'annotation': 1},\n        {'num': 8, 'annotation': 2},\n        {'num': 9, 'annotation': 2},\n    ]\n    self.assertCountEqual(union_qs, expected_results)\n"], "sample_101": ["    def test_invalid_application(self):\n        \"\"\"\n        If ``WSGI_APPLICATION`` is an invalid path, an ImproperlyConfigured exception is raised.\n        \"\"\"\n        msg = \"WSGI application 'wsgi.invalid.app' could not be loaded; Error importing\"\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            get_internal_wsgi_application()\n", "    def test_get_script_name(self):\n        \"\"\"\n        Ensure get_script_name correctly derives script name from environ.\n        \"\"\"\n        environ = {\n            'SCRIPT_NAME': '/test',\n            'SCRIPT_URL': '/test/url',\n            'REDIRECT_URL': '/test/url',\n            'PATH_INFO': '/url',\n        }\n        with self.settings(FORCE_SCRIPT_NAME=None):\n            script_name = get_script_name(environ)\n        self.assertEqual(script_name, '/test')\n\n        # Test with FORCE_SCRIPT_NAME setting\n        with self.settings(FORCE_SCRIPT_NAME='/forced'):\n            script_name = get_script_name(environ)\n        self.assertEqual(script_name, '/forced')\n\n        # Test with mod_rewrite information\n        del environ['SCRIPT_NAME']\n        script_name = get_script_name(environ)\n        self.assertEqual(script_name, '/test')\n", "    def test_wsgi_request_initialization(self):\n        \"\"\"\n        Test WSGIRequest initialization with various environment settings.\n        \"\"\"\n        environ = {\n            'REQUEST_METHOD': 'POST',\n            'PATH_INFO': '/test/path',\n            'SCRIPT_NAME': '/test',\n            'CONTENT_LENGTH': '10',\n            'wsgi.input': BytesIO(b'1234567890'),\n            'QUERY_STRING': 'name=value',\n            'HTTP_COOKIE': 'cookie1=value1; cookie2=value2',\n        }\n        request = WSGIRequest(environ)\n        \n        self.assertEqual(request.method, 'POST')\n        self.assertEqual(request.path_info, '/test/path')\n        self.assertEqual(request.path, '/test/test/path')\n        self.assertEqual(request.META['CONTENT_LENGTH'], '10')\n        self.assertEqual(request.environ['REQUEST_METHOD'], 'POST')\n        self.assertEqual(request.GET.get('name'), 'value')\n        self.assertEqual(request.COOKIES['cookie1'], 'value1')\n        self.assertEqual(request.COOKIES['cookie2'], 'value2')\n        self.assertEqual(request._stream.read(), b'1234567890')\n        self.assertEqual(request._stream.remaining, 0)\n", "    def test_get_script_name_force_script_name(self):\n        \"\"\"\n        get_script_name() should return the value of settings.FORCE_SCRIPT_NAME if it is set.\n        \"\"\"\n        with self.settings(FORCE_SCRIPT_NAME='/forced/script'):\n            environ = self.request_factory._base_environ(PATH_INFO=\"/\")\n            script_name = get_script_name(environ)\n            self.assertEqual(script_name, '/forced/script')\n", "    def test_wsgi_request_initialization(self):\n        \"\"\"\n        Test the initialization of WSGIRequest with a given environment.\n        \"\"\"\n        environ = {\n            'REQUEST_METHOD': 'GET',\n            'PATH_INFO': '/test/',\n            'SCRIPT_NAME': '/app',\n            'wsgi.input': BytesIO(b''),\n            'CONTENT_LENGTH': '0',\n            'wsgi.url_scheme': 'http',\n            'QUERY_STRING': 'param1=value1&param2=value2',\n            'HTTP_COOKIE': 'cookie1=value1; cookie2=value2',\n        }\n\n        request = WSGIRequest(environ)\n\n        self.assertEqual(request.method, 'GET')\n        self.assertEqual(request.path_info, '/test/')\n        self.assertEqual(request.path, '/app/test/')\n        self.assertEqual(request.META['PATH_INFO'], '/test/')\n        self.assertEqual(request.META['SCRIPT_NAME'], '/app')\n        self.assertEqual(request._get_scheme(), 'http')\n        self.assertEqual(request.GET['param1'], 'value1')\n        self.assertEqual(request.GET['param2'], 'value2')\n        self.assertEqual(request.COOKIES['cookie1'], 'value1')\n        self.assertEqual(request.COOKIES['cookie2'], 'value2')\n"], "sample_103": ["    def test_stddev_variance(self):\n        # Test StdDev with sample\n        vals = Book.objects.aggregate(StdDev(\"pages\", sample=True))\n        self.assertAlmostEqual(vals[\"pages__stddev\"], 299.45, places=2)\n\n        # Test StdDev with population\n        vals = Book.objects.aggregate(StdDev(\"pages\", sample=False))\n        self.assertAlmostEqual(vals[\"pages__stddev\"], 273.91, places=2)\n\n        # Test Variance with sample\n        vals = Book.objects.aggregate(Variance(\"pages\", sample=True))\n        self.assertAlmostEqual(vals[\"pages__variance\"], 89670.97, places=2)\n\n        # Test Variance with population\n        vals = Book.objects.aggregate(Variance(\"pages\", sample=False))\n        self.assertAlmostEqual(vals[\"pages__variance\"], 75085.81, places=2)\n", "    def test_stddev_variance_aggregates(self):\n        # Test StdDev and Variance aggregates\n\n        # Standard Deviation (Population)\n        result = Author.objects.aggregate(stddev_age=StdDev('age'))\n        self.assertAlmostEqual(result['stddev_age'], 8.9, places=1)\n\n        # Standard Deviation (Sample)\n        result = Author.objects.aggregate(stddev_age_sample=StdDev('age', sample=True))\n        self.assertAlmostEqual(result['stddev_age_sample'], 9.6, places=1)\n\n        # Variance (Population)\n        result = Author.objects.aggregate(variance_age=Variance('age'))\n        self.assertAlmostEqual(result['variance_age'], 79.2, places=1)\n\n        # Variance (Sample)\n        result = Author.objects.aggregate(variance_age_sample=Variance('age', sample=True))\n        self.assertAlmostEqual(result['variance_age_sample'], 92.3, places=1)\n", "    def test_stddev(self):\n        # Test population standard deviation\n        vals = Book.objects.aggregate(stddev=StdDev('pages'))\n        self.assertAlmostEqual(vals['stddev'], 298.699, places=3)\n        \n        # Test sample standard deviation\n        vals = Book.objects.aggregate(stddev_sample=StdDev('pages', sample=True))\n        self.assertAlmostEqual(vals['stddev_sample'], 325.269, places=3)\n", "    def test_stddev_variance_aggregates(self):\n        authors = Author.objects.all()\n\n        stddev_population = authors.aggregate(StdDev('age'))\n        self.assertAlmostEqual(stddev_population['age__stddev'], 9.075, places=3)\n\n        stddev_sample = authors.aggregate(StdDev('age', sample=True))\n        self.assertAlmostEqual(stddev_sample['age__stddev'], 9.577, places=3)\n\n        variance_population = authors.aggregate(Variance('age'))\n        self.assertAlmostEqual(variance_population['age__variance'], 82.36, places=2)\n\n        variance_sample = authors.aggregate(Variance('age', sample=True))\n        self.assertAlmostEqual(variance_sample['age__variance'], 91.51, places=2)\n", "    def test_stddev_variance_aggregate(self):\n        # Test for StdDev\n        vals = Author.objects.aggregate(age_stddev=StdDev('age'))\n        self.assertIsInstance(vals['age_stddev'], float)\n        self.assertAlmostEqual(vals['age_stddev'], 9.49, places=2)\n\n        # Test for Variance\n        vals = Author.objects.aggregate(age_variance=Variance('age'))\n        self.assertIsInstance(vals['age_variance'], float)\n        self.assertAlmostEqual(vals['age_variance'], 90.1, places=1)\n\n        # Test for StdDev with sample=True\n        vals = Author.objects.aggregate(age_stddev_sample=StdDev('age', sample=True))\n        self.assertIsInstance(vals['age_stddev_sample'], float)\n        self.assertAlmostEqual(vals['age_stddev_sample'], 10.02, places=2)\n\n        # Test for Variance with sample=True\n        vals = Author.objects.aggregate(age_variance_sample=Variance('age', sample=True))\n        self.assertIsInstance(vals['age_variance_sample'], float)\n        self.assertAlmostEqual(vals['age_variance_sample'], 100.11, places=2)\n"], "sample_104": ["    def test_static_files_storage_without_static_root(self):\n        with self.assertRaises(ImproperlyConfigured):\n            storage.StaticFilesStorage()\n", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.manifest_file = os.path.join(self.temp_dir, 'staticfiles.json')\n        self.storage = ManifestFilesMixin()\n        self.storage.manifest_name = self.manifest_file\n", "    def setUp(self):\n        self.test_storage = storage.StaticFilesStorage()\n", "    def setUp(self):\n        self.location = '/test/static'\n        self.base_url = '/static/'\n        self.storage = StaticFilesStorage(location=self.location, base_url=self.base_url)\n", "    def setUp(self):\n        self.storage = ManifestFilesMixin(location=tempfile.mkdtemp())\n"], "sample_107": ["    def test_get_cleansed_multivaluedict(self):\n        request = self.rf.post('/some_url/', {'sensitive_key': 'secret_value', 'normal_key': 'normal_value'})\n        request.sensitive_post_parameters = ['sensitive_key']\n\n        filter = SafeExceptionReporterFilter()\n        cleansed_multivaluedict = filter.get_cleansed_multivaluedict(request, request.POST)\n\n        self.assertEqual(cleansed_multivaluedict['sensitive_key'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_multivaluedict['normal_key'], 'normal_value')\n", "    def test_cleanse_setting_nested_dict(self):\n        settings_dict = {\n            'API_KEY': 'super_secret_key',\n            'NESTED': {\n                'TOKEN': 'nested_secret_token',\n                'NON_SENSITIVE': 'visible_value'\n            }\n        }\n        cleansed_dict = cleanse_setting('SETTINGS', settings_dict)\n        self.assertEqual(cleansed_dict['API_KEY'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_dict['NESTED']['TOKEN'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_dict['NESTED']['NON_SENSITIVE'], 'visible_value')\n", "    def test_cleanse_setting_with_dict(self):\n        sensitive_dict = {\n            'API_KEY': 'super_secret_key',\n            'nested': {\n                'SECRET': 'another_secret',\n                'non_sensitive': 'value'\n            },\n            'other': 'data'\n        }\n        cleansed = cleanse_setting('SETTINGS_DICT', sensitive_dict)\n        self.assertEqual(cleansed['API_KEY'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed['nested']['SECRET'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed['nested']['non_sensitive'], 'value')\n        self.assertEqual(cleansed['other'], 'data')\n", "    def test_cleanse_setting_dict_with_sensitive_keys(self):\n        test_dict = {\n            'API_KEY': 'super_secret_api_key',\n            'nested': {\n                'TOKEN': 'super_secret_token',\n                'regular_key': 'regular_value'\n            }\n        }\n        expected_result = {\n            'API_KEY': CLEANSED_SUBSTITUTE,\n            'nested': {\n                'TOKEN': CLEANSED_SUBSTITUTE,\n                'regular_key': 'regular_value'\n            }\n        }\n        self.assertEqual(cleanse_setting('CONFIG', test_dict), expected_result)\n", "    def setUp(self):\n        self.factory = RequestFactory()\n"], "sample_106": ["    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600, public=True)\n        self.assertIn('Cache-Control', response)\n        cc_directives = set(response['Cache-Control'].split(', '))\n        self.assertIn('max-age=3600', cc_directives)\n        self.assertIn('public', cc_directives)\n", "    def test_patch_cache_control_updates_existing_header(self):\n        response = HttpResponse()\n        response['Cache-Control'] = 'max-age=60, public'\n        patch_cache_control(response, max_age=120, private=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=120, private')\n", "    def test_patch_cache_control_add_header(self):\n        \"\"\"Test adding Cache-Control header to a response.\"\"\"\n        response = HttpResponse()\n        patch_cache_control(response, no_cache=True)\n        self.assertEqual(response['Cache-Control'], 'no-cache')\n", "    def setUp(self):\n        self.factory = RequestFactory()\n", "    def test_patch_cache_control(self):\n        \"\"\"\n        Test patch_cache_control function for various scenarios.\n        \"\"\"\n        tests = [\n            # Initial Cache-Control, kwargs to patch_cache_control, expected Cache-Control parts\n            (None, {'private': True}, {'private'}),\n            ('', {'private': True}, {'private'}),\n            ('private', {'private': True}, {'private'}),\n            ('private', {'public': True}, {'public'}),\n            ('public', {'public': True}, {'public'}),\n            ('public', {'private': True}, {'private'}),\n            ('must-revalidate,max-age=60,private', {'public': True}, {'must-revalidate', 'max-age=60', 'public'}),\n            ('must-revalidate,max-age=60,public', {'private': True}, {'must-revalidate', 'max-age=60', 'private'}),\n            ('must-revalidate,max-age=60', {'public': True}, {'must-revalidate', 'max-age=60', 'public'}),\n            # Test max-age merging logic\n            ('max-age=3600', {'max_age': 1800}, {'max-age=1800'}),\n            ('max-age=1800', {'max_age': 3600}, {'max-age=1800'}),\n        ]\n\n        for initial_cc, new_headers, expected_cc in tests:\n            with self.subTest(initial_cc=initial_cc, new_headers=new_headers):\n                response = HttpResponse()\n                if initial_cc is not None:\n                    response['Cache-Control'] = initial_cc\n                patch_cache_control(response, **new_headers)\n                self.assertEqual(set(response['Cache-Control'].split(', ')), expected_cc)\n"], "sample_105": ["    def test_redirect_view_with_pattern_name_and_query_string(self):\n        \"\"\"\n        Test that RedirectView correctly handles URL pattern names with query strings.\n        \"\"\"\n        response = RedirectView.as_view(pattern_name='artist_detail', query_string=True)(\n            self.rf.get('/foo/?param=value'), pk=1)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/detail/artist/1/?param=value')\n", "    def test_redirect_method_not_allowed(self):\n        \"\"\"\n        Test that RedirectView returns HTTP 405 for unsupported HTTP methods.\n        \"\"\"\n        unsupported_methods = ['CONNECT', 'TRACE']\n        for method in unsupported_methods:\n            request = self.rf.request(REQUEST_METHOD=method, PATH_INFO='/foo/')\n            response = RedirectView.as_view(url='/bar/')(request)\n            self.assertEqual(response.status_code, 405)\n", "    def test_render_to_response_with_different_context(self):\n        \"\"\"\n        Test the render_to_response method with different context data\n        \"\"\"\n        class TestTemplateView(TemplateView):\n            template_name = 'generic_views/about.html'\n\n        view = TestTemplateView.as_view()\n        request = self.rf.get('/about/')\n        response = view(request)\n        response.render()\n        self.assertContains(response, '<h1>About</h1>')\n\n        class AnotherTestTemplateView(TemplateView):\n            template_name = 'generic_views/about.html'\n\n                context = super().get_context_data(**kwargs)\n                context['extra'] = 'Extra context'\n                return context\n\n        view = AnotherTestTemplateView.as_view()\n        response = view(request)\n        response.render()\n        self.assertContains(response, 'Extra context')\n", "    def test_redirect_with_pattern_name_and_query_string(self):\n        \"\"\"\n        RedirectView should correctly construct the URL using pattern_name\n        and include query strings if specified.\n        \"\"\"\n        response = RedirectView.as_view(pattern_name='artist_detail', query_string=True)(\n            self.rf.get('/foo/', {'name': 'John'})\n        )\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/detail/artist/1/?name=John')\n", "    def test_extra_context(self):\n        \"\"\"\n        Test that the extra_context attribute is correctly added to the context.\n        \"\"\"\n        response = self.ExtraContextTemplateView.as_view()(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('extra_key', response.context_data)\n        self.assertEqual(response.context_data['extra_key'], 'extra_value')\n"], "sample_109": ["    def test_get_related_url(self):\n        rel = Album._meta.get_field('band').remote_field\n        w = AutocompleteSelect(rel, admin.site)\n        related_url = w.get_related_url(('album', 'band'), 'change', 1)\n        self.assertEqual(related_url, '/admin/album/band/change/1/')\n", "def test_autocomplete_mixin_media(self):\n        form = AlbumForm()\n        media = form['band'].field.widget.media\n        self.assertIn('admin/js/vendor/jquery/jquery.min.js', str(media))\n        self.assertIn('admin/js/vendor/select2/select2.full.min.js', str(media))\n        self.assertIn('admin/css/vendor/select2/select2.min.css', str(media))\n        self.assertIn('admin/css/autocomplete.css', str(media))\n", "    def test_autocomplete_mixin_optgroups(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        form = AlbumForm()\n        widget = form['band'].field.widget\n        optgroups = widget.optgroups(name='band', value=[beatles.pk], attr={})\n        self.assertEqual(len(optgroups), 1)\n        self.assertEqual(len(optgroups[0][1]), 1)\n        self.assertEqual(optgroups[0][1][0]['value'], str(beatles.pk))\n        self.assertTrue(optgroups[0][1][0]['selected'])\n        self.assertEqual(optgroups[0][1][0]['label'], 'The Beatles')\n", "    def test_media_property(self):\n        form = AlbumForm()\n        media = form['band'].field.widget.media\n        self.assertIn('admin/js/vendor/jquery/jquery.min.js', media._js)\n        self.assertIn('admin/js/vendor/select2/select2.full.min.js', media._js)\n        self.assertIn('admin/js/jquery.init.js', media._js)\n        self.assertIn('admin/js/autocomplete.js', media._js)\n        self.assertIn('admin/css/vendor/select2/select2.min.css', media._css['screen'])\n        self.assertIn('admin/css/autocomplete.css', media._css['screen'])\n", "    def test_autocomplete_mixin_optgroups(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        form = AlbumForm(initial={'band': beatles.pk, 'featuring': [beatles.pk, who.pk]})\n        \n        band_widget = form['band'].field.widget\n        featuring_widget = form['featuring'].field.widget\n\n        band_optgroups = band_widget.optgroups(name='band', value=[beatles.pk])\n        featuring_optgroups = featuring_widget.optgroups(name='featuring', value=[beatles.pk, who.pk])\n\n        self.assertEqual(len(band_optgroups[0][1]), 1)\n        self.assertEqual(band_optgroups[0][1][0]['label'], 'The Beatles')\n        \n        self.assertEqual(len(featuring_optgroups[0][1]), 2)\n        featuring_labels = [option['label'] for option in featuring_optgroups[0][1]]\n        self.assertIn('The Beatles', featuring_labels)\n        self.assertIn('The Who', featuring_labels)\n"], "sample_111": ["    def test_get_filters_params(self):\n        m = ChildAdmin(Child, custom_site)\n        request = self.factory.get('/child/', data={'name': 'test', 'age': '5', ALL_VAR: '1', ORDER_VAR: 'name', ERROR_FLAG: '1'})\n        request.user = self.superuser\n        cl = m.get_changelist_instance(request)\n        filtered_params = cl.get_filters_params()\n        self.assertEqual(filtered_params, {'name': 'test', 'age': '5'})\n", "    def test_get_ordering_with_invalid_ordering(self):\n        \"\"\"\n        Test the ChangeList.get_ordering method with invalid ordering in the query string.\n        \"\"\"\n        class InvalidOrderingBandAdmin(admin.ModelAdmin):\n            list_display = ['name', 'genres', 'nr_of_members']\n\n        m = InvalidOrderingBandAdmin(Band, custom_site)\n        request = self.factory.get('/band/', data={'o': 'invalid_ordering'})\n        request.user = self.superuser\n        cl = m.get_changelist_instance(request)\n        self.assertEqual(cl.get_ordering(request, cl.queryset), ['-pk'])\n", "    def test_get_filters_params(self):\n        \"\"\"\n        Test that get_filters_params method excludes IGNORED_PARAMS.\n        \"\"\"\n        m = BandAdmin(Band, custom_site)\n        request = self.factory.get('/band/', data={\n            SEARCH_VAR: 'test',\n            ALL_VAR: '1',\n            ORDER_VAR: 'name',\n            PAGE_VAR: '1',\n        })\n        request.user = self.superuser\n        cl = m.get_changelist_instance(request)\n        filters_params = cl.get_filters_params()\n        self.assertNotIn(SEARCH_VAR, filters_params)\n        self.assertNotIn(ALL_VAR, filters_params)\n        self.assertNotIn(ORDER_VAR, filters_params)\n        self.assertIn(PAGE_VAR, filters_params)  # PAGE_VAR should be removed later in the code.\n        self.assertEqual(filters_params.get(PAGE_VAR), '1')\n", "    def test_get_filters_params(self):\n        \"\"\"\n        Test that get_filters_params() correctly removes ignored parameters.\n        \"\"\"\n        class TestAdmin(admin.ModelAdmin):\n            list_display = ['name']\n            list_filter = ['genres']\n\n        request = self.factory.get('/band/', data={\n            ALL_VAR: '1', ORDER_VAR: 'name', SEARCH_VAR: 'test', IS_POPUP_VAR: '1',\n            TO_FIELD_VAR: 'field', PAGE_VAR: '2', ERROR_FLAG: 'error'\n        })\n        request.user = self.superuser\n\n        m = TestAdmin(Band, custom_site)\n        cl = m.get_changelist_instance(request)\n        filtered_params = cl.get_filters_params()\n\n        self.assertNotIn(ALL_VAR, filtered_params)\n        self.assertNotIn(ORDER_VAR, filtered_params)\n        self.assertNotIn(SEARCH_VAR, filtered_params)\n        self.assertNotIn(IS_POPUP_VAR, filtered_params)\n        self.assertNotIn(TO_FIELD_VAR, filtered_params)\n        self.assertNotIn(PAGE_VAR, filtered_params)\n        self.assertNotIn(ERROR_FLAG, filtered_params)\n", "    def test_get_filters_params(self):\n        \"\"\"\n        Test ChangeList.get_filters_params() to ensure it returns correct params\n        excluding IGNORED_PARAMS.\n        \"\"\"\n        m = ParentAdmin(Parent, custom_site)\n        request = self.factory.get('/parent/', data={\n            'name': 'test',\n            ALL_VAR: '1',\n            ORDER_VAR: 'name',\n            ORDER_TYPE_VAR: 'asc',\n            SEARCH_VAR: 'query',\n            IS_POPUP_VAR: '1',\n            TO_FIELD_VAR: 'field',\n        })\n        request.user = self.superuser\n        cl = m.get_changelist_instance(request)\n        filtered_params = cl.get_filters_params()\n        self.assertEqual(filtered_params, {'name': 'test'})\n"], "sample_110": ["    def test_combined_expression_addition(self):\n        expr = CombinedExpression(F('number1'), CombinedExpression.ADD, F('number2'))\n        self.assertEqual(str(expr), \"number1 + number2\")\n", "    def test_combined_expression_sql(self):\n        lhs = Value(1)\n        rhs = Value(2)\n        combined_expr = CombinedExpression(lhs, CombinedExpression.ADD, rhs)\n        mock_compiler = lambda expr: (\"mock_sql\", [])\n        sql, params = combined_expr.as_sql(mock_compiler, connection)\n        self.assertEqual(sql, '(mock_sql + mock_sql)')\n        self.assertEqual(params, [])\n", "    def test_expression_combined(self):\n        lhs = F('number1')\n        rhs = Value(2)\n        combined_expr = lhs + rhs\n        self.assertIsInstance(combined_expr, CombinedExpression)\n        self.assertEqual(combined_expr.connector, '+')\n        self.assertEqual(str(combined_expr), \"number1 + 2\")\n", "    def test_combined_expression_as_sql(self):\n        lhs = F('price')\n        rhs = Value(10)\n        combined_expr = CombinedExpression(lhs, CombinedExpression.ADD, rhs)\n        with connection.cursor() as cursor:\n            compiler = connection.ops.query_class().get_compiler(connection=connection)\n            sql, params = combined_expr.as_sql(compiler, connection)\n            self.assertIn('price +', sql)\n            self.assertIn('%s', sql)\n            self.assertEqual(params, [10])\n", "    def test_combined_expression_addition(self):\n        # Test addition of two expressions\n        expr1 = F('number1')\n        expr2 = F('number2')\n        combined_expr = expr1 + expr2\n        expected_expr_str = f\"{expr1} + {expr2}\"\n        self.assertEqual(str(combined_expr), expected_expr_str)\n"], "sample_112": ["    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js template tag should include prepopulated fields in context.\n        \"\"\"\n        class MockField:\n                self.name = name\n                self.auto_id = auto_id\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        field1 = MockField(\"field1\", \"id_field1\")\n        field2 = MockField(\"field2\", \"id_field2\", allow_unicode=True)\n        dependency = MockField(\"dependency\", \"id_dependency\")\n        prepopulated_fields = [\n            {\"field\": field1, \"dependencies\": [dependency]},\n            {\"field\": field2, \"dependencies\": []}\n        ]\n\n        context = {\n            \"adminform\": MockAdminForm(prepopulated_fields)\n        }\n        \n        new_context = prepopulated_fields_js(context)\n        self.assertIn('prepopulated_fields', new_context)\n        self.assertIn('prepopulated_fields_json', new_context)\n        \n        prepopulated_fields_json = json.loads(new_context['prepopulated_fields_json'])\n        self.assertEqual(len(prepopulated_fields_json), 2)\n        self.assertEqual(prepopulated_fields_json[0]['id'], '#id_field1')\n        self.assertEqual(prepopulated_fields_json[0]['name'], 'field1')\n        self.assertEqual(prepopulated_fields_json[0]['dependency_ids'], ['#id_dependency'])\n        self.assertEqual(prepopulated_fields_json[0]['dependency_list'], ['dependency'])\n        self.assertEqual(prepopulated_fields_json[0]['maxLength'], 50)\n        self.assertEqual(prepopulated_fields_json[0]['allowUnicode'], False)\n        self.assertEqual(prepopulated_fields_json[1]['id'], '#id_field2')\n        self.assertEqual(prepopulated_fields_json[1]['name'], 'field2')\n        self.assertEqual(prepopulated_fields_json[1]['dependency_ids'], [])\n        self.assertEqual(prepopulated_fields_json[1]['dependency_list'], [])\n        self.assertEqual(prepopulated_fields_json[1]['maxLength'], 50)\n        self.assertEqual(prepopulated_fields_json[1]['allowUnicode'], True)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js should correctly process context to generate prepopulated fields JSON.\n        \"\"\"\n        class MockField:\n                self.name = name\n                self.auto_id = auto_id\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        context = {\n            'adminform': MockAdminForm([\n                {\"field\": MockField(\"title\", \"id_title\"), \"dependencies\": [MockField(\"slug\", \"id_slug\")]}\n            ])\n        }\n\n        result_context = prepopulated_fields_js(context)\n        expected_json = json.dumps([{\n            \"id\": \"#id_title\",\n            \"name\": \"title\",\n            \"dependency_ids\": [\"#id_slug\"],\n            \"dependency_list\": [\"slug\"],\n            \"maxLength\": 50,\n            \"allowUnicode\": False\n        }])\n        self.assertEqual(result_context['prepopulated_fields_json'], expected_json)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js should add correct context for prepopulated fields.\n        \"\"\"\n        # Creating mock context with prepopulated fields\n        class MockField:\n                self.name = name\n                self.auto_id = auto_id\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        mock_field_1 = MockField(\"field1\", \"id_field1\", 100, True)\n        mock_field_2 = MockField(\"field2\", \"id_field2\")\n        mock_dependencies = [MockField(\"dep1\", \"id_dep1\"), MockField(\"dep2\", \"id_dep2\")]\n\n        mock_prepopulated_fields = [\n            {\"field\": mock_field_1, \"dependencies\": mock_dependencies},\n            {\"field\": mock_field_2, \"dependencies\": mock_dependencies},\n        ]\n\n        mock_context = {\n            \"adminform\": type('obj', (object,), {\"prepopulated_fields\": mock_prepopulated_fields})(),\n            \"inline_admin_formsets\": []\n        }\n\n        # Call the function\n        new_context = prepopulated_fields_js(mock_context)\n\n        # Check that the context is updated correctly\n        self.assertIn('prepopulated_fields', new_context)\n        self.assertIn('prepopulated_fields_json', new_context)\n\n        # Check the content of prepopulated_fields_json\n        prepopulated_fields_json = json.loads(new_context['prepopulated_fields_json'])\n        self.assertEqual(len(prepopulated_fields_json), 2)\n        self.assertEqual(prepopulated_fields_json[0]['id'], '#id_field1')\n        self.assertEqual(prepopulated_fields_json[0]['name'], 'field1')\n        self.assertEqual(prepopulated_fields_json[0]['maxLength'], 100)\n        self.assertTrue(prepopulated_fields_json[0]['allowUnicode'])\n        self.assertEqual(prepopulated_fields_json[0]['dependency_ids'], ['#id_dep1', '#id_dep2'])\n        self.assertEqual(prepopulated_fields_json[0]['dependency_list'], ['dep1', 'dep2'])\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js should add prepopulated_fields and prepopulated_fields_json to context.\n        \"\"\"\n        request = self.request_factory.get(reverse('admin:auth_user_add'))\n        request.user = self.superuser\n        admin = UserAdmin(User, site)\n        extra_context = {\n            'adminform': type('AdminForm', (object,), {\n                'prepopulated_fields': [{'field': type('Field', (object,), {'auto_id': 'id_username', 'name': 'username', 'field': type('FieldType', (object,), {'max_length': 150, 'allow_unicode': True})}), 'dependencies': [type('Dependency', (object,), {'auto_id': 'id_first_name', 'name': 'first_name'})]}]\n            })(),\n            'inline_admin_formsets': []\n        }\n        response = admin.add_view(request, extra_context=extra_context)\n        context = prepopulated_fields_js(response.context_data)\n        self.assertIn('prepopulated_fields', context)\n        self.assertIn('prepopulated_fields_json', context)\n        self.assertEqual(len(json.loads(context['prepopulated_fields_json'])), 1)\n        self.assertEqual(json.loads(context['prepopulated_fields_json'])[0]['id'], '#id_username')\n        self.assertEqual(json.loads(context['prepopulated_fields_json'])[0]['name'], 'username')\n        self.assertEqual(json.loads(context['prepopulated_fields_json'])[0]['dependency_ids'], ['#id_first_name'])\n        self.assertEqual(json.loads(context['prepopulated_fields_json'])[0]['dependency_list'], ['first_name'])\n        self.assertEqual(json.loads(context['prepopulated_fields_json'])[0]['maxLength'], 150)\n        self.assertTrue(json.loads(context['prepopulated_fields_json'])[0]['allowUnicode'])\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js should correctly serialize prepopulated fields to JSON.\n        \"\"\"\n        class MockField:\n                self.name = name\n                self.auto_id = auto_id\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        field1 = MockField(\"field1\", \"id_field1\", 20, False)\n        field2 = MockField(\"field2\", \"id_field2\", 50, True)\n        field3 = MockField(\"field3\", \"id_field3\", None, False)\n\n        mock_context = {\n            'adminform': MockAdminForm([\n                {\"field\": field1, \"dependencies\": [field2]},\n                {\"field\": field2, \"dependencies\": [field1, field3]},\n            ]),\n            'inline_admin_formsets': [],\n        }\n\n        context = prepopulated_fields_js(mock_context)\n        prepopulated_fields_json = json.loads(context['prepopulated_fields_json'])\n\n        self.assertEqual(len(prepopulated_fields_json), 2)\n        self.assertEqual(prepopulated_fields_json[0][\"id\"], \"#id_field1\")\n        self.assertEqual(prepopulated_fields_json[0][\"name\"], \"field1\")\n        self.assertEqual(prepopulated_fields_json[0][\"dependency_ids\"], [\"#id_field2\"])\n        self.assertEqual(prepopulated_fields_json[0][\"dependency_list\"], [\"field2\"])\n        self.assertEqual(prepopulated_fields_json[0][\"maxLength\"], 20)\n        self.assertFalse(prepopulated_fields_json[0][\"allowUnicode\"])\n\n        self.assertEqual(prepopulated_fields_json[1][\"id\"], \"#id_field2\")\n        self.assertEqual(prepopulated_fields_json[1][\"name\"], \"field2\")\n        self.assertEqual(prepopulated_fields_json[1][\"dependency_ids\"], [\"#id_field1\", \"#id_field3\"])\n        self.assertEqual(prepopulated_fields_json[1][\"dependency_list\"], [\"field1\", \"field3\"])\n        self.assertEqual(prepopulated_fields_json[1][\"maxLength\"], 50)\n        self.assertTrue(prepopulated_fields_json[1][\"allowUnicode\"])\n"], "sample_113": ["    def test_trim_docstring(self):\n        docstring = \"\"\"\n        This is a test function.\n\n        It does something very important.\n        \"\"\"\n        expected_output = \"This is a test function.\\n\\nIt does something very important.\"\n        self.assertEqual(trim_docstring(docstring), expected_output)\n", "    def test_trim_docstring(self):\n        docstring = \"\"\"\n        Test function.\n\n        This function does something.\n        \n        \"\"\"\n        expected_trimmed = \"Test function.\\n\\nThis function does something.\"\n        self.assertEqual(utils.trim_docstring(docstring), expected_trimmed)\n\n        self.assertEqual(utils.trim_docstring(\"\"), \"\")\n        self.assertEqual(utils.trim_docstring(\"   \"), \"\")\n        self.assertEqual(utils.trim_docstring(\"Test\"), \"Test\")\n", "    def test_trim_docstring(self):\n        docstring = \"\"\"\n        This is a sample docstring.\n\n        It has multiple lines.\n        \"\"\"\n        expected_result = \"This is a sample docstring.\\n\\nIt has multiple lines.\"\n        self.assertEqual(utils.trim_docstring(docstring), expected_result)\n", "    def test_get_view_name_function(self):\n        class MockView:\n            __module__ = 'mock_module'\n            __qualname__ = 'MockView'\n\n        mock_view = MockView()\n        self.assertEqual(utils.get_view_name(mock_view), 'mock_module.MockView')\n", "    def test_trim_docstring(self):\n        docstring = \"\"\"\n        This is a test docstring.\n\n        It has multiple lines and indentation.\n        \"\"\"\n        expected_trimmed = \"This is a test docstring.\\n\\nIt has multiple lines and indentation.\"\n        self.assertEqual(utils.trim_docstring(docstring), expected_trimmed)\n"], "sample_114": ["    def test_alter_field_with_function_default(self):\n        \"\"\"\n        #24007 - Changing a field with a function as default should\n        correctly detect the change.\n        \"\"\"\n        before = [\n            ModelState(\"testapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"created_at\", models.DateTimeField(default=None)),\n            ]),\n        ]\n        after = [\n            ModelState(\"testapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"created_at\", models.DateTimeField(default=models.functions.Now)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='created_at')\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=models.functions.Now)\n", "    def test_deep_deconstruct_dict_with_partial_and_regex(self):\n        \"\"\"\n        Test deep deconstruction of a dictionary containing functools.partial \n        and compiled regex objects.\n        \"\"\"\n            return x\n\n        test_dict = {\n            'partial': functools.partial(sample_function, 1),\n            'regex': re.compile(r'^[a-z]+$'),\n        }\n\n        deconstructed_dict = MigrationAutodetector(None, None).deep_deconstruct(test_dict)\n\n        self.assertEqual(deconstructed_dict['partial'][0], sample_function)\n        self.assertEqual(deconstructed_dict['partial'][1], (1,))\n        self.assertEqual(deconstructed_dict['partial'][2], {})\n\n        self.assertIsInstance(deconstructed_dict['regex'], RegexObject)\n        self.assertEqual(deconstructed_dict['regex'].pattern, '^[a-z]+$')\n", "    def test_alter_field_preserve_default(self):\n        \"\"\"\n        #29929 - When altering a field, the preserve_default flag should be set\n        appropriately.\n        \"\"\"\n        before_state = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=\"Default Name\")),\n        ])\n        after_state = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=400, default=\"Default Name\")),\n        ])\n        changes = self.get_changes([before_state], [after_state])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True)\n", "    def test_alter_model_constraints(self):\n        \"\"\"\n        Test that altering constraints on a model is correctly detected.\n        \"\"\"\n        changes = self.get_changes([self.author_name], [self.author_name_check_constraint])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddConstraint\"])\n        self.assertOperationAttributes(\n            changes, 'testapp', 0, 0, model_name='author', constraint=models.CheckConstraint(\n                check=models.Q(name__contains='Bob'), name='name_contains_bob'\n            )\n        )\n\n        # Change the constraint and check detection\n        altered_constraint = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n            {'constraints': [models.CheckConstraint(check=models.Q(name__contains='Alice'), name='name_contains_alice')]},\n        )\n        changes = self.get_changes([self.author_name_check_constraint], [altered_constraint])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RemoveConstraint\", \"AddConstraint\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author', name='name_contains_bob')\n        self.assertOperationAttributes(\n            changes, 'testapp', 0, 1, model_name='author', constraint=models.CheckConstraint(\n                check=models.Q(name__contains='Alice'), name='name_contains_alice'\n            )\n        )\n", "    def test_generate_renamed_models(self):\n        \"\"\"Tests detection and generation of renamed models.\"\"\"\n        before_states = [self.author_empty, self.publisher]\n        after_states = [self.author_renamed_with_book, self.publisher]\n        changes = self.get_changes(before_states, after_states, MigrationQuestioner({\"ask_rename_model\": True}))\n        \n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Writer\")\n        \n        # Make sure no extra changes were detected\n        self.assertEqual(len(changes), 1)\n        self.assertEqual(len(changes['testapp'][0].operations), 1)\n"], "sample_115": ["    def test_cleanse_setting_callable(self):\n        \"\"\"\n        Ensure that callable settings are wrapped correctly and their\n        representation is handled without being called.\n        \"\"\"\n            return \"Sensitive Info\"\n\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed = reporter_filter.cleanse_setting('SECRET_CALLABLE', sensitive_callable)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertNotIn(\"Sensitive Info\", repr(cleansed))\n", "    def test_cleanse_special_types_multivaluedict(self):\n        \"\"\"\n        Test that MultiValueDict values are cleansed properly.\n        \"\"\"\n        reporter_filter = SafeExceptionReporterFilter()\n        request = self.rf.post('/some_url/', self.breakfast_data)\n        value = MultiValueDict({'sensitive': ['secret_value'], 'non_sensitive': ['value']})\n        cleansed_value = reporter_filter.cleanse_special_types(request, value)\n        self.assertEqual(cleansed_value['sensitive'], reporter_filter.cleansed_substitute)\n        self.assertEqual(cleansed_value['non_sensitive'], ['value'])\n", "    def setUp(self):\n        self.reporter_filter = SafeExceptionReporterFilter()\n        self.request = RequestFactory().post('/some_url/', {\n            'non_sensitive': 'visible',\n            'password': 'should_not_be_visible'\n        })\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.reporter_filter = SafeExceptionReporterFilter()\n"], "sample_116": ["    def test_with_empty_fragment_name(self):\n        key = make_template_fragment_key('')\n        self.assertEqual(key, 'template.cache..d41d8cd98f00b204e9800998ecf8427e')\n", "    def test_empty_fragment_name(self):\n        key = make_template_fragment_key('', ['test'])\n        self.assertEqual(key, 'template.cache..098f6bcd4621d373cade4e832627b4f6')\n", "    def test_empty_string_vary_on(self):\n        key = make_template_fragment_key('foo', [''])\n        self.assertEqual(key, 'template.cache.foo.68b329da9893e34099c7d8ad5cb9c940')\n", "    def test_with_special_characters_vary_on(self):\n        key = make_template_fragment_key('foo', ['@!#$', '%^&*'])\n        self.assertEqual(key, 'template.cache.foo.9f4c5f4c1f09e82c3e5e6dc5a9e9c0b7')\n", "    def test_with_empty_string_vary_on(self):\n        key = make_template_fragment_key('foo', [''])\n        self.assertEqual(key, 'template.cache.foo.9cfa9b3ee9dd3500d7d74f05ab0f3a88')\n"], "sample_118": ["    def test_year_lookups(self):\n        # Testing YearExact lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year=2005),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ]\n        )\n        \n        # Testing YearGt lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__gt=2004),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ]\n        )\n        \n        # Testing YearGte lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__gte=2005),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ]\n        )\n        \n        # Testing YearLt lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__lt=2006),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ]\n        )\n        \n        # Testing YearLte lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__lte=2005),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ]\n        )\n", "    def test_contains_lookup(self):\n        \"\"\"\n        Test the 'contains' lookup for filtering querysets.\n        \"\"\"\n        # Test for a single matching article with 'contains' lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(headline__contains='Article 1'),\n            ['<Article: Article 1>']\n        )\n        # Test for multiple matching articles with 'contains' lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(headline__contains='Article'),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ],\n            ordered=False\n        )\n        # Test for case insensitive 'contains' lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(headline__icontains='article 1'),\n            ['<Article: Article 1>']\n        )\n        # Test for no matching articles\n        self.assertQuerysetEqual(\n            Article.objects.filter(headline__contains='Nonexistent Article'),\n            []\n        )\n", "    def test_greater_than_or_equal(self):\n        # Testing GreaterThanOrEqual lookup for integer fields\n        season_2020 = Season.objects.create(year=2020, gt=500)\n        season_2021 = Season.objects.create(year=2021, gt=600)\n        season_2022 = Season.objects.create(year=2022, gt=700)\n\n        self.assertCountEqual(\n            Season.objects.filter(gt__gte=600),\n            [season_2021, season_2022]\n        )\n", "def test_year_lookups(self):\n    season_2009 = Season.objects.create(year=2009)\n    season_2010 = Season.objects.create(year=2010)\n    season_2011 = Season.objects.create(year=2011)\n    \n    # Test YearExact lookup\n    self.assertCountEqual(Season.objects.filter(year__exact=2009), [season_2009])\n    self.assertCountEqual(Season.objects.filter(year__year=2009), [season_2009])\n    \n    # Test YearGt lookup\n    self.assertCountEqual(Season.objects.filter(year__gt=2009), [season_2010, season_2011])\n    \n    # Test YearGte lookup\n    self.assertCountEqual(Season.objects.filter(year__gte=2010), [season_2010, season_2011])\n    \n    # Test YearLt lookup\n    self.assertCountEqual(Season.objects.filter(year__lt=2011), [season_2009, season_2010])\n    \n    # Test YearLte lookup\n    self.assertCountEqual(Season.objects.filter(year__lte=2010), [season_2009, season_2010])\n", "    def test_year_lookup(self):\n        # Create test data with different years\n        Article.objects.bulk_create([\n            Article(headline='Article 2001', pub_date=datetime(2001, 1, 1)),\n            Article(headline='Article 2002', pub_date=datetime(2002, 2, 2)),\n            Article(headline='Article 2003', pub_date=datetime(2003, 3, 3)),\n        ])\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year=2001),\n            ['<Article: Article 2001>']\n        )\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__gte=2002),\n            ['<Article: Article 2002>', '<Article: Article 2003>'],\n            ordered=False\n        )\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__lte=2002),\n            ['<Article: Article 2001>', '<Article: Article 2002>'],\n            ordered=False\n        )\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__range=(2001, 2002)),\n            ['<Article: Article 2001>', '<Article: Article 2002>'],\n            ordered=False\n        )\n"], "sample_119": ["    def test_raw_query_clone(self):\n        raw_query = RawQuery('SELECT * FROM my_table WHERE id = %s', 'default', params=(1,))\n        cloned_query = raw_query.clone('default')\n        \n        self.assertEqual(cloned_query.sql, raw_query.sql)\n        self.assertEqual(cloned_query.using, raw_query.using)\n        self.assertEqual(cloned_query.params, raw_query.params)\n        self.assertIsNot(cloned_query, raw_query)\n", "    def test_multiple_joins(self):\n        query = Query(Author)\n        where = query.build_where(Q(item__ranking__rank__gt=2))\n        self.assertEqual(len(query.alias_map), 3)\n        self.assertEqual(len(where.children), 1)\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, GreaterThan)\n        self.assertEqual(lookup.rhs, 2)\n        self.assertEqual(lookup.lhs.target, Ranking._meta.get_field('rank'))\n", "    def test_chained_query(self):\n        query = Query(Author)\n        cloned_query = query.chain()\n        self.assertIsInstance(cloned_query, Query)\n        self.assertNotEqual(id(query), id(cloned_query))\n        self.assertEqual(cloned_query.model, query.model)\n\n        query_with_class = query.chain(klass=Query)\n        self.assertIsInstance(query_with_class, Query)\n        self.assertNotEqual(id(query), id(query_with_class))\n        self.assertEqual(query_with_class.model, query.model)\n\n        # Check if used_aliases is cleared in cloned query\n        query.used_aliases.add('alias1')\n        cloned_query_with_aliases = query.chain()\n        self.assertEqual(cloned_query_with_aliases.used_aliases, set())\n\n        # Check if filter_is_sticky is handled correctly\n        query.filter_is_sticky = True\n        sticky_cloned_query = query.chain()\n        self.assertEqual(sticky_cloned_query.used_aliases, {'alias1'})\n        self.assertTrue(sticky_cloned_query.filter_is_sticky)\n\n        # Check if klass argument changes the type of the Query\n        from django.db.models.sql.subqueries import UpdateQuery\n        chained_query_with_class = query.chain(klass=UpdateQuery)\n        self.assertIsInstance(chained_query_with_class, UpdateQuery)\n        self.assertNotEqual(id(query), id(chained_query_with_class))\n        self.assertEqual(chained_query_with_class.model, query.model)\n", "def test_add_annotation(self):\n        query = Query(Author)\n        query.add_annotation(Count('num'), alias='num_count', is_summary=True)\n        self.assertIn('num_count', query.annotations)\n        self.assertEqual(query.annotations['num_count'].source_expressions[0].name, 'num')\n", "def test_annotate_query(self):\n    query = Query(Author)\n    query.add_annotation(Count('num'), alias='num_count')\n    where = query.build_where(Q(num_count__gt=2))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.output_field.name, 'num_count')\n"], "sample_120": ["    def test_serialize_custom_type(self):\n        class CustomType:\n                self.value = value\n            \n                return ('migrations.test_writer.CustomType', [self.value], {})\n        \n        custom_instance = CustomType('custom_value')\n        self.assertSerializedResultEqual(\n            custom_instance,\n            (\"migrations.test_writer.CustomType('custom_value')\", {'import migrations.test_writer'})\n        )\n", "    def test_serialize_custom_deconstructable_object(self):\n        @deconstructible\n        class CustomDeconstructable:\n                self.a = a\n                self.b = b\n\n                return (\n                    'migrations.test_writer.CustomDeconstructable',\n                    [self.a, self.b],\n                    {}\n                )\n\n        obj = CustomDeconstructable(1, 2)\n        string, imports = MigrationWriter.serialize(obj)\n        self.assertEqual(\n            string,\n            \"migrations.test_writer.CustomDeconstructable(1, 2)\"\n        )\n        self.assertEqual(imports, {\"import migrations.test_writer\"})\n        self.serialize_round_trip(obj)\n", "    def test_serialize_custom_class_with_deconstruct(self):\n        class CustomClass:\n                self.value = value\n\n                return (\n                    '%s.%s' % (self.__class__.__module__, self.__class__.__name__),\n                    [self.value],\n                    {}\n                )\n\n        custom_obj = CustomClass('test')\n        self.assertSerializedResultEqual(\n            custom_obj,\n            (\"migrations.test_writer.CustomClass('test')\", {'import migrations.test_writer'})\n        )\n", "    def test_serialize_function_type(self):\n            return x * 2\n\n        self.assertSerializedResultEqual(\n            custom_function,\n            ('migrations.test_writer.custom_function', {'import migrations.test_writer'})\n        )\n        with self.assertRaisesMessage(ValueError, 'Cannot serialize function: lambda'):\n            self.assertSerializedEqual(lambda x: x)\n        with self.assertRaisesMessage(ValueError, 'Cannot serialize function <function'):\n            local_func = lambda x: x\n            self.assertSerializedEqual(local_func)\n", "    def test_serialize_function_type_serializer(self):\n            return \"custom\"\n\n        self.assertSerializedResultEqual(\n            custom_function,\n            (\"migrations.test_writer.custom_function\", {\"import migrations.test_writer\"})\n        )\n\n        class SomeClass:\n            @staticmethod\n                return \"static\"\n\n            @classmethod\n                return \"class\"\n\n        self.assertSerializedResultEqual(\n            SomeClass.static_method,\n            (\"migrations.test_writer.SomeClass.static_method\", {\"import migrations.test_writer\"})\n        )\n\n        self.assertSerializedResultEqual(\n            SomeClass.class_method,\n            (\"migrations.test_writer.SomeClass.class_method\", {\"import migrations.test_writer\"})\n        )\n\n        with self.assertRaisesMessage(ValueError, \"Cannot serialize function: lambda\"):\n            self.serialize_round_trip(lambda x: x * 2)\n\n        with self.assertRaisesMessage(ValueError, \"Cannot serialize function <function\"):\n            self.serialize_round_trip(types.FunctionType(lambda x: x * 2, {}))\n"], "sample_121": ["    def test_base_manager_property(self):\n        class CustomManager(models.Manager):\n            pass\n\n        class ModelWithCustomBaseManager(models.Model):\n            objects = CustomManager()\n\n            class Meta:\n                base_manager_name = 'objects'\n\n        self.assertIsInstance(ModelWithCustomBaseManager._base_manager, CustomManager)\n", "    def test_model_name_collision_with_related_field(self):\n        class Person(models.Model):\n            pass\n\n        class PersonDetails(models.Model):\n            person = models.ForeignKey(Person, on_delete=models.CASCADE)\n\n        self.assertEqual(PersonDetails.check(), [])\n", "    def test_proxy_model_with_fields(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n            name = models.CharField(max_length=255)\n\n        class ConcreteModel(AbstractModel):\n            class Meta:\n                abstract = False\n            age = models.IntegerField()\n\n        with self.assertRaisesMessage(\n            TypeError, \"Proxy model 'ProxyModel' contains model fields.\"\n        ):\n            class ProxyModel(ConcreteModel):\n                class Meta:\n                    proxy = True\n", "    def test_proxy_model_with_fields(self):\n        class Parent(models.Model):\n            field1 = models.IntegerField()\n\n        class ProxyModel(Parent):\n            field2 = models.IntegerField()\n\n            class Meta:\n                proxy = True\n\n        self.assertEqual(ProxyModel.check(), [\n            Error(\n                \"Proxy model 'ProxyModel' contains model fields.\",\n                obj=ProxyModel,\n                id='models.E017',\n            )\n        ])\n", "    def test_model_creation_with_meta_options(self):\n        class Model(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.CharField(max_length=100)\n\n            class Meta:\n                app_label = 'invalid_models_tests'\n                ordering = ['field1']\n\n        # Create an instance of the model and check its attributes\n        instance = Model(field1=1, field2='test')\n        self.assertEqual(instance.field1, 1)\n        self.assertEqual(instance.field2, 'test')\n        self.assertEqual(instance._meta.ordering, ['field1'])\n"], "sample_122": ["    def test_patch_cache_control_existing_headers(self):\n        response = HttpResponse()\n        response['Cache-Control'] = 'public, max-age=60'\n        patch_cache_control(response, private=True)\n        self.assertIn('private', response['Cache-Control'])\n        self.assertNotIn('public', response['Cache-Control'])\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, public=True, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'public, max-age=3600')\n\n        # Test adding multiple Cache-Control directives\n        patch_cache_control(response, must_revalidate=True, no_cache=True)\n        self.assertIn('public', response['Cache-Control'])\n        self.assertIn('max-age=3600', response['Cache-Control'])\n        self.assertIn('must-revalidate', response['Cache-Control'])\n        self.assertIn('no-cache', response['Cache-Control'])\n", "    def test_patch_cache_control_simple(self):\n        response = HttpResponse()\n        patch_cache_control(response, public=True)\n        self.assertEqual(response['Cache-Control'], 'public')\n", "    def test_patch_cache_control_no_cache_directive(self):\n        \"\"\"\n        Test patch_cache_control to correctly handle 'no-cache' directive with multiple values.\n        \"\"\"\n        response = HttpResponse()\n        response['Cache-Control'] = 'no-cache=Set-Cookie'\n        patch_cache_control(response, no_cache='Authorization')\n        self.assertIn('no-cache=Set-Cookie', response['Cache-Control'])\n        self.assertIn('no-cache=Authorization', response['Cache-Control'])\n", "    def test_patch_cache_control_no_cache_directive(self):\n        \"\"\"\n        Test patch_cache_control for no-cache directive with multiple field names.\n        \"\"\"\n        response = HttpResponse()\n        patch_cache_control(response, no_cache='Set-Cookie')\n        self.assertEqual(response['Cache-Control'], 'no-cache=Set-Cookie')\n\n        # Add another no-cache field name\n        patch_cache_control(response, no_cache='Link')\n        self.assertEqual(response['Cache-Control'], 'no-cache=Set-Cookie, no-cache=Link')\n\n        # Override no-cache with a single True value\n        patch_cache_control(response, no_cache=True)\n        self.assertEqual(response['Cache-Control'], 'no-cache')\n"], "sample_123": ["    def test_empty_string(self):\n        self.assertEqual(urlsafe_base64_encode(b''), '')\n        self.assertEqual(urlsafe_base64_decode(''), b'')\n", "    def test_urlunquote(self):\n        self.assertEqual(urlunquote('https%3A%2F%2Fexample.com%2Fpath%3Fquery%3Dvalue'), 'https://example.com/path?query=value')\n", "    def test_urlparse(self):\n        result = _urlparse(\"http://example.com/path;params?query#fragment\")\n        self.assertEqual(result.scheme, \"http\")\n        self.assertEqual(result.netloc, \"example.com\")\n        self.assertEqual(result.path, \"/path\")\n        self.assertEqual(result.params, \"params\")\n        self.assertEqual(result.query, \"query\")\n        self.assertEqual(result.fragment, \"fragment\")\n", "    def test_invalid_decode(self):\n        invalid_base64 = 'invalid_base64'\n        with self.assertRaises(ValueError):\n            urlsafe_base64_decode(invalid_base64)\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2Fexample.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n        self.assertEqual(escape_leading_slashes('https://example.com'), 'https://example.com')\n        self.assertEqual(escape_leading_slashes('/path//to/resource'), '/path//to/resource')\n"], "sample_124": ["    def test_charfield_strip(self):\n        \"\"\"\n        Test that the CharField correctly strips whitespace from the input.\n        \"\"\"\n        f = CharField(strip=True)\n        self.assertEqual(f.clean('  hello  '), 'hello')\n        self.assertEqual(f.clean(' world'), 'world')\n        self.assertEqual(f.clean('django'), 'django')\n", "    def test_integer_field_validation(self):\n        class NumberForm(Form):\n            number = IntegerField(min_value=1, max_value=100)\n\n        # Test with valid data\n        form = NumberForm(data={'number': 50})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['number'], 50)\n\n        # Test with a value below the minimum\n        form = NumberForm(data={'number': 0})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['number'], ['Ensure this value is greater than or equal to 1.'])\n\n        # Test with a value above the maximum\n        form = NumberForm(data={'number': 101})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['number'], ['Ensure this value is less than or equal to 100.'])\n\n        # Test with a non-integer value\n        form = NumberForm(data={'number': 'abc'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['number'], ['Enter a whole number.'])\n\n        # Test with a valid edge case (minimum)\n        form = NumberForm(data={'number': 1})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['number'], 1)\n\n        # Test with a valid edge case (maximum)\n        form = NumberForm(data={'number': 100})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['number'], 100)\n", "    def test_integer_field_to_python(self):\n        class TestForm(Form):\n            age = IntegerField()\n\n        form = TestForm(data={'age': '25'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['age'], 25)\n\n        form = TestForm(data={'age': '25.0'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['age'], 25)\n\n        form = TestForm(data={'age': '25.5'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('age', form.errors)\n        self.assertEqual(form.errors['age'], ['Enter a whole number.'])\n\n        form = TestForm(data={'age': ''})\n        self.assertFalse(form.is_valid())\n        self.assertIn('age', form.errors)\n        self.assertEqual(form.errors['age'], ['This field is required.'])\n\n        form = TestForm(data={'age': None})\n        self.assertFalse(form.is_valid())\n        self.assertIn('age', form.errors)\n        self.assertEqual(form.errors['age'], ['This field is required.'])\n\n        form = TestForm(data={'age': 'abc'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('age', form.errors)\n        self.assertEqual(form.errors['age'], ['Enter a whole number.'])\n", "def test_emailfield_validation(self):\n    # Test that EmailField properly validates email addresses.\n    class EmailForm(Form):\n        email = EmailField()\n\n    # Test valid email\n    form = EmailForm({'email': 'test@example.com'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['email'], 'test@example.com')\n\n    # Test invalid email\n    form = EmailForm({'email': 'invalid-email'})\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors['email'], ['Enter a valid email address.'])\n\n    # Test empty email\n    form = EmailForm({'email': ''})\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors['email'], ['This field is required.'])\n\n    # Test empty email with required=False\n    class OptionalEmailForm(Form):\n        email = EmailField(required=False)\n\n    form = OptionalEmailForm({'email': ''})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['email'], '')\n\n    # Test email with initial value\n    form = EmailForm(initial={'email': 'initial@example.com'})\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for=\"id_email\">Email:</label> <input type=\"email\" name=\"email\" value=\"initial@example.com\" id=\"id_email\" required></p>'\n    )\n", "    def test_duration_field(self):\n        class DurationForm(Form):\n            duration = DurationField()\n\n        # Valid cases\n        valid_data = {'duration': '1 00:00:00'}\n        form = DurationForm(valid_data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['duration'], datetime.timedelta(days=1))\n\n        valid_data = {'duration': '2 days, 3:04:05'}\n        form = DurationForm(valid_data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['duration'], datetime.timedelta(days=2, hours=3, minutes=4, seconds=5))\n\n        valid_data = {'duration': '5:00:00'}\n        form = DurationForm(valid_data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['duration'], datetime.timedelta(hours=5))\n\n        valid_data = {'duration': '00:05:00'}\n        form = DurationForm(valid_data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['duration'], datetime.timedelta(minutes=5))\n\n        valid_data = {'duration': '00:00:10'}\n        form = DurationForm(valid_data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['duration'], datetime.timedelta(seconds=10))\n\n        # Invalid cases\n        invalid_data = {'duration': 'invalid'}\n        form = DurationForm(invalid_data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['duration'], ['Enter a valid duration.'])\n\n        invalid_data = {'duration': '1000 days, 00:00:00'}\n        form = DurationForm(invalid_data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors['duration'],\n            [f'The number of days must be between {datetime.timedelta.min.days} and {datetime.timedelta.max.days}.']\n        )\n\n        invalid_data = {'duration': ''}\n        form = DurationForm(invalid_data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['duration'], ['This field is required.'])\n"], "sample_125": ["    def test_set_header(self):\n        \"\"\"HttpResponseBase can set headers.\"\"\"\n        response = HttpResponse()\n        response['X-Test-Header'] = 'TestValue'\n        self.assertEqual(response['X-Test-Header'], 'TestValue')\n", "    def test_reason_phrase_default(self):\n        \"\"\"Test the default reason phrase based on the status code.\"\"\"\n        response = HttpResponse(status=404)\n        self.assertEqual(response.reason_phrase, 'Not Found')\n", "    def test_default_response(self):\n        response = HttpResponse()\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.reason_phrase, 'OK')\n        self.assertEqual(response.charset, settings.DEFAULT_CHARSET)\n        self.assertEqual(response['Content-Type'], f'text/html; charset={settings.DEFAULT_CHARSET}')\n", "    def test_charset_extraction(self):\n        \"\"\"HttpResponse correctly extracts charset from Content-Type header.\"\"\"\n        response = HttpResponse(content_type='text/html; charset=iso-8859-1')\n        self.assertEqual(response.charset, 'iso-8859-1')\n", "    def test_initial_status_code(self):\n        \"\"\"HttpResponse should have a default status code of 200.\"\"\"\n        response = HttpResponse()\n        self.assertEqual(response.status_code, 200)\n"], "sample_126": ["    def test_generate_altered_constraints(self):\n        \"\"\"Test the generation of altered constraints.\"\"\"\n        before_state = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ], {'constraints': [models.CheckConstraint(check=models.Q(name__contains='Alice'), name='name_contains_alice')]})\n\n        after_state = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ], {'constraints': [models.CheckConstraint(check=models.Q(name__contains='Bob'), name='name_contains_bob')]})\n\n        changes = self.get_changes([before_state], [after_state])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveConstraint', 'AddConstraint'])\n        \n        self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author', name='name_contains_alice')\n        added_constraint = models.CheckConstraint(check=models.Q(name__contains='Bob'), name='name_contains_bob')\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='author', constraint=added_constraint)\n", "    def test_add_non_field_attribute(self):\n        \"\"\"Tests autodetection of added non-field attributes.\"\"\"\n        before = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        after = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"_meta\", {\"verbose_name\": \"Author Model\"}),\n        ])\n        changes = self.get_changes([before], [after])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelOptions\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", options={\"verbose_name\": \"Author Model\"})\n", "    def test_alter_field_preserving_db_column(self):\n        \"\"\"\n        Tests that altering a field preserves the db_column attribute if it hasn't changed.\n        \"\"\"\n        # Initial state with a field having a db_column attribute.\n        before = [\n            ModelState('app', 'Model', [\n                ('id', models.AutoField(primary_key=True)),\n                ('field', models.IntegerField(db_column='custom_column')),\n            ]),\n        ]\n        # State after altering the field's max_length but preserving db_column.\n        after = [\n            ModelState('app', 'Model', [\n                ('id', models.AutoField(primary_key=True)),\n                ('field', models.CharField(max_length=255, db_column='custom_column')),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        # Verify that the changes are correct.\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, model_name='model', name='field')\n        self.assertOperationFieldAttributes(changes, 'app', 0, 0, db_column='custom_column')\n", "    def test_create_model_with_constraints(self):\n        \"\"\"\n        Test the creation of a new model with multiple types of constraints.\n        \"\"\"\n        constraints = [\n            models.CheckConstraint(check=models.Q(name__contains='Test'), name='name_contains_test'),\n            models.UniqueConstraint(fields=['name'], name='unique_name')\n        ]\n        author_with_constraints = ModelState('otherapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ], {'constraints': constraints})\n\n        changes = self.get_changes([], [author_with_constraints])\n        added_constraint_1 = models.CheckConstraint(check=models.Q(name__contains='Test'), name='name_contains_test')\n        added_constraint_2 = models.UniqueConstraint(fields=['name'], name='unique_name')\n\n        # Right number of migrations?\n        self.assertEqual(len(changes['otherapp']), 1)\n        # Right number of actions?\n        migration = changes['otherapp'][0]\n        self.assertEqual(len(migration.operations), 3)\n        # Right actions order?\n        self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel', 'AddConstraint', 'AddConstraint'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Author')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, model_name='author', constraint=added_constraint_1)\n        self.assertOperationAttributes(changes, 'otherapp', 0, 2, model_name='author', constraint=added_constraint_2)\n", "    def test_alter_field_to_unique(self):\n        \"\"\"Test alteration of a field to have a unique constraint.\"\"\"\n        before = [ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])]\n        after = [ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, unique=True)),\n        ])]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", unique=True)\n"], "sample_127": ["    def setUp(self):\n        self.countries = [\n            Country.objects.create(name=\"United States of America\", iso_two_letter=\"US\"),\n            Country.objects.create(name=\"The Netherlands\", iso_two_letter=\"NL\"),\n            Country.objects.create(name=\"Germany\", iso_two_letter=\"DE\"),\n            Country.objects.create(name=\"Czech Republic\", iso_two_letter=\"CZ\")\n        ]\n", "    def setUp(self):\n        self.country = Country.objects.create(name=\"Test Country\", iso_two_letter=\"TC\")\n        self.restaurant = Restaurant.objects.create(name=\"Test Restaurant\")\n        self.state = State.objects.create(two_letter_code=\"TS\")\n", "    def setUp(self):\n        self.country1 = Country.objects.create(name=\"United States of America\", iso_two_letter=\"US\")\n        self.country2 = Country.objects.create(name=\"The Netherlands\", iso_two_letter=\"NL\")\n        self.country3 = Country.objects.create(name=\"Germany\", iso_two_letter=\"DE\")\n        self.country4 = Country.objects.create(name=\"Czech Republic\", iso_two_letter=\"CZ\")\n", "    def test_bulk_update(self):\n        # Create initial data\n        countries = Country.objects.bulk_create([\n            Country(name=\"Country A\", iso_two_letter=\"A\"),\n            Country(name=\"Country B\", iso_two_letter=\"B\"),\n            Country(name=\"Country C\", iso_two_letter=\"C\")\n        ])\n        # Update iso_two_letter codes\n        for country in countries:\n            country.iso_two_letter = country.iso_two_letter.lower()\n        Country.objects.bulk_update(countries, ['iso_two_letter'])\n        # Validate updates\n        self.assertQuerysetEqual(Country.objects.order_by(\"name\"), [\n            \"a\", \"b\", \"c\"\n        ], attrgetter(\"iso_two_letter\"))\n", "    def test_bulk_update(self):\n        countries = Country.objects.bulk_create(self.data)\n        self.assertEqual(len(countries), 4)\n\n        # Update the names of the countries\n        countries[0].name = \"USA\"\n        countries[1].name = \"Holland\"\n        countries[2].name = \"Deutschland\"\n        countries[3].name = \"Czechia\"\n        Country.objects.bulk_update(countries, ['name'])\n\n        updated_countries = Country.objects.order_by(\"name\")\n        self.assertQuerysetEqual(updated_countries, [\n            \"Czechia\", \"Deutschland\", \"Holland\", \"USA\"\n        ], attrgetter(\"name\"))\n"], "sample_128": ["    def test_deconstruct_without_optional_args(self):\n        index = Index(fields=['headline'], name='headline_idx')\n        path, args, kwargs = index.deconstruct()\n        self.assertEqual(path, 'indexes.Index')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'fields': ['headline'], 'name': 'headline_idx'})\n", "    def test_index_initialization(self):\n        \"\"\"\n        Test initialization of Index with various parameters.\n        \"\"\"\n        index = Index(fields=['headline', 'pub_date'], name='test_idx', db_tablespace='pg_default')\n        self.assertEqual(index.fields, ['headline', 'pub_date'])\n        self.assertEqual(index.name, 'test_idx')\n        self.assertEqual(index.db_tablespace, 'pg_default')\n        self.assertEqual(index.opclasses, ())\n        self.assertIsNone(index.condition)\n        self.assertEqual(index.include, ())\n", "    def test_index_deconstruct(self):\n        index = Index(\n            name='deconstruct_idx',\n            fields=['headline', 'pub_date'],\n            db_tablespace='pg_default',\n            opclasses=['varchar_pattern_ops', 'text_pattern_ops'],\n            condition=Q(headline__contains='Django'),\n            include=['published']\n        )\n        path, args, kwargs = index.deconstruct()\n        self.assertEqual(path, 'your_module_path.Index')  # Replace 'your_module_path' with the actual module path\n        self.assertEqual(kwargs['name'], 'deconstruct_idx')\n        self.assertEqual(kwargs['fields'], ['headline', 'pub_date'])\n        self.assertEqual(kwargs['db_tablespace'], 'pg_default')\n        self.assertEqual(kwargs['opclasses'], ['varchar_pattern_ops', 'text_pattern_ops'])\n        self.assertEqual(kwargs['condition'], Q(headline__contains='Django'))\n        self.assertEqual(kwargs['include'], ('published',))\n", "    def test_index_initialization_with_valid_parameters(self):\n        \"\"\"\n        Test Index initialization with valid parameters.\n        \"\"\"\n        index = Index(fields=['headline', 'pub_date'], name='headline_pub_date_idx', include=['author'])\n        self.assertEqual(index.fields, ['headline', 'pub_date'])\n        self.assertEqual(index.name, 'headline_pub_date_idx')\n        self.assertEqual(index.include, ('author',))\n", "    def test_index_deconstruction(self):\n        index = Index(\n            fields=['headline', 'pub_date'],\n            name='test_index',\n            db_tablespace='default',\n            opclasses=['text_pattern_ops', 'date_ops'],\n            condition=Q(pub_date__gt=datetime.datetime(2020, 1, 1)),\n            include=['published'],\n        )\n        deconstructed = index.deconstruct()\n        expected_deconstructed = (\n            'django.db.models.Index',\n            (),\n            {\n                'fields': ['headline', 'pub_date'],\n                'name': 'test_index',\n                'db_tablespace': 'default',\n                'opclasses': ['text_pattern_ops', 'date_ops'],\n                'condition': Q(pub_date__gt=datetime.datetime(2020, 1, 1)),\n                'include': ('published',),\n            }\n        )\n        self.assertEqual(deconstructed, expected_deconstructed)\n"], "sample_129": ["    def test_edge_case_large_numbers(self):\n        self.assertEqual(floatformat(1e+25, 2), '10000000000000000000000000.00')\n        self.assertEqual(floatformat(-1e+25, -2), '-10000000000000000000000000')\n        self.assertEqual(floatformat(1.234567890123456e+25, 10), '12345678901234560000000.0000000000')\n", "    def test_string_inputs(self):\n        self.assertEqual(floatformat(\"7.7\"), '7.7')\n        self.assertEqual(floatformat(\"0.07\"), '0.1')\n        self.assertEqual(floatformat(\"0.007\"), '0.0')\n        self.assertEqual(floatformat(\"7.7\", 0), '8')\n        self.assertEqual(floatformat(\"13.1031\", -3), '13.103')\n        self.assertEqual(floatformat(\"11.1197\", -2), '11.12')\n        self.assertEqual(floatformat(\"11.0000\", -2), '11')\n        self.assertEqual(floatformat(\"11.000001\", -2), '11.00')\n        self.assertEqual(floatformat(\"8.2798\", 3), '8.280')\n        self.assertEqual(floatformat(\"5555.555\", 2), '5555.56')\n", "    def test_edge_cases(self):\n        # Test with very large integer value\n        self.assertEqual(floatformat(10**10, 3), '10000000000.000')\n        self.assertEqual(floatformat(10**10, -3), '10000000000')\n\n        # Test with very small float value\n        self.assertEqual(floatformat(1e-10, 10), '0.0000000001')\n        self.assertEqual(floatformat(1e-10, -10), '0.0000000001')\n\n        # Test with string that can be converted to float\n        self.assertEqual(floatformat(\"123.456\", 2), '123.46')\n        self.assertEqual(floatformat(\"123.456\", -2), '123.46')\n\n        # Test with negative float values\n        self.assertEqual(floatformat(-123.456, 2), '-123.46')\n        self.assertEqual(floatformat(-123.456, -2), '-123.46')\n\n        # Test with empty string\n        self.assertEqual(floatformat('', 2), '')\n\n        # Test with special float values\n        self.assertEqual(floatformat(float('nan')), 'nan')\n        self.assertEqual(floatformat(float('inf')), 'inf')\n        self.assertEqual(floatformat(float('-inf')), '-inf')\n", "    def test_trailing_zeros(self):\n        self.assertEqual(floatformat(1.2300000, 2), '1.23')\n        self.assertEqual(floatformat(1.2300000, -2), '1.23')\n        self.assertEqual(floatformat(1.0000000, 2), '1.00')\n        self.assertEqual(floatformat(1.0000000, -2), '1')\n", "    def test_float_edge_cases(self):\n        self.assertEqual(floatformat(\"1e-2\"), '0.0')\n        self.assertEqual(floatformat(Decimal('1e-2')), '0.01')\n        self.assertEqual(floatformat(Decimal('1e-2'), 3), '0.010')\n        self.assertEqual(floatformat(Decimal('1e-2'), -3), '0.01')\n"], "sample_130": ["def test_raw_query_repr(self):\n        raw_query = RawQuery(\"SELECT * FROM author WHERE num > %s\", \"default\", params=(2,))\n        self.assertEqual(repr(raw_query), \"<RawQuery: SELECT * FROM author WHERE num > 2>\")\n", "def test_rawquery_get_columns(self):\n    sql = \"SELECT id, name FROM my_table\"\n    raw_query = RawQuery(sql, using='default')\n    with connections['default'].cursor() as cursor:\n        raw_query.cursor = cursor\n        raw_query.cursor.execute(\"CREATE TEMP TABLE my_table (id INTEGER, name TEXT)\")\n        raw_query.cursor.execute(\"INSERT INTO my_table (id, name) VALUES (1, 'foo'), (2, 'bar')\")\n        columns = raw_query.get_columns()\n    self.assertEqual(columns, ['id', 'name'])\n", "def test_clone_with_annotations(self):\n        query = Query(Author)\n        query.add_annotation(Count('id'), alias='author_count')\n        clone = query.clone()\n        self.assertIn('author_count', clone.annotations)\n        self.assertEqual(clone.annotations['author_count'].source_expressions[0].name, 'id')\n        self.assertIsInstance(clone.annotations['author_count'], Count)\n", "    def test_subquery_alias_bump(self):\n        query1 = Query(Author)\n        query2 = Query(Author)\n        query1.bump_prefix(query2)\n        \n        self.assertNotEqual(query1.alias_prefix, query2.alias_prefix)\n        \n        initial_alias1 = query1.get_initial_alias()\n        initial_alias2 = query2.get_initial_alias()\n        \n        self.assertNotEqual(initial_alias1, initial_alias2)\n        \n        query1.add_fields(['name'])\n        query2.add_fields(['name'])\n        \n        self.assertEqual(query1.select[0].alias, initial_alias1)\n        self.assertEqual(query2.select[0].alias, initial_alias2)\n", "def test_query_set_limits(self):\n    query = Query(Author)\n    query.set_limits(low=10, high=20)\n    self.assertEqual(query.low_mark, 10)\n    self.assertEqual(query.high_mark, 20)\n    query.set_limits(low=5)\n    self.assertEqual(query.low_mark, 15)\n    query.set_limits(high=10)\n    self.assertEqual(query.high_mark, 25)\n    query.clear_limits()\n    self.assertEqual(query.low_mark, 0)\n    self.assertIsNone(query.high_mark)\n"], "sample_131": ["    def test_create_test_db_with_serialize(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'), \\\n                 mock.patch.object(creation, 'serialize_db_to_string', return_value='{}'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n            mocked_migrate.assert_called()\n            self.assertEqual(connection._test_serialized_contents, '{}')\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_serialize_db_to_string(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            with mock.patch.object(creation.connection, 'ensure_connection'), \\\n                 mock.patch.object(creation, 'serialize_db_to_string', return_value='{}'), \\\n                 mock.patch.object(creation, '_create_test_db'):\n                result = creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n                self.assertEqual(creation.connection._test_serialized_contents, '{}')\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_createcachetable_called(self, mocked_call_command, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_call_command.assert_any_call('createcachetable', database=connection.alias)\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_serialize_deserialize_db(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        with mock.patch.object(creation, '_create_test_db'):\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n        \n        serialized_data = creation.connection._test_serialized_contents\n        self.assertIsNotNone(serialized_data)\n        \n        with mock.patch.object(creation.connection, '_test_serialized_contents', new=serialized_data):\n            creation.deserialize_db_from_string(serialized_data)\n        \n        # Check if the deserialization did not raise any errors\n        self.assertTrue(True)\n", "    def test_create_test_db_with_serialize(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'), \\\n                 mock.patch.object(creation, 'serialize_db_to_string', return_value='{}'), \\\n                 mock.patch('django.core.management.call_command'):\n                test_db_name = creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n                self.assertEqual(test_db_name, creation._get_test_db_name())\n                self.assertEqual(creation.connection._test_serialized_contents, '{}')\n        finally:\n            connection.settings_dict = saved_settings\n"], "sample_132": ["    def test_cleanse_multivaluedict(self):\n        \"\"\"Test that sensitive MultiValueDict keys are cleansed properly.\"\"\"\n        reporter_filter = SafeExceptionReporterFilter()\n        request = self.rf.post('/some_url/', {'key1': 'value1', 'password': 'secret'})\n        request.sensitive_post_parameters = ['password']\n        cleansed_multivaluedict = reporter_filter.get_cleansed_multivaluedict(request, request.POST)\n        self.assertEqual(cleansed_multivaluedict['password'], reporter_filter.cleansed_substitute)\n        self.assertEqual(cleansed_multivaluedict['key1'], 'value1')\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_get_cleansed_multivaluedict(self):\n        \"\"\"\n        Ensure that sensitive POST parameters in MultiValueDicts are cleansed.\n        \"\"\"\n        request = self.rf.post('/test_view/', data={\n            'normal_key': 'normal_value',\n            'password': 'super_secret',\n        })\n        request.sensitive_post_parameters = ['password']\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed_dict = reporter_filter.get_cleansed_multivaluedict(request, request.POST)\n        self.assertEqual(cleansed_dict['normal_key'], 'normal_value')\n        self.assertEqual(cleansed_dict['password'], reporter_filter.cleansed_substitute)\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.factory = RequestFactory()\n"], "sample_135": ["    def test_rfc_5322_format(self):\n        dt = datetime(2000, 12, 21, 16, 1, 7, tzinfo=utc)\n        self.assertEqual(dateformat.format(dt, 'r'), 'Thu, 21 Dec 2000 16:01:07 +0000')\n", "    def test_iso_year_number(self):\n        # ISO 8601 year number matching the ISO week number\n        dt = datetime(2009, 12, 28)\n        self.assertEqual(dateformat.format(dt, 'o'), '2009')\n        dt = datetime(2010, 1, 4)\n        self.assertEqual(dateformat.format(dt, 'o'), '2010')\n", "    def test_iso8601_year_number(self):\n        dt = datetime(2023, 1, 1)  # First day of the ISO year 2023\n        self.assertEqual(dateformat.format(dt, 'o'), '2023')\n\n        dt = datetime(2023, 12, 31)  # Last day of the ISO year 2023\n        self.assertEqual(dateformat.format(dt, 'o'), '2023')\n\n        dt = datetime(2023, 1, 2)  # Second day of the ISO year 2023\n        self.assertEqual(dateformat.format(dt, 'o'), '2023')\n", "    def test_iso_year_and_week(self):\n        dt = datetime(2023, 10, 10)\n        self.assertEqual(dateformat.format(dt, 'o'), '2023')\n        self.assertEqual(dateformat.format(dt, 'W'), '41')\n", "    def test_iso_year_number(self):\n        dt = datetime(2023, 1, 1)\n        self.assertEqual(dateformat.format(dt, 'o'), '2022')  # ISO year number, since Jan 1, 2023 is part of the last week of 2022\n        dt = datetime(2023, 12, 31)\n        self.assertEqual(dateformat.format(dt, 'o'), '2024')  # ISO year number, since Dec 31, 2023 is part of the first week of 2024\n"], "sample_134": ["    def test_serialize_custom_function_type(self):\n            return \"This is a custom function\"\n\n        self.assertSerializedEqual(custom_function)\n\n        class TestClass:\n            @staticmethod\n                return \"This is a static method\"\n\n            @classmethod\n                return \"This is a class method\"\n\n        self.assertSerializedEqual(TestClass.static_method)\n        self.assertSerializedEqual(TestClass.class_method)\n", "    def test_serialize_functools_partial_arguments(self):\n        \"\"\"\n        Test serializing functools.partial with various types of arguments.\n        \"\"\"\n        partial_func = functools.partial(int, base=2)\n        self.assertSerializedResultEqual(\n            partial_func,\n            (\"functools.partial(int, base=2)\", {\"import functools\"})\n        )\n        partial_func_no_kwargs = functools.partial(int, '1010')\n        self.assertSerializedResultEqual(\n            partial_func_no_kwargs,\n            (\"functools.partial(int, '1010')\", {\"import functools\"})\n        )\n        partial_func_only_args = functools.partial(int, '1010', 2)\n        self.assertSerializedResultEqual(\n            partial_func_only_args,\n            (\"functools.partial(int, '1010', 2)\", {\"import functools\"})\n        )\n", "    def test_serialize_custom_deconstructable_object(self):\n        @deconstructible\n        class CustomDeconstructable:\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n                return (\n                    'migrations.test_writer.CustomDeconstructable',\n                    [self.arg1, self.arg2],\n                    {}\n                )\n\n        value = CustomDeconstructable(1, 'test')\n        self.assertSerializedResultEqual(\n            value,\n            (\n                \"migrations.test_writer.CustomDeconstructable(1, 'test')\",\n                {'import migrations.test_writer'}\n            )\n        )\n        self.assertEqual(self.serialize_round_trip(value).arg1, 1)\n        self.assertEqual(self.serialize_round_trip(value).arg2, 'test')\n", "    def test_serialize_function_type_instance_method(self):\n        class SampleClass:\n                pass\n\n        instance_method = SampleClass().instance_method\n        string, imports = MigrationWriter.serialize(instance_method)\n        self.assertEqual(string, 'migrations.test_writer.SampleClass.instance_method')\n        self.assertEqual(imports, {'import migrations.test_writer'})\n        self.serialize_round_trip(instance_method)\n", "    def test_serialize_custom_deconstructable(self):\n        @deconstructible\n        class CustomDeconstructable:\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n                path = \"%s.%s\" % (self.__class__.__module__, self.__class__.__name__)\n                return (path, [self.arg1, self.arg2], {})\n\n        instance = CustomDeconstructable('value1', 'value2')\n        self.assertSerializedEqual(instance)\n        self.assertSerializedResultEqual(\n            instance,\n            (\n                \"migrations.test_writer.CustomDeconstructable('value1', 'value2')\",\n                {'import migrations.test_writer'},\n            ),\n        )\n"], "sample_136": ["    def test_http_request_custom_scheme(self):\n        class CustomHttpRequest(HttpRequest):\n                return 'custom-scheme'\n\n        request = CustomHttpRequest()\n        self.assertEqual(request.scheme, 'custom-scheme')\n        self.assertEqual(request.is_secure(), False)\n", "    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES['test_cookie'] = signing.get_cookie_signer(salt='test_salt').sign('cookie_value')\n        \n        # Valid signed cookie\n        self.assertEqual(request.get_signed_cookie('test_cookie', salt='test_salt'), 'cookie_value')\n\n        # Invalid signed cookie\n        request.COOKIES['test_cookie'] = 'invalid_signed_cookie'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('test_cookie', salt='test_salt')\n\n        # Non-existent cookie\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('non_existent_cookie', salt='test_salt')\n\n        # Non-existent cookie with default\n        self.assertEqual(request.get_signed_cookie('non_existent_cookie', default='default_value', salt='test_salt'), 'default_value')\n\n        # Expired signed cookie\n        request.COOKIES['test_cookie'] = signing.get_cookie_signer(salt='test_salt').sign('cookie_value')\n        with self.assertRaises(signing.SignatureExpired):\n            request.get_signed_cookie('test_cookie', salt='test_salt', max_age=-1)\n", "    def test_http_request_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES['signed_cookie'] = signing.get_cookie_signer(salt='test_salt').sign('test_value')\n        \n        # Test successfully getting a signed cookie\n        self.assertEqual(request.get_signed_cookie('signed_cookie', salt='test_salt'), 'test_value')\n        \n        # Test getting a signed cookie with a bad signature\n        request.COOKIES['bad_signed_cookie'] = 'bad_value'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('bad_signed_cookie', salt='test_salt')\n        \n        # Test getting a signed cookie that doesn't exist, with default value\n        self.assertEqual(request.get_signed_cookie('nonexistent_cookie', default='default_value', salt='test_salt'), 'default_value')\n        \n        # Test getting a signed cookie that doesn't exist, without default value\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('nonexistent_cookie', salt='test_salt')\n", "    def test_get_signed_cookie_valid_signature(self):\n        request = HttpRequest()\n        request.COOKIES['my_cookie'] = signing.get_cookie_signer(salt='mysalt').sign('cookie_value')\n        self.assertEqual(request.get_signed_cookie('my_cookie', salt='mysalt'), 'cookie_value')\n", "    def test_set_invalid_encoding(self):\n        \"\"\"\n        Setting an invalid encoding should not raise an error immediately,\n        but should fall back to the default charset when accessing GET or POST.\n        \"\"\"\n        request = HttpRequest()\n        request.encoding = 'invalid-charset'\n        self.assertEqual(request.GET.encoding, settings.DEFAULT_CHARSET)\n        self.assertEqual(request.POST.encoding, settings.DEFAULT_CHARSET)\n"], "sample_139": ["    def test_get_content_type_for_model(self):\n        \"\"\"\n        Test if get_content_type_for_model returns the correct ContentType for a given model.\n        \"\"\"\n        band = Band.objects.create(name=\"Test Band\", nr_of_members=4)\n        content_type = get_content_type_for_model(band)\n        self.assertEqual(content_type.model, 'band')\n        self.assertEqual(content_type.app_label, 'admin_changelist')\n", "    def test_formfield_for_dbfield_choices(self):\n        \"\"\"\n        Test `formfield_for_dbfield` method for a db field that has choices\n        defined.\n        \"\"\"\n        class TestModel(models.Model):\n            STATUS_CHOICES = (\n                ('draft', 'Draft'),\n                ('published', 'Published'),\n            )\n            status = models.CharField(max_length=50, choices=STATUS_CHOICES)\n\n        class TestModelAdmin(admin.ModelAdmin):\n            pass\n\n        model_admin = TestModelAdmin(TestModel, admin.site)\n        request = self.factory.get('/testmodel/')\n        request.user = self.superuser\n        db_field = TestModel._meta.get_field('status')\n        formfield = model_admin.formfield_for_dbfield(db_field, request)\n        self.assertIsInstance(formfield.widget, widgets.AdminRadioSelect)\n        self.assertEqual(formfield.choices, db_field.get_choices(include_blank=db_field.blank, blank_choice=[('', _('None'))]))\n", "    def test_get_view_on_site_url(self):\n        \"\"\"\n        Test the get_view_on_site_url method in ModelAdmin.\n        \"\"\"\n        class TestModel(models.Model):\n            name = models.CharField(max_length=50)\n                return \"/test_model/%s/\" % self.pk\n\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = True\n\n        site = admin.AdminSite()\n        site.register(TestModel, TestModelAdmin)\n\n        m = TestModelAdmin(TestModel, site)\n        obj = TestModel(name=\"test\")\n        obj.save()\n        request = self.factory.get('/test_model/')\n        request.user = self.superuser\n        \n        url = m.get_view_on_site_url(obj)\n        self.assertEqual(url, reverse('admin:view_on_site', kwargs={\n            'content_type_id': get_content_type_for_model(obj).pk,\n            'object_id': obj.pk\n        }))\n", "def test_get_view_on_site_url(self):\n    \"\"\"\n    Test if `get_view_on_site_url` returns the correct URL when\n    `view_on_site` is enabled and the object has a `get_absolute_url` method.\n    \"\"\"\n    class TestModel(models.Model):\n        name = models.CharField(max_length=100)\n\n            return f\"/testmodel/{self.id}/\"\n\n    class TestModelAdmin(admin.ModelAdmin):\n        view_on_site = True\n\n    site = admin.AdminSite(name='admin')\n    site.register(TestModel, TestModelAdmin)\n    test_instance = TestModel(id=1, name=\"Test Name\")\n    admin_instance = TestModelAdmin(TestModel, site)\n    \n    url = admin_instance.get_view_on_site_url(test_instance)\n    self.assertEqual(url, \"/testmodel/1/\")\n\n    site.unregister(TestModel)\n", "def test_get_exclude(self):\n        \"\"\"\n        Test ModelAdmin.get_exclude() method to ensure it returns expected\n        exclude fields.\n        \"\"\"\n        class CustomExcludeModelAdmin(admin.ModelAdmin):\n            exclude = ('field1', 'field2')\n\n        m = CustomExcludeModelAdmin(Parent, custom_site)\n        request = self.factory.get('/parent/')\n        request.user = self.superuser\n        exclude_fields = m.get_exclude(request)\n        self.assertEqual(exclude_fields, ('field1', 'field2'))\n\n        # Check overriding get_exclude method\n        class OverrideExcludeModelAdmin(admin.ModelAdmin):\n                return ('field3', 'field4')\n\n        m = OverrideExcludeModelAdmin(Parent, custom_site)\n        request = self.factory.get('/parent/')\n        request.user = self.superuser\n        exclude_fields = m.get_exclude(request)\n        self.assertEqual(exclude_fields, ('field3', 'field4'))\n"], "sample_137": ["    def test_replace_named_groups(self):\n        pattern = r'^(?P<name>\\w+)/b/(?P<age>\\d+)$'\n        expected_output = r'^<name>/b/<age>$'\n        self.assertEqual(replace_named_groups(pattern), expected_output)\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n        expected_output = r'^<a>/b/<c>/$'\n        self.assertEqual(replace_named_groups(pattern), expected_output)\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<slug>[\\w-]+)/$'\n        expected = r'^<year>/<month>/<slug>/$'\n        result = replace_named_groups(pattern)\n        self.assertEqual(result, expected)\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n        expected_result = r'^<a>/b/<c>/$'\n        self.assertEqual(replace_named_groups(pattern), expected_result)\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)$'\n        expected = r'^<a>/b/<c>$'\n        self.assertEqual(replace_named_groups(pattern), expected)\n        \n        pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n        expected = r'^<a>/b/(\\w+)$'\n        self.assertEqual(replace_named_groups(pattern), expected)\n        \n        pattern = r'^(?P<a>\\w+)/b/(?P<c>(x|y)\\w+)$'\n        expected = r'^<a>/b/<c>$'\n        self.assertEqual(replace_named_groups(pattern), expected)\n        \n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n        expected = r'^<a>/b/<c>'\n        self.assertEqual(replace_named_groups(pattern), expected)\n"], "sample_140": ["    def test_sensitive_variables_all(self):\n        @sensitive_variables()\n            return a + b + c\n\n        wrapped_func = test_func.__wrapped__\n        self.assertEqual(test_func.sensitive_variables, '__ALL__')\n        self.assertEqual(wrapped_func(1, 2, 3), 6)\n", "    def test_sensitive_variables_with_specified_variables(self):\n        @sensitive_variables('user', 'password', 'credit_card')\n            password = user.pass_word\n            credit_card = user.credit_card_number\n            return password, credit_card\n\n        func = my_function\n        self.assertEqual(func.sensitive_variables, ('user', 'password', 'credit_card'))\n", "    def test_sensitive_variables_decorator_all_vars(self):\n        \"\"\"\n        Test the sensitive_variables decorator when no specific variables\n        are marked as sensitive, i.e., all variables are treated as sensitive.\n        \"\"\"\n        @sensitive_variables()\n            return user, password, credit_card\n\n        func = my_function.sensitive_variables_wrapper\n        self.assertEqual(func.sensitive_variables, '__ALL__')\n        result = func('user1', 'pass1', 'card1')\n        self.assertEqual(result, ('user1', 'pass1', 'card1'))\n", "    def test_sensitive_variables_with_specified_vars(self):\n        @sensitive_variables('password', 'credit_card')\n            password = user['password']\n            credit_card = user['credit_card']\n            raise ValueError(\"Simulated exception\")\n\n        user_data = {'password': 'super_secret', 'credit_card': '1234-5678-9101-1121'}\n        \n        try:\n            test_func(user_data)\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n            frame = tb.tb_frame.f_back\n            sensitive_vars = frame.f_locals.get('test_func').sensitive_variables\n            self.assertEqual(sensitive_vars, ('password', 'credit_card'))\n            ", "    def test_sensitive_variables_all(self):\n        @sensitive_variables()\n            return password, credit_card\n\n        wrapped_func = test_func.__wrapped__\n        self.assertEqual(wrapped_func.sensitive_variables, '__ALL__')\n        self.assertEqual(test_func('my_password', '1234-5678-9876-5432'), ('my_password', '1234-5678-9876-5432'))\n"], "sample_141": ["def test_progress_bar_update(self):\n    \"\"\"\n    Test the ProgressBar update functionality.\n    \"\"\"\n    from io import StringIO\n    output = StringIO()\n    progress_bar = ProgressBar(output, total_count=10)\n    progress_bar.update(1)\n    self.assertIn('[.         ]', output.getvalue())\n    progress_bar.update(5)\n    self.assertIn('[......... ]', output.getvalue())\n    progress_bar.update(10)\n    self.assertIn('[...........]', output.getvalue())\n", "    def test_serialization_with_progress_bar(self):\n        \"\"\"\n        Test serialization with progress output.\n        \"\"\"\n        class TestProgressBar(ProgressBar):\n                super().__init__(output, total_count)\n                self.updates = []\n\n                super().update(count)\n                self.updates.append(count)\n\n        class TestSerializer(Serializer):\n                self.stream.write('[')\n\n                self.stream.write(']')\n\n                if not self.first:\n                    self.stream.write(', ')\n                self.stream.write('{\"model\": \"%s\"}' % obj)\n\n                pass\n\n                pass\n\n                pass\n\n                pass\n\n        class MockModel:\n            _meta = type('Meta', (object,), {\n                'concrete_model': type('ConcreteModel', (object,), {'_meta': type('Meta', (object,), {\n                    'local_fields': [],\n                    'local_many_to_many': [],\n                })}),\n            })\n                self.name = name\n\n        queryset = [MockModel('object1'), MockModel('object2'), MockModel('object3')]\n        output = StringIO()\n        serializer = TestSerializer()\n        serializer.serialize(queryset, stream=output, progress_output=output, object_count=len(queryset))\n        self.assertIn('[', output.getvalue())\n        self.assertIn(']', output.getvalue())\n        self.assertEqual(output.getvalue().count('['), 2)\n        self.assertEqual(output.getvalue().count(']'), 2)\n", "    def test_serialization_error_raised(self):\n        class DummyModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        class DummySerializer(Serializer):\n                pass\n\n                pass\n\n                raise SerializationError(\"Dummy serialization error\")\n\n                pass\n\n                pass\n\n                pass\n\n        queryset = [DummyModel(name=\"Test\")]\n        serializer = DummySerializer()\n\n        with self.assertRaises(SerializationError) as e:\n            serializer.serialize(queryset)\n        \n        self.assertEqual(str(e.exception), \"Dummy serialization error\")\n", "    def test_serializer_not_implemented_methods(self):\n        \"\"\"\n        Ensure that NotImplementedError is raised for abstract methods in Serializer.\n        \"\"\"\n        serializer = Serializer()\n        \n        with self.assertRaises(NotImplementedError):\n            serializer.start_serialization()\n        \n        with self.assertRaises(NotImplementedError):\n            serializer.start_object(None)\n        \n        with self.assertRaises(NotImplementedError):\n            serializer.handle_field(None, None)\n        \n        with self.assertRaises(NotImplementedError):\n            serializer.handle_fk_field(None, None)\n        \n        with self.assertRaises(NotImplementedError):\n            serializer.handle_m2m_field(None, None)\n", "    def test_progress_bar_update(self):\n        output = StringIO()\n        progress_bar = ProgressBar(output, total_count=100)\n        progress_bar.update(50)\n        self.assertIn('[..................................................                     ]', output.getvalue())\n        progress_bar.update(100)\n        self.assertIn('[...........................................................................]', output.getvalue())\n"], "sample_142": ["    def test_model_form_factory(self):\n        \"\"\"\n        Test the `modelform_factory` function to ensure that it creates a ModelForm\n        with the expected fields.\n        \"\"\"\n        from django.db import models\n\n        class Artist(models.Model):\n            name = models.CharField(max_length=100)\n            genre = models.CharField(max_length=100, blank=True)\n\n            class Meta:\n                app_label = 'admin_checks'\n\n        form_class = modelform_factory(Artist, fields=['name', 'genre'])\n        form = form_class()\n\n        self.assertIn('name', form.fields)\n        self.assertIn('genre', form.fields)\n        self.assertNotIn('id', form.fields)\n\n        # Ensure that the form raises an ImproperlyConfigured error when neither fields nor exclude is specified\n        with self.assertRaises(ImproperlyConfigured):\n            modelform_factory(Artist)\n", "    def test_modelform_factory_missing_fields_and_exclude(self):\n        \"\"\"\n        Test that modelform_factory raises an ImproperlyConfigured error\n        when neither 'fields' nor 'exclude' are provided.\n        \"\"\"\n        with self.assertRaises(ImproperlyConfigured):\n            modelform_factory(Song)\n", "    def test_modelform_without_fields_or_exclude(self):\n        class MyModelForm(forms.ModelForm):\n            class Meta:\n                model = Song\n\n        with self.assertRaises(ImproperlyConfigured) as cm:\n            MyModelForm()\n\n        self.assertEqual(\n            str(cm.exception),\n            \"Creating a ModelForm without either the 'fields' attribute or the 'exclude' attribute is prohibited; form MyModelForm needs updating.\"\n        )\n", "    def test_inlineformset_factory_with_max_num(self):\n        class CityInline(admin.TabularInline):\n            model = City\n\n        class StateAdmin(admin.ModelAdmin):\n            inlines = [CityInline]\n\n        FormSet = inlineformset_factory(State, City, extra=3, max_num=5)\n        formset = FormSet(instance=State())\n        self.assertEqual(formset.max_num, 5)\n", "    def test_inlineformset_factory_without_fields_or_exclude(self):\n        \"\"\"\n        Test that inlineformset_factory raises ImproperlyConfigured if neither\n        'fields' nor 'exclude' is specified.\n        \"\"\"\n        with self.assertRaises(ImproperlyConfigured):\n            inlineformset_factory(Album, Song)\n"], "sample_143": ["def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('CamelCase'), 'camel case')\n        self.assertEqual(text.camel_case_to_spaces('camelCaseWord'), 'camel case word')\n        self.assertEqual(text.camel_case_to_spaces('CamelCaseWord'), 'camel case word')\n        self.assertEqual(text.camel_case_to_spaces('camelcaseword'), 'camelcaseword')\n        self.assertEqual(text.camel_case_to_spaces('Camel'), 'camel')\n        self.assertEqual(text.camel_case_to_spaces(''), '')\n        self.assertEqual(text.camel_case_to_spaces(lazystr('CamelCase')), 'camel case')\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst('hello'), 'Hello')\n        self.assertEqual(text.capfirst('Hello'), 'Hello')\n        self.assertEqual(text.capfirst(''), '')\n        self.assertEqual(text.capfirst(lazystr('lazy')), 'Lazy')\n        self.assertEqual(text.capfirst('123abc'), '123abc')\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst(\"hello\"), \"Hello\")\n        self.assertEqual(text.capfirst(\"Hello\"), \"Hello\")\n        self.assertEqual(text.capfirst(\"\"), \"\")\n        self.assertEqual(text.capfirst(\"123abc\"), \"123abc\")\n        self.assertEqual(text.capfirst(lazystr(\"hello\")), \"Hello\")\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst('hello'), 'Hello')\n        self.assertEqual(text.capfirst('Hello'), 'Hello')\n        self.assertEqual(text.capfirst('h'), 'H')\n        self.assertEqual(text.capfirst(''), '')\n        self.assertEqual(text.capfirst('123abc'), '123abc')\n        self.assertEqual(text.capfirst(lazystr('hello')), 'Hello')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('CamelCase'), 'camel case')\n    self.assertEqual(text.camel_case_to_spaces('camelCase'), 'camel case')\n    self.assertEqual(text.camel_case_to_spaces('CamelCamelCase'), 'camel camel case')\n    self.assertEqual(text.camel_case_to_spaces('Camel2Camel2Case'), 'camel2 camel2 case')\n    self.assertEqual(text.camel_case_to_spaces('getHTTPResponseCode'), 'get http response code')\n    self.assertEqual(text.camel_case_to_spaces('get200HTTPResponseCodeOK'), 'get200 http response code ok')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('CamelCase')), 'camel case')\n"], "sample_144": ["    def test_custom_manager_creation(self):\n        # Ensure that a custom manager is created automatically if not specified.\n        class CustomModel(Model):\n            class Meta:\n                app_label = 'test'\n\n        self.assertTrue(hasattr(CustomModel, 'objects'))\n        self.assertIsInstance(CustomModel.objects, Manager)\n\n        # Ensure that a model with a field named 'objects' raises a ValueError when no custom manager is provided.\n        with self.assertRaises(ValueError):\n            class ModelWithObjectsField(Model):\n                objects = IntegerField()\n                class Meta:\n                    app_label = 'test'\n        \n        # Ensure that a model with a custom manager named 'objects' doesn't raise an error.\n        class CustomManager(Manager):\n            pass\n\n        class ModelWithCustomManager(Model):\n            objects = CustomManager()\n            class Meta:\n                app_label = 'test'\n\n        self.assertIsInstance(ModelWithCustomManager.objects, CustomManager)\n", "def test_save_base_with_unsaved_related_object(self):\n        \"\"\"\n        Regression test for saving a model instance with an unsaved related object.\n        \"\"\"\n        # Create a Place instance\n        place = Place(name=\"Uncle Bob's Diner\", address='123 Main St')\n        \n        # Attempt to create a Restaurant instance with the unsaved Place instance\n        restaurant = Restaurant(\n            place_ptr=place,\n            serves_hot_dogs=True,\n            serves_pizza=True,\n        )\n        \n        with self.assertRaises(ValueError):\n            restaurant.save()\n        \n        # Save the place first\n        place.save()\n        \n        # Now saving the restaurant should succeed\n        restaurant.save()\n        self.assertEqual(restaurant.place_ptr_id, place.id)\n        self.assertEqual(Restaurant.objects.count(), 1)\n", "def test_multiple_inheritance_with_m2m(self):\n        \"\"\"\n        Test that multiple inheritance with many-to-many fields works correctly.\n        \"\"\"\n        class Base1(models.Model):\n            tags = models.ManyToManyField(Tag, related_name='base1_set')\n\n        class Base2(models.Model):\n            tags = models.ManyToManyField(Tag, related_name='base2_set')\n\n        class Child(Base1, Base2):\n            pass\n\n        # Create some tags\n        tag1 = Tag.objects.create(name='tag1')\n        tag2 = Tag.objects.create(name='tag2')\n\n        # Create a child instance and add tags\n        child = Child.objects.create()\n        child.tags.add(tag1, tag2)\n\n        # Ensure tags are correctly associated\n        self.assertEqual(list(child.tags.all()), [tag1, tag2])\n        self.assertEqual(list(tag1.base1_set.all()), [child])\n        self.assertEqual(list(tag2.base2_set.all()), [child])\n\n        # Remove a tag and check associations again\n        child.tags.remove(tag1)\n        self.assertEqual(list(child.tags.all()), [tag2])\n        self.assertEqual(list(tag1.base1_set.all()), [])\n        self.assertEqual(list(tag2.base2_set.all()), [child])\n", "    def test_model_hashing(self):\n        \"\"\"\n        Test for __hash__ method of the Model to ensure models with primary keys\n        are hashable and models without primary keys raise TypeError.\n        \"\"\"\n        p1 = Place.objects.create(name='Test Place', address='123 Test St')\n        r1 = Restaurant.objects.create(place_ptr=p1, serves_hot_dogs=True, serves_pizza=False)\n\n        # Ensure the model is hashable when it has a primary key.\n        try:\n            hash(p1)\n            hash(r1)\n        except TypeError:\n            self.fail(\"Hashing a model with a primary key raised TypeError.\")\n\n        # Ensure TypeError is raised when model does not have a primary key.\n        p2 = Place(name='Unhashed Place', address='456 Test St')\n        r2 = Restaurant(place_ptr=p2, serves_hot_dogs=False, serves_pizza=True)\n\n        with self.assertRaises(TypeError):\n            hash(p2)\n\n        with self.assertRaises(TypeError):\n            hash(r2)\n", "    def test_model_init_with_deferred_field(self):\n        \"\"\"\n        Test the initialization of a model with deferred fields.\n        \"\"\"\n        place = Place(name=\"Guido's House of Pasta\", address='944 W. Fullerton')\n        place.save()\n\n        # Create a new instance with deferred fields\n        new_place = Place(name=DEFERRED, address='New Address')\n        new_place.save_base(raw=True)\n\n        # Ensure the deferred field is handled correctly and not set\n        self.assertEqual(new_place.name, DEFERRED)\n        self.assertEqual(new_place.address, 'New Address')\n"], "sample_145": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_readonly_fields_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n"], "sample_146": ["    def test_consistent_language_settings(self):\n        for tag in ['en', 'en-US', 'zh-Hans']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        for tag in ['en', 'fr']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        \"\"\"Test LANGUAGE_CODE that is consistent with LANGUAGES setting.\"\"\"\n        for tag in ['en', 'fr']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        \"\"\"Test for LANGUAGE_CODE present in the LANGUAGES setting.\"\"\"\n        for tag in ['en', 'fr']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_147": ["def test_prefetch_related_objects(self):\n    ReservedName.objects.create(name='a', order=1)\n    ReservedName.objects.create(name='b', order=2)\n    qs1 = ReservedName.objects.prefetch_related('name')\n    with self.assertNumQueries(2):\n        names = list(qs1)\n        self.assertEqual(names[0].name, 'a')\n        self.assertEqual(names[1].name, 'b')\n    with self.assertNumQueries(0):\n        self.assertEqual(names[0].name, 'a')\n        self.assertEqual(names[1].name, 'b')\n", "    def test_bulk_create_with_empty_objs(self):\n        initial_count = Number.objects.count()\n        Number.objects.bulk_create([])\n        self.assertEqual(Number.objects.count(), initial_count)\n", "def test_union_with_annotations(self):\n    qs1 = Number.objects.annotate(annotated_num=F('num') + 1).filter(num__lte=3)\n    qs2 = Number.objects.annotate(annotated_num=F('num') - 1).filter(num__gte=7)\n    result = qs1.union(qs2).values_list('annotated_num', flat=True)\n    self.assertCountEqual(result, [1, 2, 3, 4, 6, 7, 8])\n", "    def test_union_with_prefetch_related(self):\n        rn1 = ReservedName.objects.create(name='rn1', order=1)\n        rn2 = ReservedName.objects.create(name='rn2', order=2)\n        qs1 = ReservedName.objects.filter(name='rn1').prefetch_related('number_set')\n        qs2 = ReservedName.objects.filter(name='rn2').prefetch_related('number_set')\n        union_qs = qs1.union(qs2)\n        for obj in union_qs:\n            self.assertTrue(hasattr(obj, '_prefetched_objects_cache'))\n", "    def test_union_with_select_related(self):\n        qs1 = Number.objects.filter(num__lte=1).select_related('related_field')\n        qs2 = Number.objects.filter(num__gte=8).select_related('related_field')\n        union_qs = qs1.union(qs2)\n        for obj in union_qs:\n            self.assertTrue(hasattr(obj, 'related_field'))\n"], "sample_148": ["    def test_quote_unquote(self):\n        \"\"\"\n        Test the quote() and unquote() utility functions.\n        \"\"\"\n        test_cases = {\n            'simple_text': 'simple_text',\n            'text_with_special_chars': 'text_with_special_chars',\n            'text_with_slash/': 'text_with_slash%2F',\n            'text_with_underscore_': 'text_with_underscore%5F',\n            'text_with_colon:': 'text_with_colon%3A',\n        }\n\n        for original, expected in test_cases.items():\n            quoted = quote(original)\n            self.assertEqual(quoted, expected)\n            self.assertEqual(unquote(quoted), original)\n", "    def test_prepare_lookup_value(self):\n        \"\"\"\n        Test prepare_lookup_value function.\n        \"\"\"\n        self.assertEqual(prepare_lookup_value('key__in', 'a,b,c'), ['a', 'b', 'c'])\n        self.assertEqual(prepare_lookup_value('key__isnull', ''), False)\n        self.assertEqual(prepare_lookup_value('key__isnull', 'false'), False)\n        self.assertEqual(prepare_lookup_value('key__isnull', '0'), False)\n        self.assertEqual(prepare_lookup_value('key__isnull', 'true'), True)\n        self.assertEqual(prepare_lookup_value('key__isnull', '1'), True)\n        self.assertEqual(prepare_lookup_value('key', 'value'), 'value')\n", "    def test_quote_unquote(self):\n        \"\"\"\n        Test the quote and unquote functions.\n        \"\"\"\n        test_strings = [\n            \"simple\",\n            \"with spaces\",\n            \"special/characters:_\",\n            \"chars&=?$\",\n            \"escape\\nnewline\",\n        ]\n        for s in test_strings:\n            quoted = quote(s)\n            unquoted = unquote(quoted)\n            self.assertEqual(s, unquoted)\n", "    def test_quote_unquote(self):\n        \"\"\"\n        Test the `quote` and `unquote` functions for encoding and decoding strings.\n        \"\"\"\n        original = 'test:/_string#'\n        quoted = quote(original)\n        self.assertEqual(quoted, 'test%3A%2F%5Fstring%23')\n        unquoted = unquote(quoted)\n        self.assertEqual(unquoted, original)\n        \n        original_with_special_chars = 'example:/path/to/resource?query=param&another=param#fragment'\n        quoted_special_chars = quote(original_with_special_chars)\n        self.assertEqual(quoted_special_chars, 'example%3A%2Fpath%2Fto%2Fresource%3Fquery%3Dparam%26another%3Dparam%23fragment')\n        unquoted_special_chars = unquote(quoted_special_chars)\n        self.assertEqual(unquoted_special_chars, original_with_special_chars)\n", "    def test_quote_unquote(self):\n        \"\"\"\n        Test that the quote and unquote functions work correctly.\n        \"\"\"\n        test_cases = [\n            (\"simple\", \"simple\"),\n            (\"complex/string_with:weird_characters\", \"complex_string_with_weird_characters\"),\n            (\"value_with%percent\", \"value_with_25percent\"),\n            (\"newline\\n\", \"newline_0A\"),\n            ('\"quotes\"', '_22quotes_22'),\n        ]\n        for original, quoted in test_cases:\n            self.assertEqual(quote(original), quoted)\n            self.assertEqual(unquote(quoted), original)\n"], "sample_151": ["    def test_alter_field_type_with_same_name(self):\n        \"\"\"Tests autodetection of changes in field type with the same name.\"\"\"\n        before = [ModelState(\"testapp\", \"Book\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"release_date\", models.DateField()),\n        ])]\n        after = [ModelState(\"testapp\", \"Book\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"release_date\", models.DateTimeField()),\n        ])]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"release_date\", model_name=\"book\")\n", "    def test_order_with_respect_to_dependency(self):\n        \"\"\"\n        Test that AlterOrderWithRespectTo depends on the creation of the field it references.\n        \"\"\"\n        # Initial state without 'order_with_respect_to' set\n        before = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n            ]),\n            ModelState(\"otherapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n            ]),\n        ]\n        # State after setting 'order_with_respect_to'\n        after = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n            ], options={\"order_with_respect_to\": \"book\"}),\n            ModelState(\"otherapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterOrderWithRespectTo\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", order_with_respect_to=\"book\")\n        self.assertMigrationDependencies(changes, 'testapp', 0, [(\"testapp\", \"__first__\")])\n", "    def test_rename_model_with_conflicting_name(self):\n        \"\"\"Tests renaming a model to a name that already exists in another app.\"\"\"\n        before_states = [self.author_name, self.other_pony]\n        after_states = [\n            ModelState(\"testapp\", \"Pony\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ]),\n            self.other_pony,\n        ]\n        changes = self.get_changes(before_states, after_states, MigrationQuestioner({\"ask_rename_model\": True}))\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Pony\")\n", "    def test_alter_field_with_different_validator(self):\n        \"\"\"\n        Test altering a field with a different validator.\n        \"\"\"\n        before = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, validators=[RegexValidator(r'^[a-z]+$', 'Enter a valid name.', 'invalid')])),\n        ])\n        after = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, validators=[RegexValidator(r'^[A-Z]+$', 'Enter a valid name.', 'invalid')])),\n        ])\n        changes = self.get_changes([before], [after])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n        self.assertNotEqual(before.fields[1][1].validators, after.fields[1][1].validators)\n", "    def test_remove_foreignkey_dependency(self):\n        \"\"\"\n        Removing a ForeignKey dependency removes the dependency correctly.\n        \"\"\"\n        # Initial state with ForeignKey dependency\n        before_states = [\n            ModelState(\"app1\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n            ]),\n            ModelState(\"app2\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"app1.Author\", models.CASCADE)),\n            ]),\n        ]\n        # Final state without ForeignKey dependency\n        after_states = [\n            ModelState(\"app1\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n            ]),\n            ModelState(\"app2\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n            ]),\n        ]\n        changes = self.get_changes(before_states, after_states)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'app2', 1)\n        self.assertOperationTypes(changes, 'app2', 0, [\"RemoveField\"])\n        self.assertOperationAttributes(changes, 'app2', 0, 0, name=\"author\", model_name=\"book\")\n"], "sample_149": ["    def test_custom_permission_with_builtin_permission(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Can add checked (clashes with builtin permission)'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n", "    def test_verbose_name_max_length_with_diff_permission_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'short name'\n                permissions = [\n                    ('perm_code', 'Some very long permission name that exceeds the allowed limit' * 5)\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission named 'Some very long permission name that exceeds the allowed limitSome very long permission name that exceeds the allowed limitSome very long permission name that exceeds the allowed limitSome very long permission name that exceeds the allowed limitSome very long permission name that exceeds the allowed limit' of model 'auth_tests.Checked' is longer than 255 characters.\",\n                obj=Checked,\n                id='auth.E008',\n            ),\n        ])\n", "    def test_custom_permission_codename_clashing_with_builtin(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Clashing with builtin add permission'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin permission \"\n                \"for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n", "    def test_custom_permission_codename_clashing_with_builtin(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Custom add permission clashing with builtin'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin permission \"\n                \"for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n", "    def test_custom_permission_name_clashes_with_builtin(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Some permission clashing with builtin add_checked'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin permission \"\n                \"for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n"], "sample_152": ["    def test_set_callable(self):\n            return 'dynamic'\n\n        set_callable = SET(dynamic_value)\n        a = create_a('set_callable')\n        set_callable(Collector('default'), a._meta.get_field('desc'), [a], 'default')\n        a.refresh_from_db()\n        self.assertEqual(a.desc, 'dynamic')\n", "    def test_set_callable(self):\n            return self.DEFAULT\n\n        a = create_a('set_callable')\n        set_callable_func = SET(callable_value)\n        set_callable_func(Collector(using='default'), a._meta.get_field('setvalue'), [a], 'default')\n        a = A.objects.get(pk=a.pk)\n        self.assertEqual(a.setvalue, self.DEFAULT)\n", "    def test_custom_set_on_delete(self):\n        \"\"\"\n        Test the custom SET on_delete behavior by providing a callable value.\n        \"\"\"\n        new_value = R.objects.create()\n        \n            return new_value\n        \n        set_on_delete_handler = SET(custom_value)\n        self.assertTrue(callable(set_on_delete_handler))\n        \n        a = create_a('custom_set')\n        a.setvalue.delete()\n        \n        a = A.objects.get(pk=a.pk)\n        self.assertEqual(a.setvalue.pk, new_value.pk)\n", "    def test_protected_error_message(self):\n        a = create_a('protect')\n        msg = (\n            \"Cannot delete some instances of model 'R' because they are \"\n            \"referenced through protected foreign keys: 'A.protect'.\"\n        )\n        with self.assertRaisesMessage(ProtectedError, msg) as cm:\n            a.protect.delete()\n        self.assertEqual(cm.exception.protected_objects[0], a.protect)\n", "    def test_protect_error_content(self):\n        \"\"\"\n        Test that the ProtectedError contains the correct message and list of protected objects.\n        \"\"\"\n        a = create_a('protect_error_content')\n        with self.assertRaises(ProtectedError) as cm:\n            a.protect.delete()\n        exception = cm.exception\n        self.assertEqual(\n            str(exception),\n            \"Cannot delete some instances of model 'R' because they are referenced through protected foreign keys: 'A.protect'.\"\n        )\n        self.assertIn(a.protect, exception.protected_objects)\n"], "sample_150": ["    def setUp(self):\n        self.command = BaseCommand()\n", "    def test_missing_args_message(self):\n        parser = CommandParser(missing_args_message=\"You must provide arguments.\")\n        with self.assertRaises(CommandError) as cm:\n            parser.parse_args([])\n        self.assertEqual(str(cm.exception), \"Error: You must provide arguments.\")\n", "    def test_command_error(self):\n        with self.assertRaises(CommandError) as cm:\n            raise CommandError(\"An error occurred\")\n        self.assertEqual(str(cm.exception), \"An error occurred\")\n", "    def test_parse_args_with_missing_args_message(self):\n        parser = CommandParser(missing_args_message='Missing args message')\n        with self.assertRaises(CommandError) as cm:\n            parser.parse_args([])\n        self.assertEqual(str(cm.exception), 'Error: Missing args message')\n", "compilation error"], "sample_153": ["    def test_model_save(self):\n        class MyModel(Model):\n            name = models.CharField(max_length=100)\n\n        instance = MyModel(name='Test')\n        with mock.patch.object(MyModel, 'save_base', return_value=None) as mocked_save_base:\n            instance.save()\n            mocked_save_base.assert_called_once_with(using=None, force_insert=False, force_update=False, update_fields=None)\n", "    def test_model_instantiation(self):\n        class MyModel(Model):\n            pass\n\n        instance = MyModel()\n        self.assertIsInstance(instance, MyModel)\n        self.assertIsNotNone(instance._state)\n        ", "    def test_modelbase_creation(self):\n        class Meta:\n            app_label = 'test_app'\n\n        class BaseModel(metaclass=ModelBase):\n            class Meta:\n                abstract = True\n\n        class TestModel(BaseModel):\n            name = 'test'\n\n        # Assert that the class was created and _meta is set correctly.\n        self.assertTrue(hasattr(TestModel, '_meta'))\n        self.assertEqual(TestModel._meta.app_label, 'test_app')\n        self.assertEqual(TestModel._meta.model_name, 'testmodel')\n", "    def test_model_str_and_repr(self):\n        class MyModel(Model):\n            class Meta:\n                app_label = 'myapp'\n        \n        instance = MyModel()\n        self.assertEqual(str(instance), 'MyModel object (None)')\n        self.assertEqual(repr(instance), '<MyModel: MyModel object (None)>')\n", "    def test_model_initialization(self):\n        class MyModel(Model):\n            pass\n        \n        obj = MyModel()\n        self.assertIsInstance(obj, Model)\n        self.assertIsNotNone(obj._state)\n        self.assertTrue(obj._state.adding)\n"], "sample_154": ["compilation error", "    def test_database_checks_with_issues(self, mocked_check):\n        mocked_check.return_value = ['issue1', 'issue2']\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['issue1', 'issue2', 'issue1', 'issue2'])\n", "    def test_no_databases_passed(self, mocked_check):\n        check_database_backends(databases=[])\n        self.assertFalse(mocked_check.called)\n", "    def test_database_checks_with_specific_errors(self, mocked_connections):\n        mock_conn = mock.Mock()\n        mock_conn.validation.check.return_value = ['Error 1', 'Error 2']\n        mocked_connections.__getitem__.return_value = mock_conn\n\n        issues = check_database_backends(databases=self.databases)\n        \n        self.assertEqual(issues, ['Error 1', 'Error 2'])\n        mock_conn.validation.check.assert_called()\n", "    def test_database_checks_with_issues(self, mocked_check):\n        mocked_check.return_value = ['issue1', 'issue2']\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['issue1', 'issue2', 'issue1', 'issue2'])\n"], "sample_155": ["def test_file_response_with_filename_override(self):\n        response = FileResponse(io.BytesIO(b'binary content'), filename='custom_name.txt')\n        self.assertEqual(response['Content-Length'], '14')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.txt\"')\n        self.assertEqual(list(response), [b'binary content'])\n", "def test_file_response_with_custom_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename=\"custom_name.txt\")\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_response_with_custom_filename(self):\n    custom_filename = 'custom_name.txt'\n    response = FileResponse(io.BytesIO(b'binary content'), filename=custom_filename)\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(response['Content-Disposition'], f'inline; filename=\"{custom_filename}\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_with_custom_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='custom_name.bin')\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.bin\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "    def test_file_response_with_custom_filename(self):\n        response = FileResponse(io.BytesIO(b'binary content'), filename='custom_name.bin')\n        self.assertEqual(response['Content-Length'], '14')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.bin\"')\n        self.assertEqual(list(response), [b'binary content'])\n"], "sample_156": ["def test_form_clean_method_called(self):\n    # Verify that the clean method is called and can modify cleaned_data.\n    class MyForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        \n            cleaned_data = super().clean()\n            cleaned_data['field1'] = cleaned_data.get('field1', '').upper()\n            return cleaned_data\n\n    form = MyForm({'field1': 'test', 'field2': 'data'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['field1'], 'TEST')\n    self.assertEqual(form.cleaned_data['field2'], 'data')\n", "    def test_cleaned_data_after_clean_method(self):\n        class CustomForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n\n                cleaned_data = super().clean()\n                cleaned_data['field1'] = cleaned_data['field1'].upper()\n                if cleaned_data.get('field2') == 'invalid':\n                    raise ValidationError('Invalid value for field2')\n                return cleaned_data\n\n        # Valid data\n        form = CustomForm({'field1': 'test', 'field2': 'valid'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['field1'], 'TEST')\n        self.assertEqual(form.cleaned_data['field2'], 'valid')\n\n        # Invalid data\n        form = CustomForm({'field1': 'test', 'field2': 'invalid'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['__all__'], ['Invalid value for field2'])\n\n        # Cleaned data should still contain 'field1' in uppercase even after validation error\n        self.assertEqual(form.cleaned_data['field1'], 'TEST')\n", "    def test_cleaned_data_with_callable_initial(self):\n        \"\"\"\n        Ensure that callable initial values are correctly evaluated and not used\n        as fallback if data is provided.\n        \"\"\"\n            return 'John'\n\n        class UserForm(Form):\n            first_name = CharField(max_length=30, initial=initial_first_name)\n            last_name = CharField(max_length=30)\n\n        # Ensure initial callable is used when no data is provided\n        form = UserForm()\n        self.assertEqual(form['first_name'].value(), 'John')\n\n        # Ensure initial callable is overridden by provided data\n        form = UserForm(data={'first_name': 'Jane', 'last_name': 'Doe'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['first_name'], 'Jane')\n\n        # Ensure validation errors if required field is missing, even with initial callable\n        form = UserForm(data={'last_name': 'Doe'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['first_name'], ['This field is required.'])\n", "    def test_clean_field(self):\n        # Custom clean method for field\n        class CustomCleanForm(Form):\n            name = CharField()\n            age = IntegerField()\n\n                age = self.cleaned_data.get('age')\n                if age < 0:\n                    raise ValidationError(\"Age cannot be negative\")\n                return age\n\n        form = CustomCleanForm({'name': 'John Doe', 'age': '-1'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['age'], ['Age cannot be negative'])\n\n        form = CustomCleanForm({'name': 'John Doe', 'age': '25'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['age'], 25)\n", "    def test_custom_field_order_in_init(self):\n        class CustomOrderForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n            field5 = CharField()\n\n                field_order = kwargs.pop('field_order', None)\n                super().__init__(*args, **kwargs)\n                if field_order:\n                    self.order_fields(field_order)\n\n        form = CustomOrderForm(field_order=['field3', 'field1', 'field4'])\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field4', 'field2', 'field5'])\n\n        form = CustomOrderForm(field_order=['field5', 'field2'])\n        self.assertEqual(list(form.fields), ['field5', 'field2', 'field1', 'field3', 'field4'])\n\n        form = CustomOrderForm()\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3', 'field4', 'field5'])\n"], "sample_157": ["    def test_create_test_db_with_cache_table(self, mocked_createcachetable, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=1, autoclobber=True, serialize=False)\n            mocked_createcachetable.assert_called_once()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_create_test_db_with_keepdb_true(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db') as mock_create_test_db:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n                mock_create_test_db.assert_called_once_with(verbosity=0, autoclobber=True, keepdb=True)\n            mocked_migrate.assert_called_once()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_serialize_and_deserialize_db(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n            # Check if the database was serialized\n            self.assertIsNotNone(test_connection._test_serialized_contents)\n            # Deserialize the database from the serialized string\n            creation.deserialize_db_from_string(test_connection._test_serialized_contents)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_serialize_db_to_string(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch.object(creation, '_create_test_db'), mock.patch('django.apps.apps.get_app_configs', return_value=[]):\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n            self.assertIsInstance(test_connection._test_serialized_contents, str)\n            self.assertTrue(test_connection._test_serialized_contents.startswith('['))\n", "    def test_serialize_db_to_string(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch('django.core.serializers.serialize') as mock_serialize:\n            mock_serialize.return_value = 'mocked_serialized_data'\n            serialized_data = creation.serialize_db_to_string()\n            self.assertEqual(serialized_data, 'mocked_serialized_data')\n"], "sample_158": ["    def test_invalid_on_delete_action(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(Target, on_delete='INVALID_ACTION')\n\n        field = Model._meta.get_field('foreign_key')\n        with self.assertRaises(TypeError, msg='on_delete must be callable.'):\n            field.check()\n", "    def test_swappable_field(self):\n        class CustomUser(models.Model):\n            username = models.CharField(max_length=30, unique=True)\n\n        class Model(models.Model):\n            user = models.ForeignKey(settings.AUTH_USER_MODEL, models.CASCADE)\n\n        field = Model._meta.get_field('user')\n        self.assertEqual(field.check(), [])\n", "    def test_foreign_key_to_self(self):\n        \"\"\"\n        Test a ForeignKey to the same model (self-referential).\n        \"\"\"\n        class Person(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, related_name='children', null=True, blank=True)\n\n        field = Person._meta.get_field('parent')\n        self.assertEqual(field.check(), [])\n", "    def test_lazy_related_operation_with_app_label(self):\n        \"\"\"Test lazy_related_operation with an app_label.ModelName string.\"\"\"\n        class Model(models.Model):\n            pass\n\n            pass\n\n        with mock.patch('django.apps.apps.lazy_model_operation') as mock_lazy_op:\n            lazy_related_operation(dummy_function, Model, 'invalid_models_tests.OtherModel')\n            mock_lazy_op.assert_called_once()\n            function, *model_keys = mock_lazy_op.call_args[0]\n            self.assertEqual(function.func, dummy_function)\n            self.assertEqual(model_keys, [(Model._meta.app_label, Model._meta.object_name), ('invalid_models_tests', 'OtherModel')])\n", "    def test_on_delete_cascade(self):\n        class Person(models.Model):\n            pass\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey('Person', models.CASCADE)\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [])\n"], "sample_159": ["    def test_non_list_or_tuple_permissions(self):\n        \"\"\"\n        Permissions should be a list or tuple.\n        \"\"\"\n        class Checked(models.Model):\n            class Meta:\n                permissions = 'not_a_list_or_tuple'\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The 'permissions' attribute of model 'auth_tests.Checked' must be a list or tuple.\",\n                obj=Checked,\n                id='auth.E013',\n            ),\n        ])\n", "    def test_builtin_permission_name_length_exceeds(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'A' * 256  # Exceeds the default permission name length limit\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_builtin_permission_name_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'a' * 250  # This will create a permission name longer than 255 characters\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 10 characters \"\n                \"for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_builtin_permission_name_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'a' * 251  # This would make the 'add', 'change', 'delete' permission names exceed 255 characters.\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n                \"characters for its builtin permission names to be at \"\n                \"most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_custom_permission_with_empty_codename(self):\n        \"\"\"\n        Test to ensure that custom permissions with an empty codename are flagged.\n        \"\"\"\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('', 'Empty codename permission'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed '' of model 'auth_tests.Checked' is \"\n                \"longer than 100 characters.\",\n                obj=Checked,\n                id='auth.E012',\n            ),\n        ])\n"], "sample_160": ["    def test_edge_cases(self):\n        # Testing zero with different decimal positions\n        self.assertEqual(nformat(0, '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat(0, '.', decimal_pos=0), '0')\n        self.assertEqual(nformat(0, '.', decimal_pos=5), '0.00000')\n\n        # Testing negative zero\n        self.assertEqual(nformat(-0, '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat(-0.0, '.', decimal_pos=2), '0.00')\n\n        # Testing large number with no grouping\n        large_number = int('1' + '0' * 201)\n        self.assertEqual(nformat(large_number, '.'), '1' + '0' * 201)\n        self.assertEqual(nformat(Decimal(large_number), '.'), '1' + '0' * 201)\n\n        # Testing large negative number with no grouping\n        large_negative_number = -large_number\n        self.assertEqual(nformat(large_negative_number, '.'), '-' + '1' + '0' * 201)\n        self.assertEqual(nformat(Decimal(large_negative_number), '.'), '-' + '1' + '0' * 201)\n\n        # Testing large number with grouping\n        self.assertEqual(\n            nformat(large_number, '.', thousand_sep=',', grouping=3, force_grouping=True),\n            '1,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000'\n        )\n\n        # Testing small negative number with high precision\n        self.assertEqual(nformat(-0.0000000001, '.', decimal_pos=10), '-0.0000000001')\n\n        # Testing Decimal value with exponent and high decimal positions\n        self.assertEqual(nformat(Decimal('1.23e-8'), '.', decimal_pos=10), '0.0000000123')\n", "    def test_scientific_notation(self):\n        self.assertEqual(nformat(Decimal('1e-10'), '.'), '1e-10')\n        self.assertEqual(nformat(Decimal('1e10'), '.'), '1e+10')\n        self.assertEqual(nformat(Decimal('1.23e10'), '.'), '1.23e+10')\n        self.assertEqual(nformat(Decimal('-1.23e10'), '.'), '-1.23e+10')\n        self.assertEqual(nformat(Decimal('1e-200'), '.'), '1e-200')\n", "    def test_edge_cases(self):\n        # Test zero with various decimal positions\n        self.assertEqual(nformat(0, '.', decimal_pos=3), '0.000')\n        self.assertEqual(nformat(0, '.', decimal_pos=0), '0')\n        self.assertEqual(nformat(0, '.', decimal_pos=1), '0.0')\n        \n        # Test negative zero\n        self.assertEqual(nformat(-0.0, '.', decimal_pos=2), '0.00')\n\n        # Test very large and very small float numbers\n        self.assertEqual(nformat(1e-50, '.', decimal_pos=52), '0.00000000000000000000000000000000000000000000000000')\n        self.assertEqual(nformat(1e50, '.', decimal_pos=2), '100000000000000007629769841091887003294964970946560.00')\n\n        # Test string representation of large and small float numbers\n        self.assertEqual(nformat('1e-50', '.', decimal_pos=52), '0.00000000000000000000000000000000000000000000000000')\n        self.assertEqual(nformat('1e50', '.', decimal_pos=2), '100000000000000007629769841091887003294964970946560.00')\n\n        # Test custom grouping sequence\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,78,90')\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0), thousand_sep=',', force_grouping=True), '12,34,56,78,90')\n\n        # Test custom thousand separator longer than one character\n        self.assertEqual(nformat(1000000, '.', grouping=3, thousand_sep='.'), '1.000.000')\n        self.assertEqual(nformat(1000000, '.', grouping=3, thousand_sep='coma', force_grouping=True), '1coma000coma000')\n", "    def test_negative_zero(self):\n        self.assertEqual(nformat(-0.0, '.'), '0')\n        self.assertEqual(nformat(-0.0, '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat(Decimal('-0.0'), '.'), '0')\n        self.assertEqual(nformat(Decimal('-0.0'), '.', decimal_pos=2), '0.00')\n", "    def test_edge_cases(self):\n        # Test with zero\n        self.assertEqual(nformat(0, '.'), '0')\n        self.assertEqual(nformat(0.0, '.'), '0.0')\n        self.assertEqual(nformat(Decimal('0.00'), '.'), '0.00')\n\n        # Test with negative zero\n        self.assertEqual(nformat(-0, '.'), '0')\n        self.assertEqual(nformat(-0.0, '.'), '0.0')\n        self.assertEqual(nformat(Decimal('-0.00'), '.'), '0.00')\n\n        # Test with very large number of digits\n        large_number = '1' * 201\n        self.assertEqual(nformat(Decimal(large_number), '.'), large_number)\n\n        # Test with None as input\n        with self.assertRaises(TypeError):\n            nformat(None, '.')\n\n        # Test with empty string\n        self.assertEqual(nformat('', '.'), '')\n\n        # Test with invalid grouping value\n        with self.assertRaises(TypeError):\n            nformat(1234, '.', grouping='invalid')\n\n        # Test with invalid decimal separator\n        self.assertEqual(nformat(1234.56, 'invalid'), '1234invalid56')\n\n        # Test with invalid thousand separator\n        self.assertEqual(nformat(1234567.89, '.', grouping=3, thousand_sep='invalid', force_grouping=True), '1invalid234invalid567.89')\n"], "sample_161": ["    def test_foreign_key_to_unique_constraint(self):\n        class Target(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['field1', 'field2'], name='unique_constraint')\n                ]\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey('Target', models.CASCADE, to_field='field1')\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [])\n", "    def test_swappable_model_field_check(self):\n        class SwappedModelReplacement(models.Model):\n            pass\n\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        class Model(models.Model):\n            explicit_fk = models.ForeignKey(SwappableModel, models.CASCADE)\n            implicit_fk = models.ForeignKey('invalid_models_tests.SwappableModel', models.CASCADE)\n            explicit_m2m = models.ManyToManyField(SwappableModel)\n            implicit_m2m = models.ManyToManyField('invalid_models_tests.SwappableModel')\n\n        expected_error = Error(\n            (\"Field defines a relation with the model \"\n             \"'invalid_models_tests.SwappableModel', which has been swapped out.\"),\n            hint=\"Update the relation to point at 'settings.TEST_SWAPPABLE_MODEL'.\",\n            id='fields.E301',\n        )\n\n        fields = [\n            Model._meta.get_field('explicit_fk'),\n            Model._meta.get_field('implicit_fk'),\n            Model._meta.get_field('explicit_m2m'),\n            Model._meta.get_field('implicit_m2m'),\n        ]\n        \n        for field in fields:\n            expected_error.obj = field\n            self.assertEqual(field.check(), [expected_error])\n", "    def test_foreign_key_to_unique_together(self):\n        class Target(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.IntegerField()\n\n            class Meta:\n                unique_together = (('field1', 'field2'),)\n\n        class Model(models.Model):\n            target = models.ForeignKey(Target, models.CASCADE, to_field='field1', related_name='models')\n\n        field = Model._meta.get_field('target')\n        self.assertEqual(field.check(), [\n            Error(\n                \"'Target.field1' must be unique because it is referenced by a foreign key.\",\n                hint=(\n                    'Add unique=True to this field or add a UniqueConstraint '\n                    '(without condition) in the model Meta.constraints.'\n                ),\n                obj=field,\n                id='fields.E311',\n            ),\n        ])\n", "    def test_foreign_key_to_swapped_out_model(self):\n        class Replacement(models.Model):\n            pass\n\n        class SwappedModel(models.Model):\n            class Meta:\n                swappable = 'SWAPPED_MODEL'\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(\n                SwappedModel,\n                models.CASCADE,\n                related_name='related_fk',\n            )\n\n        field = Model._meta.get_field('foreign_key')\n        expected_error = Error(\n            \"Field defines a relation with the model 'invalid_models_tests.SwappedModel', which has been swapped out.\",\n            hint=\"Update the relation to point at 'settings.SWAPPED_MODEL'.\",\n            obj=field,\n            id='fields.E301',\n        )\n        self.assertEqual(field.check(from_model=Model), [expected_error])\n", "    def test_explicit_swappable_foreign_key(self):\n        class Replacement(models.Model):\n            pass\n\n        class SwappedModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        class Model(models.Model):\n            explicit_fk = models.ForeignKey('invalid_models_tests.SwappedModel', models.CASCADE, related_name='explicit_fk')\n\n        field = Model._meta.get_field('explicit_fk')\n        expected_error = Error(\n            \"Field defines a relation with the model 'invalid_models_tests.SwappedModel', which has been swapped out.\",\n            hint=\"Update the relation to point at 'settings.TEST_SWAPPABLE_MODEL'.\",\n            obj=field,\n            id='fields.E301',\n        )\n        self.assertEqual(field.check(from_model=Model), [expected_error])\n"], "sample_162": ["    def setUp(self):\n        self.command = mock.Mock()\n        self.translatable = TranslatableFile('path/to/dir', 'file.html', 'path/to/locale')\n        self.build_file = BuildFile(self.command, 'django', self.translatable)\n", "    def test_normalize_eols(self):\n        \"\"\"\n        Test that normalize_eols correctly normalizes EOL sequences.\n        \"\"\"\n        raw_text = \"line1\\r\\nline2\\r\\nline3\\n\"\n        expected_text = \"line1\\nline2\\nline3\\n\"\n        self.assertEqual(normalize_eols(raw_text), expected_text)\n\n        raw_text = \"line1\\nline2\\nline3\\r\\n\"\n        expected_text = \"line1\\nline2\\nline3\\n\"\n        self.assertEqual(normalize_eols(raw_text), expected_text)\n\n        raw_text = \"line1\\r\\nline2\\nline3\\r\\nline4\\n\"\n        expected_text = \"line1\\nline2\\nline3\\nline4\\n\"\n        self.assertEqual(normalize_eols(raw_text), expected_text)\n", "    def test_is_templatized_property(self):\n        \"\"\"\n        Test BuildFile's is_templatized property for various domains and file types.\n        \"\"\"\n        translatable_django_py = self.translatable_file_class('dir', 'test.py', 'locale_dir')\n        translatable_django_html = self.translatable_file_class('dir', 'test.html', 'locale_dir')\n        translatable_djangojs_js = self.translatable_file_class('dir', 'test.js', 'locale_dir')\n\n        build_file_django_py = self.build_file_class(self, 'django', translatable_django_py)\n        build_file_django_html = self.build_file_class(self, 'django', translatable_django_html)\n        build_file_djangojs_js = self.build_file_class(self, 'djangojs', translatable_djangojs_js)\n\n        # For 'django' domain, .py files should not be templatized\n        self.assertFalse(build_file_django_py.is_templatized)\n        # For 'django' domain, non-.py files should be templatized\n        self.assertTrue(build_file_django_html.is_templatized)\n        # For 'djangojs' domain, files should be templatized if gettext_version < (0, 18, 3)\n        self.assertFalse(build_file_djangojs_js.is_templatized)\n", "    def test_write_pot_file_create_new(self):\n        \"\"\"\n        Test that write_pot_file creates a new .pot file with correct contents.\n        \"\"\"\n        msgs = (\n            '# SOME DESCRIPTIVE TITLE.\\n'\n            '# (some lines truncated as they are not relevant)\\n'\n            '\"Content-Type: text/plain; charset=UTF-8\\\\n\"\\n'\n            '\"Content-Transfer-Encoding: 8bit\\\\n\"\\n'\n            '\\n'\n            '#: somefile.py:8\\n'\n            'msgid \"Some message ID\"\\n'\n            'msgstr \"\"\\n'\n        )\n        with tempfile.NamedTemporaryFile(delete=False) as temp_pot_file:\n            temp_pot_file_path = temp_pot_file.name\n        try:\n            write_pot_file(temp_pot_file_path, msgs)\n            with open(temp_pot_file_path, 'r', encoding='utf-8') as pot_file:\n                pot_contents = pot_file.read()\n                self.assertIn('Content-Type: text/plain; charset=UTF-8', pot_contents)\n                self.assertIn('msgid \"Some message ID\"', pot_contents)\n        finally:\n            os.remove(temp_pot_file_path)\n", "    def test_normalize_eols(self):\n        \"\"\"\n        Test that normalize_eols correctly normalizes EOL sequences.\n        \"\"\"\n        raw_contents = \"line1\\r\\nline2\\r\\nline3\"\n        expected_output = \"line1\\nline2\\nline3\\n\"\n        self.assertEqual(normalize_eols(raw_contents), expected_output)\n\n        raw_contents = \"line1\\nline2\\nline3\\n\"\n        expected_output = \"line1\\nline2\\nline3\\n\"\n        self.assertEqual(normalize_eols(raw_contents), expected_output)\n\n        raw_contents = \"line1\\rline2\\rline3\"\n        expected_output = \"line1\\nline2\\nline3\\n\"\n        self.assertEqual(normalize_eols(raw_contents), expected_output)\n"], "sample_163": ["    def test_redirect_authenticated_user(self):\n        \"\"\"Test redirect for authenticated users trying to access login page.\"\"\"\n        self.login()\n        response = self.client.get(\"/login/redirect_authenticated_user/\")\n        self.assertRedirects(response, settings.LOGIN_REDIRECT_URL, fetch_redirect_response=False)\n", "    def test_logout_then_login_unsafe_url(self):\n        \"\"\"Test that logout_then_login redirects to a safe URL.\"\"\"\n        self.login()\n        req = HttpRequest()\n        req.method = \"POST\"\n        csrf_token = get_token(req)\n        req.COOKIES[settings.CSRF_COOKIE_NAME] = csrf_token\n        req.POST = {\"csrfmiddlewaretoken\": csrf_token}\n        req.session = self.client.session\n        req.GET = {\"next\": \"http://evil.com\"}\n        response = logout_then_login(req)\n        self.confirm_logged_out()\n        self.assertRedirects(response, settings.LOGIN_URL, fetch_redirect_response=False)\n", "    def test_redirect_authenticated_user_with_next_param(self):\n        self.login()\n        response = self.client.get(\"/login/redirect_authenticated_user_with_next_param/?next=/custom_next/\")\n        self.assertRedirects(response, \"/custom_next/\", fetch_redirect_response=False)\n", "    def test_password_change_view_renders_correct_template(self):\n        self.login()\n        response = self.client.get(\"/password_change/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/password_change_form.html\")\n        self.assertContains(response, \"Change your password\")\n", "    def test_logout_then_login_with_get(self):\n        \"\"\"\n        Ensure logout_then_login works with GET requests and issues a\n        deprecation warning.\n        \"\"\"\n        self.login()\n        req = HttpRequest()\n        req.method = \"GET\"\n        req.session = self.client.session\n        msg = (\n            \"Log out via GET requests is deprecated and will be removed in Django 5.0. \"\n            \"Use POST requests for logging out.\"\n        )\n        with self.assertWarnsMessage(RemovedInDjango50Warning, msg):\n            response = logout_then_login(req)\n        self.assertRedirects(response, settings.LOGIN_URL, fetch_redirect_response=False)\n        self.assertNotIn(SESSION_KEY, self.client.session)\n"], "sample_164": ["    def setUp(self):\n        self.logger = logging.getLogger('django.request')\n        self.request_factory = RequestFactory()\n        self.response = self.client.get('/')\n", "    def test_configure_logging_with_settings(self):\n        \"\"\"\n        Test configure_logging with valid logging settings.\n        \"\"\"\n        logging_settings = {\n            'version': 1,\n            'handlers': {\n                'console': {\n                    'level': 'DEBUG',\n                    'class': 'logging.StreamHandler',\n                }\n            },\n            'loggers': {\n                'django': {\n                    'handlers': ['console'],\n                    'level': 'DEBUG',\n                }\n            }\n        }\n        configure_logging('logging.config.dictConfig', logging_settings)\n        logger = logging.getLogger('django')\n        self.assertEqual(logger.level, logging.DEBUG)\n", "    def setUp(self):\n        super().setUp()\n        self.request_factory = RequestFactory()\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.logger = logging.getLogger('django.request')\n", "    def setUp(self):\n        self.logger = logging.getLogger('django.request')\n        self.factory = RequestFactory()\n"], "sample_165": ["    def test_construct_instance(self):\n        from django.forms import ModelForm\n        from ..models import ChoiceModel\n\n        class TestForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ['name']\n\n        instance = ChoiceModel(name='initial')\n        form_data = {'name': 'updated'}\n        form = TestForm(data=form_data, instance=instance)\n        \n        instance = construct_instance(form, instance)\n        self.assertEqual(instance.name, 'updated')\n", "    def test_modelform_factory_creation(self):\n        from .models import ChoiceModel\n\n        class ChoiceModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ['name']\n\n        form = modelform_factory(ChoiceModel, form=ChoiceModelForm, fields=('name',))\n        self.assertIn('name', form.base_fields)\n", "    def test_modelform_factory(self):\n        # Create a simple model for testing\n        from django.db import models\n\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField()\n\n        # Generate a ModelForm for the TestModel\n        TestModelForm = modelform_factory(TestModel, fields=[\"name\", \"age\"])\n\n        # Check form fields\n        form = TestModelForm()\n        self.assertIn(\"name\", form.fields)\n        self.assertIn(\"age\", form.fields)\n", "    def test_modelform_factory_without_fields_or_exclude(self):\n        from django.db import models\n\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        with self.assertRaises(ImproperlyConfigured):\n            modelform_factory(TestModel)\n", "    def test_invalid_fields_type(self):\n        class InvalidFieldsTypeForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = 'name'\n\n        with self.assertRaises(TypeError):\n            InvalidFieldsTypeForm()\n"], "sample_166": ["    def test_get_random_string_default(self):\n        result = get_random_string()\n        self.assertEqual(len(result), 12)\n        self.assertTrue(all(c in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789' for c in result))\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(8)), 8)\n        self.assertEqual(len(get_random_string(16)), 16)\n        self.assertEqual(len(get_random_string(32)), 32)\n", "    def test_get_random_string_length(self):\n        # Test that a random string of the specified length is generated\n        for length in range(1, 100):\n            random_string = get_random_string(length=length)\n            self.assertEqual(len(random_string), length)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(8)), 8)\n        self.assertEqual(len(get_random_string(16)), 16)\n        self.assertEqual(len(get_random_string(32)), 32)\n", "    def test_get_random_string_with_custom_length(self):\n        self.assertEqual(len(get_random_string(8)), 8)\n        self.assertEqual(len(get_random_string(16)), 16)\n"], "sample_167": ["    def test_apnumber_with_non_integer(self):\n        test_list = ['a', '1.5', '3.14', [], {}, None]\n        result_list = ['a', '1.5', '3.14', [], {}, None]\n        with translation.override('en'):\n            self.humanize_ttester(test_list, result_list, 'apnumber')\n", "    def test_invalid_intword(self):\n        # Test invalid inputs for intword filter.\n        test_list = ('invalid', '123abc', '', None)\n        result_list = ('invalid', '123abc', '', None)\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'intword')\n", "    def test_apnumber_non_integer(self):\n        # Test apnumber with non-integer values\n        test_list = ['0', '10', '11', '12', 'string', 15.5, Decimal('4.5'), None]\n        result_list = ['0', '10', '11', '12', 'string', '15.5', '4.5', None]\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'apnumber')\n", "    def test_apnumber_i18n(self):\n        \"\"\"\n        Test the apnumber filter with internationalization.\n        \"\"\"\n        test_list = [str(x) for x in range(1, 11)]\n        test_list.append(None)\n        result_list_fr = ('un', 'deux', 'trois', 'quatre', 'cinq', 'six', 'sept', 'huit', 'neuf', '10', None)\n        result_list_es = ('uno', 'dos', 'tres', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve', '10', None)\n        \n        with translation.override('fr'):\n            self.humanize_tester(test_list, result_list_fr, 'apnumber')\n\n        with translation.override('es'):\n            self.humanize_tester(test_list, result_list_es, 'apnumber')\n", "    def test_apnumber_i18n(self):\n        test_list = [str(x) for x in range(1, 11)]\n        test_list.append(None)\n        result_list = ('uno', 'dos', 'tres', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve', '10', None)\n        with translation.override('es'):\n            self.humanize_tester(test_list, result_list, 'apnumber')\n"], "sample_168": ["    def test_no_content_types_to_remove(self):\n        \"\"\"No content types are deleted if there are no stale content types.\"\"\"\n        ContentType.objects.filter(app_label='contenttypes_tests', model='Fake').delete()\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', verbosity=2)\n        self.assertNotIn(\"Deleting stale content type\", stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count - 1)\n", "    def test_noinput_option(self):\n        \"\"\"Test that the --noinput option prevents user prompts.\"\"\"\n        post = Post.objects.create(title='post', content_type=self.content_type)\n        ModelWithNullFKToSite.objects.create(post=post)\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', noinput=True, verbosity=2)\n        self.assertIn(\"Deleting stale content type\", stdout.getvalue())\n        self.assertEqual(Post.objects.count(), 0)\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n", "    def test_no_content_types_to_remove(self):\n        \"\"\"No content types to remove should result in no deletions.\"\"\"\n        ContentType.objects.filter(app_label='contenttypes_tests', model='Fake').delete()\n        with mock.patch('builtins.input', return_value='yes'):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', verbosity=2)\n        self.assertNotIn('Deleting stale content type', stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count - 1)\n", "    def test_non_default_database(self):\n        \"\"\"Test that the command works with a non-default database.\"\"\"\n        with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', database='non_default_db', verbosity=2)\n        self.assertIn('Deleting stale content type', stdout.getvalue())\n        self.assertEqual(ContentType.objects.using('non_default_db').count(), self.before_count)\n", "    def test_non_default_database(self):\n        \"\"\"Test deletion of stale content types in a non-default database.\"\"\"\n        # Create a ContentType in a non-default database.\n        with self.settings(DATABASES={\n            'default': self.databases['default'],\n            'other': self.databases['default']\n        }):\n            other_db_content_type = ContentType.objects.db_manager('other').create(\n                app_label='contenttypes_tests', model='OtherFake'\n            )\n            self.assertEqual(ContentType.objects.using('other').count(), 1)\n            \n            # Remove the content type using the management command.\n            with mock.patch('builtins.input', return_value='yes'):\n                with captured_stdout() as stdout:\n                    call_command('remove_stale_contenttypes', database='other', verbosity=2, stdout=stdout)\n            \n            # Ensure the content type was deleted from the non-default database.\n            self.assertEqual(ContentType.objects.using('other').count(), 0)\n            self.assertIn(\"Deleting stale content type 'contenttypes_tests | OtherFake'\", stdout.getvalue())\n"], "sample_169": ["    def test_handle_unserializable_content_error(self):\n        class FakeField:\n            name = \"fake_field\"\n                return \"CharField\"\n                return \"unserializable \\udc80 content\"\n        \n        class FakeObject:\n                self.pk = pk\n                self._meta = self\n        \n        instance = FakeObject(pk=1)\n        serializer = serializers.get_serializer('xml')()\n        serializer.use_natural_primary_keys = False\n        serializer.start_serialization()\n        serializer.start_object(instance)\n        with self.assertRaises(ValueError) as cm:\n            serializer.handle_field(instance, FakeField())\n        self.assertIn(\"contains unserializable characters\", str(cm.exception))\n", "    def setUp(self):\n        self.xml_data = (\n            '<django-objects version=\"1.0\">'\n            '<object model=\"model_fields.nullablejsonmodel\">'\n            '<field name=\"value\" type=\"JSONField\">{\"name\": \"John\", \"age\": 30}</field>'\n            '</object></django-objects>'\n        )\n", "    def test_dtd_forbidden(self):\n        parser = DefusedExpatParser()\n        with self.assertRaises(DTDForbidden):\n            parser.start_doctype_decl(\"test\", None, None, False)\n", "    def test_xml_serialization_with_fk(self):\n        related_instance = RelatedModel.objects.create(name='related')\n        instance = JSONModel(value={'related': related_instance.pk})\n        data = serializers.serialize('xml', [instance], fields=['value'])\n        expected_xml = (\n            '<django-objects version=\"1.0\">'\n            '<object model=\"model_fields.jsonmodel\">'\n            '<field name=\"value\" type=\"JSONField\">'\n            '{\"related\": \"%s\"}</field></object></django-objects>'\n        ) % related_instance.pk\n        self.assertXMLEqual(data, expected_xml)\n        new_instance = list(serializers.deserialize('xml', data))[0].object\n        self.assertEqual(new_instance.value, instance.value)\n", "    def test_xml_serialization_with_fk(self):\n        class RelatedModel(models.Model):\n            name = models.CharField(max_length=50)\n\n                return (self.name,)\n\n        class MainModel(models.Model):\n            related = models.ForeignKey(RelatedModel, on_delete=models.CASCADE)\n            value = models.JSONField()\n\n                return (self.related.name, self.value)\n\n            natural_key.dependencies = ['testapp.relatedmodel']\n\n        # Create instances\n        related = RelatedModel.objects.create(name='related_obj')\n        main = MainModel.objects.create(related=related, value={\"key\": \"value\"})\n\n        # Serialize to XML\n        data = serializers.serialize('xml', [main])\n\n        # Deserialize from XML\n        deserialized_objs = list(serializers.deserialize('xml', data))\n\n        # Check if the deserialized object is correct\n        self.assertEqual(len(deserialized_objs), 1)\n        deserialized_main = deserialized_objs[0].object\n        self.assertEqual(deserialized_main.related.name, related.name)\n        self.assertEqual(deserialized_main.value, main.value)\n"], "sample_171": ["    def test_migrate_skip_checks(self):\n        \"\"\"\n        Tests the migrate command with --skip-checks option.\n        \"\"\"\n        # No tables are created\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n        # Run the migrations to 0001 only with --skip-checks\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', '0001', verbosity=1, stdout=stdout, no_color=True, skip_checks=True)\n        stdout = stdout.getvalue()\n        self.assertIn('Target specific migration: 0001_initial, from migrations', stdout)\n        self.assertIn('Applying migrations.0001_initial... OK', stdout)\n        # The correct tables exist\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n        # Unmigrate everything\n        call_command('migrate', 'migrations', 'zero', verbosity=0)\n        # Tables are gone\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n", "    def test_migrate_with_fake(self):\n        \"\"\"\n        Tests the --fake option which marks migrations as run without actually running them.\n        \"\"\"\n        # Ensure the tables do not exist initially\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n\n        # Run the migrations to 0001 with --fake\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', '0001', verbosity=1, stdout=stdout, no_color=True, fake=True)\n        stdout = stdout.getvalue()\n        self.assertIn('Applying migrations.0001_initial... FAKED', stdout)\n\n        # Tables should still not exist as migrations were faked\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n\n        # Run the migrations to zero with --fake\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', 'zero', verbosity=1, stdout=stdout, no_color=True, fake=True)\n        stdout = stdout.getvalue()\n        self.assertIn('Unapplying migrations.0001_initial... FAKED', stdout)\n\n        # Tables should still not exist\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n", "    def test_migrate_plan_with_no_input(self):\n        \"\"\"\n        Tests the --plan output of the migrate command with --noinput flag.\n        \"\"\"\n        out = io.StringIO()\n        call_command('migrate', 'migrations', '0001', plan=True, noinput=True, stdout=out, no_color=True)\n        self.assertIn('Planned operations:', out.getvalue())\n        self.assertIn('migrations.0001_initial', out.getvalue())\n        self.assertIn('Create model Salamander', out.getvalue())\n        self.assertIn('Raw Python operation -> Grow salamander tail.', out.getvalue())\n", "    def test_migrate_fake(self):\n        \"\"\"\n        Tests that --fake option marks migrations as run without actually running them.\n        \"\"\"\n        # Ensure no tables are initially created\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n        \n        # Run the migrations with --fake\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', '0001', verbosity=1, stdout=stdout, no_color=True, fake=True)\n        stdout = stdout.getvalue()\n        \n        # Check the expected output\n        self.assertIn('Applying migrations.0001_initial... FAKED', stdout)\n        \n        # Ensure no tables have been created since migration was faked\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n        \n        # Unmigrate with --fake option\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', 'zero', verbosity=1, stdout=stdout, no_color=True, fake=True)\n        stdout = stdout.getvalue()\n        \n        # Check the expected output\n        self.assertIn('Unapplying migrations.0001_initial... FAKED', stdout)\n        \n        # Ensure tables still do not exist\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n", "    def test_migrate_with_fake(self):\n        \"\"\"\n        Tests the --fake option of the migrate command.\n        \"\"\"\n        # Ensure no tables exist\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n        \n        stdout = io.StringIO()\n        # Run the migrations with --fake\n        call_command('migrate', 'migrations', '0001', verbosity=1, stdout=stdout, no_color=True, fake=True)\n        stdout = stdout.getvalue()\n        \n        # Check output for faked migration\n        self.assertIn('Applying migrations.0001_initial... FAKED', stdout)\n        \n        # Ensure tables do not actually exist\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n        \n        # Run migrations all the way with --fake\n        call_command(\"migrate\", verbosity=0, fake=True)\n        \n        # Ensure tables still do not exist\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n        \n        # Unmigrate everything with --fake\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', 'zero', verbosity=1, stdout=stdout, no_color=True, fake=True)\n        stdout = stdout.getvalue()\n        \n        # Check output for faked unapply\n        self.assertIn('Unapplying migrations.0002_second... FAKED', stdout)\n        \n        # Ensure tables still do not exist\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n"], "sample_170": ["    def test_cleanse_special_types_multivaluedict(self):\n        filter = SafeExceptionReporterFilter()\n        request = mock.MagicMock()\n        sensitive_data = MultiValueDict({'password': ['secret1', 'secret2']})\n        request.sensitive_post_parameters = ['password']\n        request.POST = sensitive_data\n        cleansed = filter.cleanse_special_types(request, sensitive_data)\n        self.assertEqual(cleansed, {'password': filter.cleansed_substitute})\n", "    def test_technical_500_response_html(self):\n        try:\n            raise ValueError(\"Test ValueError\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        \n        request = self.rf.get('/test_500_response/', HTTP_ACCEPT='text/html')\n        response = technical_500_response(request, exc_type, exc_value, tb)\n\n        self.assertEqual(response.status_code, 500)\n        self.assertEqual(response['Content-Type'], 'text/html')\n        self.assertIn(\"<h1>ValueError at /test_500_response/</h1>\", response.content.decode())\n        self.assertIn(\"Test ValueError\", response.content.decode())\n", "    def setUp(self):\n        self.reporter_filter = SafeExceptionReporterFilter()\n", "    def test_cleanse_setting_callable(self):\n        \"\"\"\n        Test that cleanse_setting properly wraps callable values.\n        \"\"\"\n            return \"Sensitive callable\"\n\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed = reporter_filter.cleanse_setting('API_KEY', callable_setting)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed), repr(callable_setting))\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n"], "sample_172": ["    def test_get_extra_default(self):\n        \"\"\"\n        Test that the default number of extra forms is 3.\n        \"\"\"\n        class MyInlineAdmin(admin.TabularInline):\n            model = Member\n\n        mia = MyInlineAdmin(ParentModel, admin.site)\n        self.assertEqual(mia.get_extra(request=None), 3)\n", "    def test_get_field_queryset_respects_ordering(self):\n        \"\"\"\n        Ensure the get_field_queryset method respects ordering defined in related ModelAdmin.\n        \"\"\"\n        class RelatedModelAdmin(admin.ModelAdmin):\n            ordering = ['-name']\n\n        class ParentModelAdmin(admin.ModelAdmin):\n                if db_field.name == 'related_model':\n                    return super().get_field_queryset(db, db_field, request).order_by('name')\n\n        related_model_admin = RelatedModelAdmin(RelatedModel, admin.site)\n        admin.site.register(RelatedModel, RelatedModelAdmin)\n        parent_model_admin = ParentModelAdmin(ParentModel, admin.site)\n        admin.site.register(ParentModel, ParentModelAdmin)\n\n        # Create some related model instances\n        RelatedModel.objects.create(name='Alpha')\n        RelatedModel.objects.create(name='Beta')\n        RelatedModel.objects.create(name='Gamma')\n\n        # Check if the queryset respects the ordering defined in RelatedModelAdmin\n        queryset = parent_model_admin.get_field_queryset(None, ParentModel._meta.get_field('related_model'), None)\n        self.assertEqual(list(queryset.values_list('name', flat=True)), ['Alpha', 'Beta', 'Gamma'])\n\n        # Cleanup\n        admin.site.unregister(RelatedModel)\n        admin.site.unregister(ParentModel)\n", "    def setUp(self):\n        super().setUp()\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_get_autocomplete_fields(self):\n        \"\"\"\n        Test that get_autocomplete_fields returns the correct fields.\n        \"\"\"\n        class TestModelAdmin(admin.ModelAdmin):\n            autocomplete_fields = ('test_field',)\n\n        model_admin = TestModelAdmin(Band, admin.site)\n        self.assertEqual(model_admin.get_autocomplete_fields(None), ('test_field',))\n"], "sample_173": ["    def test_random_function_sql(self):\n        self.assertEqual(self.ops.random_function_sql(), 'RANDOM()')\n", "    def test_force_no_ordering(self):\n        self.assertEqual(self.ops.force_no_ordering(), [])\n", "    def test_bulk_batch_size(self):\n        fields = ['field1', 'field2']\n        objs = ['obj1', 'obj2', 'obj3']\n        self.assertEqual(self.ops.bulk_batch_size(fields, objs), len(objs))\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n"], "sample_174": ["    def test_autoinc_sql(self):\n        # Test if autoinc_sql method returns None for auto-incrementing primary keys.\n        self.assertIsNone(self.ops.autoinc_sql('table', 'column'))\n", "    def test_adapt_decimalfield_value(self):\n        value = decimal.Decimal('3.14')\n        max_digits = 5\n        decimal_places = 2\n        self.assertEqual(self.ops.adapt_decimalfield_value(value, max_digits, decimal_places), '3.14')\n", "    def test_field_cast_sql(self):\n        self.assertEqual(self.ops.field_cast_sql('VARCHAR', 'CharField'), '%s')\n", "    def test_explain_query_prefix(self):\n        msg = 'This backend does not support explaining query execution.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            self.ops.explain_query_prefix()\n", "    def test_window_frame_start(self):\n        self.assertEqual(self.ops.window_frame_start(None), self.ops.UNBOUNDED_PRECEDING)\n        self.assertEqual(self.ops.window_frame_start(0), self.ops.CURRENT_ROW)\n        self.assertEqual(self.ops.window_frame_start(-1), '1 PRECEDING')\n        with self.assertRaisesMessage(ValueError, \"start argument must be a negative integer, zero, or None, but got '1'.\"):\n            self.ops.window_frame_start(1)\n"], "sample_175": ["    def test_protected_error_initialization(self):\n        protected_objects = [\"obj1\", \"obj2\"]\n        msg = \"This is a protected error message.\"\n        error = ProtectedError(msg, protected_objects)\n        self.assertEqual(error.protected_objects, protected_objects)\n        self.assertEqual(str(error), msg)\n", "def test_protected_error_initialization(self):\n    protected_objects = [A.objects.create(name='protected1'), A.objects.create(name='protected2')]\n    error_message = \"Cannot delete because they are referenced through a protected foreign key.\"\n    error = ProtectedError(error_message, protected_objects)\n    self.assertEqual(error.protected_objects, protected_objects)\n    self.assertEqual(str(error), error_message)\n    self.assertEqual(error.args[0], error_message)\n    self.assertEqual(error.args[1], protected_objects)\n", "    def test_set_callable(self):\n            return self.DEFAULT\n\n        a = create_a('set_callable')\n        a.set_callable = callable_value\n        a.save()\n\n        a.set_callable.delete()\n        a = A.objects.get(pk=a.pk)\n        self.assertEqual(self.DEFAULT, a.set_callable)\n", "    def test_collect_add(self):\n        \"\"\"\n        Test that the collect method in the Collector class adds objects correctly.\n        \"\"\"\n        r = R.objects.create()\n        a = create_a('collect_test')\n        collector = Collector(using='default')\n        collector.collect([a], source=R)\n        self.assertIn(a, collector.data[A])\n        self.assertIn(R, collector.dependencies)\n        self.assertIn(A, collector.dependencies[R])\n", "def test_collect_related_objects(self):\n    # Create initial objects\n    parent = R.objects.create()\n    child1 = S.objects.create(r=parent)\n    child2 = S.objects.create(r=parent)\n    grandchild1 = T.objects.create(s=child1)\n    grandchild2 = T.objects.create(s=child2)\n    \n    # Instantiate the Collector\n    collector = Collector(using='default')\n    \n    # Collect related objects\n    collector.collect([parent], collect_related=True)\n    \n    # Check that the related objects are collected\n    self.assertIn(parent, collector.data[R])\n    self.assertIn(child1, collector.data[S])\n    self.assertIn(child2, collector.data[S])\n    self.assertIn(grandchild1, collector.data[T])\n    self.assertIn(grandchild2, collector.data[T])\n\n    # Ensure no restricted objects are collected\n    self.assertFalse(collector.restricted_objects)\n"], "sample_176": ["    def test_deep_deconstruct_partial(self):\n        \"\"\"\n        Test deep_deconstruct with functools.partial to ensure nested \n        partials are properly deconstructed.\n        \"\"\"\n            return '{}/{}'.format(instance, filename)\n\n            return functools.partial(_nested_partial, key, **kwargs)\n\n        class NestedPartialModel(models.Model):\n            id = models.AutoField(primary_key=True)\n            file = models.FileField(max_length=200, upload_to=nested_partial('file'))\n\n        before = ModelState(\"testapp\", \"NestedPartialModel\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"file\", models.FileField(max_length=200, upload_to=nested_partial('file'))),\n        ])\n        after = ModelState(\"testapp\", \"NestedPartialModel\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"file\", models.FileField(max_length=200, upload_to=nested_partial('new_file'))),\n        ])\n\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n        value = changes['testapp'][0].operations[0].field.upload_to\n        self.assertEqual(\n            (_nested_partial, ('new_file',), {}),\n            (value.func, value.args, value.keywords)\n        )\n", "def test_alter_field_preserve_default(self):\n    \"\"\"\n    #24538 - Tests if altering a field with preserve_default=True preserves\n    the default value.\n    \"\"\"\n    before = [\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=\"Default Author\")),\n        ])\n    ]\n    after = [\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=400, default=\"Default Author\")),\n        ])\n    ]\n    changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=\"Default Author\")\n", "    def test_generate_renamed_models(self):\n        \"\"\"\n        Tests the generation of renamed models.\n        \"\"\"\n        before_states = [self.author_empty]\n        after_states = [self.author_renamed_with_book]\n        changes = self.get_changes(before_states, after_states, MigrationQuestioner({\"ask_rename_model\": True}))\n        \n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='Writer')\n", "    def test_remove_index_and_field(self):\n        \"\"\"\n        Removing an index and a field in the same change must remove the index\n        before the field to maintain consistency.\n        \"\"\"\n        before = [self.book_indexes]\n        after = [self.book]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, ['RemoveIndex', 'RemoveField'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name='book', name='book_title_author_idx')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, model_name='book', name='author')\n", "    def test_generate_altered_order_with_respect_to(self):\n        \"\"\"\n        Tests whether generate_altered_order_with_respect_to creates the correct operations.\n        \"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_book_order_wrt])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AlterOrderWithRespectTo\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name=\"author\", name=\"book\")\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"author\", order_with_respect_to=\"book\")\n"], "sample_177": ["    def test_related_models_tuples(self):\n        \"\"\"\n        Tests the get_related_models_tuples function to ensure it returns\n        the correct set of (app_label, model_name) tuples for related models.\n        \"\"\"\n        new_apps = Apps([\"migrations\"])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class Book(models.Model):\n            title = models.CharField(max_length=1000)\n            author = models.ForeignKey(Author, models.CASCADE)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        project_state = ProjectState.from_apps(new_apps)\n        author_state = project_state.models['migrations', 'author']\n        book_state = project_state.models['migrations', 'book']\n        author_model = author_state.render(project_state.apps)\n        book_model = book_state.render(project_state.apps)\n\n        related_models_tuples = get_related_models_tuples(author_model)\n        self.assertEqual(related_models_tuples, {('migrations', 'book')})\n\n        related_models_tuples = get_related_models_tuples(book_model)\n        self.assertEqual(related_models_tuples, {('migrations', 'author')})\n", "    def test_remove_model_updates_related_models(self):\n        \"\"\"\n        Removing a model should also unregister its related models and clear the cache.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState('migrations', 'A', []))\n        project_state.add_model(ModelState('migrations', 'B', [\n            ('a', models.ForeignKey('A', models.CASCADE)),\n        ]))\n        project_state.add_model(ModelState('migrations', 'C', [\n            ('b', models.ForeignKey('B', models.CASCADE)),\n        ]))\n        \n        # Ensure models are added correctly\n        self.assertIn(('migrations', 'a'), project_state.models)\n        self.assertIn(('migrations', 'b'), project_state.models)\n        self.assertIn(('migrations', 'c'), project_state.models)\n        \n        # Render the state to ensure apps property is set\n        project_state.apps\n        \n        # Remove model B and ensure related models are also updated\n        project_state.remove_model('migrations', 'b')\n        self.assertNotIn(('migrations', 'b'), project_state.models)\n        \n        # Ensure related models are unregistered and cache is cleared\n        with self.assertRaises(LookupError):\n            project_state.apps.get_model('migrations', 'b')\n        \n        # Ensure model C is still related to model A through the foreign key chain\n        A = project_state.apps.get_model('migrations', 'a')\n        C = project_state.apps.get_model('migrations', 'c')\n        self.assertEqual([r.related_model for r in A._meta.related_objects], [])\n        self.assertEqual([r.related_model for r in C._meta.related_objects], [])\n", "    def test_reload_model_delayed(self):\n        \"\"\"\n        Test that the reload_model method with delay=True properly delays the model reloading\n        and updates the related models once the delay is cleared.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState('migrations', 'A', [\n            ('name', models.CharField(max_length=255)),\n        ]))\n        project_state.add_model(ModelState('migrations', 'B', [\n            ('a', models.ForeignKey('A', models.CASCADE)),\n            ('description', models.TextField()),\n        ]))\n        project_state.add_model(ModelState('migrations', 'C', [\n            ('b', models.ForeignKey('B', models.CASCADE)),\n        ]))\n\n        # Render the initial state\n        project_state.apps\n\n        # Add a new field to model A and delay the reload\n        operation = AddField(\n            model_name='a',\n            name='age',\n            field=models.IntegerField(null=True),\n        )\n        operation.state_forwards('migrations', project_state)\n        project_state.reload_model('migrations', 'a', delay=True)\n\n        # Ensure the models are not updated yet\n        A = project_state.apps.get_model('migrations', 'A')\n        B = project_state.apps.get_model('migrations', 'B')\n        C = project_state.apps.get_model('migrations', 'C')\n        with self.assertRaises(FieldDoesNotExist):\n            A._meta.get_field('age')\n        self.assertIs(B._meta.get_field('a').related_model, A)\n        self.assertIs(C._meta.get_field('b').related_model, B)\n\n        # Clear the delayed apps cache and ensure the models are updated\n        project_state.clear_delayed_apps_cache()\n        A = project_state.apps.get_model('migrations', 'A')\n        self.assertIsInstance(A._meta.get_field('age'), models.IntegerField)\n        self.assertIs(B._meta.get_field('a').related_model, A)\n        self.assertIs(C._meta.get_field('b').related_model, B)\n", "    def test_reload_model_with_m2m(self):\n        \"\"\"\n        Adding a ManyToManyField to a model should reload all related models,\n        including the through model.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState('migrations', 'A', [\n            ('name', models.CharField(max_length=100)),\n        ]))\n        project_state.add_model(ModelState('migrations', 'B', [\n            ('name', models.CharField(max_length=100)),\n        ]))\n\n        project_state.apps  # Render project state\n\n        old_state = project_state.clone()\n        model_a_old = old_state.apps.get_model('migrations', 'A')\n        model_b_old = old_state.apps.get_model('migrations', 'B')\n\n        # Add a ManyToManyField\n        operation = AddField('a', 'related_b', models.ManyToManyField('B'))\n        operation.state_forwards('migrations', project_state)\n\n        project_state.reload_model('migrations', 'a', delay=True)\n\n        model_a_new = project_state.apps.get_model('migrations', 'A')\n        model_b_new = project_state.apps.get_model('migrations', 'B')\n        through_model = model_a_new._meta.get_field('related_b').remote_field.through\n\n        self.assertIsNot(model_a_old, model_a_new)\n        self.assertIsNot(model_b_old, model_b_new)\n        self.assertIsNot(model_a_new._meta.get_field('related_b').remote_field.through, None)\n        self.assertTrue(through_model._meta.auto_created)\n        self.assertEqual(\n            [f.name for f in through_model._meta.local_fields],\n            ['id', 'a', 'b']\n        )\n", "    def test_reload_model_with_recursive_relations(self):\n        \"\"\"\n        Test that reload_model correctly handles models with recursive relationships.\n        \"\"\"\n        new_apps = Apps(['migrations'])\n\n        class Node(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, null=True, blank=True, related_name='children')\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state = ProjectState.from_apps(new_apps)\n        node_state = project_state.models['migrations', 'node']\n\n        self.assertEqual(node_state.app_label, 'migrations')\n        self.assertEqual(node_state.name, 'Node')\n        self.assertEqual(list(node_state.fields), ['id', 'parent'])\n        self.assertEqual(node_state.fields['parent'].related_model, 'self')\n        self.assertIs(node_state.fields['parent'].null, True)\n\n        # Make a change and reload\n        operation = AlterField(\n            model_name='node',\n            name='parent',\n            field=models.ForeignKey('self', models.CASCADE, null=True, blank=True, related_name='off_spring'),\n        )\n        operation.state_forwards('migrations', project_state)\n        project_state.reload_model('migrations', 'node', delay=True)\n\n        Node = project_state.apps.get_model('migrations', 'node')\n        self.assertEqual(Node._meta.get_field('parent').related_name, 'off_spring')\n"], "sample_178": ["    def test_custom_management_form_fields(self):\n        \"\"\"\n        Test to ensure custom fields (MIN_NUM_FORMS and MAX_NUM_FORMS) in the ManagementForm are correctly handled.\n        \"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '1',\n            'choices-MAX_NUM_FORMS': '2',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(\n            [form.cleaned_data for form in formset.forms],\n            [{'votes': 0, 'choice': 'Zero'}, {'votes': 1, 'choice': 'One'}]\n        )\n", "    def test_management_form_creation(self):\n        \"\"\"\n        Test that the management form is created correctly with the appropriate\n        initial values and prefixes.\n        \"\"\"\n        class MyForm(Form):\n            field = CharField()\n\n        MyFormSet = formset_factory(MyForm)\n        formset = MyFormSet(data={'form-TOTAL_FORMS': '3', 'form-INITIAL_FORMS': '1'}, initial=[{'field': 'initial'}])\n        management_form = formset.management_form\n        self.assertEqual(management_form[TOTAL_FORM_COUNT].value(), '3')\n        self.assertEqual(management_form[INITIAL_FORM_COUNT].value(), '1')\n        self.assertEqual(management_form[MIN_NUM_FORM_COUNT].value(), '0')\n        self.assertEqual(management_form[MAX_NUM_FORM_COUNT].value(), '1000')\n        self.assertEqual(management_form.prefix, 'form')\n", "    def test_formset_with_custom_field_widget(self):\n        \"\"\"\n        A FormSet with forms having a custom widget should render correctly\n        and handle data appropriately.\n        \"\"\"\n        class CustomWidgetForm(Form):\n            name = CharField(widget=HiddenInput)\n\n        CustomWidgetFormSet = formset_factory(CustomWidgetForm, extra=2)\n        formset = CustomWidgetFormSet(auto_id=False, prefix='custom')\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li><input type=\"hidden\" name=\"custom-0-name\"></li>", "    def test_management_form_creation_with_bound_data(self):\n        \"\"\"ManagementForm is correctly created with bound data.\"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '1',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '2',\n            'choices-0-choice': 'Choice1',\n            'choices-0-votes': '10',\n            'choices-1-choice': 'Choice2',\n            'choices-1-votes': '20'\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_bound)\n        self.assertIsInstance(formset.management_form, ManagementForm)\n        self.assertTrue(formset.management_form.is_valid())\n        self.assertEqual(formset.management_form.cleaned_data[TOTAL_FORM_COUNT], 2)\n        self.assertEqual(formset.management_form.cleaned_data[INITIAL_FORM_COUNT], 1)\n", "    def test_management_form_initialization(self):\n        \"\"\"\n        Test that the ManagementForm initializes correctly with both bound and unbound data.\n        \"\"\"\n        # Test with bound data\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.management_form.is_bound)\n        self.assertTrue(formset.management_form.is_valid())\n\n        # Test with unbound data\n        formset = ChoiceFormSet(auto_id=False, prefix='choices')\n        self.assertFalse(formset.management_form.is_bound)\n        self.assertEqual(\n            formset.management_form.initial,\n            {\n                'TOTAL_FORMS': formset.total_form_count(),\n                'INITIAL_FORMS': formset.initial_form_count(),\n                'MIN_NUM_FORMS': formset.min_num,\n                'MAX_NUM_FORMS': formset.max_num\n            }\n        )\n"], "sample_180": ["    def test_order_with_respect_to_missing_field(self):\n        class Question(models.Model):\n            pass\n\n        class Answer(models.Model):\n            question = models.ForeignKey(Question, models.CASCADE)\n\n            class Meta:\n                order_with_respect_to = 'nonexistent_field'\n\n        self.assertEqual(Answer.check(), [\n            Error(\n                \"'order_with_respect_to' refers to the nonexistent field 'nonexistent_field'.\",\n                obj=Answer,\n                id='models.E021',\n            ),\n        ])\n", "    def test_proxy_model_with_fields(self):\n        class Base(models.Model):\n            name = models.CharField(max_length=255)\n\n        class ProxyModel(Base):\n            some_field = models.CharField(max_length=255)\n\n            class Meta:\n                proxy = True\n\n        self.assertEqual(ProxyModel.check(), [\n            Error(\n                \"Proxy model 'ProxyModel' contains model fields.\",\n                obj=ProxyModel,\n                id='models.E017',\n            ),\n        ])\n", "    def test_validate_unique(self):\n        class Model(models.Model):\n            field = models.IntegerField(unique=True)\n\n        instance = Model(field=1)\n        with self.assertRaises(ValidationError):\n            instance.validate_unique()\n", "    def test_model_name_clash_with_existing_model(self):\n        class ExistingModel(models.Model):\n            pass\n\n        # Attempt to create another model with the same name in the same app.\n        class ExistingModel(models.Model):\n            pass\n\n        self.assertEqual(ExistingModel.check(), [\n            Error(\n                \"The model name 'ExistingModel' already exists in the app 'invalid_models_tests'.\",\n                obj=ExistingModel,\n                id='models.E020',\n            )\n        ])\n", "    def test_proxy_model_with_fields(self):\n        class AbstractBase(models.Model):\n            field1 = models.CharField(max_length=100)\n\n            class Meta:\n                abstract = True\n\n        class RealModel(AbstractBase):\n            field2 = models.CharField(max_length=100)\n\n        class ProxyModel(RealModel):\n            class Meta:\n                proxy = True\n\n        self.assertEqual(ProxyModel.check(), [\n            Error(\n                \"Proxy model 'ProxyModel' contains model fields.\",\n                obj=ProxyModel,\n                id='models.E017',\n            ),\n        ])\n"], "sample_179": ["    def test_ordering_invalid_expression(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                ordering = [Lower('name')]\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"'ordering' refers to the nonexistent field, related field, or lookup 'Lower'.\",\n                obj=Model,\n                id='models.E015',\n            )\n        ])\n", "    def test_model_name_clash_with_reverse_field_accessor(self):\n        class Parent(models.Model):\n            pass\n\n        class Child(models.Model):\n            parent = models.ForeignKey(Parent, models.CASCADE)\n\n        class Grandchild(models.Model):\n            child = models.ForeignKey(Child, models.CASCADE)\n\n            class Meta:\n                ordering = ['child__parent']\n\n        self.assertEqual(Grandchild.check(), [])\n", "    def test_model_name_starting_with_underscore(self):\n        class _Model(models.Model):\n            pass\n\n        self.assertEqual(_Model.check(), [\n            Error(\n                \"The model name '_Model' cannot start or end with an underscore \"\n                \"as it collides with the query lookup syntax.\",\n                obj=_Model,\n                id='models.E023',\n            )\n        ])\n", "    def test_different_m2m_through_models(self):\n        class Category(models.Model):\n            pass\n\n        class Product(models.Model):\n            categories = models.ManyToManyField(Category, through='ProductCategory')\n\n        class ProductCategory(models.Model):\n            product = models.ForeignKey(Product, models.CASCADE)\n            category = models.ForeignKey(Category, models.CASCADE)\n\n        class Discount(models.Model):\n            products = models.ManyToManyField(Product, through='DiscountedProduct')\n\n        class DiscountedProduct(models.Model):\n            discount = models.ForeignKey(Discount, models.CASCADE)\n            product = models.ForeignKey(Product, models.CASCADE)\n\n        self.assertEqual(Product.check(), [])\n        self.assertEqual(Discount.check(), [])\n", "    def test_model_meta_options(self):\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=100)\n            field2 = models.IntegerField()\n\n            class Meta:\n                ordering = ['field1']\n                get_latest_by = 'field2'\n\n        self.assertEqual(TestModel._meta.ordering, ['field1'])\n        self.assertEqual(TestModel._meta.get_latest_by, 'field2')\n"], "sample_182": ["    def test_annotate_with_aggregate(self):\n        qs1 = Number.objects.filter(num__lte=5).annotate(max_num=F('num') + 10)\n        qs2 = Number.objects.filter(num__gte=5).annotate(max_num=F('num') + 20)\n        combined_qs = qs1.union(qs2).annotate(total=F('num') + F('max_num'))\n        for obj in combined_qs:\n            if obj.num <= 5:\n                self.assertEqual(obj.total, obj.num + (obj.num + 10))\n            else:\n                self.assertEqual(obj.total, obj.num + (obj.num + 20))\n", "    def test_combined_queries_with_annotations(self):\n        qs1 = Number.objects.annotate(is_even=F('num') % 2 == 0)\n        qs2 = Number.objects.annotate(is_odd=F('num') % 2 != 0)\n        combined_qs = qs1.union(qs2).order_by('num')\n        for num, is_even, is_odd in combined_qs.values_list('num', 'is_even', 'is_odd'):\n            self.assertEqual(is_even, num % 2 == 0)\n            self.assertEqual(is_odd, num % 2 != 0)\n", "    def test_annotate_union(self):\n        qs1 = Number.objects.filter(num__lte=5).annotate(new_field=F('num') + 100)\n        qs2 = Number.objects.filter(num__gte=5).annotate(new_field=F('num') + 200)\n        results = qs1.union(qs2).order_by('num')\n        expected = [\n            {'num': 0, 'new_field': 100},\n            {'num': 1, 'new_field': 101},\n            {'num': 2, 'new_field': 102},\n            {'num': 3, 'new_field': 103},\n            {'num': 4, 'new_field': 104},\n            {'num': 5, 'new_field': 205},\n            {'num': 6, 'new_field': 206},\n            {'num': 7, 'new_field': 207},\n            {'num': 8, 'new_field': 208},\n            {'num': 9, 'new_field': 209},\n        ]\n        self.assertQuerysetEqual(results, expected, transform=lambda x: {'num': x.num, 'new_field': x.new_field})\n", "def test_union_with_raw_query(self):\n    raw_query = \"SELECT id, num FROM app_number WHERE num <= 1\"\n    qs1 = Number.objects.raw(raw_query)\n    qs2 = Number.objects.filter(num__gte=8)\n    self.assertNumbersEqual(Number.objects.filter(id__in=[obj.id for obj in qs1.union(qs2)]), [0, 1, 8, 9], ordered=False)\n", "    def test_union_with_defer(self):\n        Number.objects.create(num=11, other_num=1)\n        qs1 = Number.objects.defer('other_num')\n        qs2 = Number.objects.filter(num=11).defer('num')\n        union_qs = qs1.union(qs2)\n        self.assertEqual(len(union_qs), 11)\n        # Ensure the deferred fields are not loaded.\n        with self.assertNumQueries(1):\n            for obj in union_qs:\n                with self.assertRaisesMessage(\n                    AttributeError, \"'Number' object has no attribute 'other_num'\"\n                ):\n                    _ = obj.other_num\n"], "sample_181": ["def test_raw_query_clone(self):\n    raw_query = RawQuery(\"SELECT * FROM test_table\", using=\"default\")\n    cloned_query = raw_query.clone(using=\"new_db\")\n    self.assertEqual(cloned_query.sql, raw_query.sql)\n    self.assertEqual(cloned_query.params, raw_query.params)\n    self.assertEqual(cloned_query.using, \"new_db\")\n", "    def test_clone_query(self):\n        original_query = Author.objects.filter(name='test').query\n        cloned_query = original_query.clone()\n        self.assertEqual(str(original_query), str(cloned_query))\n        self.assertIsNot(original_query, cloned_query)\n        self.assertIsNot(original_query.where, cloned_query.where)\n", "def test_raw_query_get_columns(self):\n        raw_query = RawQuery(\"SELECT name, age FROM author\", using='default')\n        columns = raw_query.get_columns()\n        self.assertEqual(columns, ['name', 'age'])\n", "    def test_clone_query(self):\n        query = Author.objects.filter(name='test').query\n        cloned_query = query.clone()\n        self.assertEqual(str(query), str(cloned_query))\n", "    def test_rawquery_clone(self):\n        raw_query = RawQuery(\"SELECT * FROM my_table WHERE id = %s\", using='default', params=(1,))\n        cloned_query = raw_query.clone('new_db')\n        self.assertEqual(cloned_query.sql, raw_query.sql)\n        self.assertEqual(cloned_query.params, raw_query.params)\n        self.assertEqual(cloned_query.using, 'new_db')\n"], "sample_183": ["    def test_expression_combination(self):\n        expr1 = Expression(output_field=IntegerField())\n        expr2 = Value(5, output_field=IntegerField())\n        combined_add = expr1 + expr2\n        combined_sub = expr1 - expr2\n        combined_mul = expr1 * expr2\n        combined_div = expr1 / expr2\n        combined_mod = expr1 % expr2\n        combined_pow = expr1 ** expr2\n\n        self.assertIsInstance(combined_add, CombinedExpression)\n        self.assertIsInstance(combined_sub, CombinedExpression)\n        self.assertIsInstance(combined_mul, CombinedExpression)\n        self.assertIsInstance(combined_div, CombinedExpression)\n        self.assertIsInstance(combined_mod, CombinedExpression)\n        self.assertIsInstance(combined_pow, CombinedExpression)\n", "    def test_combined_arithmetic_expressions(self):\n        expr1 = CombinedExpression(F('integer'), CombinedExpression.ADD, Value(1))\n        expr2 = CombinedExpression(F('integer'), CombinedExpression.SUB, Value(1))\n        expr3 = CombinedExpression(F('integer'), CombinedExpression.MUL, Value(2))\n        expr4 = CombinedExpression(F('integer'), CombinedExpression.DIV, Value(2))\n        \n        # Mock compiler and connection to test as_sql method\n        class MockCompiler:\n                return '%s' % expr, []\n        \n        class MockConnection:\n            features = type('Features', (), {'has_native_duration_field': False})\n            ops = type('Ops', (), {\n                'combine_expression': lambda self, connector, expressions: f' {connector} '.join(expressions),\n                'check_expression_support': lambda self, expression: None\n            })\n        \n        compiler = MockCompiler()\n        connection = MockConnection()\n        \n        sql1, params1 = expr1.as_sql(compiler, connection)\n        sql2, params2 = expr2.as_sql(compiler, connection)\n        sql3, params3 = expr3.as_sql(compiler, connection)\n        sql4, params4 = expr4.as_sql(compiler, connection)\n        \n        self.assertEqual(sql1, '(integer + %s)')\n        self.assertEqual(sql2, '(integer - %s)')\n        self.assertEqual(sql3, '(integer * %s)')\n        self.assertEqual(sql4, '(integer / %s)')\n        self.assertEqual(params1, [1])\n        self.assertEqual(params2, [1])\n        self.assertEqual(params3, [2])\n        self.assertEqual(params4, [2])\n    ", "    def test_combined_expression_sql_generation(self):\n        lhs = F('field1')\n        rhs = Value(5)\n        combined_exp = CombinedExpression(lhs, CombinedExpression.ADD, rhs)\n\n        class DummyCompiler:\n                if isinstance(node, F):\n                    return '\"field1\"', []\n                elif isinstance(node, Value):\n                    return '%s', [node.value]\n                return '', []\n\n        compiler = DummyCompiler()\n        connection = connection  # Assuming connection is set in the test environment\n\n        sql, params = combined_exp.as_sql(compiler, connection)\n        self.assertEqual(sql, '(\"field1\" + %s)')\n        self.assertEqual(params, [5])\n", "    def test_combined_expression_sqlite_numeric(self):\n        lhs = Value(10, output_field=IntegerField())\n        rhs = Value(3, output_field=IntegerField())\n        expr = CombinedExpression(lhs, CombinedExpression.DIV, rhs, output_field=DecimalField())\n        compiler = MagicMock()\n        connection = MagicMock()\n        connection.vendor = 'sqlite'\n        connection.ops.combine_expression.side_effect = lambda connector, expressions: ' / '.join(expressions)\n\n        sql, params = expr.as_sql(compiler, connection)\n\n        self.assertEqual(sql, 'CAST((10 / 3) AS NUMERIC)')\n        self.assertEqual(params, [])\n", "    def test_combined_expression_sql(self):\n        lhs = F('lhs_field')\n        rhs = F('rhs_field')\n        combined_expr = CombinedExpression(lhs, CombinedExpression.ADD, rhs)\n        compiler = mock.Mock()\n        connection = mock.Mock()\n        connection.ops.combine_expression = lambda connector, expressions: f'{expressions[0]} {connector} {expressions[1]}'\n        connection.ops.quote_name_unless_alias = lambda x: x\n        connection.features.has_native_duration_field = False\n        lhs.output_field = fields.IntegerField()\n        rhs.output_field = fields.IntegerField()\n\n        sql, params = combined_expr.as_sql(compiler, connection)\n        self.assertEqual(sql, '(lhs_field + rhs_field)')\n        self.assertEqual(params, [])\n"], "sample_184": ["    def test_model_save(self):\n        class TestModel(models.Model):\n            field = models.CharField(max_length=100)\n\n        obj = TestModel(field='test_value')\n        obj.save()\n        self.assertEqual(obj.pk, 1)\n        self.assertEqual(obj.field, 'test_value')\n", "    def test_fields_cache_initialization(self):\n        class Model(models.Model):\n            pass\n\n        instance = Model()\n        self.assertEqual(instance._state.fields_cache, {})\n", "    def test_model_meta_with_abstract(self):\n        class AbstractModel(models.Model):\n            field = models.CharField(max_length=100)\n\n            class Meta:\n                abstract = True\n\n        self.assertEqual(AbstractModel.check(), [])\n", "    def test_model_without_explicit_app_label(self):\n        \"\"\"\n        Test that a model without an explicit app label in Meta and not in an application\n        in INSTALLED_APPS raises a RuntimeError.\n        \"\"\"\n        with self.assertRaises(RuntimeError):\n            class TestModel(models.Model):\n                class Meta:\n                    app_label = None\n\n                field = models.CharField(max_length=100)\n", "    def test_save_instance(self):\n        class SimpleModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        instance = SimpleModel(name=\"Test\")\n        instance.save()\n\n        self.assertEqual(instance.pk, 1)\n        self.assertEqual(SimpleModel.objects.get(pk=1).name, \"Test\")\n"], "sample_185": ["    def test_date_format(self):\n        date = datetime.date(2023, 10, 5)\n        self.assertEqual(date_format(date), '05/10/2023')\n        datetime_obj = datetime.datetime(2023, 10, 5, 15, 30, 45)\n        self.assertEqual(date_format(datetime_obj), '05/10/2023')\n", "    def setUp(self):\n        super().setUp()\n        self.decimal_num = decimal.Decimal('12345.678')\n        self.float_num = 98765.4321\n        self.int_num = 10000\n        self.date = datetime.date(2021, 4, 12)\n        self.time = datetime.time(14, 30, 45)\n        self.datetime = datetime.datetime(2021, 4, 12, 14, 30, 45)\n        self.context = Context({\n            'decimal_num': self.decimal_num,\n            'float_num': self.float_num,\n            'int_num': self.int_num,\n            'date': self.date,\n            'time': self.time,\n            'datetime': self.datetime,\n        })\n", "    def setUp(self):\n        self.n = decimal.Decimal('12345.678')\n        self.date = datetime.date(2022, 1, 15)\n        self.time = datetime.time(14, 30, 45)\n        self.datetime = datetime.datetime(2022, 1, 15, 14, 30, 45)\n", "    def test_date_format(self):\n        with translation.override('en'):\n            self.assertEqual(date_format(datetime.date(2023, 10, 1)), 'Oct. 1, 2023')\n            self.assertEqual(date_format(datetime.date(2023, 10, 1), format='j M Y'), '1 Oct 2023')\n", "    def setUp(self):\n        super().setUp()\n        reset_format_cache()\n"], "sample_186": ["    def test_check_autocomplete_fields(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = ['author']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields[0]' refers to 'author', which is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E037',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_invalid_autocomplete_fields(self):\n        \"\"\"\n        Test the case where autocomplete_fields contains a field that is not a\n        ForeignKey or ManyToManyField.\n        \"\"\"\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = ['title']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields[0]' must be a foreign key or a many-to-many field.\",\n                obj=SongAdmin,\n                id='admin.E038',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_invalid_autocomplete_fields(self):\n        \"\"\"\n        Test that autocomplete_fields raises the appropriate errors when it\n        contains invalid fields.\n        \"\"\"\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = ['nonexistent_field', 'author']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields[0]' refers to 'nonexistent_field', \"\n                \"which is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E037',\n            ),\n            checks.Error(\n                \"The value of 'autocomplete_fields[1]' refers to 'author', which is not a \"\n                \"foreign key or a many-to-many field.\",\n                obj=SongAdmin,\n                id='admin.E038',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_ordering_nonexistent_field(self):\n        class SongAdmin(admin.ModelAdmin):\n            ordering = ['nonexistent']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'ordering[0]' refers to 'nonexistent', which is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E033',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_autocomplete_fields_not_list_or_tuple(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = 'test'\n\n        self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n            checks.Error(\n                \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                obj=SongAdmin,\n                id='admin.E036',\n            )\n        ])\n"], "sample_187": ["def test_camel_case_to_spaces(self):\n    testdata = [\n        ('CamelCase', 'camel case'),\n        ('camelCaseToSpaces', 'camel case to spaces'),\n        ('CamelCaseWithNumbers123', 'camel case with numbers123'),\n        ('SimpleTest', 'simple test'),\n        ('TestWithCAPSAnd123', 'test with caps and123'),\n    ]\n    for test, expected in testdata:\n        self.assertEqual(text.camel_case_to_spaces(test), expected)\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('CamelCaseTest'), 'camel case test')\n        self.assertEqual(text.camel_case_to_spaces('camelCaseTest'), 'camel case test')\n        self.assertEqual(text.camel_case_to_spaces('Camel2Case3Test'), 'camel2 case3 test')\n        self.assertEqual(text.camel_case_to_spaces('CamelCase'), 'camel case')\n        self.assertEqual(text.camel_case_to_spaces('Camel'), 'camel')\n        self.assertEqual(text.camel_case_to_spaces('camel'), 'camel')\n        self.assertEqual(text.camel_case_to_spaces('camelCase123Test'), 'camel case123 test')\n        self.assertEqual(text.camel_case_to_spaces('CamelCase Test'), 'camel case test')\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst('hello'), 'Hello')\n        self.assertEqual(text.capfirst('Hello'), 'Hello')\n        self.assertEqual(text.capfirst('h'), 'H')\n        self.assertEqual(text.capfirst(''), '')\n        self.assertEqual(text.capfirst(lazystr('hello')), 'Hello')\n", "    def test_compress_string(self):\n        data = b'The quick brown fox jumped over the lazy dog.'\n        compressed_data = text.compress_string(data)\n        self.assertTrue(len(compressed_data) < len(data))\n        self.assertEqual(compressed_data[:2], b'\\x1f\\x8b')  # Check for gzip magic number\n\n        empty_data = b''\n        compressed_empty_data = text.compress_string(empty_data)\n        self.assertEqual(compressed_empty_data[:2], b'\\x1f\\x8b')  # Check for gzip magic number\n        self.assertTrue(len(compressed_empty_data) < len(empty_data))\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst(\"hello\"), \"Hello\")\n        self.assertEqual(text.capfirst(\"Hello\"), \"Hello\")\n        self.assertEqual(text.capfirst(\"\"), \"\")\n        self.assertEqual(text.capfirst(\"h\"), \"H\")\n        self.assertEqual(text.capfirst(lazystr(\"hello\")), \"Hello\")\n"], "sample_188": ["    def test_combined_expression_as_sql(self):\n        expr1 = CombinedExpression(F('field1'), '+', F('field2'))\n        expr2 = CombinedExpression(F('field1'), '-', F('field2'))\n        compiler = mock.Mock()\n        connection = mock.Mock()\n        connection.ops.combine_expression.return_value = '%s + %s'\n        connection.ops.check_expression_support.return_value = None\n        compiler.compile.side_effect = [('field1', []), ('field2', [])]\n\n        sql, params = expr1.as_sql(compiler, connection)\n        self.assertEqual(sql, '(field1 + field2)')\n        self.assertEqual(params, [])\n\n        connection.ops.combine_expression.return_value = '%s - %s'\n        sql, params = expr2.as_sql(compiler, connection)\n        self.assertEqual(sql, '(field1 - field2)')\n        self.assertEqual(params, [])\n", "        def __init__(self, output_field):\n            self.output_field = output_field\n", "    def setUpTestData(cls):\n        cls.num = Number.objects.create(integer=42, float=15.5)\n        cls.max = Employee.objects.create(firstname='Max', lastname='Mustermann', salary=30)\n        cls.gmbh = Company.objects.create(name='Test GmbH', num_employees=32, num_chairs=1, ceo=cls.max)\n", "    def test_combined_expression_with_decimal_field(self):\n        lhs = Value(1, output_field=IntegerField())\n        rhs = Value(2, output_field=IntegerField())\n        combined_expr = CombinedExpression(lhs, CombinedExpression.ADD, rhs, output_field=DecimalField())\n        compiler = mock.MagicMock()\n        connection = mock.MagicMock()\n        connection.ops.combine_expression = lambda connector, sub_expressions: ' + '.join(sub_expressions)\n        connection.vendor = 'sqlite'\n        sql, params = combined_expr.as_sql(compiler, connection)\n        self.assertEqual(sql, '(CAST((%s + %s) AS NUMERIC))')\n        self.assertEqual(params, [1, 2])\n", "    def test_repr(self):\n        func = Func(Value(1), function='TEST_FUNC')\n        self.assertEqual(repr(func), \"Func(Value(1), function=TEST_FUNC)\")\n"], "sample_189": ["    def test_make_key(self):\n        cache = BaseCache({'KEY_PREFIX': 'testprefix', 'VERSION': 1})\n        # Normal key generation\n        self.assertEqual(cache.make_key('mykey'), 'testprefix:1:mykey')\n        # Custom version key generation\n        self.assertEqual(cache.make_key('mykey', version=2), 'testprefix:2:mykey')\n        # No prefix key generation\n        cache_no_prefix = BaseCache({'VERSION': 1})\n        self.assertEqual(cache_no_prefix.make_key('mykey'), ':1:mykey')\n", "    def setUp(self):\n        self.params = {\n            'timeout': 300,\n            'max_entries': 300,\n            'cull_frequency': 3,\n            'KEY_PREFIX': 'prefix',\n            'VERSION': 1\n        }\n        self.cache = BaseCache(self.params)\n", "    def setUp(self):\n        self.cache = BaseCache({\n            'TIMEOUT': 300,\n            'OPTIONS': {\n                'MAX_ENTRIES': 30,\n                'CULL_FREQUENCY': 3\n            },\n            'KEY_PREFIX': 'test_prefix',\n            'VERSION': 1,\n            'KEY_FUNCTION': None\n        })\n", "    def setUp(self):\n        self.cache_params = {\n            'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            'LOCATION': 'unique-snowflake',\n        }\n        self.cache = caches['default']\n", "    def test_make_key(self):\n        params = {\n            'KEY_PREFIX': 'test_prefix',\n            'VERSION': 1,\n            'KEY_FUNCTION': None,\n        }\n        base_cache = BaseCache(params)\n        key = base_cache.make_key('my_key')\n        self.assertEqual(key, 'test_prefix:1:my_key')\n"], "sample_190": ["    def test_lookup_with_uuid_fields(self):\n        import uuid\n        from django.db import models\n        \n        class UUIDModel(models.Model):\n            uuid_field = models.UUIDField()\n\n        UUIDModel.objects.create(uuid_field=uuid.uuid4())\n        test_uuid = uuid.uuid4()\n        obj = UUIDModel.objects.create(uuid_field=test_uuid)\n\n        # Test exact lookup with UUID field\n        self.assertTrue(UUIDModel.objects.filter(uuid_field__exact=test_uuid).exists())\n\n        # Test iexact lookup with UUID field\n        self.assertTrue(UUIDModel.objects.filter(uuid_field__iexact=str(test_uuid)).exists())\n\n        # Test contains lookup with UUID field\n        test_uuid_str = str(test_uuid)\n        self.assertTrue(UUIDModel.objects.filter(uuid_field__contains=test_uuid_str[:8]).exists())\n\n        # Test icontains lookup with UUID field\n        self.assertTrue(UUIDModel.objects.filter(uuid_field__icontains=test_uuid_str[:8].lower()).exists())\n\n        # Test startswith lookup with UUID field\n        self.assertTrue(UUIDModel.objects.filter(uuid_field__startswith=test_uuid_str[:8]).exists())\n\n        # Test istartswith lookup with UUID field\n        self.assertTrue(UUIDModel.objects.filter(uuid_field__istartswith=test_uuid_str[:8].lower()).exists())\n\n        # Test endswith lookup with UUID field\n        self.assertTrue(UUIDModel.objects.filter(uuid_field__endswith=test_uuid_str[-8:]).exists())\n\n        # Test iendswith lookup with UUID field\n        self.assertTrue(UUIDModel.objects.filter(uuid_field__iendswith=test_uuid_str[-8:].lower()).exists())\n", "def test_exact_lookup_uuid(self):\n        import uuid\n        unique_id1 = uuid.uuid4()\n        unique_id2 = uuid.uuid4()\n        player1 = Player.objects.create(name='Player1', unique_id=unique_id1)\n        player2 = Player.objects.create(name='Player2', unique_id=unique_id2)\n        \n        # Test __exact lookup with UUID\n        self.assertEqual(Player.objects.filter(unique_id__exact=unique_id1).count(), 1)\n        self.assertEqual(Player.objects.filter(unique_id__exact=unique_id1).first(), player1)\n        self.assertEqual(Player.objects.filter(unique_id__exact=unique_id2).count(), 1)\n        self.assertEqual(Player.objects.filter(unique_id__exact=unique_id2).first(), player2)\n\n        # Test __iexact lookup with UUID\n        self.assertEqual(Player.objects.filter(unique_id__iexact=str(unique_id1)).count(), 1)\n        self.assertEqual(Player.objects.filter(unique_id__iexact=str(unique_id1)).first(), player1)\n        self.assertEqual(Player.objects.filter(unique_id__iexact=str(unique_id2)).count(), 1)\n        self.assertEqual(Player.objects.filter(unique_id__iexact=str(unique_id2)).first(), player2)\n\n        # Test __contains lookup with UUID\n        uuid_substr1 = str(unique_id1)[:8]\n        uuid_substr2 = str(unique_id2)[:8]\n        self.assertEqual(Player.objects.filter(unique_id__contains=uuid_substr1).count(), 1)\n        self.assertEqual(Player.objects.filter(unique_id__contains=uuid_substr1).first(), player1)\n        self.assertEqual(Player.objects.filter(unique_id__contains=uuid_substr2).count(), 1)\n        self.assertEqual(Player.objects.filter(unique_id__contains=uuid_substr2).first(), player2)\n\n        # Test __icontains lookup with UUID\n        self.assertEqual(Player.objects.filter(unique_id__icontains=uuid_substr1.upper()).count(), 1)\n        self.assertEqual(Player.objects.filter(unique_id__icontains=uuid_substr1.upper()).first(), player1)\n        self.assertEqual(Player.objects.filter(unique_id__icontains=uuid_substr2.upper()).count(), 1)\n        self.assertEqual(Player.objects.filter(unique_id__icontains=uuid_substr2.upper()).first(), player2)\n\n        # Test __startswith lookup with UUID\n        self.assertEqual(Player.objects.filter(unique_id__", "    def test_boolean_field_exact_lookup(self):\n        bool_field = models.BooleanField()\n        bool_field.set_attributes_from_name('bool_field')\n        lookup = Exact(bool_field.get_transform(bool_field), True)\n        \n        # Check process_rhs for True\n        rhs_sql, rhs_params = lookup.process_rhs(None, connection)\n        self.assertEqual(rhs_sql, '%s')\n        self.assertEqual(rhs_params, [True])\n        \n        # Check process_rhs for False\n        lookup = Exact(bool_field.get_transform(bool_field), False)\n        rhs_sql, rhs_params = lookup.process_rhs(None, connection)\n        self.assertEqual(rhs_sql, '%s')\n        self.assertEqual(rhs_params, [False])\n\n        # Check as_sql method\n        lookup = Exact(bool_field.get_transform(bool_field), True)\n        lhs_sql, lhs_params = lookup.process_lhs(None, connection)\n        rhs_sql, rhs_params = lookup.process_rhs(None, connection)\n        sql, params = lookup.as_sql(None, connection)\n        self.assertEqual(sql, '%s = %s' % (lhs_sql, rhs_sql))\n        self.assertEqual(params, lhs_params + rhs_params)\n", "    def test_uuid_lookups(self):\n        \"\"\"\n        Test UUID lookups to ensure they work correctly with various patterns.\n        \"\"\"\n        from uuid import UUID, uuid4\n\n        # Create some UUIDs for testing\n        uuid1 = uuid4()\n        uuid2 = uuid4()\n        uuid3 = uuid4()\n\n        # Create Articles with UUID slugs\n        article1 = Article.objects.create(headline=\"UUID Article 1\", pub_date=datetime(2023, 1, 1), author=self.au1, slug=uuid1)\n        article2 = Article.objects.create(headline=\"UUID Article 2\", pub_date=datetime(2023, 1, 2), author=self.au1, slug=uuid2)\n        article3 = Article.objects.create(headline=\"UUID Article 3\", pub_date=datetime(2023, 1, 3), author=self.au2, slug=uuid3)\n\n        # Test exact lookup\n        self.assertQuerysetEqual(Article.objects.filter(slug__uuidiexact=uuid1), [article1], transform=repr)\n\n        # Test contains lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(slug__uuidcontains=str(uuid1).split('-')[0]), [article1], transform=repr\n        )\n\n        # Test icontains lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(slug__uuidicontains=str(uuid2).split('-')[1].upper()), [article2], transform=repr\n        )\n\n        # Test startswith lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(slug__uuidstartswith=str(uuid3)[:8]), [article3], transform=repr\n        )\n\n        # Test istartswith lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(slug__uuidistartswith=str(uuid2).split('-')[0].upper()), [article2], transform=repr\n        )\n\n        # Test endswith lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(slug__uuidendswith=str(uuid1)[-12:]), [article1], transform=repr\n        )\n\n        # Test iendswith lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(slug__uuidiendswith=str(uuid3).split('-')[-1].upper()), [article3], transform=repr\n        )\n", "def test_uuid_lookups(self):\n    import uuid\n    from django.db.utils import ConnectionHandler\n    from django.conf import settings\n\n    # Ensure the database supports UUID fields\n    if not hasattr(ConnectionHandler(settings.DATABASES)['default'].features, 'has_native_uuid_field'):\n        self.skipTest(\"This test requires a database backend with native UUID field support.\")\n\n    # Create a UUID field object\n    uuid1 = uuid.uuid4()\n    uuid2 = uuid.uuid4()\n    article1 = Article.objects.create(headline='Article with UUID 1', pub_date=datetime(2020, 1, 1), author=self.au1, slug='uuid1', uuid_field=uuid1)\n    article2 = Article.objects.create(headline='Article with UUID 2', pub_date=datetime(2020, 1, 2), author=self.au2, slug='uuid2', uuid_field=uuid2)\n\n    # Test exact lookup\n    self.assertEqual(Article.objects.get(uuid_field__exact=uuid1), article1)\n\n    # Test IExact lookup\n    self.assertEqual(Article.objects.get(uuid_field__iexact=str(uuid1)), article1)\n\n    # Test Contains lookup\n    self.assertEqual(Article.objects.get(uuid_field__contains=str(uuid1).split('-')[0]), article1)\n\n    # Test IContains lookup\n    self.assertEqual(Article.objects.get(uuid_field__icontains=str(uuid1).split('-')[0]), article1)\n\n    # Test StartsWith lookup\n    self.assertEqual(Article.objects.get(uuid_field__startswith=str(uuid1).split('-')[0]), article1)\n\n    # Test IStartsWith lookup\n    self.assertEqual(Article.objects.get(uuid_field__istartswith=str(uuid1).split('-')[0]), article1)\n\n    # Test EndsWith lookup\n    self.assertEqual(Article.objects.get(uuid_field__endswith=str(uuid1).split('-')[-1]), article1)\n\n    # Test IEndsWith lookup\n    self.assertEqual(Article.objects.get(uuid_field__iendswith=str(uuid1).split('-')[-1]), article1)\n"], "sample_191": ["    def test_ensure_echo_on(self, mocked_tcsetattr, mocked_tcgetattr, mocked_isatty):\n        # Mock termios attributes\n        attrs = [0, 0, 0, 0]\n        mocked_tcgetattr.return_value = attrs\n\n        # Ensure ECHO is off initially\n        attrs[3] = attrs[3] & ~termios.ECHO\n\n        # Call the function to test\n        autoreload.ensure_echo_on()\n\n        # Verify that ECHO was enabled\n        attrs[3] = attrs[3] | termios.ECHO\n        mocked_tcsetattr.assert_called_with(sys.stdin, termios.TCSANOW, attrs)\n", "    def test_ensure_echo_on_when_echo_off(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        mock_tcgetattr.return_value = [0, 0, 0, 0, 0, 0, 0, 0]\n        termios.ECHO = 0x00000008\n        autoreload.ensure_echo_on()\n        self.assertTrue(mock_tcsetattr.called)\n", "    def test_no_termios_no_tty(self, mocked_isatty):\n        with mock.patch('django.utils.autoreload.termios', None):\n            autoreload.ensure_echo_on()\n            mocked_isatty.assert_called_once()\n", "    def test_ensure_echo_on_enabled(self, mocked_termios, mocked_isatty):\n        # Mock current termios attributes\n        mocked_termios.TCSANOW = 0\n        attrs = [0, 0, 0, 0b1000]  # ECHO bit is set\n        mocked_termios.tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        mocked_termios.tcsetattr.assert_not_called()\n", "    def test_raises_last_exception_with_multiple_exceptions(self):\n        class FirstException(Exception):\n            pass\n\n        class SecondException(Exception):\n            pass\n\n        # Create a chain of exceptions\n        try:\n            raise FirstException('First exception occurred')\n        except FirstException:\n            try:\n                raise SecondException('Second exception occurred')\n            except SecondException:\n                exc_info = sys.exc_info()\n\n        with mock.patch('django.utils.autoreload._exception', exc_info):\n            with self.assertRaises(SecondException) as cm:\n                autoreload.raise_last_exception()\n            self.assertEqual(cm.exception.args[0], 'Second exception occurred')\n            self.assertEqual(cm.exception.__context__.args[0], 'First exception occurred')\n"], "sample_192": ["    def test_formset_with_custom_widget(self):\n        \"\"\"\n        Formset can use a custom widget for one of its fields.\n        \"\"\"\n        class CustomWidgetForm(Form):\n            field_with_widget = IntegerField(widget=HiddenInput)\n\n        CustomWidgetFormSet = formset_factory(CustomWidgetForm, extra=2)\n        formset = CustomWidgetFormSet(auto_id=False, prefix='custom_widget')\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li><input type=\"hidden\" name=\"custom_widget-0-field_with_widget\"></li>", "    def test_custom_management_form(self):\n        \"\"\"\n        ManagementForm can be customized by subclassing and modifying fields.\n        \"\"\"\n        class CustomManagementForm(ManagementForm):\n            custom_field = CharField(widget=HiddenInput, initial=\"custom_value\")\n\n        class CustomManagementFormSet(BaseFormSet):\n            @cached_property\n                \"\"\"Return the CustomManagementForm instance for this FormSet.\"\"\"\n                if self.is_bound:\n                    form = CustomManagementForm(self.data, auto_id=self.auto_id, prefix=self.prefix)\n                    if not form.is_valid():\n                        raise ValidationError(\n                            _('ManagementForm data is missing or has been tampered with'),\n                            code='missing_management_form',\n                        )\n                else:\n                    form = CustomManagementForm(auto_id=self.auto_id, prefix=self.prefix, initial={\n                        TOTAL_FORM_COUNT: self.total_form_count(),\n                        INITIAL_FORM_COUNT: self.initial_form_count(),\n                        MIN_NUM_FORM_COUNT: self.min_num,\n                        MAX_NUM_FORM_COUNT: self.max_num\n                    })\n                return form\n\n        CustomFormSet = formset_factory(Choice, formset=CustomManagementFormSet, extra=1)\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-custom_field': 'custom_value'\n        }\n        formset = CustomFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.management_form.cleaned_data['custom_field'], 'custom_value')\n", "    def test_management_form_initialization(self):\n        \"\"\"\n        The management form is correctly initialized based on the formset data.\n        \"\"\"\n        formset = self.make_choiceformset(total_forms=2, initial_forms=1)\n        self.assertTrue(hasattr(formset, 'management_form'))\n        management_form = formset.management_form\n        self.assertEqual(management_form.cleaned_data[TOTAL_FORM_COUNT], 2)\n        self.assertEqual(management_form.cleaned_data[INITIAL_FORM_COUNT], 1)\n        self.assertEqual(management_form.cleaned_data[MIN_NUM_FORM_COUNT], 0)\n        self.assertEqual(management_form.cleaned_data[MAX_NUM_FORM_COUNT], 1000)\n", "    def test_max_num_with_initial_data_and_extra(self):\n        \"\"\"\n        If initial data and extra forms combined exceed max_num,\n        only max_num forms should be rendered.\n        \"\"\"\n        initial = [{'choice': 'Calexico', 'votes': 100}, {'choice': 'Fergie', 'votes': 900}]\n        ChoiceFormSet = formset_factory(Choice, extra=2, max_num=3)\n        formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "def test_management_form_initial_values(self):\n    \"\"\"\n    Ensure that the management form includes the correct initial values for\n    TOTAL_FORMS, INITIAL_FORMS, MIN_NUM_FORMS, and MAX_NUM_FORMS.\n    \"\"\"\n    initial = [{'choice': 'Calexico', 'votes': 100}]\n    formset = self.make_choiceformset(initial=initial)\n    management_form = formset.management_form\n    self.assertEqual(management_form.initial[TOTAL_FORM_COUNT], 2)\n    self.assertEqual(management_form.initial[INITIAL_FORM_COUNT], 1)\n    self.assertEqual(management_form.initial[MIN_NUM_FORM_COUNT], 0)\n    self.assertEqual(management_form.initial[MAX_NUM_FORM_COUNT], 1000)\n"], "sample_193": ["    def test_resolve_relation(self):\n        \"\"\"\n        Test the resolve_relation function with different types of relation arguments.\n        \"\"\"\n        new_apps = Apps([\"migrations\"])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        author = Author()\n\n        # Test recursive relation\n        self.assertEqual(resolve_relation(author, 'self'), author)\n\n        # Test bare model name without app_label\n        self.assertEqual(resolve_relation(author, 'Book'), 'migrations.Book')\n\n        # Test \"app_label.ModelName\" string\n        self.assertEqual(resolve_relation(author, 'library.Book'), 'library.Book')\n\n        # Test model class\n        self.assertEqual(resolve_relation(author, Author), Author)\n\n        # Test invalid input\n        with self.assertRaises(AttributeError):\n            resolve_relation(author, None)\n", "    def test_deconstruct_related_field(self):\n        \"\"\"\n        Test deconstruct method of RelatedField and its subclasses.\n        \"\"\"\n        new_apps = Apps(['migrations'])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class Book(models.Model):\n            title = models.CharField(max_length=1000)\n            author = models.ForeignKey(Author, models.CASCADE)\n            contributors = models.ManyToManyField(Author)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        book_author_field = Book._meta.get_field('author')\n        book_contributors_field = Book._meta.get_field('contributors')\n\n        name, path, args, kwargs = book_author_field.deconstruct()\n        self.assertEqual(name, 'author')\n        self.assertEqual(path, 'django.db.models.ForeignKey')\n        self.assertEqual(kwargs['to'], 'migrations.author')\n        self.assertEqual(kwargs['on_delete'], models.CASCADE)\n\n        name, path, args, kwargs = book_contributors_field.deconstruct()\n        self.assertEqual(name, 'contributors')\n        self.assertEqual(path, 'django.db.models.ManyToManyField')\n        self.assertEqual(kwargs['to'], 'migrations.author')\n", "    def test_related_fields_resolution(self):\n        \"\"\"\n        Test related_fields resolution in ForeignObject.\n        \"\"\"\n        new_apps = Apps(['migrations'])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class Book(models.Model):\n            author = models.ForeignKey('migrations.Author', models.CASCADE, to_field='name')\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        author = Author(name=\"Test Author\")\n        book = Book(author=author)\n\n        foreign_object = book._meta.get_field('author')\n        related_fields = foreign_object.related_fields\n\n        self.assertEqual(len(related_fields), 1)\n        self.assertEqual(related_fields[0][0].name, 'id')\n        self.assertEqual(related_fields[0][1].name, 'name')\n", "    def test_resolve_relation(self):\n        \"\"\"\n        Test resolve_relation function.\n        \"\"\"\n        class Author(models.Model):\n            class Meta:\n                app_label = \"migrations\"\n                apps = Apps()\n\n        self.assertEqual(resolve_relation(Author, 'self'), Author)\n        self.assertEqual(resolve_relation(Author, 'Book'), 'migrations.Book')\n        self.assertEqual(resolve_relation(Author, 'migrations.Book'), 'migrations.Book')\n        self.assertEqual(resolve_relation(Author, Author), Author)\n", "    def test_foreign_key_to_self(self):\n        \"\"\"\n        Tests a model with a ForeignKey relation to itself.\n        \"\"\"\n        new_apps = Apps(['migrations'])\n\n        class TreeNode(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, related_name='children', null=True, blank=True)\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state = ProjectState.from_apps(new_apps)\n        node_state = project_state.models['migrations', 'treenode']\n        \n        self.assertEqual(node_state.app_label, 'migrations')\n        self.assertEqual(node_state.name, 'TreeNode')\n        self.assertEqual(list(node_state.fields), ['id', 'parent', 'name'])\n        self.assertEqual(node_state.fields['name'].max_length, 100)\n        self.assertTrue(node_state.fields['parent'].null)\n        self.assertTrue(node_state.fields['parent'].blank)\n        self.assertEqual(node_state.fields['parent'].related_model, 'migrations.treenode')\n        self.assertEqual(node_state.fields['parent'].related_name, 'children')\n"], "sample_194": ["    def test_invalid_fields_argument(self):\n        msg = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(fields=[], name='unique_no_fields')\n", "    def test_invalid_fields_argument(self):\n        msg = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(fields=[], name='unique_empty_fields')\n", "    def test_invalid_fields_argument(self):\n        msg = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(fields=[], name='uniq_empty_fields')\n", "    def test_remove_sql(self):\n        constraint = models.UniqueConstraint(fields=['name'], name='remove_test')\n        schema_editor = mock.Mock()\n        sql = constraint.remove_sql(UniqueConstraintProduct, schema_editor)\n        schema_editor._delete_unique_sql.assert_called_once_with(\n            UniqueConstraintProduct, 'remove_test', condition=None, deferrable=None, include=(), opclasses=()\n        )\n", "def test_clone(self):\n    fields = ['foo', 'bar']\n    name = 'unique_fields'\n    constraint = models.UniqueConstraint(fields=fields, name=name)\n    cloned_constraint = constraint.clone()\n    self.assertEqual(cloned_constraint, constraint)\n    self.assertIsInstance(cloned_constraint, models.UniqueConstraint)\n"], "sample_195": ["    def test_bulk_batch_size(self):\n        # Test when there is only one field\n        self.assertEqual(self.ops.bulk_batch_size(['field1'], [1]*600), 500)\n        # Test when there are multiple fields\n        self.assertEqual(self.ops.bulk_batch_size(['field1', 'field2'], [1]*600), self.ops.connection.features.max_query_params // 2)\n        # Test when there are no fields\n        self.assertEqual(self.ops.bulk_batch_size([], [1]*600), 600)\n", "    def setUp(self):\n        self.ops = DatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = DatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = DatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = DatabaseOperations(connection=connection)\n"], "sample_196": ["    def test_autoinc_sql(self):\n        self.assertIsNone(self.ops.autoinc_sql('test_table', 'id_column'))\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def test_limit_offset_sql(self):\n        self.assertEqual(self.ops.limit_offset_sql(0, 10), 'LIMIT 10')\n        self.assertEqual(self.ops.limit_offset_sql(5, 15), 'LIMIT 10 OFFSET 5')\n        self.assertEqual(self.ops.limit_offset_sql(None, None), '')\n", "    def test_prepare_sql_script(self):\n        sql_script = \"\"\"\n        CREATE TABLE test_table (id INT PRIMARY KEY, name VARCHAR(100));\n        INSERT INTO test_table (id, name) VALUES (1, 'Alice'), (2, 'Bob');\n        \"\"\"\n        expected_output = [\n            \"CREATE TABLE test_table (id INT PRIMARY KEY, name VARCHAR(100));\",\n            \"INSERT INTO test_table (id, name) VALUES (1, 'Alice'), (2, 'Bob');\"\n        ]\n        self.assertEqual(self.ops.prepare_sql_script(sql_script), expected_output)\n"], "sample_198": ["    def setUp(self):\n        self.field_int = IntegerField()\n        self.field_float = fields.FloatField()\n", "        def get_internal_type(self):\n            return 'DecimalField'\n", "        def get_internal_type(self):\n            return 'DecimalField'\n", "    def test_combined_expression_addition(self):\n        # Testing addition in CombinedExpression\n        n = Number.objects.create(integer=10)\n        Number.objects.filter(pk=n.pk).update(integer=CombinedExpression(F('integer'), CombinedExpression.ADD, Value(5)))\n        self.assertEqual(Number.objects.get(pk=n.pk).integer, 15)\n", "    def test_combined_expression_add(self):\n        combined_expr = CombinedExpression(F('integer'), CombinedExpression.ADD, Value(5))\n        self.assertEqual(\n            str(combined_expr),\n            \"F(integer) + Value(5)\"\n        )\n        "], "sample_197": ["    def test_custom_time_strings(self):\n        \"\"\" Test with custom time strings. \"\"\"\n        custom_time_strings = {\n            'year': ngettext_lazy('%d yr', '%d yrs'),\n            'month': ngettext_lazy('%d mon', '%d mons'),\n            'week': ngettext_lazy('%d wk', '%d wks'),\n            'day': ngettext_lazy('%d d', '%d ds'),\n            'hour': ngettext_lazy('%d hr', '%d hrs'),\n            'minute': ngettext_lazy('%d min', '%d mins'),\n        }\n        t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour + self.oneminute\n        self.assertEqual(timesince(self.t, t, time_strings=custom_time_strings, depth=6), '1\\xa0yr, 1\\xa0mon, 1\\xa0wk, 1\\xa0d, 1\\xa0hr, 1\\xa0min')\n        self.assertEqual(timeuntil(t, self.t, time_strings=custom_time_strings, depth=6), '1\\xa0yr, 1\\xa0mon, 1\\xa0wk, 1\\xa0d, 1\\xa0hr, 1\\xa0min')\n", "def test_custom_time_strings(self):\n    \"\"\" Test with custom time strings. \"\"\"\n    custom_time_strings = {\n        'year': '%d y',\n        'month': '%d m',\n        'week': '%d w',\n        'day': '%d d',\n        'hour': '%d h',\n        'minute': '%d min',\n    }\n    self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1\\xa0y')\n    self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1\\xa0m')\n    self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), '1\\xa0w')\n    self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), '1\\xa0d')\n    self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), '1\\xa0h')\n    self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), '1\\xa0min')\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneyear + self.onemonth, time_strings=custom_time_strings, depth=2),\n        '1\\xa0y, 1\\xa0m'\n    )\n", "    def test_custom_time_strings(self):\n        \"\"\" Test with custom time strings. \"\"\"\n        custom_strings = {\n            'year': lambda n: f'{n}y',\n            'month': lambda n: f'{n}m',\n            'week': lambda n: f'{n}w',\n            'day': lambda n: f'{n}d',\n            'hour': lambda n: f'{n}h',\n            'minute': lambda n: f'{n}min',\n        }\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_strings), '1y')\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_strings), '1m')\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_strings), '1w')\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_strings), '1d')\n        self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_strings), '1h')\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_strings), '1min')\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneyear + self.onemonth, time_strings=custom_strings, depth=2),\n            '1y, 1m'\n        )\n", "def test_custom_time_strings(self):\n        \"\"\" Test custom time strings. \"\"\"\n        custom_time_strings = {\n            'year': ngettext_lazy('%d yr', '%d yrs'),\n            'month': ngettext_lazy('%d mo', '%d mos'),\n            'week': ngettext_lazy('%d wk', '%d wks'),\n            'day': ngettext_lazy('%d dy', '%d dys'),\n            'hour': ngettext_lazy('%d hr', '%d hrs'),\n            'minute': ngettext_lazy('%d min', '%d mins'),\n        }\n        t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour + self.oneminute\n        self.assertEqual(timesince(self.t, t, time_strings=custom_time_strings, depth=6), '1\\xa0yr, 1\\xa0mo, 1\\xa0wk, 1\\xa0dy, 1\\xa0hr, 1\\xa0min')\n        self.assertEqual(timeuntil(t, self.t, time_strings=custom_time_strings, depth=6), '1\\xa0yr, 1\\xa0mo, 1\\xa0wk, 1\\xa0dy, 1\\xa0hr, 1\\xa0min')\n", "    def test_timesince_with_custom_time_strings(self):\n        \"\"\" Test timesince with custom time_strings. \"\"\"\n        custom_time_strings = {\n            'year': 'rok', \n            'month': 'm\u011bs\u00edc', \n            'week': 't\u00fdden', \n            'day': 'den', \n            'hour': 'hodina', \n            'minute': 'minuta'\n        }\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1\\xa0rok')\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1\\xa0m\u011bs\u00edc')\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), '1\\xa0t\u00fdden')\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), '1\\xa0den')\n        self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), '1\\xa0hodina')\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), '1\\xa0minuta')\n"], "sample_199": ["    def test_combined_expressions(self):\n        test_book = self.b1\n        combined_expr = (F('pages') - F('rating')) * (F('price') / 2)\n        annotated_book = Book.objects.annotate(combined_value=ExpressionWrapper(combined_expr, output_field=DecimalField())).get(pk=test_book.pk)\n        expected_value = (test_book.pages - test_book.rating) * (test_book.price / 2)\n        self.assertAlmostEqual(annotated_book.combined_value, expected_value)\n", "    def test_combined_expression_with_duration(self):\n        duration = datetime.timedelta(days=5)\n        Ticket.objects.create(active_at=datetime.datetime(2023, 10, 1, 10, 0, 0), duration=duration)\n        ticket = Ticket.objects.annotate(\n            total_duration=ExpressionWrapper(F('duration') + datetime.timedelta(days=2), output_field=DurationField())\n        ).first()\n        self.assertEqual(ticket.total_duration, duration + datetime.timedelta(days=2))\n", "    def test_combined_expression_with_negation(self):\n        book = Book.objects.annotate(\n            neg_combined=ExpressionWrapper(Value(-3) * F('pages'), output_field=IntegerField()),\n        ).first()\n        self.assertEqual(book.neg_combined, -3 * book.pages)\n", "def test_combined_expression_with_decimal_field(self):\n    book = Book.objects.annotate(\n        combined=ExpressionWrapper(Value(Decimal('2.5')) + Value(Decimal('3.5')), output_field=DecimalField()),\n    ).first()\n    self.assertEqual(book.combined, Decimal('6.0'))\n", "def test_combined_bitwise_operations(self):\n        author = Author.objects.create(name='Test Author', age=42)\n        Author.objects.annotate(\n            bitand_result=ExpressionWrapper(F('age').bitand(Value(3)), output_field=IntegerField()),\n            bitor_result=ExpressionWrapper(F('age').bitor(Value(2)), output_field=IntegerField()),\n            bitxor_result=ExpressionWrapper(F('age').bitxor(Value(1)), output_field=IntegerField()),\n            bitleftshift_result=ExpressionWrapper(F('age').bitleftshift(Value(1)), output_field=IntegerField()),\n            bitrightshift_result=ExpressionWrapper(F('age').bitrightshift(Value(1)), output_field=IntegerField())\n        ).filter(pk=author.pk).get()\n\n        self.assertEqual(author.bitand_result, author.age & 3)\n        self.assertEqual(author.bitor_result, author.age | 2)\n        self.assertEqual(author.bitxor_result, author.age ^ 1)\n        self.assertEqual(author.bitleftshift_result, author.age << 1)\n        self.assertEqual(author.bitrightshift_result, author.age >> 1)\n"], "sample_200": ["    def test_sanitize_address_with_invalid_tld(self):\n        \"\"\"\n        Ensure sanitize_address raises a ValueError for email addresses with invalid TLDs.\n        \"\"\"\n        invalid_email = 'user@invalid_tld.xyz'\n        with self.assertRaises(ValueError):\n            sanitize_address(invalid_email, 'utf-8')\n", "    def test_attachment_without_filename(self):\n        \"\"\"\n        Test attaching content without a filename.\n        \"\"\"\n        email = EmailMessage('Subject', 'Body', 'from@example.com', ['to@example.com'])\n        email.attach(content='This is the attached content.', mimetype='text/plain')\n        message = email.message()\n        payload = message.get_payload()\n        self.assertEqual(len(payload), 2)\n        self.assertEqual(payload[1].get_payload(), 'This is the attached content.')\n        self.assertEqual(payload[1].get_content_type(), 'text/plain')\n        self.assertIsNone(payload[1].get_filename())\n", "    def test_attach_invalid_mimetype(self):\n        \"\"\"\n        Test attaching a file with an invalid mimetype. The file should still\n        be attached and sent properly even if an invalid mimetype is specified.\n        \"\"\"\n        email = EmailMessage('subject', 'body', 'from@example.com', ['to@example.com'])\n        invalid_mimetype = 'invalid/type'\n        email.attach('file.invalid', 'file content', mimetype=invalid_mimetype)\n        self.assertEqual(len(email.attachments), 1)\n        attachment = email.attachments[0]\n        self.assertEqual(attachment[2], 'application/octet-stream')\n        msg = email.message()\n        self.assertIn('application/octet-stream', str(msg))\n", "    def test_sanitize_address_with_invalid_tuple(self):\n        \"\"\"\n        Ensure sanitize_address raises ValueError when given a tuple with invalid address parts.\n        \"\"\"\n        invalid_tuples = [\n            ('Name', 'to@example@com'),\n            ('Name', 'to@'),\n            ('Name', '@example.com'),\n            ('Name', 'example.com'),\n        ]\n        for addr in invalid_tuples:\n            with self.subTest(addr=addr):\n                with self.assertRaises(ValueError):\n                    sanitize_address(addr, 'utf-8')\n", "    def test_sanitize_address_with_unicode_and_long_display_name(self):\n        \"\"\"\n        Test sanitize_address with a long display name containing unicode characters.\n        \"\"\"\n        email_address = ('T\u00f3 Example very long' * 4, 'to@exampl\u20ac.com')\n        encoding = 'utf-8'\n        expected_result = (\n            '=?utf-8?q?T=C3=B3_Example_very_longT=C3=B3_Example_very_longT'\n            '=C3=B3_Example_?=\\n'\n            ' =?utf-8?q?very_longT=C3=B3_Example_very_long?= '\n            '<to@xn--exampl-nc1c.com>'\n        )\n        self.assertEqual(sanitize_address(email_address, encoding), expected_result)\n"], "sample_201": ["    def test_legacy_hash(self):\n        \"\"\"\n        Test that the legacy hash method generates the expected hash value.\n        \"\"\"\n        storage = self.storage_class(self.get_request())\n        value = 'test_value'\n        expected_hash = salted_hmac('django.contrib.messages', value).hexdigest()\n        self.assertEqual(storage._legacy_hash(value), expected_hash)\n", "    def test_legacy_hash(self):\n        \"\"\"\n        Ensure that the legacy hash is calculated correctly.\n        \"\"\"\n        storage = self.get_storage()\n        message = 'legacy message'\n        legacy_hashed_value = storage._legacy_hash(message)\n        expected_hashed_value = salted_hmac('django.contrib.messages', message).hexdigest()\n        self.assertEqual(legacy_hashed_value, expected_hashed_value)\n", "    def test_legacy_hash(self):\n        \"\"\"\n        Test the legacy hash generation to ensure it produces the expected output.\n        \"\"\"\n        storage = self.storage_class(self.get_request())\n        test_value = \"test_legacy_value\"\n        expected_hash = salted_hmac('django.contrib.messages', test_value).hexdigest()\n        self.assertEqual(storage._legacy_hash(test_value), expected_hash)\n", "    def test_legacy_hash(self):\n        \"\"\"\n        Test the _legacy_hash method to ensure it generates a consistent hash.\n        \"\"\"\n        storage = self.get_storage()\n        value = 'test_legacy_hash_value'\n        expected_hash = salted_hmac('django.contrib.messages', value).hexdigest()\n        legacy_hash = storage._legacy_hash(value)\n        self.assertEqual(legacy_hash, expected_hash)\n", "    def test_store_empty_message(self):\n        \"\"\"\n        Storing an empty list of messages should remove the cookie.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add a message to set the cookie initially.\n        storage.add(constants.INFO, 'initial message')\n        storage.update(response)\n        self.assertIn(CookieStorage.cookie_name, response.cookies)\n\n        # Now store an empty list of messages.\n        storage._store([], response)\n        self.assertNotIn(CookieStorage.cookie_name, response.cookies)\n"], "sample_202": ["    def test_store_empty_messages(self):\n        \"\"\"\n        Test that storing an empty list of messages results in the cookie being deleted.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        storage._store([], response)\n        self.assertEqual(response.cookies['messages'].value, '')\n        self.assertEqual(response.cookies['messages']['max-age'], 0)\n", "    def test_store_and_retrieve_messages(self):\n        \"\"\"\n        Store a list of messages in the cookie and retrieve them to ensure\n        that the storage and retrieval process is working correctly.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add messages to storage\n        messages = [\n            Message(constants.INFO, 'info message'),\n            Message(constants.WARNING, 'warning message', extra_tags='extra'),\n            Message(constants.ERROR, mark_safe('safe error message')),\n        ]\n        for message in messages:\n            storage.add(message.level, message.message, extra_tags=message.extra_tags)\n        \n        # Store messages in cookie\n        storage.update(response)\n\n        # Retrieve messages from cookie\n        retrieved_messages = list(storage)\n\n        # Assert that the retrieved messages are the same as the original messages\n        self.assertEqual(len(retrieved_messages), len(messages))\n        for original, retrieved in zip(messages, retrieved_messages):\n            self.assertEqual(original.level, retrieved.level)\n            self.assertEqual(original.message, retrieved.message)\n            self.assertEqual(original.extra_tags, retrieved.extra_tags)\n            if isinstance(original.message, SafeData):\n                self.assertIsInstance(retrieved.message, SafeData)\n", "def test_not_finished_sentinel(self):\n    \"\"\"\n    If the not_finished sentinel is present in the messages, it should be\n    removed and all_retrieved should be False.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    example_messages = [Message(constants.INFO, 'message 1'), Message(constants.INFO, 'message 2')]\n    set_cookie_data(storage, example_messages + [CookieStorage.not_finished])\n\n    # Retrieve messages\n    messages, all_retrieved = storage._get()\n    self.assertEqual(messages, example_messages)\n    self.assertFalse(all_retrieved)\n\n    # Store messages and ensure the not_finished sentinel is re-added\n    unstored_messages = storage._store(messages, response, remove_oldest=False)\n    self.assertIn(CookieStorage.not_finished, json.loads(response.cookies[storage.cookie_name].value.split('$', 1)[1], cls=MessageDecoder))\n    self.assertEqual(unstored_messages, [])\n", "compilation error", "    def test_store_empty_messages(self):\n        \"\"\"\n        Test storing an empty list of messages in the cookie.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        unstored_messages = storage._store([], response)\n        self.assertEqual(unstored_messages, [])\n        self.assertIsNone(response.cookies.get(storage.cookie_name))\n"], "sample_203": ["    def test_prohibit_null_characters(self):\n        validator = validators.ProhibitNullCharactersValidator()\n        with self.assertRaises(ValidationError) as e:\n            validator('test\\x00string')\n        self.assertEqual(e.exception.messages, ['Null characters are not allowed.'])\n", "    def test_prohibit_null_characters_validator(self):\n        validator = validators.ProhibitNullCharactersValidator()\n        with self.assertRaises(ValidationError) as e:\n            validator('Null character: \\x00')\n        self.assertEqual(e.exception.message, 'Null characters are not allowed.')\n        ", "    def test_null_characters_not_allowed(self):\n        validator = validators.ProhibitNullCharactersValidator()\n\n        with self.assertRaises(ValidationError) as e:\n            validator(\"test\\x00string\")\n\n        self.assertEqual(e.exception.message, 'Null characters are not allowed.')\n        self.assertEqual(e.exception.code, 'null_characters_not_allowed')\n", "    def test_valid_extension(self):\n        validator = validators.FileExtensionValidator(allowed_extensions=['txt', 'jpg'])\n        file = SimpleUploadedFile('test.txt', b'file_content')\n        # Should not raise an error\n        validator(file)\n", "    def test_null_character_validation(self):\n        validator = validators.ProhibitNullCharactersValidator()\n\n        # Test input containing null character\n        with self.assertRaises(ValidationError) as e:\n            validator('test\\x00string')\n        self.assertEqual(str(e.exception), '[\"Null characters are not allowed.\"]')\n\n        # Test input without null character\n        try:\n            validator('teststring')\n        except ValidationError:\n            self.fail(\"Validation raised unexpectedly for a string without null characters.\")\n"], "sample_204": ["    def test_cyclic_migrations(self):\n        \"\"\"\n        Tests detection of cyclic migrations.\n        \"\"\"\n        with self.assertRaisesMessage(ValueError, \"Migration graph contains a cycle\"):\n            MigrationLoader(connection)\n", "    def test_invalid_migration_class(self):\n        \"\"\"\n        Tests loading migrations with an invalid Migration class.\n        \"\"\"\n        with self.assertRaisesMessage(BadMigrationError, \"Migration 0001_initial in app migrations has no Migration class\"):\n            MigrationLoader(connection)\n", "    def test_collect_sql(self):\n        \"\"\"\n        Tests the collect_sql method of MigrationLoader to ensure it collects\n        the correct SQL statements for the given migration plan.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        # Define a migration plan\n        plan = [\n            (loader.get_migration(\"migrations\", \"0001_initial\"), False),\n            (loader.get_migration(\"migrations\", \"0002_second\"), False),\n        ]\n        sql_statements = loader.collect_sql(plan)\n        # Check if the SQL statements are collected properly\n        self.assertIsInstance(sql_statements, list)\n        self.assertTrue(all(isinstance(stmt, str) for stmt in sql_statements))\n        # Ensure statements have some content (indicative of collected SQL)\n        self.assertGreater(len(sql_statements), 0)\n", "    def test_get_migration(self):\n        \"\"\"Test getting a migration by app label and name prefix.\"\"\"\n        loader = MigrationLoader(connection)\n        migration = loader.get_migration(\"migrations\", \"0001_initial\")\n        self.assertEqual(migration.name, \"0001_initial\")\n        with self.assertRaises(NodeNotFoundError):\n            loader.get_migration(\"migrations\", \"nonexistent\")\n", "    def test_load_no_migration_class(self):\n        \"\"\"\n        Ensure that a BadMigrationError is raised when a migration file\n        does not contain a Migration class.\n        \"\"\"\n        with self.assertRaises(BadMigrationError):\n            MigrationLoader(connection)\n"], "sample_205": ["    def test_str(self):\n        error1 = ValidationError('message')\n        self.assertEqual(str(error1), \"['message']\")\n\n        error2 = ValidationError({'field1': ['error1', 'error2']})\n        self.assertEqual(str(error2), \"{'field1': ['error1', 'error2']}\")\n\n        error3 = ValidationError([\n            ValidationError('error1'),\n            ValidationError('error2'),\n        ])\n        self.assertEqual(str(error3), \"['error1', 'error2']\")\n", "    def test_init(self):\n        # Test initializing with a single message\n        error = ValidationError('single error message')\n        self.assertEqual(error.message, 'single error message')\n        self.assertEqual(error.error_list, [error])\n\n        # Test initializing with a list of messages\n        error_list = ['error message 1', 'error message 2']\n        error = ValidationError(error_list)\n        self.assertEqual(len(error.error_list), 2)\n        self.assertEqual(str(error), \"['error message 1', 'error message 2']\")\n\n        # Test initializing with a dictionary of messages\n        error_dict = {'field1': 'error message 1', 'field2': 'error message 2'}\n        error = ValidationError(error_dict)\n        self.assertEqual(len(error.error_dict), 2)\n        self.assertEqual(error.error_dict['field1'][0].message, 'error message 1')\n        self.assertEqual(error.error_dict['field2'][0].message, 'error message 2')\n\n        # Test initializing with another ValidationError instance\n        nested_error = ValidationError('nested error')\n        error = ValidationError(nested_error)\n        self.assertEqual(error.message, 'nested error')\n        self.assertEqual(error.error_list, [nested_error])\n\n        # Test initializing with another ValidationError instance with error_dict\n        nested_error_dict = ValidationError({'field1': 'nested field error'})\n        error = ValidationError(nested_error_dict)\n        self.assertEqual(error.error_dict['field1'][0].message, 'nested field error')\n", "compilation error", "    def test_init_with_nested_validation_errors(self):\n        nested_error = ValidationError('nested error')\n        error_dict = {\n            'field1': 'error1',\n            'field2': nested_error,\n        }\n        error = ValidationError(error_dict)\n        \n        # Check if the error_dict is correctly set\n        self.assertTrue(hasattr(error, 'error_dict'))\n        self.assertIn('field1', error.error_dict)\n        self.assertIn('field2', error.error_dict)\n        \n        # Check the error list for each field\n        self.assertEqual(error.error_dict['field1'], [ValidationError('error1')])\n        self.assertEqual(error.error_dict['field2'], [nested_error])\n        \n        # Check the messages property\n        self.assertEqual(error.messages, ['error1', 'nested error'])\n        \n        # Check __str__ and __repr__\n        self.assertEqual(str(error), \"{'field1': ['error1'], 'field2': ['nested error']}\")\n        self.assertEqual(repr(error), \"ValidationError({'field1': ['error1'], 'field2': ['nested error']})\")\n", "    def test_update_error_dict(self):\n        error1 = ValidationError('message1')\n        error2 = ValidationError('message2')\n        error_dict = {}\n        updated_error_dict = error1.update_error_dict(error_dict)\n        self.assertEqual(updated_error_dict, {NON_FIELD_ERRORS: ['message1']})\n\n        error_dict_with_field = {'field1': ['existing error']}\n        updated_error_dict = error2.update_error_dict(error_dict_with_field)\n        self.assertEqual(updated_error_dict, {'field1': ['existing error'], NON_FIELD_ERRORS: ['message2']})\n\n        nested_error = ValidationError({'field2': ['error3', 'error4']})\n        error_dict_with_field_and_non_field = {'field1': ['existing error'], NON_FIELD_ERRORS: ['existing non-field error']}\n        updated_error_dict = nested_error.update_error_dict(error_dict_with_field_and_non_field)\n        self.assertEqual(updated_error_dict, {\n            'field1': ['existing error'],\n            'field2': ['error3', 'error4'],\n            NON_FIELD_ERRORS: ['existing non-field error']\n        })\n"], "sample_206": ["    def test_generate_filename_callable(self):\n        \"\"\"\n        FileField's generate_filename works with callable upload_to.\n        \"\"\"\n        class TestModel(models.Model):\n            myfile = FileField(upload_to=lambda instance, filename: f'test/{filename}')\n\n        field = TestModel._meta.get_field('myfile')\n        instance = TestModel()\n\n        generated_filename = field.generate_filename(instance, 'example.txt')\n        self.assertTrue(generated_filename.startswith('test/example'))\n", "    def test_file_field_equality(self):\n        \"\"\"\n        Test the __eq__ method of FieldFile.\n        \"\"\"\n        d1 = Document(myfile='something.txt')\n        d2 = Document(myfile='something.txt')\n        self.assertEqual(d1.myfile, d2.myfile)\n        d3 = Document(myfile='different.txt')\n        self.assertNotEqual(d1.myfile, d3.myfile)\n", "    def test_field_file_equality(self):\n        \"\"\"\n        Test the equality operation for FieldFile objects.\n        \"\"\"\n        d1 = Document(myfile='something.txt')\n        d2 = Document(myfile='something.txt')\n        d3 = Document(myfile='different.txt')\n        self.assertEqual(d1.myfile, d2.myfile)\n        self.assertNotEqual(d1.myfile, d3.myfile)\n", "    def test_file_field_save(self):\n        \"\"\"\n        FileField.save() should correctly save the file and update instance attribute.\n        \"\"\"\n        d = Document()\n        file_content = ContentFile(b\"dummy content\", name=\"dummy.txt\")\n        d.myfile.save(\"dummy.txt\", file_content)\n        self.assertEqual(d.myfile.name, \"dummy.txt\")\n        self.assertTrue(d.myfile._committed)\n        self.assertTrue(d.myfile.storage.exists(\"dummy.txt\"))\n\n        # Clean up\n        d.myfile.delete()\n        self.assertFalse(d.myfile.storage.exists(\"dummy.txt\"))\n", "    def test_file_save_and_delete(self):\n        \"\"\"\n        Test the save and delete methods of the FieldFile class.\n        \"\"\"\n        d = Document.objects.create(myfile='something.txt')\n        d.myfile.save('newfile.txt', ContentFile(b'new content'))\n        self.assertEqual(d.myfile.name, 'newfile.txt')\n        \n        # Ensure the file is saved in the storage\n        self.assertTrue(d.myfile.storage.exists('newfile.txt'))\n        \n        # Now delete the file and check it's been deleted\n        d.myfile.delete()\n        self.assertIsNone(d.myfile.name)\n        self.assertFalse(d.myfile.storage.exists('newfile.txt'))\n"], "sample_207": ["    def test_key_transform_lexical_lookups(self):\n        obj = NullableJSONModel.objects.create(value={'name': 'John', 'surname': 'Doe'})\n        \n        # Test KeyTransformStartsWith\n        self.assertIs(NullableJSONModel.objects.filter(value__name__startswith='Jo').exists(), True)\n        self.assertIs(NullableJSONModel.objects.filter(value__name__startswith='jo').exists(), False)\n        \n        # Test KeyTransformIStartsWith\n        self.assertIs(NullableJSONModel.objects.filter(value__name__istartswith='jo').exists(), True)\n\n        # Test KeyTransformEndsWith\n        self.assertIs(NullableJSONModel.objects.filter(value__surname__endswith='oe').exists(), True)\n        self.assertIs(NullableJSONModel.objects.filter(value__surname__endswith='OE').exists(), False)\n\n        # Test KeyTransformIEndsWith\n        self.assertIs(NullableJSONModel.objects.filter(value__surname__iendswith='OE').exists(), True)\n\n        # Test KeyTransformContains\n        self.assertIs(NullableJSONModel.objects.filter(value__name__contains='oh').exists(), True)\n        self.assertIs(NullableJSONModel.objects.filter(value__name__contains='Oh').exists(), False)\n\n        # Test KeyTransformIContains\n        self.assertIs(NullableJSONModel.objects.filter(value__name__icontains='Oh').exists(), True)\n\n        # Test KeyTransformRegex\n        self.assertIs(NullableJSONModel.objects.filter(value__name__regex=r'^John$').exists(), True)\n        self.assertIs(NullableJSONModel.objects.filter(value__name__regex=r'^john$').exists(), False)\n\n        # Test KeyTransformIRegex\n        self.assertIs(NullableJSONModel.objects.filter(value__name__iregex=r'^john$').exists(), True)\n", "    def test_invalid_json_string(self):\n        field = models.JSONField()\n        msg = 'Value must be valid JSON.'\n        with self.assertRaisesMessage(ValidationError, msg):\n            field.clean('invalid json string', None)\n", "    def test_key_transform_lt(self):\n        with CaptureQueriesContext(connection) as queries:\n            self.assertIs(NullableJSONModel.objects.filter(\n                value__d__0__lt=5\n            ).exists(), False)\n        self.assertIn(\n            \"\"\".\"value\" -> 'd' ->> '0' < '5'\"\"\",\n            queries[0]['sql'],\n        )\n", "    def test_get_prep_value(self):\n        field = models.JSONField()\n        value = {'name': 'John', 'age': 30}\n        prepped_value = field.get_prep_value(value)\n        self.assertEqual(prepped_value, json.dumps(value))\n        \n        value = None\n        prepped_value = field.get_prep_value(value)\n        self.assertIsNone(prepped_value)\n        \n        with self.assertRaises(TypeError):\n            field.get_prep_value(set(['invalid']))\n", "    def test_contains_key_transform(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__contains=KeyTransform('foo', Value('bar'))\n            ),\n            [self.objs[7]]\n        )\n"], "sample_208": ["    def test_alter_order_with_respect_to_preservation(self):\n        \"\"\"\n        Ensure that order_with_respect_to is preserved correctly when altering a model.\n        \"\"\"\n        initial_state = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n        ], options={\"order_with_respect_to\": \"book\"})\n\n        altered_state = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n            (\"publisher\", models.CharField(max_length=200)),\n        ], options={\"order_with_respect_to\": \"book\"})\n\n        changes = self.get_changes([self.book, initial_state], [self.book, altered_state])\n        # Ensure the number and type of migrations\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name=\"author\", name=\"publisher\")\n        # Ensure order_with_respect_to is preserved\n        self.assertNotIn(\"AlterOrderWithRespectTo\", [op.__class__.__name__ for op in changes['testapp'][0].operations])\n", "    def test_alter_field_type(self):\n        \"\"\"\n        Test altering the field type while keeping other attributes the same.\n        \"\"\"\n        before = ModelState(\"testapp\", \"Book\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"title\", models.CharField(max_length=200)),\n        ])\n        after = ModelState(\"testapp\", \"Book\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"title\", models.TextField()),  # Change from CharField to TextField\n        ])\n        changes = self.get_changes([before], [after])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"title\")\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, __class__=models.TextField)\n", "    def test_remove_model_with_dependent_fk(self):\n        \"\"\"\n        Removing a model that has a ForeignKey to it should first remove the\n        ForeignKey field before deleting the model itself.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_book, self.book],\n            [self.book],\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RemoveField\", \"DeleteModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name=\"author\", name=\"book\")\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"Author\")\n", "    def test_generate_altered_db_table(self):\n        \"\"\"\n        Tests the detection and generation of an AlterModelTable operation\n        for models with altered db_table options.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_db_table_options],\n            [self.author_with_new_db_table_options]\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", table=\"author_two\")\n", "    def test_deep_deconstruct_with_compiled_regex(self):\n        \"\"\"\n        Test deep_deconstruct with a COMPILED_REGEX_TYPE object.\n        \"\"\"\n        obj = re.compile(r'\\d+')\n        autodetector = MigrationAutodetector(None, None)\n        result = autodetector.deep_deconstruct(obj)\n        self.assertEqual(result.pattern, r'\\d+')\n"], "sample_209": ["    def test_model_init_unexpected_keyword(self):\n        \"\"\"\n        Test that __init__() raises a TypeError when an unexpected keyword\n        argument is passed.\n        \"\"\"\n        msg = \"Worker() got an unexpected keyword argument 'unexpected_arg'\"\n        with self.assertRaisesMessage(TypeError, msg):\n            Worker(name='John Doe', unexpected_arg='unexpected value')\n", "    def test_model_deferred_fields(self):\n        \"\"\"\n        Test to check the get_deferred_fields method of the Model class.\n        \"\"\"\n        class TestModel(Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.IntegerField()\n\n            class Meta:\n                app_label = \"tests\"\n\n        obj = TestModel(field1='test', field2=42)\n        self.assertEqual(obj.get_deferred_fields(), set())\n        \n        obj.field1 = DEFERRED\n        self.assertEqual(obj.get_deferred_fields(), {'field1'})\n        \n        obj.field2 = DEFERRED\n        self.assertEqual(obj.get_deferred_fields(), {'field1', 'field2'})\n\n        obj.field1 = 'test'\n        self.assertEqual(obj.get_deferred_fields(), {'field2'})\n", "    def test_modelbase_create_non_model_subclass(self):\n        \"\"\"\n        Ensure ModelBase.__new__() returns correct class when creating non-subclass of Model.\n        \"\"\"\n        class NotAModel(metaclass=ModelBase):\n            pass\n\n        self.assertFalse(hasattr(NotAModel, '_meta'))\n        self.assertEqual(NotAModel.__module__, __name__)\n", "    def test_model_save_force_insert_and_update(self):\n        \"\"\"\n        Test the save() method with force_insert and force_update options.\n        \"\"\"\n        # Create a new Department instance\n        d = Department.objects.create(id=20, name=\"HR\")\n\n        # Create a new Worker instance and save it with force_insert\n        w = Worker(department=d, name=\"Part-time\")\n        w.save(force_insert=True)\n\n        # Verify that the Worker instance was inserted\n        self.assertIsNotNone(w.pk)\n\n        # Change the name of the Worker instance and save it with force_update\n        w.name = \"Temporary\"\n        w.save(force_update=True)\n\n        # Verify that the Worker instance was updated\n        w.refresh_from_db()\n        self.assertEqual(w.name, \"Temporary\")\n\n        # Verify that using both force_insert and force_update raises a ValueError\n        with self.assertRaises(ValueError):\n            w.save(force_insert=True, force_update=True)\n", "    def test_base_manager(self):\n        \"\"\"\n        Test the `_base_manager` property of the `ModelBase` class.\n        \"\"\"\n        class MyModel(models.Model):\n            name = models.CharField(max_length=255)\n\n        self.assertEqual(MyModel._base_manager, MyModel.objects)\n"], "sample_210": ["    def test_redirect_with_invalid_pattern_name(self):\n        \"Named pattern with invalid pattern name should raise an error\"\n        with self.assertRaises(NoReverseMatch):\n            RedirectView.as_view(pattern_name='invalid_pattern_name')(self.rf.get('/foo/'))\n", "    def test_render_to_response(self):\n        \"\"\"\n        Test the render_to_response method of TemplateResponseMixin.\n        \"\"\"\n        class TestView(TemplateResponseMixin, View):\n            template_name = 'generic_views/test.html'\n\n                context = self.get_context_data()\n                return self.render_to_response(context)\n\n        rf = RequestFactory()\n        request = rf.get('/test/')\n        view = TestView.as_view()\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'generic_views/test.html')\n", "    def test_redirect_method_override(self):\n        \"\"\"\n        Test a view that overrides the get_redirect_url method.\n        \"\"\"\n        class CustomRedirectView(RedirectView):\n                return '/custom-redirect/'\n\n        response = CustomRedirectView.as_view()(self.rf.get('/foo/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/custom-redirect/')\n", "    def test_redirect_with_unicode_characters(self):\n        \"Redirection URL can contain unicode characters\"\n        response = RedirectView.as_view(url='/bar/\u2713/')(self.rf.get('/foo/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/\u2713/')\n", "    def test_template_view_with_extra_context(self):\n        \"\"\"\n        Test a TemplateView that provides extra context.\n        \"\"\"\n        class ExtraContextTemplateView(TemplateView):\n            template_name = 'generic_views/about.html'\n            extra_context = {'title': 'Extra Context Title'}\n\n        response = ExtraContextTemplateView.as_view()(self.rf.get('/about/'))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Extra Context Title')\n"], "sample_211": ["    def test_render_to_response_custom_response_class(self):\n        \"\"\"\n        Test that a view can use a custom response class.\n        \"\"\"\n        class CustomResponse(HttpResponse):\n            pass\n\n        class CustomResponseView(TemplateView):\n            response_class = CustomResponse\n            template_name = 'generic_views/about.html'\n\n        response = CustomResponseView.as_view()(self.rf.get('/'))\n        self.assertIsInstance(response, CustomResponse)\n        self.assertEqual(response.status_code, 200)\n", "    def test_render_to_response_content_type(self):\n        \"\"\"\n        Test that the content_type parameter is passed correctly to the response.\n        \"\"\"\n        class CustomTemplateView(TemplateView):\n            template_name = 'generic_views/about.html'\n            content_type = 'application/json'\n\n        response = CustomTemplateView.as_view()(self.rf.get('/about/'))\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'application/json')\n", "    def test_template_view_without_context(self):\n        \"\"\"\n        Test TemplateView rendering without extra context.\n        \"\"\"\n        class NoContextTemplateView(TemplateView):\n            template_name = 'generic_views/about.html'\n\n        response = NoContextTemplateView.as_view()(self.rf.get('/about/'))\n        response.render()\n        self.assertContains(response, '<h1>About</h1>')\n        self.assertNotIn('foo', response.context_data)\n        self.assertNotIn('bar', response.context_data)\n", "    def test_context_mixin_with_extra_context(self):\n        class TestView(ContextMixin):\n            extra_context = {'key': 'value'}\n\n        view = TestView()\n        context = view.get_context_data()\n        self.assertIn('key', context)\n        self.assertEqual(context['key'], 'value')\n", "    def test_redirect_with_no_url_or_pattern_name(self):\n        \"\"\"\n        Test that RedirectView returns HTTP 410 GONE if neither url nor pattern_name is provided.\n        \"\"\"\n        response = RedirectView.as_view()(self.rf.get('/no-redirect/'))\n        self.assertEqual(response.status_code, 410)\n"], "sample_213": ["    def setUp(self):\n        self.storage = FileSystemStorage(location=tempfile.mkdtemp())\n        self.instance = unittest.mock.Mock()\n        self.field = FileField(upload_to='uploads/', storage=self.storage)\n", "    def test_file_field_generate_filename(self):\n        \"\"\"\n        Test that FileField's generate_filename method correctly applies\n        the upload_to parameter.\n        \"\"\"\n        class TestModel:\n            pass\n\n        # Test with upload_to as a string\n        field = FileField(upload_to='uploads/%Y/%m/%d')\n        instance = TestModel()\n        generated_name = field.generate_filename(instance, 'testfile.txt')\n        self.assertRegex(generated_name, r'^uploads/\\d{4}/\\d{2}/\\d{2}/testfile.txt$')\n\n        # Test with upload_to as a callable\n            return f'custom/{filename}'\n\n        field = FileField(upload_to=custom_upload_to)\n        generated_name = field.generate_filename(instance, 'testfile.txt')\n        self.assertEqual(generated_name, 'custom/testfile.txt')\n", "    def setUp(self):\n        # Create a temporary directory to serve as the storage location\n        self.temp_dir = tempfile.mkdtemp()\n        self.storage = FileSystemStorage(location=self.temp_dir)\n        self.field = FileField(upload_to='test_upload_to', storage=self.storage)\n        self.instance = unittest.mock.Mock()\n        self.instance.save = unittest.mock.Mock()\n    ", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.storage = FileSystemStorage(location=self.temp_dir)\n        self.field = FileField(upload_to='uploads/', storage=self.storage)\n        self.instance = Storage()\n", "    def setUp(self):\n        self.storage = FileSystemStorage(location=tempfile.mkdtemp())\n"], "sample_212": ["    def test_session_middleware_process_request_and_response(self):\n        \"\"\"\n        Test that SessionMiddleware correctly processes the request and response\n        when session data is accessed and modified.\n        \"\"\"\n        get_response = lambda request: HttpResponse()\n        middleware = SessionMiddleware(get_response)\n\n        # Create a fake request with session cookie\n        request = HttpRequest()\n        request.COOKIES[settings.SESSION_COOKIE_NAME] = 'session_key'\n\n        # Process request\n        middleware.process_request(request)\n        self.assertIsNotNone(request.session)\n        self.assertEqual(request.session.session_key, 'session_key')\n\n        # Modify session data\n        request.session['key'] = 'value'\n        request.session.modified = True\n\n        # Process response\n        response = HttpResponse()\n        response = middleware.process_response(request, response)\n\n        # Check if session cookie is set\n        self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n        self.assertEqual(response.cookies[settings.SESSION_COOKIE_NAME].value, request.session.session_key)\n", "    def test_session_middleware_process_request(self):\n        \"\"\"\n        Test that SessionMiddleware correctly initializes the session for a request.\n        \"\"\"\n        class MockSessionStore:\n                self.session_key = session_key\n\n        class MockRequest:\n            COOKIES = {settings.SESSION_COOKIE_NAME: 'test_session_key'}\n\n        session_middleware = SessionMiddleware()\n        session_middleware.SessionStore = MockSessionStore\n\n        request = MockRequest()\n        session_middleware.process_request(request)\n        \n        self.assertEqual(request.session.session_key, 'test_session_key')\n", "    def test_session_middleware_process_request(self):\n        \"\"\"\n        Test that SessionMiddleware correctly initializes session from cookies.\n        \"\"\"\n        request = HttpRequest()\n        request.COOKIES[settings.SESSION_COOKIE_NAME] = 'test_session_key'\n        middleware = SessionMiddleware(lambda req: HttpResponse())\n        middleware.process_request(request)\n        self.assertEqual(request.session.session_key, 'test_session_key')\n", "    def test_session_middleware_process_request(self):\n        \"\"\"\n        Test that the SessionMiddleware correctly sets the session on the request.\n        \"\"\"\n        request = HttpRequest()\n        request.COOKIES[settings.SESSION_COOKIE_NAME] = 'test_session_key'\n        middleware = SessionMiddleware(lambda req: HttpResponse())\n        middleware.process_request(request)\n        self.assertEqual(request.session.session_key, 'test_session_key')\n", "    def test_session_middleware_process_request(self):\n        \"\"\"\n        Test the process_request method of SessionMiddleware to ensure it properly\n        initializes the session from the request's cookies.\n        \"\"\"\n        request = HttpRequest()\n        request.COOKIES[settings.SESSION_COOKIE_NAME] = 'testsessionkey'\n        \n        middleware = SessionMiddleware()\n        middleware.process_request(request)\n        \n        self.assertIsNotNone(request.session)\n        self.assertEqual(request.session.session_key, 'testsessionkey')\n"], "sample_214": ["    def test_compile_json_path_include_root(self):\n        path = compile_json_path(['a', 'b', 1, 'c'])\n        self.assertEqual(path, '$.a.b[1].\"c\"')\n", "    def test_compile_json_path(self):\n        test_cases = [\n            ([], '$'),\n            (['a'], '$.\"a\"'),\n            (['a', 'b'], '$.\"a\".\"b\"'),\n            (['a', 1, 'b'], '$.\"a\"[1].\"b\"'),\n            ([1, 'a', 2], '$.[1].\"a\".[2]'),\n        ]\n        for key_transforms, expected_path in test_cases:\n            with self.subTest(key_transforms=key_transforms):\n                self.assertEqual(compile_json_path(key_transforms), expected_path)\n", "    def test_key_transform_contains(self):\n        value = {\n            'name': 'John',\n            'details': {\n                'age': 30,\n                'address': {'city': 'New York', 'zipcode': '10001'},\n            },\n            'skills': ['Python', 'Django'],\n        }\n        obj = JSONModel.objects.create(value=value)\n        obj.refresh_from_db()\n        self.assertSequenceEqual(\n            JSONModel.objects.filter(value__details__address__contains={'city': 'New York'}),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            JSONModel.objects.filter(value__details__address__contains={'zipcode': '10001'}),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            JSONModel.objects.filter(value__skills__contains=['Python']),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            JSONModel.objects.filter(value__skills__contains=['Django']),\n            [obj],\n        )\n", "    def test_key_transform_numeric_lookups(self):\n        tests = [\n            ('value__c__lt', 15, [self.objs[3], self.objs[4]]),\n            ('value__c__lte', 14, [self.objs[3], self.objs[4]]),\n            ('value__c__gt', 13, [self.objs[3], self.objs[4]]),\n            ('value__c__gte', 14, [self.objs[3], self.objs[4]]),\n        ]\n        for lookup, value, expected in tests:\n            with self.subTest(lookup=lookup, value=value):\n                self.assertSequenceEqual(\n                    NullableJSONModel.objects.filter(**{lookup: value}),\n                    expected,\n                )\n", "    def test_key_transform_preprocess_lhs(self):\n        key_transform = KeyTransform('name', 'value')\n        class MockCompiler:\n                return value, []\n        lhs, params, key_transforms = key_transform.preprocess_lhs(MockCompiler(), connection)\n        self.assertEqual(lhs, 'value')\n        self.assertEqual(params, [])\n        self.assertEqual(key_transforms, ['name'])\n"], "sample_215": ["    def test_cleansing_of_callable_settings(self):\n        \"\"\"\n        Ensure that callable settings are wrapped with CallableSettingWrapper\n        without being executed.\n        \"\"\"\n            raise RuntimeError(\"This should not be called\")\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed = reporter_filter.cleanse_setting('CALLABLE_SETTING', sensitive_callable)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed), repr(sensitive_callable))\n", "    def test_cleanse_setting_callable(self):\n        \"\"\"Callable settings should be wrapped properly.\"\"\"\n            return \"secret\"\n\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed = reporter_filter.cleanse_setting('CALLABLE_SETTING', my_callable)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed), repr(my_callable))\n", "    def test_technical_404_response(self):\n        \"\"\"Test the technical_404_response function.\"\"\"\n        request = self.rf.get('/nonexistent_path/')\n        exception = Http404(\"Page not found\")\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, '<title>Page not found at /nonexistent_path/</title>', status_code=404)\n        self.assertContains(response, 'Page not found', status_code=404)\n        self.assertContains(response, 'Django tried these URL patterns', status_code=404)\n        self.assertContains(response, '<code>/nonexistent_path/</code>', status_code=404)\n", "    def test_cleanse_setting_callable(self):\n        \"\"\"\n        Ensure that callables are wrapped in CallableSettingWrapper.\n        \"\"\"\n        reporter_filter = SafeExceptionReporterFilter()\n        initial = {'password': 'secret', 'callable': lambda: \"callable\"}\n        cleansed = reporter_filter.cleanse_setting('SETTING_NAME', initial)\n        self.assertIsInstance(cleansed['callable'], CallableSettingWrapper)\n        self.assertEqual(cleansed['password'], reporter_filter.cleansed_substitute)\n", "    def test_cleansed_substitute(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        self.assertEqual(reporter_filter.cleansed_substitute, '********************')\n"], "sample_216": ["    def test_field_is_referenced(self):\n        \"\"\"Test if a field is referenced by any state models.\"\"\"\n        state = ProjectState()\n        author_state = ModelState('testapp', 'Author', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        book_state = ModelState('otherapp', 'Book', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n        ])\n        state.add_model(author_state)\n        state.add_model(book_state)\n        \n        # Check if 'author' field in 'Book' model references 'Author' model.\n        self.assertTrue(field_is_referenced(state, ('testapp', 'author'), ('otherapp', 'author')))\n        \n        # Check if 'id' field in 'Author' model is referenced in 'Book' model.\n        self.assertTrue(field_is_referenced(state, ('testapp', 'author'), ('testapp', 'id')))\n        \n        # Negative test: 'title' field does not exist in 'Book' model.\n        self.assertFalse(field_is_referenced(state, ('testapp', 'author'), ('otherapp', 'title')))\n", "    def test_field_references_recursive_relationship(self):\n        \"\"\"Tests field_references function with recursive relationships.\"\"\"\n        class FakeModel:\n            _meta = type('Meta', (), {'app_label': 'testapp', 'model_name': 'author'})\n        \n        class FakeField:\n            remote_field = type('RemoteField', (), {'model': RECURSIVE_RELATIONSHIP_CONSTANT, 'through': None})\n        \n        model_tuple = ('testapp', 'author')\n        reference_model_tuple = ('testapp', 'author')\n        \n        # Test with recursive relationship and missing app_label/model_name\n        with self.assertRaises(TypeError):\n            field_references(model_tuple, FakeField(), reference_model_tuple)\n        \n        # Test with recursive relationship and provided app_label/model_name\n        resolve_relation_mock = mock.patch('django.db.models.fields.related.RECURSIVE_RELATIONSHIP_CONSTANT', new_callable=mock.PropertyMock)\n        with resolve_relation_mock as mocked_constant:\n            mocked_constant.return_value = RECURSIVE_RELATIONSHIP_CONSTANT\n            field = FakeField()\n            field.remote_field.model = RECURSIVE_RELATIONSHIP_CONSTANT\n            result = field_references(model_tuple, field, reference_model_tuple, app_label='testapp', model_name='author')\n            self.assertEqual(result, FieldReference((field.remote_field, None), None))\n", "def test_resolve_relation_recursive_relation_no_app_label_model_name(self):\n    \"\"\"Test resolve_relation raises TypeError for recursive relation with no app_label or model_name.\"\"\"\n    with self.assertRaises(TypeError) as context:\n        resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT)\n    self.assertEqual(\n        str(context.exception),\n        'app_label and model_name must be provided to resolve recursive relationships.'\n    )\n", "    def test_resolve_relation(self):\n        \"\"\"Test resolve_relation for various input scenarios.\"\"\"\n        class MockModel:\n            class _meta:\n                app_label = \"mockapp\"\n                model_name = \"mockmodel\"\n\n        mock_model = MockModel()\n        \n        self.assertEqual(resolve_relation(\"mockapp.MockModel\"), (\"mockapp\", \"mockmodel\"))\n        self.assertEqual(resolve_relation(\"MockModel\", \"mockapp\"), (\"mockapp\", \"mockmodel\"))\n        self.assertEqual(resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, \"mockapp\", \"mockmodel\"), (\"mockapp\", \"mockmodel\"))\n        self.assertEqual(resolve_relation(mock_model), (\"mockapp\", \"mockmodel\"))\n\n        with self.assertRaises(TypeError):\n            resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT)\n        \n        with self.assertRaises(TypeError):\n            resolve_relation(\"MockModel\")\n", "def test_resolve_relation_recursive_relationship_constant():\n    \"\"\"\n    Test resolve_relation function with RECURSIVE_RELATIONSHIP_CONSTANT.\n    \"\"\"\n    with self.assertRaises(TypeError):\n        resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT)\n    self.assertEqual(\n        resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, 'app_label', 'model_name'),\n        ('app_label', 'model_name')\n    )\n"], "sample_217": ["    def test_input_widget_type_override(self):\n        # Test that the input_type can be overridden by passing 'type' in attrs.\n        widget = Input(attrs={'type': 'email'})\n        self.assertEqual(widget.input_type, 'email')\n        self.assertEqual(widget.get_context('name', 'value', {})['widget']['type'], 'email')\n\n        widget = Input(attrs={'type': 'number'})\n        self.assertEqual(widget.input_type, 'number')\n        self.assertEqual(widget.get_context('name', 'value', {})['widget']['type'], 'number')\n\n        # Ensure it falls back to the default input_type if not provided in attrs.\n        widget = TextInput()\n        self.assertEqual(widget.input_type, 'text')\n        self.assertEqual(widget.get_context('name', 'value', {})['widget']['type'], 'text')\n", "    def test_widget_deepcopy(self):\n        ###############################################################\n        # Testing deepcopy of widgets\n        ###############################################################\n\n        # A widget should be able to deepcopy itself including its attributes\n        widget = TextInput(attrs={'class': 'my-class', 'id': 'my-id'})\n        widget_copy = copy.deepcopy(widget)\n        \n        self.assertIsNot(widget, widget_copy)\n        self.assertEqual(widget.attrs, widget_copy.attrs)\n        self.assertIsNot(widget.attrs, widget_copy.attrs)\n\n        # Ensure that the original widget's attributes are not affected\n        widget.attrs['class'] = 'new-class'\n        self.assertNotEqual(widget.attrs['class'], widget_copy.attrs['class'])\n", "    def test_media_with_no_files(self):\n        # Test Media object initialized with no files.\n        m = Media()\n        self.assertEqual(str(m), '')\n        self.assertEqual(repr(m), \"Media(css={}, js=[])\")\n        self.assertEqual(m._css, {})\n        self.assertEqual(m._js, [])\n\n        # Test Media object initialized with empty lists.\n        m = Media(css={}, js=[])\n        self.assertEqual(str(m), '')\n        self.assertEqual(repr(m), \"Media(css={}, js=[])\")\n        self.assertEqual(m._css, {})\n        self.assertEqual(m._js, [])\n\n        # Test Media object initialized with empty dictionary and lists.\n        m = Media(css={}, js=[])\n        self.assertEqual(str(m), '')\n        self.assertEqual(repr(m), \"Media(css={}, js=[])\")\n        self.assertEqual(m._css, {})\n        self.assertEqual(m._js, [])\n\n        # Test Media object initialized with empty media attribute.\n        class Foo:\n            css = {}\n            js = []\n\n        m = Media(Foo)\n        self.assertEqual(str(m), '')\n        self.assertEqual(repr(m), \"Media(css={}, js=[])\")\n        self.assertEqual(m._css, {})\n        self.assertEqual(m._js, [])\n", "    def test_media_repr_str(self):\n        \"\"\"Test the __repr__ and __str__ methods of the Media class.\"\"\"\n\n        # Test __repr__ method\n        media1 = Media(\n            css={'all': ['path/to/css1', 'path/to/css2']},\n            js=['path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3'],\n        )\n        expected_repr = \"Media(css={'all': ['path/to/css1', 'path/to/css2']}, js=['path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3'])\"\n        self.assertEqual(repr(media1), expected_repr)\n\n        # Test __str__ method\n        expected_str = \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_clearable_file_input_initial(self):\n        \"\"\"\n        Test the behavior of ClearableFileInput with an initial file value.\n        \"\"\"\n        class MyForm(Form):\n            file_field = CharField(widget=ClearableFileInput())\n\n        form = MyForm()\n        context = form['file_field'].as_widget(attrs={'value': 'testfile.txt'})\n        self.assertIn('Currently', context)\n        self.assertIn('Change', context)\n        self.assertIn('Clear', context)\n"], "sample_218": ["    def test_now_function(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        now = DTModel.objects.annotate(now=Now()).first().now\n        self.assertIsInstance(now, datetime)\n        \n        if settings.USE_TZ:\n            self.assertTrue(timezone.is_aware(now))\n        else:\n            self.assertTrue(timezone.is_naive(now))\n\n        self.assertGreaterEqual(now, start_datetime)\n        self.assertGreaterEqual(now, end_datetime)\n", "    def test_extract_minute_with_timezone(self):\n        start_datetime = datetime(2015, 6, 15, 23, 45, 50, 321)\n        end_datetime = datetime(2015, 6, 16, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        melb = pytz.timezone('Australia/Melbourne')\n        tokyo = pytz.timezone('Asia/Tokyo')\n\n        qs = DTModel.objects.annotate(\n            minute=ExtractMinute('start_datetime'),\n            minute_melb=ExtractMinute('start_datetime', tzinfo=melb),\n            minute_tokyo=ExtractMinute('start_datetime', tzinfo=tokyo),\n        ).order_by('start_datetime')\n\n        model = qs.get()\n        self.assertEqual(model.minute, 45)\n        self.assertEqual(model.minute_melb, 45)\n        self.assertEqual(model.minute_tokyo, 45)\n\n        with timezone.override(melb):\n            melb_model = qs.get()\n            self.assertEqual(melb_model.minute, 45)\n            self.assertEqual(melb_model.minute_melb, 45)\n            self.assertEqual(melb_model.minute_tokyo, 45)\n\n        with timezone.override(tokyo):\n            tokyo_model = qs.get()\n            self.assertEqual(tokyo_model.minute, 45)\n            self.assertEqual(tokyo_model.minute_melb, 45)\n            self.assertEqual(tokyo_model.minute_tokyo, 45)\n", "    def test_now_function(self):\n        now = datetime(2020, 1, 1, 12, 0, 0)\n        if settings.USE_TZ:\n            now = timezone.make_aware(now, is_dst=False)\n        \n        with mock.patch('django.utils.timezone.now', return_value=now):\n            obj = DTModel.objects.create(name=\"Test Now\", start_datetime=timezone.now())\n            self.assertEqual(obj.start_datetime, now)\n            self.assertQuerysetEqual(\n                DTModel.objects.annotate(current_time=Now()).order_by('start_datetime'),\n                [(obj.start_datetime, now)],\n                lambda m: (m.start_datetime, m.current_time)\n            )\n", "    def test_extract_with_different_timezones(self):\n        \"\"\"\n        Test Extract function with various timezones to ensure proper\n        timezone handling and conversion in the extraction.\n        \"\"\"\n        start_datetime = datetime(2023, 3, 10, 10, 45, 30)\n        end_datetime = datetime(2023, 9, 25, 22, 15, 45)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        # Define various timezones\n        ny_tz = pytz.timezone('America/New_York')\n        tokyo_tz = pytz.timezone('Asia/Tokyo')\n        sydney_tz = pytz.timezone('Australia/Sydney')\n\n        qs = DTModel.objects.annotate(\n            ny_hour=ExtractHour('start_datetime', tzinfo=ny_tz),\n            tokyo_hour=ExtractHour('start_datetime', tzinfo=tokyo_tz),\n            sydney_hour=ExtractHour('start_datetime', tzinfo=sydney_tz),\n        ).order_by('start_datetime')\n\n        model = qs.get()\n\n        self.assertEqual(model.ny_hour, start_datetime.astimezone(ny_tz).hour)\n        self.assertEqual(model.tokyo_hour, start_datetime.astimezone(tokyo_tz).hour)\n        self.assertEqual(model.sydney_hour, start_datetime.astimezone(sydney_tz).hour)\n\n        with timezone.override(ny_tz):\n            model_ny = qs.get()\n            self.assertEqual(model_ny.ny_hour, start_datetime.astimezone(ny_tz).hour)\n\n        with timezone.override(tokyo_tz):\n            model_tokyo = qs.get()\n            self.assertEqual(model_tokyo.tokyo_hour, start_datetime.astimezone(tokyo_tz).hour)\n\n        with timezone.override(sydney_tz):\n            model_sydney = qs.get()\n            self.assertEqual(model_sydney.sydney_hour, start_datetime.astimezone(sydney_tz).hour)\n", "    def test_extract_month_func_boundaries(self):\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n\n        last_month_2014 = datetime(2014, 12, 31, 13, 0)\n        first_month_2015 = datetime(2015, 1, 1, 13, 0)\n        if settings.USE_TZ:\n            last_month_2014 = timezone.make_aware(last_month_2014, is_dst=False)\n            first_month_2015 = timezone.make_aware(first_month_2015, is_dst=False)\n        dates = [last_month_2014, first_month_2015]\n        self.create_model(last_month_2014, end_datetime)\n        self.create_model(first_month_2015, end_datetime)\n        qs = DTModel.objects.filter(start_datetime__in=dates).annotate(\n            extracted=ExtractMonth('start_datetime'),\n        ).order_by('start_datetime')\n        self.assertQuerysetEqual(qs, [\n            (last_month_2014, 12),\n            (first_month_2015, 1),\n        ], lambda m: (m.start_datetime, m.extracted))\n"], "sample_219": ["    def test_as_sqlite_with_decimal_field(self):\n        expr = CombinedExpression(Value(1, output_field=DecimalField()), Combinable.ADD, Value(1, output_field=DecimalField()))\n        compiler = mock.Mock()\n        connection = mock.Mock()\n        connection.ops.combine_expression = lambda connector, expressions: ' + '.join(expressions)\n        connection.vendor = 'sqlite'\n        sql, params = expr.as_sqlite(compiler, connection)\n        self.assertEqual(sql, 'CAST((%s + %s) AS NUMERIC)')\n        self.assertEqual(params, [1, 1])\n", "    def test_combined_expression_sqlite_numeric_cast(self):\n        # Ensure SQLiteNumericMixin correctly casts to NUMERIC for DecimalField\n        expr = CombinedExpression(\n            Expression(DecimalField(max_digits=5, decimal_places=2)),\n            Combinable.ADD,\n            Expression(DecimalField(max_digits=5, decimal_places=2)),\n        )\n        compiler = connection.ops.compiler('SQLCompiler')(Query(model=None), connection, 'default')\n        sql, params = expr.as_sqlite(compiler, connection)\n        self.assertTrue(sql.startswith('CAST('))\n        self.assertIn('AS NUMERIC', sql)\n", "    def test_rawsql_as_sql(self):\n        rawsql = RawSQL(\"SELECT 1\", [])\n        self.assertEqual(rawsql.as_sql(None, connection), (\"(SELECT 1)\", []))\n", "    def test_as_sqlite_with_decimal_field(self):\n        class DecimalFieldExpression(SQLiteNumericMixin, Expression):\n                super().__init__()\n                self.output_field = DecimalField()\n\n                return 'mock_sql', []\n\n        expr = DecimalFieldExpression()\n        sql, params = expr.as_sqlite(None, connection)\n        self.assertEqual(sql, 'CAST(mock_sql AS NUMERIC)')\n        self.assertEqual(params, [])\n", "    def test_as_sqlite_casting(self):\n        expr = CombinedExpression(Value(1, output_field=DecimalField()), CombinedExpression.ADD, Value(1))\n        compiler = mock.Mock()\n        connection = mock.Mock()\n        connection.vendor = 'sqlite'\n        sql, params = expr.as_sql(compiler, connection)\n        self.assertIn('CAST', sql)\n        self.assertIn('AS NUMERIC', sql)\n"], "sample_220": ["    def test_serialize_headers(self):\n        \"\"\"Test serialization of HTTP headers to bytestring.\"\"\"\n        response = HttpResponse(content='Hello, World!', content_type='text/plain')\n        response['X-Test-Header'] = 'Test'\n        expected_headers = b'Content-Type: text/plain; charset=utf-8\\r\\nX-Test-Header: Test'\n        self.assertEqual(response.serialize_headers(), expected_headers)\n", "    def test_init_with_status_code(self):\n        \"\"\"HttpResponse initializes with the given status code.\"\"\"\n        response = HttpResponse(status=404)\n        self.assertEqual(response.status_code, 404)\n", "    def test_set_signed_cookie(self):\n        \"\"\"HttpResponse.set_signed_cookie() correctly signs the cookie value.\"\"\"\n        response = HttpResponse()\n        response.set_signed_cookie('signed_cookie', 'value', salt='salt')\n        signed_cookie_value = response.cookies['signed_cookie'].value\n        self.assertNotEqual(signed_cookie_value, 'value')\n        self.assertTrue(signed_cookie_value.startswith('value:'))\n", "    def test_default_status_code(self):\n        \"\"\"HttpResponse uses status code 200 by default.\"\"\"\n        response = HttpResponse()\n        self.assertEqual(response.status_code, 200)\n", "    def test_content_setter(self):\n        \"\"\"HttpResponse content setter encodes content properly.\"\"\"\n        response = HttpResponse()\n        response.content = \"Hello, World!\"\n        self.assertEqual(response.content, b\"Hello, World!\")\n"], "sample_221": ["    def setUpTestData(cls):\n        cls.group1 = Group.objects.create(name=\"Group 1\")\n        cls.group2 = Group.objects.create(name=\"Group 2\")\n        cls.event1 = Event.objects.create(title=\"Event 1\", group=cls.group1)\n        cls.event2 = Event.objects.create(title=\"Event 2\", group=cls.group2)\n", "    def setUpTestData(cls):\n        cls.group = Group.objects.create(name='Group 1')\n        cls.event1 = Event.objects.create(title='Event 1', group=cls.group)\n        cls.event2 = Event.objects.create(title='Event 2', group=cls.group)\n", "    def setUpTestData(cls):\n        cls.group1 = Group.objects.create(name='Group 1')\n        cls.group2 = Group.objects.create(name='Group 2')\n        cls.event1 = Event.objects.create(title='Event 1', group=cls.group1)\n        cls.event2 = Event.objects.create(title='Event 2', group=cls.group2)\n", "def test_empty_queryset(self):\n    \"\"\"Ensure that EmptyQuerySet behaves correctly when pickled and unpickled.\"\"\"\n    empty_qs = Event.objects.none()\n    self.assert_pickles(empty_qs)\n    self.assertIsInstance(empty_qs, EmptyQuerySet)\n    unpickled_qs = pickle.loads(pickle.dumps(empty_qs))\n    self.assertIsInstance(unpickled_qs, EmptyQuerySet)\n    self.assertEqual(list(unpickled_qs), list(empty_qs))\n", "def test_queryset_count(self):\n    \"\"\"\n    Test that the count method returns the correct number of objects\n    in the QuerySet.\n    \"\"\"\n    Group.objects.create(name='group1')\n    Group.objects.create(name='group2')\n    groups = Group.objects.all()\n    self.assertEqual(groups.count(), 2)\n    groups = Group.objects.filter(name='group1')\n    self.assertEqual(groups.count(), 1)\n"], "sample_222": ["    def test_lock_unlock_file(self):\n        file_path = tempfile.mktemp()\n        with open(file_path, 'w') as f:\n            f.write(\"Test content\")\n\n        with open(file_path, 'r') as f:\n            # Try to lock the file exclusively\n            self.assertTrue(locks.lock(f, locks.LOCK_EX))\n            # Try to unlock the file\n            self.assertTrue(locks.unlock(f))\n\n        os.remove(file_path)\n", "    def test_lock_unlock(self):\n        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n            try:\n                # Test exclusive lock\n                self.assertTrue(locks.lock(tmp_file, locks.LOCK_EX))\n                self.assertTrue(locks.unlock(tmp_file))\n                \n                # Test shared lock\n                self.assertTrue(locks.lock(tmp_file, locks.LOCK_SH))\n                self.assertTrue(locks.unlock(tmp_file))\n                \n                # Test non-blocking exclusive lock\n                self.assertTrue(locks.lock(tmp_file, locks.LOCK_EX | locks.LOCK_NB))\n                self.assertTrue(locks.unlock(tmp_file))\n                \n                # Test non-blocking shared lock\n                self.assertTrue(locks.lock(tmp_file, locks.LOCK_SH | locks.LOCK_NB))\n                self.assertTrue(locks.unlock(tmp_file))\n            finally:\n                os.unlink(tmp_file.name)\n", "    def test_lock_unlock_nonexistent_file(self):\n        with self.assertRaises(FileNotFoundError):\n            with open('nonexistent_file.txt', 'rb') as f:\n                locks.lock(f, locks.LOCK_EX)\n        with self.assertRaises(FileNotFoundError):\n            with open('nonexistent_file.txt', 'rb') as f:\n                locks.unlock(f)\n", "    def test_lock_unlock(self):\n        \"\"\"Test basic lock and unlock functionality.\"\"\"\n        with tempfile.NamedTemporaryFile() as temp_file:\n            self.assertTrue(locks.lock(temp_file, locks.LOCK_EX))\n            self.assertTrue(locks.unlock(temp_file))\n", "    def test_lock_unlock_cycle(self):\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path) as f:\n            self.assertTrue(locks.lock(f, locks.LOCK_EX))\n            self.assertTrue(locks.unlock(f))\n            self.assertTrue(locks.lock(f, locks.LOCK_SH))\n            self.assertTrue(locks.unlock(f))\n"], "sample_223": ["    def test_values_list_and_values_with_subquery(self):\n        \"\"\"\n        Ensure that values_list and values can be combined with subqueries correctly.\n        \"\"\"\n        qs1 = Tag.objects.values_list('name', flat=True)\n        qs2 = Tag.objects.values('name')\n        \n        self.assertQuerysetEqual(\n            Tag.objects.filter(name__in=qs1),\n            Tag.objects.all(),\n            transform=lambda x: x.name,\n            ordered=False\n        )\n        \n        self.assertQuerysetEqual(\n            Tag.objects.filter(name__in=qs2),\n            Tag.objects.all(),\n            transform=lambda x: x.name,\n            ordered=False\n        )\n", "    def test_queryset_clone(self):\n        \"\"\"\n        Test the _clone method of QuerySet to ensure that it correctly clones\n        all aspects of the QuerySet.\n        \"\"\"\n        # Create initial objects\n        cat1 = NamedCategory.objects.create(name=\"Cat1\")\n        cat2 = NamedCategory.objects.create(name=\"Cat2\")\n        tag1 = Tag.objects.create(name='tag1', category=cat1)\n        tag2 = Tag.objects.create(name='tag2', category=cat2)\n\n        # Create initial queryset\n        queryset = Tag.objects.filter(name__startswith='tag').select_related('category')\n\n        # Clone the queryset\n        cloned_queryset = queryset._clone()\n\n        # Ensure the cloned queryset is identical to the original\n        self.assertEqual(list(queryset), list(cloned_queryset))\n        self.assertEqual(queryset.query.get_compiler(queryset.db).as_sql(), cloned_queryset.query.get_compiler(cloned_queryset.db).as_sql())\n\n        # Ensure modifications to the cloned queryset do not affect the original\n        cloned_queryset = cloned_queryset.filter(category=cat1)\n        self.assertNotEqual(queryset.query.get_compiler(queryset.db).as_sql(), cloned_queryset.query.get_compiler(cloned_queryset.db).as_sql())\n        self.assertEqual(list(queryset), [tag1, tag2])\n        self.assertEqual(list(cloned_queryset), [tag1])\n", "    def setUpTestData(cls):\n        cls.category = NamedCategory.objects.create(name=\"TestCategory\")\n        cls.tag1 = Tag.objects.create(name='tag1', category=cls.category)\n        cls.tag2 = Tag.objects.create(name='tag2', category=cls.category)\n        cls.tag3 = Tag.objects.create(name='tag3', category=cls.category)\n", "    def test_annotate(self):\n        \"\"\"\n        Test that the annotate() method correctly adds annotations to the queryset.\n        \"\"\"\n        Note.objects.create(note='note1', misc='misc1')\n        Note.objects.create(note='note2', misc='misc2')\n        queryset = Note.objects.annotate(note_count=Count('note'))\n        self.assertEqual(queryset.count(), 2)\n        for note in queryset:\n            self.assertEqual(note.note_count, 1)\n", "    def test_queryset_repr(self):\n        item1 = Item.objects.create(name='item1', created=datetime.datetime.now(), creator=self.a1, note=self.n1)\n        item2 = Item.objects.create(name='item2', created=datetime.datetime.now(), creator=self.a1, note=self.n1)\n        expected_repr = f'<QuerySet [<Item: {item1}>, <Item: {item2}>]>'\n        self.assertEqual(repr(Item.objects.all()), expected_repr)\n"], "sample_224": ["    def test_values_list_annotation(self):\n        \"\"\"\n        Test the ValuesListIterable by using values_list() on a QuerySet.\n        Ensure that it yields a tuple for each row.\n        \"\"\"\n        books = (\n            Book.objects\n            .filter(rating__gte=4.0)\n            .values_list('name', 'rating', 'price')\n            .order_by('name')\n        )\n        self.assertEqual(\n            list(books), [\n                ('Artificial Intelligence: A Modern Approach', 4.0, Decimal('82.80')),\n                ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 5.0, Decimal('75.00')),\n                ('Practical Django Projects', 4.0, Decimal('29.69')),\n                ('Python Web Development with Django', 4.0, Decimal('29.69')),\n                ('The Definitive Guide to Django: Web Development Done Right', 4.5, Decimal('30.00'))\n            ]\n        )\n", "    def test_bulk_create(self):\n        new_authors = [\n            Author(name='Author 1', age=30),\n            Author(name='Author 2', age=40),\n            Author(name='Author 3', age=50),\n        ]\n        created_authors = Author.objects.bulk_create(new_authors)\n        self.assertEqual(Author.objects.count(), 12)  # 9 existing + 3 new authors\n        self.assertEqual(created_authors[0].name, 'Author 1')\n        self.assertEqual(created_authors[1].age, 40)\n        self.assertEqual(created_authors[2].name, 'Author 3')\n", "def test_values_list_iterable(self):\n    authors = Author.objects.values_list('name', flat=True).order_by('name')\n    self.assertQuerysetEqual(\n        authors, [\n            'Adrian Holovaty',\n            'Brad Dayley',\n            'Jacob Kaplan-Moss',\n            'James Bennett',\n            'Jeffrey Forcier',\n            'Paul Bissex',\n            'Peter Norvig',\n            'Stuart Russell',\n            'Wesley J. Chun'\n        ],\n        lambda x: x\n    )\n\n    books = Book.objects.values_list('name', 'rating').order_by('rating', 'name')\n    self.assertQuerysetEqual(\n        books, [\n            ('Sams Teach Yourself Django in 24 Hours', 3.0),\n            ('Artificial Intelligence: A Modern Approach', 4.0),\n            ('Practical Django Projects', 4.0),\n            ('Python Web Development with Django', 4.0),\n            ('The Definitive Guide to Django: Web Development Done Right', 4.5),\n            ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 5.0),\n        ],\n        lambda x: x\n    )\n", "def test_get_or_create(self):\n    # Test get_or_create when the object already exists\n    author, created = Author.objects.get_or_create(name='Adrian Holovaty', age=34)\n    self.assertFalse(created)\n    self.assertEqual(author.name, 'Adrian Holovaty')\n    self.assertEqual(author.age, 34)\n\n    # Test get_or_create when the object does not exist\n    new_author, created = Author.objects.get_or_create(name='New Author', defaults={'age': 40})\n    self.assertTrue(created)\n    self.assertEqual(new_author.name, 'New Author')\n    self.assertEqual(new_author.age, 40)\n\n    # Test get_or_create when the object does not exist and no defaults provided\n    another_author, created = Author.objects.get_or_create(name='Another Author', age=50)\n    self.assertTrue(created)\n    self.assertEqual(another_author.name, 'Another Author')\n    self.assertEqual(another_author.age, 50)\n", "    def test_queryset_order_by(self):\n        \"\"\"\n        Test the QuerySet's order_by method to ensure it orders correctly.\n        \"\"\"\n        books_ordered_by_rating = Book.objects.order_by('rating')\n        self.assertQuerysetEqual(\n            books_ordered_by_rating, [\n                \"Sams Teach Yourself Django in 24 Hours\",\n                \"Practical Django Projects\",\n                \"Python Web Development with Django\",\n                \"Artificial Intelligence: A Modern Approach\",\n                \"The Definitive Guide to Django: Web Development Done Right\",\n                \"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\"\n            ],\n            lambda b: b.name\n        )\n\n        books_ordered_by_pubdate_desc = Book.objects.order_by('-pubdate')\n        self.assertQuerysetEqual(\n            books_ordered_by_pubdate_desc, [\n                \"Python Web Development with Django\",\n                \"Practical Django Projects\",\n                \"Sams Teach Yourself Django in 24 Hours\",\n                \"The Definitive Guide to Django: Web Development Done Right\",\n                \"Artificial Intelligence: A Modern Approach\",\n                \"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\"\n            ],\n            lambda b: b.name\n        )\n"], "sample_225": ["    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n"], "sample_226": ["    def test_serialize_deserialize(self):\n        # Create initial objects\n        obj_1 = Object.objects.create(name='Object 1')\n        obj_2 = Object.objects.create(name='Object 2', reference=obj_1)\n        \n        # Serialize current database state to string\n        serialized_data = connection.creation.serialize_db_to_string()\n        \n        # Delete the objects to ensure we are deserializing correctly\n        Object.objects.all().delete()\n        \n        # Deserialize the database state from string\n        connection.creation.deserialize_db_from_string(serialized_data)\n        \n        # Verify objects have been correctly restored\n        obj_1_restored = Object.objects.get(name='Object 1')\n        obj_2_restored = Object.objects.get(name='Object 2')\n        self.assertEqual(obj_1_restored.name, 'Object 1')\n        self.assertEqual(obj_2_restored.reference, obj_1_restored)\n", "    def test_create_test_db(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'), \\\n                 mock.patch.object(creation, 'log'), \\\n                 mock.patch.object(creation, 'serialize_db_to_string', return_value='{}'), \\\n                 mock.patch('django.core.management.call_command') as mock_call_command:\n                test_db_name = creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n                self.assertEqual(test_db_name, creation._get_test_db_name())\n                mock_call_command.assert_any_call('migrate', verbosity=0, interactive=False, database=test_connection.alias, run_syncdb=True)\n                mock_call_command.assert_any_call('createcachetable', database=test_connection.alias)\n                self.assertEqual(test_connection.settings_dict[\"NAME\"], test_db_name)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        with mock.patch.object(creation, '_clone_test_db') as mock_clone_test_db:\n            creation.clone_test_db(suffix='clone', verbosity=1, autoclobber=True, keepdb=False)\n            mock_clone_test_db.assert_called_once_with('clone', verbosity=1, keepdb=False)\n", "    def test_create_test_db(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        test_connection.settings_dict['TEST']['NAME'] = 'test_db'\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        \n        with mock.patch.object(creation, '_create_test_db') as mocked_create, \\\n             mock.patch.object(creation, 'log') as mocked_log, \\\n             mock.patch('django.core.management.call_command') as mocked_call_command:\n            creation.create_test_db(verbosity=2, autoclobber=True, serialize=True, keepdb=False)\n            \n            # Check that _create_test_db was called\n            mocked_create.assert_called_once_with(2, True, False)\n            \n            # Check the correct messages were logged\n            mocked_log.assert_any_call(\"Creating test database for alias 'default' ('test_db')...\")\n            \n            # Check that migrate was called\n            mocked_call_command.assert_any_call('migrate', verbosity=1, interactive=False, database='default', run_syncdb=True)\n            \n            # Check that createcachetable was called\n            mocked_call_command.assert_any_call('createcachetable', database='default')\n            \n            # Ensure that the serialized contents are stored on the connection\n            self.assertTrue(hasattr(test_connection, '_test_serialized_contents'))\n", "    def test_create_test_db_calls_internal_methods(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'test_db'\n        creation = BaseDatabaseCreation(test_connection)\n\n        with mock.patch.object(creation, '_get_test_db_name', return_value='test_db') as mock_get_test_db_name, \\\n             mock.patch.object(creation, '_create_test_db') as mock_create_test_db, \\\n             mock.patch.object(creation, 'serialize_db_to_string', return_value='{}') as mock_serialize_db, \\\n             mock.patch('django.core.management.call_command') as mock_call_command:\n\n            creation.create_test_db(verbosity=2, autoclobber=True, serialize=True, keepdb=False)\n\n            mock_get_test_db_name.assert_called_once()\n            mock_create_test_db.assert_called_once_with(2, True, False)\n            mock_serialize_db.assert_called_once()\n            mock_call_command.assert_any_call('migrate', verbosity=1, interactive=False, database=test_connection.alias, run_syncdb=True)\n            mock_call_command.assert_any_call('createcachetable', database=test_connection.alias)\n            "], "sample_227": ["    def test_genericforeignkey_get_filter_kwargs_for_object(self):\n        \"\"\"\n        Test the get_filter_kwargs_for_object method of GenericForeignKey.\n        \"\"\"\n        class SampleModel(models.Model):\n            content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n            object_id = models.PositiveIntegerField()\n            generic_foreign = GenericForeignKey('content_type', 'object_id')\n\n        # Create instances of ContentType and SampleModel\n        content_type = ContentType.objects.create(app_label='app', model='model')\n        sample_obj = SampleModel.objects.create(content_type=content_type, object_id=1)\n\n        gfk = sample_obj._meta.get_field('generic_foreign')\n\n        # Test get_filter_kwargs_for_object method\n        filter_kwargs = gfk.get_filter_kwargs_for_object(sample_obj)\n        self.assertEqual(filter_kwargs, {'content_type': content_type, 'object_id': 1})\n", "def test_genericforeignkey_get_filter_kwargs_for_object(self):\n    \"\"\"\n    Test that get_filter_kwargs_for_object method of GenericForeignKey returns\n    the correct filter kwargs.\n    \"\"\"\n    class TaggedItem(models.Model):\n        tag = models.CharField(max_length=30)\n        content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n        object_id = models.PositiveIntegerField()\n        content_object = GenericForeignKey('content_type', 'object_id')\n\n    class Note(models.Model):\n        message = models.TextField()\n\n    note = Note.objects.create(message=\"This is a test note.\")\n    content_type = ContentType.objects.get_for_model(Note)\n    tagged_item = TaggedItem.objects.create(tag=\"test\", content_type=content_type, object_id=note.pk)\n\n    generic_fk = GenericForeignKey('content_type', 'object_id')\n    expected_filter_kwargs = {\n        'object_id': note.pk,\n        'content_type': content_type.pk,\n    }\n    self.assertEqual(generic_fk.get_filter_kwargs_for_object(tagged_item), expected_filter_kwargs)\n", "    def test_generic_foreign_key(self):\n        class ModelWithGenericFK(models.Model):\n            content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n            object_id = models.PositiveIntegerField()\n            content_object = GenericForeignKey('content_type', 'object_id')\n\n        class RelatedModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        related_instance = RelatedModel.objects.create(name=\"Related Instance\")\n        content_type = ContentType.objects.get_for_model(related_instance)\n        instance = ModelWithGenericFK.objects.create(content_type=content_type, object_id=related_instance.pk)\n\n        self.assertEqual(instance.content_object, related_instance)\n\n        new_related_instance = RelatedModel.objects.create(name=\"New Related Instance\")\n        instance.content_object = new_related_instance\n        instance.save()\n\n        self.assertEqual(instance.content_object, new_related_instance)\n        self.assertEqual(instance.content_type, ContentType.objects.get_for_model(new_related_instance))\n        self.assertEqual(instance.object_id, new_related_instance.pk)\n", "    def test_genericforeignkey_get_forward_related_filter(self):\n        from django.contrib.contenttypes.models import ContentType\n        from .models import Bookmark, TaggedItem\n\n        # Create a Bookmark instance\n        bookmark = Bookmark.objects.create(url='https://www.example.com/')\n        # Create a TaggedItem instance related to the Bookmark instance\n        content_type = ContentType.objects.get_for_model(bookmark)\n        tagged_item = TaggedItem.objects.create(content_object=bookmark, tag='example')\n\n        # Create an instance of GenericForeignKey and set the fields\n        generic_foreign_key = GenericForeignKey(ct_field='content_type', fk_field='object_id')\n        generic_foreign_key.model = TaggedItem\n\n        # Get the forward related filter\n        filter_kwargs = generic_foreign_key.get_forward_related_filter(bookmark)\n\n        # Verify the filter kwargs\n        self.assertEqual(filter_kwargs['content_type'], content_type.pk)\n        self.assertEqual(filter_kwargs['object_id'], bookmark.pk)\n", "def test_generic_foreign_key_get_forward_related_filter(self):\n    class DummyModel(models.Model):\n        content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n        object_id = models.PositiveIntegerField()\n        generic_field = GenericForeignKey('content_type', 'object_id')\n\n    class RelatedModel(models.Model):\n        name = models.CharField(max_length=100)\n\n    related_instance = RelatedModel.objects.create(name='Related')\n    dummy_instance = DummyModel.objects.create(\n        content_type=ContentType.objects.get_for_model(related_instance),\n        object_id=related_instance.pk\n    )\n\n    generic_field = dummy_instance._meta.get_field('generic_field')\n    filter_kwargs = generic_field.get_forward_related_filter(related_instance)\n\n    expected_filter_kwargs = {\n        'object_id': related_instance.pk,\n        'content_type': ContentType.objects.get_for_model(related_instance).pk,\n    }\n    self.assertEqual(filter_kwargs, expected_filter_kwargs)\n"], "sample_228": ["    def test_formset_initial_form_count(self):\n        \"\"\"\n        A FormSet's initial_form_count should return the correct number of forms\n        initialized with data.\n        \"\"\"\n        initial_data = [{'choice': 'Choice 1', 'votes': 10}, {'choice': 'Choice 2', 'votes': 20}]\n        formset = self.make_choiceformset(initial=initial_data, total_forms=len(initial_data))\n        self.assertEqual(formset.initial_form_count(), len(initial_data))\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Choice 1\"></li>", "    def test_management_form_validation(self):\n        \"\"\"\n        A FormSet's ManagementForm should validate correctly when given proper data\n        and raise a ValidationError when data is missing or tampered with.\n        \"\"\"\n        valid_data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1000',\n        }\n        formset = ChoiceFormSet(data=valid_data, auto_id=False, prefix='choices')\n        # ManagementForm should be valid and not raise a ValidationError\n        try:\n            formset.management_form.is_valid()\n        except ValidationError:\n            self.fail(\"management_form.is_valid() raised ValidationError unexpectedly!\")\n\n        invalid_data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            # Missing 'choices-MAX_NUM_FORMS'\n        }\n        formset = ChoiceFormSet(data=invalid_data, auto_id=False, prefix='choices')\n        with self.assertRaises(ValidationError):\n            formset.management_form.is_valid()\n", "    def test_add_prefix(self):\n        \"\"\"\n        The add_prefix method should correctly concatenate the prefix and index\n        with a hyphen.\n        \"\"\"\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(auto_id=False, prefix='choices')\n        self.assertEqual(formset.add_prefix(0), 'choices-0')\n        self.assertEqual(formset.add_prefix(1), 'choices-1')\n        self.assertEqual(formset.add_prefix('custom'), 'choices-custom')\n", "    def test_formset_initial_data_with_ordering(self):\n        \"\"\"\n        A FormSet can be prefilled with existing data and maintain the specified order.\n        \"\"\"\n        initial_data = [\n            {'choice': 'Calexico', 'votes': 100, 'ORDER': 2},\n            {'choice': 'Fergie', 'votes': 900, 'ORDER': 1},\n            {'choice': 'The Decemberists', 'votes': 500, 'ORDER': 3},\n        ]\n        ChoiceFormSet = formset_factory(Choice, can_order=True, extra=1)\n        formset = ChoiceFormSet(initial=initial_data, auto_id=False, prefix='choices')\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "    def test_custom_formset_prefix(self):\n        \"\"\"\n        Custom prefixes can be used to differentiate between multiple formsets\n        on the same page.\n        \"\"\"\n        data1 = {\n            'form1-TOTAL_FORMS': '2',\n            'form1-INITIAL_FORMS': '0',\n            'form1-MIN_NUM_FORMS': '0',\n            'form1-MAX_NUM_FORMS': '0',\n            'form1-0-choice': 'First Choice',\n            'form1-0-votes': '10',\n            'form1-1-choice': 'Second Choice',\n            'form1-1-votes': '20',\n        }\n        data2 = {\n            'form2-TOTAL_FORMS': '2',\n            'form2-INITIAL_FORMS': '0',\n            'form2-MIN_NUM_FORMS': '0',\n            'form2-MAX_NUM_FORMS': '0',\n            'form2-0-choice': 'Third Choice',\n            'form2-0-votes': '30',\n            'form2-1-choice': 'Fourth Choice',\n            'form2-1-votes': '40',\n        }\n        ChoiceFormSet1 = formset_factory(Choice)\n        ChoiceFormSet2 = formset_factory(Choice)\n        formset1 = ChoiceFormSet1(data1, auto_id=False, prefix='form1')\n        formset2 = ChoiceFormSet2(data2, auto_id=False, prefix='form2')\n        self.assertTrue(formset1.is_valid())\n        self.assertTrue(formset2.is_valid())\n        self.assertEqual([form.cleaned_data for form in formset1.forms], [{'votes': 10, 'choice': 'First Choice'}, {'votes': 20, 'choice': 'Second Choice'}])\n        self.assertEqual([form.cleaned_data for form in formset2.forms], [{'votes': 30, 'choice': 'Third Choice'}, {'votes': 40, 'choice': 'Fourth Choice'}])\n"], "sample_229": ["    def test_order_by_with_annotations(self):\n        qs1 = Number.objects.annotate(neg_num=F('num') * -1)\n        qs2 = Number.objects.annotate(neg_num=F('num') * -1)\n        union_qs = qs1.union(qs2)\n        ordered_qs = union_qs.order_by('neg_num')\n        self.assertNumbersEqual(ordered_qs, [9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n", "    def test_values_list_flat(self):\n        qs = Number.objects.filter(num__lt=5)\n        expected_values = [0, 1, 2, 3, 4]\n        self.assertEqual(list(qs.values_list('num', flat=True)), expected_values)\n", "    def test_empty_qs_operations(self):\n        qs = Number.objects.none()\n        self.assertEqual(qs.count(), 0)\n        with self.assertRaises(Number.DoesNotExist):\n            qs.get()\n        self.assertEqual(len(qs), 0)\n        self.assertFalse(qs.exists())\n        self.assertSequenceEqual(qs.values(), [])\n        self.assertSequenceEqual(qs.values_list(), [])\n        self.assertSequenceEqual(qs.order_by('num'), [])\n        self.assertSequenceEqual(qs.reverse(), [])\n", "def test_union_with_different_field_names(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = ReservedName.objects.all().values('name', 'order')\n    qs2 = ReservedName.objects.all().values(n='name', o='order')\n    result = list(qs1.union(qs2))\n    self.assertEqual(len(result), 1)\n    self.assertIn({'name': 'a', 'order': 2}, result)\n    self.assertIn({'n': 'a', 'o': 2}, result)\n", "    def test_exclude_with_combined_qs(self):\n        qs1 = Number.objects.filter(num__lte=5)\n        qs2 = Number.objects.filter(num__gte=2)\n        combined_qs = qs1.union(qs2)\n        msg = 'Calling QuerySet.exclude() after union() is not supported.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            combined_qs.exclude(num=4)\n"], "sample_230": ["    def test_initial_value(self):\n        class JSONForm(Form):\n            json_field = JSONField(initial='{\"key\": \"value\"}')\n        \n        form = JSONForm()\n        self.assertEqual(form['json_field'].value(), '{\"key\": \"value\"}')\n", "def test_custom_error_messages(self):\n    field = JSONField(error_messages={'invalid': 'Custom invalid message.'})\n    with self.assertRaisesMessage(ValidationError, 'Custom invalid message.'):\n        field.clean('{some badly formed: json}')\n", "def test_invalid_json_in_bound_data(self):\n    field = JSONField()\n    invalid_json = '{\"a\": 1,'\n    result = field.bound_data(invalid_json, None)\n    self.assertIsInstance(result, InvalidJSONInput)\n    self.assertEqual(result, invalid_json)\n", "    def test_invalid_json_input(self):\n        field = JSONField()\n        invalid_json_input = InvalidJSONInput('invalid')\n        self.assertEqual(field.clean(invalid_json_input), invalid_json_input)\n", "    def test_bound_data_invalid_json_input(self):\n        field = JSONField()\n        initial_data = {\"initial\": \"data\"}\n        bound_data = field.bound_data('{\"a\": \"b\"', initial_data)\n        self.assertIsInstance(bound_data, InvalidJSONInput)\n        self.assertEqual(bound_data, '{\"a\": \"b\"')\n"], "sample_231": ["    def test_cleanse_setting_callable(self):\n        \"\"\"\n        Callable settings should be wrapped with CallableSettingWrapper.\n        \"\"\"\n            return \"This should not be displayed\"\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed = reporter_filter.cleanse_setting('CALLABLE_SETTING', callable_setting)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n", "    def test_cleanse_special_types(self):\n        \"\"\"\n        Test that cleanse_special_types correctly handles MultiValueDict and other types.\n        \"\"\"\n        request = mock.Mock()\n        multivaluedict = MultiValueDict({'key1': ['sensitive1'], 'key2': ['non-sensitive']})\n        non_multivaluedict = {'key3': 'value3'}\n\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed_multivaluedict = reporter_filter.cleanse_special_types(request, multivaluedict)\n        cleansed_non_multivaluedict = reporter_filter.cleanse_special_types(request, non_multivaluedict)\n\n        self.assertEqual(cleansed_multivaluedict, {'key1': '********************', 'key2': '********************'})\n        self.assertEqual(cleansed_non_multivaluedict, {'key3': 'value3'})\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_cleanse_setting_callable(self):\n        \"\"\"\n        Callable settings should be wrapped and not called.\n        \"\"\"\n            return 'Sensitive Data'\n        \n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed = reporter_filter.cleanse_setting('SENSITIVE_CALLABLE', sensitive_callable)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed), repr(sensitive_callable))\n        "], "sample_232": ["    def test_deep_lookup_multiple_conditions(self):\n        # Test multiple conditions in a deep lookup\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__k__l='m',\n                value__d__1__f='g',\n            ),\n            [self.objs[4]],\n        )\n", "    def test_database_backend_not_support_json_fields(self):\n        with mock.patch('django.db.connections') as connections_mock:\n            connections_mock.__getitem__.return_value.features.supports_json_field = False\n            connections_mock.__getitem__.return_value.display_name = 'TestDB'\n            class TestModel(models.Model):\n                field = JSONField()\n                class Meta:\n                    required_db_features = []\n\n            field = TestModel._meta.get_field('field')\n            field.model = TestModel\n            errors = field.check(databases=['default'])\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'fields.E180')\n            self.assertIn('TestDB does not support JSONFields.', errors[0].msg)\n", "    def test_compile_json_path(self):\n        # Test cases with various key_transforms.\n        tests = [\n            ([], True, '$'),\n            ([], False, ''),\n            (['key1'], True, '$.\"key1\"'),\n            (['key1'], False, '.\"key1\"'),\n            ([1, 'key2', 3], True, '$[1].\"key2\"[3]'),\n            ([1, 'key2', 3], False, '[1].\"key2\"[3]'),\n        ]\n        for key_transforms, include_root, expected in tests:\n            with self.subTest(key_transforms=key_transforms, include_root=include_root):\n                self.assertEqual(compile_json_path(key_transforms, include_root), expected)\n", "    def test_compile_json_path_empty(self):\n        self.assertEqual(compile_json_path([]), '$')\n", "    def test_key_transform_iexact_non_key_transform(self):\n        transform = Transform('test')\n        msg = 'Transform should be an instance of KeyTransform in order to use this lookup.'\n        with self.assertRaisesMessage(TypeError, msg):\n            KeyTransformIExact(transform)\n    "], "sample_233": ["    def test_token_with_changed_password(self):\n        \"\"\"Changing the user's password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_different_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_different_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_changed_password(self):\n        \"\"\"Changing the user's password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_legacy_algorithm(self):\n        \"\"\"\n        A valid token can be checked with the legacy algorithm if the token\n        was created with it.\n        \"\"\"\n        user = User.objects.create_user('legacyuser', 'legacy@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        p0.algorithm = 'sha1'\n        tk1 = p0.make_token(user)\n        # Check token with legacy algorithm\n        self.assertIs(p0.check_token(user, tk1), True)\n        # Check token with new algorithm should fail\n        p0.algorithm = settings.DEFAULT_HASHING_ALGORITHM\n        self.assertIs(p0.check_token(user, tk1), False)\n"], "sample_234": ["    def test_union_with_aggregate(self):\n        qs1 = Number.objects.filter(num__lte=1).annotate(max_num=Max('num'))\n        qs2 = Number.objects.filter(num__gte=8).annotate(max_num=Max('num'))\n        result = qs1.union(qs2).aggregate(Max('max_num'))\n        self.assertEqual(result['max_num__max'], 9)\n", "    def test_empty_intersection_with_values_list(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = ReservedName.objects.filter(name='b')\n        reserved_name = qs1.intersection(qs1).values_list('name', 'order', 'id').first()\n        self.assertIsNone(reserved_name)\n", "    def test_combining_multiple_models_with_difference(self):\n        ReservedName.objects.create(name='99 little bugs', order=99)\n        Number.objects.create(num=99, other_num=1)\n        qs1 = Number.objects.filter(num=99).values_list('num', flat=True)\n        qs2 = ReservedName.objects.values_list('order')\n        self.assertEqual(list(qs1.difference(qs2)), [99])\n", "    def test_get_with_filter_after_union(self):\n        qs1 = Number.objects.filter(num__lte=1)\n        qs2 = Number.objects.filter(num__gte=8)\n        union_qs = qs1.union(qs2)\n        with self.assertRaises(NotSupportedError):\n            union_qs.get(num=0)\n", "    def test_values_annotation(self):\n        Number.objects.create(num=10, other_num=0)\n        qs1 = Number.objects.filter(num__lte=5).annotate(\n            doubled_num=F('num') * 2\n        ).values('num', 'doubled_num')\n        qs2 = Number.objects.filter(num__gte=6).annotate(\n            doubled_num=F('num') * 2\n        ).values('num', 'doubled_num')\n        union_qs = qs1.union(qs2).order_by('num')\n        self.assertEqual(\n            list(union_qs),\n            [{'num': 0, 'doubled_num': 0}, {'num': 1, 'doubled_num': 2}, {'num': 2, 'doubled_num': 4}, \n             {'num': 3, 'doubled_num': 6}, {'num': 4, 'doubled_num': 8}, {'num': 5, 'doubled_num': 10}, \n             {'num': 6, 'doubled_num': 12}, {'num': 7, 'doubled_num': 14}, {'num': 8, 'doubled_num': 16}, \n             {'num': 9, 'doubled_num': 18}, {'num': 10, 'doubled_num': 20}]\n        )\n"], "sample_235": ["    def test_on_commit_callable_type_check(self):\n        with transaction.atomic():\n            with self.assertRaises(TypeError):\n                transaction.on_commit(\"not a callable\")\n", "    def test_on_commit_callable_check(self):\n        with self.assertRaises(TypeError):\n            transaction.on_commit(\"not_a_callable\")\n", "    def test_on_commit_with_exceptions(self):\n        \"\"\"\n        Test to ensure that exceptions raised within on_commit callbacks\n        don't prevent other callbacks from executing and don't break the transaction.\n        \"\"\"\n        try:\n            with transaction.atomic():\n                transaction.on_commit(lambda: self.notify(1))\n                transaction.on_commit(lambda: self.notify('error'))  # This will raise ForcedError\n                transaction.on_commit(lambda: self.notify(2))\n        except ForcedError:\n            pass\n\n        self.assertDone([1, 2])\n", "    def test_on_commit_callable_type_error(self):\n        with self.assertRaises(TypeError):\n            transaction.on_commit(\"not a callable\")\n", "def test_on_commit_not_callable(self):\n        with self.assertRaises(TypeError):\n            transaction.on_commit(\"not_callable\")\n"], "sample_236": ["    def test_restrict_with_null_field(self):\n        a = create_a('restrict')\n        # Assign a None value to a restrict field to ensure it doesn't raise an error.\n        a.restrict = None\n        a.save()\n        try:\n            a.delete()\n        except RestrictedError:\n            self.fail(\"RestrictedError was raised even though the field was set to None.\")\n", "def test_collect_with_no_objects(self):\n    \"\"\"\n    Test the `collect` method of the `Collector` class when no objects are passed.\n    \"\"\"\n    collector = Collector(using='default')\n    collector.collect([])\n    self.assertEqual(len(collector.data), 0)\n    self.assertEqual(len(collector.field_updates), 0)\n    self.assertEqual(len(collector.restricted_objects), 0)\n    self.assertEqual(len(collector.fast_deletes), 0)\n", "    def test_protect_error_message(self):\n        a = create_a('protect')\n        msg = (\n            \"Cannot delete some instances of model 'R' because they are \"\n            \"referenced through protected foreign keys: 'A.protect'.\"\n        )\n        try:\n            a.protect.delete()\n        except ProtectedError as e:\n            self.assertEqual(str(e), msg)\n", "    def test_set_callable(self):\n        \"\"\"\n        Test the SET() function with a callable value to ensure it updates \n        the related field with the return value of the callable.\n        \"\"\"\n            return self.DEFAULT\n\n        set_callable = models.SET(callable_value)\n        a = create_a('set_callable')\n        a.setvalue_on_delete = set_callable\n        a.save()\n        a.setvalue_on_delete.delete()\n        a = A.objects.get(pk=a.pk)\n        self.assertEqual(a.setvalue_on_delete, self.DEFAULT)\n", "    def test_cascade_with_signals(self):\n        \"\"\"\n        Test the CASCADE deletion handler along with pre_delete and post_delete signals.\n        \"\"\"\n        pre_delete_called = []\n        post_delete_called = []\n\n            pre_delete_called.append(kwargs['instance'])\n\n            post_delete_called.append(kwargs['instance'])\n\n        models.signals.pre_delete.connect(pre_delete_callback)\n        models.signals.post_delete.connect(post_delete_callback)\n\n        try:\n            a = create_a('cascade_with_signals')\n            a.cascade_with_signals.delete()\n\n            self.assertFalse(A.objects.filter(name='cascade_with_signals').exists())\n            self.assertIn(a, pre_delete_called)\n            self.assertIn(a, post_delete_called)\n\n        finally:\n            models.signals.pre_delete.disconnect(pre_delete_callback)\n            models.signals.post_delete.disconnect(post_delete_callback)\n"], "sample_237": ["    def test_custom_permission_name_max_length_boundary(self):\n        custom_permission_name = 'x' * 255\n\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('my_custom_permission', custom_permission_name),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_no_permissions_defined(self):\n        class Checked(models.Model):\n            pass\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_builtin_permission_name_max_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'a' * 256  # Exceeds the max length to cause error\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 255 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_custom_permission_clashing_with_builtin(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Permission clashing with builtin add permission'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n", "    def test_builtin_permissions_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'short name'\n                permissions = []\n\n        Permission = apps.get_model('auth', 'Permission')\n        permission_name_max_length = Permission._meta.get_field('name').max_length\n\n        builtin_permissions = dict(_get_builtin_permissions(Checked._meta))\n        long_permission_name = 'a' * (permission_name_max_length + 1)\n        builtin_permissions['long_permission'] = long_permission_name\n\n        with self.assertRaises(ValueError):\n            if max(len(name) for name in builtin_permissions.values()) > permission_name_max_length:\n                raise ValueError(\"Builtin permission name exceeds maximum length\")\n\n        Checked._meta.permissions = [(codename, name) for codename, name in builtin_permissions.items()]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most %d characters for its builtin permission names to be at most %d characters.\" % (\n                    permission_name_max_length - (len(long_permission_name) - len(Checked._meta.verbose_name_raw)),\n                    permission_name_max_length\n                ),\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n"], "sample_238": ["    def test_trigonometric_functions(self):\n        books = Book.objects.annotate(\n            cosine=Cos(F('rating')),\n            sine=Sin(F('rating')),\n            tangent=Tan(F('rating')),\n            cotangent=Cot(F('rating')),\n            ceiling=Ceil(F('rating')),\n            floor=Floor(F('rating')),\n            abs_value=Abs(F('rating')),\n            acosine=ACos(F('rating')),\n            asine=ASin(F('rating')),\n            atan=ATan(F('rating')),\n            atan2=ATan2(F('pages'), F('rating')),\n            degrees=Degrees(F('rating')),\n            radians=Radians(F('rating')),\n            exp=Exp(F('rating')),\n            ln=Ln(F('rating')),\n            log=Log(Value(10), F('rating')),\n            mod=Mod(F('pages'), Value(2)),\n            pi=Pi(),\n            power=Power(F('rating'), Value(2)),\n            sqrt=Sqrt(F('rating'))\n        ).values(\n            'name', 'cosine', 'sine', 'tangent', 'cotangent', 'ceiling', 'floor', 'abs_value', \n            'acosine', 'asine', 'atan', 'atan2', 'degrees', 'radians', 'exp', 'ln', 'log', \n            'mod', 'pi', 'power', 'sqrt'\n        ).order_by('name')\n        \n        for book in books:\n            rating = float(Book.objects.get(name=book['name']).rating)\n            self.assertAlmostEqual(book['cosine'], math.cos(rating), places=5)\n            self.assertAlmostEqual(book['sine'], math.sin(rating), places=5)\n            self.assertAlmostEqual(book['tangent'], math.tan(rating), places=5)\n            self.assertAlmostEqual(book['cotangent'], 1 / math.tan(rating), places=5)\n            self.assertEqual(book['ceiling'], math.ceil(rating))\n            self.assertEqual(book['floor'], math.floor(rating))\n            self.assertAlmostEqual(book['abs_value'], abs(rating), places=5)\n            self.assertAlmostEqual(book['acosine'], math.acos(rating), places=5)\n            self.assertAlmostEqual(book['asine'], math.asin(rating), places=5)\n            self.assertAlmostEqual(book['atan'], math.atan(rating), places=5)\n            pages = float(Book.objects.get(name=book['name']).", "    def test_math_functions(self):\n        vals = Book.objects.annotate(\n            acos_val=ACos('rating'),\n            asin_val=ASin('rating'),\n            atan_val=ATan('rating'),\n            atan2_val=ATan2('price', 'rating'),\n            ceil_val=Ceil('price'),\n            cos_val=Cos('rating'),\n            cot_val=Cot('rating'),\n            degrees_val=Degrees('rating'),\n            exp_val=Exp('rating'),\n            floor_val=Floor('price'),\n            ln_val=Ln('price'),\n            log_val=Log('price', 'rating'),\n            mod_val=Mod('price', 'rating'),\n            pi_val=Pi(),\n            power_val=Power('rating', 'price'),\n            radians_val=Radians('rating'),\n            random_val=Random(),\n            round_val=Round('price'),\n            sign_val=Sign('price'),\n            sin_val=Sin('rating'),\n            sqrt_val=Sqrt('price'),\n            tan_val=Tan('rating')\n        ).filter(pk=self.b1.pk).values(\n            'acos_val', 'asin_val', 'atan_val', 'atan2_val', 'ceil_val',\n            'cos_val', 'cot_val', 'degrees_val', 'exp_val', 'floor_val',\n            'ln_val', 'log_val', 'mod_val', 'pi_val', 'power_val', \n            'radians_val', 'random_val', 'round_val', 'sign_val', \n            'sin_val', 'sqrt_val', 'tan_val'\n        ).first()\n\n        self.assertIsNotNone(vals)\n        self.assertAlmostEqual(vals['acos_val'], math.acos(self.b1.rating), places=2)\n        self.assertAlmostEqual(vals['asin_val'], math.asin(self.b1.rating / 5), places=2)  # rating is not normalized\n        self.assertAlmostEqual(vals['atan_val'], math.atan(self.b1.rating), places=2)\n        self.assertAlmostEqual(vals['atan2_val'], math.atan2(self.b1.price, self.b1.rating), places=2)\n        self.assertEqual(vals['ceil_val'], math.ceil(self.b1.price))\n        self.assertAlmostEqual(vals['cos_val'], math.cos(self.b1.rating), places=2)\n        self.assertAlmostEqual(vals['cot_val'], 1 / math.tan(self.b1.rating), places=2)\n        self.assertAlmostEqual(vals['degrees_val'], math.degrees(self.b1.rating),", "    def test_math_functions(self):\n        # Test Abs\n        authors = Author.objects.annotate(abs_age=Abs(F('age'))).order_by('name')\n        self.assertQuerysetEqual(\n            authors, [\n                ('Adrian Holovaty', 34),\n                ('Brad Dayley', 45),\n                ('Jacob Kaplan-Moss', 35),\n                ('James Bennett', 29),\n                ('Jeffrey Forcier', 37),\n                ('Paul Bissex', 29),\n                ('Peter Norvig', 57),\n                ('Stuart Russell', 46),\n                ('Wesley J. Chun', 25)\n            ],\n            lambda a: (a.name, a.abs_age)\n        )\n\n        # Test Ceil\n        books = Book.objects.annotate(ceil_price=Ceil(F('price'))).order_by('name')\n        self.assertQuerysetEqual(\n            books, [\n                ('Artificial Intelligence: A Modern Approach', 83),\n                ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 75),\n                ('Practical Django Projects', 30),\n                ('Python Web Development with Django', 30),\n                ('Sams Teach Yourself Django in 24 Hours', 24),\n                ('The Definitive Guide to Django: Web Development Done Right', 30)\n            ],\n            lambda b: (b.name, b.ceil_price)\n        )\n\n        # Test Floor\n        books = Book.objects.annotate(floor_price=Floor(F('price'))).order_by('name')\n        self.assertQuerysetEqual(\n            books, [\n                ('Artificial Intelligence: A Modern Approach', 82),\n                ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 75),\n                ('Practical Django Projects', 29),\n                ('Python Web Development with Django', 29),\n                ('Sams Teach Yourself Django in 24 Hours', 23),\n                ('The Definitive Guide to Django: Web Development Done Right', 30)\n            ],\n            lambda b: (b.name, b.floor_price)\n        )\n\n        # Test Degrees\n        authors = Author.objects.annotate(degrees_age=Degrees(F('age'))).order_by('name')\n        for author in authors:\n            self.assertAlmostEqual(author.degrees_age, math.degrees(author.age))\n\n        # Test Radians\n        authors = Author.objects.annotate(radians_age=Radians(F('age'))).order_by('name')\n        for author in authors:\n", "    def test_transform_functions(self):\n        \"\"\"\n        Test the transform functions like Abs, ACos, ASin, ATan, Ceil, Cos, Cot, Degrees, Exp, Floor, Ln, Log, \n        Mod, Pi, Power, Radians, Random, Round, Sign, Sin, Sqrt, Tan.\n        \"\"\"\n        with self.assertNumQueries(1):\n            # Test Abs\n            result = Author.objects.annotate(abs_age=Abs('age')).filter(abs_age=F('age')).count()\n            self.assertEqual(result, Author.objects.count())\n\n            # Test ACos\n            result = Book.objects.annotate(acos_rating=ACos('rating')).filter(acos_rating__isnull=False).count()\n            self.assertEqual(result, Book.objects.count())\n\n            # Test ASin\n            result = Book.objects.annotate(asin_rating=ASin('rating')).filter(asin_rating__isnull=False).count()\n            self.assertEqual(result, Book.objects.count())\n\n            # Test ATan\n            result = Book.objects.annotate(atan_rating=ATan('rating')).filter(atan_rating__isnull=False).count()\n            self.assertEqual(result, Book.objects.count())\n\n            # Test Ceil\n            result = Book.objects.annotate(ceil_price=Ceil('price')).filter(ceil_price=F('price')).count()\n            self.assertEqual(result, 0)\n\n            # Test Cos\n            result = Book.objects.annotate(cos_rating=Cos('rating')).filter(cos_rating__isnull=False).count()\n            self.assertEqual(result, Book.objects.count())\n\n            # Test Cot\n            result = Book.objects.annotate(cot_rating=Cot('rating')).filter(cot_rating__isnull=False).count()\n            self.assertEqual(result, Book.objects.count())\n\n            # Test Degrees\n            result = Book.objects.annotate(degrees_rating=Degrees('rating')).filter(degrees_rating__isnull=False).count()\n            self.assertEqual(result, Book.objects.count())\n\n            # Test Exp\n            result = Book.objects.annotate(exp_rating=Exp('rating')).filter(exp_rating__isnull=False).count()\n            self.assertEqual(result, Book.objects.count())\n\n            # Test Floor\n            result = Book.objects.annotate(floor_price=Floor('price')).filter(floor_price=F('price')).count()\n            self.assertEqual(result, 0)\n\n            # Test Ln\n            result = Book.objects.annotate(ln_rating=Ln('rating", "    def test_trigonometric_transforms(self):\n        # Test ABS function\n        authors = Author.objects.annotate(abs_age=Abs('age'))\n        for author in authors:\n            self.assertEqual(author.abs_age, abs(author.age))\n\n        # Test ACOS function\n        if connection.vendor != 'sqlite':  # SQLite does not support ACOS\n            authors = Author.objects.annotate(acos_age=ACos('age'))\n            for author in authors:\n                self.assertAlmostEqual(author.acos_age, math.acos(author.age), places=5)\n\n        # Test ASIN function\n        if connection.vendor != 'sqlite':  # SQLite does not support ASIN\n            authors = Author.objects.annotate(asin_age=ASin('age'))\n            for author in authors:\n                self.assertAlmostEqual(author.asin_age, math.asin(author.age), places=5)\n\n        # Test ATAN function\n        authors = Author.objects.annotate(atan_age=ATan('age'))\n        for author in authors:\n            self.assertAlmostEqual(author.atan_age, math.atan(author.age), places=5)\n\n        # Test ATAN2 function\n        if connection.vendor != 'sqlite':  # SQLite does not support ATAN2\n            authors = Author.objects.annotate(atan2_age=ATan2('age', Value(1)))\n            for author in authors:\n                self.assertAlmostEqual(author.atan2_age, math.atan2(author.age, 1), places=5)\n\n        # Test COS function\n        authors = Author.objects.annotate(cos_age=Cos('age'))\n        for author in authors:\n            self.assertAlmostEqual(author.cos_age, math.cos(author.age), places=5)\n\n        # Test COT function\n        if connection.vendor != 'sqlite':  # SQLite does not support COT\n            authors = Author.objects.annotate(cot_age=Cot('age'))\n            for author in authors:\n                self.assertAlmostEqual(author.cot_age, 1 / math.tan(author.age), places=5)\n\n        # Test SIN function\n        authors = Author.objects.annotate(sin_age=Sin('age'))\n        for author in authors:\n            self.assertAlmostEqual(author.sin_age, math.sin(author.age), places=5)\n\n        # Test SQRT function\n        authors = Author.objects.annotate(sqrt_age=Sqrt('age'))\n        for author in authors:\n            self.assertAlmostEqual(author.sqrt_age, math.sqrt(author.age), places=5)\n\n       "], "sample_239": ["    def test_custom_management_form(self):\n        class CustomManagementForm(Form):\n            custom_total = IntegerField(widget=HiddenInput)\n            custom_initial = IntegerField(widget=HiddenInput)\n\n        class CustomFormSet(BaseFormSet):\n            @cached_property\n                if self.is_bound:\n                    form = CustomManagementForm(self.data, auto_id=self.auto_id, prefix=self.prefix)\n                    if not form.is_valid():\n                        raise ValidationError(\n                            'ManagementForm data is missing or has been tampered with.'\n                        )\n                else:\n                    form = CustomManagementForm(auto_id=self.auto_id, prefix=self.prefix, initial={\n                        'custom_total': self.total_form_count(),\n                        'custom_initial': self.initial_form_count(),\n                    })\n                return form\n\n        CustomFormSetFactory = formset_factory(FavoriteDrinkForm, formset=CustomFormSet)\n        data = {\n            'form-TOTAL_FORMS': '1',\n            'form-INITIAL_FORMS': '0',\n            'form-0-name': 'Gin and Tonic',\n            'custom_total': '1',\n            'custom_initial': '0',\n        }\n        formset = CustomFormSetFactory(data)\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [{'name': 'Gin and Tonic'}])\n", "def test_management_form_missing_fields(self):\n    \"\"\"\n    If management form fields are missing, a ValidationError should be raised.\n    \"\"\"\n    # TOTAL_FORMS and INITIAL_FORMS are missing\n    data = {\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    with self.assertRaises(ValidationError) as cm:\n        ChoiceFormSet(data, auto_id=False, prefix='choices').is_valid()\n    self.assertEqual(\n        str(cm.exception),\n        'ManagementForm data is missing or has been tampered with. Missing fields: choices-TOTAL_FORMS, choices-INITIAL_FORMS'\n    )\n", "    def test_custom_prefix(self):\n        \"\"\"\n        Formsets can have custom prefixes.\n        \"\"\"\n        data = {\n            'custom-TOTAL_FORMS': '1',\n            'custom-INITIAL_FORMS': '0',\n            'custom-MIN_NUM_FORMS': '0',\n            'custom-MAX_NUM_FORMS': '0',\n            'custom-0-choice': 'Test Choice',\n            'custom-0-votes': '123',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='custom')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(\n            formset.cleaned_data,\n            [{'choice': 'Test Choice', 'votes': 123}]\n        )\n        self.assertHTMLEqual(\n            str(formset.management_form),\n            (\n                '<input type=\"hidden\" name=\"custom-TOTAL_FORMS\" value=\"1\">'\n                '<input type=\"hidden\" name=\"custom-INITIAL_FORMS\" value=\"0\">'\n                '<input type=\"hidden\" name=\"custom-MIN_NUM_FORMS\" value=\"0\">'\n                '<input type=\"hidden\" name=\"custom-MAX_NUM_FORMS\" value=\"0\">'\n            )\n        )\n", "def test_custom_management_form_prefix(self):\n    \"\"\"Test that a custom management form prefix works correctly.\"\"\"\n    class CustomPrefixFormSet(BaseFormSet):\n        @classmethod\n            return 'customprefix'\n\n    CustomPrefixChoiceFormSet = formset_factory(Choice, formset=CustomPrefixFormSet)\n    data = {\n        'customprefix-TOTAL_FORMS': '1',\n        'customprefix-INITIAL_FORMS': '0',\n        'customprefix-0-choice': 'Test',\n        'customprefix-0-votes': '123',\n    }\n    formset = CustomPrefixChoiceFormSet(data)\n    self.assertEqual(formset.management_form.prefix, 'customprefix')\n    self.assertEqual(formset.prefix, 'customprefix')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual([form.cleaned_data for form in formset.forms], [{'choice': 'Test', 'votes': 123}])\n", "    def test_management_form_missing_field(self):\n        \"\"\"\n        If the management form is missing a required field, a ValidationError\n        is raised indicating the missing field.\n        \"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-0-choice': 'Calexico',\n            'choices-0-votes': '100',\n        }\n        # Remove 'INITIAL_FORMS' to trigger ValidationError\n        ChoiceFormSet = formset_factory(Choice)\n        with self.assertRaisesMessage(ValidationError, 'ManagementForm data is missing or has been tampered with. Missing fields: choices-INITIAL_FORMS'):\n            formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n            formset.is_valid()\n"], "sample_240": ["def test_token_with_different_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_updated_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('passwordchangeuser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_invalid_after_password_change(self):\n        \"\"\"The token becomes invalid after the user changes their password.\"\"\"\n        user = User.objects.create_user('passwordchangeuser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        # Change the user's password\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_make_token_with_legacy_timestamp(self):\n        \"\"\"\n        Test that a token generated with a legacy timestamp (number of days since 2001-1-1) \n        is correctly parsed and validated by check_token().\n        \"\"\"\n        class LegacyPasswordResetTokenGenerator(MockedPasswordResetTokenGenerator):\n            \"\"\"Pre-Django 3.1 tokens generator.\"\"\"\n                # Pre-Django 3.1 tokens use days instead of seconds.\n                return (dt.date() - date(2001, 1, 1)).days\n\n        user = User.objects.create_user('legacytokentestuser', 'legacytest@example.com', 'testpw')\n        now = datetime.now()\n        p0 = LegacyPasswordResetTokenGenerator(now)\n        tk1 = p0.make_token(user)\n        # Check token with Legacy generator should be valid\n        self.assertIs(p0.check_token(user, tk1), True)\n        # Check token with current generator should also be valid\n        p1 = MockedPasswordResetTokenGenerator(now)\n        self.assertIs(p1.check_token(user, tk1), True)\n", "    def test_token_with_modified_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n"], "sample_241": ["    def setUp(self):\n        self.connection = connections[DEFAULT_DB_ALIAS]\n        self.cursor = self.connection.cursor()\n        self.sql = \"SELECT * FROM %s\" % self.connection.introspection.identifier_converter('my_table')\n        self.raw_query = RawQuery(self.sql, using=DEFAULT_DB_ALIAS)\n", "    def test_clone(self):\n        # Ensure Query.clone() correctly duplicates a Query instance.\n        query = Query(model=Company)\n        query.add_fields(['name', 'num_employees'])\n        query.add_filter(Q(num_employees__gt=10))\n        query.add_ordering('name')\n\n        cloned_query = query.clone()\n\n        self.assertEqual(query.model, cloned_query.model)\n        self.assertEqual(query.select, cloned_query.select)\n        self.assertEqual(str(query.where), str(cloned_query.where))\n        self.assertEqual(query.order_by, cloned_query.order_by)\n", "    def setUpTestData(cls):\n        # Create a database connection for the test\n        cls.connection = connections[DEFAULT_DB_ALIAS]\n        cls.raw_query = RawQuery(\n            \"SELECT 1 AS number UNION SELECT 2\", \n            using=DEFAULT_DB_ALIAS\n        )\n", "    def test_query_initialization(self):\n        query = Query(model=Employee)\n        self.assertEqual(query.model, Employee)\n        self.assertIsInstance(query.where, WhereNode)\n        self.assertEqual(query.alias_prefix, 'T')\n        self.assertIs(query.default_cols, True)\n        self.assertIs(query.default_ordering, True)\n        self.assertEqual(query.low_mark, 0)\n        self.assertIsNone(query.high_mark)\n        self.assertIs(query.distinct, False)\n        self.assertIs(query.select_for_update, False)\n        ", "    def setUp(self):\n        self.raw_query = RawQuery(\n            sql=\"SELECT * FROM expressions_company WHERE num_employees > %s\",\n            using=DEFAULT_DB_ALIAS,\n            params=(10,)\n        )\n"], "sample_242": ["    def test_get_prep_lookup(self):\n        lookup = Lookup(Value(1), 2)\n        self.assertEqual(lookup.get_prep_lookup(), 2)\n        lookup = Lookup(Value(1, output_field=DateTimeField()), datetime(2023, 1, 1))\n        self.assertEqual(lookup.get_prep_lookup(), datetime(2023, 1, 1))\n        lookup = Lookup(Value(1), Value(2))\n        self.assertEqual(lookup.get_prep_lookup(), Value(2))\n", "def test_process_lhs(self):\n        lookup = Lookup(Value(1), Value(2))\n        compiler_mock = mock.Mock()\n        connection_mock = mock.Mock()\n        connection_mock.ops.field_cast_sql.return_value = '%s'\n        connection_mock.ops.lookup_cast.return_value = '%s'\n        lhs_sql, params = lookup.process_lhs(compiler_mock, connection_mock)\n        self.assertEqual(lhs_sql, '%s')\n        self.assertEqual(params, [])\n", "    def test_contains_aggregate(self):\n        lookup_with_aggregate = Lookup(Value(1, contains_aggregate=True), Value(2))\n        self.assertTrue(lookup_with_aggregate.contains_aggregate)\n\n        lookup_without_aggregate = Lookup(Value(1), Value(2))\n        self.assertFalse(lookup_without_aggregate.contains_aggregate)\n", "    def test_get_prep_lookup(self):\n        lookup = Lookup(Value(1), Value(2))\n        self.assertEqual(lookup.get_prep_lookup(), 2)\n        lookup = Lookup(Value(1), Value(Value(2)))\n        self.assertIsInstance(lookup.get_prep_lookup(), Value)\n", "    def test_get_prep_lookup(self):\n        lookup = Lookup(Value(1), Value(2))\n        self.assertEqual(lookup.get_pprep_lookup(), 2)\n        custom_rhs = Value(3)\n        lookup_custom = Lookup(Value(1), custom_rhs)\n        self.assertEqual(lookup_custom.get_prep_lookup(), custom_rhs)\n"], "sample_243": ["    def test_add_deferred_loading(self):\n        query = Query(Item)\n        query.add_deferred_loading(['name', 'modified'])\n        self.assertIn('name', query.deferred_loading[0])\n        self.assertIn('modified', query.deferred_loading[0])\n        self.assertTrue(query.deferred_loading[1])\n        \n        query.add_deferred_loading(['created'])\n        self.assertIn('name', query.deferred_loading[0])\n        self.assertIn('modified', query.deferred_loading[0])\n        self.assertIn('created', query.deferred_loading[0])\n        self.assertTrue(query.deferred_loading[1])\n", "    def test_subquery(self):\n        subquery = Query(Author)\n        subquery.add_select_related(['item'])\n        subquery.add_filter(Q(item__name='foo'))\n        query = Query(Author)\n        query.add_filter(Q(id__in=subquery))\n        where = query.where\n        self.assertEqual(len(where.children), 1)\n        subquery_condition = where.children[0]\n        self.assertIsInstance(subquery_condition, Exists)\n        self.assertEqual(subquery_condition.query.model, Author)\n        self.assertEqual(subquery_condition.query.where.children[0].children[0].lhs.target, Item._meta.get_field('name'))\n", "    def test_raw_query_get_columns(self):\n        raw_query = RawQuery(\"SELECT id, name FROM author\", using='default')\n        # Mocking the cursor description to simulate executed query\n        raw_query.cursor = type('Cursor', (object,), {'description': [('id',), ('name',)]})\n        columns = raw_query.get_columns()\n        self.assertEqual(columns, ['id', 'name'])\n", "    def test_subquery_annotation(self):\n        subquery = Query(Author)\n        subquery.add_annotation(Count('item'), alias='item_count')\n        query = Query(Author)\n        query.add_annotation(subquery.resolve_expression(query), alias='subquery_item_count')\n        annotation = query.annotations['subquery_item_count']\n\n        self.assertIsInstance(annotation, Query)\n        self.assertEqual(annotation.model, Author)\n        self.assertIn('item_count', annotation.annotations)\n        self.assertEqual(annotation.annotations['item_count'].source_expressions[0].name, 'item')\n", "    def test_add_annotation(self):\n        query = Query(Author)\n        query.add_annotation(Count('id'), alias='count_id')\n        self.assertIn('count_id', query.annotations)\n        self.assertIsInstance(query.annotations['count_id'], Count)\n        self.assertEqual(query.annotations['count_id'].source_expressions[0].name, 'id')\n"], "sample_244": ["    def test_management_form_is_valid(self):\n        \"\"\"\n        Test that the management form within the formset is valid when provided with proper data.\n        \"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '3',\n            'choices-INITIAL_FORMS': '1',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1000',\n            'choices-0-choice': 'Option 1',\n            'choices-0-votes': '10',\n            'choices-1-choice': 'Option 2',\n            'choices-1-votes': '20',\n            'choices-2-choice': 'Option 3',\n            'choices-2-votes': '30',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.management_form.is_valid())\n        self.assertTrue(formset.is_valid())\n", "    def test_initial_forms_beyond_max_num(self):\n        \"\"\"\n        If initial forms exceed max_num, all initial forms should still be rendered.\n        \"\"\"\n        initial_data = [\n            {'choice': 'Choice 1', 'votes': 10},\n            {'choice': 'Choice 2', 'votes': 20},\n            {'choice': 'Choice 3', 'votes': 30},\n        ]\n        ChoiceFormSet = formset_factory(Choice, extra=1, max_num=2)\n        formset = ChoiceFormSet(initial=initial_data, auto_id=False, prefix='choices')\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Choice 1\"></li>", "    def test_max_num_and_absolute_max(self):\n        \"\"\"\n        Test formset with max_num and absolute_max constraints.\n        \"\"\"\n        data = {\n            'form-TOTAL_FORMS': '1501',\n            'form-INITIAL_FORMS': '0',\n            'form-MAX_NUM_FORMS': '1500',  # Ignored\n        }\n        CustomMaxFavoriteDrinkFormSet = formset_factory(\n            FavoriteDrinkForm,\n            extra=1,\n            max_num=1500,\n            absolute_max=1500,\n        )\n        formset = CustomMaxFavoriteDrinkFormSet(data=data)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(len(formset.forms), 1500)\n        self.assertEqual(\n            formset.non_form_errors(),\n            ['Please submit at most 1500 forms.'],\n        )\n", "    def test_management_form_cleaned_data(self):\n        \"\"\"ManagementForm's cleaned_data should return the correct values.\"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '3',\n            'choices-INITIAL_FORMS': '1',\n            'choices-MIN_NUM_FORMS': '2',\n            'choices-MAX_NUM_FORMS': '4',\n            'choices-0-choice': 'Choice 1',\n            'choices-0-votes': '10',\n            'choices-1-choice': 'Choice 2',\n            'choices-1-votes': '20',\n            'choices-2-choice': 'Choice 3',\n            'choices-2-votes': '30',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.management_form.cleaned_data[TOTAL_FORM_COUNT], 3)\n        self.assertEqual(formset.management_form.cleaned_data[INITIAL_FORM_COUNT], 1)\n        self.assertEqual(formset.management_form.cleaned_data[MIN_NUM_FORM_COUNT], 2)\n        self.assertEqual(formset.management_form.cleaned_data[MAX_NUM_FORM_COUNT], 4)\n", "def test_initial_form_count_with_bound_data(self):\n    \"\"\"\n    Ensure that the initial_form_count is correctly calculated \n    when formset is bound and has initial data.\n    \"\"\"\n    initial_data = [{'choice': 'Calexico', 'votes': 100}, {'choice': 'Fergie', 'votes': 900}]\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '2',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '1000',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n    }\n    formset = ChoiceFormSet(data, initial=initial_data, auto_id=False, prefix='choices')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.initial_form_count(), 2)\n    self.assertEqual(len(formset.forms), 2)\n"], "sample_245": ["    def test_plural_forms_header_inclusion(self):\n        \"\"\"\n        Ensure that the 'Plural-Forms' header is correctly included in the PO file.\n        \"\"\"\n        management.call_command('makemessages', locale=['es'], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE_ES))\n        with open(self.PO_FILE_ES, encoding='utf-8') as fp:\n            po_contents = fp.read()\n            self.assertIn('Plural-Forms: nplurals=2; plural=(n != 1)', po_contents)\n            self.assertNotIn('charset=CHARSET', po_contents)  # Ensure charset placeholder is replaced\n", "    def test_command_initialization(self):\n        \"\"\"\n        Test that Command class initializes with correct attributes.\n        \"\"\"\n        cmd = MakeMessagesCommand()\n        self.assertEqual(cmd.translatable_file_class, TranslatableFile)\n        self.assertEqual(cmd.build_file_class, BuildFile)\n        self.assertEqual(cmd.requires_system_checks, [])\n        self.assertEqual(cmd.msgmerge_options, ['-q', '--previous'])\n        self.assertEqual(cmd.msguniq_options, ['--to-code=utf-8'])\n        self.assertEqual(cmd.msgattrib_options, ['--no-obsolete'])\n        self.assertEqual(cmd.xgettext_options, ['--from-code=UTF-8', '--add-comments=Translators'])\n", "    def setUp(self):\n        self.test_dir = tempfile.mkdtemp()\n        self.command = MakeMessagesCommand()\n", "    def test_preprocess_templatize_django(self):\n        \"\"\"\n        Ensure templatize preprocessing for 'django' domain works correctly.\n        \"\"\"\n        test_file = os.path.join(self.test_dir, 'templates', 'test.html')\n        translatable = TranslatableFile(os.path.dirname(test_file), os.path.basename(test_file), NO_LOCALE_DIR)\n        build_file = BuildFile(MakeMessagesCommand(), 'django', translatable)\n        build_file.preprocess()\n        self.assertTrue(os.path.exists(build_file.work_path))\n        with open(build_file.work_path, 'r', encoding='utf-8') as fp:\n            content = fp.read()\n        self.assertIn('Translatable literal #6a', content)\n        self.assertIn('Translatable literal #6b', content)\n", "    def test_cleanup_after_preprocess(self):\n        \"\"\"\n        Test that temporary files created during preprocessing are cleaned up\n        after processing.\n        \"\"\"\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        \n        # Check that no temporary files are left after processing\n        for root, dirs, files in os.walk(self.test_dir):\n            for file in files:\n                self.assertFalse(file.endswith('.py'), f\"Temporary file {file} was not cleaned up\")\n"], "sample_246": ["    def test_preprocess_js(self):\n        \"\"\"\n        Test that JavaScript files are preprocessed correctly.\n        \"\"\"\n        translatable = TranslatableFile(dirpath='.', file_name='test.js', locale_dir='.')\n        build_file = BuildFile(Command(), 'djangojs', translatable)\n        with mock.patch('builtins.open', mock.mock_open(read_data='/* Translators: This is a comment */\\ngettext(\"test\");')) as mock_open:\n            with mock.patch('django.utils.jslex.prepare_js_for_gettext', return_value='gettext(\"test\");') as mock_prepare_js_for_gettext:\n                build_file.preprocess()\n                mock_open.assert_called_with('./test.js', encoding='utf-8')\n                mock_prepare_js_for_gettext.assert_called_once()\n                mock_open().write.assert_called_once_with('gettext(\"test\");')\n", "    def test_normalize_eols(self):\n        raw_contents = \"Line one\\r\\nLine two\\r\\nLine three\\r\"\n        expected_output = \"Line one\\nLine two\\nLine three\\n\"\n        self.assertEqual(normalize_eols(raw_contents), expected_output)\n", "    def test_write_pot_file_creates_file(self):\n        \"\"\"\n        Test that write_pot_file creates a .pot file with the correct contents.\n        \"\"\"\n        pot_file = 'locale/test.pot'\n        msgs = (\n            '# SOME DESCRIPTIVE TITLE.\\n'\n            '# (some lines truncated as they are not relevant)\\n'\n            '\"Content-Type: text/plain; charset=UTF-8\\\\n\"\\n'\n            '\"Content-Transfer-Encoding: 8bit\\\\n\"\\n'\n            '\\n'\n            '#: somefile.py:8\\n'\n            'msgid \"test message\"\\n'\n            'msgstr \"\"\\n'\n        )\n        write_pot_file(pot_file, msgs)\n        self.assertTrue(os.path.exists(pot_file))\n        with open(pot_file, encoding='utf-8') as fp:\n            pot_contents = fp.read()\n            self.assertIn('Content-Type: text/plain; charset=UTF-8', pot_contents)\n            self.assertIn('msgid \"test message\"', pot_contents)\n        os.remove(pot_file)\n", "    def test_normalize_eols(self):\n        raw_contents = \"line1\\r\\nline2\\r\\nline3\"\n        normalized = normalize_eols(raw_contents)\n        self.assertEqual(normalized, \"line1\\nline2\\nline3\\n\")\n", "    def test_normalize_eols(self):\n        \"\"\"\n        Test that normalize_eols correctly normalizes various EOL sequences to \\n.\n        \"\"\"\n        input_text = \"First line\\r\\nSecond line\\nThird line\\rFourth line\\n\"\n        expected_output = \"First line\\nSecond line\\nThird line\\nFourth line\\n\"\n        self.assertEqual(normalize_eols(input_text), expected_output)\n"], "sample_247": ["def test_clone_query_with_alias_and_annotation(self):\n        # Test cloning a QuerySet with both alias and annotation\n        qs = Book.objects.alias(price_alias=F('price')).annotate(discounted_price=F('price_alias') * 0.9)\n        cloned_qs = qs._clone()\n        for book in cloned_qs:\n            with self.subTest(book=book):\n                self.assertEqual(book.discounted_price, book.price * Decimal('0.9'))\n", "    def setUpTestData(cls):\n        cls.a1 = Author.objects.create(name='Adrian Holovaty', age=34)\n        cls.a2 = Author.objects.create(name='Jacob Kaplan-Moss', age=35)\n        cls.a3 = Author.objects.create(name='James Bennett', age=34)\n        cls.a4 = Author.objects.create(name='Peter Norvig', age=57)\n        cls.a5 = Author.objects.create(name='Stuart Russell', age=46)\n        p1 = Publisher.objects.create(name='Apress', num_awards=3)\n        cls.b1 = Book.objects.create(\n            isbn='159059725', pages=447, rating=4.5, price=Decimal('30.00'),\n            contact=cls.a1, publisher=p1, pubdate=datetime.date(2007, 12, 6),\n            name='The Definitive Guide to Django: Web Development Done Right',\n        )\n        cls.b2 = Book.objects.create(\n            isbn='159059996', pages=300, rating=4.0, price=Decimal('29.69'),\n            contact=cls.a3, publisher=p1, pubdate=datetime.date(2008, 6, 23),\n            name='Practical Django Projects',\n        )\n        cls.b3 = Book.objects.create(\n            isbn='013790395', pages=1132, rating=4.0, price=Decimal('82.80'),\n            contact=cls.a4, publisher=p1, pubdate=datetime.date(1995, 1, 15),\n            name='Artificial Intelligence: A Modern Approach',\n        )\n        cls.b4 = Book.objects.create(\n            isbn='155860191', pages=946, rating=5.0, price=Decimal('75.00'),\n            contact=cls.a4, publisher=p1, pubdate=datetime.date(1991, 10, 15),\n            name='Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp',\n        )\n        cls.b1.authors.add(cls.a1, cls.a2)\n        cls.b2.authors.add(cls.a3)\n        cls.b3.authors.add(cls.a4, cls.a5)\n        cls.b4.authors.add(cls.a4)\n", "def test_rawquery_repr(self):\n    raw_query = RawQuery(\"SELECT * FROM test_table\", using=\"default\")\n    self.assertEqual(repr(raw_query), \"<RawQuery: SELECT * FROM test_table>\")\n", "    def test_clone_method_in_raw_query(self):\n        original_query = RawQuery(\"SELECT * FROM my_table\", using=\"default\", params={\"param1\": \"value1\"})\n        cloned_query = original_query.clone(\"new_db\")\n\n        self.assertEqual(cloned_query.sql, original_query.sql)\n        self.assertEqual(cloned_query.params, original_query.params)\n        self.assertNotEqual(cloned_query.using, original_query.using)\n        self.assertEqual(cloned_query.using, \"new_db\")\n", "    def setUp(self):\n        self.raw_query = RawQuery(\n            sql=\"SELECT * FROM annotations_author WHERE age > %s\",\n            using='default',\n            params=[30]\n        )\n"], "sample_248": ["    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython')\n", "    def test_shell_with_ipython_and_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n            call_command('shell', interface='python')\n", "    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython')\n", "    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython')\n", "    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython')\n"], "sample_249": ["    def test_create_test_db(self, mocked_call_command):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        \n        # Mock necessary methods\n        with mock.patch.object(creation, '_create_test_db'), \\\n             mock.patch.object(creation, 'log'), \\\n             mock.patch.object(creation, 'serialize_db_to_string', return_value=\"{}\"):\n            test_db_name = creation.create_test_db(verbosity=2, autoclobber=True, serialize=True, keepdb=True)\n        \n        # Ensure that the test database name is prefixed correctly\n        self.assertTrue(test_db_name.startswith(TEST_DATABASE_PREFIX))\n        # Ensure that 'migrate' and 'createcachetable' commands are called\n        mocked_call_command.assert_any_call(\n            'migrate',\n            verbosity=1,\n            interactive=False,\n            database=test_connection.alias,\n            run_syncdb=True,\n        )\n        mocked_call_command.assert_any_call('createcachetable', database=test_connection.alias)\n", "    def test_create_test_db(self, mock_create_test_db, mock_call_command):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = BaseDatabaseCreation(test_connection)\n        test_database_name = creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n\n        # Assert that the test database name is correct\n        self.assertEqual(test_database_name, creation._get_test_db_name())\n\n        # Ensure the _create_test_db method was called\n        mock_create_test_db.assert_called_once_with(verbosity=0, autoclobber=True, keepdb=False)\n\n        # Ensure the migrate command was called with the correct parameters\n        mock_call_command.assert_any_call(\n            'migrate',\n            verbosity=0,\n            interactive=False,\n            database=test_connection.alias,\n            run_syncdb=True,\n        )\n\n        # Ensure the createcachetable command was called\n        mock_call_command.assert_any_call('createcachetable', database=test_connection.alias)\n", "    def test_create_and_destroy_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n\n        try:\n            with mock.patch.object(creation, '_create_test_db') as mock_create_test_db:\n                test_db_name = creation.create_test_db(verbosity=0, autoclobber=True)\n                self.assertEqual(test_db_name, creation._get_test_db_name())\n                mock_create_test_db.assert_called_once()\n\n            with mock.patch.object(creation, '_destroy_test_db') as mock_destroy_test_db:\n                creation.destroy_test_db(old_database_name, verbosity=0)\n                mock_destroy_test_db.assert_called_once_with(test_db_name, 0)\n        finally:\n            # Ensure that the old database name is restored\n            self.assertEqual(test_connection.settings_dict['NAME'], old_database_name)\n", "    def test_clone_test_db(self, mock_clone_test_db):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        \n        creation.clone_test_db(suffix=\"clone_suffix\", verbosity=1, keepdb=True)\n        \n        mock_clone_test_db.assert_called_once_with(\"clone_suffix\", verbosity=1, keepdb=True)\n", "    def test_sql_table_creation_suffix(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        suffix = creation.sql_table_creation_suffix()\n        self.assertEqual(suffix, '')\n"], "sample_250": ["    def test_iso_8601_format(self):\n        dt = datetime(2023, 10, 13, 15, 45, 30, 123456)\n        self.assertEqual(dateformat.format(dt, 'c'), '2023-10-13T15:45:30.123456')\n", "def test_iso_8601_year(self):\n    dt = datetime(2023, 1, 1)\n    self.assertEqual(dateformat.format(dt, 'o'), '2022')\n    \n    dt = datetime(2023, 12, 31)\n    self.assertEqual(dateformat.format(dt, 'o'), '2023')\n    \n    dt = datetime(2024, 1, 1)\n    self.assertEqual(dateformat.format(dt, 'o'), '2024')\n    \n    dt = datetime(2024, 12, 31)\n    self.assertEqual(dateformat.format(dt, 'o'), '2024')\n", "    def test_iso8601_year(self):\n        dt = datetime(2021, 1, 4)  # January 4th is in the first week of ISO week date year 2021\n        self.assertEqual(dateformat.format(dt, 'o'), '2021')\n", "    def test_ordinal_suffix(self):\n        dates_suffixes = {\n            1: 'st', 2: 'nd', 3: 'rd', 4: 'th', 11: 'th', 21: 'st', 22: 'nd', 23: 'rd', 31: 'st'\n        }\n        for day, suffix in dates_suffixes.items():\n            d = date(2023, 1, day)\n            self.assertEqual(dateformat.format(d, 'S'), suffix)\n", "    def test_iso_year_week_number(self):\n        # ISO week date: 2023-W05-5 corresponds to Friday, 3rd Feb 2023\n        dt = datetime(2023, 2, 3)\n        self.assertEqual(dateformat.format(dt, 'o'), '2023')\n        self.assertEqual(dateformat.format(dt, 'W'), '05')\n"], "sample_251": ["def test_combined_expression_with_different_connectors(self):\n    # Test combined expressions with different connectors\n    book = Book.objects.filter(isbn='159059725').annotate(\n        combined_add=ExpressionWrapper(F('pages') + F('rating'), output_field=IntegerField()),\n        combined_sub=ExpressionWrapper(F('pages') - F('rating'), output_field=IntegerField()),\n        combined_mul=ExpressionWrapper(F('pages') * F('rating'), output_field=FloatField()),\n        combined_div=ExpressionWrapper(F('pages') / F('rating'), output_field=FloatField()),\n        combined_mod=ExpressionWrapper(F('pages') % 100, output_field=IntegerField()),\n        combined_pow=ExpressionWrapper(F('pages') ** 2, output_field=IntegerField()),\n    ).first()\n    self.assertEqual(book.combined_add, int(book.pages + book.rating))\n    self.assertEqual(book.combined_sub, int(book.pages - book.rating))\n    self.assertEqual(book.combined_mul, float(book.pages * book.rating))\n    self.assertEqual(book.combined_div, float(book.pages / book.rating))\n    self.assertEqual(book.combined_mod, book.pages % 100)\n    self.assertEqual(book.combined_pow, book.pages ** 2)\n", "def test_combined_expression_with_addition(self):\n    books = Book.objects.annotate(\n        total_value=F('price') + F('rating')\n    )\n    for book in books:\n        self.assertEqual(book.total_value, book.price + book.rating)\n", "    def test_combined_expression(self):\n        # Test for CombinedExpression using various operators\n        combined_add = CombinedExpression(Value(5), Combinable.ADD, Value(3))\n        combined_sub = CombinedExpression(Value(5), Combinable.SUB, Value(3))\n        combined_mul = CombinedExpression(Value(5), Combinable.MUL, Value(3))\n        combined_div = CombinedExpression(Value(5), Combinable.DIV, Value(3))\n        combined_pow = CombinedExpression(Value(5), Combinable.POW, Value(3))\n        combined_mod = CombinedExpression(Value(5), Combinable.MOD, Value(3))\n\n        self.assertEqual(str(combined_add), '5 + 3')\n        self.assertEqual(str(combined_sub), '5 - 3')\n        self.assertEqual(str(combined_mul), '5 * 3')\n        self.assertEqual(str(combined_div), '5 / 3')\n        self.assertEqual(str(combined_pow), '5 ^ 3')\n        self.assertEqual(str(combined_mod), '5 %% 3')\n\n        # Check the output field resolution\n        self.assertEqual(combined_add.output_field, IntegerField())\n        self.assertEqual(combined_sub.output_field, IntegerField())\n        self.assertEqual(combined_mul.output_field, IntegerField())\n        self.assertEqual(combined_div.output_field, IntegerField())\n        self.assertEqual(combined_pow.output_field, IntegerField())\n        self.assertEqual(combined_mod.output_field, IntegerField())\n", "def test_combined_expression_with_raw_sql_annotation(self):\n    # Use RawSQL to create a custom SQL expression and combine it with another expression.\n    books = Book.objects.annotate(\n        custom_pages=RawSQL(\"pages + %s\", [5], output_field=IntegerField())\n    ).annotate(\n        combined_expression=ExpressionWrapper(F('custom_pages') + 10, output_field=IntegerField())\n    )\n    for book in books:\n        self.assertEqual(book.combined_expression, book.pages + 15)\n", "def test_combined_expression(self):\n    combined_expr = CombinedExpression(Value(3), CombinedExpression.ADD, Value(5))\n    annotated_books = Book.objects.annotate(combined_value=combined_expr)\n    for book in annotated_books:\n        self.assertEqual(book.combined_value, 8)\n"], "sample_252": ["    def test_deep_key_transform_numeric_lookups(self):\n        tests = [\n            ('value__p__lt', 5, [self.objs[4]]),\n            ('value__p__lte', 4.2, [self.objs[4]]),\n            ('value__p__gt', 4, [self.objs[4]]),\n            ('value__p__gte', 4.2, [self.objs[4]]),\n        ]\n        for lookup, value, expected in tests:\n            with self.subTest(lookup=lookup, value=value):\n                self.assertSequenceEqual(\n                    NullableJSONModel.objects.filter(**{lookup: value}),\n                    expected,\n                )\n", "    def test_key_transform_with_nested_keys(self):\n        obj = NullableJSONModel.objects.create(value={\"a\": {\"b\": {\"c\": \"d\"}}})\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__a__b__c=\"d\").get(), \n            obj\n        )\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__a__b__c__exact=\"d\").get(), \n            obj\n        )\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__a__b__c__iexact=\"D\").get(), \n            obj\n        )\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__a__b__c__contains=\"d\").get(), \n            obj\n        )\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__a__b__c__icontains=\"D\").get(), \n            obj\n        )\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__a__b__c__startswith=\"d\").get(), \n            obj\n        )\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__a__b__c__istartswith=\"D\").get(), \n            obj\n        )\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__a__b__c__endswith=\"d\").get(), \n            obj\n        )\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__a__b__c__iendswith=\"D\").get(), \n            obj\n        )\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__a__b__c__regex=r'^d$').get(), \n            obj\n        )\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__a__b__c__iregex=r'^D$').get(), \n            obj\n        )\n", "    def test_contains_mixed(self):\n        # Test if JSONField contains can handle mixed types within arrays\n        value = {\n            'a': [1, 'b', {'c': [2, 'd']}, [3, 'e']],\n            'f': {'g': [4, 'h'], 'i': [{'j': 5}, 'k']}\n        }\n        obj = NullableJSONModel.objects.create(value=value)\n        obj.refresh_from_db()\n        tests = [\n            ({'a': [1, 'b']}, [obj]),\n            ({'a': [1, 'b', {'c': [2]}]}, [obj]),\n            ({'a': [{'c': [2, 'd']}]}, [obj]),\n            ({'a': [[3, 'e']]}, [obj]),\n            ({'f': {'g': [4]}}, [obj]),\n            ({'f': {'i': [{'j': 5}]}}, [obj]),\n            ({'f': {'i': ['k']}}, [obj]),\n        ]\n        for sub_value, expected in tests:\n            with self.subTest(sub_value=sub_value):\n                qs = NullableJSONModel.objects.filter(value__contains=sub_value)\n                self.assertSequenceEqual(qs, expected)\n", "    def test_get_internal_type(self):\n        field = models.JSONField()\n        self.assertEqual(field.get_internal_type(), 'JSONField')\n", "    def test_custom_json_path_compilation(self):\n        key_transforms = ['a', 'b', '1', 'c']\n        expected_path = '$.\"a\".\"b\"[1].\"c\"'\n        self.assertEqual(compile_json_path(key_transforms), expected_path)\n\n        key_transforms_without_root = ['x', 'y', 'z']\n        expected_path_without_root = '.\"x\".\"y\".\"z\"'\n        self.assertEqual(compile_json_path(key_transforms_without_root, include_root=False), expected_path_without_root)\n"], "sample_253": ["    def test_ensure_echo_on_with_echo_off(self, mocked_stdin, mocked_termios):\n        # Simulate terminal state with ECHO off\n        attrs = [0] * 4\n        attrs[3] = 0  # ECHO is off\n        mocked_termios.tcgetattr.return_value = attrs\n        mocked_stdin.isatty.return_value = True\n\n        autoreload.ensure_echo_on()\n\n        # Check if ECHO was turned on\n        attrs[3] |= mocked_termios.ECHO\n        mocked_termios.tcsetattr.assert_called_once_with(mocked_stdin, mocked_termios.TCSANOW, attrs)\n", "    def test_ensure_echo_on(self, mocked_stdin, mocked_termios):\n        mocked_stdin.isatty.return_value = True\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 0)\n\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0, 0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value[3] = 0\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "    def test_ensure_echo_on_not_a_tty(self, mocked_stdin, mocked_termios):\n        mocked_stdin.isatty.return_value = False\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_termios.tcgetattr.called)\n        self.assertFalse(mocked_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_enabled(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        attrs = [0, 0, 0, 0b00000000]\n        mock_tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        self.assertEqual(mock_tcsetattr.call_count, 0)\n", "    def test_ensure_echo_on(self, mocked_signal, mocked_tcsetattr, mocked_tcgetattr, mocked_isatty):\n        attrs = [0, 0, 0, 0]\n        mocked_tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_signal.called)\n        self.assertFalse(mocked_tcsetattr.called)\n\n        # When ECHO is disabled\n        attrs[3] &= ~termios.ECHO\n        mocked_tcgetattr.return_value = attrs\n\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_signal.called)\n        self.assertTrue(mocked_tcsetattr.called)\n"], "sample_254": ["    def test_save_as_new_functionality(self):\n        \"\"\"\n        Test the 'save as new' functionality in the admin interface.\n        \"\"\"\n        # Create initial Holder instance\n        holder = Holder.objects.create(dummy=42)\n        Inner.objects.create(dummy=42, holder=holder)\n\n        # Log in as superuser\n        self.client.force_login(self.superuser)\n\n        # Access change form for the created Holder instance\n        response = self.client.get(reverse('admin:admin_inlines_holder_change', args=(holder.id,)))\n        self.assertEqual(response.status_code, 200)\n\n        # Post data to save the existing Holder instance as a new one\n        data = {\n            'dummy': 43,\n            'inner_set-TOTAL_FORMS': 1,\n            'inner_set-INITIAL_FORMS': 1,\n            'inner_set-MAX_NUM_FORMS': 1000,\n            'inner_set-0-id': '',\n            'inner_set-0-dummy': 43,\n            'inner_set-0-holder': holder.id,\n            '_saveasnew': 'Save as new',\n        }\n        response = self.client.post(reverse('admin:admin_inlines_holder_change', args=(holder.id,)), data)\n\n        # Check that the response is a redirect to the new object's change form\n        self.assertEqual(response.status_code, 302)\n        new_holder = Holder.objects.get(dummy=43)\n        self.assertRedirects(response, reverse('admin:admin_inlines_holder_change', args=(new_holder.id,)))\n\n        # Check that the new object is created with the correct data\n        self.assertEqual(Holder.objects.count(), 2)\n        self.assertEqual(Inner.objects.count(), 2)\n        self.assertEqual(new_holder.inner_set.first().dummy, 43)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_get_content_type_for_model(self):\n        \"\"\"\n        Test get_content_type_for_model utility function.\n        \"\"\"\n        from django.contrib.contenttypes.models import ContentType\n        from .models import Holder\n\n        # Create an instance of the Holder model\n        holder = Holder.objects.create(dummy=13)\n\n        # Use the utility function to get the content type\n        content_type = get_content_type_for_model(holder)\n\n        # Check if the content type matches the Holder model\n        expected_content_type = ContentType.objects.get_for_model(Holder, for_concrete_model=False)\n        self.assertEqual(content_type, expected_content_type)\n", "    def setUpTestData(cls):\n        super().setUpTestData()\n        cls.holder = Holder.objects.create(dummy=13)\n", "    def test_foreignkey_widget_wrapping(self):\n        \"\"\"Test if the ForeignKey field widget is properly wrapped with RelatedFieldWidgetWrapper.\"\"\"\n        class ForeignKeyModelAdmin(ModelAdmin):\n            pass\n\n        class RelatedModelAdmin(ModelAdmin):\n            pass\n\n        related_model = Author.objects.create(name=\"Related\")\n        modeladmin = ForeignKeyModelAdmin(Holder2, admin_site)\n        admin_site.register(Author, RelatedModelAdmin)\n        request = self.factory.get(reverse('admin:admin_inlines_holder2_add'))\n        request.user = User(username='super', is_superuser=True)\n\n        # Ensure the ForeignKey field is wrapped with RelatedFieldWidgetWrapper\n        field = Holder2._meta.get_field('author')\n        form_field = modeladmin.formfield_for_dbfield(field, request)\n        self.assertIsInstance(form_field.widget, widgets.RelatedFieldWidgetWrapper)\n        self.assertEqual(form_field.widget.related_model, Author)\n        \n        # Clean up by unregistering the model admin\n        admin_site.unregister(Author)\n"], "sample_256": ["    def test_invalid_password_format(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'invalid_password_format'\n        context = widget.get_context('password', value, {})\n        self.assertIn({'label': str(_(\"Invalid password format or unknown hashing algorithm.\"))}, context['summary'])\n", "    def test_unicode_ci_compare_equal(self):\n        self.assertTrue(_unicode_ci_compare('test', 'TEST'))\n        self.assertTrue(_unicode_ci_compare('Stra\u00dfe', 'STRASSE'))\n        self.assertTrue(_unicode_ci_compare('\u0130stanbul', 'ISTANBUL'))\n", "    def test_invalid_password_format(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'invalid_password_format'\n        rendered = widget.render('name', value, {'id': 'id_password'})\n        self.assertIn(_(\"Invalid password format or unknown hashing algorithm.\"), rendered)\n", "    def test_case_insensitive_match(self):\n        self.assertTrue(_unicode_ci_compare('test', 'TEST'))\n        self.assertTrue(_unicode_ci_compare('tEst', 'TEst'))\n        self.assertTrue(_unicode_ci_compare('TESt', 'test'))\n", "    def test_unicode_ci_compare_equal(self):\n        self.assertTrue(_unicode_ci_compare('Stra\u00dfe', 'strasse'))\n"], "sample_255": ["    def test_handle_one_request(self):\n        \"\"\"Test WSGIRequestHandler's handle_one_request method.\"\"\"\n            start_response('200 OK', [])\n            return [b\"Hello World\"]\n\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n\n        rfile = BytesIO()\n        rfile.write(b\"GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n\")\n        rfile.seek(0)\n\n        wfile = BytesIO()\n        \n        handler = WSGIRequestHandler(Stub(rfile=rfile, wfile=wfile, makefile=lambda *args, **kwargs: rfile if 'rb' in args else wfile), '192.168.0.2', None)\n        server = WSGIServer(('localhost', 0), WSGIRequestHandler)\n        server.set_app(test_app)\n        handler.server = server\n\n        handler.handle_one_request()\n\n        wfile.seek(0)\n        response = wfile.read().decode('utf-8')\n        self.assertIn(\"200 OK\", response)\n        self.assertIn(\"Hello World\", response)\n", "    def test_handle_one_request(self):\n        \"\"\"Test that handle_one_request processes the request and closes the connection.\"\"\"\n            start_response('200 OK', [])\n            return [b\"Hello World\"]\n\n        rfile = BytesIO()\n        rfile.write(b\"GET / HTTP/1.1\\r\\n\")\n        rfile.write(b\"Host: localhost\\r\\n\")\n        rfile.write(b\"Content-Length: 0\\r\\n\")\n        rfile.write(b\"\\r\\n\")\n        rfile.seek(0)\n\n        class UnclosableBytesIO(BytesIO):\n                pass\n\n        wfile = UnclosableBytesIO()\n\n            if mode == 'rb':\n                return rfile\n            elif mode == 'wb':\n                return wfile\n\n        request = Stub(makefile=makefile, get_environ=lambda: {}, connection=socket.socket())\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n        handler.server = Stub(get_app=lambda: test_app)\n        handler.handle_one_request()\n\n        output = wfile.getvalue()\n        self.assertIn(b\"HTTP/1.1 200 OK\", output)\n        self.assertIn(b\"Hello World\", output)\n        self.assertTrue(handler.close_connection)\n", "    def test_handle_error_broken_pipe(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n        with captured_stderr() as stderr:\n            handler.handle_error()\n        self.assertNotIn('Broken pipe from', stderr.getvalue())\n", "    def test_handle_error_broken_pipe(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n        with captured_stderr() as stderr, self.assertLogs('django.server', 'INFO') as cm:\n            try:\n                raise BrokenPipeError\n            except BrokenPipeError:\n                handler.handle_error(request, ('192.168.0.2', 12345))\n\n        self.assertIn('Broken pipe from (\\'192.168.0.2\\', 12345)', stderr.getvalue())\n        self.assertIn('Broken pipe from', cm.output[0])\n", "    def test_handle_error_broken_pipe(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n        with captured_stderr() as stderr:\n            try:\n                raise BrokenPipeError\n            except BrokenPipeError:\n                handler.handle_error()\n        \n        self.assertIn(\"Broken pipe from\", stderr.getvalue())\n"], "sample_257": ["    def test_preprocess_lhs(self):\n        key_transform = KeyTransform('key_name')\n        with mock.patch.object(KeyTransform, 'lhs', new_callable=mock.PropertyMock) as mock_lhs:\n            mock_lhs.return_value = KeyTransform('inner_key')\n            compiler = mock.Mock()\n            connection = mock.Mock()\n            lhs, params, key_transforms = key_transform.preprocess_lhs(compiler, connection)\n            self.assertEqual(lhs, compiler.compile.return_value[0])\n            self.assertEqual(params, compiler.compile.return_value[1])\n            self.assertEqual(key_transforms, ['inner_key', 'key_name'])\n", "    def test_compile_json_path_empty(self):\n        self.assertEqual(compile_json_path([]), '$')\n", "    def test_preprocess_lhs(self):\n        key_transform = KeyTransform('key_name', F('field'))\n        compiler = mock.Mock()\n        connection = mock.Mock(vendor='sqlite')\n        lhs, params, key_transforms = key_transform.preprocess_lhs(compiler, connection)\n        self.assertEqual(lhs, compiler.compile.return_value[0])\n        self.assertEqual(params, compiler.compile.return_value[1])\n        self.assertEqual(key_transforms, ['key_name'])\n", "    def test_call(self):\n        factory = KeyTransformFactory('test_key')\n        transform_instance = factory('lhs_expression')\n        self.assertIsInstance(transform_instance, KeyTransform)\n        self.assertEqual(transform_instance.key_name, 'test_key')\n        self.assertEqual(transform_instance.lhs, 'lhs_expression')\n", "    def test_key_transform_factory(self):\n        factory = KeyTransformFactory('key_name')\n        transform = factory('value')\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, 'key_name')\n"], "sample_258": ["    def test_receiver_signal_list(self):\n        @receiver([a_signal, b_signal, c_signal])\n            self.state.append(val)\n        self.state = []\n        a_signal.send(sender=self, val='a_signal')\n        b_signal.send(sender=self, val='b_signal')\n        c_signal.send(sender=self, val='c_signal')\n        self.assertEqual(self.state, ['a_signal', 'b_signal', 'c_signal'])\n", "    def test_multiple_signals(self):\n        @receiver([a_signal, b_signal])\n            self.state.append(val)\n        \n        self.state = []\n        a_signal.send(sender=self, val='a_signal_value')\n        b_signal.send(sender=self, val='b_signal_value')\n        self.assertEqual(self.state, ['a_signal_value', 'b_signal_value'])\n        a_signal.disconnect(f)\n        b_signal.disconnect(f)\n        self.assertTestIsClean(a_signal)\n        self.assertTestIsClean(b_signal)\n", "    def test_disconnect_by_dispatch_uid(self):\n            self.state = val\n\n            self.state = val\n\n        a_signal.connect(receiver_1, dispatch_uid=\"uid1\")\n        a_signal.connect(receiver_2, dispatch_uid=\"uid2\")\n        \n        self.assertEqual(len(a_signal.receivers), 2)\n        \n        # Disconnect using dispatch_uid\n        a_signal.disconnect(dispatch_uid=\"uid1\")\n        self.assertEqual(len(a_signal.receivers), 1)\n\n        # Ensure the correct receiver remains connected\n        a_signal.send(sender=self, val=True)\n        self.assertTrue(self.state)\n\n        # Clean up\n        a_signal.disconnect(dispatch_uid=\"uid2\")\n        self.assertTestIsClean(a_signal)\n", "    def test_disconnect_with_dispatch_uid(self):\n            pass\n\n            pass\n\n        dispatch_uid = \"unique-id\"\n        a_signal.connect(receiver_1, dispatch_uid=dispatch_uid)\n        a_signal.connect(receiver_2, dispatch_uid=dispatch_uid)\n        self.assertEqual(len(a_signal.receivers), 1)\n\n        disconnected = a_signal.disconnect(dispatch_uid=dispatch_uid)\n        self.assertTrue(disconnected)\n        self.assertTestIsClean(a_signal)\n", "    def test_disconnect_by_dispatch_uid(self):\n            pass\n\n            pass\n\n        a_signal.connect(receiver_1, dispatch_uid=\"uid1\")\n        a_signal.connect(receiver_2, dispatch_uid=\"uid2\")\n        \n        # Ensure both receivers are connected\n        self.assertEqual(len(a_signal.receivers), 2)\n        \n        # Disconnect receiver_1 using its dispatch_uid\n        a_signal.disconnect(dispatch_uid=\"uid1\")\n        \n        # Ensure only one receiver is left\n        self.assertEqual(len(a_signal.receivers), 1)\n        \n        # Disconnect receiver_2 using its dispatch_uid\n        a_signal.disconnect(dispatch_uid=\"uid2\")\n        \n        # Ensure no receivers are left\n        self.assertEqual(len(a_signal.receivers), 0)\n        \n        self.assertTestIsClean(a_signal)\n"], "sample_259": ["    def test_nested_prefetch(self):\n        \"\"\"Test nested prefetch_related_objects with multiple levels of relationships.\"\"\"\n        authors = list(Author.objects.all())\n        with self.assertNumQueries(3):\n            prefetch_related_objects(authors, 'books__authors__first_book')\n\n        with self.assertNumQueries(0):\n            for author in authors:\n                for book in author.books.all():\n                    for nested_author in book.authors.all():\n                        self.assertIsNotNone(nested_author.first_book)\n", "def test_empty_queryset(self):\n    empty_books = Book.objects.none()\n    with self.assertNumQueries(0):\n        prefetch_related_objects(empty_books, 'authors')\n    with self.assertNumQueries(0):\n        for book in empty_books:\n            self.assertEqual(list(book.authors.all()), [])\n", "    def test_bulk_create(self):\n        initial_count = Book.objects.count()\n        new_books = [\n            Book(title='New Book 1'),\n            Book(title='New Book 2'),\n            Book(title='New Book 3'),\n        ]\n        created_books = Book.objects.bulk_create(new_books)\n        \n        self.assertEqual(Book.objects.count(), initial_count + 3)\n        self.assertEqual(len(created_books), 3)\n        self.assertEqual(created_books[0].title, 'New Book 1')\n        self.assertEqual(created_books[1].title, 'New Book 2')\n        self.assertEqual(created_books[2].title, 'New Book 3')\n", "def test_foreignkey_forward_with_filter(self):\n    authors = list(Author.objects.filter(name__startswith='C'))\n    with self.assertNumQueries(1):\n        prefetch_related_objects(authors, 'first_book')\n\n    with self.assertNumQueries(0):\n        [author.first_book for author in authors]\n", "    def test_bulk_create(self):\n        new_books = [\n            Book(title='The Great Gatsby'),\n            Book(title='To Kill a Mockingbird'),\n            Book(title='1984'),\n        ]\n        with self.assertNumQueries(1):\n            created_books = Book.objects.bulk_create(new_books)\n        \n        self.assertEqual(len(created_books), 3)\n        self.assertTrue(all(book.id for book in created_books))\n"], "sample_260": ["    def test_create_model_add_constraint(self):\n        \"\"\"\n        AddConstraint should optimize into CreateModel.\n        \"\"\"\n        constraint = models.CheckConstraint(check=models.Q(age__gte=18), name=\"age_gte_18\")\n        managers = [('objects', EmptyManager())]\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n                migrations.AddConstraint(\"Foo\", constraint),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                    constraints=[constraint],\n                ),\n            ],\n        )\n", "    def test_create_model_add_constraint(self):\n        \"\"\"\n        AddConstraint should optimize into CreateModel.\n        \"\"\"\n        constraint = models.UniqueConstraint(fields=['name'], name='unique_name')\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={'verbose_name': 'Foo'},\n                ),\n                migrations.AddConstraint(\"Foo\", constraint),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\n                        'verbose_name': 'Foo',\n                        'constraints': [constraint],\n                    },\n                ),\n            ],\n        )\n", "    def test_create_model_rename_field_reordering(self):\n        \"\"\"\n        RenameField optimizes into CreateModel even when CreateModel operations\n        are reordered due to dependencies.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel('Link', [('url', models.TextField())]),\n                migrations.CreateModel('Foo', [('name', models.CharField(max_length=255))]),\n                migrations.RenameField('Foo', 'name', 'title'),\n                migrations.AddField('Foo', 'link', models.ForeignKey('migrations.Link', models.CASCADE)),\n            ],\n            [\n                migrations.CreateModel('Link', [('url', models.TextField())]),\n                migrations.CreateModel('Foo', [\n                    ('title', models.CharField(max_length=255)),\n                    ('link', models.ForeignKey('migrations.Link', models.CASCADE)),\n                ]),\n            ],\n        )\n", "    def test_rename_model_delete_model(self):\n        \"\"\"\n        RenameModel followed by DeleteModel should optimize to just DeleteModel.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.RenameModel(\"Foo\", \"Bar\"),\n                migrations.DeleteModel(\"Bar\"),\n            ],\n            [\n                migrations.DeleteModel(\"Foo\"),\n            ],\n        )\n", "    def test_create_model_add_index(self):\n        \"\"\"\n        AddIndex should optimize into CreateModel.\n        \"\"\"\n        index = models.Index(fields=['name'], name='foo_name_idx')\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                ),\n                migrations.AddIndex(\"Foo\", index),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={'indexes': [index]},\n                ),\n            ],\n        )\n"], "sample_261": ["def test_invalid_duration(self):\n    invalid_durations = [\n        '',  # Empty string\n        'P1Y2M3DT4H5M6S',  # ISO 8601 with unsupported parts (years, months)\n        '1 year 2 months',  # Natural language\n        '1:61',  # Invalid minutes\n        '24:00:00',  # Invalid hour for non-day duration\n        '1:1:1:1',  # Invalid format\n        'abcd',  # Completely invalid\n    ]\n    for value in invalid_durations:\n        with self.subTest(value=value):\n            self.assertIsNone(parse_duration(value))\n", "    def test_iso8601_format(self):\n        test_values = (\n            ('P3D', timedelta(days=3)),\n            ('PT5H', timedelta(hours=5)),\n            ('PT45M', timedelta(minutes=45)),\n            ('PT30S', timedelta(seconds=30)),\n            ('P1DT12H', timedelta(days=1, hours=12)),\n            ('P2DT3H4M', timedelta(days=2, hours=3, minutes=4)),\n            ('P2DT3H4M5S', timedelta(days=2, hours=3, minutes=4, seconds=5)),\n            ('P2DT3H4M5.678S', timedelta(days=2, hours=3, minutes=4, seconds=5, microseconds=678000)),\n            ('-P3D', timedelta(days=-3)),\n            ('-PT5H', timedelta(hours=-5)),\n            ('-PT45M', timedelta(minutes=-45)),\n            ('-PT30S', timedelta(seconds=-30)),\n            ('-P1DT12H', timedelta(days=-1, hours=-12)),\n            ('-P2DT3H4M', timedelta(days=-2, hours=-3, minutes=-4)),\n            ('-P2DT3H4M5S', timedelta(days=-2, hours=-3, minutes=-4, seconds=-5)),\n            ('-P2DT3H4M5.678S', timedelta(days=-2, hours=-3, minutes=-4, seconds=-5, microseconds=-678000)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P1D', timedelta(days=1)),\n        ('P1DT1H', timedelta(days=1, hours=1)),\n        ('P1DT1H30M', timedelta(days=1, hours=1, minutes=30)),\n        ('P1DT1H30M15S', timedelta(days=1, hours=1, minutes=30, seconds=15)),\n        ('PT1H30M', timedelta(hours=1, minutes=30)),\n        ('PT1H30M15S', timedelta(hours=1, minutes=30, seconds=15)),\n        ('PT0.5H', timedelta(minutes=30)),\n        ('P1DT1H30M15.5S', timedelta(days=1, hours=1, minutes=30, seconds=15.5)),\n        ('P1.5D', timedelta(days=1.5)),\n        ('-P1D', timedelta(days=-1)),\n        ('-P1DT1H', timedelta(days=-1, hours=-1)),\n        ('-P1DT1H30M', timedelta(days=-1, hours=-1, minutes=-30)),\n        ('-PT1H30M', timedelta(hours=-1, minutes=-30)),\n        ('-PT1H30M15S', timedelta(hours=-1, minutes=-30, seconds=-15)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "    def test_invalid_duration(self):\n        invalid_inputs = [\n            '',  # Empty string\n            'P',  # Incomplete ISO 8601 duration\n            'P1Y',  # Year is not supported\n            'P1M',  # Month is not supported\n            'P1DT',  # Incomplete ISO 8601 duration\n            '15:30:30:30',  # Too many colons\n            '4 days 25:00:00',  # Invalid hour in PostgreSQL format\n            '4 days 10:61:00',  # Invalid minute in PostgreSQL format\n            '4 days 10:00:61',  # Invalid second in PostgreSQL format\n        ]\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_parse_invalid_duration_formats(self):\n        # Test invalid duration formats\n        invalid_durations = [\n            'invalid',  # not a duration at all\n            '1 hour',  # not a valid format\n            'P1Y',  # ISO 8601 duration with years, which is unsupported\n            'P1M',  # ISO 8601 duration with months, which is unsupported\n            'P1W',  # ISO 8601 duration with weeks, which is unsupported\n            'P1DT1H',  # ISO 8601 duration with mixed days and hours, invalid for timedelta\n            'P1DT',  # ISO 8601 duration with just day part\n            'PT1H',  # ISO 8601 duration with just hour part\n            'PT1M',  # ISO 8601 duration with just minute part\n            'PT1S',  # ISO 8601 duration with just second part\n            '4 days 25:00:00',  # hours out of range\n            '4 days 00:61:00',  # minutes out of range\n            '4 days 00:00:61',  # seconds out of range\n            '4 days -25:00:00',  # negative hours out of range\n            '4 days 00:-61:00',  # negative minutes out of range\n            '4 days 00:00:-61',  # negative seconds out of range\n        ]\n        for invalid_duration in invalid_durations:\n            with self.subTest(invalid_duration=invalid_duration):\n                self.assertIsNone(parse_duration(invalid_duration))\n"], "sample_262": ["    def test_simple_lazy_object(self):\n        call_count = 0\n\n            nonlocal call_count\n            call_count += 1\n            return [1, 2, 3]\n\n        obj = SimpleLazyObject(setup_func)\n        self.assertEqual(call_count, 0)\n        self.assertEqual(obj._wrapped, empty)\n        self.assertEqual(obj[0], 1)\n        self.assertEqual(call_count, 1)\n        self.assertEqual(obj[0], 1)\n        self.assertEqual(call_count, 1)\n        self.assertEqual(list(obj), [1, 2, 3])\n        self.assertEqual(call_count, 1)\n", "    def test_partition_function(self):\n        \"\"\"partition function splits values correctly based on predicate.\"\"\"\n        values = [1, 2, 3, 4, 5]\n        predicate = lambda x: x > 3\n        true_values, false_values = partition(predicate, values)\n        self.assertEqual(true_values, [4, 5])\n        self.assertEqual(false_values, [1, 2, 3])\n", "    def test_lazy_str_bytes_incompatibility(self):\n        \"\"\"Test that lazy() raises an error when called with both str and bytes result classes.\"\"\"\n        with self.assertRaises(AssertionError):\n            lazy(lambda: \"test\", str, bytes)\n", "    def test_keep_lazy(self):\n        # Define a function that returns the sum of its arguments\n        @keep_lazy(int)\n            return x + y\n\n        # Test immediate evaluation when no lazy arguments are passed\n        self.assertEqual(add(1, 2), 3)\n\n        # Test lazy evaluation when lazy arguments are passed\n        lazy_4 = lazy(lambda: 4, int)\n        lazy_5 = lazy(lambda: 5, int)\n        lazy_sum = add(lazy_4(), lazy_5())\n        self.assertEqual(lazy_sum, 9)\n", "    def test_lazy_proxy_unpickle(self):\n        original_object = 42\n        lazified = lazy(lambda: original_object, int)\n        proxy_instance = lazified()\n        pickled_proxy = proxy_instance.__reduce__()\n        unpickled_object = _lazy_proxy_unpickle(*pickled_proxy[1])\n        self.assertEqual(unpickled_object(), original_object)\n"], "sample_263": ["    def test_handle_exception_traceback(self):\n        # Mock the options for the handle method to raise an exception and ensure traceback is shown\n        options = {\n            'format': 'json',\n            'indent': None,\n            'database': 'default',\n            'exclude': [],\n            'output': None,\n            'traceback': True,\n            'use_natural_foreign_keys': False,\n            'use_natural_primary_keys': False,\n            'use_base_manager': False,\n            'primary_keys': ''\n        }\n        with self.assertRaises(CommandError):\n            with mock.patch.object(Command, 'handle', side_effect=Exception('Test Exception')) as mocked_handle:\n                management.call_command('dumpdata', 'fixtures', **options)\n                mocked_handle.assert_called()\n", "    def test_invalid_format_argument(self):\n        with self.assertRaisesMessage(CommandError, \"Unknown serialization format: invalid_format\"):\n            management.call_command('dumpdata', 'fixtures', format='invalid_format')\n", "    def test_dumpdata_with_invalid_format(self):\n        with self.assertRaisesMessage(management.CommandError, \"Unknown serialization format: invalidformat\"):\n            management.call_command('dumpdata', 'fixtures', format='invalidformat')\n", "    def test_handle_with_multiple_excludes(self):\n        \"\"\"\n        Test handle method with multiple exclude arguments.\n        \"\"\"\n        management.call_command('loaddata', 'fixture1.json', verbosity=0)\n        output = StringIO()\n        with mock.patch('django.core.management.base.BaseCommand.stdout', new=output):\n            management.call_command(\n                'dumpdata', 'fixtures', \n                exclude=['fixtures.Article', 'fixtures.Book'], \n                format='json'\n            )\n        self.assertJSONEqual(\n            output.getvalue().strip(),\n            '[{\"pk\": 1, \"model\": \"fixtures.category\", \"fields\": {\"description\": \"Latest news stories\", \"title\": \"News Stories\"}}]'\n        )\n", "def test_handle_method_with_invalid_format(self):\n    with self.assertRaisesMessage(CommandError, \"Unknown serialization format: invalid_format\"):\n        management.call_command('dumpdata', '--format=invalid_format', verbosity=0)\n"], "sample_264": ["    def test_empty_messages(self):\n        \"\"\"\n        Test that an empty list of messages is handled correctly by the storage.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Test storing an empty list of messages\n        storage._store([], response)\n        self.assertEqual(storage._decode(response.cookies['messages'].value), [])\n\n        # Test retrieving an empty list of messages\n        set_cookie_data(storage, [])\n        self.assertEqual(list(storage), [])\n", "    def test_empty_message(self):\n        \"\"\"\n        Test that an empty message list is correctly handled and does not set a cookie.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add no messages to the storage\n        storage.update(response)\n\n        # Check that no messages are stored in the cookie\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n        self.assertEqual(response.cookies.get(storage.cookie_name), None)\n", "    def test_empty_message_handling(self):\n        \"\"\"\n        Test that the storage properly handles empty messages.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        empty_messages = []\n        set_cookie_data(storage, empty_messages, encode_empty=True)\n        self.assertEqual(list(storage), [])\n\n        # Also ensure an empty message encoded and decoded properly\n        encoded = storage._encode(empty_messages, encode_empty=True)\n        self.assertIsNotNone(encoded)\n        decoded = storage._decode(encoded)\n        self.assertEqual(decoded, empty_messages)\n", "    def test_empty_message_handling(self):\n        \"\"\"\n        Test that an empty message list is properly handled and does not\n        cause any errors or unexpected behavior.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Store an empty list of messages\n        storage.update(response)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n        self.assertEqual(list(storage), [])\n\n        # Ensure the cookie is deleted (empty value)\n        self.assertEqual(response.cookies['messages'].value, '')\n        self.assertEqual(response.cookies['messages']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n", "    def test_store_and_retrieve_large_number_of_messages(self):\n        \"\"\"\n        Test storing and retrieving a large number of messages to ensure\n        that the cookie storage can handle a large payload without data loss.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        large_number_of_messages = [f\"message {i}\" for i in range(100)]\n        \n        for msg in large_number_of_messages:\n            storage.add(constants.INFO, msg)\n        \n        unstored_messages = storage.update(response)\n        \n        # Ensure all messages are stored\n        self.assertEqual(len(unstored_messages), 0)\n        \n        # Retrieve the messages from storage\n        retrieved_messages = list(storage)\n        \n        # Ensure the retrieved messages match the original messages\n        self.assertEqual(retrieved_messages, large_number_of_messages)\n"], "sample_265": ["    def test_template_render_with_invalid_template(self):\n        \"\"\"\n        Test rendering with a non-existent template should raise TemplateDoesNotExist.\n        \"\"\"\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': True,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.get_template('nonexistent_template.html').render({})\n", "    def test_template_does_not_exist_error(self):\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.get_template('non_existent_template.html')\n", "    def test_template_does_not_exist_exception(self):\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.get_template('non_existent_template.html')\n", "    def test_get_template_template_does_not_exist(self):\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.get_template('non_existent_template.html')\n", "    def test_get_template_non_existent(self):\n        \"\"\"\n        Test if get_template raises TemplateDoesNotExist for non-existent templates.\n        \"\"\"\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.get_template('non_existent_template.html')\n"], "sample_266": ["    def test_collect_sql(self):\n        \"\"\"Test collecting SQL statements from a migration plan.\"\"\"\n        migration_loader = MigrationLoader(connection)\n        plan = migration_loader.graph.forwards_plan((\"migrations\", \"0002_second\"))\n        sql_statements = migration_loader.collect_sql(plan)\n        \n        # Ensure some SQL statements were collected\n        self.assertTrue(sql_statements)\n        # Check that the collected SQL contains expected operations\n        self.assertIn('CREATE TABLE', ' '.join(sql_statements))\n        self.assertIn('ALTER TABLE', ' '.join(sql_statements))\n", "    def test_collect_sql(self):\n        \"\"\"\n        Test that collect_sql method collects SQL statements as expected.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        plan = [\n            (migration_loader.get_migration(\"migrations\", \"0001_initial\"), False),\n            (migration_loader.get_migration(\"migrations\", \"0002_second\"), False),\n        ]\n        sql_statements = migration_loader.collect_sql(plan)\n        self.assertTrue(sql_statements)\n        self.assertTrue(any(\"CREATE TABLE\" in stmt for stmt in sql_statements))\n        self.assertTrue(any(\"ALTER TABLE\" in stmt for stmt in sql_statements))\n", "    def test_detect_conflicts(self):\n        \"\"\"\n        Tests detecting conflicts with multiple leaf migrations.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n\n        # Record initial migration\n        recorder.record_applied('migrations', '0001_initial')\n        migration_loader.build_graph()\n\n        # There should be no conflicts initially\n        self.assertEqual(migration_loader.detect_conflicts(), {})\n\n        # Record conflicting migrations\n        recorder.record_applied('migrations', '0002_second')\n        recorder.record_applied('migrations', '0003_third')\n        migration_loader.build_graph()\n\n        # Now there should be conflicts\n        self.assertEqual(migration_loader.detect_conflicts(), {'migrations': ['0002_second', '0003_third']})\n", "    def test_get_migration(self):\n        \"\"\"\n        Tests the get_migration method of MigrationLoader to ensure it correctly\n        retrieves a migration.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        migration = migration_loader.get_migration(\"migrations\", \"0001_initial\")\n        self.assertEqual(migration.name, \"0001_initial\")\n", "    def test_check_key(self):\n        \"\"\"Tests the check_key method for special cases.\"\"\"\n        migration_loader = MigrationLoader(connection)\n        migration_loader.build_graph()\n\n        # Test with existing node\n        key = ('migrations', '0001_initial')\n        self.assertEqual(migration_loader.check_key(key, 'migrations'), key)\n\n        # Test with __first__ special case for the same app\n        key = ('migrations', '__first__')\n        self.assertIsNone(migration_loader.check_key(key, 'migrations'))\n\n        # Test with __latest__ special case for the same app\n        key = ('migrations', '__latest__')\n        self.assertEqual(migration_loader.check_key(key, 'migrations'), ('migrations', '0002_second'))\n\n        # Test with __first__ special case for a different app\n        key = ('another_app', '__first__')\n        migration_loader.unmigrated_apps.add('another_app')\n        self.assertIsNone(migration_loader.check_key(key, 'migrations'))\n\n        # Test with __latest__ special case for an unknown app\n        key = ('unknown_app', '__latest__')\n        with self.assertRaises(ValueError):\n            migration_loader.check_key(key, 'migrations')\n"], "sample_267": ["    def test_custom_functions(self):\n        custom_functions = [\n            ('ACOS', 1, math.acos, [0.5]),\n            ('ASIN', 1, math.asin, [0.5]),\n            ('ATAN', 1, math.atan, [1]),\n            ('ATAN2', 2, math.atan2, [1, 1]),\n            ('BITXOR', 2, operator.xor, [1, 2]),\n            ('CEILING', 1, math.ceil, [1.5]),\n            ('COS', 1, math.cos, [0]),\n            ('COT', 1, lambda x: 1 / math.tan(x), [1]),\n            ('DEGREES', 1, math.degrees, [math.pi]),\n            ('EXP', 1, math.exp, [1]),\n            ('FLOOR', 1, math.floor, [1.5]),\n            ('LN', 1, math.log, [math.e]),\n            ('LOG', 2, lambda x, y: math.log(y, x), [10, 100]),\n            ('MD5', 1, lambda x: hashlib.md5(x.encode()).hexdigest(), ['test']),\n            ('MOD', 2, math.fmod, [10, 3]),\n            ('PI', 0, lambda: math.pi, []),\n            ('POWER', 2, operator.pow, [2, 3]),\n            ('RADIANS', 1, math.radians, [180]),\n            ('REPEAT', 2, operator.mul, ['test', 2]),\n            ('REVERSE', 1, lambda x: x[::-1], ['test']),\n            ('SHA1', 1, lambda x: hashlib.sha1(x.encode()).hexdigest(), ['test']),\n            ('SHA224', 1, lambda x: hashlib.sha224(x.encode()).hexdigest(), ['test']),\n            ('SHA256', 1, lambda x: hashlib.sha256(x.encode()).hexdigest(), ['test']),\n            ('SHA384', 1, lambda x: hashlib.sha384(x.encode()).hexdigest(), ['test']),\n            ('SHA512', 1, lambda x: hashlib.sha512(x.encode()).hexdigest(), ['test']),\n            ('SIGN', 1, lambda x: (x > 0) - (x < 0), [0]),\n            ('SIN', 1", "    def setUp(self):\n        self.settings_dict = {\n            'NAME': ':memory:',\n            'OPTIONS': {},\n        }\n        self.wrapper = DatabaseWrapper(self.settings_dict)\n", "    def test_get_connection_params_no_name(self):\n        \"\"\"Test get_connection_params raises ImproperlyConfigured if NAME is missing.\"\"\"\n        from django.db.backends.sqlite3.base import DatabaseWrapper\n        settings_dict = {\n            'NAME': '',\n            'OPTIONS': {},\n        }\n        wrapper = DatabaseWrapper(settings_dict)\n        with self.assertRaisesMessage(ImproperlyConfigured, \"settings.DATABASES is improperly configured. Please supply the NAME value.\"):\n            wrapper.get_connection_params()\n", "    def setUp(self):\n        self.settings_dict = {\n            'NAME': ':memory:',\n            'OPTIONS': {},\n        }\n        self.wrapper = DatabaseWrapper(self.settings_dict)\n", "    def test_get_connection_params(self):\n        from django.db.backends.sqlite3.base import DatabaseWrapper\n        settings_dict = {\n            'NAME': 'test.db',\n            'OPTIONS': {'timeout': 20}\n        }\n        db_wrapper = DatabaseWrapper(settings_dict)\n        conn_params = db_wrapper.get_connection_params()\n        self.assertEqual(conn_params['database'], 'test.db')\n        self.assertEqual(conn_params['timeout'], 20)\n        self.assertFalse(conn_params['check_same_thread'])\n        self.assertTrue(conn_params['uri'])\n"], "sample_268": ["    def test_enable_echo(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        mock_tcgetattr.return_value = [0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        self.assertTrue(mock_tcsetattr.called)\n        self.assertEqual(mock_tcgetattr.call_count, 1)\n        self.assertEqual(mock_isatty.call_count, 1)\n", "    def test_ensure_echo_on_with_termios(self):\n        if not hasattr(sys.stdin, 'isatty') or not sys.stdin.isatty():\n            self.skipTest(\"sys.stdin is not a terminal\")\n\n            return [0, 0, 0, 0x8]  # ECHO off\n\n            self.assertTrue(attributes[3] & termios.ECHO)\n\n        with mock.patch('termios.tcgetattr', mock_tcgetattr):\n            with mock.patch('termios.tcsetattr', mock_tcsetattr):\n                autoreload.ensure_echo_on()\n", "    def test_ensure_echo_on_enabled(self, mock_isatty, mock_termios):\n        # Mock termios.tcgetattr to return a list of attributes where ECHO is off.\n        attrs = [0, 0, 0, 0]\n        attrs[3] = 0\n        mock_termios.tcgetattr.return_value = attrs\n        # Call the function\n        autoreload.ensure_echo_on()\n        # Verify that ECHO was turned on\n        self.assertTrue(attrs[3] & mock_termios.ECHO)\n", "    def test_termios_not_imported(self):\n        # Ensure the function returns early if termios is not imported\n        autoreload.ensure_echo_on()\n", "    def test_termios_not_imported(self):\n        \"\"\"Test ensure_echo_on() when termios is not imported.\"\"\"\n        autoreload.ensure_echo_on()  # Should not raise any exceptions.\n"], "sample_269": ["    def test_js_catalog_template_rendering(self):\n        \"\"\"\n        The JavaScriptCatalog view renders the JavaScript catalog template correctly.\n        \"\"\"\n        class TestJavaScriptCatalog(JavaScriptCatalog):\n                return {\n                    'catalog': {\n                        'Hello': 'Hola',\n                        'Goodbye': 'Adi\u00f3s',\n                        'month name\\x04May': 'Mayo'\n                    },\n                    'formats': {\n                        'DATE_FORMAT': 'd M Y',\n                        'TIME_FORMAT': 'H:i:s',\n                        'FIRST_DAY_OF_WEEK': 1\n                    },\n                    'plural': 'n != 1'\n                }\n\n        view = TestJavaScriptCatalog.as_view()\n        request = RequestFactory().get('/')\n        response = view(request)\n        content = response.content.decode('utf-8')\n        self.assertIn('\"Hello\": \"Hola\"', content)\n        self.assertIn('\"Goodbye\": \"Adi\u00f3s\"', content)\n        self.assertIn('\"month name\\\\u0004May\": \"Mayo\"', content)\n        self.assertIn('\"DATE_FORMAT\": \"d M Y\"', content)\n        self.assertIn('\"TIME_FORMAT\": \"H:i:s\"', content)\n        self.assertIn('\"FIRST_DAY_OF_WEEK\": 1', content)\n        self.assertIn('django.pluralidx = function(n) { return n != 1; };', content)\n", "    def test_get_plural(self):\n        \"\"\"\n        Test the get_plural method of JavaScriptCatalog to ensure it correctly\n        returns the plural form expression.\n        \"\"\"\n        view = JavaScriptCatalog()\n        with override('ru'):\n            view.translation = DjangoTranslation('ru', domain=view.domain)\n            self.assertEqual(view.get_plural(), '(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2)')\n        with override('en'):\n            view.translation = DjangoTranslation('en', domain=view.domain)\n            self.assertEqual(view.get_plural(), 'n != 1')\n", "    def test_get_plural_string(self):\n        \"\"\"\n        Test that the JavaScriptCatalog view correctly identifies and returns \n        the plural string from the translation catalog.\n        \"\"\"\n        view = JavaScriptCatalog()\n        view.translation = DjangoTranslation('en')\n        view.translation._catalog = {\n            '': 'Plural-Forms: nplurals=2; plural=(n != 1);\\n'\n        }\n        self.assertEqual(view._plural_string, 'nplurals=2; plural=(n != 1);')\n    ", "    def test_jsi18n_with_invalid_language(self):\n        \"\"\"\n        The javascript_catalog should return an empty catalog if the requested\n        language is invalid.\n        \"\"\"\n        with override('invalid-lang'):\n            response = self.client.get('/jsi18n/')\n            self.assertEqual(response.headers['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n            self.assertContains(response, '\"catalog\": {}')\n            self.assertContains(response, '\"formats\":')\n            self.assertContains(response, '\"plural\":')\n", "    def test_jsi18n_with_custom_package(self):\n        \"\"\"\n        The JavaScriptCatalog can handle custom packages.\n        \"\"\"\n        with override('fr'):\n            response = self.client.get('/jsi18n/app0/')\n            self.assertContains(response, 'this app0 string is to be translated')\n            self.assertEqual(response.headers['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n"], "sample_270": ["    def test_model_base_new_non_model_subclass(self):\n        class NotAModel:\n            pass\n\n        with self.assertRaises(RuntimeError):\n            class MyModel(NotAModel, metaclass=ModelBase):\n                pass\n", "    def test_default_auto_field_warning(self):\n        class Model(models.Model):\n            pass\n\n        with override_settings(DEFAULT_AUTO_FIELD='django.db.models.BigAutoField'):\n            self.assertEqual(Model.check(), [])\n\n        with override_settings(DEFAULT_AUTO_FIELD='django.db.models.AutoField'):\n            self.assertEqual(Model.check(), [])\n\n        class TestAppConfig(AppConfig):\n            name = 'invalid_models_tests'\n            default_auto_field = 'django.db.models.BigAutoField'\n\n        class ModelWithAppConfig(models.Model):\n            class Meta:\n                app_config = TestAppConfig\n\n        self.assertEqual(ModelWithAppConfig.check(), [])\n\n        class TestAppConfigWithOverride(AppConfig):\n            name = 'invalid_models_tests'\n            default_auto_field = 'django.db.models.AutoField'\n            _is_default_auto_field_overridden = True\n\n        class ModelWithAppConfigOverride(models.Model):\n            class Meta:\n                app_config = TestAppConfigWithOverride\n\n        self.assertEqual(ModelWithAppConfigOverride.check(), [])\n\n        class ModelWithoutDefaultPK(models.Model):\n            id = models.AutoField(primary_key=True)\n\n        self.assertEqual(ModelWithoutDefaultPK.check(), [])\n\n        class ModelWithDefaultPK(models.Model):\n            pass\n\n        self.assertEqual(ModelWithDefaultPK.check(), [\n            Warning(\n                f\"Auto-created primary key used when not defining a \"\n                f\"primary key type, by default 'django.db.models.AutoField'.\",\n                hint=(\n                    f\"Configure the DEFAULT_AUTO_FIELD setting or the \"\n                    f\"InvalidModelsTestsConfig.default_auto_field attribute to point to a subclass \"\n                    f\"of AutoField, e.g. 'django.db.models.BigAutoField'.\"\n                ),\n                obj=ModelWithDefaultPK,\n                id='models.W042',\n            ),\n        ])\n", "    def test_model_with_custom_manager(self):\n        class CustomManager(models.Manager):\n                return super().get_queryset().order_by('pk')\n\n        class Model(models.Model):\n            objects = CustomManager()\n\n        instance1 = Model.objects.create()\n        instance2 = Model.objects.create()\n\n        with self.assertRaises(Model.MultipleObjectsReturned):\n            Model.objects.get(pk__in=[instance1.pk, instance2.pk])\n", "    def test_primary_key_is_auto_created(self):\n        class ModelWithoutPK(models.Model):\n            field = models.IntegerField()\n\n        self.assertTrue(ModelWithoutPK._meta.auto_created)\n        self.assertEqual(ModelWithoutPK._meta.pk.name, 'id')\n        self.assertEqual(ModelWithoutPK._meta.pk.auto_created, True)\n", "    def test_proxy_model_with_local_fields(self):\n        class ProxyModel(models.Model):\n            field = models.IntegerField()\n\n            class Meta:\n                proxy = True\n\n        self.assertEqual(ProxyModel.check(), [\n            Error(\n                \"Proxy model 'ProxyModel' contains model fields.\",\n                obj=ProxyModel,\n                id='models.E017',\n            ),\n        ])\n"], "sample_271": ["    def test_ensure_echo_on_with_tty(self, mock_termios, mock_stdin):\n        mock_stdin.isatty.return_value = True\n        attrs = [0, 0, 0, 0]\n        mock_termios.tcgetattr.return_value = attrs\n        # Simulate ECHO being off\n        attrs[3] &= ~mock_termios.ECHO\n        autoreload.ensure_echo_on()\n        # Ensure ECHO is turned on\n        self.assertTrue(attrs[3] & mock_termios.ECHO)\n        mock_termios.tcsetattr.assert_called_once_with(mock_stdin, mock_termios.TCSANOW, attrs)\n", "    def test_ensure_echo_on_with_termios(self, mock_stdin, mock_termios):\n        mock_stdin.isatty.return_value = True\n        attr_list = [0, 0, 0, 0]\n        mock_termios.tcgetattr.return_value = attr_list\n        autoreload.ensure_echo_on()\n        self.assertTrue(attr_list[3] & mock_termios.ECHO)\n        self.assertEqual(mock_termios.tcsetattr.call_count, 1)\n", "    def test_echo_on(self, mocked_tcsetattr, mocked_tcgetattr, mocked_isatty):\n        attr_list = [0, 0, 0, 0b0000000000001000]  # ECHO bit is off.\n        mocked_tcgetattr.return_value = attr_list\n\n        autoreload.ensure_echo_on()\n\n        self.assertTrue(attr_list[3] & termios.ECHO)\n        self.assertEqual(mocked_tcsetattr.call_count, 1)\n", "    def test_ensure_echo_on(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        attrs = [0, 0, 0, 0]\n        attrs[3] = attrs[3] & ~termios.ECHO\n        mock_tcgetattr.return_value = attrs\n        \n        autoreload.ensure_echo_on()\n        \n        self.assertTrue(attrs[3] & termios.ECHO)\n        mock_tcsetattr.assert_called_once_with(sys.stdin, termios.TCSANOW, attrs)\n", "    def test_ensure_echo_on(self, mocked_isatty, mocked_termios):\n        # Simulate ECHO being off\n        attr_list = [0, 1, 2, 0]  # Mocked attributes\n        mocked_termios.tcgetattr.return_value = attr_list\n        mocked_termios.ECHO = 8  # Mock ECHO bit\n\n        autoreload.ensure_echo_on()\n\n        # Check if ECHO was turned on\n        self.assertTrue(attr_list[3] & 8)\n        mocked_termios.tcsetattr.assert_called_once_with(sys.stdin, mocked_termios.TCSANOW, attr_list)\n"], "sample_272": ["    def test_mixed_plan_forwards_only(self):\n        \"\"\"\n        Ensure migration plan with only forward migrations is executed without errors.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Prepare a forward-only plan\n        plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n            ],\n        )\n        # Execute the forward-only plan\n        state = executor.migrate([(\"migrations\", \"0002_second\")])\n        # Verify the final state includes the applied migrations\n        self.assertIn(('migrations', 'book'), state.models)\n        self.assertIn(('migrations', 'author'), state.models)\n        # Cleanup by migrating back\n        executor.loader.build_graph()\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n", "    def test_migrate_all_forwards(self):\n        \"\"\"\n        Tests applying all forward migrations in the correct order.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n            ],\n        )\n        state = executor._migrate_all_forwards(\n            ProjectState(),\n            plan,\n            executor.migration_plan(executor.loader.graph.leaf_nodes(), clean_start=True),\n            fake=False,\n            fake_initial=False,\n        )\n        self.assertIn(('migrations', 'book'), state.models)\n        self.assertIn(('migrations', 'author'), state.models)\n        # Verify the migration state after applying all forward migrations\n        self.assertTrue(\n            executor.recorder.migration_qs.filter(app='migrations', name='0002_second').exists()\n        )\n", "    def test_detect_soft_applied_no_initial(self):\n        \"\"\"\n        Tests that detect_soft_applied returns False for non-initial migrations.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        migration = executor.loader.get_migration(\"migrations\", \"0002_second\")\n        self.assertIs(executor.detect_soft_applied(None, migration)[0], False)\n", "    def test_invalid_migration_plan(self):\n        \"\"\"\n        Ensure that an InvalidMigrationPlan exception is raised when a mixed\n        migration plan is attempted.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Prepare a mixed plan with both forwards and backwards migrations\n        plan = [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], True),\n        ]\n        # Expecting an InvalidMigrationPlan exception to be raised\n        with self.assertRaises(InvalidMigrationPlan):\n            executor.migrate(None, plan)\n", "    def test_mixed_migrations_forward_and_backward(self):\n        \"\"\"\n        Ensure that a mixed migration plan with both forwards and backwards \n        migrations raises an InvalidMigrationPlan exception.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Apply a migration to set up initial state\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Check initial migration applied\n        self.assertIn(('migrations', '0001_initial'), executor.loader.applied_migrations)\n        \n        # Generate a mixed migration plan with both forwards and backwards migrations\n        plan = executor.migration_plan([\n            (\"migrations\", \"0002_second\"),  # Forward migration\n            (\"migrations\", \"0001_initial\")  # Backward migration\n        ])\n        \n        msg = (\n            'Migration plans with both forwards and backwards migrations are '\n            'not supported. Please split your migration process into separate '\n            'plans of only forwards OR backwards migrations.'\n        )\n        with self.assertRaisesMessage(InvalidMigrationPlan, msg) as cm:\n            executor.migrate(None, plan)\n        self.assertEqual(\n            cm.exception.args[1],\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n            ],\n        )\n        # Clean up by migrating everything back to None\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n"], "sample_273": ["    def test_multiple_primary_keys(self):\n        class Model(models.Model):\n            id = models.AutoField(primary_key=True)\n            another_id = models.AutoField(primary_key=True)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The model cannot have more than one field with 'primary_key=True'.\",\n                obj=Model,\n                id='models.E026',\n            ),\n        ])\n", "    def test_field_name_clash(self):\n        class ParentModel(models.Model):\n            name = models.CharField(max_length=30)\n\n        class ChildModel(ParentModel):\n            name = models.CharField(max_length=30)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The field 'name' from parent model 'check_framework.ParentModel' clashes with the field 'name' from model 'check_framework.ChildModel'.\",\n                obj=ChildModel,\n                id='models.E005',\n            ),\n        ])\n", "    def test_model_str(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=30)\n\n                return self.name\n\n        instance = Model(name=\"Test Model\")\n        self.assertEqual(str(instance), \"Test Model\")\n", "    def test_unique_together_valid(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=20)\n            field2 = models.CharField(max_length=20)\n\n            class Meta:\n                unique_together = ('field1', 'field2')\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_field_name_clash_with_parent(self):\n        class Parent(models.Model):\n            name = models.CharField(max_length=255)\n\n        class Child(Parent):\n            name = models.IntegerField()\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The field 'name' from parent model 'check_framework.Parent' clashes with the field 'name' from model 'check_framework.Child'.\",\n                obj=Child._meta.get_field('name'),\n                id='models.E006',\n            )\n        ])\n"], "sample_274": ["    def test_modelform_factory(self):\n        # Define a dummy model for testing\n        from django.db import models\n\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField()\n\n        # Use modelform_factory to create a form class\n        FormClass = modelform_factory(TestModel, fields=('name', 'age'))\n\n        # Check if the form class is created properly\n        self.assertTrue(issubclass(FormClass, ModelForm))\n\n        # Create an instance of the form\n        form = FormClass(data={'name': 'John', 'age': 30})\n        self.assertTrue(form.is_valid())\n\n        # Check if form fields are correctly populated\n        self.assertIn('name', form.fields)\n        self.assertIn('age', form.fields)\n", "def test_modelform_defines_fields(self):\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n\n        class Meta:\n            fields = ['field1']\n\n    class TestModelForm(ModelForm):\n        class Meta:\n            model = ChoiceModel\n            fields = '__all__'\n\n    class TestModelFormNoFields(ModelForm):\n        class Meta:\n            model = ChoiceModel\n\n    # Test form that defines fields\n    self.assertTrue(modelform_defines_fields(TestForm))\n    # Test ModelForm that defines fields\n    self.assertTrue(modelform_defines_fields(TestModelForm))\n    # Test ModelForm that does not define fields\n    self.assertFalse(modelform_defines_fields(TestModelFormNoFields))\n", "    def test_modelform_factory_no_fields_or_exclude(self):\n        with self.assertRaises(ImproperlyConfigured) as cm:\n            modelform_factory(ChoiceModel)\n        self.assertEqual(\n            str(cm.exception),\n            \"Calling modelform_factory without defining 'fields' or 'exclude' explicitly is prohibited.\"\n        )\n", "    def test_construct_instance(self):\n        from django.forms import ModelForm\n        from django.db import models\n        \n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField()\n        \n        class TestModelForm(ModelForm):\n            class Meta:\n                model = TestModel\n                fields = ['name', 'age']\n\n        instance = TestModel(name='John Doe', age=30)\n        form = TestModelForm(data={'name': 'Jane Doe', 'age': 25}, instance=instance)\n        self.assertTrue(form.is_valid())\n        instance = construct_instance(form, instance)\n        self.assertEqual(instance.name, 'Jane Doe')\n        self.assertEqual(instance.age, 25)\n", "    def test_modelform_without_model(self):\n        with self.assertRaises(ValueError):\n            class TestForm(ModelForm):\n                class Meta:\n                    fields = '__all__'\n"], "sample_275": ["    def setUp(self):\n        self.p1 = Person.objects.create(name='Alice')\n        self.p2 = Person.objects.create(name='Bob')\n        self.p3 = Person.objects.create(name='Charlie')\n        self.book1 = Book.objects.create(id=1, pagecount=100, author=self.p1)\n        self.book2 = Book.objects.create(id=2, pagecount=200, author=self.p2)\n        self.book3 = Book.objects.create(id=3, pagecount=300, author=self.p3)\n", "    def setUp(self):\n        self.book1 = Book.objects.create(id=1, pagecount=150)\n        self.book2 = Book.objects.create(id=2, pagecount=200)\n        self.book3 = Book.objects.create(id=3, pagecount=300)\n", "    def test_queryset_len(self):\n        Book.objects.create(title=\"Book 1\", pagecount=100)\n        Book.objects.create(title=\"Book 2\", pagecount=200)\n        qs = Book.objects.all()\n        self.assertEqual(len(qs), 2)\n", "    def test_queryset_get(self):\n        \"\"\"\n        Test that QuerySet.get() returns the correct object and raises the appropriate exceptions.\n        \"\"\"\n        Book.objects.create(id=1, pagecount=100)\n        Book.objects.create(id=2, pagecount=200)\n\n        # Test successful get\n        book = Book.objects.get(id=1)\n        self.assertEqual(book.pagecount, 100)\n\n        # Test get with non-existent id\n        with self.assertRaises(Book.DoesNotExist):\n            Book.objects.get(id=3)\n\n        # Test get with multiple results\n        with self.assertRaises(Book.MultipleObjectsReturned):\n            Book.objects.get(pagecount__lt=300)\n", "    def test_create_and_get(self):\n        \"\"\"\n        Test that an object can be created and retrieved using the QuerySet's \n        create() and get() methods.\n        \"\"\"\n        Book.objects.create(title=\"Test Book\", pagecount=123)\n        book = Book.objects.get(title=\"Test Book\")\n        self.assertEqual(book.pagecount, 123)\n"], "sample_276": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_named_groups(self):\n        pattern = r'^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$'\n        simplified = simplify_regex(pattern)\n        self.assertEqual(simplified, '/<sport_slug>/athletes/<athlete_slug>/')\n", "    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_list'), 'List')\n        self.assertEqual(get_return_data_type('get_count'), 'Integer')\n        self.assertEqual(get_return_data_type('some_other_function'), '')\n", "    def test_named_groups(self):\n        pattern = r\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        simplified = simplify_regex(pattern)\n        self.assertEqual(simplified, \"/<sport_slug>/athletes/<athlete_slug>/\")\n", "    def test_named_groups(self):\n        pattern = r\"^(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<day>\\d{2})/$\"\n        simplified = simplify_regex(pattern)\n        self.assertEqual(simplified, \"/<year>/<month>/<day>/\")\n"], "sample_277": ["    def test_invert(self):\n        q = Q(price__gt=F('discounted_price'))\n        inverted_q = ~q\n        self.assertTrue(inverted_q.negated)\n        self.assertEqual(inverted_q.connector, Q.AND)\n        self.assertEqual(inverted_q.children, [q])\n", "    def test_negate(self):\n        q = Q(price__gt=F('discounted_price'))\n        q_negated = ~q\n        self.assertTrue(q_negated.negated)\n        self.assertEqual(q_negated.connector, Q.AND)\n        self.assertEqual(q_negated.children, [q])\n", "    def test_invert(self):\n        q = Q(price__gt=F('discounted_price'))\n        inverted_q = ~q\n        self.assertTrue(inverted_q.negated)\n        self.assertEqual(inverted_q.connector, Q.AND)\n        self.assertEqual(inverted_q.children, [q])\n", "    def test_invert(self):\n        q = Q(price__gt=F('discounted_price'))\n        inverted_q = ~q\n        self.assertTrue(inverted_q.negated)\n        self.assertEqual(inverted_q.connector, 'AND')\n        self.assertEqual(inverted_q.children, [q])\n", "    def test_invert(self):\n        q = Q(x=1)\n        inverted_q = ~q\n        self.assertTrue(inverted_q.negated)\n        self.assertEqual(inverted_q.children, [q])\n"], "sample_278": ["    def setUpTestData(cls):\n        cls.company = Company.objects.create(name=\"Test Company\", num_employees=10, num_chairs=5)\n        cls.employee_1 = Employee.objects.create(firstname=\"John\", lastname=\"Doe\", salary=50, company=cls.company)\n        cls.employee_2 = Employee.objects.create(firstname=\"Jane\", lastname=\"Smith\", salary=60, company=cls.company)\n        cls.filtered_relation = FilteredRelation(\n            relation_name=\"employee\",\n            condition=Q(lastname=\"Doe\")\n        )\n", "    def test_filtered_relation_initialization(self):\n        relation_name = 'test_relation'\n        condition = Q(test_field='test_value')\n        fr = FilteredRelation(relation_name, condition=condition)\n        self.assertEqual(fr.relation_name, relation_name)\n        self.assertEqual(fr.condition, condition)\n        self.assertIsNone(fr.alias)\n        self.assertEqual(fr.path, [])\n", "    def test_register_lookup(self):\n        class TestLookup:\n            lookup_name = 'test'\n\n        class TestModel(RegisterLookupMixin):\n            pass\n\n        TestModel.register_lookup(TestLookup)\n        self.assertIn('test', TestModel.get_lookups())\n        self.assertEqual(TestModel.get_lookups()['test'], TestLookup)\n        TestModel._unregister_lookup(TestLookup)\n", "    def test_filtered_relation_initialization(self):\n        with self.assertRaises(ValueError):\n            FilteredRelation('')\n        with self.assertRaises(ValueError):\n            FilteredRelation('relation', condition='not_a_Q_instance')\n", "    def test_register_lookup(self):\n        field = self.FieldMock()\n        field.register_lookup(self.LookupMock)\n        self.assertIn('mock_lookup', field.get_lookups())\n"], "sample_279": ["    def test_invalid_fields_argument(self):\n        msg = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(fields=[], name='invalid')\n", "    def test_invalid_fields_argument(self):\n        msg = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(name='uniq_no_fields', fields=[])\n", "    def test_invalid_opclasses_length(self):\n        msg = (\n            'UniqueConstraint.fields and UniqueConstraint.opclasses must '\n            'have the same number of elements.'\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                name='uniq_opclasses_length',\n                fields=['field1', 'field2'],\n                opclasses=['jsonb_path_ops'],\n            )\n", "def test_invalid_fields_argument(self):\n    msg = 'At least one field is required to define a unique constraint.'\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(fields=[], name='unique')\n", "    def test_invalid_fields_argument(self):\n        msg = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(name='uniq')\n"], "sample_280": ["    def test_count_with_filter(self):\n        \"\"\"\n        Ensure Count works correctly with filters.\n        \"\"\"\n        # Count the number of books with a rating greater than 4.0\n        vals = Book.objects.filter(rating__gt=4.0).aggregate(Count(\"id\"))\n        self.assertEqual(vals, {\"id__count\": 1})\n        \n        # Count the number of authors with age less than 30\n        vals = Author.objects.filter(age__lt=30).aggregate(Count(\"id\"))\n        self.assertEqual(vals, {\"id__count\": 3})\n        \n        # Ensure that Count returns zero for no matches\n        vals = Publisher.objects.filter(num_awards__gt=100).aggregate(Count(\"id\"))\n        self.assertEqual(vals, {\"id__count\": 0})\n", "    def test_count_with_filter(self):\n        vals = Book.objects.aggregate(filtered_count=Count('id', filter=Q(rating__gte=4.0)))\n        self.assertEqual(vals, {\"filtered_count\": 5})\n\n        vals = Book.objects.aggregate(filtered_count=Count('id', filter=Q(price__lt=Decimal('30.00'))))\n        self.assertEqual(vals, {\"filtered_count\": 1})\n", "    def test_distinct_aggregate_with_filter(self):\n        aggs = Book.objects.aggregate(\n            filtered_distinct_ratings=Count(Case(When(pages__gt=300, then='rating')), distinct=True, filter=Q(price__gt=30)),\n        )\n        self.assertEqual(aggs['filtered_distinct_ratings'], 2)\n", "    def test_stddev_sample(self):\n        vals = Author.objects.aggregate(stddev_sample=StdDev(\"age\", sample=True))\n        self.assertAlmostEqual(vals[\"stddev_sample\"], 9.6, places=1)\n", "    def test_count_with_filter(self):\n        # Test Count aggregation with a filter applied\n        vals = Book.objects.filter(pages__gt=400).aggregate(count=Count('id'))\n        self.assertEqual(vals, {'count': 3})\n\n        # Test Count aggregation with a different filter applied\n        vals = Book.objects.filter(price__lt=Decimal('30.00')).aggregate(count=Count('id'))\n        self.assertEqual(vals, {'count': 1})\n\n        # Test Count aggregation with no results\n        vals = Book.objects.filter(rating__gt=5).aggregate(count=Count('id'))\n        self.assertEqual(vals, {'count': 0})\n"], "sample_281": ["    def test_permission_denied_due_to_missing_model_admin(self):\n        \"\"\"\n        Test that PermissionDenied is raised if the model admin is not\n        registered in the admin site.\n        \"\"\"\n        q = Question.objects.create(question='Is this a question?')\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'admin_views', 'model_name': 'unknownmodel', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_missing_model_admin(self):\n        class NoModelAdmin:\n            pass\n\n        with model_admin(Question, NoModelAdmin):\n            request = self.factory.get(self.url, {'term': 'is', **self.opts})\n            request.user = self.superuser\n            with self.assertRaises(PermissionDenied):\n                AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_invalid_model_name(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'admin_views', 'model_name': 'invalid_model', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_invalid_app_label(self):\n        \"\"\"\n        Test that an invalid app_label in the request raises a PermissionDenied error.\n        \"\"\"\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid_app', 'model_name': 'Answer', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_invalid_model_name(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'admin_views', 'model_name': 'nonexistent_model', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_282": ["    def test_boundfield_as_text(self):\n        form = ComplexFieldForm()\n        bound_field = form['field1']\n        self.assertHTMLEqual(\n            bound_field.as_text(),\n            '<input type=\"text\" name=\"field1\" id=\"id_field1\" required>'\n        )\n", "    def test_boundfield_label_tag(self):\n        class TestForm(Form):\n            test_field = CharField(label=\"Test Field\")\n\n        form = TestForm()\n        bound_field = form['test_field']\n        self.assertHTMLEqual(bound_field.label_tag(), '<label for=\"id_test_field\">Test Field:</label>')\n\n        custom_label = \"Custom Label\"\n        self.assertHTMLEqual(bound_field.label_tag(contents=custom_label), '<label for=\"id_test_field\">Custom Label:</label>')\n\n        no_label_suffix_form = TestForm(auto_id=\"id_%s\", label_suffix=\"\")\n        bound_field_no_suffix = no_label_suffix_form['test_field']\n        self.assertHTMLEqual(bound_field_no_suffix.label_tag(), '<label for=\"id_test_field\">Test Field</label>')\n\n        with self.assertRaises(TypeError):\n            bound_field.label_tag(contents=123)  # Non-string content should raise TypeError\n", "def test_bound_field_as_text(self):\n        class TestForm(Form):\n            field1 = CharField(widget=TextInput())\n\n        form = TestForm()\n        bound_field = form['field1']\n        self.assertHTMLEqual(\n            bound_field.as_text(),\n            '<input type=\"text\" name=\"field1\" id=\"id_field1\" required>'\n        )\n", "def test_boundfield_as_text(self):\n    field = CharField()\n    form = Form()\n    bound_field = BoundField(form, field, 'name')\n    self.assertHTMLEqual(\n        bound_field.as_text(),\n        '<input type=\"text\" name=\"name\" id=\"id_name\">',\n    )\n", "    def test_label_tag(self):\n        form = ComplexFieldForm()\n        field = form['field1']\n        label = field.label_tag()\n        self.assertHTMLEqual(\n            label,\n            '<label for=\"id_field1_0\">Field1:</label>'\n        )\n"], "sample_283": ["    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({'USER': 'someuser'}),\n            (['psql', '-U', 'someuser', 'postgres'], {})\n        )\n", "    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\n                'USER': 'someuser',\n                'HOST': 'somehost',\n                'PORT': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n                {},\n            )\n        )\n", "    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\n                'USER': 'someuser',\n                'PASSWORD': 'somepassword',\n                'HOST': 'somehost',\n                'PORT': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\n                'USER': 'someuser',\n                'PASSWORD': 'somepassword',\n                'HOST': 'somehost',\n                'PORT': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\n                'USER': 'defaultuser',\n                'HOST': 'defaulthost',\n                'PORT': '5432',\n            }), (\n                ['psql', '-U', 'defaultuser', '-h', 'defaulthost', '-p', '5432', 'postgres'],\n                {},\n            )\n        )\n"], "sample_284": ["    def test_manifest_load_failure(self, mock_load_manifest):\n        \"\"\"\n        Test that ValueError is raised when the manifest load fails.\n        \"\"\"\n        mock_load_manifest.side_effect = ValueError(\"Couldn't load manifest 'staticfiles.json'\")\n        storage = ManifestFilesMixin()\n        with self.assertRaises(ValueError) as cm:\n            storage.load_manifest()\n        self.assertEqual(str(cm.exception), \"Couldn't load manifest 'staticfiles.json' (version 1.0)\")\n", "    def setUp(self):\n        super().setUp()\n        self._original_hash_key = storage.staticfiles_storage.hash_key\n        storage.staticfiles_storage.hash_key = lambda name: name.upper()\n", "    def test_file_hash(self):\n        \"\"\"\n        Test the file_hash method of HashedFilesMixin for correct hash generation.\n        \"\"\"\n        storage_instance = storage.ManifestStaticFilesStorage()\n\n        # Create a temporary file with known content\n        temp_file = tempfile.NamedTemporaryFile(delete=False)\n        temp_file.write(b'Content to hash')\n        temp_file.close()\n\n        # Open the file and get the content\n        with open(temp_file.name, 'rb') as f:\n            content = f.read()\n            content_file = ContentFile(content)\n\n        # Calculate the expected hash using hashlib\n        expected_hash = hashlib.md5(content).hexdigest()[:12]\n\n        # Calculate the hash using the file_hash method\n        actual_hash = storage_instance.file_hash(temp_file.name, content_file)\n\n        # Assert that the hashes match\n        self.assertEqual(expected_hash, actual_hash)\n\n        # Cleanup\n        os.remove(temp_file.name)\n", "    def test_file_hash(self):\n        storage_class = storage.ManifestStaticFilesStorage\n        content = ContentFile(b\"Test content\")\n        name = \"test/file.txt\"\n        expected_hash = hashlib.md5(b\"Test content\").hexdigest()[:12]\n\n        # Save the file and get its hashed name\n        storage_class.save(name, content)\n        hashed_name = storage_class.hashed_name(name, content)\n\n        # Extract the hash part from the hashed name\n        hashed_part = hashed_name.split('.')[1]\n\n        self.assertEqual(hashed_part, expected_hash)\n", "    def setUp(self):\n        super().setUp()\n        self.storage = ManifestStaticFilesStorage()\n"], "sample_285": ["    def test_nonexistent_directory_in_staticfiles_dirs(self):\n        self.assertEqual(check_finders(None), [\n            Warning(\n                \"The directory '/nonexistent/path' in the STATICFILES_DIRS setting does not exist.\",\n                id='staticfiles.W004',\n            ),\n        ])\n", "    def test_nonexistent_directory_in_staticfiles_dirs(self):\n        with self.settings(STATICFILES_DIRS=['/nonexistent/directory']):\n            self.assertEqual(check_finders(None), [\n                Warning(\n                    \"The directory '/nonexistent/directory' in the STATICFILES_DIRS setting does not exist.\",\n                    id='staticfiles.W004',\n                )\n            ])\n", "    def test_non_existent_directory_in_staticfiles_dirs(self):\n        with self.settings(STATICFILES_DIRS=['/non/existent/directory']):\n            self.assertEqual(check_finders(None), [\n                Warning(\n                    \"The directory '/non/existent/directory' in the STATICFILES_DIRS setting does not exist.\",\n                    id='staticfiles.W004',\n                )\n            ])\n", "    def test_non_existent_directory_in_staticfiles_dirs(self):\n        with self.settings(STATICFILES_DIRS=['non_existent_directory']):\n            self.assertEqual(check_finders(None), [\n                Warning(\n                    \"The directory 'non_existent_directory' in the STATICFILES_DIRS setting \"\n                    \"does not exist.\",\n                    id='staticfiles.W004',\n                ),\n            ])\n", "    def test_dirs_non_existent_directory(self):\n        self.assertEqual(check_finders(None), [\n            Warning(\n                \"The directory '/non/existent/path' in the STATICFILES_DIRS setting \"\n                \"does not exist.\",\n                id='staticfiles.W004',\n            )\n        ])\n"], "sample_286": ["    def test_modelbase_new_class_creation(self):\n        \"\"\"\n        Test that the ModelBase __new__ method correctly creates a new class\n        with the appropriate attributes and meta options.\n        \"\"\"\n        class Meta:\n            app_label = \"testapp\"\n            ordering = ['-id']\n\n        class TestModel(metaclass=ModelBase):\n            class Meta:\n                app_label = \"testapp\"\n\n        self.assertTrue(hasattr(TestModel, '_meta'))\n        self.assertEqual(TestModel._meta.app_label, 'testapp')\n        self.assertEqual(TestModel._meta.ordering, ['-id'])\n", "    def test_deferred_field_access(self):\n        a = Article.objects.create(headline='Deferred test', pub_date=datetime(2023, 10, 1))\n        a_deferred = Article.objects.only('headline').get(pk=a.pk)\n        with self.assertNumQueries(1):\n            self.assertEqual(a_deferred.headline, 'Deferred test')\n            # Accessing the deferred field should trigger a database query.\n            self.assertEqual(a_deferred.pub_date, datetime(2023, 10, 1))\n", "    def test_clean_fields_with_blank_and_nullable(self):\n        \"\"\"\n        Test that clean_fields() works correctly with blank and nullable fields.\n        \"\"\"\n        class TestModel(Model):\n            name = models.CharField(max_length=255, blank=True, null=True)\n            age = models.IntegerField(blank=True, null=True)\n\n        instance = TestModel()\n        instance.clean_fields()\n\n        instance = TestModel(name='', age=None)\n        instance.clean_fields()\n\n        instance = TestModel(name=None, age=None)\n        instance.clean_fields()\n", "    def test_get_next_or_previous_by_FIELD(self):\n        \"\"\"\n        Test get_next_by_FIELD and get_previous_by_FIELD methods.\n        \"\"\"\n        article1 = Article.objects.create(headline='Article 1', pub_date=datetime(2023, 1, 1))\n        article2 = Article.objects.create(headline='Article 2', pub_date=datetime(2023, 1, 2))\n        article3 = Article.objects.create(headline='Article 3', pub_date=datetime(2023, 1, 3))\n\n        self.assertEqual(article2.get_next_by_pub_date(), article3)\n        self.assertEqual(article2.get_previous_by_pub_date(), article1)\n", "    def test_model_state_initialization(self):\n        \"\"\"Test that ModelState initializes correctly.\"\"\"\n        state = ModelState()\n        self.assertIsNone(state.db)\n        self.assertTrue(state.adding)\n        self.assertIsInstance(state.fields_cache, dict)\n        self.assertEqual(state.fields_cache, {})\n"], "sample_287": ["    def test_autocomplete_fields_not_list_or_tuple(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = 'title'\n\n        self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n            checks.Error(\n                \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                obj=SongAdmin,\n                id='admin.E036',\n            )\n        ])\n", "    def test_save_as_not_boolean(self):\n        \"\"\"\n        Test that the 'save_as' attribute must be a boolean.\n        \"\"\"\n        class SongAdmin(admin.ModelAdmin):\n            save_as = \"yes\"\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'save_as' must be a boolean.\",\n                obj=SongAdmin,\n                id='admin.E101',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_autocomplete_fields_not_list_or_tuple(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = 'test'\n\n        self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n            checks.Error(\n                \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                obj=SongAdmin,\n                id='admin.E036',\n            )\n        ])\n", "    def test_invalid_save_as(self):\n        \"\"\"\n        Test that a non-boolean save_as attribute in ModelAdmin raises an error.\n        \"\"\"\n        class InvalidSaveAsAdmin(admin.ModelAdmin):\n            save_as = \"not_a_boolean\"\n\n        errors = InvalidSaveAsAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'save_as' must be a boolean.\",\n                obj=InvalidSaveAsAdmin,\n                id='admin.E101',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_prepopulated_fields_invalid_field_type(self):\n        \"\"\"\n        Check that prepopulated_fields does not reference DateTimeField, ForeignKey,\n        OneToOneField, or ManyToManyField.\n        \"\"\"\n        class BookAdmin(admin.ModelAdmin):\n            prepopulated_fields = {'pub_date': ('title',)}\n\n        errors = BookAdmin(Book, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'prepopulated_fields' refers to 'pub_date', which must not be a DateTimeField, \"\n                \"a ForeignKey, a OneToOneField, or a ManyToManyField.\",\n                obj=BookAdmin,\n                id='admin.E028',\n            )\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_288": ["    def test_compile_json_path(self):\n        self.assertEqual(compile_json_path(['a', 'b']), '$.\"a\".\"b\"')\n        self.assertEqual(compile_json_path(['a', 1, 'b']), '$.\"a\"[1].\"b\"')\n        self.assertEqual(compile_json_path([1, 'a', 2], include_root=False), '[1].\"a\".[2]')\n        self.assertEqual(compile_json_path([]), '$')\n        self.assertEqual(compile_json_path([], include_root=False), '')\n", "    def test_compile_json_path(self):\n        tests = [\n            ([], '$'),\n            (['a'], '$.\"a\"'),\n            (['a', 'b'], '$.\"a\".\"b\"'),\n            (['a', 1, 'b'], '$.\"a\"[1].\"b\"'),\n            (['a', 'b', 1], '$.\"a\".\"b\"[1]'),\n            (['a', 'b', 1, 'c'], '$.\"a\".\"b\"[1].\"c\"'),\n            (['a', 1, 'b', 2], '$.\"a\"[1].\"b\"[2]'),\n        ]\n        for key_transforms, expected in tests:\n            with self.subTest(key_transforms=key_transforms):\n                self.assertEqual(compile_json_path(key_transforms), expected)\n", "    def test_compile_json_path(self):\n        tests = [\n            (['a'], \"$.a\"),\n            (['a', 'b', 'c'], \"$.a.b.c\"),\n            (['a', 0, 'b', 1], \"$.a[0].b[1]\"),\n            ([0, 1, 2], \"$[0][1][2]\"),\n            ([], \"$\"),\n            (['a', 'b', 'c'], \"a.b.c\", False),\n            (['a', 0, 'b', 1], \"a[0].b[1]\", False),\n            ([], \"\")\n        ]\n        for key_transforms, expected, *include_root in tests:\n            with self.subTest(key_transforms=key_transforms, include_root=include_root):\n                self.assertEqual(compile_json_path(key_transforms, *include_root), expected)\n", "    def test_key_transform_factory(self):\n        factory = KeyTransformFactory('key')\n        transform = factory('value')\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, 'key')\n", "    def test_key_transform_factory(self):\n        factory = KeyTransformFactory('key_name')\n        transform = factory('source_expression')\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, 'key_name')\n        self.assertEqual(transform.source_expressions[0], 'source_expression')\n"], "sample_289": ["    def test_dictwrapper_with_prefix(self):\n            return f\"[{x}]\"\n        d = DictWrapper({'prefix_key': 'value'}, f, 'prefix_')\n        self.assertEqual(d['key'], '[value]')\n", "    def test_init_empty(self):\n        s = OrderedSet()\n        self.assertEqual(len(s), 0)\n        self.assertEqual(list(s), [])\n", "def test_update_with_iterable_of_invalid_pairs(self):\n    invalid_pairs = [[('a', 1), ('b',)], [('a', 1), 'b'], [('a', 1), (2, 'value')]]\n    for value in invalid_pairs:\n        with self.subTest(value=value):\n            d = MultiValueDict()\n            with self.assertRaises(ValueError):\n                d.update(value)\n", "def test_setlistdefault(self):\n    d = MultiValueDict({'a': [1, 2], 'b': [3]})\n    self.assertEqual(d.setlistdefault('a', [4, 5]), [1, 2])\n    self.assertEqual(d.setlistdefault('c', [6, 7]), [6, 7])\n    self.assertEqual(d.getlist('c'), [6, 7])\n    self.assertEqual(d.setlistdefault('d'), [])\n    self.assertEqual(d.getlist('d'), [])\n", "    def test_setlistdefault(self):\n        d = MultiValueDict({'a': [1, 2], 'b': [3]})\n        default_list = [4, 5]\n        self.assertEqual(d.setlistdefault('a'), [1, 2])\n        self.assertEqual(d.setlistdefault('c', default_list), default_list)\n        self.assertEqual(d.getlist('c'), default_list)\n"], "sample_290": ["    def test_suggest_name_length_limit(self):\n        \"\"\"Test that the suggested migration name does not exceed the length limit.\"\"\"\n        class Migration(migrations.Migration):\n            operations = [migrations.CreateModel('a' * 26, fields=[]), migrations.DeleteModel('b' * 26)]\n\n        migration = Migration('some_migration', 'test_app')\n        suggested_name = migration.suggest_name()\n        self.assertLessEqual(len(suggested_name), 52)\n        self.assertTrue(suggested_name.endswith('_and_more'))\n", "def test_migration_apply_operations(self):\n    \"\"\"\n    Test that operations are applied correctly during a migration.\n    \"\"\"\n    class MockOperation:\n            self.reduces_to_sql = reduces_to_sql\n            self.atomic = atomic\n            self.reversible = reversible\n\n            pass\n\n            return \"Mock operation\"\n\n            pass\n\n            pass\n\n    class MockSchemaEditor:\n            self.collected_sql = []\n            self.atomic_migration = False\n            self.connection = mock.MagicMock()\n            self.connection.alias = 'default'\n\n    initial_state = ProjectState()\n    migration = migrations.Migration('0001_initial', 'testapp')\n    migration.operations = [MockOperation(), MockOperation(reduces_to_sql=False)]\n\n    schema_editor = MockSchemaEditor()\n    new_state = migration.apply(initial_state, schema_editor, collect_sql=True)\n\n    self.assertIsInstance(new_state, ProjectState)\n    self.assertEqual(len(schema_editor.collected_sql), 6)  # 2 operations, 1 reduces to SQL\n", "    def test_rename_field_with_constraints(self):\n        \"\"\"\n        Tests autodetection of renamed fields with constraints.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_name_check_constraint],\n            [self.author_name_renamed],\n            MigrationQuestioner({\"ask_rename\": True})\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 2)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RemoveConstraint\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name_contains_bob\")\n        self.assertOperationTypes(changes, 'testapp', 1, [\"RenameField\", \"AddConstraint\"])\n        self.assertOperationAttributes(changes, 'testapp', 1, 0, old_name=\"name\", new_name=\"names\")\n        self.assertOperationAttributes(\n            changes, 'testapp', 1, 1, model_name='author', constraint=models.CheckConstraint(check=models.Q(names__contains='Bob'), name='names_contains_bob')\n        )\n", "def test_suggest_name_auto_generated(self):\n    \"\"\"Test that suggest_name generates 'auto_' prefix when no operations have valid fragments.\"\"\"\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.RunSQL('SELECT 1;'),\n            migrations.RunPython(lambda apps, schema_editor: None)\n        ]\n\n    migration = Migration('0003_auto', 'test_app')\n    suggest_name = migration.suggest_name()\n    self.assertTrue(suggest_name.startswith('auto_'))\n", "    def test_migration_name_timestamp(self):\n        \"\"\"\n        Test that the migration name timestamp is generated correctly.\n        \"\"\"\n        from django.db.migrations.utils import get_migration_name_timestamp\n        import time\n        timestamp = get_migration_name_timestamp()\n        expected_timestamp = time.strftime(\"%Y%m%d_%H%M\")\n        self.assertTrue(timestamp.startswith(expected_timestamp))\n"], "sample_291": ["    def test_redirect_with_no_kwargs(self):\n        \"\"\"\n        Test RedirectView with a URL that doesn't require any kwargs.\n        \"\"\"\n        response = RedirectView.as_view(url='/simple/')(self.rf.get('/simple_redirect/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/simple/')\n", "    def test_template_response_mixin_with_custom_response_class(self):\n        \"\"\"\n        Test that TemplateResponseMixin can be used with a custom response class.\n        \"\"\"\n        class CustomResponse(HttpResponse):\n            pass\n\n        class CustomTemplateView(TemplateView):\n            response_class = CustomResponse\n\n        response = CustomTemplateView.as_view(template_name='generic_views/about.html')(self.rf.get('/about/'))\n        self.assertIsInstance(response, CustomResponse)\n", "    def test_template_view_with_extra_context(self):\n        \"\"\"\n        Test that a TemplateView can accept extra context and merge it with the\n        context provided by the view.\n        \"\"\"\n        class CustomTemplateView(TemplateView):\n            extra_context = {'extra_key': 'extra_value'}\n\n        response = CustomTemplateView.as_view(template_name='generic_views/about.html')(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context_data['extra_key'], 'extra_value')\n", "    def test_template_view_with_extra_context(self):\n        \"\"\"\n        Test a TemplateView that uses extra_context to provide additional context data.\n        \"\"\"\n        class ExtraContextTemplateView(TemplateView):\n            extra_context = {'extra_key': 'extra_value'}\n            template_name = 'generic_views/about.html'\n\n        response = ExtraContextTemplateView.as_view()(self.rf.get('/about/'))\n        response.render()\n        self.assertContains(response, '<h1>About</h1>')\n        self.assertEqual(response.context_data['extra_key'], 'extra_value')\n", "    def test_template_response_mixin(self):\n        \"\"\"\n        Test the TemplateResponseMixin to ensure it raises an error when\n        template_name is not provided and get_template_names is not overridden.\n        \"\"\"\n        class TestTemplateResponseMixin(TemplateResponseMixin):\n            pass\n\n        msg = (\n            \"TemplateResponseMixin requires either a definition of \"\n            \"'template_name' or an implementation of 'get_template_names()'\"\n        )\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            TestTemplateResponseMixin().get_template_names()\n"], "sample_292": ["    def test_good_origin_subdomain_wildcard_trusted_origin_allowed(self):\n        \"\"\"\n        A POST request with an origin that matches a subdomain wildcard in\n        CSRF_TRUSTED_ORIGINS is accepted.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_ORIGIN'] = 'https://sub.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), True)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(response)\n        self.assertEqual(mw.allowed_origins_exact, set())\n        self.assertEqual(mw.allowed_origin_subdomains, {'https': ['.example.com']})\n", "def test_https_bad_referer_no_trusted_origins(self):\n    \"\"\"\n    A POST HTTPS request with a bad referer is rejected even if no trusted origins are set.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_REFERER'] = 'https://www.evil.org/somepage'\n    req.META['SERVER_PORT'] = '443'\n    with self.settings(CSRF_TRUSTED_ORIGINS=[]):\n        mw = CsrfViewMiddleware(post_form_view)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertContains(\n            response,\n            'Referer checking failed - https://www.evil.org/somepage does not match any trusted origins.',\n            status_code=403,\n        )\n", "    def test_https_good_origin_wildcard_trusted_origin_allowed(self):\n        \"\"\"\n        A POST request with an origin that matches a wildcard in CSRF_TRUSTED_ORIGINS\n        is accepted.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_ORIGIN'] = 'https://sub.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), True)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(response)\n        self.assertEqual(mw.allowed_origins_exact, set())\n        self.assertEqual(mw.allowed_origin_subdomains, {'https': ['.example.com']})\n", "    def test_rotate_token(self):\n        \"\"\"\n        Ensure that rotate_token() sets a new token in the request and marks it for reset.\n        \"\"\"\n        req = self._get_GET_csrf_cookie_request()\n        initial_token = get_token(req)\n        rotate_token(req)\n        new_token = get_token(req)\n        self.assertNotEqual(initial_token, new_token, \"rotate_token() did not change the CSRF token.\")\n        self.assertTrue(req.csrf_cookie_needs_reset, \"rotate_token() did not mark the CSRF token for reset.\")\n", "def test_rotate_token(self):\n        \"\"\"\n        The rotate_token() function should change the CSRF token in use for a request.\n        \"\"\"\n        req = self._get_GET_csrf_cookie_request()\n        old_token = get_token(req)\n        rotate_token(req)\n        new_token = get_token(req)\n        self.assertNotEqual(old_token, new_token)\n        self.assertTrue(req.csrf_cookie_needs_reset)\n        self.assertEqual(req.META['CSRF_COOKIE'], new_token)\n"], "sample_293": ["    def test_regex_pattern_match(self):\n        \"\"\"\n        Test RegexPattern's match method for correctly matching and extracting args and kwargs.\n        \"\"\"\n        pattern = RegexPattern(r'^articles/(?P<year>[0-9]{4})/(?P<month>[0-9]{2})/$', name='article')\n        result = pattern.match('articles/2023/10/')\n        self.assertIsNotNone(result)\n        new_path, args, kwargs = result\n        self.assertEqual(new_path, '')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'year': '2023', 'month': '10'})\n", "    def test_compile_valid_regex(self):\n        \"\"\"\n        Test the _compile method of RegexPattern with valid regex.\n        \"\"\"\n        pattern = RegexPattern(r'^test/(?P<id>[0-9]+)/$')\n        compiled_regex = pattern._compile(pattern._regex)\n        self.assertIsInstance(compiled_regex, re.Pattern)\n        self.assertEqual(compiled_regex.pattern, '^test/(?P<id>[0-9]+)/$')\n", "    def test_urlresolver_repr(self):\n        resolver = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse.urls')\n        self.assertEqual(\n            repr(resolver),\n            \"<URLResolver 'urlpatterns_reverse.urls' (None:None) '^/'>\"\n        )\n", "    def test_route_to_regex(self):\n        \"\"\"\n        Test the conversion of routes with path parameters into regular expressions.\n        \"\"\"\n        test_routes = [\n            ('foo/<int:pk>/', r'^foo\\/(?P<pk>[0-9]+)'),\n            ('bar/<str:name>/', r'^bar\\/(?P<name>[^\\/]+)'),\n            ('baz/<slug:slug>/', r'^baz\\/(?P<slug>[-a-zA-Z0-9_]+)'),\n            ('<year>/<month>/<day>/', r'^(?P<year>[0-9]{4})\\/(?P<month>[0-9]{1,2})\\/(?P<day>[0-9]{1,2})'),\n        ]\n        for route, expected_regex in test_routes:\n            with self.subTest(route=route):\n                regex, _ = _route_to_regex(route)\n                self.assertEqual(regex, expected_regex)\n", "    def test_urlpattern_repr(self):\n        \"\"\"\n        Test the __repr__ method of URLPattern.\n        \"\"\"\n        url_pattern = URLPattern(RegexPattern(r'^test/$'), views.empty_view, name='test-view')\n        self.assertEqual(repr(url_pattern), \"<URLPattern '^test/$' [name='test-view']>\")\n"], "sample_294": ["    def test_process_request_no_origin_header(self):\n        \"\"\"\n        If no HTTP_ORIGIN header is present, the middleware should proceed without\n        performing origin verification.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "    def test_process_request_with_http_origin_header(self):\n        \"\"\"\n        If the Origin header is provided and matches an allowed value, the request is accepted.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req.META['HTTP_ORIGIN'] = 'http://www.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "    def test_origin_verified_wildcard_subdomain(self):\n        \"\"\"\n        A POST request with an origin that matches a CSRF_TRUSTED_ORIGINS\n        wildcard subdomain is accepted.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'sub.example.com'\n        req.META['HTTP_ORIGIN'] = 'https://sub.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), True)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(response)\n", "def test_process_view_rejects_http_post_with_no_referer(self):\n        \"\"\"\n        An HTTP POST request with no Referer should be rejected when the request is secure.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_REFERER'] = ''\n        req.META['SERVER_PORT'] = '443'\n        mw = CsrfViewMiddleware(post_form_view)\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            response = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % REASON_NO_REFERER)\n", "    def test_process_request_csrf_exempt_decorator(self):\n        \"\"\"\n        Ensure that a view decorated with @csrf_exempt is not subjected to CSRF validation.\n        \"\"\"\n        req = self._get_POST_no_csrf_cookie_request()\n        req.POST['csrfmiddlewaretoken'] = self._csrf_id\n        mw = CsrfViewMiddleware(csrf_exempt(post_form_view))\n        mw.process_request(req)\n        resp = mw.process_view(req, csrf_exempt(post_form_view), (), {})\n        self.assertIsNone(resp)\n"], "sample_295": ["    def test_resolve_output_field(self):\n        tests = [\n            (IntegerField, AutoField, IntegerField),\n            (AutoField, IntegerField, IntegerField),\n            (IntegerField, DecimalField, DecimalField),\n            (DecimalField, IntegerField, DecimalField),\n            (IntegerField, FloatField, FloatField),\n            (FloatField, IntegerField, FloatField),\n        ]\n        connectors = [Combinable.ADD, Combinable.SUB, Combinable.MUL, Combinable.DIV]\n        for lhs, rhs, combined in tests:\n            for connector in connectors:\n                with self.subTest(lhs=lhs, connector=connector, rhs=rhs, combined=combined):\n                    expr = CombinedExpression(\n                        Expression(lhs()),\n                        connector,\n                        Expression(rhs()),\n                    )\n                    self.assertIsInstance(expr.output_field, combined)\n", "    def setUpTestData(cls):\n        cls.company = Company.objects.create(name='Test Company', num_employees=100)\n", "    def test_combined_expression_arithmetic(self):\n        # Set up test data\n        n1 = Number.objects.create(integer=10)\n        n2 = Number.objects.create(integer=20)\n\n        # Test addition\n        qs = Number.objects.annotate(result=CombinedExpression(F('integer'), '+', Value(5)))\n        self.assertEqual(qs.get(pk=n1.pk).result, 15)\n        self.assertEqual(qs.get(pk=n2.pk).result, 25)\n\n        # Test subtraction\n        qs = Number.objects.annotate(result=CombinedExpression(F('integer'), '-', Value(5)))\n        self.assertEqual(qs.get(pk=n1.pk).result, 5)\n        self.assertEqual(qs.get(pk=n2.pk).result, 15)\n\n        # Test multiplication\n        qs = Number.objects.annotate(result=CombinedExpression(F('integer'), '*', Value(2)))\n        self.assertEqual(qs.get(pk=n1.pk).result, 20)\n        self.assertEqual(qs.get(pk=n2.pk).result, 40)\n\n        # Test division\n        qs = Number.objects.annotate(result=CombinedExpression(F('integer'), '/', Value(2)))\n        self.assertEqual(qs.get(pk=n1.pk).result, 5)\n        self.assertEqual(qs.get(pk=n2.pk).result, 10)\n", "    def setUpTestData(cls):\n        cls.company = Company.objects.create(name='TestCorp', num_employees=50, num_chairs=25,\n                                             ceo=Employee.objects.create(firstname='Alice', lastname='Doe'))\n", "    def test_as_sqlite_with_decimal_field(self):\n        class DecimalFieldExpression(SQLiteNumericMixin, Expression):\n                super().__init__(output_field=DecimalField())\n\n                return 'some_sql', []\n\n        expr = DecimalFieldExpression()\n        compiler = mock.Mock()\n        connection = mock.Mock()\n        sql, params = expr.as_sqlite(compiler, connection)\n        self.assertEqual(sql, 'CAST(some_sql AS NUMERIC)')\n"], "sample_296": ["    def test_empty_message_storage(self):\n        \"\"\"\n        Test that storing and retrieving an empty list of messages\n        works correctly.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.update(response)\n        \n        # Check that no cookies are set when there are no messages\n        self.assertIsNone(response.cookies.get(storage.cookie_name))\n\n        # Set an empty list of messages and verify storage behavior\n        set_cookie_data(storage, [], encode_empty=True)\n        self.assertEqual(list(storage), [])\n        storage.update(response)\n\n        # Check that the cookie is set to an empty value when there are no messages\n        self.assertEqual(response.cookies[storage.cookie_name].value, '')\n        self.assertEqual(response.cookies[storage.cookie_name]['max-age'], 0)\n        self.assertEqual(response.cookies[storage.cookie_name]['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n", "    def test_store_and_retrieve_safe_data(self):\n        \"\"\"\n        Ensure that SafeData messages are properly stored and retrieved\n        maintaining their SafeData status.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        \n        safe_message = mark_safe(\"Safe <b>HTML</b> message\")\n        storage.add(constants.INFO, safe_message)\n        storage.update(response)\n        \n        retrieved_messages = storage._decode(response.cookies['messages'].value)\n        self.assertEqual(len(retrieved_messages), 1)\n        self.assertIsInstance(retrieved_messages[0].message, SafeData)\n        self.assertEqual(retrieved_messages[0].message, safe_message)\n", "    def test_message_serializer(self):\n        \"\"\"\n        Test that the MessageSerializer can correctly serialize and\n        deserialize message objects.\n        \"\"\"\n        messages = [\n            Message(constants.INFO, 'test message 1', extra_tags='tag1'),\n            Message(constants.WARNING, mark_safe('test message 2'), extra_tags='tag2'),\n        ]\n        serializer = MessageSerializer()\n        serialized = serializer.dumps(messages)\n        deserialized = serializer.loads(serialized)\n        self.assertEqual(messages, deserialized)\n        self.assertIsInstance(deserialized[1].message, SafeData)\n", "    def test_empty_message_list(self):\n        \"\"\"\n        Ensure that storing and retrieving an empty list of messages works correctly.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = []\n\n        # Store the empty message list\n        set_cookie_data(storage, messages, encode_empty=True)\n        self.assertEqual(list(storage), [])\n\n        # Update the response and check that the cookie is correctly set/deleted\n        unstored_messages = storage.update(response)\n        self.assertEqual(unstored_messages, [])\n        self.assertEqual(response.cookies['messages'].value, '')\n", "    def test_invalid_message_format(self):\n        \"\"\"\n        Test that an invalid message format in the cookie results in the\n        messages being marked as used and returning an empty list.\n        \"\"\"\n        request = self.get_request()\n        storage = self.storage_class(request)\n        response = self.get_response()\n\n        # Set an invalid message format (not a JSON array).\n        invalid_message = '{\"invalid\": \"message\"}'\n        request.COOKIES[storage.cookie_name] = storage.signer.sign(invalid_message)\n\n        # The message storage should mark the message as used and return an empty list.\n        self.assertEqual(list(storage), [])\n        self.assertTrue(storage.used)\n\n        # Check that the cookie is deleted.\n        storage.update(response)\n        self.assertEqual(response.cookies[storage.cookie_name].value, '')\n        self.assertEqual(response.cookies[storage.cookie_name]['max-age'], 0)\n"], "sample_297": ["    def setUpTestData(cls):\n        cls.parent1 = NamedCategory.objects.create(name=\"Parent1\")\n        cls.parent2 = NamedCategory.objects.create(name=\"Parent2\")\n        cls.child1 = Tag.objects.create(name='Child1', category=cls.parent1)\n        cls.child2 = Tag.objects.create(name='Child2', category=cls.parent1)\n        cls.child3 = Tag.objects.create(name='Child3', category=cls.parent2)\n", "    def test_raw_query_initialization(self):\n        raw_query = RawQuery(\"SELECT * FROM table\", \"default\")\n        self.assertEqual(raw_query.sql, \"SELECT * FROM table\")\n        self.assertEqual(raw_query.using, \"default\")\n        self.assertEqual(raw_query.params, ())\n        self.assertIsNone(raw_query.cursor)\n        self.assertEqual(raw_query.low_mark, 0)\n        self.assertIsNone(raw_query.high_mark)\n        self.assertEqual(raw_query.extra_select, {})\n        self.assertEqual(raw_query.annotation_select, {})\n", "    def setUpTestData(cls):\n        cls.note1 = Note.objects.create(note='note1', misc='misc1', id=1)\n        cls.note2 = Note.objects.create(note='note2', misc='misc2', id=2)\n        cls.note3 = Note.objects.create(note='note3', misc='misc3', id=3)\n        cls.note4 = Note.objects.create(note='note4', misc='misc4', id=4)\n", "    def setUp(self):\n        self.raw_query = RawQuery(\n            sql=\"SELECT * FROM queries_note WHERE note = %s\",\n            using=DEFAULT_DB_ALIAS,\n            params=('n1',)\n        )\n", "    def test_query_clone_identity(self):\n        \"\"\"\n        Test that the clone method creates a new Query object with the same\n        identity as the original, but not the same reference.\n        \"\"\"\n        query = Query(model=Author)\n        cloned_query = query.clone()\n        \n        # Ensure that the cloned query is not the same object\n        self.assertIsNot(query, cloned_query)\n        \n        # Ensure that the identity is the same\n        self.assertEqual(query.identity, cloned_query.identity)\n        \n        # Modify the cloned query and ensure the original is unchanged\n        cloned_query.add_filter(('name', 'test'))\n        self.assertNotEqual(query.where, cloned_query.where)\n        \n        # Ensure the original query's attributes remain unaltered\n        self.assertEqual(query.alias_map, cloned_query.alias_map)\n        self.assertEqual(query.alias_refcount, cloned_query.alias_refcount)\n"], "sample_298": ["def test_make_token_with_custom_algorithm(self):\n    \"\"\"\n    Test that a token can be created and validated with a custom algorithm.\n    \"\"\"\n    class CustomAlgorithmTokenGenerator(PasswordResetTokenGenerator):\n        algorithm = 'sha1'\n\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    custom_algo_generator = CustomAlgorithmTokenGenerator()\n    tk_custom = custom_algo_generator.make_token(user)\n    self.assertIs(custom_algo_generator.check_token(user, tk_custom), True)\n\n    default_algo_generator = PasswordResetTokenGenerator()\n    self.assertNotEqual(\n        custom_algo_generator.algorithm,\n        default_algo_generator.algorithm,\n    )\n    self.assertEqual(default_algo_generator.algorithm, 'sha256')\n    # Tokens created with a different algorithm don't validate.\n    tk_default = default_algo_generator.make_token(user)\n    self.assertIs(custom_algo_generator.check_token(user, tk_default), False)\n    self.assertIs(default_algo_generator.check_token(user, tk_custom), False)\n", "    def test_token_with_changed_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_different_algorithm(self):\n        \"\"\"\n        A valid token can be created with an algorithm other than the default\n        by using the PasswordResetTokenGenerator.algorithm attribute.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        new_algorithm = 'sha1'\n        # Create and check a token with a different algorithm.\n        p0 = PasswordResetTokenGenerator()\n        p0.algorithm = new_algorithm\n        tk0 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk0), True)\n        # Create and check a token with the default algorithm.\n        p1 = PasswordResetTokenGenerator()\n        self.assertEqual(p1.algorithm, 'sha256')\n        self.assertNotEqual(p1.algorithm, new_algorithm)\n        tk1 = p1.make_token(user)\n        # Tokens created with a different algorithm don't validate.\n        self.assertIs(p0.check_token(user, tk1), False)\n        self.assertIs(p1.check_token(user, tk0), False)\n", "    def test_token_with_changed_password(self):\n        \"\"\"Changing the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        # Change the user's password\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_make_token_with_none_password(self):\n        \"\"\"\n        Ensure that a token is still generated correctly when the user's password is None.\n        \"\"\"\n        user = User.objects.create_user('nopassworduser', 'test5@example.com', password=None)\n        user.set_unusable_password()\n        user.save()\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n"], "sample_299": ["    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path/to/cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path/to/cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path/to/cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path/to/cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path/to/cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\",\n                    id='caches.W003',\n                )\n            ])\n"], "sample_300": ["def test_field_error_for_invalid_field(self):\n    query = Query(Author)\n    msg = \"Cannot resolve keyword 'invalid_field' into field. Choices are: %s\" % \", \".join(\n        sorted([f.name for f in Author._meta.get_fields()])\n    )\n    with self.assertRaisesMessage(FieldError, msg):\n        query.add_filter(('invalid_field', 'value'))\n", "def test_rawquery_get_columns(self):\n    sql = \"SELECT id, name FROM test_table\"\n    raw_query = RawQuery(sql, using='default')\n    raw_query.cursor = connections['default'].cursor()\n    raw_query.cursor.description = [(\"id\",), (\"name\",)]\n    columns = raw_query.get_columns()\n    self.assertEqual(columns, [\"id\", \"name\"])\n", "def test_chain_query(self):\n    query = Query(Author)\n    chain_query = query.chain(using='default')\n    self.assertNotEqual(query, chain_query)\n    self.assertEqual(chain_query.using, 'default')\n    self.assertEqual(chain_query.sql, query.sql)\n", "def test_raw_query_clone(self):\n    raw_query = RawQuery(\"SELECT * FROM my_table\", using=\"default\", params=(\"param1\", \"param2\"))\n    cloned_query = raw_query.clone(\"other_db\")\n    self.assertEqual(cloned_query.sql, raw_query.sql)\n    self.assertEqual(cloned_query.params, raw_query.params)\n    self.assertEqual(cloned_query.using, \"other_db\")\n", "def test_subquery_filter(self):\n    subquery = Query(Author)\n    subquery.add_filter(Q(num__gt=2))\n    main_query = Query(Item)\n    msg = 'Cannot use subquery for filtering in this query'\n    with self.assertRaisesMessage(FieldError, msg):\n        main_query.build_where(Q(creator__in=subquery))\n"], "sample_301": ["    def test_ensure_echo_on_with_termios(self):\n        with mock.patch('sys.stdin.isatty', return_value=True), \\\n             mock.patch('termios.tcgetattr', return_value=[0, 0, 0, 0, 0, 0]), \\\n             mock.patch('termios.tcsetattr') as mock_tcsetattr:\n            autoreload.ensure_echo_on()\n            mock_tcsetattr.assert_called_once()\n", "    def test_ensure_echo_on(self, mocked_isatty, mocked_termios):\n        # Set up initial conditions for tcgetattr and tcsetattr\n        mocked_termios.ECHO = 0x00000008\n        attr_list = [0, 1, 2, 0]\n        mocked_termios.tcgetattr.return_value = attr_list\n\n        # Call ensure_echo_on\n        autoreload.ensure_echo_on()\n\n        # Verify that ECHO is set in the attribute list\n        self.assertTrue(attr_list[3] & mocked_termios.ECHO)\n\n        # Verify that tcsetattr was called with the modified attribute list\n        mocked_termios.tcsetattr.assert_called_with(sys.stdin, mocked_termios.TCSANOW, attr_list)\n", "    def test_ensure_echo_on_with_tty(self, mock_tcsetattr, mock_tcgetattr, mock_stdin):\n        mock_stdin.isatty.return_value = True\n        attrs = [0, 0, 0, 0]\n        mock_tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        self.assertFalse(mock_tcsetattr.called)\n", "    def test_ensure_echo_on_without_termios(self):\n        with mock.patch('django.utils.autoreload.termios', None):\n            with mock.patch('sys.stdin.isatty', return_value=True):\n                autoreload.ensure_echo_on()  # Should not raise any exception\n", "    def test_echo_on_enabled(self, mocked_signal, mocked_stdin, mocked_termios):\n        # Mock attributes for termios\n        mocked_stdin.isatty.return_value = True\n        attributes = [0, 0, 0, 0, 0]\n        attributes[3] = 0\n        mocked_termios.tcgetattr.return_value = attributes\n\n        autoreload.ensure_echo_on()\n\n        # Assert that ECHO is enabled\n        self.assertTrue(attributes[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called_once_with(mocked_stdin, mocked_termios.TCSANOW, attributes)\n        mocked_signal.assert_called_once_with(signal.SIGTTOU, signal.SIG_IGN)\n"], "sample_302": ["    def test_runshell_sigint_handler(self, mock_settings_to_cmd_args_env):\n        mock_settings_to_cmd_args_env.return_value = (['psql'], None)\n        client = DatabaseClient(connection.settings_dict)\n        original_sigint_handler = signal.getsignal(signal.SIGINT)\n\n        with mock.patch('signal.signal') as mock_signal:\n            client.runshell([])\n\n            # Check that SIGINT was set to SIG_IGN\n            mock_signal.assert_any_call(signal.SIGINT, signal.SIG_IGN)\n            # Check that SIGINT handler was restored to the original\n            mock_signal.assert_any_call(signal.SIGINT, original_sigint_handler)\n", "    def test_settings_to_cmd_args_env_with_all_options(self, mock_settings):\n        client = DatabaseClient(connection=connection)\n        settings_dict = mock_settings\n        parameters = ['-c', 'SELECT 1;']\n        \n        expected_args = [\n            'psql',\n            '-U', 'test_user',\n            '-h', 'localhost',\n            '-p', '5432',\n            'test_db',\n            '-c', 'SELECT 1;',\n        ]\n        expected_env = {\n            'PGPASSWORD': 'test_pass',\n            'PGSERVICE': 'test_service',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': '/path/to/root.crt',\n            'PGSSLCERT': '/path/to/client.crt',\n            'PGSSLKEY': '/path/to/client.key',\n            'PGPASSFILE': '/path/to/.pgpass',\n        }\n\n        args, env = client.settings_to_cmd_args_env(settings_dict, parameters)\n        \n        self.assertEqual(args, expected_args)\n        self.assertEqual(env, expected_env)\n", "    def test_runshell_sigint_handling(self, mock_signal):\n        mock_getsignal = mock.Mock()\n        mock_signal.getsignal.return_value = mock_getsignal\n        mock_connection = mock.Mock()\n        client = DatabaseClient(connection=mock_connection)\n        \n        client.runshell(parameters=[])\n        \n        mock_signal.signal.assert_any_call(signal.SIGINT, signal.SIG_IGN)\n        mock_signal.signal.assert_any_call(signal.SIGINT, mock_getsignal)\n", "    def test_runshell_signal_handling(self, mock_signal, mock_runshell):\n        client = DatabaseClient(connection=connection)\n        mock_signal.return_value = 'original_handler'\n        parameters = ['param1', 'param2']\n\n        client.runshell(parameters)\n\n        mock_signal.assert_any_call(signal.SIGINT, signal.SIG_IGN)\n        mock_runshell.assert_called_once_with(parameters)\n        mock_signal.assert_any_call(signal.SIGINT, 'original_handler')\n", "    def test_settings_to_cmd_args_env_with_parameters(self):\n        parameters = ['-c', 'search_path=test_schema']\n        args, env = DatabaseClient.settings_to_cmd_args_env(connection.settings_dict, parameters)\n        expected_args = [\n            'psql', '-U', 'test_user', '-h', 'localhost', '-p', '5432', 'test_db', '-c', 'search_path=test_schema'\n        ]\n        expected_env = {\n            'PGPASSWORD': 'test_pass',\n            'PGSERVICE': 'test_service',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': '/path/to/root.crt',\n            'PGSSLCERT': '/path/to/cert.crt',\n            'PGSSLKEY': '/path/to/key.key',\n            'PGPASSFILE': '/path/to/passfile'\n        }\n        self.assertEqual(args, expected_args)\n        self.assertEqual(env, expected_env)\n"], "sample_303": ["    def test_runshell(self, mock_run):\n        class MockDatabaseClient(BaseDatabaseClient):\n            executable_name = 'mock_executable'\n\n            @classmethod\n                return ['mock_executable', 'param1'], {'ENV_VAR': 'value'}\n\n        client = MockDatabaseClient(connection=connection)\n        client.runshell(parameters=['param1'])\n\n        mock_run.assert_called_once_with(\n            ['mock_executable', 'param1'],\n            env={**os.environ, 'ENV_VAR': 'value'},\n            check=True\n        )\n", "    def test_runshell(self, mock_run):\n        class TestDatabaseClient(BaseDatabaseClient):\n            @classmethod\n                return ['test_command'], {'TEST_ENV': 'value'}\n\n        client = TestDatabaseClient(connection=connection)\n        client.runshell(parameters=None)\n\n        mock_run.assert_called_once_with(['test_command'], env={**os.environ, 'TEST_ENV': 'value'}, check=True)\n", "    def test_runshell_calls_subprocess_run(self, mock_run):\n        class CustomDatabaseClient(BaseDatabaseClient):\n            executable_name = 'dummy_executable'\n\n            @classmethod\n                return ['dummy_executable'], {'ENV_VAR': 'value'}\n\n        client = CustomDatabaseClient(connection=connection)\n        client.runshell(parameters=None)\n        mock_run.assert_called_once_with(\n            ['dummy_executable'],\n            env={**os.environ, 'ENV_VAR': 'value'},\n            check=True\n        )\n", "    def test_runshell(self, mock_run):\n        # Mock the settings_to_cmd_args_env to return predefined arguments and environment\n        with mock.patch.object(BaseDatabaseClient, 'settings_to_cmd_args_env', return_value=(['test_executable'], {'TEST_ENV': 'value'})):\n            self.client.runshell(None)\n            mock_run.assert_called_once_with(['test_executable'], env={**os.environ, 'TEST_ENV': 'value'}, check=True)\n", "    def test_runshell(self, mock_run):\n        class TestDatabaseClient(BaseDatabaseClient):\n            executable_name = 'test_executable'\n\n            @classmethod\n                return ['test_executable', 'arg1', 'arg2'], {'TEST_ENV_VAR': 'value'}\n\n        test_client = TestDatabaseClient(connection=connection)\n        test_client.runshell(parameters=['param1', 'param2'])\n\n        mock_run.assert_called_once_with(\n            ['test_executable', 'arg1', 'arg2'],\n            env={**os.environ, 'TEST_ENV_VAR': 'value'},\n            check=True\n        )\n"], "sample_304": ["    def test_null_characters_present(self):\n        validator = ProhibitNullCharactersValidator()\n        with self.assertRaises(ValidationError):\n            validator(\"This string contains a null character \\x00\")\n", "def test_url_validator_with_custom_schemes(self):\n    validator = URLValidator(schemes=['custom', 'another'])\n    valid_urls = [\n        'custom://example.com',\n        'another://example.com',\n    ]\n    invalid_urls = [\n        'http://example.com',\n        'https://example.com',\n        'ftp://example.com',\n        'custom://example.com\\n',\n        'custom://example.com\\r',\n        'custom://example.com\\t',\n    ]\n    for url in valid_urls:\n        with self.subTest(url=url):\n            self.assertIsNone(validator(url))\n    for url in invalid_urls:\n        with self.subTest(url=url):\n            with self.assertRaises(ValidationError):\n                validator(url)\n", "def test_max_value_validator_callable_limit_value(self):\n        return 10\n\n    validator = MaxValueValidator(get_limit_value)\n    self.assertIsNone(validator(5))\n    self.assertIsNone(validator(10))\n    with self.assertRaises(ValidationError):\n        validator(15)\n", "    def test_int_list_validator_custom_separator(self):\n        validator = int_list_validator(sep=';')\n        self.assertEqual(validator('1;2;3'), None)\n        with self.assertRaises(ValidationError):\n            validator('1; 2;3')\n        with self.assertRaises(ValidationError):\n            validator('1,2,3')\n", "def test_ip_address_validators(self):\n    with self.assertRaisesMessage(ValueError, \"You can only use `unpack_ipv4` if `protocol` is set to 'both'\"):\n        ip_address_validators('ipv4', unpack_ipv4=True)\n\n    with self.assertRaisesMessage(ValueError, \"The protocol 'unknown' is unknown. Supported: ['both', 'ipv4', 'ipv6']\"):\n        ip_address_validators('unknown', unpack_ipv4=False)\n\n    validators, message = ip_address_validators('both', unpack_ipv4=False)\n    self.assertEqual(validators, [validate_ipv46_address])\n    self.assertEqual(message, 'Enter a valid IPv4 or IPv6 address.')\n\n    validators, message = ip_address_validators('ipv4', unpack_ipv4=False)\n    self.assertEqual(validators, [validate_ipv4_address])\n    self.assertEqual(message, 'Enter a valid IPv4 address.')\n\n    validators, message = ip_address_validators('ipv6', unpack_ipv4=False)\n    self.assertEqual(validators, [validate_ipv6_address])\n    self.assertEqual(message, 'Enter a valid IPv6 address.')\n"], "sample_305": ["    def test_exact_lookup(self):\n        # Test Exact lookup on CharField\n        book = Book.objects.filter(name__exact='Practical Django Projects').first()\n        self.assertIsNotNone(book)\n        self.assertEqual(book.name, 'Practical Django Projects')\n\n        # Test Exact lookup on IntegerField\n        publisher = Publisher.objects.filter(num_awards__exact=7).first()\n        self.assertIsNotNone(publisher)\n        self.assertEqual(publisher.name, 'Prentice Hall')\n\n        # Test Exact lookup on DecimalField\n        book = Book.objects.filter(price__exact=Decimal('23.09')).first()\n        self.assertIsNotNone(book)\n        self.assertEqual(book.name, 'Sams Teach Yourself Django in 24 Hours')\n\n        # Test Exact lookup on DateTimeField\n        store = Store.objects.filter(original_opening__exact=datetime.datetime(1994, 4, 23, 9, 17, 42)).first()\n        self.assertIsNotNone(store)\n        self.assertEqual(store.name, 'Amazon.com')\n", "    def test_builtin_lookups(self):\n        # Testing 'exact' lookup\n        self.assertEqual(Book.objects.filter(price__exact=Decimal('30.00')).count(), 1)\n        \n        # Testing 'iexact' lookup\n        self.assertEqual(Book.objects.filter(name__iexact='the definitive guide to django: web development done right').count(), 1)\n        \n        # Testing 'gt' lookup\n        self.assertEqual(Book.objects.filter(price__gt=Decimal('50.00')).count(), 2)\n        \n        # Testing 'gte' lookup\n        self.assertEqual(Book.objects.filter(price__gte=Decimal('30.00')).count(), 3)\n        \n        # Testing 'lt' lookup\n        self.assertEqual(Book.objects.filter(price__lt=Decimal('30.00')).count(), 1)\n        \n        # Testing 'lte' lookup\n        self.assertEqual(Book.objects.filter(price__lte=Decimal('30.00')).count(), 2)\n        \n        # Testing 'in' lookup\n        self.assertEqual(Book.objects.filter(id__in=[self.b1.id, self.b2.id, self.b3.id]).count(), 3)\n        \n        # Testing 'contains' lookup\n        self.assertEqual(Book.objects.filter(name__contains='Django').count(), 3)\n        \n        # Testing 'icontains' lookup\n        self.assertEqual(Book.objects.filter(name__icontains='django').count(), 3)\n        \n        # Testing 'startswith' lookup\n        self.assertEqual(Book.objects.filter(name__startswith='Python').count(), 1)\n        \n        # Testing 'istartswith' lookup\n        self.assertEqual(Book.objects.filter(name__istartswith='python').count(), 1)\n        \n        # Testing 'endswith' lookup\n        self.assertEqual(Book.objects.filter(name__endswith='Lisp').count(), 1)\n        \n        # Testing 'iendswith' lookup\n        self.assertEqual(Book.objects.filter(name__iendswith='lisp').count(), 1)\n        \n        # Testing 'range' lookup\n        self.assertEqual(Book.objects.filter(price__range=(Decimal('20.00'), Decimal('30.00'))).count(), 3)\n        \n        # Testing 'isnull' lookup\n        self.assertEqual(Book.objects.filter(pubdate__isnull=False).count(), 6)\n        \n        # Testing 'regex' lookup\n        self.assertEqual(Book.objects.filter(name__regex=r'^Python').count", "    def test_contains_lookup(self):\n        \"\"\"\n        Test the 'contains' lookup to ensure it correctly filters queryset results.\n        \"\"\"\n        books_with_django = Book.objects.filter(name__contains='Django')\n        self.assertQuerysetEqual(\n            books_with_django, \n            ['The Definitive Guide to Django: Web Development Done Right', 'Sams Teach Yourself Django in 24 Hours', 'Practical Django Projects'], \n            lambda b: b.name\n        )\n", "    def test_lookup_contains(self):\n        \"\"\"\n        Test the `Contains` lookup.\n        \"\"\"\n        books_with_python = Book.objects.filter(name__contains='Python').order_by('name')\n        self.assertQuerysetEqual(\n            books_with_python, [\n                'Artificial Intelligence: A Modern Approach',\n                'Python Web Development with Django',\n                'Sams Teach Yourself Django in 24 Hours',\n            ],\n            lambda b: b.name\n        )\n", "    def test_boolean_field_exact_lookup(self):\n        \"\"\"\n        Test exact lookup for BooleanField.\n        \"\"\"\n        Author.objects.create(name='Author With True Bool', age=30, is_active=True)\n        Author.objects.create(name='Author With False Bool', age=31, is_active=False)\n\n        true_author = Author.objects.get(is_active__exact=True)\n        false_author = Author.objects.get(is_active__exact=False)\n\n        self.assertEqual(true_author.name, 'Author With True Bool')\n        self.assertEqual(false_author.name, 'Author With False Bool')\n"], "sample_306": ["def test_parse_time_with_invalid_microseconds(self):\n    # Invalid microsecond values\n    invalid_inputs = (\n        '10:20:30.1234567',  # More than 6 digits in microseconds\n        '10:20:30,1234567',  # More than 6 digits in microseconds\n    )\n    for invalid_input in invalid_inputs:\n        with self.subTest(invalid_input=invalid_input):\n            self.assertIsNone(parse_time(invalid_input))\n", "def test_parse_invalid_durations(self):\n    invalid_durations = [\n        '4 days 15:30:61',  # Invalid seconds\n        '4 days 15:61:30',  # Invalid minutes\n        '4 days 25:30:30',  # Invalid hours\n        'P2Y',  # Unsupported ISO 8601 year duration\n        'P2M',  # Unsupported ISO 8601 month duration\n        'P1Y2M3DT4H5M6S',  # Unsupported mixed ISO 8601 duration\n        '4 days 15:30:60.1',  # Invalid seconds with fraction\n        '4 days 15:30:30,-1',  # Invalid negative microseconds\n        '4 days 15:30:30.0000001',  # Too precise fraction of seconds\n        '4 days 15:30:30.1000000',  # Too precise fraction of seconds\n    ]\n    for duration in invalid_durations:\n        with self.subTest(duration=duration):\n            self.assertIsNone(parse_duration(duration))\n", "    def test_iso8601_duration_format(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P2.5D', timedelta(days=2, hours=12)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT2.5H', timedelta(hours=2, minutes=30)),\n            ('PT30M', timedelta(minutes=30)),\n            ('PT45.5M', timedelta(minutes=45, seconds=30)),\n            ('PT15S', timedelta(seconds=15)),\n            ('PT0.5S', timedelta(milliseconds=500)),\n            ('P1DT1H', timedelta(days=1, hours=1)),\n            ('P1DT2H30M', timedelta(days=1, hours=2, minutes=30)),\n            ('-P1D', timedelta(days=-1)),\n            ('-P1DT1H', timedelta(days=-1, hours=-1)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration_format(self):\n        test_values = (\n            ('P4DT15H30M10S', timedelta(days=4, hours=15, minutes=30, seconds=10)),\n            ('P4D', timedelta(days=4)),\n            ('PT15H30M', timedelta(hours=15, minutes=30)),\n            ('PT0.5S', timedelta(seconds=0.5)),\n            ('PT0,5S', timedelta(seconds=0.5)),\n            ('P-4DT-15H-30M-10S', timedelta(days=-4, hours=-15, minutes=-30, seconds=-10)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P4DT15H30M', timedelta(days=4, hours=15, minutes=30)),\n            ('P4DT15H30.5M', timedelta(days=4, hours=15, minutes=30.5)),\n            ('P4DT15H30M10.5S', timedelta(days=4, hours=15, minutes=30, seconds=10.5)),\n            ('PT15H30M', timedelta(hours=15, minutes=30)),\n            ('PT15H30.5M', timedelta(hours=15, minutes=30.5)),\n            ('PT15H30M10.5S', timedelta(hours=15, minutes=30, seconds=10.5)),\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('P-1D', timedelta(days=-1)),\n            ('PT-1H', timedelta(hours=-1)),\n            ('PT-1M', timedelta(minutes=-1)),\n            ('PT-1S', timedelta(seconds=-1)),\n            ('P1.5D', timedelta(days=1.5)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_307": ["    def test_iso8601_year_number(self):\n        dt = datetime(2020, 1, 1)\n        self.assertEqual(dateformat.format(dt, 'o'), '2020')\n", "    def test_format_with_escaped_characters(self):\n        dt = datetime(2023, 10, 25, 14, 45, 30)\n        self.assertEqual(dateformat.format(dt, r'Y \\y\\e\\a\\r z'), '2023 year 298')\n        self.assertEqual(dateformat.format(dt, r'H \\h\\o\\u\\r\\s \\a\\n\\d i \\m\\i\\n\\u\\t\\e\\s'), '14 hours and 45 minutes')\n", "    def test_z_format(self):\n        # Test 'z' format (day of the year)\n        dates_and_days = [\n            (datetime(2020, 1, 1), '1'),       # New Year's Day\n            (datetime(2020, 12, 31), '366'),   # Last day of a leap year\n            (datetime(2021, 1, 1), '1'),       # New Year's Day in a non-leap year\n            (datetime(2021, 12, 31), '365'),   # Last day of a non-leap year\n            (datetime(2023, 3, 1), '60'),      # March 1st, non-leap year\n            (datetime(2024, 3, 1), '61'),      # March 1st, leap year\n        ]\n        for dt, expected_day in dates_and_days:\n            with self.subTest(dt=dt):\n                self.assertEqual(dateformat.format(dt, 'z'), expected_day)\n", "    def test_ISO_8601_format(self):\n        timestamp = datetime(2008, 5, 19, 11, 45, 23, 123456)\n        self.assertEqual(dateformat.format(timestamp, 'c'), '2008-05-19T11:45:23.123456')\n", "    def test_invalid_date_format_specifiers(self):\n        dt = datetime(2023, 10, 10, 10, 10, 10)\n\n        for specifier in ['E', 'N', 'b']:\n            msg = (\n                \"The format for date objects may not contain time-related \"\n                \"format specifiers (found '%s').\" % specifier\n            )\n            with self.assertRaisesMessage(TypeError, msg):\n                dateformat.format(dt, specifier)\n\n        # Ensure valid output for valid date format specifiers\n        self.assertEqual(dateformat.format(dt, 'Y-m-d'), '2023-10-10')\n"], "sample_308": ["    def test_iso_year_number(self):\n        self.assertEqual(dateformat.format(datetime(2023, 1, 1), 'o'), '2022')\n        self.assertEqual(dateformat.format(datetime(2023, 1, 2), 'o'), '2023')\n        self.assertEqual(dateformat.format(datetime(2023, 12, 31), 'o'), '2023')\n        self.assertEqual(dateformat.format(datetime(2024, 1, 1), 'o'), '2024')\n", "    def test_timezone_name_format(self):\n        tz = get_fixed_timezone(-510)\n        dt = make_aware(datetime(2009, 5, 16, 5, 30, 30), tz)\n        self.assertEqual(dateformat.format(dt, 'e'), '-0830')\n\n        dt_naive = datetime(2009, 5, 16, 5, 30, 30)\n        self.assertEqual(dateformat.format(dt_naive, 'e'), '')\n\n        dt_no_tzname = make_aware(datetime(2009, 5, 16, 5, 30, 30), tz)\n        self.assertEqual(dateformat.format(dt_no_tzname, 'e'), '-0830')\n", "def test_month_names_in_different_locales(self):\n    # Test month names in different locales\n    dt = datetime(2023, 1, 1)\n    with translation.override('en'):\n        self.assertEqual(dateformat.format(dt, 'F'), 'January')\n        self.assertEqual(dateformat.format(dt, 'M'), 'Jan')\n        self.assertEqual(dateformat.format(dt, 'b'), 'jan')\n    with translation.override('fr'):\n        self.assertEqual(dateformat.format(dt, 'F'), 'janvier')\n        self.assertEqual(dateformat.format(dt, 'M'), 'Jan')\n        self.assertEqual(dateformat.format(dt, 'b'), 'jan')\n", "def test_time_format(self):\n    dt = datetime(2023, 10, 5, 14, 30, 45)\n    \n    self.assertEqual(dateformat.time_format(dt, 'H:i:s'), '14:30:45')\n    self.assertEqual(dateformat.time_format(dt, 'h:i A'), '02:30 PM')\n    self.assertEqual(dateformat.time_format(dt, 'G:i:s'), '14:30:45')\n    self.assertEqual(dateformat.time_format(dt, 'g:i a'), '2:30 p.m.')\n", "    def test_iso8601_year_week(self):\n        timestamp = datetime(2023, 1, 5)  # 2023-01-05 is in the first week of 2023\n        self.assertEqual(dateformat.format(timestamp, 'o'), '2023')\n        self.assertEqual(dateformat.format(timestamp, 'W'), '01')\n"], "sample_309": ["    def test_valid_dates(self):\n        valid_dates = [\n            'Sun, 06 Nov 1994 08:49:37 GMT',\n            'Sunday, 06-Nov-94 08:49:37 GMT',\n            'Sun Nov  6 08:49:37 1994'\n        ]\n        for date in valid_dates:\n            with self.subTest(date=date):\n                self.assertIsNotNone(parse_http_date_safe(date))\n", "    def test_valid_date(self):\n        valid_date = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        parsed = parse_http_date_safe(valid_date)\n        expected = datetime(1994, 11, 6, 8, 49, 37, tzinfo=timezone.utc).timestamp()\n        self.assertEqual(parsed, expected)\n", "    def test_valid_dates(self):\n        valid_dates = [\n            'Sun, 06 Nov 1994 08:49:37 GMT',  # RFC1123\n            'Sunday, 06-Nov-94 08:49:37 GMT',  # RFC850\n            'Sun Nov  6 08:49:37 1994'         # asctime\n        ]\n        for date in valid_dates:\n            with self.subTest(date=date):\n                self.assertEqual(parse_http_date_safe(date), parse_http_date(date))\n", "    def test_invalid_base64(self):\n        invalid_strings = [\n            'invalid$$$',\n            '!!!notbase64',\n            'abc=def',\n        ]\n        for s in invalid_strings:\n            with self.subTest(s=s):\n                with self.assertRaises(ValueError):\n                    urlsafe_base64_decode(s)\n", "    def test_valid_date(self):\n        valid_date = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertEqual(\n            datetime.fromtimestamp(parse_http_date_safe(valid_date), timezone.utc),\n            datetime(1994, 11, 6, 8, 49, 37, tzinfo=timezone.utc),\n        )\n"], "sample_310": ["    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type(\"get_user_list\"), \"List\")\n        self.assertEqual(get_return_data_type(\"get_user_count\"), \"Integer\")\n        self.assertEqual(get_return_data_type(\"get_user_info\"), \"\")\n        self.assertEqual(get_return_data_type(\"fetch_data\"), \"\")\n", "    def test_simplify_regex_named_groups(self):\n        regex = r'^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$'\n        simplified = simplify_regex(regex)\n        self.assertEqual(simplified, '/<sport_slug>/athletes/<athlete_slug>/')\n", "    def test_named_groups(self):\n        regex = r'^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$'\n        expected = '/<sport_slug>/athletes/<athlete_slug>/'\n        self.assertEqual(simplify_regex(regex), expected)\n", "def test_template_tag_index_no_engine(self):\n    \"\"\"\n    If the default template engine is not configured, the template tag index\n    view should handle it gracefully.\n    \"\"\"\n    with override_settings(TEMPLATES=[]):\n        response = self.client.get(reverse('django-admindocs-tags'))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"<h1>Template tags</h1>\", html=True)\n        self.assertNotContains(response, '<h3 id=\"built_in-extends\">extends</h3>')\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n"], "sample_312": ["    def test_add_squash_false(self):\n        node = Node(['a'])\n        added_node = Node(['b'])\n        result = node.add(added_node, node.default, squash=False)\n        self.assertEqual(result, added_node)\n        self.assertEqual(len(node), 2)\n        self.assertIn(added_node, node)\n", "def test_add_squash(self):\n        node = Node(['a', 'b'], 'AND')\n        added_node = Node(['c', 'd'], 'AND')\n        self.assertEqual(node.add(added_node, 'AND'), node)\n        self.assertEqual(str(node), \"(AND: a, b, c, d)\")\n        \n        node = Node(['a', 'b'], 'AND')\n        added_node = Node(['c'], 'AND')\n        self.assertEqual(node.add(added_node, 'AND'), node)\n        self.assertEqual(str(node), \"(AND: a, b, c)\")\n        \n        node = Node(['a'], 'AND')\n        added_node = Node(['b', 'c'], 'AND')\n        self.assertEqual(node.add(added_node, 'AND'), added_node)\n        self.assertEqual(str(node), \"(AND: a, (AND: b, c))\")\n", "    def test_add_squash_false(self):\n        node = Node(['a', 'b'], 'AND')\n        node.add('c', 'AND', squash=False)\n        self.assertEqual(node, Node(['a', 'b', 'c'], 'AND'))\n", "def test_add_squash(self):\n    node = Node(['a', 'b'], 'AND')\n    node2 = Node(['c', 'd'], 'AND')\n    node.add(node2, 'AND')\n    self.assertEqual(str(node), \"(AND: a, b, c, d)\")\n    \n    node3 = Node(['e'], 'AND')\n    node.add(node3, 'AND')\n    self.assertEqual(str(node), \"(AND: a, b, c, d, e)\")\n", "    def test_add_with_squash(self):\n        node = Node(['a', 'b'], 'DEFAULT')\n        child_node = Node(['c', 'd'], 'DEFAULT')\n        # Test squashing behavior\n        node.add(child_node, 'DEFAULT')\n        self.assertEqual(str(node), \"(DEFAULT: a, b, c, d)\")\n        \n        # Test non-squashing behavior\n        node2 = Node(['a', 'b'], 'DEFAULT')\n        node2.add(child_node, 'DEFAULT', squash=False)\n        self.assertEqual(str(node2), \"(DEFAULT: a, b, (DEFAULT: c, d))\")\n"], "sample_311": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.staff_user = User.objects.create_user(username='staff', password='secret', is_staff=True)\n        cls.regular_user = User.objects.create_user(username='regular', password='secret')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def test_register_functionality(self):\n        \"\"\"\n        Ensure that the register function works correctly, including abstract model\n        and duplicate registration checks.\n        \"\"\"\n        class AbstractModelBase:\n            class Meta:\n                abstract = True\n        \n        class ConcreteModelBase:\n            class Meta:\n                abstract = False\n\n        admin_site = AdminSite()\n\n        # Register a concrete model\n        admin_site.register(ConcreteModelBase)\n        self.assertTrue(admin_site.is_registered(ConcreteModelBase))\n\n        # Trying to register an abstract model should raise an ImproperlyConfigured error\n        with self.assertRaises(ImproperlyConfigured):\n            admin_site.register(AbstractModelBase)\n\n        # Trying to register the same concrete model again should raise an AlreadyRegistered error\n        with self.assertRaises(AlreadyRegistered):\n            admin_site.register(ConcreteModelBase)\n\n        # Unregister the concrete model\n        admin_site.unregister(ConcreteModelBase)\n        self.assertFalse(admin_site.is_registered(ConcreteModelBase))\n\n        # Trying to unregister a model that is not registered should raise a NotRegistered error\n        with self.assertRaises(NotRegistered):\n            admin_site.unregister(ConcreteModelBase)\n\n        # Testing dynamic admin class creation with **options\n        admin_site.register(ConcreteModelBase, list_display=('field1', 'field2'))\n        self.assertTrue(admin_site.is_registered(ConcreteModelBase))", "    def setUp(self):\n        self.superuser = User.objects.create_superuser(\n            username='super', password='secret', email='super@example.com'\n        )\n        self.client.force_login(self.superuser)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.s1 = Section.objects.create(name='Test section')\n        cls.a1 = Article.objects.create(\n            content='<p>Middle content</p>', date=datetime.datetime(2008, 3, 18, 11, 54, 58), section=cls.s1\n        )\n        cls.a2 = Article.objects.create(\n            content='<p>Oldest content</p>', date=datetime.datetime(2000, 3, 18, 11, 54, 58), section=cls.s1\n        )\n        cls.a3 = Article.objects.create(\n            content='<p>Newest content</p>', date=datetime.datetime(2009, 3, 18, 11, 54, 58), section=cls.s1\n        )\n"], "sample_313": ["    def test_cached_loader_reset(self):\n        with mock.patch('django.template.loaders.filesystem.Loader.reset') as mock_fs_reset, \\\n             mock.patch('django.template.loaders.app_directories.Loader.reset') as mock_ad_reset:\n            autoreload.reset_loaders()\n            mock_fs_reset.assert_called_once()\n            mock_ad_reset.assert_called_once()\n", "    def test_get_template_directories_excludes_django_paths(self, mock_is_django_path):\n        directories = autoreload.get_template_directories()\n        # Ensure that no directories considered as Django paths are included\n        for directory in directories:\n            self.assertFalse(mock_is_django_path(directory))\n", "    def test_django_template_excluded(self, mock_is_django_path):\n        directories = autoreload.get_template_directories()\n        self.assertNotIn(ROOT / 'templates_extra', directories)\n        self.assertNotIn(ROOT / 'templates', directories)\n        mock_is_django_path.assert_called()\n", "    def test_no_template_directories(self, mock_get_dirs):\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        mock_reloader.watch_dir.assert_not_called()\n", "    def test_template_dir_not_in_django_path(self, mock_is_django_path):\n        template_dirs = autoreload.get_template_directories()\n        self.assertIn(ROOT / 'templates_extra', template_dirs)\n        self.assertIn(ROOT / 'templates', template_dirs)\n        mock_is_django_path.assert_any_call(str(ROOT / 'templates'))\n        mock_is_django_path.assert_any_call(str(ROOT / 'templates_extra'))\n"], "sample_314": ["    def test_has_changed_with_identical_values(self):\n        field = ReadOnlyPasswordHashField()\n        # Identical values should not indicate a change\n        self.assertFalse(field.has_changed('aaa', 'aaa'))\n", "    def test_unicode_ci_compare_equal_strings(self):\n        # Test with equal strings.\n        self.assertTrue(_unicode_ci_compare('test', 'test'))\n", "    def test_summary_with_usable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'pbkdf2_sha256$100000$a6Pucb1qSFcD$WmCkn9Hqidj48NVe5x0FEM6A9YiOqQcl/83m2Z5udm0='\n        context = widget.get_context('password', value, {'id': 'id_password'})\n        self.assertIn('summary', context)\n        self.assertEqual(len(context['summary']), 4)\n        self.assertEqual(context['summary'][0]['label'], 'algorithm')\n        self.assertEqual(context['summary'][1]['label'], 'iterations')\n        self.assertEqual(context['summary'][2]['label'], 'salt')\n        self.assertEqual(context['summary'][3]['label'], 'hash')\n", "    def test_unicode_ci_compare_equal(self):\n        self.assertTrue(_unicode_ci_compare('Hello', 'hello'))\n        self.assertTrue(_unicode_ci_compare('Stra\u00dfe', 'strasse'))\n        self.assertTrue(_unicode_ci_compare('\u03a3', '\u03c3'))\n", "    def test_unicode_ci_compare_equal(self):\n        self.assertTrue(_unicode_ci_compare('Stra\u00dfe', 'strasse'))\n        self.assertTrue(_unicode_ci_compare('TEST', 'test'))\n        self.assertTrue(_unicode_ci_compare('Aa', 'aA'))\n"], "sample_315": ["    def test_activate_language_from_request(self):\n        request = RequestFactory().get('/nl/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        middleware.process_request(request)\n        self.assertEqual(translation.get_language(), 'nl')\n        self.assertEqual(request.LANGUAGE_CODE, 'nl')\n", "    def test_language_prefix_patterns_used(self):\n        with translation.override('nl'):\n            response = self.client.get('/nl/prefixed/')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers['content-language'], 'nl')\n            self.assertEqual(response.context['LANGUAGE_CODE'], 'nl')\n\n        with translation.override('en'):\n            response = self.client.get('/en/prefixed/')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers['content-language'], 'en')\n            self.assertEqual(response.context['LANGUAGE_CODE'], 'en')\n", "    def test_redirect_with_invalid_path(self):\n        response = self.client.get('/nonexistent/', HTTP_ACCEPT_LANGUAGE='en')\n        self.assertEqual(response.status_code, 404)\n", "    def test_language_from_path_redirect(self):\n        response = self.client.get('/nl/not-prefixed/', HTTP_ACCEPT_LANGUAGE='en')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers['content-language'], 'nl')\n        self.assertEqual(response.context['LANGUAGE_CODE'], 'nl')\n", "    def test_fallback_language(self):\n        # Request in unsupported language, should fallback to default 'en-us'\n        response = self.client.get('/some-url/', HTTP_ACCEPT_LANGUAGE='fr')\n        self.assertEqual(response.status_code, 404)\n        self.assertEqual(response.headers['content-language'], 'en')\n        self.assertEqual(response.context['LANGUAGE_CODE'], 'en')\n\n        # Request in supported language\n        response = self.client.get('/nl/some-url/', HTTP_ACCEPT_LANGUAGE='nl')\n        self.assertEqual(response.status_code, 404)\n        self.assertEqual(response.headers['content-language'], 'nl')\n        self.assertEqual(response.context['LANGUAGE_CODE'], 'nl')\n"], "sample_316": ["    def test_imagefile_dimensions(self):\n        \"\"\"\n        Test that the ImageFile class returns correct width and height.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, 540)\n            self.assertEqual(image_file.height, 405)\n", "    def test_imagefile_dimensions(self):\n        \"\"\"\n        Test that the ImageFile class correctly retrieves width and height.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, 800)  # Assuming test.png has width of 800\n            self.assertEqual(image_file.height, 600)  # Assuming test.png has height of 600)\n", "    def test_image_file_dimensions(self):\n        \"\"\"\n        Test that ImageFile correctly returns image dimensions.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, Image.open(fh).width)\n            self.assertEqual(image_file.height, Image.open(fh).height)\n", "    def test_imagefile_properties(self):\n        \"\"\"\n        Test ImageFile width and height properties.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, 540)\n            self.assertEqual(image_file.height, 405)\n", "    def test_imagefile_width_height(self):\n        \"\"\"\n        ImageFile should correctly provide width and height properties.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, images.get_image_dimensions(fh)[0])\n            self.assertEqual(image_file.height, images.get_image_dimensions(fh)[1])\n"], "sample_317": ["    def test_rfc3339_date_naive_datetime(self):\n        \"\"\"\n        Test rfc3339_date with a naive datetime object.\n        \"\"\"\n        naive_datetime = datetime.datetime(2021, 5, 4, 12, 30)\n        formatted_date = rfc3339_date(naive_datetime)\n        self.assertEqual(formatted_date, '2021-05-04T12:30:00Z')\n", "    def test_rfc2822_date_format(self):\n        \"\"\"\n        Test the formatting of dates using rfc2822_date function.\n        \"\"\"\n        naive_datetime = datetime.datetime(2023, 10, 1, 15, 23, 45)\n        aware_datetime = timezone.make_aware(naive_datetime, timezone=timezone.utc)\n        \n        self.assertEqual(rfc2822_date(naive_datetime), 'Sun, 01 Oct 2023 15:23:45 -0000')\n        self.assertEqual(rfc2822_date(aware_datetime), 'Sun, 01 Oct 2023 15:23:45 +0000')\n", "    def test_rss_feed_with_enclosure(self):\n        \"\"\"\n        Test the structure and content of RSS feeds with an enclosure.\n        \"\"\"\n        feed = Rss201rev2Feed(\n            title=\"Sample RSS Feed\",\n            link=\"http://example.com/feed/\",\n            description=\"This is a sample RSS feed with an enclosure.\",\n        )\n        feed.add_item(\n            title=\"Sample Item\",\n            link=\"http://example.com/item/1/\",\n            description=\"This is a sample item with an enclosure.\",\n            enclosures=[Enclosure(\"http://example.com/enclosure.mp3\", \"12345\", \"audio/mpeg\")]\n        )\n        rss_content = feed.writeString('utf-8')\n        doc = minidom.parseString(rss_content)\n        chan = doc.getElementsByTagName('channel')[0]\n        items = chan.getElementsByTagName('item')\n\n        self.assertEqual(len(items), 1)\n        self.assertChildNodeContent(items[0], {\n            'title': 'Sample Item',\n            'link': 'http://example.com/item/1/',\n            'description': 'This is a sample item with an enclosure.',\n        })\n        enclosures = items[0].getElementsByTagName('enclosure')\n        self.assertEqual(len(enclosures), 1)\n        enclosure = enclosures[0]\n        self.assertEqual(enclosure.getAttribute('url'), \"http://example.com/enclosure.mp3\")\n        self.assertEqual(enclosure.getAttribute('length'), \"12345\")\n        self.assertEqual(enclosure.getAttribute('type'), \"audio/mpeg\")\n", "def test_rfc2822_date(self):\n    \"\"\"\n    Test the conversion of datetime objects to RFC 2822 date format.\n    \"\"\"\n    naive_dt = datetime.datetime(2023, 10, 1, 15, 30, 45)\n    aware_dt = timezone.make_aware(naive_dt, timezone=TZ)\n    expected_naive = email.utils.format_datetime(datetime.datetime(2023, 10, 1, 15, 30, 45))\n    expected_aware = email.utils.format_datetime(aware_dt)\n    self.assertEqual(rfc2822_date(naive_dt), expected_naive)\n    self.assertEqual(rfc2822_date(aware_dt), expected_aware)\n", "def test_rfc2822_date(self):\n    \"\"\"\n    Test the rfc2822_date function to ensure it properly formats datetime objects.\n    \"\"\"\n    date = datetime.datetime(2023, 10, 1, 12, 30, 45)\n    formatted_date = rfc2822_date(date)\n    self.assertEqual(formatted_date, \"Sun, 01 Oct 2023 12:30:45 -0000\")\n"], "sample_318": ["    def test_regex_pattern_match(self):\n        pattern = RegexPattern(r'^test/(?P<arg1>\\d+)/(?P<arg2>\\w+)/$', name='test-pattern')\n        match = pattern.match('test/123/abc/')\n        self.assertIsNotNone(match)\n        self.assertEqual(match[1], ())\n        self.assertEqual(match[2], {'arg1': '123', 'arg2': 'abc'})\n", "    def test_urlpattern_repr(self):\n        \"\"\"\n        Test that the __repr__ of URLPattern provides accurate details.\n        \"\"\"\n        pattern = URLPattern(RegexPattern(r'^test/$'), views.empty_view, name='test-pattern')\n        self.assertEqual(\n            repr(pattern),\n            \"<URLPattern 'test-pattern'>\"\n        )\n", "    def test_compile_valid_regex(self):\n        pattern = RegexPattern(r'^[a-zA-Z0-9_]+$', name='valid-regex')\n        compiled_regex = pattern._compile(pattern._regex)\n        self.assertIsInstance(compiled_regex, re.Pattern)\n", "    def test_compile_valid_regex(self):\n        pattern = RegexPattern(r'^test/$')\n        compiled_regex = pattern._compile(pattern._regex)\n        self.assertTrue(compiled_regex.match(\"test/\"))\n", "    def test_urlpattern_repr(self):\n        \"\"\"\n        Test repr of URLPattern to ensure it correctly describes the URL pattern.\n        \"\"\"\n        pattern = URLPattern(RegexPattern(r'^test/$'), views.empty_view, name='test-view')\n        expected_repr = \"<URLPattern 'test' [name='test-view']>\"\n        self.assertEqual(repr(pattern), expected_repr)\n"], "sample_319": ["    def test_remove_unique_together_and_add_index(self):\n        \"\"\"\n        Removing unique_together and adding an index on the same fields.\n        \"\"\"\n        before = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"birthdate\", models.DateField()),\n            ],\n            {\n                \"unique_together\": {(\"name\", \"birthdate\")},\n            },\n        )\n        after = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"birthdate\", models.DateField()),\n            ],\n            {\n                \"indexes\": [\n                    models.Index(fields=[\"name\", \"birthdate\"], name=\"author_name_birthdate_idx\")\n                ]\n            },\n        )\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes, \"testapp\", 0, [\"AlterUniqueTogether\", \"AddIndex\"]\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"author\",\n            unique_together=set(),\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            model_name=\"author\",\n            index=models.Index(fields=[\"name\", \"birthdate\"], name=\"author_name_birthdate_idx\"),\n        )\n", "    def test_alter_field_and_index_together_with_renamed_field(self):\n        \"\"\"Fields are renamed before updating index_together.\"\"\"\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\", \"age\")},\n            },\n        )\n        renamed_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"full_name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"full_name\", \"age\")},\n            },\n        )\n        changes = self.get_changes(\n            [initial_author], \n            [renamed_author], \n            MigrationQuestioner({\"ask_rename\": True}),\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameField\", \"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            model_name=\"author\",\n            old_name=\"name\",\n            new_name=\"full_name\",\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"author\",\n            index_together={(\"full_name\", \"age\")},\n        )\n", "    def test_add_field_with_preserve_default(self):\n        \"\"\"Test adding a field with preserve_default set to False.\"\"\"\n        before_state = [\n            ModelState(\n                \"app\",\n                \"ModelWithField\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n        ]\n        after_state = [\n            ModelState(\n                \"app\",\n                \"ModelWithField\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"new_field\", models.CharField(max_length=255, default=\"default value\")),\n                ],\n            ),\n        ]\n        changes = self.get_changes(before_state, after_state)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"app\", 1)\n        self.assertOperationTypes(changes, \"app\", 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"new_field\", preserve_default=False)\n", "def test_alter_field_to_not_null_with_blank_and_default(self):\n    \"\"\"\n    Tests autodetection of nullable to non-nullable alterations when\n    the field has both blank=True and a default value.\n    \"\"\"\n    author_with_blank_and_default = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, null=True, blank=True, default=\"Anonymous\")),\n        ],\n    )\n    author_with_non_null = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, null=False, blank=True, default=\"Anonymous\")),\n        ],\n    )\n    changes = self.get_changes([author_with_blank_and_default], [author_with_non_null])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True\n    )\n    self.assertOperationFieldAttributes(\n        changes, \"testapp\", 0, 0, default=\"Anonymous\"\n    )\n", "    def test_deep_deconstruct(self):\n        \"\"\"Tests the deep deconstruction of various objects.\"\"\"\n        class CustomField(models.CharField):\n                name, path, args, kwargs = super().deconstruct()\n                kwargs['custom'] = True\n                return name, path, args, kwargs\n\n        compiled_regex = re.compile(r'^test$')\n        objects = [\n            [1, 2, 3],\n            (1, 2, 3),\n            {'a': 1, 'b': 2},\n            functools.partial(print, 'Hello'),\n            compiled_regex,\n            CustomField(max_length=100),\n        ]\n        autodetector = MigrationAutodetector(None, None)\n        deconstructed_objects = autodetector.deep_deconstruct(objects)\n\n        self.assertEqual(deconstructed_objects[0], [1, 2, 3])\n        self.assertEqual(deconstructed_objects[1], (1, 2, 3))\n        self.assertEqual(deconstructed_objects[2], {'a': 1, 'b': 2})\n        self.assertEqual(deconstructed_objects[3], (print, ('Hello',), None))\n        self.assertEqual(deconstructed_objects[4], RegexObject(compiled_regex))\n        self.assertEqual(\n            deconstructed_objects[5],\n            (\n                'path.to.CustomField', \n                [100], \n                {'max_length': 100, 'custom': True}\n            )\n        )\n"], "sample_320": ["    def test_delete_model_state_forwards(self):\n        \"\"\"\n        Test the state forwards method of DeleteModel operation.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_dlmosf\")\n        operation = migrations.DeleteModel(\"Pony\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_dlmosf\", new_state)\n        self.assertNotIn((\"test_dlmosf\", \"pony\"), new_state.models)\n        self.assertIn((\"test_dlmosf\", \"rider\"), new_state.models)\n", "    def test_alter_model_table_comment(self):\n        app_label = \"test_alter_table_comment\"\n        project_state = self.set_up_test_model(app_label)\n        pony_table = f\"{app_label}_pony\"\n        # Add table comment.\n        operation = migrations.AlterModelTableComment(\"Pony\", \"This table stores pony data.\")\n        self.assertEqual(operation.describe(), \"Alter Pony table comment\")\n        self.assertEqual(operation.migration_name_fragment, \"alter_pony_table_comment\")\n        new_state = project_state.clone()\n        operation.state_forwards(app_label, new_state)\n        self.assertEqual(\n            new_state.models[app_label, \"pony\"].options[\"db_table_comment\"],\n            \"This table stores pony data.\",\n        )\n        self.assertTableCommentNotExists(pony_table)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(app_label, editor, project_state, new_state)\n        self.assertTableComment(pony_table, \"This table stores pony data.\")\n        # Reversal.\n        with connection.schema_editor() as editor:\n            operation.database_backwards(app_label, editor, new_state, project_state)\n        self.assertTableCommentNotExists(pony_table)\n        # Deconstruction.\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AlterModelTableComment\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(\n            definition[2], {\"name\": \"Pony\", \"table_comment\": \"This table stores pony data.\"}\n        )\n", "def test_alter_field_with_db_index(self):\n    \"\"\"\n    Tests the AlterField operation on a field with db_index=True.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_alfl_db_index\")\n    # Add the field with db_index=True\n    add_field_operation = migrations.AddField(\n        \"Pony\",\n        \"nickname\",\n        models.CharField(max_length=100, db_index=True, default=\"Unknown\"),\n    )\n    new_state = project_state.clone()\n    add_field_operation.state_forwards(\"test_alfl_db_index\", new_state)\n    with connection.schema_editor() as editor:\n        add_field_operation.database_forwards(\n            \"test_alfl_db_index\", editor, project_state, new_state\n        )\n    self.assertColumnExists(\"test_alfl_db_index_pony\", \"nickname\")\n    self.assertIndexExists(\"test_alfl_db_index_pony\", [\"nickname\"])\n    \n    # Alter the field to remove the index\n    alter_field_operation = migrations.AlterField(\n        \"Pony\",\n        \"nickname\",\n        models.CharField(max_length=100, db_index=False, default=\"Unknown\"),\n    )\n    altered_state = new_state.clone()\n    alter_field_operation.state_forwards(\"test_alfl_db_index\", altered_state)\n    with connection.schema_editor() as editor:\n        alter_field_operation.database_forwards(\n            \"test_alfl_db_index\", editor, new_state, altered_state\n        )\n    self.assertColumnExists(\"test_alfl_db_index_pony\", \"nickname\")\n    self.assertIndexNotExists(\"test_alfl_db_index_pony\", [\"nickname\"])\n\n    # Test reversal\n    with connection.schema_editor() as editor:\n        alter_field_operation.database_backwards(\n            \"test_alfl_db_index\", editor, altered_state, new_state\n        )\n    self.assertIndexExists(\"test_alfl_db_index_pony\", [\"nickname\"])\n", "    def test_references_model_with_inheritance(self):\n        \"\"\"\n        Tests that references_model correctly identifies models referenced\n        through inheritance.\n        \"\"\"\n        operation = migrations.CreateModel(\n            \"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"color\", models.CharField(max_length=20)),\n            ],\n            bases=(\"migrations.UnicodeModel\",),\n        )\n        self.assertTrue(operation.references_model(\"unicodemodel\", \"migrations\"))\n        self.assertFalse(operation.references_model(\"nonexistentmodel\", \"migrations\"))\n\n        operation = migrations.CreateModel(\n            \"Horse\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=50)),\n            ],\n            bases=(\"test_crmo.Pony\",),\n        )\n        self.assertTrue(operation.references_model(\"pony\", \"test_crmo\"))\n        self.assertFalse(operation.references_model(\"unicodemodel\", \"test_crmo\"))\n", "    def test_alter_field_pk_with_unique_constraint(self):\n        \"\"\"\n        Test the AlterField operation on primary keys with unique constraints.\n        \"\"\"\n        app_label = \"test_alflpkuc\"\n        # Setting up the test model with unique constraint\n        project_state = self.set_up_test_model(\n            app_label,\n            constraints=[\n                models.UniqueConstraint(fields=[\"id\"], name=\"test_unique_constraint\")\n            ],\n        )\n        # Alter the primary key field\n        operation = migrations.AlterField(\n            \"Pony\", \"id\", models.BigIntegerField(primary_key=True)\n        )\n        new_state = project_state.clone()\n        operation.state_forwards(app_label, new_state)\n        self.assertIsInstance(\n            new_state.models[app_label, \"pony\"].fields[\"id\"],\n            models.BigIntegerField,\n        )\n        # Test the database alteration\n        with connection.schema_editor() as editor:\n            operation.database_forwards(app_label, editor, project_state, new_state)\n        \n        if connection.features.supports_foreign_keys:\n            self.assertFKExists(\n                f\"{app_label}_pony\",\n                [\"id\"],\n                (f\"{app_label}_pony\", \"id\"),\n            )\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(app_label, editor, new_state, project_state)\n        self.assertIsInstance(\n            project_state.models[app_label, \"pony\"].fields[\"id\"],\n            models.AutoField,\n        )\n        if connection.features.supports_foreign_keys:\n            self.assertFKExists(\n                f\"{app_label}_pony\",\n                [\"id\"],\n                (f\"{app_label}_pony\", \"id\"),\n            )\n"], "sample_321": ["    def test_process_request_invalid_csrf_cookie(self):\n        \"\"\"\n        If an invalid CSRF cookie is present, the middleware should replace it with a new one.\n        \"\"\"\n        req = self._get_GET_csrf_cookie_request(cookie=64 * '*')\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_request(req)\n        self.assertTrue(req.csrf_cookie_needs_reset)\n        self.assertIsNotNone(req.META.get('CSRF_COOKIE', None))\n        self.assertNotEqual(req.META['CSRF_COOKIE'], 64 * '*')\n", "    def test_origin_with_port(self):\n        \"\"\"\n        A POST request with a valid origin that includes a port number is accepted.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'www.example.com:8080'\n        req.META['HTTP_ORIGIN'] = 'https://www.example.com:8080'\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "    def test_get_token_without_existing_csrf_cookie(self):\n        \"\"\"\n        Test the scenario where get_token() is called without an existing CSRF cookie.\n        A new CSRF token should be created and set in request.META.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        token = get_token(req)\n        self.assertIn('CSRF_COOKIE', req.META)\n        self.assertEqual(token, req.META['CSRF_COOKIE'])\n        self.assertTrue(req.META['CSRF_COOKIE_USED'])\n", "def test_process_response_no_csrf_cookie_used(self):\n    \"\"\"\n    If no CSRF_COOKIE_USED is set in request.META, the CSRF cookie should not be set in the response.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    resp = HttpResponse()\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_view(req, post_form_view, (), {})\n    response = mw.process_response(req, resp)\n    self.assertNotIn(settings.CSRF_COOKIE_NAME, response.cookies)\n", "    def test_rotate_token(self):\n        \"\"\"\n        Ensures that the rotate_token function updates the CSRF token correctly.\n        \"\"\"\n        req = self._get_GET_csrf_cookie_request()\n        original_token = get_token(req)\n        rotate_token(req)\n        new_token = req.META['CSRF_COOKIE']\n        self.assertNotEqual(original_token, new_token)\n        self.assertTrue(equivalent_tokens(original_token, new_token))\n"], "sample_322": ["    def test_migrate_all_forwards(self):\n        \"\"\"\n        Test `_migrate_all_forwards` method to ensure all forward migrations are applied correctly.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        state = executor._create_project_state(with_applied_migrations=True)\n        plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n        full_plan = executor.migration_plan(executor.loader.graph.leaf_nodes(), clean_start=True)\n        new_state = executor._migrate_all_forwards(state, plan, full_plan, fake=False, fake_initial=False)\n        # Check final state to ensure migrations were applied\n        self.assertIn(('migrations', 'book'), new_state.models)\n        self.assertIn(('migrations', 'author'), new_state.models)\n        # Ensure tables are created\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_book\")\n", "    def test_apply_migration_rollback_on_failure(self):\n        \"\"\"\n        Test that if a migration fails to apply, the process rolls back and does not leave the DB in an inconsistent state.\n        \"\"\"\n        class FailingMigration(migrations.Migration):\n            atomic = False\n\n                raise RuntimeError(\"Simulated migration failure\")\n\n        executor = MigrationExecutor(connection)\n        # Initial state: no tables present\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n\n        try:\n            # This should raise an error and not apply the migration\n            with self.assertRaisesMessage(RuntimeError, \"Simulated migration failure\"):\n                executor.apply_migration(ProjectState(), FailingMigration(\"0003_fail\", \"migrations\"))\n        finally:\n            # Ensure database is rolled back and no tables were created\n            self.assertTableNotExists(\"migrations_author\")\n            self.assertTableNotExists(\"migrations_book\")\n            # Ensure the migration is not recorded as applied\n            self.assertNotIn(\n                (\"migrations\", \"0003_fail\"),\n                executor.recorder.applied_migrations(),\n            )\n", "    def test_migrate_to_specific_intermediate_target(self):\n        \"\"\"\n        Tests migrating to a specific intermediate target.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Let's look at the plan to migrate to 0001_initial first\n        plan = executor.migration_plan([(\"migrations\", \"0001_initial\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            ],\n        )\n        # Were the tables there before?\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        # Migrate to the first migration\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        # Are the tables there now?\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Now migrate to the second migration\n        plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n            ],\n        )\n        executor.migrate([(\"migrations\", \"0002_second\")])\n        # Are the tables there now?\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_book\")\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Finally, migrate back to the initial state\n        plan = executor.migration_plan([(\"migrations\", None)])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], True),\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n            ],\n        )\n        executor.migrate([(\"migrations\", None)])\n        # Are the tables gone?\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n", "    def test_custom_migration_signals(self):\n        \"\"\"\n        Test that custom signals defined in migrations are properly handled.\n        \"\"\"\n        signals_called = []\n\n            signals_called.append('pre_migrate')\n\n            signals_called.append('post_migrate')\n\n        # Connect custom signals\n        migrations.signals.pre_migrate.connect(pre_migrate)\n        migrations.signals.post_migrate.connect(post_migrate)\n\n        executor = MigrationExecutor(connection)\n\n        try:\n            # Initial state: tables should not exist\n            self.assertTableNotExists(\"migrations_customsignalmodel\")\n\n            # Apply migration that triggers custom signals\n            executor.migrate([(\"migrations\", \"0001_initial\")])\n\n            # Check if tables are created\n            self.assertTableExists(\"migrations_customsignalmodel\")\n\n            # Validate that the signals were called in the correct order\n            self.assertEqual(signals_called, [\"pre_migrate\", \"post_migrate\"])\n        finally:\n            # Disconnect custom signals to avoid side effects in other tests\n            migrations.signals.pre_migrate.disconnect(pre_migrate)\n            migrations.signals.post_migrate.disconnect(post_migrate)\n\n            # Unmigrate everything\n            executor.migrate([(\"migrations\", None)])\n            self.assertTableNotExists(\"migrations_customsignalmodel\")\n", "    def test_apply_migration(self):\n        \"\"\"\n        Test that the apply_migration method correctly applies a migration and records it.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Fake recording of the migration to simulate the apply_migration process\n        executor.record_migration = mock.Mock()\n        # Create a simple migration instance\n        migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n        state = ProjectState()\n\n        # Apply migration\n        new_state = executor.apply_migration(state, migration)\n\n        # Check that the migration was correctly applied and recorded\n        self.assertTrue(executor.record_migration.called)\n        self.assertIn((\"migrations\", \"0001_initial\"), executor.recorder.applied_migrations())\n        self.assertIn(('migrations', 'author'), new_state.models)\n        self.assertIn(('migrations', 'tribble'), new_state.models)\n\n        # Cleanup by removing the applied migration\n        executor.recorder.record_unapplied(\"migrations\", \"0001_initial\")\n        executor.migrate([(\"migrations\", None)], fake=True)\n"], "sample_323": ["    def test_apply_migration_with_fake_initial(self):\n        \"\"\"\n        Tests that applying a migration with fake_initial=True properly detects\n        already applied initial migrations.\n        \"\"\"\n        state = {\"faked\": None}\n\n            state[\"faked\"] = fake\n\n        executor = MigrationExecutor(connection, progress_callback=fake_storer)\n        # Were the tables there before?\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        # Run it with fake_initial=True\n        self.assertEqual(\n            executor.migration_plan([(\"migrations\", \"0001_initial\")]),\n            [(executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False)],\n        )\n        executor.migrate([(\"migrations\", \"0001_initial\")], fake_initial=True)\n        # Are the tables there now?\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_book\")\n        # It should have faked that one\n        self.assertIs(state[\"faked\"], True)\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Migrate back to clean up the database\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n", "    def test_unapply_migration_records(self):\n        \"\"\"\n        Tests that unapplying a migration correctly records the unapplied state.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Apply the initial migration first\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        self.assertTableExists(\"migrations_author\")\n\n        # Ensure the migration is recorded as applied\n        self.assertIn(\n            (\"migrations\", \"0001_initial\"),\n            executor.recorder.applied_migrations(),\n        )\n\n        # Unapply the migration\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n\n        # Ensure the migration is recorded as unapplied\n        self.assertNotIn(\n            (\"migrations\", \"0001_initial\"),\n            executor.recorder.applied_migrations(),\n        )\n", "    def test_record_migration_called(self):\n        \"\"\"\n        Tests that `record_migration` method is called during migration.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Mock the record_migration method to track its calls\n        with mock.patch.object(executor, 'record_migration', wraps=executor.record_migration) as mock_record_migration:\n            # Run a simple migration\n            executor.migrate([(\"migrations\", \"0001_initial\")])\n            # Check if record_migration was called\n            self.assertTrue(mock_record_migration.called)\n            self.assertEqual(mock_record_migration.call_count, 1)\n            self.assertEqual(\n                mock_record_migration.call_args[0][0],\n                executor.loader.graph.nodes[\"migrations\", \"0001_initial\"]\n            )\n            # Undo the migration\n            executor.migrate([(\"migrations\", None)])\n            # Check if record_migration was called again for unapply\n            self.assertEqual(mock_record_migration.call_count, 2)\n            self.assertEqual(\n                mock_record_migration.call_args[0][0],\n                executor.loader.graph.nodes[\"migrations\", \"0001_initial\"]\n            )\n", "    def test_fallback_on_failure(self):\n        \"\"\"\n        Tests that the executor falls back gracefully when a migration fails and\n        ensures the database remains in a consistent state.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Apply initial migration\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_tribble\")\n        \n        # Simulate a migration failure\n        with mock.patch('django.db.migrations.executor.MigrationExecutor.apply_migration') as apply_migration:\n            apply_migration.side_effect = RuntimeError('Simulated migration failure.')\n            with self.assertRaisesMessage(RuntimeError, 'Simulated migration failure.'):\n                executor.migrate([(\"migrations\", \"0002_second\")])\n        \n        # Ensure the migration did not get recorded as applied\n        self.assertNotIn(\n            (\"migrations\", \"0002_second\"),\n            executor.recorder.applied_migrations()\n        )\n        \n        # Ensure the previous state is still intact\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_tribble\")\n        \n        # Clean up\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n", "    def test_migrate_with_empty_targets(self):\n        \"\"\"\n        Test migrating with an empty list of targets.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Initial state with no migrations applied\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        # Migrate with empty targets\n        state = executor.migrate([])\n        # Ensure no migrations were applied\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        self.assertEqual(state.models, {})\n"], "sample_324": ["    def test_reject_request_exception(self):\n        \"\"\"\n        Tests that the RejectRequest exception sets the reason correctly.\n        \"\"\"\n        reason = \"This is a test reason.\"\n        exception = RejectRequest(reason)\n        self.assertEqual(exception.reason, reason)\n", "    def test_sanitize_token_invalid_characters(self):\n        \"\"\"\n        Test that _sanitize_token raises InvalidTokenFormat exception when\n        the token contains invalid characters.\n        \"\"\"\n        with self.assertRaises(InvalidTokenFormat) as context:\n            _sanitize_token('invalid_token!')\n        self.assertEqual(context.exception.reason, REASON_INVALID_CHARACTERS)\n", "    def test_rotate_token(self):\n        \"\"\"\n        Test that rotate_token properly updates the CSRF token in the request and sets the\n        csrf_cookie_needs_reset attribute to True.\n        \"\"\"\n        req = self._get_GET_csrf_cookie_request()\n        original_token = req.COOKIES[settings.CSRF_COOKIE_NAME]\n        rotate_token(req)\n        new_token = req.META['CSRF_COOKIE']\n        self.assertNotEqual(original_token, new_token)\n        self.assertTrue(req.csrf_cookie_needs_reset)\n", "def test_rotate_token_resets_csrf_cookie(self):\n    \"\"\"\n    The rotate_token function should reset the CSRF token in use for a request.\n    \"\"\"\n    req = self._get_GET_csrf_cookie_request()\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    old_token = req.META['CSRF_COOKIE']\n    rotate_token(req)\n    new_token = req.META['CSRF_COOKIE']\n    self.assertNotEqual(old_token, new_token, \"CSRF token was not reset by rotate_token()\")\n    self.assertTrue(req.csrf_cookie_needs_reset, \"csrf_cookie_needs_reset was not set to True\")\n", "    def test_csrf_trusted_origins_hosts_property(self):\n        \"\"\"\n        Test the `csrf_trusted_origins_hosts` property to ensure it correctly\n        parses and returns the host portion of the trusted origins.\n        \"\"\"\n        with self.settings(CSRF_TRUSTED_ORIGINS=[\n            'https://trusted.com', 'http://sub.trusted.com',\n            'https://*.wildcard.com', 'http://sub.wildcard.com'\n        ]):\n            mw = CsrfViewMiddleware(post_form_view)\n            expected_hosts = ['trusted.com', 'sub.trusted.com', '.wildcard.com', 'sub.wildcard.com']\n            self.assertEqual(mw.csrf_trusted_origins_hosts, expected_hosts)\n"], "sample_325": ["    def test_boundfield_css_classes_with_errors(self):\n        \"\"\"\n        Test that css_classes method of BoundField correctly includes error\n        class when the field has errors.\n        \"\"\"\n        class SampleForm(Form):\n            name = CharField()\n\n        form = SampleForm(data={'name': ''})  # Invalid data, name field required\n        field = form['name']\n        self.assertEqual(field.css_classes(), form.error_css_class)\n        self.assertEqual(field.css_classes(extra_classes='custom'), f'custom {form.error_css_class}')\n", "def test_boundfield_css_classes_with_errors(self):\n    class CustomForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n\n    form = CustomForm(data={'field1': '', 'field2': ''})\n    form.is_valid()  # Trigger validation to populate errors\n\n    field1 = form['field1']\n    field2 = form['field2']\n    \n    self.assertIn('error', field1.css_classes())\n    self.assertIn('error', field2.css_classes())\n\n    self.assertEqual(field1.css_classes(extra_classes='extra'), 'extra error')\n    self.assertEqual(field2.css_classes(extra_classes='extra'), 'extra error')\n\n    # Test without extra_classes\n    self.assertEqual(field1.css_classes(), 'error')\n    self.assertEqual(field2.css_classes(), 'error')\n", "    def test_boundfield_widget_type_with_custom_widget(self):\n        class CustomWidget(TextInput):\n            pass\n\n        class CustomForm(Form):\n            custom_field = CharField(widget=CustomWidget)\n\n        form = CustomForm()\n        self.assertEqual(form['custom_field'].widget_type, 'custom')\n", "    def test_boundfield_as_textarea(self):\n        class CustomForm(Form):\n            description = CharField(widget=Textarea)\n\n        form = CustomForm({'description': 'This is a test.'})\n        self.assertHTMLEqual(\n            form['description'].as_textarea(),\n            '<textarea name=\"description\" rows=\"10\" cols=\"40\" required>This is a test.</textarea>'\n        )\n", "    def test_boundfield_initial_property(self):\n        \"\"\"\n        Test that the initial property of BoundField correctly returns\n        the initial value for the field.\n        \"\"\"\n        class TestForm(Form):\n            name = CharField(initial='John Doe')\n            age = IntegerField(initial=30)\n\n        form = TestForm()\n        self.assertEqual(form['name'].initial, 'John Doe')\n        self.assertEqual(form['age'].initial, 30)\n\n        form = TestForm(initial={'name': 'Jane Doe', 'age': 25})\n        self.assertEqual(form['name'].initial, 'Jane Doe')\n        self.assertEqual(form['age'].initial, 25)\n\n        form = TestForm(data={'name': 'Alice', 'age': 40})\n        self.assertEqual(form['name'].initial, 'John Doe')\n        self.assertEqual(form['age'].initial, 30)\n"], "sample_326": ["    def test_avoid_wrapping(self):\n        tests = (\n            ('Avoid wrapping this text', 'Avoid\\xa0wrapping\\xa0this\\xa0text'),\n            ('Multiple  spaces here', 'Multiple\\xa0\\xa0spaces\\xa0here'),\n            ('\\tTabbed text should\\tnot be affected', '\\tTabbed\\xa0text\\xa0should\\tnot\\xa0be\\xa0affected'),\n            ('Trailing space ', 'Trailing\\xa0space\\xa0'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n            (\"No wrapping here\", \"No\\xa0wrapping\\xa0here\"),\n            (\"Avoid wrapping\", \"Avoid\\xa0wrapping\"),\n            (lazystr(\"Lazy string test\"), \"Lazy\\xa0string\\xa0test\"),\n            (\"Already\\xa0non-breaking\", \"Already\\xa0non-breaking\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"hello world\", \"hello\\xa0world\"),\n            (\"Django is awesome\", \"Django\\xa0is\\xa0awesome\"),\n            (lazystr(\"lazy evaluation\"), \"lazy\\xa0evaluation\"),\n            (mark_safe(\"safe content\"), \"safe\\xa0content\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n", "    def test_avoid_wrapping(self):\n        tests = (\n            (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n            (\"Hello World!\", \"Hello\\xa0World!\"),\n            (\"No wrapping here\", \"No\\xa0wrapping\\xa0here\"),\n            (lazystr(\"Lazy string test\"), \"Lazy\\xa0string\\xa0test\"),\n        )\n        for value, expected in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), expected)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"Keep these words together\", \"Keep\\xa0these\\xa0words\\xa0together\"),\n            (\"Avoid line breaks here\", \"Avoid\\xa0line\\xa0breaks\\xa0here\"),\n            (\"Preserve formatting\", \"Preserve\\xa0formatting\"),\n            (\"SingleWord\", \"SingleWord\"),\n            (\"Multiple   spaces\", \"Multiple\\xa0\\xa0\\xa0spaces\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n"], "sample_327": ["def test_invalid_json_string(self):\n    field = JSONField()\n    invalid_json_strings = [\n        'Just a plain string',\n        '{\"missing\": \"value\",}',\n        '[\"unclosed array\"',\n        '{\"unclosed_dict\": \"value\"'\n    ]\n    for json_string in invalid_json_strings:\n        with self.subTest(json_string=json_string):\n            with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n                field.clean(json_string)\n", "    def test_invalid_json_input(self):\n        class JSONForm(Form):\n            json_field = JSONField()\n\n        invalid_json_string = '{invalid: json}'\n        form = JSONForm({'json_field': invalid_json_string})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['json_field'], ['Enter a valid JSON.'])\n", "    def test_bound_data(self):\n        field = JSONField()\n        self.assertEqual(field.bound_data('{\"a\": \"b\"}', '{\"a\": \"c\"}'), {\"a\": \"b\"})\n        self.assertEqual(field.bound_data(None, {\"a\": \"c\"}), {\"a\": \"c\"})\n        self.assertEqual(field.bound_data(None, None), None)\n        self.assertEqual(field.bound_data('invalid json', '{\"a\": \"c\"}'), 'invalid json')\n", "    def test_invalid_json_input(self):\n        field = JSONField()\n        invalid_input = '{\"key\": \"value\",}'\n        result = field.bound_data(invalid_input, initial=None)\n        self.assertIsInstance(result, InvalidJSONInput)\n        self.assertEqual(result, invalid_input)\n", "    def test_invalid_json_input(self):\n        field = JSONField()\n        invalid_json = '{\"a\": \"b\" \"c\": \"d\"}'\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean(invalid_json)\n"], "sample_328": ["    def setUp(self):\n        self.note = Note.objects.create(note='initial note', misc='initial misc')\n", "    def test_bulk_update_partial_update(self):\n        \"\"\"\n        Test that bulk_update properly updates only the specified fields.\n        \"\"\"\n        Note.objects.update(note='original', misc='original')\n        notes = list(Note.objects.all())\n        for note in notes:\n            note.note = 'updated'\n        Note.objects.bulk_update(notes, ['note'])\n        self.assertCountEqual(Note.objects.values_list('note', flat=True), ['updated'] * len(notes))\n        self.assertCountEqual(Note.objects.values_list('misc', flat=True), ['original'] * len(notes))\n", "    def test_bulk_update_with_prefetch_related(self):\n        member = Member.objects.create(name=\"member1\")\n        note1 = Note.objects.create(note=\"note1\", misc=\"misc1\", member=member)\n        note2 = Note.objects.create(note=\"note2\", misc=\"misc2\", member=member)\n        \n        notes = [note1, note2]\n        for note in notes:\n            note.note = \"updated-\" + note.note\n        \n        with self.assertNumQueries(2):  # One query for bulk update and one for prefetch_related\n            Note.objects.bulk_update(notes, ['note'])\n            notes = Note.objects.prefetch_related('member').all()\n        \n        self.assertEqual(notes[0].member, member)\n        self.assertEqual(notes[1].member, member)\n        self.assertEqual(Note.objects.get(pk=note1.pk).note, \"updated-note1\")\n        self.assertEqual(Note.objects.get(pk=note2.pk).note, \"updated-note2\")\n", "    def test_bulk_update_with_invalid_field_type(self):\n        with self.assertRaisesMessage(ValueError, 'bulk_update() received non-expression(s): invalid_field.'):\n            Note.objects.bulk_update(self.notes, ['invalid_field'])\n", "def test_bulk_update_json_field(self):\n    json_objects = [\n        JSONFieldNullable.objects.create(data={\"key\": \"value-%s\" % i})\n        for i in range(10)\n    ]\n    for obj in json_objects:\n        obj.data = {\"key\": \"updated-value-%s\" % obj.pk}\n    JSONFieldNullable.objects.bulk_update(json_objects, ['data'])\n    self.assertCountEqual(\n        JSONFieldNullable.objects.values_list('data', flat=True),\n        [obj.data for obj in json_objects]\n    )\n"], "sample_329": ["    def test_serialize_custom_class_instance(self):\n        instance = DeconstructibleInstances()\n        self.assertSerializedResultEqual(\n            instance,\n            (\"migrations.test_writer.DeconstructibleInstances()\", {'import migrations.test_writer'}),\n        )\n        self.assertSerializedEqual(instance)\n", "    def test_serialize_custom_serializer(self):\n        class CustomType:\n                self.data = data\n\n        class CustomTypeSerializer(BaseSerializer):\n                return \"CustomType(%r)\" % self.value.data, set()\n\n        # Register the custom serializer\n        Serializer.register(CustomType, CustomTypeSerializer)\n\n        # Test serialization\n        custom_value = CustomType(\"test data\")\n        serialized_value, imports = serializer_factory(custom_value).serialize()\n        self.assertEqual(serialized_value, \"CustomType('test data')\")\n        self.assertEqual(imports, set())\n\n        # Unregister the custom serializer\n        Serializer.unregister(CustomType)\n\n        # Ensure ValueError is raised after unregistering\n        with self.assertRaises(ValueError):\n            serializer_factory(custom_value)\n", "    def test_serialize_deconstructable(self):\n        value = DeconstructibleInstances()\n        result = self.serialize_round_trip(value)\n        self.assertEqual(result.__class__, value.__class__)\n        self.assertEqual(result.deconstruct(), value.deconstruct())\n", "    def test_serialize_deconstructable(self):\n        class CustomDeconstructible:\n                self.arg1 = arg1\n                self.arg2 = arg2\n            \n                return (\n                    'migrations.test_writer.CustomDeconstructible',\n                    [self.arg1, self.arg2],\n                    {}\n                )\n\n        value = CustomDeconstructible(1, 2)\n        self.assertSerializedResultEqual(\n            value,\n            (\n                \"migrations.test_writer.CustomDeconstructible(1, 2)\",\n                {\"import migrations.test_writer\"},\n            )\n        )\n        self.assertSerializedEqual(value)\n", "    def test_serialize_deconstructable(self):\n        @deconstructible\n        class CustomDeconstructible:\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n                return (\n                    'migrations.test_writer.CustomDeconstructible',\n                    [self.arg1, self.arg2],\n                    {}\n                )\n\n        value = CustomDeconstructible('test_arg1', 'test_arg2')\n        serialized_value = self.serialize_round_trip(value)\n        self.assertIsInstance(serialized_value, CustomDeconstructible)\n        self.assertEqual(serialized_value.arg1, 'test_arg1')\n        self.assertEqual(serialized_value.arg2, 'test_arg2')\n"], "sample_330": ["    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2023-10-01'), datetime.date(2023, 10, 1))\n        self.assertIsNone(typecast_date(''))\n        self.assertIsNone(typecast_date(None))\n", "    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2023-10-05'), datetime.date(2023, 10, 5))\n        self.assertEqual(typecast_date(''), None)\n", "    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2023-10-01'), datetime.date(2023, 10, 1))\n        self.assertEqual(typecast_date(''), None)\n        self.assertEqual(typecast_date(None), None)\n", "    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2023-10-06'), datetime.date(2023, 10, 6))\n        self.assertIsNone(typecast_date(''))\n", "    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2023-10-01'), datetime.date(2023, 10, 1))\n        self.assertIsNone(typecast_date(''))\n"], "sample_331": ["    def test_parse_invalid_duration(self):\n        test_values = (\n            'P',  # Empty ISO 8601 duration\n            'P1Y',  # Unsupported years in ISO 8601\n            'P1M',  # Unsupported months in ISO 8601\n            '10 days 10:20',  # Mixed invalid format\n            '10.5:30',  # Invalid floating point hours\n            '10:60:30',  # Invalid minutes\n            '10:30:60',  # Invalid seconds\n            '10:30:30.1000000',  # Invalid microseconds\n            '10:30:30,-1000000',  # Invalid microseconds with comma\n            'invalid',  # Completely invalid string\n        )\n        for value in test_values:\n            with self.subTest(value=value):\n                self.assertIsNone(parse_duration(value))\n", "    def test_mixed_invalid_inputs(self):\n        test_values = (\n            '2022-13-01',      # Invalid month\n            '2022-00-01',      # Invalid month\n            '2022-01-32',      # Invalid day\n            '2022-02-29',      # Invalid day (not a leap year)\n            '25:00:00',        # Invalid hour\n            '12:60:00',        # Invalid minute\n            '12:00:60',        # Invalid second\n            '2022-01-01T25:00:00',  # Invalid hour in datetime\n            '2022-01-01T12:60:00',  # Invalid minute in datetime\n            '2022-01-01T12:00:60',  # Invalid second in datetime\n            'P1Y1M',           # Unsupported ISO 8601 format (year and month not supported)\n            '1 year 2 days',   # Unsupported format with year\n        )\n        for value in test_values:\n            with self.subTest(value=value):\n                self.assertIsNone(parse_date(value))\n                self.assertIsNone(parse_time(value))\n                self.assertIsNone(parse_datetime(value))\n                self.assertIsNone(parse_duration(value))\n", "    def test_parse_invalid_duration(self):\n        invalid_durations = [\n            'random text',\n            'P1Y2M3DT4H5M6S',  # ISO 8601 format not supported\n            'T10:15:30',  # ISO 8601 time-only format not supported\n            '1 day 25:00:00',  # Invalid hour\n            '1 day 10:60:00',  # Invalid minute\n            '1 day 10:00:60',  # Invalid second\n            '1 day 10:00:00.0000001',  # Invalid microsecond\n            'P1Y2M3DT4H5M6S',  # Invalid ISO 8601 format with years and months\n            '10.1000:15:30',  # Invalid day format\n        ]\n        for source in invalid_durations:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_invalid_inputs(self):\n        invalid_inputs = [\n            ('invalid-date', parse_date),\n            ('2012-13-01', parse_date),  # invalid month\n            ('2012-00-01', parse_date),  # invalid month\n            ('2012-04-31', parse_date),  # invalid day\n            ('24:00:00', parse_time),  # invalid hour\n            ('12:60:00', parse_time),  # invalid minute\n            ('12:00:60', parse_time),  # invalid second\n            ('2012-04-23T25:00:00', parse_datetime),  # invalid hour\n            ('2012-04-23T09:60:00', parse_datetime),  # invalid minute\n            ('2012-04-23T09:15:60', parse_datetime),  # invalid second\n            ('P1Y2M', parse_duration),  # unsupported ISO 8601 duration\n            ('P1W', parse_duration),  # unsupported ISO 8601 week duration\n            ('1 year 2 months', parse_duration),  # unsupported format\n        ]\n        for source, parse_func in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_func(source))\n", "    def test_parse_duration_iso8601(self):\n        test_values = (\n            ('P3D', timedelta(days=3)),\n            ('P1DT12H', timedelta(days=1, hours=12)),\n            ('PT4H', timedelta(hours=4)),\n            ('PT36M', timedelta(minutes=36)),\n            ('PT36.5S', timedelta(seconds=36.5)),\n            ('-P3D', timedelta(days=-3)),\n            ('-P1DT12H', timedelta(days=-1, hours=-12)),\n            ('-PT4H', timedelta(hours=-4)),\n            ('-PT36M', timedelta(minutes=-36)),\n            ('-PT36.5S', timedelta(seconds=-36.5)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_332": ["    def test_management_form_initial_data(self):\n        \"\"\"\n        Ensure that the ManagementForm initializes correctly with initial data.\n        \"\"\"\n        initial_data = {\n            'choices-TOTAL_FORMS': '3',\n            'choices-INITIAL_FORMS': '2',\n            'choices-MIN_NUM_FORMS': '1',\n            'choices-MAX_NUM_FORMS': '4',\n        }\n        formset = self.make_choiceformset(initial=[{'choice': 'Option1', 'votes': 10}, {'choice': 'Option2', 'votes': 20}], **initial_data)\n        management_form = formset.management_form\n        self.assertEqual(management_form.initial[TOTAL_FORM_COUNT], 3)\n        self.assertEqual(management_form.initial[INITIAL_FORM_COUNT], 2)\n        self.assertEqual(management_form.initial[MIN_NUM_FORM_COUNT], 1)\n        self.assertEqual(management_form.initial[MAX_NUM_FORM_COUNT], 4)\n", "    def test_total_form_count_dos_protection(self):\n        \"\"\"\n        Test to ensure that total_form_count() method provides protection against DoS attacks\n        by limiting the number of forms instantiated.\n        \"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '2000',  # a large number of forms to test DoS protection\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '2000',\n        }\n        ChoiceFormSet = formset_factory(Choice, max_num=1000, validate_max=True)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertEqual(formset.total_form_count(), 1000)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['Please submit at most 1000 forms.'])\n", "    def test_management_form_initial_data(self):\n        \"\"\"\n        Management form should return initial data correctly.\n        \"\"\"\n        class TestForm(Form):\n            test_field = CharField()\n\n        TestFormSet = formset_factory(TestForm)\n        initial_data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '1',\n            'form-0-test_field': 'Initial value',\n            'form-1-test_field': '',\n        }\n        formset = TestFormSet(data=initial_data, initial=[{'test_field': 'Initial value'}])\n        self.assertTrue(formset.management_form.is_valid())\n        self.assertEqual(formset.management_form.initial[TOTAL_FORM_COUNT], 2)\n        self.assertEqual(formset.management_form.initial[INITIAL_FORM_COUNT], 1)\n", "    def test_cleaned_data_property(self):\n        \"\"\"\n        Ensure that the cleaned_data property raises an AttributeError if the formset is not valid.\n        \"\"\"\n        formset = self.make_choiceformset([('Calexico', '')])\n        self.assertFalse(formset.is_valid())\n        with self.assertRaises(AttributeError):\n            _ = formset.cleaned_data\n        # Make the formset valid and check cleaned_data property.\n        formset = self.make_choiceformset([('Calexico', '100')])\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [{'votes': 100, 'choice': 'Calexico'}])\n", "    def test_management_form_field_types(self):\n        \"\"\"\n        Validate that the management form contains the correct types of fields.\n        \"\"\"\n        formset = self.make_choiceformset()\n        management_form = formset.management_form\n        self.assertIsInstance(management_form.fields[TOTAL_FORM_COUNT], IntegerField)\n        self.assertIsInstance(management_form.fields[INITIAL_FORM_COUNT], IntegerField)\n        self.assertIsInstance(management_form.fields[MIN_NUM_FORM_COUNT], IntegerField)\n        self.assertIsInstance(management_form.fields[MAX_NUM_FORM_COUNT], IntegerField)\n        self.assertIsInstance(management_form.fields[TOTAL_FORM_COUNT].widget, HiddenInput)\n        self.assertIsInstance(management_form.fields[INITIAL_FORM_COUNT].widget, HiddenInput)\n        self.assertIsInstance(management_form.fields[MIN_NUM_FORM_COUNT].widget, HiddenInput)\n        self.assertIsInstance(management_form.fields[MAX_NUM_FORM_COUNT].widget, HiddenInput)\n"], "sample_333": ["def test_add_error_to_non_existent_field(self):\n    \"\"\"\n    Adding an error to a non-existent field should raise a ValueError.\n    \"\"\"\n    class TestForm(Form):\n        name = CharField()\n\n    form = TestForm({'name': 'Test'})\n    with self.assertRaisesMessage(ValueError, \"'TestForm' has no field named 'non_existent_field'.\"):\n        form.add_error('non_existent_field', 'This field does not exist.')\n", "def test_order_fields(self):\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field4 = CharField()\n\n    form = TestForm()\n    form.order_fields(['field4', 'field3', 'field1'])\n\n    self.assertEqual(list(form.fields.keys()), ['field4', 'field3', 'field1', 'field2'])\n\n    form.order_fields(['field2', 'field4'])\n    self.assertEqual(list(form.fields.keys()), ['field2', 'field4', 'field1', 'field3'])\n", "    def test_form_order_fields(self):\n        class OrderedForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n            field5 = CharField()\n\n        form = OrderedForm()\n        # Default order\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3', 'field4', 'field5'])\n\n        form.order_fields(['field5', 'field4', 'field3', 'field2', 'field1'])\n        self.assertEqual(list(form.fields), ['field5', 'field4', 'field3', 'field2', 'field1'])\n\n        # Partial order, should preserve the rest in original order\n        form.order_fields(['field3', 'field1'])\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field2', 'field4', 'field5'])\n\n        # Order with unknown field should ignore the unknown field\n        form.order_fields(['field3', 'unknown_field', 'field1'])\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field2', 'field4', 'field5'])\n", "    def test_form_with_empty_permitted_false(self):\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = IntegerField(required=False)\n\n        # Empty permitted is False, but form is empty, so it's invalid.\n        form = TestForm(data={}, empty_permitted=False)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['field1'], ['This field is required.'])\n\n        # Empty permitted is False, but form has required field filled in.\n        form = TestForm(data={'field1': 'test'}, empty_permitted=False)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['field1'], 'test')\n        self.assertEqual(form.cleaned_data['field2'], None)\n\n        # Empty permitted is False, but form has optional field filled in.\n        form = TestForm(data={'field2': 123}, empty_permitted=False)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['field1'], ['This field is required.'])\n        self.assertEqual(form.cleaned_data, {'field2': 123})\n", "    def test_form_with_custom_initial_prefix(self):\n        # Test add_initial_prefix method\n        class CustomForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n\n        form = CustomForm(initial={'field1': 'initial_value'}, prefix='custom')\n        self.assertEqual(form.add_initial_prefix('field1'), 'initial-custom-field1')\n        self.assertEqual(form.add_initial_prefix('field2'), 'initial-custom-field2')\n\n        # Test with no prefix\n        form_no_prefix = CustomForm(initial={'field1': 'initial_value'})\n        self.assertEqual(form_no_prefix.add_initial_prefix('field1'), 'initial-field1')\n        self.assertEqual(form_no_prefix.add_initial_prefix('field2'), 'initial-field2')\n"], "sample_334": ["def test_order_fields_with_unknown_field(self):\n    # Test that order_fields ignores unknown fields in field_order.\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n\n    form = TestForm()\n    form.order_fields(['field3', 'unknown_field', 'field1'])\n    self.assertEqual(list(form.fields), ['field3', 'field1', 'field2'])\n\n    # Test that order_fields keeps original order if field_order is None.\n    form = TestForm()\n    form.order_fields(None)\n    self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n", "def test_order_fields_with_partial_order(self):\n    class PartialOrderForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field4 = CharField()\n        field5 = CharField()\n\n    form = PartialOrderForm(field_order=['field3', 'field1', 'field4'])\n    expected_order = ['field3', 'field1', 'field4', 'field2', 'field5']\n    self.assertEqual(list(form.fields), expected_order)\n", "def test_dynamic_initial_callable(self):\n    class ProductForm(Form):\n        name = CharField(max_length=100)\n        price = FloatField()\n\n        return 'Unnamed Product'\n\n        return 9.99\n\n    p = ProductForm(initial={'name': default_name, 'price': default_price}, auto_id=False)\n    self.assertHTMLEqual(\n        p.as_ul(),\n        \"\"\"<li>Name: <input type=\"text\" name=\"name\" value=\"Unnamed Product\" maxlength=\"100\" required></li>", "def test_order_fields(self):\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field4 = CharField()\n\n    # Default order\n    form = TestForm()\n    self.assertEqual(list(form.fields.keys()), ['field1', 'field2', 'field3', 'field4'])\n\n    # Custom order\n    form = TestForm(field_order=['field3', 'field1', 'field4', 'field2'])\n    self.assertEqual(list(form.fields.keys()), ['field3', 'field1', 'field4', 'field2'])\n\n    # Invalid field in order should be ignored\n    form = TestForm(field_order=['field3', 'field1', 'field5', 'field2'])\n    self.assertEqual(list(form.fields.keys()), ['field3', 'field1', 'field2', 'field4'])\n\n    # Empty order should retain original order\n    form = TestForm(field_order=[])\n    self.assertEqual(list(form.fields.keys()), ['field1', 'field2', 'field3', 'field4'])\n", "    def test_order_fields_method(self):\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n\n        form = TestForm()\n        # Default order\n        self.assertEqual(list(form.fields.keys()), ['field1', 'field2', 'field3', 'field4'])\n\n        # Changing order\n        form.order_fields(['field4', 'field2'])\n        self.assertEqual(list(form.fields.keys()), ['field4', 'field2', 'field1', 'field3'])\n\n        # Including a non-existent field (should be ignored)\n        form.order_fields(['field3', 'field2', 'nonexistent'])\n        self.assertEqual(list(form.fields.keys()), ['field3', 'field2', 'field1', 'field4'])\n\n        # No order specified should retain original order\n        form.order_fields(None)\n        self.assertEqual(list(form.fields.keys()), ['field3', 'field2', 'field1', 'field4'])\n"], "sample_335": ["    def test_decimalfield_invalid_json_input(self):\n        f = DecimalField(max_digits=4, decimal_places=2)\n        invalid_json_inputs = [\n            '{\"invalid\": \"json}',  # Missing closing quote and brace\n            '{\"key\": \"value\",}',   # Trailing comma\n            '[1, 2, 3,]',          # Trailing comma in array\n            '{\"key\": value}',      # Unquoted value\n            '{key: \"value\"}',      # Unquoted key\n        ]\n        for input in invalid_json_inputs:\n            with self.subTest(input=input), self.assertRaisesMessage(ValidationError, \"'Enter a valid JSON.'\"):\n                f.clean(input)\n", "    def test_charfield(self):\n        f = CharField(max_length=20, min_length=5, strip=True)\n        self.assertWidgetRendersTo(f, '<input id=\"id_f\" maxlength=\"20\" minlength=\"5\" type=\"text\" name=\"f\" required>')\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean('')\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(None)\n        self.assertEqual(f.clean('  hello world  '), 'hello world')\n        self.assertEqual(f.clean('  hello  '), 'hello')\n        with self.assertRaisesMessage(ValidationError, \"'Ensure this value has at least 5 characters (it has 3).\"):\n            f.clean('abc')\n        with self.assertRaisesMessage(ValidationError, \"'Ensure this value has at most 20 characters (it has 21).\"):\n            f.clean('a' * 21)\n        self.assertEqual(f.clean('a' * 20), 'a' * 20)\n        self.assertEqual(f.clean('a' * 5), 'a' * 5)\n        self.assertEqual(f.max_length, 20)\n        self.assertEqual(f.min_length, 5)\n        self.assertTrue(f.strip)\n", "def test_floatfield_validation(self):\n    f = FloatField(max_value=100.5, min_value=10.5)\n    self.assertWidgetRendersTo(\n        f,\n        '<input step=\"any\" name=\"f\" min=\"10.5\" max=\"100.5\" type=\"number\" id=\"id_f\" required>',\n    )\n    with self.assertRaisesMessage(ValidationError, \"'Ensure this value is less than or equal to 100.5.'\"):\n        f.clean('101.5')\n    with self.assertRaisesMessage(ValidationError, \"'Ensure this value is greater than or equal to 10.5.'\"):\n        f.clean('9.5')\n    self.assertEqual(f.clean('50.5'), float(50.5))\n    self.assertEqual(f.clean('10.5'), float(10.5))\n    self.assertEqual(f.clean('100.5'), float(100.5))\n    self.assertEqual(f.clean('0.1e3'), float(100.0))\n    self.assertEqual(f.clean('0.1e-1'), float(0.01))\n    self.assertEqual(f.clean('5e+1'), float(50.0))\n    self.assertEqual(f.clean('0.0001e4'), float(1.0))\n    self.assertEqual(f.clean('0.1234e2'), float(12.34))\n    self.assertEqual(f.clean('-10.5'), float(-10.5))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean('abc')\n", "    def test_floatfield_1(self):\n        f = FloatField()\n        self.assertWidgetRendersTo(f, '<input id=\"id_f\" step=\"any\" type=\"number\" name=\"f\" required>')\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean('')\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(None)\n        self.assertEqual(f.clean('1.23'), 1.23)\n        self.assertIsInstance(f.clean('1.23'), float)\n        self.assertEqual(f.clean('23.45'), 23.45)\n        self.assertEqual(f.clean('3.14'), 3.14)\n        self.assertEqual(f.clean(3.14), 3.14)\n        self.assertEqual(f.clean('1.0 '), 1.0)\n        self.assertEqual(f.clean(' 1.0'), 1.0)\n        self.assertEqual(f.clean(' 1.0 '), 1.0)\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean('abc')\n        self.assertEqual(f.clean('-12.34'), -12.34)\n        self.assertEqual(f.clean('-.12'), -0.12)\n        self.assertEqual(f.clean('-00.12'), -0.12)\n        self.assertEqual(f.clean('-000.12'), -0.12)\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean('-000.abc')\n", "    def test_decimalfield_invalid_decimal(self):\n        f = DecimalField(max_digits=5, decimal_places=2)\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean('abc')\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean('123.abc')\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean('12,34')\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean('12.34.56')\n"], "sample_336": ["    def test_route_to_regex(self):\n        test_routes = [\n            ('foo/<int:pk>/', '^foo/(?P<pk>[0-9]+)$'),\n            ('bar/<slug:slug>/', '^bar/(?P<slug>[-a-zA-Z0-9_]+)$'),\n            ('baz/<uuid:uuid>/', '^baz/(?P<uuid>[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})$'),\n            ('/simple/', '^/simple/$'),\n            ('/<str:word>/', '^/(?P<word>[^/]+)$')\n        ]\n        for route, expected_regex in test_routes:\n            with self.subTest(route=route):\n                regex, converters = _route_to_regex(route, is_endpoint=True)\n                self.assertEqual(regex, expected_regex)\n", "    def test_urlpattern_repr(self):\n        \"\"\"\n        Test the __repr__ method of URLPattern to ensure it outputs correctly\n        formatted strings.\n        \"\"\"\n        pattern = URLPattern(RegexPattern(r'^test/$'), views.empty_view, name='test-view')\n        expected_repr = \"<URLPattern 'test-view'>\"\n        self.assertEqual(repr(pattern), expected_repr)\n", "    def test_pattern_starts_with_slash(self):\n        pattern = RegexPattern(r'^/test/', name='test-pattern')\n        warnings = pattern._check_pattern_startswith_slash()\n        self.assertEqual(len(warnings), 1)\n        self.assertEqual(warnings[0].id, 'urls.W002')\n        self.assertIn(\"Your URL pattern 'test-pattern'\", warnings[0].msg)\n", "    def test_pattern_startswith_slash_warning(self):\n        pattern_with_slash = RegexPattern('^/test')\n        warnings = pattern_with_slash.check()\n        self.assertEqual(len(warnings), 1)\n        self.assertEqual(warnings[0].id, 'urls.W002')\n", "    def test_compiles_regex_for_given_language(self):\n        class MockPattern:\n                self._regex = regex\n                self._regex_dict = {}\n            regex = LocaleRegexDescriptor('_regex')\n            \n                return re.compile(regex)\n\n        pattern = MockPattern(r'\\bword\\b')\n        self.assertEqual(pattern.regex.pattern, r'\\bword\\b')\n        self.assertIn(get_language(), pattern._regex_dict)\n        self.assertEqual(pattern._regex_dict[get_language()].pattern, r'\\bword\\b')\n"], "sample_337": ["    def test_invalid_token_format_raises_reject_request(self):\n        \"\"\"\n        If an invalid token format is provided, the middleware raises a\n        RejectRequest exception with the appropriate reason.\n        \"\"\"\n        req = self._get_POST_csrf_cookie_request(post_token='invalid-token')\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            resp = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(resp.status_code, 403)\n        self.assertEqual(cm.records[0].getMessage(), 'Forbidden (CSRF token from POST has invalid characters.): ')\n", "    def test_sanitized_token(self):\n        \"\"\"\n        Test _sanitize_token function to ensure it raises InvalidTokenFormat\n        for tokens with incorrect length or invalid characters, and correctly\n        masks secrets of CSRF_SECRET_LENGTH.\n        \"\"\"\n        cases = [\n            (16 * 'a', REASON_INCORRECT_LENGTH),\n            (64 * '*', REASON_INVALID_CHARACTERS),\n            (32 * 'a', None),  # valid unmasked secret\n            (64 * 'a', None),  # valid masked token\n        ]\n        for token, expected_error in cases:\n            with self.subTest(token=token):\n                if expected_error:\n                    with self.assertRaises(InvalidTokenFormat) as cm:\n                        _sanitize_token(token)\n                    self.assertEqual(cm.exception.reason, expected_error)\n                else:\n                    sanitized_token = _sanitize_token(token)\n                    self.assertMaskedSecretCorrect(sanitized_token, token[:CSRF_SECRET_LENGTH])\n", "def test_origin_protocol_mismatch(self):\n    \"\"\"\n    A POST request with a mismatched protocol between Origin and Referer is rejected.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://www.example.com'\n    req.META['HTTP_REFERER'] = 'http://www.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n        response = mw.process_view(req, post_form_view, (), {})\n    self.assertEqual(response.status_code, 403)\n    self.assertEqual(\n        cm.records[0].getMessage(),\n        'Forbidden (%s): ' % REASON_INSECURE_REFERER,\n    )\n", "def test_rotate_token_creates_new_token(self):\n    \"\"\"\n    rotate_token() generates a new CSRF token and sets it in the request META.\n    \"\"\"\n    req = self._get_request()\n    initial_token = get_token(req)\n    self.assertTrue(initial_token)\n    \n    rotate_token(req)\n    new_token = req.META[\"CSRF_COOKIE\"]\n    self.assertNotEqual(initial_token, new_token)\n    self.assertTrue(new_token)\n    self.assertEqual(len(new_token), CSRF_TOKEN_LENGTH)\n", "def test_process_request_invalid_method(self):\n    \"\"\"\n    Test that the middleware rejects requests with an invalid HTTP method.\n    \"\"\"\n    req = self._get_request(method='INVALID')\n    mw = CsrfViewMiddleware(post_form_view)\n    with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n        resp = mw.process_view(req, post_form_view, (), {})\n    self.assertEqual(403, resp.status_code)\n    self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % REASON_NO_CSRF_COOKIE)\n"], "sample_338": ["    def test_alter_field_with_custom_db_column(self):\n        \"\"\"\n        Ensure that altering a field to include a custom db_column is detected\n        and generates the correct migration operation.\n        \"\"\"\n        before = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, db_column='custom_name')),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, db_column='custom_name')\n", "def test_alter_field_unique_together(self):\n    \"\"\"Tests altering a field and also modifying unique_together.\"\"\"\n    initial_author = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n        ('age', models.IntegerField(unique=True)),\n    ], {\n        'unique_together': {('name',)},\n    })\n    altered_author = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=300, unique=True)),\n        ('age', models.IntegerField(unique=True)),\n    ], {\n        'unique_together': {('name', 'age')},\n    })\n    changes = self.get_changes([initial_author], [altered_author])\n    \n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\n        'AlterField', 'AlterUniqueTogether'\n    ])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author', name='name')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name='author', unique_together={('name', 'age')})\n", "    def test_alter_model_table(self):\n        \"\"\"Tests the detection of AlterModelTable operations.\"\"\"\n        model_with_table = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], {\"db_table\": \"custom_table\"})\n        model_with_new_table = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], {\"db_table\": \"new_custom_table\"})\n\n        # Test adding a custom db_table\n        changes = self.get_changes([self.author_empty], [model_with_table])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=\"custom_table\")\n\n        # Test changing an existing custom db_table\n        changes = self.get_changes([model_with_table], [model_with_new_table])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=\"new_custom_table\")\n\n        # Test removing a custom db_table\n        changes = self.get_changes([model_with_table], [self.author_empty])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=None)\n", "    def test_add_custom_validators(self):\n        \"\"\"\n        Test adding a custom validator to a field.\n        \"\"\"\n        author_without_validator = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n        author_with_validator = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, validators=[DeconstructibleObject()])),\n        ])\n        changes = self.get_changes([author_without_validator], [author_with_validator])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n", "    def test_generate_altered_db_table(self):\n        \"\"\"\n        Tests the detection and creation of the AlterModelTable operation for changes to db_table.\n        \"\"\"\n        changes = self.get_changes([self.author_with_db_table_options], [self.author_with_new_db_table_options])\n        # Ensure the correct number and type of migrations.\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=\"author_two\")\n        \n        # Now test removing the db_table option\n        changes = self.get_changes([self.author_with_db_table_options], [self.author_empty])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=None)\n"], "sample_339": ["    def test_construct_instance(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n            description = models.TextField(blank=True)\n\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = TestModel\n                fields = ['name', 'description']\n\n        form_data = {'name': 'Test Name', 'description': 'Test Description'}\n        form = TestForm(data=form_data)\n        self.assertTrue(form.is_valid())\n\n        instance = TestModel()\n        instance = construct_instance(form, instance)\n        self.assertEqual(instance.name, 'Test Name')\n        self.assertEqual(instance.description, 'Test Description')\n", "    def test_construct_instance_excludes_field(self):\n        class FakeField:\n            name = 'test_field'\n            editable = True\n                self.default = default\n                return self.default\n                setattr(instance, self.name, value)\n        \n        class FakeForm:\n            cleaned_data = {'test_field': 'some_value'}\n                return self\n            \n            class FakeWidget:\n                    return False\n            field = FakeWidget()\n\n        class FakeModel:\n            _meta = type('meta', (), {'fields': [FakeField(default=False)]})\n\n        form = FakeForm()\n        instance = FakeModel()\n        instance = construct_instance(form, instance, exclude=['test_field'])\n        self.assertFalse(hasattr(instance, 'test_field'))\n", "    def test_modelformset_factory_applies_limit_choices_to(self):\n        \"\"\"\n        Test that apply_limit_choices_to is applied correctly to form fields.\n        \"\"\"\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = Book\n                fields = '__all__'\n\n        AuthorFormSet = modelformset_factory(Author, form=TestForm, fields='__all__', extra=0)\n        formset = AuthorFormSet()\n        for form in formset:\n            for field_name, field in form.fields.items():\n                if hasattr(field, 'queryset'):\n                    self.assertIsNotNone(field.queryset)\n", "    def setUp(self):\n        self.author1 = Author.objects.create(name='Charles Baudelaire')\n        self.author2 = Author.objects.create(name='Paul Verlaine')\n", "    def test_apply_limit_choices_to_to_formfield(self):\n        from django.db import models\n\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n            related = models.ForeignKey(\n                'self', on_delete=models.CASCADE, related_name='related_set', null=True, blank=True\n            )\n\n            class Meta:\n                app_label = 'tests'\n\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = TestModel\n                fields = ['name', 'related']\n\n        # Create a formfield with a queryset and limit_choices_to condition\n        formfield = TestForm().fields['related']\n        formfield.queryset = TestModel.objects.all()\n        formfield.get_limit_choices_to = lambda: {'name__startswith': 'A'}\n\n        # Apply limit_choices_to to the formfield\n        apply_limit_choices_to_to_formfield(formfield)\n\n        # Verify that the queryset has been filtered correctly\n        self.assertIn('name__startswith', str(formfield.queryset.query))\n        self.assertEqual(formfield.queryset.query.where.children[0].rhs, 'A')\n"], "sample_340": ["    def test_get_migration(self):\n        \"\"\"Tests the get_migration method to ensure correct migration retrieval.\"\"\"\n        migration_loader = MigrationLoader(connection)\n        migration = migration_loader.get_migration(\"migrations\", \"0001_initial\")\n        self.assertEqual(migration.name, \"0001_initial\")\n        self.assertEqual(migration.app_label, \"migrations\")\n\n        with self.assertRaises(NodeNotFoundError):\n            migration_loader.get_migration(\"migrations\", \"non_existent\")\n", "    def test_collect_sql(self):\n        \"\"\"\n        Tests the collect_sql method to ensure it correctly collects SQL statements\n        for the given migration plan.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n        \n        # Apply initial migration\n        self.record_applied(recorder, 'migrations', '0001_initial')\n        \n        # Prepare a migration plan\n        plan = [\n            (loader.get_migration('migrations', '0002_second'), False),\n            (loader.get_migration('migrations', '0001_initial'), True),\n        ]\n        \n        # Collect SQL statements for the migration plan\n        sql_statements = loader.collect_sql(plan)\n        \n        # Ensure collected SQL statements are not empty\n        self.assertGreater(len(sql_statements), 0)\n        \n        # Check if the SQL statements contain expected operations\n        expected_operations = [\n            'CREATE TABLE',  # For applying migrations.migrations.0002_second\n            'DROP TABLE',    # For unapplying migrations.migrations.0001_initial\n        ]\n        for operation in expected_operations:\n            self.assertTrue(any(operation in statement for statement in sql_statements))\n", "    def test_collect_sql(self):\n        \"\"\"\n        Tests the collect_sql method to ensure it collects SQL statements correctly.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        plan = migration_loader.graph.forwards_plan((\"migrations\", \"0002_second\"))\n        collected_sql = migration_loader.collect_sql(plan)\n        \n        # We expect some SQL statements to be collected in the process.\n        self.assertTrue(collected_sql)\n        self.assertIsInstance(collected_sql, list)\n        self.assertTrue(all(isinstance(stmt, str) for stmt in collected_sql))\n", "    def test_inconsistent_migration_history(self):\n        \"\"\"\n        Tests that InconsistentMigrationHistory is raised when a migration has\n        unapplied dependencies.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n        self.record_applied(recorder, 'migrations', '0002_second')\n        loader.build_graph()\n        msg = (\n            \"Migration migrations.0002_second is applied before its dependency \"\n            \"migrations.0001_initial on database 'default'.\"\n        )\n        with self.assertRaisesMessage(InconsistentMigrationHistory, msg):\n            loader.check_consistent_history(connection)\n", "    def test_collect_sql(self):\n        \"\"\"\n        Test the collect_sql method to ensure it collects SQL statements\n        for a given migration plan.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        # Create a plan for applying migrations\n        plan = migration_loader.graph.forwards_plan((\"migrations\", \"0002_second\"))\n        # Collect SQL statements for the plan\n        sql_statements = migration_loader.collect_sql(plan)\n        \n        # Check that the collected SQL statements are non-empty for our migrations\n        self.assertGreater(len(sql_statements), 0)\n        # Example check - ensure the 'migrations' table is mentioned in the SQL\n        self.assertTrue(any(\"migrations\" in statement for statement in sql_statements))\n"], "sample_341": ["    def test_management_form_missing_total_forms(self):\n        \"\"\"\n        If the TOTAL_FORMS field is missing from the management form,\n        a ValidationError should be raised.\n        \"\"\"\n        data = {\n            'choices-INITIAL_FORMS': '1',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.non_form_errors(),\n            [\n                'ManagementForm data is missing or has been tampered with. '\n                'Missing fields: choices-TOTAL_FORMS. '\n                'You may need to file a bug report if the issue persists.',\n            ],\n        )\n", "    def test_empty_formset_total_form_count(self):\n        \"\"\"Ensure total_form_count returns 0 for an empty formset.\"\"\"\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet()\n        self.assertEqual(formset.total_form_count(), 0)\n", "    def test_formset_cleaned_data_access_before_validation(self):\n        \"\"\"\n        Accessing formset.cleaned_data before calling is_valid() should raise AttributeError.\n        \"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        with self.assertRaises(AttributeError):\n            _ = formset.cleaned_data\n", "    def test_custom_deletion_widget(self):\n        \"\"\"\n        Testing the custom deletion widget in the formset.\n        \"\"\"\n        class CustomDeletionWidgetFormSet(BaseFormSet):\n                return HiddenInput(attrs={'class': 'custom-delete'})\n\n        CustomDeletionWidgetChoiceFormSet = formset_factory(Choice, formset=CustomDeletionWidgetFormSet, can_delete=True)\n        initial = [{'choice': 'Calexico', 'votes': 100}, {'choice': 'Fergie', 'votes': 900}]\n        formset = CustomDeletionWidgetChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "def test_max_num_zero_with_initial_and_extra(self):\n        \"\"\"\n        max_num=0 should not render extra forms even if specified, but should\n        still render the initial forms.\n        \"\"\"\n        initial = [\n            {'choice': 'Gin Tonic'},\n            {'choice': 'Bloody Mary'},\n        ]\n        ChoiceFormSet = formset_factory(Choice, extra=2, max_num=0)\n        formset = ChoiceFormSet(initial=initial)\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"form-0-choice\" value=\"Gin Tonic\"></li>"], "sample_342": ["def test_invalid_model_name(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'admin_views', 'model_name': 'InvalidModel', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_invalid_app_label(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid', 'model_name': 'answer', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_invalid_app_label(self):\n        \"\"\"\n        Test that a request with an invalid app_label raises PermissionDenied.\n        \"\"\"\n        request = self.factory.get(self.url, {'term': 'is', **{**self.opts, 'app_label': 'invalid_app_label'}})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid_app', **self.opts, 'model_name': 'Answer', 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_invalid_app_label(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid', 'model_name': 'question', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_343": ["    def test_generic_foreign_key_check(self):\n        class Model(models.Model):\n            content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n            object_id = models.PositiveIntegerField()\n            field = GenericForeignKey('content_type', 'object_id')\n\n        errors = Model.field.check()\n        self.assertEqual(errors, [])\n", "    def test_generic_foreign_key_set_and_get(self):\n        question = Question.objects.create(text='What is the capital of France?')\n        post = Post.objects.create(title='Answer', parent=question)\n\n        self.assertEqual(post.parent, question)  # Ensure the GenericForeignKey points to the correct object\n\n        new_question = Question.objects.create(text='What is the capital of Germany?')\n        post.parent = new_question\n        post.save()\n        \n        self.assertEqual(post.parent, new_question)  # Ensure the GenericForeignKey updates to the new object\n", "    def test_generic_foreign_key_get_forward_related_filter(self):\n        question = Question.objects.create(text='Who?')\n        post = Post.objects.create(title='Answer', parent=question)\n\n        expected_filter = {\n            'object_id': question.pk,\n            'content_type': post.parent.get_content_type(obj=question).pk,\n        }\n        self.assertEqual(post.parent.get_forward_related_filter(question), expected_filter)\n", "    def test_set_and_get_generic_foreign_key(self):\n        question = Question.objects.create(text='Who?')\n        post = Post.objects.create(title='Answer', parent=question)\n        \n        # Test the __get__ method\n        post = Post.objects.get(pk=post.pk)\n        self.assertEqual(post.parent.pk, question.pk)\n        self.assertEqual(post.parent.text, 'Who?')\n        \n        # Test the __set__ method\n        new_question = Question.objects.create(text='What?')\n        post.parent = new_question\n        post.save()\n\n        post = Post.objects.get(pk=post.pk)\n        self.assertEqual(post.parent.pk, new_question.pk)\n        self.assertEqual(post.parent.text, 'What?')\n", "    def test_check_field_name_with_underscore(self):\n        class Model(models.Model):\n            field_with_underscore_ = GenericForeignKey()\n        errors = Model.field_with_underscore_.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'fields.E001')\n"], "sample_344": ["    def test_rename_model(self):\n        \"\"\"\n        Tests renaming a model and ensuring all references are updated accordingly.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"OldName\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n            ],\n        ))\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"RelatedModel\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"old_name\", models.ForeignKey(\"OldName\", models.CASCADE)),\n            ],\n        ))\n\n        # Render the initial state\n        project_state.apps\n\n        # Rename the model and update references\n        project_state.rename_model(\"migrations\", \"OldName\", \"NewName\")\n\n        # Verify the state of the renamed model and the related model\n        new_name_state = project_state.models['migrations', 'newname']\n        related_model_state = project_state.models['migrations', 'relatedmodel']\n\n        self.assertEqual(new_name_state.name, \"NewName\")\n        self.assertEqual(list(new_name_state.fields), [\"id\", \"name\"])\n        self.assertEqual(new_name_state.fields['name'].max_length, 100)\n\n        self.assertEqual(related_model_state.name, \"RelatedModel\")\n        self.assertEqual(list(related_model_state.fields), [\"id\", \"old_name\"])\n        self.assertEqual(related_model_state.fields['old_name'].remote_field.model, \"migrations.NewName\")\n\n        # Ensure the apps registry has been updated correctly\n        NewName = project_state.apps.get_model(\"migrations\", \"NewName\")\n        RelatedModel = project_state.apps.get_model(\"migrations\", \"RelatedModel\")\n        self.assertEqual(RelatedModel._meta.get_field(\"old_name\").remote_field.model, NewName)\n", "    def test_appconfigstub_import_models(self):\n        \"\"\"\n        Test if AppConfigStub imports models correctly and assigns them to the app's models dictionary.\n        \"\"\"\n        new_apps = Apps(['migrations'])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        app_config_stub = AppConfigStub(\"migrations\")\n        app_config_stub.apps = new_apps\n        app_config_stub.import_models()\n\n        self.assertIn('author', app_config_stub.models)\n        self.assertEqual(app_config_stub.models['author'], new_apps.get_model('migrations', 'author'))\n", "    def test_add_remove_index(self):\n        \"\"\"\n        Tests adding and removing an index from a model state.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"Tag\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n            ],\n        ))\n        index = models.Index(fields=[\"name\"], name=\"tag_name_idx\")\n        project_state.add_index(\"migrations\", \"tag\", index)\n        self.assertIn(index, project_state.models[\"migrations\", \"tag\"].options[\"indexes\"])\n\n        project_state.remove_index(\"migrations\", \"tag\", \"tag_name_idx\")\n        self.assertNotIn(index, project_state.models[\"migrations\", \"tag\"].options[\"indexes\"])\n", "    def test_remove_model(self):\n        \"\"\"\n        Test removing a model from the ProjectState and ensuring related models are updated.\n        \"\"\"\n        new_apps = Apps(['migrations'])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class Book(models.Model):\n            title = models.CharField(max_length=1000)\n            author = models.ForeignKey(Author, models.CASCADE)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state = ProjectState.from_apps(new_apps)\n        author_state = project_state.models['migrations', 'author']\n        book_state = project_state.models['migrations', 'book']\n\n        self.assertIn(('migrations', 'author'), project_state.models)\n        self.assertIn(('migrations', 'book'), project_state.models)\n\n        # Remove the Author model and check if the Book model is updated\n        project_state.remove_model('migrations', 'author')\n\n        self.assertNotIn(('migrations', 'author'), project_state.models)\n        self.assertIn(('migrations', 'book'), project_state.models)\n\n        # Check if Book model's ForeignKey to Author is now broken\n        with self.assertRaises(LookupError):\n            new_apps.get_model('migrations', 'book')._meta.get_field('author')\n", "    def test_clone_project_state(self):\n        \"\"\"\n        Test cloning a ProjectState to ensure that the cloned state is an exact\n        copy and changes to the cloned state do not affect the original state.\n        \"\"\"\n        new_apps = Apps([\"migrations\"])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n            bio = models.TextField()\n            age = models.IntegerField(blank=True, null=True)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class Book(models.Model):\n            title = models.CharField(max_length=1000)\n            author = models.ForeignKey(Author, models.CASCADE)\n            contributors = models.ManyToManyField(Author)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        project_state = ProjectState.from_apps(new_apps)\n        cloned_state = project_state.clone()\n        \n        # Check if the cloned state is equal to the original\n        self.assertEqual(project_state, cloned_state)\n        \n        # Modify the cloned state\n        cloned_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"Publisher\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=255)),\n            ],\n        ))\n        \n        # Check that the original state is not affected\n        self.assertNotIn(('migrations', 'publisher'), project_state.models)\n        self.assertIn(('migrations', 'publisher'), cloned_state.models)\n"], "sample_345": ["    def test_ensure_echo_on(self, mock_signal, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        # Test when ECHO is off\n        attrs = [0, 0, 0, 0]\n        mock_tcgetattr.return_value = attrs\n        with mock.patch('termios.ECHO', 1):\n            autoreload.ensure_echo_on()\n            self.assertTrue(attrs[3] & termios.ECHO)\n            self.assertTrue(mock_tcsetattr.called)\n\n        # Test when ECHO is already on\n        attrs = [0, 0, 0, termios.ECHO]\n        mock_tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        mock_tcsetattr.assert_called_once_with(sys.stdin, termios.TCSANOW, attrs)\n", "    def test_ensure_echo_on_when_disabled(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        mock_tcgetattr.return_value = [0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        self.assertTrue(mock_tcsetattr.called)\n", "    def test_ensure_echo_on(self, mock_tcsetattr, mock_tcgetattr):\n        if not hasattr(termios, 'ECHO'):\n            self.skipTest(\"termios.ECHO not available\")\n        # Mocking isatty to return True\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            # Simulate echo being off\n            attrs = [0, 0, 0, 0]\n            attrs[3] = 0  # Echo is off\n            mock_tcgetattr.return_value = attrs\n            autoreload.ensure_echo_on()\n            # Ensure that tcgetattr is called once\n            mock_tcgetattr.assert_called_once_with(sys.stdin)\n            # Ensure that tcsetattr is called once\n            mock_tcsetattr.assert_called_once_with(sys.stdin, termios.TCSANOW, attrs)\n            # Ensure that echo is enabled in the attrs\n            self.assertTrue(attrs[3] & termios.ECHO)\n", "    def test_ensure_echo_on_with_echo_disabled(self, mocked_stdin, mocked_termios):\n        mocked_stdin.isatty.return_value = True\n        attributes = [0, 0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attributes\n        attributes[3] &= ~mocked_termios.ECHO\n        mocked_signal_handler = mock.MagicMock()\n        with mock.patch('signal.signal', return_value=mocked_signal_handler):\n            autoreload.ensure_echo_on()\n            self.assertTrue(attributes[3] & mocked_termios.ECHO)\n            mocked_termios.tcsetattr.assert_called_once_with(mocked_stdin, mocked_termios.TCSANOW, attributes)\n            mocked_signal_handler.assert_called_once()\n", "    def test_enable_echo_mode(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        # Simulate ECHO being off initially\n        attrs = [0, 0, 0, 0]\n        attrs[3] = attrs[3] & ~termios.ECHO\n        mock_tcgetattr.return_value = attrs\n\n        autoreload.ensure_echo_on()\n\n        # ECHO should now be enabled\n        attrs[3] |= termios.ECHO\n        mock_tcsetattr.assert_called_with(sys.stdin, termios.TCSANOW, attrs)\n"], "sample_346": ["    def test_decorator_from_middleware(self):\n        \"\"\"\n        Test the decorator_from_middleware functionality.\n        \"\"\"\n        class MyMiddleware:\n                self.get_response = get_response\n\n                request.processed = True\n                return None\n\n                response = self.get_response(request)\n                return response\n\n        @decorator_from_middleware(MyMiddleware)\n            if hasattr(request, 'processed'):\n                return HttpResponse(\"Processed\")\n            return HttpResponse(\"Not Processed\")\n\n        request = HttpRequest()\n        response = a_view(request)\n        self.assertEqual(response.content, b\"Processed\")\n", "    def test_sync_and_async_middleware(self):\n        @sync_and_async_middleware\n                response = get_response(request)\n                response['X-Sync-Async'] = 'True'\n                return response\n            return middleware\n\n            return HttpResponse()\n\n        middleware = middleware_factory(get_response)\n        request = HttpRequest()\n        response = middleware(request)\n        self.assertEqual(response['X-Sync-Async'], 'True')\n        self.assertTrue(middleware_factory.sync_capable)\n        self.assertTrue(middleware_factory.async_capable)\n", "    def test_decorator_from_middleware(self):\n        \"\"\"\n        Ensure that decorator_from_middleware returns a decorator that\n        applies the middleware to the view function.\n        \"\"\"\n        class DummyMiddleware:\n                self.get_response = get_response\n\n                response = self.get_response(request)\n                response['X-Dummy'] = 'dummy'\n                return response\n\n        @decorator_from_middleware(DummyMiddleware)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response['X-Dummy'], 'dummy')\n", "    def __init__(self, get_response, *args, **kwargs):\n        self.get_response = get_response\n", "    def test_make_middleware_decorator(self):\n        \"\"\"Test make_middleware_decorator function with middleware_class.\"\"\"\n        class DummyMiddleware:\n                self.get_response = get_response\n\n                request.processed = True\n                return None\n\n                if hasattr(request, 'processed'):\n                    return HttpResponse(\"Processed\")\n                return self.get_response(request)\n\n        @make_middleware_decorator(DummyMiddleware)\n            return HttpResponse(\"Not Processed\")\n\n        request = HttpRequest()\n        response = my_view(request)\n        self.assertEqual(response.content, b\"Processed\")\n"], "sample_347": ["    def test_get_fixed_timezone(self):\n        self.assertEqual(timezone.get_fixed_timezone(120).utcoffset(None), datetime.timedelta(minutes=120))\n        self.assertEqual(timezone.get_fixed_timezone(-120).utcoffset(None), datetime.timedelta(minutes=-120))\n        self.assertEqual(timezone.get_fixed_timezone(datetime.timedelta(hours=2)).utcoffset(None), datetime.timedelta(hours=2))\n        self.assertEqual(timezone.get_fixed_timezone(datetime.timedelta(hours=-2)).utcoffset(None), datetime.timedelta(hours=-2))\n", "    def test_get_fixed_timezone(self):\n        self.assertEqual(timezone.get_fixed_timezone(60).utcoffset(None), datetime.timedelta(minutes=60))\n        self.assertEqual(timezone.get_fixed_timezone(-60).utcoffset(None), datetime.timedelta(minutes=-60))\n        self.assertEqual(timezone.get_fixed_timezone(datetime.timedelta(hours=1)).utcoffset(None), datetime.timedelta(hours=1))\n        self.assertEqual(timezone.get_fixed_timezone(datetime.timedelta(hours=-1)).utcoffset(None), datetime.timedelta(hours=-1))\n        self.assertEqual(timezone.get_fixed_timezone(0).tzname(None), '+0000')\n        self.assertEqual(timezone.get_fixed_timezone(-330).tzname(None), '-0530')\n        self.assertEqual(timezone.get_fixed_timezone(330).tzname(None), '+0530')\n", "    def test_get_fixed_timezone(self):\n        self.assertEqual(timezone.get_fixed_timezone(330).utcoffset(None), datetime.timedelta(minutes=330))\n        self.assertEqual(timezone.get_fixed_timezone(-450).utcoffset(None), datetime.timedelta(minutes=-450))\n        self.assertEqual(timezone.get_fixed_timezone(datetime.timedelta(hours=5, minutes=30)).utcoffset(None), datetime.timedelta(minutes=330))\n        self.assertEqual(timezone.get_fixed_timezone(datetime.timedelta(hours=-7, minutes=-15)).utcoffset(None), datetime.timedelta(minutes=-435))\n", "    def test_get_fixed_timezone(self):\n        self.assertEqual(timezone.get_fixed_timezone(-300).utcoffset(None), datetime.timedelta(hours=-5))\n        self.assertEqual(timezone.get_fixed_timezone(datetime.timedelta(hours=5, minutes=30)).utcoffset(None), datetime.timedelta(hours=5, minutes=30))\n        self.assertEqual(timezone.get_fixed_timezone(-180).tzname(None), '-0300')\n        self.assertEqual(timezone.get_fixed_timezone(datetime.timedelta(hours=3)).tzname(None), '+0300')\n", "    def test_get_fixed_timezone(self):\n        delta = datetime.timedelta(hours=3, minutes=15)\n        tz = timezone.get_fixed_timezone(delta)\n        self.assertEqual(tz.utcoffset(None), delta)\n        self.assertEqual(tz.tzname(None), '+0315')\n\n        minutes_offset = -330\n        tz = timezone.get_fixed_timezone(minutes_offset)\n        self.assertEqual(tz.utcoffset(None), datetime.timedelta(minutes=minutes_offset))\n        self.assertEqual(tz.tzname(None), '-0530')\n"], "sample_348": ["    def test_model_to_dict_includes_all_fields(self):\n        band = Band(name=\"The Beatles\", bio=\"A famous band.\", sign_date=\"1960-01-01\")\n        expected_dict = {\n            'name': \"The Beatles\",\n            'bio': \"A famous band.\",\n            'sign_date': \"1960-01-01\",\n        }\n        self.assertEqual(model_to_dict(band), expected_dict)\n", "    def test_modelform_valid_case(self):\n        class Artist(Model):\n            name = models.CharField(max_length=100)\n            genre = models.CharField(max_length=100)\n\n        class ArtistForm(ModelForm):\n            class Meta:\n                model = Artist\n                fields = ['name', 'genre']\n\n        form = ArtistForm(data={'name': 'The Beatles', 'genre': 'Rock'})\n        self.assertTrue(form.is_valid())\n        artist = form.save(commit=False)\n        self.assertEqual(artist.name, 'The Beatles')\n        self.assertEqual(artist.genre, 'Rock')\n", "    def test_model_to_dict(self):\n        # Create instances\n        band = Band.objects.create(name=\"Test Band\", bio=\"Test Bio\", sign_date=\"2022-01-01\")\n        song = Song.objects.create(name=\"Test Song\", pub_date=\"2022-01-02\", band=band)\n\n        # Test model_to_dict for Band instance\n        band_data = model_to_dict(band)\n        self.assertEqual(band_data['name'], \"Test Band\")\n        self.assertEqual(band_data['bio'], \"Test Bio\")\n        self.assertEqual(band_data['sign_date'], \"2022-01-01\")\n\n        # Test model_to_dict for Song instance\n        song_data = model_to_dict(song)\n        self.assertEqual(song_data['name'], \"Test Song\")\n        self.assertEqual(song_data['pub_date'], \"2022-01-02\")\n        self.assertEqual(song_data['band'], band.pk)\n", "    def setUp(self):\n        class TestModel(Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField()\n            class Meta:\n                app_label = 'test'\n\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = TestModel\n                fields = ['name', 'age']\n\n        self.TestModel = TestModel\n        self.TestForm = TestForm\n", "    def test_model_to_dict(self):\n        class TestModel(Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField()\n            hidden_field = models.CharField(max_length=100, editable=False)\n\n        instance = TestModel(name=\"Test Name\", age=25, hidden_field=\"Hidden\")\n        data = model_to_dict(instance, fields=[\"name\", \"age\"], exclude=[\"age\"])\n        self.assertEqual(data, {\"name\": \"Test Name\"})\n"], "sample_349": ["    def test_foreign_key_raw_id_widget_get_context(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        rubber_soul = Album.objects.create(name='Rubber Soul', band=beatles)\n        rel = VideoStream._meta.get_field('release_event').remote_field\n        widget = ForeignKeyRawIdWidget(rel, admin.site)\n        context = widget.get_context('release_event', rubber_soul.pk, {})\n        self.assertIn('related_url', context)\n        self.assertIn('link_label', context)\n        self.assertIn('link_title', context)\n        self.assertEqual(context['link_label'], 'Rubber Soul')\n        self.assertEqual(context['link_title'], 'Lookup')\n", "    def test_label_and_url_for_value(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        rubber_soul = Album.objects.create(name='Rubber Soul', band=beatles)\n        release_event = ReleaseEvent.objects.create(name='Test Target', album=rubber_soul)\n        \n        # Test for ForeignKeyRawIdWidget\n        rel = VideoStream._meta.get_field('release_event').remote_field\n        widget = ForeignKeyRawIdWidget(rel, admin.site)\n        label, url = widget.label_and_url_for_value(release_event.pk)\n        expected_url = reverse('admin:admin_widgets_releaseevent_change', args=(release_event.pk,))\n        self.assertEqual(label, 'Test Target')\n        self.assertEqual(url, expected_url)\n        \n        # Test for ManyToManyRawIdWidget\n        widget = ManyToManyRawIdWidget(rel, admin.site)\n        label, url = widget.label_and_url_for_value(release_event.pk)\n        self.assertEqual(label, '')\n        self.assertEqual(url, '')\n", "    def test_admin_date_widget(self):\n        widget = AdminDateWidget()\n        context = widget.get_context('date', None, {})\n        self.assertEqual(context['widget']['attrs']['class'], 'vDateField')\n        self.assertEqual(context['widget']['attrs']['size'], '10')\n        self.assertIn('admin/js/calendar.js', widget.media._js)\n        self.assertIn('admin/js/admin/DateTimeShortcuts.js', widget.media._js)\n", "    def test_media_inclusion(self):\n        form = AlbumForm()\n        media = form.media\n        self.assertIn('admin/js/vendor/jquery/jquery.min.js', media._js)\n        self.assertIn('admin/js/vendor/select2/select2.full.min.js', media._js)\n        self.assertIn('admin/js/autocomplete.js', media._js)\n        self.assertIn('admin/css/vendor/select2/select2.min.css', media._css['screen'])\n        self.assertIn('admin/css/autocomplete.css', media._css['screen'])\n", "    def test_autocomplete_select_multiple(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        form = AlbumForm(initial={'featuring': [beatles.pk, who.pk]})\n        attrs = form['featuring'].field.widget.get_context(name='featuring', value=[beatles.pk, who.pk], attrs={})['widget']['attrs']\n        self.assertEqual(attrs['class'], 'admin-autocomplete')\n        self.assertIn('data-ajax--cache', attrs)\n        self.assertIn('data-ajax--delay', attrs)\n        self.assertIn('data-ajax--type', attrs)\n        self.assertIn('data-ajax--url', attrs)\n        self.assertIn('data-theme', attrs)\n        self.assertIn('data-allow-clear', attrs)\n        self.assertIn('data-app-label', attrs)\n        self.assertIn('data-field-name', attrs)\n        self.assertIn('data-model-name', attrs)\n        self.assertIn('data-placeholder', attrs)\n        self.assertIn('lang', attrs)\n"], "sample_350": ["    def test_complex_filter_with_combined_qs(self):\n        qs1 = Number.objects.filter(num__lte=1)\n        qs2 = Number.objects.filter(num__gte=8)\n        combined_qs = qs1.union(qs2)\n        with self.assertRaisesMessage(NotSupportedError, 'Calling QuerySet.filter() after union() is not supported.'):\n            combined_qs.filter(num=0)\n", "    def test_values_list_with_empty_qs(self):\n        qs1 = Number.objects.none().values_list('num', flat=True)\n        self.assertEqual(list(qs1), [])\n        qs2 = ReservedName.objects.none().values_list('name', flat=True)\n        self.assertEqual(list(qs2), [])\n", "def test_combined_with_empty_qs_and_order_by(self):\n    qs1 = Number.objects.filter(num__gte=5)\n    qs2 = Number.objects.none()\n    combined_qs = qs1.union(qs2).order_by('num')\n    self.assertNumbersEqual(combined_qs, [5, 6, 7, 8, 9])\n\n    qs3 = Number.objects.filter(num__lte=1)\n    combined_qs = qs2.union(qs3).order_by('num')\n    self.assertNumbersEqual(combined_qs, [0, 1])\n", "    def test_combining_with_multiple_expressions(self):\n        # Test combining querysets with multiple expressions and filters.\n        qs1 = Number.objects.annotate(multiplied_num=F('num') * 2).filter(multiplied_num__lt=10)\n        qs2 = Number.objects.annotate(added_num=F('num') + 2).filter(added_num__gt=5)\n        combined_qs = qs1.union(qs2)\n        expected_numbers = sorted(set(qs1.values_list('num', flat=True)) | set(qs2.values_list('num', flat=True)))\n        self.assertNumbersEqual(combined_qs, expected_numbers, ordered=False)\n", "    def test_intersection_with_annotation(self):\n        qs1 = Number.objects.annotate(is_even=F('num') % 2 == 0).filter(is_even=True)\n        qs2 = Number.objects.annotate(is_greater_than_5=F('num') > 5).filter(is_greater_than_5=True)\n        expected_numbers = [6, 8]\n        self.assertNumbersEqual(qs1.intersection(qs2), expected_numbers, ordered=False)\n"], "sample_351": ["    def test_model_multiple_choice_field_clean(self):\n        field = forms.ModelMultipleChoiceField(Category.objects.all())\n        self.assertEqual(list(field.clean([self.c1.pk, self.c2.pk])), [self.c1, self.c2])\n        with self.assertRaises(ValidationError):\n            field.clean(['invalid_pk'])\n", "    def test_model_to_dict(self):\n        article = Article.objects.create(\n            title=\"Sample Article\",\n            pub_date=datetime.date(2023, 10, 1),\n            writer=Writer.objects.create(name='Test writer'),\n        )\n        initial = model_to_dict(article)\n        self.assertEqual(initial['title'], \"Sample Article\")\n        self.assertEqual(initial['pub_date'], datetime.date(2023, 10, 1))\n        self.assertEqual(initial['writer'], article.writer.pk)\n", "def test_modelmultiplechoicefield_basics(self):\n        f = forms.ModelMultipleChoiceField(Category.objects.all())\n        self.assertEqual(list(f.choices), [\n            (self.c1.pk, 'Entertainment'),\n            (self.c2.pk, 'A test'),\n            (self.c3.pk, 'Third'),\n        ])\n        with self.assertRaises(ValidationError):\n            f.clean('')\n        with self.assertRaises(ValidationError):\n            f.clean(None)\n        with self.assertRaises(ValidationError):\n            f.clean([0])\n\n        # Invalid types that require TypeError to be caught.\n        with self.assertRaises(ValidationError):\n            f.clean([['fail']])\n        with self.assertRaises(ValidationError):\n            f.clean([{'foo': 'bar'}])\n\n        self.assertEqual([cat.name for cat in f.clean([self.c2.id])], ['A test'])\n        self.assertEqual([cat.name for cat in f.clean([self.c3.id])], ['Third'])\n\n        # Add a Category object *after* the ModelMultipleChoiceField has already been\n        # instantiated. This proves clean() checks the database during clean()\n        # rather than caching it at instantiation time.\n        c4 = Category.objects.create(name='Fourth', slug='4th', url='4th')\n        self.assertEqual([cat.name for cat in f.clean([c4.id])], ['Fourth'])\n\n        # Delete a Category object *after* the ModelMultipleChoiceField has already been\n        # instantiated. This proves clean() checks the database during clean()\n        # rather than caching it at instantiation time.\n        Category.objects.get(url='4th').delete()\n        msg = \"['Select a valid choice. That choice is not one of the available choices.']\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean([c4.id])\n", "def test_model_to_dict_function(self):\n    \"\"\"\n    Test the model_to_dict function to ensure it returns the correct dictionary\n    representation of a model instance.\n    \"\"\"\n    book = Book.objects.create(title='Test Book', author=Author.objects.create(name='Test Author'))\n    book_dict = model_to_dict(book, fields=['title', 'author'], exclude=None)\n    self.assertEqual(book_dict, {'title': 'Test Book', 'author': book.author.pk})\n\n    # Test with exclude parameter\n    book_dict_exclude = model_to_dict(book, fields=None, exclude=['title'])\n    self.assertEqual(book_dict_exclude, {'author': book.author.pk})\n\n    # Test with fields parameter\n    book_dict_fields = model_to_dict(book, fields=['title'], exclude=None)\n    self.assertEqual(book_dict_fields, {'title': 'Test Book'})\n\n    # Test with both fields and exclude parameters\n    book_dict_fields_exclude = model_to_dict(book, fields=['title', 'author'], exclude=['author'])\n    self.assertEqual(book_dict_fields_exclude, {'title': 'Test Book'})\n", "def test_model_to_dict(self):\n        writer = Writer.objects.create(name='Test writer')\n        article = Article.objects.create(\n            pub_date=datetime.date(1988, 1, 4),\n            writer=writer,\n            headline='Test headline',\n            content='Test content',\n        )\n        article.categories.set([self.c1, self.c2])\n        \n        expected_dict = {\n            'id': article.pk,\n            'pub_date': datetime.date(1988, 1, 4),\n            'writer': writer.pk,\n            'headline': 'Test headline',\n            'content': 'Test content',\n            'categories': [self.c1.pk, self.c2.pk],\n        }\n\n        result_dict = model_to_dict(article)\n        self.assertEqual(result_dict, expected_dict)\n\n        # Test with fields argument\n        expected_dict_fields = {\n            'headline': 'Test headline',\n            'content': 'Test content',\n        }\n        result_dict_fields = model_to_dict(article, fields=['headline', 'content'])\n        self.assertEqual(result_dict_fields, expected_dict_fields)\n\n        # Test with exclude argument\n        expected_dict_exclude = {\n            'id': article.pk,\n            'pub_date': datetime.date(1988, 1, 4),\n            'writer': writer.pk,\n            'headline': 'Test headline',\n            'content': 'Test content',\n        }\n        result_dict_exclude = model_to_dict(article, exclude=['categories'])\n        self.assertEqual(result_dict_exclude, expected_dict_exclude)\n"], "sample_352": ["    def test_split_having_with_aggregate(self):\n        \"\"\"\n        Test the split_having() function when the node contains an aggregate.\n        \"\"\"\n        where_node = WhereNode(connector=AND)\n        where_node.negated = False\n\n        # Create dummy child nodes, one containing an aggregate and one not.\n        class DummyNode:\n            contains_aggregate = False\n\n                if self.contains_aggregate:\n                    return None, self\n                return self, None\n\n        class AggregateNode(DummyNode):\n            contains_aggregate = True\n\n        where_node.children = [DummyNode(), AggregateNode()]\n\n        where_clause, having_clause = where_node.split_having()\n\n        # where_clause should only contain the non-aggregate node\n        self.assertEqual(len(where_clause.children), 1)\n        self.assertFalse(where_clause.children[0].contains_aggregate)\n\n        # having_clause should only contain the aggregate node\n        self.assertEqual(len(having_clause.children), 1)\n        self.assertTrue(having_clause.children[0].contains_aggregate)\n", "    def test_split_having_no_aggregate(self):\n        # Test split_having method when there is no aggregate.\n        node = WhereNode(children=[NothingNode(), NothingNode()])\n        where_node, having_node = node.split_having()\n        self.assertIs(where_node, node)\n        self.assertIs(having_node, None)\n", "    def test_split_having_with_aggregate(self):\n        where_node = WhereNode(children=[NothingNode()])\n        where_node.contains_aggregate = True\n\n        where_node.connector = OR\n        where_node.negated = False\n        where, having = where_node.split_having()\n        self.assertIsNone(where)\n        self.assertIsNotNone(having)\n\n        where_node.connector = AND\n        where_node.negated = True\n        where, having = where_node.split_having()\n        self.assertIsNone(where)\n        self.assertIsNotNone(having)\n", "    def test_split_having(self):\n        class MockNode:\n                self.contains_aggregate = contains_aggregate\n                self.negated = negated\n\n                if self.contains_aggregate:\n                    return None, self\n                return self, None\n\n        # Create a WhereNode with mixed children\n        children = [\n            MockNode(contains_aggregate=False),\n            MockNode(contains_aggregate=True)\n        ]\n        where_node = WhereNode(children=children, connector=AND, negated=False)\n\n        where_part, having_part = where_node.split_having()\n        self.assertIsNotNone(where_part)\n        self.assertIsNotNone(having_part)\n        self.assertEqual(len(where_part.children), 1)\n        self.assertEqual(len(having_part.children), 1)\n", "    def test_split_having(self):\n        # Test split_having method when there are no aggregates\n        node = WhereNode([WhereNodeTests.DummyNode(), WhereNodeTests.DummyNode()], AND)\n        where, having = node.split_having()\n        self.assertIsNotNone(where)\n        self.assertIsNone(having)\n        \n        # Test split_having method when there are aggregates\n        class AggregateNode(WhereNodeTests.DummyNode):\n            contains_aggregate = True\n            \n        node = WhereNode([AggregateNode(), WhereNodeTests.DummyNode()], AND)\n        where, having = node.split_having()\n        self.assertIsNotNone(where)\n        self.assertIsNotNone(having)\n"], "sample_353": ["    def test_command_error_on_invalid_many_to_many_through_model(self):\n        \"\"\"\n        Test that CommandError is raised when a required field specifies a many-to-many\n        relation through a model that is not auto-created.\n        \"\"\"\n        class MockUserModel:\n            REQUIRED_FIELDS = ['custom_field']\n            USERNAME_FIELD = 'username'\n            _default_manager = mock.Mock()\n            _meta = mock.Mock()\n            _meta.get_field = mock.Mock()\n\n        class MockField:\n            many_to_many = True\n            remote_field = mock.Mock()\n            remote_field.through = mock.Mock()\n            remote_field.through._meta.auto_created = False\n\n        MockUserModel._meta.get_field.side_effect = lambda name: MockField() if name == 'custom_field' else None\n\n        with mock.patch('django.contrib.auth.management.commands.createsuperuser.get_user_model', return_value=MockUserModel):\n            with self.assertRaisesMessage(CommandError, \"Required field 'custom_field' specifies a many-to-many relation through model, which is not supported.\"):\n                call_command('createsuperuser')\n", "    def test_non_interactive_custom_user_creation(self):\n        \"\"\"Test non-interactive superuser creation with a custom user model.\"\"\"\n        new_io = StringIO()\n        call_command('createsuperuser', interactive=False, verbosity=0)\n        user = CustomUser._default_manager.get(username='test_user')\n        self.assertEqual(user.email, 'test@example.com')\n        self.assertEqual(user.date_of_birth, date(1990, 1, 1))\n        self.assertFalse(user.has_usable_password())\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n", "    def test_create_superuser_with_fk_non_interactive(self):\n        new_io = StringIO()\n        group = Group.objects.create(name='mygroup')\n        email = Email.objects.create(email='mymail@gmail.com')\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username=email.pk,\n            email=email.email,\n            group=group.pk,\n            stdout=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        u = CustomUserWithFK._default_manager.get(email=email)\n        self.assertEqual(u.username, email)\n        self.assertEqual(u.group, group)\n", "    def test_missing_required_field_noinput(self):\n        \"\"\"\n        Test that a CommandError is raised if a required field is missing\n        in non-interactive mode.\n        \"\"\"\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, 'You must use --email with --noinput.'):\n            call_command('createsuperuser', interactive=False, username='joe', stdout=new_io)\n", "    def test_noinput_option_without_required_fields(self):\n        \"\"\"\n        Test `createsuperuser` with --noinput option but without providing required fields.\n        \"\"\"\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, 'You must use --email with --noinput.'):\n            call_command(\n                'createsuperuser',\n                interactive=False,\n                username='joe',\n                stdout=new_io\n            )\n        with self.assertRaisesMessage(CommandError, 'You must use --date_of_birth with --noinput.'):\n            call_command(\n                'createsuperuser',\n                interactive=False,\n                email='joe@example.com',\n                stdout=new_io\n            )\n"], "sample_354": ["    def test_invalid_username_length(self):\n        \"\"\"\n        Test that the management command fails gracefully when an invalid\n        username that exceeds the maximum length is provided.\n        \"\"\"\n        user_field = User._meta.get_field(User.USERNAME_FIELD)\n        new_io = StringIO()\n        too_long_username = 'a' * (user_field.max_length + 1)\n\n        @mock_inputs({\n            'username': too_long_username,\n            'password': 'password',\n            'email': 'user@example.com'\n        })\n            with self.assertRaisesMessage(CommandError, \"Ensure this value has at most %s characters (it has %s).\" % (user_field.max_length, len(too_long_username))):\n                call_command(\n                    'createsuperuser',\n                    interactive=True,\n                    stdin=MockTTY(),\n                    stdout=new_io,\n                    stderr=new_io,\n                )\n\n        test(self)\n", "    def setUp(self):\n        self.stdout = StringIO()\n        self.stderr = StringIO()\n", "    def test_non_field_error_during_validation(self, mock_validate_username):\n        \"\"\"\n        If a non-field error occurs during validation, it should be raised as a\n        CommandError.\n        \"\"\"\n        mock_validate_username.side_effect = CommandError(\"A non-field error occurred.\")\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, \"A non-field error occurred.\"):\n            call_command(\n                'createsuperuser',\n                interactive=False,\n                username='joe',\n                email='joe@somewhere.org',\n                stdout=new_io,\n            )\n", "    def setUp(self):\n        self.command = createsuperuser.Command()\n        self.command.UserModel = User\n        self.command.username_field = User._meta.get_field(User.USERNAME_FIELD)\n", "    def setUp(self):\n        self.stdout = StringIO()\n        self.stderr = StringIO()\n"], "sample_355": ["    def test_create_user_without_username(self):\n        with self.assertRaisesMessage(ValueError, 'The given username must be set'):\n            User.objects.create_user(username=None, email='test@example.com', password='password')\n", "    def setUpTestData(cls):\n        cls.content_type = ContentType.objects.create(app_label='auth', model='permission')\n        cls.permission = Permission.objects.create(\n            codename='test_permission',\n            name='Can test permission',\n            content_type=cls.content_type\n        )\n", "    def test_create_user(self):\n        \"\"\"\n        Test creating a regular user.\n        \"\"\"\n        user = User.objects.create_user(username='normaluser', email='user@example.com', password='foo')\n        self.assertEqual(user.username, 'normaluser')\n        self.assertEqual(user.email, 'user@example.com')\n        self.assertTrue(user.check_password('foo'))\n        self.assertFalse(user.is_staff)\n        self.assertFalse(user.is_superuser)\n", "    def setUp(self):\n        self.anonymous_user = AnonymousUser()\n", "    def test_create_user(self):\n        user = User.objects.create_user(username=\"testuser\", email=\"testuser@example.com\", password=\"testpassword\")\n        self.assertEqual(user.username, \"testuser\")\n        self.assertEqual(user.email, \"testuser@example.com\")\n        self.assertTrue(user.check_password(\"testpassword\"))\n        self.assertFalse(user.is_staff)\n        self.assertFalse(user.is_superuser)\n"], "sample_356": ["    def test_generate_added_indexes(self):\n        \"\"\"\n        Test the generation of added indexes.\n        \"\"\"\n        # Create initial and modified state\n        before = self.make_project_state([self.book])\n        after = self.make_project_state([self.book_indexes])\n        \n        # Detect changes and generate migrations\n        autodetector = MigrationAutodetector(before, after)\n        changes = autodetector._detect_changes()\n        \n        # Ensure the right number of migrations are generated\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        \n        # Ensure the generated migration has the correct operation\n        self.assertOperationTypes(changes, 'otherapp', 0, ['AddIndex'])\n        \n        # Validate the attributes of the generated operation\n        self.assertOperationAttributes(\n            changes,\n            'otherapp',\n            0,\n            0,\n            model_name='book',\n            index=models.Index(fields=['author', 'title'], name='book_title_author_idx')\n        )\n", "    def test_deep_deconstruct_functools_partial(self):\n        \"\"\"\n        Test deep_deconstruct method with functools.partial object.\n        \"\"\"\n            return a + b\n\n        partial_obj = functools.partial(dummy_function, 1, b=2)\n        autodetector = MigrationAutodetector(None, None)\n        deconstructed = autodetector.deep_deconstruct(partial_obj)\n        self.assertEqual(deconstructed, (dummy_function, (1,), {'b': 2}))\n", "    def test_generate_renamed_models(self):\n        \"\"\"\n        Test the generate_renamed_models method to ensure it correctly detects and handles model renames.\n        \"\"\"\n        before = [\n            ModelState(\"app\", \"OldModel\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState(\"app\", \"NewModel\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ]),\n        ]\n        changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename_model': True}))\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"app\", 1)\n        self.assertOperationTypes(changes, \"app\", 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, \"app\", 0, 0, old_name=\"OldModel\", new_name=\"NewModel\")\n", "    def test_generate_renamed_models(self):\n        \"\"\"\n        Tests the detection and handling of renamed models.\n        \"\"\"\n        before = [\n            ModelState(\"app\", \"OldModel\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ]),\n            ModelState(\"app\", \"AnotherModel\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"related\", models.ForeignKey(\"app.OldModel\", models.CASCADE)),\n            ]),\n        ]\n        after = [\n            ModelState(\"app\", \"NewModel\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ]),\n            ModelState(\"app\", \"AnotherModel\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"related\", models.ForeignKey(\"app.NewModel\", models.CASCADE)),\n            ]),\n        ]\n        autodetector = MigrationAutodetector(self.make_project_state(before), self.make_project_state(after), MigrationQuestioner({'ask_rename_model': True}))\n        changes = autodetector._detect_changes()\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"app\", 1)\n        self.assertOperationTypes(changes, \"app\", 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, \"app\", 0, 0, old_name=\"OldModel\", new_name=\"NewModel\")\n", "def test_alter_field_with_deconstructible_default(self):\n    \"\"\"\n    Tests altering a field with a deconstructible default value.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_2],\n    )\n    # No changes should be detected because deconstructible defaults are equal\n    self.assertEqual(len(changes), 0)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_3],\n    )\n    # Changes should be detected as the default value has changed\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n"], "sample_357": ["    def test_deep_deconstruct_partial(self):\n        \"\"\"\n        Test the deep deconstruction of a functools.partial object.\n        \"\"\"\n            return a + b + (c if c else 0)\n\n        partial_func = functools.partial(my_function, 1, c=2)\n        autodetector = MigrationAutodetector(None, None)\n        deconstructed = autodetector.deep_deconstruct(partial_func)\n        expected_deconstructed = (my_function, (1,), {'c': 2})\n\n        self.assertEqual(deconstructed, expected_deconstructed)\n", "def test_alter_field_with_custom_validator(self):\n        \"\"\"\n        Test altering a field to include a custom validator.\n        \"\"\"\n        before = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n        after = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, validators=[validate_slug])),\n        ])\n        changes = self.get_changes([before], [after])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n", "    def test_deep_deconstruct_partial_function(self):\n        \"\"\"Tests deep_deconstruct with functools.partial functions.\"\"\"\n            return x + y + z\n\n        partial_func = functools.partial(sample_function, 1, z=4)\n        obj = {\n            'partial_func': partial_func,\n            'nested': {\n                'partial_func': functools.partial(sample_function, 2, 3)\n            },\n            'list_partial_func': [functools.partial(sample_function, 4)]\n        }\n        autodetector = MigrationAutodetector(None, None)\n\n        deep_deconstructed = autodetector.deep_deconstruct(obj)\n\n        self.assertEqual(\n            deep_deconstructed,\n            {\n                'partial_func': (\n                    sample_function, \n                    [1], \n                    {'z': 4}\n                ),\n                'nested': {\n                    'partial_func': (\n                        sample_function,\n                        [2, 3],\n                        {}\n                    )\n                },\n                'list_partial_func': [\n                    (\n                        sample_function,\n                        [4],\n                        {}\n                    )\n                ]\n            }\n        )\n", "    def test_deep_deconstruct_with_regex(self):\n        \"\"\"Tests the deep deconstruction of a model with a regex validator.\"\"\"\n        model_with_regex = ModelState(\"testapp\", \"ModelWithRegex\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"slug\", models.CharField(max_length=255, validators=[\n                RegexValidator(\n                    re.compile(r'^[a-zA-Z0-9-_]+$', re.IGNORECASE),\n                    'Enter a valid \u201cslug\u201d consisting of letters, numbers, underscores, or hyphens.',\n                    'invalid'\n                )\n            ])),\n        ])\n        changes = self.get_changes([], [model_with_regex])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"CreateModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"ModelWithRegex\")\n", "    def test_alter_field_with_function_default(self):\n        \"\"\"\n        Test that altering a field's default to a function is detected.\n        \"\"\"\n        author_with_default = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=lambda: \"default_value\")),\n        ])\n        author_with_no_default = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n        \n        changes = self.get_changes([author_with_no_default], [author_with_default])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n"], "sample_358": ["    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), opclasses=['opclass1', 'opclass2']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', \n            ['first_column', 'second_column'], \n            lambda column: column.upper(), \n            col_suffixes=['ASC', 'DESC'], \n            opclasses=['text_pattern_ops', 'varchar_pattern_ops']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table',\n            ['first_column', 'second_column'],\n            lambda column: column.upper(),\n            opclasses=['opclass1', 'opclass2']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], \n            quote_name=lambda column: column.upper(), \n            col_suffixes=['ASC', 'DESC'],\n            opclasses=['text_ops', 'varchar_ops']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), opclasses=['opclass1', 'opclass2']\n        )\n"], "sample_359": ["    def test_delete_model_with_duplicate_field_name(self):\n        \"\"\"\n        Tests the DeleteModel operation with a model having duplicate field names.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_dlmodupfield\")\n        # Add a model with duplicate field names\n        create_model_operation = migrations.CreateModel(\n            \"DuplicateFieldModel\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n                (\"pink\", models.CharField(max_length=100)),\n            ],\n        )\n        project_state = project_state.clone()\n        create_model_operation.state_forwards(\"test_dlmodupfield\", project_state)\n        with connection.schema_editor() as editor:\n            create_model_operation.database_forwards(\"test_dlmodupfield\", editor, project_state, project_state.clone())\n        \n        # Test the DeleteModel operation\n        operation = migrations.DeleteModel(\"DuplicateFieldModel\")\n        self.assertEqual(operation.describe(), \"Delete model DuplicateFieldModel\")\n        self.assertEqual(operation.migration_name_fragment, 'delete_duplicatefieldmodel')\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_dlmodupfield\", new_state)\n        self.assertNotIn((\"test_dlmodupfield\", \"duplicatefieldmodel\"), new_state.models)\n        # Test the database alteration\n        self.assertTableExists(\"test_dlmodupfield_duplicatefieldmodel\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_dlmodupfield\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_dlmodupfield_duplicatefieldmodel\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_dlmodupfield\", editor, new_state, project_state)\n        self.assertTableExists(\"test_dlmodupfield_duplicatefieldmodel\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"DeleteModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(list(definition[2]), [\"name\"])\n", "    def test_create_model_with_options(self):\n        \"\"\"\n        Tests the CreateModel operation with additional options.\n        \"\"\"\n        unique_together = {('pink', 'weight')}\n        index_together = {('pink', 'weight')}\n        options = {\n            'ordering': ['pink'],\n            'unique_together': unique_together,\n            'index_together': index_together,\n        }\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n                (\"weight\", models.FloatField()),\n            ],\n            options=options,\n        )\n        self.assertEqual(operation.describe(), \"Create model Pony\")\n        self.assertEqual(operation.migration_name_fragment, 'pony')\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmo_options\", new_state)\n        self.assertEqual(new_state.models[\"test_crmo_options\", \"pony\"].name, \"Pony\")\n        self.assertEqual(len(new_state.models[\"test_crmo_options\", \"pony\"].fields), 3)\n        self.assertEqual(new_state.models[\"test_crmo_options\", \"pony\"].options['ordering'], ['pink'])\n        self.assertEqual(new_state.models[\"test_crmo_options\", \"pony\"].options['unique_together'], unique_together)\n        self.assertEqual(new_state.models[\"test_crmo_options\", \"pony\"].options['index_together'], index_together)\n        # Test the database alteration\n        self.assertTableNotExists(\"test_crmo_options_pony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_crmo_options\", editor, project_state, new_state)\n        self.assertTableExists(\"test_crmo_options_pony\")\n        # Verify unique_together works\n        Pony = new_state.apps.get_model(\"test_crmo_options\", \"Pony\")\n        Pony.objects.create(pink=1, weight=2.0)\n        with self.assertRaises(IntegrityError):\n            Pony.objects.create(pink=1, weight=2.0)\n        # Verify index_together exists\n        self.assertIndexExists(\"test_crmo_options_pony\", [\"pink\", \"weight\"])\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crmo_options\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test", "    def test_create_model_with_duplicate_field_and_base(self):\n        \"\"\"\n        Test CreateModel operation raises errors for both duplicate field name and base.\n        \"\"\"\n        message_fields = 'Found duplicate value pink in CreateModel fields argument.'\n        message_bases = 'Found duplicate value test_crmo.pony in CreateModel bases argument.'\n        with self.assertRaisesMessage(ValueError, message_fields):\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.TextField()),\n                    (\"pink\", models.IntegerField(default=1)),\n                ],\n                bases=(\"test_crmo.Pony\", \"test_crmo.pony\"),\n            )\n        with self.assertRaisesMessage(ValueError, message_bases):\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"blue\", models.TextField()),\n                ],\n                bases=(\"test_crmo.Pony\", \"test_crmo.pony\"),\n            )\n", "    def test_create_model_with_bases(self):\n        \"\"\"\n        Tests the CreateModel operation with different types of bases.\n        \"\"\"\n        # Test with string base\n        operation = migrations.CreateModel(\n            \"Pony\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n            bases=(\"migrations.UnicodeModel\",),\n        )\n        self.assertEqual(operation.describe(), \"Create model Pony\")\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmo\", new_state)\n        self.assertIn((\"test_crmo\", \"pony\"), new_state.models)\n        self.assertEqual(new_state.models[\"test_crmo\", \"pony\"].bases, (\"migrations.unicodemodel\",))\n        \n        # Test with actual model base\n        operation = migrations.CreateModel(\n            \"Pony\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n            bases=(UnicodeModel,),\n        )\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmo\", new_state)\n        self.assertIn((\"test_crmo\", \"pony\"), new_state.models)\n        self.assertEqual(new_state.models[\"test_crmo\", \"pony\"].bases, (UnicodeModel,))\n        \n        # Test with multiple bases\n        operation = migrations.CreateModel(\n            \"Pony\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n            bases=(\"migrations.UnicodeModel\", Mixin),\n        )\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmo\", new_state)\n        self.assertIn((\"test_crmo\", \"pony\"), new_state.models)\n        self.assertEqual(new_state.models[\"test_crmo\", \"pony\"].bases, (\"migrations.unicodemodel\", Mixin))\n        \n        # Test the database alteration\n        self.assertTableNotExists(\"test_crmo_pony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_crmo\", editor, project_state, new_state)\n        self.assertTableExists(\"test_crmo_pony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crmo\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_crmo_pony\")\n", "    def test_create_model_with_duplicate_option_name(self):\n        \"\"\"\n        Test the CreateModel operation with a duplicate option name.\n        \"\"\"\n        with self.assertRaisesMessage(ValueError, 'Found duplicate value migrations.unicodemodel in CreateModel options argument.'):\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[],\n                options={\"proxy\": \"migrations.unicodemodel\", \"proxy\": \"migrations.unicodemodel\"},\n            )\n"], "sample_360": ["    def setUp(self):\n        self.default_cache = caches['default']\n        self.other_cache = caches['other']\n", "    def setUp(self):\n        self.cache = caches[DEFAULT_CACHE_ALIAS]\n        self.cache.clear()\n        self.middleware = UpdateCacheMiddleware(empty_response)\n", "    def setUp(self):\n        self.factory = RequestFactory()\n", "    def setUp(self):\n        self.default_cache = caches['default']\n        translation.activate('en')\n", "    def test_should_update_cache(self):\n        # Ensure that _should_update_cache returns the correct values.\n        middleware = UpdateCacheMiddleware(empty_response)\n        request = self.factory.get('/')\n\n        # Case when request._cache_update_cache is not set.\n        self.assertFalse(middleware._should_update_cache(request, HttpResponse()))\n\n        # Case when request._cache_update_cache is set to True.\n        request._cache_update_cache = True\n        self.assertTrue(middleware._should_update_cache(request, HttpResponse()))\n"], "sample_361": ["    def test_avoid_wrapping(self):\n        tests = (\n            ('Avoid wrapping this text', 'Avoid\\xa0wrapping\\xa0this\\xa0text'),\n            ('Multiple words to avoid wrapping', 'Multiple\\xa0words\\xa0to\\xa0avoid\\xa0wrapping'),\n            ('SingleWord', 'SingleWord'),\n            ('   Leading and trailing spaces   ', '\\xa0\\xa0\\xa0Leading\\xa0and\\xa0trailing\\xa0spaces\\xa0\\xa0\\xa0'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.check_output(avoid_wrapping, value, output)\n", "def test_avoid_wrapping(self):\n    items = (\n        ('This is a test', 'This\\xa0is\\xa0a\\xa0test'),\n        ('Avoid wrapping', 'Avoid\\xa0wrapping'),\n        ('Multiple words in a sentence', 'Multiple\\xa0words\\xa0in\\xa0a\\xa0sentence'),\n        ('Non-breaking space\\u00a0in text', 'Non-breaking\\xa0space\\u00a0in\\xa0text'),\n    )\n    for value, output in items:\n        with self.subTest(value=value, output=output):\n            self.check_output(avoid_wrapping, value, output)\n            self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"hello world\", \"hello\\xa0world\"),\n            (\"avoid wrapping here\", \"avoid\\xa0wrapping\\xa0here\"),\n            (\"no wrap\", \"no\\xa0wrap\"),\n            (\"a b c\", \"a\\xa0b\\xa0c\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.check_output(avoid_wrapping, value, output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"Hello world\", \"Hello\\xa0world\"),\n            (\"Avoid wrapping\", \"Avoid\\xa0wrapping\"),\n            (\"Multiple words to test\", \"Multiple\\xa0words\\xa0to\\xa0test\"),\n            (\"\", \"\"),\n            (\" \", \"\\xa0\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            ('Hello world', 'Hello\\xa0world'),\n            ('No wrap here', 'No\\xa0wrap\\xa0here'),\n            ('Multiple words to avoid wrapping', 'Multiple\\xa0words\\xa0to\\xa0avoid\\xa0wrapping'),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n"], "sample_362": ["    def test_rename_field_and_check_alter_model_managers(self):\n        \"\"\"\n        Tests renaming a field and altering model managers in one go.\n        \"\"\"\n        before = [\n            ModelState('testapp', 'Author', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ], managers=[\n                ('default_manager', models.Manager()),\n            ]),\n        ]\n        after = [\n            ModelState('testapp', 'Author', [\n                ('id', models.AutoField(primary_key=True)),\n                ('names', models.CharField(max_length=200)),\n            ], managers=[\n                ('new_manager', models.Manager()),\n            ]),\n        ]\n        changes = self.get_changes(\n            before, after, MigrationQuestioner({\"ask_rename\": True})\n        )\n        # Ensure the right number/type of migrations.\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameField\", \"AlterModelManagers\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='name', new_name='names')\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='author')\n        self.assertEqual([name for name, mgr in changes['testapp'][0].operations[1].managers], ['new_manager'])\n", "def test_handle_dependency_with_resolved_app_label(self):\n    \"\"\"Tests handling of dependencies with resolved app labels.\"\"\"\n    before = [\n        ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ]),\n    ]\n    after = [\n        ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ]),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertEqual(changes, {})\n", "    def test_alter_field_preserve_default(self):\n        \"\"\"Tests that altering a field with preserve_default=True is detected correctly.\"\"\"\n        changes = self.get_changes([self.author_name_default], [self.author_name_longer])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True)\n", "    def test_alter_field_custom_deconstruct(self):\n        \"\"\"Tests altering a field with a custom deconstruct method.\"\"\"\n        class CustomField(models.IntegerField):\n                self.custom_arg = custom_arg\n                super().__init__(*args, **kwargs)\n\n                name, path, args, kwargs = super().deconstruct()\n                kwargs['custom_arg'] = self.custom_arg\n                return name, path, args, kwargs\n\n        before = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"custom_field\", CustomField(custom_arg='old_value')),\n        ])\n        after = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"custom_field\", CustomField(custom_arg='new_value')),\n        ])\n        changes = self.get_changes([before], [after])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"custom_field\")\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, custom_arg='new_value')\n", "    def test_alter_field_default(self):\n        \"\"\"\n        Tests detection of field default changes.\n        \"\"\"\n        author_with_default_name = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=\"Unnamed\")),\n        ])\n        changes = self.get_changes([self.author_empty], [author_with_default_name])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=\"Unnamed\")\n\n        updated_author_with_default_name = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=\"Known\")),\n        ])\n        changes = self.get_changes([author_with_default_name], [updated_author_with_default_name])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=\"Known\")\n"], "sample_363": ["    def test_attrs(self):\n        w = widgets.AdminBigIntegerFieldWidget()\n        self.assertHTMLEqual(\n            w.render('test', 12345678901234567890),\n            '<input value=\"12345678901234567890\" type=\"number\" class=\"vBigIntegerField\" name=\"test\">',\n        )\n        w = widgets.AdminBigIntegerFieldWidget(attrs={'class': 'myBigIntegerField'})\n        self.assertHTMLEqual(\n            w.render('test', 12345678901234567890),\n            '<input value=\"12345678901234567890\" type=\"number\" class=\"myBigIntegerField\" name=\"test\">',\n        )\n", "    def test_optgroups(self):\n        \"\"\"\n        Test the optgroups method to ensure it returns the correct selected options\n        based on the ModelChoiceIterator.\n        \"\"\"\n        class TestField(forms.ModelChoiceField):\n                super().__init__(queryset=Band.objects.all(), *args, **kwargs)\n        \n        field = TestField()\n        admin_site = widget_admin_site\n        widget = widgets.AutocompleteSelect(field, admin_site)\n        band1 = Band.objects.create(name='Band1')\n        band2 = Band.objects.create(name='Band2')\n\n        optgroups = widget.optgroups('test', [band1.pk, band2.pk], None)\n        self.assertEqual(len(optgroups), 1)\n        self.assertEqual(len(optgroups[0][1]), 2)\n        self.assertEqual(optgroups[0][1][0]['value'], str(band1.pk))\n        self.assertEqual(optgroups[0][1][1]['value'], str(band2.pk))\n", "    def test_attrs(self):\n        w = widgets.AdminIntegerFieldWidget()\n        self.assertHTMLEqual(\n            w.render('test', 123),\n            '<input value=\"123\" type=\"number\" class=\"vIntegerField\" name=\"test\">',\n        )\n        # pass attrs to widget\n        w = widgets.AdminIntegerFieldWidget(attrs={'size': 20, 'class': 'myIntegerField'})\n        self.assertHTMLEqual(\n            w.render('test', 123),\n            '<input value=\"123\" type=\"number\" class=\"myIntegerField\" name=\"test\" size=\"20\">',\n        )\n", "    def test_attrs(self):\n        w = widgets.AdminIntegerFieldWidget()\n        self.assertHTMLEqual(\n            w.render('test', 123),\n            '<input value=\"123\" type=\"number\" class=\"vIntegerField\" name=\"test\">',\n        )\n        # pass attrs to widget\n        w = widgets.AdminIntegerFieldWidget(attrs={'size': 10, 'class': 'myIntegerField'})\n        self.assertHTMLEqual(\n            w.render('test', 123),\n            '<input value=\"123\" type=\"number\" class=\"myIntegerField\" name=\"test\" size=\"10\">',\n        )\n", "    def test_attrs(self):\n        w = widgets.AdminTextInputWidget()\n        self.assertHTMLEqual(\n            w.render('test', 'sample text'),\n            '<input value=\"sample text\" type=\"text\" class=\"vTextField\" name=\"test\">',\n        )\n        # pass attrs to widget\n        w = widgets.AdminTextInputWidget(attrs={'size': 20, 'class': 'myTextField'})\n        self.assertHTMLEqual(\n            w.render('test', 'sample text'),\n            '<input value=\"sample text\" type=\"text\" class=\"myTextField\" name=\"test\" size=\"20\">',\n        )\n"], "sample_364": ["    def test_include_with_valid_tuple(self):\n        module = 'urlpatterns.path_urls'\n        patterns = [path('test/', empty_view, name='test')]\n        app_name = 'test_app'\n        self.assertEqual(include((module, app_name)), (import_module(module), app_name, app_name))\n        ", "    def test_include_with_invalid_tuple_length(self):\n        msg = 'Passing a 3-tuple to include() is not supported. Pass a 2-tuple containing the list of patterns and app_name, and provide the namespace argument to include() instead.'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            include(('module', 'app_name', 'extra'), namespace='namespace')\n", "    def test_include_with_tuple_argument(self):\n        from django.urls import include\n\n        urlconf = ([path('test/', empty_view, name='test')], 'test_app')\n        urlconf_module, app_name, namespace = include(urlconf)\n\n        self.assertEqual(app_name, 'test_app')\n        self.assertEqual(namespace, 'test_app')\n        self.assertTrue(hasattr(urlconf_module, '__iter__'))\n        self.assertEqual(len(urlconf_module), 1)\n        self.assertIsInstance(urlconf_module[0], URLPattern)\n        self.assertEqual(urlconf_module[0].name, 'test')\n", "def test_include_with_invalid_tuple_length(self):\n        msg = 'Passing a 3-tuple to include() is not supported. Pass a 2-tuple containing the list of patterns and app_name, and provide the namespace argument to include() instead.'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            include(([], 'app_name', 'extra_value'))\n", "    def test_include_with_valid_tuple(self):\n        patterns = [path('test/', lambda r: None)]\n        app_name = 'my_app'\n        urlconf = (patterns, app_name)\n        result = include(urlconf)\n        self.assertEqual(result[0], patterns)\n        self.assertEqual(result[1], app_name)\n        self.assertEqual(result[2], app_name)\n"], "sample_365": ["    def test_classproperty_with_instance(self):\n        \"\"\"Ensure classproperty does not return instance attribute.\"\"\"\n        class Foo:\n            foo_attr = 123\n\n                self.foo_attr = 456\n\n            @classproperty\n                return cls.foo_attr\n\n        foo_instance = Foo()\n        self.assertEqual(foo_instance.foo, 123)\n", "    def test_lazy_object_deepcopy(self):\n        class Klazz:\n                self.value = value\n\n                return self.value == other.value\n\n        obj = lazy(lambda: Klazz(5), Klazz)()\n        copied_obj = copy.deepcopy(obj)\n        self.assertEqual(obj, copied_obj)\n", "    def test_keep_lazy_decorator(self):\n        \"\"\"\n        Test the keep_lazy decorator to ensure it returns a lazy proxy if any arguments are lazy.\n        \"\"\"\n        lazy_func = lazy(lambda x: x * 2, int)\n        \n        @keep_lazy(int)\n            return x * 2\n\n        # If argument is lazy, the result should be a lazy proxy.\n        result = multiply_by_two(lazy_func(3))\n        self.assertTrue(isinstance(result, Promise))\n        self.assertEqual(result(), 6)\n\n        # If argument is not lazy, the result should be computed immediately.\n        result = multiply_by_two(3)\n        self.assertFalse(isinstance(result, Promise))\n        self.assertEqual(result, 6)\n", "    def test_lazy_deepcopy(self):\n        \"\"\"\n        deepcopy works correctly for lazy objects.\n        \"\"\"\n        import copy\n        lazy_obj = lazy(lambda: [1, 2, 3], list)\n        original = lazy_obj()\n        copied = copy.deepcopy(original)\n        self.assertEqual(original, copied)\n        self.assertIsNot(original, copied)\n", "    def test_partition_function(self):\n        \"\"\"Test the partition function with different predicates and values.\"\"\"\n            return x % 2 == 0\n\n            return x > 0\n\n        values = range(-5, 6)  # -5 to 5 inclusive\n\n        even_partition = partition(is_even, values)\n        self.assertEqual(even_partition, ([-5, -3, -1, 1, 3, 5], [-4, -2, 0, 2, 4]))\n\n        positive_partition = partition(is_positive, values)\n        self.assertEqual(positive_partition, ([-5, -4, -3, -2, -1, 0], [1, 2, 3, 4, 5]))\n\n        # Test with an empty list\n        empty_partition = partition(is_even, [])\n        self.assertEqual(empty_partition, ([], []))\n\n        # Test with all values returning True\n        always_true_partition = partition(lambda x: True, values)\n        self.assertEqual(always_true_partition, ([], list(values)))\n\n        # Test with all values returning False\n        always_false_partition = partition(lambda x: False, values)\n        self.assertEqual(always_false_partition, (list(values), []))\n"], "sample_366": ["def test_parse_invalid_duration(self):\n        test_values = (\n            'abc',\n            '1day',\n            '1 days',\n            '1 25:00:00',\n            '1:60:00',\n            '1:00:60',\n            'P1DT0H0M0S',  # Invalid ISO 8601 format for duration\n            'PT0.5S',  # Incomplete ISO 8601 format\n            'T10:15:30',  # Missing 'P' in ISO 8601 format\n            'P10H15M30S',  # Incorrect order in ISO 8601 format\n        )\n        for value in test_values:\n            with self.subTest(value=value):\n                self.assertIsNone(parse_duration(value))\n", "    def test_invalid_duration_formats(self):\n        invalid_inputs = [\n            'P1Y2M3DT4H5M6S',  # Complete ISO 8601 duration string which is not supported\n            '1Y 2M',  # Years and months not supported\n            '1 day 25:00:00',  # Invalid hour value\n            '1 day 24:60:00',  # Invalid minute value\n            '1 day 24:00:61',  # Invalid second value\n            '1 day 24:00:60.1',  # Invalid second with fraction\n            '1 day 24:00:00,1000000',  # Invalid microseconds\n            'P1Y2M3D',  # Partial ISO 8601 duration string without 'T'\n            'T4H5M6S',  # ISO 8601 duration string without 'P' and date part\n            'days 4 15:30',  # Incorrect order\n            '4 days 15:30:60',  # Invalid second value in PostgreSQL format\n            '4 days 15:30.1000000',  # Invalid microseconds in PostgreSQL format\n            '4 days -15:30:60',  # Negative duration with invalid second value\n        ]\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_parse_invalid_duration(self):\n        # Invalid formats for durations\n        invalid_durations = [\n            'P1Y1M',          # year-month mixed interval\n            'P2W',            # weeks are not supported\n            '4 days 25:00',   # invalid time component\n            '1:60:60',        # invalid minutes and seconds\n            '1 day 24:01',    # hour component too high\n            'P1DT24H',        # ISO format with invalid hour\n            'P1DT25H',        # ISO format with invalid hour\n            '4 days 15:30.0000001',  # too precise fraction\n            '4 days 15:30,-000001',  # invalid comma fraction\n        ]\n        for value in invalid_durations:\n            with self.subTest(value=value):\n                self.assertIsNone(parse_duration(value))\n", "def test_invalid_parse_date(self):\n        # Invalid date formats\n        self.assertIsNone(parse_date('2012-13-01'))  # Invalid month\n        self.assertIsNone(parse_date('2012-00-01'))  # Invalid month\n        self.assertIsNone(parse_date('2012-01-32'))  # Invalid day\n        self.assertIsNone(parse_date('2012-01-00'))  # Invalid day\n        self.assertIsNone(parse_date('abcd-ef-gh'))  # Non-numeric\n        self.assertIsNone(parse_date('2012/04/23'))  # Wrong separator\n        self.assertIsNone(parse_date('April 23, 2012'))  # Wrong format\n        self.assertIsNone(parse_date('23-04-2012'))  # Wrong order\n        self.assertIsNone(parse_date('2012-04-23T09:15:00'))  # Contains time part\n", "    def test_iso8601_format(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P2.5D', timedelta(days=2, hours=12)),\n            ('PT5H', timedelta(hours=5)),\n            ('PT5.5H', timedelta(hours=5, minutes=30)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1.5M', timedelta(minutes=1, seconds=30)),\n            ('PT1S', timedelta(seconds=1)),\n            ('PT1.5S', timedelta(seconds=1, microseconds=500000)),\n            ('P1DT1H', timedelta(days=1, hours=1)),\n            ('P1DT1H1M', timedelta(days=1, hours=1, minutes=1)),\n            ('P1DT1H1M1S', timedelta(days=1, hours=1, minutes=1, seconds=1)),\n            ('-P1D', timedelta(days=-1)),\n            ('-P1DT1H', timedelta(days=-1, hours=-1)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_367": ["    def test_cache_control_decorator_headers(self):\n        @cache_control(max_age=3600, public=True)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'max-age=3600, public')\n", "    def test_cache_control_decorator_headers(self):\n        @cache_control(public=True, max_age=3600)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'public, max-age=3600')\n", "    def test_cache_control_decorator_headers(self):\n        @cache_control(private=True, max_age=3600)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'private, max-age=3600')\n", "    def test_cache_control_decorator_headers(self):\n        @cache_control(private=True, max_age=3600)\n            return HttpResponse()\n\n        request = HttpRequest()\n        response = a_view(request)\n        self.assertEqual(response.headers['Cache-Control'], 'private, max-age=3600')\n", "    def test_cache_page_with_custom_cache(self):\n            return HttpResponse(\"OK\")\n\n        with mock.patch('django.middleware.cache.CacheMiddleware.__call__') as mock_cache:\n            mock_cache.return_value = HttpResponse(\"Cached Response\")\n            my_view_cached = cache_page(60 * 15, cache='custom_cache')(my_view)\n            request = HttpRequest()\n            response = my_view_cached(request)\n            self.assertEqual(response.content, b\"Cached Response\")\n            mock_cache.assert_called_once_with(request)\n"], "sample_368": ["    def test_mixed_plan_not_supported_unapplied_migration(self):\n        \"\"\"\n        Test that a mixed migration plan with unapplied migrations raises an error.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Simulate applying the first migration\n        executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n        self.assertIn(('migrations', '0001_initial'), executor.loader.applied_migrations)\n\n        # Create a mixed plan with unapplied migrations\n        plan = executor.migration_plan([\n            (\"migrations\", \"0002_second\"),\n            (\"migrations\", None),\n        ])\n        msg = (\n            'Migration plans with both forwards and backwards migrations are '\n            'not supported. Please split your migration process into separate '\n            'plans of only forwards OR backwards migrations.'\n        )\n        with self.assertRaisesMessage(InvalidMigrationPlan, msg):\n            executor.migrate(None, plan)\n\n        # Clean up\n        executor.loader.build_graph()\n        executor.migrate([(\"migrations\", None)], fake=True)\n        self.assertNotIn(('migrations', '0001_initial'), executor.loader.applied_migrations)\n", "    def test_migrate_with_empty_targets(self):\n        \"\"\"\n        Test handling of migrate() with empty targets.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Start with no tables present.\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        # Run migrate with empty targets.\n        state = executor.migrate([])\n        # Ensure no tables were created.\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        # Ensure _create_project_state was called with with_applied_migrations=False.\n        self.assertEqual(state.apps.all_models['migrations'], {})\n", "    def test_migrate_with_no_operations(self):\n        \"\"\"\n        Tests migration behavior when no operations are present in migration.\n        \"\"\"\n        class NoOpMigration(migrations.Migration):\n            operations = []\n\n        executor = MigrationExecutor(connection)\n        executor.loader.graph.add_node((\"migrations\", \"0001_noop\"), NoOpMigration(\"0001_noop\", \"migrations\"))\n        # Check the plan\n        plan = executor.migration_plan([(\"migrations\", \"0001_noop\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_noop\"], False),\n            ],\n        )\n        # Apply the migration\n        executor.migrate([(\"migrations\", \"0001_noop\")])\n        self.assertIn((\"migrations\", \"0001_noop\"), executor.loader.applied_migrations)\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Unapply the migration\n        plan = executor.migration_plan([(\"migrations\", None)])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_noop\"], True),\n            ],\n        )\n        executor.migrate([(\"migrations\", None)])\n        self.assertNotIn((\"migrations\", \"0001_noop\"), executor.loader.applied_migrations)\n", "    def test_migrate_with_applied_migrations(self):\n        \"\"\"\n        Tests the creation of a project state with applied migrations.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n        executor.migrate([(\"migrations\", \"0002_second\")], fake=True)\n        # Create project state including applied migrations\n        state = executor._create_project_state(with_applied_migrations=True)\n        self.assertIn(('migrations', 'author'), state.models)\n        self.assertIn(('migrations', 'book'), state.models)\n        # Clean up\n        executor.migrate([(\"migrations\", None)], fake=True)\n        self.assertNotIn(('migrations', 'author'), state.models)\n        self.assertNotIn(('migrations', 'book'), state.models)\n", "    def test_partial_replacement_application(self):\n        \"\"\"\n        Test that partially applied replacements behave correctly.\n        \"\"\"\n        recorder = MigrationRecorder(connection)\n        # Apply the first replaced migration.\n        recorder.record_applied(\"migrations\", \"0001_initial\")\n        # Apply the second replaced migration.\n        recorder.record_applied(\"migrations\", \"0002_second\")\n        executor = MigrationExecutor(connection)\n        # Now apply the replacement migration.\n        executor.migrate([(\"migrations\", \"0001_squashed_0003\")])\n        # Check if the replacement migration is marked as applied.\n        self.assertIn(\n            (\"migrations\", \"0001_squashed_0003\"),\n            recorder.applied_migrations(),\n        )\n        # Unapply the replacement migration.\n        executor.migrate([(\"migrations\", None)])\n        self.assertNotIn(\n            (\"migrations\", \"0001_squashed_0003\"),\n            recorder.applied_migrations(),\n        )\n        self.assertNotIn(\n            (\"migrations\", \"0001_initial\"),\n            recorder.applied_migrations(),\n        )\n        self.assertNotIn(\n            (\"migrations\", \"0002_second\"),\n            recorder.applied_migrations(),\n        )\n"], "sample_369": ["    def test_rename_model_with_fk_dependency(self):\n        \"\"\"\n        Tests renaming a model that is referenced by a foreign key in another model.\n        \"\"\"\n        before = [\n            ModelState('testapp', 'Author', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ]),\n            ModelState('otherapp', 'Book', [\n                ('id', models.AutoField(primary_key=True)),\n                ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('testapp', 'Writer', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ]),\n            ModelState('otherapp', 'Book', [\n                ('id', models.AutoField(primary_key=True)),\n                ('author', models.ForeignKey('testapp.Writer', models.CASCADE)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename_model': True}))\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='Writer')\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, ['AlterField'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name='book', name='author')\n", "    def test_create_model_with_constraints(self):\n        \"\"\"Test detection of a new model with multiple constraints.\"\"\"\n        constraints = [\n            models.CheckConstraint(check=models.Q(name__contains='Alice'), name='name_contains_alice'),\n            models.UniqueConstraint(fields=['name'], name='unique_name')\n        ]\n        author_with_constraints = ModelState('otherapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ], {'constraints': constraints})\n        changes = self.get_changes([], [author_with_constraints])\n        added_check_constraint = models.CheckConstraint(check=models.Q(name__contains='Alice'), name='name_contains_alice')\n        added_unique_constraint = models.UniqueConstraint(fields=['name'], name='unique_name')\n        # Right number of migrations?\n        self.assertEqual(len(changes['otherapp']), 1)\n        # Right number of actions?\n        migration = changes['otherapp'][0]\n        self.assertEqual(len(migration.operations), 3)\n        # Right actions order?\n        self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel', 'AddConstraint', 'AddConstraint'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Author')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, model_name='author', constraint=added_check_constraint)\n        self.assertOperationAttributes(changes, 'otherapp', 0, 2, model_name='author', constraint=added_unique_constraint)\n", "    def test_add_model_with_duplicate_field_names(self):\n        \"\"\"Tests adding a model with duplicate field names raises an error.\"\"\"\n        with self.assertRaisesMessage(ValueError, \"Found duplicate value name in CreateModel fields argument.\"):\n            CreateModel(\n                name=\"Author\",\n                fields=[\n                    (\"name\", models.CharField(max_length=200)),\n                    (\"name\", models.CharField(max_length=200)),\n                ],\n            )\n", "    def test_create_model_with_fields(self):\n        \"\"\"Test creation of a model with multiple fields.\"\"\"\n        fields = [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n            ('age', models.IntegerField(null=True)),\n            ('bio', models.TextField(blank=True)),\n        ]\n        author = ModelState('testapp', 'Author', fields)\n        changes = self.get_changes([], [author])\n        # Right number of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        # Right number of operations?\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author')\n        self.assertEqual(len(changes['testapp'][0].operations[0].fields), 4)\n        self.assertEqual(changes['testapp'][0].operations[0].fields[1][1].max_length, 200)\n        self.assertEqual(changes['testapp'][0].operations[0].fields[2][1].null, True)\n        self.assertEqual(changes['testapp'][0].operations[0].fields[3][1].blank, True)\n", "    def test_remove_model_with_indexes_and_constraints(self):\n        \"\"\"Test removal of model with indexes and constraints.\"\"\"\n        author_with_constraints = ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ], {\n            'indexes': [models.Index(fields=['name'], name='author_name_idx')],\n            'constraints': [models.CheckConstraint(check=models.Q(name__contains='John'), name='name_contains_john')]\n        })\n        changes = self.get_changes([author_with_constraints], [])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveIndex', 'RemoveConstraint', 'DeleteModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author', name='author_name_idx')\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='author', name='name_contains_john')\n        self.assertOperationAttributes(changes, 'testapp', 0, 2, name='Author')\n"], "sample_370": ["    def test_foreign_key_deferred_attribute_set(self):\n        class DummyModel:\n                self.__dict__ = {}\n        \n        class DummyField:\n            attname = 'dummy_attr'\n                return 'dummy_attr' in instance.__dict__\n                if 'dummy_attr' in instance.__dict__:\n                    del instance.__dict__['dummy_attr']\n\n        instance = DummyModel()\n        field = DummyField()\n        descriptor = ForeignKeyDeferredAttribute(field)\n        \n        # Test setting a value\n        descriptor.__set__(instance, 'new_value')\n        self.assertEqual(instance.__dict__['dummy_attr'], 'new_value')\n        \n        # Test setting a new value when it's already cached\n        descriptor.__set__(instance, 'another_value')\n        self.assertEqual(instance.__dict__['dummy_attr'], 'another_value')\n", "    def test_foreign_key_deferred_attribute_set(self):\n        class MockField:\n            attname = 'mock_field'\n                return 'mock_field_cache' in instance.__dict__\n                del instance.__dict__['mock_field_cache']\n\n        class MockInstance:\n                self.__dict__ = {}\n        \n        mock_field = MockField()\n        instance = MockInstance()\n        deferred_attr = ForeignKeyDeferredAttribute(mock_field)\n        \n        # Test setting the attribute when the value is different\n        deferred_attr.__set__(instance, 'new_value')\n        self.assertEqual(instance.__dict__['mock_field'], 'new_value')\n        self.assertFalse('mock_field_cache' in instance.__dict__)\n\n        # Test setting the attribute when the value is the same\n        instance.__dict__['mock_field_cache'] = 'cached_value'\n        deferred_attr.__set__(instance, 'new_value')\n        self.assertEqual(instance.__dict__['mock_field'], 'new_value')\n        self.assertFalse('mock_field_cache' in instance.__dict__)\n", "def test_reverse_onetoone_forward_foreignkey(self):\n    \"\"\"\n    Test that ReverseOneToOneDescriptor works correctly when followed by a forward ForeignKey.\n    \"\"\"\n    author_bio = Bio.objects.create(author=self.author1)\n    address = AuthorAddress.objects.create(author=self.author1, address='SomeStreet 1')\n\n    with self.assertNumQueries(2):\n        authors = list(Author.objects.prefetch_related('bio__addresses'))\n\n    self.assertEqual(authors[0].bio.addresses.all()[0], address)\n", "    def test_forward_onetoone(self):\n        \"\"\"\n        Ensure that ForwardOneToOneDescriptor correctly handles forward one-to-one relationships.\n        \"\"\"\n        place = Place.objects.create(name='Some Place')\n        restaurant = Restaurant.objects.create(place=place, name='Some Restaurant')\n\n        with self.assertNumQueries(1):\n            fetched_restaurant = place.restaurant\n        \n        self.assertEqual(fetched_restaurant, restaurant)\n", "    def test_forward_one_to_one_get_object(self):\n        book_with_year = BookWithYear.objects.create(book=self.book1, published_year=2019)\n        author = Author.objects.create(name='New Author', first_book=self.book1)\n        with self.assertNumQueries(1):\n            fetched_book_with_year = author.first_book.bookwithyear\n        self.assertEqual(fetched_book_with_year, book_with_year)\n"], "sample_371": ["    def test_cleanse_setting_callable(self):\n        \"\"\"\n        Test that callable settings are wrapped in CallableSettingWrapper.\n        \"\"\"\n            return \"This should not be displayed\"\n        \n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed_setting = reporter_filter.cleanse_setting('CALLABLE_SETTING', callable_setting)\n        self.assertIsInstance(cleansed_setting, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed_setting), repr(callable_setting))\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_cleanse_setting_callable(self):\n        \"\"\"\n        Callable settings should be wrapped by CallableSettingWrapper\n        and not evaluated.\n        \"\"\"\n        filter = SafeExceptionReporterFilter()\n            return \"Sensitive Data\"\n\n        cleansed = filter.cleanse_setting(\"SENSITIVE_CALLABLE\", sensitive_callable)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed), repr(sensitive_callable))\n", "    def test_cleanse_special_types_multivalue_dict(self):\n        \"\"\"\n        Ensure that MultiValueDicts are cleansed appropriately.\n        \"\"\"\n        reporter_filter = SafeExceptionReporterFilter()\n        request = HttpRequest()\n        request.POST = MultiValueDict({\n            'sensitive_key': ['secret_value1', 'secret_value2'],\n            'normal_key': ['value1', 'value2']\n        })\n        request.sensitive_post_parameters = ['sensitive_key']\n        cleansed_post = reporter_filter.get_post_parameters(request)\n        self.assertEqual(cleansed_post['sensitive_key'], reporter_filter.cleansed_substitute)\n        self.assertEqual(cleansed_post['normal_key'], ['value1', 'value2'])\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n"], "sample_372": ["    def test_locale_regex_descriptor(self):\n        \"\"\"\n        Test that LocaleRegexDescriptor correctly compiles regex based on active language.\n        \"\"\"\n        class MockPattern:\n            regex = LocaleRegexDescriptor('_regex')\n\n                self._regex = regex\n                self._regex_dict = {}\n\n                return re.compile(regex)\n\n        pattern = MockPattern(r'^test/$')\n        self.assertIsInstance(pattern.regex, re.Pattern)\n        self.assertEqual(pattern.regex.pattern, '^test/$')\n", "    def test_pattern_startswith_slash(self):\n        \"\"\"\n        Test that a warning is issued if the pattern starts with a slash\n        and APPEND_SLASH setting is True.\n        \"\"\"\n        pattern = RegexPattern(r'^/test/$')\n        with override_settings(APPEND_SLASH=True):\n            warnings = pattern.check()\n            self.assertEqual(len(warnings), 1)\n            self.assertEqual(warnings[0].id, 'urls.W002')\n        with override_settings(APPEND_SLASH=False):\n            warnings = pattern.check()\n            self.assertEqual(len(warnings), 0)\n", "        def __init__(self, pattern):\n            self._regex = pattern\n            self._regex_dict = {}\n", "    def test_regex_pattern_match(self):\n        pattern = RegexPattern(r'^test/(?P<id>[0-9]+)/$')\n        match = pattern.match('test/123/')\n        self.assertIsNotNone(match)\n        self.assertEqual(match[1], ())\n        self.assertEqual(match[2], {'id': '123'})\n", "    def test_resolver_match_repr(self):\n        \"\"\"\n        Test the __repr__ method of ResolverMatch with various scenarios.\n        \"\"\"\n        func = views.empty_view\n        args = ('42', '37')\n        kwargs = {'arg1': '42', 'arg2': '37'}\n        url_name = 'test-url'\n        app_names = ['app1', 'app2']\n        namespaces = ['ns1', 'ns2']\n        route = '^test/(?P<arg1>[0-9]+)/(?P<arg2>[0-9]+)/$'\n        tried = [['pattern1'], ['pattern2']]\n        \n        match = ResolverMatch(func, args, kwargs, url_name, app_names, namespaces, route, tried)\n        expected_repr = (\n            \"ResolverMatch(func=urlpatterns_reverse.views.empty_view, args=('42', '37'), kwargs={'arg1': '42', 'arg2': '37'}, url_name='test-url', \"\n            \"app_names=['app1', 'app2'], namespaces=['ns1', 'ns2'], route='^test/(?P<arg1>[0-9]+)/(?P<arg2>[0-9]+)/$')\"\n        )\n        self.assertEqual(repr(match), expected_repr)\n"], "sample_373": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_remove_non_capturing_groups(self):\n        self.assertEqual(\n            views.remove_non_capturing_groups(r'^(?:foo)/bar/$'),\n            r'^foo/bar/$'\n        )\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_model_detail_missing_model(self):\n        \"\"\"\n        Test that the ModelDetailView correctly handles a missing model\n        by raising a 404 error.\n        \"\"\"\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'NonExistentModel']))\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, \"Model 'NonExistentModel' not found in app 'admin_docs'\")\n"], "sample_374": ["    def test_deepcopy(self):\n        qs = Book.objects.filter(title=\"Poems\").prefetch_related('authors')\n        qs_copy = copy.deepcopy(qs)\n        self.assertIsInstance(qs_copy, QuerySet)\n        self.assertEqual(list(qs), list(qs_copy))\n", "    def test_iterator_chunked_fetch(self):\n        with self.assertNumQueries(3):\n            books = list(Book.objects.iterator(chunk_size=2))\n            self.assertEqual(books, list(Book.objects.all()))\n", "    def test_create(self):\n        book_count = Book.objects.count()\n        new_book = Book.objects.create(title='New Book')\n        self.assertEqual(Book.objects.count(), book_count + 1)\n        self.assertEqual(new_book.title, 'New Book')\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Magic Book 1')\n        cls.book2 = Book.objects.create(title='Magic Book 2')\n        cls.author1 = Author.objects.create(name='Author 1', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Author 2', first_book=cls.book2)\n        cls.book1.authors.add(cls.author1)\n        cls.book2.authors.add(cls.author2)\n", "    def test_custom_queryset_with_annotations(self):\n        with self.assertNumQueries(2):\n            annotated_qs = Book.objects.annotate(num_authors=Count('authors')).prefetch_related('authors')\n            books = list(annotated_qs)\n            for book in books:\n                self.assertEqual(book.num_authors, book.authors.count())\n"], "sample_375": ["    def test_alter_field_to_fk(self):\n        \"\"\"\n        Alter a non-relation field to a foreign key and check the relations.\n        \"\"\"\n        project_state = self.get_base_project_state()\n        self.assertNotIn(('tests', 'comment'), project_state.relations['tests', 'user'])\n        # Alter a non-relation field to a foreign key.\n        foreign_key = models.ForeignKey('tests.user', models.CASCADE)\n        project_state.alter_field(\n            'tests', 'comment', 'text', foreign_key, preserve_default=True,\n        )\n        self.assertEqual(\n            list(project_state.relations['tests', 'user']),\n            [('tests', 'comment'), ('tests', 'post')],\n        )\n        self.assertEqual(\n            project_state.relations['tests', 'user']['tests', 'comment'],\n            {'text': foreign_key},\n        )\n", "    def test_clone_project_state(self):\n        \"\"\"\n        Test the clone method of ProjectState to ensure it creates an exact copy.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"Tag\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n                (\"hidden\", models.BooleanField()),\n            ],\n        ))\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"SubTag\",\n            fields=[\n                ('tag_ptr', models.OneToOneField(\n                    'migrations.Tag',\n                    models.CASCADE,\n                    auto_created=True,\n                    parent_link=True,\n                    primary_key=True,\n                    to_field='id',\n                    serialize=False,\n                )),\n                (\"awesome\", models.BooleanField()),\n            ],\n            bases=(\"migrations.Tag\",),\n        ))\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"Food\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            managers=[\n                ('default', models.Manager()),\n                ('food_mgr', FoodManager('a', 'b')),\n                ('food_qs', FoodQuerySet.as_manager()),\n            ]\n        ))\n\n        clone_state = project_state.clone()\n\n        # Check that the clone is not the same object as the original\n        self.assertIsNot(clone_state, project_state)\n        self.assertIsNot(clone_state.models, project_state.models)\n\n        # Check that the cloned models are the same as the original models\n        for key, model_state in project_state.models.items():\n            self.assertIn(key, clone_state.models)\n            self.assertEqual(model_state, clone_state.models[key])\n\n        # Check that the clone's apps property is properly cloned\n        self.assertNotEqual(clone_state.apps, project_state.apps)\n        self.assertEqual(clone_state.apps.all_models, project_state.apps.all_models)\n", "    def test_clone_project_state(self):\n        \"\"\"\n        Tests cloning a ProjectState.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"Tag\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n                (\"hidden\", models.BooleanField()),\n            ],\n        ))\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"SubTag\",\n            fields=[\n                ('tag_ptr', models.OneToOneField(\n                    'migrations.Tag',\n                    models.CASCADE,\n                    auto_created=True,\n                    parent_link=True,\n                    primary_key=True,\n                    to_field='id',\n                    serialize=False,\n                )),\n                (\"awesome\", models.BooleanField()),\n            ],\n            bases=(\"migrations.Tag\",),\n        ))\n\n        cloned_state = project_state.clone()\n        self.assertEqual(project_state, cloned_state)\n        self.assertIsNot(project_state, cloned_state)\n        self.assertIsNot(project_state.models, cloned_state.models)\n        self.assertEqual(project_state.models, cloned_state.models)\n\n        # Verify that modifying the cloned state does not affect the original state\n        cloned_state.remove_model('migrations', 'tag')\n        self.assertNotEqual(project_state, cloned_state)\n        self.assertIn(('migrations', 'tag'), project_state.models)\n        self.assertNotIn(('migrations', 'tag'), cloned_state.models)\n", "    def test_add_remove_index(self):\n        \"\"\"\n        Test add_index and remove_index methods on ProjectState.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label='migrations',\n            name='Tag',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=100)),\n            ],\n        ))\n\n        tag_state = project_state.models['migrations', 'tag']\n        index = models.Index(fields=['name'])\n        index.set_name_with_model(tag_state.render(project_state.apps))\n\n        # Add index\n        project_state.add_index('migrations', 'tag', index)\n        self.assertIn(index, project_state.models['migrations', 'tag'].options['indexes'])\n\n        # Remove index\n        project_state.remove_index('migrations', 'tag', index.name)\n        self.assertNotIn(index, project_state.models['migrations', 'tag'].options['indexes'])\n", "    def test_add_constraint(self):\n        \"\"\"\n        Tests adding a constraint to a model and verifying its presence.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"Item\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"price\", models.DecimalField(max_digits=10, decimal_places=2)),\n            ],\n            options={\"constraints\": []},\n        ))\n\n        constraint = models.CheckConstraint(check=models.Q(price__gte=0), name='price_gte_0')\n        project_state.add_constraint(\"migrations\", \"item\", constraint)\n\n        item_state = project_state.models[\"migrations\", \"item\"]\n        self.assertEqual(len(item_state.options['constraints']), 1)\n        self.assertEqual(item_state.options['constraints'][0].name, 'price_gte_0')\n        self.assertEqual(item_state.options['constraints'][0].check, models.Q(price__gte=0))\n"], "sample_376": ["def test_store_empty_messages(self):\n    \"\"\"\n    Ensure that storing an empty list of messages does not set a cookie.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertEqual(response.cookies.get(storage.cookie_name), None)\n", "def test_empty_messages(self):\n    \"\"\"\n    Ensure that storing and retrieving an empty list of messages does not\n    cause any errors and behaves as expected.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    # Add an empty list of messages and update the response\n    storage._store([], response)\n    self.assertEqual(storage._decode(response.cookies[storage.cookie_name].value), [])\n    self.assertEqual(self.stored_messages_count(storage, response), 0)\n\n    # Ensure the cookie is deleted when there are no messages\n    storage.update(response)\n    self.assertEqual(response.cookies[storage.cookie_name].value, '')\n    self.assertEqual(response.cookies[storage.cookie_name]['max-age'], 0)\n", "    def test_empty_message_storage(self):\n        \"\"\"\n        Test that storing no messages results in the cookie being deleted.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.update(response)\n        self.assertEqual(response.cookies['messages'].value, '')\n        self.assertEqual(response.cookies['messages']['domain'], '.example.com')\n        self.assertEqual(response.cookies['messages']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(response.cookies['messages']['samesite'], settings.SESSION_COOKIE_SAMESITE)\n", "    def test_message_serializer(self):\n        \"\"\"\n        Ensure that the MessageSerializer correctly serializes and deserializes\n        messages, preserving their content and type.\n        \"\"\"\n        serializer = MessageSerializer()\n        original_messages = [\n            Message(constants.INFO, 'info message'),\n            Message(constants.WARNING, 'warning message with tags', extra_tags='tag1 tag2'),\n            mark_safe('<b>safe message</b>'),\n        ]\n        # Serialize the messages\n        serialized = serializer.dumps(original_messages)\n        # Deserialize the messages\n        deserialized = serializer.loads(serialized)\n\n        # Check that the deserialized messages match the original messages\n        self.assertEqual(len(original_messages), len(deserialized))\n        for original, deserialized in zip(original_messages, deserialized):\n            if isinstance(original, Message):\n                self.assertEqual(original.level, deserialized.level)\n                self.assertEqual(original.message, deserialized.message)\n                self.assertEqual(original.extra_tags, deserialized.extra_tags)\n            else:\n                self.assertEqual(original, deserialized)\n                self.assertIsInstance(deserialized, SafeData)\n", "    def test_empty_messages(self):\n        \"\"\"\n        Storing and retrieving an empty message list should not set any cookie.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        \n        # Attempt to store an empty message list\n        storage.update(response)\n        \n        # No cookie should be set\n        self.assertNotIn('messages', response.cookies)\n        \n        # The stored messages count should be zero\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n"], "sample_377": ["    def test_cleanse_setting_callable(self):\n        \"\"\"\n        Ensure that callables within settings are wrapped with CallableSettingWrapper.\n        \"\"\"\n            return \"Sensitive Data\"\n\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed_value = reporter_filter.cleanse_setting(\"CALLABLE_SETTING\", sample_callable)\n        self.assertIsInstance(cleansed_value, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed_value), repr(sample_callable))\n", "    def test_cleanse_setting_with_callable(self):\n            return \"Sensitive Data\"\n\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed = reporter_filter.cleanse_setting(\"CALLABLE_SETTING\", sensitive_callable)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertNotIn(\"Sensitive Data\", repr(cleansed))\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_cleanse_setting_callable(self):\n        \"\"\"Ensure callables are wrapped correctly\"\"\"\n        reporter_filter = SafeExceptionReporterFilter()\n\n            return \"secret\"\n\n        cleansed = reporter_filter.cleanse_setting(\"CALLABLE_SETTING\", callable_setting)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed), repr(callable_setting))\n", "    def setUp(self):\n        self.rf = RequestFactory()\n"], "sample_378": ["def test_bulk_update_with_custom_prefetch_related(self):\n    # Create some related objects\n    related_objects = [RelatedObject.objects.create() for _ in range(5)]\n    # Create some main objects and assign related objects\n    single_objects = [SingleObject.objects.create(related=related_objects[i % 5]) for i in range(10)]\n    \n    for single in single_objects:\n        single.related = related_objects[(single.pk + 1) % 5]\n    \n    with self.assertNumQueries(1):\n        SingleObject.objects.bulk_update(single_objects, ['related'])\n    \n    for single in single_objects:\n        self.assertEqual(single.related, RelatedObject.objects.get(pk=(single.pk + 1) % 5))\n    \n    # Ensure the prefetch_related works correctly\n    with self.assertNumQueries(2):\n        objs = SingleObject.objects.prefetch_related('related')\n        for obj in objs:\n            self.assertEqual(obj.related, RelatedObject.objects.get(pk=(obj.pk + 1) % 5))\n", "    def test_bulk_create_with_ignore_conflicts(self):\n        notes = [\n            Note.objects.create(note='test_note_1', misc='test_misc_1'),\n            Note.objects.create(note='test_note_2', misc='test_misc_2')\n        ]\n        new_notes = [\n            Note(note='test_note_1', misc='new_misc_1'),  # Conflict with existing note\n            Note(note='test_note_3', misc='new_misc_3')   # No conflict\n        ]\n        Note.objects.bulk_create(new_notes, ignore_conflicts=True)\n        self.assertEqual(Note.objects.filter(note='test_note_3').count(), 1)\n        self.assertEqual(Note.objects.filter(note='test_note_1').count(), 1)\n", "def test_bulk_update_with_mixed_field_types(self):\n    articles = [\n        Article.objects.create(\n            name=str(i),\n            created=datetime.datetime.today(),\n            content=f\"Content {i}\",\n            published=True if i % 2 == 0 else False,\n        )\n        for i in range(10)\n    ]\n    new_content = \"Updated Content\"\n    for article in articles:\n        article.content = new_content\n        article.published = not article.published\n\n    Article.objects.bulk_update(articles, ['content', 'published'])\n    \n    self.assertCountEqual(\n        Article.objects.filter(content=new_content).values_list('published', flat=True),\n        [not article.published for article in articles]\n    )\n", "    def test_query_set_repr(self):\n        notes = [Note.objects.create(note='test_note', misc='test_misc') for _ in range(5)]\n        queryset = Note.objects.all()\n        self.assertIn(repr(queryset), f\"<QuerySet [{', '.join([repr(note) for note in notes])}]>\")\n", "    def test_prefetch_related(self):\n        self.create_tags()\n        for note, tag in zip(self.notes, self.tags):\n            note.tag = tag\n        Note.objects.bulk_update(self.notes, ['tag'])\n        with self.assertNumQueries(1):\n            notes_with_tags = list(Note.objects.prefetch_related('tag'))\n        for note in notes_with_tags:\n            self.assertIsNotNone(note.tag)\n        self.assertEqual(len(notes_with_tags), len(self.notes))\n"], "sample_379": ["    def test_safe_string_concat_with_unsafe_string(self):\n        \"\"\"\n        Concatenating a SafeString with a regular string results in an unsafe string.\n        \"\"\"\n        safe_str = mark_safe('safe')\n        unsafe_str = 'unsafe'\n        result = safe_str + unsafe_str\n\n        self.assertNotIsInstance(result, SafeString)\n        self.assertEqual(result, 'safeunsafe')\n", "def test_safe_string_addition_with_non_safe_string(self):\n        \"\"\"\n        Ensure that adding a SafeString to a non-safe string results in a non-safe string.\n        \"\"\"\n        safe_str = mark_safe('safe')\n        non_safe_str = 'non-safe'\n        result = safe_str + non_safe_str\n        self.assertIsInstance(result, str)\n        self.assertNotIsInstance(result, SafeData)\n        self.assertEqual(result, 'safenon-safe')\n", "def test_add_safe_string_and_unsafe_string(self):\n    \"\"\"\n    Concatenating a SafeString with a regular string results in an unsafe string.\n    \"\"\"\n    s1 = mark_safe('safe')\n    s2 = 'unsafe'\n    result = s1 + s2\n    self.assertNotIsInstance(result, SafeData)\n    self.assertEqual(result, 'safeunsafe')\n", "def test_safe_string_concatenation(self):\n    \"\"\"\n    Test concatenation of SafeString with other strings and SafeData objects.\n    \"\"\"\n    safe_str1 = SafeString('safe1')\n    safe_str2 = SafeString('safe2')\n    regular_str = 'unsafe'\n    \n    # Concatenate SafeString with another SafeString\n    result = safe_str1 + safe_str2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, 'safe1safe2')\n\n    # Concatenate SafeString with a regular string\n    result = safe_str1 + regular_str\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, 'safe1unsafe')\n\n    # Concatenate SafeString with SafeData that is not SafeString\n    class OtherSafeData(SafeData):\n            return 'other_safe'\n    \n    other_safe = OtherSafeData()\n    result = safe_str1 + other_safe\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, 'safe1other_safe')\n", "def test_safe_string_concatenation_with_non_safe_string(self):\n    s1 = mark_safe('a&b')\n    s2 = 'c<d'\n    result = s1 + s2\n\n    self.assertNotIsInstance(result, SafeData)\n    self.assertEqual(result, 'a&bc<d')\n"], "sample_380": ["    def test_aggregate_with_filter(self):\n        \"\"\"\n        Test the application of filter within an aggregate function.\n        \"\"\"\n        vals = Book.objects.aggregate(\n            rating_sum=Sum('rating', filter=Q(publisher=self.p1))\n        )\n        self.assertEqual(vals, {'rating_sum': Decimal('8.5')})\n\n        vals = Publisher.objects.filter(name='Apress').aggregate(\n            total_awards=Sum('num_awards', filter=Q(duration__isnull=False))\n        )\n        self.assertEqual(vals, {'total_awards': 3})\n\n        vals = Author.objects.aggregate(\n            total_friends=Count('friends', filter=Q(age__gt=30))\n        )\n        self.assertEqual(vals, {'total_friends': 5})\n\n        vals = Store.objects.aggregate(\n            total_books=Count('books', filter=Q(name__contains='Books'))\n        )\n        self.assertEqual(vals, {'total_books': 7})\n", "    def test_aggregate_with_distinct(self):\n        # Test distinct aggregation for Avg\n        vals = Book.objects.aggregate(avg_distinct_rating=Avg('rating', distinct=True))\n        self.assertEqual(vals, {\"avg_distinct_rating\": 4.125})\n\n        # Test distinct aggregation for Sum\n        vals = Book.objects.aggregate(sum_distinct_price=Sum('price', distinct=True))\n        self.assertEqual(vals, {\"sum_distinct_price\": Decimal('270.27')})\n\n        # Test distinct aggregation for Count\n        vals = Book.objects.aggregate(count_distinct_pages=Count('pages', distinct=True))\n        self.assertEqual(vals, {\"count_distinct_pages\": 6})\n\n        # Test distinct aggregation for Max\n        vals = Book.objects.aggregate(max_distinct_rating=Max('rating', distinct=True))\n        self.assertEqual(vals, {\"max_distinct_rating\": 5.0})\n\n        # Test distinct aggregation for Min\n        vals = Book.objects.aggregate(min_distinct_rating=Min('rating', distinct=True))\n        self.assertEqual(vals, {\"min_distinct_rating\": 3.0})\n\n        # Test distinct aggregation for StdDev\n        vals = Book.objects.aggregate(stddev_distinct_rating=StdDev('rating', distinct=True))\n        self.assertAlmostEqual(vals['stddev_distinct_rating'], Decimal('0.82915619758885'), places=5)\n\n        # Test distinct aggregation for Variance\n        vals = Book.objects.aggregate(variance_distinct_rating=Variance('rating', distinct=True))\n        self.assertAlmostEqual(vals['variance_distinct_rating'], Decimal('0.6875'), places=4)\n", "    def test_aggregate_with_case_expression(self):\n        # Test Sum with Case-When expression\n        vals = Author.objects.annotate(\n            award_points=Sum(Case(\n                When(age__gt=30, then=2),\n                default=1,\n                output_field=IntegerField()\n            ))\n        ).aggregate(total_award_points=Sum('award_points'))\n        self.assertEqual(vals['total_award_points'], 15)\n", "def test_aggregate_with_default_alias(self):\n    msg = \"Complex expressions require an alias\"\n    with self.assertRaisesMessage(TypeError, msg):\n        Author.objects.aggregate(Avg(F('age') + F('friends__age')))\n    with self.assertRaisesMessage(TypeError, msg):\n        Author.objects.annotate(avg_age=Avg(F('age') + F('friends__age')))\n", "    def test_aggregate_filter(self):\n        # Test that the filter argument in the Aggregate class works correctly.\n        vals = Author.objects.aggregate(\n            num_young_authors=Count('id', filter=Q(age__lt=30)),\n            num_old_authors=Count('id', filter=Q(age__gte=30))\n        )\n        self.assertEqual(vals, {\n            'num_young_authors': 4,  # Authors with age < 30: James Bennett, Paul Bissex, Wesley J. Chun, and Peter Norvig\n            'num_old_authors': 5     # Authors with age >= 30: Adrian Holovaty, Jacob Kaplan-Moss, Brad Dayley, Jeffrey Forcier, and Stuart Russell\n        })\n"], "sample_381": ["    def test_create_model_with_constraint_and_index(self):\n        \"\"\"Test creation of new model with both constraints and indexes already defined.\"\"\"\n        author = ModelState('otherapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ], {\n            'indexes': [models.Index(fields=['name'], name='create_model_with_indexes_idx')],\n            'constraints': [models.CheckConstraint(check=models.Q(name__contains='Bob'), name='name_contains_bob')],\n        })\n        changes = self.get_changes([], [author])\n        added_index = models.Index(fields=['name'], name='create_model_with_indexes_idx')\n        added_constraint = models.CheckConstraint(check=models.Q(name__contains='Bob'), name='name_contains_bob')\n        # Right number of migrations?\n        self.assertEqual(len(changes['otherapp']), 1)\n        # Right number of actions?\n        migration = changes['otherapp'][0]\n        self.assertEqual(len(migration.operations), 3)\n        # Right actions order?\n        self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel', 'AddIndex', 'AddConstraint'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Author')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, model_name='author', index=added_index)\n        self.assertOperationAttributes(changes, 'otherapp', 0, 2, model_name='author', constraint=added_constraint)\n", "    def test_deep_deconstruct_partial_function(self):\n        \"\"\"\n        Tests the deep deconstruction of a functools.partial function.\n        \"\"\"\n            return a + b + c + d\n        \n        partial_func = functools.partial(sample_function, 1, d=5)\n        \n        before = ModelState(\"testapp\", \"SampleModel\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"partial_field\", models.IntegerField(default=partial_func)),\n        ])\n        after = ModelState(\"testapp\", \"SampleModel\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"partial_field\", models.IntegerField(default=partial_func)),\n        ])\n        \n        changes = self.get_changes([before], [after])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 0)\n", "def test_alter_field_with_functional_default(self):\n        \"\"\"Tests detection of altering fields with a functional default value.\"\"\"\n        before = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"created_at\", models.DateTimeField(default=timezone.now)),\n        ])\n        after = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"created_at\", models.DateTimeField(default=timezone.utcnow)),\n        ])\n        changes = self.get_changes([before], [after])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"created_at\", preserve_default=True)\n", "    def test_alter_field_unique_with_callable_default(self):\n        \"\"\"\n        Tests altering a field to be unique with a callable default.\n        \"\"\"\n        before = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n        after = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, unique=True, default=lambda: \"unique_default\")),\n        ])\n        changes = self.get_changes([before], [after], MigrationQuestioner({\"ask_unique_callable_default_addition\": True}))\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True)\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, unique=True)\n", "    def test_generate_altered_db_table(self):\n        \"\"\"\n        Tests the generation of an AlterModelTable operation when the db_table option is altered.\n        \"\"\"\n        changes = self.get_changes([self.author_with_db_table_options], [self.author_with_new_db_table_options])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=\"author_two\")\n\n        changes = self.get_changes([self.author_with_db_table_options], [self.author_empty])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=None)\n"], "sample_382": ["    def test_template_dirs_with_cached_loader(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates_extra',\n            }\n        )\n", "    def test_template_changed_ignored_django_path(self, mock_reset, mock_is_django_path):\n        template_path = Path(__file__).parent / 'templates' / 'index.html'\n        self.assertIsNone(autoreload.template_changed(None, template_path))\n        mock_reset.assert_not_called()\n", "    def test_template_dirs_exclude_django_paths(self, mock_is_django_path):\n        template_dirs = autoreload.get_template_directories()\n        self.assertFalse(any(is_django_path(dir) for dir in template_dirs))\n        mock_is_django_path.assert_called()\n", "    def test_template_changed_with_django_path(self, mock_reset, mock_is_django_path):\n        template_path = Path(__file__).parent / 'templates' / 'index.html'\n        self.assertIsNone(autoreload.template_changed(None, template_path))\n        mock_reset.assert_not_called()\n", "    def test_reset_specific_loader(self, mock_reset):\n        autoreload.reset_loaders()\n        mock_reset.assert_called_once()\n"], "sample_383": ["def test_related_in_lookup(self):\n    \"\"\"\n    Test for the RelatedIn class to ensure it properly handles multi-column relations\n    and value normalization.\n    \"\"\"\n    c1 = CustomPk.objects.create(name=\"custom1\", extra=\"extra1\")\n    c2 = CustomPk.objects.create(name=\"custom2\", extra=\"extra2\")\n    rel1 = Related.objects.create(custom=c1)\n    rel2 = Related.objects.create(custom=c2)\n    \n    # Ensure that the RelatedIn class normalizes values correctly and handles queries\n    # involving multi-column relations.\n    qs = Related.objects.filter(custom__in=[c1, c2])\n    self.assertQuerysetEqual(qs.order_by('id'), [rel1, rel2], lambda x: x)\n    \n    # Test a case where we are excluding certain multi-column relations using RelatedIn.\n    qs_exclude = Related.objects.exclude(custom__in=[c1])\n    self.assertQuerysetEqual(qs_exclude.order_by('id'), [rel2], lambda x: x)\n    \n    # Ensure that using a queryset for the 'in' lookup works as expected.\n    qs_queryset = Related.objects.filter(custom__in=CustomPk.objects.filter(name__in=[\"custom1\", \"custom2\"]))\n    self.assertQuerysetEqual(qs_queryset.order_by('id'), [rel1, rel2], lambda x: x)\n", "    def test_related_in_multicolsource(self):\n        # Test RelatedIn with MultiColSource\n        lhs = MultiColSource(alias=\"alias\", targets=[\"target1\", \"target2\"], sources=[\"source1\", \"source2\"], field=\"field\")\n        rhs = [(\"value1\", \"value2\"), (\"value3\", \"value4\")]\n        \n        related_in = RelatedIn(lhs, rhs)\n        self.assertEqual(related_in.get_prep_lookup(), None)\n        self.assertEqual(related_in.lhs, lhs)\n        self.assertEqual(related_in.rhs, rhs)\n        \n        from django.db.models.sql.where import WhereNode, AND, OR\n        \n        compiler = Mock()\n        connection = Mock()\n        \n        sql, params = related_in.as_sql(compiler, connection)\n        self.assertIsInstance(sql, str)\n        self.assertIsInstance(params, list)\n        self.assertIn(\"alias\", sql)\n        self.assertIn(\"target1\", sql)\n        self.assertIn(\"target2\", sql)\n        self.assertIn(\"source1\", sql)\n        self.assertIn(\"source2\", sql)\n        self.assertIn(\"value1\", params)\n        self.assertIn(\"value2\", params)\n        self.assertIn(\"value3\", params)\n        self.assertIn(\"value4\", params)\n", "    def test_get_normalized_value_with_unsaved_model(self):\n        # Test get_normalized_value with an unsaved model instance\n        n = Note()\n        with self.assertWarnsMessage(RemovedInDjango50Warning, \"Passing unsaved model instances to related filters is deprecated.\"):\n            result = get_normalized_value(n, None)\n        self.assertEqual(result, (None,))\n", "    def test_get_normalized_value(self):\n        \"\"\"\n        Test the behavior of `get_normalized_value` function.\n        \"\"\"\n        # Create an instance of the necessary classes\n        author = Author.objects.create(name=\"test_author\")\n        book = Book.objects.create(title=\"test_book\", author=author)\n\n        # Case 1: value is a Model instance with pk\n        normalized_value = get_normalized_value(author, book._meta.get_field(\"author\"))\n        self.assertEqual(normalized_value, (author.pk,))\n\n        # Case 2: value is a Model instance without pk\n        unsaved_author = Author(name=\"unsaved_author\")\n        with self.assertWarnsMessage(RemovedInDjango50Warning, \"Passing unsaved model instances to related filters is deprecated.\"):\n            normalized_value = get_normalized_value(unsaved_author, book._meta.get_field(\"author\"))\n        self.assertEqual(normalized_value, (None,))\n\n        # Case 3: value is a tuple\n        normalized_value = get_normalized_value((1, 2), book._meta.get_field(\"author\"))\n        self.assertEqual(normalized_value, (1, 2))\n\n        # Case 4: value is not a tuple or Model instance\n        normalized_value = get_normalized_value(3, book._meta.get_field(\"author\"))\n        self.assertEqual(normalized_value, (3,))\n", "    def setUp(self):\n        self.alias = 'alias'\n        self.targets = ['target1', 'target2']\n        self.sources = ['source1', 'source2']\n        self.field = 'field'\n"], "sample_384": ["def test_update_with_empty_field_list(self):\n    note = Note.objects.create(note=\"initial\")\n    note.note = \"updated\"\n    with self.assertRaisesMessage(ValueError, \"Field names must be given to bulk_update().\"):\n        Note.objects.bulk_update([note], fields=[])\n", "    def setUpTestData(cls):\n        cls.notes = [Note.objects.create(note=str(i), misc=str(i)) for i in range(10)]\n", "    def setUp(self):\n        self.notes = [Note.objects.create(note=str(i), misc=str(i)) for i in range(5)]\n", "    def setUp(self):\n        self.notes = [Note.objects.create(note=str(i), misc=str(i)) for i in range(10)]\n", "    def setUp(self):\n        self.note1 = Note.objects.create(note=\"note1\", misc=\"misc1\")\n        self.note2 = Note.objects.create(note=\"note2\", misc=\"misc2\")\n        self.queryset = Note.objects.all()\n"], "sample_385": ["    def test_get_select2_language(self):\n        with translation.override('fr'):\n            self.assertEqual(get_select2_language(), 'fr')\n        with translation.override('zh-hans'):\n            self.assertEqual(get_select2_language(), 'zh-CN')\n        with translation.override('zh-hant'):\n            self.assertEqual(get_select2_language(), 'zh-TW')\n        with translation.override('zh-hant-tw'):\n            self.assertEqual(get_select2_language(), 'zh-TW')\n        with translation.override('unknown-lang'):\n            self.assertIsNone(get_select2_language())\n", "    def test_get_select2_language(self):\n        original_language = translation.get_language()\n        try:\n            for lang_code, expected in SELECT2_TRANSLATIONS.items():\n                translation.activate(lang_code)\n                self.assertEqual(get_select2_language(), expected)\n        finally:\n            translation.activate(original_language)\n", "    def test_optgroups(self):\n        beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n        who = Band.objects.create(name=\"The Who\", style=\"rock\")\n        form = AlbumForm()\n        widget = form[\"band\"].field.widget\n        name = \"band\"\n        value = [str(beatles.uuid)]\n        optgroups = widget.optgroups(name, value)\n        self.assertEqual(len(optgroups), 1)\n        self.assertEqual(len(optgroups[0][1]), 1)\n        self.assertEqual(optgroups[0][1][0][\"value\"], str(beatles.uuid))\n        self.assertEqual(optgroups[0][1][0][\"label\"], \"The Beatles\")\n        self.assertTrue(optgroups[0][1][0][\"selected\"])\n", "def test_related_field_widget_wrapper_context(self):\n        rel = Album._meta.get_field(\"band\").remote_field\n        wrapper = RelatedFieldWidgetWrapper(\n            widget=forms.TextInput(),\n            rel=rel,\n            admin_site=admin.site,\n            can_add_related=True,\n            can_change_related=True,\n            can_delete_related=True,\n            can_view_related=True,\n        )\n        context = wrapper.get_context(name=\"band\", value=None, attrs={})\n        self.assertIn(\"add_related_url\", context)\n        self.assertIn(\"change_related_template_url\", context)\n        self.assertIn(\"delete_related_template_url\", context)\n        self.assertEqual(context[\"can_add_related\"], True)\n        self.assertEqual(context[\"can_change_related\"], True)\n        self.assertEqual(context[\"can_delete_related\"], True)\n        self.assertEqual(context[\"can_view_related\"], True)\n        self.assertEqual(context[\"model\"], \"album\")\n        self.assertEqual(context[\"name\"], \"band\")\n", "    def test_get_context_with_related_url(self):\n        beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n        rel = Album._meta.get_field(\"band\").remote_field\n        w = AutocompleteSelect(rel, admin.site)\n        context = w.get_context(name=\"band\", value=beatles.pk, attrs={})\n        self.assertEqual(context[\"widget\"][\"attrs\"][\"data-ajax--url\"], \"/autocomplete/\")\n        self.assertEqual(context[\"widget\"][\"attrs\"][\"data-app-label\"], \"admin_widgets\")\n        self.assertEqual(context[\"widget\"][\"attrs\"][\"data-model-name\"], \"album\")\n        self.assertEqual(context[\"widget\"][\"attrs\"][\"data-field-name\"], \"band\")\n        self.assertEqual(context[\"widget\"][\"attrs\"][\"class\"], \"admin-autocomplete\")\n"], "sample_386": ["def test_safe_string_concat_with_unsafe_string(self):\n        \"\"\"\n        Concatenating a SafeString with an unsafe string results in an unsafe string.\n        \"\"\"\n        s = mark_safe(\"a&b\")\n        result = s + \"unsafe\"\n        self.assertIsInstance(result, str)\n        self.assertNotIsInstance(result, SafeData)\n        self.assertEqual(result, \"a&bunsafe\")\n", "def test_safe_string_concatenation(self):\n    \"\"\"\n    Test concatenation of SafeString with SafeData and regular strings.\n    \"\"\"\n    safe_str1 = mark_safe(\"Hello\")\n    safe_str2 = mark_safe(\" World\")\n    regular_str = \"!\"\n    \n    # Concatenation with another SafeString\n    result = safe_str1 + safe_str2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, \"Hello World\")\n    \n    # Concatenation with a regular string\n    result = safe_str1 + regular_str\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, \"Hello!\")\n\n    # Concatenation with SafeData\n    class SafeDataSubClass(SafeData, str):\n            return str.__new__(cls, *args, **kwargs)\n\n    safe_data_instance = SafeDataSubClass(\" Data\")\n    result = safe_str1 + safe_data_instance\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, \"Hello Data\")\n", "    def test_safe_string_concat(self):\n        \"\"\"\n        Concatenating a SafeString with another SafeString should produce a SafeString.\n        Concatenating a SafeString with a regular string should produce a regular string.\n        \"\"\"\n        safe_str1 = mark_safe(\"Hello\")\n        safe_str2 = mark_safe(\" World\")\n        regular_str = \" Regular String\"\n\n        result_safe = safe_str1 + safe_str2\n        result_regular = safe_str1 + regular_str\n\n        self.assertIsInstance(result_safe, SafeString)\n        self.assertNotIsInstance(result_regular, SafeString)\n        self.assertEqual(result_safe, \"Hello World\")\n        self.assertEqual(result_regular, \"Hello Regular String\")\n", "    def test_safe_string_addition(self):\n        \"\"\"\n        Test the addition of SafeString instances and verify the safe status.\n        \"\"\"\n        s1 = mark_safe(\"safe1\")\n        s2 = mark_safe(\"safe2\")\n        result = s1 + s2\n        self.assertIsInstance(result, SafeString)\n        self.assertEqual(result, \"safe1safe2\")\n\n        s3 = \"unsafe\"\n        result = s1 + s3\n        self.assertNotIsInstance(result, SafeString)\n        self.assertEqual(result, \"safe1unsafe\")\n", "def test_safe_string_addition_with_non_safe_string(self):\n    s1 = SafeString(\"safe\")\n    s2 = \"unsafe\"\n    result = s1 + s2\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, \"safeunsafe\")\n"], "sample_387": ["    def test_get_list_display(self):\n        model_admin = self.TestModelAdmin(Profile, admin.site)\n        self.assertEqual(model_admin.get_list_display(request=None), (\"name\", \"age\"))\n", "    def test_view_on_site_url_not_callable(self):\n        \"\"\"\n        Test that get_view_on_site_url returns correct URL when view_on_site is not callable.\n        \"\"\"\n        from django.contrib.contenttypes.models import ContentType\n\n        class MyModel:\n            pk = 1\n\n                return \"/some-url/\"\n\n        class MyModelAdmin(admin.ModelAdmin):\n            view_on_site = True\n\n        ma = MyModelAdmin(MyModel, admin.site)\n\n        obj = MyModel()\n        with self.settings(ROOT_URLCONF='django.contrib.admin.urls'):\n            url = ma.get_view_on_site_url(obj)\n            self.assertEqual(url, \"/admin/view_on_site/%d/%d/\" % (ContentType.objects.get_for_model(MyModel).pk, obj.pk))\n", "    def test_UUIDField(self):\n        self.assertFormfield(Bee, \"honeycomb\", widgets.AdminUUIDInputWidget)\n", "    def test_get_content_type_for_model(self):\n        \"\"\"\n        Test get_content_type_for_model utility function.\n        \"\"\"\n        from django.contrib.contenttypes.models import ContentType\n\n        content_type = get_content_type_for_model(Event)\n        self.assertEqual(content_type, ContentType.objects.get_for_model(Event, for_concrete_model=False))\n", "    def test_to_field_allowed_primary_key(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            pass\n\n        admin_site = admin.AdminSite()\n        ma = MyModelAdmin(Member, admin_site)\n        request = self.client.request().wsgi_request\n        self.assertTrue(ma.to_field_allowed(request, \"id\"))\n"], "sample_388": ["    def test_clean_username_method(self):\n        \"\"\"\n        Test that the clean_username method correctly processes the username.\n        \"\"\"\n        backend = CustomRemoteUserBackend()\n        self.assertEqual(backend.clean_username(\"username@example.com\"), \"username\")\n        self.assertEqual(backend.clean_username(\"user@domain.com\"), \"user\")\n        self.assertEqual(backend.clean_username(\"testuser\"), \"testuser\")\n", "    def test_inactive_user(self):\n        \"\"\"Inactive users should still be authenticated.\"\"\"\n        user = User.objects.create(username=\"knownuser\", is_active=False)\n        response = self.client.get(\"/remote_user/\", **{self.header: self.known_user})\n        self.assertEqual(response.context[\"user\"].username, user.username)\n", "    def setUp(self):\n        self.backend = ModelBackend()\n        self.user = UserModel.objects.create_user(username='testuser', password='password', is_active=True)\n        self.inactive_user = UserModel.objects.create_user(username='inactiveuser', password='password', is_active=False)\n", "def test_clean_username(self):\n    \"\"\"\n    Test the clean_username method of CustomRemoteUserBackend.\n    \"\"\"\n    backend = CustomRemoteUserBackend()\n    self.assertEqual(backend.clean_username(\"user@example.com\"), \"user\")\n    self.assertEqual(backend.clean_username(\"anotheruser@domain.com\"), \"anotheruser\")\n    self.assertEqual(backend.clean_username(\"simpleuser\"), \"simpleuser\")\n", "    def setUp(self):\n        self.patched_settings = modify_settings(\n            AUTHENTICATION_BACKENDS={\"append\": self.backend}\n        )\n        self.patched_settings.enable()\n"], "sample_389": ["    def test_get_raw_host_with_forwarded_host(self):\n        req = HttpRequest()\n        req.META = {\n            'HTTP_X_FORWARDED_HOST': 'forwarded.example.com',\n            'HTTP_HOST': 'host.example.com',\n            'SERVER_NAME': 'server.example.com',\n            'SERVER_PORT': '80',\n        }\n        with self.settings(USE_X_FORWARDED_HOST=True):\n            self.assertEqual(req._get_raw_host(), 'forwarded.example.com')\n        ", "    def test_get_raw_host_with_forwarded_host(self):\n        with override_settings(USE_X_FORWARDED_HOST=True):\n            req = HttpRequest()\n            req.META[\"HTTP_X_FORWARDED_HOST\"] = \"forwarded.example.com\"\n            self.assertEqual(req._get_raw_host(), \"forwarded.example.com\")\n", "    def test_get_full_path(self):\n        req = HttpRequest()\n        req.path = \"/test/path\"\n        req.META[\"QUERY_STRING\"] = \"key=value\"\n        self.assertEqual(req.get_full_path(), \"/test/path?key=value\")\n", "    def test_get_host(self):\n        req = HttpRequest()\n        req.META['HTTP_HOST'] = 'example.com'\n        with self.settings(ALLOWED_HOSTS=['example.com']):\n            self.assertEqual(req.get_host(), 'example.com')\n", "    def test_get_host_with_valid_host(self):\n        req = HttpRequest()\n        req.META = {\n            'HTTP_HOST': 'validhost.com',\n            'SERVER_NAME': 'server.com',\n            'SERVER_PORT': '80'\n        }\n        with self.settings(ALLOWED_HOSTS=['validhost.com']):\n            self.assertEqual(req.get_host(), 'validhost.com')\n"], "sample_390": ["    def test_directory_index_no_show_indexes(self):\n        \"The static view should raise Http404 for directories if show_indexes is False\"\n        response = self.client.get(\"/%s/subdir/\" % self.prefix, follow=True)\n        self.assertEqual(response.status_code, 404)\n", "    def test_directory_index_template_fallback(self):\n        \"\"\"Test that the directory index falls back to the default template if custom template is missing\"\"\"\n        with override_settings(TEMPLATES=[]):\n            response = self.client.get(\"/%s/subdir/\" % self.prefix)\n            self.assertContains(response, \"Index of subdir/\")\n            # Ensure that the response contains content from the default template\n            self.assertIn(\"meta name=\\\"robots\\\" content=\\\"NONE,NOARCHIVE\\\"\", response.content.decode())\n", "    def test_directory_index_template_fallback(self):\n        \"\"\"Test that the fallback directory index template is used if custom template is not found.\"\"\"\n        with override_settings(TEMPLATES=[\n            {\n                \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                \"OPTIONS\": {\n                    \"loaders\": [\n                        (\n                            \"django.template.loaders.locmem.Loader\",\n                            {\n                                # Intentionally leaving out \"static/directory_index.html\"\n                            },\n                        ),\n                    ],\n                },\n            }\n        ]):\n            response = self.client.get(\"/%s/\" % self.prefix)\n            self.assertContains(response, \"Index of ./\")\n            # Verify that fallback template content is used\n            self.assertIn(\"<title>Index of ./</title>\", response.content.decode())\n", "    def test_serve_directory_no_index(self):\n        \"\"\"Test serving a directory with show_indexes=False raises Http404\"\"\"\n        with self.assertRaises(Http404):\n            self.client.get(\"/%s/subdir\" % self.prefix)\n", "    def test_directory_index_with_hidden_files(self):\n        \"Ensure hidden files are not listed in the directory index\"\n        hidden_file_path = path.join(media_dir, \".hidden_file.txt\")\n        with open(hidden_file_path, \"w\") as hidden_file:\n            hidden_file.write(\"This is a hidden file\")\n\n        response = self.client.get(\"/%s/\" % self.prefix)\n        self.assertContains(response, \"Index of ./\")\n        self.assertNotIn(\".hidden_file.txt\", response.context[\"file_list\"])\n\n        # Clean up\n        if path.exists(hidden_file_path):\n            path.remove(hidden_file_path)\n"], "sample_391": ["    def test_create_model_add_index(self):\n        \"\"\"\n        AddIndex should optimize into CreateModel.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                ),\n                migrations.AddIndex(\n                    \"Foo\",\n                    models.Index(fields=[\"name\"], name=\"foo_name_idx\"),\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\n                        \"indexes\": [models.Index(fields=[\"name\"], name=\"foo_name_idx\")]\n                    },\n                ),\n            ],\n        )\n", "    def test_add_index_to_create_model(self):\n        \"\"\"\n        AddIndex should optimize into CreateModel.\n        \"\"\"\n        index = models.Index(fields=['name'], name='name_idx')\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                ),\n                migrations.AddIndex(\"Foo\", index),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"indexes\": [index]},\n                ),\n            ],\n        )\n", "    def test_alter_model_table(self):\n        \"\"\"\n        AlterModelTable should optimize into CreateModel.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                ),\n                migrations.AlterModelTable(\"Foo\", \"new_table_name\"),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\", \"db_table\": \"new_table_name\"},\n                    bases=(UnicodeModel,),\n                ),\n            ],\n        )\n", "    def test_create_model_add_index(self):\n        \"\"\"\n        AddIndex should optimize into CreateModel.\n        \"\"\"\n        index = models.Index(fields=[\"name\"], name=\"name_idx\")\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                ),\n                migrations.AddIndex(\"Foo\", index),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"indexes\": [index]},\n                ),\n            ],\n        )\n", "    def test_create_model_rename_field_add_field(self):\n        \"\"\"\n        RenameField followed by AddField should optimize into a single CreateModel.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"description\", models.TextField()),\n                    ],\n                ),\n                migrations.RenameField(\"Foo\", \"name\", \"title\"),\n                migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"title\", models.CharField(max_length=255)),\n                        (\"description\", models.TextField()),\n                        (\"age\", models.IntegerField()),\n                    ],\n                ),\n            ],\n        )\n"], "sample_392": ["    def test_compile_json_path_root(self):\n        self.assertEqual(compile_json_path([], include_root=True), \"$\")\n    ", "    def test_key_transform(self):\n        key_transform = KeyTransform('key_name')\n        self.assertEqual(key_transform.key_name, 'key_name')\n", "    def test_compile_json_path(self):\n        self.assertEqual(compile_json_path(['a', 'b', 'c']), '$.\"a\".\"b\".\"c\"')\n        self.assertEqual(compile_json_path([1, 'a', 2]), '$[1].\"a\"[2]')\n        self.assertEqual(compile_json_path(['a', '1b', 2]), '$.\"a\".\"1b\"[2]')\n        self.assertEqual(compile_json_path(['a', 'b', 'c'], include_root=False), '.\"a\".\"b\".\"c\"')\n        self.assertEqual(compile_json_path([1, 'a', 2], include_root=False), '[1].\"a\"[2]')\n        self.assertEqual(compile_json_path(['a', '1b', 2], include_root=False), '.\"a\".\"1b\"[2]')\n", "    def test_compile_json_path(self):\n        tests = [\n            ([\"a\", \"b\", \"c\"], \"$.a.b.c\"),\n            ([\"1\", \"2\", \"3\"], \"$[1][2][3]\"),\n            ([\"a\", \"1\", \"b\", \"2\"], \"$.a[1].b[2]\"),\n            ([\"a\", \"b\", \"c\"], \"a.b.c\", False),\n            ([\"1\", \"2\", \"3\"], \"[1][2][3]\", False),\n            ([\"a\", \"1\", \"b\", \"2\"], \"a[1].b[2]\", False),\n        ]\n        for key_transforms, expected_path, *include_root in tests:\n            with self.subTest(key_transforms=key_transforms, include_root=include_root):\n                self.assertEqual(\n                    compile_json_path(key_transforms, *include_root), expected_path\n                )\n", "    def test_compile_json_path(self):\n        self.assertEqual(compile_json_path([]), \"$\")\n        self.assertEqual(compile_json_path([\"a\"]), '$.\"a\"')\n        self.assertEqual(compile_json_path([\"a\", \"b\"]), '$.\"a\".\"b\"')\n        self.assertEqual(compile_json_path([\"a\", 0, \"b\"]), '$.\"a\"[0].\"b\"')\n        self.assertEqual(compile_json_path([0]), \"$[0]\")\n        self.assertEqual(compile_json_path([0, \"a\"]), '$[0].\"a\"')\n        self.assertEqual(compile_json_path([\"a\", 0], include_root=False), '.\"a\"[0]')\n"], "sample_393": ["    def setUp(self):\n        self.command = MakeMessagesCommand()\n        self.translatable = TranslatableFile(\"dirpath\", \"filename.html\", \"locale_dir\")\n        self.build_file = BuildFile(self.command, \"django\", self.translatable)\n", "    def test_translatable_file_ordering(self):\n        file1 = TranslatableFile(\"/path/to/dir1\", \"file1.py\", \"/path/to/locale\")\n        file2 = TranslatableFile(\"/path/to/dir2\", \"file2.py\", \"/path/to/locale\")\n\n        self.assertLess(file1, file2)\n        self.assertGreater(file2, file1)\n        self.assertEqual(file1, file1)\n        self.assertNotEqual(file1, file2)\n", "    def test_preprocess_js_domain(self):\n        \"\"\"\n        Test BuildFile preprocessing for domain 'djangojs' to ensure JS files are\n        correctly preprocessed with prepare_js_for_gettext.\n        \"\"\"\n        src_js = \"\"\"\n        // Some JS code\n        gettext('Translate me');\n        \"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            src_file = os.path.join(tmpdir, \"test.js\")\n            with open(src_file, \"w\", encoding=\"utf-8\") as f:\n                f.write(src_js)\n            \n            translatable = MakeMessagesCommand.translatable_file_class(tmpdir, \"test.js\", tmpdir)\n            build_file = MakeMessagesCommand.build_file_class(mock.Mock(gettext_version=(0, 19, 0)), \"djangojs\", translatable)\n            build_file.preprocess()\n            \n            with open(build_file.work_path, \"r\", encoding=\"utf-8\") as f:\n                processed_content = f.read()\n            \n            self.assertNotEqual(src_js, processed_content)\n            self.assertIn(\"gettext('Translate me')\", processed_content)\n", "    def setUp(self):\n        self.command = MakeMessagesCommand()\n        self.translatable = TranslatableFile(dirpath=\"templates\", file_name=\"test.html\", locale_dir=\"locale\")\n        self.build_file = BuildFile(command=self.command, domain=\"django\", translatable=self.translatable)\n", "    def test_invalid_domain_error(self):\n        msg = \"currently makemessages only supports domains 'django' and 'djangojs'\"\n        with self.assertRaisesMessage(CommandError, msg):\n            management.call_command(\"makemessages\", domain=\"invalid_domain\", locale=[LOCALE], verbosity=0)\n            "], "sample_394": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n\n        cls.s1 = State.objects.create(name=\"New York\")\n        cls.s2 = State.objects.create(name=\"Illinois\")\n        cls.s3 = State.objects.create(name=\"California\")\n        cls.c1 = City.objects.create(state=cls.s1, name=\"New York\")\n        cls.c2 = City.objects.create(state=cls.s2, name=\"Chicago\")\n        cls.c3 = City.objects.create(state=cls.s3, name=\"San Francisco\")\n        cls.r1 = Restaurant.objects.create(city=cls.c1, name=\"Italian Pizza\")\n        cls.r2 = Restaurant.objects.create(city=cls.c1, name=\"Boulevard\")\n        cls.r3 = Restaurant.objects.create(city=cls.c2, name=\"Chinese Dinner\")\n        cls.r4 = Restaurant.objects.create(city=cls.c2, name=\"Angels\")\n        cls.r5 = Restaurant.objects.create(city=cls.c2, name=\"Take Away\")\n        cls.r6 = Restaurant.objects.create(city=cls.c3, name=\"The Unknown Restaurant\")\n        cls.w1 = Worker.objects.create(work_at=cls.r1, name=\"Mario\", surname=\"Rossi\")\n        cls.w2 = Worker.objects.create(\n            work_at=cls.r1, name=\"Antonio\", surname=\"Bianchi\"\n        )\n        cls.w3 = Worker.objects.create(work_at=cls.r1, name=\"John\", surname=\"Doe\")\n", "    def test_formfield_overrides(self):\n        model_admin = ModelAdmin(Post, site)\n        factory = RequestFactory()\n        request = factory.get(reverse(\"admin:admin_views_post_add\"))\n        form = model_admin.get_form(request)\n        form_instance = form()\n\n        # Check that form fields use the overridden widgets and form_classes\n        self.assertIsInstance(form_instance.fields['posted'].widget, widgets.AdminDateWidget)\n        self.assertIsInstance(form_instance.fields['posted'].field, forms.SplitDateTimeField)\n        self.assertIsInstance(form_instance.fields['content'].widget, widgets.AdminTextareaWidget)\n        self.assertIsInstance(form_instance.fields['title'].widget, widgets.AdminTextInputWidget)\n        self.assertIsInstance(form_instance.fields['slug'].widget, widgets.AdminTextInputWidget)\n", "    def test_get_content_type_for_model(self):\n        \"Test get_content_type_for_model returns correct ContentType\"\n        from django.contrib.contenttypes.models import ContentType\n        state = State.objects.create(name=\"Test State\")\n        content_type = get_content_type_for_model(state)\n        expected_content_type = ContentType.objects.get_for_model(state, for_concrete_model=False)\n        self.assertEqual(content_type, expected_content_type)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n"], "sample_395": ["    def test_get_template_directories_no_templates(self, mock_get_dirs):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {ROOT / \"no_templates_dir\"},\n        )\n", "    def test_template_changed_in_subdirectory(self, mock_reset, mock_get_dirs):\n        subdirectory_path = Path(__file__).parent / \"templates\" / \"subdir\" / \"index.html\"\n        mock_get_dirs.return_value = {Path(__file__).parent / \"templates\"}\n        self.assertTrue(autoreload.template_changed(None, subdirectory_path))\n        mock_reset.assert_called_once()\n", "    def test_template_changed_django_path(self, mock_reset, mock_is_django_path):\n        template_path = Path(__file__).parent / \"templates\" / \"index.html\"\n        self.assertIsNone(autoreload.template_changed(None, template_path))\n        mock_reset.assert_not_called()\n", "    def test_template_dirs_with_invalid_path(self):\n        with self.assertRaises(ValueError):\n            autoreload.get_template_directories()\n", "    def test_template_dirs_filter_out_django_paths(self, mock_is_django_path):\n        directories = autoreload.get_template_directories()\n        self.assertSetEqual(directories, set())\n        mock_is_django_path.assert_called()\n"], "sample_396": ["    def test_ticket_24961(self):\n        \"\"\"\n        Ensure that subqueries involving EXISTS and column aliases handle\n        aliasing correctly and work as expected.\n        \"\"\"\n        # Setup test data\n        category = NamedCategory.objects.create(name=\"Test Category\")\n        tag = Tag.objects.create(name=\"Test Tag\", category=category)\n        annotation = Annotation.objects.create(name=\"Test Annotation\", tag=tag)\n        note = Note.objects.create(note=\"Test Note\", misc=\"Misc\", id=1)\n        annotation.notes.add(note)\n        \n        # Perform a complex query involving EXISTS and column aliases\n        subquery = Annotation.objects.filter(\n            Exists(Annotation.objects.filter(id=OuterRef(\"id\"), name=\"Test Annotation\"))\n        ).values(\"id\")\n        \n        result = Annotation.objects.filter(id__in=subquery)\n\n        # Check if aliasing is handled correctly\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0].name, \"Test Annotation\")\n", "    def setUpTestData(cls):\n        cls.tag1 = Tag.objects.create(name=\"tag1\")\n        cls.tag2 = Tag.objects.create(name=\"tag2\")\n        cls.tag3 = Tag.objects.create(name=\"tag3\")\n        cls.note1 = Note.objects.create(note=\"note1\", misc=\"misc1\")\n        cls.note2 = Note.objects.create(note=\"note2\", misc=\"misc2\")\n        cls.note3 = Note.objects.create(note=\"note3\", misc=\"misc3\")\n", "    def test_ticket_24889(self):\n        \"\"\"\n        Test that complex filter conditions involving subqueries with annotations \n        and aggregations work as expected.\n        \"\"\"\n        n1 = Note.objects.create(note=\"n1\", misc=\"misc1\")\n        n2 = Note.objects.create(note=\"n2\", misc=\"misc2\")\n        e1 = ExtraInfo.objects.create(info=\"e1\", note=n1)\n        e2 = ExtraInfo.objects.create(info=\"e2\", note=n2)\n        a1 = Author.objects.create(name=\"author1\", num=1, extra=e1)\n        a2 = Author.objects.create(name=\"author2\", num=2, extra=e2)\n        i1 = Item.objects.create(name=\"item1\", creator=a1, note=n1)\n        i2 = Item.objects.create(name=\"item2\", creator=a2, note=n2)\n\n        subquery = Item.objects.filter(creator=OuterRef('pk')).annotate(max_num=Max('creator__num')).values('max_num')\n        authors = Author.objects.annotate(max_item_creator_num=Subquery(subquery)).filter(max_item_creator_num__gte=1)\n\n        self.assertSequenceEqual(authors, [a1, a2])\n", "    def test_ticket_23605_nested_q(self):\n        # Create complex nested Q objects to test the handling of nested queries\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=5000.0)\n        c2 = Ticket23605C.objects.create(field_c0=20000.0)\n        Ticket23605B.objects.create(\n            field_b0=5000.0, field_b1=True, modelc_fk=c1, modela_fk=a1\n        )\n        Ticket23605B.objects.create(\n            field_b0=20000.0, field_b1=False, modelc_fk=c2, modela_fk=a2\n        )\n\n        nested_q = Q(\n            pk__in=Ticket23605A.objects.filter(\n                Q(ticket23605b__field_b1=True)\n                & Q(ticket23605b__field_b0__gte=F(\"ticket23605b__modelc_fk__field_c0\"))\n                & ~Q(ticket23605b__field_b0__lt=1000)\n            )\n            & Q(ticket23605b__modelc_fk__field_c0__gte=5000)\n        )\n\n        qs1 = Ticket23605A.objects.filter(nested_q)\n        self.assertSequenceEqual(qs1, [a1])\n\n        qs2 = Ticket23605A.objects.exclude(nested_q)\n        self.assertSequenceEqual(qs2, [a2])\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name=\"Author 1\", num=1)\n        cls.author2 = Author.objects.create(name=\"Author 2\", num=2)\n        cls.report1 = Report.objects.create(name=\"Report 1\", creator=cls.author1)\n        cls.report2 = Report.objects.create(name=\"Report 2\", creator=cls.author2)\n        cls.tag1 = Tag.objects.create(name=\"Tag 1\")\n        cls.tag2 = Tag.objects.create(name=\"Tag 2\")\n        cls.annotation1 = Annotation.objects.create(name=\"Annotation 1\", tag=cls.tag1)\n        cls.annotation2 = Annotation.objects.create(name=\"Annotation 2\", tag=cls.tag2)\n        cls.annotation1.notes.set([cls.report1, cls.report2])\n        cls.annotation2.notes.set([cls.report1])\n"], "sample_397": ["    def test_find_template_loader_invalid_value(self):\n        \"\"\"\n        Test find_template_loader with an invalid loader value.\n        \"\"\"\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {},\n            }\n        ).engine\n\n        invalid_loader = 12345  # Invalid loader value\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            \"Invalid value in template loaders configuration: %r\" % invalid_loader\n        ):\n            engine.find_template_loader(invalid_loader)\n", "    def test_find_template_loader_invalid_value(self):\n        \"\"\"\n        Ensure find_template_loader raises ImproperlyConfigured when given an invalid loader value.\n        \"\"\"\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {},\n            }\n        )\n        with self.assertRaisesMessage(ImproperlyConfigured, \"Invalid value in template loaders configuration: 'invalid.loader'\"):\n            engine.engine.find_template_loader(\"invalid.loader\")\n", "    def test_find_template_loader_invalid_value(self):\n        \"\"\"Test Engine.find_template_loader with an invalid value.\"\"\"\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": True,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {},\n            }\n        )\n        with self.assertRaisesMessage(ImproperlyConfigured, \"Invalid value in template loaders configuration\"):\n            engine.engine.find_template_loader(123)\n", "    def test_get_template_libraries(self):\n        libraries = {\n            \"test_lib\": \"template_backends.apps.good.templatetags.good_tags\",\n        }\n        engine = Engine(\n            libraries=libraries,\n        )\n        self.assertIn(\"test_lib\", engine.template_libraries)\n        self.assertEqual(\n            engine.template_libraries[\"test_lib\"].__name__,\n            \"template_backends.apps.good.templatetags.good_tags\"\n        )\n", "    def test_improperly_configured_loaders_with_app_dirs(self):\n        \"\"\"\n        An ImproperlyConfigured error should be raised if both loaders and\n        app_dirs are specified.\n        \"\"\"\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            \"app_dirs must not be set when loaders is defined.\",\n        ):\n            Engine(\n                dirs=[],\n                app_dirs=True,\n                loaders=[\"django.template.loaders.filesystem.Loader\"]\n            )\n"], "sample_398": ["    def test_get_context_data_with_extra_context(self):\n        class TestView(PasswordContextMixin, TemplateView):\n            template_name = \"registration/test.html\"\n            extra_context = {\"extra_key\": \"extra_value\"}\n            title = \"Test Title\"\n\n        request = self.client.get(\"/test/\")\n        view = TestView.as_view()\n        response = view(request)\n        context = response.context_data\n        \n        self.assertEqual(context[\"extra_key\"], \"extra_value\")\n        self.assertEqual(context[\"title\"], \"Test Title\")\n        self.assertEqual(context[\"subtitle\"], None)\n", "    def confirm_logged_out(self):\n        self.assertNotIn(SESSION_KEY, self.client.session)\n", "    def test_password_reset_view_context(self):\n        \"\"\"\n        Test that PasswordResetView sets the correct context data.\n        \"\"\"\n        response = self.client.get(\"/password_reset/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context[\"title\"], \"Password reset\")\n        self.assertIsInstance(response.context[\"form\"], PasswordResetForm)\n        ", "    def test_logout_then_login_redirect_loop(self):\n        \"\"\"Test that logout_then_login correctly handles redirect loops.\"\"\"\n        self.login()\n        with self.settings(LOGIN_URL=\"/logout_then_login/\"):\n            req = HttpRequest()\n            req.method = \"POST\"\n            csrf_token = get_token(req)\n            req.COOKIES[settings.CSRF_COOKIE_NAME] = csrf_token\n            req.POST = {\"csrfmiddlewaretoken\": csrf_token}\n            req.session = self.client.session\n            msg = (\n                \"Redirection loop for authenticated user detected. Check that \"\n                \"your LOGIN_URL doesn't point to a login page.\"\n            )\n            with self.assertRaisesMessage(ValueError, msg):\n                logout_then_login(req)\n", "    def test_redirect_authenticated_user(self):\n        \"\"\"\n        If redirect_authenticated_user is True, authenticated users should be\n        redirected to the success URL.\n        \"\"\"\n        self.login()\n        response = self.client.get(\"/login/redirect_authenticated_user/\")\n        self.assertRedirects(response, settings.LOGIN_REDIRECT_URL, fetch_redirect_response=False)\n"], "sample_399": ["def test_rawquery_clone(self):\n    query = RawQuery(\"SELECT * FROM my_table\", using=\"default\", params=(1, 2, 3))\n    cloned_query = query.clone(using=\"new_db\")\n    self.assertEqual(cloned_query.sql, \"SELECT * FROM my_table\")\n    self.assertEqual(cloned_query.params, (1, 2, 3))\n    self.assertEqual(cloned_query.using, \"new_db\")\n    self.assertIsNone(cloned_query.cursor)\n", "    def test_raw_query_execution(self):\n        raw_sql = \"SELECT name, age FROM aggregation_author WHERE age > %s\"\n        age_threshold = 30\n        raw_query = RawQuery(sql=raw_sql, using='default', params=(age_threshold,))\n        columns = raw_query.get_columns()\n        self.assertEqual(columns, ['name', 'age'])\n        results = list(raw_query)\n        expected_results = [\n            ('Adrian Holovaty', 34),\n            ('Jacob Kaplan-Moss', 35),\n            ('Brad Dayley', 45),\n            ('Jeffrey Forcier', 37),\n            ('Peter Norvig', 57),\n            ('Stuart Russell', 46)\n        ]\n        self.assertEqual(results, expected_results)\n", "    def test_raw_query_execution(self):\n        sql = \"SELECT COUNT(*) FROM aggregation_author\"\n        raw_query = RawQuery(sql, using=DEFAULT_DB_ALIAS)\n        count = list(raw_query)[0][0]\n        self.assertEqual(count, Author.objects.count())\n        \n        # Test with parameters\n        sql = \"SELECT COUNT(*) FROM aggregation_author WHERE age > %s\"\n        raw_query = RawQuery(sql, using=DEFAULT_DB_ALIAS, params=(30,))\n        count = list(raw_query)[0][0]\n        self.assertEqual(count, Author.objects.filter(age__gt=30).count())\n        \n        # Test cloning with different parameters\n        raw_query_clone = raw_query.clone(using=DEFAULT_DB_ALIAS)\n        raw_query_clone.params = (40,)\n        count = list(raw_query_clone)[0][0]\n        self.assertEqual(count, Author.objects.filter(age__gt=40).count())\n", "    def test_raw_query_get_columns(self):\n        raw_query = RawQuery(\"SELECT id, name FROM aggregation_author\", \"default\")\n        with connection.cursor() as cursor:\n            raw_query.cursor = cursor\n            raw_query._execute_query()\n            columns = raw_query.get_columns()\n        self.assertEqual(columns, [\"id\", \"name\"])\n", "def test_raw_query_get_columns(self):\n    raw_query = RawQuery(\"SELECT id, name FROM aggregation_author\", using=\"default\")\n    columns = raw_query.get_columns()\n    self.assertEqual(columns, [\"id\", \"name\"])\n"], "sample_400": ["    def test_alter_model_table_with_model_change(self):\n        \"\"\"\n        Tests when model and db_table changes, autodetector must create two\n        operations.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_db_table_options],\n            [self.author_renamed_with_new_db_table_options],\n            MigrationQuestioner({\"ask_rename_model\": True}),\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes, \"testapp\", 0, [\"RenameModel\", \"AlterModelTable\"]\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 1, name=\"newauthor\", table=\"author_three\"\n        )\n", "    def test_deep_deconstruct_with_partial(self):\n        \"\"\"\n        Test deep_deconstruct method with functools.partial objects.\n        \"\"\"\n            return x + y\n\n        partial_func = functools.partial(sample_func, x=10, y=20)\n        instance = MigrationAutodetector(None, None)\n\n        obj = {\n            \"key1\": [partial_func],\n            \"key2\": (partial_func,),\n            \"key3\": {\"nested_key\": partial_func},\n        }\n\n        result = instance.deep_deconstruct(obj)\n\n        expected = {\n            \"key1\": [(sample_func, (10, 20), None)],\n            \"key2\": ((sample_func, (10, 20), None),),\n            \"key3\": {\"nested_key\": (sample_func, (10, 20), None)},\n        }\n\n        self.assertEqual(result, expected)\n", "def test_resolve_dependency(self):\n    \"\"\"\n    Tests resolving dependencies correctly.\n    \"\"\"\n    with override_settings(DEP_TEST=\"testapp.Model\"):\n        dependency = (\"__setting__\", \"DEP_TEST\", None, True)\n        resolved_dependency, is_swappable = MigrationAutodetector._resolve_dependency(dependency)\n        self.assertEqual(resolved_dependency, (\"testapp\", \"model\", None, True))\n        self.assertTrue(is_swappable)\n\n        dependency = (\"testapp\", \"Model\", None, True)\n        resolved_dependency, is_swappable = MigrationAutodetector._resolve_dependency(dependency)\n        self.assertEqual(resolved_dependency, (\"testapp\", \"Model\", None, True))\n        self.assertFalse(is_swappable)\n", "    def test_alter_model_table(self):\n        \"\"\"\n        Tests detection for altering db_table in model's options when\n        renaming the model at the same time.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_db_table_options],\n            [self.author_renamed_with_new_db_table_options],\n            MigrationQuestioner({\"ask_rename_model\": True}),\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes, \"testapp\", 0, [\"RenameModel\", \"AlterModelTable\"]\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 1, name=\"newauthor\", table=\"author_three\"\n        )\n", "def test_add_model_with_default_manager(self):\n    \"\"\"\n    Adding a model with a default manager should correctly detect the change.\n    \"\"\"\n    before = []\n    after = [\n        ModelState(\n            \"app\",\n            \"MyModel\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            managers=[\n                (\"objects\", models.Manager()),\n            ],\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(\n        changes,\n        \"app\",\n        0,\n        0,\n        name=\"MyModel\",\n        managers=[(\"objects\", models.Manager())],\n    )\n"], "sample_401": ["    def test_min_num_with_extra_forms(self):\n        \"\"\"\n        A formset with min_num set requires at least min_num forms to be\n        completed, considering both initial and extra forms.\n        \"\"\"\n        ChoiceFormSet = formset_factory(Choice, extra=2, min_num=2, validate_min=True)\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"2\",\n            \"choices-MAX_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Choice 1\",\n            \"choices-0-votes\": \"10\",\n            \"choices-1-choice\": \"\",\n            \"choices-1-votes\": \"\",\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), [\"Please submit at least 2 forms.\"])\n        self.assertEqual(\n            str(formset.non_form_errors()),\n            '<ul class=\"errorlist nonform\"><li>Please submit at least 2 forms.</li></ul>',\n        )\n        # Adding data to the second form to satisfy min_num requirement\n        data[\"choices-1-choice\"] = \"Choice 2\"\n        data[\"choices-1-votes\"] = \"20\"\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), [])\n", "    def test_invalid_management_form(self):\n        \"\"\"Formset with invalid ManagementForm should raise an error.\"\"\"\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"1000\",\n            # Missing required management form fields\n            \"choices-0-choice\": \"Test\",\n            \"choices-0-votes\": \"100\",\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.non_form_errors(),\n            [\"ManagementForm data is missing or has been tampered with. Missing fields: choices-TOTAL_FORMS. You may need to file a bug report if the issue persists.\"]\n        )\n", "    def test_formset_with_non_form_errors_only(self):\n        \"\"\"\n        A formset with only non-form errors should still report as invalid.\n        \"\"\"\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Test\",\n            \"choices-0-votes\": \"100\",\n        }\n        ChoiceFormSetWithError = formset_factory(\n            Choice, formset=ChoiceFormsetWithNonFormError\n        )\n        formset = ChoiceFormSetWithError(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), [\"non-form error\"])\n        self.assertEqual(formset.errors, [{}])\n", "    def test_formset_factory_with_custom_prefix(self):\n        \"\"\"\n        Ensure formset_factory correctly sets custom prefix and form names\n        are generated appropriately.\n        \"\"\"\n        CustomPrefixFormSet = formset_factory(Choice, extra=2)\n        formset = CustomPrefixFormSet(auto_id=False, prefix=\"custom\")\n        self.assertHTMLEqual(\n            str(formset.management_form),\n            \"\"\"<input type=\"hidden\" name=\"custom-TOTAL_FORMS\" value=\"2\">", "    def test_custom_management_form(self):\n        \"\"\"\n        Custom ManagementForm class can be used to override default behavior.\n        \"\"\"\n\n        class CustomManagementForm(ManagementForm):\n                cleaned_data = super().clean()\n                if cleaned_data[TOTAL_FORM_COUNT] > 1:\n                    self.add_error(TOTAL_FORM_COUNT, \"Too many forms!\")\n                return cleaned_data\n\n        class CustomFormSet(BaseFormSet):\n            @cached_property\n                if self.is_bound:\n                    form = CustomManagementForm(\n                        self.data, auto_id=self.auto_id, prefix=self.prefix, renderer=self.renderer\n                    )\n                    form.full_clean()\n                else:\n                    form = CustomManagementForm(\n                        auto_id=self.auto_id,\n                        prefix=self.prefix,\n                        initial={\n                            TOTAL_FORM_COUNT: self.total_form_count(),\n                            INITIAL_FORM_COUNT: self.initial_form_count(),\n                            MIN_NUM_FORM_COUNT: self.min_num,\n                            MAX_NUM_FORM_COUNT: self.max_num,\n                        },\n                        renderer=self.renderer,\n                    )\n                return form\n\n        CustomFormSetFactory = formset_factory(Choice, formset=CustomFormSet)\n        \n        # Test with valid data\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"1000\",\n            \"choices-0-choice\": \"Test\",\n            \"choices-0-votes\": \"100\",\n        }\n        formset = CustomFormSetFactory(data, auto_id=False, prefix=\"choices\")\n        self.assertTrue(formset.is_valid())\n        \n        # Test with invalid data\n        data[\"choices-TOTAL_FORMS\"] = \"2\"\n        formset = CustomFormSetFactory(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.management_form.errors[TOTAL_FORM_COUNT], [\"Too many forms!\"])\n"], "sample_402": ["    def test_append_slash_no_redirect_on_POST_in_DEBUG_with_www(self):\n        \"\"\"\n        While in debug mode, an exception is raised with a warning\n        when a failed attempt is made to POST, PUT, or PATCH to an URL which\n        would normally be redirected to a slashed version with www prepended.\n        \"\"\"\n        msg = \"maintaining %s data. Change your form to point to testserver/slash/\"\n        request = self.rf.get(\"/slash\")\n        request.method = \"POST\"\n        with self.assertRaisesMessage(RuntimeError, msg % request.method):\n            CommonMiddleware(get_response_404)(request)\n        request = self.rf.get(\"/slash\")\n        request.method = \"PUT\"\n        with self.assertRaisesMessage(RuntimeError, msg % request.method):\n            CommonMiddleware(get_response_404)(request)\n        request = self.rf.get(\"/slash\")\n        request.method = \"PATCH\"\n        with self.assertRaisesMessage(RuntimeError, msg % request.method):\n            CommonMiddleware(get_response_404)(request)\n", "def test_prepend_www_without_host(self):\n    \"\"\"\n    PREPEND_WWW should not redirect if the host is not present in the request.\n    \"\"\"\n    request = self.rf.get(\"/path/\")\n    request.META.pop('HTTP_HOST', None)  # Remove host to simulate missing host scenario\n    response = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertIsNone(response)\n", "def test_prepend_www_no_host(self):\n    \"\"\"\n    PREPEND_WWW should not redirect if the host is empty.\n    \"\"\"\n    request = self.rf.get(\"/path/\")\n    request.META[\"HTTP_HOST\"] = \"\"\n    response = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertIsNone(response)\n", "    def test_broken_link_emails_middleware_sends_email(self):\n        \"\"\"\n        BrokenLinkEmailsMiddleware should send an email when a 404 is encountered and DEBUG is False.\n        \"\"\"\n        request = self.rf.get(\"/nonexistent_page/\")\n        request.META[\"HTTP_REFERER\"] = \"http://example.com/\"\n        with self.settings(DEBUG=False):\n            response = HttpResponseNotFound()\n            response = BrokenLinkEmailsMiddleware(lambda req: response)(request)\n            self.assertEqual(response.status_code, 404)\n            self.assertEqual(len(mail.outbox), 1)\n            self.assertIn(\"Broken link on\", mail.outbox[0].subject)\n            self.assertIn(\"Referrer: http://example.com/\", mail.outbox[0].body)\n", "    def test_append_slash_on_post_method(self):\n        \"\"\"\n        APPEND_SLASH should not redirect POST requests to a valid pattern.\n        \"\"\"\n            return HttpResponse(\"POST request received.\")\n\n        request = self.rf.post(\"/slash\")\n        r = CommonMiddleware(get_response).process_request(request)\n        self.assertIsNone(r)\n        response = HttpResponseNotFound()\n        r = CommonMiddleware(get_response).process_response(request, response)\n        self.assertEqual(r.status_code, 404)\n"], "sample_403": ["    def test_create_model_with_base_model(self):\n        \"\"\"\n        Test CreateModel operation with a custom base model.\n        \"\"\"\n        project_state = ProjectState()\n        operation = migrations.CreateModel(\n            \"SpecialPony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"color\", models.CharField(max_length=20)),\n            ],\n            bases=(\"migrations.UnicodeModel\",),\n        )\n        self.assertEqual(operation.describe(), \"Create model SpecialPony\")\n        self.assertEqual(operation.migration_name_fragment, \"specialpony\")\n        # Test the state alteration\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmbm\", new_state)\n        self.assertEqual(new_state.models[\"test_crmbm\", \"specialpony\"].name, \"SpecialPony\")\n        self.assertEqual(len(new_state.models[\"test_crmbm\", \"specialpony\"].fields), 2)\n        self.assertEqual(new_state.models[\"test_crmbm\", \"specialpony\"].bases, (\"migrations.UnicodeModel\",))\n        # Test the database alteration\n        self.assertTableNotExists(\"test_crmbm_specialpony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_crmbm\", editor, project_state, new_state)\n        self.assertTableExists(\"test_crmbm_specialpony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crmbm\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_crmbm_specialpony\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"CreateModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"bases\", \"fields\", \"name\"])\n", "    def test_delete_model_with_fk(self):\n        \"\"\"\n        Test the DeleteModel operation on a model with a ForeignKey.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_dlmodfk\", related_model=True)\n        # Test the state alteration\n        operation = migrations.DeleteModel(\"Rider\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_dlmodfk\", new_state)\n        self.assertNotIn((\"test_dlmodfk\", \"rider\"), new_state.models)\n        # Test the database alteration\n        self.assertTableExists(\"test_dlmodfk_rider\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_dlmodfk\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_dlmodfk_rider\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_dlmodfk\", editor, new_state, project_state)\n        self.assertTableExists(\"test_dlmodfk_rider\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"DeleteModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(list(definition[2]), [\"name\"])\n", "    def test_create_model_with_non_duplicate_bases_and_managers(self):\n        \"\"\"\n        Tests the CreateModel operation with unique bases and managers.\n        \"\"\"\n        # Create the operation\n        operation = migrations.CreateModel(\n            \"UniquePony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"color\", models.CharField(max_length=100)),\n            ],\n            bases=(\"app.BaseModel1\", \"app.BaseModel2\"),\n            managers=[\n                (\"manager1\", models.Manager()),\n                (\"manager2\", models.Manager()),\n            ],\n        )\n        self.assertEqual(operation.describe(), \"Create model UniquePony\")\n        self.assertEqual(operation.migration_name_fragment, \"uniquepony\")\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmo_unique\", new_state)\n        self.assertEqual(new_state.models[\"test_crmo_unique\", \"uniquepony\"].name, \"UniquePony\")\n        self.assertEqual(len(new_state.models[\"test_crmo_unique\", \"uniquepony\"].fields), 2)\n        # Test the database alteration\n        self.assertTableNotExists(\"test_crmo_unique_uniquepony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_crmo_unique\", editor, project_state, new_state)\n        self.assertTableExists(\"test_crmo_unique_uniquepony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crmo_unique\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_crmo_unique_uniquepony\")\n        # Ensure no duplicate bases or managers\n        operation = migrations.CreateModel(\n            \"NoDuplicates\",\n            fields=[],\n            bases=(\"app.BaseModel1\", \"app.BaseModel2\"),\n            managers=[\n                (\"manager1\", models.Manager()),\n                (\"manager2\", models.Manager()),\n            ],\n        )\n        # This should not raise any ValueError\n        operation.deconstruct()\n", "    def test_alter_model_table_with_m2m(self):\n        \"\"\"\n        Tests the AlterModelTable operation on a model with ManyToManyField.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_almotwm2m\", second_model=True)\n        # Add the M2M field\n        operation = migrations.AddField(\n            \"Pony\", \"stables\", models.ManyToManyField(\"Stable\", related_name=\"ponies\")\n        )\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_almotwm2m\", new_state)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_almotwm2m\", editor, project_state, new_state)\n        self.assertTableExists(\"test_almotwm2m_pony_stables\")\n        # Now alter the model table name\n        operation = migrations.AlterModelTable(\"Pony\", \"new_pony_table\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_almotwm2m\", new_state)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_almotwm2m\", editor, project_state, new_state)\n        self.assertTableExists(\"new_pony_table_stables\")\n        # Test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_almotwm2m\", editor, new_state, project_state)\n        self.assertTableExists(\"test_almotwm2m_pony_stables\")\n", "def test_alter_model_options_with_abstract_base(self):\n        \"\"\"\n        Tests the AlterModelOptions operation with an abstract base model.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_almoopab\", options=True)\n        # Define an abstract base model\n        class AbstractBase(models.Model):\n            class Meta:\n                abstract = True\n\n        # Add the abstract base model to the state's model options\n        operation = migrations.AlterModelOptions(\n            \"Pony\", {\"abstract\": True}\n        )\n        self.assertEqual(operation.describe(), \"Change Meta options on Pony\")\n        self.assertEqual(operation.migration_name_fragment, \"alter_pony_options\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_almoopab\", new_state)\n        self.assertEqual(\n            new_state.models[\"test_almoopab\", \"pony\"].options.get(\"abstract\", False),\n            True,\n        )\n        self.assertIn(\n            AbstractBase,\n            new_state.models[\"test_almoopab\", \"pony\"].options.get(\"bases\", ()),\n        )\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AlterModelOptions\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(\n            definition[2],\n            {\"name\": \"Pony\", \"options\": {\"abstract\": True}},\n        )\n"], "sample_404": ["    def test_variable_resolution(self):\n        \"\"\"\n        Test that variables are resolved correctly within the template context.\n        \"\"\"\n        engine = self._engine()\n        context = Context({\"name\": \"World\"})\n        t = engine.from_string(\"Hello, {{ name }}!\")\n        self.assertEqual(t.render(context), \"Hello, World!\")\n        \n        context = Context({\"name\": \"Alice\"})\n        self.assertEqual(t.render(context), \"Hello, Alice!\")\n", "def test_variable_resolution(self):\n    \"\"\"\n    Ensure Variable class can resolve variables correctly in different contexts.\n    \"\"\"\n    engine = self._engine()\n    context = Context({\n        \"article\": {\"section\": \"News\"},\n        \"number\": 123,\n        \"nested\": {\"attribute\": {\"value\": \"Nested Value\"}},\n    })\n\n    var = Variable(\"article.section\")\n    self.assertEqual(var.resolve(context), \"News\")\n\n    var = Variable(\"number\")\n    self.assertEqual(var.resolve(context), 123)\n\n    var = Variable(\"nested.attribute.value\")\n    self.assertEqual(var.resolve(context), \"Nested Value\")\n\n    with self.assertRaises(VariableDoesNotExist):\n        var = Variable(\"nonexistent\")\n        var.resolve(context)\n", "def test_variable_does_not_exist(self):\n    \"\"\"\n    Test that VariableDoesNotExist exception is properly handled and formatted.\n    \"\"\"\n    with self.assertRaises(VariableDoesNotExist) as e:\n        raise VariableDoesNotExist(\"Variable '%s' does not exist\", (\"test_variable\",))\n    self.assertEqual(str(e.exception), \"Variable 'test_variable' does not exist\")\n", "def test_variable_node_render(self):\n    \"\"\"\n    Ensure VariableNode renders the correct value from context.\n    \"\"\"\n    engine = self._engine()\n    template = engine.from_string(\"{{ varname }}\")\n    context = Context({\"varname\": \"test_value\"})\n    self.assertEqual(template.render(context), \"test_value\")\n", "def test_text_node_rendering(self):\n    \"\"\"\n    Ensure that TextNode correctly renders text content.\n    \"\"\"\n    node = TextNode(\"Hello, World!\")\n    context = Context()\n    self.assertEqual(node.render(context), \"Hello, World!\")\n\n"], "sample_405": ["    def test_modeloperation_can_reduce_through(self):\n        \"\"\"\n        Tests the ModelOperation's can_reduce_through method.\n        \"\"\"\n        # CreateModel operation\n        create_model = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n            ],\n        )\n        # AlterModelOptions operation\n        alter_model_options = migrations.AlterModelOptions(\n            name=\"Pony\", options={\"permissions\": [(\"can_groom\", \"Can groom\")]}\n        )\n        # Check can_reduce_through\n        self.assertTrue(create_model.can_reduce_through(alter_model_options, \"test\"))\n        # Check cannot reduce through an unrelated model\n        alter_unrelated_model = migrations.AlterModelOptions(\n            name=\"Unicorn\", options={\"permissions\": [(\"can_fly\", \"Can fly\")]}\n        )\n        self.assertFalse(create_model.can_reduce_through(alter_unrelated_model, \"test\"))\n", "    def test_delete_model_with_related_objects(self):\n        \"\"\"\n        Test the DeleteModel operation on a model that has related objects.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_dlmr\", related_model=True)\n        # Test the state alteration\n        operation = migrations.DeleteModel(\"Rider\")\n        self.assertEqual(operation.describe(), \"Delete model Rider\")\n        self.assertEqual(operation.migration_name_fragment, \"delete_rider\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_dlmr\", new_state)\n        self.assertNotIn((\"test_dlmr\", \"rider\"), new_state.models)\n        # Test the database alteration\n        self.assertTableExists(\"test_dlmr_rider\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_dlmr\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_dlmr_rider\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_dlmr\", editor, new_state, project_state)\n        self.assertTableExists(\"test_dlmr_rider\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"DeleteModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(definition[2], {\"name\": \"Rider\"})\n", "def test_create_model_with_duplicate_model_name(self):\n        \"\"\"\n        Tests that creating a model with a duplicate name raises an error.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_crmo\")\n        with self.assertRaisesMessage(\n            ValueError, \"Found duplicate value Pony in CreateModel name argument.\"\n        ):\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(default=1)),\n                ],\n            )\n", "    def test_rename_model_with_m2m_field(self):\n        \"\"\"\n        Tests RenameModel operation on a model with a ManyToMany field.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_rnmwm2m\")\n        # Add the M2M field\n        operation = migrations.AddField(\n            \"Pony\", \"riders\", models.ManyToManyField(\"Rider\")\n        )\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_rnmwm2m\", new_state)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_rnmwm2m\", editor, project_state, new_state)\n        \n        # Rename the model\n        rename_operation = migrations.RenameModel(\"Pony\", \"Horse\")\n        renamed_state = new_state.clone()\n        rename_operation.state_forwards(\"test_rnmwm2m\", renamed_state)\n        \n        # Test the database alteration\n        self.assertTableExists(\"test_rnmwm2m_pony_riders\")\n        self.assertTableNotExists(\"test_rnmwm2m_horse_riders\")\n        \n        atomic_rename = connection.features.supports_atomic_references_rename\n        with connection.schema_editor(atomic=atomic_rename) as editor:\n            rename_operation.database_forwards(\n                \"test_rnmwm2m\", editor, new_state, renamed_state\n            )\n        \n        self.assertTableNotExists(\"test_rnmwm2m_pony_riders\")\n        self.assertTableExists(\"test_rnmwm2m_horse_riders\")\n        \n        # Test reversal\n        with connection.schema_editor(atomic=atomic_rename) as editor:\n            rename_operation.database_backwards(\n                \"test_rnmwm2m\", editor, renamed_state, new_state\n            )\n        \n        self.assertTableExists(\"test_rnmwm2m_pony_riders\")\n        self.assertTableNotExists(\"test_rnmwm2m_horse_riders\")\n", "    def test_delete_model_with_constraint(self):\n        \"\"\"\n        Tests that DeleteModel operation removes a model with constraints.\n        \"\"\"\n        # Create initial state with a model and a constraint\n        project_state = self.set_up_test_model(\"test_dlwmc\", constraints=[\n            models.CheckConstraint(check=models.Q(pink__gte=0), name=\"check_pink_gte_0\")\n        ])\n        # Test initial state and database\n        self.assertTableExists(\"test_dlwmc_pony\")\n        Pony = project_state.apps.get_model(\"test_dlwmc\", \"Pony\")\n        with self.assertRaises(IntegrityError):\n            with atomic():\n                Pony.objects.create(pink=-1)\n        Pony.objects.create(pink=0)\n\n        # Create DeleteModel operation\n        operation = migrations.DeleteModel(\"Pony\")\n\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_dlwmc\", new_state)\n        self.assertNotIn((\"test_dlwmc\", \"pony\"), new_state.models)\n\n        # Test the database alteration\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_dlwmc\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_dlwmc_pony\")\n\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_dlwmc\", editor, new_state, project_state)\n        self.assertTableExists(\"test_dlwmc_pony\")\n        with self.assertRaises(IntegrityError):\n            with atomic():\n                Pony.objects.create(pink=-1)\n        Pony.objects.create(pink=0)\n"], "sample_406": ["    def test_deconstruct_auto_created_manager(self):\n        manager = Manager()\n        manager.auto_created = True\n        manager._built_with_as_manager = False\n        manager._queryset_class = QuerySet\n        deconstructed = manager.deconstruct()\n        self.assertEqual(deconstructed[0], False)\n        self.assertEqual(deconstructed[1], \"tests.test_models.Manager\")\n        self.assertEqual(deconstructed[2], None)\n        self.assertEqual(deconstructed[3], manager._constructor_args[0])\n        self.assertEqual(deconstructed[4], manager._constructor_args[1])\n", "    def test_manager_descriptor_get(self):\n        class TestModel(models.Model):\n            objects = Manager()\n            class Meta:\n                abstract = True\n\n        with self.assertRaises(AttributeError) as cm:\n            TestModel.objects.__get__(None, TestModel)\n        self.assertEqual(str(cm.exception), \"Manager isn't available; TestModel is abstract\")\n\n        class NonAbstractModel(models.Model):\n            objects = Manager()\n\n        with self.assertRaises(AttributeError) as cm:\n            NonAbstractModel.objects.__get__(NonAbstractModel(), NonAbstractModel)\n        self.assertEqual(str(cm.exception), \"Manager isn't accessible via NonAbstractModel instances\")\n\n        non_abstract_instance = NonAbstractModel()\n        manager = NonAbstractModel.objects.__get__(None, NonAbstractModel)\n        self.assertEqual(manager, NonAbstractModel.objects)\n", "    def setUp(self):\n        self.manager = BaseManager()\n", "    def test_manager_creation_counter(self):\n        manager1 = BaseManager()\n        initial_counter = manager1.creation_counter\n        manager2 = BaseManager()\n        self.assertEqual(manager2.creation_counter, initial_counter + 1)\n", "    def setUp(self):\n        class CustomQuerySet(QuerySet):\n                return \"custom_method_called\"\n\n        class CustomManager(BaseManager.from_queryset(CustomQuerySet)):\n            pass\n\n        self.CustomManager = CustomManager\n        self.CustomQuerySet = CustomQuerySet\n        self.manager = self.CustomManager()\n        self.model = mock.Mock()\n        self.manager.model = self.model\n"], "sample_407": ["    def test_save_related_instance_in_bulk(self):\n        \"\"\"\n        Test that bulk_create does not populate the ForeignKey cache.\n        \"\"\"\n        parent = Parent.objects.create(name=\"jeff\")\n        child1 = Child(name=\"frank\", parent=parent)\n        child2 = Child(name=\"randy\", parent=parent)\n        Child.objects.bulk_create([child1, child2])\n        self.assertFalse(Child.parent.is_cached(child1))\n        self.assertFalse(Child.parent.is_cached(child2))\n        # Accessing the parent should now populate the cache.\n        self.assertEqual(child1.parent, parent)\n        self.assertTrue(Child.parent.is_cached(child1))\n        self.assertEqual(child2.parent, parent)\n        self.assertTrue(Child.parent.is_cached(child2))\n", "    def test_multiple_levels_of_inheritance(self):\n        \"\"\"\n        Test that multiple levels of inheritance are handled correctly\n        when saving and retrieving models.\n        \"\"\"\n        class GrandParent(models.Model):\n            name = models.CharField(max_length=100)\n            class Meta:\n                abstract = True\n\n        class ParentModel(GrandParent):\n            pass\n\n        class ChildModel(ParentModel):\n            pass\n\n        grandparent = GrandParent(name=\"Grandparent\")\n        parent = ParentModel(name=\"Parent\")\n        child = ChildModel(name=\"Child\")\n        parent.save()\n        child.save()\n\n        self.assertEqual(ParentModel.objects.count(), 1)\n        self.assertEqual(ChildModel.objects.count(), 1)\n        self.assertEqual(parent.name, \"Parent\")\n        self.assertEqual(child.name, \"Child\")\n\n        fetched_parent = ParentModel.objects.get(pk=parent.pk)\n        fetched_child = ChildModel.objects.get(pk=child.pk)\n\n        self.assertEqual(fetched_parent.name, \"Parent\")\n        self.assertEqual(fetched_child.name, \"Child\")\n", "    def test_create_with_deferred_fields(self):\n        # Test model instantiation with deferred fields.\n\n        # Create a Parent object and defer the 'name' field.\n        parent = Parent.objects.create(name=\"Parent\")\n        parent = Parent.objects.only('id').get(id=parent.id)\n        \n        # Verify the name field is deferred\n        self.assertIn('name', parent.get_deferred_fields())\n\n        # Create a Child object and defer the 'name' field.\n        child = Child.objects.create(name=\"Child\", parent=parent)\n        child = Child.objects.only('id', 'parent_id').get(id=child.id)\n        \n        # Verify the name field is deferred\n        self.assertIn('name', child.get_deferred_fields())\n\n        # Accessing deferred fields should trigger a database query.\n        with self.assertNumQueries(1):\n            self.assertEqual(parent.name, \"Parent\")\n\n        with self.assertNumQueries(1):\n            self.assertEqual(child.name, \"Child\")\n", "    def test_proxy_model_inheritance(self):\n        # Define a base model with a single field\n        class BaseModel(Model):\n            name = models.CharField(max_length=100)\n            class Meta:\n                abstract = True\n\n        # Define a concrete model inheriting from the base model\n        class ConcreteModel(BaseModel):\n            age = models.IntegerField()\n\n        # Define a proxy model inheriting from the concrete model\n        class ProxyModel(ConcreteModel):\n            class Meta:\n                proxy = True\n\n        # Create instances of the concrete model\n        base_instance = ConcreteModel.objects.create(name=\"John\", age=30)\n\n        # Ensure the proxy model instance behaves correctly\n        proxy_instance = ProxyModel.objects.get(pk=base_instance.pk)\n        self.assertEqual(proxy_instance.name, \"John\")\n        self.assertEqual(proxy_instance.age, 30)\n\n        # Ensure the proxy model instance is an instance of the concrete model\n        self.assertIsInstance(proxy_instance, ConcreteModel)\n        self.assertIsInstance(proxy_instance, ProxyModel)\n\n        # Validate that saving a proxy model instance updates the underlying concrete model\n        proxy_instance.name = \"Jane\"\n        proxy_instance.save()\n        updated_instance = ConcreteModel.objects.get(pk=base_instance.pk)\n        self.assertEqual(updated_instance.name, \"Jane\")\n", "    def test_update_related_object(self):\n        # Test updating a related object using the set method.\n        initial_reporter = Reporter.objects.create(\n            first_name=\"Initial\", last_name=\"Reporter\", email=\"initial@example.com\"\n        )\n        article = Article.objects.create(\n            headline=\"Initial Article\",\n            pub_date=datetime.date(2023, 10, 1),\n            reporter=initial_reporter,\n        )\n        new_reporter = Reporter.objects.create(\n            first_name=\"New\", last_name=\"Reporter\", email=\"new@example.com\"\n        )\n        article.reporter = new_reporter\n        article.save()\n        self.assertEqual(article.reporter, new_reporter)\n\n        # Ensure the reverse side is also updated.\n        self.assertSequenceEqual(initial_reporter.article_set.all(), [])\n        self.assertSequenceEqual(new_reporter.article_set.all(), [article])\n\n        # Ensure the update is reflected in the database.\n        updated_article = Article.objects.get(id=article.id)\n        self.assertEqual(updated_article.reporter, new_reporter)\n"], "sample_408": ["    def test_rename_field_preserved_db_column_with_rename_model(self):\n        \"\"\"\n        RenameField is used if a field is renamed and db_column equal to the\n        old field's column is added along with RenameModel.\n        \"\"\"\n        before = [\n            ModelState(\n                \"app\",\n                \"Foo\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"field\", models.IntegerField()),\n                ],\n            ),\n        ]\n        after = [\n            ModelState(\n                \"app\",\n                \"RenamedFoo\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"renamed_field\", models.IntegerField(db_column=\"field\")),\n                ],\n            ),\n        ]\n        changes = self.get_changes(\n            before, after, MigrationQuestioner({\"ask_rename\": True, \"ask_rename_model\": True})\n        )\n        self.assertNumberMigrations(changes, \"app\", 1)\n        self.assertOperationTypes(changes, \"app\", 0, [\"RenameModel\", \"AlterField\", \"RenameField\"])\n        self.assertOperationAttributes(\n            changes,\n            \"app\",\n            0,\n            0,\n            old_name=\"Foo\",\n            new_name=\"RenamedFoo\",\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"app\",\n            0,\n            1,\n            model_name=\"renamedfoo\",\n            name=\"field\",\n        )\n        self.assertEqual(\n            changes[\"app\"][0].operations[1].field.deconstruct(),\n            (\n                \"field\",\n                \"django.db.models.IntegerField\",\n                [],\n                {\"db_column\": \"field\"},\n            ),\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"app\",\n            0,\n            2,\n            model_name=\"renamedfoo\",\n            old_name=\"field\",\n            new_name=\"renamed_field\",\n        )\n", "    def test_deep_deconstruct_with_complex_object(self):\n        \"\"\"\n        Test deep_deconstruct method with a complex object containing various data types.\n        \"\"\"\n        class ComplexObject:\n                return (\n                    \"tests.test_autodetector.ComplexObject\",\n                    [],\n                    {\"attribute\": \"value\"},\n                )\n\n        complex_obj = ComplexObject()\n        nested_obj = DeconstructibleObject(\n            DeconstructibleObject(1),\n            (\n                DeconstructibleObject(\"t1\"),\n                complex_obj,\n            ),\n            a=DeconstructibleObject(\"A\"),\n            b=complex_obj,\n        )\n        result = MigrationAutodetector(None, None).deep_deconstruct(nested_obj)\n        expected = (\n            \"tests.test_autodetector.DeconstructibleObject\",\n            [\n                (\n                    \"tests.test_autodetector.DeconstructibleObject\",\n                    [1],\n                    {},\n                ),\n                (\n                    (\n                        \"tests.test_autodetector.DeconstructibleObject\",\n                        [\"t1\"],\n                        {},\n                    ),\n                    (\n                        \"tests.test_autodetector.ComplexObject\",\n                        [],\n                        {\"attribute\": \"value\"},\n                    ),\n                ),\n            ],\n            {\n                \"a\": (\n                    \"tests.test_autodetector.DeconstructibleObject\",\n                    [\"A\"],\n                    {},\n                ),\n                \"b\": (\n                    \"tests.test_autodetector.ComplexObject\",\n                    [],\n                    {\"attribute\": \"value\"},\n                ),\n            },\n        )\n        self.assertEqual(result, expected)\n", "    def test_deep_deconstruct(self):\n        \"\"\"Tests the deep deconstruction of complex objects.\"\"\"\n        obj = DeconstructibleObject(\n            DeconstructibleObject(1),\n            (DeconstructibleObject(\"t1\"), DeconstructibleObject(\"t2\")),\n            a=DeconstructibleObject(\"A\"),\n            b=DeconstructibleObject(B=DeconstructibleObject(\"c\"))\n        )\n        autodetector = MigrationAutodetector(self.make_project_state([]), self.make_project_state([]))\n        deconstructed = autodetector.deep_deconstruct(obj)\n        expected = (\n            'tests.test_autodetector.DeconstructibleObject',\n            (\n                ('tests.test_autodetector.DeconstructibleObject', (1,), {}),\n                (('tests.test_autodetector.DeconstructibleObject', ('t1',), {}), ('tests.test_autodetector.DeconstructibleObject', ('t2',), {}))\n            ),\n            {\n                'a': ('tests.test_autodetector.DeconstructibleObject', ('A',), {}),\n                'b': ('tests.test_autodetector.DeconstructibleObject', (), {'B': ('tests.test_autodetector.DeconstructibleObject', ('c',), {})}),\n            }\n        )\n        self.assertEqual(deconstructed, expected)\n", "def test_alter_field_with_partial_function(self):\n    \"\"\"\n    Tests that altering a field that uses a functools.partial function\n    is detected correctly.\n    \"\"\"\n        return f\"{instance}_{filename}\"\n\n    before = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"file\",\n                    models.FileField(\n                        max_length=200, upload_to=functools.partial(partial_function, key=\"before\")\n                    ),\n                ),\n            ],\n        )\n    ]\n\n    after = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"file\",\n                    models.FileField(\n                        max_length=200, upload_to=functools.partial(partial_function, key=\"after\")\n                    ),\n                ),\n            ],\n        )\n    ]\n\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    value = changes[\"testapp\"][0].operations[0].field.upload_to\n    self.assertEqual(\n        (partial_function, (), {\"key\": \"after\"}),\n        (value.func, value.args, value.keywords),\n    )\n", "def test_create_model_with_custom_deconstructible_field(self):\n    \"\"\"\n    Test creation of a model with a custom deconstructible field type.\n    \"\"\"\n    class CustomDeconstructibleField(models.CharField):\n            self.custom_attr = kwargs.pop(\"custom_attr\", None)\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            if self.custom_attr:\n                kwargs['custom_attr'] = self.custom_attr\n            return name, path, args, kwargs\n\n    custom_model = ModelState(\n        \"testapp\",\n        \"CustomModel\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"custom_field\", CustomDeconstructibleField(max_length=200, custom_attr=\"test\")),\n        ],\n    )\n    changes = self.get_changes([], [custom_model])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"CustomModel\")\n    # Check custom field attributes\n    self.assertEqual(\n        changes[\"testapp\"][0].operations[0].fields[1][1].custom_attr, \"test\"\n    )\n"], "sample_409": ["    def test_language_filters(self):\n        with translation.override(\"de\"):\n            # Test language_name filter\n            output = self.engine.render_to_string(\"i18n_language_filter_name\")\n            self.assertEqual(output, \"English\")\n            \n            # Test language_name_translated filter\n            output = self.engine.render_to_string(\"i18n_language_filter_name_translated\")\n            self.assertEqual(output, \"Englisch\")\n            \n            # Test language_name_local filter\n            output = self.engine.render_to_string(\"i18n_language_filter_name_local\")\n            self.assertEqual(output, \"English\")\n            \n            # Test language_bidi filter\n            output = self.engine.render_to_string(\"i18n_language_filter_bidi\")\n            self.assertEqual(output, \"False\")\n", "def test_get_language_info(self):\n    \"\"\"Test retrieving language information for a specific language code\"\"\"\n    output = self.engine.render_to_string(\"i18n_get_language_info\")\n    self.assertEqual(output, \"English\")\n", "    def test_language_tag(self):\n        \"\"\"Test the 'language' tag for setting a block-specific language.\"\"\"\n        output = self.engine.render_to_string(\"language_tag\")\n        self.assertEqual(output, \"Bonjour\")\n", "def test_i18n42(self):\n    \"\"\"simple translation with hardcoded variable\"\"\"\n    with translation.override(\"es\"):\n        output = self.engine.render_to_string(\"i18n42\")\n    self.assertEqual(output, \"\u00a1Hola, world!\")\n", "def test_i18n42(self):\n    \"\"\"\n    Test blocktranslate with 'with', 'context', and 'asvar' options.\n    \"\"\"\n    with translation.override(\"de\"):\n        output = self.engine.render_to_string(\"i18n42\")\n    self.assertEqual(output, \">This is a test with bar<\")\n"], "sample_410": ["    def test_set_unusable_password(self):\n        user = User.objects.create_user(username=\"user\", password=\"foo\")\n        user.set_unusable_password()\n        self.assertFalse(user.has_usable_password())\n        self.assertTrue(user.check_password(None))\n        ", "    def test_normalize_email(self):\n        self.assertEqual(BaseUserManager.normalize_email('TEST@EXAMPLE.COM'), 'TEST@example.com')\n        self.assertEqual(BaseUserManager.normalize_email(' TEST@EXAMPLE.COM '), 'TEST@example.com')\n        self.assertEqual(BaseUserManager.normalize_email('TEST@EXAMPLE.COM '), 'TEST@example.com')\n        self.assertEqual(BaseUserManager.normalize_email(' TEST@EXAMPLE.COM'), 'TEST@example.com')\n        self.assertEqual(BaseUserManager.normalize_email(None), '')\n", "    def test_set_unusable_password(self):\n        \"\"\"\n        Ensure set_unusable_password makes the password unusable.\n        \"\"\"\n        user = AbstractBaseUser()\n        user.set_unusable_password()\n        self.assertFalse(user.has_usable_password())\n", "    def setUp(self):\n        self.user = AbstractBaseUser()\n", "    def setUp(self):\n        self.user = AbstractBaseUser()\n"], "sample_411": ["def test_outputwrapper_isatty(self):\n    out = StringIO()\n    wrapper = OutputWrapper(out)\n    with mock.patch.object(out, 'isatty', return_value=True):\n        self.assertTrue(wrapper.isatty())\n    with mock.patch.object(out, 'isatty', return_value=False):\n        self.assertFalse(wrapper.isatty())\n", "    def test_command_error_message(self):\n        \"\"\"Test that CommandError correctly formats its error message.\"\"\"\n        error_message = \"This is a test error message.\"\n        with self.assertRaises(CommandError) as cm:\n            raise CommandError(error_message)\n        self.assertEqual(str(cm.exception), error_message)\n", "def test_command_error_initialization(self):\n    error = CommandError(\"test error\", returncode=2)\n    self.assertEqual(error.returncode, 2)\n    self.assertEqual(str(error), \"test error\")\n", "def test_base_command_initialization(self):\n    \"\"\"Test initialization of BaseCommand with different combinations of no_color and force_color.\"\"\"\n    with self.assertRaises(CommandError):\n        BaseCommand(no_color=True, force_color=True)\n    \n    cmd = BaseCommand(no_color=True)\n    self.assertEqual(cmd.style, no_style())\n    self.assertIsNone(cmd.stderr.style_func)\n\n    cmd = BaseCommand(force_color=True)\n    self.assertEqual(cmd.style, color_style(force_color=True))\n    self.assertEqual(cmd.stderr.style_func, cmd.style.ERROR)\n", "def test_basecommand_get_version(self):\n    \"\"\"\n    Test if the get_version method of BaseCommand returns the correct Django version.\n    \"\"\"\n    command = BaseCommand()\n    self.assertEqual(command.get_version(), django.get_version())\n"], "sample_412": ["    def test_avoid_wrapping(self):\n        items = (\n            (\"hello world\", \"hello\\xa0world\"),\n            (\"keep wrapping text safe\", \"keep\\xa0wrapping\\xa0text\\xa0safe\"),\n            (\"multiple  spaces\", \"multiple\\xa0\\xa0spaces\"),\n            (\"no_wrapping_needed\", \"no_wrapping_needed\"),\n            (\" \", \"\\xa0\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"This is a test string\", \"This\\xa0is\\xa0a\\xa0test\\xa0string\"),\n            (\"Another example\", \"Another\\xa0example\"),\n            (\"Text with  multiple  spaces\", \"Text\\xa0with\\xa0\\xa0multiple\\xa0\\xa0spaces\"),\n            (\"NoSpaces\", \"NoSpaces\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"Hello world\", \"Hello\\xa0world\"),\n            (\"Keep this sentence together\", \"Keep\\xa0this\\xa0sentence\\xa0together\"),\n            (\"No breaking\", \"No\\xa0breaking\"),\n            (\"\", \"\"),  # Test for empty string\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n            (\"Wrap this text\", \"Wrap\\xa0this\\xa0text\"),\n            (\"Multiple   spaces\", \"Multiple\\xa0\\xa0\\xa0spaces\"),\n            (\"NoWrap\", \"NoWrap\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"Hello World\", \"Hello\\xa0World\"),\n            (\"No wrapping here\", \"No\\xa0wrapping\\xa0here\"),\n            (\"Keep spaces intact\", \"Keep\\xa0spaces\\xa0intact\"),\n            (\"Multiple   spaces\", \"Multiple\\xa0\\xa0\\xa0spaces\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n"], "sample_413": ["    def test_template_tags_no_libraries_option(self):\n        with self.settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"OPTIONS\": {},\n                },\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def test_template_tags_with_no_libraries_specified(self):\n        with self.settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"OPTIONS\": {\n                        \"libraries\": {}\n                    },\n                }\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "def test_template_tags_with_multiple_libraries_and_no_conflict(self):\n    with self.settings(\n        TEMPLATES=[\n            self.get_settings(\n                \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n            ),\n            self.get_settings(\n                \"different_tags\", \"same_tags_app_2.templatetags.different_tags\"\n            ),\n        ]\n    ):\n        self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def test_template_tags_with_different_library_and_module_names(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"first_tag\", \"first_app.templatetags.first_tag\"\n                ),\n                self.get_settings(\n                    \"second_tag\", \"second_app.templatetags.second_tag\"\n                ),\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def test_template_tags_no_libraries_defined(self):\n        with self.settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"OPTIONS\": {},\n                },\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n"], "sample_414": ["    def test_model_admin_permissions(self):\n        \"\"\"\n        Ensure that the has_add_permission, has_change_permission,\n        has_delete_permission, and has_view_permission methods of ModelAdmin\n        correctly check user permissions.\n        \"\"\"\n        class MockUser:\n                self.perms = perms\n\n                return perm in self.perms\n\n                return app_label in self.perms\n\n        class TestModelAdmin(ModelAdmin):\n            model = Band\n\n        perms = {\n            \"admin_widgets.add_band\",\n            \"admin_widgets.change_band\",\n            \"admin_widgets.delete_band\",\n            \"admin_widgets.view_band\",\n        }\n        user = MockUser(perms)\n        \n        model_admin = TestModelAdmin(Band, admin.site)\n        \n        request = type(\"Request\", (), {\"user\": user})\n\n        # Test add permission\n        self.assertTrue(model_admin.has_add_permission(request))\n\n        # Test change permission\n        self.assertTrue(model_admin.has_change_permission(request))\n\n        # Test delete permission\n        self.assertTrue(model_admin.has_delete_permission(request))\n\n        # Test view permission\n        self.assertTrue(model_admin.has_view_permission(request))\n", "    def test_has_module_permission(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            pass\n\n        ma = MyModelAdmin(Event, admin.site)\n        request = self.client.request().wsgi_request\n        request.user = self.superuser\n        self.assertTrue(ma.has_module_permission(request))\n", "    def setUp(self):\n        self.model_admin = admin.ModelAdmin(Event, admin.site)\n", "    def test_has_module_permission(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            pass\n\n        ma = MyModelAdmin(Event, admin.site)\n        self.client.force_login(self.superuser)\n        request = self.client.request().wsgi_request\n        self.assertTrue(ma.has_module_permission(request))\n", "    def test_default_get_extra(self):\n        class MyInline(admin.StackedInline):\n            model = Member\n\n        inline = MyInline(Individual, admin.site)\n        self.assertEqual(inline.get_extra(None), 3)\n"], "sample_415": ["    def test_unique_constraint_with_custom_violation_error_message(self):\n        custom_message = \"Custom unique constraint violation error.\"\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"unique_name\",\n            violation_error_message=custom_message,\n        )\n        non_unique_product = UniqueConstraintProduct(name=self.p1.name)\n        with self.assertRaisesMessage(ValidationError, custom_message):\n            constraint.validate(UniqueConstraintProduct, non_unique_product)\n", "    def test_clone(self):\n        constraint_1 = models.UniqueConstraint(\n            fields=[\"foo\", \"bar\"],\n            name=\"unique\",\n            condition=models.Q(foo=models.F(\"bar\")),\n            deferrable=models.Deferrable.DEFERRED,\n            include=[\"baz_1\", \"baz_2\"],\n            opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n        )\n        constraint_2 = constraint_1.clone()\n        self.assertEqual(constraint_1, constraint_2)\n        self.assertIsNot(constraint_1, constraint_2)\n", "    def test_deferrable_enum_repr(self):\n        self.assertEqual(repr(models.Deferrable.DEFERRED), \"Deferrable.DEFERRED\")\n        self.assertEqual(repr(models.Deferrable.IMMEDIATE), \"Deferrable.IMMEDIATE\")\n", "    def test_invalid_violation_error_message(self):\n        msg = \"BaseConstraint.violation_error_message must be a string or None.\"\n        with self.assertRaisesMessage(TypeError, msg):\n            BaseConstraint(\"name\", violation_error_message=123)\n", "    def test_violation_error_message_default(self):\n        constraint = models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n        self.assertEqual(\n            constraint.get_violation_error_message(),\n            \"Constraint \u201cunique_name\u201d is violated.\"\n        )\n"], "sample_416": ["    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"defaultuser\",\n                    \"PASSWORD\": \"defaultpassword\",\n                    \"HOST\": \"defaulthost\",\n                    \"PORT\": \"5432\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"defaultuser\", \"-h\", \"defaulthost\", \"-p\", \"5432\", \"postgres\"],\n                {\"PGPASSWORD\": \"defaultpassword\"},\n            ),\n        )\n", "    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"someuser\",\n                    \"PASSWORD\": \"somepassword\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"444\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n                {\"PGPASSWORD\": \"somepassword\"},\n            ),\n        )\n", "    def test_no_dbname_or_service(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"someuser\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"444\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n                None,\n            ),\n        )\n", "    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"someuser\",\n                    \"PASSWORD\": \"somepassword\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"444\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n                {\"PGPASSWORD\": \"somepassword\"},\n            ),\n        )\n", "    def test_no_dbname_no_service(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"someuser\",\n                    \"PASSWORD\": \"somepassword\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"444\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n                {\"PGPASSWORD\": \"somepassword\"},\n            ),\n        )\n"], "sample_417": ["    def test_floatformat_with_safe_string(self):\n        self.assertEqual(floatformat(mark_safe(\"123.456\"), 2), \"123.46\")\n", "    def test_invalid_types(self):\n        self.assertEqual(floatformat([1, 2, 3]), \"\")\n        self.assertEqual(floatformat({\"a\": 1}), \"\")\n        self.assertEqual(floatformat(set([1, 2, 3])), \"\")\n        self.assertEqual(floatformat((1, 2, 3)), \"\")\n        self.assertEqual(floatformat(True), \"\")\n        self.assertEqual(floatformat(False), \"\")\n", "    def test_string_inputs(self):\n        self.assertEqual(floatformat(\"123.456\", 2), \"123.46\")\n        self.assertEqual(floatformat(\"123.451\", 2), \"123.45\")\n        self.assertEqual(floatformat(\"-123.456\", 2), \"-123.46\")\n        self.assertEqual(floatformat(\"-123.451\", 2), \"-123.45\")\n        self.assertEqual(floatformat(\"123.456\", \"2g\"), \"123.46\")\n        self.assertEqual(floatformat(\"-123.456\", \"2g\"), \"-123.46\")\n        self.assertEqual(floatformat(\"123.456\", \"-2\"), \"123.46\")\n        self.assertEqual(floatformat(\"-123.456\", \"-2\"), \"-123.46\")\n", "    def test_floatformat_with_large_numbers(self):\n        # Testing large numbers to ensure correct formatting and rounding.\n        self.assertEqual(floatformat(1234567890123456.789, 2), \"1234567890123456.79\")\n        self.assertEqual(floatformat(1234567890123456.789, -2), \"1234567890123456.79\")\n        self.assertEqual(floatformat(98765432109876543210.123456, 6), \"98765432109876543210.123456\")\n        self.assertEqual(floatformat(98765432109876543210.123456, -6), \"98765432109876543210.123456\")\n        self.assertEqual(floatformat(98765432109876543210.000001, 6), \"98765432109876543210.000001\")\n        self.assertEqual(floatformat(98765432109876543210.000001, -6), \"98765432109876543210.000001\")\n", "    def test_string_inputs(self):\n        # Testing string inputs that should be converted to Decimal\n        self.assertEqual(floatformat(\"12345.6789\", 2), \"12345.68\")\n        self.assertEqual(floatformat(\"-9876.54321\", 3), \"-9876.543\")\n        self.assertEqual(floatformat(\"123.456789\", -3), \"123.457\")\n        self.assertEqual(floatformat(\"0.000123456\", 8), \"0.00012346\")\n        self.assertEqual(floatformat(\"-0.000987654\", 6), \"-0.000988\")\n"], "sample_418": ["def test_length_is12(self):\n    output = self.engine.render_to_string(\n        \"length_is12\", {\"some_list\": [1, 2, 3]}\n    )\n    self.assertEqual(output, \"Three\")\n", "    def test_length_is12(self):\n        output = self.engine.render_to_string(\"length_is12\", {\"mydict\": {\"a\": 1, \"b\": 2}})\n        self.assertEqual(output, \"Length is 2\")\n", "def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {\"some_dict\": {\"key1\": \"value1\", \"key2\": \"value2\"}})\n    self.assertEqual(output, \"Length is 2\")\n", "def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {\"mylist\": [1, 2, 3]})\n    self.assertEqual(output, \"Length is 3\")\n", "    def test_floatformat01(self):\n        output = self.engine.render_to_string(\"floatformat01\", {\"number\": 34.23234})\n        self.assertEqual(output, \"34.2\")\n"], "sample_419": ["    def test_management_form_clean(self):\n        \"\"\"\n        The ManagementForm should correctly clean and set default values\n        for TOTAL_FORM_COUNT and INITIAL_FORM_COUNT.\n        \"\"\"\n        formset = self.make_choiceformset()\n        management_form = formset.management_form\n        self.assertTrue(management_form.is_valid())\n        cleaned_data = management_form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 1)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n", "    def test_management_form_clean(self):\n        \"\"\"\n        Test that the clean method in ManagementForm correctly sets default values\n        for TOTAL_FORM_COUNT and INITIAL_FORM_COUNT when the form is invalid.\n        \"\"\"\n        # Define data with missing TOTAL_FORM_COUNT\n        data = {\n            \"choices-INITIAL_FORMS\": \"0\",\n        }\n        management_form = ManagementForm(data, prefix=\"choices\")\n        cleaned_data = management_form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n\n        # Define data with missing INITIAL_FORM_COUNT\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n        }\n        management_form = ManagementForm(data, prefix=\"choices\")\n        cleaned_data = management_form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 1)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n", "def test_baseformset_repr_with_empty_formset(self):\n    \"\"\"\n    Test the __repr__ method of BaseFormSet when the formset is empty.\n    \"\"\"\n    class EmptyForm(Form):\n        pass\n\n    EmptyFormSet = formset_factory(EmptyForm, extra=0)\n    empty_formset = EmptyFormSet()\n    self.assertEqual(\n        repr(empty_formset), \"<EmptyFormFormSet: bound=False valid=Unknown total_forms=0>\"\n    )\n", "    def test_management_form_clean(self):\n        \"\"\"\n        ManagementForm should clean correctly and set defaults for TOTAL_FORMS\n        and INITIAL_FORMS.\n        \"\"\"\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"1\",\n            \"choices-0-choice\": \"Choice 1\",\n            \"choices-0-votes\": \"10\",\n            \"choices-1-choice\": \"Choice 2\",\n            \"choices-1-votes\": \"20\",\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        management_form = formset.management_form\n        self.assertTrue(management_form.is_valid())\n        self.assertEqual(management_form.cleaned_data[TOTAL_FORM_COUNT], 2)\n        self.assertEqual(management_form.cleaned_data[INITIAL_FORM_COUNT], 1)\n", "    def test_initial_form_count(self):\n        \"\"\"\n        Test initial form count calculation based on initial data.\n        \"\"\"\n        initial_data = [{\"choice\": \"Choice 1\", \"votes\": 10}, {\"choice\": \"Choice 2\", \"votes\": 20}]\n        formset = self.make_choiceformset(initial=initial_data)\n        self.assertEqual(formset.initial_form_count(), 2)\n"], "sample_420": ["    def test_baseform_with_widgets_in_meta(self):\n        \"\"\"\n        Using base forms with widgets defined in Meta should not raise errors.\n        \"\"\"\n        widget = forms.Textarea()\n\n        class BaseForm(forms.ModelForm):\n            class Meta:\n                model = Person\n                widgets = {\"name\": widget}\n                fields = \"__all__\"\n\n        Form = modelform_factory(Person, form=BaseForm)\n        self.assertIsInstance(Form.base_fields[\"name\"].widget, forms.Textarea)\n", "    def test_model_form_options(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n            description = models.TextField()\n\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = TestModel\n                fields = ['name']\n                exclude = ['description']\n                widgets = {'name': forms.TextInput(attrs={'class': 'name-field'})}\n                localized_fields = ['name']\n                labels = {'name': 'Custom Name'}\n                help_texts = {'name': 'Enter the name here'}\n                error_messages = {'name': {'required': 'Name is required'}}\n                field_classes = {'name': forms.CharField}\n\n        options = ModelFormOptions(TestForm.Meta)\n        self.assertEqual(options.model, TestModel)\n        self.assertEqual(options.fields, ['name'])\n        self.assertEqual(options.exclude, ['description'])\n        self.assertEqual(options.widgets, {'name': forms.TextInput(attrs={'class': 'name-field'})})\n        self.assertEqual(options.localized_fields, ['name'])\n        self.assertEqual(options.labels, {'name': 'Custom Name'})\n        self.assertEqual(options.help_texts, {'name': 'Enter the name here'})\n        self.assertEqual(options.error_messages, {'name': {'required': 'Name is required'}})\n        self.assertEqual(options.field_classes, {'name': forms.CharField})\n", "    def test_metaclass_model_validation(self):\n        \"\"\"\n        Ensure ModelFormMetaclass raises ImproperlyConfigured error when neither\n        fields nor exclude are specified in Meta.\n        \"\"\"\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            \"Creating a ModelForm without either the 'fields' attribute or the 'exclude' \"\n            \"attribute is prohibited; form TestForm needs updating.\",\n        ):\n            class TestForm(forms.ModelForm):\n                class Meta:\n                    model = Category\n", "    def test_limit_choices_to_applied_to_queryset(self):\n        \"\"\"\n        Test that apply_limit_choices_to_to_formfield correctly applies\n        limit_choices_to to a formfield's queryset.\n        \"\"\"\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n            is_active = models.BooleanField(default=True)\n\n            class Meta:\n                app_label = 'tests'\n\n        field = models.ForeignKey(TestModel, on_delete=models.CASCADE, limit_choices_to={'is_active': True})\n\n        formfield = field.formfield()\n        apply_limit_choices_to_to_formfield(formfield)\n\n        self.assertIn('is_active', str(formfield.queryset.query))\n", "    def test_construct_instance_with_no_fields(self):\n        \"\"\"\n        Test construct_instance with no fields should not modify instance fields.\n        \"\"\"\n        person = Person.objects.create(name=\"John Doe\")\n        form = modelform_factory(Person, fields=\"__all__\")({\"name\": \"Jane Doe\"})\n        updated_instance = construct_instance(form, person, fields=())\n        self.assertEqual(updated_instance.name, \"John Doe\")\n"], "sample_421": ["    def test_combined_expression_as_sql(self):\n        lhs = F(\"some_field\")\n        rhs = Value(5)\n        expr = CombinedExpression(lhs, Combinable.ADD, rhs)\n        compiler = connection.ops.compiler(\"SQLCompiler\")(\n            query=None, connection=connection, using=None\n        )\n        sql, params = expr.as_sql(compiler, connection)\n        self.assertEqual(sql, \"(some_field + %s)\")\n        self.assertEqual(params, [5])\n", "    def test_combined_expression_addition(self):\n        expr1 = CombinedExpression(Value(2, output_field=IntegerField()), Combinable.ADD, Value(3, output_field=IntegerField()))\n        sql, params = expr1.as_sql(None, connection)\n        self.assertEqual(sql, \"(%s + %s)\")\n        self.assertEqual(params, [2, 3])\n", "    def test_combined_expression_arithmetic(self):\n        value1 = Value(5, output_field=IntegerField())\n        value2 = Value(3, output_field=IntegerField())\n        \n        addition = CombinedExpression(value1, Combinable.ADD, value2)\n        self.assertEqual(str(addition), \"5 + 3\")\n        \n        subtraction = CombinedExpression(value1, Combinable.SUB, value2)\n        self.assertEqual(str(subtraction), \"5 - 3\")\n        \n        multiplication = CombinedExpression(value1, Combinable.MUL, value2)\n        self.assertEqual(str(multiplication), \"5 * 3\")\n        \n        division = CombinedExpression(value1, Combinable.DIV, value2)\n        self.assertEqual(str(division), \"5 / 3\")\n        \n        modulo = CombinedExpression(value1, Combinable.MOD, value2)\n        self.assertEqual(str(modulo), \"5 %% 3\")\n        \n        power = CombinedExpression(value1, Combinable.POW, value2)\n        self.assertEqual(str(power), \"5 ^ 3\")\n        ", "    def test_combined_expression_with_subquery(self):\n        subquery = CaseTestModel.objects.filter(integer=F('integer2')).values('pk')[:1]\n        self.assertQuerysetEqual(\n            CaseTestModel.objects.annotate(\n                test=Case(\n                    When(pk__in=subquery, then=Value(\"Match\")),\n                    default=Value(\"No Match\"),\n                )\n            ).order_by(\"pk\"),\n            [\n                (1, \"Match\"),\n                (2, \"No Match\"),\n                (3, \"No Match\"),\n                (2, \"No Match\"),\n                (3, \"No Match\"),\n                (3, \"Match\"),\n                (4, \"No Match\"),\n            ],\n            transform=attrgetter(\"integer\", \"test\"),\n        )\n", "    def test_expression_equality(self):\n        expr1 = Expression(output_field=IntegerField())\n        expr2 = Expression(output_field=IntegerField())\n        expr3 = Expression(output_field=CharField())\n        self.assertEqual(expr1, expr2)\n        self.assertNotEqual(expr1, expr3)\n"], "sample_422": ["    def test_foreignkey_forward_with_none(self):\n        \"\"\"\n        Test ForwardManyToOneDescriptor with a ForeignKey field that allows None.\n        \"\"\"\n        orphan_book = Book.objects.create(title=\"Orphan Book\")\n        Author.objects.create(name=\"Orphan Author\", first_book=None)\n\n        with self.assertNumQueries(2):\n            books = [\n                a.first_book for a in Author.objects.prefetch_related(\"first_book\")\n            ]\n\n        normal_books = [a.first_book for a in Author.objects.all()]\n        self.assertEqual(books, normal_books)\n        self.assertIn(orphan_book, books)\n        self.assertIsNone(normal_books[-1])  # The last author has no first_book\n", "def test_forward_many_to_one_descriptor(self):\n    \"\"\"\n    Test the ForwardManyToOneDescriptor for accessing related objects on the forward side.\n    \"\"\"\n    # Create a new author and assign an existing book as their first book\n    author = Author.objects.create(name=\"George\", first_book=self.book3)\n    \n    with self.assertNumQueries(1):\n        # Access the first book of the newly created author\n        first_book = author.first_book\n    \n    self.assertEqual(first_book, self.book3)\n    \n    with self.assertNumQueries(1):\n        # Change the first book of the author and check if the relation is updated\n        new_book = Book.objects.create(title=\"New Book\")\n        author.first_book = new_book\n        author.save()\n    \n    with self.assertNumQueries(1):\n        # Access the updated first book\n        updated_first_book = Author.objects.get(pk=author.pk).first_book\n    \n    self.assertEqual(updated_first_book, new_book)\n", "    def test_foreignkey_deferred_attribute(self):\n        book = Book.objects.create(title=\"Gulliver's Travels\")\n        author = Author.objects.create(name=\"Jonathan Swift\", first_book=book)\n        foreign_key_deferred_attr = ForeignKeyDeferredAttribute(field=Author._meta.get_field('first_book'))\n        \n        # Test setting value through ForeignKeyDeferredAttribute\n        with self.assertNumQueries(1):\n            foreign_key_deferred_attr.__set__(author, book)\n            self.assertEqual(author.first_book, book)\n\n        # Test setting value to None\n        with self.assertNumQueries(1):\n            foreign_key_deferred_attr.__set__(author, None)\n            self.assertIsNone(author.first_book)\n", "def test_forward_onetoone_descriptor(self):\n    \"\"\"\n    Test the `ForwardOneToOneDescriptor` to ensure it correctly handles\n    setting and getting related objects in one-to-one relationships.\n    \"\"\"\n    # Create related models\n    book = Book.objects.create(title=\"A Tale of Two Cities\")\n    author = Author.objects.create(name=\"Charles Dickens\", first_book=book)\n\n    # Create the one-to-one relationship\n    bio = Bio.objects.create(author=author)\n\n    with self.assertNumQueries(2):\n        # Test retrieving the related object\n        fetched_author = Author.objects.select_related(\"bio\").get(pk=author.pk)\n        self.assertEqual(fetched_author.bio, bio)\n\n    with self.assertNumQueries(1):\n        # Test setting the related object\n        new_bio = Bio.objects.create(author=author)\n        fetched_author.bio = new_bio\n        self.assertEqual(fetched_author.bio, new_bio)\n\n    with self.assertNumQueries(1):\n        # Test setting the related object to None\n        fetched_author.bio = None\n        self.assertIsNone(fetched_author.bio)\n\n    with self.assertNumQueries(2):\n        # Test the reverse side of one-to-one relationship\n        bio_with_author = Bio.objects.select_related(\"author\").get(pk=new_bio.pk)\n        self.assertEqual(bio_with_author.author, author)\n", "    def test_foreignkey_deferred_attribute(self):\n        \"\"\"\n        Test that ForeignKeyDeferredAttribute correctly sets and deletes cache.\n        \"\"\"\n        with self.assertNumQueries(1):\n            author = Author.objects.get(pk=self.author1.pk)\n        # The related first_book should be cached\n        self.assertTrue(Author.first_book.is_cached(author))\n        # Change first_book to another book\n        author.first_book = self.book2\n        author.save()\n        # The related first_book should be updated and cached\n        self.assertEqual(author.first_book, self.book2)\n        self.assertTrue(Author.first_book.is_cached(author))\n"], "sample_423": ["    def test_add_custom_deconstructible_object(self):\n        \"\"\"Test addition of a field with a custom deconstructible object default.\"\"\"\n        custom_object = DeconstructibleObject(\"arg1\", kwarg1=\"kwarg_value\")\n        author_with_custom_deconstructible = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"custom_field\",\n                    models.CharField(max_length=200, default=custom_object),\n                ),\n            ],\n        )\n        changes = self.get_changes(\n            [self.author_empty], [self.author_empty, author_with_custom_deconstructible]\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"custom_field\")\n        self.assertOperationFieldAttributes(\n            changes, \"testapp\", 0, 0, default=custom_object\n        )\n", "    def test_generate_renamed_models(self):\n        \"\"\"Test the detection and generation of renamed models.\"\"\"\n        before = [\n            ModelState(\n                \"app\",\n                \"OldModel\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            ModelState(\n                \"app\",\n                \"OtherModel\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n        ]\n        after = [\n            ModelState(\n                \"app\",\n                \"NewModel\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            ModelState(\n                \"app\",\n                \"OtherModel\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n        ]\n        changes = self.get_changes(\n            before, after, MigrationQuestioner({\"ask_rename_model\": True})\n        )\n        self.assertNumberMigrations(changes, \"app\", 1)\n        self.assertOperationTypes(changes, \"app\", 0, [\"RenameModel\"])\n        self.assertOperationAttributes(\n            changes,\n            \"app\",\n            0,\n            0,\n            old_name=\"OldModel\",\n            new_name=\"NewModel\",\n        )\n", "    def test_alter_model_table_and_alter_field(self):\n        \"\"\"\n        Test scenario where both table name and a field are altered simultaneously.\n        \"\"\"\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                ],\n                {\"db_table\": \"author_one\"},\n            ),\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=250)),\n                ],\n                {\"db_table\": \"author_two\"},\n            ),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\", \"AlterField\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"author\", table=\"author_two\"\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 1, name=\"name\", max_length=250\n        )\n", "    def test_alter_field_with_different_regex_flags(self):\n        \"\"\"\n        Ensure that altering a field with a regex validator having different flags\n        results in an AlterField operation.\n        \"\"\"\n        from_state = ModelState(\n            \"testapp\",\n            \"ModelWithRegex\",\n            [\n                (\n                    \"id\",\n                    models.AutoField(primary_key=True),\n                ),\n                (\n                    \"regex_field\",\n                    models.CharField(\n                        max_length=255,\n                        validators=[\n                            RegexValidator(\n                                regex=re.compile(r\"^[a-z]+$\", re.IGNORECASE),\n                                message=\"Only lowercase letters allowed.\"\n                            )\n                        ],\n                    ),\n                ),\n            ],\n        )\n        to_state = ModelState(\n            \"testapp\",\n            \"ModelWithRegex\",\n            [\n                (\n                    \"id\",\n                    models.AutoField(primary_key=True),\n                ),\n                (\n                    \"regex_field\",\n                    models.CharField(\n                        max_length=255,\n                        validators=[\n                            RegexValidator(\n                                regex=re.compile(r\"^[a-z]+$\", re.IGNORECASE | re.MULTILINE),\n                                message=\"Only lowercase letters allowed.\"\n                            )\n                        ],\n                    ),\n                ),\n            ],\n        )\n        changes = self.get_changes([from_state], [to_state])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"regex_field\")\n", "    def test_alter_multiple_fields_same_model(self):\n        \"\"\"\n        Test altering multiple fields within the same model in a single migration.\n        \"\"\"\n        before = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"birthdate\", models.DateField(null=True)),\n            ],\n        )\n        after = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=400)),  # Altering max_length\n                (\"birthdate\", models.DateField(null=False)),  # Changing null to False\n            ],\n        )\n        changes = self.get_changes([before], [after])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\", \"AlterField\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 1, name=\"birthdate\", preserve_default=True\n        )\n"], "sample_424": ["    def test_create_model_with_duplicate_field_name_case_insensitive(self):\n        with self.assertRaisesMessage(\n            ValueError, \"Found duplicate value Pink in CreateModel fields argument.\"\n        ):\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"Pink\", models.TextField()),\n                    (\"pink\", models.IntegerField(default=1)),\n                ],\n            )\n", "    def test_create_model_with_duplicate_options(self):\n        \"\"\"\n        Tests the CreateModel operation with duplicate options.\n        \"\"\"\n        duplicate_options = {\n            \"unique_together\": {(\"field1\", \"field2\"), (\"field1\", \"field2\")},\n            \"index_together\": {(\"field3\", \"field4\"), (\"field3\", \"field4\")},\n        }\n        with self.assertRaisesMessage(\n            ValueError, \"Found duplicate value ('field1', 'field2') in CreateModel options argument.\"\n        ):\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"field1\", models.IntegerField(default=1)),\n                    (\"field2\", models.IntegerField(default=2)),\n                    (\"field3\", models.IntegerField(default=3)),\n                    (\"field4\", models.IntegerField(default=4)),\n                ],\n                options=duplicate_options,\n            )\n", "    def test_create_model_with_duplicate_field_name(self):\n        with self.assertRaisesMessage(\n            ValueError, \"Found duplicate value pink in CreateModel fields argument.\"\n        ):\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.TextField()),\n                    (\"pink\", models.IntegerField(default=1)),\n                ],\n            )\n", "    def test_delete_model_with_constraints(self):\n        \"\"\"\n        Tests the DeleteModel operation with constraints.\n        \"\"\"\n        project_state = self.set_up_test_model(\n            \"test_dlmc\",\n            constraints=[\n                models.CheckConstraint(check=models.Q(pink__gt=0), name=\"check_pink_gt_0\"),\n            ],\n        )\n        # Test the state alteration\n        operation = migrations.DeleteModel(\"Pony\")\n        self.assertEqual(operation.describe(), \"Delete model Pony\")\n        self.assertEqual(operation.migration_name_fragment, \"delete_pony\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_dlmc\", new_state)\n        self.assertNotIn((\"test_dlmc\", \"pony\"), new_state.models)\n        # Test the database alteration\n        self.assertTableExists(\"test_dlmc_pony\")\n        self.assertConstraintExists(\"test_dlmc_pony\", \"check_pink_gt_0\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_dlmc\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_dlmc_pony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_dlmc\", editor, new_state, project_state)\n        self.assertTableExists(\"test_dlmc_pony\")\n        self.assertConstraintExists(\"test_dlmc_pony\", \"check_pink_gt_0\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"DeleteModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(list(definition[2]), [\"name\"])\n", "    def test_references_model_via_bases(self):\n        \"\"\"\n        Test that references_model correctly identifies models referenced through bases.\n        \"\"\"\n        operation = migrations.CreateModel(\n            \"DerivedModel\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            bases=(\"OtherModel\",)\n        )\n        self.assertTrue(operation.references_model(\"OtherModel\", \"migrations\"))\n        self.assertFalse(operation.references_model(\"NonReferencedModel\", \"migrations\"))\n"], "sample_425": ["    def test_serialize_function_type_with_self(self):\n        class ClassWithMethod:\n                pass\n\n        instance = ClassWithMethod()\n        value = instance.method\n        result = self.serialize_round_trip(value)\n        self.assertEqual(result, value)\n        self.assertEqual(result.__self__, instance)\n        self.assertEqual(result.__name__, \"method\")\n", "    def test_serialize_deconstructable(self):\n        class CustomDeconstructable:\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n                return (\n                    'migrations.test_writer.CustomDeconstructable',\n                    [self.arg1, self.arg2],\n                    {}\n                )\n\n        instance = CustomDeconstructable(\"value1\", 42)\n        self.assertSerializedResultEqual(\n            instance,\n            (\n                \"migrations.test_writer.CustomDeconstructable('value1', 42)\",\n                {\"import migrations.test_writer\"},\n            ),\n        )\n        self.assertSerializedEqual(instance)\n", "    def test_serialize_models_field(self):\n        \"\"\"\n        Test serialization of Django model fields.\n        \"\"\"\n        field = models.CharField(max_length=100, null=True, blank=True)\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(\n            string,\n            \"models.CharField(blank=True, max_length=100, null=True)\"\n        )\n        self.assertEqual(imports, {\"from django.db import models\"})\n        self.serialize_round_trip(field)\n\n        field = models.IntegerField(default=42)\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, \"models.IntegerField(default=42)\")\n        self.assertEqual(imports, {\"from django.db import models\"})\n        self.serialize_round_trip(field)\n\n        field = models.DateTimeField(auto_now=True)\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, \"models.DateTimeField(auto_now=True)\")\n        self.assertEqual(imports, {\"from django.db import models\"})\n        self.serialize_round_trip(field)\n", "    def test_serialize_custom_deconstructable(self):\n        @deconstructible\n        class CustomClass:\n                self.name = name\n                self.value = value\n\n                return (\n                    \"migrations.test_writer.CustomClass\",\n                    [self.name, self.value],\n                    {},\n                )\n\n        instance = CustomClass(\"test_name\", 42)\n        string, imports = MigrationWriter.serialize(instance)\n        self.assertEqual(\n            string, \n            \"migrations.test_writer.CustomClass('test_name', 42)\"\n        )\n        self.assertEqual(\n            imports, \n            {\"import migrations.test_writer\"}\n        )\n        self.assertEqual(\n            self.serialize_round_trip(instance).__dict__,\n            instance.__dict__\n        )\n", "    def test_serialize_regex_flags(self):\n        pattern = re.compile(r\"^foo$\", re.IGNORECASE | re.MULTILINE)\n        serialized_string, imports = MigrationWriter.serialize(pattern)\n        self.assertEqual(\n            serialized_string,\n            \"re.compile('^foo$', re.RegexFlag['IGNORECASE'] | re.RegexFlag['MULTILINE'])\"\n        )\n        self.assertEqual(imports, {\"import re\"})\n        self.assertEqual(self.serialize_round_trip(pattern).flags, pattern.flags)\n"], "sample_426": ["def test_custom_time_strings(self):\n    \"\"\"Test timesince with custom time strings.\"\"\"\n    custom_time_strings = {\n        \"year\": npgettext_lazy(\"custom\", \"%(num)d yr\", \"%(num)d yrs\", \"num\"),\n        \"month\": npgettext_lazy(\"custom\", \"%(num)d mo\", \"%(num)d mos\", \"num\"),\n        \"week\": npgettext_lazy(\"custom\", \"%(num)d wk\", \"%(num)d wks\", \"num\"),\n        \"day\": npgettext_lazy(\"custom\", \"%(num)d d\", \"%(num)d ds\", \"num\"),\n        \"hour\": npgettext_lazy(\"custom\", \"%(num)d hr\", \"%(num)d hrs\", \"num\"),\n        \"minute\": npgettext_lazy(\"custom\", \"%(num)d min\", \"%(num)d mins\", \"num\"),\n    }\n    self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), \"1\\xa0hr\")\n    self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), \"1\\xa0d\")\n    self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), \"1\\xa0wk\")\n    self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), \"1\\xa0mo\")\n    self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), \"1\\xa0yr\")\n    self.assertEqual(\n        timesince(self.t, self.t + 2 * self.oneday + 6 * self.onehour, time_strings=custom_time_strings),\n        \"2\\xa0ds, 6\\xa0hrs\",\n    )\n", "    def test_custom_time_strings(self):\n        \"\"\"Test custom time strings.\"\"\"\n        custom_time_strings = {\n            \"year\": \"%(num)d yr\",\n            \"month\": \"%(num)d mo\",\n            \"week\": \"%(num)d wk\",\n            \"day\": \"%(num)d d\",\n            \"hour\": \"%(num)d hr\",\n            \"minute\": \"%(num)d min\"\n        }\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings),\n            \"1\\xa0yr\"\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings),\n            \"1\\xa0mo\"\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings),\n            \"1\\xa0wk\"\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings),\n            \"1\\xa0d\"\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings),\n            \"1\\xa0hr\"\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings),\n            \"1\\xa0min\"\n        )\n", "    def test_fractional_days(self):\n        \"\"\"Test timesince with fractional days.\"\"\"\n        t = datetime.datetime(2022, 1, 1, 0, 0)\n        tests = [\n            (t + datetime.timedelta(hours=12), \"12\\xa0hours\"),\n            (t + datetime.timedelta(hours=36), \"1\\xa0day, 12\\xa0hours\"),\n            (t + datetime.timedelta(days=1, hours=6), \"1\\xa0day, 6\\xa0hours\"),\n            (t + datetime.timedelta(days=2, hours=12), \"2\\xa0days, 12\\xa0hours\"),\n        ]\n        for value, expected in tests:\n            with self.subTest():\n                self.assertEqual(timesince(t, value), expected)\n", "def test_time_strings_override(self):\n    \"\"\"Test overriding time strings.\"\"\"\n    custom_time_strings = {\n        \"year\": npgettext_lazy(\"custom\", \"%(num)d y\", \"%(num)d ys\", \"num\"),\n        \"month\": npgettext_lazy(\"custom\", \"%(num)d mth\", \"%(num)d mths\", \"num\"),\n        \"week\": npgettext_lazy(\"custom\", \"%(num)d wk\", \"%(num)d wks\", \"num\"),\n        \"day\": npgettext_lazy(\"custom\", \"%(num)d d\", \"%(num)d ds\", \"num\"),\n        \"hour\": npgettext_lazy(\"custom\", \"%(num)d hr\", \"%(num)d hrs\", \"num\"),\n        \"minute\": npgettext_lazy(\"custom\", \"%(num)d min\", \"%(num)d mins\", \"num\"),\n    }\n    t = datetime.datetime(2021, 1, 1)\n    self.assertEqual(timesince(t, t + self.oneweek, time_strings=custom_time_strings), \"1\\xa0wk\")\n    self.assertEqual(timesince(t, t + self.onemonth, time_strings=custom_time_strings), \"1\\xa0mth\")\n    self.assertEqual(timesince(t, t + self.oneyear, time_strings=custom_time_strings), \"1\\xa0y\")\n    self.assertEqual(\n        timesince(t, t + self.oneyear + self.onemonth + self.oneweek + self.oneday, time_strings=custom_time_strings, depth=4),\n        \"1\\xa0y, 1\\xa0mth, 1\\xa0wk, 1\\xa0d\"\n    )\n", "    def test_non_standard_times(self):\n        \"\"\"Test times that aren't exactly on the boundary of defined units.\"\"\"\n        t = datetime.datetime(2023, 1, 1, 0, 0, 0)\n        tests = [\n            (datetime.datetime(2023, 1, 1, 0, 0, 59), \"0\\xa0minutes\"),  # 59 seconds\n            (datetime.datetime(2023, 1, 1, 0, 1, 1), \"1\\xa0minute\"),    # 1 minute, 1 second\n            (datetime.datetime(2023, 1, 1, 1, 1, 1), \"1\\xa0hour, 1\\xa0minute\"),  # 1 hour, 1 minute, 1 second\n            (datetime.datetime(2023, 1, 2, 1, 1, 1), \"1\\xa0day, 1\\xa0hour\"),  # 1 day, 1 hour, 1 minute, 1 second\n            (datetime.datetime(2023, 1, 8, 1, 1, 1), \"1\\xa0week, 1\\xa0day\"),  # 1 week, 1 day, 1 hour, 1 minute, 1 second\n            (datetime.datetime(2023, 2, 2, 1, 1, 1), \"1\\xa0month, 1\\xa0day\"),  # 1 month, 1 day, 1 hour, 1 minute, 1 second\n            (datetime.datetime(2024, 1, 2, 1, 1, 1), \"1\\xa0year, 1\\xa0day\")   # 1 year, 1 day, 1 hour, 1 minute, 1 second\n        ]\n        for value, expected in tests:\n            with self.subTest():\n                self.assertEqual(timesince(t, value), expected)\n"], "sample_427": ["    def test_formset_management_form_clean(self):\n        \"\"\"\n        Test ManagementForm's clean method to ensure it sets default values\n        for TOTAL_FORMS and INITIAL_FORMS when the management form is invalid.\n        \"\"\"\n        class CustomManagementForm(ManagementForm):\n                cleaned_data = super().clean()\n                cleaned_data[\"extra_field\"] = \"extra_value\"\n                return cleaned_data\n\n        data = {\n            \"form-TOTAL_FORMS\": \"invalid\",\n            \"form-INITIAL_FORMS\": \"invalid\",\n        }\n        formset = formset_factory(FavoriteDrinkForm, formset=BaseFormSet)()\n        management_form = CustomManagementForm(data, prefix=\"form\")\n        self.assertFalse(management_form.is_valid())\n        cleaned_data = management_form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n        self.assertEqual(cleaned_data[\"extra_field\"], \"extra_value\")\n", "    def test_formset_with_custom_template_name(self):\n        \"\"\"\n        Ensure that formsets respect a custom template name if provided.\n        \"\"\"\n        class CustomTemplateFormSet(BaseFormSet):\n            template_name = \"custom_template.html\"\n\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Calexico\",\n            \"choices-0-votes\": \"100\",\n        }\n        CustomFormSet = formset_factory(Choice, formset=CustomTemplateFormSet)\n        formset = CustomFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertEqual(formset.template_name, \"custom_template.html\")\n", "    def test_total_form_count_dos_protection(self):\n        \"\"\"\n        Ensures that total_form_count() method provides DoS protection by limiting the\n        number of forms instantiated when total form count in the data exceeds absolute_max.\n        \"\"\"\n        class CustomForm(Form):\n            field = CharField()\n\n        data = {\n            \"form-TOTAL_FORMS\": \"2000\",  # Large number to test DoS protection.\n            \"form-INITIAL_FORMS\": \"0\",\n            \"form-MIN_NUM_FORMS\": \"0\",\n            \"form-MAX_NUM_FORMS\": \"2000\",\n        }\n\n        CustomFormSet = formset_factory(CustomForm, extra=0, absolute_max=1000)\n        formset = CustomFormSet(data, auto_id=False, prefix=\"form\")\n\n        # Even though TOTAL_FORMS in data is 2000, it should be capped by absolute_max\n        self.assertEqual(formset.total_form_count(), 1000)\n        self.assertEqual(len(formset.forms), 1000)\n", "    def test_custom_deletion_widget(self):\n        \"\"\"\n        A formset should use a custom widget for the deletion field if specified.\n        \"\"\"\n        class CustomDeletionWidgetFormSet(BaseFormSet):\n                return HiddenInput(attrs={\"class\": \"custom-deletion\"})\n\n        ChoiceFormSet = formset_factory(Choice, formset=CustomDeletionWidgetFormSet, can_delete=True)\n        formset = ChoiceFormSet()\n        self.assertHTMLEqual(\n            \"\\n\".join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"form-0-choice\"></li>", "    def test_cleaned_data_property(self):\n        \"\"\"\n        The cleaned_data property should return the cleaned data for all forms\n        in the formset.\n        \"\"\"\n        data = {\n            \"choices-TOTAL_FORMS\": \"3\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Choice 1\",\n            \"choices-0-votes\": \"10\",\n            \"choices-1-choice\": \"Choice 2\",\n            \"choices-1-votes\": \"20\",\n            \"choices-2-choice\": \"Choice 3\",\n            \"choices-2-votes\": \"30\",\n        }\n        formset = ChoiceFormSet(data)\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(\n            formset.cleaned_data,\n            [\n                {\"choice\": \"Choice 1\", \"votes\": 10},\n                {\"choice\": \"Choice 2\", \"votes\": 20},\n                {\"choice\": \"Choice 3\", \"votes\": 30},\n            ],\n        )\n"], "sample_428": ["    def test_none_and_empty(self):\n        self.assertEqual(nformat(None, \".\"), None)\n        self.assertEqual(nformat(\"\", \".\"), \"\")\n", "    def test_invalid_input(self):\n        self.assertEqual(nformat(None, \".\"), None)\n        self.assertEqual(nformat(\"\", \".\"), \"\")\n        self.assertEqual(nformat(\"invalid\", \".\"), \"invalid\")\n        self.assertEqual(nformat(\"1234.invalid\", \".\"), \"1234.invalid\")\n", "    def test_none_and_empty_input(self):\n        self.assertEqual(nformat(None, \".\"), \"\")\n        self.assertEqual(nformat(\"\", \".\"), \"\")\n", "    def test_format_none_and_empty_string(self):\n        self.assertEqual(nformat(None, \".\"), None)\n        self.assertEqual(nformat(\"\", \".\"), \"\")\n", "    def test_format_edge_cases(self):\n        self.assertEqual(nformat(\"\", \".\"), \"\")\n        self.assertEqual(nformat(None, \".\"), None)\n        self.assertEqual(nformat(\"0\", \".\", decimal_pos=2), \"0.00\")\n        self.assertEqual(nformat(\"-0\", \".\", decimal_pos=2), \"0.00\")\n        self.assertEqual(nformat(Decimal(\"0.00\"), \".\", decimal_pos=2), \"0.00\")\n        self.assertEqual(nformat(Decimal(\"-0.00\"), \".\", decimal_pos=2), \"0.00\")\n        self.assertEqual(nformat(Decimal(\"0.00001\"), \".\", decimal_pos=5), \"0.00001\")\n        self.assertEqual(nformat(Decimal(\"0.00001\"), \".\", decimal_pos=4), \"0.0000\")\n"], "sample_429": ["    def test_url_validator_ipv6(self):\n        validator = URLValidator()\n        valid_ipv6_urls = [\n            \"http://[2001:db8::1]/\",\n            \"http://[2001:db8:0:0:0:0:0:1]/\",\n            \"http://[::ffff:192.0.2.128]/\",\n            \"http://[::1]/\",\n            \"http://[::ffff:192.0.2.1]/\",\n            \"http://[::192.9.5.5]/\",\n            \"http://[2001:db8::1]:8080/\",\n            \"http://[::1]:8000/\",\n        ]\n        invalid_ipv6_urls = [\n            \"http://[2001:db8::12345]/\",\n            \"http://[::zzz]/\",\n            \"http://[12345::]/\",\n            \"http://[::ffff:192.0.2.999]/\",\n        ]\n\n        for url in valid_ipv6_urls:\n            with self.subTest(url=url):\n                try:\n                    validator(url)\n                except ValidationError:\n                    self.fail(f\"URLValidator raised ValidationError for valid URL {url}\")\n\n        for url in invalid_ipv6_urls:\n            with self.subTest(url=url):\n                with self.assertRaises(ValidationError):\n                    validator(url)\n", "    def test_url_validator(self):\n        valid_urls = [\n            \"http://example.com\",\n            \"https://example.com\",\n            \"ftp://example.com\",\n            \"ftps://example.com\",\n            \"http://example.com/path\",\n            \"http://example.com:8080/path\",\n            \"http://user:pass@example.com\",\n            \"http://user:pass@example.com:8080/path\",\n            \"http://user:pass@example.com:8080/path?query=1\",\n            \"http://user:pass@example.com:8080/path?query=1#fragment\",\n            \"http://example.com?query=1#fragment\",\n            \"http://example.com#fragment\",\n            \"http://example.com.\",\n            \"http://\u4f8b\u5b50.\u6d4b\u8bd5\",\n            \"http://\u0909\u0926\u093e\u0939\u0930\u0923.\u092a\u0930\u0940\u0915\u094d\u0937\u093e\",\n            \"http://xn--7sbb4ac0ad0be6cf.xn--p1ai\",\n            \"http://[::1]\",\n            \"http://[::1]:8080\",\n            \"http://[::ffff:192.168.1.1]\",\n            \"http://[::ffff:192.168.1.1]:8080\",\n            \"http://localhost\",\n            \"http://localhost:8000\",\n        ]\n        invalid_urls = [\n            \"http://\",\n            \"ftp://\",\n            \"http://example\",\n            \"http://-example.com\",\n            \"http://example-.com\",\n            \"http://example.com-\",\n            \"http://example..com\",\n            \"http://example.com..\",\n            \"http://example.com:-1\",\n            \"http://example.com:65536\",\n            \"http://example.com:999999\",\n            \"http://example.com:000000080\",\n            \"http://user:pass@example.com:999999/path\",\n            \"http://user:pass@example.com:000000080/path\",\n            \"http://user:pass@example.com:65536/path\",\n            \"http://user:pass@example.com:-1/path\",\n            \"http://user:pass@example.com..\",\n            \"http://user:pass@example-.com\",\n            \"http://user:pass@-example.com\",\n            \"http://user:pass@example\",\n            \"http://[::1:2::3]\",\n            \"http://[::1:2::3]:8080\",\n            \"http://[::1]:999999\",\n            \"http://", "    def test_ip_address_validators(self):\n        with self.subTest(protocol=\"both\"):\n            validators, message = ip_address_validators(\"both\", unpack_ipv4=False)\n            self.assertEqual(validators, [validate_ipv46_address])\n            self.assertEqual(message, \"Enter a valid IPv4 or IPv6 address.\")\n\n        with self.subTest(protocol=\"ipv4\"):\n            validators, message = ip_address_validators(\"ipv4\", unpack_ipv4=False)\n            self.assertEqual(validators, [validate_ipv4_address])\n            self.assertEqual(message, \"Enter a valid IPv4 address.\")\n\n        with self.subTest(protocol=\"ipv6\"):\n            validators, message = ip_address_validators(\"ipv6\", unpack_ipv4=False)\n            self.assertEqual(validators, [validate_ipv6_address])\n            self.assertEqual(message, \"Enter a valid IPv6 address.\")\n\n        with self.subTest(protocol=\"unknown\"):\n            with self.assertRaises(ValueError):\n                ip_address_validators(\"unknown\", unpack_ipv4=False)\n\n        with self.subTest(protocol=\"ipv4 with unpack_ipv4=True\"):\n            with self.assertRaises(ValueError):\n                ip_address_validators(\"ipv4\", unpack_ipv4=True)\n", "def test_url_validator_with_invalid_ports(self):\n    invalid_ports = [\n        \"http://example.com:99999\",\n        \"http://example.com:65536\",\n        \"http://example.com:123456\",\n        \"http://example.com:80abc\",\n        \"http://example.com:80808\",\n        \"http://example.com:-8080\",\n    ]\n    for url in invalid_ports:\n        with self.subTest(url=url):\n            with self.assertRaises(ValidationError):\n                URLValidator()(url)\n", "    def test_url_validator_ipv6(self):\n        valid_ipv6_urls = [\n            \"http://[2001:db8::1]/\",\n            \"http://[::1]/\",\n            \"http://[::ffff:192.0.2.128]/\",\n            \"http://[2001:db8:0:0:0:0:2:1]/\",\n            \"http://[2001:db8::2:1]/\",\n        ]\n        invalid_ipv6_urls = [\n            \"http://[2001:db8::g]/\",\n            \"http://[2001:db8::1::2]/\",\n            \"http://[2001:db8:0:0:0:0:1]/\",\n            \"http://[::1::]/\",\n            \"http://[::ffff:192.0.2.256]/\",\n        ]\n        validator = URLValidator()\n\n        for url in valid_ipv6_urls:\n            with self.subTest(url=url):\n                validator(url)\n\n        for url in invalid_ipv6_urls:\n            with self.subTest(url=url):\n                with self.assertRaises(ValidationError):\n                    validator(url)\n"], "sample_430": ["    def test_generate_altered_order_with_respect_to(self):\n        \"\"\"Tests detection of changes to order_with_respect_to option.\"\"\"\n        # Setup initial and altered states.\n        initial_state = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n            ],\n        )\n        altered_state = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n            ],\n            options={\"order_with_respect_to\": \"book\"},\n        )\n\n        # Detect changes\n        changes = self.get_changes([initial_state], [altered_state])\n\n        # Assertions to ensure correct migration operations\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterOrderWithRespectTo\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"author\", order_with_respect_to=\"book\"\n        )\n", "    def test_alter_field_db_default(self):\n        \"\"\"Tests autodetection of db_default changes in fields.\"\"\"\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200, db_default=\"Old Default\")),\n                ],\n            )\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200, db_default=\"New Default\")),\n                ],\n            )\n        ]\n        changes = self.get_changes(before, after)\n        # Check that the correct migration is detected\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationFieldAttributes(\n            changes, \"testapp\", 0, 0, db_default=models.Value(\"New Default\")\n        )\n", "    def test_remove_non_nullable_field_with_default(self):\n        \"\"\"\n        Removing a non-nullable field with a default value should not prompt for a default.\n        \"\"\"\n        author_name_non_nullable_default = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default=\"Jane Doe\")),\n            ],\n        )\n        changes = self.get_changes(\n            [author_name_non_nullable_default], [self.author_empty]\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n", "    def test_rename_model_with_altered_options(self):\n        \"\"\"\n        Tests autodetection of renamed models while also altering model options.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_options],\n            [\n                ModelState(\n                    \"testapp\",\n                    \"RenamedAuthor\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                    ],\n                    options={\"permissions\": [(\"can_fire\", \"Can fire\")], \"verbose_name\": \"Renamed Authi\"},\n                ),\n            ],\n            MigrationQuestioner({\"ask_rename_model\": True})\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"AlterModelOptions\"])\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            old_name=\"Author\",\n            new_name=\"RenamedAuthor\",\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"renamedauthor\",\n            options={\"permissions\": [(\"can_fire\", \"Can fire\")], \"verbose_name\": \"Renamed Authi\"},\n        )\n", "def test_rename_model_and_field_simultaneously(self):\n    \"\"\"\n    Tests autodetection of renaming a model and a field within it simultaneously.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_name],\n        [\n            ModelState(\n                \"testapp\",\n                \"RenamedAuthor\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"renamed_name\", models.CharField(max_length=200)),\n                ],\n            ),\n        ],\n        MigrationQuestioner({\"ask_rename_model\": True, \"ask_rename\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"RenameField\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        old_name=\"Author\",\n        new_name=\"RenamedAuthor\",\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        old_name=\"name\",\n        new_name=\"renamed_name\",\n    )\n"], "sample_431": ["    def test_modelbase_new_abstract_model_instantiation(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        with self.assertRaisesMessage(TypeError, \"Abstract models cannot be instantiated.\"):\n            AbstractModel()\n", "    def test_get_deferred_fields(self):\n        a = Article.objects.create(pub_date=datetime.now())\n        deferred_article = Article.objects.defer(\"headline\").get(pk=a.pk)\n        self.assertEqual(deferred_article.get_deferred_fields(), {\"headline\"})\n        self.assertEqual(a.get_deferred_fields(), set())\n", "    def test_model_base_new_without_parents(self):\n        class NoParentModel(metaclass=ModelBase):\n            pass\n\n        self.assertTrue(issubclass(NoParentModel, object))\n", "    def test_subclass_exception(self):\n        class Parent:\n            __qualname__ = \"Parent\"\n\n        exc = subclass_exception(\"CustomError\", (Exception,), \"test_module\", Parent)\n        self.assertEqual(exc.__name__, \"CustomError\")\n        self.assertEqual(exc.__module__, \"test_module\")\n        self.assertEqual(exc.__qualname__, \"Parent.CustomError\")\n", "    def test_model_base_new_with_no_parents(self):\n        class DummyBase:\n            pass\n\n        class DummyModel(metaclass=ModelBase):\n            pass\n\n        # DummyModel should be an instance of DummyBase as well as ModelBase\n        self.assertIsInstance(DummyModel, ModelBase)\n        self.assertIsInstance(DummyModel, DummyBase)\n"], "sample_432": ["    def test_get_empty_value_display(self):\n        # Create a mock AdminSite with custom empty_value_display.\n        class MockAdminSite:\n            empty_value_display = \"No Data\"\n\n        # Create an instance of BaseModelAdmin with the mock AdminSite.\n        class MockModelAdmin(BaseModelAdmin):\n            admin_site = MockAdminSite()\n\n        admin_instance = MockModelAdmin()\n\n        # Check the get_empty_value_display method.\n        self.assertEqual(admin_instance.get_empty_value_display(), \"No Data\")\n", "    def test_get_inlines(self):\n        \"\"\"\n        Test the get_inlines method of BaseModelAdmin to ensure it returns the correct inlines.\n        \"\"\"\n        class Inline1(admin.TabularInline):\n            model = Child\n\n        class Inline2(admin.StackedInline):\n            model = Child\n\n        class CustomModelAdmin(admin.ModelAdmin):\n            inlines = [Inline1, Inline2]\n\n        m = CustomModelAdmin(Parent, custom_site)\n        request = self._mocked_authenticated_request(\"/parent/\", self.superuser)\n        obj = Parent.objects.create(name=\"test_parent\")\n\n        inlines = m.get_inline_instances(request, obj)\n        self.assertEqual(len(inlines), 2)\n        self.assertIsInstance(inlines[0], Inline1)\n        self.assertIsInstance(inlines[1], Inline2)\n", "    def test_formfield_for_choice_field(self):\n        \"\"\"\n        Test that formfield_for_choice_field returns the correct form field\n        and respects custom widgets and choices.\n        \"\"\"\n        class TestModel(models.Model):\n            CHOICES = [\n                ('A', 'Choice A'),\n                ('B', 'Choice B')\n            ]\n            choice_field = models.CharField(max_length=1, choices=CHOICES)\n\n        class TestAdmin(ModelAdmin):\n            radio_fields = {\"choice_field\": HORIZONTAL}\n\n        admin_site = admin.AdminSite(name=\"admin\")\n        admin_site.register(TestModel, TestAdmin)\n        model_admin = TestAdmin(TestModel, admin_site)\n\n        request = self.factory.get(\"/testmodel/\")\n        request.user = self.superuser\n\n        db_field = TestModel._meta.get_field(\"choice_field\")\n        form_field = model_admin.formfield_for_choice_field(db_field, request)\n\n        self.assertIsInstance(form_field.widget, widgets.AdminRadioSelect)\n        self.assertEqual(\n            form_field.widget.attrs[\"class\"],\n            \"radiolist inline\"\n        )\n        self.assertEqual(\n            form_field.choices,\n            [(\"\", \"None\"), ('A', 'Choice A'), ('B', 'Choice B')]\n        )\n", "    def test_formfield_overrides_for_db_fields(self):\n        \"\"\"\n        Test that formfield_overrides are properly applied to form fields.\n        \"\"\"\n        class CustomAdmin(ModelAdmin):\n            formfield_overrides = {\n                models.CharField: {\"widget\": forms.Textarea, \"help_text\": \"custom help\"},\n            }\n\n        custom_admin = CustomAdmin(Child, custom_site)\n        request = self.factory.get(\"/child/\")\n        request.user = self.superuser\n\n        # Create a CharField to test\n        db_field = Child._meta.get_field(\"name\")\n        formfield = custom_admin.formfield_for_dbfield(db_field, request)\n\n        # Check if the widget and help_text are overridden as specified\n        self.assertIsInstance(formfield.widget, forms.Textarea)\n        self.assertEqual(formfield.help_text, \"custom help\")\n", "    def test_get_deleted_objects(self):\n        \"\"\"\n        Test get_deleted_objects method to ensure it returns the correct data structure\n        and handles protected related objects correctly.\n        \"\"\"\n        from django.contrib.admin import site\n\n        class RelatedModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n            related = models.ForeignKey(RelatedModel, on_delete=models.PROTECT)\n\n        class TestModelAdmin(admin.ModelAdmin):\n            pass\n\n        class RelatedModelAdmin(admin.ModelAdmin):\n            pass\n\n        site.register(TestModel, TestModelAdmin)\n        site.register(RelatedModel, RelatedModelAdmin)\n\n        related = RelatedModel.objects.create(name=\"Related\")\n        test_obj = TestModel.objects.create(name=\"Test\", related=related)\n\n        request = self.factory.get(\"/testmodel/\")\n        request.user = self.superuser\n        model_admin = site._registry[TestModel]\n\n        deleted_objects, model_count, perms_needed, protected = model_admin.get_deleted_objects([test_obj], request)\n\n        self.assertEqual(len(deleted_objects), 2)  # One for the object, one for the related object\n        self.assertIn(str(test_obj), deleted_objects[0])\n        self.assertIn(str(related), deleted_objects[1])\n        self.assertEqual(len(model_count), 2)\n        self.assertEqual(model_count[(RelatedModel._meta.verbose_name, RelatedModel._meta.verbose_name_plural)], 1)\n        self.assertEqual(model_count[(TestModel._meta.verbose_name, TestModel._meta.verbose_name_plural)], 1)\n        self.assertEqual(len(perms_needed), 0)\n        self.assertEqual(len(protected), 1)\n        self.assertIn(str(related), protected[0])\n\n        site.unregister(TestModel)\n        site.unregister(RelatedModel)\n"], "sample_433": ["def test_swappable_dependency():\n    \"\"\"Test swappable_dependency function creates correct SwappableTuple.\"\"\"\n    swappable_dep = swappable_dependency(\"auth.User\")\n    self.assertEqual(swappable_dep, (\"auth\", \"__first__\"))\n    self.assertEqual(swappable_dep.setting, \"auth.User\")\n", "def test_migrate_state(self):\n    \"\"\"\n    Tests the mutate_state method of the Migration class.\n    \"\"\"\n    class AddFieldOperation:\n            self.name = name\n\n            state.add_model(ModelState(app_label, \"TestModel\", [(self.name, models.CharField(max_length=100))]))\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState(\"testapp\", \"TestModel\", [(\"id\", models.AutoField(primary_key=True))]))\n\n    migration = Migration(\"0001_initial\", \"testapp\")\n    migration.operations = [AddFieldOperation(\"new_field\")]\n\n    new_state = migration.mutate_state(project_state)\n\n    self.assertEqual(new_state.models[\"testapp\", \"TestModel\"].fields, [(\"id\", models.AutoField(primary_key=True)), (\"new_field\", models.CharField(max_length=100))])\n", "def test_suggest_name_with_no_operations_and_non_initial(self):\n    class Migration(migrations.Migration):\n        operations = []\n        initial = False\n\n    migration = Migration(\"0001_initial\", \"test_app\")\n    self.assertEqual(migration.suggest_name(), \"auto_%s\" % get_migration_name_timestamp())\n", "def test_migration_eq(self):\n    \"\"\"\n    Tests the equality operator for Migration class.\n    \"\"\"\n    migration1 = Migration(\"0001_initial\", \"testapp\")\n    migration2 = Migration(\"0001_initial\", \"testapp\")\n    migration3 = Migration(\"0002_alter_field\", \"testapp\")\n    migration4 = Migration(\"0001_initial\", \"otherapp\")\n\n    self.assertEqual(migration1, migration2)\n    self.assertNotEqual(migration1, migration3)\n    self.assertNotEqual(migration1, migration4)\n", "def test_swappable_dependency(self):\n    \"\"\"\n    Tests swappable_dependency function.\n    \"\"\"\n    result = swappable_dependency(\"auth.User\")\n    expected = SwappableTuple((\"auth\", \"__first__\"), \"auth.User\")\n    self.assertEqual(result, expected)\n    self.assertEqual(result.setting, \"auth.User\")\n"], "sample_434": ["    def test_template_view_renders_template(self):\n        class MyTemplateView(TemplateView):\n            template_name = \"my_template.html\"\n\n                context = super().get_context_data(**kwargs)\n                context[\"key\"] = \"value\"\n                return context\n\n        request = RequestFactory().get(\"/\")\n        view = MyTemplateView.as_view()\n        response = view(request)\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.context_data[\"key\"], \"value\")\n", "    def test_redirect_view_get_method(self):\n        class TestRedirectView(RedirectView):\n            url = \"/target/\"\n            permanent = True\n\n        request_factory = RequestFactory()\n        request = request_factory.get(\"/redirect/\")\n\n        view = TestRedirectView.as_view()\n        response = view(request)\n\n        self.assertIsInstance(response, HttpResponsePermanentRedirect)\n        self.assertEqual(response.url, \"/target/\")\n", "    def test_template_view_renders_correctly(self):\n        class MyTemplateView(TemplateView):\n            template_name = \"test_template.html\"\n\n                context = super().get_context_data(**kwargs)\n                context['extra'] = 'value'\n                return context\n\n        request = RequestFactory().get(\"/\")\n        view = MyTemplateView.as_view()\n        response = view(request)\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.context_data['extra'], 'value')\n        self.assertEqual(response.template_name, ['test_template.html'])\n", "    def test_template_response_mixin_raises_improperly_configured(self):\n        class IncompleteTemplateView(TemplateResponseMixin, View):\n            pass\n\n        view_instance = IncompleteTemplateView()\n        with self.assertRaises(ImproperlyConfigured):\n            view_instance.get_template_names()\n", "    def test_template_response_mixin(self):\n        class TestTemplateView(TemplateView):\n            template_name = \"test_template.html\"\n\n        request = RequestFactory().get(\"/\")\n        view = TestTemplateView.as_view()\n        response = view(request)\n        self.assertIsInstance(response, HttpResponse)\n"], "sample_435": ["    def test_get_context_no_value(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(\"password\", None, {})\n        self.assertIn(\"summary\", context)\n        self.assertEqual(context[\"summary\"], [{\"label\": gettext(\"No password set.\")}])\n", "    def test_unicode_ci_compare_equal(self):\n        self.assertTrue(_unicode_ci_compare(\"test\", \"TEST\"))\n        self.assertTrue(_unicode_ci_compare(\"caf\u00e9\", \"CAFE\"))\n        self.assertTrue(_unicode_ci_compare(\"Stra\u00dfe\", \"STRASSE\"))\n", "    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(\"password\", None, {})\n        self.assertIn(\"summary\", context)\n        self.assertEqual(context[\"summary\"], [{\"label\": gettext(\"No password set.\")}])\n", "    def test_get_context_with_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(name=\"password\", value=None, attrs={})\n        self.assertIn(\"summary\", context)\n        self.assertEqual(context[\"summary\"], [{\"label\": gettext(\"No password set.\")}])\n", "    def test_get_context(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = (\n            \"pbkdf2_sha256$100000$a6Pucb1qSFcD$WmCkn9Hqidj48NVe5x0FEM6A9YiOqQcl/83m2Z5udm0=\"\n        )\n        context = widget.get_context(\"password\", value, {\"id\": \"id_password\"})\n        self.assertIn(\"summary\", context)\n        self.assertIn(\"id\", context)\n        self.assertEqual(context[\"summary\"], [\n            {\"label\": _(\"algorithm\"), \"value\": \"pbkdf2_sha256\"},\n            {\"label\": _(\"iterations\"), \"value\": 100000},\n            {\"label\": _(\"salt\"), \"value\": \"a6Pucb******\"},\n            {\"label\": _(\"hash\"), \"value\": \"WmCkn9**************************************\"},\n        ])\n        "], "sample_436": ["    def test_runserver_with_ipv6_and_noreload(self, mock_run):\n        out = StringIO()\n        call_command(\"runserver\", \"--ipv6\", \"--noreload\", stdout=out)\n        self.assertIn(\"Starting development server at http://[::1]:8000/\", out.getvalue())\n        self.assertTrue(mock_run.called)\n", "    def setUp(self):\n        super().setUp()\n        self.write_settings(\n            \"settings.py\",\n            sdict={\"DEBUG\": True, \"ALLOWED_HOSTS\": [\"*\"]},\n        )\n", "    def test_invalid_addrport(self):\n        \"\"\"\n        Test runserver command with an invalid address:port combination.\n        \"\"\"\n        self.write_settings(\n            \"settings.py\",\n            sdict={\n                \"ALLOWED_HOSTS\": [\"*\"],\n                \"DEBUG\": True,\n            },\n        )\n        args = [\"runserver\", \"not-an-ip:port\"]\n        out, err = self.run_manage(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, '\"not-an-ip:port\" is not a valid port number or address:port pair.')\n", "    def setUp(self):\n        self.stdout = StringIO()\n        self.stderr = StringIO()\n        self.runserver_command = RunserverCommand(stdout=self.stdout, stderr=self.stderr)\n", "    def setUp(self):\n        super().setUp()\n        self.write_settings(\n            \"settings.py\",\n            sdict={\n                \"ALLOWED_HOSTS\": [\"*\"],\n                \"DEBUG\": True,\n            },\n        )\n"], "sample_437": ["    def test_savepoint_creation(self):\n        conn = connections[DEFAULT_DB_ALIAS]\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT 1\")\n        sid = conn.savepoint()\n        self.assertIsNotNone(sid)\n        self.assertTrue(sid.startswith(\"s\"))\n        self.assertIn(\"_x\", sid)\n", "    def test_timezone_constructor_pytz(self):\n        with override_settings(USE_DEPRECATED_PYTZ=True):\n            tzname = \"America/New_York\"\n            with patch(\"django.conf.settings.USE_DEPRECATED_PYTZ\", True):\n                with patch(\"pytz.timezone\") as mock_pytz_timezone:\n                    timezone_constructor(tzname)\n                    mock_pytz_timezone.assert_called_once_with(tzname)\n", "    def setUp(self):\n        self.conn = connections[DEFAULT_DB_ALIAS]\n", "    def test_timezone_constructor_with_pytz(self):\n        with override_settings(USE_DEPRECATED_PYTZ=True):\n            import pytz\n            tzname = 'America/New_York'\n            self.assertEqual(timezone_constructor(tzname), pytz.timezone(tzname))\n", "    def test_validate_thread_sharing(self):\n        conn = connections[DEFAULT_DB_ALIAS]\n        original_thread_ident = conn._thread_ident\n\n        # Test that validation passes when using the same thread\n        try:\n            conn.validate_thread_sharing()\n        except DatabaseError:\n            self.fail(\"validate_thread_sharing raised DatabaseError unexpectedly!\")\n\n        # Simulate thread sharing without incrementing\n        new_thread_ident = original_thread_ident + 1\n        with patch.object(_thread, 'get_ident', return_value=new_thread_ident):\n            with self.assertRaises(DatabaseError):\n                conn.validate_thread_sharing()\n\n        # Increment thread sharing count and validate again\n        conn.inc_thread_sharing()\n        try:\n            conn.validate_thread_sharing()\n        except DatabaseError:\n            self.fail(\"validate_thread_sharing raised DatabaseError unexpectedly after incrementing thread sharing!\")\n\n        # Decrement thread sharing count and validate again\n        conn.dec_thread_sharing()\n        with self.assertRaises(DatabaseError):\n            conn.validate_thread_sharing()\n"], "sample_438": ["    def test_proxy_model_with_non_abstract_base_class(self):\n        class AbstractBase(models.Model):\n            class Meta:\n                abstract = True\n\n        class ConcreteBase(models.Model):\n            pass\n\n        class ProxyModel(AbstractBase, ConcreteBase):\n            class Meta:\n                proxy = True\n\n        self.assertTrue(ProxyModel._meta.proxy)\n        self.assertEqual(ProxyModel._meta.concrete_model, ConcreteBase)\n", "    def test_str(self):\n        class Model(models.Model):\n            field = GenericForeignKey()\n\n        self.assertEqual(str(Model.field), \"contenttypes_tests.Model.field\")\n", "    def test_model_initialization(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=255)\n            age = models.IntegerField()\n\n        # Instantiate the model with positional arguments\n        instance = TestModel(\"John Doe\", 30)\n        self.assertEqual(instance.name, \"John Doe\")\n        self.assertEqual(instance.age, 30)\n\n        # Instantiate the model with keyword arguments\n        instance = TestModel(name=\"Jane Doe\", age=25)\n        self.assertEqual(instance.name, \"Jane Doe\")\n        self.assertEqual(instance.age, 25)\n\n        # Test instantiation with both positional and keyword arguments\n        with self.assertRaisesMessage(\n            TypeError, \"TestModel() got both positional and keyword arguments for field 'name'.\"\n        ):\n            TestModel(\"John Doe\", name=\"John Doe\", age=30)\n\n        # Test instantiation with more positional arguments than fields\n        with self.assertRaisesMessage(IndexError, \"Number of args exceeds number of fields\"):\n            TestModel(\"John Doe\", 30, \"extra argument\")\n", "    def test_deferred_field_repr(self):\n        self.assertEqual(repr(DEFERRED), \"<Deferred field>\")\n        self.assertEqual(str(DEFERRED), \"<Deferred field>\")\n", "    def test_model_instance_equality(self):\n        class SampleModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        instance1 = SampleModel(name=\"Instance 1\")\n        instance2 = SampleModel(name=\"Instance 2\")\n        instance3 = SampleModel(name=\"Instance 1\")\n\n        self.assertNotEqual(instance1, instance2)\n        self.assertEqual(instance1, instance3)\n"], "sample_439": ["    def test_order_fields(self):\n        class CustomForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n\n        form = CustomForm(field_order=[\"field3\", \"field1\"])\n        expected_field_order = [\"field3\", \"field1\", \"field2\", \"field4\"]\n        self.assertEqual(list(form.fields), expected_field_order)\n\n        form_no_order = CustomForm()\n        expected_field_order_no_order = [\"field1\", \"field2\", \"field3\", \"field4\"]\n        self.assertEqual(list(form_no_order.fields), expected_field_order_no_order)\n\n        form_invalid_order = CustomForm(field_order=[\"field5\", \"field3\"])\n        expected_field_order_invalid = [\"field3\", \"field1\", \"field2\", \"field4\"]\n        self.assertEqual(list(form_invalid_order.fields), expected_field_order_invalid)\n", "    def test_form_order_fields(self):\n        class OrderForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n\n        # Testing the default field order\n        form = OrderForm()\n        self.assertEqual(list(form.fields.keys()), ['field1', 'field2', 'field3', 'field4'])\n\n        # Testing custom field order\n        form = OrderForm(field_order=['field4', 'field1', 'field3'])\n        self.assertEqual(list(form.fields.keys()), ['field4', 'field1', 'field3', 'field2'])\n\n        # Testing custom field order with non-existing field\n        form = OrderForm(field_order=['field4', 'nonexistent', 'field1'])\n        self.assertEqual(list(form.fields.keys()), ['field4', 'field1', 'field2', 'field3'])\n", "def test_order_fields(self):\n    class OrderedForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n\n    form = OrderedForm(field_order=[\"field3\", \"field1\"])\n    expected_order = [\"field3\", \"field1\", \"field2\"]\n    self.assertEqual(list(form.fields), expected_order)\n\n    # Test with invalid field names in field_order\n    form = OrderedForm(field_order=[\"field3\", \"nonexistent\", \"field1\"])\n    expected_order = [\"field3\", \"field1\", \"field2\"]\n    self.assertEqual(list(form.fields), expected_order)\n", "    def test_order_fields(self):\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        # Default order\n        form = TestForm()\n        self.assertEqual(list(form.fields.keys()), [\"field1\", \"field2\", \"field3\"])\n\n        # Custom order\n        form.order_fields([\"field3\", \"field1\"])\n        self.assertEqual(list(form.fields.keys()), [\"field3\", \"field1\", \"field2\"])\n\n        # Custom order with non-existent field\n        form.order_fields([\"field3\", \"non_existent\", \"field1\"])\n        self.assertEqual(list(form.fields.keys()), [\"field3\", \"field1\", \"field2\"])\n", "    def test_empty_permitted_with_initial_data(self):\n        # Form should not be valid if empty_permitted=True and fields have initial data.\n        class InitialDataForm(Form):\n            field1 = CharField(initial=\"initial\")\n            field2 = IntegerField(initial=10)\n\n        form = InitialDataForm(data={}, empty_permitted=True, use_required_attribute=False)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"field1\"], [\"This field is required.\"])\n        self.assertEqual(form.errors[\"field2\"], [\"This field is required.\"])\n"], "sample_440": ["    def setUp(self):\n        self.country1 = Country.objects.create(name=\"Country1\", iso_two_letter=\"C1\")\n        self.country2 = Country.objects.create(name=\"Country2\", iso_two_letter=\"C2\")\n        self.state1 = State.objects.create(two_letter_code=\"S1\")\n        self.state2 = State.objects.create(two_letter_code=\"S2\")\n", "    def test_bulk_create_with_deferred_model(self):\n        \"\"\"\n        Test bulk_create with deferred fields in the model.\n        \"\"\"\n        # Create some data with deferred fields.\n        data = [\n            Country(name=\"Country A\", iso_two_letter=\"CA\"),\n            Country(name=\"Country B\", iso_two_letter=\"CB\"),\n        ]\n        created = Country.objects.only(\"name\").bulk_create(data)\n        self.assertEqual(len(created), 2)\n        self.assertEqual(Country.objects.count(), 2)\n        self.assertQuerySetEqual(\n            Country.objects.order_by(\"name\"),\n            [\"Country A\", \"Country B\"],\n            attrgetter(\"name\"),\n        )\n        # Confirm that iso_two_letter is deferred.\n        for country in Country.objects.all():\n            with self.assertNumQueries(1):\n                self.assertIsNotNone(country.iso_two_letter)\n", "    def test_bulk_create_with_non_concrete_field(self):\n        msg = \"bulk_create() can only be used with concrete fields in update_fields.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            TwoFields.objects.bulk_create(\n                [TwoFields(f1=1, f2=1)],\n                update_conflicts=True,\n                update_fields=[\"non_concrete_field\"],\n                unique_fields=[\"f1\"],\n            )\n", "    def setUp(self):\n        self.country1 = Country.objects.create(name=\"Country1\", iso_two_letter=\"C1\")\n        self.country2 = Country.objects.create(name=\"Country2\", iso_two_letter=\"C2\")\n        self.country3 = Country.objects.create(name=\"Country3\", iso_two_letter=\"C3\")\n", "    def test_flatvalueslistiterable(self):\n        country_names = [\"USA\", \"Germany\", \"Canada\"]\n        countries = [Country(name=name) for name in country_names]\n        Country.objects.bulk_create(countries)\n        \n        queryset = Country.objects.values_list('name', flat=True)\n        self.assertEqual(list(queryset), country_names)\n"], "sample_441": ["    def test_get_context_with_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(\"password\", None, {})\n        self.assertIn(\"summary\", context)\n        self.assertEqual(context[\"summary\"], [{\"label\": gettext(\"No password set.\")}])\n", "    def test_get_context_no_password_set(self):\n        widget = ReadOnlyPasswordHashWidget()\n        name = \"password\"\n        value = None\n        attrs = {}\n        context = widget.get_context(name, value, attrs)\n        self.assertIn(\"summary\", context)\n        self.assertEqual(context[\"summary\"][0][\"label\"], _(\"No password set.\"))\n", "    def test_get_users(self):\n        (user, username, email) = self.create_dummy_user()\n        form = PasswordResetForm({\"email\": email})\n        users = form.get_users(email)\n        self.assertEqual(list(users), [user])\n", "    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(\"password\", None, {})\n        self.assertIn(\"summary\", context)\n        self.assertEqual(context[\"summary\"], [{\"label\": gettext(\"No password set.\")}])\n", "    def test_get_context_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = f\"{UNUSABLE_PASSWORD_PREFIX}hashed\"\n        context = widget.get_context(\"password\", value, {})\n        self.assertEqual(context[\"summary\"], [{\"label\": gettext(\"No password set.\")}])\n"], "sample_442": ["    def test_dumps_loads(self):\n        serializer = signing.JSONSerializer()\n        obj = {\"key\": \"value\", \"int\": 42, \"list\": [1, 2, 3]}\n        dumped = serializer.dumps(obj)\n        self.assertIsInstance(dumped, bytes)\n        loaded = serializer.loads(dumped)\n        self.assertEqual(obj, loaded)\n", "    def test_sign_object_with_custom_serializer(self):\n        class CustomSerializer:\n                return json.dumps(obj, separators=(\";\", \"=\")).encode(\"latin-1\")\n\n                return json.loads(data.decode(\"latin-1\"))\n\n        signer = signing.Signer(key=\"predictable-secret\")\n        obj = {\"key1\": \"value1\", \"key2\": \"value2\"}\n        signed_obj = signer.sign_object(obj, serializer=CustomSerializer)\n        self.assertNotEqual(obj, signed_obj)\n        self.assertEqual(obj, signer.unsign_object(signed_obj, serializer=CustomSerializer))\n", "    def test_dumps_loads_with_fallback_keys(self):\n        \"dumps and loads should work correctly with fallback keys\"\n        objects = [\n            [\"a\", \"list\"],\n            \"a string \\u2019\",\n            {\"a\": \"dictionary\"},\n        ]\n        for o in objects:\n            signed = signing.dumps(o, key=\"new-secret\")\n            self.assertEqual(o, signing.loads(signed, fallback_keys=[\"old-secret\", \"new-secret\"]))\n            signed_compressed = signing.dumps(o, key=\"new-secret\", compress=True)\n            self.assertEqual(o, signing.loads(signed_compressed, fallback_keys=[\"old-secret\", \"new-secret\"]))\n", "def test_sign_unsign_with_different_separators(self):\n    separators = [\"|\", \":\", \";\", \"#\"]\n    for sep in separators:\n        signer = signing.Signer(key=\"predictable-secret\", sep=sep)\n        value = \"test_string\"\n        signed_value = signer.sign(value)\n        self.assertIsInstance(signed_value, str)\n        self.assertNotEqual(signed_value, value)\n        self.assertEqual(value, signer.unsign(signed_value))\n", "def test_compression_effectiveness(self):\n    signer = signing.Signer(key=\"predictable-secret\")\n    long_string = \"a\" * 1000  # Create a long string that can benefit from compression\n    signed_obj_compressed = signer.sign_object(long_string, compress=True)\n    signed_obj_uncompressed = signer.sign_object(long_string, compress=False)\n    # Ensure the compressed object is actually smaller than the uncompressed one\n    self.assertLess(len(signed_obj_compressed), len(signed_obj_uncompressed))\n    # Ensure both the compressed and uncompressed objects can be correctly unsign and return the original value\n    self.assertEqual(signer.unsign_object(signed_obj_compressed), long_string)\n    self.assertEqual(signer.unsign_object(signed_obj_uncompressed), long_string)\n"], "sample_443": ["    def test_set_expired_key(self):\n        # Setting a key with a timeout that has already expired\n        cache.set(\"expired_key\", \"value\", timeout=-1)\n        self.assertIsNone(cache.get(\"expired_key\"))\n        self.assertFalse(cache.has_key(\"expired_key\"))\n", "    def setUp(self):\n        super().setUp()\n        self.cache = caches[\"default\"]\n        self.cache.clear()\n", "    def setUp(self):\n        self.cache = caches[\"default\"]\n        self.cache.clear()\n", "    def setUp(self):\n        self.cache = caches[\"default\"]\n        self.cache.clear()\n", "    def setUp(self):\n        super().setUp()\n        self.dirname = tempfile.mkdtemp()\n        self.cache = FileBasedCache(self.dirname, {'OPTIONS': {'MAX_ENTRIES': 100}})\n"], "sample_444": ["    def setUp(self):\n        self.storage_location = tempfile.mkdtemp()\n        self.base_url = '/static/'\n        self.storage = StaticFilesStorage(location=self.storage_location, base_url=self.base_url)\n        self.test_file_name = 'test_file.txt'\n        self.test_file_content = 'This is a test file.'\n        with open(os.path.join(self.storage_location, self.test_file_name), 'w') as test_file:\n            test_file.write(self.test_file_content)\n", "    def setUp(self):\n        self.storage = storage.StaticFilesStorage()\n", "    def test_hashed_name_with_special_characters(self):\n        \"\"\"\n        Ensure that files with special characters in their names are correctly hashed.\n        \"\"\"\n        special_filename = \"special!@#$%^&*().css\"\n        with open(self._get_filename_path(special_filename), \"w\") as f:\n            f.write(\"body { background: #fff; }\")\n        self.run_collectstatic()\n        relpath = self.hashed_file_path(special_filename)\n        self.assertTrue(relpath.startswith(\"special!@#$%^&*().\"))\n        self.assertTrue(relpath.endswith(\".css\"))\n", "    def setUp(self):\n        self.storage = ManifestFilesMixin()\n", "    def test_hashed_name_with_content(self):\n        \"\"\"\n        Ensure that hashed_name generates correct hash when content is provided.\n        \"\"\"\n        filename = \"test_file.txt\"\n        content = ContentFile(b\"Sample content for hashing\")\n        expected_hash = \"d4c93c8c8b56\"\n\n        hashed_name = storage.staticfiles_storage.hashed_name(filename, content=content)\n        self.assertIn(expected_hash, hashed_name)\n        self.assertTrue(hashed_name.endswith(\".txt\"))\n"], "sample_445": ["def test_custom_time_strings(self):\n    \"\"\"Custom time strings should be used if provided.\"\"\"\n    custom_time_strings = {\n        \"year\": \"year(s)\",\n        \"month\": \"month(s)\",\n        \"week\": \"week(s)\",\n        \"day\": \"day(s)\",\n        \"hour\": \"hour(s)\",\n        \"minute\": \"minute(s)\",\n    }\n    self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), \"1\\xa0year(s)\")\n    self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), \"1\\xa0month(s)\")\n    self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), \"1\\xa0week(s)\")\n    self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), \"1\\xa0day(s)\")\n    self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), \"1\\xa0hour(s)\")\n    self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), \"1\\xa0minute(s)\")\n", "    def test_timesince_with_custom_time_strings(self):\n        \"\"\"Test timesince with custom time_strings dictionary.\"\"\"\n        custom_time_strings = {\n            \"year\": \"yr\",\n            \"month\": \"mo\",\n            \"week\": \"wk\",\n            \"day\": \"d\",\n            \"hour\": \"hr\",\n            \"minute\": \"min\",\n        }\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), \"1\\xa0yr\")\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), \"1\\xa0mo\")\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), \"1\\xa0wk\")\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), \"1\\xa0d\")\n        self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), \"1\\xa0hr\")\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), \"1\\xa0min\")\n        self.assertEqual(\n            timesince(self.t, self.t + 2 * self.oneday + 6 * self.onehour, time_strings=custom_time_strings),\n            \"2\\xa0d, 6\\xa0hr\"\n        )\n", "    def test_custom_time_strings(self):\n        \"\"\"Test using custom time strings.\"\"\"\n        custom_strings = {\n            \"year\": npgettext_lazy(\"custom\", \"%(num)d yr\", \"%(num)d yrs\", \"num\"),\n            \"month\": npgettext_lazy(\"custom\", \"%(num)d mo\", \"%(num)d mos\", \"num\"),\n            \"week\": npgettext_lazy(\"custom\", \"%(num)d wk\", \"%(num)d wks\", \"num\"),\n            \"day\": npgettext_lazy(\"custom\", \"%(num)d dy\", \"%(num)d dys\", \"num\"),\n            \"hour\": npgettext_lazy(\"custom\", \"%(num)d hr\", \"%(num)d hrs\", \"num\"),\n            \"minute\": npgettext_lazy(\"custom\", \"%(num)d min\", \"%(num)d mins\", \"num\"),\n        }\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_strings), \"1\\xa0min\")\n        self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_strings), \"1\\xa0hr\")\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_strings), \"1\\xa0dy\")\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_strings), \"1\\xa0wk\")\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_strings), \"1\\xa0mo\")\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_strings), \"1\\xa0yr\")\n", "    def test_future_date(self):\n        \"\"\"Test timesince with a date in the future.\"\"\"\n        future_date = self.t + datetime.timedelta(days=400)\n        self.assertEqual(timesince(self.t, future_date), \"1\\xa0year, 1\\xa0month\")\n        future_date = self.t + datetime.timedelta(days=800)\n        self.assertEqual(timesince(self.t, future_date), \"2\\xa0years, 1\\xa0month\")\n", "    def test_custom_time_strings(self):\n        \"\"\"Test timesince with custom time_strings.\"\"\"\n        custom_time_strings = {\n            \"year\": npgettext_lazy(\"time\", \"%(num)d yr\", \"%(num)d yrs\", \"num\"),\n            \"month\": npgettext_lazy(\"time\", \"%(num)d mon\", \"%(num)d mons\", \"num\"),\n            \"week\": npgettext_lazy(\"time\", \"%(num)d wk\", \"%(num)d wks\", \"num\"),\n            \"day\": npgettext_lazy(\"time\", \"%(num)d dy\", \"%(num)d dys\", \"num\"),\n            \"hour\": npgettext_lazy(\"time\", \"%(num)d hr\", \"%(num)d hrs\", \"num\"),\n            \"minute\": npgettext_lazy(\"time\", \"%(num)d min\", \"%(num)d mins\", \"num\"),\n        }\n        self.assertEqual(\n            timesince(self.t, self.t + self.onehour + self.oneminute, time_strings=custom_time_strings),\n            \"1\\xa0hr, 1\\xa0min\"\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + 2 * self.oneday + 3 * self.onehour, time_strings=custom_time_strings),\n            \"2\\xa0dys, 3\\xa0hrs\"\n        )\n"], "sample_446": ["    def test_floatformat_non_string_iterable(self):\n        self.assertEqual(floatformat([7.7]), \"[7.7]\")\n        self.assertEqual(floatformat((7.7,)), \"(7.7,)\")\n        self.assertEqual(floatformat({\"a\": 7.7}), \"{'a': 7.7}\")\n        self.assertEqual(floatformat({7.7}), \"{7.7}\")\n", "    def test_string_inputs(self):\n        self.assertEqual(floatformat(\"13.1031\", 3), \"13.103\")\n        self.assertEqual(floatformat(\"11.1197\", -2), \"11.12\")\n        self.assertEqual(floatformat(\"invalid\", 2), \"\")\n        self.assertEqual(floatformat(\"12345.6789\", \"2g\"), \"12,345.68\")\n        self.assertEqual(floatformat(\"12345.6789\", \"2u\"), \"12345.68\")\n", "    def test_floatformat_with_safe_data(self):\n        self.assertEqual(floatformat(mark_safe(\"1.42\")), \"1.4\")\n        self.assertEqual(floatformat(mark_safe(\"6.200000\"), 3), \"6.200\")\n", "    def test_floatformat_edge_cases(self):\n        self.assertEqual(floatformat(Decimal(\"NaN\")), \"NaN\")\n        self.assertEqual(floatformat(Decimal(\"Infinity\")), \"Infinity\")\n        self.assertEqual(floatformat(Decimal(\"-Infinity\")), \"-Infinity\")\n        self.assertEqual(floatformat(\"1e30000\"), \"100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "    def test_floatformat_with_non_numeric_string(self):\n        self.assertEqual(floatformat(\"not_a_number\"), \"\")\n        self.assertEqual(floatformat(\"not_a_number\", 2), \"\")\n        self.assertEqual(floatformat(\"not_a_number\", -2), \"\")\n"], "sample_447": ["    def test_combined_expression_with_different_data_types(self):\n        # Create a CombinedExpression with different data types.\n        combined = CombinedExpression(\n            Value(10, output_field=IntegerField()),\n            Combinable.ADD,\n            Value(2.5, output_field=FloatField()),\n            output_field=FloatField()\n        )\n        result = combined.resolve_expression()\n        self.assertIsInstance(result.output_field, FloatField)\n\n        combined = CombinedExpression(\n            Value(10, output_field=IntegerField()),\n            Combinable.DIV,\n            Value(2, output_field=IntegerField()),\n            output_field=FloatField()\n        )\n        result = combined.resolve_expression()\n        self.assertIsInstance(result.output_field, FloatField)\n\n        combined = CombinedExpression(\n            Value(Decimal(\"2.5\"), output_field=DecimalField()),\n            Combinable.MUL,\n            Value(2, output_field=IntegerField()),\n            output_field=DecimalField()\n        )\n        result = combined.resolve_expression()\n        self.assertIsInstance(result.output_field, DecimalField)\n\n        combined = CombinedExpression(\n            Value(10, output_field=IntegerField()),\n            Combinable.POW,\n            Value(3, output_field=IntegerField()),\n            output_field=IntegerField()\n        )\n        result = combined.resolve_expression()\n        self.assertIsInstance(result.output_field, IntegerField)\n", "    def test_combined_expression_with_negation(self):\n        books = Book.objects.annotate(\n            combined=ExpressionWrapper(~F(\"rating\"), output_field=IntegerField())\n        )\n        for book in books:\n            self.assertEqual(book.combined, ~int(book.rating))\n", "def test_resolve_combined_type(self):\n    self.assertEqual(\n        _resolve_combined_type(Combinable.ADD, fields.IntegerField, fields.IntegerField),\n        fields.IntegerField,\n    )\n    self.assertEqual(\n        _resolve_combined_type(Combinable.ADD, fields.IntegerField, fields.FloatField),\n        fields.FloatField,\n    )\n    self.assertEqual(\n        _resolve_combined_type(Combinable.ADD, fields.DateField, fields.DurationField),\n        fields.DateTimeField,\n    )\n    self.assertEqual(\n        _resolve_combined_type(Combinable.SUB, fields.DateTimeField, fields.DateTimeField),\n        fields.DurationField,\n    )\n    self.assertEqual(\n        _resolve_combined_type(Combinable.BITAND, fields.IntegerField, fields.IntegerField),\n        fields.IntegerField,\n    )\n    self.assertIsNone(\n        _resolve_combined_type(Combinable.ADD, fields.CharField, fields.IntegerField)\n    )\n", "def test_combined_expression_with_subtraction(self):\n    book = (\n        Book.objects.filter(isbn=\"159059725\")\n        .annotate(\n            combined=ExpressionWrapper(\n                F(\"price\") - F(\"pages\"), output_field=FloatField()\n            ),\n            rating_count=Count(\"rating\"),\n        )\n        .first()\n    )\n    self.assertEqual(book.combined, -417.0)\n    self.assertEqual(book.rating_count, 1)\n", "    def test_combined_expression_with_different_types(self):\n        test = self.b2\n        b = Book.objects.annotate(\n            combined=ExpressionWrapper(\n                F(\"price\") + F(\"rating\"), output_field=DecimalField()\n            )\n        ).get(isbn=test.isbn)\n        combined = Decimal(test.price + Decimal(test.rating))\n        self.assertEqual(b.combined, combined)\n"], "sample_448": ["def test_deferrable_enum_repr(self):\n    self.assertEqual(repr(models.Deferrable.DEFERRED), \"Deferrable.DEFERRED\")\n    self.assertEqual(repr(models.Deferrable.IMMEDIATE), \"Deferrable.IMMEDIATE\")\n", "    def test_clone_with_expressions(self):\n        constraint = models.UniqueConstraint(\n            Lower(\"name\"),\n            name=\"unique_with_expression\",\n            deferrable=models.Deferrable.IMMEDIATE,\n        ).clone()\n        self.assertEqual(\n            repr(constraint),\n            \"<UniqueConstraint: expressions=(Lower(F(name))) name='unique_with_expression' \"\n            \"deferrable=Deferrable.IMMEDIATE>\",\n        )\n", "    def test_invalid_include_argument_type(self):\n        msg = \"UniqueConstraint.include must be a list or tuple.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=[\"name\"],\n                name=\"invalid_include_type\",\n                include=\"invalid_include_type\"\n            )\n", "    def test_invalid_include_argument_type(self):\n        msg = \"UniqueConstraint.include must be a list or tuple.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                name=\"uniq_include\",\n                fields=[\"field\"],\n                include={\"not\": \"a list\"},\n            )\n", "    def test_invalid_violation_error_message_type(self):\n        msg = \"Violation error message must be a string.\"\n        with self.assertRaisesMessage(TypeError, msg):\n            BaseConstraint(\"name\", violation_error_message=123)\n"], "sample_449": ["def test_handle_one_request_large_request_line(self):\n        \"\"\"Test handling of a request line that exceeds the maximum allowed size.\"\"\"\n        request_line = b\"GET /\" + b\"a\" * 65537 + b\" HTTP/1.1\\r\\n\"\n        rfile = BytesIO(request_line)\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: (lambda environ, start_response: []))\n\n        # Prevent logging from appearing in test output.\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            handler = WSGIRequestHandler(request, \"192.168.0.2\", server)\n            handler.handle_one_request()\n\n        wfile.seek(0)\n        lines = wfile.readlines()\n        self.assertIn(b\"414 Request-URI Too Long\", lines[0])\n", "    def test_server_error_handling(self):\n            \"\"\"A WSGI app that raises an exception to simulate a server error.\"\"\"\n            start_response(\"500 Internal Server Error\", [])\n            raise RuntimeError(\"Simulated Server Error\")\n\n        rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        # Prevent logging from appearing in test output.\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            # Instantiating a handler runs the request as side effect.\n            WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n        wfile.seek(0)\n        lines = list(wfile.readlines())\n\n        # Check that a 500 error response was generated.\n        self.assertIn(b\"HTTP/1.0 500 Internal Server Error\", lines[0])\n        self.assertIn(b\"Connection: close\\r\\n\", lines)\n", "    def test_handle_error(self):\n        class TestWSGIServer(WSGIServer):\n                if is_broken_pipe_error():\n                    logger.info(\"- Broken pipe from %s\", client_address)\n                else:\n                    super().handle_error(request, client_address)\n\n        test_server = TestWSGIServer(server_address=(\"127.0.0.1\", 0))\n        with captured_stderr() as stderr:\n            test_server.handle_error(None, (\"127.0.0.1\", 0))\n\n        self.assertIn(\"Request timed out\", stderr.getvalue())\n", "    def test_handle_one_request(self):\n            \"\"\"A WSGI app that returns a hello world.\"\"\"\n            start_response(\"200 OK\", [])\n            return [b\"Hello World\"]\n\n        rfile = BytesIO(b\"GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n\")\n        rfile.seek(0)\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        # Prevent logging from appearing in test output.\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            handler = WSGIRequestHandler(request, \"192.168.0.2\", server)\n            handler.handle()\n\n        wfile.seek(0)\n        lines = wfile.readlines()\n        self.assertIn(b\"HTTP/1.1 200 OK\\r\\n\", lines)\n        self.assertIn(b\"Hello World\", lines)\n", "    def test_handle_error(self):\n        class CustomWSGIServer(WSGIServer):\n                super().handle_error(request, client_address)\n                self.error_handled = True\n\n            start_response(\"200 OK\", [])\n            yield b\"Hello World\"\n\n        server_address = (\"127.0.0.1\", 0)\n        server = CustomWSGIServer(server_address, WSGIRequestHandler)\n        server.set_app(test_app)\n\n        with captured_stderr() as stderr:\n            request = Stub(makefile=lambda mode, *args, **kwargs: BytesIO())\n            handler = WSGIRequestHandler(request, \"127.0.0.1\", server)\n            handler.handle_error()\n\n        self.assertTrue(server.error_handled)\n        self.assertIn(\"Exception happened during processing of request\", stderr.getvalue())\n"], "sample_450": ["    def test_get_admin_log_tag(self):\n        \"\"\"\n        Test the custom template tag `get_admin_log` to ensure it correctly \n        populates the context with the admin log entries.\n        \"\"\"\n        template_code = (\n            '{% load custom_admin_log %}'\n            '{% get_admin_log 1 as log_entries for_user user %}'\n        )\n        template_obj = template.Template(template_code)\n        context = template.Context({'user': self.user, 'log_entries': LogEntry.objects.all()})\n        rendered = template_obj.render(context)\n        self.assertEqual(rendered, \"\")\n        self.assertIn('log_entries', context)\n        self.assertEqual(len(context['log_entries']), 1)\n        self.assertEqual(context['log_entries'][0], LogEntry.objects.first())\n", "    def test_get_admin_log_tag(self):\n        \"\"\"\n        Test the 'get_admin_log' template tag to ensure it populates the context\n        variable with the correct log entries based on the given criteria.\n        \"\"\"\n        from django.template import Context, Template\n\n        # Create additional log entries\n        content_type_pk = ContentType.objects.get_for_model(Article).pk\n        LogEntry.objects.log_action(\n            self.user.pk,\n            content_type_pk,\n            self.a1.pk,\n            repr(self.a1),\n            CHANGE,\n            change_message=\"Another change\",\n        )\n\n        template_code = (\n            '{% load admin_log_tags %}'\n            '{% get_admin_log 2 as recent_logs for_user user %}'\n            '{% for log in recent_logs %}{{ log.change_message }}{% endfor %}'\n        )\n        t = Template(template_code)\n        context = Context({\"user\": self.user, \"log_entries\": LogEntry.objects.all()})\n        rendered = t.render(context)\n\n        self.assertIn(\"Changed something\", rendered)\n        self.assertIn(\"Another change\", rendered)\n        self.assertEqual(len(context[\"recent_logs\"]), 2)\n", "    def test_get_admin_log_tag(self):\n        \"\"\"\n        Test the custom template tag get_admin_log.\n        \"\"\"\n        template_code = (\n            \"{% load admin_log_tags %}\"\n            \"{% get_admin_log 5 as admin_log for_user user %}\"\n        )\n        context = {\n            \"log_entries\": LogEntry.objects.all(),\n            \"user\": self.user,\n        }\n        rendered = template.Template(template_code).render(template.Context(context))\n        self.assertIn(\"admin_log\", context)\n        self.assertEqual(list(context[\"admin_log\"]), list(LogEntry.objects.filter(user=self.user)[:5]))\n        self.assertEqual(rendered, \"\")\n", "    def test_get_admin_log_tag(self):\n        \"\"\"\n        Tests the `get_admin_log` template tag.\n        \"\"\"\n        template_str = '{% load get_admin_log %}{% get_admin_log 1 as admin_log for_user user %}'\n        t = template.Template(template_str)\n        context = template.Context({'log_entries': LogEntry.objects.all(), 'user': self.user})\n        t.render(context)\n        self.assertIn('admin_log', context)\n        self.assertEqual(len(context['admin_log']), 1)\n        self.assertEqual(context['admin_log'][0].user, self.user)\n", "    def test_get_admin_log_tag(self):\n        \"\"\"\n        Test get_admin_log template tag functionality.\n        \"\"\"\n        from django.template import Context, Template\n\n        # Create multiple log entries\n        for i in range(3):\n            LogEntry.objects.log_action(\n                self.user.pk,\n                ContentType.objects.get_for_model(Article).pk,\n                self.a1.pk,\n                repr(self.a1),\n                CHANGE,\n                change_message=f\"Changed something {i}\",\n            )\n\n        # Test template with user ID\n        template_code = (\n            '{% load admin_log %}{% get_admin_log 2 as admin_log for_user user.pk %}'\n        )\n        template = Template(template_code)\n        context = Context({\"user\": self.user, \"log_entries\": LogEntry.objects.all()})\n        template.render(context)\n        admin_log = context[\"admin_log\"]\n        self.assertEqual(len(admin_log), 2)\n        self.assertEqual(admin_log[0].change_message, \"Changed something 2\")\n        self.assertEqual(admin_log[1].change_message, \"Changed something 1\")\n\n        # Test template without user\n        template_code = '{% load admin_log %}{% get_admin_log 2 as admin_log %}'\n        template = Template(template_code)\n        context = Context({\"log_entries\": LogEntry.objects.all()})\n        template.render(context)\n        admin_log = context[\"admin_log\"]\n        self.assertEqual(len(admin_log), 2)\n        self.assertEqual(admin_log[0].change_message, \"Changed something 2\")\n        self.assertEqual(admin_log[1].change_message, \"Changed something 1\")\n\n        # Test template with user context variable\n        template_code = '{% load admin_log %}{% get_admin_log 2 as admin_log for_user user %}'\n        template = Template(template_code)\n        context = Context({\"user\": self.user, \"log_entries\": LogEntry.objects.all()})\n        template.render(context)\n        admin_log = context[\"admin_log\"]\n        self.assertEqual(len(admin_log), 2)\n        self.assertEqual(admin_log[0].change_message, \"Changed something 2\")\n        self.assertEqual(admin_log[1].change_message, \"Changed something 1\")\n"], "sample_451": ["    def test_replace_named_groups(self):\n        pattern = r'^(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<day>\\d{2})/details/$'\n        expected_output = r'^<year>/<month>/<day>/details/$'\n        self.assertEqual(replace_named_groups(pattern), expected_output)\n", "    def test_replace_metacharacters(self):\n        self.assertEqual(replace_metacharacters(r'abc?def'), 'abcdef')\n        self.assertEqual(replace_metacharacters(r'abc\\?def'), 'abc\\?def')\n        self.assertEqual(replace_metacharacters(r'abc*def'), 'abcdef')\n        self.assertEqual(replace_metacharacters(r'abc\\\\*def'), 'abc\\\\def')\n", "def test_replace_named_groups(self):\n        pattern = r\"^(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<day>\\d{2})$\"\n        replaced_pattern = replace_named_groups(pattern)\n        expected_pattern = r\"^<year>/<month>/<day>$\"\n        self.assertEqual(replaced_pattern, expected_pattern)\n", "    def test_replace_named_groups(self):\n        pattern = r\"^(?P<a>\\w+)/b/(?P<c>\\w+)$\"\n        expected = r\"^<a>/b/<c>$\"\n        self.assertEqual(replace_named_groups(pattern), expected)\n        ", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<day>\\d{2})$'\n        result = replace_named_groups(pattern)\n        expected = r'^<year>/<month>/<day>$'\n        self.assertEqual(result, expected)\n"], "sample_453": ["    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js function should correctly process context and\n        return the expected JSON structure.\n        \"\"\"\n        class FakeField:\n                self.auto_id = auto_id\n                self.name = name\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class FakeAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        field1 = {\"field\": FakeField(\"id_title\", \"title\"), \"dependencies\": [FakeField(\"id_slug\", \"slug\")]}\n        field2 = {\"field\": FakeField(\"id_description\", \"description\", max_length=100, allow_unicode=True), \"dependencies\": [FakeField(\"id_title\", \"title\")]}\n\n        context = {\n            \"adminform\": FakeAdminForm([field1, field2]),\n            \"inline_admin_formsets\": []\n        }\n\n        updated_context = prepopulated_fields_js(context)\n\n        expected_json = json.dumps([\n            {\n                \"id\": \"#id_title\",\n                \"name\": \"title\",\n                \"dependency_ids\": [\"#id_slug\"],\n                \"dependency_list\": [\"slug\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": False,\n            },\n            {\n                \"id\": \"#id_description\",\n                \"name\": \"description\",\n                \"dependency_ids\": [\"#id_title\"],\n                \"dependency_list\": [\"title\"],\n                \"maxLength\": 100,\n                \"allowUnicode\": True,\n            }\n        ])\n\n        self.assertEqual(updated_context[\"prepopulated_fields_json\"], expected_json)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        Test that prepopulated_fields_js function properly processes context \n        and generates the expected JSON output.\n        \"\"\"\n        class MockField:\n                self.name = name\n                self.auto_id = auto_id\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        mock_field_1 = {\"field\": MockField(\"title\", \"id_title\"), \"dependencies\": [MockField(\"name\", \"id_name\")]}\n        mock_field_2 = {\"field\": MockField(\"slug\", \"id_slug\", 100, True), \"dependencies\": [MockField(\"title\", \"id_title\")]}\n\n        context = {\n            \"adminform\": MockAdminForm(prepopulated_fields=[mock_field_1, mock_field_2]),\n            \"inline_admin_formsets\": [],\n        }\n\n        updated_context = prepopulated_fields_js(context)\n        expected_json = json.dumps([\n            {\n                \"id\": \"#id_title\",\n                \"name\": \"title\",\n                \"dependency_ids\": [\"#id_name\"],\n                \"dependency_list\": [\"name\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": False,\n            },\n            {\n                \"id\": \"#id_slug\",\n                \"name\": \"slug\",\n                \"dependency_ids\": [\"#id_title\"],\n                \"dependency_list\": [\"title\"],\n                \"maxLength\": 100,\n                \"allowUnicode\": True,\n            },\n        ])\n\n        self.assertIn(\"prepopulated_fields_json\", updated_context)\n        self.assertEqual(updated_context[\"prepopulated_fields_json\"], expected_json)\n        ", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js should include correct JSON data in context.\n        \"\"\"\n        class MockField:\n                self.name = name\n                self.auto_id = auto_id\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        field1 = {\"field\": MockField(\"title\", \"id_title\"), \"dependencies\": [MockField(\"dependency1\", \"id_dependency1\")]}\n        field2 = {\"field\": MockField(\"slug\", \"id_slug\", max_length=100, allow_unicode=True), \"dependencies\": [MockField(\"dependency2\", \"id_dependency2\")]}\n\n        context = {\n            \"adminform\": MockAdminForm(prepopulated_fields=[field1, field2]),\n            \"inline_admin_formsets\": []\n        }\n\n        updated_context = prepopulated_fields_js(context)\n        self.assertIn(\"prepopulated_fields_json\", updated_context)\n\n        expected_json = json.dumps([\n            {\n                \"id\": \"#id_title\",\n                \"name\": \"title\",\n                \"dependency_ids\": [\"#id_dependency1\"],\n                \"dependency_list\": [\"dependency1\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": False\n            },\n            {\n                \"id\": \"#id_slug\",\n                \"name\": \"slug\",\n                \"dependency_ids\": [\"#id_dependency2\"],\n                \"dependency_list\": [\"dependency2\"],\n                \"maxLength\": 100,\n                \"allowUnicode\": True\n            }\n        ])\n        self.assertEqual(updated_context[\"prepopulated_fields_json\"], expected_json)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        Test prepopulated_fields_js template tag context update.\n        \"\"\"\n        class MockField:\n                self.name = name\n                self.auto_id = auto_id\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        mock_field1 = MockField(name=\"field1\", auto_id=\"id_field1\")\n        mock_field2 = MockField(name=\"field2\", auto_id=\"id_field2\", max_length=100, allow_unicode=True)\n        prepopulated_fields = [\n            {\"field\": mock_field1, \"dependencies\": [mock_field2]},\n            {\"field\": mock_field2, \"dependencies\": [mock_field1]}\n        ]\n        context = {\"adminform\": MockAdminForm(prepopulated_fields)}\n\n        updated_context = prepopulated_fields_js(context)\n        \n        expected_json = json.dumps([\n            {\n                \"id\": \"#id_field1\",\n                \"name\": \"field1\",\n                \"dependency_ids\": [\"#id_field2\"],\n                \"dependency_list\": [\"field2\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": False,\n            },\n            {\n                \"id\": \"#id_field2\",\n                \"name\": \"field2\",\n                \"dependency_ids\": [\"#id_field1\"],\n                \"dependency_list\": [\"field1\"],\n                \"maxLength\": 100,\n                \"allowUnicode\": True,\n            }\n        ])\n\n        self.assertEqual(updated_context[\"prepopulated_fields\"], prepopulated_fields)\n        self.assertEqual(updated_context[\"prepopulated_fields_json\"], expected_json)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js should add the correct fields to the context.\n        \"\"\"\n        class MockField:\n                self.auto_id = auto_id\n                self.name = name\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        field1 = MockField(\"id_title\", \"title\")\n        field2 = MockField(\"id_slug\", \"slug\", allow_unicode=True)\n        context = {\n            \"adminform\": MockAdminForm(prepopulated_fields=[\n                {\"field\": field1, \"dependencies\": [field2]},\n                {\"field\": field2, \"dependencies\": [field1]},\n            ]),\n            \"inline_admin_formsets\": [],\n        }\n\n        updated_context = prepopulated_fields_js(context)\n        self.assertIn(\"prepopulated_fields\", updated_context)\n        self.assertIn(\"prepopulated_fields_json\", updated_context)\n\n        expected_json = json.dumps([\n            {\n                \"id\": \"#id_title\",\n                \"name\": \"title\",\n                \"dependency_ids\": [\"#id_slug\"],\n                \"dependency_list\": [\"slug\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": False,\n            },\n            {\n                \"id\": \"#id_slug\",\n                \"name\": \"slug\",\n                \"dependency_ids\": [\"#id_title\"],\n                \"dependency_list\": [\"title\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": True,\n            },\n        ])\n        self.assertEqual(updated_context[\"prepopulated_fields_json\"], expected_json)\n"], "sample_452": ["    def test_rename_model_with_constraints(self):\n        \"\"\"\n        Tests the RenameModel operation on a model with constraints.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_rmwconst\", constraints=True)\n        # Test the state alteration\n        operation = migrations.RenameModel(\"Pony\", \"Horse\")\n        self.assertEqual(operation.describe(), \"Rename model Pony to Horse\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_rmwconst\", new_state)\n        self.assertNotIn((\"test_rmwconst\", \"pony\"), new_state.models)\n        self.assertIn((\"test_rmwconst\", \"horse\"), new_state.models)\n        # RenameModel also repoints all constraints\n        self.assertEqual(\n            new_state.models[\"test_rmwconst\", \"horse\"].options[\"constraints\"][0].name,\n            \"test_constraint_pony_pink_gt_2\",\n        )\n        self.assertEqual(\n            new_state.models[\"test_rmwconst\", \"horse\"].options[\"constraints\"][0].check,\n            models.Q(pink__gt=2),\n        )\n        # Test the database alteration\n        self.assertTableNotExists(\"test_rmwconst_pony\")\n        self.assertTableExists(\"test_rmwconst_horse\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_rmwconst\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_rmwconst_pony\")\n        self.assertTableExists(\"test_rmwconst_horse\")\n        # Check that constraints have been renamed\n        self.assertConstraintExists(\"test_rmwconst_horse\", \"test_constraint_pony_pink_gt_2\")\n        self.assertConstraintNotExists(\"test_rmwconst_pony\", \"test_constraint_pony_pink_gt_2\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_rmwconst\", editor, new_state, project_state)\n        self.assertTableExists(\"test_rmwconst_pony\")\n        self.assertTableNotExists(\"test_rmwconst_horse\")\n        self.assertConstraintExists(\"test_rmwconst_pony\", \"test_constraint_pony_pink_gt_2\")\n        self.assertConstraintNotExists(\"test_rmwconst_horse\", \"test_constraint_pony_pink_gt_2\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"RenameModel\")\n        self", "    def test_alter_model_table_comment_none(self):\n        \"\"\"\n        Tests the AlterModelTableComment operation if the table comment is set to None.\n        \"\"\"\n        app_label = \"test_altermodeltablecommentnone\"\n        project_state = self.set_up_test_model(app_label)\n        pony_table = f\"{app_label}_pony\"\n        # Add table comment.\n        operation = migrations.AlterModelTableComment(\"Pony\", None)\n        self.assertEqual(operation.describe(), \"Alter Pony table comment\")\n        self.assertEqual(operation.migration_name_fragment, \"alter_pony_table_comment\")\n        new_state = project_state.clone()\n        operation.state_forwards(app_label, new_state)\n        self.assertNotIn(\"db_table_comment\", new_state.models[app_label, \"pony\"].options)\n        self.assertTableCommentNotExists(pony_table)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(app_label, editor, project_state, new_state)\n        self.assertTableCommentNotExists(pony_table)\n        # Reversal.\n        with connection.schema_editor() as editor:\n            operation.database_backwards(app_label, editor, new_state, project_state)\n        self.assertTableCommentNotExists(pony_table)\n        # Deconstruction.\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AlterModelTableComment\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(definition[2], {\"name\": \"Pony\", \"table_comment\": None})\n", "    def test_create_model_with_invalid_options(self):\n        \"\"\"\n        Test the CreateModel operation raises an error for invalid options.\n        \"\"\"\n        invalid_options = {\"non_existent_option\": True}\n        with self.assertRaisesMessage(ValueError, \"invalid option 'non_existent_option'\"):\n            migrations.CreateModel(\n                \"InvalidPony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(default=1)),\n                ],\n                options=invalid_options\n            )\n", "    def test_create_model_with_options(self):\n        \"\"\"\n        Test the CreateModel operation with various model options.\n        \"\"\"\n        options = {\n            \"verbose_name\": \"Pony\",\n            \"permissions\": [(\"can_groom\", \"Can groom\")],\n            \"ordering\": [\"-pink\"],\n        }\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n            ],\n            options=options,\n        )\n        self.assertEqual(operation.describe(), \"Create model Pony\")\n        self.assertEqual(operation.migration_name_fragment, \"pony\")\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmo\", new_state)\n        self.assertEqual(new_state.models[\"test_crmo\", \"pony\"].name, \"Pony\")\n        self.assertEqual(len(new_state.models[\"test_crmo\", \"pony\"].fields), 2)\n        self.assertEqual(new_state.models[\"test_crmo\", \"pony\"].options[\"ordering\"], [\"-pink\"])\n        # Test the database alteration\n        self.assertTableNotExists(\"test_crmo_pony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_crmo\", editor, project_state, new_state)\n        self.assertTableExists(\"test_crmo_pony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crmo\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_crmo_pony\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"CreateModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"fields\", \"name\", \"options\"])\n", "    def test_delete_model_with_constraint(self):\n        \"\"\"\n        Tests the DeleteModel operation that involves removing a table with a constraint.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_dlmc\", constraints=[\n            models.CheckConstraint(check=models.Q(pink__gt=2), name=\"pink_gt_2_constraint\")\n        ])\n        # Verify the constraint exists\n        self.assertTableExists(\"test_dlmc_pony\")\n        if connection.features.supports_table_check_constraints:\n            with connection.cursor() as cursor:\n                cursor.execute(\"INSERT INTO test_dlmc_pony (id, pink, weight) VALUES (1, 3, 4.0)\")\n                cursor.execute(\"INSERT INTO test_dlmc_pony (id, pink, weight) VALUES (2, 1, 5.0)\")\n\n        operation = migrations.DeleteModel(\"Pony\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_dlmc\", new_state)\n        self.assertNotIn((\"test_dlmc\", \"pony\"), new_state.models)\n\n        # Test the database alteration\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_dlmc\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_dlmc_pony\")\n        \n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_dlmc\", editor, new_state, project_state)\n        self.assertTableExists(\"test_dlmc_pony\")\n        if connection.features.supports_table_check_constraints:\n            with connection.cursor() as cursor:\n                cursor.execute(\"DELETE FROM test_dlmc_pony WHERE id = 1\")\n                with self.assertRaises(IntegrityError):\n                    cursor.execute(\"INSERT INTO test_dlmc_pony (id, pink, weight) VALUES (2, 1, 5.0)\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"DeleteModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(list(definition[2]), [\"name\"])\n"], "sample_454": ["    def test_eq(self):\n        expressions1 = [(F(\"field1\"), \"=\"), (F(\"field2\"), \"<>\")]\n        expressions2 = [(F(\"field1\"), \"=\"), (F(\"field3\"), \"<>\")]\n        self.assertEqual(\n            ExclusionConstraint(name=\"exclude1\", expressions=expressions1),\n            ExclusionConstraint(name=\"exclude1\", expressions=expressions1),\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(name=\"exclude1\", expressions=expressions1),\n            ExclusionConstraint(name=\"exclude2\", expressions=expressions1),\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(name=\"exclude1\", expressions=expressions1),\n            ExclusionConstraint(name=\"exclude1\", expressions=expressions2),\n        )\n        ", "    def test_init_with_valid_arguments(self):\n        constraint = ExclusionConstraint(\n            name=\"exclude_test\",\n            expressions=[(\"field1\", \"=\"), (\"field2\", \"&&\")],\n            index_type=\"GIST\",\n            condition=Q(field1__gt=0),\n            deferrable=Deferrable.DEFERRED,\n            include=[\"field3\"],\n            violation_error_code=\"exclude_error\",\n            violation_error_message=\"Exclusion constraint violated\",\n        )\n        self.assertEqual(constraint.name, \"exclude_test\")\n        self.assertEqual(constraint.expressions, [(\"field1\", \"=\"), (\"field2\", \"&&\")])\n        self.assertEqual(constraint.index_type, \"GIST\")\n        self.assertEqual(constraint.condition, Q(field1__gt=0))\n        self.assertEqual(constraint.deferrable, Deferrable.DEFERRED)\n        self.assertEqual(constraint.include, (\"field3\",))\n        self.assertEqual(constraint.violation_error_code, \"exclude_error\")\n        self.assertEqual(constraint.violation_error_message, \"Exclusion constraint violated\")\n", "    def test_eq(self):\n        expressions1 = [(F(\"col1\"), \"&&\"), (F(\"col2\"), \"=\")]\n        expressions2 = [(F(\"col3\"), \"&&\"), (F(\"col4\"), \"=\")]\n        self.assertEqual(\n            ExclusionConstraint(name=\"exclude\", expressions=expressions1),\n            ExclusionConstraint(name=\"exclude\", expressions=expressions1),\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(name=\"exclude\", expressions=expressions1),\n            ExclusionConstraint(name=\"exclude\", expressions=expressions2),\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(name=\"exclude\", expressions=expressions1),\n            ExclusionConstraint(name=\"exclude2\", expressions=expressions1),\n        )\n", "    def test_init_invalid_index_type(self):\n        msg = \"Exclusion constraints only support GiST or SP-GiST indexes.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.ExclusionConstraint(\n                name=\"exclude_invalid_index\",\n                expressions=[(\"field\", \"&&\")],\n                index_type=\"btree\",\n            )\n", "    def test_exclusion_constraint_init(self):\n        valid_expressions = [(F(\"field1\"), \"=\"), (F(\"field2\"), \"&&\")]\n        invalid_expressions = [(\"field1\", \"=\"), \"invalid\"]\n        valid_condition = Q(field1__gt=0)\n        valid_deferrable = Deferrable.DEFERRED\n        valid_include = [\"field3\"]\n\n        # Test valid initialization\n        try:\n            constraint = ExclusionConstraint(\n                name=\"exclude_test\",\n                expressions=valid_expressions,\n                index_type=\"GIST\",\n                condition=valid_condition,\n                deferrable=valid_deferrable,\n                include=valid_include,\n            )\n        except ValueError:\n            self.fail(\"ExclusionConstraint raised ValueError unexpectedly!\")\n\n        # Test invalid index_type\n        with self.assertRaisesMessage(\n            ValueError, \"Exclusion constraints only support GiST or SP-GiST indexes.\"\n        ):\n            ExclusionConstraint(\n                name=\"exclude_test\",\n                expressions=valid_expressions,\n                index_type=\"BTREE\",\n            )\n\n        # Test invalid expressions\n        with self.assertRaisesMessage(\n            ValueError, \"The expressions must be a list of 2-tuples.\"\n        ):\n            ExclusionConstraint(\n                name=\"exclude_test\",\n                expressions=invalid_expressions,\n            )\n\n        # Test invalid condition\n        with self.assertRaisesMessage(\n            ValueError, \"ExclusionConstraint.condition must be a Q instance.\"\n        ):\n            ExclusionConstraint(\n                name=\"exclude_test\",\n                expressions=valid_expressions,\n                condition=\"invalid\",\n            )\n\n        # Test invalid deferrable\n        with self.assertRaisesMessage(\n            ValueError, \"ExclusionConstraint.deferrable must be a Deferrable instance.\"\n        ):\n            ExclusionConstraint(\n                name=\"exclude_test\",\n                expressions=valid_expressions,\n                deferrable=\"invalid\",\n            )\n\n        # Test invalid include\n        with self.assertRaisesMessage(\n            ValueError, \"ExclusionConstraint.include must be a list or tuple.\"\n        ):\n            ExclusionConstraint(\n                name=\"exclude_test\",\n                expressions=valid_expressions,\n                include=\"invalid\",\n            )\n"], "sample_455": ["    def test_invalid_fields_argument(self):\n        msg = \"UniqueConstraint.fields must be a list or tuple.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                name=\"uniq_invalid_fields\",\n                fields=\"field\",\n            )\n", "    def test_enum_members(self):\n        self.assertEqual(Deferrable.DEFERRED.value, \"deferred\")\n        self.assertEqual(Deferrable.IMMEDIATE.value, \"immediate\")\n", "    def test_invalid_deferrable_argument_type(self):\n        msg = \"UniqueConstraint.deferrable must be a Deferrable instance.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            UniqueConstraint(\n                fields=[\"field\"],\n                name=\"unique_invalid_deferrable\",\n                deferrable=\"invalid_deferrable_type\",\n            )\n", "def test_deferrable_repr(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique_fields\",\n        deferrable=models.Deferrable.DEFERRED,\n    )\n    self.assertEqual(\n        repr(constraint),\n        \"<UniqueConstraint: fields=('foo', 'bar') name='unique_fields' deferrable=Deferrable.DEFERRED>\",\n    )\n", "    def test_repr(self):\n        self.assertEqual(repr(Deferrable.DEFERRED), \"Deferrable.DEFERRED\")\n        self.assertEqual(repr(Deferrable.IMMEDIATE), \"Deferrable.IMMEDIATE\")\n"], "sample_456": ["    def test_management_form_clean(self):\n        \"\"\"\n        Test the `clean` method of the ManagementForm to ensure it sets default values\n        for TOTAL_FORM_COUNT and INITIAL_FORM_COUNT when they are missing.\n        \"\"\"\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        # Manipulate management form data to simulate missing fields.\n        formset.management_form.cleaned_data.pop(TOTAL_FORM_COUNT, None)\n        formset.management_form.cleaned_data.pop(INITIAL_FORM_COUNT, None)\n        cleaned_data = formset.management_form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n", "    def test_management_form_clean_method(self):\n        \"\"\"\n        Test that the clean method of ManagementForm sets default values correctly.\n        \"\"\"\n        class TestManagementForm(ManagementForm):\n                cleaned_data = super().clean()\n                cleaned_data['additional_field'] = 'default_value'\n                return cleaned_data\n\n        form = TestManagementForm(data={})\n        form.full_clean()\n        self.assertEqual(form.cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(form.cleaned_data[INITIAL_FORM_COUNT], 0)\n        self.assertEqual(form.cleaned_data['additional_field'], 'default_value')\n", "    def test_management_form_clean_defaults(self):\n        \"\"\"\n        ManagementForm's clean method should set default values for TOTAL_FORMS\n        and INITIAL_FORMS if they are missing from the cleaned_data.\n        \"\"\"\n        class CustomManagementForm(ManagementForm):\n                cleaned_data = super().clean()\n                if TOTAL_FORM_COUNT not in cleaned_data:\n                    cleaned_data[TOTAL_FORM_COUNT] = 0\n                if INITIAL_FORM_COUNT not in cleaned_data:\n                    cleaned_data[INITIAL_FORM_COUNT] = 0\n                return cleaned_data\n\n        form = CustomManagementForm(data={})\n        self.assertFalse(form.is_valid())\n        form.cleaned_data = {}\n        cleaned_data = form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n", "    def test_formset_factory_with_invalid_max_num(self):\n        \"\"\"\n        Test formset_factory raises ValueError when max_num is set greater than absolute_max.\n        \"\"\"\n        with self.assertRaises(ValueError) as context:\n            formset_factory(Choice, max_num=5, absolute_max=3)\n        self.assertEqual(str(context.exception), \"'absolute_max' must be greater or equal to 'max_num'.\")\n", "    def test_management_form_clean(self):\n        \"\"\"\n        Tests that the ManagementForm clean method correctly sets the default values\n        for TOTAL_FORM_COUNT and INITIAL_FORM_COUNT when the form is invalid.\n        \"\"\"\n        form = ManagementForm(data={})\n        self.assertFalse(form.is_valid())\n        cleaned_data = form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n"], "sample_457": ["    def test_invalid_deferrable_expressions_opclasses(self):\n        msg = \"UniqueConstraint.opclasses cannot be used with expressions. Use django.contrib.postgres.indexes.OpClass() instead.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_opclass\",\n                opclasses=[\"jsonb_path_ops\"],\n                deferrable=models.Deferrable.DEFERRED,\n            )\n", "def test_unique_constraint_violation_error_message(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"name\", \"color\"],\n        name=\"name_color_uniq\",\n        violation_error_message=\"Custom violation error: name and color must be unique.\",\n    )\n    msg = \"Custom violation error: name and color must be unique.\"\n    non_unique_product = UniqueConstraintProduct(name=self.p1.name, color=self.p1.color)\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n", "    def test_deferrable_enum_repr(self):\n        self.assertEqual(repr(models.Deferrable.DEFERRED), \"Deferrable.DEFERRED\")\n        self.assertEqual(repr(models.Deferrable.IMMEDIATE), \"Deferrable.IMMEDIATE\")\n", "    def test_clone(self):\n        constraint = models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint, cloned_constraint)\n        self.assertIsNot(constraint, cloned_constraint)\n", "    def test_deconstruction_with_violation_error_message(self):\n        fields = [\"foo\", \"bar\"]\n        name = \"unique_fields\"\n        violation_error_message = \"custom violation message for %(name)s\"\n        constraint = models.UniqueConstraint(\n            fields=fields, name=name, violation_error_message=violation_error_message\n        )\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, \"django.db.models.UniqueConstraint\")\n        self.assertEqual(args, ())\n        self.assertEqual(\n            kwargs,\n            {\n                \"fields\": tuple(fields),\n                \"name\": name,\n                \"violation_error_message\": violation_error_message,\n            },\n        )\n"], "sample_458": ["    def test_edge_cases(self):\n        # Test edge cases with very large and very small numbers\n        self.assertEqual(floatformat(1e100), \"1e+100\")\n        self.assertEqual(floatformat(-1e100), \"-1e+100\")\n        self.assertEqual(floatformat(1e-100), \"1e-100\")\n        self.assertEqual(floatformat(-1e-100), \"-1e-100\")\n        # Test with None as argument\n        self.assertEqual(floatformat(42.42, None), \"42.4\")\n        # Test with empty string as argument\n        self.assertEqual(floatformat(42.42, \"\"), \"42.4\")\n        # Test with invalid arg types\n        self.assertEqual(floatformat(42.42, [1, 2, 3]), \"42.42\")\n        self.assertEqual(floatformat(42.42, {\"a\": 1}), \"42.42\")\n        self.assertEqual(floatformat(42.42, (1, 2)), \"42.42\")\n", "    def test_floatformat_edge_cases(self):\n        # Test with a very large float\n        self.assertEqual(floatformat(1e100, 2), \"1e+100\")\n        # Test with a very small float\n        self.assertEqual(floatformat(1e-100, 10), \"0.0000000001\")\n        # Test with integer argument as string\n        self.assertEqual(floatformat(7.7, \"0\"), \"8\")\n        # Test with a float having more decimal places than specified\n        self.assertEqual(floatformat(7.123456789, 5), \"7.12346\")\n        # Test with a non-numeric string value\n        self.assertEqual(floatformat(\"non-numeric\", 2), \"\")\n", "    def test_floatformat_with_custom_object(self):\n        class CustomFloat:\n                self.value = value\n\n                return str(self.value)\n\n        self.assertEqual(floatformat(CustomFloat(\"45.6789\")), \"45.7\")\n        self.assertEqual(floatformat(CustomFloat(\"12345.6789\"), 2), \"12345.68\")\n        self.assertEqual(floatformat(CustomFloat(\"0.00000123\")), \"0.0\")\n        self.assertEqual(floatformat(CustomFloat(\"10000.0000\"), \"g\"), \"10,000\")\n        self.assertEqual(floatformat(CustomFloat(\"66666.6666\"), \"2g\"), \"66,666.67\")\n", "    def test_float_as_string(self):\n        self.assertEqual(floatformat(\"12.345\", 2), \"12.35\")\n        self.assertEqual(floatformat(\"12.345\", -2), \"12.35\")\n        self.assertEqual(floatformat(\"12.0\", 0), \"12\")\n        self.assertEqual(floatformat(\"12.0\", 3), \"12.000\")\n        self.assertEqual(floatformat(\"12.345\", \"2g\"), \"12.35\")\n        self.assertEqual(floatformat(\"12.345\", \"2u\"), \"12.35\")\n        self.assertEqual(floatformat(\"12.345\", \"2gu\"), \"12.35\")\n        self.assertEqual(floatformat(\"12,345\", \"2u\"), \"\")\n", "    def test_addslashes(self):\n        self.assertEqual(addslashes('He said, \"Hello.\"'), 'He said, \\\\\"Hello.\\\\\"')\n        self.assertEqual(addslashes(\"It's a test.\"), \"It\\\\'s a test.\")\n        self.assertEqual(addslashes(\"Backslash: \\\\\"), \"Backslash: \\\\\\\\\")\n"], "sample_459": ["    def test_uuidfield_exact_lookup(self):\n        from uuid import uuid4\n        test_uuid = uuid4()\n        instance = UUIDFieldModel.objects.create(uuid_field=test_uuid)\n        qs = UUIDFieldModel.objects.filter(uuid_field__exact=test_uuid)\n        self.assertEqual(qs.count(), 1)\n        self.assertEqual(qs[0], instance)\n", "    def test_uuid_exact_lookup(self):\n        from django.db.models import UUIDField\n        from uuid import uuid4\n\n        class UUIDModel(models.Model):\n            value = UUIDField()\n\n        value = uuid4()\n        obj = UUIDModel.objects.create(value=value)\n        self.assertEqual(UUIDModel.objects.get(value__exact=value), obj)\n", "    def test_exact_lookup(self):\n        obj = IntegerModel.objects.create(value=10)\n        self.assertEqual(IntegerModel.objects.get(value__exact=10), obj)\n        self.assertEqual(IntegerModel.objects.filter(value__exact=10).count(), 1)\n", "    def setUp(self):\n        self.model = self.UUIDModel\n", "    def test_uuid_iexact_lookup(self):\n        from uuid import UUID\n        uuid_value = UUID('12345678123456781234567812345678')\n        instance = models.UUIDFieldModel(uuid=uuid_value)\n        instance.save()\n        retrieved_instance = models.UUIDFieldModel.objects.get(uuid__iexact='12345678-1234-5678-1234-567812345678')\n        self.assertEqual(instance, retrieved_instance)\n"], "sample_460": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n    ", "    def test_url_conflicts_with_change(self):\n        \"A model with a primary key that ends with change should be visible\"\n        change_model = ModelWithStringPrimaryKey.objects.create(pk=\"change\")\n        response = self.client.get(\n            reverse(\n                \"admin:admin_views_modelwithstringprimarykey_change\",\n                args=(quote(change_model.pk),),\n            )\n        )\n        self.assertContains(response, \"\"\"<h1>Change model with string primary key</h1>\"\"\")\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"superuser\", password=\"secret\", email=\"superuser@example.com\"\n        )\n        cls.staff_user = User.objects.create_user(\n            username=\"staffuser\", password=\"secret\", email=\"staffuser@example.com\", is_staff=True\n        )\n        cls.non_staff_user = User.objects.create_user(\n            username=\"nonstaffuser\", password=\"secret\", email=\"nonstaffuser@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"superuser\", password=\"superpassword\", email=\"super@example.com\"\n        )\n        cls.staffuser = User.objects.create_user(\n            username=\"staffuser\", password=\"staffpassword\", is_staff=True\n        )\n        cls.nostaffuser = User.objects.create_user(\n            username=\"nostaffuser\", password=\"nostaffpassword\", is_staff=False\n        )\n"], "sample_461": ["def test_urlfield_invalid_characters(self):\n    f = URLField()\n    tests = [\n        \"http://<invalid>.com\",\n        \"http://example.com/{invalid}\",\n        \"http://example.com/[invalid]\",\n        \"http://example.com/(invalid)\",\n        \"http://example.com/|invalid|\",\n        \"http://example.com/\\\\invalid\\\\\",\n        \"http://example.com/\\\"invalid\\\"\",\n        \"http://example.com/^invalid^\",\n        \"http://example.com/`invalid`\",\n        \"http://example.com/~invalid~\",\n    ]\n    msg = \"'Enter a valid URL.'\"\n    for value in tests:\n        with self.subTest(value=value):\n            with self.assertRaisesMessage(ValidationError, msg):\n                f.clean(value)\n", "    def test_urlfield_custom_validator(self):\n            if not value.startswith(\"https://secure.\"):\n                raise ValidationError(\"URL must start with 'https://secure.'\")\n        \n        f = URLField(validators=[custom_validator])\n        self.assertEqual(f.clean(\"https://secure.example.com\"), \"https://secure.example.com\")\n        \n        msg = \"'URL must start with 'https://secure.''\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"http://example.com\")\n", "    def test_urlfield_invalid_scheme(self):\n        f = URLField()\n        invalid_urls = [\n            \"ftp://example.com\",\n            \"mailto:someone@example.com\",\n            \"file:///path/to/file\",\n            \"data:text/plain;base64,SGVsbG8sIFdvcmxkIQ==\",\n            \"javascript:alert('Hello');\"\n        ]\n        msg = \"'Enter a valid URL.'\"\n        for url in invalid_urls:\n            with self.subTest(url=url):\n                with self.assertRaisesMessage(ValidationError, msg):\n                    f.clean(url)\n", "    def test_urlfield_invalid_scheme(self):\n        f = URLField()\n        invalid_urls = [\n            \"ftp://example.com\",\n            \"file://localhost/path\",\n            \"gopher://example.com\",\n            \"mailto:user@example.com\",\n            \"news:comp.infosystems.www.servers.unix\",\n            \"tel:+1-800-555-5555\",\n            \"urn:oasis:names:specification:docbook:dtd:xml:4.1.2\",\n        ]\n        msg = \"'Enter a valid URL.'\"\n        for url in invalid_urls:\n            with self.subTest(url=url):\n                with self.assertRaisesMessage(ValidationError, msg):\n                    f.clean(url)\n", "    def test_urlfield_valid_with_custom_scheme(self):\n        f = URLField(assume_scheme=\"ftp\")\n        self.assertEqual(f.clean(\"example.com\"), \"ftp://example.com\")\n"], "sample_462": ["    def test_choicefield_initial_value(self):\n        f = ChoiceField(choices=[(\"J\", \"John\"), (\"P\", \"Paul\")], initial=\"J\")\n        self.assertEqual(\"J\", f.clean(f.initial))\n        self.assertEqual(\"P\", f.clean(\"P\"))\n        self.assertEqual(\"J\", f.clean(\"J\"))\n        msg = \"'Select a valid choice. K is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"K\")\n", "    def test_choicefield_bound_data(self):\n        class TestForm(Form):\n            choice = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")])\n\n        form = TestForm({\"choice\": \"2\"})\n        bound_field = form[\"choice\"]\n        self.assertEqual(bound_field.value(), \"2\")\n        self.assertEqual(bound_field.initial, None)\n", "    def test_choicefield_invalid_type(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")])\n        msg = \"'Enter a list of values.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(['1', '2'])\n", "    def test_choicefield_empty_label(self):\n        f = ChoiceField(choices=[(\"\", \"Choose an option\"), (\"J\", \"John\"), (\"P\", \"Paul\")], required=False)\n        self.assertEqual(\"\", f.clean(\"\"))\n        self.assertEqual(\"J\", f.clean(\"J\"))\n        self.assertEqual(\"P\", f.clean(\"P\"))\n        msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"3\")\n", "    def test_choicefield_initial(self):\n        f = ChoiceField(choices=[(\"A\", \"Alpha\"), (\"B\", \"Beta\")], initial=\"B\")\n        form_field = f.get_bound_field(Form(), \"field\")\n        self.assertEqual(form_field.value(), \"B\")\n        self.assertEqual(f.clean(\"B\"), \"B\")\n        self.assertEqual(f.clean(\"A\"), \"A\")\n        msg = \"'Select a valid choice. C is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"C\")\n"], "sample_463": ["    def test_alter_model_table(self):\n        \"\"\"\n        Tests that changing the `db_table` option in a model's options creates\n        an AlterModelTable operation.\n        \"\"\"\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            options={\"db_table\": \"author_initial\"},\n        )\n        changed_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            options={\"db_table\": \"author_changed\"},\n        )\n        changes = self.get_changes([initial_author], [changed_author])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"author\",\n            table=\"author_changed\",\n        )\n", "    def test_fk_dependency_within_same_app(self):\n        \"\"\"\n        A model with a ForeignKey to another model within the same app should\n        not create an inter-app dependency.\n        \"\"\"\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            ModelState(\n                \"testapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                ],\n            ),\n        ]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Book\")\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n", "    def test_generate_renamed_models(self):\n        \"\"\"\n        Tests the detection and generation of renamed models.\n        \"\"\"\n        before = [\n            ModelState(\n                \"app\",\n                \"OldModel\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                ],\n            )\n        ]\n        after = [\n            ModelState(\n                \"app\",\n                \"NewModel\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                ],\n            )\n        ]\n        changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename_model\": True}))\n        self.assertNumberMigrations(changes, \"app\", 1)\n        self.assertOperationTypes(changes, \"app\", 0, [\"RenameModel\"])\n        self.assertOperationAttributes(\n            changes, \"app\", 0, 0, old_name=\"OldModel\", new_name=\"NewModel\"\n        )\n", "def test_only_relation_agnostic_fields(self):\n    \"\"\"\n    Tests the only_relation_agnostic_fields method to ensure it correctly\n    ignores field names and related field targets.\n    \"\"\"\n    before = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"publisher\", models.ForeignKey(\"testapp.Publisher\", models.CASCADE)),\n            ],\n        )\n    ]\n    after = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"full_name\", models.CharField(max_length=200)),\n                (\"publisher\", models.ForeignKey(\"otherapp.NewPublisher\", models.CASCADE)),\n            ],\n        )\n    ]\n    autodetector = MigrationAutodetector(before, after)\n    fields_def_before = autodetector.only_relation_agnostic_fields(before[0].fields)\n    fields_def_after = autodetector.only_relation_agnostic_fields(after[0].fields)\n    # Ensure field definitions without relation agnostic differences are equal\n    self.assertEqual(fields_def_before[0], fields_def_after[0])  # AutoField\n    self.assertNotEqual(fields_def_before[1], fields_def_after[1])  # CharField names\n    self.assertEqual(fields_def_before[2], fields_def_after[2])  # ForeignKey without 'to'\n", "    def test_alter_field_with_partial_function(self):\n        \"\"\"Test that altering a field with a functools.partial function is detected correctly.\"\"\"\n            return f\"{prefix}/{filename}\"\n\n        old_state = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"file\",\n                        models.FileField(\n                            upload_to=functools.partial(my_custom_upload_to, prefix=\"old\")\n                        ),\n                    ),\n                ],\n            )\n        ]\n\n        new_state = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"file\",\n                        models.FileField(\n                            upload_to=functools.partial(my_custom_upload_to, prefix=\"new\")\n                        ),\n                    ),\n                ],\n            )\n        ]\n\n        changes = self.get_changes(old_state, new_state)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        value = changes[\"testapp\"][0].operations[0].field.upload_to\n        self.assertEqual(\n            (my_custom_upload_to, (), {'prefix': 'new'}),\n            (value.func, value.args, value.keywords),\n        )\n"], "sample_464": ["    def test_file_response_with_non_existent_file(self):\n        \"\"\"\n        Ensure that FileResponse raises an appropriate error when initialized\n        with a non-existent file.\n        \"\"\"\n        non_existent_file = \"non_existent_file.txt\"\n        with self.assertRaises(FileNotFoundError):\n            FileResponse(open(non_existent_file, \"rb\"))\n", "compilation error", "def test_file_response_closed_file(self):\n    \"\"\"\n    Test that FileResponse raises a ValueError when an attempt is made to\n    wrap a closed file.\n    \"\"\"\n    file = open(__file__, \"rb\")\n    file.close()\n    with self.assertRaises(ValueError):\n        FileResponse(file)\n", "    def test_file_response_with_nonseekable_file(self):\n        \"\"\"\n        Test FileResponse with a file-like object that does not support seeking.\n        \"\"\"\n        class NonSeekableFile:\n                self.content = content\n                self.read_pos = 0\n\n                if size == -1:\n                    size = len(self.content) - self.read_pos\n                data = self.content[self.read_pos:self.read_pos + size]\n                self.read_pos += size\n                return data\n\n                return False\n\n                pass\n\n        file_like = NonSeekableFile(b\"non-seekable file content\")\n        response = FileResponse(file_like)\n        response_content = list(response)\n        response.close()\n        self.assertEqual(response_content, [b\"non-seekable file content\"])\n        self.assertFalse(response.has_header(\"Content-Length\"))\n", "    def test_file_response_with_custom_block_size(self):\n        \"\"\"\n        Test that FileResponse reads the file content correctly with a custom block size.\n        \"\"\"\n        class CustomBlockSizeFile(io.BytesIO):\n                return super().read(n)\n\n        file_content = b\"0123456789\" * 100\n        custom_block_size = 10\n        file = CustomBlockSizeFile(file_content)\n        response = FileResponse(file, block_size=custom_block_size)\n        self.assertEqual(response.block_size, custom_block_size)\n        self.assertEqual(list(response), [file_content[i:i+custom_block_size] for i in range(0, len(file_content), custom_block_size)])\n"], "sample_465": ["def test_custom_formfield_overrides(self):\n    \"\"\"\n    Ensure that formfield_overrides works as expected, changing widgets\n    for specific fields.\n    \"\"\"\n    class CustomBandAdmin(ModelAdmin):\n        formfield_overrides = {\n            models.DateField: {\"widget\": AdminDateWidget(attrs={\"class\": \"custom-date\"})},\n        }\n\n    ma = CustomBandAdmin(Band, self.site)\n    form = ma.get_form(request)()\n\n    self.assertIsInstance(form.fields[\"sign_date\"].widget, AdminDateWidget)\n    self.assertIn(\"class\", form.fields[\"sign_date\"].widget.attrs)\n    self.assertEqual(form.fields[\"sign_date\"].widget.attrs[\"class\"], \"custom-date\")\n\n    self.assertNotIn(\"class\", form.fields[\"name\"].widget.attrs)\n    self.assertNotIn(\"class\", form.fields[\"bio\"].widget.attrs)\n", "def test_get_sortable_by(self):\n    \"\"\"\n    Test that get_sortable_by returns the correct fields for sorting in the changelist.\n    \"\"\"\n    class BandAdmin(ModelAdmin):\n        sortable_by = [\"name\"]\n\n    ma = BandAdmin(Band, self.site)\n    self.assertEqual(ma.get_sortable_by(request), [\"name\"])\n\n    # When sortable_by is None, it should default to list_display.\n    class BandAdmin(ModelAdmin):\n        list_display = [\"name\", \"bio\"]\n\n    ma = BandAdmin(Band, self.site)\n    self.assertEqual(ma.get_sortable_by(request), [\"name\", \"bio\"])\n", "    def test_get_view_on_site_url(self):\n        class BandAdmin(ModelAdmin):\n            view_on_site = True\n\n        ma = BandAdmin(Band, self.site)\n        obj = Band(name=\"The Beatles\", bio=\"\", sign_date=date(1962, 1, 1))\n        obj.pk = 1  # Simulate that object has been saved\n        expected_url = reverse(\n            \"admin:view_on_site\",\n            kwargs={\n                \"content_type_id\": get_content_type_for_model(obj).pk,\n                \"object_id\": obj.pk,\n            },\n            current_app=self.site.name,\n        )\n        self.assertEqual(ma.get_view_on_site_url(obj), expected_url)\n\n        # Test with callable view_on_site\n            return f\"/custom/{obj.pk}/\"\n\n        class BandAdminWithCallable(ModelAdmin):\n            view_on_site = custom_view_on_site\n\n        ma_callable = BandAdminWithCallable(Band, self.site)\n        self.assertEqual(ma_callable.get_view_on_site_url(obj), \"/custom/1/\")\n", "def test_get_readonly_fields(self):\n    \"\"\"\n    Test that get_readonly_fields returns the correct fields for different \n    request and obj scenarios.\n    \"\"\"\n    class BandAdmin(ModelAdmin):\n        readonly_fields = [\"name\", \"bio\"]\n\n            if obj:\n                return [\"name\"]\n            return [\"bio\"]\n\n    ma = BandAdmin(Band, self.site)\n    # When obj is not provided\n    self.assertEqual(ma.get_readonly_fields(request), [\"bio\"])\n    # When obj is provided\n    self.assertEqual(ma.get_readonly_fields(request, self.band), [\"name\"])\n", "def test_get_view_on_site_url(self):\n    class ConcertAdmin(ModelAdmin):\n            return \"/concerts/%s/\" % obj.pk\n\n    ma = ConcertAdmin(Concert, self.site)\n    concert = Concert.objects.create(main_band=self.band, opening_band=self.band, day=1)\n    # Test when view_on_site is a callable.\n    self.assertEqual(ma.get_view_on_site_url(concert), \"/concerts/%s/\" % concert.pk)\n\n    class BandAdmin(ModelAdmin):\n        view_on_site = True\n\n    ma = BandAdmin(Band, self.site)\n    # Test when view_on_site is True and the model has get_absolute_url method.\n    self.band.get_absolute_url = lambda: \"/bands/%s/\" % self.band.pk\n    self.assertEqual(\n        ma.get_view_on_site_url(self.band),\n        reverse(\n            \"admin:view_on_site\",\n            kwargs={\n                \"content_type_id\": get_content_type_for_model(self.band).pk,\n                \"object_id\": self.band.pk,\n            },\n            current_app=self.site.name,\n        ),\n    )\n\n    # Test when view_on_site is None.\n    ma.view_on_site = None\n    self.assertIsNone(ma.get_view_on_site_url(self.band))\n"], "sample_466": ["    def test_serialize_model_instance(self):\n        class MyModel(models.Model):\n            char_field = models.CharField(max_length=100)\n            int_field = models.IntegerField()\n\n                return self.char_field\n\n        my_instance = MyModel(char_field=\"test\", int_field=42)\n        with self.assertRaisesMessage(ValueError, \"Cannot serialize model instances.\"):\n            MigrationWriter.serialize(my_instance)\n", "    def test_serialize_complex_number(self):\n        complex_number = complex(1, 2)\n        self.assertSerializedEqual(complex_number)\n        self.assertSerializedResultEqual(\n            complex_number, \n            (\"complex(1, 2)\", set())\n        )\n", "    def test_migration_with_replaces(self):\n        \"\"\"\n        Tests a migration with 'replaces' attribute.\n        \"\"\"\n        migration = type(\n            \"Migration\",\n            (migrations.Migration,),\n            {\n                \"operations\": [\n                    migrations.CreateModel(\n                        \"MyModel\", [], {}, (models.Model,)\n                    ),\n                ],\n                \"dependencies\": [(\"testapp\", \"some_other_one\")],\n                \"replaces\": [(\"testapp\", \"0001_initial\"), (\"testapp\", \"0002_auto\")],\n            },\n        )\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"replaces = [('testapp', '0001_initial'), ('testapp', '0002_auto')]\", output)\n", "    def test_serialize_custom_class(self):\n        \"\"\"\n        Test serialization of a custom class instance.\n        \"\"\"\n        class CustomClass:\n                self.x = x\n                self.y = y\n\n                return (\n                    \"migrations.test_writer.CustomClass\",\n                    [self.x, self.y],\n                    {},\n                )\n\n        instance = CustomClass(1, 2)\n        string, imports = MigrationWriter.serialize(instance)\n        self.assertEqual(\n            string,\n            \"migrations.test_writer.CustomClass(1, 2)\",\n        )\n        self.assertEqual(imports, {\"import migrations.test_writer\"})\n        self.assertEqual(self.serialize_round_trip(instance), instance)\n", "    def test_serialize_custom_manager(self):\n        \"\"\"\n        Tests serialization of a custom manager with additional methods.\n        \"\"\"\n        class CustomManager(models.Manager):\n                return \"custom method result\"\n\n                return (\n                    \"%s.%s\" % (self.__class__.__module__, self.__class__.__name__),\n                    [],\n                    {}\n                )\n\n        manager = CustomManager()\n        string, imports = MigrationWriter.serialize(manager)\n        self.assertEqual(\n            string,\n            \"migrations.test_writer.test_serialize_custom_manager.<locals>.CustomManager()\"\n        )\n        self.assertEqual(\n            imports,\n            {\"import migrations.test_writer\"}\n        )\n        self.serialize_round_trip(manager)\n"], "sample_467": ["def test_use_required_attribute(self):\n    widget = SelectDateWidget(years=(\"2013\",))\n    self.assertTrue(widget.use_required_attribute(initial=False))\n\n    widget_with_values = SelectDateWidget(years=(\"2013\",), empty_label=\"empty_label\")\n    self.assertFalse(widget_with_values.use_required_attribute(initial=False))\n\n    widget_multiple = SelectDateWidget(\n        years=(\"2013\",), empty_label=(\"empty_year\", \"empty_month\", \"empty_day\")\n    )\n    self.assertFalse(widget_multiple.use_required_attribute(initial=False))\n", "    def test_initial_disabled(self):\n        widget = SelectDateWidget(\n            years=(\"2010\", \"2011\"),\n            empty_label=\"empty\",\n            attrs={\"disabled\": True},\n        )\n        self.check_html(\n            widget,\n            \"mydate\",\n            \"\",\n            html=(\n                \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\" disabled>\n                <option selected value=\"\">empty</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\" disabled>\n                <option selected value=\"\">empty</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"19\">19</option>\n                <option value=\"20\">20</option>\n                <option value=\"21\">21</option>\n                <option value=\"22\">22</option>\n                <option value=\"23\">23</option>\n                <option value", "    def test_render_with_initial_data(self):\n        widget = SelectDateWidget(\n            years=(\n                \"2007\",\n                \"2008\",\n                \"2009\",\n                \"2010\",\n                \"2011\",\n                \"2012\",\n                \"2013\",\n                \"2014\",\n                \"2015\",\n                \"2016\",\n            ),\n        )\n        self.check_html(\n            widget,\n            \"mydate\",\n            date(2012, 5, 20),\n            html=(\n                \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\" selected>May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"19\">19</option>\n                <option value=\"20\" selected>", "    def test_value_from_datadict_edge_cases(self):\n        \"\"\"\n        Test edge cases for value_from_datadict such as missing components\n        and invalid combinations.\n        \"\"\"\n        edge_cases = [\n            ({\"mydate_year\": \"2020\", \"mydate_month\": \"\", \"mydate_day\": \"15\"}, \"2020-0-15\"),\n            ({\"mydate_year\": \"\", \"mydate_month\": \"04\", \"mydate_day\": \"15\"}, \"0-4-15\"),\n            ({\"mydate_year\": \"2020\", \"mydate_month\": \"02\", \"mydate_day\": \"\"}, \"2020-2-0\"),\n            ({\"mydate_year\": \"\", \"mydate_month\": \"\", \"mydate_day\": \"\"}, None),\n            ({\"mydate_year\": \"2020\", \"mydate_month\": \"13\", \"mydate_day\": \"15\"}, \"2020-0-15\"),  # Invalid month\n            ({\"mydate_year\": \"2020\", \"mydate_month\": \"04\", \"mydate_day\": \"32\"}, \"2020-4-0\"),   # Invalid day\n        ]\n        for data, expected in edge_cases:\n            with self.subTest(data=data):\n                self.assertEqual(self.widget.value_from_datadict(data, {}, \"mydate\"), expected)\n", "def test_render_with_attrs(self):\n    widget = SelectDateWidget(attrs={\"class\": \"custom-class\"})\n    self.check_html(\n        widget,\n        \"mydate\",\n        \"\",\n        html=(\n            \"\"\"\n        <select name=\"mydate_month\" id=\"id_mydate_month\" class=\"custom-class\">\n            <option selected value=\"\">---</option>\n            <option value=\"1\">January</option>\n            <option value=\"2\">February</option>\n            <option value=\"3\">March</option>\n            <option value=\"4\">April</option>\n            <option value=\"5\">May</option>\n            <option value=\"6\">June</option>\n            <option value=\"7\">July</option>\n            <option value=\"8\">August</option>\n            <option value=\"9\">September</option>\n            <option value=\"10\">October</option>\n            <option value=\"11\">November</option>\n            <option value=\"12\">December</option>\n        </select>\n\n        <select name=\"mydate_day\" id=\"id_mydate_day\" class=\"custom-class\">\n            <option selected value=\"\">---</option>\n            <option value=\"1\">1</option>\n            <option value=\"2\">2</option>\n            <option value=\"3\">3</option>\n            <option value=\"4\">4</option>\n            <option value=\"5\">5</option>\n            <option value=\"6\">6</option>\n            <option value=\"7\">7</option>\n            <option value=\"8\">8</option>\n            <option value=\"9\">9</option>\n            <option value=\"10\">10</option>\n            <option value=\"11\">11</option>\n            <option value=\"12\">12</option>\n            <option value=\"13\">13</option>\n            <option value=\"14\">14</option>\n            <option value=\"15\">15</option>\n            <option value=\"16\">16</option>\n            <option value=\"17\">17</option>\n            <option value=\"18\">18</option>\n            <option value=\"19\">19</option>\n            <option value=\"20\">20</option>\n            <option value=\"21\">21</option>\n            <option value=\"22\">22</option>\n            <option value=\"23\">23</option>\n            <option value=\"24\">24</option>\n            <option value=\"25\">"], "sample_469": ["    def test_subquery_annotation(self):\n        long_books_qs = Book.objects.filter(pages__gt=400).values('publisher').annotate(count=Count('pk')).values('count')\n        qs = Publisher.objects.annotate(long_books=Subquery(long_books_qs, output_field=IntegerField()))\n        for publisher in qs:\n            if publisher.long_books:\n                self.assertGreater(publisher.long_books, 0)\n            else:\n                self.assertEqual(publisher.long_books, None)\n", "    def setUpTestData(cls):\n        cls.sql_query = \"SELECT name, age FROM author WHERE age > %s\"\n        cls.params = (30,)\n        cls.raw_query = RawQuery(cls.sql_query, using=DEFAULT_DB_ALIAS, params=cls.params)\n", "    def test_sql_with_params(self):\n        query = Book.objects.all().query\n        sql, params = query.sql_with_params()\n        self.assertIsInstance(sql, str)\n        self.assertIsInstance(params, (list, tuple))\n        self.assertIn('%s', sql)\n        self.assertEqual(len(params), 0)\n", "    def test_clone_query_with_different_using(self):\n        query = RawQuery(\"SELECT * FROM Book\", using=\"default\")\n        cloned_query = query.clone(using=\"other\")\n        self.assertEqual(query.sql, cloned_query.sql)\n        self.assertEqual(cloned_query.using, \"other\")\n        self.assertEqual(query.params, cloned_query.params)\n", "    def test_raw_query_execute(self):\n        # Test RawQuery execution with basic SQL command\n        raw_query = RawQuery(\"SELECT name FROM annotations_author WHERE age > %s\", \"default\", params=(30,))\n        results = list(raw_query)\n        self.assertEqual(len(results), 6)\n        self.assertIn((\"Adrian Holovaty\",), results)\n        self.assertIn((\"Jacob Kaplan-Moss\",), results)\n        self.assertIn((\"Brad Dayley\",), results)\n        self.assertIn((\"Peter Norvig\",), results)\n        self.assertIn((\"Stuart Russell\",), results)\n        self.assertIn((\"Jeffrey Forcier\",), results)\n"], "sample_468": ["    def test_request_context_bind_template(self):\n        request = self.request_factory.get(\"/\")\n        template = Template(\"sample template\")\n        ctx = RequestContext(request, {})\n        \n        with ctx.bind_template(template):\n            self.assertEqual(ctx.template, template)\n        \n        self.assertIsNone(ctx.template)\n\n        with self.assertRaises(RuntimeError):\n            with ctx.bind_template(template):\n                with ctx.bind_template(template):\n                    pass\n", "def test_make_context_with_dict_and_request(self):\n    request = self.request_factory.get(\"/\")\n    context_dict = {\"a\": 1, \"b\": \"xyzzy\"}\n    context = make_context(context_dict, request=request)\n    self.assertIsInstance(context, RequestContext)\n    self.assertEqual(context[\"a\"], 1)\n    self.assertEqual(context[\"b\"], \"xyzzy\")\n", "def test_context_manager_bind_template(self):\n    engine = Engine(\n        loaders=[\n            (\n                \"django.template.loaders.locmem.Loader\",\n                {\n                    \"template\": \"Content: {{ foo }}\",\n                },\n            ),\n        ]\n    )\n    template = engine.get_template(\"template\")\n    context = Context({\"foo\": \"bar\"})\n    with context.bind_template(template):\n        self.assertEqual(context.template, template)\n    self.assertIsNone(context.template)\n", "    def test_bind_template_context_manager(self):\n        \"\"\"\n        Test the bind_template context manager in Context class.\n        \"\"\"\n        c = Context({\"a\": 1})\n        template = mock.Mock()\n        with c.bind_template(template):\n            self.assertEqual(c.template, template)\n        self.assertIsNone(c.template)\n\n        # Test that RuntimeError is raised if context is already bound to a template\n        with c.bind_template(template):\n            with self.assertRaises(RuntimeError):\n                with c.bind_template(template):\n                    pass\n", "    def test_context_pop_exception(self):\n        c = Context({\"a\": 1})\n        c.pop()  # This should leave only the builtins in the stack\n        with self.assertRaises(ContextPopException):\n            c.pop()\n"], "sample_470": ["    def test_partition(self):\n            return num % 2 == 0\n\n        evens, odds = partition(is_even, range(10))\n        self.assertEqual(evens, [0, 2, 4, 6, 8])\n        self.assertEqual(odds, [1, 3, 5, 7, 9])\n", "    def test_partition(self):\n        \"\"\"Test the partition function to ensure it correctly splits values based on predicate.\"\"\"\n\n            return x % 2 == 0\n\n        values = [1, 2, 3, 4, 5, 6]\n        evens, odds = partition(is_even, values)\n        \n        self.assertEqual(evens, [2, 4, 6])\n        self.assertEqual(odds, [1, 3, 5])\n", "    def test_lazy_object_deepcopy(self):\n        \"\"\"Test deepcopy functionality of LazyObject.\"\"\"\n\n        class Klazz:\n                self.value = value\n\n        lazy_obj = lazy(lambda: Klazz(10), Klazz)()\n        copied_obj = copy.deepcopy(lazy_obj)\n\n        self.assertEqual(copied_obj.value, 10)\n        self.assertIsNot(copied_obj, lazy_obj)\n", "    def test_lazy_object_deepcopy(self):\n        \"\"\"\n        Ensure deepcopy works correctly on a lazy object.\n        \"\"\"\n        class Foo:\n                self.value = value\n\n                return Foo(copy.deepcopy(self.value, memo))\n\n        lazy_foo = lazy(lambda: Foo([1, 2, 3]), Foo)()\n        lazy_foo_deepcopy = copy.deepcopy(lazy_foo)\n        self.assertEqual(lazy_foo.value, lazy_foo_deepcopy.value)\n        self.assertIsNot(lazy_foo.value, lazy_foo_deepcopy.value)\n", "    def test_lazy_proxy_method_delegation(self):\n        \"\"\"\n        Ensure that magic methods are properly delegated to the underlying\n        object in lazy proxies.\n        \"\"\"\n        class Underlying:\n                self.value = value\n\n                return self.value[index]\n\n                self.value[index] = value\n\n                del self.value[index]\n\n                return iter(self.value)\n\n                return len(self.value)\n\n        lazy_underlying = lazy(lambda: Underlying([1, 2, 3]), Underlying)()\n        self.assertEqual(lazy_underlying[0], 1)\n        lazy_underlying[1] = 10\n        self.assertEqual(lazy_underlying[1], 10)\n        del lazy_underlying[2]\n        self.assertEqual(len(lazy_underlying), 2)\n        self.assertEqual(list(lazy_underlying), [1, 10])\n"], "sample_471": ["    def test_charfield_basic(self):\n        f = CharField()\n        self.assertWidgetRendersTo(f, '<input type=\"text\" name=\"f\" id=\"id_f\" required>')\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(\"\")\n        self.assertEqual(\"test\", f.clean(\"test\"))\n        self.assertEqual(\"  test\", f.clean(\"  test\"))\n        self.assertEqual(\"test  \", f.clean(\"test  \"))\n        self.assertEqual(\"\", f.clean(None))\n        ", "    def test_charfield_basic(self):\n        f = CharField()\n        self.assertWidgetRendersTo(f, '<input type=\"text\" name=\"f\" id=\"id_f\" required>')\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(\"\")\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(None)\n        self.assertEqual(\"hello\", f.clean(\"hello\"))\n        self.assertEqual(\"hello\", f.clean(\" hello \"))\n        self.assertEqual(\"\", f.clean(\"\"))\n", "def test_integerfield_custom_error_messages(self):\n        custom_error_messages = {\n            \"required\": \"This field cannot be empty.\",\n            \"invalid\": \"Please enter a valid integer.\",\n        }\n        f = IntegerField(error_messages=custom_error_messages)\n        self.assertWidgetRendersTo(\n            f, '<input type=\"number\" name=\"f\" id=\"id_f\" required>'\n        )\n        with self.assertRaisesMessage(ValidationError, \"'This field cannot be empty.'\"):\n            f.clean(\"\")\n        with self.assertRaisesMessage(ValidationError, \"'This field cannot be empty.'\"):\n            f.clean(None)\n        with self.assertRaisesMessage(ValidationError, \"'Please enter a valid integer.'\"):\n            f.clean(\"a\")\n        with self.assertRaisesMessage(ValidationError, \"'Please enter a valid integer.'\"):\n            f.clean(\"1.5\")\n", "    def test_charfield_1(self):\n        f = CharField(max_length=10)\n        self.assertWidgetRendersTo(\n            f, '<input type=\"text\" name=\"f\" id=\"id_f\" maxlength=\"10\" required>'\n        )\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(\"\")\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(None)\n        self.assertEqual(\"hello\", f.clean(\"hello\"))\n        self.assertIsInstance(f.clean(\"hello\"), str)\n        self.assertEqual(\"12345\", f.clean(\"12345\"))\n        with self.assertRaisesMessage(ValidationError, \"'Ensure this value has at most 10 characters (it has 11).\"):\n            f.clean(\"a\" * 11)\n        self.assertEqual(\"short\", f.clean(\"short\"))\n        self.assertEqual(\"short\", f.clean(\" short \"))\n        self.assertEqual(\"short\", f.clean(\"short \"))\n        with self.assertRaisesMessage(ValidationError, \"'Null characters are not allowed.\"):\n            f.clean(\"null\\0char\")\n        self.assertEqual(f.max_length, 10)\n", "    def test_charfield_basic(self):\n        f = CharField()\n        self.assertWidgetRendersTo(\n            f, '<input type=\"text\" name=\"f\" id=\"id_f\" required>'\n        )\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(\"\")\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(None)\n        self.assertEqual(\"test\", f.clean(\"test\"))\n        self.assertIsInstance(f.clean(\"test\"), str)\n        self.assertEqual(\"test\", f.clean(\" test \"))\n        self.assertEqual(\"\", f.clean(\"\"))\n"], "sample_472": ["    def test_paginate_with_non_list_iterable(self):\n        \"\"\"\n        Test paginator with a non-list iterable to ensure it can handle any iterable.\n        \"\"\"\n            for i in range(1, 11):\n                yield i\n\n        paginator = Paginator(generator(), 3)\n        self.assertEqual(paginator.count, 10)\n        self.assertEqual(paginator.num_pages, 4)\n        self.assertEqual(list(paginator.page_range), [1, 2, 3, 4])\n\n        page1 = paginator.page(1)\n        self.assertEqual(list(page1), [1, 2, 3])\n        self.assertTrue(page1.has_next())\n        self.assertFalse(page1.has_previous())\n\n        page4 = paginator.page(4)\n        self.assertEqual(list(page4), [10])\n        self.assertFalse(page4.has_next())\n        self.assertTrue(page4.has_previous())\n", "    def test_paginating_custom_sequence_class(self):\n        class CustomSequence:\n                self.data = data\n\n                return len(self.data)\n\n                return self.data[index]\n\n        data = CustomSequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        paginator = Paginator(data, 4)\n        self.assertEqual(paginator.count, 10)\n        self.assertEqual(paginator.num_pages, 3)\n        self.assertEqual(list(paginator.page_range), [1, 2, 3])\n        self.assertEqual(list(paginator.page(1)), [1, 2, 3, 4])\n        self.assertEqual(list(paginator.page(2)), [5, 6, 7, 8])\n        self.assertEqual(list(paginator.page(3)), [9, 10])\n", "    def test_validate_number(self):\n        \"\"\"\n        Paginator.validate_number() returns the valid page number or raises the\n        appropriate exceptions for invalid page numbers.\n        \"\"\"\n        paginator = Paginator(range(30), 10)\n        # Valid page numbers\n        self.assertEqual(paginator.validate_number(1), 1)\n        self.assertEqual(paginator.validate_number(2), 2)\n        self.assertEqual(paginator.validate_number(3), 3)\n        # Page number out of range\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(4)\n        # Page number less than 1\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(0)\n", "    def test_custom_page_class(self):\n        \"\"\"\n        Test that Paginator can work with a custom Page class.\n        \"\"\"\n        class CustomPage(Page):\n                return \"custom\"\n\n        class CustomPaginator(Paginator):\n                return CustomPage(*args, **kwargs)\n\n        paginator = CustomPaginator(range(1, 11), 4)\n        page = paginator.page(1)\n        self.assertEqual(page.custom_method(), \"custom\")\n        self.assertEqual(str(page), \"<Page 1 of 3>\")\n        self.assertEqual(page.start_index(), 1)\n        self.assertEqual(page.end_index(), 4)\n        self.assertTrue(page.has_next())\n        self.assertFalse(page.has_previous())\n        self.assertEqual(page.next_page_number(), 2)\n        with self.assertRaises(InvalidPage):\n            page.previous_page_number()\n", "    def test_paginator_with_callable_object_list(self):\n        \"\"\"\n        Tests the paginator with an object_list that is callable.\n        \"\"\"\n        class CallableObjectList:\n                return range(1, 101)\n\n                return 100\n\n        paginator = Paginator(CallableObjectList(), 10)\n        self.assertEqual(paginator.count, 100)\n        self.assertEqual(paginator.num_pages, 10)\n        self.assertEqual(list(paginator.page_range), list(range(1, 11)))\n        page = paginator.page(1)\n        self.assertEqual(list(page.object_list), list(range(1, 11)))\n"], "sample_473": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_474": ["    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\", alias=\"rhondar\")\n    ", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\", alias=\"rhonda\")\n"], "sample_475": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_readonly_fields_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"not a callable or boolean\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_not_list_or_tuple(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"name\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_readonly_fields_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"name\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n"], "sample_476": ["    def test_generate_filename_with_callable(self):\n        \"\"\"\n        Tests that the generate_filename method correctly uses a callable upload_to.\n        \"\"\"\n            return \"generated_path/\" + filename\n\n        field = ImageField(upload_to=callable_upload_to)\n        instance = self.PersonModel(name=\"Joe\")\n        filename = \"test_image.jpg\"\n        generated_filename = field.generate_filename(instance, filename)\n        self.assertEqual(generated_filename, \"generated_path/test_image.jpg\")\n", "    def setUp(self):\n        \"\"\"\n        Set up a temporary storage directory and a file instance for testing.\n        \"\"\"\n        if os.path.exists(temp_storage_dir):\n            shutil.rmtree(temp_storage_dir)\n        os.mkdir(temp_storage_dir)\n\n        file_path = os.path.join(os.path.dirname(__file__), \"testfile.txt\")\n        self.file = File(open(file_path, \"rb\"), name=\"testfile.txt\")\n", "compilation error", "def test_invalid_image_file(self):\n    \"\"\"\n    Tests behavior when an invalid image file is provided.\n    \"\"\"\n    p = self.PersonModel(name=\"Joe\")\n    invalid_file = File(open(os.path.join(os.path.dirname(__file__), \"invalid_file.txt\"), \"rb\"), name=\"invalid_file.txt\")\n\n    with self.assertRaises(Exception):  # Expecting an exception due to invalid image file\n        p.mugshot.save(\"invalid_file\", invalid_file)\n", "    def test_save_form_data(self):\n        \"\"\"\n        Tests the save_form_data method of FileField.\n        \"\"\"\n        p = self.PersonModel(name=\"Jane\")\n        # Initially, the mugshot should be an instance of the attr_class with no file.\n        self.assertIsInstance(p.mugshot, TestImageFieldFile)\n        self.assertFalse(p.mugshot)\n\n        # Save form data with an image file.\n        p.mugshot.field.save_form_data(p, self.file1)\n        self.check_dimensions(p, 4, 8)\n\n        # Save form data with None (should clear the image).\n        p.mugshot.field.save_form_data(p, None)\n        self.assertIsInstance(p.mugshot, TestImageFieldFile)\n        self.assertFalse(p.mugshot)\n        self.check_dimensions(p, None, None)\n\n        # Save form data with a false value (should clear the image).\n        p.mugshot.field.save_form_data(p, False)\n        self.assertIsInstance(p.mugshot, TestImageFieldFile)\n        self.assertFalse(p.mugshot)\n        self.check_dimensions(p, None, None)\n"], "sample_477": ["    def test_random03(self):\n        output = self.engine.render_to_string(\n            \"random03\", {\"a\": [\"a&b\", \"a&b\"], \"b\": [mark_safe(\"a&b\"), mark_safe(\"a&b\")]}\n        )\n        self.assertEqual(output, \"2 2\")\n", "    def test_addslashes01(self):\n        output = self.engine.render_to_string(\"addslashes01\", {\"value\": 'She said \"Hello\" and left.'})\n        self.assertEqual(output, 'She said \\\\\"Hello\\\\\" and left.')\n", "    def test_random03(self):\n        output = self.engine.render_to_string(\"random03\", {\"c\": []})\n        self.assertEqual(output, \"\")\n", "    def test_random03(self):\n        output = self.engine.render_to_string(\n            \"random03\", {\"a\": [], \"b\": [mark_safe(\"a&b\")]}\n        )\n        self.assertEqual(output, \" \")\n", "    def test_random03(self):\n        output = self.engine.render_to_string(\n            \"random03\", {\"c\": []}\n        )\n        self.assertEqual(output, \"\")\n"], "sample_478": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"name\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_view_on_site_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"not_callable_or_boolean\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_exclude_duplicate_fields(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = [\"name\", \"name\"]\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' contains duplicate field(s).\",\n            \"admin.E015\",\n        )\n", "    def test_readonly_fields_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n"], "sample_479": ["    def test_create_alter_table_comment(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"MyModel\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.AlterModelTableComment(\"MyModel\", \"New comment\"),\n            ],\n            [\n                migrations.CreateModel(\n                    \"MyModel\",\n                    [(\"name\", models.CharField(max_length=255))],\n                    options={\"db_table_comment\": \"New comment\"},\n                ),\n            ],\n        )\n", "    def test_create_model_alter_table_comment(self):\n        \"\"\"\n        AlterModelTableComment should optimize into CreateModel.\n        \"\"\"\n        managers = [(\"objects\", EmptyManager())]\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n                migrations.AlterModelTableComment(\n                    name=\"Foo\", table_comment=\"This is a test comment\"\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\", \"db_table_comment\": \"This is a test comment\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n            ],\n        )\n", "    def test_create_model_alter_table_comment(self):\n        \"\"\"\n        AlterModelTableComment should optimize into CreateModel.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                ),\n                migrations.AlterModelTableComment(\n                    name=\"Foo\", table_comment=\"This is a comment.\"\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\", \"db_table_comment\": \"This is a comment.\"},\n                    bases=(UnicodeModel,),\n                ),\n            ],\n        )\n", "    def test_alter_model_table_comment(self):\n        \"\"\"\n        AlterModelTableComment should optimize into CreateModel.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                ),\n                migrations.AlterModelTableComment(\"Foo\", \"New comment for the table\"),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                    options={\n                        \"verbose_name\": \"Foo\",\n                        \"db_table_comment\": \"New comment for the table\",\n                    },\n                ),\n            ],\n        )\n", "    def test_alter_model_table(self):\n        \"\"\"\n        AlterModelTable should optimize into CreateModel.\n        \"\"\"\n        managers = [(\"objects\", EmptyManager())]\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n                migrations.AlterModelTable(\"Foo\", \"new_table\"),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\", \"db_table\": \"new_table\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n            ],\n        )\n"], "sample_480": ["    def test_custom_encoder_decoder_error(self):\n        value = {\"uuid\": uuid.UUID(\"{d85e2076-b67c-4ee7-8c3a-2bf5a2cc2475}\")}\n        obj = NullableJSONModel(value_custom=value)\n        obj.clean_fields()\n        obj.save()\n        obj.refresh_from_db()\n        with self.assertRaises(json.JSONDecodeError):\n            obj.value_custom = '{\"uuid\": \"not a valid uuid\"}'\n            obj.clean_fields()\n", "    def test_key_transform_numeric_lookups(self):\n        obj = NullableJSONModel.objects.create(value={\"a\": 1, \"b\": 2.5, \"c\": -3})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a__lt=2),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__b__lte=2.5),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__c__gt=-4),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__c__gte=-3),\n            [obj],\n        )\n", "    def test_jsonfield_deconstruct_with_kwargs(self):\n        field = models.JSONField(verbose_name=\"JSON Data\", name=\"json_data\", encoder=DjangoJSONEncoder, decoder=CustomJSONDecoder)\n        name, path, args, kwargs = field.deconstruct()\n        self.assertEqual(path, \"django.db.models.JSONField\")\n        self.assertEqual(kwargs[\"verbose_name\"], \"JSON Data\")\n        self.assertEqual(kwargs[\"name\"], \"json_data\")\n        self.assertEqual(kwargs[\"encoder\"], DjangoJSONEncoder)\n        self.assertEqual(kwargs[\"decoder\"], CustomJSONDecoder)\n", "    def test_key_transform_factory(self):\n        key_name = \"test_key\"\n        factory = KeyTransformFactory(key_name)\n        transform = factory(\"lhs\")\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, key_name)\n        self.assertEqual(transform.lhs, \"lhs\")\n", "    def test_compile_json_path_with_root(self):\n        self.assertEqual(compile_json_path([\"a\", \"b\", \"c\"]), '$.\"a\".\"b\".\"c\"')\n        self.assertEqual(compile_json_path([\"a\", 1, \"c\"]), '$.\"a\"[1].\"c\"')\n"], "sample_481": ["    def test_join_with_non_string_iterable(self):\n        output = self.engine.render_to_string(\n            \"join09\", {\"a\": [1, 2, 3], \"var\": mark_safe(\", \")}\n        )\n        self.assertEqual(output, \"1, 2, 3\")\n", "def test_join09(self):\n    output = self.engine.render_to_string(\"join09\", {\"a\": [1, 2, 3], \"b\": \", \"})\n    self.assertEqual(output, \"1, 2, 3\")\n", "    def test_join_with_empty_list(self):\n        output = self.engine.render_to_string(\n            \"join09\", {\"a\": [], \"var\": \" & \"}\n        )\n        self.assertEqual(output, \"\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\n            \"join09\", {\"a\": [\"alpha\", \"beta\", \"gamma\"]}\n        )\n        self.assertEqual(output, \"alphabeta\u03b3\u03b3mma\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\n            \"join09\", {\"a\": [\"alpha\", \"beta & me\"], \"var\": mark_safe(\" & \")}\n        )\n        self.assertEqual(output, \"ALPHA & BETA &AMP; ME\")\n"], "sample_482": ["    def test_default(self):\n        output = self.engine.render_to_string(\"floatformat_default\", {\"num\": 34.23234})\n        self.assertEqual(output, \"34.2\")\n", "    def test_safe_in_list(self):\n        output = self.engine.render_to_string(\n            \"escapeseq_safe_in_list\",\n            {\"a\": [mark_safe(\"x&y\"), mark_safe(\"<p>\"), \"safe & <escaped>\"]},\n        )\n        self.assertEqual(output, \"x&y, <p>, safe &amp; &lt;escaped&gt;\")\n", "    def test_special_chars(self):\n        output = self.engine.render_to_string(\"escapeseq_special_chars\", {\"a\": [\"<>&'\\\"\"]})\n        self.assertEqual(output, \"&lt;&gt;&amp;&#39;&quot;\")\n", "    def test_make_list_string(self):\n        output = self.engine.render_to_string(\"make_list_string\")\n        self.assertEqual(output, \"['h', 'e', 'l', 'l', 'o']\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"cut_basic\",\n            {\"a\": \"hello world\", \"b\": \"foo bar\"},\n        )\n        self.assertEqual(output, \"helloworld -- f bar\")\n"], "sample_483": ["    def test_prepopulated_fields_not_a_dict(self):\n        class SongAdmin(admin.ModelAdmin):\n            prepopulated_fields = [\"title\"]\n\n        self.assertEqual(\n            SongAdmin(Song, AdminSite()).check(),\n            [\n                checks.Error(\n                    \"The value of 'prepopulated_fields' must be a dictionary.\",\n                    obj=SongAdmin,\n                    id=\"admin.E026\",\n                )\n            ],\n        )\n", "    def test_check_invalid_autocomplete_fields(self):\n        class AlbumAdmin(admin.ModelAdmin):\n            autocomplete_fields = [\"nonexistent\"]\n\n        errors = AlbumAdmin(Album, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields[0]' refers to 'nonexistent', which is not a field of 'admin_checks.Album'.\",\n                obj=AlbumAdmin,\n                id=\"admin.E037\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_autocomplete_fields_not_list_or_tuple(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = \"title\"\n\n        self.assertEqual(\n            SongAdmin(Song, AdminSite()).check(),\n            [\n                checks.Error(\n                    \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                    obj=SongAdmin,\n                    id=\"admin.E036\",\n                )\n            ],\n        )\n", "    def test_check_autocomplete_fields_not_list_or_tuple(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = \"title\"\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                obj=SongAdmin,\n                id=\"admin.E036\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_prepopulated_fields_invalid_field_type(self):\n        \"\"\"\n        Test that `prepopulated_fields` does not contain fields of invalid types.\n        \"\"\"\n\n        class SongAdmin(admin.ModelAdmin):\n            prepopulated_fields = {\"release_date\": (\"title\",)}\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'prepopulated_fields' refers to 'release_date', which must not be a \"\n                \"DateTimeField, a ForeignKey, a OneToOneField, or a ManyToManyField.\",\n                obj=SongAdmin,\n                id=\"admin.E028\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_484": ["    def test_basic_concat(self):\n        author = Author.objects.create(name=\"John\", alias=\"Doe\")\n        authors = Author.objects.annotate(full_name=Concat(Value(\"Mr. \"), \"name\", Value(\" \"), \"alias\"))\n        self.assertQuerySetEqual(\n            authors.order_by(\"name\"), [\"Mr. John Doe\"], lambda a: a.full_name\n        )\n", "    def test_basic_concat(self):\n        result = Author.objects.annotate(full_name=Concat(Value(\"John \"), Value(\"Smith\")))\n        self.assertQuerySetEqual(result, [\"John Smith\"], lambda a: a.full_name)\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n        Author.objects.create(name=\"Alice\", alias=\"\")\n", "    def test_basic_concat(self):\n        authors = Author.objects.annotate(full_name=Concat(Value(\"Mr. \"), \"name\"))\n        self.assertQuerySetEqual(\n            authors.order_by(\"name\"), [\"Mr. John Smith\", \"Mr. Rhonda\"], lambda a: a.full_name\n        )\n", "    def test_basic_concat(self):\n        authors = Author.objects.annotate(full_name=Concat(\"name\", Value(\" \"), \"alias\"))\n        self.assertQuerySetEqual(\n            authors.order_by(\"name\"),\n            [\"John Smith smithj\", \"Rhonda \"],\n            lambda a: a.full_name\n        )\n"], "sample_485": ["    def test_avoid_wrapping(self):\n        items = (\n            (\"Don't break this\", \"Don't\\xa0break\\xa0this\"),\n            (\"Multiple spaces  here\", \"Multiple\\xa0spaces\\xa0\\xa0here\"),\n            (\"Edge  case  with  more spaces\", \"Edge\\xa0\\xa0case\\xa0\\xa0with\\xa0\\xa0more\\xa0spaces\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"Hello world\", \"Hello\\xa0world\"),\n            (\"No wrapping here\", \"No\\xa0wrapping\\xa0here\"),\n            (\"A quick brown fox\", \"A\\xa0quick\\xa0brown\\xa0fox\"),\n            (\"\", \"\"),  # Edge case: empty string\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n            (\"Avoid wrapping text\", \"Avoid\\xa0wrapping\\xa0text\"),\n            (\"Non-breaking space\", \"Non-breaking\\xa0space\"),\n            (lazystr(\"Lazy string test\"), \"Lazy\\xa0string\\xa0test\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"Hello World\", \"Hello\\xa0World\"),\n            (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n            (\"No wrapping here\", \"No\\xa0wrapping\\xa0here\"),\n            (\"\", \"\"),\n            (\"SingleWord\", \"SingleWord\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"Hello world\", \"Hello\\xa0world\"),\n            (\"No wrapping here\", \"No\\xa0wrapping\\xa0here\"),\n            (\"SingleWord\", \"SingleWord\"),\n            (\"Multiple words with spaces\", \"Multiple\\xa0words\\xa0with\\xa0spaces\"),\n            (\"Ends with space \", \"Ends\\xa0with\\xa0space\\xa0\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n"], "sample_486": ["    def test_modelform_factory_validates_exclude_and_fields(self):\n        \"\"\"\n        Ensure that calling modelform_factory without defining 'fields' or 'exclude'\n        explicitly raises an ImproperlyConfigured exception.\n        \"\"\"\n        with self.assertRaises(ImproperlyConfigured):\n            modelform_factory(UUIDPKParent)\n        with self.assertRaises(ImproperlyConfigured):\n            modelform_factory(UUIDPKParent, fields=None, exclude=None)\n", "    def test_construct_instance_ignores_non_editable_fields(self):\n        \"\"\"\n        Test that construct_instance ignores non-editable fields when constructing the instance.\n        \"\"\"\n        from django import forms\n        from django.db import models\n\n        class TestModel(models.Model):\n            editable_field = models.CharField(max_length=100)\n            non_editable_field = models.CharField(max_length=100, editable=False)\n\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = TestModel\n                fields = ['editable_field', 'non_editable_field']\n\n        instance = TestModel(editable_field=\"initial\", non_editable_field=\"initial\")\n        form = TestForm({'editable_field': 'changed', 'non_editable_field': 'changed'}, instance=instance)\n        form.is_valid()\n        new_instance = construct_instance(form, instance)\n        self.assertEqual(new_instance.editable_field, \"changed\")\n        self.assertEqual(new_instance.non_editable_field, \"initial\")\n", "    def test_inlineformset_factory_applies_limit_choices_to(self):\n        \"\"\"\n        Test that limit_choices_to is applied to the formfield's queryset.\n        \"\"\"\n        FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n        parent_instance = UUIDPKParent.objects.create(name=\"Parent\")\n        child_instance1 = UUIDPKChild.objects.create(name=\"Child 1\", parent=parent_instance)\n        child_instance2 = UUIDPKChild.objects.create(name=\"Child 2\", parent=parent_instance)\n        \n        formset = FormSet(instance=parent_instance)\n        queryset = formset.forms[0].fields[\"parent\"].queryset\n        \n        # Verify limit_choices_to is applied\n        self.assertQuerysetEqual(\n            queryset, \n            UUIDPKParent.objects.filter(name=\"Parent\"), \n            transform=lambda x: x\n        )\n", "    def test_inlineformset_factory_with_initial_data(self):\n        \"\"\"\n        Test that the inline formset factory correctly initializes forms\n        with initial data when provided.\n        \"\"\"\n        parent = UUIDPKParent.objects.create(name=\"Parent\")\n        child1 = UUIDPKChild.objects.create(parent=parent, name=\"Child 1\")\n        child2 = UUIDPKChild.objects.create(parent=parent, name=\"Child 2\")\n        \n        FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n        formset = FormSet(instance=parent)\n        \n        self.assertEqual(len(formset.forms), 2)\n        self.assertEqual(formset.forms[0].initial[\"name\"], \"Child 1\")\n        self.assertEqual(formset.forms[1].initial[\"name\"], \"Child 2\")\n        self.assertEqual(formset.forms[0].instance, child1)\n        self.assertEqual(formset.forms[1].instance, child2)\n", "    def test_inlineformset_factory_with_custom_fk_name(self):\n        \"\"\"\n        Test inlineformset_factory with a custom foreign key name.\n        \"\"\"\n        FormSet = inlineformset_factory(\n            UUIDPKParent, UUIDPKChild, fk_name='parent_custom', fields=\"__all__\"\n        )\n        formset = FormSet()\n        self.assertIsNone(formset.forms[0].fields[\"parent_custom\"].initial)\n"], "sample_487": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_readonly_fields_is_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_readonly_fields_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"name\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n"], "sample_488": ["    def test_register_model(self):\n        \"\"\"\n        Tests the register_model() method.\n        \"\"\"\n        new_apps = Apps([\"apps\"])\n        meta_contents = {\n            \"app_label\": \"apps\",\n            \"apps\": new_apps,\n        }\n\n        # Create a new model and register it\n        body = {}\n        body[\"Meta\"] = type(\"Meta\", (), meta_contents)\n        body[\"__module__\"] = TotallyNormal.__module__\n        temp_model = type(\"NewModel\", (models.Model,), body)\n        new_apps.register_model(\"apps\", temp_model)\n\n        # Ensure the model is registered correctly\n        self.assertEqual(new_apps.get_registered_model(\"apps\", \"NewModel\"), temp_model)\n\n        # Registering the same model again should issue a warning\n        with self.assertRaisesMessage(\n            RuntimeWarning,\n            \"Model 'apps.newmodel' was already registered. Reloading models is not advised as it can lead to inconsistencies, most notably with related models.\"\n        ):\n            new_apps.register_model(\"apps\", temp_model)\n\n        # Registering a different model with the same name should raise a RuntimeError\n        new_temp_model = type(\"NewModel\", (models.Model,), body)\n        new_temp_model.__module__ = \"different_module\"\n        with self.assertRaisesMessage(\n            RuntimeError,\n            \"Conflicting 'newmodel' models in application 'apps': <class 'apps.models.NewModel'> and <class 'different_module.NewModel'>.\"\n        ):\n            new_apps.register_model(\"apps\", new_temp_model)\n", "    def test_register_model(self):\n        \"\"\"\n        Tests the register_model() method.\n        \"\"\"\n        new_apps = Apps([\"apps\"])\n        class DummyModel:\n            class _meta:\n                model_name = \"dummymodel\"\n                app_label = \"apps\"\n        new_apps.register_model(\"apps\", DummyModel)\n        self.assertEqual(new_apps.get_model(\"apps\", \"dummymodel\"), DummyModel)\n\n        with self.assertRaises(RuntimeError):\n            new_apps.register_model(\"apps\", DummyModel)\n\n        class ConflictingModel:\n            class _meta:\n                model_name = \"dummymodel\"\n                app_label = \"apps\"\n        with self.assertRaisesMessage(RuntimeError, \"Conflicting 'dummymodel' models in application 'apps'\"):\n            new_apps.register_model(\"apps\", ConflictingModel)\n", "    def test_register_model(self):\n        \"\"\"\n        Tests the register_model method of the Apps class.\n        \"\"\"\n        new_apps = Apps([\"apps\"])\n        old_models = list(new_apps.get_app_config(\"apps\").get_models())\n        \n        class NewModel(models.Model):\n            class Meta:\n                app_label = \"apps\"\n                apps = new_apps\n        \n        # Register the model\n        new_apps.register_model(\"apps\", NewModel)\n        \n        # Ensure the model is registered\n        self.assertIn(NewModel, new_apps.get_app_config(\"apps\").get_models())\n        \n        # Ensure the model wasn't added to the old_apps registry\n        self.assertNotIn(NewModel, apps.get_app_config(\"apps\").get_models())\n\n        # Test re-registering the same model should not raise an error\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            new_apps.register_model(\"apps\", NewModel)\n            self.assertEqual(len(w), 1)\n            self.assertTrue(issubclass(w[-1].category, RuntimeWarning))\n\n        # Test registering a model with the same name but different module should raise an error\n        class ConflictingModel(models.Model):\n            class Meta:\n                app_label = \"apps\"\n                apps = new_apps\n                model_name = NewModel._meta.model_name\n        \n        ConflictingModel.__module__ = \"conflicting_module\"\n        with self.assertRaises(RuntimeError):\n            new_apps.register_model(\"apps\", ConflictingModel)\n", "    def test_get_registered_model(self):\n        \"\"\"\n        Tests apps.get_registered_model().\n        \"\"\"\n        # Valid model\n        self.assertEqual(apps.get_registered_model(\"admin\", \"logentry\"), LogEntry)\n        # Invalid model\n        with self.assertRaises(LookupError):\n            apps.get_registered_model(\"admin\", \"nonexistentmodel\")\n        # Case-insensitive model name\n        self.assertEqual(apps.get_registered_model(\"admin\", \"lOGeNTRY\"), LogEntry)\n", "    def test_register_model(self, apps):\n        \"\"\"\n        Tests apps.register_model().\n        \"\"\"\n        class TestModel1(models.Model):\n            class Meta:\n                app_label = \"apps\"\n                model_name = \"testmodel1\"\n\n        class TestModel2(models.Model):\n            class Meta:\n                app_label = \"apps\"\n                model_name = \"testmodel2\"\n\n        # Register the model\n        apps.register_model(\"apps\", TestModel1)\n        self.assertEqual(apps.get_registered_model(\"apps\", \"testmodel1\"), TestModel1)\n\n        # Register another model with a different name\n        apps.register_model(\"apps\", TestModel2)\n        self.assertEqual(apps.get_registered_model(\"apps\", \"testmodel2\"), TestModel2)\n\n        # Test re-registering the same model should issue a warning\n        with self.assertRaisesMessage(\n            RuntimeWarning,\n            \"Model 'apps.testmodel1' was already registered. Reloading models is not advised as it can lead to inconsistencies, most notably with related models.\"\n        ):\n            apps.register_model(\"apps\", TestModel1)\n\n        # Test conflicting model registration should raise a RuntimeError\n        class ConflictingModel(models.Model):\n            class Meta:\n                app_label = \"apps\"\n                model_name = \"testmodel1\"\n\n        with self.assertRaisesMessage(\n            RuntimeError,\n            \"Conflicting 'testmodel1' models in application 'apps':\"\n        ):\n            apps.register_model(\"apps\", ConflictingModel)\n"], "sample_489": ["    def test_bulk_create_with_deferred_fields(self):\n        \"\"\"\n        Test bulk_create with a model having deferred fields.\n        \"\"\"\n        countries = [\n            Country(name=\"Country A\", iso_two_letter=\"CA\"),\n            Country(name=\"Country B\", iso_two_letter=\"CB\"),\n        ]\n        Country.objects.bulk_create(countries)\n        deferred_countries = Country.objects.defer(\"name\").all()\n        self.assertEqual(deferred_countries.count(), 2)\n        self.assertEqual(deferred_countries[0].iso_two_letter, \"CA\")\n        self.assertEqual(deferred_countries[1].iso_two_letter, \"CB\")\n", "    def test_bulk_insert_exceptions(self):\n        # Ensure bulk_create raises an exception with invalid data types\n        with self.assertRaises(ValueError):\n            Country.objects.bulk_create([Country(name=123, iso_two_letter=\"USA\")])\n        with self.assertRaises(ValueError):\n            Country.objects.bulk_create([Country(name=\"Germany\", iso_two_letter=123)])\n        with self.assertRaises(IntegrityError):\n            Country.objects.bulk_create([Country(name=\"Germany\", iso_two_letter=None)])\n", "    def test_bulk_create_returning_fields(self):\n        objs = [\n            Country(name=\"Country1\", iso_two_letter=\"C1\"),\n            Country(name=\"Country2\", iso_two_letter=\"C2\"),\n        ]\n        created_objs = Country.objects.bulk_create(objs)\n        self.assertEqual(len(created_objs), len(objs))\n        for obj, created_obj in zip(objs, created_objs):\n            self.assertIsNotNone(created_obj.pk)\n            self.assertEqual(obj.name, created_obj.name)\n            self.assertEqual(obj.iso_two_letter, created_obj.iso_two_letter)\n", "    def setUp(self):\n        self.countries = [\n            Country(name=\"United States of America\", iso_two_letter=\"US\"),\n            Country(name=\"The Netherlands\", iso_two_letter=\"NL\"),\n            Country(name=\"Germany\", iso_two_letter=\"DE\"),\n            Country(name=\"Czech Republic\", iso_two_letter=\"CZ\"),\n        ]\n        Country.objects.bulk_create(self.countries)\n", "    def setUp(self):\n        self.data = [\n            Country(name=\"USA\", iso_two_letter=\"US\"),\n            Country(name=\"Canada\", iso_two_letter=\"CA\"),\n            Country(name=\"Mexico\", iso_two_letter=\"MX\"),\n        ]\n        Country.objects.bulk_create(self.data)\n"], "sample_490": ["    def test_unique_constraint_deferred(self):\n        with self.assertRaisesMessage(\n            ValueError,\n            \"UniqueConstraint with expressions cannot be deferred.\"\n        ):\n            models.UniqueConstraint(\n                Lower(\"title\"),\n                name=\"deferred_unique_constraint\",\n                deferrable=models.Deferrable.DEFERRED,\n            )\n", "    def test_custom_violation_error_message(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"unique_name\",\n            violation_error_message=\"This name must be unique.\",\n        )\n        non_unique_product = UniqueConstraintProduct(name=self.p1.name)\n        msg = \"This name must be unique.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, non_unique_product)\n        constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=\"unique_name_2\"))\n", "    def test_unique_constraint_custom_violation_message(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"unique_name_custom_error\",\n            violation_error_message=\"Custom violation message for %(name)s\",\n        )\n        non_unique_product = UniqueConstraintProduct(name=self.p1.name)\n        msg = \"Custom violation message for unique_name_custom_error\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, non_unique_product)\n        self.assertEqual(\n            constraint.get_violation_error_message(),\n            \"Custom violation message for unique_name_custom_error\"\n        )\n", "    def test_unique_constraint_with_combined_fields_and_expressions(self):\n        msg = \"UniqueConstraint.fields and expressions are mutually exclusive.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field_1\"),\n                F(\"field_2\"),\n                fields=[\"field_3\"],\n                name=\"unique_combined\"\n            )\n", "    def test_deferrable_enum_values(self):\n        self.assertEqual(models.Deferrable.DEFERRED.value, \"deferred\")\n        self.assertEqual(models.Deferrable.IMMEDIATE.value, \"immediate\")\n"], "sample_491": ["    def test_boundfield_css_classes_with_errors(self):\n        class PersonForm(Form):\n            name = CharField()\n            email = EmailField()\n\n        form = PersonForm({\"name\": \"\", \"email\": \"invalid\"})\n        form.is_valid()  # Trigger validation\n\n        name_field = form[\"name\"]\n        email_field = form[\"email\"]\n\n        # Check css_classes without extra_classes\n        self.assertEqual(name_field.css_classes(), \"error\")\n        self.assertEqual(email_field.css_classes(), \"error\")\n\n        # Check css_classes with extra_classes\n        self.assertEqual(name_field.css_classes(extra_classes=\"required\"), \"required error\")\n        self.assertEqual(email_field.css_classes(extra_classes=\"required\"), \"required error\")\n\n        # Check css_classes with multiple extra_classes\n        self.assertEqual(name_field.css_classes(extra_classes=\"required custom\"), \"required custom error\")\n        self.assertEqual(email_field.css_classes(extra_classes=\"required custom\"), \"required custom error\")\n", "    def test_boundfield_auto_id_property(self):\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = CharField(widget=TextInput(attrs={\"id\": \"custom_id\"}))\n\n        form = TestForm(auto_id=\"prefix_%s\")\n        self.assertEqual(form[\"field1\"].auto_id, \"prefix_field1\")\n        self.assertEqual(form[\"field2\"].auto_id, \"custom_id\")\n\n        form = TestForm(auto_id=True)\n        self.assertEqual(form[\"field1\"].auto_id, \"field1\")\n        self.assertEqual(form[\"field2\"].auto_id, \"custom_id\")\n\n        form = TestForm(auto_id=False)\n        self.assertEqual(form[\"field1\"].auto_id, \"\")\n        self.assertEqual(form[\"field2\"].auto_id, \"custom_id\")\n", "def test_boundfield_css_classes_with_errors(self):\n    class MyForm(Form):\n        field = CharField()\n    \n    form = MyForm({})\n    form.is_valid()  # Trigger validation to populate errors\n    field = form[\"field\"]\n    \n    # Test the css_classes method with extra classes\n    self.assertEqual(field.css_classes(\"extra-class\"), \"error-class extra-class\")\n    \n    # Trigger an error on the field\n    form = MyForm({\"field\": \"\"})\n    form.is_valid()\n    field = form[\"field\"]\n    \n    # Test the css_classes method with extra classes when there's an error\n    self.assertEqual(field.css_classes(\"extra-class\"), \"error-class extra-class\")\n    \n    # Test the css_classes method without extra classes when there's an error\n    self.assertEqual(field.css_classes(), \"error-class\")\n", "def test_boundfield_subwidgets(self):\n    class TestForm(Form):\n        option = ChoiceField(\n            choices=[(\"opt1\", \"Option 1\"), (\"opt2\", \"Option 2\")],\n            widget=RadioSelect\n        )\n\n    form = TestForm()\n    subwidgets = form[\"option\"].subwidgets\n    self.assertEqual(len(subwidgets), 2)\n    self.assertEqual(subwidgets[0].choice_label, \"Option 1\")\n    self.assertEqual(subwidgets[1].choice_label, \"Option 2\")\n    self.assertHTMLEqual(\n        str(subwidgets[0]),\n        '<label for=\"id_option_0\">'\n        '<input type=\"radio\" id=\"id_option_0\" value=\"opt1\" name=\"option\" required> Option 1'\n        '</label>'\n    )\n    self.assertHTMLEqual(\n        str(subwidgets[1]),\n        '<label for=\"id_option_1\">'\n        '<input type=\"radio\" id=\"id_option_1\" value=\"opt2\" name=\"option\" required> Option 2'\n        '</label>'\n    )\n", "    def test_boundfield_as_widget_attrs(self):\n        class CustomForm(Form):\n            field = CharField(widget=TextInput(attrs={\"class\": \"custom-class\"}))\n\n        form = CustomForm()\n        self.assertHTMLEqual(\n            form[\"field\"].as_widget(attrs={\"placeholder\": \"Enter text here\"}),\n            '<input type=\"text\" name=\"field\" class=\"custom-class\" placeholder=\"Enter text here\" required id=\"id_field\">'\n        )\n        self.assertHTMLEqual(\n            form[\"field\"].as_widget(attrs={\"class\": \"override-class\"}),\n            '<input type=\"text\" name=\"field\" class=\"custom-class override-class\" required id=\"id_field\">'\n        )\n"], "sample_492": ["    def test_built_in_import(self):\n        \"\"\"\n        Test to ensure built-in imports are serialized correctly.\n        \"\"\"\n        migration = type(\n            \"Migration\",\n            (migrations.Migration,),\n            {\n                \"operations\": [\n                    migrations.RunPython(\n                        code=migrations.RunPython.noop,\n                        reverse_code=migrations.RunPython.noop,\n                    )\n                ],\n            },\n        )\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"from django.db import migrations\\n\", output)\n        self.assertIn(\"migrations.RunPython(\\n        code=migrations.RunPython.noop,\\n        reverse_code=migrations.RunPython.noop\\n    ),\", output)\n", "    def test_as_string_with_replaces(self):\n        class TestOperation(migrations.operations.base.Operation):\n                return \"TestOperation\", [], {}\n\n                pass\n\n        migration = migrations.Migration(\n            name=\"0001_initial\",\n            app_label=\"testapp\",\n            replaces=[(\"app1\", \"0001_initial\"), (\"app2\", \"0002_auto\")],\n            dependencies=[(\"app3\", \"0003_auto\")],\n            operations=[TestOperation()],\n        )\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\n            \"replaces = [\\n\"\n            \"        ('app1', '0001_initial'),\\n\"\n            \"        ('app2', '0002_auto'),\\n\"\n            \"    ]\",\n            output,\n        )\n        self.assertIn(\n            \"dependencies = [\\n\"\n            \"        ('app3', '0003_auto'),\\n\"\n            \"    ]\",\n            output,\n        )\n        self.assertIn(\n            \"operations = [\\n\"\n            \"        migrations.TestOperation(),\\n\"\n            \"    ]\",\n            output,\n        )\n", "    def test_migration_writer_basedir(self):\n        \"\"\"Test the basedir property of MigrationWriter\"\"\"\n        with mock.patch('django.db.migrations.writer.import_module') as mock_import_module, \\\n             mock.patch('django.db.migrations.writer.module_dir') as mock_module_dir, \\\n             mock.patch('django.apps.apps.get_app_config') as mock_get_app_config, \\\n             mock.patch('os.makedirs') as mock_makedirs, \\\n             mock.patch('builtins.open', mock.mock_open()):\n            \n            mock_get_app_config.return_value.name = 'test_app'\n            mock_get_app_config.return_value.path = '/path/to/test_app'\n            mock_import_module.side_effect = ImportError\n\n            migration = migrations.Migration(\"0001_initial\", \"test_app\")\n\n            writer = MigrationWriter(migration)\n\n            # Test case when migrations module doesn't exist and is created\n            self.assertEqual(writer.basedir, '/path/to/test_app/migrations')\n\n            # Verify if makedirs and open were called for creating __init__.py\n            mock_makedirs.assert_called_once_with('/path/to/test_app/migrations', exist_ok=True)\n            self.assertTrue(mock_import_module.called)\n            self.assertTrue(mock_module_dir.called)\n", "    def test_migration_writer_basedir(self):\n        \"\"\"\n        Tests the `basedir` property of MigrationWriter to ensure it correctly\n        locates the migration directory.\n        \"\"\"\n        migration = migrations.Migration(\"0001_initial\", \"testapp\")\n        writer = MigrationWriter(migration)\n        \n        # Mocking apps.get_app_config to simulate an app configuration\n        with mock.patch('django.apps.apps.get_app_config') as mock_get_app_config:\n            mock_app_config = mock.Mock()\n            mock_app_config.name = 'testapp'\n            mock_app_config.path = '/path/to/testapp'\n            mock_get_app_config.return_value = mock_app_config\n            \n            # Mocking the import_module to simulate importing a migrations module\n            with mock.patch('importlib.import_module') as mock_import_module:\n                mock_migrations_module = mock.Mock()\n                mock_import_module.return_value = mock_migrations_module\n                \n                # Mocking module_dir to return a specific directory path\n                with mock.patch('django.utils.module_loading.module_dir') as mock_module_dir:\n                    mock_module_dir.return_value = '/path/to/testapp/migrations'\n                    \n                    # Asserting the base directory\n                    self.assertEqual(writer.basedir, '/path/to/testapp/migrations')\n", "    def test_operation_writer_serialize_with_deconstructible_instances(self):\n        operation = custom_migration_operations.operations.ArgsKwargsOperation(\n            DeconstructibleInstances(), kwarg2=DeconstructibleInstances()\n        )\n        buff, imports = OperationWriter(operation, indentation=0).serialize()\n        self.assertEqual(imports, {\"import custom_migration_operations.operations\"})\n        self.assertEqual(\n            buff,\n            \"custom_migration_operations.operations.ArgsKwargsOperation(\\n\"\n            \"    arg1=DeconstructibleInstances,\\n\"\n            \"    kwarg2=DeconstructibleInstances,\\n\"\n            \"),\",\n        )\n"], "sample_493": ["    def test_raw_query_get_columns(self):\n        raw_sql = RawQuery(\"SELECT * FROM aggregation_author\", \"default\")\n        columns = raw_sql.get_columns()\n        self.assertEqual(columns, [\"id\", \"name\", \"age\", \"friends\"])\n", "    def test_raw_query_clone_and_chain(self):\n        raw_sql = \"SELECT * FROM aggregation_author WHERE age > %s\"\n        params = (30,)\n        raw_query = RawQuery(raw_sql, using='default', params=params)\n        raw_query_clone = raw_query.clone(using='default')\n        self.assertEqual(raw_query.sql, raw_query_clone.sql)\n        self.assertEqual(raw_query.params, raw_query_clone.params)\n        self.assertEqual(raw_query.using, raw_query_clone.using)\n        \n        raw_query_chain = raw_query.chain(using='default')\n        self.assertEqual(raw_query.sql, raw_query_chain.sql)\n        self.assertEqual(raw_query.params, raw_query_chain.params)\n        self.assertEqual(raw_query.using, raw_query_chain.using)\n", "    def test_rawquery_get_columns(self):\n        raw_query = RawQuery(\"SELECT id, name FROM aggregation_author\", using=DEFAULT_DB_ALIAS)\n        columns = raw_query.get_columns()\n        self.assertEqual(columns, [\"id\", \"name\"])\n", "    def test_raw_query_clone(self):\n        # Create a RawQuery instance\n        raw_query = RawQuery(\"SELECT * FROM my_table\", \"default\", params={\"param1\": 1})\n        \n        # Clone the RawQuery instance\n        cloned_query = raw_query.clone(\"other_db\")\n\n        # Verify that the cloned RawQuery is different from the original\n        self.assertNotEqual(raw_query, cloned_query)\n\n        # Verify that the SQL statement is the same\n        self.assertEqual(raw_query.sql, cloned_query.sql)\n\n        # Verify that the parameters are the same\n        self.assertEqual(raw_query.params, cloned_query.params)\n\n        # Verify that the 'using' attribute is different\n        self.assertNotEqual(raw_query.using, cloned_query.using)\n        self.assertEqual(cloned_query.using, \"other_db\")\n", "    def test_clone_query(self):\n        q1 = Book.objects.values('publisher_id').annotate(Count('authors'))\n        q2 = q1.clone()\n        self.assertEqual(str(q1.query), str(q2.query))\n        q2 = q2.filter(rating__gte=4)\n        self.assertNotEqual(str(q1.query), str(q2.query))\n"], "sample_494": ["    def test_serialize_custom_class_with_deconstruct(self):\n        class CustomDeconstructibleClass:\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n                return (\n                    \"migrations.test_writer.CustomDeconstructibleClass\",\n                    [self.arg1, self.arg2],\n                    {},\n                )\n\n        instance = CustomDeconstructibleClass(\"value1\", 2)\n        self.assertSerializedEqual(instance)\n        self.assertSerializedResultEqual(\n            instance,\n            (\n                \"migrations.test_writer.CustomDeconstructibleClass('value1', 2)\",\n                {\"import migrations.test_writer\"},\n            ),\n        )\n", "    def test_serialize_ordered_dict(self):\n        \"\"\"\n        Test serialization of OrderedDict.\n        \"\"\"\n        ordered_dict = collections.OrderedDict([('a', 1), ('b', 2), ('c', 3)])\n        string, imports = MigrationWriter.serialize(ordered_dict)\n        self.assertEqual(string, \"collections.OrderedDict([('a', 1), ('b', 2), ('c', 3)])\")\n        self.assertEqual(imports, {\"import collections\"})\n        self.assertSerializedEqual(ordered_dict)\n", "    def test_serialize_regex_flags(self):\n        \"\"\"\n        Ensure that regex flags are properly serialized.\n        \"\"\"\n        regex = re.compile(r\"^\\w+$\", re.IGNORECASE | re.MULTILINE)\n        self.assertSerializedResultEqual(\n            regex,\n            (\n                \"re.compile('^\\\\\\\\w+$', flags=re.IGNORECASE|re.MULTILINE)\",\n                {\"import re\"},\n            ),\n        )\n        self.assertSerializedEqual(regex)\n", "    def test_serialize_frozenset(self):\n        value = frozenset([1, 2, 3])\n        string, imports = MigrationWriter.serialize(value)\n        self.assertEqual(string, \"frozenset([1, 2, 3])\")\n        self.assertEqual(imports, set())\n        self.assertSerializedEqual(frozenset())\n", "    def test_serialize_custom_model_field(self):\n        class CustomField(models.Field):\n                name, path, args, kwargs = super().deconstruct()\n                return name, \"migrations.test_writer.CustomField\", args, kwargs\n\n        field = CustomField()\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(\n            string,\n            \"migrations.test_writer.CustomField()\",\n        )\n        self.assertEqual(imports, {\"import migrations.test_writer\"})\n"], "sample_495": ["    def test_custom_page_class(self):\n        \"\"\"\n        Tests that a custom page class can be used with a Paginator subclass.\n        \"\"\"\n        class CustomPage(Page):\n                return 'custom_method_called'\n        \n        class CustomPaginator(Paginator):\n                return CustomPage(*args, **kwargs)\n        \n        paginator = CustomPaginator([1, 2, 3, 4, 5], 2)\n        page = paginator.page(1)\n        self.assertIsInstance(page, CustomPage)\n        self.assertEqual(page.custom_method(), 'custom_method_called')\n", "    def test_negative_page_number(self):\n        \"\"\"Negative page numbers should raise an EmptyPage exception.\"\"\"\n        paginator = Paginator([1, 2, 3], 2)\n        with self.assertRaises(EmptyPage):\n            paginator.page(-1)\n", "    def test_page_has_next_previous(self):\n        \"\"\"\n        Test the has_next and has_previous methods of Page class.\n        \"\"\"\n        paginator = Paginator([1, 2, 3, 4, 5], 2)\n        first_page = paginator.page(1)\n        self.assertTrue(first_page.has_next())\n        self.assertFalse(first_page.has_previous())\n\n        second_page = paginator.page(2)\n        self.assertTrue(second_page.has_next())\n        self.assertTrue(second_page.has_previous())\n\n        last_page = paginator.page(3)\n        self.assertFalse(last_page.has_next())\n        self.assertTrue(last_page.has_previous())\n", "    def test_page_has_other_pages(self):\n        \"\"\"\n        Page.has_other_pages() returns True if there are previous or next pages,\n        otherwise it returns False.\n        \"\"\"\n        paginator = Paginator([1, 2, 3, 4], 2)\n        first_page = paginator.page(1)\n        second_page = paginator.page(2)\n\n        self.assertTrue(first_page.has_other_pages())\n        self.assertTrue(second_page.has_other_pages())\n\n        paginator = Paginator([1], 1)\n        single_page = paginator.page(1)\n        \n        self.assertFalse(single_page.has_other_pages())\n", "    def test_validate_number(self):\n        paginator = Paginator([1, 2, 3, 4, 5], 2)\n        self.assertEqual(paginator.validate_number(1), 1)\n        self.assertEqual(paginator.validate_number(2), 2)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(0)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(3)\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number(\"not a number\")\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number(1.5)\n"], "sample_496": ["    def setUp(self):\n        self.stdout = StringIO()\n        self.stderr = StringIO()\n        self.stdout_wrapper = OutputWrapper(self.stdout)\n        self.stderr_wrapper = OutputWrapper(self.stderr, color_style().ERROR)\n", "    def test_command_parser_missing_args(self):\n        class TestCommand(BaseCommand):\n            missing_args_message = \"This command requires arguments.\"\n\n                pass\n\n        command = TestCommand()\n        command._called_from_command_line = True\n        parser = command.create_parser(\"prog_name\", \"subcommand\")\n\n        with self.assertRaises(CommandError) as cm:\n            parser.parse_args([])\n        self.assertEqual(str(cm.exception), \"Error: This command requires arguments.\")\n", "    def test_no_migrations(self):\n        \"\"\"\n        Ensure the check_migrations method correctly identifies when there are\n        no migrations to apply.\n        \"\"\"\n        with mock.patch.object(MigrationRecorder, 'has_table', return_value=True), \\\n             mock.patch('django.db.migrations.executor.MigrationExecutor.migration_plan', return_value=[]):\n            out = StringIO()\n            cmd = RunserverCommand(stdout=out)\n            cmd.check_migrations()\n            self.assertNotIn(\"unapplied migration(s)\", out.getvalue())\n            self.assertNotIn(\"apply the migrations for app(s)\", out.getvalue())\n", "    def test_missing_args_message(self):\n        \"\"\"\n        Test that the custom error message is shown when required positional arguments are missing.\n        \"\"\"\n        class MyCommand(BaseCommand):\n            missing_args_message = \"This is a custom missing args message.\"\n\n        parser = CommandParser(MyCommand())\n        parser.add_argument('args', metavar='arg', nargs='*')\n        with self.assertRaises(CommandError) as cm:\n            parser.parse_args([])\n        self.assertEqual(str(cm.exception), \"Error: This is a custom missing args message.\")\n", "    def test_handle_default_options_with_settings(self):\n        \"\"\"handle_default_options sets DJANGO_SETTINGS_MODULE environment variable.\"\"\"\n        options = mock.Mock()\n        options.settings = 'myproject.settings'\n        options.pythonpath = None\n        handle_default_options(options)\n        self.assertEqual(os.environ['DJANGO_SETTINGS_MODULE'], 'myproject.settings')\n"], "sample_497": ["def test_tick_contains():\n    fig, ax = plt.subplots()\n    xtick = XTick(ax, 0, '')\n    ytick = YTick(ax, 0, '')\n\n    # Create a mock event at a position within the axis bounds\n    class MockEvent:\n            self.x = x\n            self.y = y\n\n    # Test contains method for XTick\n    event = MockEvent(0, 0)\n    assert not xtick.contains(event)[0]\n\n    # Test contains method for YTick\n    event = MockEvent(0, 0)\n    assert not ytick.contains(event)[0]\n\n    # Test contains method for XAxis\n    event = MockEvent(0, 0)\n    assert not ax.xaxis.contains(event)[0]\n\n    # Test contains method for YAxis\n    event = MockEvent(0, 0)\n    assert not ax.yaxis.contains(event)[0]\n", "def test_tick_label_rotation():\n    fig, ax = plt.subplots()\n    ax.xaxis.set_tick_params(rotation=45)\n    tick_labels = ax.xaxis.get_ticklabels()\n    for label in tick_labels:\n        assert label.get_rotation() == 45\n\n    ax.yaxis.set_tick_params(rotation=90)\n    tick_labels = ax.yaxis.get_ticklabels()\n    for label in tick_labels:\n        assert label.get_rotation() == 90\n", "def test_tick_apply_tickdir():\n    fig, ax = plt.subplots()\n    ax.xaxis.set_major_locator(mticker.FixedLocator([0.25, 0.5, 0.75]))\n    ax.xaxis.set_minor_locator(mticker.FixedLocator([0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]))\n    ax.xaxis.set_tick_params(direction='inout', length=10, width=2, pad=5)\n    ax.xaxis.set_minor_tick_params(direction='in', length=5, width=1, pad=3)\n    \n    majorticks = ax.xaxis.get_major_ticks()\n    minorticks = ax.xaxis.get_minor_ticks()\n    \n    assert all([tick._tickdir == 'inout' for tick in majorticks])\n    assert all([tick._size == 10 for tick in majorticks])\n    assert all([tick._width == 2 for tick in majorticks])\n    assert all([tick._base_pad == 5 for tick in majorticks])\n    \n    assert all([tick._tickdir == 'in' for tick in minorticks])\n    assert all([tick._size == 5 for tick in minorticks])\n    assert all([tick._width == 1 for tick in minorticks])\n    assert all([tick._base_pad == 3 for tick in minorticks])\n", "def test_tick_initialization():\n    fig, ax = plt.subplots()\n    x_tick = ax.xaxis._get_tick(major=True)\n    y_tick = ax.yaxis._get_tick(major=True)\n    \n    assert isinstance(x_tick, XTick)\n    assert isinstance(y_tick, YTick)\n    \n    # Check initial visibility settings\n    assert x_tick.tick1line.get_visible() == True\n    assert x_tick.tick2line.get_visible() == True\n    assert x_tick.gridline.get_visible() == (rcParams['axes.grid'] and rcParams['axes.grid.which'] in ('both', 'major'))\n    \n    assert y_tick.tick1line.get_visible() == True\n    assert y_tick.tick2line.get_visible() == True\n    assert y_tick.gridline.get_visible() == (rcParams['axes.grid'] and rcParams['axes.grid.which'] in ('both', 'major'))\n    \n    # Check initial label settings\n    assert x_tick.label1.get_visible() == True\n    assert x_tick.label2.get_visible() == False\n    \n    assert y_tick.label1.get_visible() == True\n    assert y_tick.label2.get_visible() == False\n    \n    # Check initial color settings\n    assert x_tick.tick1line.get_color() == rcParams['xtick.color']\n    assert y_tick.tick1line.get_color() == rcParams['ytick.color']\n", "    def setup_method(self):\n        self.fig, self.ax = plt.subplots()\n        self.xtick = XTick(self.ax, 0, '')\n        self.ytick = YTick(self.ax, 0, '')\n"], "sample_498": ["def test_draggable_legend_update_loc():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend(draggable=True, update='loc')\n    assert leg._draggable is not None\n    leg._draggable.finalize_offset()\n    assert leg._update == 'loc'\n    bbox = leg.get_bbox_to_anchor()\n    assert bbox.width > 0 and bbox.height > 0\n", "def test_draggable_legend():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], label='Line')\n    leg = ax.legend(draggable=True)\n    \n    assert isinstance(leg._draggable, mlegend.DraggableLegend)\n    assert leg._draggable._update == 'loc'\n    \n    # Move the legend and finalize to update its location\n    initial_loc = leg._loc\n    leg._draggable.finalize_offset()\n    assert leg._loc != initial_loc\n", "def test_draggable_legend():\n    \"\"\"\n    Test the DraggableLegend class to ensure legends can be dragged and\n    updated correctly.\n    \"\"\"\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], label='draggable legend')\n    legend = ax.legend(draggable=True, loc='upper left')\n\n    # Initially, legend should be in the upper left\n    assert legend.get_loc() == 'upper left'\n\n    # Simulate dragging the legend to a new position\n    draggable = legend.get_draggable()\n    assert draggable is not None\n    initial_loc = draggable.get_loc_in_canvas()\n    \n    # Move the legend to a new location\n    new_loc = (initial_loc[0] + 50, initial_loc[1] + 50)\n    draggable._update_loc(new_loc)\n    draggable.finalize_offset()\n\n    # Check if the location was updated correctly\n    new_bbox = legend.get_bbox_to_anchor().bounds\n    assert new_bbox[0] == initial_loc[0] + 50\n    assert new_bbox[1] == initial_loc[1] + 50\n", "def test_draggable_legend_update_loc():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend(draggable=True)\n    draggable_leg = leg.get_draggable()\n    assert draggable_leg is not None\n\n    # Simulate dragging the legend\n    draggable_leg.start_offset = (0, 0)\n    draggable_leg.update_offset((10, 10))\n    draggable_leg.finalize_offset()\n\n    # Check if the location has been updated\n    new_bbox = leg.get_bbox_to_anchor().transformed(ax.transAxes.inverted())\n    assert new_bbox.x0 != 0\n    assert new_bbox.y0 != 0\n", "def test_legend_set_bbox_to_anchor():\n    # Test set_bbox_to_anchor method\n    fig, ax = plt.subplots()\n    ax.plot([0, 1, 2], [2, 3, 4], label='line')\n    leg = ax.legend(loc='best')\n\n    # Test setting bbox_to_anchor with 2-tuple\n    leg.set_bbox_to_anchor((0.5, 0.5))\n    assert_allclose(leg.get_bbox_to_anchor().bounds, (0.5, 0.5, 0, 0))\n\n    # Test setting bbox_to_anchor with 4-tuple\n    leg.set_bbox_to_anchor((0.1, 0.1, 0.4, 0.4))\n    assert_allclose(leg.get_bbox_to_anchor().bounds, (0.1, 0.1, 0.4, 0.4))\n\n    # Test setting bbox_to_anchor with a Bbox instance\n    bbox = Bbox.from_bounds(0.2, 0.2, 0.3, 0.3)\n    leg.set_bbox_to_anchor(bbox)\n    assert leg.get_bbox_to_anchor().bounds == bbox.bounds\n\n    # Test with a TransformedBbox\n    tbbox = TransformedBbox(bbox, ax.transAxes)\n    leg.set_bbox_to_anchor(tbbox)\n    assert leg.get_bbox_to_anchor().bounds == tbbox.bounds\n\n    # Test resetting bbox_to_anchor to None\n    leg.set_bbox_to_anchor(None)\n    assert leg.get_bbox_to_anchor() == ax.bbox\n"], "sample_499": ["def test_legend_set_title():\n    # Test set_title method of Legend class\n    fig, ax = plt.subplots()\n    line, = ax.plot(range(10), label='Line')\n    leg = ax.legend()\n    \n    # Check default title properties\n    assert leg.get_title().get_text() == \"\"\n    assert not leg.get_title().get_visible()\n\n    # Set a title and check properties\n    leg.set_title(\"Legend Title\")\n    assert leg.get_title().get_text() == \"Legend Title\"\n    assert leg.get_title().get_visible()\n\n    # Change title properties and check\n    fontprop = FontProperties(size=16, family='serif')\n    leg.set_title(\"New Legend Title\", prop=fontprop)\n    assert leg.get_title().get_text() == \"New Legend Title\"\n    assert leg.get_title().get_size() == 16\n    assert leg.get_title().get_family() == ['serif']\n", "def test_legend_fancybox():\n    # Test the fancybox property of the legend\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    y = np.sin(x)\n    ax.plot(x, y, label='sine wave')\n    legend = ax.legend(fancybox=True)\n    assert legend.get_frame().get_boxstyle().get_path() != mpl.patches.BoxStyle(\"square\", pad=0).get_path()\n    legend = ax.legend(fancybox=False)\n    assert legend.get_frame().get_boxstyle().get_path() == mpl.patches.BoxStyle(\"square\", pad=0).get_path()\n", "def test_legend_draggable_update_bbox():\n    # Test the draggable legend with update='bbox' parameter\n    fig, ax = plt.subplots()\n    line, = ax.plot(range(10), label='test')\n    leg = ax.legend(loc='upper right')\n    draggable_legend = leg.set_draggable(True, update='bbox')\n    assert draggable_legend is not None\n    assert isinstance(draggable_legend, mlegend.DraggableLegend)\n    assert draggable_legend._update == 'bbox'\n    \n    # Simulate dragging the legend\n    draggable_legend.start_drag([5, 5])\n    draggable_legend.drag([10, 10])\n    draggable_legend.end_drag()\n    \n    # The bbox_to_anchor should be updated\n    assert leg.get_bbox_to_anchor().bounds == (10, 10, 0, 0)\n", "def test_legend_get_set_loc():\n    fig, ax = plt.subplots()\n    lines = ax.plot(range(10), label='line')\n    leg = ax.legend()\n    \n    # Test default location\n    assert leg._loc == mpl.rcParams['legend.loc']\n    \n    # Test setting and getting location\n    leg.set_loc('upper left')\n    assert leg.get_loc() == 'upper left'\n    \n    leg.set_loc('lower right')\n    assert leg.get_loc() == 'lower right'\n    \n    # Test invalid location\n    with pytest.raises(ValueError):\n        leg.set_loc('invalid loc')\n", "def test_draggable_legend():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend()\n\n    assert not leg.get_draggable()\n\n    draggable = leg.set_draggable(True)\n    assert leg.get_draggable()\n    assert isinstance(draggable, mlegend.DraggableLegend)\n\n    # Simulate dragging the legend\n    mock_event = mock.Mock()\n    mock_event.x, mock_event.y = 100, 100\n    draggable.on_press(mock_event)\n    mock_event.x, mock_event.y = 200, 200\n    draggable.on_motion(mock_event)\n    draggable.on_release(mock_event)\n\n    # Check if the legend location has been updated after dragging\n    new_loc = leg.get_bbox_to_anchor()._bbox\n    assert new_loc.x0 == 200\n    assert new_loc.y0 == 200\n\n    leg.set_draggable(False)\n    assert not leg.get_draggable()\n"], "sample_500": ["def test_colorbar_set_ticks_warning():\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable(norm=mcolors.Normalize(), cmap='viridis')\n    cb = fig.colorbar(sm, ax=ax)\n    \n    with pytest.warns(UserWarning, match=\"Use the colorbar set_ticks\\(\\) method instead.\"):\n        cb.ax.set_xticks([0.1, 0.5, 0.9])\n        cb.ax.set_yticks([0.1, 0.5, 0.9])\n\n    # Verifying that the warnings were raised and set_ticks was not used incorrectly\n    np.testing.assert_allclose(cb.ax.get_xticks(), [])\n    np.testing.assert_allclose(cb.ax.get_yticks(), [])\n", "def test_colorbar_set_ticks_and_labels():\n    fig, ax = plt.subplots()\n    pcm = ax.pcolormesh(np.random.rand(10, 10))\n    cbar = fig.colorbar(pcm)\n    # Set custom ticks and labels\n    ticks = [0.2, 0.4, 0.6, 0.8]\n    labels = ['low', 'medium-low', 'medium-high', 'high']\n    cbar.set_ticks(ticks)\n    cbar.set_ticklabels(labels)\n    np.testing.assert_allclose(cbar.get_ticks(), ticks)\n    assert [tick.get_text() for tick in cbar.ax.get_yticklabels()] == labels\n\n    # Test minor ticks\n    minor_ticks = [0.1, 0.3, 0.5, 0.7, 0.9]\n    cbar.set_ticks(minor_ticks, minor=True)\n    np.testing.assert_allclose(cbar.get_ticks(minor=True), minor_ticks)\n", "def test_colorbar_remove_multiple():\n    fig, ax = plt.subplots()\n    sc = ax.scatter([1, 2], [3, 4], c=[5, 6])\n    cb1 = fig.colorbar(sc)\n    cb2 = fig.colorbar(sc, orientation='horizontal')\n    assert len(ax.get_children()) == 3  # Axes, cb1 and cb2\n\n    cb1.remove()\n    assert len(ax.get_children()) == 2  # Axes and cb2\n    assert cb1.ax not in ax.get_children()\n\n    cb2.remove()\n    assert len(ax.get_children()) == 1  # Only Axes left\n    assert cb2.ax not in ax.get_children()\n", "def test_colorbar_set_alpha():\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    cax = ax.imshow(data)\n    cbar = fig.colorbar(cax)\n\n    # Test setting alpha to a scalar value\n    cbar.set_alpha(0.5)\n    assert cbar.alpha == 0.5\n\n    # Test setting alpha to None\n    cbar.set_alpha(None)\n    assert cbar.alpha is None\n\n    # Test setting alpha to an array\n    alpha_array = np.linspace(0, 1, 10)\n    cbar.set_alpha(alpha_array)\n    assert cbar.alpha is None  # should be None when array is provided\n", "def test_colorbar_set_label():\n    \"\"\"Test the set_label method of the Colorbar class.\"\"\"\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im)\n\n    # Test setting a vertical label\n    cbar.set_label('Vertical Label')\n    assert cbar.ax.get_ylabel() == 'Vertical Label'\n    \n    # Test setting a horizontal label\n    cbar2 = fig.colorbar(im, orientation='horizontal')\n    cbar2.set_label('Horizontal Label')\n    assert cbar2.ax.get_xlabel() == 'Horizontal Label'\n    \n    # Test updating the label\n    cbar.set_label('Updated Vertical Label')\n    assert cbar.ax.get_ylabel() == 'Updated Vertical Label'\n    \n    cbar2.set_label('Updated Horizontal Label')\n    assert cbar2.ax.get_xlabel() == 'Updated Horizontal Label'\n"], "sample_501": ["def test_draggable_legend():\n    fig, ax = plt.subplots()\n    line, = ax.plot(np.arange(10), label='test')\n    leg = ax.legend()\n    draggable = leg.set_draggable(True)\n    assert draggable is not None\n    assert leg.get_draggable() is True\n\n    # Simulate a drag event\n    class Event:\n            self.x = x\n            self.y = y\n\n    initial_pos = leg._legend_box.get_offset(leg.get_window_extent().width, leg.get_window_extent().height, 0, 0, fig.canvas.get_renderer())\n    event_press = Event(initial_pos[0] + 10, initial_pos[1] + 10)\n    draggable.on_press(event_press)\n    event_release = Event(initial_pos[0] + 50, initial_pos[1] + 50)\n    draggable.on_release(event_release)\n\n    new_pos = leg._legend_box.get_offset(leg.get_window_extent().width, leg.get_window_extent().height, 0, 0, fig.canvas.get_renderer())\n    assert new_pos != initial_pos\n\n    # Disable dragging\n    leg.set_draggable(False)\n    assert leg.get_draggable() is False\n", "def test_legend_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='Line')\n    leg = ax.legend()\n\n    # Test setting bbox with a tuple of 4 floats (left, bottom, width, height)\n    leg.set_bbox_to_anchor((0.5, 0.5, 0.1, 0.1))\n    bbox = leg.get_bbox_to_anchor().bounds\n    assert bbox == (0.5, 0.5, 0.1, 0.1)\n\n    # Test setting bbox with a BboxBase instance\n    bbox_instance = mpl.transforms.Bbox.from_bounds(0.2, 0.2, 0.3, 0.3)\n    leg.set_bbox_to_anchor(bbox_instance)\n    bbox = leg.get_bbox_to_anchor().bounds\n    assert bbox == (0.2, 0.2, 0.3, 0.3)\n\n    # Test setting bbox with a tuple of 2 floats (left, bottom)\n    leg.set_bbox_to_anchor((0.1, 0.1))\n    bbox = leg.get_bbox_to_anchor().bounds\n    assert bbox == (0.1, 0.1, 0, 0)\n\n    # Test setting bbox to None to reset to parent bbox\n    leg.set_bbox_to_anchor(None)\n    bbox = leg.get_bbox_to_anchor()\n    assert bbox == leg.parent.bbox\n", "def test_draggable_legend():\n    # Test DraggableLegend functionality\n    fig, ax = plt.subplots()\n    line, = ax.plot(np.arange(10), label='line')\n    leg = ax.legend()\n    draggable_leg = leg.set_draggable(True)\n    assert draggable_leg is not None\n    assert leg.get_draggable() is True\n\n    # Simulate dragging\n    initial_loc = leg._loc\n    draggable_leg._update_loc((0.5, 0.5))\n    new_loc = leg._loc\n    assert new_loc != initial_loc\n\n    # Test disabling dragging\n    leg.set_draggable(False)\n    assert leg.get_draggable() is False\n", "def test_legend_update_default_handler_map():\n    original_handler_map = Legend.get_default_handler_map().copy()\n    custom_handler = legend_handler.HandlerLine2D()\n    Legend.update_default_handler_map({Line2D: custom_handler})\n    updated_handler_map = Legend.get_default_handler_map()\n\n    assert updated_handler_map[Line2D] == custom_handler\n\n    # Restore the original handler map to avoid side effects\n    Legend.set_default_handler_map(original_handler_map)\n    restored_handler_map = Legend.get_default_handler_map()\n\n    assert restored_handler_map == original_handler_map\n", "def test_legend_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 10)\n    y = np.sin(x)\n    line, = ax.plot(x, y, label='sine wave')\n    leg = ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n    \n    # Verify initial bbox_to_anchor\n    assert leg.get_bbox_to_anchor().bounds == (1, 1, 0, 0)\n    \n    # Update bbox_to_anchor and verify\n    leg.set_bbox_to_anchor((0.5, 0.5, 0.2, 0.2), transform=ax.transAxes)\n    assert leg.get_bbox_to_anchor().bounds == (0.5, 0.5, 0.2, 0.2)\n    assert leg.get_bbox_to_anchor()._transform == ax.transAxes\n"], "sample_502": ["def test_switch_backend():\n    initial_backend = mpl.get_backend()\n    try:\n        plt.switch_backend('agg')\n        assert mpl.get_backend() == 'agg'\n        plt.switch_backend('svg')\n        assert mpl.get_backend() == 'svg'\n    finally:\n        plt.switch_backend(initial_backend)\n        assert mpl.get_backend() == initial_backend\n", "def test_switch_backend():\n    # Create a figure to ensure there's something to close\n    plt.figure()\n    initial_backend = mpl.get_backend()\n    \n    # Switch to a non-interactive backend\n    plt.switch_backend('agg')\n    assert mpl.get_backend() == 'agg'\n    \n    # Switch to an interactive backend, if available\n    if 'TkAgg' in rcsetup.interactive_bk:\n        plt.switch_backend('TkAgg')\n        assert mpl.get_backend() == 'TkAgg'\n    \n    # Switch back to the initial backend\n    plt.switch_backend(initial_backend)\n    assert mpl.get_backend() == initial_backend\n", "def test_set_loglevel():\n    # Test setting log level using pyplot wrapper\n    initial_loglevel = mpl.get_logger().level\n    plt.set_loglevel(\"critical\")\n    assert mpl.get_logger().level == logging.CRITICAL\n    plt.set_loglevel(\"debug\")\n    assert mpl.get_logger().level == logging.DEBUG\n    # Restore initial log level\n    plt.set_loglevel(initial_loglevel)\n    assert mpl.get_logger().level == initial_loglevel\n", "def test_switch_backend():\n    initial_backend = mpl.get_backend()\n    try:\n        plt.switch_backend('agg')\n        assert mpl.get_backend() == 'agg'\n        with pytest.raises(ImportError):\n            plt.switch_backend('invalid_backend')\n\n        plt.switch_backend(initial_backend)\n        assert mpl.get_backend() == initial_backend\n    finally:\n        plt.switch_backend(initial_backend)\n", "def test_switch_backend():\n    original_backend = mpl.get_backend()\n    try:\n        plt.switch_backend('agg')\n        assert mpl.get_backend() == 'agg'\n        plt.switch_backend('svg')\n        assert mpl.get_backend() == 'svg'\n    finally:\n        plt.switch_backend(original_backend)\n"], "sample_503": ["def test_marker_fillstyle_combinations():\n    \"\"\"\n    Test combinations of marker and fillstyle for proper rendering.\n    \"\"\"\n    fig, ax = plt.subplots()\n\n    markers = ['o', 's', 'D']\n    fillstyles = ['full', 'left', 'right', 'bottom', 'top', 'none']\n\n    y = np.arange(len(markers) * len(fillstyles))\n    x = np.ones_like(y)\n\n    for i, marker in enumerate(markers):\n        for j, fillstyle in enumerate(fillstyles):\n            idx = i * len(fillstyles) + j\n            ax.plot(x * idx, y[idx], marker=marker, fillstyle=fillstyle, \n                    markersize=15, linestyle='None', color='r',\n                    markeredgecolor='b', markerfacecolor='g')\n\n    fig.canvas.draw()\n    for i, marker in enumerate(markers):\n        for j, fillstyle in enumerate(fillstyles):\n            idx = i * len(fillstyles) + j\n            line = ax.lines[idx]\n            assert line.get_marker() == marker\n            assert line.get_fillstyle() == fillstyle\n            assert line.get_markeredgecolor() == 'b'\n            assert line.get_markerfacecolor() == 'g'\n", "def test_invalid_markevery():\n    line = mlines.Line2D([], [])\n    with pytest.raises(ValueError):\n        line.set_markevery((None, None))\n    with pytest.raises(ValueError):\n        line.set_markevery(('start', 'invalid'))\n    with pytest.raises(ValueError):\n        line.set_markevery(3.14j)\n    with pytest.raises(ValueError):\n        line.set_markevery(object())\n", "def test_set_marker_colors():\n    line = mlines.Line2D([], [])\n    line.set_markeredgecolor('blue')\n    assert line.get_markeredgecolor() == 'blue'\n    line.set_markerfacecolor('red')\n    assert line.get_markerfacecolor() == 'red'\n    line.set_markerfacecoloralt('green')\n    assert line.get_markerfacecoloralt() == 'green'\n", "def test_set_markerfacecolor():\n    line = mlines.Line2D([], [], marker='o')\n    line.set_markerfacecolor('red')\n    assert line.get_markerfacecolor() == 'red'\n    assert line._markerfacecolor == 'red'\n\n    # Test 'auto' functionality\n    line.set_color('blue')\n    line.set_markerfacecolor('auto')\n    assert line.get_markerfacecolor() == 'blue'\n\n    # Test None functionality\n    line.set_markerfacecolor(None)\n    assert line.get_markerfacecolor() == rcParams['lines.markerfacecolor']\n", "def test_markevery_tuple_float():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    line, = ax.plot(x, y, marker='o', markevery=(0.5, 0.1))\n    # Check that the markers are set correctly\n    assert line.get_markevery() == (0.5, 0.1)\n    fig.canvas.draw()\n"], "sample_504": ["def test_set_ticks_with_labels():\n    fig, ax = plt.subplots()\n    data = np.random.randn(10, 10)\n    im = ax.imshow(data, cmap='viridis')\n    cbar = fig.colorbar(im)\n    \n    # Set ticks with labels\n    ticks = [np.min(data), 0, np.max(data)]\n    labels = ['Low', 'Zero', 'High']\n    cbar.set_ticks(ticks)\n    cbar.set_ticklabels(labels)\n    \n    # Check if the ticks and labels are set correctly\n    assert cbar.get_ticks().tolist() == ticks\n    ticklabels = [tick.get_text() for tick in cbar.ax.yaxis.get_ticklabels()]\n    assert ticklabels == labels\n", "def test_colorbar_set_ticks_labels():\n    # Test set_ticks and set_ticklabels methods\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cbar = fig.colorbar(pc)\n\n    # Test set_ticks\n    ticks = [0, 20, 40, 60, 80, 100]\n    cbar.set_ticks(ticks)\n    np.testing.assert_array_equal(cbar.get_ticks(), ticks)\n    \n    # Test set_ticklabels\n    labels = ['a', 'b', 'c', 'd', 'e', 'f']\n    cbar.set_ticklabels(labels)\n    assert [label.get_text() for label in cbar.ax.yaxis.get_ticklabels()] == labels\n\n    # Test set_ticks and set_ticklabels together\n    ticks = [10, 30, 50, 70, 90]\n    labels = ['one', 'two', 'three', 'four', 'five']\n    cbar.set_ticks(ticks)\n    cbar.set_ticklabels(labels)\n    np.testing.assert_array_equal(cbar.get_ticks(), ticks)\n    assert [label.get_text() for label in cbar.ax.yaxis.get_ticklabels()] == labels\n", "def test_colorbar_set_ticks_labels():\n    fig, ax = plt.subplots()\n    data = np.arange(100).reshape(10, 10)\n    cax = ax.imshow(data)\n    cbar = fig.colorbar(cax)\n\n    # Set custom ticks and labels\n    ticks = [10, 30, 50, 70, 90]\n    labels = ['A', 'B', 'C', 'D', 'E']\n    cbar.set_ticks(ticks)\n    cbar.set_ticklabels(labels)\n\n    # Verify that the ticks and labels are set correctly\n    np.testing.assert_array_equal(cbar.get_ticks(), ticks)\n    tick_labels = [tick.get_text() for tick in cbar.ax.get_yticklabels()]\n    assert tick_labels == labels\n\n    # Test setting ticks and labels again with different values\n    new_ticks = [20, 40, 60, 80]\n    new_labels = ['W', 'X', 'Y', 'Z']\n    cbar.set_ticks(new_ticks)\n    cbar.set_ticklabels(new_labels)\n\n    np.testing.assert_array_equal(cbar.get_ticks(), new_ticks)\n    tick_labels = [tick.get_text() for tick in cbar.ax.get_yticklabels()]\n    assert tick_labels == new_labels\n", "def test_colorbar_ticks_with_custom_locator():\n    \"\"\"\n    Test that the colorbar can correctly handle custom locators.\n    \"\"\"\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n\n    contour = ax.contourf(data, levels=levels)\n    cbar = fig.colorbar(contour)\n\n    # Set custom locator with fewer ticks\n    locator = FixedLocator([100, 300, 500, 700, 900, 1100])\n    cbar.set_ticks(locator)\n    fig.canvas.draw()\n\n    # Ensure the custom ticks are applied correctly\n    np.testing.assert_almost_equal(cbar.get_ticks(), [100, 300, 500, 700, 900, 1100])\n\n    # Test with a different set of custom ticks\n    cbar.set_ticks([150, 450, 750, 1050])\n    fig.canvas.draw()\n    np.testing.assert_almost_equal(cbar.get_ticks(), [150, 450, 750, 1050])\n", "def test_colorbar_tick_location():\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    im = ax.imshow(data)\n    \n    # Test colorbar with different tick locations and orientations\n    locations = ['left', 'right', 'top', 'bottom']\n    orientations = ['vertical', 'horizontal']\n    \n    for loc in locations:\n        for orient in orientations:\n            cb = fig.colorbar(im, ax=ax, location=loc, orientation=orient)\n            assert cb.ticklocation == loc\n            assert cb.orientation == orient\n            cb.remove()\n"], "sample_505": ["def test_julian2num():\n    julian_dates = [2457388.5, 2457389.5]  # Julian dates\n    matplotlib_dates = mdates.julian2num(julian_dates)\n    expected_dates = [mdates.date2num(np.datetime64('2016-01-01')),\n                      mdates.date2num(np.datetime64('2016-01-02'))]\n    np.testing.assert_allclose(matplotlib_dates, expected_dates)\n\n", "def test_julian_conversion():\n    # Test conversion between Julian dates and Matplotlib dates\n    julian_dates = [2451545.0, 2451545.5, 2451546.0, 2451546.5, 2451547.0]\n    expected_matplotlib_dates = [\n        mdates.date2num(datetime.datetime(2000, 1, 1)),\n        mdates.date2num(datetime.datetime(2000, 1, 1, 12)),\n        mdates.date2num(datetime.datetime(2000, 1, 2)),\n        mdates.date2num(datetime.datetime(2000, 1, 2, 12)),\n        mdates.date2num(datetime.datetime(2000, 1, 3))\n    ]\n    \n    # Convert Julian to Matplotlib\n    mpl_dates = mdates.julian2num(julian_dates)\n    np.testing.assert_allclose(mpl_dates, expected_matplotlib_dates)\n\n    # Convert back to Julian\n    reconverted_julian_dates = mdates.num2julian(mpl_dates)\n    np.testing.assert_allclose(reconverted_julian_dates, julian_dates)\n", "def test_set_epoch():\n    # Test the set_epoch and get_epoch functions\n    original_epoch = mdates.get_epoch()\n    \n    # Reset epoch and set to a new valid date\n    mdates._reset_epoch_test_example()\n    new_epoch = '1950-01-01'\n    mdates.set_epoch(new_epoch)\n    assert mdates.get_epoch() == new_epoch\n\n    # Try setting epoch again after it's been set\n    with pytest.raises(RuntimeError):\n        mdates.set_epoch('2000-01-01')\n\n    # Reset to original epoch for other tests\n    mdates._reset_epoch_test_example()\n    mdates.set_epoch(original_epoch)\n", "def test_julian_conversion():\n    # Test conversion between Julian dates and Matplotlib dates\n    julian_dates = [2451545.0, 2451545.5, 2451546.0]  # Julian dates for 2000-01-01, 2000-01-01 12:00, 2000-01-02\n    expected_matplotlib_dates = [mdates.date2num(datetime.datetime(2000, 1, 1)),\n                                 mdates.date2num(datetime.datetime(2000, 1, 1, 12)),\n                                 mdates.date2num(datetime.datetime(2000, 1, 2))]\n\n    # Convert Julian to Matplotlib dates\n    mpl_dates = mdates.julian2num(julian_dates)\n    np.testing.assert_allclose(mpl_dates, expected_matplotlib_dates, rtol=1e-6)\n\n    # Convert back from Matplotlib dates to Julian dates\n    round_trip_julian_dates = mdates.num2julian(mpl_dates)\n    np.testing.assert_allclose(round_trip_julian_dates, julian_dates, rtol=1e-6)\n", "def test_julian_conversions():\n    # Test conversion from Julian to Matplotlib dates and vice versa\n    julian_dates = [2415020.5, 2451545.0, 2457388.5]  # Example Julian dates\n    expected_mpl_dates = [mdates.date2num(datetime.datetime(1900, 1, 1)),\n                          mdates.date2num(datetime.datetime(2000, 1, 1)),\n                          mdates.date2num(datetime.datetime(2016, 1, 1))]\n    \n    # Convert Julian to Matplotlib\n    mpl_dates = mdates.julian2num(julian_dates)\n    np.testing.assert_allclose(mpl_dates, expected_mpl_dates)\n    \n    # Convert back from Matplotlib to Julian\n    julian_dates_converted = mdates.num2julian(mpl_dates)\n    np.testing.assert_allclose(julian_dates, julian_dates_converted)\n"], "sample_506": ["def test_spine_set_patch_arc():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    spine.set_patch_arc((0.5, 0.5), 1, 0, 180)\n    assert spine._patch_type == 'arc'\n    assert spine._center == (0.5, 0.5)\n    assert spine._width == 2\n    assert spine._height == 2\n    assert spine._theta1 == 0\n    assert spine._theta2 == 180\n    assert spine.get_transform() == ax.transAxes\n    assert spine.stale\n", "def test_spine_set_position():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n\n    # Test setting position to 'center' shorthand\n    spine.set_position('center')\n    assert spine.get_position() == ('axes', 0.5)\n\n    # Test setting position to 'zero' shorthand\n    spine.set_position('zero')\n    assert spine.get_position() == ('data', 0.0)\n\n    # Test setting position to an outward tuple\n    spine.set_position(('outward', 10))\n    assert spine.get_position() == ('outward', 10)\n\n    # Test setting position to an axes tuple\n    spine.set_position(('axes', 0.25))\n    assert spine.get_position() == ('axes', 0.25)\n\n    # Test setting position to a data tuple\n    spine.set_position(('data', 1.5))\n    assert spine.get_position() == ('data', 1.5)\n\n    # Check for invalid position type\n    with pytest.raises(ValueError, match=\"position[0] should be one of 'outward', 'axes', or 'data'\"):\n        spine.set_position(('invalid', 1.5))\n\n    # Check for invalid position tuple length\n    with pytest.raises(ValueError, match=\"position should be 'center' or 2-tuple\"):\n        spine.set_position(('data', 1.5, 2.0))\n\n    # Test get_spine_transform method for 'outward' position\n    transform = spine.get_spine_transform()\n    assert isinstance(transform, mtransforms.CompositeGenericTransform)\n\n    # Test get_spine_transform method for 'axes' position\n    spine.set_position(('axes', 0.25))\n    transform = spine.get_spine_transform()\n    assert isinstance(transform, mtransforms.CompositeGenericTransform)\n\n    # Test get_spine_transform method for 'data' position\n    spine.set_position(('data', 1.5))\n    transform = spine.get_spine_transform()\n    assert isinstance(transform, mtransforms.BlendedGenericTransform)\n", "def test_spine_set_and_get_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    \n    # Test default bounds\n    assert spine.get_bounds() is None\n    \n    # Set and get bounds\n    spine.set_bounds(0.1, 0.9)\n    assert spine.get_bounds() == (0.1, 0.9)\n    \n    # Test setting only lower bound\n    spine.set_bounds(0.2, None)\n    assert spine.get_bounds() == (0.2, 0.9)\n    \n    # Test setting only upper bound\n    spine.set_bounds(None, 0.8)\n    assert spine.get_bounds() == (0.2, 0.8)\n    \n    # Test clearing bounds\n    spine.set_bounds(None, None)\n    assert spine.get_bounds() == (0.2, 0.8)  # should remain unchanged since None means no change\n\n    # Test exception on circular spine\n    spine_circle = ax.spines['left']\n    spine_circle._patch_type = 'circle'\n    with pytest.raises(ValueError, match='incompatible with circular spines'):\n        spine_circle.set_bounds(0.1, 0.9)\n", "def test_spine_set_patch_types():\n    fig, ax = plt.subplots()\n\n    spine = ax.spines['left']\n    spine.set_patch_arc((0.5, 0.5), 1.0, 0, 180)\n    assert spine._patch_type == 'arc'\n    assert spine._center == (0.5, 0.5)\n    assert spine._width == 2.0\n    assert spine._height == 2.0\n    assert spine._theta1 == 0\n    assert spine._theta2 == 180\n\n    spine.set_patch_circle((0.5, 0.5), 1.0)\n    assert spine._patch_type == 'circle'\n    assert spine._center == (0.5, 0.5)\n    assert spine._width == 2.0\n    assert spine._height == 2.0\n\n    spine.set_patch_line()\n    assert spine._patch_type == 'line'\n", "def test_spine_set_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    spine.set_bounds(-1.0, 1.0)\n    assert spine.get_bounds() == (-1.0, 1.0)\n\n    # Test bounds with None values\n    spine.set_bounds(None, 2.0)\n    assert spine.get_bounds() == (-1.0, 2.0)\n    spine.set_bounds(0.0, None)\n    assert spine.get_bounds() == (0.0, 2.0)\n\n    # Test error when setting bounds for circular spine\n    with pytest.raises(ValueError, match=\"set_bounds() method incompatible with circular spines\"):\n        circular_spine = ax.spines['left'].circular_spine(ax, (0.5, 0.5), 0.5)\n        circular_spine.set_bounds(0.0, 1.0)\n"], "sample_507": ["def test_UnitData_str_is_convertible():\n    \"\"\"\n    Test the _str_is_convertible method to ensure it correctly identifies\n    strings that can be parsed as floats or dates.\n    \"\"\"\n    unit_data = cat.UnitData()\n    \n    # Test strings that should be convertible\n    assert unit_data._str_is_convertible(\"3.14\") == True\n    assert unit_data._str_is_convertible(\"2023-01-01\") == True\n    \n    # Test strings that should not be convertible\n    assert unit_data._str_is_convertible(\"hello\") == False\n    assert unit_data._str_is_convertible(\"world\") == False\n", "    def test_empty_update(self):\n        unit = cat.UnitData(['a', 'b'])\n        assert list(unit._mapping.keys()) == ['a', 'b']\n        assert list(unit._mapping.values()) == [0, 1]\n        \n        unit.update([])\n        assert list(unit._mapping.keys()) == ['a', 'b']\n        assert list(unit._mapping.values()) == [0, 1]\n", "    def test_str_is_convertible(self, value, expected):\n        assert cat.UnitData._str_is_convertible(value) == expected\n", "    def test_empty_string(self):\n        unit = cat.UnitData([\"\"])\n        assert unit._mapping[\"\"] == 0\n", "compilation error"], "sample_508": ["def test_set_transform():\n    # Create an artist and set its transform\n    art = martist.Artist()\n    transform = mtransforms.Affine2D().scale(2.0)\n    \n    # Check initial transform state\n    assert not art.is_transform_set()\n    \n    # Set the transform and verify\n    art.set_transform(transform)\n    assert art.is_transform_set()\n    assert art.get_transform() == transform\n\n    # Ensure changing transform triggers property change\n    art.pchanged = lambda: setattr(art, '_pchanged_called', True)\n    art.stale = True\n    art.set_transform(mtransforms.Affine2D().scale(3.0))\n    assert art._pchanged_called\n    assert art.stale\n", "def test_set_in_layout():\n    art = martist.Artist()\n    \n    # Ensure default value is True\n    assert art.get_in_layout() is True\n    \n    # Set to False and check\n    art.set_in_layout(False)\n    assert art.get_in_layout() is False\n    \n    # Set back to True and check\n    art.set_in_layout(True)\n    assert art.get_in_layout() is True\n", "def test_get_set_methods():\n    artist = martist.Artist()\n\n    # Test setting and getting alpha\n    artist.set_alpha(0.5)\n    assert artist.get_alpha() == 0.5\n\n    # Test setting and getting visibility\n    artist.set_visible(False)\n    assert artist.get_visible() is False\n\n    # Test setting and getting animated\n    artist.set_animated(True)\n    assert artist.get_animated() is True\n\n    # Test setting and getting label\n    artist.set_label(\"Test Label\")\n    assert artist.get_label() == \"Test Label\"\n\n    # Test setting and getting zorder\n    artist.set_zorder(5)\n    assert artist.get_zorder() == 5\n\n    # Test setting and getting snap\n    artist.set_snap(True)\n    assert artist.get_snap() is True\n\n    # Test setting and getting rasterized\n    artist.set_rasterized(True)\n    assert artist.get_rasterized() is True\n\n    # Test setting and getting in_layout\n    artist.set_in_layout(False)\n    assert artist.get_in_layout() is False\n\n    # Test setting and getting url\n    artist.set_url(\"http://example.com\")\n    assert artist.get_url() == \"http://example.com\"\n\n    # Test setting and getting gid\n    artist.set_gid(\"example_gid\")\n    assert artist.get_gid() == \"example_gid\"\n\n    # Test setting and getting transform\n    transform = mtransforms.Affine2D().rotate_deg(45)\n    artist.set_transform(transform)\n    assert artist.get_transform() == transform\n\n    # Test setting and getting clip_on\n    artist.set_clip_on(False)\n    assert artist.get_clip_on() is False\n\n    # Test setting and getting sketch_params\n    artist.set_sketch_params(scale=2, length=5, randomness=1)\n    assert artist.get_sketch_params() == (2, 5, 1)\n\n    # Test setting and getting path_effects\n    path_effects = [mpl.patheffects.SimpleLineShadow(), mpl.patheffects.Normal()]\n    artist.set_path_effects(path_effects)\n    assert artist.get_path_effects() == path_effects\n\n    # Test getting children (should be empty by default)\n    assert artist.get_children() == []\n\n    # Test if the artist is transform set\n    assert artist.is_transform_set() is True\n", "def test_add_remove_callback():\n    \"\"\"Test adding and removing multiple callbacks.\"\"\"\n        callback1.counter += 1\n\n        callback2.counter += 1\n\n    callback1.counter = 0\n    callback2.counter = 0\n\n    art = martist.Artist()\n    oid1 = art.add_callback(callback1)\n    oid2 = art.add_callback(callback2)\n\n    # Trigger callbacks\n    art.pchanged()\n    assert callback1.counter == 1\n    assert callback2.counter == 1\n\n    # Remove first callback and trigger again\n    art.remove_callback(oid1)\n    art.pchanged()\n    assert callback1.counter == 1\n    assert callback2.counter == 2\n\n    # Remove second callback and trigger again\n    art.remove_callback(oid2)\n    art.pchanged()\n    assert callback1.counter == 1\n    assert callback2.counter == 2\n", "def test_draw_methods():\n    class MockRenderer:\n            self._raster_depth = 0\n            self._rasterizing = False\n            self._filter_active = False\n\n            self._rasterizing = True\n\n            self._rasterizing = False\n\n            self._filter_active = True\n\n            self._filter_active = False\n\n    class TestArtist(martist.Artist):\n        @martist.allow_rasterization\n            pass\n\n    renderer = MockRenderer()\n    artist = TestArtist()\n    artist.set_rasterized(True)\n    artist.set_agg_filter(lambda x, y: x)\n\n    artist.draw(renderer)\n    assert renderer._raster_depth == 0\n    assert not renderer._rasterizing\n    assert not renderer._filter_active\n\n    artist.set_rasterized(False)\n    artist.draw(renderer)\n    assert renderer._raster_depth == 0\n    assert not renderer._rasterizing\n    assert not renderer._filter_active\n\n    # Test the finalize rasterization decorator\n    class OuterArtist(martist.Artist):\n        @martist._finalize_rasterization\n            renderer.start_rasterizing()\n\n    outer_artist = OuterArtist()\n    outer_artist.draw(renderer)\n    assert not renderer._rasterizing\n"], "sample_509": ["def test_microsecond_locator():\n    # Test MicrosecondLocator with different intervals\n    start = datetime.datetime(2022, 1, 1, 0, 0, 0)\n    end = datetime.datetime(2022, 1, 1, 0, 0, 1)\n    \n    for interval in [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 500000, 1000000]:\n        locator = mdates.MicrosecondLocator(interval=interval)\n        locator.create_dummy_axis()\n        locator.axis.set_view_interval(mdates.date2num(start), mdates.date2num(end))\n        ticks = locator()\n        assert len(ticks) > 0, f\"Failed for interval: {interval}\"\n\n    # Test invalid interval\n    with pytest.raises(ValueError):\n        mdates.MicrosecondLocator(interval=0)\n    with pytest.raises(ValueError):\n        mdates.MicrosecondLocator(interval=-1)\n    with pytest.raises(ValueError):\n        mdates.MicrosecondLocator(interval=1.5)\n", "def test_date2num_list_of_str_dates():\n    # Test date2num with list of string dates\n    dates_str = ['2022-01-01', '2022-01-02', '2022-01-03']\n    dates_num = mdates.date2num(dates_str)\n    expected_num = [mdates.date2num(datetime.datetime.strptime(date, \"%Y-%m-%d\"))\n                    for date in dates_str]\n    np.testing.assert_array_equal(dates_num, expected_num)\n", "def test_set_epoch_runtime_error():\n    mdates._reset_epoch_test_example()\n    mdates.get_epoch()  # Set default.\n    mdates.date2num(datetime.datetime(2000, 1, 1))  # Trigger epoch usage\n    with pytest.raises(RuntimeError, match=\"set_epoch must be called before dates plotted.\"):\n        mdates.set_epoch('0000-01-01')\n    mdates._reset_epoch_test_example()  # Reset epoch for other tests\n", "def test__get_tzinfo():\n    # Test for retrieving default timezone from rcParams\n    with rc_context({'timezone': 'UTC'}):\n        assert mdates._get_tzinfo() == datetime.timezone.utc\n\n    # Test for passing a timezone string\n    assert mdates._get_tzinfo('UTC') == datetime.timezone.utc\n    assert mdates._get_tzinfo('Europe/Brussels') == dateutil.tz.gettz('Europe/Brussels')\n\n    # Test for passing a tzinfo instance\n    tzinfo = dateutil.tz.gettz('Europe/Brussels')\n    assert mdates._get_tzinfo(tzinfo) == tzinfo\n\n    # Test for invalid timezone string\n    with pytest.raises(ValueError, match=\"Invalid/Timezone is not a valid timezone as parsed by dateutil.tz.gettz.\"):\n        mdates._get_tzinfo('Invalid/Timezone')\n\n    # Test for invalid type\n    with pytest.raises(TypeError, match=\"tz must be string or tzinfo subclass.\"):\n        mdates._get_tzinfo(123)\n", "def test_auto_date_locator_interval_multiples():\n    \"\"\"\n    Test to check the behavior of AutoDateLocator with interval_multiples=True\n    and interval_multiples=False.\n    \"\"\"\n    d1 = datetime.datetime(1997, 1, 1)\n    d2 = d1 + datetime.timedelta(weeks=5)\n\n    locator_true = mdates.AutoDateLocator(interval_multiples=True)\n    locator_true.create_dummy_axis()\n    locator_true.axis.set_view_interval(mdates.date2num(d1), mdates.date2num(d2))\n    ticks_true = mdates.num2date(locator_true())\n\n    locator_false = mdates.AutoDateLocator(interval_multiples=False)\n    locator_false.create_dummy_axis()\n    locator_false.axis.set_view_interval(mdates.date2num(d1), mdates.date2num(d2))\n    ticks_false = mdates.num2date(locator_false())\n\n    assert ticks_true != ticks_false\n    assert all(tick_true.day == 1 for tick_true in ticks_true)\n    assert not all(tick_false.day == 1 for tick_false in ticks_false)\n"], "sample_510": ["def test_get_fignums():\n    fig1 = plt.figure()\n    fig2 = plt.figure()\n    fig3 = plt.figure()\n    fignums = plt.get_fignums()\n    assert len(fignums) == 3\n    assert fig1.number in fignums\n    assert fig2.number in fignums\n    assert fig3.number in fignums\n    plt.close('all')\n", "def test_switch_backend():\n    initial_backend = plt.get_backend()\n    try:\n        plt.switch_backend('agg')\n        assert plt.get_backend() == 'agg'\n        plt.switch_backend('tkagg')\n        assert plt.get_backend() == 'tkagg'\n    except ImportError:\n        pytest.skip(\"tkagg backend not available\")\n    finally:\n        plt.switch_backend(initial_backend)\n", "def test_switch_backend():\n    original_backend = plt.get_backend()\n    try:\n        plt.switch_backend('agg')\n        assert plt.get_backend() == 'agg'\n        \n        with pytest.raises(ImportError, match=\"Cannot load backend\"):\n            plt.switch_backend('nonexistent_backend')\n\n        plt.switch_backend('agg')  # Should succeed without issues\n    finally:\n        plt.switch_backend(original_backend)\n", "def test_set_loglevel():\n    original_level = mpl.get_loglevel()\n    plt.set_loglevel(\"warning\")\n    assert mpl.get_loglevel() == \"warning\"\n    plt.set_loglevel(original_level)\n    assert mpl.get_loglevel() == original_level\n", "def test_switch_backend():\n    current_backend = mpl.get_backend()\n    \n    try:\n        plt.switch_backend('agg')\n        new_backend = mpl.get_backend()\n        assert new_backend == 'agg'\n        \n        plt.switch_backend('svg')\n        new_backend = mpl.get_backend()\n        assert new_backend == 'svg'\n        \n        with pytest.raises(ImportError):\n            plt.switch_backend('non_existent_backend')\n    finally:\n        # Restore the original backend\n        plt.switch_backend(current_backend)\n"], "sample_511": ["def test_show_function():\n    # Ensure that plt.show() calls the backend's show function\n    fig, ax = plt.subplots()\n    with mpl._api.deprecation.suppress_matplotlib_deprecation_warning():\n        backend_show = plt._get_backend_mod().show\n    with pytest.raises(RuntimeError, match=\"Starting a Matplotlib GUI outside of the main thread will likely fail.\"):\n        plt.show()\n    \n    # Check that the signature of plt.show matches the backend show function\n    assert inspect.signature(plt.show) == inspect.signature(backend_show)\n    \n    # Test show with block=True (should raise a RuntimeError due to GUI threading)\n    with pytest.raises(RuntimeError, match=\"Starting a Matplotlib GUI outside of the main thread will likely fail.\"):\n        plt.show(block=True)\n\n    # Ensure that plt.show() can be called without arguments and does not error\n    try:\n        plt.show()\n    except RuntimeError:\n        pass  # Expected in some environments without GUI support\n\n    # Check that the signature of plt.show matches the backend show function\n    assert inspect.signature(plt.show) == inspect.signature(backend_show)\n", "def test_switch_backend():\n    # Initial backend\n    initial_backend = plt.get_backend()\n\n    # Switch to a different backend\n    new_backend = 'agg' if initial_backend != 'agg' else 'tkagg'\n    plt.switch_backend(new_backend)\n    assert plt.get_backend() == new_backend\n\n    # Restore original backend\n    plt.switch_backend(initial_backend)\n    assert plt.get_backend() == initial_backend\n", "def test_pause():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1])\n    start_time = time.time()\n    plt.pause(0.1)\n    end_time = time.time()\n    assert end_time - start_time >= 0.1\n    assert line in ax.get_lines()\n", "def test_switch_backend():\n    import importlib\n    from matplotlib import cbook\n    \n    # Test switching to a valid backend\n    plt.switch_backend('agg')\n    assert mpl.get_backend() == 'agg'\n\n    # Test switching to an invalid backend\n    with pytest.raises(ImportError):\n        plt.switch_backend('invalid_backend')\n\n    # Test automatic backend switching based on interactive framework\n    original_backend = mpl.get_backend()\n    interactive_framework = cbook._get_running_interactive_framework()\n    if interactive_framework:\n        plt.switch_backend(rcsetup._auto_backend_sentinel)\n        assert mpl.get_backend() in {'macosx', 'qtagg', 'gtk4agg', 'gtk3agg', 'tkagg', 'wxagg'}\n        plt.switch_backend(original_backend)\n\n    # Ensure it falls back to 'agg' when no suitable backend is found\n        return 'nonexistent_framework'\n    \n    original_func = cbook._get_running_interactive_framework\n    cbook._get_running_interactive_framework = mock_get_running_interactive_framework\n    plt.switch_backend(rcsetup._auto_backend_sentinel)\n    assert mpl.get_backend() == 'agg'\n    cbook._get_running_interactive_framework = original_func\n\n    # Reset to original backend\n    plt.switch_backend(original_backend)\n", "def test_figure_kwargs():\n    fig1 = plt.figure(figsize=(4, 4), dpi=80, facecolor='r', edgecolor='g', frameon=False)\n    assert fig1.get_size_inches() == (4, 4)\n    assert fig1.dpi == 80\n    assert fig1.get_facecolor() == (1.0, 0.0, 0.0, 1.0)  # red in RGBA\n    assert fig1.get_edgecolor() == (0.0, 0.5, 0.0, 1.0)  # green in RGBA\n    assert not fig1.get_frameon()\n\n    fig2 = plt.figure(num=\"TestFigure\", clear=True)\n    assert fig2.get_label() == \"TestFigure\"\n    assert len(plt.get_fignums()) == 2  # Two figures should be open now\n    assert \"TestFigure\" in plt.get_figlabels()\n\n    plt.close(\"TestFigure\")\n    assert len(plt.get_fignums()) == 1  # Only one figure should remain\n"], "sample_512": ["def test_rc_context():\n    original_rc = plt.rcParams.copy()\n    with plt.rc_context({'lines.linewidth': 2}):\n        assert plt.rcParams['lines.linewidth'] == 2\n    assert plt.rcParams == original_rc\n", "def test_xlim_ylim():\n    # Create a figure and an axis\n    fig, ax = plt.subplots()\n    \n    # Test setting xlim and ylim with tuple\n    plt.xlim((0, 10))\n    plt.ylim((0, 20))\n    assert ax.get_xlim() == (0, 10)\n    assert ax.get_ylim() == (0, 20)\n    \n    # Test setting xlim and ylim with separate arguments\n    plt.xlim(5, 15)\n    plt.ylim(10, 30)\n    assert ax.get_xlim() == (5, 15)\n    assert ax.get_ylim() == (10, 30)\n    \n    # Test getting xlim and ylim\n    assert plt.xlim() == (5, 15)\n    assert plt.ylim() == (10, 30)\n    \n    # Test setting xlim and ylim with kwargs\n    plt.xlim(left=0)\n    plt.ylim(top=25)\n    assert ax.get_xlim() == (0, 15)\n    assert ax.get_ylim() == (10, 25)\n", "def test_switch_backend():\n    initial_backend = mpl.get_backend()\n    try:\n        plt.switch_backend('agg')\n        assert mpl.get_backend() == 'agg'\n        plt.switch_backend('svg')\n        assert mpl.get_backend() == 'svg'\n    finally:\n        plt.switch_backend(initial_backend)\n        assert mpl.get_backend() == initial_backend\n", "def test_subplot_mosaic():\n    # Test basic functionality\n    fig, axs = plt.subplot_mosaic([['A', 'B'], ['C', 'D']])\n    assert len(axs) == 4\n    assert set(axs.keys()) == {'A', 'B', 'C', 'D'}\n\n    # Test with nested list\n    fig, axs = plt.subplot_mosaic([['A', 'B'], ['C', 'D', ['E', 'F']]])\n    assert len(axs) == 6\n    assert set(axs.keys()) == {'A', 'B', 'C', 'D', 'E', 'F'}\n\n    # Test with string input\n    fig, axs = plt.subplot_mosaic('AB;CD')\n    assert len(axs) == 4\n    assert set(axs.keys()) == {'A', 'B', 'C', 'D'}\n\n    # Test with empty sentinel\n    fig, axs = plt.subplot_mosaic([['A', '.', 'B'], ['C', 'D', '.']], empty_sentinel='.')\n    assert len(axs) == 4\n    assert set(axs.keys()) == {'A', 'B', 'C', 'D'}\n\n    plt.close('all')\n", "def test_gcf_creates_new_figure():\n    initial_fignums = plt.get_fignums()\n    fig = plt.gcf()\n    new_fignums = plt.get_fignums()\n    assert fig is not None\n    assert fig in [plt.figure(num) for num in new_fignums]\n    assert len(new_fignums) == len(initial_fignums) + 1\n"], "sample_513": ["def test_legend_title_fontproperties():\n    # test setting title font properties directly\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    font_prop = FontProperties(family='serif', style='italic', weight='bold', size=16)\n    leg = ax.legend(title='Aardvark', title_fontproperties=font_prop)\n    \n    assert leg.get_title().get_fontproperties().get_family() == ['serif']\n    assert leg.get_title().get_fontproperties().get_style() == 'italic'\n    assert leg.get_title().get_fontproperties().get_weight() == 'bold'\n    assert leg.get_title().get_fontproperties().get_size() == 16\n", "def test_legend_update_default_handler_map():\n    \"\"\"Test updating the default handler map.\"\"\"\n    # Define a custom handler map\n    custom_handler_map = {Line2D: legend_handler.HandlerLine2D(numpoints=2)}\n    \n    # Update the default handler map\n    Legend.update_default_handler_map(custom_handler_map)\n    \n    # Verify the update\n    updated_handler_map = Legend.get_default_handler_map()\n    assert isinstance(updated_handler_map[Line2D], legend_handler.HandlerLine2D)\n    assert updated_handler_map[Line2D].numpoints == 2\n    \n    # Reset to the original default handler map for other tests\n    Legend.set_default_handler_map(Legend._default_handler_map)\n", "def test_draggable_legend():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 1, 10)\n    y = np.sin(2 * np.pi * x)\n    ax.plot(x, y, label='sin')\n    legend = ax.legend()\n    \n    draggable = legend.set_draggable(True, use_blit=True, update='loc')\n    \n    assert draggable is not None\n    assert legend.get_draggable() == True\n    \n    draggable.finalize_offset()\n    legend_loc = legend._loc\n    assert legend_loc is not None\n\n    draggable.disconnect()\n    assert legend.get_draggable() == False\n", "def test_draggable_legend_update_loc():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], label='line')\n    leg = ax.legend()\n    draggable_leg = leg.set_draggable(True, update='loc')\n    draggable_leg.finalize_offset()\n    assert leg._update == 'loc'\n    assert draggable_leg.get_loc_in_canvas() == leg._loc\n", "def test_draggable_legend():\n    # Test that a legend can be made draggable and moved\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend()\n    assert not leg.get_draggable()\n    draggable = leg.set_draggable(True)\n    assert leg.get_draggable()\n    # Simulate dragging the legend to a new location\n    init_pos = leg._findoffset(100, 50, 0, 0, fig.canvas.get_renderer())\n    draggable._update_loc((init_pos[0] + 10, init_pos[1] + 10))\n    assert leg._loc != 'best'\n    assert leg._loc != init_pos\n    assert isinstance(leg.get_draggable(), mlegend.DraggableLegend)\n"], "sample_514": ["def test_colorbar_custom_extendfrac():\n    \"\"\"\n    Test custom extendfrac values for colorbars.\n    \"\"\"\n    fig, axs = plt.subplots(3, 1, figsize=(5, 8))\n\n    data = np.linspace(0, 1, 100).reshape((10, 10))\n    cmap = plt.get_cmap(\"viridis\")\n    norm = Normalize(vmin=0, vmax=1)\n\n    extendfracs = [0.1, 'auto', (0.1, 0.2)]\n    for ax, extendfrac in zip(axs, extendfracs):\n        img = ax.imshow(data, cmap=cmap, norm=norm)\n        cb = fig.colorbar(img, ax=ax, extend='both', extendfrac=extendfrac)\n        assert cb.extendfrac == extendfrac\n\n    plt.close(fig)\n", "def test_colorbar_remove_with_multiple_axes():\n    # Test removing colorbar when multiple axes are present\n    fig, axs = plt.subplots(2, 2)\n    sm = cm.ScalarMappable(norm=mcolors.Normalize(), cmap='viridis')\n    cb = fig.colorbar(sm, ax=axs, orientation='vertical')\n    initial_positions = [ax.get_position() for ax in axs.flat]\n    cb.remove()\n    fig.subplots_adjust()\n    final_positions = [ax.get_position() for ax in axs.flat]\n    for initial_pos, final_pos in zip(initial_positions, final_positions):\n        np.testing.assert_allclose(initial_pos.bounds, final_pos.bounds)\n", "def test_colorbar_custom_ticks():\n    \"\"\"\n    Test custom tick locations and labels using set_ticks and set_ticklabels.\n    \"\"\"\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n\n    # Set custom tick locations\n    custom_ticks = [0, 2, 4, 6, 8, 10]\n    cb.set_ticks(custom_ticks)\n\n    # Verify custom tick locations\n    np.testing.assert_almost_equal(cb.get_ticks(), custom_ticks)\n\n    # Set custom tick labels\n    custom_labels = ['zero', 'two', 'four', 'six', 'eight', 'ten']\n    cb.set_ticklabels(custom_labels)\n\n    # Verify custom tick labels\n    tick_labels = [label.get_text() for label in cb.ax.yaxis.get_ticklabels()]\n    assert tick_labels == custom_labels\n", "def test_colorbar_set_ticks():\n    # Test setting major and minor ticks and their labels\n    fig, ax = plt.subplots()\n    pcm = ax.pcolormesh(np.random.randn(10, 10))\n    cbar = fig.colorbar(pcm)\n    \n    major_ticks = [0, 2, 4, 6, 8, 10]\n    major_labels = ['0', 'two', 'four', 'six', 'eight', 'ten']\n    minor_ticks = [1, 3, 5, 7, 9]\n    minor_labels = ['one', 'three', 'five', 'seven', 'nine']\n\n    cbar.set_ticks(major_ticks)\n    cbar.set_ticklabels(major_labels)\n\n    assert cbar.get_ticks() == major_ticks\n    assert [tick.get_text() for tick in cbar.ax.get_yticklabels()] == major_labels\n\n    cbar.set_ticks(minor_ticks, minor=True)\n    cbar.set_ticklabels(minor_labels, minor=True)\n\n    assert cbar.get_ticks(minor=True) == minor_ticks\n    assert [tick.get_text() for tick in cbar.ax.get_yticklabels(minor=True)] == minor_labels\n", "def test_colorbar_set_alpha():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.set_alpha(0.5)\n    assert cb.alpha == 0.5\n\n    # Set alpha to a numpy array and ensure colorbar alpha is None\n    alpha_array = np.linspace(0, 1, 100).reshape(10, 10)\n    pc.set_alpha(alpha_array)\n    cb.set_alpha(alpha_array)\n    assert cb.alpha is None\n    assert np.all(pc.get_alpha() == alpha_array)\n    fig.draw_without_rendering()\n"], "sample_515": ["def test_colorbar_set_ticks_with_minor():\n    # Test the set_ticks method with minor ticks enabled.\n    fig, ax = plt.subplots()\n    pcm = ax.pcolormesh(np.random.randn(10, 10))\n    cbar = fig.colorbar(pcm)\n    \n    # Set major ticks\n    cbar.set_ticks([0, 2, 4, 6, 8])\n    np.testing.assert_allclose(cbar.get_ticks(), [0, 2, 4, 6, 8])\n    \n    # Set minor ticks\n    cbar.set_ticks([1, 3, 5, 7, 9], minor=True)\n    np.testing.assert_allclose(cbar.get_ticks(minor=True), [1, 3, 5, 7, 9])\n\n    # Ensure major and minor ticks are different\n    assert not np.array_equal(cbar.get_ticks(), cbar.get_ticks(minor=True))\n", "def test_colorbar_axes_locator():\n    \"\"\"\n    Test that _ColorbarAxesLocator correctly adjusts the position of the colorbar axes\n    when there are triangular or rectangular extensions.\n    \"\"\"\n    fig, ax = plt.subplots()\n    data = np.linspace(0, 1, 100).reshape(10, 10)\n    im = ax.imshow(data, cmap='viridis')\n    cbar = fig.colorbar(im, extend='both')\n    locator = _ColorbarAxesLocator(cbar)\n    # Mock a renderer\n    renderer = fig.canvas.get_renderer()\n    # Get the position before adjustment\n    original_pos = ax.get_position()\n    # Adjust position using the locator\n    adjusted_pos = locator(ax, renderer)\n    # Check that the position has been adjusted correctly\n    assert adjusted_pos.width < original_pos.width or adjusted_pos.height < original_pos.height\n", "def test_colorbar_orientation():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh([[0, 1], [2, 3]])\n    # Test vertical orientation\n    cbar = fig.colorbar(pc, ax=ax, orientation='vertical')\n    assert cbar.orientation == 'vertical'\n    # Test horizontal orientation\n    cbar = fig.colorbar(pc, ax=ax, orientation='horizontal')\n    assert cbar.orientation == 'horizontal'\n", "def test_colorbar_spine():\n    \"\"\"Test the visibility and window extent of the custom spine in Colorbar.\"\"\"\n    fig, ax = plt.subplots()\n    data = np.arange(100).reshape(10, 10)\n    im = ax.imshow(data)\n    cbar = fig.colorbar(im)\n\n    # Check that the spine is not visible\n    assert not cbar.ax.spines['outline'].get_visible()\n\n    # Ensure that the custom _ColorbarSpine is used\n    assert isinstance(cbar.outline, _ColorbarSpine)\n\n    # Check the window extent method\n    renderer = fig.canvas.get_renderer()\n    extent = cbar.outline.get_window_extent(renderer)\n    assert extent.width > 0\n    assert extent.height > 0\n\n    # Check that the spine is drawn correctly\n    cbar.outline.set_xy([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n    assert cbar.outline.stale\n    cbar.ax.figure.canvas.draw()\n    assert not cbar.outline.stale\n", "def test_colorbar_remove_twice():\n    # Test that calling remove() twice on a colorbar doesn't raise an error.\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im)\n    \n    # First removal\n    cb.remove()\n    \n    # Second removal, should not raise an error\n    cb.remove()\n"], "sample_516": ["def test_pdf_repr():\n    from matplotlib.backends.backend_pdf import pdfRepr, Name\n    assert pdfRepr(123) == b\"123\"\n    assert pdfRepr(123.456) == b\"123.456\"\n    assert pdfRepr(True) == b\"true\"\n    assert pdfRepr(False) == b\"false\"\n    assert pdfRepr(\"hello\") == b\"(hello)\"\n    assert pdfRepr(\"hello world\") == b\"(hello world)\"\n    assert pdfRepr(b\"hello\") == b\"(hello)\"\n    assert pdfRepr(b\"hello (world)\") == b\"(hello \\\\(world\\\\))\"\n    assert pdfRepr({\"key\": \"value\"}) == b\"<< /key (value) >>\"\n    assert pdfRepr([1, 2, 3]) == b\"[1 2 3]\"\n    assert pdfRepr(None) == b\"null\"\n    assert pdfRepr(datetime.datetime(2023, 1, 1, 12, 0, 0)) == b\"(D:20230101120000)\"\n    assert pdfRepr(Name(\"TestName\")) == b\"/TestName\"\n", "def test_pdf_repr():\n    assert pdfRepr(42) == b\"42\"\n    assert pdfRepr(3.14) == b\"3.14\"\n    assert pdfRepr(True) == b\"true\"\n    assert pdfRepr(False) == b\"false\"\n    assert pdfRepr(\"hello\") == b\"(hello)\"\n    assert pdfRepr(b\"hello\") == b\"(hello)\"\n    assert pdfRepr(None) == b\"null\"\n    assert pdfRepr(datetime.datetime(2023, 1, 1, 0, 0, 0)) == b\"D:20230101000000Z\"\n    assert pdfRepr(BboxBase.from_bounds(0, 0, 1, 1)) == b\"0.000000 0.000000 1.000000 1.000000\"\n    assert pdfRepr([1, 2, 3]) == b\"[1 2 3]\"\n    assert pdfRepr({\"key\": \"value\"}) == b\"<< /key (value) >>\"\n    \n    class CustomObject:\n            return b\"custom\"\n    \n    assert pdfRepr(CustomObject()) == b\"custom\"\n", "def test_pdf_repr():\n    from matplotlib.backends.backend_pdf import pdfRepr\n\n    assert pdfRepr(123) == b\"123\"\n    assert pdfRepr(123.456) == b\"123.456\"\n    assert pdfRepr(True) == b\"true\"\n    assert pdfRepr(False) == b\"false\"\n    assert pdfRepr(None) == b\"null\"\n    assert pdfRepr(\"hello\") == b\"(hello)\"\n    assert pdfRepr(b\"hello\") == b\"(hello)\"\n    assert pdfRepr({\"key\": \"value\"}) == b\"<< /key (value) >>\"\n    assert pdfRepr([1, 2, 3]) == b\"[ 1 2 3 ]\"\n", "def test_create_pdf_info_dict():\n    metadata = {\n        'Title': 'Test PDF',\n        'Author': 'John Doe',\n        'Subject': 'Testing',\n        'Keywords': 'test, pdf, matplotlib',\n        'Creator': 'Test Creator',\n        'Producer': 'Test Producer',\n        'CreationDate': datetime.datetime(2023, 10, 8, 12, 0, 0, tzinfo=datetime.timezone.utc),\n        'ModDate': datetime.datetime(2023, 10, 8, 12, 0, 0, tzinfo=datetime.timezone.utc),\n        'Trapped': 'Unknown'\n    }\n    info_dict = _create_pdf_info_dict('pdf', metadata)\n    assert info_dict['Title'] == 'Test PDF'\n    assert info_dict['Author'] == 'John Doe'\n    assert info_dict['Subject'] == 'Testing'\n    assert info_dict['Keywords'] == 'test, pdf, matplotlib'\n    assert info_dict['Creator'] == 'Test Creator'\n    assert info_dict['Producer'] == 'Test Producer'\n    assert info_dict['CreationDate'] == datetime.datetime(2023, 10, 8, 12, 0, 0, tzinfo=datetime.timezone.utc)\n    assert info_dict['ModDate'] == datetime.datetime(2023, 10, 8, 12, 0, 0, tzinfo=datetime.timezone.utc)\n    assert info_dict['Trapped'] == Name(b'Unknown')\n", "def test_pdf_string_escape():\n    input_string = b\"String with special characters: \\\\ ( ) \\r \\n\"\n    expected_output = b\"String with special characters: \\\\\\\\ \\\\( \\\\) \\\\r \\\\n\"\n    escaped_string = _string_escape_regex.sub(_string_escape, input_string)\n    assert escaped_string == expected_output\n"], "sample_517": ["def test_invalid_bbox_properties():\n    fig, ax = plt.subplots()\n    txt = ax.text(0.5, 0.5, 'test')\n    with pytest.raises(TypeError, match=\"bbox properties should be a dictionary\"):\n        txt.set_bbox([\"invalid\", \"bbox\", \"properties\"])\n", "def test_text_background_color():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Test background color\", color=\"white\", fontsize=20, bbox=dict(facecolor='red', alpha=0.5))\n    fig.canvas.draw()\n\n    # Verify the text background color\n    assert text.get_bbox_patch().get_facecolor() == (1.0, 0.0, 0.0, 0.5)  # RGBA for red with 50% transparency\n", "def test_get_rotation_mode():\n    text = Text(0, 0, 'Test', rotation_mode='anchor')\n    assert text.get_rotation_mode() == 'anchor'\n    text.set_rotation_mode('default')\n    assert text.get_rotation_mode() == 'default'\n    text.set_rotation_mode(None)\n    assert text.get_rotation_mode() is None\n", "def test_text_set_get_properties():\n    text = Text(x=0.5, y=0.5, text='test')\n\n    # Test setting and getting color\n    text.set_color('red')\n    assert text.get_color() == 'red'\n\n    # Test setting and getting font properties\n    font_properties = FontProperties(family='serif', style='italic', weight='bold', size=20)\n    text.set_fontproperties(font_properties)\n    assert text.get_fontfamily() == ['serif']\n    assert text.get_fontstyle() == 'italic'\n    assert text.get_fontweight() == 'bold'\n    assert text.get_fontsize() == 20\n\n    # Test setting and getting horizontal alignment\n    text.set_horizontalalignment('center')\n    assert text.get_horizontalalignment() == 'center'\n\n    # Test setting and getting vertical alignment\n    text.set_verticalalignment('top')\n    assert text.get_verticalalignment() == 'top'\n\n    # Test setting and getting text\n    text.set_text('new text')\n    assert text.get_text() == 'new text'\n\n    # Test setting and getting position\n    text.set_position((0.2, 0.3))\n    assert text.get_position() == (0.2, 0.3)\n\n    # Test setting and getting rotation\n    text.set_rotation(45)\n    assert text.get_rotation() == 45.0\n\n    # Test setting and getting wrap\n    text.set_wrap(True)\n    assert text.get_wrap() is True\n\n    # Test setting and getting bbox\n    bbox_props = dict(boxstyle=\"round,pad=0.3\", edgecolor='black', facecolor='yellow')\n    text.set_bbox(bbox_props)\n    assert text.get_bbox_patch().get_edgecolor() == mpl.colors.to_rgba('black')\n    assert text.get_bbox_patch().get_facecolor() == mpl.colors.to_rgba('yellow')\n", "def test_wrap_edge_case():\n    fig = plt.figure(figsize=(6, 4))\n    s = 'Thisisaverylongwordthatshouldnotbebroken'\n    text = fig.text(0.7, 0.5, s, wrap=True)\n    fig.canvas.draw()\n    # In case of a very long word that cannot be broken, it should return the word itself.\n    assert text._get_wrapped_text() == s\n"], "sample_518": ["def test_patch_setters():\n    patch = Patch()\n    \n    # Test edgecolor setters and getters\n    patch.set_edgecolor('red')\n    assert patch.get_edgecolor() == mcolors.to_rgba('red')\n    \n    patch.set_edgecolor(None)\n    assert patch.get_edgecolor() == mcolors.to_rgba(rcParams['patch.edgecolor'])\n    \n    # Test facecolor setters and getters\n    patch.set_facecolor('blue')\n    assert patch.get_facecolor() == mcolors.to_rgba('blue')\n    \n    patch.set_facecolor(None)\n    assert patch.get_facecolor() == mcolors.to_rgba(rcParams['patch.facecolor'])\n    \n    # Test linewidth setters and getters\n    patch.set_linewidth(2.0)\n    assert patch.get_linewidth() == 2.0\n    \n    patch.set_linewidth(None)\n    assert patch.get_linewidth() == rcParams['patch.linewidth']\n    \n    # Test linestyle setters and getters\n    patch.set_linestyle('--')\n    assert patch.get_linestyle() == '--'\n    \n    patch.set_linestyle(None)\n    assert patch.get_linestyle() == 'solid'\n    \n    # Test antialiased setters and getters\n    patch.set_antialiased(True)\n    assert patch.get_antialiased() is True\n    \n    patch.set_antialiased(None)\n    assert patch.get_antialiased() == rcParams['patch.antialiased']\n    \n    # Test hatch setters and getters\n    patch.set_hatch('/')\n    assert patch.get_hatch() == '/'\n    \n    patch.set_hatch(None)\n    assert patch.get_hatch() is None\n    \n    # Test fill setters and getters\n    patch.set_fill(False)\n    assert patch.get_fill() is False\n    \n    patch.set_fill(True)\n    assert patch.get_fill() is True\n    \n    # Test capstyle setters and getters\n    patch.set_capstyle('round')\n    assert patch.get_capstyle() == 'round'\n    \n    patch.set_capstyle('butt')\n    assert patch.get_capstyle() == 'butt'\n    \n    # Test joinstyle setters and getters\n    patch.set_joinstyle('bevel')\n    assert patch.get_joinstyle() == 'bevel'\n    \n    patch.set_joinstyle('miter')\n    assert patch.get_joinstyle() == 'miter'\n", "def test_fancyarrow_setdata_units():\n    from datetime import datetime\n    # Test set_data method with datetime objects to ensure compatibility with units\n    dtime = datetime(2000, 1, 1)\n    fig, ax = plt.subplots()\n    arrow = FancyArrowPatch((0, dtime), (0.01, dtime))\n    arrow.set_data(x=1, y=dtime, dx=15, dy=dtime, width=2, head_width=2, head_length=1)\n    expected = np.array(\n        [[16.71, 16.71],\n         [16.71, 15.29],\n         [16.71, 15.29],\n         [1.71, 0.29],\n         [0.29, 1.71],\n         [15.29, 16.71],\n         [15.29, 16.71],\n         [16.71, 16.71]]\n    )\n    assert np.allclose(expected, np.round(arrow.verts, 2))\n", "def test_patch_contains():\n    # Test the contains method of the Patch class.\n    rect = Rectangle((0.0, 0.0), 1.0, 1.0)\n    assert rect.contains_point((0.5, 0.5))\n    assert not rect.contains_point((1.5, 0.5))\n    \n    circle = mpatches.Circle((0.0, 0.0), 1.0)\n    assert circle.contains_point((0.5, 0.5))\n    assert not circle.contains_point((1.5, 0.5))\n    \n    ellipse = Ellipse((0.0, 0.0), 1.0, 2.0)\n    assert ellipse.contains_point((0.5, 0.5))\n    assert not ellipse.contains_point((1.5, 0.5))\n    \n    polygon = Polygon([[0.0, 0.0], [1.0, 0.0], [0.5, 1.0]])\n    assert polygon.contains_point((0.5, 0.5))\n    assert not polygon.contains_point((1.5, 0.5))\n    \n    wedge = mpatches.Wedge((0.0, 0.0), 1.0, 0, 90)\n    assert wedge.contains_point((0.5, 0.5))\n    assert not wedge.contains_point((1.5, 0.5))\n", "def test_patch_transform():\n    \"\"\"\n    Test to ensure that the correct transform is returned for a patch.\n    \"\"\"\n    rect = Rectangle((0, 0), 2, 3, angle=45)\n    identity_transform = transforms.IdentityTransform()\n    patch_transform = rect.get_patch_transform()\n\n    # Check that the returned transform includes the rotation\n    expected_transform = transforms.BboxTransformTo(rect.get_bbox()) \\\n                         + transforms.Affine2D().translate(-1, -1.5) \\\n                         .rotate_deg(45) \\\n                         .translate(1, 1.5)\n\n    transformed_corners = patch_transform.transform(rect.get_path().vertices)\n    expected_transformed_corners = expected_transform.transform(rect.get_path().vertices)\n\n    assert_array_equal(transformed_corners, expected_transformed_corners)\n", "def test_fancyarrow_rotation():\n    # Test FancyArrow rotation\n    fig, ax = plt.subplots()\n    arrow = FancyArrow(0, 0, 1, 0, width=0.1, head_width=0.2, head_length=0.2)\n    ax.add_patch(arrow)\n    # Rotate 90 degrees\n    arrow.set_data(x=0, y=0, dx=0, dy=1, width=0.1, head_width=0.2, head_length=0.2)\n    expected_verts = np.array([[0.2, 1.  ],\n                               [0.1, 0.9 ],\n                               [0.  , 0.9 ],\n                               [0.  , 0.  ],\n                               [0.1, 0.1 ],\n                               [0.2, 0.  ],\n                               [0.1, 0.1 ],\n                               [0.  , 1.  ]])\n    assert np.allclose(np.round(arrow.verts, 2), expected_verts)\n"], "sample_519": ["def test_subplotparams_update():\n    fig, ax = plt.subplots()\n    sp = fig.subplotpars\n\n    # Test valid updates\n    sp.update(left=0.2, right=0.8, bottom=0.1, top=0.9, wspace=0.2, hspace=0.3)\n    assert sp.left == 0.2\n    assert sp.right == 0.8\n    assert sp.bottom == 0.1\n    assert sp.top == 0.9\n    assert sp.wspace == 0.2\n    assert sp.hspace == 0.3\n\n    # Test invalid updates\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        sp.update(left=0.9, right=0.8)\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        sp.update(bottom=0.9, top=0.8)\n\n    # Ensure old values are kept\n    sp.update(left=None, right=None, bottom=None, top=None, wspace=None, hspace=None)\n    assert sp.left == 0.2\n    assert sp.right == 0.8\n    assert sp.bottom == 0.1\n    assert sp.top == 0.9\n    assert sp.wspace == 0.2\n    assert sp.hspace == 0.3\n", "def test_subplotparams_update():\n    # Test the SubplotParams update method for validation\n    fig = plt.figure()\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        fig.subplotpars.update(left=0.9, right=0.1)\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        fig.subplotpars.update(bottom=0.9, top=0.1)\n    \n    # Test valid updates\n    fig.subplotpars.update(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.5, hspace=0.5)\n    assert fig.subplotpars.left == 0.1\n    assert fig.subplotpars.bottom == 0.1\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.top == 0.9\n    assert fig.subplotpars.wspace == 0.5\n    assert fig.subplotpars.hspace == 0.5\n\n    # Test partial updates\n    fig.subplotpars.update(wspace=0.2)\n    assert fig.subplotpars.left == 0.1  # unchanged\n    assert fig.subplotpars.wspace == 0.2  # updated\n", "def test_subplot_params_update():\n    # Test SubplotParams.update method and validation logic.\n    subplot_params = SubplotParams()\n    \n    # Updating with valid parameters\n    subplot_params.update(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n    assert subplot_params.left == 0.1\n    assert subplot_params.bottom == 0.1\n    assert subplot_params.right == 0.9\n    assert subplot_params.top == 0.9\n    assert subplot_params.wspace == 0.2\n    assert subplot_params.hspace == 0.2\n\n    # Updating with invalid parameters\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        subplot_params.update(left=1.0, right=0.5)\n    \n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        subplot_params.update(bottom=1.0, top=0.5)\n\n    # Ensure that valid parameters are still intact after invalid updates\n    assert subplot_params.left == 0.1\n    assert subplot_params.right == 0.9\n    assert subplot_params.bottom == 0.1\n    assert subplot_params.top == 0.9\n    assert subplot_params.wspace == 0.2\n    assert subplot_params.hspace == 0.2\n", "def test_add_subfigure_with_invalid_subplot_spec():\n    fig = plt.figure()\n    with pytest.raises(TypeError, match=\"SubplotSpec instance required\"):\n        fig.add_subfigure(subplotspec=None)\n    with pytest.raises(ValueError, match=\"SubplotSpec does not belong to this figure\"):\n        gs = gridspec.GridSpec(2, 2)\n        fig2 = plt.figure()\n        fig2.add_subfigure(gs[0])\n", "def test_add_subfigure():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n\n    subfig1 = fig.add_subfigure(gs[0, 0])\n    subfig2 = fig.add_subfigure(gs[0, 1])\n    subfig3 = fig.add_subfigure(gs[1, :])\n\n    assert subfig1 in fig.subfigs\n    assert subfig2 in fig.subfigs\n    assert subfig3 in fig.subfigs\n\n    ax1 = subfig1.add_subplot(111)\n    ax2 = subfig2.add_subplot(111)\n    ax3 = subfig3.add_subplot(111)\n\n    assert ax1 in fig.axes\n    assert ax2 in fig.axes\n    assert ax3 in fig.axes\n\n    assert subfig1.get_axes() == [ax1]\n    assert subfig2.get_axes() == [ax2]\n    assert subfig3.get_axes() == [ax3]\n\n    fig.clear()\n    assert fig.subfigs == []\n    assert fig.axes == []\n"], "sample_520": ["def test_line3d_init():\n    # Test initialization of Line3D with various inputs\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    \n    # Basic initialization\n    line = art3d.Line3D([0, 1], [0, 1], [0, 1])\n    ax.add_line(line)\n    np.testing.assert_array_equal(line.get_data_3d(), ([0, 1], [0, 1], [0, 1]))\n\n    # Initialization with different zdir\n    line = art3d.Line3D([0, 1], [0, 1], [0, 1], zdir='y')\n    ax.add_line(line)\n    np.testing.assert_array_equal(line.get_data_3d(), ([0, 1], [0, 1], [0, 1]))\n\n    # Initialization with different lengths of data\n    line = art3d.Line3D([0, 1, 2], [0, 1, 2], [0, 1, 2])\n    ax.add_line(line)\n    np.testing.assert_array_equal(line.get_data_3d(), ([0, 1, 2], [0, 1, 2], [0, 1, 2]))\n\n    # Initialization with single-point data\n    line = art3d.Line3D([1], [1], [1])\n    ax.add_line(line)\n    np.testing.assert_array_equal(line.get_data_3d(), ([1], [1], [1]))\n", "def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array((1, 0, 0)))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array((0, 1, 0)))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array((0, 0, 1)))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array((0, 0, 0)))\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array((1, 2, 3)))\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector('invalid')\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector((1, 2))\n", "def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array([1, 2, 3]))\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector('a')\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector([1, 2])\n", "def test_text3d_set_3d_properties(fig_test, fig_ref):\n    # Setting 3D properties after creation should work correctly\n    ax1 = fig_test.add_subplot(projection='3d')\n    t = art3d.Text3D(0, 0, 0, 'Initial text')\n    t.set_3d_properties(z=5, zdir='y')\n    ax1.add_artist(t)\n    assert t.get_position_3d() == (0, 0, 5)\n    np.testing.assert_array_equal(t._dir_vec, get_dir_vector('y'))\n\n    ax2 = fig_ref.add_subplot(projection='3d')\n    t2 = art3d.Text3D(0, 0, 5, 'Initial text', zdir='y')\n    ax2.add_artist(t2)\n    assert t2.get_position_3d() == (0, 0, 5)\n    np.testing.assert_array_equal(t2._dir_vec, get_dir_vector('y'))\n", "def test_line3d_set_data_3d_single_argument():\n    x, y, z = [0, 1, 2], [3, 4, 5], [6, 7, 8]\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    line = art3d.Line3D(x, y, z)\n    np.testing.assert_array_equal(line.get_data_3d(), (x, y, z))\n\n    new_data = ([9, 10, 11], [12, 13, 14], [15, 16, 17])\n    line.set_data_3d(new_data)\n    np.testing.assert_array_equal(line.get_data_3d(), new_data)\n"], "sample_521": ["def test_get_dir_vector():\n    # Test known direction vectors\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n\n    # Test custom direction vector\n    custom_vector = (1, 2, 3)\n    assert np.array_equal(art3d.get_dir_vector(custom_vector), np.array(custom_vector))\n\n    # Test invalid direction vector\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector('invalid')\n", "def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array([1, 2, 3]))\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector('invalid')\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector((1, 2))\n", "def test_get_dir_vector():\n    # Test valid inputs\n    np.testing.assert_array_equal(art3d.get_dir_vector('x'), [1, 0, 0])\n    np.testing.assert_array_equal(art3d.get_dir_vector('y'), [0, 1, 0])\n    np.testing.assert_array_equal(art3d.get_dir_vector('z'), [0, 0, 1])\n    np.testing.assert_array_equal(art3d.get_dir_vector(None), [0, 0, 0])\n    np.testing.assert_array_equal(art3d.get_dir_vector((1, 2, 3)), [1, 2, 3])\n\n    # Test invalid input\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector('invalid')\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector((1, 2))\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector((1, 2, 3, 4))\n", "def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector([1, 2, 3]), np.array([1, 2, 3]))\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector('invalid')\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector([1, 2])\n", "def test_line3d_properties():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection=\"3d\")\n\n    # Create a Line3D object\n    xs, ys, zs = [0, 1], [1, 2], [2, 3]\n    line = art3d.Line3D(xs, ys, zs)\n    ax.add_line(line)\n\n    # Test initial properties\n    np.testing.assert_array_equal(line.get_data_3d(), (xs, ys, zs))\n    assert line.get_zorder() == 2\n    assert line.get_linestyle() == '-'\n\n    # Modify properties\n    line.set_3d_properties(zs=[3, 4], zdir='z')\n    np.testing.assert_array_equal(line.get_data_3d(), (xs, ys, [3, 4]))\n    line.set_linestyle('--')\n    line.set_zorder(5)\n    \n    # Verify modified properties\n    assert line.get_linestyle() == '--'\n    assert line.get_zorder() == 5\n\n    # Test if draw method works without errors\n    fig.canvas.draw()\n"], "sample_522": ["def test_subplotparams_update(left, bottom, right, top, wspace, hspace, expected_exception):\n    subplot_params = SubplotParams()\n    if expected_exception:\n        with pytest.raises(expected_exception):\n            subplot_params.update(left, bottom, right, top, wspace, hspace)\n    else:\n        subplot_params.update(left, bottom, right, top, wspace, hspace)\n        if left is not None:\n            assert subplot_params.left == left\n        if bottom is not None:\n            assert subplot_params.bottom == bottom\n        if right is not None:\n            assert subplot_params.right == right\n        if top is not None:\n            assert subplot_params.top == top\n        if wspace is not None:\n            assert subplot_params.wspace == wspace\n        if hspace is not None:\n            assert subplot_params.hspace == hspace\n", "def test_figure_suptitle():\n    \"\"\"Test the suptitle method of the Figure class.\"\"\"\n    fig = plt.figure()\n    title_text = \"Test Suptitle\"\n    fig.suptitle(title_text)\n    assert fig._suptitle.get_text() == title_text\n    assert fig._suptitle.get_position() == (0.5, 0.98)\n    assert fig._suptitle.get_horizontalalignment() == 'center'\n    assert fig._suptitle.get_verticalalignment() == 'top'\n    \n    # Update the suptitle\n    new_title_text = \"Updated Suptitle\"\n    fig.suptitle(new_title_text, x=0.6, y=0.9, ha='left', va='bottom', fontsize=16, weight='bold')\n    assert fig._suptitle.get_text() == new_title_text\n    assert fig._suptitle.get_position() == (0.6, 0.9)\n    assert fig._suptitle.get_horizontalalignment() == 'left'\n    assert fig._suptitle.get_verticalalignment() == 'bottom'\n    assert fig._suptitle.get_fontsize() == 16\n    assert fig._suptitle.get_weight() == 'bold'\n", "def test_subplot_params_update():\n    \"\"\"Test the update method of SubplotParams.\"\"\"\n    params = SubplotParams(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.2, hspace=0.2)\n    \n    # Update with valid values\n    params.update(left=0.15, right=0.85, bottom=0.15, top=0.85)\n    assert params.left == 0.15\n    assert params.right == 0.85\n    assert params.bottom == 0.15\n    assert params.top == 0.85\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n    \n    # Update with invalid values, should raise ValueError\n    with pytest.raises(ValueError):\n        params.update(left=0.9, right=0.1)\n    with pytest.raises(ValueError):\n        params.update(bottom=0.9, top=0.1)\n\n    # Ensure values are unchanged after the invalid update attempts\n    assert params.left == 0.15\n    assert params.right == 0.85\n    assert params.bottom == 0.15\n    assert params.top == 0.85\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n", "def test_figure_aspect_ratio():\n    # Test that Figure correctly handles aspect ratio\n    aspect_ratio = 2.0  # height is twice the width\n    width, height = figaspect(aspect_ratio)\n    fig = Figure(figsize=(width, height))\n    assert fig.get_figwidth() == width\n    assert fig.get_figheight() == height\n\n    # Test that aspect ratio is maintained for different inputs\n    aspect_ratio = 0.5  # width is twice the height\n    width, height = figaspect(aspect_ratio)\n    fig.set_size_inches(width, height)\n    assert fig.get_figwidth() == width\n    assert fig.get_figheight() == height\n\n    # Test that figure aspect ratio adjusts correctly for arrays\n    data = np.random.rand(4, 2)\n    width, height = figaspect(data)\n    fig.set_size_inches(width, height)\n    assert fig.get_figwidth() == width\n    assert fig.get_figheight() == height\n", "def test_autofmt_xdate():\n    # Test autofmt_xdate method on a single axes figure\n    fig, ax = plt.subplots()\n    fig.autofmt_xdate(rotation=45, ha='center')\n    labels = ax.get_xticklabels()\n    for label in labels:\n        assert label.get_rotation() == 45\n        assert label.get_ha() == 'center'\n\n    # Test autofmt_xdate on multiple subplots with shared x-axis\n    fig, axs = plt.subplots(2, 1, sharex=True)\n    fig.autofmt_xdate(rotation=60, ha='left')\n    labels_top = axs[0].get_xticklabels()\n    labels_bottom = axs[1].get_xticklabels()\n    for label in labels_bottom:\n        assert label.get_rotation() == 60\n        assert label.get_ha() == 'left'\n        assert label.get_visible() is True\n    for label in labels_top:\n        assert label.get_visible() is False\n"], "sample_523": ["def test_draggable_legend_loc_update():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend(draggable=True, update='loc')\n    draggable_leg = leg.get_draggable()\n    \n    # Simulate dragging the legend to a new location\n    draggable_leg.start_drag(0, 0)\n    draggable_leg.end_drag(100, 100)\n    \n    # Check that the legend's location has been updated\n    assert leg._loc != 'best'  # The location should now be set to specific coordinates\n\n    # Also check that the bounding box to anchor has been updated\n    assert leg.get_bbox_to_anchor() is not None\n", "def test_draggable_legend_final_location_update():\n    \"\"\"\n    Test the finalize_offset method for draggable legends.\n    \"\"\"\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], label=\"line\")\n    leg = ax.legend()\n    draggable_legend = leg.set_draggable(True)\n\n    # Mock the get_loc_in_canvas method\n    with mock.patch.object(DraggableOffsetBox, 'get_loc_in_canvas', return_value=(0.5, 0.5)):\n        # Use the 'loc' update mode\n        draggable_legend._update = 'loc'\n        draggable_legend.finalize_offset()\n        assert leg._loc == (0.5, 0.5)\n\n    with mock.patch.object(DraggableOffsetBox, 'get_loc_in_canvas', return_value=(0.2, 0.3)):\n        # Use the 'bbox' update mode\n        draggable_legend._update = 'bbox'\n        draggable_legend.finalize_offset()\n        assert leg._bbox_to_anchor.get_points().tolist() == [[0.2, 0.3], [0.2, 0.3]]\n", "def test_legend_set_ncols():\n    fig, ax = plt.subplots()\n    x = np.arange(100)\n    ax.plot(x, 50 - x, 'o', label='y=1')\n    ax.plot(x, x - 50, 'o', label='y=-1')\n    leg = ax.legend(loc='upper left')\n    leg.set_ncols(2)\n    assert leg._ncols == 2\n    leg.set_ncols(1)\n    assert leg._ncols == 1\n", "def test_legend_set_loc():\n    # Test setting the legend location using set_loc method\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), label='test')\n    leg = ax.legend()\n    leg.set_loc('upper right')\n    assert leg._get_loc() == 1  # 'upper right' corresponds to code 1\n    leg.set_loc('lower left')\n    assert leg._get_loc() == 3  # 'lower left' corresponds to code 3\n    leg.set_loc((0.5, 0.5))  # Custom location\n    assert leg._get_loc() == (0.5, 0.5)\n", "def test_draggable_legend():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], label='line')\n    leg = ax.legend(draggable=True)\n    assert isinstance(leg._draggable, mlegend.DraggableLegend)\n    leg.set_draggable(False)\n    assert leg._draggable is None\n"], "sample_524": ["def test_subplotparams_update():\n    \"\"\"Test the update method of SubplotParams.\"\"\"\n    params = SubplotParams(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n    assert params.left == 0.1\n    assert params.bottom == 0.1\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    # Update some values and check\n    params.update(left=0.15, bottom=0.15)\n    assert params.left == 0.15\n    assert params.bottom == 0.15\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    # Test invalid updates\n    with pytest.raises(ValueError):\n        params.update(left=1.0)  # left >= right\n    with pytest.raises(ValueError):\n        params.update(bottom=1.0)  # bottom >= top\n", "def test_subplot_mosaic(shape):\n    \"\"\"Test the subplot_mosaic method with various grid shapes.\"\"\"\n    fig = plt.figure()\n    mosaic = np.arange(np.prod(shape)).reshape(shape).astype(str)\n    ax_dict = fig.subplot_mosaic(mosaic)\n    assert len(ax_dict) == np.prod(shape)\n    for key, ax in ax_dict.items():\n        assert key in mosaic.flatten()\n        assert isinstance(ax, plt.Axes)\n", "def test_figure_add_axes():\n    fig = plt.figure()\n    ax1 = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    assert ax1 in fig.axes\n    assert ax1.get_position().bounds == (0.1, 0.1, 0.8, 0.8)\n\n    # Test adding an axes instance directly\n    ax2 = plt.Axes(fig, [0.2, 0.2, 0.6, 0.6])\n    fig.add_axes(ax2)\n    assert ax2 in fig.axes\n    assert ax2.get_position().bounds == (0.2, 0.2, 0.6, 0.6)\n\n    # Test error on adding an existing axes from another figure\n    fig2 = plt.figure()\n    ax3 = fig2.add_axes([0.3, 0.3, 0.5, 0.5])\n    with pytest.raises(ValueError, match=\"The Axes must have been created in the present figure\"):\n        fig.add_axes(ax3)\n", "def test_add_subplot_axes():\n    # Test adding a subplot with different configurations\n    fig = plt.figure()\n\n    # Adding a simple subplot\n    ax1 = fig.add_subplot(111)\n    assert isinstance(ax1, mpl.axes.Axes)\n    assert ax1.get_geometry() == (1, 1, 1)\n\n    # Adding a subplot with a custom projection\n    ax2 = fig.add_subplot(122, projection='polar')\n    assert isinstance(ax2, mpl.projections.polar.PolarAxes)\n    assert ax2.get_geometry() == (1, 2, 2)\n\n    # Adding a subplot with a custom axes class\n    ax3 = fig.add_subplot(223, axes_class=mpl.axes.Axes)\n    assert isinstance(ax3, mpl.axes.Axes)\n    assert ax3.get_geometry() == (2, 2, 3)\n\n    # Adding a subplot with shared x-axis\n    ax4 = fig.add_subplot(224, sharex=ax1)\n    assert ax4.get_shared_x_axes().joined(ax1, ax4)\n    assert ax4.get_geometry() == (2, 2, 4)\n\n    # Adding a subplot with shared y-axis\n    ax5 = fig.add_subplot(221, sharey=ax1)\n    assert ax5.get_shared_y_axes().joined(ax1, ax5)\n    assert ax5.get_geometry() == (2, 2, 1)\n\n    # Ensure the correct number of axes\n    assert len(fig.axes) == 5\n", "def test_subplotparams_update_invalid():\n    # Test SubplotParams update with invalid values\n    params = SubplotParams()\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=0.9, right=0.1)\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=0.9, top=0.1)\n"], "sample_525": ["def test_subplotparams_update():\n    subplot_params = SubplotParams(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n    assert subplot_params.left == 0.1\n    assert subplot_params.bottom == 0.1\n    assert subplot_params.right == 0.9\n    assert subplot_params.top == 0.9\n    assert subplot_params.wspace == 0.2\n    assert subplot_params.hspace == 0.2\n\n    subplot_params.update(left=0.2, bottom=0.2)\n    assert subplot_params.left == 0.2\n    assert subplot_params.bottom == 0.2\n    assert subplot_params.right == 0.9\n    assert subplot_params.top == 0.9\n    assert subplot_params.wspace == 0.2\n    assert subplot_params.hspace == 0.2\n\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        subplot_params.update(left=1.0)\n\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        subplot_params.update(bottom=1.0)\n", "def test_figure_get_children():\n    fig = plt.figure()\n\n    # Initially, the figure should have only the patch as its child\n    children = fig.get_children()\n    assert len(children) == 1\n    assert isinstance(children[0], mpl.patches.Rectangle)\n\n    # Add a text artist and verify it is added to the children\n    text = fig.text(0.5, 0.5, \"Test\")\n    children = fig.get_children()\n    assert text in children\n\n    # Add a line artist and verify it is added to the children\n    line = plt.Line2D([0, 1], [0, 1])\n    fig.add_artist(line)\n    children = fig.get_children()\n    assert line in children\n\n    # Add a subplot and verify it is added to the children\n    ax = fig.add_subplot(111)\n    children = fig.get_children()\n    assert ax in children\n", "def test_add_axes_unique():\n    fig = plt.figure()\n\n    # Create the first Axes instance\n    ax1 = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    assert ax1 in fig.axes\n\n    # Adding the same Axes should not change the number of Axes in the figure\n    fig.add_axes(ax1)\n    assert len(fig.axes) == 1\n    assert fig.gca() is ax1\n\n    # Create a new Axes instance with different parameters\n    ax2 = fig.add_axes([0.2, 0.2, 0.6, 0.6])\n    assert ax2 in fig.axes\n    assert len(fig.axes) == 2\n    assert fig.gca() is ax2\n\n    # Ensure the order of axes is preserved\n    assert fig.axes == [ax1, ax2]\n\n    # Test the ValueError when adding an axes created in another figure\n    fig2 = plt.figure()\n    ax3 = fig2.add_axes([0.1, 0.1, 0.8, 0.8])\n    with pytest.raises(ValueError, match=\"The Axes must have been created in the present figure\"):\n        fig.add_axes(ax3)\n", "def test_add_artist_clip():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    circle = plt.Circle((0.5, 0.5), 0.3, color='blue')\n    \n    # Add artist without clipping\n    fig.add_artist(circle)\n    assert circle.get_clip_path() is None\n    \n    # Add artist with clipping\n    circle_clipped = plt.Circle((0.5, 0.5), 0.3, color='red')\n    fig.add_artist(circle_clipped, clip=True)\n    assert circle_clipped.get_clip_path() is not None\n    assert circle_clipped.get_clip_path()._path == fig.patch.get_path()\n", "def test_figure_contains():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    assert fig.contains(SimpleNamespace(x=0, y=0)) == (True, {})\n    assert fig.contains(SimpleNamespace(x=fig.bbox.width + 1, y=fig.bbox.height + 1)) == (False, {})\n\n    ax.plot([0, 1], [0, 1])\n    fig.draw_without_rendering()\n    assert fig.contains(SimpleNamespace(x=ax.bbox.x0, y=ax.bbox.y0)) == (True, {})\n    assert fig.contains(SimpleNamespace(x=ax.bbox.x0 - 1, y=ax.bbox.y0 - 1)) == (False, {})\n"], "sample_526": ["def test_drange_with_microseconds():\n    # Test drange with small delta involving microseconds\n    start = datetime.datetime(2023, 1, 1, 0, 0, 0, tzinfo=mdates.UTC)\n    end = datetime.datetime(2023, 1, 1, 0, 0, 1, tzinfo=mdates.UTC)\n    delta = datetime.timedelta(microseconds=100000)  # 0.1 seconds\n    expected_length = 10  # 1 second / 0.1 seconds = 10 intervals\n    drange_result = mdates.drange(start, end, delta)\n    assert len(drange_result) == expected_length\n\n    # Ensure the last date in the range is correct\n    expected_last_date = start + datetime.timedelta(microseconds=900000)  # 0.9 seconds\n    assert mdates.num2date(drange_result[-1]) == expected_last_date\n\n    # Test drange with delta larger than the span\n    span = datetime.timedelta(microseconds=100)\n    drange_result = mdates.drange(start, start + span, delta)\n    assert len(drange_result) == 1\n    assert mdates.num2date(drange_result[0]) == start\n", "def test_num2timedelta_with_sequence():\n    x = [1, 1.5, 2.5]\n    expected = [datetime.timedelta(days=1), \n                datetime.timedelta(days=1.5), \n                datetime.timedelta(days=2.5)]\n    result = mdates.num2timedelta(x)\n    assert result == expected\n", "def test_num2date_array():\n    # Test that num2date can handle numpy arrays of floats correctly.\n    dates = [19002.0, 19003.5, 19004.75]\n    expected = [\n        datetime.datetime(2022, 1, 10, 0, 0, tzinfo=datetime.timezone.utc),\n        datetime.datetime(2022, 1, 11, 12, 0, tzinfo=datetime.timezone.utc),\n        datetime.datetime(2022, 1, 12, 18, 0, tzinfo=datetime.timezone.utc)\n    ]\n    converted = mdates.num2date(np.array(dates))\n    assert all(a == b for a, b in zip(converted, expected))\n", "def test_get_set_epoch():\n    mdates._reset_epoch_test_example()\n    assert mdates.get_epoch() == '1970-01-01T00:00:00'\n\n    mdates.set_epoch('2000-01-01')\n    assert mdates.get_epoch() == '2000-01-01'\n\n    with pytest.raises(RuntimeError):\n        mdates.set_epoch('2010-01-01')\n\n    mdates._reset_epoch_test_example()\n    assert mdates.get_epoch() == '1970-01-01T00:00:00'\n", "def test_set_epoch_runtime_error():\n    # Test if setting the epoch after dates are converted raises RuntimeError\n    mdates._reset_epoch_test_example()\n    mdates.set_epoch('1970-01-01')\n    # Convert a date to trigger the epoch setting internally\n    mdates.date2num(datetime.datetime(2000, 1, 1))\n    with pytest.raises(RuntimeError, match='set_epoch must be called before dates plotted.'):\n        mdates.set_epoch('2000-01-01')\n"], "sample_527": ["def test_figure_suptitle_supxlabel_supylabel():\n    fig = plt.figure()\n    suptitle_text = \"This is a super title\"\n    supxlabel_text = \"This is a super xlabel\"\n    supylabel_text = \"This is a super ylabel\"\n    \n    # Test suptitle\n    suptitle = fig.suptitle(suptitle_text)\n    assert suptitle.get_text() == suptitle_text\n    assert suptitle.get_position() == (0.5, 0.98)\n    assert suptitle.get_ha() == 'center'\n    assert suptitle.get_va() == 'top'\n    \n    # Test supxlabel\n    supxlabel = fig.supxlabel(supxlabel_text)\n    assert supxlabel.get_text() == supxlabel_text\n    assert supxlabel.get_position() == (0.5, 0.01)\n    assert supxlabel.get_ha() == 'center'\n    assert supxlabel.get_va() == 'bottom'\n    \n    # Test supylabel\n    supylabel = fig.supylabel(supylabel_text)\n    assert supylabel.get_text() == supylabel_text\n    assert supylabel.get_position() == (0.02, 0.5)\n    assert supylabel.get_ha() == 'left'\n    assert supylabel.get_va() == 'center'\n    assert supylabel.get_rotation() == 'vertical'\n", "def test_add_axes():\n    fig = plt.figure()\n    ax1 = fig.add_axes([0.1, 0.1, 0.8, 0.8], label='axes1')\n    ax2 = fig.add_axes([0.2, 0.2, 0.6, 0.6], label='axes2')\n\n    assert len(fig.axes) == 2\n    assert fig.axes[0] is ax1\n    assert fig.axes[1] is ax2\n    assert ax1.get_label() == 'axes1'\n    assert ax2.get_label() == 'axes2'\n\n    with pytest.raises(TypeError):\n        fig.add_axes()\n\n    with pytest.raises(ValueError):\n        fig.add_axes(ax2)\n\n    fig.delaxes(ax1)\n    assert len(fig.axes) == 1\n    assert fig.axes[0] is ax2\n", "def test_figure_base_get_children():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    ax2 = fig.add_axes([0.2, 0.2, 0.3, 0.3])\n    text = fig.text(0.5, 0.5, \"hello\", ha=\"center\", va=\"center\")\n    line, = ax1.plot([0, 1], [0, 1])\n    patch = plt.Circle((0.5, 0.5), 0.1, facecolor='blue', edgecolor='red')\n    ax1.add_patch(patch)\n    \n    children = fig.get_children()\n    \n    assert fig.patch in children\n    assert ax1 in children\n    assert ax2 in children\n    assert text in children\n    assert line in children\n    assert patch in children\n", "def test_figure_suptitle():\n    fig = plt.figure()\n    title_text = \"Super Title\"\n    fig.suptitle(title_text)\n    assert fig._suptitle.get_text() == title_text\n    assert fig._suptitle.get_position() == (0.5, 0.98)\n    assert fig._suptitle.get_ha() == 'center'\n    assert fig._suptitle.get_va() == 'top'\n", "def test_axes_stack():\n    stack = _AxesStack()\n    fig, ax1 = plt.subplots()\n    ax2 = fig.add_subplot(111)\n\n    # Test adding and removing axes\n    stack.add(ax1)\n    stack.add(ax2)\n    assert stack.as_list() == [ax1, ax2]\n    stack.remove(ax1)\n    assert stack.as_list() == [ax2]\n\n    # Test bubble method\n    stack.add(ax1)\n    stack.bubble(ax1)\n    assert stack.as_list() == [ax2, ax1]\n\n    # Test current method\n    assert stack.current() == ax1\n    stack.bubble(ax2)\n    assert stack.current() == ax2\n\n    # Test error when bubbling non-existing axes\n    with pytest.raises(ValueError, match=\"Axes has not been added yet\"):\n        stack.bubble(Figure().add_subplot(111))\n"], "sample_528": ["def test_reload_library():\n    with temp_style('test_reload', DUMMY_SETTINGS):\n        assert 'test_reload' in style.available\n        # Remove the temporary style and reload library.\n        USER_LIBRARY_PATHS.pop()\n        style.reload_library()\n        assert 'test_reload' not in style.available\n", "def test_reload_library():\n    original_styles = set(style.available)\n    new_style_name = '_test_reload_'\n    new_style_settings = {PARAM: 'blue'}\n\n    with temp_style(new_style_name, new_style_settings):\n        assert new_style_name in style.available\n\n    style.reload_library()\n    reloaded_styles = set(style.available)\n\n    assert new_style_name not in reloaded_styles\n    assert original_styles == reloaded_styles\n", "def test_remove_blacklisted_style_params():\n    settings = {'interactive': True, PARAM: VALUE, 'backend': 'Agg'}\n    filtered_settings = style._remove_blacklisted_style_params(settings)\n    assert PARAM in filtered_settings\n    assert 'interactive' not in filtered_settings\n    assert 'backend' not in filtered_settings\n", "def test_blacklisted_style_params():\n    blacklisted_param = 'backend'\n    valid_param = 'axes.facecolor'\n    valid_value = 'blue'\n    settings = {blacklisted_param: 'TkAgg', valid_param: valid_value}\n    with temp_style('blacklist_test', settings):\n        with style.context('blacklist_test'):\n            assert blacklisted_param not in mpl.rcParams\n            assert mpl.rcParams[valid_param] == valid_value\n", "def test_remove_blacklisted_style_params():\n    original_params = {\n        'interactive': True,\n        'backend': 'Agg',\n        PARAM: VALUE,\n        'webagg.port': 8888\n    }\n    expected_params = {PARAM: VALUE}\n    cleaned_params = style._remove_blacklisted_style_params(original_params)\n    assert cleaned_params == expected_params\n"], "sample_529": ["def test_legend_draggable_box_update():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='shabnams')\n    leg = ax.legend(draggable=True, update='bbox')\n    assert leg.get_draggable() is True\n    assert leg._draggable._update == 'bbox'\n    # Simulate drag event to update bbox\n    draggable = leg._draggable\n    draggable.start_drag(None)\n    draggable.drag(None, 20, 30)\n    draggable.end_drag(None)\n    assert leg.get_bbox_to_anchor().x0 != 0\n    assert leg.get_bbox_to_anchor().y0 != 0\n", "def test_draggable_legend():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), np.arange(10), label='line')\n    leg = ax.legend(draggable=True)\n    assert leg.get_draggable()\n\n    draggable_legend = leg.set_draggable(True)\n    assert isinstance(draggable_legend, mlegend.DraggableLegend)\n\n    draggable_legend.finalize_offset()\n    assert leg._draggable is not None\n    assert leg._draggable._update == 'loc'\n\n    # Test dragging mode update to bbox\n    draggable_legend_bbox = leg.set_draggable(True, update='bbox')\n    assert draggable_legend_bbox._update == 'bbox'\n", "def test_draggable_legend_update_loc():\n    \"\"\"\n    Test the functionality of the DraggableLegend class with update='loc'.\n    Specifically, this test checks if the legend location is updated correctly\n    after dragging.\n    \"\"\"\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], label='test')\n    leg = ax.legend(draggable=True, update='loc')\n    draggable = leg.get_draggable()\n    assert isinstance(draggable, mlegend.DraggableLegend)\n    assert draggable._update == 'loc'\n\n    # Simulate dragging\n    draggable._update_loc((0.5, 0.5))\n    assert leg._loc_real == (0.5, 0.5)\n", "def test_draggable_legend():\n    # Test the draggable functionality of the legend\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1, 2], [2, 3, 4], label='line')\n    leg = ax.legend(draggable=True)\n    assert leg.get_draggable() == True\n\n    # Simulate dragging the legend\n    draggable = leg.get_draggable()\n    initial_loc = leg._loc_real\n    draggable.start_drag(0, 0)\n    draggable.drag(10, 10)\n    draggable.stop_drag()\n\n    # Ensure the location has changed after dragging\n    assert leg._loc_real != initial_loc\n    assert leg._loc_real == (10, 10)\n", "def test_draggablelegend_update_loc():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='draggable legend test')\n    leg = ax.legend(draggable=True, loc='upper left')\n    draggable = leg.get_draggable()\n    assert isinstance(draggable, mlegend.DraggableLegend)\n    \n    # Simulate dragging the legend to a new location\n    draggable.start_offset = (0, 0)\n    new_loc = (0.5, 0.5)\n    draggable.onrelease(None, *new_loc)\n    \n    # Check if the legend's location was updated correctly\n    assert leg._loc_real == 2  # 'upper left' -> 2\n    new_bbox = leg.get_bbox_to_anchor()\n    expected_bbox = mtransforms.Bbox.from_bounds(0.5, 0.5, 0, 0)\n    assert_allclose(new_bbox.bounds, expected_bbox.bounds, atol=1e-2)\n"], "sample_530": ["def test_offsetbox_set_get_offset():\n    fig, ax = plt.subplots()\n    box = OffsetBox()\n\n    # Test setting and getting a static offset\n    box.set_offset((10, 20))\n    assert box.get_offset(0, 0, 0, 0, None) == (10, 20)\n\n    # Test setting and getting a dynamic offset\n    box.set_offset(lambda w, h, xd, yd, renderer: (w + 5, h + 5))\n    assert box.get_offset(100, 200, 0, 0, None) == (105, 205)\n", "def test_drawing_area_transform():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    rect = mpatches.Rectangle((0, 0), 50, 50, facecolor='blue')\n    da.add_artist(rect)\n    ax.add_artist(da)\n\n    # Check initial transform\n    assert rect.get_transform() == da.get_transform()\n\n    # Modify the DrawingArea's transform\n    da.set_offset((100, 100))\n    da.dpi_transform.scale(2.0)\n\n    fig.canvas.draw()\n\n    # Check modified transform\n    assert rect.get_transform() == da.get_transform()\n    assert_allclose(da.get_offset(), (100, 100))\n", "def test_offsetbox_contains():\n    fig, ax = plt.subplots()\n    size = 100\n    da = DrawingArea(size, size)\n    bg = mpatches.Rectangle((0, 0), size, size, facecolor='#CCCCCC', edgecolor='None', linewidth=0)\n    da.add_artist(bg)\n\n    anchored_box = AnchoredOffsetbox(loc='center', child=da, pad=0., frameon=False, bbox_to_anchor=(.5, .5), bbox_transform=ax.transAxes, borderpad=0.)\n    ax.add_artist(anchored_box)\n\n    fig.canvas.draw()\n    assert anchored_box.contains(MouseEvent('button_press_event', fig.canvas, 0.5, 0.5))[0]\n\n    # Check that clicking outside the box does not return contained\n    assert not anchored_box.contains(MouseEvent('button_press_event', fig.canvas, 0, 0))[0]\n", "def test_offsetbox_set_offset_callable():\n    # Test that set_offset works correctly when a callable is provided\n        return width / 2, height / 2\n\n    fig, ax = plt.subplots()\n    offset_box = OffsetBox()\n    offset_box.set_offset(offset_fn)\n\n    w, h, xd, yd = 100, 50, 10, 5\n    renderer = fig.canvas.get_renderer()\n    assert offset_box.get_offset(w, h, xd, yd, renderer) == (w / 2, h / 2)\n", "def test_offsetbox_contains():\n    fig, ax = plt.subplots()\n    size = 50\n    da = DrawingArea(size, size, clip=True)\n    bg = mpatches.Rectangle((0, 0), size, size, facecolor='#CCCCCC', edgecolor='None', linewidth=0)\n    da.add_artist(bg)\n    ax.add_artist(da)\n    \n    fig.canvas.draw()\n    inside, details = da.contains(MouseEvent(\"button_press_event\", fig.canvas, 25, 25, MouseButton.LEFT))\n    assert inside, \"OffsetBox should contain point within its bounds.\"\n    \n    inside, details = da.contains(MouseEvent(\"button_press_event\", fig.canvas, 60, 60, MouseButton.LEFT))\n    assert not inside, \"OffsetBox should not contain point outside its bounds.\"\n"], "sample_531": ["def test_subplot_mosaic_invalid_keys():\n    fig = plt.figure()\n    with pytest.raises(ValueError, match=\"Duplicate key 'A' in mosaic layout\"):\n        fig.subplot_mosaic([['A', 'B'], ['A', 'C']])\n    with pytest.raises(ValueError, match=\"The keys .* are in .* but not in the mosaic\"):\n        fig.subplot_mosaic([['A', 'B'], ['C', 'D']], per_subplot_kw={'E': {}})\n    with pytest.raises(ValueError, match=\"must be 2D\"):\n        fig.subplot_mosaic([['A', 'B'], 'C'])\n    with pytest.raises(ValueError, match=\"All of the rows must be the same length\"):\n        fig.subplot_mosaic([['A', 'B'], ['C']])\n", "def test_subplot_params_update():\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        SubplotParams().update(left=0.8, right=0.2)\n\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        SubplotParams().update(bottom=0.8, top=0.2)\n\n    params = SubplotParams()\n    params.update(left=0.1, bottom=0.2, right=0.9, top=0.8, wspace=0.3, hspace=0.4)\n\n    assert params.left == 0.1\n    assert params.bottom == 0.2\n    assert params.right == 0.9\n    assert params.top == 0.8\n    assert params.wspace == 0.3\n    assert params.hspace == 0.4\n", "def test_subplotparams_update():\n    # Test SubplotParams update method with valid parameters\n    sp = mpl.figure.SubplotParams(left=0.1, bottom=0.2, right=0.9, top=0.8, wspace=0.5, hspace=0.3)\n    assert sp.left == 0.1\n    assert sp.bottom == 0.2\n    assert sp.right == 0.9\n    assert sp.top == 0.8\n    assert sp.wspace == 0.5\n    assert sp.hspace == 0.3\n\n    sp.update(left=0.2, right=0.8)\n    assert sp.left == 0.2\n    assert sp.right == 0.8\n\n    sp.update(bottom=0.3, top=0.7)\n    assert sp.bottom == 0.3\n    assert sp.top == 0.7\n\n    sp.update(wspace=0.4, hspace=0.2)\n    assert sp.wspace == 0.4\n    assert sp.hspace == 0.2\n", "def test_stale_figure_callback():\n    fig, ax = plt.subplots()\n    fig.stale = False\n    _stale_figure_callback(ax, True)\n    assert fig.stale is True\n    _stale_figure_callback(ax, False)\n    assert fig.stale is False\n", "def test_set_get_facecolor():\n    fig = plt.figure()\n    initial_color = fig.get_facecolor()\n    new_color = 'blue'\n    \n    # Set a new face color\n    fig.set_facecolor(new_color)\n    assert fig.get_facecolor() == new_color\n    \n    # Restore initial color\n    fig.set_facecolor(initial_color)\n    assert fig.get_facecolor() == initial_color\n"], "sample_532": ["def test_clabel_deprecated_properties():\n    x, y = np.meshgrid(np.linspace(-3.0, 3.0, 50), np.linspace(-2.0, 2.0, 40))\n    z = np.sin(x) * np.cos(y)\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    clabels = cs.clabel()\n    \n    with pytest.deprecated_call(match=\"labelFontProps\"):\n        assert cs.labelFontProps == clabels[0].get_font_properties()\n    \n    with pytest.deprecated_call(match=\"labelFontSizeList\"):\n        assert cs.labelFontSizeList == [clabels[0].get_font_properties().get_size()] * len(cs.labelLevelList)\n    \n    with pytest.deprecated_call(match=\"labelTextsList\"):\n        assert cs.labelTextsList == cbook.silent_list('text.Text', cs.labelTexts)\n", "def test_clabel_properties():\n    x = np.linspace(-3.0, 3.0, 100)\n    y = np.linspace(-3.0, 3.0, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X**2 + Y**2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(X, Y, Z, levels=5)\n\n    # Test deprecated property labelFontProps\n    with pytest.deprecated_call():\n        font_props = cs.labelFontProps\n    assert isinstance(font_props, mpl.font_manager.FontProperties)\n\n    # Test deprecated property labelFontSizeList\n    with pytest.deprecated_call():\n        font_size_list = cs.labelFontSizeList\n    assert len(font_size_list) == 5\n    assert all(size == font_props.get_size() for size in font_size_list)\n\n    # Test deprecated property labelTextsList\n    with pytest.deprecated_call():\n        label_texts_list = cs.labelTextsList\n    assert isinstance(label_texts_list, cbook.silent_list)\n", "def test_clabel_event_handler():\n    from matplotlib.contour import _contour_labeler_event_handler\n    from matplotlib.backend_bases import MouseEvent, KeyEvent\n\n    class MockCanvas:\n            self.events = []\n\n            self.events.append('draw')\n\n            self.events.append('stop')\n\n    class MockAxes:\n            self.figure = MockCanvas()\n\n    class MockCS:\n            self.axes = MockAxes()\n            self.labels = []\n\n            self.labels.append((x, y))\n\n            if self.labels:\n                self.labels.pop()\n\n    cs = MockCS()\n    inline = True\n    inline_spacing = 5\n\n    # Test MIDDLE mouse button (stop event)\n    event = MouseEvent(name='button_press_event', canvas=cs.axes.figure, button=MouseButton.MIDDLE, x=0, y=0)\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    assert 'stop' in cs.axes.figure.events\n\n    # Test ESC key (stop event)\n    event = KeyEvent(name='key_press_event', canvas=cs.axes.figure, key='escape')\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    assert cs.axes.figure.events.count('stop') == 2\n\n    # Test BACKSPACE key (pop label)\n    event = KeyEvent(name='key_press_event', canvas=cs.axes.figure, key='backspace')\n    cs.labels = [(10, 10), (20, 20)]\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    assert len(cs.labels) == 2  # No pop in inline mode\n\n    # Test LEFT mouse button (add label)\n    event = MouseEvent(name='button_press_event', canvas=cs.axes.figure, button=MouseButton.LEFT, x=10, y=20, inaxes=cs.axes)\n    _contour_labeler_event_handler(cs, inline, inline_spacing, event)\n    assert cs.labels[-1] == (10, 20)\n    assert 'draw' in cs.axes.figure.events\n", "def test_contour_labeler_event_handler():\n    from matplotlib.backend_bases import MouseEvent, KeyEvent\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(np.random.rand(10, 10))\n\n    # Simulate Middle mouse button press to stop event loop\n    event = MouseEvent('button_press_event', fig.canvas, 0, 0, button=MouseButton.MIDDLE)\n    _contour_labeler_event_handler(cs, inline=False, inline_spacing=5, event=event)\n    # No assertion, just checking for no exceptions\n\n    # Simulate Escape key press to stop event loop\n    event = KeyEvent('key_press_event', fig.canvas, key='escape')\n    _contour_labeler_event_handler(cs, inline=False, inline_spacing=5, event=event)\n\n    # Simulate Right mouse button press to pop last label\n    event = MouseEvent('button_press_event', fig.canvas, 0, 0, button=MouseButton.RIGHT)\n    _contour_labeler_event_handler(cs, inline=False, inline_spacing=5, event=event)\n\n    # Simulate Backspace key press to pop last label\n    event = KeyEvent('key_press_event', fig.canvas, key='backspace')\n    _contour_labeler_event_handler(cs, inline=False, inline_spacing=5, event=event)\n\n    # Simulate Left mouse button press to add label\n    event = MouseEvent('button_press_event', fig.canvas, 100, 100, button=MouseButton.LEFT)\n    _contour_labeler_event_handler(cs, inline=False, inline_spacing=5, event=event)\n", "def test_clabel_text_transform_rotation():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z, zorder=2)\n    clabels = cs.clabel(use_clabeltext=True)\n    for label in clabels:\n        assert label.get_transform_rotates_text() is True\n"], "sample_533": ["def test_clabeltext_deprecated():\n    with pytest.warns(mpl._api.deprecation.MatplotlibDeprecationWarning, match=\"ClabelText is deprecated\"):\n        t = ClabelText(0, 0, text='test')\n    assert t.get_rotation() == 0\n\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.linspace(-3.0, 3.0, 100), np.linspace(-2.0, 2.0, 100))\n    z = np.exp(-x**2 - y**2)\n\n    with pytest.warns(mpl._api.deprecation.MatplotlibDeprecationWarning, match=\"use_clabeltext is deprecated\"):\n        cs = ax.contour(x, y, z)\n        cs.clabel(use_clabeltext=True)\n", "def test_clabeltext_get_rotation():\n    # Test the deprecated ClabelText class\n    fig, ax = plt.subplots()\n    CS = ax.contour(np.random.rand(10, 10))\n    clabels = ax.clabel(CS, use_clabeltext=True)\n    for label in clabels:\n        if isinstance(label, ClabelText):\n            rotation = label.get_rotation()\n            assert isinstance(rotation, float)\n", "def test_clabel_text_rotation():\n    # Test that ClabelText returns the correct rotation\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    clabels = cs.clabel(use_clabeltext=True)\n\n    for label in clabels:\n        assert isinstance(label, ClabelText)\n        original_rotation = label.get_rotation()\n        transformed_rotation = label.get_transform().transform_angles(\n            [original_rotation], [label.get_position()])[0]\n        assert original_rotation == transformed_rotation\n", "def test_clabeltext_deprecation():\n    # Test the deprecation warning for ClabelText\n    with pytest.warns(MatplotlibDeprecationWarning, match=\"ClabelText is deprecated since Matplotlib 3.7\"):\n        text = ClabelText(0, 0, \"test\")\n    assert isinstance(text, ClabelText)\n", "def test_contour_label_font_properties():\n    # Test deprecated labelFontProps property\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    fig, ax = plt.subplots()\n    CS = ax.contour(X, Y, Z, 6, colors='k')\n    CS.clabel(fontsize=12)\n    # Access the deprecated property\n    with pytest.deprecated_call():\n        font_props = CS.labelFontProps\n    assert font_props.get_size() == 12\n"], "sample_534": ["def test_ContourLabeler_set_label_props():\n    fig, ax = plt.subplots()\n    x = np.linspace(-3.0, 3.0, 100)\n    y = np.linspace(-3.0, 3.0, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X) * np.cos(Y)\n\n    cs = ax.contour(X, Y, Z)\n    cs.clabel()\n\n    label = cs.labelTexts[0]\n    original_text = label.get_text()\n    original_color = label.get_color()\n\n    # Change label properties using set_label_props\n    new_text = \"new label\"\n    new_color = \"red\"\n    cs.set_label_props(label, new_text, new_color)\n\n    assert label.get_text() == new_text\n    assert label.get_color() == new_color\n", "def test_clabeltext_deprecation():\n    fig, ax = plt.subplots()\n    cs = ax.contour(np.random.rand(10, 10))\n    with pytest.warns(MatplotlibDeprecationWarning, match=\"ClabelText is deprecated\"):\n        cs.add_label(5, 5, 0, 'test', 0)\n    with pytest.warns(MatplotlibDeprecationWarning, match=\"cs.labelTexts[0].get_font()\"):\n        font_props = cs.labelFontProps\n    with pytest.warns(MatplotlibDeprecationWarning, match=\"cs.labelTexts\"):\n        texts_list = cs.labelTextsList\n    with pytest.warns(MatplotlibDeprecationWarning, match=\"cs.labelTexts[0].get_font().get_size()\"):\n        font_size_list = cs.labelFontSizeList\n", "def test_ContourSet_get_transform():\n    # Test that get_transform returns the correct transform object\n    x = np.arange(10)\n    y = np.arange(10)\n    z = np.random.random((10, 10))\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    assert cs.get_transform() == ax.transData\n\n    # Set a custom transform and verify\n    custom_transform = mtransforms.Affine2D().rotate_deg(45)\n    cs = ax.contour(x, y, z, transform=custom_transform)\n    assert cs.get_transform() == custom_transform\n", "def test_clabel_text_properties():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    labels = cs.clabel(fontsize=12, colors='red', fmt='%1.1f')\n\n    for label in labels:\n        assert label.get_fontproperties().get_size() == 12\n        assert label.get_color() == 'red'\n        assert re.match(r'\\d\\.\\d', label.get_text())\n", "def test_contour_clabeltext_deprecated():\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    with pytest.deprecated_call():\n        cs = ax.contour(x, y, z, zorder=2)\n        cs.clabel(zorder=3, use_clabeltext=True)\n        assert cs._use_clabeltext is True\n\n    with pytest.deprecated_call():\n        text = ClabelText(0, 0, \"deprecated text\")\n        assert text.get_rotation() is not None\n"], "sample_535": ["def test_table_scale():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n\n    # Add cells to the table\n    table.add_cell(0, 0, width=1, height=1, text='Cell 0,0')\n    table.add_cell(0, 1, width=1, height=1, text='Cell 0,1')\n    table.add_cell(1, 0, width=1, height=1, text='Cell 1,0')\n    table.add_cell(1, 1, width=1, height=1, text='Cell 1,1')\n\n    # Scale the table\n    table.scale(2, 3)\n\n    # Check the scaled dimensions\n    assert table[0, 0].get_width() == 2\n    assert table[0, 0].get_height() == 3\n    assert table[0, 1].get_width() == 2\n    assert table[0, 1].get_height() == 3\n    assert table[1, 0].get_width() == 2\n    assert table[1, 0].get_height() == 3\n    assert table[1, 1].get_width() == 2\n    assert table[1, 1].get_height() == 3\n\n    # Check that the cells' positions are updated correctly\n    table.draw(ax.figure._cachedRenderer)\n    assert table[0, 1].get_x() == 2  # 2 because we scaled by 2 in x direction\n    assert table[1, 0].get_y() == 3  # 3 because we scaled by 3 in y direction\n", "def test_cell_text_properties():\n    # Test that setting text properties on a cell works correctly\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    \n    cell = table.add_cell(0, 0, width=1, height=1, text='Test', loc='center')\n    cell.set_text_props(color='red', fontsize=14, fontweight='bold')\n    \n    text = cell.get_text()\n    assert text.get_color() == 'red'\n    assert text.get_fontsize() == 14\n    assert text.get_fontweight() == 'bold'\n", "def test_cell_set_text_props():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n\n    cell = table.add_cell(0, 0, 1, 1, text=\"Initial Text\")\n    assert cell.get_text().get_text() == \"Initial Text\"\n\n    cell.set_text_props(text=\"Updated Text\", fontsize=14, color='red')\n    assert cell.get_text().get_text() == \"Updated Text\"\n    assert cell.get_text().get_fontsize() == 14\n    assert cell.get_text().get_color() == 'red'\n", "def test_set_text_props():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n\n    cell = table.add_cell(1, 1, width=1, height=1, text='original text')\n    assert cell.get_text().get_text() == 'original text'\n    cell.set_text_props(text='new text', color='red', fontsize=14)\n    \n    assert cell.get_text().get_text() == 'new text'\n    assert cell.get_text().get_color() == 'red'\n    assert cell.get_text().get_fontsize() == 14\n", "def test_visible_edges():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    \n    cell = table.add_cell(0, 0, width=1, height=1, visible_edges='horizontal')\n    assert cell.visible_edges == 'BT'\n    \n    cell = table.add_cell(0, 1, width=1, height=1, visible_edges='vertical')\n    assert cell.visible_edges == 'RL'\n    \n    cell = table.add_cell(0, 2, width=1, height=1, visible_edges='open')\n    assert cell.visible_edges == ''\n    \n    cell = table.add_cell(0, 3, width=1, height=1, visible_edges='closed')\n    assert cell.visible_edges == 'BRTL'\n    \n    with pytest.raises(ValueError):\n        table.add_cell(0, 4, width=1, height=1, visible_edges='invalid')\n"], "sample_536": ["def test_lock_draw():\n    lock = widgets.LockDraw()\n    widget1 = object()\n    widget2 = object()\n\n    assert lock.available(widget1)\n    lock(widget1)\n    assert lock.isowner(widget1)\n    assert not lock.available(widget2)\n    assert lock.locked()\n\n    lock.release(widget1)\n    assert not lock.locked()\n    assert lock.available(widget2)\n\n    with pytest.raises(ValueError):\n        lock.release(widget2)\n\n    lock(widget2)\n    with pytest.raises(ValueError):\n        lock(widget1)\n", "def test_lockdraw():\n    lock = widgets.LockDraw()\n    widget1 = object()\n    widget2 = object()\n    \n    # Initially, lock should be available\n    assert lock.available(widget1)\n    assert lock.available(widget2)\n    assert not lock.locked()\n\n    # Acquire the lock with widget1\n    lock(widget1)\n    assert lock.locked()\n    assert lock.isowner(widget1)\n    assert not lock.available(widget2)\n\n    # Trying to acquire the lock with widget2 should raise ValueError\n    with pytest.raises(ValueError, match='already locked'):\n        lock(widget2)\n\n    # Releasing the lock with widget1\n    lock.release(widget1)\n    assert not lock.locked()\n    assert lock.available(widget1)\n    assert lock.available(widget2)\n\n    # Releasing the lock with widget2 should raise ValueError\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(widget2)\n", "def test_lock_draw():\n    lock = widgets.LockDraw()\n    widget1 = object()\n    widget2 = object()\n\n    # Test initial state\n    assert lock.available(widget1)\n    assert not lock.locked()\n    assert not lock.isowner(widget1)\n\n    # Test lock acquisition\n    lock(widget1)\n    assert not lock.available(widget2)\n    assert lock.locked()\n    assert lock.isowner(widget1)\n\n    # Test lock release\n    lock.release(widget1)\n    assert lock.available(widget2)\n    assert not lock.locked()\n    assert not lock.isowner(widget1)\n\n    # Test ValueError when trying to acquire a locked lock\n    lock(widget1)\n    with pytest.raises(ValueError):\n        lock(widget2)\n\n    # Test ValueError when trying to release a lock not owned by the widget\n    with pytest.raises(ValueError):\n        lock.release(widget2)\n", "def test_lock_draw():\n    lock = widgets.LockDraw()\n\n    assert lock.available('widget1')\n    lock('widget1')\n    assert not lock.available('widget2')\n    with pytest.raises(ValueError, match='already locked'):\n        lock('widget2')\n    assert lock.isowner('widget1')\n    assert lock.locked()\n\n    lock.release('widget1')\n    assert lock.available('widget2')\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release('widget2')\n    assert not lock.locked()\n", "def test_lockdraw():\n    lock = widgets.LockDraw()\n    \n    class WidgetMock:\n        pass\n\n    widget1 = WidgetMock()\n    widget2 = WidgetMock()\n\n    # Initially, no widget owns the lock\n    assert lock.available(widget1)\n    assert lock.available(widget2)\n\n    # widget1 acquires the lock\n    lock(widget1)\n    assert lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n    assert lock.locked()\n    assert not lock.available(widget2)\n\n    # widget1 releases the lock\n    lock.release(widget1)\n    assert not lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n    assert not lock.locked()\n    assert lock.available(widget1)\n    assert lock.available(widget2)\n\n    # Ensure ValueError is raised when trying to acquire/release lock improperly\n    lock(widget1)\n    with pytest.raises(ValueError):\n        lock(widget2)  # widget2 tries to acquire the lock while owned by widget1\n    with pytest.raises(ValueError):\n        lock.release(widget2)  # widget2 tries to release the lock it doesn't own\n"], "sample_537": ["def test_detrend_mean_multi_axis():\n    # Test detrend_mean with multi-dimensional array along different axes\n    x = np.array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9],\n    ])\n    \n    # Detrend along axis 0\n    expected_result_axis0 = np.array([\n        [-3, -3, -3],\n        [0, 0, 0],\n        [3, 3, 3],\n    ])\n    result_axis0 = mlab.detrend_mean(x, axis=0)\n    assert_array_equal(result_axis0, expected_result_axis0)\n\n    # Detrend along axis 1\n    expected_result_axis1 = np.array([\n        [-1, 0, 1],\n        [-1, 0, 1],\n        [-1, 0, 1],\n    ])\n    result_axis1 = mlab.detrend_mean(x, axis=1)\n    assert_array_equal(result_axis1, expected_result_axis1)\n", "def test_detrend_invalid_key():\n    \"\"\"\n    Test for `detrend` function with an invalid key to ensure it raises a ValueError.\n    \"\"\"\n    x = np.linspace(0., 100, 1000)\n    invalid_key = \"invalid_key\"\n    with pytest.raises(ValueError, match=f\"Unknown value for key: '{invalid_key}'\"):\n        mlab.detrend(x, key=invalid_key)\n", "def test_stride_windows_no_overlap():\n    x = np.arange(100)\n    n = 20\n    noverlap = 0\n    y = mlab.stride_windows(x, n, noverlap=noverlap)\n    expected_shape = (n, (100 - noverlap) // n)\n    yt = np.array([x[i:i + n] for i in range(0, len(x) - n + 1, n)]).T\n    assert y.shape == expected_shape\n    assert_array_equal(y, yt)\n    assert y.base is x\n", "def test_window_hanning():\n    data = np.array([1, 2, 3, 4, 5])\n    expected_output = np.hanning(len(data)) * data\n    assert_array_equal(mlab.window_hanning(data), expected_output)\n", "def test_detrend_unknown_key():\n    \"\"\"Test detrend function with an unknown key.\"\"\"\n    x = np.arange(10)\n    with pytest.raises(ValueError, match=\"Unknown value for key\"):\n        mlab.detrend(x, key=\"unknown_key\")\n"], "sample_538": ["def test_transformed_bbox_get_points():\n    bbox = mtransforms.Bbox.from_extents(1, 1, 4, 4)\n    trans = mtransforms.Affine2D().rotate_deg(45)\n    trans_bbox = mtransforms.TransformedBbox(bbox, trans)\n    \n    # Manually compute the expected transformed points\n    points = bbox.get_points()\n    transformed_points = trans.transform(\n        [[points[0, 0], points[0, 1]],\n         [points[1, 0], points[0, 1]],\n         [points[0, 0], points[1, 1]],\n         [points[1, 0], points[1, 1]]])\n    \n    expected_points = [\n        [np.min(transformed_points[:, 0]), np.min(transformed_points[:, 1])],\n        [np.max(transformed_points[:, 0]), np.max(transformed_points[:, 1])]\n    ]\n\n    assert_array_almost_equal(trans_bbox.get_points(), expected_points)\n", "def test_bbox_shrunk():\n    bbox = mtransforms.Bbox.from_bounds(1, 2, 4, 6)\n    shrunk_bbox = bbox.shrunk(0.5, 0.5)\n    expected_bbox = mtransforms.Bbox.from_bounds(1, 2, 2, 3)\n    assert_bbox_eq(shrunk_bbox, expected_bbox)\n\n    shrunk_bbox = bbox.shrunk(1.5, 1.5)\n    expected_bbox = mtransforms.Bbox.from_bounds(1, 2, 6, 9)\n    assert_bbox_eq(shrunk_bbox, expected_bbox)\n", "def test_transform_node_invalidation():\n    # Test the invalidation mechanism of TransformNode\n    class SimpleTransformNode(mtransforms.TransformNode):\n            super().__init__(shorthand_name=name)\n            self.invalid_count = 0\n\n            self.invalid_count += 1\n            super()._invalidate_internal(value, invalidating_node)\n\n    parent = SimpleTransformNode('parent')\n    child = SimpleTransformNode('child')\n    parent.set_children(child)\n\n    # Initial invalidation count should be zero\n    assert parent.invalid_count == 0\n    assert child.invalid_count == 0\n\n    # Invalidate child and check if parent is invalidated\n    child.invalidate()\n    assert parent.invalid_count == 1\n    assert child.invalid_count == 1\n\n    # Invalidate child again and check counts\n    child.invalidate()\n    assert parent.invalid_count == 2\n    assert child.invalid_count == 2\n\n    # Invalidate parent directly and check counts\n    parent.invalidate()\n    assert parent.invalid_count == 3\n    assert child.invalid_count == 2  # Child count should not change\n", "def test_bbox_transform():\n    bbox = mtransforms.Bbox([[1, 1], [2, 2]])\n    transform = mtransforms.BboxTransformTo(mtransforms.Bbox([[0, 0], [10, 10]]))\n    transformed_bbox = transform.transform_bbox(bbox)\n    expected_bbox = mtransforms.Bbox([[0, 0], [10, 10]])\n    assert_allclose(transformed_bbox.get_points(), expected_bbox.get_points())\n\n    inverse_transform = transform.inverted()\n    original_bbox = inverse_transform.transform_bbox(transformed_bbox)\n    assert_allclose(original_bbox.get_points(), bbox.get_points())\n", "def test_bbox_translated():\n    bbox = mtransforms.Bbox.from_extents(0, 0, 1, 1)\n    translated_bbox = bbox.translated(2, 3)\n\n    assert_array_equal(translated_bbox.bounds, [2, 3, 3, 4])\n    assert bbox.bounds == (0, 0, 1, 1)  # Ensure the original bbox is unchanged\n"], "sample_539": ["def test_slider_on_changed(ax):\n    on_changed = mock.Mock(spec=noop, return_value=None)\n    slider = widgets.Slider(ax=ax, label='', valmin=0, valmax=10, valinit=5)\n    slider.on_changed(on_changed)\n\n    slider.set_val(7)\n    on_changed.assert_called_once_with(7)\n\n    slider.set_val(3)\n    assert on_changed.call_count == 2\n    on_changed.assert_called_with(3)\n\n    # Test disconnecting the callback\n    callback_id = slider.on_changed(on_changed)\n    slider.disconnect(callback_id)\n    slider.set_val(8)\n    # Callback should not be called again after disconnect\n    assert on_changed.call_count == 2\n", "def test_lockdraw():\n    lock = widgets.LockDraw()\n    widget1 = mock.Mock()\n    widget2 = mock.Mock()\n\n    # Initially, no owner.\n    assert lock.available(widget1)\n    assert lock.available(widget2)\n\n    # Locking with widget1 should succeed.\n    lock(widget1)\n    assert not lock.available(widget2)\n    assert lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n    assert lock.locked()\n\n    # Trying to lock with widget2 should fail.\n    with pytest.raises(ValueError, match=\"already locked\"):\n        lock(widget2)\n\n    # Releasing lock with widget2 should fail.\n    with pytest.raises(ValueError, match=\"you do not own this lock\"):\n        lock.release(widget2)\n\n    # Releasing lock with widget1 should succeed.\n    lock.release(widget1)\n    assert lock.available(widget2)\n    assert not lock.locked()\n", "def test_cursor_visibility(ax):\n    tool = widgets.Cursor(ax)\n    assert tool.visible\n    tool.visible = False\n    assert not tool.visible\n    tool.visible = True\n    assert tool.visible\n\n", "def test_lock_draw(ax, lock_owner):\n    lock = widgets.LockDraw()\n    widget1 = mock.Mock(spec=noop, return_value=None)\n    widget2 = mock.Mock(spec=noop, return_value=None)\n\n    if lock_owner is not None:\n        lock(widget1 if lock_owner == 'widget1' else widget2)\n        assert not lock.available(widget1)\n        assert not lock.available(widget2)\n    else:\n        assert lock.available(widget1)\n        assert lock.available(widget2)\n\n    if lock_owner == 'widget1':\n        with pytest.raises(ValueError, match='already locked'):\n            lock(widget2)\n    elif lock_owner == 'widget2':\n        with pytest.raises(ValueError, match='already locked'):\n            lock(widget1)\n    else:\n        lock(widget1)\n        assert lock.isowner(widget1)\n        assert not lock.isowner(widget2)\n\n    if lock_owner in [None, 'widget1']:\n        lock.release(widget1)\n        assert lock.available(widget1)\n        assert lock.available(widget2)\n    else:\n        with pytest.raises(ValueError, match='you do not own this lock'):\n            lock.release(widget1)\n", "def test_button_widget(ax):\n    on_clicked = mock.Mock(spec=noop, return_value=None)\n\n    button = widgets.Button(ax, \"Click Me\")\n    button.on_clicked(on_clicked)\n    \n    # Simulate a button press event\n    do_event(button, 'button_press_event', xdata=0.5, ydata=0.5, button=1)\n    do_event(button, 'button_release_event', xdata=0.5, ydata=0.5, button=1)\n\n    # Verify that the callback was called once\n    on_clicked.assert_called_once()\n    \n    # Check if button hover works\n    do_event(button, 'motion_notify_event', xdata=0.5, ydata=0.5)\n    assert mcolors.same_color(ax.get_facecolor(), button.hovercolor)\n    \n    do_event(button, 'motion_notify_event', xdata=1.5, ydata=1.5)\n    assert mcolors.same_color(ax.get_facecolor(), button.color)\n"], "sample_540": ["def test_adjusted_figsize():\n    # Test the adjusted_figsize function to ensure it returns expected figure sizes.\n    w, h, dpi, n = 5, 3, 100, 2\n    expected_w, expected_h = 5, 3\n    new_w, new_h = adjusted_figsize(w, h, dpi, n)\n    assert new_w == expected_w\n    assert new_h == expected_h\n\n    # Test with non-standard dpi and n values\n    w, h, dpi, n = 5, 3, 150, 3\n    new_w, new_h = adjusted_figsize(w, h, dpi, n)\n    assert new_w == 4.98  # Expected rounded value\n    assert new_h == 3.0033333333333334  # Expected rounded value\n\n    # Test the correct_roundoff functionality\n    w, h, dpi, n = 5, 3, 99, 5\n    new_w, new_h = adjusted_figsize(w, h, dpi, n)\n    assert new_w != 5  # Ensure the values are adjusted\n    assert new_h != 3  # Ensure the values are adjusted\n", "def test_adjusted_figsize():\n    w, h, dpi, n = 8, 6, 100, 2\n    new_w, new_h = adjusted_figsize(w, h, dpi, n)\n    assert new_w == 8\n    assert new_h == 6\n\n    # Check for rounding cases\n    w, h, dpi, n = 8.3, 6.2, 100, 2\n    new_w, new_h = adjusted_figsize(w, h, dpi, n)\n    assert round(new_w, 2) == 8.32\n    assert round(new_h, 2) == 6.20\n", "def test_adjusted_figsize():\n    # Test adjusted_figsize function for various inputs.\n    assert adjusted_figsize(10, 5, 100, 2) == (10, 5)\n    assert adjusted_figsize(10, 5, 100, 3) == (10, 5)\n    assert adjusted_figsize(10, 5, 100, 10) == (10, 5)\n    assert adjusted_figsize(10.5, 5.25, 200, 2) == (10.5, 5.25)\n    assert adjusted_figsize(10.1, 5.05, 100, 10) == (10.1, 5.05)\n", "def test_adjusted_figsize():\n    # Test adjusted_figsize function.\n    w, h, dpi, n = 5, 4, 100, 2\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert (wnew, hnew) == (5.0, 4.0)\n\n    # Test with a different n\n    n = 3\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert (wnew, hnew) == (5.0, 4.0)\n    \n    # Test edge case where rounding is required\n    w, h, dpi, n = 5.1, 4.1, 100, 2\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert (wnew, hnew) == (5.1, 4.1)\n", "def test_movie_writer_registry_iteration():\n    # Register a dummy writer\n    @animation.writers.register('dummy')\n    class DummyWriter(animation.AbstractMovieWriter):\n        @classmethod\n            return True\n        \n            pass\n\n            pass\n\n            pass\n    \n    # Ensure the dummy writer is in the registry and available\n    assert 'dummy' in animation.writers._registered\n    assert animation.writers.is_available('dummy')\n\n    # Iterate over available writers and check if 'dummy' is in the list\n    available_writers = list(animation.writers)\n    assert 'dummy' in available_writers\n"], "sample_541": ["def test_lockdraw():\n    lock = widgets.LockDraw()\n\n    # Initial state\n    assert lock._owner is None\n    assert lock.available(None)\n    assert not lock.locked()\n\n    # Acquire lock\n    obj1 = object()\n    lock(obj1)\n    assert lock._owner is obj1\n    assert not lock.available(None)\n    assert lock.locked()\n    assert lock.isowner(obj1)\n    assert not lock.isowner(object())\n\n    # Try to acquire lock again should raise ValueError\n    with pytest.raises(ValueError, match='already locked'):\n        lock(object())\n\n    # Release lock\n    lock.release(obj1)\n    assert lock._owner is None\n    assert lock.available(None)\n    assert not lock.locked()\n\n    # Try to release lock not owned should raise ValueError\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(obj1)\n\n    # Acquire lock with different object\n    obj2 = object()\n    lock(obj2)\n    assert lock._owner is obj2\n    assert not lock.available(obj1)\n    assert lock.available(obj2)\n", "def test_lock_draw():\n    lock = widgets.LockDraw()\n    widget1 = object()\n    widget2 = object()\n\n    assert lock.available(widget1)\n    lock(widget1)\n    assert not lock.available(widget2)\n    assert lock.available(widget1)\n    assert lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n    assert lock.locked()\n\n    with pytest.raises(ValueError, match='already locked'):\n        lock(widget2)\n    \n    lock.release(widget1)\n    assert lock.available(widget2)\n    assert not lock.locked()\n\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(widget2)\n", "def test_rectangle_selector_set_active(ax):\n    onselect = mock.Mock(spec=noop, return_value=None)\n    tool = widgets.RectangleSelector(ax, onselect, interactive=True)\n    \n    assert tool.get_active() is True  # Initially active\n    \n    # Deactivate the tool and verify\n    tool.set_active(False)\n    assert tool.get_active() is False\n    \n    # Try to create a rectangle with the tool deactivated\n    click_and_drag(tool, start=(10, 10), end=(50, 50))\n    assert not tool._selection_completed  # No selection should be completed\n    \n    # Activate the tool again\n    tool.set_active(True)\n    assert tool.get_active() is True\n    \n    # Now create a rectangle with the tool activated\n    click_and_drag(tool, start=(10, 10), end=(50, 50))\n    assert tool._selection_completed  # Selection should be completed\n    assert tool.extents == (10, 50, 10, 50)\n    onselect.assert_called_once()\n", "def test_lock_draw():\n    lock = widgets.LockDraw()\n\n    # Initial state\n    assert lock._owner is None\n    assert lock.available(None)\n    assert not lock.locked()\n\n    # Lock by an owner\n    lock('owner1')\n    assert lock._owner == 'owner1'\n    assert not lock.available('owner2')\n    assert lock.available('owner1')\n    assert lock.locked()\n\n    # Release by the owner\n    lock.release('owner1')\n    assert lock._owner is None\n    assert lock.available(None)\n    assert not lock.locked()\n\n    # Attempt to release by a non-owner should raise an error\n    lock('owner1')\n    with pytest.raises(ValueError):\n        lock.release('owner2')\n    lock.release('owner1')\n", "def test_slider_horizontal_and_vertical_set_val(ax):\n    fig, axs = plt.subplots(2, 1)\n    slider_h = widgets.Slider(ax=axs[0], label='Horiz', valmin=0, valmax=10, valinit=5, orientation='horizontal')\n    slider_v = widgets.Slider(ax=axs[1], label='Vert', valmin=0, valmax=10, valinit=5, orientation='vertical')\n\n    # Set horizontal slider value and check\n    slider_h.set_val(8)\n    assert slider_h.val == 8\n    assert_allclose(slider_h._handle.get_xdata(), [8])\n\n    # Set vertical slider value and check\n    slider_v.set_val(3)\n    assert slider_v.val == 3\n    assert_allclose(slider_v._handle.get_ydata(), [3])\n"], "sample_542": ["def test_text_contains_with_bbox():\n    fig = plt.figure()\n    ax = plt.axes()\n\n    mevent = MouseEvent('button_press_event', fig.canvas, 0.5, 0.5, 1, None)\n    txt = plt.text(\n        0.5, 0.5, 'test', ha='center', fontsize=20, bbox=dict(facecolor='red', edgecolor='black'))\n\n    fig.canvas.draw()\n\n    xs = np.linspace(0.25, 0.75, 10)\n    ys = np.linspace(0.25, 0.75, 10)\n    xs, ys = np.meshgrid(xs, ys)\n\n    for x, y in zip(xs.flat, ys.flat):\n        mevent.x, mevent.y = plt.gca().transAxes.transform([x, y])\n        contains, _ = txt.contains(mevent)\n        color = 'yellow' if contains else 'red'\n        ax.plot(x, y, 'o', color=color)\n\n    vl = ax.viewLim.frozen()\n    fig.canvas.draw()\n    ax.viewLim.set(vl)\n", "def test_text_repr_math_text():\n    # Test to ensure that repr works with math text\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, r'$E=mc^2$')\n    repr_str = repr(text)\n    assert \"Text(0.5, 0.5, '$E=mc^2$')\" in repr_str\n", "def test_text_backgroundcolor():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Test Background Color\", backgroundcolor=\"yellow\")\n    fig.canvas.draw()\n    bbox = text.get_window_extent()\n    assert text.get_bbox_patch() is not None\n    assert text.get_bbox_patch().get_facecolor() == (1.0, 1.0, 0.0, 1.0)\n", "def test_text_set_get_methods():\n    # Test setting and getting various properties of the Text class\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Test text\")\n\n    # Set properties\n    text.set_text(\"Updated text\")\n    text.set_color(\"blue\")\n    text.set_fontfamily(\"serif\")\n    text.set_fontsize(20)\n    text.set_fontstyle(\"italic\")\n    text.set_fontweight(\"bold\")\n    text.set_fontvariant(\"small-caps\")\n    text.set_fontstretch(\"expanded\")\n    text.set_horizontalalignment(\"right\")\n    text.set_verticalalignment(\"top\")\n    text.set_rotation(45)\n    text.set_wrap(True)\n    text.set_linespacing(1.5)\n\n    # Get properties and assert correctness\n    assert text.get_text() == \"Updated text\"\n    assert text.get_color() == \"blue\"\n    assert text.get_fontfamily() == [\"serif\"]\n    assert text.get_fontsize() == 20\n    assert text.get_fontstyle() == \"italic\"\n    assert text.get_fontweight() == \"bold\"\n    assert text.get_fontvariant() == \"small-caps\"\n    assert text.get_stretch() == \"expanded\"\n    assert text.get_horizontalalignment() == \"right\"\n    assert text.get_verticalalignment() == \"top\"\n    assert text.get_rotation() == 45\n    assert text.get_wrap() is True\n    assert text._linespacing == 1.5\n\n    # Draw to ensure no errors during rendering\n    fig.canvas.draw()\n", "def test_set_font_properties():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Sample text\")\n\n    fp = FontProperties(family='serif', style='italic', weight='bold', size=14)\n    text.set_fontproperties(fp)\n\n    assert text.get_fontfamily() == ['serif']\n    assert text.get_fontstyle() == 'italic'\n    assert text.get_fontweight() == 'bold'\n    assert text.get_fontsize() == 14\n\n    fig.canvas.draw()\n"], "sample_543": ["def test_lock_draw():\n    lock = widgets.LockDraw()\n\n    widget1 = object()\n    widget2 = object()\n\n    assert lock.available(widget1)\n    lock(widget1)\n    assert not lock.available(widget2)\n    assert lock.available(widget1)\n    assert lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n    assert lock.locked()\n\n    with pytest.raises(ValueError, match=\"already locked\"):\n        lock(widget2)\n\n    with pytest.raises(ValueError, match=\"you do not own this lock\"):\n        lock.release(widget2)\n\n    lock.release(widget1)\n    assert not lock.locked()\n    assert lock.available(widget2)\n", "def test_lock_draw():\n    canvas_mock = mock.Mock()\n    lock_draw = widgets.LockDraw()\n\n    assert lock_draw.available(canvas_mock)\n    assert not lock_draw.locked()\n    assert not lock_draw.isowner(canvas_mock)\n\n    lock_draw(canvas_mock)\n    assert not lock_draw.available(None)\n    assert lock_draw.locked()\n    assert lock_draw.isowner(canvas_mock)\n\n    with pytest.raises(ValueError, match='already locked'):\n        lock_draw(mock.Mock())\n\n    lock_draw.release(canvas_mock)\n    assert lock_draw.available(canvas_mock)\n    assert not lock_draw.locked()\n    assert not lock_draw.isowner(canvas_mock)\n\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock_draw.release(mock.Mock())\n", "def test_slider_set_val(ax):\n    slider = widgets.Slider(ax=ax, label='', valmin=0.0, valmax=24.0, valinit=12.0)\n    \n    # set value within range\n    slider.set_val(8.0)\n    assert slider.val == 8.0\n    \n    # set value below valmin\n    slider.set_val(-5.0)\n    assert slider.val == slider.valmin\n    \n    # set value above valmax\n    slider.set_val(30.0)\n    assert slider.val == slider.valmax\n    \n    # set value with snapping enabled\n    slider.valstep = 2.0\n    slider.set_val(5.3)\n    assert slider.val == 6.0\n", "def test_lockdraw():\n    lock = widgets.LockDraw()\n\n    class DummyWidget:\n        pass\n\n    widget1 = DummyWidget()\n    widget2 = DummyWidget()\n\n    assert lock.available(widget1)\n    assert lock.available(widget2)\n\n    lock(widget1)\n    assert lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n    assert lock.locked()\n    assert not lock.available(widget2)\n\n    with pytest.raises(ValueError, match=\"already locked\"):\n        lock(widget2)\n\n    with pytest.raises(ValueError, match=\"you do not own this lock\"):\n        lock.release(widget2)\n\n    lock.release(widget1)\n    assert not lock.locked()\n    assert lock.available(widget2)\n", "def test_cursor_widget_lock(ax):\n    tool = widgets.Cursor(ax, useblit=True)\n    do_event(tool, 'onmove', xdata=100, ydata=100)\n    assert tool.lineh.get_visible()\n    assert tool.linev.get_visible()\n\n    # Simulate acquiring widget lock and moving cursor\n    ax.figure.canvas.widgetlock(tool)\n    do_event(tool, 'onmove', xdata=150, ydata=150)\n    assert not tool.lineh.get_visible()\n    assert not tool.linev.get_visible()\n\n    # Release widget lock and check cursor visibility again\n    ax.figure.canvas.widgetlock.release(tool)\n    do_event(tool, 'onmove', xdata=100, ydata=100)\n    assert tool.lineh.get_visible()\n    assert tool.linev.get_visible()\n"], "sample_544": ["def test_axes_image_get_size():\n    ax = plt.gca()\n    im = AxesImage(ax)\n    data = np.arange(12, dtype=float).reshape((3, 4))\n    im.set_data(data)\n    assert im.get_size() == (3, 4)\n", "def test_axesimage_set_alpha():\n    fig, ax = plt.subplots()\n    im = AxesImage(ax)\n    z = np.arange(12, dtype=float).reshape((4, 3))\n    im.set_data(z)\n    im.set_alpha(0.5)\n    assert im._get_scalar_alpha() == 0.5\n    im.set_alpha(np.array([[0.2, 0.4, 0.6], [0.8, 1.0, 0.1], [0.3, 0.5, 0.7], [0.9, 0.2, 0.4]]))\n    assert im._alpha.shape == (4, 3)\n    with pytest.raises(TypeError):\n        im.set_alpha([0.5, 0.5])\n", "def test_composite_images_empty():\n    # Test composite_images with an empty list of images\n    output, offset_x, offset_y = mimage.composite_images([], None)\n    assert output.shape == (0, 0, 4)\n    assert output.dtype == np.uint8\n    assert offset_x == 0\n    assert offset_y == 0\n", "def test_set_alpha(fig_test, fig_ref):\n    \"\"\"Test setting alpha value for AxesImage.\"\"\"\n    arr = np.random.rand(10, 10)\n    ax = fig_test.subplots()\n    im = ax.imshow(arr)\n    im.set_alpha(0.5)\n\n    ax = fig_ref.subplots()\n    im = ax.imshow(arr, alpha=0.5)\n", "def test_pil_to_array():\n    # Create a simple RGB image using PIL.\n    img = Image.new('RGB', (10, 10), color='red')\n    np_array = mimage.pil_to_array(img)\n    assert np_array.shape == (10, 10, 3), \"Shape mismatch for RGB image\"\n    assert np_array.dtype == np.uint8, \"Data type mismatch for RGB image\"\n\n    # Create a simple RGBA image using PIL.\n    img = Image.new('RGBA', (10, 10), color='red')\n    np_array = mimage.pil_to_array(img)\n    assert np_array.shape == (10, 10, 4), \"Shape mismatch for RGBA image\"\n    assert np_array.dtype == np.uint8, \"Data type mismatch for RGBA image\"\n\n    # Create a simple grayscale image using PIL.\n    img = Image.new('L', (10, 10), color='gray')\n    np_array = mimage.pil_to_array(img)\n    assert np_array.shape == (10, 10), \"Shape mismatch for grayscale image\"\n    assert np_array.dtype == np.uint8, \"Data type mismatch for grayscale image\"\n\n    # Create a simple 16-bit grayscale image using PIL.\n    img = Image.new('I;16', (10, 10))\n    np_array = mimage.pil_to_array(img)\n    assert np_array.shape == (10, 10), \"Shape mismatch for 16-bit grayscale image\"\n    assert np_array.dtype == np.uint16, \"Data type mismatch for 16-bit grayscale image\"\n"], "sample_545": ["def test_subfigure_facecolor():\n    fig = plt.figure(figsize=(4, 3))\n    sf1, sf2 = fig.subfigures(1, 2)\n    sf1.set_facecolor('red')\n    sf2.set_facecolor('blue')\n\n    assert sf1.get_facecolor() == (1.0, 0.0, 0.0, 1.0)  # red in RGBA\n    assert sf2.get_facecolor() == (0.0, 0.0, 1.0, 1.0)  # blue in RGBA\n\n    sf1.subplots()\n    sf2.subplots()\n    fig.suptitle(\"Subfigures with different facecolors\")\n\n    assert sf1.get_facecolor() == (1.0, 0.0, 0.0, 1.0)  # red in RGBA\n    assert sf2.get_facecolor() == (0.0, 0.0, 1.0, 1.0)  # blue in RGBA\n", "def test_subfigure_picking():\n    # Test if picking events are correctly registered in subfigures\n    fig = plt.figure()\n    subfig = fig.add_subfigure(111)\n    ax = subfig.subplots()\n    scatter = ax.scatter([0.5], [0.5], s=100, picker=True)\n    fig.canvas.draw()\n\n    assert not fig.stale\n\n    mouse_event = SimpleNamespace(\n        x=ax.bbox.x0 + ax.bbox.width / 2,\n        y=ax.bbox.y0 + ax.bbox.height / 2,\n        inaxes=ax, guiEvent=None\n    )\n    fig.pick(mouse_event)\n    assert not fig.stale\n\n    # Ensure that the scatter object is detected by the picking event\n    assert scatter.contains(mouse_event)[0]\n", "def test_subplot_params_update():\n    params = SubplotParams(left=0.1, bottom=0.2, right=0.9, top=0.8, wspace=0.3, hspace=0.4)\n    \n    # Verify initial values\n    assert params.left == 0.1\n    assert params.bottom == 0.2\n    assert params.right == 0.9\n    assert params.top == 0.8\n    assert params.wspace == 0.3\n    assert params.hspace == 0.4\n    \n    # Update some parameters and verify changes\n    params.update(left=0.2, hspace=0.5)\n    assert params.left == 0.2\n    assert params.hspace == 0.5\n    \n    # Verify unchanged parameters\n    assert params.bottom == 0.2\n    assert params.right == 0.9\n    assert params.top == 0.8\n    assert params.wspace == 0.3\n\n    # Test invalid update that should raise ValueError\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=1.0)\n\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=1.0)\n", "def test_subplotparams():\n    # Test SubplotParams initialization\n    params = mpl.figure.SubplotParams(left=0.1, bottom=0.2, right=0.9, top=0.8, wspace=0.5, hspace=0.7)\n    assert params.left == 0.1\n    assert params.bottom == 0.2\n    assert params.right == 0.9\n    assert params.top == 0.8\n    assert params.wspace == 0.5\n    assert params.hspace == 0.7\n\n    # Test updating values\n    params.update(left=0.2, bottom=0.3)\n    assert params.left == 0.2\n    assert params.bottom == 0.3\n    assert params.right == 0.9\n    assert params.top == 0.8\n    assert params.wspace == 0.5\n    assert params.hspace == 0.7\n\n    # Test invalid updates\n    with pytest.raises(ValueError):\n        params.update(left=1.0, right=0.5)\n    with pytest.raises(ValueError):\n        params.update(bottom=0.9, top=0.1)\n", "def test_suplabels():\n    fig = plt.figure()\n    fig.suptitle('Main Title', x=0.5, y=0.98, ha='center', va='top', fontsize=20, weight='bold')\n    fig.supxlabel('X-Axis Title', x=0.5, y=0.01, ha='center', va='bottom', fontsize=15, weight='light')\n    fig.supylabel('Y-Axis Title', x=0.02, y=0.5, ha='left', va='center', rotation='vertical', fontsize=15, weight='light')\n\n    assert fig._suptitle.get_text() == 'Main Title'\n    assert fig._suptitle.get_position() == (0.5, 0.98)\n    assert fig._suptitle.get_fontsize() == 20\n    assert fig._suptitle.get_weight() == 'bold'\n\n    assert fig._supxlabel.get_text() == 'X-Axis Title'\n    assert fig._supxlabel.get_position() == (0.5, 0.01)\n    assert fig._supxlabel.get_fontsize() == 15\n    assert fig._supxlabel.get_weight() == 'light'\n\n    assert fig._supylabel.get_text() == 'Y-Axis Title'\n    assert fig._supylabel.get_position() == (0.02, 0.5)\n    assert fig._supylabel.get_rotation() == 90  # rotation='vertical'\n    assert fig._supylabel.get_fontsize() == 15\n    assert fig._supylabel.get_weight() == 'light'\n"], "sample_546": ["def test_suptitle():\n    fig = plt.figure()\n    fig.suptitle(\"My Figure Title\", fontsize=14, fontweight='bold')\n    assert fig._suptitle is not None\n    assert fig._suptitle.get_text() == \"My Figure Title\"\n    assert fig._suptitle.get_fontsize() == 14\n    assert fig._suptitle.get_fontweight() == 'bold'\n    plt.close(fig)\n", "def test_suptitle():\n    fig = plt.figure()\n    title_text = \"This is a super title\"\n    suptitle = fig.suptitle(title_text, fontsize=16)\n    \n    assert suptitle.get_text() == title_text\n    assert suptitle.get_fontsize() == 16\n    assert suptitle.get_position() == (0.5, 0.98)\n    \n    plt.close(fig)\n", "def test_figure_base_contains():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    fig.canvas.draw()\n\n    # Test contains method with a point inside the figure\n    inside, info = fig.contains(MouseEvent('button_press_event', fig.canvas, 100, 100, MouseButton.LEFT))\n    assert inside\n\n    # Test contains method with a point outside the figure\n    inside, info = fig.contains(MouseEvent('button_press_event', fig.canvas, -10, -10, MouseButton.LEFT))\n    assert not inside\n", "def test_suptitle_update():\n    fig = plt.figure()\n    title = fig.suptitle(\"Initial Title\")\n    assert title.get_text() == \"Initial Title\"\n    \n    # Update the title\n    title = fig.suptitle(\"Updated Title\")\n    assert title.get_text() == \"Updated Title\"\n    plt.close(fig)\n", "def test_subfigure_addition():\n    fig = plt.figure()\n    subfig = fig.add_subfigure(GridSpec(1, 1)[0])\n\n    assert isinstance(subfig, SubFigure)\n    assert subfig in fig.subfigs\n    assert subfig.figure == fig\n    assert subfig._parent == fig\n\n    ax = subfig.add_subplot(111)\n    assert ax in subfig.axes\n    assert ax.get_figure() == fig\n\n    subfig2 = subfig.add_subfigure(GridSpec(1, 1)[0])\n    assert isinstance(subfig2, SubFigure)\n    assert subfig2 in subfig.subfigs\n    assert subfig2.figure == fig\n    assert subfig2._parent == subfig\n\n    ax2 = subfig2.add_subplot(111)\n    assert ax2 in subfig2.axes\n    assert ax2.get_figure() == fig\n"], "sample_547": ["def test_annotationbbox_set_get_fontsize():\n    fig, ax = plt.subplots()\n    da = DrawingArea(20, 20, 0, 0, clip=True)\n    ab = AnnotationBbox(da, [.5, .5], xybox=(-0.2, 0.5), xycoords='data',\n                        boxcoords=\"axes fraction\", box_alignment=(0., .5),\n                        arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ab)\n\n    # Set fontsize and check if it is correctly updated\n    ab.set_fontsize(14)\n    assert ab.get_fontsize() == 14\n\n    # Change fontsize and check again\n    ab.set_fontsize(20)\n    assert ab.get_fontsize() == 20\n\n    # Check default fontsize\n    ab.set_fontsize(None)\n    assert ab.get_fontsize() == plt.rcParams[\"legend.fontsize\"]\n", "def test_offsetbox_contains():\n    fig, ax = plt.subplots()\n    size = 100\n    da = DrawingArea(size, size, clip=True)\n    bg = mpatches.Rectangle((0, 0), size, size,\n                            facecolor='#CCCCCC',\n                            edgecolor='None',\n                            linewidth=0)\n    line = mlines.Line2D([-size*.5, size*1.5], [size/2, size/2],\n                         color='black',\n                         linewidth=10)\n    anchored_box = AnchoredOffsetbox(\n        loc='center',\n        child=da,\n        pad=0.,\n        frameon=False,\n        bbox_to_anchor=(.5, .5),\n        bbox_transform=ax.transAxes,\n        borderpad=0.)\n\n    da.add_artist(bg)\n    da.add_artist(line)\n    ax.add_artist(anchored_box)\n\n    fig.canvas.draw()\n    \n    event_inside = MouseEvent(\"button_press_event\", fig.canvas, 0.5, 0.5, MouseButton.LEFT)\n    event_outside = MouseEvent(\"button_press_event\", fig.canvas, 1.5, 1.5, MouseButton.LEFT)\n    \n    assert da.contains(event_inside)[0]\n    assert not da.contains(event_outside)[0]\n", "def test_offsetimage_zoom():\n    fig, ax = plt.subplots()\n\n    data = np.random.rand(10, 10)\n    zoom_levels = [0.5, 1, 2]\n\n    for zoom in zoom_levels:\n        offset_image = OffsetImage(data, zoom=zoom)\n        ab = AnnotationBbox(offset_image, (0.5, 0.5), xycoords='data')\n        ax.add_artist(ab)\n        fig.canvas.draw()\n\n        expected_width = data.shape[1] * zoom\n        expected_height = data.shape[0] * zoom\n        bbox = offset_image.get_bbox(fig.canvas.get_renderer())\n        assert_allclose(bbox.width, expected_width, atol=0.1)\n        assert_allclose(bbox.height, expected_height, atol=0.1)\n", "def test_annotationbbox_update_positions():\n    fig, ax = plt.subplots()\n    \n    da = DrawingArea(20, 20, 0, 0, clip=True)\n    ab = AnnotationBbox(da, (0.5, 0.5), xybox=(1, 1), xycoords='data',\n                        boxcoords='offset points', arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ab)\n\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n\n    # Save original positions\n    original_xybox = ab.xybox\n    original_boxcoords = ab.boxcoords\n\n    # Update positions\n    ab.update_positions(renderer)\n\n    # Ensure positions are updated correctly\n    assert ab.xybox == original_xybox\n    assert ab.boxcoords == original_boxcoords\n\n    # Check if the arrow is correctly positioned\n    assert ab.arrow_patch is not None\n    arrow_start, arrow_end = ab.arrow_patch.get_path().vertices\n    assert_allclose(arrow_start, ab.offsetbox.get_window_extent(renderer).p0, atol=1e-2)\n    assert_allclose(arrow_end, ab._get_position_xy(renderer), atol=1e-2)\n", "def test_offsetbox_get_set_methods():\n    fig, ax = plt.subplots()\n    ob = OffsetBox()\n    ob.set_width(10)\n    assert ob.width == 10\n    ob.set_height(20)\n    assert ob.height == 20\n\n    ob.set_offset((5, 5))\n    assert ob.get_offset(Bbox.from_bounds(0, 0, 10, 10), None) == (5, 5)\n\n    child1 = TextArea(\"child1\")\n    child2 = TextArea(\"child2\")\n    ob._children = [child1, child2]\n    assert ob.get_children() == [child1, child2]\n    assert ob.get_visible_children() == [child1, child2]\n\n    child1.set_visible(False)\n    assert ob.get_visible_children() == [child2]\n\n    bbox, offsets = ob._get_bbox_and_child_offsets(None)\n    assert bbox.width == 10\n    assert bbox.height == 20\n"], "sample_548": ["def test_colorbar_removal_cleanup():\n    \"\"\"\n    Test that removing a colorbar properly disconnects callbacks\n    and cleans up attributes.\n    \"\"\"\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable(norm=mcolors.Normalize(), cmap='viridis')\n    cb = fig.colorbar(sm, ax=ax)\n    \n    # Ensure callbacks are connected\n    assert sm.callbacks.callbacks\n    assert cb._extend_cid1 in cb.ax.callbacks.callbacks['xlim_changed']\n    assert cb._extend_cid2 in cb.ax.callbacks.callbacks['ylim_changed']\n    \n    # Remove the colorbar and check cleanup\n    cb.remove()\n    \n    assert sm.callbacks.callbacks == {}\n    assert cb._extend_cid1 not in cb.ax.callbacks.callbacks['xlim_changed']\n    assert cb._extend_cid2 not in cb.ax.callbacks.callbacks['ylim_changed']\n    assert sm.colorbar is None\n    assert sm.colorbar_cid is None\n", "def test_colorbar_constructor_default_values():\n    fig, ax = plt.subplots()\n    mappable = cm.ScalarMappable(norm=mcolors.Normalize(), cmap='viridis')\n    cbar = Colorbar(ax, mappable)\n\n    # Check default values\n    assert cbar.orientation == 'vertical'\n    assert cbar.ticklocation == 'right'\n    assert cbar.extend == 'neither'\n    assert cbar.alpha is None\n    assert cbar.drawedges is False\n    assert cbar.spacing == 'uniform'\n    assert cbar.extendfrac is None\n    assert cbar.extendrect is False\n    assert cbar.label == ''\n    assert cbar.cmap == mappable.cmap\n    assert cbar.norm == mappable.norm\n    assert cbar.ax == ax\n", "def test_colorbar_remove_patches():\n    # Test that removing a colorbar also removes its patches\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cb = fig.colorbar(im)\n    assert len(cb._extend_patches) == 2  # by default, both ends are extended\n    cb.remove()\n    assert len(cb._extend_patches) == 0  # patches should be removed\n", "def test_colorbar_set_alpha():\n    \"\"\"Test setting alpha transparency for Colorbar.\"\"\"\n    fig, ax = plt.subplots()\n    data = np.linspace(0, 1, 100).reshape(10, 10)\n    cmap = mpl.colormaps[\"viridis\"]\n    norm = mpl.colors.Normalize(vmin=0, vmax=1)\n    img = ax.imshow(data, cmap=cmap, norm=norm)\n\n    # Create colorbar with alpha set to 0.5\n    cbar = fig.colorbar(img, ax=ax)\n    cbar.set_alpha(0.5)\n    cbar.draw_all()  # redraw to apply alpha\n\n    # Check if the alpha property is correctly set\n    assert cbar.alpha == 0.5\n    assert all(patch.get_alpha() == 0.5 for patch in cbar.ax.patches)\n\n    # Set alpha to None and verify\n    cbar.set_alpha(None)\n    cbar.draw_all()  # redraw to apply alpha\n    assert cbar.alpha is None\n", "def test_colorbar_set_alpha():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]], interpolation=\"none\")\n    cbar = fig.colorbar(im)\n    \n    # Test setting a scalar alpha value\n    cbar.set_alpha(0.5)\n    assert cbar.alpha == 0.5\n    \n    # Test setting an array-like alpha value\n    alpha_array = np.array([[0.1, 0.2], [0.3, 0.4]])\n    cbar.set_alpha(alpha_array)\n    assert cbar.alpha is None\n\n    # Ensure that the ScalarMappable's alpha is also updated\n    assert im.get_alpha() == 1.0\n\n    # Draw the figure to ensure no errors with alpha settings\n    fig.draw_without_rendering()\n"], "sample_549": ["def test_weak_or_strong_ref():\n    class Foo:\n            pass\n\n        pass\n\n    foo_instance = Foo()\n    weak_ref = cbook._weak_or_strong_ref(foo_instance.method, dummy_callback)\n    \n    # If weak references are supported, weak_ref should be an instance of weakref.WeakMethod\n    if isinstance(weak_ref, weakref.WeakMethod):\n        assert weak_ref() is foo_instance.method\n    else:  # Otherwise, it should be a _StrongRef\n        assert isinstance(weak_ref, cbook._StrongRef)\n        assert weak_ref() is foo_instance.method\n\n    # Test with a non-method callable to ensure _StrongRef is used\n        pass\n\n    strong_ref = cbook._weak_or_strong_ref(standalone_function, dummy_callback)\n    assert isinstance(strong_ref, cbook._StrongRef)\n    assert strong_ref() is standalone_function\n", "def test_weak_or_strong_ref():\n    class TestClass:\n            pass\n        \n    # Test with a bound method, should return a WeakMethod\n    obj = TestClass()\n    ref = cbook._weak_or_strong_ref(obj.method, None)\n    assert isinstance(ref, weakref.WeakMethod)\n    assert ref() == obj.method\n\n    # Test with an unbound function, should return a _StrongRef\n        pass\n\n    ref = cbook._weak_or_strong_ref(standalone_function, None)\n    assert isinstance(ref, cbook._StrongRef)\n    assert ref() == standalone_function\n", "def test_silent_list_repr():\n    # Test an empty silent_list\n    sl = cbook.silent_list(type=None)\n    assert repr(sl) == \"<an empty list>\"\n\n    # Test a non-empty silent_list with explicit type\n    sl = cbook.silent_list(type=\"Line2D\", seq=[1, 2, 3])\n    assert repr(sl) == \"<a list of 3 Line2D objects>\"\n\n    # Test a non-empty silent_list with inferred type\n    class Dummy:\n        pass\n\n    sl = cbook.silent_list(type=None, seq=[Dummy(), Dummy()])\n    assert repr(sl) == \"<a list of 2 Dummy objects>\"\n\n    # Test silent_list with mixed types\n    sl = cbook.silent_list(type=None, seq=[Dummy(), 1])\n    assert repr(sl) == \"<a list of 2 Dummy objects>\"\n\n    # Test silent_list with empty sequence but explicit type\n    sl = cbook.silent_list(type=\"Line2D\", seq=[])\n    assert repr(sl) == \"<a list of 0 Line2D objects>\"\n", "def test_delete_masked_points_with_different_lengths():\n    # First argument of length 4, second argument of length 5\n    a1 = ['a', 'b', 'c', 'd']\n    a2 = [1, 2, 3, 4, 5]\n    result1, result2 = delete_masked_points(a1, a2)\n    assert result1 == a1\n    assert result2 == a2\n\n    # First argument of length 5, second argument of length 4\n    a1 = ['a', 'b', 'c', 'd', 'e']\n    a2 = [1, 2, 3, 4]\n    result1, result2 = delete_masked_points(a1, a2)\n    assert result1 == a1\n    assert result2 == a2\n", "def test_silent_list():\n    # Test with an empty list and no type specified\n    sl = cbook.silent_list(None)\n    assert repr(sl) == \"<an empty list>\"\n\n    # Test with a list of integers and type specified\n    sl = cbook.silent_list(\"int\", [1, 2, 3])\n    assert repr(sl) == \"<a list of 3 int objects>\"\n    assert len(sl) == 3\n    assert sl[0] == 1\n    assert sl[1] == 2\n    assert sl[2] == 3\n\n    # Test with a list of strings and no type specified\n    sl = cbook.silent_list(None, [\"a\", \"b\", \"c\"])\n    assert repr(sl) == \"<a list of 3 str objects>\"\n    assert len(sl) == 3\n    assert sl[0] == \"a\"\n    assert sl[1] == \"b\"\n    assert sl[2] == \"c\"\n\n    # Test with a list of mixed types and no type specified\n    sl = cbook.silent_list(None, [1, \"b\", 3.0])\n    assert repr(sl) == \"<a list of 3 int objects>\"\n    assert len(sl) == 3\n    assert sl[0] == 1\n    assert sl[1] == \"b\"\n    assert sl[2] == 3.0\n\n    # Test with a list of matplotlib Line2D objects\n    import matplotlib.lines as mlines\n    line1 = mlines.Line2D([], [])\n    line2 = mlines.Line2D([], [])\n    sl = cbook.silent_list(\"Line2D\", [line1, line2])\n    assert repr(sl) == \"<a list of 2 Line2D objects>\"\n"], "sample_550": ["def test_shared_axes_limits():\n    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n    ax1.plot([0, 1], [0, 1])\n    ax2.plot([0, 1], [1, 0])\n\n    ax1.set_xlim(0, 2)\n    assert ax2.get_xlim() == (0, 2)\n\n    ax2.set_xlim(-1, 3)\n    assert ax1.get_xlim() == (-1, 3)\n\n    ax1.set_autoscalex_on(True)\n    ax2.set_autoscalex_on(True)\n    ax1.autoscale()\n    assert ax1.get_xlim() == (0, 1)\n    assert ax2.get_xlim() == (0, 1)\n", "def test_process_plot_format():\n    # Test valid format strings\n    assert _process_plot_format('ko') == ('None', 'o', (0, 0, 0, 1))\n    assert _process_plot_format('.b') == ('None', '.', (0, 0, 1, 1))\n    assert _process_plot_format('r--') == ('--', 'None', (1, 0, 0, 1))\n    assert _process_plot_format('C2--') == ('--', 'None', (0.4980392156862745, 0.4980392156862745, 0, 1))\n\n    # Test invalid format strings\n    with pytest.raises(ValueError, match=r\"'--.' is not a valid format string\"):\n        _process_plot_format('--.')\n\n    with pytest.raises(ValueError, match=r\"'foobar' is not a valid format string\"):\n        _process_plot_format('foobar')\n\n    with pytest.raises(ValueError, match=r\"'??' is neither a data key nor a valid format string\"):\n        _process_plot_format('??', ambiguous_fmt_datakey=True)\n\n    # Test grayscale differentiation\n    assert _process_plot_format('1.0') == ('None', 'None', (1.0, 1.0, 1.0, 1.0))\n\n    # Test if linestyle or marker not defined in fmt\n    assert _process_plot_format('k', ambiguous_fmt_datakey=False) == ('None', 'None', (0, 0, 0, 1))\n", "def test_process_plot_format():\n    assert _process_plot_format('ko') == ('-', 'o', (0.0, 0.0, 0.0, 1.0))  # black circles\n    assert _process_plot_format('.b') == ('None', '.', (0.0, 0.0, 1.0, 1.0))  # blue dots\n    assert _process_plot_format('r--') == ('--', 'None', (1.0, 0.0, 0.0, 1.0))  # red dashed lines\n    assert _process_plot_format('C2--') == ('--', 'None', (0.0, 0.4980392156862745, 0.0, 1.0))  # third color in cycle, dashed lines\n\n    with pytest.raises(ValueError, match=\"two linestyle symbols\"):\n        _process_plot_format('r---')\n    with pytest.raises(ValueError, match=\"two marker symbols\"):\n        _process_plot_format('oo')\n    with pytest.raises(ValueError, match=\"two color symbols\"):\n        _process_plot_format('rb')\n    with pytest.raises(ValueError, match=\"unrecognized character 'z'\"):\n        _process_plot_format('r-z')\n", "def test__process_plot_format():\n    # Test the `_process_plot_format` function with various cases\n    assert _process_plot_format('ko') == (None, 'o', (0, 0, 0, 1))\n    assert _process_plot_format('.b') == (None, '.', (0, 0, 1, 1))\n    assert _process_plot_format('r--') == ('--', None, (1, 0, 0, 1))\n    assert _process_plot_format('C2--') == ('--', None, (0.0, 0.0, 1.0, 1.0))\n    assert _process_plot_format('1') == (None, '1', None)\n    \n    # Test an invalid format string\n    with pytest.raises(ValueError, match=r\".*unrecognized character.*\"):\n        _process_plot_format('invalid')\n        \n    # Test ambiguous format string with ambiguous_fmt_datakey=True\n    assert _process_plot_format('1', ambiguous_fmt_datakey=True) == (None, '1', None)\n    assert _process_plot_format('1', ambiguous_fmt_datakey=False) == (None, '1', None)\n", "def test_process_plot_format():\n    # Test color formats\n    assert _process_plot_format('r') == (None, None, (1.0, 0.0, 0.0, 1.0))\n    assert _process_plot_format('C1') == (None, None, mcolors.to_rgba('C1'))\n    assert _process_plot_format('.5') == (None, None, (0.5, 0.5, 0.5, 1.0))\n    \n    # Test linestyle formats\n    assert _process_plot_format('--') == ('--', None, None)\n    assert _process_plot_format('-.') == ('-.', None, None)\n    \n    # Test marker formats\n    assert _process_plot_format('o') == (None, 'o', None)\n    assert _process_plot_format('x') == (None, 'x', None)\n\n    # Test combined formats\n    assert _process_plot_format('r--') == ('--', None, (1.0, 0.0, 0.0, 1.0))\n    assert _process_plot_format('bo') == (None, 'o', (0.0, 0.0, 1.0, 1.0))\n    assert _process_plot_format('g-.') == ('-.', None, (0.0, 0.5, 0.0, 1.0))\n\n    # Test invalid format\n    with pytest.raises(ValueError, match=\"is not a valid format string\"):\n        _process_plot_format('invalid')\n\n    # Test ambiguous format/data key\n    with pytest.raises(ValueError, match=\"is neither a data key nor a valid format string\"):\n        _process_plot_format('invalid', ambiguous_fmt_datakey=True)\n"], "sample_551": ["def test_text3d_properties():\n    text = art3d.Text3D(x=1, y=2, z=3, text='Hello 3D', zdir='x', color='red')\n    \n    assert text.get_position_3d() == (1, 2, 3)\n    assert np.array_equal(text._dir_vec, [1, 0, 0])\n    assert text.get_text() == 'Hello 3D'\n    assert text.get_color() == 'red'\n    \n    text.set_position_3d((4, 5, 6), zdir='y')\n    assert text.get_position_3d() == (4, 5, 6)\n    assert np.array_equal(text._dir_vec, [0, 1, 0])\n    \n    text.set_z(7)\n    assert text.get_position_3d() == (4, 5, 7)\n    \n    text.set_3d_properties(z=8, zdir='z')\n    assert text.get_position_3d() == (4, 5, 8)\n    assert np.array_equal(text._dir_vec, [0, 0, 1])\n", "def test_text3d_properties():\n    text = art3d.Text3D(1, 2, 3, \"test\", zdir='y')\n    assert text.get_position_3d() == (1, 2, 3)\n    text.set_position_3d((4, 5, 6), zdir='x')\n    assert text.get_position_3d() == (4, 5, 6)\n    text.set_z(10)\n    assert text.get_position_3d() == (4, 5, 10)\n    text.set_3d_properties(z=12, zdir='z')\n    assert text.get_position_3d() == (4, 5, 12)\n", "def test_text3d_properties():\n    # Test Text3D object properties and methods.\n    text = art3d.Text3D(x=1, y=2, z=3, text='Test Text', zdir='y')\n    assert text.get_position_3d() == (1, 2, 3)\n    assert np.array_equal(text._dir_vec, np.array((0, 1, 0)))\n\n    text.set_position_3d((4, 5, 6), zdir='x')\n    assert text.get_position_3d() == (4, 5, 6)\n    assert np.array_equal(text._dir_vec, np.array((1, 0, 0)))\n\n    text.set_z(7)\n    assert text.get_position_3d()[2] == 7\n\n    text.set_3d_properties(z=8, zdir='z')\n    assert text.get_position_3d()[2] == 8\n    assert np.array_equal(text._dir_vec, np.array((0, 0, 1)))\n", "def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array([1, 2, 3]))\n    try:\n        art3d.get_dir_vector('invalid')\n    except ValueError as e:\n        assert str(e) == \"'x', 'y', 'z', None or vector of length 3 expected\"\n", "def test_text3d_position():\n    # Test the position setting and getting for Text3D object\n    text = art3d.Text3D(1, 2, 3, 'Hello', zdir='x')\n    assert text.get_position_3d() == (1, 2, 3)\n    \n    text.set_position_3d((4, 5, 6), zdir='y')\n    assert text.get_position_3d() == (4, 5, 6)\n    \n    text.set_z(9)\n    assert text.get_position_3d() == (4, 5, 9)\n    \n    text.set_3d_properties(7, zdir='z')\n    assert text.get_position_3d() == (4, 5, 7)\n"], "sample_552": ["def test_suplabels():\n    fig = plt.figure()\n    fig.suplabels('Test Supxlabel', {'name': '_supxlabel', 'x0': 0.5, 'y0': 0.01,\n                'ha': 'center', 'va': 'bottom', 'rotation': 0,\n                'size': 'figure.labelsize', 'weight': 'figure.labelweight'})\n    fig.suplabels('Test Suptitle', {'name': '_suptitle', 'x0': 0.5, 'y0': 0.98,\n                'ha': 'center', 'va': 'top', 'rotation': 0,\n                'size': 'figure.titlesize', 'weight': 'figure.titleweight'})\n    fig.suplabels('Test Supylabel', {'name': '_supylabel', 'x0': 0.02, 'y0': 0.5,\n                'ha': 'left', 'va': 'center', 'rotation': 'vertical',\n                'rotation_mode': 'anchor', 'size': 'figure.labelsize',\n                'weight': 'figure.labelweight'})\n\n    # Check if suplabels were added correctly\n    assert fig._supxlabel.get_text() == 'Test Supxlabel'\n    assert fig._supxlabel.get_position() == (0.5, 0.01)\n    assert fig._suptitle.get_text() == 'Test Suptitle'\n    assert fig._suptitle.get_position() == (0.5, 0.98)\n    assert fig._supylabel.get_text() == 'Test Supylabel'\n    assert fig._supylabel.get_position() == (0.02, 0.5)\n\n    # Check if attributes were set correctly\n    assert fig._supxlabel.get_horizontalalignment() == 'center'\n    assert fig._supxlabel.get_verticalalignment() == 'bottom'\n    assert fig._supxlabel.get_rotation() == 0\n    assert fig._suptitle.get_horizontalalignment() == 'center'\n    assert fig._suptitle.get_verticalalignment() == 'top'\n    assert fig._suptitle.get_rotation() == 0\n    assert fig._supylabel.get_horizontalalignment() == 'left'\n    assert fig._supylabel.get_verticalalignment() == 'center'\n    assert fig._supylabel.get_rotation() == 'vertical'\n    assert fig._supylabel.get_rotation_mode() == 'anchor'\n", "def test_update_subplotparams():\n    sp = SubplotParams(left=0.1, bottom=0.2, right=0.9, top=0.8, wspace=0.5, hspace=0.4)\n    \n    # Check initial values\n    assert sp.left == 0.1\n    assert sp.bottom == 0.2\n    assert sp.right == 0.9\n    assert sp.top == 0.8\n    assert sp.wspace == 0.5\n    assert sp.hspace == 0.4\n\n    # Update values\n    sp.update(left=0.2, bottom=0.3)\n    assert sp.left == 0.2\n    assert sp.bottom == 0.3\n    assert sp.right == 0.9\n    assert sp.top == 0.8\n    assert sp.wspace == 0.5\n    assert sp.hspace == 0.4\n\n    # Test invalid updates\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        sp.update(left=1.0)\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        sp.update(bottom=1.0)\n", "def test_subplotparams_update():\n    params = SubplotParams(left=0.2, bottom=0.2, right=0.8, top=0.8, wspace=0.5, hspace=0.5)\n    assert params.left == 0.2\n    assert params.bottom == 0.2\n    assert params.right == 0.8\n    assert params.top == 0.8\n    assert params.wspace == 0.5\n    assert params.hspace == 0.5\n\n    params.update(left=0.1, bottom=0.1)\n    assert params.left == 0.1\n    assert params.bottom == 0.1\n    assert params.right == 0.8  # unchanged\n    assert params.top == 0.8    # unchanged\n\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=0.9)\n    \n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=0.9)\n", "def test_figure_add_artist():\n    fig = plt.figure()\n\n    # Add a Rectangle artist directly to the figure\n    rect = plt.Rectangle((0.1, 0.1), 0.5, 0.5, color='blue')\n    fig.add_artist(rect)\n\n    # Check if the artist was added correctly\n    assert rect in fig.artists\n    assert rect.get_figure() is fig\n\n    # Check the artist properties\n    assert rect.get_x() == 0.1\n    assert rect.get_y() == 0.1\n    assert rect.get_width() == 0.5\n    assert rect.get_height() == 0.5\n    assert rect.get_facecolor() == (0.0, 0.0, 1.0, 1.0)  # RGBA for blue\n\n    # Remove the artist and check if it was removed\n    fig.artists.remove(rect)\n    assert rect not in fig.artists\n", "def test_subfigure_layout_engine_swap():\n    # Test that changing layout engines on subfigures behaves correctly\n    fig = plt.figure(layout='constrained')\n    subfig = fig.subfigures(1, 2)\n\n    for sf in subfig:\n        assert isinstance(sf.get_layout_engine(), ConstrainedLayoutEngine)\n\n    # Change layout engine on subfigures to 'tight'\n    for sf in subfig:\n        sf.set_layout_engine('tight')\n        assert isinstance(sf.get_layout_engine(), TightLayoutEngine)\n\n    # Change layout engine on subfigures to 'none'\n    for sf in subfig:\n        sf.set_layout_engine('none')\n        assert sf.get_layout_engine() is None\n\n    # Verify that changing layout engine on main figure does not affect subfigures\n    fig.set_layout_engine('constrained')\n    for sf in subfig:\n        assert sf.get_layout_engine() is None\n"], "sample_553": ["def test_adjusted_figsize():\n    # Test adjusted_figsize function with different inputs\n    w, h, dpi, n = 5, 5, 100, 2\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 5\n    assert hnew == 5\n\n    w, h, dpi, n = 5, 5, 100, 3\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == pytest.approx(5, rel=1e-5)\n    assert hnew == pytest.approx(5, rel=1e-5)\n\n    w, h, dpi, n = 7.5, 5, 200, 4\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == pytest.approx(7.5, rel=1e-5)\n    assert hnew == pytest.approx(5, rel=1e-5)\n", "def test_adjusted_figsize():\n    # Test the adjusted_figsize function.\n    w, h = 4.5, 3.5\n    dpi = 100\n    n = 2\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == pytest.approx(4.5, rel=1e-2)\n    assert hnew == pytest.approx(3.5, rel=1e-2)\n    \n    n = 5\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == pytest.approx(4.5, rel=1e-1)\n    assert hnew == pytest.approx(3.5, rel=1e-1)\n", "def test_adjusted_figsize():\n    # Test adjusted_figsize function with various inputs\n    w, h, dpi, n = 10, 5, 100, 2\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 10\n    assert hnew == 5\n\n    w, h, dpi, n = 10.1, 5.1, 100, 2\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew != 10.1\n    assert hnew != 5.1\n\n    w, h, dpi, n = 10.1, 5.1, 100, 3\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 10.1\n    assert hnew == 5.1\n\n    w, h, dpi, n = 12.8, 7.6, 100, 5\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 12.8\n    assert hnew == 7.6\n", "def test_adjusted_figsize():\n    # Test adjusted_figsize function with various inputs and ensure correct rounding\n    w, h = 10, 5\n    dpi = 100\n    n = 2\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 10\n    assert hnew == 5\n\n    w, h = 10.3, 5.7\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 10.3\n    assert hnew == 5.5\n\n    w, h = 10.7, 5.3\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 10.5\n    assert hnew == 5.5\n\n    w, h = 10.25, 5.75\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 10.25\n    assert hnew == 5.75\n\n    w, h = 10.75, 5.25\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 10.75\n    assert hnew == 5.25\n", "def test_adjusted_figsize():\n    w, h = 10, 5\n    dpi = 100\n    n = 10\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew * dpi % n == 0\n    assert hnew * dpi % n == 0\n"], "sample_554": ["def test_text_fontstyle():\n    fig, ax = plt.subplots()\n    t = ax.text(0.5, 0.5, 'Italic Text', fontstyle='italic')\n    assert t.get_fontstyle() == 'italic'\n\n    t.set_fontstyle('normal')\n    assert t.get_fontstyle() == 'normal'\n\n    t.set_fontstyle('oblique')\n    assert t.get_fontstyle() == 'oblique'\n\n    with pytest.raises(ValueError):\n        t.set_fontstyle('invalid_style')\n", "def test_text_font_properties():\n    fig = plt.figure()\n    txt = plt.text(0.5, 0.5, \"Test text\")\n\n    # Test default font properties\n    assert txt.get_fontfamily() == ['sans-serif']\n    assert txt.get_fontname() == 'DejaVu Sans'\n    assert txt.get_fontstyle() == 'normal'\n    assert txt.get_fontsize() == 12\n    assert txt.get_fontvariant() == 'normal'\n    assert txt.get_fontweight() == 'normal'\n    assert txt.get_stretch() == 'normal'\n\n    # Test setting individual font properties\n    txt.set_fontfamily('serif')\n    assert txt.get_fontfamily() == ['serif']\n\n    txt.set_fontname('Times New Roman')\n    assert txt.get_fontname() == 'Times New Roman'\n\n    txt.set_fontstyle('italic')\n    assert txt.get_fontstyle() == 'italic'\n\n    txt.set_fontsize(20)\n    assert txt.get_fontsize() == 20\n\n    txt.set_fontvariant('small-caps')\n    assert txt.get_fontvariant() == 'small-caps'\n\n    txt.set_fontweight('bold')\n    assert txt.get_fontweight() == 'bold'\n\n    txt.set_fontstretch('expanded')\n    assert txt.get_stretch() == 'expanded'\n", "def test_text_set_backgroundcolor():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Test background color', fontsize=20)\n    text.set_backgroundcolor('blue')\n    assert text.get_bbox_patch().get_facecolor() == (0, 0, 1, 1)  # RGBA for blue\n\n    text.set_backgroundcolor('red')\n    assert text.get_bbox_patch().get_facecolor() == (1, 0, 0, 1)  # RGBA for red\n\n    fig.canvas.draw()  # Ensure it draws without error\n", "def test_set_backgroundcolor():\n    fig, ax = plt.subplots()\n    txt = ax.text(0.5, 0.5, 'Test text')\n    fig.canvas.draw()\n\n    # Set background color to red and verify\n    txt.set_backgroundcolor('red')\n    fig.canvas.draw()\n    bbox = txt.get_bbox_patch()\n    assert bbox.get_facecolor() == (1.0, 0.0, 0.0, 1.0)  # RGBA for red\n\n    # Set background color to blue and verify\n    txt.set_backgroundcolor('blue')\n    fig.canvas.draw()\n    bbox = txt.get_bbox_patch()\n    assert bbox.get_facecolor() == (0.0, 0.0, 1.0, 1.0)  # RGBA for blue\n\n    # Remove background color and verify\n    txt.set_backgroundcolor(None)\n    fig.canvas.draw()\n    bbox = txt.get_bbox_patch()\n    assert bbox is not None  # Patch still exists but has no color\n    assert bbox.get_facecolor() == (0.0, 0.0, 0.0, 0.0)  # RGBA for transparent\n", "def test_set_rotation_mode():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Test Rotation Mode\", rotation=45)\n    text.set_rotation_mode(\"anchor\")\n    assert text.get_rotation_mode() == \"anchor\"\n    text.set_rotation_mode(\"default\")\n    assert text.get_rotation_mode() == \"default\"\n    text.set_rotation_mode(None)\n    assert text.get_rotation_mode() is None\n\n    with pytest.raises(ValueError, match=\"rotation_mode must be 'anchor', 'default' or None\"):\n        text.set_rotation_mode(\"invalid\")\n"], "sample_555": ["def test_contains():\n    rect = Rectangle((0, 0), 2, 2)\n    points = [(0.5, 0.5), (1.5, 1.5), (2.5, 2.5), (-0.5, -0.5)]\n    expected = [True, True, False, False]\n    result = [rect.contains_point(point) for point in points]\n    assert result == expected\n\n    circle = Circle((0, 0), radius=1)\n    points = [(0, 0), (0.5, 0.5), (1, 0), (1.5, 1.5)]\n    expected = [True, True, True, False]\n    result = [circle.contains_point(point) for point in points]\n    assert result == expected\n\n    ellipse = Ellipse((0, 0), 2, 1)\n    points = [(0, 0), (1, 0.5), (2, 1), (0.5, 0.5)]\n    expected = [True, True, False, True]\n    result = [ellipse.contains_point(point) for point in points]\n    assert result == expected\n\n    wedge = Wedge((0, 0), 1, 0, 90)\n    points = [(0, 0), (0.5, 0.5), (1, 1), (-0.5, 0.5)]\n    expected = [True, True, False, False]\n    result = [wedge.contains_point(point) for point in points]\n    assert result == expected\n\n    polygon = Polygon([(0, 0), (2, 0), (2, 2), (0, 2)])\n    points = [(0, 0), (1, 1), (2, 2), (2.5, 2.5)]\n    expected = [True, True, True, False]\n    result = [polygon.contains_point(point) for point in points]\n    assert result == expected\n", "def test_contains():\n    rect = Rectangle((0, 0), 1, 1)\n    event = SimpleNamespace(x=0.5, y=0.5)\n    assert rect.contains(event)[0] is True\n\n    event = SimpleNamespace(x=1.5, y=0.5)\n    assert rect.contains(event)[0] is False\n\n    event = SimpleNamespace(x=0.5, y=1.5)\n    assert rect.contains(event)[0] is False\n\n    circle = Circle((0, 0), radius=1)\n    event = SimpleNamespace(x=0.5, y=0.5)\n    assert circle.contains(event)[0] is True\n\n    event = SimpleNamespace(x=1.5, y=0.5)\n    assert circle.contains(event)[0] is False\n\n    event = SimpleNamespace(x=0.5, y=1.5)\n    assert circle.contains(event)[0] is False\n", "def test_patch_properties():\n    \"\"\"\n    Test setting and getting various properties of Patch.\n    \"\"\"\n    p = Patch()\n    # Antialiased\n    assert p.get_antialiased() == rcParams['patch.antialiased']\n    p.set_antialiased(False)\n    assert not p.get_antialiased()\n    p.set_antialiased(True)\n    assert p.get_antialiased()\n\n    # Edge color\n    assert p.get_edgecolor() == colors.to_rgba(rcParams['patch.edgecolor'])\n    p.set_edgecolor('blue')\n    assert p.get_edgecolor() == colors.to_rgba('blue')\n    p.set_edgecolor(None)\n    assert p.get_edgecolor() == colors.to_rgba(rcParams['patch.edgecolor'])\n\n    # Face color\n    assert p.get_facecolor() == colors.to_rgba(rcParams['patch.facecolor'])\n    p.set_facecolor('red')\n    assert p.get_facecolor() == colors.to_rgba('red')\n    p.set_facecolor(None)\n    assert p.get_facecolor() == colors.to_rgba(rcParams['patch.facecolor'])\n\n    # Line width\n    assert p.get_linewidth() == rcParams['patch.linewidth']\n    p.set_linewidth(5)\n    assert p.get_linewidth() == 5.0\n    p.set_linewidth(None)\n    assert p.get_linewidth() == rcParams['patch.linewidth']\n\n    # Line style\n    assert p.get_linestyle() == 'solid'\n    p.set_linestyle('--')\n    assert p.get_linestyle() == '--'\n    p.set_linestyle(None)\n    assert p.get_linestyle() == 'solid'\n\n    # Alpha\n    p.set_alpha(0.5)\n    assert p.get_facecolor()[3] == 0.5\n    assert p.get_edgecolor()[3] == 0.5\n    p.set_alpha(None)\n    assert p.get_facecolor()[3] == 1.0\n    assert p.get_edgecolor()[3] == 1.0\n\n    # Hatch\n    assert p.get_hatch() is None\n    p.set_hatch('/')\n    assert p.get_hatch() == '/'\n    p.set_hatch(None)\n    assert p.get_hatch() is None\n\n    # Capstyle\n    assert p.get_capstyle() == 'butt'\n    p.set_capstyle('round')\n    assert p.get_capstyle() == 'round'\n\n    # Joinstyle\n    assert p", "def test_patch_initialization():\n    # Test default initialization\n    patch = Patch()\n    assert patch.get_edgecolor() == colors.to_rgba(mpl.rcParams['patch.edgecolor'])\n    assert patch.get_facecolor() == colors.to_rgba(mpl.rcParams['patch.facecolor'])\n    assert patch.get_linewidth() == float(mpl.rcParams['patch.linewidth'])\n    assert patch.get_antialiased() == mpl.rcParams['patch.antialiased']\n    \n    # Test initialization with parameters\n    patch = Patch(edgecolor=\"red\", facecolor=\"blue\", linewidth=2.5, antialiased=True)\n    assert patch.get_edgecolor() == colors.to_rgba(\"red\")\n    assert patch.get_facecolor() == colors.to_rgba(\"blue\")\n    assert patch.get_linewidth() == 2.5\n    assert patch.get_antialiased() is True\n\n    # Test warning for overriding color properties\n    with pytest.warns(UserWarning, match=\"Setting the 'color' property will override the edgecolor or facecolor properties.\"):\n        patch = Patch(color='green', edgecolor='red', facecolor='blue')\n    assert patch.get_edgecolor() == colors.to_rgba(\"green\")\n    assert patch.get_facecolor() == colors.to_rgba(\"green\")\n", "def test_patch_contains():\n    # Test the contains method of Patch with a Rectangle\n    rect = Rectangle((0.1, 0.1), 0.8, 0.8)\n    fig, ax = plt.subplots()\n    ax.add_patch(rect)\n\n    # Mock a mouse event inside the rectangle\n    class MockEvent:\n            self.x, self.y = x, y\n\n    inside_event = MockEvent(0.5, 0.5)\n    outside_event = MockEvent(1.5, 1.5)\n\n    assert rect.contains(inside_event)[0] is True\n    assert rect.contains(outside_event)[0] is False\n\n"], "sample_556": ["def test_get_children():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    line, = ax.plot([0, 1], [1, 0], label='test line')\n    text = fig.text(0.5, 0.5, 'test text')\n    rect = mpl.patches.Rectangle((0.1, 0.1), 0.5, 0.5, edgecolor='r', facecolor='none')\n    fig.patches.append(rect)\n    \n    children = fig.get_children()\n    assert fig.patch in children\n    assert ax in children\n    assert line in children\n    assert text in children\n    assert rect in children\n", "def test_figure_aspect_ratio_invalid_values():\n    with pytest.raises(ValueError, match='must be positive finite'):\n        plt.figaspect(-1)\n    with pytest.raises(ValueError, match='must be positive finite'):\n        plt.figaspect(np.nan)\n    with pytest.raises(ValueError, match='must be positive finite'):\n        plt.figaspect(np.inf)\n\n    fig = plt.figure()\n    with pytest.raises(ValueError, match='must be positive finite'):\n        figaspect(-1)\n    with pytest.raises(ValueError, match='must be positive finite'):\n        figaspect(np.nan)\n    with pytest.raises(ValueError, match='must be positive finite'):\n        figaspect(np.inf)\n", "def test_subplotparams_update():\n    # Test the SubplotParams update function\n    subplot_params = mpl.figure.SubplotParams(left=0.2, bottom=0.3, right=0.8, top=0.9, wspace=0.5, hspace=0.5)\n    assert subplot_params.left == 0.2\n    assert subplot_params.bottom == 0.3\n    assert subplot_params.right == 0.8\n    assert subplot_params.top == 0.9\n    assert subplot_params.wspace == 0.5\n    assert subplot_params.hspace == 0.5\n    \n    subplot_params.update(left=0.1, bottom=0.2)\n    assert subplot_params.left == 0.1\n    assert subplot_params.bottom == 0.2\n    assert subplot_params.right == 0.8\n    assert subplot_params.top == 0.9\n    assert subplot_params.wspace == 0.5\n    assert subplot_params.hspace == 0.5\n\n    with pytest.raises(ValueError, match=\"left cannot be >= right\"):\n        subplot_params.update(left=0.9)\n        \n    with pytest.raises(ValueError, match=\"bottom cannot be >= top\"):\n        subplot_params.update(bottom=1)\n", "def test_subplotparams_update():\n    params = SubplotParams(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n    assert params.left == 0.1\n    assert params.bottom == 0.1\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    params.update(left=0.2, bottom=0.2)\n    assert params.left == 0.2\n    assert params.bottom == 0.2\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=1.0)\n    \n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=1.0)\n", "def test_subplotparams_update():\n    # Test that the SubplotParams update method correctly updates parameters\n    sp = mpl.figure.SubplotParams(left=0.1, bottom=0.2, right=0.9, top=0.8, wspace=0.3, hspace=0.4)\n    \n    # Check initial values\n    assert sp.left == 0.1\n    assert sp.bottom == 0.2\n    assert sp.right == 0.9\n    assert sp.top == 0.8\n    assert sp.wspace == 0.3\n    assert sp.hspace == 0.4\n    \n    # Update some parameters\n    sp.update(left=0.2, top=0.9, wspace=0.5)\n    \n    # Check updated values\n    assert sp.left == 0.2\n    assert sp.bottom == 0.2  # Unchanged\n    assert sp.right == 0.9  # Unchanged\n    assert sp.top == 0.9\n    assert sp.wspace == 0.5\n    assert sp.hspace == 0.4  # Unchanged\n\n    # Check for ValueError when left >= right or bottom >= top\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        sp.update(left=1.0)\n    \n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        sp.update(bottom=1.0)\n"], "sample_557": ["def test_add_artist_invalid():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    line = plt.Line2D([0, 1], [0, 1], transform=None)  # line without a transform\n\n    # Test adding an artist without a transform\n    with pytest.raises(AttributeError, match=\"Artist instance has no transform\"):\n        fig.add_artist(line)\n    \n    # Correct the transform and add it to the figure\n    line.set_transform(fig.transFigure)\n    fig.add_artist(line)\n    assert line in fig.artists\n", "def test_axes_stack():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    ax2 = fig.add_subplot(212)\n    ax_stack = fig._axstack\n    \n    assert ax_stack.current() == ax2  # The last added axes should be the current one\n    assert ax_stack.as_list() == [ax1, ax2]\n    \n    ax_stack.bubble(ax1)\n    assert ax_stack.current() == ax1  # Axes1 should be bubbled to the top\n    \n    ax_stack.remove(ax1)\n    assert ax_stack.current() == ax2  # Removing ax1 should make ax2 the current one\n    assert ax_stack.as_list() == [ax2]\n", "def test_add_artist_clipping():\n    fig = plt.figure()\n    rect = plt.Rectangle((0.1, 0.1), 0.8, 0.8, linewidth=1, edgecolor='r', facecolor='none')\n    artist = plt.Line2D([0, 1], [0, 1], linewidth=2)\n    \n    # Add artist without clipping\n    fig.add_artist(artist, clip=False)\n    assert artist.get_clip_path() is None\n\n    # Add artist with clipping\n    fig.add_artist(rect, clip=True)\n    assert rect.get_clip_path() == fig.patch\n", "def test_subplotparams_update():\n    # Initialize with default values\n    params = mpl.figure.SubplotParams()\n\n    # Update with new values\n    params.update(left=0.2, bottom=0.2, right=0.8, top=0.8, wspace=0.5, hspace=0.5)\n    \n    assert params.left == 0.2\n    assert params.bottom == 0.2\n    assert params.right == 0.8\n    assert params.top == 0.8\n    assert params.wspace == 0.5\n    assert params.hspace == 0.5\n\n    # Test invalid update\n    with pytest.raises(ValueError):\n        params.update(left=0.9, right=0.1)\n        \n    with pytest.raises(ValueError):\n        params.update(bottom=0.9, top=0.1)\n", "def test_subplotparams_update():\n    # Test to ensure the update method in SubplotParams correctly updates parameters\n    params = SubplotParams(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n    assert params.left == 0.1\n    assert params.bottom == 0.1\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    # Update some parameters\n    params.update(left=0.2, top=0.8, wspace=0.1)\n    assert params.left == 0.2\n    assert params.top == 0.8\n    assert params.wspace == 0.1\n    assert params.bottom == 0.1  # unchanged\n    assert params.right == 0.9  # unchanged\n    assert params.hspace == 0.2  # unchanged\n\n    # Test invalid updates\n    with pytest.raises(ValueError, match=\"left cannot be >= right\"):\n        params.update(left=1.0)\n\n    with pytest.raises(ValueError, match=\"bottom cannot be >= top\"):\n        params.update(bottom=1.0)\n"], "sample_558": ["def test_grid_get_set_methods():\n    fig = plt.figure()\n    grid = Grid(fig, 111, nrows_ncols=(2, 2))\n\n    # Test get_geometry\n    assert grid.get_geometry() == (2, 2)\n\n    # Test set_axes_pad and get_axes_pad\n    grid.set_axes_pad((0.3, 0.4))\n    assert grid.get_axes_pad() == (0.3, 0.4)\n\n    # Test set_aspect and get_aspect\n    grid.set_aspect(True)\n    assert grid.get_aspect() == True\n    grid.set_aspect(False)\n    assert grid.get_aspect() == False\n\n    # Test set_label_mode\n    grid.set_label_mode(\"all\")\n    for ax in grid:\n        assert ax.xaxis.get_tick_params(which='both')['labelbottom'] == True\n        assert ax.yaxis.get_tick_params(which='both')['labelleft'] == True\n\n    grid.set_label_mode(\"1\")\n    assert grid.axes_llc.xaxis.get_tick_params(which='both')['labelbottom'] == True\n    assert grid.axes_llc.yaxis.get_tick_params(which='both')['labelleft'] == True\n    for ax in grid:\n        if ax != grid.axes_llc:\n            assert ax.xaxis.get_tick_params(which='both')['labelbottom'] == False\n            assert ax.yaxis.get_tick_params(which='both')['labelleft'] == False\n\n    # Test get_divider\n    assert isinstance(grid.get_divider(), Divider)\n", "def test_grid_set_axes_pad():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 3))\n    initial_pad = grid.get_axes_pad()\n    new_pad = (0.1, 0.15)\n    grid.set_axes_pad(new_pad)\n    updated_pad = grid.get_axes_pad()\n    assert initial_pad != updated_pad\n    assert updated_pad == new_pad\n", "def test_grid_set_and_get_aspect():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2))\n    # Test default aspect is False\n    assert not grid.get_aspect()\n    # Change aspect to True and verify\n    grid.set_aspect(True)\n    assert grid.get_aspect()\n    # Set aspect back to False and verify\n    grid.set_aspect(False)\n    assert not grid.get_aspect()\n", "def test_axes_grid_set_axes_pad():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2))\n    initial_pad = grid.get_axes_pad()\n    new_pad = (0.1, 0.2)\n    grid.set_axes_pad(new_pad)\n    updated_pad = grid.get_axes_pad()\n    \n    assert initial_pad != new_pad, \"Initial pad should differ from new pad\"\n    assert updated_pad == new_pad, \"Updated pad should match new pad\"\n", "def test_grid_set_get_aspect():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2))\n    assert grid.get_aspect() is False\n    grid.set_aspect(True)\n    assert grid.get_aspect() is True\n"], "sample_559": ["def test_indicate_inset_zoom():\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    ax_inset = ax.inset_axes([0.5, 0.5, 0.4, 0.4])\n    ax_inset.plot(range(10))\n    rect, connects = ax.indicate_inset_zoom(ax_inset)\n    \n    assert isinstance(rect, mpatches.Rectangle)\n    assert len(connects) == 4\n    for connect in connects:\n        assert isinstance(connect, mpatches.ConnectionPatch)\n        assert not connect.get_visible()  # Default visibility of connection patches\n\n    # Test visibility toggling\n    connects[0].set_visible(True)\n    assert connects[0].get_visible()\n\n    fig.canvas.draw()  # Smoke test to ensure drawing works without error\n", "def test_set_title():\n    fig, ax = plt.subplots()\n    ax.set_title(\"Main Title\")\n    assert ax.get_title() == \"Main Title\"\n    ax.set_title(\"Left Title\", loc='left')\n    assert ax.get_title(loc='left') == \"Left Title\"\n    ax.set_title(\"Right Title\", loc='right')\n    assert ax.get_title(loc='right') == \"Right Title\"\n\n    title = ax.set_title(\"New Main Title\", fontdict={'fontsize': 14, 'fontweight': 'bold'}, loc='center', pad=10, color='red')\n    assert ax.get_title() == \"New Main Title\"\n    assert title.get_fontsize() == 14\n    assert title.get_fontweight() == 'bold'\n    assert title.get_position()[1] == 1.0\n    assert title.get_color() == 'red'\n", "def test_inset_axes_zorder():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    \n    ins1 = inset_axes(ax, width=1, height=1, loc='upper right', zorder=10)\n    ins2 = inset_axes(ax, width=1, height=1, loc='upper left', zorder=5)\n    \n    assert ins1.get_zorder() == 10\n    assert ins2.get_zorder() == 5\n    assert ax.get_zorder() < ins1.get_zorder()\n    assert ax.get_zorder() < ins2.get_zorder()\n", "def test_axes_inset_axes():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1, 2, 3], [0, 1, 0, 1])\n\n    # Create an inset axes within the main plot\n    inset_ax = ax.inset_axes([0.5, 0.5, 0.4, 0.4])\n    inset_ax.plot([0, 1, 2, 3], [1, 0, 1, 0])\n\n    # Assert that the inset axes has been added to the main axes\n    assert inset_ax in ax.child_axes\n\n    # Assert the position of the inset axes\n    expected_position = [0.5, 0.5, 0.4, 0.4]\n    np.testing.assert_array_almost_equal(inset_ax.get_position().bounds, expected_position)\n\n    # Test with transform set to ax.transData\n    inset_ax2 = ax.inset_axes([0.5, 1, 0.4, 0.4], transform=ax.transData)\n    inset_ax2.plot([0, 1, 2, 3], [1, 0, 1, 0])\n\n    # Assert that the inset axes has been added to the main axes\n    assert inset_ax2 in ax.child_axes\n\n    # Test that setting zorder works correctly\n    inset_ax3 = ax.inset_axes([0.1, 0.1, 0.2, 0.2], zorder=10)\n    inset_ax3.plot([0, 1, 2, 3], [1, 0, 1, 0])\n    assert inset_ax3.get_zorder() == 10\n\n    plt.close(fig)\n", "def test_axes_legend_handles_labels():\n    fig, ax = plt.subplots()\n    lines = [ax.plot([0, 1], label=f'line {i}')[0] for i in range(3)]\n    handles, labels = ax.get_legend_handles_labels()\n    assert handles == lines\n    assert labels == [f'line {i}' for i in range(3)]\n\n    # Test with explicit handles and labels\n    handles, labels = ax.get_legend_handles_labels({'line 0': lines[0]})\n    assert handles == [lines[0]]\n    assert labels == ['line 0']\n"], "sample_560": ["def test_draggable_legend_finalize_offset():\n    # Test that the finalize_offset method updates the legend location correctly.\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], label='Line')\n    leg = ax.legend()\n    draggable_leg = leg.set_draggable(True)\n    original_loc = leg._loc\n\n    # Simulate dragging and finalizing the offset\n    draggable_leg.finalize_offset()\n    new_loc = leg._loc\n\n    assert original_loc != new_loc\n", "def test_legend_auto_update_loc():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend(draggable=True, loc='upper right')\n\n    orig_loc = leg._get_loc()\n    leg._draggable._update_loc((0.1, 0.1))\n    new_loc = leg._get_loc()\n\n    assert orig_loc != new_loc\n    assert new_loc == (0.1, 0.1)\n", "def test_draggable_legend_update_loc():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], label='test')\n    leg = ax.legend()\n    draggable_leg = leg.set_draggable(True, update='loc')\n    \n    # Simulate dragging to a new location\n    draggable_leg.finalize_offset()\n    new_loc = draggable_leg.get_loc_in_canvas()\n    leg_loc = leg._loc\n    \n    # Check if the location is updated as expected\n    assert leg_loc == new_loc\n", "def test_draggable_legend():\n    \"\"\"\n    Test that the DraggableLegend can be created, toggled, and behaves as expected.\n    \"\"\"\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='line')\n    legend = ax.legend()\n\n    # Check initial state\n    assert not legend.get_draggable()\n\n    # Enable dragging and check state\n    legend.set_draggable(True)\n    assert legend.get_draggable()\n    \n    # Test dragging update with 'loc'\n    draggable = legend.set_draggable(True, update='loc')\n    assert draggable is not None\n    assert draggable._update == 'loc'\n    \n    # Test dragging update with 'bbox'\n    draggable = legend.set_draggable(True, update='bbox')\n    assert draggable is not None\n    assert draggable._update == 'bbox'\n    \n    # Disable dragging and check state\n    legend.set_draggable(False)\n    assert not legend.get_draggable()\n", "def test_set_loc_invalid_type():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend()\n    with pytest.raises(ValueError, match='loc must be string, coordinate tuple, or an integer 0-10, not 5.5'):\n        leg.set_loc(5.5)\n    with pytest.raises(ValueError, match='loc must be string, coordinate tuple, or an integer 0-10, not \\\\[1, 2, 3\\\\]'):\n        leg.set_loc([1, 2, 3])\n    with pytest.raises(ValueError, match=\"loc must be string, coordinate tuple, or an integer 0-10, not {'loc': 'best'}\"):\n        leg.set_loc({'loc': 'best'})\n"], "sample_561": ["def test_marker_init_with_dict():\n    marker_dict = {\n        'marker': 'o',\n        'fillstyle': 'full',\n        'transform': Affine2D().rotate_deg(45),\n        'capstyle': 'round',\n        'joinstyle': 'bevel'\n    }\n    marker = markers.MarkerStyle(**marker_dict)\n    assert marker.get_marker() == 'o'\n    assert marker.get_fillstyle() == 'full'\n    assert marker.get_user_transform() == Affine2D().rotate_deg(45)\n    assert marker.get_capstyle() == 'round'\n    assert marker.get_joinstyle() == 'bevel'\n", "def test_marker_custom_marker():\n    path = Path([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]], [Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY])\n    marker = markers.MarkerStyle(path)\n    assert marker.get_marker() == path\n    assert marker.get_path() == path\n    assert marker.get_transform().contains_branch(Affine2D().scale(0.5 / np.max(np.abs(path.vertices))))\n", "def test_marker_set_custom_marker():\n    # Custom marker path\n    custom_path = Path([[0, 0], [1, 1], [1, 0]])\n    marker_style = markers.MarkerStyle(custom_path)\n    \n    # Check if the custom marker path was set correctly\n    assert marker_style.get_path() == custom_path\n\n    # Check if the transform for the custom marker is set correctly\n    rescale = np.max(np.abs(custom_path.vertices))\n    expected_transform = Affine2D().scale(0.5 / rescale)\n    assert marker_style.get_transform() == expected_transform.frozen()\n", "def test_marker_paths(marker, expected_path, expected_filled):\n    marker_style = markers.MarkerStyle(marker)\n    assert np.allclose(marker_style.get_path().vertices, expected_path.vertices)\n    assert marker_style.is_filled() == expected_filled\n", "def test_marker_is_filled():\n    filled_markers = markers.MarkerStyle.filled_markers\n    for marker in filled_markers:\n        marker_style = markers.MarkerStyle(marker=marker)\n        assert marker_style.is_filled(), f\"Marker {marker} should be filled\"\n    \n    non_filled_markers = ['+', 'x', '|', '_', markers.TICKLEFT, markers.TICKRIGHT, markers.TICKUP, markers.TICKDOWN]\n    for marker in non_filled_markers:\n        marker_style = markers.MarkerStyle(marker=marker)\n        assert not marker_style.is_filled(), f\"Marker {marker} should not be filled\"\n"], "sample_562": ["def test_line2d_str_repr():\n    # Test the string representation of Line2D\n    line = mlines.Line2D([0, 1, 2], [2, 1, 0])\n    assert str(line) == \"Line2D((0,2),(1,1),...,(2,0))\"\n\n    line.set_label(\"test_label\")\n    assert str(line) == \"Line2D(test_label)\"\n\n    line.set_data([], [])\n    assert str(line) == \"Line2D()\"\n\n    line.set_data([0.5], [0.5])\n    assert str(line) == \"Line2D((0.5,0.5))\"\n", "def test_line2d_str_representation():\n    line_with_label = mlines.Line2D([0, 1], [0, 1], label=\"TestLine\")\n    line_no_data = mlines.Line2D([], [])\n    line_short_data = mlines.Line2D([0, 1], [0, 1])\n    line_long_data = mlines.Line2D([0, 1, 2, 3], [0, 1, 2, 3])\n\n    assert str(line_with_label) == \"Line2D(TestLine)\"\n    assert str(line_no_data) == \"Line2D()\"\n    assert str(line_short_data) == \"Line2D((0,0),(1,1),(2,2),(3,3))\"\n    assert str(line_long_data) == \"Line2D((0,0),(1,1),...,(3,3))\"\n", "def test_set_and_get_solid_capstyle():\n    line = mlines.Line2D([], [])\n    for capstyle in ['butt', 'round', 'projecting']:\n        line.set_solid_capstyle(capstyle)\n        assert line.get_solid_capstyle() == capstyle\n", "def test_set_marker_properties():\n    line = mlines.Line2D([], [])\n    line.set_markeredgecolor('blue')\n    assert line.get_markeredgecolor() == 'blue'\n    line.set_markerfacecolor('green')\n    assert line.get_markerfacecolor() == 'green'\n    line.set_markerfacecoloralt('yellow')\n    assert line.get_markerfacecoloralt() == 'yellow'\n    line.set_markeredgewidth(2.5)\n    assert line.get_markeredgewidth() == 2.5\n    line.set_markersize(10)\n    assert line.get_markersize() == 10\n", "def test_invalid_markevery_values():\n    line = mlines.Line2D([], [])\n    with pytest.raises(ValueError, match=\"markevery=.* is not a recognized value\"):\n        line.set_markevery(\"invalid\")\n    with pytest.raises(ValueError, match=\"markevery=.* is iterable but not a valid numpy fancy index\"):\n        line.set_markevery([True, False, \"invalid\"])\n    with pytest.raises(ValueError, match=\"markevery=.* is a tuple with len 2, but its second element is not an int or a float\"):\n        line.set_markevery((0, \"invalid\"))\n    with pytest.raises(ValueError, match=\"markevery=.* is a tuple with len 2 and second element is an int, but the first element is not an int\"):\n        line.set_markevery((0.5, 5))\n"], "sample_563": ["def test_drawing_area_get_transform():\n    # Test to ensure that the correct transform is returned\n    da = DrawingArea(100, 100)\n    assert isinstance(da.get_transform(), mtransforms.CompositeGenericTransform)\n", "def test_offsetbox_contains():\n    fig, ax = plt.subplots()\n    size = 50\n    da = DrawingArea(size, size)\n    bg = mpatches.Rectangle((0, 0), size, size, facecolor='gray')\n    da.add_artist(bg)\n    ab = AnchoredOffsetbox(loc='center', child=da, frameon=False)\n    ax.add_artist(ab)\n    fig.canvas.draw()\n\n    # Create a MouseEvent at the center of the anchored box (50, 50)\n    x, y = ax.transData.transform((0.5, 0.5))\n    event = MouseEvent(\"button_press_event\", fig.canvas, x, y, MouseButton.LEFT)\n\n    # Check that the contains method works correctly\n    contains, details = ab.contains(event)\n    assert contains\n\n    # Check a point outside the anchored box\n    event = MouseEvent(\"button_press_event\", fig.canvas, x + 100, y + 100, MouseButton.LEFT)\n    contains, details = ab.contains(event)\n    assert not contains\n", "def test_offsetimage_properties():\n    # Test for setting and getting properties of OffsetImage\n    data = np.random.rand(10, 10)\n    zoom = 2\n    offset_image = OffsetImage(data, zoom=zoom)\n\n    # Check initial properties\n    assert np.array_equal(offset_image.get_data(), data)\n    assert offset_image.get_zoom() == zoom\n\n    # Change properties\n    new_data = np.random.rand(5, 5)\n    new_zoom = 3\n    offset_image.set_data(new_data)\n    offset_image.set_zoom(new_zoom)\n\n    # Verify changed properties\n    assert np.array_equal(offset_image.get_data(), new_data)\n    assert offset_image.get_zoom() == new_zoom\n", "def test_offsetbox_get_set_methods():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    ab = AnchoredOffsetbox(loc=\"center\", child=da)\n\n    # Test set_offset and get_offset methods\n    ab.set_offset((10, 20))\n    assert ab.get_offset(Bbox.from_bounds(0, 0, 100, 100), None) == (10, 20)\n\n    # Test set_width and set_height methods\n    da.set_width(200)\n    da.set_height(300)\n    assert da.width == 200\n    assert da.height == 300\n\n    # Test get_visible_children method\n    assert da.get_visible_children() == []\n\n    # Test get_children method\n    children = [mpatches.Rectangle((0, 0), 50, 50)]\n    da._children = children\n    assert da.get_children() == children\n\n    # Test contains method\n    event = namedtuple('MouseEvent', ['x', 'y', 'canvas'])\n    event.x, event.y, event.canvas = 50, 50, fig.canvas\n    assert not da.contains(event)[0]\n", "def test_offsetbox_set_offset_callable():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    \n        return width / 2, height / 2\n\n    da.set_offset(dynamic_offset)\n    anchored_box = AnchoredOffsetbox(loc='center', child=da)\n    ax.add_artist(anchored_box)\n    \n    fig.canvas.draw()\n    \n    renderer = fig.canvas.get_renderer()\n    bbox = da.get_bbox(renderer)\n    offset = da.get_offset(bbox, renderer)\n    \n    expected_offset = (bbox.width / 2, bbox.height / 2)\n    assert offset == expected_offset, f\"Expected offset {expected_offset}, got {offset}\"\n"], "sample_564": ["def test_set_aspect():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    X, Y, Z = axes3d.get_test_data(0.05)\n    ax.plot_wireframe(X, Y, Z)\n    ax.set_aspect('equal')\n    ax.set_box_aspect((1, 1, 0.5))\n    ax.set_aspect('auto')\n    fig.canvas.draw()\n", "def test_axes3d_set_zmargin(fig_test, fig_ref):\n    # Test that setting zmargin works correctly and does not affect x and y margins\n    ax_test = fig_test.add_subplot(111, projection='3d')\n    ax_test.plot([0, 1], [0, 1], [0, 1])\n    ax_test.set_zmargin(0.1)\n\n    ax_ref = fig_ref.add_subplot(111, projection='3d')\n    ax_ref.plot([0, 1], [0, 1], [0, 1])\n    ax_ref.margins(z=0.1)\n\n    # Ensure that setting zmargin has no effect on x and y margins\n    assert ax_test._xmargin == ax_ref._xmargin\n    assert ax_test._ymargin == ax_ref._ymargin\n    assert ax_test._zmargin == ax_ref._zmargin\n", "def test_set_box_aspect_zoom(fig_test, fig_ref):\n    # Test if zoom parameter in set_box_aspect works correctly\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.set_box_aspect((4, 4, 3), zoom=2)\n    x, y, z = [0, 1], [2, 3], [4, 5]\n    ax_test.plot(x, y, z)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.set_box_aspect((4, 4, 3))\n    ax_ref.plot(x, y, z)\n    ax_ref.set_box_aspect((4, 4, 3), zoom=2)\n", "def test_set_box_aspect():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.set_box_aspect([1, 2, 3])\n    plot_cuboid(ax, scale=[1, 2, 3])\n    ax.set_title('Box aspect set to [1, 2, 3]')\n", "def test_set_aspect():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.set_aspect('auto')\n    assert ax._aspect == 'auto'\n    ax.set_aspect('equal')\n    assert ax._aspect == 'equal'\n    ax.set_aspect('equalxy')\n    assert ax._aspect == 'equalxy'\n    ax.set_aspect('equalxz')\n    assert ax._aspect == 'equalxz'\n    ax.set_aspect('equalyz')\n    assert ax._aspect == 'equalyz'\n    \n    with pytest.raises(ValueError, match=\"argument 'aspect' is invalid\"):\n        ax.set_aspect('invalid_aspect')\n"], "sample_565": ["def test_bbox_connector_patch():\n    fig, ax = plt.subplots(figsize=(6, 6))\n    bbox1 = Bbox.from_bounds(0, 0, 1, 1)\n    bbox2 = Bbox.from_bounds(2, 2, 1, 1)\n    \n    patch = BboxConnectorPatch(bbox1, bbox2, loc1a=1, loc2a=2, loc1b=4, loc2b=3, ec=\"blue\", fc=\"red\")\n    ax.add_patch(patch)\n\n    fig.canvas.draw()\n\n    path = patch.get_path()\n    expected_path = Path([\n        [1, 1], [2, 3],  # Line from upper right of bbox1 to upper left of bbox2\n        [2, 2], [1, 0],  # Line from lower right of bbox2 to lower left of bbox1\n        [1, 1]           # Closing the path\n    ])\n    \n    assert_array_almost_equal(path.vertices, expected_path.vertices)\n", "def test_bbox_connector():\n    fig, ax = plt.subplots()\n    bbox1 = Bbox.from_extents(0.1, 0.1, 0.4, 0.4)\n    bbox2 = Bbox.from_extents(0.5, 0.5, 0.8, 0.8)\n    bc = BboxConnector(bbox1, bbox2, loc1=1, loc2=3, color=\"blue\")\n    ax.add_patch(bc)\n    assert_array_equal(\n        bc.connect_bbox(bbox1, bbox2, loc1=1, loc2=3).vertices,\n        [[0.4, 0.4], [0.1, 0.1], [0.8, 0.8], [0.5, 0.5]]\n    )\n    plt.close(fig)\n", "def test_bbox_connector():\n    # Create two Bbox instances\n    bbox1 = Bbox.from_bounds(0, 0, 1, 1)\n    bbox2 = Bbox.from_bounds(2, 2, 1, 1)\n    \n    # Create a BboxConnector instance\n    connector = BboxConnector(bbox1, bbox2, loc1=1, loc2=3, color='red')\n\n    # Check if the path is correctly created\n    path = connector.get_path()\n    assert_array_almost_equal(path.vertices, [[1, 1], [2, 2]])\n\n    # Add the connector to a plot and draw\n    fig, ax = plt.subplots()\n    ax.add_patch(connector)\n    fig.canvas.draw()\n\n    # Verify visual properties\n    assert connector.get_edgecolor() == (1.0, 0.0, 0.0, 1.0)  # red\n    assert connector.get_facecolor() == (0.0, 0.0, 0.0, 0.0)  # no fill\n", "def test_inset_position_call():\n    fig, ax = plt.subplots()\n    ax.set_position([0.1, 0.1, 0.8, 0.8])\n\n    inset_ax = plt.axes([0, 0, 1, 1])\n    ip = InsetPosition(ax, [0.2, 0.2, 0.4, 0.4])\n    inset_ax.set_axes_locator(ip)\n\n    fig.canvas.draw()\n    bb = ip(inset_ax, fig.canvas.get_renderer())\n    assert_array_almost_equal(bb.extents, ax.get_position().extents + [0.2, 0.2, -0.4, -0.4])\n", "def test_bbox_connector_patch():\n    fig, ax = plt.subplots(figsize=(5, 5))\n    bbox1 = Bbox.from_bounds(0.1, 0.1, 0.4, 0.4)\n    bbox2 = Bbox.from_bounds(0.6, 0.6, 0.3, 0.3)\n\n    connector_patch = BboxConnectorPatch(\n        bbox1, bbox2, loc1a=1, loc2a=2, loc1b=4, loc2b=3, ec='blue', fc='red'\n    )\n    ax.add_patch(connector_patch)\n\n    fig.canvas.draw()\n    path = connector_patch.get_path()\n    \n    expected_vertices = [\n        [0.5, 0.5], [0.1, 0.5], [0.6, 0.9], [0.9, 0.6], [0.5, 0.5]\n    ]\n    \n    assert_array_almost_equal(path.vertices, expected_vertices)\n"], "sample_566": ["def test_subplot_mosaic_invalid_input():\n    fig = plt.figure()\n\n    # Test invalid input: non-2D list\n    with pytest.raises(ValueError, match=\"List mosaic specification must be 2D\"):\n        fig.subplot_mosaic(['A', 'B', 'C'])\n\n    # Test invalid input: inconsistent row lengths\n    with pytest.raises(ValueError, match=\"All of the rows must be the same length\"):\n        fig.subplot_mosaic([['A', 'B'], ['C']])\n\n    # Test invalid input: non-string keys in layout\n    with pytest.raises(TypeError, match=\"Keys in the mosaic layout must be hashable\"):\n        fig.subplot_mosaic([[1, 2], [3, {}]])\n\n    # Test invalid input: non-hashable keys in per_subplot_kw\n    with pytest.raises(TypeError, match=\"Keys in per_subplot_kw must be hashable\"):\n        fig.subplot_mosaic([[1, 2], [3, 4]], per_subplot_kw={{1, 2}: {}})\n\n    # Test invalid input: empty layout\n    with pytest.raises(ValueError, match=\"Layout must not be empty\"):\n        fig.subplot_mosaic([])\n\n    # Test invalid input: non-string, non-hashable keys in nested layout\n    with pytest.raises(TypeError, match=\"Keys in the mosaic layout must be hashable\"):\n        fig.subplot_mosaic([[['A', 'B'], ['C', {}]], ['D', 'E']])\n", "def test_subplot_params_update():\n    # Test SubplotParams to ensure proper update behavior\n    params = SubplotParams(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n    assert params.left == 0.1\n    assert params.bottom == 0.1\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    params.update(left=0.2, top=0.8)\n    assert params.left == 0.2\n    assert params.top == 0.8\n    assert params.bottom == 0.1\n    assert params.right == 0.9\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    # Ensure that invalid updates raise ValueError\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=1.0)\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=1.0)\n\n    # Test with partial parameters update\n    params.update(wspace=0.3)\n    assert params.wspace == 0.3\n    assert params.hspace == 0.2\n", "def test_figure_base_contains():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    assert fig.contains(MouseEvent(\"motion_notify_event\", fig.canvas, 0.5, 0.5, 1))[0]\n    assert fig.contains(MouseEvent(\"motion_notify_event\", fig.canvas, -1, -1, 1))[0] == False\n", "def test_autofmt_xdate_default():\n    date = ['3 Jan 2013', '4 Jan 2013', '5 Jan 2013', '6 Jan 2013',\n            '7 Jan 2013', '8 Jan 2013', '9 Jan 2013', '10 Jan 2013',\n            '11 Jan 2013', '12 Jan 2013', '13 Jan 2013', '14 Jan 2013']\n\n    time = ['16:44:00', '16:45:00', '16:46:00', '16:47:00', '16:48:00',\n            '16:49:00', '16:51:00', '16:52:00', '16:53:00', '16:55:00',\n            '16:56:00', '16:57:00']\n\n    x = mdates.datestr2num(date)\n    y = mdates.datestr2num(time)\n\n    fig, ax = plt.subplots()\n\n    ax.plot(x, y)\n    ax.yaxis_date()\n    ax.xaxis_date()\n\n    fig.autofmt_xdate()\n\n    for label in fig.axes[0].get_xticklabels(which='both'):\n        assert int(label.get_rotation()) == 30\n", "def test_figure_clipping():\n    fig, ax = plt.subplots()\n    rect = mpl.patches.Rectangle((0.1, 0.1), 0.5, 0.5, transform=fig.transFigure, clip_on=False)\n    fig.add_artist(rect)\n    fig.draw_without_rendering()\n    assert not rect.get_clip_on(), \"The rectangle should not be clipped by the figure boundaries\"\n\n    circle = mpl.patches.Circle((0.5, 0.5), 0.2, transform=fig.transFigure, clip_on=True)\n    fig.add_artist(circle)\n    fig.draw_without_rendering()\n    assert circle.get_clip_on(), \"The circle should be clipped by the figure boundaries\"\n"], "sample_567": ["def test_set_fontproperties():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Hello, World!', fontproperties='Arial')\n    fig.canvas.draw()\n\n    # Test that fontproperties was set correctly\n    assert isinstance(text.get_fontproperties(), FontProperties)\n    assert text.get_fontproperties().get_name() == 'Arial'\n\n    # Update fontproperties and test again\n    new_fp = FontProperties(family='sans-serif', style='italic', size=20)\n    text.set_fontproperties(new_fp)\n    fig.canvas.draw()\n\n    assert text.get_fontproperties().get_family() == ['sans-serif']\n    assert text.get_fontproperties().get_style() == 'italic'\n    assert text.get_fontproperties().get_size_in_points() == 20\n", "def test_set_bbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test bbox', fontsize=12)\n    \n    # Set a bounding box with specific properties\n    bbox_props = dict(boxstyle=\"round,pad=0.3\", edgecolor=\"red\", facecolor=\"blue\", linewidth=2)\n    text.set_bbox(bbox_props)\n    \n    fig.canvas.draw()\n    \n    bbox_patch = text.get_bbox_patch()\n    assert bbox_patch is not None\n    assert bbox_patch.get_boxstyle().get_name() == 'Round'\n    assert bbox_patch.get_edgecolor() == mpl.colors.to_rgba(\"red\")\n    assert bbox_patch.get_facecolor() == mpl.colors.to_rgba(\"blue\")\n    assert bbox_patch.get_linewidth() == 2.0\n\n    # Remove the bounding box\n    text.set_bbox(None)\n    fig.canvas.draw()\n    assert text.get_bbox_patch() is None\n", "def test_offset_from_transform():\n    fig, ax = plt.subplots()\n    trans = mtransforms.Affine2D().scale(2).translate(3, 4)\n    offset_from = OffsetFrom(trans, (0.5, 0.5))\n    result = offset_from(fig.canvas.get_renderer()).transform((1, 1))\n    expected = trans.transform((1, 1))\n    assert_almost_equal(result, expected)\n", "def test_set_bbox():\n    fig, ax = plt.subplots()\n    txt = ax.text(0.5, 0.5, 'Test Bbox', fontsize=14)\n    txt.set_bbox(dict(facecolor='red', alpha=0.5))\n    fig.canvas.draw()\n    bbox_patch = txt.get_bbox_patch()\n    \n    assert bbox_patch is not None\n    assert bbox_patch.get_facecolor() == (1.0, 0.0, 0.0, 0.5)  # RGBA for red with 50% opacity\n\n    # Modify bbox properties and check if updated correctly\n    txt.set_bbox(dict(facecolor='blue', edgecolor='yellow', boxstyle='round,pad=0.3'))\n    fig.canvas.draw()\n    bbox_patch = txt.get_bbox_patch()\n    \n    assert bbox_patch.get_facecolor() == (0.0, 0.0, 1.0, 1.0)  # RGBA for blue with full opacity\n    assert bbox_patch.get_edgecolor() == (1.0, 1.0, 0.0, 1.0)  # RGBA for yellow with full opacity\n    assert bbox_patch.get_boxstyle().split(',')[0] == 'round'\n", "def test_set_backgroundcolor():\n    fig, ax = plt.subplots()\n    txt = ax.text(0.5, 0.5, 'Hello World', backgroundcolor='blue')\n    fig.canvas.draw()\n    assert txt.get_bbox_patch().get_facecolor() == mpl.colors.to_rgba('blue')\n\n    txt.set_backgroundcolor('red')\n    fig.canvas.draw()\n    assert txt.get_bbox_patch().get_facecolor() == mpl.colors.to_rgba('red')\n"], "sample_568": ["def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector((1, 1, 1)), np.array([1, 1, 1]))\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector('a')\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector((1, 1))\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector((1, 1, 1, 1))\n", "def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array([1, 2, 3]))\n    with pytest.raises(ValueError, match=\"expected\"):\n        art3d.get_dir_vector('invalid')\n    with pytest.raises(ValueError, match=\"expected\"):\n        art3d.get_dir_vector([1, 2])\n", "def test_text3d_modifications(fig_test, fig_ref):\n    # Test modifying Text3D properties post-creation\n    ax_test = fig_test.add_subplot(projection='3d')\n    text3d = art3d.Text3D(1, 1, 1, 'Test Text')\n    text3d.set_position_3d((2, 2, 2), zdir='y')\n    text3d.set_text('Modified Text')\n    text3d.set_color('blue')\n    text3d.set_fontsize(15)\n    ax_test.add_artist(text3d)\n    \n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ref_text3d = art3d.Text3D(2, 2, 2, 'Modified Text', color='blue', fontsize=15)\n    ref_text3d.set_3d_properties(z=2, zdir='y')\n    ax_ref.add_artist(ref_text3d)\n", "def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array((1, 0, 0)))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array((0, 1, 0)))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array((0, 0, 1)))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array((0, 0, 0)))\n    assert np.array_equal(art3d.get_dir_vector((1, 1, 1)), np.array((1, 1, 1)))\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector('a')\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector((1, 1))\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector((1, 1, 1, 1))\n", "def test_get_dir_vector():\n    # Test known direction vectors\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array((1, 0, 0)))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array((0, 1, 0)))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array((0, 0, 1)))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array((0, 0, 0)))\n\n    # Test with iterable input\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array((1, 2, 3)))\n\n    # Test invalid input\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector('invalid')\n"], "sample_569": ["    def test_establish_variables_from_dict(self):\n\n        p = lm._LinearPlotter()\n        p.establish_variables(None, x=self.df.x.to_dict(), y=self.df.y.to_dict())\n        npt.assert_array_equal(p.x, self.df.x)\n        npt.assert_array_equal(p.y, self.df.y)\n        assert p.data is None\n", "def test_establish_variables_with_nones(self):\n    p = lm._LinearPlotter()\n    data = {'x': [1, 2, 3], 'y': [4, 5, 6], 'z': [None, None, None]}\n    df = pd.DataFrame(data)\n    p.establish_variables(df, x='x', y='y', z='z')\n    pdt.assert_series_equal(p.x, df['x'])\n    pdt.assert_series_equal(p.y, df['y'])\n    pdt.assert_series_equal(p.z, df['z'])\n", "def test_logx_regression_plot():\n\n        x = np.linspace(1, 10, 100)\n        y = 2 + 3 * np.log(x) + np.random.normal(size=100)\n        df = pd.DataFrame({\"x\": x, \"y\": y})\n\n        f, ax = plt.subplots()\n        lm.regplot(x=\"x\", y=\"y\", data=df, logx=True, ax=ax)\n        \n        assert len(ax.lines) == 1\n        assert len(ax.collections) == 1\n\n        grid = np.linspace(1, 10, 100)\n        plotter = lm._RegressionPlotter(x, y, logx=True)\n        yhat, _ = plotter.fit_logx(grid)\n        \n        line_data = ax.lines[0].get_xydata()\n        npt.assert_almost_equal(line_data[:, 1], yhat, decimal=1)\n", "def test_lmplot_markers_deprecation_warning(self):\n\n    with pytest.warns(UserWarning, match=\"The `size` parameter has been renamed to `height`\"):\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, size=5)\n        assert g._facetgrid.height == 5\n\n    with pytest.warns(UserWarning, match=\"sharex is deprecated from the `lmplot` function signature.\"):\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, sharex=True)\n        assert g._facetgrid._sharex\n\n    with pytest.warns(UserWarning, match=\"sharey is deprecated from the `lmplot` function signature.\"):\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, sharey=True)\n        assert g._facetgrid._sharey\n\n    with pytest.warns(UserWarning, match=\"legend_out is deprecated from the `lmplot` function signature.\"):\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, legend_out=True)\n        assert g._facetgrid.legend_out\n", "def test_regplot_logx(self):\n\n    f, ax = plt.subplots()\n    lm.regplot(x=\"x\", y=\"y\", data=self.df, logx=True)\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 2\n\n    x, y = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, self.df.x)\n    npt.assert_array_equal(y, self.df.y)\n\n    line_x, line_y = ax.lines[0].get_xydata().T\n    assert np.all(line_x > 0)\n"], "sample_570": ["    def test_define_support_with_cache(self, rng):\n        x = rng.normal(0, 3, 100)\n        kde = KDE()\n        support = kde.define_support(x, cache=True)\n        assert kde.support is not None\n        assert_array_equal(kde.support, support)\n", "def test_ecdf_complementary_proportion(self, x):\n\n    ecdf = ECDF(complementary=True)\n    stat, vals = ecdf(x)\n    assert_array_equal(vals[1:], np.sort(x))\n    assert_array_almost_equal(stat[1:], 1 - np.linspace(0, 1, len(x) + 1)[:-1])\n    assert stat[0] == np.max(stat)\n", "def test_histogram_stat_invalid(self, x):\n    with pytest.raises(ValueError, match=\"`stat` must be one of\"):\n        Histogram(stat=\"invalid\")\n", "    def test_kde_cumulative_with_no_scipy(self, monkeypatch):\n        # Temporarily set _no_scipy to True\n        monkeypatch.setattr(\"_no_scipy\", True)\n        with pytest.raises(RuntimeError, match=\"Cumulative KDE evaluation requires scipy\"):\n            KDE(cumulative=True)\n", "def test_ecdf_complementary(self, x):\n\n    ecdf = ECDF(complementary=True)\n    stat, vals = ecdf(x)\n    assert_array_equal(vals[1:], np.sort(x))\n    assert_array_almost_equal(stat[1:], 1 - np.linspace(0, 1, len(x) + 1)[1:])\n    assert stat[0] == len(x)\n"], "sample_571": ["    def test_fit_regression_truncate(self):\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, truncate=True)\n        f, ax = plt.subplots()\n        ax.scatter(self.df.x, self.df.y)\n        grid, yhat, err_bands = p.fit_regression(ax)\n        assert grid.min() >= self.df.x.min()\n        assert grid.max() <= self.df.x.max()\n", "    def test_plot_partial_regression(self):\n        # Test partial regression with both x_partial and y_partial\n        x = self.rs.randn(100)\n        y = 2 * x + self.rs.randn(100)\n        z = 3 * x + self.rs.randn(100)\n\n        p = lm._RegressionPlotter(x, y, x_partial=z, y_partial=z)\n        assert hasattr(p, 'x_partial')\n        assert hasattr(p, 'y_partial')\n\n        # Check the corrected x and y values\n        corrected_x = p.x\n        corrected_y = p.y\n        assert np.corrcoef(corrected_x, corrected_y)[0, 1] < np.corrcoef(x, y)[0, 1]\n", "def test_establish_variables_with_units(self):\n    \n    p = lm._LinearPlotter()\n    p.establish_variables(self.df, x=\"x\", y=\"y\", units=\"s\")\n    pdt.assert_series_equal(p.x, self.df.x)\n    pdt.assert_series_equal(p.y, self.df.y)\n    pdt.assert_series_equal(p.units, self.df.s)\n    pdt.assert_frame_equal(p.data, self.df)\n", "def test_regplot_logx(self):\n    f, ax = plt.subplots()\n    lm.regplot(x=\"x\", y=\"y\", data=self.df, logx=True, ax=ax)\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 2\n\n    # Verify that the x-axis is transformed correctly\n    x_ticks = ax.get_xticks()\n    assert np.all(x_ticks > 0)  # Ensure all ticks are positive\n    assert np.all(np.diff(np.log(x_ticks)) > 0)  # Ensure the ticks follow a log scale\n", "    def test_plot_logx(self):\n        x = np.arange(1, 10)\n        y = np.arange(1, 10)\n        p = lm._RegressionPlotter(x, y, logx=True)\n        f, ax = plt.subplots()\n        p.plot(ax, scatter_kws={}, line_kws={})\n        assert ax.get_xlim()[0] > 0\n"], "sample_572": ["    def test_custom_errorbar_function(self, long_df):\n        \"\"\"Test EstimateAggregator with custom errorbar function.\"\"\"\n            mean = np.mean(data)\n            std = np.std(data)\n            return mean - std, mean + std\n        \n        agg = EstimateAggregator(\"mean\", custom_errorbar)\n        out = agg(long_df, \"x\")\n        expected_min = long_df[\"x\"].mean() - long_df[\"x\"].std()\n        expected_max = long_df[\"x\"].mean() + long_df[\"x\"].std()\n        \n        assert out[\"xmin\"] == pytest.approx(expected_min, abs=1e-5)\n        assert out[\"xmax\"] == pytest.approx(expected_max, abs=1e-5)\n", "def test_cumulative_complementary(self, x):\n\n    ecdf = ECDF(stat=\"proportion\", complementary=True)\n    stat, vals = ecdf(x)\n    assert_array_equal(vals[1:], np.sort(x))\n    assert_array_almost_equal(stat[1:], 1 - np.linspace(0, 1, len(x) + 1)[1:])\n    assert stat[0] == 1\n", "    def test_invalid_weights(self, x):\n\n        with pytest.raises(ValueError, match=\"weights should have the same length as x\"):\n            ecdf = ECDF()\n            weights = np.ones(len(x) - 1)\n            ecdf(x, weights=weights)\n", "def test_clip_support_bivariate(self, rng):\n    x1 = rng.normal(0, 3, 100)\n    x2 = rng.normal(0, 5, 100)\n    clip = (-1, 1), (-3, 3)\n    kde = KDE(clip=clip)\n    _, (support_x1, support_x2) = kde(x1, x2)\n\n    assert support_x1.min() >= clip[0][0]\n    assert support_x1.max() <= clip[0][1]\n    assert support_x2.min() >= clip[1][0]\n    assert support_x2.max() <= clip[1][1]\n", "def test_kde_weights(self, rng):\n\n    x = rng.normal(0, 3, 1000)\n    weights = rng.uniform(0.1, 2, 1000)\n\n    kde = KDE()\n    density, support = kde(x, weights=weights)\n\n    unweighted_kde = KDE()\n    unweighted_density, _ = unweighted_kde(x)\n\n    assert density.sum() != unweighted_density.sum()\n    assert density.max() != unweighted_density.max()\n"], "sample_573": ["    def test_different_order(self, df):\n        groupby = GroupBy([\"group\"])\n        orders = [1, 2, 3, 4]\n        for order in orders:\n            res = PolyFit(order=order, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n            assert_array_equal(res.columns, [\"x\", \"y\"])\n            grid = np.linspace(df[\"x\"].min(), df[\"x\"].max(), 100)\n            assert_array_equal(res[\"x\"], grid)\n            assert res[\"y\"].diff().diff().dropna().abs().gt(0).all()\n", "    def test_multiple_groupers(self, df):\n\n        groupby = GroupBy([\"color\", \"group\"])\n        gridsize = 20\n        res = PolyFit(gridsize=gridsize)(df, groupby, \"x\", {})\n\n        assert res.columns.to_list() == [\"x\", \"y\", \"color\", \"group\"]\n\n        ngroups = df.groupby([\"color\", \"group\"]).ngroups\n        assert_array_equal(res.index, np.arange(ngroups * gridsize))\n\n        for _, part in res.groupby([\"color\", \"group\"]):\n            grid = np.linspace(part[\"x\"].min(), part[\"x\"].max(), gridsize)\n            assert_array_equal(part[\"x\"], grid)\n            assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n", "    def test_different_orders(self, df):\n        \n        groupby = GroupBy([\"group\"])\n        orders = [1, 2, 3]\n        gridsize = 100\n        \n        for order in orders:\n            res = PolyFit(order=order, gridsize=gridsize)(df, groupby, \"x\", {})\n            \n            assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n            \n            ngroups = df[\"group\"].nunique()\n            assert_array_equal(res.index, np.arange(ngroups * gridsize))\n            \n            for _, part in res.groupby(\"group\"):\n                grid = np.linspace(part[\"x\"].min(), part[\"x\"].max(), gridsize)\n                assert_array_equal(part[\"x\"], grid)\n                assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n", "    def test_with_nan_values(self, df):\n\n        df_with_nan = df.copy()\n        df_with_nan.loc[::10, \"x\"] = np.nan\n        df_with_nan.loc[::15, \"y\"] = np.nan\n\n        groupby = GroupBy([\"group\"])\n        gridsize = 50\n        res = PolyFit(gridsize=gridsize)(df_with_nan, groupby, \"x\", {})\n\n        assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n\n        ngroups = df[\"group\"].nunique()\n        assert_array_equal(res.index, np.arange(ngroups * gridsize))\n\n        for _, part in res.groupby(\"group\"):\n            grid = np.linspace(part[\"x\"].min(), part[\"x\"].max(), gridsize)\n            assert_array_equal(part[\"x\"], grid)\n            assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n", "    def test_higher_order_fit(self, df):\n\n        groupby = GroupBy([\"group\"])\n        res = PolyFit(order=3, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        assert_array_equal(res.columns, [\"x\", \"y\"])\n\n        grid = np.linspace(df[\"x\"].min(), df[\"x\"].max(), 100)\n        assert_array_equal(res[\"x\"], grid)\n        assert_array_almost_equal(\n            res[\"y\"].diff().diff().diff().dropna(), np.zeros(grid.size - 3)\n        )\n"], "sample_574": ["    def x(self):\n        return pd.Series([\"low\", \"medium\", \"high\", \"medium\"], name=\"x\")\n", "    def test_nominal_tick(self, x):\n        ax = mpl.figure.Figure().subplots()\n        s = Nominal().tick()._setup(x, Coordinate(), ax.xaxis)\n        assert ax.xaxis.get_major_locator() is not None\n", "    def x(self):\n        return pd.Series([\"small\", \"large\", \"medium\", \"large\"], name=\"x\")\n", "    def x(self):\n        return pd.Series([\"a\", \"c\", \"b\", \"c\"], name=\"x\")\n", "def test_label_concise(self, t):\n\n    formatter = mpl.dates.ConciseDateFormatter\n    s = Temporal().label(concise=True)\n    a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n    a.set_view_interval(10, 1000)\n    assert isinstance(a.major.formatter, formatter)\n"], "sample_575": ["    def test_set_view_interval(self):\n        scale = Continuous()._get_scale(\"test\", lambda x: x, lambda x: x)\n        axis = PseudoAxis(scale)\n        axis.set_view_interval(1, 10)\n        assert axis.get_view_interval() == (1, 10)\n", "    def x(self):\n        return pd.Series([1, 4, 2, 3], name=\"x\")\n", "def test_tick_locator_with_tz_aware_dates(self, t):\n    # Test that tick locator works with timezone aware dates\n    t_tz = t.dt.tz_localize('UTC')\n    locator = mpl.dates.MonthLocator()\n    s = Temporal().tick(locator)\n    a = PseudoAxis(s._setup(t_tz, Coordinate())._matplotlib_scale)\n    a.set_view_interval(0, 365)\n    assert len(a.major.locator()) > 0  # Ensure that some ticks are generated\n", "    def x(self):\n        return pd.Series([3, 1, 2, 1], name=\"x\")\n", "    def data(self):\n        return pd.Series([3, 1, 2, 3, 2], name=\"data\")\n"], "sample_576": ["    def test_init_figure_with_target(self):\n        fig = mpl.figure.Figure()\n        ax = fig.add_subplot(111)\n        p = Plot([1], [2]).on(ax).plot()\n        assert p._figure is fig\n        assert p._subplots[0][\"ax\"] is ax\n", "    def test_scale_inference_with_heterogeneous_data(self):\n        data = {\n            \"x\": [1, 2, 3, \"a\", \"b\", \"c\"],\n            \"y\": pd.to_datetime([\"2021-01-01\", \"2021-01-02\", \"2021-01-03\"] * 2)\n        }\n        p = Plot(data).add(MockMark()).plot()\n        assert isinstance(p._scales[\"x\"], Nominal)\n        assert isinstance(p._scales[\"y\"], Continuous)\n", "    def test_temporary_rcparams(self):\n\n        initial_params = mpl.rcParams.copy()\n\n        params_to_modify = {\n            \"axes.facecolor\": \"lightgrey\",\n            \"axes.labelsize\": \"large\",\n            \"xtick.direction\": \"inout\",\n        }\n\n        with theme_context(params_to_modify):\n            for param, value in params_to_modify.items():\n                assert mpl.rcParams[param] == value\n\n        for param in params_to_modify.keys():\n            assert mpl.rcParams[param] == initial_params[param]\n", "def test_add_with_invalid_mark(self, long_df):\n\n    class InvalidMark:\n        pass\n\n    p = Plot(long_df, x=\"x\", y=\"y\")\n    with pytest.raises(TypeError, match=\"mark must be a Mark instance\"):\n        p.add(InvalidMark())\n", "def test_theme_context_restores_original_params():\n    original_params = mpl.rcParams.copy()\n    test_params = {\"axes.facecolor\": \"r\", \"axes.grid\": False}\n    with theme_context(test_params):\n        assert mpl.rcParams[\"axes.facecolor\"] == \"r\"\n        assert mpl.rcParams[\"axes.grid\"] is False\n    assert mpl.rcParams[\"axes.facecolor\"] == original_params[\"axes.facecolor\"]\n    assert mpl.rcParams[\"axes.grid\"] == original_params[\"axes.grid\"]\n"], "sample_577": ["    def test_plot_signature(self):\n        plot_sig = inspect.signature(Plot)\n        params = [param.name for param in plot_sig.parameters.values()]\n        assert \"args\" in params\n        assert \"data\" in params\n        for prop in PROPERTIES:\n            assert prop in params\n", "    def test_add_with_stat_and_move(self, long_df):\n        class MockStat(Stat):\n                data[\"y\"] = data[\"y\"] ** 2\n                return data\n\n        class MockMove(Move):\n                data[\"x\"] = data[\"x\"] + 1\n                return data\n\n        m = MockMark()\n        p = Plot(long_df, x=\"x\", y=\"y\").add(m, MockStat(), MockMove()).plot()\n\n        assert_vector_equal(m.passed_data[0][\"x\"], long_df[\"x\"] + 1)\n        assert_vector_equal(m.passed_data[0][\"y\"], long_df[\"y\"] ** 2)\n", "    def test_clone_preserves_state(self, long_df):\n\n        p1 = Plot(long_df, x=\"x\", y=\"y\").add(MockMark(), color=\"a\").facet(col=\"b\").scale(color=\"log\")\n        p2 = p1._clone()\n\n        # Check data preservation\n        assert_frame_equal(p1._data.frame, p2._data.frame)\n        \n        # Check layers preservation\n        assert p1._layers == p2._layers\n        \n        # Check scales preservation\n        assert p1._scales == p2._scales\n\n        # Check facet specification preservation\n        assert p1._facet_spec == p2._facet_spec\n\n        # Check figure and layout specifications preservation\n        assert p1._figure_spec == p2._figure_spec\n        assert p1._layout_spec == p2._layout_spec\n\n        # Check that modifications to the clone do not affect the original\n        p2._scales[\"x\"] = \"modified_scale\"\n        assert \"modified_scale\" not in p1._scales.values()\n", "    def sample_params(self):\n        return {\n            \"axes.edgecolor\": \"red\",\n            \"axes.facecolor\": \"blue\",\n            \"axes.labelcolor\": \"green\",\n        }\n", "    def test_theme_context_applies_temporary_changes(self):\n        original_rc_params = {k: mpl.rcParams[k] for k in [\"axes.facecolor\", \"lines.linewidth\"]}\n        new_params = {\"axes.facecolor\": \"lightgray\", \"lines.linewidth\": 2.5}\n        \n        with theme_context(new_params):\n            assert mpl.rcParams[\"axes.facecolor\"] == \"lightgray\"\n            assert mpl.rcParams[\"lines.linewidth\"] == 2.5\n        \n        assert mpl.rcParams[\"axes.facecolor\"] == original_rc_params[\"axes.facecolor\"]\n        assert mpl.rcParams[\"lines.linewidth\"] == original_rc_params[\"lines.linewidth\"]\n"], "sample_578": ["    def test_resolved_properties(self):\n\n        x = [\"a\", \"b\", \"c\"]\n        y = [1, 3, 2]\n        color = [\"red\", \"green\", \"blue\"]\n        edgecolor = [\"cyan\", \"magenta\", \"yellow\"]\n        fill = [0.5, 0.8, 0.3]\n\n        mark = Bar(\n            color=color,\n            edgecolor=edgecolor,\n            fill=fill,\n            alpha=1,\n            edgealpha=1,\n            edgewidth=1,\n            edgestyle=\"-\",\n        )\n\n        p = Plot(x, y).add(mark).plot()\n        ax = p._figure.axes[0]\n        for i, bar in enumerate(ax.patches):\n            assert bar.get_facecolor() == to_rgba(color[i], fill[i])\n            assert bar.get_edgecolor() == to_rgba(edgecolor[i], 1)\n            assert bar.get_linewidth() == 2\n            assert bar.get_linestyle() == (0, ())\n\n", "    def test_baseline_property(self):\n\n        x = [\"a\", \"b\", \"c\"]\n        y = [1, 3, 2]\n        baseline = 1\n\n        p = Plot(x, y).add(Bar(baseline=baseline)).plot()\n        ax = p._figure.axes[0]\n        for i, bar in enumerate(ax.patches):\n            assert bar.get_y() == pytest.approx(baseline)\n            assert bar.get_height() == pytest.approx(y[i] - baseline)\n", "    def test_baseline_property(self):\n\n        x = [\"a\", \"b\", \"c\"]\n        y = [1, 3, 2]\n        baseline = 1\n\n        mark = Bar(baseline=baseline)\n\n        p = Plot(x, y).add(mark).plot()\n        ax = p._figure.axes[0]\n        for i, bar in enumerate(ax.patches):\n            self.check_bar(bar, i - mark.width / 2, baseline, mark.width, y[i] - baseline)\n", "    def test_baseline(self):\n\n        x = [\"a\", \"b\", \"c\"]\n        y = [1, 3, 2]\n        baseline = 1\n\n        mark = Bar(baseline=baseline)\n\n        p = Plot(x, y).add(mark).plot()\n        ax = p._figure.axes[0]\n        for bar in ax.patches:\n            assert bar.get_y() == pytest.approx(baseline)\n", "    def test_baseline(self):\n        \n        x = [\"a\", \"b\", \"c\"]\n        y = [1, 3, 2]\n        baseline = 1\n\n        mark = Bar(baseline=baseline)\n\n        p = Plot(x, y).add(mark).plot()\n        ax = p._figure.axes[0]\n        for i, bar in enumerate(ax.patches):\n            assert bar.get_y() == baseline if ax.get_yaxis().get_label().get_text() else bar.get_x() == baseline\n"], "sample_579": ["def test_heatmap_with_custom_colormap():\n    data = np.random.rand(10, 10)\n    colormap = mpl.colors.ListedColormap([\"red\", \"green\", \"blue\"])\n    ax = mat.heatmap(data, cmap=colormap)\n    assert ax.collections[0].cmap == colormap\n\n    # Ensure color mapping corresponds correctly to values\n    fc = ax.collections[0].get_facecolors()\n    expected_colors = colormap(np.linspace(0, 1, data.size)).reshape(data.shape + (4,))\n    npt.assert_array_almost_equal(fc, expected_colors, decimal=2)\n", "def test_heatmap_with_square_and_center():\n    # Test heatmap with square=True and a center value\n    kws = self.default_kws.copy()\n    kws[\"square\"] = True\n    kws[\"center\"] = 0\n\n    p = mat._HeatMapper(self.df_norm, **kws)\n    assert p.vmin == self.df_norm.values.min()\n    assert p.vmax == self.df_norm.values.max()\n\n    ax = mat.heatmap(self.df_norm, square=True, center=0)\n    obs_aspect = ax.get_aspect()\n    assert obs_aspect == \"equal\" or obs_aspect == 1\n    assert ax.get_xlim() == (0, 8)\n    assert ax.get_ylim() == (4, 0)\n", "def test_convert_colors():\n    # Test with a single list of colors\n    colors = [\"red\", \"green\", \"blue\"]\n    converted = mat._convert_colors(colors)\n    expected = [mpl.colors.to_rgb(c) for c in colors]\n    assert converted == expected\n\n    # Test with nested lists of colors\n    nested_colors = [[\"red\", \"green\"], [\"blue\", \"yellow\"]]\n    converted_nested = mat._convert_colors(nested_colors)\n    expected_nested = [[mpl.colors.to_rgb(c) for c in inner] for inner in nested_colors]\n    assert converted_nested == expected\n\n    # Test with a single color\n    single_color = [\"red\"]\n    converted_single = mat._convert_colors(single_color)\n    expected_single = [mpl.colors.to_rgb(single_color[0])]\n    assert converted_single == expected\n\n    # Test with empty list\n    empty_colors = []\n    converted_empty = mat._convert_colors(empty_colors)\n    expected_empty = []\n    assert converted_empty == expected_empty\n", "def test_heatmap_with_masked_data():\n    data = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, np.nan]])\n    mask = pd.DataFrame([[False, True, False], [False, False, False], [False, False, True]])\n    kws = self.default_kws.copy()\n    kws[\"mask\"] = mask\n\n    p = mat._HeatMapper(data, **kws)\n    plot_data = np.ma.masked_where(mask, data.values)\n\n    npt.assert_array_equal(p.plot_data, plot_data)\n\n    # Ensure that masked values are not considered in vmin/vmax calculations\n    assert p.vmin == np.nanmin(data.values)\n    assert p.vmax == np.nanmax(data.values)\n", "    def test_heatmap_custom_colormap_list(self):\n        \"\"\"Test heatmap with a custom colormap provided as a list of colors.\"\"\"\n        kws = self.default_kws.copy()\n        custom_cmap = [\"#FF0000\", \"#00FF00\", \"#0000FF\"]\n        kws[\"cmap\"] = custom_cmap\n        p = mat._HeatMapper(self.df_unif, **kws)\n        assert isinstance(p.cmap, mpl.colors.ListedColormap)\n        assert p.cmap.colors == list(map(mpl.colors.to_rgb, custom_cmap))\n\n        ax = mat.heatmap(self.df_unif, cmap=custom_cmap)\n        mesh = ax.collections[0]\n        facecolors = mesh.get_facecolors()\n        assert np.array_equal(facecolors[:, :3], p.cmap(np.linspace(0, 1, len(facecolors)))[:, :3])\n"], "sample_580": ["def test_categorical_order():\n    \n    s = pd.Series([\"b\", \"a\", \"c\", \"a\"])\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n    \n    s = pd.Series([\"b\", \"a\", \"c\", np.nan, \"a\"])\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n    \n    s = pd.Series([1, 2, 3, 2])\n    assert categorical_order(s) == [1, 2, 3]\n    \n    s = pd.Series([1, 2, 3, np.nan, 2])\n    assert categorical_order(s) == [1, 2, 3]\n    \n    s = pd.Series([\"b\", \"a\", \"c\", \"a\"], dtype=\"category\")\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n    \n    s = pd.Series([\"b\", \"a\", \"c\", np.nan, \"a\"], dtype=\"category\")\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n    \n    custom_order = [\"c\", \"b\", \"a\"]\n    assert categorical_order(s, custom_order) == custom_order\n", "def test_categorical_order():\n    s = pd.Series([\"apple\", \"banana\", \"cherry\", \"apple\", \"banana\", \"cherry\"])\n    assert categorical_order(s) == [\"apple\", \"banana\", \"cherry\"]\n\n    s = pd.Series([\"apple\", \"banana\", np.nan, \"cherry\", \"apple\", \"banana\", \"cherry\"])\n    assert categorical_order(s) == [\"apple\", \"banana\", \"cherry\"]\n\n    s = pd.Series([\"apple\", \"banana\", \"cherry\", \"apple\", \"banana\", \"cherry\"], dtype=\"category\")\n    assert categorical_order(s) == [\"apple\", \"banana\", \"cherry\"]\n\n    s = pd.Series([1, 2, 3, 1, 2, 3])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([1, 2, 3, np.nan, 1, 2, 3])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([1, 2, 3, 1, 2, 3], dtype=\"category\")\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([1, 2, 3, np.nan, 1, 2, 3], dtype=\"category\")\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([pd.Timestamp(1), pd.Timestamp(2), pd.Timestamp(3)])\n    assert categorical_order(s) == [pd.Timestamp(1), pd.Timestamp(2), pd.Timestamp(3)]\n\n    s = pd.Series([pd.Timestamp(1), pd.Timestamp(2), pd.Timestamp(3)], dtype=\"category\")\n    assert categorical_order(s) == [pd.Timestamp(1), pd.Timestamp(2), pd.Timestamp(3)]\n\n    s = pd.Series([\"apple\", \"banana\", \"cherry\", \"apple\", \"banana\", \"cherry\"])\n    order = [\"cherry\", \"banana\", \"apple\"]\n    assert categorical_order(s, order=order) == order\n", "def test_categorical_order():\n\n    s = pd.Series([\"a\", \"b\", \"c\"])\n    expected_order = [\"a\", \"b\", \"c\"]\n    assert categorical_order(s) == expected_order\n\n    s = pd.Series([\"b\", \"a\", \"c\", \"a\"])\n    expected_order = [\"b\", \"a\", \"c\"]\n    assert categorical_order(s) == expected_order\n\n    s = pd.Series([\"b\", \"a\", \"c\", np.nan])\n    expected_order = [\"b\", \"a\", \"c\"]\n    assert categorical_order(s) == expected_order\n\n    s = pd.Series([1, 2, 3, np.nan])\n    expected_order = [1, 2, 3]\n    assert categorical_order(s) == expected_order\n\n    s = pd.Series([3, 1, 2, 2])\n    expected_order = [1, 2, 3]\n    assert categorical_order(s) == expected_order\n\n    s = pd.Series([3, 1, 2, 2], dtype=\"category\")\n    expected_order = [1, 2, 3]\n    assert categorical_order(s) == expected_order\n\n    s = pd.Series([3, 1, 2, 2], dtype=\"category\")\n    custom_order = [2, 3, 1]\n    assert categorical_order(s, order=custom_order) == custom_order\n\n    s = pd.Series([pd.Timestamp(\"2020-01-01\"), pd.Timestamp(\"2020-01-02\"), pd.Timestamp(\"2020-01-03\")])\n    expected_order = [pd.Timestamp(\"2020-01-01\"), pd.Timestamp(\"2020-01-02\"), pd.Timestamp(\"2020-01-03\")]\n    assert categorical_order(s) == expected_order\n\n    s = pd.Series([\"a\", \"b\", \"c\", None])\n    expected_order = [\"a\", \"b\", \"c\"]\n    assert categorical_order(s) == expected_order\n", "def test_categorical_order():\n\n    s = pd.Series([\"apple\", \"banana\", \"apple\", \"orange\"])\n    assert categorical_order(s) == [\"apple\", \"banana\", \"orange\"]\n\n    s = pd.Series([1, 3, 2, 1])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([3, 2, 1, 3])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([1, 2, pd.NA, 3])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([pd.NA, pd.NA, \"apple\", \"banana\"])\n    assert categorical_order(s) == [\"apple\", \"banana\"]\n\n    s = pd.Series([\"apple\", \"banana\", \"apple\", \"orange\"], dtype=\"category\")\n    assert categorical_order(s) == [\"apple\", \"banana\", \"orange\"]\n\n    custom_order = [\"banana\", \"orange\", \"apple\"]\n    assert categorical_order(s, custom_order) == custom_order\n", "def test_categorical_order():\n    \n    s = pd.Series([\"b\", \"a\", \"c\", \"a\"])\n    assert categorical_order(s) == [\"b\", \"a\", \"c\"]\n    \n    s = pd.Series([1, 3, 2, 1])\n    assert categorical_order(s) == [1, 2, 3]\n    \n    s = pd.Series([1, 3, 2, 1, np.nan])\n    assert categorical_order(s) == [1, 2, 3]\n    \n    s = pd.Series([\"b\", \"a\", \"c\", \"a\"], dtype=\"category\")\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n    \n    s = pd.Series([1, 3, 2, 1], dtype=\"category\")\n    assert categorical_order(s) == [1, 2, 3]\n    \n    s = pd.Series([\"b\", \"a\", \"c\", \"a\"])\n    assert categorical_order(s, order=[\"c\", \"a\", \"b\"]) == [\"c\", \"a\", \"b\"]\n    \n    s = pd.Series([1, 3, 2, 1])\n    assert categorical_order(s, order=[2, 3, 1]) == [2, 3, 1]\n"], "sample_581": ["def test_blueprint_make_setup_state():\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    # Create a mock Flask app\n    app = flask.Flask(__name__)\n\n    # Define options for Blueprint setup state\n    options = {\n        \"url_prefix\": \"/test\",\n        \"subdomain\": \"sub\",\n        \"name\": \"test_bp\",\n        \"url_defaults\": {\"key\": \"value\"},\n        \"name_prefix\": \"prefix\"\n    }\n\n    # Generate the BlueprintSetupState\n    state = bp.make_setup_state(app, options, True)\n\n    # Assertions to verify correct setup\n    assert state.app is app\n    assert state.blueprint is bp\n    assert state.options == options\n    assert state.first_registration is True\n    assert state.url_prefix == \"/test\"\n    assert state.subdomain == \"sub\"\n    assert state.name == \"test_bp\"\n    assert state.name_prefix == \"prefix\"\n    assert state.url_defaults[\"key\"] == \"value\"\n", "def test_add_url_rule_with_blueprint(app, client):\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/foo\")\n        return \"foo\"\n\n    app.register_blueprint(bp)\n\n    rv = client.get(\"/foo\")\n    assert rv.data == b\"foo\"\n\n    with pytest.raises(ValueError, match=\"'endpoint' may not contain a dot '.' character\"):\n        bp.add_url_rule(\"/bar\", endpoint=\"bp.bar\")\n\n        return \"bar\"\n\n    view_func.__name__ = \"bp.bar\"\n    with pytest.raises(ValueError, match=\"'view_func' name may not contain a dot '.' character\"):\n        bp.add_url_rule(\"/bar\", view_func=view_func)\n", "def test_blueprint_add_url_rule(app, client):\n    bp = flask.Blueprint(\"bp\", __name__, url_prefix=\"/bp\")\n\n    @bp.route(\"/foo\")\n        return \"foo\"\n\n    @bp.add_url_rule(\"/bar\", endpoint=\"bar\", view_func=lambda: \"bar\")\n    @bp.add_url_rule(\"/baz\", endpoint=\"baz\", view_func=lambda: \"baz\", methods=[\"POST\"])\n\n    app.register_blueprint(bp)\n\n    assert client.get(\"/bp/foo\").data == b\"foo\"\n    assert client.get(\"/bp/bar\").data == b\"bar\"\n    assert client.post(\"/bp/baz\").data == b\"baz\"\n    assert client.get(\"/bp/baz\").status_code == 405  # Method Not Allowed\n", "def test_blueprint_static_files(app, client):\n    bp = flask.Blueprint(\"bp\", __name__, static_folder=\"static\", static_url_path=\"/static_files\")\n    \n    @bp.route(\"/\")\n        return \"Index Page\"\n    \n    app.register_blueprint(bp)\n    \n    # Creating a temporary static file for testing\n    static_file_path = os.path.join(bp.root_path, \"static\", \"test.txt\")\n    os.makedirs(os.path.dirname(static_file_path), exist_ok=True)\n    with open(static_file_path, \"w\") as f:\n        f.write(\"Static File Content\")\n    \n    try:\n        rv = client.get(\"/static_files/test.txt\")\n        assert rv.status_code == 200\n        assert rv.data == b\"Static File Content\"\n    finally:\n        # Clean up the temporary file\n        os.remove(static_file_path)\n", "def test_blueprint_add_url_rule(app, client):\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/foo\")\n        return \"foo endpoint\"\n\n        return \"custom view\"\n\n    bp.add_url_rule(\"/custom\", endpoint=\"custom_endpoint\", view_func=custom_view_func)\n\n    app.register_blueprint(bp)\n\n    assert client.get(\"/foo\").data == b\"foo endpoint\"\n    assert client.get(\"/custom\").data == b\"custom view\"\n"], "sample_582": ["def test_find_app_by_string():\n    class Module:\n        app = Flask(\"app\")\n        create_app = lambda: Flask(\"created_app\")\n        create_app_with_args = lambda arg1, arg2: Flask(f\"created_app_{arg1}_{arg2}\")\n\n    # Test finding app by variable name\n    assert find_app_by_string(Module, \"app\").name == \"app\"\n\n    # Test finding app by function name\n    assert find_app_by_string(Module, \"create_app\").name == \"created_app\"\n\n    # Test finding app by function call with arguments\n    assert find_app_by_string(Module, 'create_app_with_args(\"foo\", \"bar\")').name == \"created_app_foo_bar\"\n\n    # Test failing to find app by invalid variable name\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"non_existent_app\")\n\n    # Test failing to find app by invalid function call\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, 'create_app_with_args(\"foo\")')\n", "def test_find_app_by_string():\n    class Module:\n        app = Flask(\"appname\")\n        create_app = staticmethod(lambda: Flask(\"createdapp\"))\n        invalid_factory = staticmethod(lambda x: Flask(\"invalidapp\"))\n        not_flask = \"not a flask app\"\n\n    # Test valid attribute access\n    assert find_app_by_string(Module, \"app\").name == \"appname\"\n\n    # Test valid factory function call without args\n    assert find_app_by_string(Module, \"create_app\").name == \"createdapp\"\n\n    # Test valid factory function call with args\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"invalid_factory(1)\")\n\n    # Test invalid attribute access\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"nonexistent\")\n\n    # Test invalid factory function call with wrong syntax\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"create_app(\")\n\n    # Test attribute that is not a Flask app\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"not_flask\")\n", "def test_find_app_by_string():\n    class Module:\n        app = Flask(\"test_app\")\n        app_factory = lambda: Flask(\"test_app_factory\")\n        app_factory_with_args = lambda x: Flask(f\"test_app_factory_{x}\")\n\n    assert find_app_by_string(Module, \"app\").name == \"test_app\"\n    assert find_app_by_string(Module, \"app_factory\").name == \"test_app_factory\"\n    assert find_app_by_string(Module, \"app_factory_with_args('arg')\").name == \"test_app_factory_arg\"\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"nonexistent_app\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"app_factory_with_args\")\n", "def test_find_app_by_string():\n    class Module:\n        app = Flask(\"appname\")\n        create_app = lambda: Flask(\"created_app\")\n        make_app = lambda: Flask(\"made_app\")\n        app_with_args = lambda x: Flask(f\"app_{x}\")\n\n    assert find_app_by_string(Module, \"app\").name == \"appname\"\n    assert find_app_by_string(Module, \"create_app\").name == \"created_app\"\n    assert find_app_by_string(Module, \"make_app\").name == \"made_app\"\n    assert find_app_by_string(Module, \"app_with_args('test')\").name == \"app_test\"\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"nonexistent_app\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"create_app('arg1', 'arg2')\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"invalid_syntax(\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"app_with_args('bad_arg')\")\n", "def test_find_app_by_string():\n    class Module:\n        app = Flask(\"appname\")\n        app_factory = staticmethod(lambda: Flask(\"appname_factory\"))\n        invalid_factory = staticmethod(lambda x: Flask(\"appname_invalid\"))\n\n    assert find_app_by_string(Module, \"app\") == Module.app\n    assert find_app_by_string(Module, \"app_factory()\").name == \"appname_factory\"\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"nonexistent\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"invalid_factory(1)\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"app_factory(invalid_syntax\")\n"], "sample_583": ["def test__index_indexer_1d():\n    array = np.arange(10)\n    # Test slice with step\n    slice_ = slice(1, 8, 2)\n    result = indexing._index_indexer_1d(slice_, slice(1, 3), len(array))\n    expected = np.array([3, 5])\n    np.testing.assert_array_equal(array[result], expected)\n    \n    # Test integer indexer\n    result = indexing._index_indexer_1d(slice_, np.array([0, 2]), len(array))\n    expected = np.array([1, 5])\n    np.testing.assert_array_equal(array[result], expected)\n\n    # Test slice(None)\n    result = indexing._index_indexer_1d(slice_, slice(None), len(array))\n    expected = np.array([1, 3, 5, 7])\n    np.testing.assert_array_equal(array[result], expected)\n\n    # Test with existing integer array\n    integer_array = np.array([1, 3, 5])\n    result = indexing._index_indexer_1d(integer_array, np.array([0, 2]), len(array))\n    expected = np.array([1, 5])\n    np.testing.assert_array_equal(array[result], expected)\n", "def test_explicit_indexing_adapter_with_mask():\n    data = np.random.randn(4, 5)\n    array = indexing.NumpyIndexingAdapter(data)\n    mask = np.array([[True, False, True, False, True],\n                     [False, True, False, True, False],\n                     [True, True, False, False, True],\n                     [False, False, True, True, False]])\n    masked_array = np.ma.array(data, mask=mask)\n    \n    explicit_adapter = indexing.ExplicitIndexingAdapter(\n        array, indexing.VectorizedIndexer)\n    actual = explicit_adapter[mask]\n    \n    expected = masked_array[mask]\n    np.testing.assert_array_equal(actual, expected)\n", "def test_expand_slice():\n    size = 10\n    slice_ = slice(2, 8, 2)\n    expected = np.array([2, 4, 6])\n    actual = indexing._expand_slice(slice_, size)\n    np.testing.assert_array_equal(expected, actual)\n\n    slice_ = slice(None, None, -1)\n    expected = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n    actual = indexing._expand_slice(slice_, size)\n    np.testing.assert_array_equal(expected, actual)\n\n    slice_ = slice(5, 2, -1)\n    expected = np.array([5, 4, 3])\n    actual = indexing._expand_slice(slice_, size)\n    np.testing.assert_array_equal(expected, actual)\n\n    slice_ = slice(-3, None, 1)\n    expected = np.array([7, 8, 9])\n    actual = indexing._expand_slice(slice_, size)\n    np.testing.assert_array_equal(expected, actual)\n\n    slice_ = slice(-3, None, -1)\n    expected = np.array([7, 6, 5, 4, 3, 2, 1, 0])\n    actual = indexing._expand_slice(slice_, size)\n    np.testing.assert_array_equal(expected, actual)\n", "def test_sanitize_slice_element():\n    from xarray.core.variable import Variable\n    from xarray.core.dataarray import DataArray\n    \n    # Test with a scalar Variable\n    scalar_var = Variable([], 5)\n    assert indexing._sanitize_slice_element(scalar_var) == 5\n\n    # Test with a non-scalar Variable\n    non_scalar_var = Variable([1], [5])\n    with pytest.raises(ValueError, match='cannot use non-scalar arrays'):\n        indexing._sanitize_slice_element(non_scalar_var)\n\n    # Test with a scalar DataArray\n    scalar_da = DataArray(5)\n    assert indexing._sanitize_slice_element(scalar_da) == 5\n\n    # Test with a non-scalar DataArray\n    non_scalar_da = DataArray([5])\n    with pytest.raises(ValueError, match='cannot use non-scalar arrays'):\n        indexing._sanitize_slice_element(non_scalar_da)\n\n    # Test with a np.timedelta64\n    td = np.timedelta64(5, 'D')\n    assert indexing._sanitize_slice_element(td) == pd.Timedelta(td)\n", "def test_explicit_indexing_adapter_vectorized():\n    data = np.random.randn(10, 5, 8)\n    adapter = indexing.ImplicitToExplicitIndexingAdapter(\n        indexing.NumpyIndexingAdapter(data), indexing.VectorizedIndexer)\n\n    indexer = indexing.VectorizedIndexer((\n        np.array([0, 2, 4]), \n        np.array([1, 3, 2]), \n        np.array([0, 1, 2])))\n    \n    expected = data[[0, 2, 4], [1, 3, 2], [0, 1, 2]]\n    actual = adapter[indexer]\n    np.testing.assert_array_equal(expected, actual)\n\n    # Check for proper handling of multidimensional indexing\n    indexer = indexing.VectorizedIndexer((\n        np.array([[0, 2], [4, 1]]), \n        np.array([[1, 3], [2, 0]]), \n        np.array([[0, 1], [2, 3]])))\n    \n    expected = data[[[0, 2], [4, 1]], [[1, 3], [2, 0]], [[0, 1], [2, 3]]]\n    actual = adapter[indexer]\n    np.testing.assert_array_equal(expected, actual)\n"], "sample_584": ["    def test_concat_once(self, create_combined_ids, concat_dim):\n        shape = (2,)\n        combined_ids = create_combined_ids(shape)\n        ds = create_test_data\n        result = _combine_all_along_first_dim(combined_ids, dim=concat_dim,\n                                              data_vars='all',\n                                              coords='different',\n                                              compat='no_conflicts')\n\n        expected_ds = concat([ds(0), ds(1)], dim=concat_dim)\n        assert_combined_tile_ids_equal(result, {(): expected_ds})\n", "    def test_different_data_vars(self):\n        ds0 = Dataset({'a': ('x', [1, 2]), 'b': ('x', [3, 4])})\n        ds1 = Dataset({'a': ('x', [5, 6]), 'c': ('x', [7, 8])})\n        \n        expected = Dataset({'a': ('x', [1, 2, 5, 6]), \n                            'b': ('x', [3, 4, np.nan, np.nan]), \n                            'c': ('x', [np.nan, np.nan, 7, 8])})\n        \n        actual = combine_nested([ds0, ds1], concat_dim='x')\n        assert_identical(expected, actual)\n", "    def test_combine_nd_with_mixed_concat_and_merge(self, create_combined_ids):\n        shape = (2, 2)\n        combined_ids = create_combined_ids(shape)\n        ds = create_test_data\n\n        # Manually modify datasets to have different variables for testing merge\n        combined_ids[(0, 0)]['new_var'] = ('dim2', [0, 0, 0, 0, 0])\n        combined_ids[(1, 1)]['new_var'] = ('dim2', [1, 1, 1, 1, 1])\n\n        result = _combine_nd(combined_ids, concat_dims=['dim1', None])\n\n        # Expected is a dataset where we first concatenate along 'dim1' and then merge the 'new_var'\n        partway1 = concat([ds(0), ds(2)], dim='dim1')\n        partway2 = concat([ds(1), ds(3)], dim='dim1')\n        \n        partway1['new_var'] = ('dim2', [0, 0, 0, 0, 0])\n        partway2['new_var'] = ('dim2', [1, 1, 1, 1, 1])\n        \n        expected = merge([partway1, partway2], compat='no_conflicts')\n\n        assert_equal(result, expected)\n", "    def test_concat_along_multiple_dims(self, create_combined_ids, concat_dim):\n        shape = (2, 3, 2)\n        combined_ids = create_combined_ids(shape)\n        result = _combine_nd(combined_ids, concat_dims=['dim1', 'dim2', concat_dim])\n\n        ds = create_test_data\n        partway1 = concat([concat([ds(0), ds(2)], dim='dim2'), concat([ds(4), ds(6)], dim='dim2')], dim='dim1')\n        partway2 = concat([concat([ds(1), ds(3)], dim='dim2'), concat([ds(5), ds(7)], dim='dim2')], dim='dim1')\n        expected = concat([partway1, partway2], dim=concat_dim)\n\n        assert_equal(result, expected)\n", "    def test_combine_nested_with_varied_depth(self):\n        ds = create_test_data\n        input = [[ds(0), [ds(1), ds(2)]], [ds(3), [ds(4), ds(5)]]]\n        \n        with raises_regex(ValueError, 'sub-lists do not have consistent depths'):\n            combine_nested(input, concat_dim=['dim1', 'dim2'])\n"], "sample_585": ["def test_unique_value_groups():\n    # Test with integers\n    ar = [1, 2, 2, 3, 1]\n    values, indices = unique_value_groups(ar)\n    expected_values = np.array([1, 2, 3])\n    expected_indices = [[0, 4], [1, 2], [3]]\n    assert np.array_equal(values, expected_values)\n    assert indices == expected_indices\n\n    # Test with strings\n    ar = ['a', 'b', 'a', 'c', 'b']\n    values, indices = unique_value_groups(ar)\n    expected_values = np.array(['a', 'b', 'c'])\n    expected_indices = [[0, 2], [1, 4], [3]]\n    assert np.array_equal(values, expected_values)\n    assert indices == expected_indices\n\n    # Test with NaN values\n    ar = [1, 2, np.nan, 1, 2, np.nan]\n    values, indices = unique_value_groups(ar)\n    expected_values = np.array([1, 2])\n    expected_indices = [[0, 3], [1, 4]]\n    assert np.array_equal(values, expected_values)\n    assert indices == expected_indices\n\n    # Test with sorting disabled\n    ar = [2, 1, 2, 3, 1]\n    values, indices = unique_value_groups(ar, sort=False)\n    expected_values = np.array([2, 1, 3])\n    expected_indices = [[0, 2], [1, 4], [3]]\n    assert np.array_equal(values, expected_values)\n    assert indices == expected_indices\n", "def test_groupby_fillna():\n    array = xr.DataArray([1, np.nan, 3, np.nan], [('x', [1, 1, 2, 2])])\n    expected = xr.DataArray([1, 0, 3, 0], [('x', [1, 1, 2, 2])])\n    actual = array.groupby('x').fillna(0)\n    assert_identical(expected, actual)\n", "def test_unique_value_groups():\n    # Test unique_value_groups function with a simple case\n    array = np.array([1, 2, 1, 3, 2])\n    expected_values = np.array([1, 2, 3])\n    expected_groups = [[0, 2], [1, 4], [3]]\n    actual_values, actual_groups = unique_value_groups(array)\n    np.testing.assert_array_equal(expected_values, actual_values)\n    assert expected_groups == actual_groups\n\n    # Test with sort=False\n    array = np.array([3, 2, 1, 3, 2])\n    expected_values = np.array([3, 2, 1])\n    expected_groups = [[0, 3], [1, 4], [2]]\n    actual_values, actual_groups = unique_value_groups(array, sort=False)\n    np.testing.assert_array_equal(expected_values, actual_values)\n    assert expected_groups == actual_groups\n\n    # Test with NaN values\n    array = np.array([1, 2, np.nan, 1, 2])\n    expected_values = np.array([1, 2])\n    expected_groups = [[0, 3], [1, 4]]\n    actual_values, actual_groups = unique_value_groups(array)\n    np.testing.assert_array_equal(expected_values, actual_values)\n    assert expected_groups == actual_groups\n\n    # Test with all unique values\n    array = np.array([1, 2, 3, 4])\n    expected_values = np.array([1, 2, 3, 4])\n    expected_groups = [[0], [1], [2], [3]]\n    actual_values, actual_groups = unique_value_groups(array)\n    np.testing.assert_array_equal(expected_values, actual_values)\n    assert expected_groups == actual_groups\n", "def test_groupby_apply_restore_coord_dims():\n    # test restore_coord_dims functionality for DataArray\n    array = xr.DataArray(\n        np.random.randn(2, 2, 2),\n        dims=['x', 'y', 'z'],\n        coords={'x': ['a', 'b'], 'y': [1, 2], 'z': [10, 20]}\n    )\n    \n        return da.mean(dim='z')\n    \n    with pytest.warns(FutureWarning):\n        result = array.groupby('x').apply(func)\n        assert result['y'].dims == ('x', 'y')\n        assert result['y'].values.tolist() == [1, 2]\n    \n    result = array.groupby('x').apply(func, restore_coord_dims=True)\n    assert result['y'].dims == ('x', 'y')\n    assert result['y'].values.tolist() == [1, 2]\n", "def test_inverse_permutation_indices():\n\n    positions = [np.array([3, 1, 2]), np.array([0, 4])]\n    expected = np.array([3, 1, 2, 0, 4])\n    actual = _inverse_permutation_indices(positions)\n    np.testing.assert_array_equal(actual, expected)\n\n    positions = [slice(0, 3), slice(3, 5)]\n    expected = None\n    actual = _inverse_permutation_indices(positions)\n    assert actual is None\n\n    positions = []\n    expected = None\n    actual = _inverse_permutation_indices(positions)\n    assert actual is None\n\n    with pytest.raises(ValueError):\n        _inverse_permutation_indices([np.array([1]), 'not a slice'])\n"], "sample_586": ["def test_concat_variable_dim_coord():\n    # Test case for _calc_concat_dim_coord function\n    from xarray.core.variable import Variable\n\n    # Test with string dimension\n    dim = \"test_dim\"\n    result_dim, result_coord = _calc_concat_dim_coord(dim)\n    assert result_dim == dim\n    assert result_coord is None\n\n    # Test with DataArray dimension\n    da = DataArray([1, 2, 3], dims=[\"test_dim\"])\n    result_dim, result_coord = _calc_concat_dim_coord(da)\n    assert result_dim == \"test_dim\"\n    assert isinstance(result_coord, DataArray)\n    assert_array_equal(result_coord.values, [1, 2, 3])\n\n    # Test with pandas Index dimension\n    idx = pd.Index([1, 2, 3], name=\"test_idx\")\n    result_dim, result_coord = _calc_concat_dim_coord(idx)\n    assert result_dim == \"test_idx\"\n    assert isinstance(result_coord, IndexVariable)\n    assert_array_equal(result_coord.values, [1, 2, 3])\n\n    # Test with Variable dimension\n    var = Variable(\"test_var\", [1, 2, 3])\n    result_dim, result_coord = _calc_concat_dim_coord(var)\n    assert result_dim == \"test_var\"\n    assert isinstance(result_coord, IndexVariable)\n    assert_array_equal(result_coord.values, [1, 2, 3])\n", "def test_concat_variable_dimension():\n    ds1 = Dataset({\"var\": ((\"x\", \"y\"), np.array([[1, 2], [3, 4]]))}, {\"x\": [1, 2], \"y\": [1, 2]})\n    ds2 = Dataset({\"var\": ((\"x\", \"y\"), np.array([[5, 6], [7, 8]]))}, {\"x\": [3, 4], \"y\": [1, 2]})\n\n    # Concatenate along existing dimension 'x'\n    expected_x = Dataset({\"var\": ((\"x\", \"y\"), np.array([[1, 2], [3, 4], [5, 6], [7, 8]]))}, {\"x\": [1, 2, 3, 4], \"y\": [1, 2]})\n    actual_x = concat([ds1, ds2], dim=\"x\")\n    assert_identical(actual_x, expected_x)\n\n    # Concatenate along existing dimension 'y'\n    expected_y = Dataset({\"var\": ((\"x\", \"y\"), np.array([[1, 2, 5, 6], [3, 4, 7, 8]]))}, {\"x\": [1, 2], \"y\": [1, 2, 1, 2]})\n    actual_y = concat([ds1, ds2], dim=\"y\")\n    assert_identical(actual_y, expected_y)\n\n    # Concatenate along new dimension 'z'\n    expected_z = Dataset({\"var\": ((\"z\", \"x\", \"y\"), np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]))}, {\"x\": [1, 2], \"y\": [1, 2], \"z\": [0, 1]})\n    actual_z = concat([ds1, ds2], dim=\"z\")\n    assert_identical(actual_z, expected_z)\n", "def test_concat_new_dim_as_array_or_index():\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2])})\n    ds2 = Dataset({\"foo\": (\"x\", [3, 4])})\n    dim_array = DataArray([10, 20], dims=\"new_dim\")\n    dim_index = pd.Index([30, 40], name=\"new_dim\")\n    \n    expected_array = Dataset({\"foo\": ((\"new_dim\", \"x\"), [[1, 2], [3, 4]]), \"new_dim\": [10, 20]})\n    expected_index = Dataset({\"foo\": ((\"new_dim\", \"x\"), [[1, 2], [3, 4]]), \"new_dim\": [30, 40]})\n    \n    actual_array = concat([ds1, ds2], dim=dim_array)\n    actual_index = concat([ds1, ds2], dim=dim_index)\n    \n    assert_identical(expected_array, actual_array)\n    assert_identical(expected_index, actual_index)\n", "    def test_concat_empty_datasets(self):\n        # Test concatenation of empty datasets\n        ds1 = Dataset()\n        ds2 = Dataset()\n        actual = concat([ds1, ds2], dim=\"x\")\n        expected = Dataset()\n        assert_identical(actual, expected)\n", "def test_concat_with_positions():\n    ds1 = Dataset(\n        {\n            \"a\": (\"x\", [1, 2]),\n            \"b\": (\"x\", [3, 4]),\n        },\n        coords={\"x\": [0, 1]},\n    )\n    ds2 = Dataset(\n        {\n            \"a\": (\"x\", [5, 6]),\n            \"b\": (\"x\", [7, 8]),\n        },\n        coords={\"x\": [2, 3]},\n    )\n    positions = [[0, 1], [2, 3]]\n    result = concat([ds1, ds2], dim=\"x\", positions=positions)\n    expected = Dataset(\n        {\n            \"a\": (\"x\", [1, 2, 5, 6]),\n            \"b\": (\"x\", [3, 4, 7, 8]),\n        },\n        coords={\"x\": [0, 1, 2, 3]},\n    )\n    assert_identical(result, expected)\n"], "sample_587": ["    def test_merge_compat_override(self):\n        ds1 = xr.Dataset({\"x\": 0, \"y\": 1})\n        ds2 = xr.Dataset({\"x\": 1, \"z\": 2})\n\n        # Test with compat set to 'override'\n        actual = ds1.merge(ds2, compat=\"override\")\n        expected = xr.Dataset({\"x\": 0, \"y\": 1, \"z\": 2})\n        assert actual.identical(expected)\n\n        # Test the reverse order, where ds2 should take precedence for 'x'\n        actual = ds2.merge(ds1, compat=\"override\")\n        expected = xr.Dataset({\"x\": 1, \"y\": 1, \"z\": 2})\n        assert actual.identical(expected)\n\n        # Ensuring no conflicts when merging with 'override'\n        ds3 = xr.Dataset({\"x\": (\"w\", [1, 2, 3])})\n        actual = ds1.merge(ds3, compat=\"override\")\n        expected = xr.Dataset({\"x\": 0, \"y\": 1})\n        assert actual.identical(expected)\n\n        # Test merging with 'override' and different dimensions\n        ds4 = xr.Dataset({\"a\": (\"w\", [1, 2, 3])})\n        actual = ds1.merge(ds4, compat=\"override\")\n        expected = xr.Dataset({\"x\": 0, \"y\": 1, \"a\": (\"w\", [1, 2, 3])})\n        assert actual.identical(expected)\n", "def test_expand_and_merge_variables():\n    data = create_test_data()\n    ds1 = data[[\"var1\"]]\n    ds2 = data[[\"var2\"]]\n    objs = [ds1, ds2]\n    expected = data[[\"var1\", \"var2\"]]\n    \n    # Test without priority_arg\n    actual = merge.expand_and_merge_variables(objs)\n    assert expected.identical(xr.Dataset(actual))\n    \n    # Test with priority_arg=0\n    actual = merge.expand_and_merge_variables(objs, priority_arg=0)\n    assert expected.identical(xr.Dataset(actual))\n    \n    # Test with priority_arg=1\n    actual = merge.expand_and_merge_variables(objs, priority_arg=1)\n    assert expected.identical(xr.Dataset(actual))\n", "    def test_merge_with_priority_arg(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"a\": (\"x\", [3, 4]), \"x\": [1, 2]})\n        ds3 = xr.Dataset({\"b\": (\"x\", [5, 6]), \"x\": [2, 3]})\n\n        # ds2 should take priority over ds1 for variable \"a\"\n        expected = xr.Dataset({\"a\": (\"x\", [1, 3, 4]), \"x\": [0, 1, 2, 3], \"b\": (\"x\", [np.nan, np.nan, 5, 6])})\n        actual = xr.merge([ds1, ds2, ds3], compat=\"no_conflicts\", priority_arg=1)\n        assert expected.identical(actual)\n\n        # ds3 should take priority over ds1 and ds2 for variable \"b\"\n        expected = xr.Dataset({\"a\": (\"x\", [1, 3, 4]), \"x\": [0, 1, 2, 3], \"b\": (\"x\", [np.nan, np.nan, 5, 6])})\n        actual = xr.merge([ds1, ds2, ds3], compat=\"no_conflicts\", priority_arg=2)\n        assert expected.identical(actual)\n", "    def test_merge_with_priority_vars(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n        expected = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n        assert expected.identical(xr.merge([ds1, ds2], compat=\"override\", join=\"right\"))\n", "    def test_merge_override_compat(self):\n        ds1 = xr.Dataset({\"x\": 0})\n        ds2 = xr.Dataset({\"x\": 1})\n        actual = ds1.merge(ds2, compat=\"override\")\n        expected = xr.Dataset({\"x\": 0})\n        assert expected.identical(actual)\n\n        actual = ds2.merge(ds1, compat=\"override\")\n        expected = xr.Dataset({\"x\": 1})\n        assert expected.identical(actual)\n\n        ds3 = xr.Dataset({\"x\": (\"y\", [1, 2])})\n        actual = ds1.merge(ds3, compat=\"override\")\n        expected = xr.Dataset({\"x\": 0})\n        assert expected.identical(actual)\n\n        actual = ds3.merge(ds1, compat=\"override\")\n        expected = xr.Dataset({\"x\": (\"y\", [1, 2])})\n        assert expected.identical(actual)\n"], "sample_588": ["    def test_combine_with_bystander_dimension(self):\n        ds1 = Dataset(\n            {\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4])},\n            coords={\"x\": [0, 1], \"y\": (\"x\", [10, 20])},\n        )\n        ds2 = Dataset(\n            {\"a\": (\"x\", [5, 6]), \"b\": (\"x\", [7, 8])},\n            coords={\"x\": [2, 3], \"y\": (\"x\", [30, 40])},\n        )\n        \n        expected = Dataset(\n            {\"a\": (\"x\", [1, 2, 5, 6]), \"b\": (\"x\", [3, 4, 7, 8])},\n            coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [10, 20, 30, 40])},\n        )\n        \n        actual = combine_by_coords([ds1, ds2])\n        assert_identical(actual, expected)\n", "    def test_combine_nested_different_shapes(self):\n        ds1 = Dataset({\"a\": ((\"x\", \"y\"), [[1, 2], [3, 4]])})\n        ds2 = Dataset({\"a\": ((\"x\", \"y\"), [[5, 6, 7], [8, 9, 10]])})\n        ds3 = Dataset({\"a\": ((\"x\", \"y\"), [[11, 12], [13, 14]])})\n        \n        expected = Dataset(\n            {\"a\": ((\"t\", \"x\", \"y\"), [[[1, 2], [3, 4]], [[5, 6, 7], [8, 9, 10]], [[11, 12], [13, 14]]])}\n        )\n        actual = combine_nested([ds1, ds2, ds3], concat_dim=\"t\")\n        assert_identical(expected, actual)\n", "    def test_empty_list_within_list(self):\n        ds = create_test_data\n        input = [[], [ds(0), ds(1)]]\n        expected = {(1, 0): ds(0), (1, 1): ds(1)}\n        actual = _infer_concat_order_from_positions(input)\n        assert_combined_tile_ids_equal(expected, actual)\n", "    def test_2d_combine_by_coords(self):\n        ds0 = Dataset({\"x\": [0, 1], \"y\": [10, 20]})\n        ds1 = Dataset({\"x\": [2, 3], \"y\": [10, 20]})\n        ds2 = Dataset({\"x\": [0, 1], \"y\": [30, 40]})\n        ds3 = Dataset({\"x\": [2, 3], \"y\": [30, 40]})\n\n        expected = {\n            (0, 0): ds0,\n            (1, 0): ds1,\n            (0, 1): ds2,\n            (1, 1): ds3,\n        }\n        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0, ds3, ds2])\n        assert_combined_tile_ids_equal(expected, actual)\n        assert concat_dims == [\"x\", \"y\"]\n", "    def test_group_datasets_correctly(self, create_combined_ids):\n        shape = (2, 2)\n        combined_ids = create_combined_ids(shape)\n        result = _combine_all_along_first_dim(\n            combined_ids,\n            dim=\"dim0\",\n            data_vars=\"all\",\n            coords=\"different\",\n            compat=\"no_conflicts\",\n        )\n\n        ds = create_test_data\n        partway1 = concat([ds(0), ds(2)], dim=\"dim0\")\n        partway2 = concat([ds(1), ds(3)], dim=\"dim0\")\n        expected = {\n            (0,): partway1,\n            (1,): partway2,\n        }\n\n        assert_combined_tile_ids_equal(result, expected)\n"], "sample_589": ["def test_spline_interpolator():\n    xi = np.array([0, 1, 2, 3, 4, 5], dtype=np.float64)\n    yi = np.array([0, 1, 0, 1, 0, 1], dtype=np.float64)\n    x = np.array([0.5, 1.5, 2.5, 3.5, 4.5], dtype=np.float64)\n    \n    interpolator = SplineInterpolator(xi, yi, method=\"spline\", order=3)\n    result = interpolator(x)\n    expected = np.array([0.5, 0.5, 0.5, 0.5, 0.5], dtype=np.float64)\n    \n    np.testing.assert_allclose(result, expected, rtol=1e-5)\n", "def test_get_clean_interp_index():\n    arr = xr.DataArray(np.random.rand(10), dims=\"x\", coords={\"x\": np.linspace(0, 10, 10)})\n    index = get_clean_interp_index(arr, dim=\"x\", use_coordinate=True)\n    expected_index = arr.coords[\"x\"].values.astype(np.float64)\n    np.testing.assert_array_equal(index, expected_index)\n\n    index_without_coord = get_clean_interp_index(arr, dim=\"x\", use_coordinate=False)\n    expected_index_without_coord = np.arange(arr.shape[0], dtype=np.float64)\n    np.testing.assert_array_equal(index_without_coord, expected_index_without_coord)\n\n    # Test with non-monotonic index\n    arr_non_monotonic = xr.DataArray(np.random.rand(10), dims=\"x\", coords={\"x\": [0, 1, 2, 3, 5, 4, 6, 7, 8, 9]})\n    with pytest.raises(ValueError, match=\"Index 'x' must be monotonically increasing\"):\n        get_clean_interp_index(arr_non_monotonic, dim=\"x\", use_coordinate=True)\n\n    # Test with duplicate values\n    arr_with_duplicates = xr.DataArray(np.random.rand(10), dims=\"x\", coords={\"x\": [0, 1, 2, 2, 4, 5, 6, 7, 8, 9]})\n    with pytest.raises(ValueError, match=\"Index 'x' has duplicate values\"):\n        get_clean_interp_index(arr_with_duplicates, dim=\"x\", use_coordinate=True)\n", "def test_numpy_interpolator_fill_value():\n    xi = np.array([0, 1, 2, 3, 4], dtype=np.float64)\n    yi = np.array([0, 1, 4, 9, 16], dtype=np.float64)\n    x = np.array([-1, 0.5, 2.5, 5], dtype=np.float64)\n\n    interpolator = NumpyInterpolator(xi, yi, fill_value=(-10, 20))\n    result = interpolator(x)\n    expected = np.array([-10, 0.25, 6.25, 20], dtype=np.float64)\n\n    assert_array_equal(result, expected)\n\n    interpolator = NumpyInterpolator(xi, yi, fill_value=0)\n    result = interpolator(x)\n    expected = np.array([0, 0.25, 6.25, 0], dtype=np.float64)\n\n    assert_array_equal(result, expected)\n", "def test_interpolate_na_max_gap_numeric():\n    da = xr.DataArray([np.nan, 1, 2, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10], dims=[\"t\"])\n    max_gap = 3\n    expected = xr.DataArray([np.nan, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, 10], dims=[\"t\"])\n    actual = da.interpolate_na(\"t\", max_gap=max_gap)\n    assert_equal(actual, expected)\n\n    max_gap = 1\n    expected = xr.DataArray([np.nan, 1, 2, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10], dims=[\"t\"])\n    actual = da.interpolate_na(\"t\", max_gap=max_gap)\n    assert_equal(actual, expected)\n", "def test_get_clean_interp_index_errors():\n    da = xr.DataArray(\n        np.array([1, 2, 3, 4, 5, 6], dtype=np.float64),\n        dims=\"x\",\n        coords={\"x\": [0, 1, 2, 3, 4, 5], \"y\": (\"x\", [10, 20, 30, 40, 50, 60])},\n    )\n\n    with raises_regex(ValueError, \"Coordinates used for interpolation must be 1D\"):\n        get_clean_interp_index(da, dim=\"x\", use_coordinate=\"y\")\n\n    da_invalid_index = xr.DataArray(\n        np.array([1, 2, 3, 4, 5, 6], dtype=np.float64),\n        dims=\"x\",\n        coords={\"x\": [0, 1, 2, 2, 4, 5]},\n    )\n\n    with raises_regex(ValueError, \"Index 'x' has duplicate values\"):\n        get_clean_interp_index(da_invalid_index, dim=\"x\")\n\n    da_non_monotonic_index = xr.DataArray(\n        np.array([1, 2, 3, 4, 5, 6], dtype=np.float64),\n        dims=\"x\",\n        coords={\"x\": [0, 2, 1, 3, 4, 5]},\n    )\n\n    with raises_regex(ValueError, \"Index 'x' must be monotonically increasing\"):\n        get_clean_interp_index(da_non_monotonic_index, dim=\"x\")\n"], "sample_590": ["def test_concat_with_new_dimension():\n    ds1 = Dataset({\"a\": (\"x\", [1, 2, 3]), \"x\": [0, 1, 2]})\n    ds2 = Dataset({\"a\": (\"x\", [4, 5, 6]), \"x\": [3, 4, 5]})\n    expected = Dataset({\"a\": ((\"new_dim\", \"x\"), [[1, 2, 3], [4, 5, 6]]), \"x\": [0, 1, 2]})\n    actual = concat([ds1, ds2], dim=\"new_dim\")\n    assert_identical(expected, actual)\n\n    # Concatenating with a new dimension using a DataArray as dim\n    new_dim = DataArray([1, 2], dims=\"new_dim\", name=\"new_dim\")\n    expected = Dataset({\"a\": ((\"new_dim\", \"x\"), [[1, 2, 3], [4, 5, 6]]), \"x\": [0, 1, 2], \"new_dim\": [1, 2]})\n    actual = concat([ds1, ds2], dim=new_dim)\n    assert_identical(expected, actual)\n\n    # Concatenating with a new dimension using a pandas Index as dim\n    new_dim_idx = pd.Index([1, 2], name=\"new_dim\")\n    expected = Dataset({\"a\": ((\"new_dim\", \"x\"), [[1, 2, 3], [4, 5, 6]]), \"x\": [0, 1, 2], \"new_dim\": [1, 2]})\n    actual = concat([ds1, ds2], dim=new_dim_idx)\n    assert_identical(expected, actual)\n", "def test_concat_with_positions():\n    # Test concatenation with specified positions\n    ds1 = Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4])})\n    ds2 = Dataset({\"a\": (\"x\", [5, 6]), \"b\": (\"x\", [7, 8])})\n    \n    # Concatenate with positions specified\n    positions = [0, 2]\n    result = concat([ds1, ds2], dim=\"x\", positions=positions)\n    expected = Dataset({\"a\": (\"x\", [1, 2, 5, 6]), \"b\": (\"x\", [3, 4, 7, 8])})\n    \n    assert_identical(result, expected)\n\n    # Concatenate with different positions\n    positions = [2, 0]\n    result = concat([ds1, ds2], dim=\"x\", positions=positions)\n    expected = Dataset({\"a\": (\"x\", [5, 6, 1, 2]), \"b\": (\"x\", [7, 8, 3, 4])})\n    \n    assert_identical(result, expected)\n\n    # Concatenate with overlapping positions\n    positions = [1, 1]\n    with raises_regex(ValueError, \"overlapping positions are not allowed\"):\n        concat([ds1, ds2], dim=\"x\", positions=positions)\n", "def test_concat_with_positions():\n    ds1 = Dataset({\"a\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]})\n    ds2 = Dataset({\"a\": (\"x\", [3, 4])}, coords={\"x\": [2, 3]})\n    positions = [np.array([0, 3]), np.array([1, 2])]\n    \n    expected = Dataset({\"a\": (\"x\", [1, 4, 3, 2])}, coords={\"x\": [0, 3, 2, 1]})\n    actual = concat([ds1, ds2], dim=\"x\", positions=positions)\n    \n    assert_identical(expected, actual)\n", "def test_concat_with_positions():\n    ds1 = Dataset({\"a\": (\"x\", [1, 2, 3])})\n    ds2 = Dataset({\"a\": (\"x\", [4, 5, 6])})\n    ds3 = Dataset({\"a\": (\"x\", [7, 8, 9])})\n\n    # Specify positions for concatenation\n    positions = [np.array([0, 1, 2]), np.array([3, 4, 5]), np.array([6, 7, 8])]\n\n    actual = concat([ds1, ds2, ds3], dim=\"x\", positions=positions)\n    expected = Dataset({\"a\": (\"x\", [1, 2, 3, 4, 5, 6, 7, 8, 9])})\n\n    assert_identical(expected, actual)\n\n    # Test with overlapping positions\n    positions_overlap = [np.array([0, 1, 2]), np.array([1, 2, 3]), np.array([2, 3, 4])]\n    actual_overlap = concat([ds1, ds2, ds3], dim=\"x\", positions=positions_overlap)\n    expected_overlap = Dataset({\"a\": (\"x\", [1, 4, 7, 8, 9])})\n\n    assert_identical(expected_overlap, actual_overlap)\n", "def test_concat_with_positions():\n    # Test the `positions` parameter to ensure datasets are concatenated at specified positions\n    ds1 = Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = Dataset({\"a\": (\"x\", [3, 4]), \"x\": [1, 2]})\n    ds3 = Dataset({\"a\": (\"x\", [5, 6]), \"x\": [2, 3]})\n\n    # Concatenate with specified positions\n    actual = concat([ds1, ds2, ds3], dim=\"y\", positions=[(0, 1), (1, 2), (2, 3)])\n    \n    expected = Dataset(\n        {\"a\": ((\"y\", \"x\"), [[1, 2, np.nan], [3, 4, np.nan], [5, 6, np.nan]]), \"x\": [0, 1, 2]},\n        coords={\"x\": [0, 1, 2]}\n    )\n    \n    assert_identical(actual, expected)\n"], "sample_591": ["def test_merge_with_conflicting_attrs():\n    # Test merging datasets with conflicting attributes\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]}, attrs={\"attr\": \"value1\"})\n    ds2 = xr.Dataset({\"a\": (\"x\", [3, 4]), \"x\": [1, 2]}, attrs={\"attr\": \"value2\"})\n    \n    with pytest.raises(xr.MergeError):\n        xr.merge([ds1, ds2], compat=\"identical\")\n    \n    # Test merging with different compatibility modes\n    ds3 = xr.Dataset({\"a\": (\"x\", [3, 4]), \"x\": [1, 2]}, attrs={\"attr\": \"value1\"})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 3, 4]), \"x\": [0, 1, 2]}, attrs={\"attr\": \"value1\"})\n    assert_identical(expected, xr.merge([ds1, ds3], compat=\"broadcast_equals\"))\n    assert_identical(expected, xr.merge([ds1, ds3], compat=\"equals\"))\n    assert_identical(expected, xr.merge([ds1, ds3], compat=\"no_conflicts\"))\n", "def test_merge_preserve_indexes():\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"x\": [0, 1, 2]})\n    ds2 = xr.Dataset({\"b\": (\"x\", [4, 5, 6]), \"x\": [1, 2, 3]})\n    \n    # Merge with inner join to check preserved indexes\n    expected = xr.Dataset({\"a\": (\"x\", [2, 3]), \"b\": (\"x\", [4, 5])}, {\"x\": [1, 2]})\n    actual = ds1.merge(ds2, join=\"inner\")\n    assert expected.identical(actual)\n    \n    # Merge with left join to check preserved indexes\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [np.nan, 4, 5])}, {\"x\": [0, 1, 2]})\n    actual = ds1.merge(ds2, join=\"left\")\n    assert expected.identical(actual)\n    \n    # Merge with right join to check preserved indexes\n    expected = xr.Dataset({\"a\": (\"x\", [2, 3, np.nan]), \"b\": (\"x\", [4, 5, 6])}, {\"x\": [1, 2, 3]})\n    actual = ds1.merge(ds2, join=\"right\")\n    assert expected.identical(actual)\n    \n    # Merge with outer join to check preserved indexes\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2, 3, np.nan]), \"b\": (\"x\", [np.nan, 4, 5, 6])}, {\"x\": [0, 1, 2, 3]}\n    )\n    actual = ds1.merge(ds2, join=\"outer\")\n    assert expected.identical(actual)\n", "def test_merge_broadcast_dims():\n    ds1 = xr.Dataset({\"a\": ((\"x\", \"y\"), [[1, 2], [3, 4]]), \"x\": [0, 1], \"y\": [0, 1]})\n    ds2 = xr.Dataset({\"b\": ((\"y\", \"z\"), [[5, 6], [7, 8]]), \"y\": [0, 1], \"z\": [0, 1]})\n    expected = xr.Dataset(\n        {\n            \"a\": ((\"x\", \"y\", \"z\"), [[[1, 1], [2, 2]], [[3, 3], [4, 4]]]),\n            \"b\": ((\"x\", \"y\", \"z\"), [[[5, 6], [7, 8]], [[5, 6], [7, 8]]]),\n            \"x\": [0, 1],\n            \"y\": [0, 1],\n            \"z\": [0, 1],\n        }\n    )\n    actual = xr.merge([ds1, ds2])\n    assert actual.identical(expected)\n", "def test_merge_with_different_dimension_lengths(self):\n    ds1 = xr.Dataset({\"a\": ((\"x\", \"y\"), [[1, 2], [3, 4]])})\n    ds2 = xr.Dataset({\"b\": ((\"x\", \"y\"), [[5, 6], [7, 8], [9, 10]])})\n    with pytest.raises(ValueError, match=\"conflicting sizes for dimension\"):\n        xr.merge([ds1, ds2])\n", "def test_merge_with_inconsistent_chunks():\n    # Create two datasets with different chunk sizes\n    ds1 = xr.Dataset({\"a\": ((\"x\", \"y\"), np.random.rand(4, 4))}).chunk({\"x\": 2, \"y\": 2})\n    ds2 = xr.Dataset({\"b\": ((\"x\", \"y\"), np.random.rand(4, 4))}).chunk({\"x\": 4, \"y\": 4})\n\n    # Merge datasets\n    merged = xr.merge([ds1, ds2])\n\n    # Check that the result has consistent chunk sizes\n    assert merged.chunks == {\"x\": (2, 2), \"y\": (2, 2)}\n\n    # Ensure the data is correctly merged\n    assert_identical(merged[\"a\"], ds1[\"a\"])\n    assert_identical(merged[\"b\"], ds2[\"b\"])\n"], "sample_592": ["def test_wrap_indent():\n    text = \"This is a sample text\\nwith multiple lines\\nto test wrapping and indenting.\"\n    start = \">> \"\n    length = 5\n    expected = \">> This is a sample text\\n     with multiple lines\\n     to test wrapping and indenting.\"\n    actual = formatting.wrap_indent(text, start, length)\n    assert actual == expected\n", "def test_format_timestamp():\n    cases = [\n        (datetime(2023, 10, 1, 14, 30, 0), \"2023-10-01T14:30:00\"),\n        (datetime(2023, 10, 1), \"2023-10-01\"),\n        (np.datetime64(\"2023-10-01T14:30:00\"), \"2023-10-01T14:30:00\"),\n        (np.datetime64(\"2023-10-01\"), \"2023-10-01\"),\n        (pd.Timestamp(\"2023-10-01T14:30:00\"), \"2023-10-01T14:30:00\"),\n        (pd.Timestamp(\"2023-10-01\"), \"2023-10-01\"),\n        (pd.Timestamp(\"NaT\"), \"NaT\"),\n        (np.datetime64('NaT'), 'NaT'),\n    ]\n    for timestamp, expected in cases:\n        actual = formatting.format_timestamp(timestamp)\n        assert expected == actual\n\n    out_of_bounds_cases = [\n        (datetime(1300, 12, 1), \"1300-12-01\"),\n        (datetime(2300, 12, 1), \"2300-12-01\"),\n        (np.datetime64(\"1000-01-01\"), \"1000-01-01\"),\n        (np.datetime64(\"3000-01-01\"), \"3000-01-01\"),\n    ]\n    for timestamp, expected in out_of_bounds_cases:\n        actual = formatting.format_timestamp(timestamp)\n        assert expected == actual\n", "    def test_wrap_indent(self):\n        text = \"This is a test string\\nwith multiple lines\\nand indentation.\"\n        start = \">>> \"\n        length = 4\n        expected = \">>> This is a test string\\n    with multiple lines\\n    and indentation.\"\n        actual = formatting.wrap_indent(text, start, length)\n        assert expected == actual\n\n        text = \"Another test\\nstring without start.\"\n        start = \"\"\n        length = 10\n        expected = \"Another test\\n          string without start.\"\n        actual = formatting.wrap_indent(text, start, length)\n        assert expected == actual\n\n        text = \"Single line test.\"\n        start = \">>> \"\n        expected = \">>> Single line test.\"\n        actual = formatting.wrap_indent(text, start)\n        assert expected == actual\n", "def test_wrap_indent():\n    text = \"Line1\\nLine2\\nLine3\"\n    expected = \"StartLine1\\n      Line2\\n      Line3\"\n    actual = formatting.wrap_indent(text, start=\"Start\", length=6)\n    assert actual == expected\n\n    text = \"SingleLine\"\n    expected = \"StartSingleLine\"\n    actual = formatting.wrap_indent(text, start=\"Start\")\n    assert actual == expected\n\n    text = \"Another\\nExample\\nWith\\nMultiple\\nLines\"\n    expected = \">>Another\\n  Example\\n  With\\n  Multiple\\n  Lines\"\n    actual = formatting.wrap_indent(text, start=\">>\", length=2)\n    assert actual == expected\n", "def test_wrap_indent():\n    text = \"This is a test string\\nwith multiple lines\\nand some more lines.\"\n    start = \">>\"\n    length = 5\n    \n    actual = formatting.wrap_indent(text, start=start, length=length)\n    expected = \">>This is a test string\\n     with multiple lines\\n     and some more lines.\"\n    assert actual == expected\n\n    actual = formatting.wrap_indent(text, start=start)\n    expected = \">>This is a test string\\n  with multiple lines\\n  and some more lines.\"\n    assert actual == expected\n\n    actual = formatting.wrap_indent(text)\n    expected = \"This is a test string\\nwith multiple lines\\nand some more lines.\"\n    assert actual == expected\n"], "sample_593": ["def test_summarize_variable():\n    var = xr.DataArray(np.random.rand(4, 6), dims=['x', 'y'])\n    name = \"test_var\"\n    formatted = fh.summarize_variable(name, var)\n    assert \"<div class='xr-var-name'><span>test_var</span></div>\" in formatted\n    assert \"<div class='xr-var-dims'>(x, y)</div>\" in formatted\n    assert \"<div class='xr-var-dtype'>float64</div>\" in formatted\n    assert \"class='xr-var-attrs-in' type='checkbox' disabled>\" in formatted\n    assert \"class='xr-var-data-in' type='checkbox'>\" in formatted\n    assert \"class='xr-var-data'>\" in formatted\n", "def test_summarize_variable():\n    var = xr.DataArray(np.random.randn(3, 4), dims=[\"x\", \"y\"], attrs={\"attr1\": \"value1\"})\n    name = \"test_var\"\n    formatted = fh.summarize_variable(name, var)\n    assert \"test_var\" in formatted\n    assert \"(x, y)\" in formatted\n    assert \"float64\" in formatted\n    assert \"value1\" in formatted\n    assert \"<pre>\" in formatted  # Check for the presence of data representation\n", "def test_summarize_variable():\n    var = xr.Variable((\"x\", \"y\"), np.random.randn(3, 2), attrs={\"units\": \"meters\"})\n    formatted = fh.summarize_variable(\"test_var\", var, is_index=True)\n    assert \"class='xr-has-index'\" in formatted\n    assert \"<div class='xr-var-name'><span class='xr-has-index'>test_var</span></div>\" in formatted\n    assert \"<div class='xr-var-dims'>(x, y)</div>\" in formatted\n    assert \"<div class='xr-var-dtype'>float64</div>\" in formatted\n    assert \"units\" in formatted\n    assert \"meters\" in formatted\n", "def test_collapsible_section():\n    section = fh.collapsible_section(\n        name=\"Test Section\",\n        inline_details=\"Test Details\",\n        details=\"Detailed content\",\n        n_items=5,\n        enabled=True,\n        collapsed=False\n    )\n    assert \"Test Section\" in section\n    assert \"Test Details\" in section\n    assert \"Detailed content\" in section\n    assert \"class='xr-section-summary'\" in section\n    assert \"type='checkbox' \" in section\n    assert \"checked\" not in section\n", "def test_collapsible_section_with_items():\n    section_name = \"Test Section\"\n    inline_details = \"Summary\"\n    details = \"Detailed content\"\n    n_items = 5\n\n    result = fh.collapsible_section(\n        name=section_name,\n        inline_details=inline_details,\n        details=details,\n        n_items=n_items,\n        enabled=True,\n        collapsed=False\n    )\n\n    assert \"Test Section: <span>(5)</span>\" in result\n    assert \"Detailed content\" in result\n    assert \"Summary\" in result\n    assert \"class='xr-section-summary-in' type='checkbox'\" in result\n"], "sample_594": ["def test_wrap_indent():\n    text = \"This is a test\\nwith multiple lines\\nand indentation.\"\n    start = \">>\"\n    length = 4\n\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    expected = \">>This is a test\\n    with multiple lines\\n    and indentation.\"\n\n    assert actual == expected\n\n    actual = formatting.wrap_indent(text, start=start)\n    expected = \">>This is a test\\n  with multiple lines\\n  and indentation.\"\n\n    assert actual == expected\n\n    actual = formatting.wrap_indent(text)\n    expected = \"This is a test\\n with multiple lines\\n and indentation.\"\n\n    assert actual == expected\n", "def test_wrap_indent():\n    cases = [\n        (\"This is a single line text.\", \"\", 4, \"This is a single line text.\"),\n        (\"This is a single line text.\", \">>\", 4, \">>This is a single line text.\"),\n        (\"First line\\nSecond line\", \">>\", 2, \">>First line\\n  Second line\"),\n        (\"First line\\nSecond line\\nThird line\", \"\", 3, \"First line\\n   Second line\\n   Third line\"),\n    ]\n    for text, start, length, expected in cases:\n        actual = formatting.wrap_indent(text, start, length)\n        assert actual == expected\n", "def test_wrap_indent():\n    text = \"This is a test\\nof the wrap_indent\\nfunction.\"\n    start = \">>\"\n    length = len(start)\n    expected = \">>This is a test\\n   of the wrap_indent\\n   function.\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert actual == expected\n\n    text = \"Another test\\nfor wrap_indent.\"\n    start = \">>>\"\n    expected = \">>>Another test\\n    for wrap_indent.\"\n    actual = formatting.wrap_indent(text, start=start)\n    assert actual == expected\n\n    text = \"Single line test\"\n    expected = \">>>Single line test\"\n    actual = formatting.wrap_indent(text, start=start)\n    assert actual == expected\n", "def test_wrap_indent():\n    text = \"This is a test\\nwith multiple lines\\nand indentation.\"\n    start = \">> \"\n    length = 4\n    actual = formatting.wrap_indent(text, start, length)\n    expected = \">> This is a test\\n    with multiple lines\\n    and indentation.\"\n    assert actual == expected\n\n    # Test with default length\n    actual = formatting.wrap_indent(text, start)\n    expected = \">> This is a test\\n       with multiple lines\\n       and indentation.\"\n    assert actual == expected\n\n    # Test with no start string\n    actual = formatting.wrap_indent(text)\n    expected = \"This is a test\\n with multiple lines\\n and indentation.\"\n    assert actual == expected\n\n    # Test with single line text\n    single_line_text = \"Single line text\"\n    actual = formatting.wrap_indent(single_line_text, start, length)\n    expected = \">> Single line text\"\n    assert actual == expected\n", "def test_wrap_indent():\n    cases = [\n        (\"simple text\", \"->\", None, \"->simple text\"),\n        (\"multi\\nline\\ntext\", \"prefix: \", 8, \"prefix: multi\\n        line\\n        text\"),\n        (\"single line\", \"\", 5, \"single line\"),\n        (\"another\\nexample\\nhere\", \">\", 2, \">another\\n  example\\n  here\"),\n    ]\n    for text, start, length, expected in cases:\n        actual = formatting.wrap_indent(text, start=start, length=length)\n        assert expected == actual\n"], "sample_595": ["def test_capitalize(dtype):\n    values = xr.DataArray([\"hello\", \"world\", \"PYTHON\", \"tEsT\"]).astype(dtype)\n    result = values.str.capitalize()\n    expected = xr.DataArray([\"Hello\", \"World\", \"Python\", \"Test\"]).astype(dtype)\n    assert_equal(result, expected)\n", "def test_repeat_zero_times(dtype):\n    values = xr.DataArray([\"a\", \"b\", \"c\", \"d\"]).astype(dtype)\n    result = values.str.repeat(0)\n    expected = xr.DataArray([\"\", \"\", \"\", \"\"]).astype(dtype)\n    assert_equal(result, expected)\n", "def test_slice_replace_edge_cases(dtype):\n    da = lambda x: xr.DataArray(x).astype(dtype)\n    values = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n\n    # Case where start index is out of bounds\n    expected = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n    result = values.str.slice_replace(100, 200, \"z\")\n    assert_equal(result, expected)\n\n    # Case where stop index is out of bounds\n    expected = da([\"shortz\", \"a bit longerz\", \"evenlongerthanthatz\", \"z\"])\n    result = values.str.slice_replace(-1, 100, \"z\")\n    assert_equal(result, expected)\n\n    # Case where both start and stop are out of bounds\n    expected = da([\"shortz\", \"a bit longerz\", \"evenlongerthanthatz\", \"z\"])\n    result = values.str.slice_replace(100, 200, \"z\")\n    assert_equal(result, expected)\n\n    # Case where start index is negative and stop index is None\n    expected = da([\"shortz\", \"a bit longerz\", \"evenlongerthanthatz\", \"z\"])\n    result = values.str.slice_replace(-1, None, \"z\")\n    assert_equal(result, expected)\n\n    # Case where start index is 0 and stop index is negative\n    expected = da([\"zshort\", \"za bit longer\", \"zevenlongerthanthat\", \"z\"])\n    result = values.str.slice_replace(0, -1, \"z\")\n    assert_equal(result, expected)\n", "def test_slice_replace_edge_cases(dtype):\n    da = lambda x: xr.DataArray(x).astype(dtype)\n    values = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n\n    # Test with start index larger than string length\n    expected = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n    result = values.str.slice_replace(start=30, repl=\"x\")\n    assert_equal(result, expected)\n\n    # Test with negative start and stop indices\n    expected = da([\"short\", \"a bit longer\", \"evenlongerthanxat\", \"\"])\n    result = values.str.slice_replace(start=-4, stop=-2, repl=\"x\")\n    assert_equal(result, expected)\n\n    # Test with start index 0 and stop index larger than string length\n    expected = da([\"x\", \"x\", \"x\", \"x\"])\n    result = values.str.slice_replace(start=0, stop=30, repl=\"x\")\n    assert_equal(result, expected)\n\n    # Test with start index 0 and stop index 0\n    expected = da([\"xshort\", \"xa bit longer\", \"xevenlongerthanthat\", \"x\"])\n    result = values.str.slice_replace(start=0, stop=0, repl=\"x\")\n    assert_equal(result, expected)\n", "def test_capitalize(dtype):\n    values = xr.DataArray([\"hello world\", \"goodbye\", \"test case\"]).astype(dtype)\n    result = values.str.capitalize()\n    expected = xr.DataArray([\"Hello world\", \"Goodbye\", \"Test case\"]).astype(dtype)\n    assert_equal(result, expected)\n"], "sample_596": ["def test_concat_with_positions():\n    # Test concatenation with the positions parameter\n    ds1 = Dataset({\"a\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"a\": (\"x\", [4, 5, 6])}, coords={\"x\": [3, 4, 5]})\n    \n    # Concatenate along dimension 'x' with specific positions\n    expected = Dataset({\"a\": (\"x\", [1, 2, 3, 4, 5, 6])}, coords={\"x\": [0, 1, 2, 3, 4, 5]})\n    actual = concat([ds1, ds2], dim=\"x\", positions=[[0, 1, 2], [3, 4, 5]])\n    assert_identical(expected, actual)\n\n    # Concatenate along a new dimension with specific positions\n    ds3 = Dataset({\"b\": (\"y\", [7, 8, 9])}, coords={\"y\": [6, 7, 8]})\n    expected = Dataset({\"a\": ((\"z\", \"x\"), [[1, 2, 3], [4, 5, 6]]), \"b\": ((\"z\", \"y\"), [[np.nan, np.nan, np.nan], [7, 8, 9]])}, coords={\"x\": [0, 1, 2, 3, 4, 5], \"y\": [6, 7, 8]})\n    actual = concat([ds1, ds2, ds3], dim=\"z\", positions=[0, 1])\n    assert_identical(expected, actual)\n", "def test_concat_infer_concat_dim_coord():\n    # Test to ensure _calc_concat_dim_coord correctly infers dimension and coordinates\n    from xarray.core.variable import Variable\n\n    da = DataArray([1, 2, 3], dims=\"x\")\n    dim_name = \"new_dim\"\n    index = pd.Index([10, 20, 30], name=dim_name)\n    \n    # Case where dim is a string\n    inferred_dim, coord = _calc_concat_dim_coord(dim_name)\n    assert inferred_dim == dim_name\n    assert coord is None\n\n    # Case where dim is a DataArray\n    inferred_dim, coord = _calc_concat_dim_coord(da)\n    assert inferred_dim == \"x\"\n    assert isinstance(coord, DataArray)\n    assert coord.equals(da)\n\n    # Case where dim is a pandas.Index\n    inferred_dim, coord = _calc_concat_dim_coord(index)\n    assert inferred_dim == dim_name\n    assert isinstance(coord, Variable)\n    assert coord.equals(IndexVariable(dim_name, index))\n\n    # Case where dim is a Variable\n    inferred_dim, coord = _calc_concat_dim_coord(Variable(\"y\", [1, 2, 3]))\n    assert inferred_dim == \"y\"\n    assert isinstance(coord, Variable)\n    assert coord.equals(Variable(\"y\", [1, 2, 3]))\n", "def test_concat_with_new_dimension():\n    # Test concatenation along a new dimension\n    ds1 = Dataset(\n        {\n            \"var1\": (\"x\", [1, 2, 3]),\n            \"var2\": (\"x\", [4, 5, 6])\n        },\n        coords={\"x\": [0, 1, 2]}\n    )\n    ds2 = Dataset(\n        {\n            \"var1\": (\"x\", [7, 8, 9]),\n            \"var2\": (\"x\", [10, 11, 12])\n        },\n        coords={\"x\": [3, 4, 5]}\n    )\n\n    expected = Dataset(\n        {\n            \"var1\": ((\"new_dim\", \"x\"), [[1, 2, 3], [7, 8, 9]]),\n            \"var2\": ((\"new_dim\", \"x\"), [[4, 5, 6], [10, 11, 12]])\n        },\n        coords={\"x\": [0, 1, 2], \"new_dim\": [0, 1]}\n    )\n\n    actual = concat([ds1, ds2], dim=\"new_dim\")\n    assert_identical(actual, expected)\n", "def test_concat_over_existing_dim():\n    ds1 = Dataset({\"data\": ((\"x\", \"y\"), [[1, 2], [3, 4]])}, coords={\"x\": [0, 1], \"y\": [0, 1]})\n    ds2 = Dataset({\"data\": ((\"x\", \"y\"), [[5, 6], [7, 8]])}, coords={\"x\": [0, 1], \"y\": [2, 3]})\n    result = concat([ds1, ds2], dim=\"y\", coords=\"minimal\")\n    expected = Dataset(\n        {\"data\": ((\"x\", \"y\"), [[1, 2, 5, 6], [3, 4, 7, 8]])}, coords={\"x\": [0, 1], \"y\": [0, 1, 2, 3]}\n    )\n    assert_identical(result, expected)\n", "def test_concat_empty_dataset():\n    # Test concatenation of empty datasets\n    ds1 = Dataset()\n    ds2 = Dataset()\n    result = concat([ds1, ds2], dim=\"x\")\n    expected = Dataset()\n    assert_identical(result, expected)\n\n    # Test concatenation of datasets with empty data variables\n    ds1 = Dataset({\"a\": (\"x\", [])})\n    ds2 = Dataset({\"a\": (\"x\", [])})\n    result = concat([ds1, ds2], dim=\"x\")\n    expected = Dataset({\"a\": (\"x\", [])})\n    assert_identical(result, expected)\n"], "sample_597": ["    def test_merge_different_variable_shapes(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"a\": ((\"y\", \"z\"), [[3, 4], [5, 6]]), \"y\": [1, 2], \"z\": [0, 1]})\n        \n        with pytest.raises(ValueError, match=\"should be coordinates or not\"):\n            xr.merge([ds1, ds2])\n        \n        ds3 = xr.Dataset({\"b\": ((\"x\", \"z\"), [[7, 8], [9, 10]])})\n        with pytest.raises(ValueError, match=\"should be coordinates or not\"):\n            xr.merge([ds2, ds3])\n", "    def test_merge_empty_datasets(self):\n        ds1 = xr.Dataset()\n        ds2 = xr.Dataset()\n        actual = xr.merge([ds1, ds2])\n        expected = xr.Dataset()\n        assert actual.identical(expected)\n", "    def test_merge_with_priority_arg(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n        ds3 = xr.Dataset({\"b\": (\"x\", [4, 5]), \"x\": [1, 2]})\n\n        expected = xr.Dataset({\"a\": (\"x\", [2, 3]), \"b\": (\"x\", [4, 5]), \"x\": [1, 2]})\n        actual = xr.merge([ds1, ds2, ds3], priority_arg=1)\n        assert expected.identical(actual)\n\n        expected = xr.Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [4, 5]), \"x\": [0, 1, 2]})\n        actual = xr.merge([ds1, ds3], priority_arg=0)\n        assert expected.identical(actual)\n\n        with pytest.raises(xr.MergeError):\n            ds4 = xr.Dataset({\"a\": (\"x\", [99, 100]), \"x\": [1, 2]})\n            xr.merge([ds1, ds4], priority_arg=1)\n", "    def test_merge_with_conflicting_indexes(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"a\": (\"x\", [3, 4]), \"x\": [2, 3]})\n        with pytest.raises(MergeError, match=\"conflicting values for index\"):\n            xr.merge([ds1, ds2])\n", "    def test_unique_variable(self):\n        v1 = xr.Variable(\"x\", [1, 2, 3])\n        v2 = xr.Variable(\"x\", [1, 2, 3])\n        v3 = xr.Variable(\"x\", [1, 2, 4])\n        variables = [v1, v2]\n        assert merge.unique_variable(\"v\", variables).identical(v1)\n\n        with pytest.raises(merge.MergeError):\n            merge.unique_variable(\"v\", [v1, v3])\n\n        variables = [v1, v3]\n        result = merge.unique_variable(\"v\", variables, compat=\"no_conflicts\")\n        expected = xr.Variable(\"x\", [1, 2, 3])\n        assert result.identical(expected)\n"], "sample_598": ["def test_wrap_indent():\n    text = \"This is a test string.\\nIt spans multiple lines.\\nEach line should be indented.\"\n    start = \">>\"\n    expected_length = len(start)\n    expected = \">>This is a test string.\\n  It spans multiple lines.\\n  Each line should be indented.\"\n    actual = formatting.wrap_indent(text, start=start, length=expected_length)\n    assert expected == actual\n\n    start = \">>\"\n    expected_length = 5\n    expected = \">>This is a test string.\\n     It spans multiple lines.\\n     Each line should be indented.\"\n    actual = formatting.wrap_indent(text, start=start, length=expected_length)\n    assert expected == actual\n", "def test_summarize_variable():\n    class DummyVariable:\n            self._in_memory = True\n            self.data = data\n            self.dims = dims\n            self.dtype = dtype\n            self._data = data\n\n            return self.data[item]\n\n    var = DummyVariable(np.arange(10), (\"dim1\",), np.int64)\n    name = \"variable_name\"\n    col_width = 20\n    marker = \"*\"\n    max_width = 50\n\n    expected = \"  * variable_name   (dim1) int64 0 1 2 3 4 5 6 7 8 9\"\n    actual = formatting.summarize_variable(name, var, col_width, marker, max_width)\n    assert actual == expected\n\n    var = DummyVariable(np.arange(100).reshape(10, 10), (\"dim1\", \"dim2\"), np.int64)\n    expected = \"  * variable_name   (dim1, dim2) int64 0 1 2 ... 98 99\"\n    actual = formatting.summarize_variable(name, var, col_width, marker, max_width)\n    assert actual == expected\n", "def test_wrap_indent():\n    cases = [\n        (\"hello\\nworld\", \"->\", \"->hello\\n   world\"),\n        (\"line1\\nline2\\nline3\", \"\", \"line1\\nline2\\nline3\"),\n        (\"line1\\nline2\\nline3\", \">>\", \">>line1\\n  line2\\n  line3\"),\n        (\"singleline\", \">>\", \">>singleline\"),\n        (\"line1\\nline2\", \">\", \">line1\\n line2\"),\n    ]\n    for text, start, expected in cases:\n        actual = formatting.wrap_indent(text, start=start, length=len(start))\n        assert actual == expected\n", "def test_wrap_indent():\n    cases = [\n        (\"This is a test string\", \"-> \", None, \"-> This is a test string\"),\n        (\"Line1\\nLine2\\nLine3\", \"  \", 2, \"  Line1\\n  Line2\\n  Line3\"),\n        (\"Indented text\", \"## \", 4, \"## Indented text\"),\n        (\"Multiline\\nText\\nHere\", \">>\", 2, \">>Multiline\\n  Text\\n  Here\"),\n    ]\n    for text, start, length, expected in cases:\n        actual = formatting.wrap_indent(text, start, length)\n        assert actual == expected\n", "def test_wrap_indent():\n    text = \"This is a test string.\\nIt has multiple lines.\\nEach line should be indented.\"\n    start = \">> \"\n    length = len(start)\n    expected = \">> This is a test string.\\n   It has multiple lines.\\n   Each line should be indented.\"\n    \n    actual = formatting.wrap_indent(text, start, length)\n    assert actual == expected\n\n    # Test with default length\n    text = \"Single line test string.\"\n    expected = \">> Single line test string.\"\n    actual = formatting.wrap_indent(text, start)\n    assert actual == expected\n\n    # Test with empty string\n    text = \"\"\n    expected = \">> \"\n    actual = formatting.wrap_indent(text, start)\n    assert actual == expected\n\n    # Test with no start and default length\n    text = \"No start string.\"\n    expected = \"No start string.\"\n    actual = formatting.wrap_indent(text)\n    assert actual == expected\n"], "sample_599": ["def test_CFScaleOffsetCoder_encode():\n    original = xr.Variable((\"x\",), [10, 20, 30], encoding={\"scale_factor\": 2.0, \"add_offset\": 1.0})\n    expected_data = (np.array([10, 20, 30], dtype=np.float32) - 1.0) / 2.0\n    expected = xr.Variable((\"x\",), expected_data, encoding={\"scale_factor\": 2.0, \"add_offset\": 1.0})\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n    assert encoded.dtype == np.float32\n", "def test_CFScaleOffsetCoder_encode():\n    original = xr.Variable(\n        (\"x\",), [10, 20, 30], encoding={\"scale_factor\": 2.0, \"add_offset\": 1.0}\n    )\n    expected_data = np.array([4.5, 9.5, 14.5], dtype=np.float32)\n    expected = xr.Variable((\"x\",), expected_data)\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert np.allclose(encoded.data, expected.data, atol=1e-6)\n    assert encoded.dims == expected.dims\n    assert encoded.dtype == expected.dtype\n    assert encoded.attrs == expected.attrs\n    assert encoded.encoding == original.encoding\n", "def test_CFScaleOffsetCoder_decode():\n    original = xr.Variable((\"x\",), [10, 20, 30], {\"scale_factor\": 2, \"add_offset\": 5})\n    expected = xr.Variable((\"x\",), [25, 45, 65])\n    coder = variables.CFScaleOffsetCoder()\n    decoded = coder.decode(original)\n    assert_identical(expected, decoded)\n", "def test_CFScaleOffsetCoder_decode():\n    original = xr.Variable((\"x\",), [0, 10, 20], {\"scale_factor\": 2, \"add_offset\": 5})\n    expected = xr.Variable((\"x\",), [5.0, 15.0, 25.0])\n    coder = variables.CFScaleOffsetCoder()\n    decoded = coder.decode(original)\n    assert_identical(expected, decoded)\n", "def test_CFScaleOffsetCoder_encode():\n    original = xr.Variable(\n        (\"x\",), [0, 10, 20], encoding={\"scale_factor\": 2.0, \"add_offset\": 1.0}\n    )\n    expected_data = np.array([-0.5, 4.5, 9.5], dtype=np.float32)\n    expected = xr.Variable((\"x\",), expected_data)\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n    assert encoded.dtype == np.float32\n"], "sample_600": ["def test_CFScaleOffsetCoder_decode():\n    original = xr.Variable((\"x\",), [10.0, 20.0, 30.0], attrs={\"scale_factor\": 2.0, \"add_offset\": 5.0})\n    expected = xr.Variable((\"x\",), [25.0, 45.0, 65.0])\n    coder = variables.CFScaleOffsetCoder()\n    decoded = coder.decode(original)\n    assert_identical(expected, decoded)\n", "def test_CFMaskCoder_encode_with_nan():\n    original = xr.Variable((\"x\",), [0.0, np.nan, 1.0], {\"_FillValue\": np.nan})\n    expected = xr.Variable((\"x\",), [0.0, np.nan, 1.0])\n    coder = variables.CFMaskCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n", "def test_CFScaleOffsetCoder_decode():\n    original = xr.Variable((\"x\",), [0, 10, 20], {\"scale_factor\": 2, \"add_offset\": 5})\n    expected = xr.Variable((\"x\",), [5, 25, 45])\n    coder = variables.CFScaleOffsetCoder()\n    decoded = coder.decode(original)\n    assert_identical(expected, decoded)\n", "def test_CFScaleOffsetCoder_decode_scale_factor_only():\n    original = xr.Variable((\"x\",), np.arange(5.0), attrs={\"scale_factor\": 2.0})\n    expected = xr.Variable((\"x\",), np.arange(0, 10, 2))\n    coder = variables.CFScaleOffsetCoder()\n    decoded = coder.decode(original)\n    assert_identical(expected, decoded)\n\n", "def test_CFScaleOffsetCoder_encode_no_scale_or_offset():\n    # Test encoding without scale_factor or add_offset\n    original = xr.Variable((\"x\",), np.arange(10.0))\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert_identical(original, encoded)\n"], "sample_601": ["def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n"], "sample_602": ["def test_normalize_path():\n    path = \"~/test.nc\"\n    normalized_path = _normalize_path(path)\n    assert normalized_path == os.path.abspath(os.path.expanduser(path))\n", "def test__get_default_engine_remote_uri():\n    with xr.set_options(enable_remote=True):\n        engine = _get_default_engine(\"http://example.org/test.nc\", allow_remote=True)\n        assert engine == \"netcdf4\"\n", "def test__normalize_path():\n    path_str = \"/example/path/to/file\"\n    path_obj = Path(\"/example/path/to/file\")\n    \n    assert _normalize_path(path_str) == os.path.abspath(os.path.expanduser(path_str))\n    assert _normalize_path(path_obj) == os.path.abspath(os.path.expanduser(str(path_obj)))\n\n    remote_path = \"http://example.org/test.nc\"\n    assert _normalize_path(remote_path) == remote_path\n\n    home_path = \"~/example/path/to/file\"\n    assert _normalize_path(home_path) == os.path.abspath(os.path.expanduser(home_path))\n", "def test__validate_dataset_names():\n    from xarray.backends.api import _validate_dataset_names\n    ds_valid = xr.Dataset({\"valid_name\": (\"dim\", [1, 2, 3])})\n    _validate_dataset_names(ds_valid)  # Should not raise any error\n\n    ds_invalid_empty = xr.Dataset({\"\": (\"dim\", [1, 2, 3])})\n    try:\n        _validate_dataset_names(ds_invalid_empty)\n    except ValueError as e:\n        assert str(e) == (\n            \"Invalid name '' for DataArray or Dataset key: \"\n            \"string must be length 1 or greater for \"\n            \"serialization to netCDF files\"\n        )\n\n    ds_invalid_type = xr.Dataset({123: (\"dim\", [1, 2, 3])})\n    try:\n        _validate_dataset_names(ds_invalid_type)\n    except TypeError as e:\n        assert str(e) == (\n            \"Invalid name 123 for DataArray or Dataset key: \"\n            \"must be either a string or None for serialization to netCDF files\"\n        )\n", "def test_normalize_path():\n    # Test with a Path object\n    path_obj = Path(\"example\")\n    expected = os.path.abspath(os.path.expanduser(\"example\"))\n    assert _normalize_path(path_obj) == expected\n\n    # Test with a string path\n    str_path = \"example\"\n    assert _normalize_path(str_path) == expected\n\n    # Test with a remote URI (no change expected)\n    remote_path = \"http://example.org/test.nc\"\n    assert _normalize_path(remote_path) == remote_path\n\n    # Test with bytes (should raise TypeError)\n    with pytest.raises(TypeError):\n        _normalize_path(b\"example\")\n"], "sample_603": ["def test_summarize_variable_with_attrs():\n    var = xr.DataArray([1, 2, 3], dims=\"x\", attrs={\"units\": \"meters\", \"description\": \"test variable\"})\n    formatted = fh.summarize_variable(\"test_var\", var)\n    assert \"test_var\" in formatted\n    assert \"(x)\" in formatted\n    assert \"int64\" in formatted\n    assert \"meters\" in formatted\n    assert \"test variable\" in formatted\n", "def test_summarize_variable_with_attrs():\n    var = xr.DataArray(np.random.RandomState(0).randn(3, 2), dims=[\"x\", \"y\"], attrs={\"unit\": \"meters\"})\n    formatted = fh.summarize_variable(\"test_var\", var)\n    assert \"test_var\" in formatted\n    assert \"(x, y)\" in formatted\n    assert \"float64\" in formatted\n    assert \"checked>\" in formatted  # Should have expanded data section by default\n    assert \"Show/Hide attributes\" in formatted\n    assert \"unit\" in formatted\n    assert \"meters\" in formatted\n", "def test_summarize_vars():\n    variables = {\n        \"var1\": xr.Variable(dims=(\"x\", \"y\"), data=np.random.rand(2, 3), attrs={\"unit\": \"m\"}),\n        \"var2\": xr.Variable(dims=(\"x\",), data=np.random.rand(2)),\n    }\n    formatted = fh.summarize_vars(variables)\n    assert \"var1\" in formatted\n    assert \"var2\" in formatted\n    assert \"unit\" in formatted\n    assert \"m\" in formatted\n", "def test_summarize_variable():\n    var = xr.DataArray([1, 2, 3], dims=\"x\", name=\"test_var\", attrs={\"units\": \"m/s\"})\n    name = \"test_var\"\n    formatted = fh.summarize_variable(name, var)\n    assert \"test_var\" in formatted\n    assert \"(x)\" in formatted\n    assert \"int64\" in formatted\n    assert \"1, 2, 3\" in formatted\n    assert \"units\" in formatted\n    assert \"m/s\" in formatted\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"<p>Inline details</p>\"\n    details = \"<p>Section details</p>\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    section = fh.collapsible_section(\n        name, inline_details, details, n_items, enabled, collapsed\n    )\n    assert name in section\n    assert inline_details in section\n    assert details in section\n    assert f\"({n_items})\" in section\n    assert \"type='checkbox' \" in section\n    assert \"checked\" in section\n"], "sample_604": ["def test_format_timedelta(self):\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\", None),\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days\", \"date\"),\n        (pd.Timedelta(\"10 days 1 hour\"), \"01:00:00\", \"time\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\", None),\n        (pd.Timedelta(\"3 hours\"), \"0 days 03:00:00\", None),\n        (pd.Timedelta(\"NaT\"), \"NaT\", None),\n    ]\n    for item, expected, fmt in cases:\n        actual = formatting.format_timedelta(item, fmt)\n        assert expected == actual\n", "def test_summarize_variable():\n    class MockVariable:\n            self.dims = dims\n            self.dtype = dtype\n            self.values = values\n            self._data = values\n            self._in_memory = True\n        \n            return self.values[key]\n        \n        @property\n            return self.values.shape\n    \n    name = 'test_var'\n    dims = ('x', 'y')\n    dtype = np.int32\n    values = np.array([[1, 2], [3, 4]], dtype=dtype)\n    var = MockVariable(dims, dtype, values)\n    col_width = 20\n    \n    expected = \"  test_var     (x, y) int32 1 2 3 4\"\n    actual = formatting.summarize_variable(name, var, col_width)\n    assert actual == expected\n", "def test_format_timedelta():\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), None, \"10 days 01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), None, \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), None, \"0 days 03:00:00\"),\n        (pd.Timedelta(\"NaT\"), None, \"NaT\"),\n        (pd.Timedelta(\"10 days\"), \"date\", \"10 days\"),\n        (pd.Timedelta(\"1 hour\"), \"time\", \"01:00:00\"),\n    ]\n    for item, format, expected in cases:\n        actual = formatting.format_timedelta(item, timedelta_format=format)\n        assert expected == actual\n", "def test_summarize_variable():\n    class MockVariable:\n            self.dims = dims\n            self.dtype = dtype\n            self._in_memory = in_memory\n            self._data = data if data is not None else np.random.rand(10, 5)\n\n            return self\n\n        @property\n            return self._data\n\n    name = \"test_var\"\n    var = MockVariable(dims=(\"x\", \"y\"), dtype=np.float32)\n    col_width = 15\n    marker = \"*\"\n    max_width = 80\n\n    # Test with in_memory variable\n    expected = formatting.pretty_print(f\"  * {name} \", col_width) + \"(x, y) float32 \" + formatting.format_array_flat(var.data, max_width - col_width - len(\"(x, y) float32 \"))\n    actual = formatting.summarize_variable(name, var, col_width, marker, max_width)\n    assert expected == actual\n\n    # Test with non in_memory variable\n    var._in_memory = False\n    var._data = MockVariable(dims=(\"x\", \"y\"), dtype=np.float32, in_memory=False, data=None)\n    expected = formatting.pretty_print(f\"  * {name} \", col_width) + \"(x, y) float32 \" + formatting.inline_variable_array_repr(var, max_width - col_width - len(\"(x, y) float32 \"))\n    actual = formatting.summarize_variable(name, var, col_width, marker, max_width)\n    assert expected == actual\n", "def test_summarize_variable():\n    array = np.random.rand(10, 5)\n    variable = xr.Variable((\"x\", \"y\"), array)\n    name = \"test_var\"\n    col_width = 20\n    max_width = 80\n    actual = formatting.summarize_variable(name, variable, col_width, max_width)\n    \n    # Ensure the result is a string and contains key parts of the summarization\n    assert isinstance(actual, str)\n    assert \"test_var\" in actual\n    assert \"float\" in actual  # Checking if dtype is mentioned\n    assert \"x\" in actual\n    assert \"y\" in actual\n    assert str(array[0, 0])[:5] in actual  # Check if some data representation is in the summary\n"], "sample_605": ["def test_groupby_fillna():\n    data = xr.DataArray(\n        [1, np.nan, 3, np.nan, 5, np.nan], \n        dims=\"x\", \n        coords={\"x\": [1, 1, 2, 2, 3, 3]}\n    )\n    \n    expected = xr.DataArray(\n        [1, 1, 3, 3, 5, 5],\n        dims=\"x\",\n        coords={\"x\": [1, 1, 2, 2, 3, 3]}\n    )\n    \n    actual = data.groupby(\"x\").fillna(1)\n    assert_identical(expected, actual)\n\n    ds = xr.Dataset(\n        {\"foo\": ((\"x\", \"y\"), [[np.nan, 2], [np.nan, 4], [5, np.nan], [7, 8]])},\n        coords={\"x\": [1, 1, 2, 2], \"y\": [1, 2]}\n    )\n    \n    expected_ds = xr.Dataset(\n        {\"foo\": ((\"x\", \"y\"), [[3, 2], [3, 4], [5, 6], [7, 8]])},\n        coords={\"x\": [1, 1, 2, 2], \"y\": [1, 2]}\n    )\n    \n    actual_ds = ds.groupby(\"x\").fillna(3)\n    assert_identical(expected_ds, actual_ds)\n", "def test_groupby_fillna():\n    # Test fillna within groups\n    array = xr.DataArray(\n        [1, np.nan, 3, np.nan, 5, 6, np.nan],\n        coords={\"x\": [1, 1, 1, 2, 2, 2, 2]},\n        dims=\"x\"\n    )\n\n    # Expected output where NaNs are filled with the value 0 within each group\n    expected = xr.DataArray(\n        [1, 0, 3, 0, 5, 6, 0],\n        coords={\"x\": [1, 1, 1, 2, 2, 2, 2]},\n        dims=\"x\"\n    )\n\n    actual = array.groupby(\"x\").fillna(0)\n    assert_identical(expected, actual)\n\n    # Test filling with a scalar value within each group\n    expected = xr.DataArray(\n        [1, 99, 3, 99, 5, 6, 99],\n        coords={\"x\": [1, 1, 1, 2, 2, 2, 2]},\n        dims=\"x\"\n    )\n\n    actual = array.groupby(\"x\").fillna(99)\n    assert_identical(expected, actual)\n\n    # Test filling with a DataArray\n    fill_array = xr.DataArray(\n        [7, 8],\n        coords={\"x\": [1, 2]},\n        dims=\"x\"\n    )\n\n    expected = xr.DataArray(\n        [1, 7, 3, 8, 5, 6, 8],\n        coords={\"x\": [1, 1, 1, 2, 2, 2, 2]},\n        dims=\"x\"\n    )\n\n    actual = array.groupby(\"x\").fillna(fill_array)\n    assert_identical(expected, actual)\n", "def test_groupby_apply_nan_handling():\n    # Test the handling of NaNs within groupby apply operations\n    array = xr.DataArray([1, 2, np.nan, 4, 5, np.nan], dims=\"x\", coords={\"x\": [0, 0, 1, 1, 2, 2]})\n    expected = xr.DataArray([1.5, 4.0, 5.0], dims=\"x\", coords={\"x\": [0, 1, 2]})\n    actual = array.groupby(\"x\").apply(lambda x: x.mean(skipna=True))\n    assert_identical(expected, actual)\n\n    # Check if the function handles NaNs correctly when they are not skipped\n    with pytest.raises(ValueError):\n        array.groupby(\"x\").apply(lambda x: x.mean(skipna=False))\n", "def test_groupby_first_last():\n    array = xr.DataArray(\n        data=[3, 1, 2, 5, 6, 4], coords={\"x\": [1, 1, 2, 2, 3, 3]}, dims=\"x\"\n    )\n\n    expected_first = xr.DataArray(data=[3, 2, 6], coords={\"x\": [1, 2, 3]}, dims=\"x\")\n    actual_first = array.groupby(\"x\").first()\n    assert_identical(expected_first, actual_first)\n\n    expected_last = xr.DataArray(data=[1, 5, 4], coords={\"x\": [1, 2, 3]}, dims=\"x\")\n    actual_last = array.groupby(\"x\").last()\n    assert_identical(expected_last, actual_last)\n", "def test_unique_value_groups():\n    # Test with a simple array\n    ar = np.array([1, 2, 2, 3, 1, 4])\n    values, indices = unique_value_groups(ar)\n    expected_values = np.array([1, 2, 3, 4])\n    expected_indices = [[0, 4], [1, 2], [3], [5]]\n    np.testing.assert_array_equal(values, expected_values)\n    assert indices == expected_indices\n\n    # Test with unsorted array and sort=False\n    ar = np.array([3, 2, 2, 1, 3, 4])\n    values, indices = unique_value_groups(ar, sort=False)\n    expected_values = np.array([3, 2, 1, 4])\n    expected_indices = [[0, 4], [1, 2], [3], [5]]\n    np.testing.assert_array_equal(values, expected_values)\n    assert indices == expected_indices\n\n    # Test with array containing NaNs\n    ar = np.array([1, 2, np.nan, 1, 2, np.nan])\n    values, indices = unique_value_groups(ar)\n    expected_values = np.array([1.0, 2.0])\n    expected_indices = [[0, 3], [1, 4]]\n    np.testing.assert_array_equal(values, expected_values)\n    assert indices == expected_indices\n\n    # Test with empty array\n    ar = np.array([])\n    values, indices = unique_value_groups(ar)\n    expected_values = np.array([])\n    expected_indices = []\n    np.testing.assert_array_equal(values, expected_values)\n    assert indices == expected_indices\n"], "sample_606": ["def test_apply_array_ufunc() -> None:\n    arr1 = np.array([1, 2, 3])\n    arr2 = np.array([4, 5, 6])\n    result = apply_array_ufunc(operator.add, arr1, arr2)\n    expected = np.array([5, 7, 9])\n    assert_array_equal(result, expected)\n\n    # Testing with dask arrays\n    if has_dask:\n        import dask.array as da\n\n        darr1 = da.from_array(arr1, chunks=(2,))\n        darr2 = da.from_array(arr2, chunks=(2,))\n        result = apply_array_ufunc(operator.add, darr1, darr2, dask=\"allowed\")\n        expected = da.from_array(expected, chunks=(2,))\n        assert_array_equal(result.compute(), expected.compute())\n\n        with pytest.raises(ValueError, match=\"unknown setting for dask array handling\"):\n            apply_array_ufunc(operator.add, darr1, darr2, dask=\"unknown\")\n", "def test_apply_dataarray_vfunc() -> None:\n        return variable * 2\n\n    array = np.arange(5)\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    \n    result = apply_dataarray_vfunc(multiply_by_two, data_array, signature=_UFuncSignature([(\"x\",)], [(\"x\",)]))\n    \n    expected_data_array = xr.DataArray(array * 2, dims=[\"x\"], coords={\"x\": -array})\n    assert_identical(expected_data_array, result)\n\n    # Test with exclude_dims\n    result = apply_dataarray_vfunc(multiply_by_two, data_array, signature=_UFuncSignature([(\"x\",)], [(\"x\",)]), exclude_dims={\"x\"})\n    assert_identical(expected_data_array, result)\n\n    # Test with keep_attrs\n    data_array.attrs[\"test_attr\"] = \"test\"\n    result = apply_dataarray_vfunc(multiply_by_two, data_array, signature=_UFuncSignature([(\"x\",)], [(\"x\",)]), keep_attrs=\"override\")\n    expected_data_array.attrs[\"test_attr\"] = \"test\"\n    assert_identical(expected_data_array, result)\n", "def test_apply_ufunc_output_dtype() -> None:\n        return x.astype(np.float32)\n\n    array = np.array([1, 2, 3], dtype=np.int64)\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    expected_array = array.astype(np.float32)\n    expected_variable = xr.Variable(\"x\", expected_array)\n    expected_data_array = xr.DataArray(expected_variable, [(\"x\", -array)])\n    expected_dataset = xr.Dataset({\"y\": expected_variable}, {\"x\": -array})\n\n    assert_identical(expected_array, apply_ufunc(custom_func, array, output_dtypes=[np.float32]))\n    assert_identical(expected_variable, apply_ufunc(custom_func, variable, output_dtypes=[np.float32]))\n    assert_identical(expected_data_array, apply_ufunc(custom_func, data_array, output_dtypes=[np.float32]))\n    assert_identical(expected_dataset, apply_ufunc(custom_func, dataset, output_dtypes=[np.float32]))\n", "def test_apply_groupby_func() -> None:\n    array = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    data_array = xr.DataArray(array, dims=[\"x\", \"y\"], coords={\"x\": [\"a\", \"b\", \"a\", \"b\"]})\n\n        return x.mean(dim=\"y\")\n\n    expected = xr.DataArray([1.5, 3.5, 5.5, 7.5], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"a\", \"b\"]})\n    actual = apply_groupby_func(func, data_array.groupby(\"x\"))\n    assert_identical(expected, actual)\n\n    # Test with a Dataset\n    dataset = xr.Dataset({\"data\": data_array})\n    expected_ds = xr.Dataset({\"data\": expected})\n    actual_ds = apply_groupby_func(func, dataset.groupby(\"x\"))\n    assert_identical(expected_ds, actual_ds)\n\n    # Test with a mix of GroupBy and other objects\n    other_array = xr.DataArray([1, 2], dims=\"y\")\n    expected_mixed = xr.DataArray([[2.5, 4.5], [3.5, 5.5]], dims=[\"x\", \"y\"], coords={\"x\": [\"a\", \"b\"], \"y\": [0, 1]})\n    actual_mixed = apply_groupby_func(lambda x, y: x + y, data_array.groupby(\"x\"), other_array)\n    assert_identical(expected_mixed, actual_mixed)\n", "def test_cross_product() -> None:\n    a = xr.DataArray([1, 2, 3], dims=[\"dim_0\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"dim_0\"])\n    expected = xr.DataArray([-3, 6, -3], dims=[\"dim_0\"])\n    result = xr.cross(a, b, dim=\"dim_0\")\n    assert_identical(result, expected)\n\n    a_2d = xr.DataArray([[1, 2], [3, 4]], dims=[\"x\", \"dim_0\"])\n    b_2d = xr.DataArray([[5, 6], [7, 8]], dims=[\"x\", \"dim_0\"])\n    expected_2d = xr.DataArray([[-4, 8], [-4, 8]], dims=[\"x\"])\n    result_2d = xr.cross(a_2d, b_2d, dim=\"dim_0\")\n    assert_identical(result_2d, expected_2d)\n\n    a_mixed = xr.DataArray([1, 2], dims=[\"dim_0\"], coords={\"dim_0\": [\"x\", \"y\"]})\n    b_mixed = xr.DataArray([3, 4, 5], dims=[\"dim_0\"], coords={\"dim_0\": [\"x\", \"y\", \"z\"]})\n    expected_mixed = xr.DataArray([8, -2, -1], dims=[\"dim_0\"], coords={\"dim_0\": [\"x\", \"y\", \"z\"]})\n    result_mixed = xr.cross(a_mixed, b_mixed, dim=\"dim_0\")\n    assert_identical(result_mixed, expected_mixed)\n\n    # Test with non-matching dimension sizes\n    a_diff = xr.DataArray([1, 2], dims=[\"dim_0\"])\n    b_diff = xr.DataArray([3, 4, 5], dims=[\"dim_0\"])\n    with pytest.raises(ValueError, match=r\"Dimension 'dim_0' on a must be 1, 2, or 3 to be compatible with a cross product but is 3\"):\n        xr.cross(a_diff, b_diff, dim=\"dim_0\")\n\n    # Test with non-matching dimension names\n    a_diff_names = xr.DataArray([1, 2], dims=[\"dim_0\"])\n    b_diff_names = xr.DataArray([3"], "sample_607": ["def test_get_backend():\n    backend_name = \"dummy\"\n    mock_backend = mock.MagicMock(spec=common.BackendEntrypoint)\n    engines = {backend_name: mock_backend}\n\n    with mock.patch(\"xarray.backends.plugins.list_engines\", return_value=engines):\n        backend = plugins.get_backend(backend_name)\n        assert backend == mock_backend\n\n    with pytest.raises(ValueError, match=f\"unrecognized engine not_a_valid_engine must be one of: {list(engines)}\"):\n        plugins.get_backend(\"not_a_valid_engine\")\n\n    with pytest.raises(TypeError, match=\"engine must be a string or a subclass of xarray.backends.BackendEntrypoint\"):\n        plugins.get_backend(123)\n", "def test_detect_parameters():\n        pass\n\n        pass\n\n        pass\n\n    assert plugins.detect_parameters(func_without_args) == ()\n    assert plugins.detect_parameters(func_with_args) == (\"arg1\", \"arg2\")\n    assert plugins.detect_parameters(func_with_kwargs) == (\"arg1\", \"arg2\", \"kwarg1\", \"kwarg2\")\n\n    with pytest.raises(TypeError, match=r\"\\*args and \\*\\*kwargs is not supported\"):\n        plugins.detect_parameters(DummyBackendEntrypointArgs.open_dataset)\n\n    with pytest.raises(TypeError, match=r\"\\*args and \\*\\*kwargs is not supported\"):\n        plugins.detect_parameters(DummyBackendEntrypointKwargs.open_dataset)\n", "def test_get_backend():\n    dummy_backend_name = \"dummy_backend\"\n    dummy_backend_entrypoint = DummyBackendEntrypoint1()\n\n    with mock.patch(\n        \"xarray.backends.plugins.list_engines\",\n        return_value={dummy_backend_name: dummy_backend_entrypoint},\n    ):\n        backend = plugins.get_backend(dummy_backend_name)\n        assert isinstance(backend, DummyBackendEntrypoint1)\n\n    with mock.patch(\n        \"xarray.backends.plugins.list_engines\",\n        return_value={dummy_backend_name: dummy_backend_entrypoint},\n    ):\n        with pytest.raises(ValueError, match=f\"unrecognized engine invalid_backend must be one of:\"):\n            plugins.get_backend(\"invalid_backend\")\n\n    with pytest.raises(TypeError, match=\"engine must be a string or a subclass of xarray.backends.BackendEntrypoint\"):\n        plugins.get_backend(123)\n", "def test_get_backend_by_string():\n    backend_name = \"dummy_backend\"\n    dummy_backend = DummyBackendEntrypoint1()\n    with mock.patch(\"xarray.backends.plugins.list_engines\", return_value={backend_name: dummy_backend}):\n        backend = plugins.get_backend(backend_name)\n    assert backend == dummy_backend\n\n", "def test_get_backend_unrecognized_engine():\n    with mock.patch(\n        \"xarray.backends.plugins.list_engines\",\n        mock.MagicMock(return_value={\"engine1\": DummyBackendEntrypoint1()}),\n    ):\n        with pytest.raises(ValueError, match=r\"unrecognized engine\"):\n            plugins.get_backend(\"invalid_engine\")\n\n"], "sample_608": ["def test_wrap_indent() -> None:\n    cases = [\n        (\"single line\", \"  \", \"  single line\"),\n        (\"first\\nsecond\", \">>\", \">>first\\n  second\"),\n        (\"a\\nb\\nc\", \"start-\", \"start-a\\n      b\\n      c\"),\n        (\"one line\", \"\", \"one line\"),\n    ]\n    for text, start, expected in cases:\n        actual = formatting.wrap_indent(text, start)\n        assert actual == expected\n", "def test_wrap_indent() -> None:\n    text = \"This is a test\\nwith multiple\\nlines\"\n    result = formatting.wrap_indent(text, start=\">>\", length=4)\n    expected = \">>This is a test\\n    with multiple\\n    lines\"\n    assert result == expected\n\n    result = formatting.wrap_indent(text, start=\">>\")\n    expected = \">>This is a test\\nwith multiple\\nlines\"\n    assert result == expected\n\n    result = formatting.wrap_indent(text)\n    expected = \"This is a test\\nwith multiple\\nlines\"\n    assert result == expected\n", "def test_inline_dask_repr() -> None:\n    import dask.array as da\n    \n    # Create a simple dask array\n    dask_arr = da.from_array(np.arange(10), chunks=(5,))\n\n    # Actual and expected representation\n    actual = formatting.inline_dask_repr(dask_arr)\n    expected = \"dask.array<chunksize=(5,), meta=np.ndarray>\"\n\n    assert actual == expected\n\n", "def test_wrap_indent() -> None:\n    text = \"This is a test string\\nwith multiple lines\\nthat should be indented correctly.\"\n    start = \">> \"\n    expected = \">> This is a test string\\n   with multiple lines\\n   that should be indented correctly.\"\n    actual = formatting.wrap_indent(text, start=start)\n    assert actual == expected\n\n    text = \"Single line test\"\n    start = \"-- \"\n    expected = \"-- Single line test\"\n    actual = formatting.wrap_indent(text, start=start)\n    assert actual == expected\n\n    text = \"Another test\\nwith\\nshort lines.\"\n    start = \"\"\n    expected = \"Another test\\nwith\\nshort lines.\"\n    actual = formatting.wrap_indent(text, start=start)\n    assert actual == expected\n\n    text = \"Indent this\"\n    start = \">\"\n    expected = \">Indent this\"\n    actual = formatting.wrap_indent(text, start=start)\n    assert actual == expected\n", "def test_wrap_indent() -> None:\n    text = \"This is a sample text\\nthat spans multiple lines\\nfor testing wrap_indent.\"\n    start = \"Prefix: \"\n    expected = \"Prefix: This is a sample text\\n        that spans multiple lines\\n        for testing wrap_indent.\"\n    actual = formatting.wrap_indent(text, start)\n    assert expected == actual\n\n    text = \"Single line text.\"\n    start = \">> \"\n    expected = \">> Single line text.\"\n    actual = formatting.wrap_indent(text, start)\n    assert expected == actual\n"], "sample_609": ["def test_apply_output_core_dimension_mismatch() -> None:\n        return np.stack([x, -x], axis=-1)\n\n    array = np.array([[1, 2], [3, 4]])\n    variable = xr.Variable([\"x\", \"y\"], array)\n    data_array = xr.DataArray(variable, {\"x\": [\"a\", \"b\"], \"y\": [-1, -2]})\n    dataset = xr.Dataset({\"data\": data_array})\n\n    # This should fail because the output_core_dims argument does not match the output shape\n    with pytest.raises(ValueError, match=\"unexpected number of dimensions\"):\n        apply_ufunc(func, variable, output_core_dims=[[\"sign1\", \"sign2\"]])\n\n    with pytest.raises(ValueError, match=\"unexpected number of dimensions\"):\n        apply_ufunc(func, data_array, output_core_dims=[[\"sign1\", \"sign2\"]])\n\n    with pytest.raises(ValueError, match=\"unexpected number of dimensions\"):\n        apply_ufunc(func, dataset, output_core_dims=[[\"sign1\", \"sign2\"]])\n", "def test_broadcast_compat_data_errors() -> None:\n    data = np.arange(6).reshape(2, 3)\n    var = xr.Variable([\"x\", \"y\"], data)\n\n    # Case where there are unexpected dimensions in the input variable\n    with pytest.raises(ValueError, match=\"unexpected dimensions\"):\n        broadcast_compat_data(var, [\"x\"], [\"z\"])\n\n    # Case where there are missing core dimensions\n    with pytest.raises(ValueError, match=\"absent on an input variable\"):\n        broadcast_compat_data(var, [\"x\", \"y\"], [\"x\", \"y\", \"z\"])\n", "def test_apply_ufunc_return_types() -> None:\n        return x + 1\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": array})\n\n    assert isinstance(apply_ufunc(func, array), np.ndarray)\n    assert isinstance(apply_ufunc(func, variable), xr.Variable)\n    assert isinstance(apply_ufunc(func, data_array), xr.DataArray)\n    assert isinstance(apply_ufunc(func, dataset), xr.Dataset)\n\n    # Check types for grouped data\n    assert isinstance(apply_ufunc(func, data_array.groupby(\"x\")), xr.DataArray)\n    assert isinstance(apply_ufunc(func, dataset.groupby(\"x\")), xr.Dataset)\n\n    # Check types for multiple outputs\n        return x + 1, x - 1\n\n    out0, out1 = apply_ufunc(multi_output_func, array, output_core_dims=[[], []])\n    assert isinstance(out0, np.ndarray)\n    assert isinstance(out1, np.ndarray)\n\n    out0, out1 = apply_ufunc(multi_output_func, variable, output_core_dims=[[], []])\n    assert isinstance(out0, xr.Variable)\n    assert isinstance(out1, xr.Variable)\n\n    out0, out1 = apply_ufunc(multi_output_func, data_array, output_core_dims=[[], []])\n    assert isinstance(out0, xr.DataArray)\n    assert isinstance(out1, xr.DataArray)\n\n    out0, out1 = apply_ufunc(multi_output_func, dataset, output_core_dims=[[], []])\n    assert isinstance(out0, xr.Dataset)\n    assert isinstance(out1, xr.Dataset)\n\n    # Check types for grouped data with multiple outputs\n    out0, out1 = apply_ufunc(multi_output_func, data_array.groupby(\"x\"), output_core_dims=[[], []])\n    assert isinstance(out0, xr.DataArray)\n    assert isinstance(out1, xr.DataArray)\n\n    out0, out1 = apply_ufunc(multi_output_func, dataset.groupby(\"x\"), output_core_dims=[[], []])\n    assert isinstance(out0, xr.Dataset)\n    assert isinstance(out1, xr.Dataset)\n", "def test_apply_output_core_dimension_with_new_dim() -> None:\n            return np.expand_dims(x, axis=-1)\n\n        return apply_ufunc(func, obj, output_core_dims=[[\"new_dim\"]])\n\n    array = np.array([[1, 2], [3, 4]])\n    variable = xr.Variable([\"x\", \"y\"], array)\n    data_array = xr.DataArray(variable, {\"x\": [\"a\", \"b\"], \"y\": [-1, -2]})\n    dataset = xr.Dataset({\"data\": data_array})\n\n    expected_variable = xr.Variable([\"x\", \"y\", \"new_dim\"], array[:, :, None])\n    expected_data_array = xr.DataArray(expected_variable, {\"x\": [\"a\", \"b\"], \"y\": [-1, -2], \"new_dim\": [0]})\n    expected_dataset = xr.Dataset({\"data\": expected_data_array})\n\n    assert_identical(expected_variable, add_new_dim(variable))\n    assert_identical(expected_data_array, add_new_dim(data_array))\n    assert_identical(expected_dataset, add_new_dim(dataset))\n", "def test_apply_ufunc_complex_function() -> None:\n    \"\"\"Test apply_ufunc with a more complex function involving multiple operations.\"\"\"\n        return np.sin(a) + np.cos(b) * np.log(c)\n\n    array_a = np.arange(1, 10, dtype=np.float64)\n    array_b = np.arange(1, 10, dtype=np.float64)\n    array_c = np.arange(1, 10, dtype=np.float64)\n    variable_a = xr.Variable(\"x\", array_a)\n    variable_b = xr.Variable(\"x\", array_b)\n    variable_c = xr.Variable(\"x\", array_c)\n    data_array_a = xr.DataArray(variable_a, [(\"x\", array_a)])\n    data_array_b = xr.DataArray(variable_b, [(\"x\", array_b)])\n    data_array_c = xr.DataArray(variable_c, [(\"x\", array_c)])\n    dataset_a = xr.Dataset({\"a\": variable_a}, {\"x\": array_a})\n    dataset_b = xr.Dataset({\"b\": variable_b}, {\"x\": array_b})\n    dataset_c = xr.Dataset({\"c\": variable_c}, {\"x\": array_c})\n\n    apply_complex_func = functools.partial(apply_ufunc, complex_func, input_core_dims=[[], [], []], output_core_dims=[[]])\n\n    assert_identical(complex_func(array_a, array_b, array_c), apply_complex_func(array_a, array_b, array_c))\n    assert_identical(complex_func(array_a, array_b, array_c), apply_complex_func(variable_a, variable_b, variable_c))\n    assert_identical(xr.DataArray(complex_func(array_a, array_b, array_c), [(\"x\", array_a)]), apply_complex_func(data_array_a, data_array_b, data_array_c))\n    assert_identical(xr.Dataset({\"complex_result\": (\"x\", complex_func(array_a, array_b, array_c))}, {\"x\": array_a}), apply_complex_func(dataset_a, dataset_b, dataset_c).rename({\"a\": \"complex_result\"}))\n"], "sample_610": ["def test_get_value_from_series_with_boolean_index():\n    date_type = _all_cftime_date_types()[\"standard\"]\n    index = CFTimeIndex([date_type(2000, 1, 1), date_type(2000, 1, 2), date_type(2000, 1, 3)])\n    series = pd.Series([10, 20, 30], index=index)\n    result = series[index == date_type(2000, 1, 2)]\n    expected = pd.Series([20], index=[date_type(2000, 1, 2)])\n    assert result.equals(expected)\n", "def test_cftimeindex_add_timedelta_types(index, input, expected):\n    result = index + input\n    assert result.equals(expected)\n    assert isinstance(result, CFTimeIndex)\n", "def test_cftimeindex_constructor_invalid_date_type():\n    import cftime\n\n    invalid_date_type = \"invalid_date_type\"\n    dates = [invalid_date_type(1, 1, 1), invalid_date_type(1, 2, 1)]\n    with pytest.raises(TypeError, match=\"CFTimeIndex requires cftime.datetime objects\"):\n        CFTimeIndex(dates)\n", "def test_parse_iso8601_like_invalid_strings():\n    # Test various invalid string formats\n    invalid_strings = [\n        \"99\",\n        \"199-01-01\",\n        \"1999-1-01\",\n        \"1999-01-1\",\n        \"1999-01-01T1\",\n        \"1999-01-01T12:3\",\n        \"1999-01-01T12:34:5\",\n        \"1999-01-01T12:34:56.789\",\n        \"1999/01/01\",\n        \"1999-01-01 12:34:56.789\",\n        \"19990101T123456789\",\n        \"\",\n        \"1999-13-01\",\n        \"1999-00-01\",\n        \"1999-01-32\",\n        \"1999-01-00\",\n        \"1999-01-01T25\",\n        \"1999-01-01T24:60\",\n        \"1999-01-01T24:59:60\"\n    ]\n    for string in invalid_strings:\n        with pytest.raises(ValueError):\n            parse_iso8601_like(string)\n", "def test_cftimeindex_repr_length_zero():\n    \"\"\"Test that CFTimeIndex with zero length is correctly represented.\"\"\"\n    index = CFTimeIndex([])\n    expected = \"CFTimeIndex([], dtype='object', length=0, calendar='None', freq=None)\"\n    assert repr(index) == expected\n"], "sample_611": ["def test__shift_month(calendar, date_args, offset, day_option, expected_date_args):\n    date_type = get_date_type(calendar)\n    date = date_type(*date_args)\n    expected_date = date_type(*expected_date_args)\n    result = _shift_month(date, offset.n, day_option=day_option)\n    assert result == expected_date\n", "def test_add_microsecond(calendar, initial_date_args, offset, expected_date_args):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    expected = date_type(*expected_date_args)\n    result = offset + initial\n    assert result == expected\n", "def test_tick_offsets(calendar, initial_date_args, offset, expected_date_args):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    result = initial + offset\n    expected = date_type(*expected_date_args)\n    assert result == expected\n", "def test_cftime_range_start_end(calendar, start, end, expected_start, expected_end):\n    result = cftime_range(start=start, end=end, freq=\"D\", calendar=calendar)\n    date_type = get_date_type(calendar)\n    expected_start_date = date_type(*expected_start)\n    expected_end_date = date_type(*expected_end)\n    \n    assert result[0] == expected_start_date\n    assert result[-1] == expected_end_date\n", "def test_cftime_range_with_sub_day_frequencies(\n    start, end, freq, periods, normalize, calendar, expected_dates"], "sample_612": ["def test_unique_value_groups() -> None:\n    # Test with simple 1D array\n    ar = np.array([1, 2, 2, 3, 3, 3])\n    values, groups = unique_value_groups(ar)\n    expected_values = np.array([1, 2, 3])\n    expected_groups = [[0], [1, 2], [3, 4, 5]]\n    assert_array_equal(values, expected_values)\n    assert groups == expected_groups\n\n    # Test with unsorted array\n    ar = np.array([3, 1, 2, 3, 2, 1])\n    values, groups = unique_value_groups(ar, sort=True)\n    expected_values = np.array([1, 2, 3])\n    expected_groups = [[1, 5], [2, 4], [0, 3]]\n    assert_array_equal(values, expected_values)\n    assert groups == expected_groups\n\n    # Test with MultiIndex\n    ar = pd.MultiIndex.from_tuples([('a', 1), ('b', 2), ('a', 1), ('a', 2)], names=['letter', 'number'])\n    values, groups = unique_value_groups(ar, sort=True)\n    expected_values = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 2)], names=['letter', 'number'])\n    expected_groups = [[0, 2], [3], [1]]\n    assert values.equals(expected_values)\n    assert groups == expected_groups\n\n    # Test with NaNs\n    ar = np.array([1, 2, np.nan, 2, np.nan, 1])\n    values, groups = unique_value_groups(ar, sort=True)\n    expected_values = np.array([1, 2])\n    expected_groups = [[0, 5], [1, 3]]\n    assert_array_equal(values, expected_values)\n    assert groups == expected_groups\n", "def test_groupby_invalid_grouper_and_bins() -> None:\n    ds = xr.Dataset(\n        {\"foo\": (\"x\", [1, 2, 3, 4, 5, 6])},\n        coords={\"x\": [0, 1, 2, 3, 4, 5]},\n    )\n    group = xr.DataArray([1, 2, 3, 4, 5, 6], dims=\"x\")\n    \n    with pytest.raises(TypeError, match=\"can't specify both `grouper` and `bins`\"):\n        xr.core.groupby.GroupBy(ds, group, grouper=pd.Grouper(freq='M'), bins=[0, 1, 2])\n", "def test_groupby_multidim_sum_and_mean() -> None:\n    array = DataArray(\n        [[[0, 1], [2, 3]], [[5, 10], [15, 20]]],\n        coords={\n            \"lon\": ([\"ny\", \"nx\"], [[30, 40], [40, 50]]),\n            \"lat\": ([\"ny\", \"nx\"], [[10, 10], [20, 20]]),\n        },\n        dims=[\"time\", \"ny\", \"nx\"],\n    )\n\n    # Test sum over 'lon'\n    expected_sum = DataArray([5, 28, 23], coords=[(\"lon\", [30.0, 40.0, 50.0])])\n    actual_sum = array.groupby(\"lon\").sum(dim=...)\n    assert_identical(expected_sum, actual_sum)\n\n    # Test mean over 'lat'\n    expected_mean = DataArray([16.0 / 4, 40.0 / 2], coords=[(\"lat\", [10.0, 20.0])])\n    actual_mean = array.groupby(\"lat\").mean(dim=...)\n    assert_identical(expected_mean, actual_mean)\n", "def test_groupby_invalid_grouper_type() -> None:\n    data = xr.DataArray([1, 2, 3, 4], dims=\"time\")\n    \n    with pytest.raises(TypeError, match=r\"unsupported type for grouper\"):\n        data.groupby(5)\n", "def test_groupby_first_last_with_nan() -> None:\n    array = DataArray([1, np.nan, 3, 4, np.nan, 6], dims=\"x\")\n    by = DataArray([\"a\", \"a\", \"b\", \"b\", \"c\", \"c\"], dims=\"x\", name=\"ab\")\n\n    expected_first = DataArray([1, 3, 6], [(\"ab\", [\"a\", \"b\", \"c\"])])\n    actual_first = array.groupby(by).first(skipna=True)\n    assert_identical(expected_first, actual_first)\n\n    expected_last = DataArray([1, 4, 6], [(\"ab\", [\"a\", \"b\", \"c\"])])\n    actual_last = array.groupby(by).last(skipna=True)\n    assert_identical(expected_last, actual_last)\n\n    expected_first_no_skip = DataArray([1, np.nan, np.nan], [(\"ab\", [\"a\", \"b\", \"c\"])])\n    actual_first_no_skip = array.groupby(by).first(skipna=False)\n    assert_identical(expected_first_no_skip, actual_first_no_skip)\n\n    expected_last_no_skip = DataArray([np.nan, 4, np.nan], [(\"ab\", [\"a\", \"b\", \"c\"])])\n    actual_last_no_skip = array.groupby(by).last(skipna=False)\n    assert_identical(expected_last_no_skip, actual_last_no_skip)\n"], "sample_613": ["def test_groupby_first_and_last_multidim() -> None:\n    array = xr.DataArray(\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]],\n        dims=[\"time\", \"lat\", \"lon\"],\n        coords={\"time\": pd.date_range(\"2022-01-01\", periods=3), \"lat\": [10, 20], \"lon\": [30, 40]},\n    )\n    by = xr.DataArray([1, 1, 2], dims=\"time\", name=\"group\")\n\n    # Test first\n    expected_first = xr.DataArray(\n        [[[1, 2], [3, 4]], [[9, 10], [11, 12]]],\n        dims=[\"group\", \"lat\", \"lon\"],\n        coords={\"group\": [1, 2], \"lat\": [10, 20], \"lon\": [30, 40]},\n    )\n    actual_first = array.groupby(by).first()\n    assert_identical(expected_first, actual_first)\n\n    # Test last\n    expected_last = xr.DataArray(\n        [[[5, 6], [7, 8]], [[9, 10], [11, 12]]],\n        dims=[\"group\", \"lat\", \"lon\"],\n        coords={\"group\": [1, 2], \"lat\": [10, 20], \"lon\": [30, 40]},\n    )\n    actual_last = array.groupby(by).last()\n    assert_identical(expected_last, actual_last)\n", "def test_groupby_multidim_assign_coords():\n    array = DataArray(\n        np.random.randn(4, 3, 2),\n        coords={\n            \"lon\": ([\"ny\", \"nx\"], [[30, 40, 50], [40, 50, 60], [50, 60, 70], [60, 70, 80]]),\n            \"lat\": ([\"ny\", \"nx\"], [[10, 20, 30], [20, 30, 40], [30, 40, 50], [40, 50, 60]]),\n        },\n        dims=[\"time\", \"ny\", \"nx\"],\n    )\n    \n    assigned_lon = DataArray([100, 200, 300, 400], dims=\"time\")\n    assigned_lat = DataArray([10, 20, 30, 40], dims=\"time\")\n    \n    expected = array.copy()\n    expected.coords[\"lon\"] = ([\"time\"], assigned_lon.values)\n    expected.coords[\"lat\"] = ([\"time\"], assigned_lat.values)\n    \n    actual = array.groupby(\"time\").assign_coords(lon=assigned_lon, lat=assigned_lat)\n    \n    assert_identical(expected, actual)\n", "def test_groupby_multidim_sum() -> None:\n    array = DataArray(\n        [[[0, 1], [2, 3]], [[4, 5], [6, 7]]],\n        coords={\"lon\": ([\"ny\", \"nx\"], [[30, 40], [50, 60]]), \"lat\": ([\"ny\", \"nx\"], [[10, 20], [30, 40]])},\n        dims=[\"time\", \"ny\", \"nx\"],\n    )\n\n    # Group by 'lon' and sum\n    expected_lon = DataArray(\n        [[0, 6], [6, 14]],\n        coords={\"lon\": [30, 40, 50, 60], \"time\": [0, 1]},\n        dims=[\"time\", \"lon\"]\n    )\n    actual_lon = array.groupby(\"lon\").sum()\n    assert_identical(expected_lon, actual_lon)\n\n    # Group by 'lat' and sum\n    expected_lat = DataArray(\n        [[0, 4], [6, 12]],\n        coords={\"lat\": [10, 20, 30, 40], \"time\": [0, 1]},\n        dims=[\"time\", \"lat\"]\n    )\n    actual_lat = array.groupby(\"lat\").sum()\n    assert_identical(expected_lat, actual_lat)\n", "def test_groupby_assign_coords_multidim() -> None:\n    # Create a multi-dimensional DataArray\n    array = DataArray(\n        np.random.randn(4, 5, 6),\n        coords={\"x\": range(4), \"y\": range(5), \"z\": range(6)},\n        dims=[\"x\", \"y\", \"z\"],\n    )\n    # Group by 'x' coordinate and assign new coordinates 'w' and 'v'\n    new_coords = {\"w\": ((\"y\", \"z\"), np.random.randn(5, 6)), \"v\": (\"y\", np.random.randn(5))}\n    actual = array.groupby(\"x\").assign_coords(new_coords)\n    expected = array.copy()\n    expected.coords[\"w\"] = new_coords[\"w\"]\n    expected.coords[\"v\"] = new_coords[\"v\"]\n    assert_identical(expected, actual)\n", "def test_groupby_map_return_different_type() -> None:\n    # Test for changing the type of the returned object in groupby map\n\n        return group.mean().to_dataset(name=\"mean_val\")\n\n    array = xr.DataArray([1, 2, 3, 4, 5, 6], coords={\"x\": [1, 1, 1, 2, 2, 2]}, dims=\"x\")\n    expected = xr.Dataset({\"mean_val\": (\"x\", [2.0, 5.0])}, coords={\"x\": [1, 2]})\n    actual = array.groupby(\"x\").map(func)\n    assert_identical(expected, actual)\n\n    dataset = xr.Dataset({\"foo\": (\"x\", [1, 2, 3, 4, 5, 6])}, coords={\"x\": [1, 1, 1, 2, 2, 2]})\n    expected = xr.Dataset({\"mean_val\": (\"x\", [2.0, 5.0])}, coords={\"x\": [1, 2]})\n    actual = dataset.groupby(\"x\").map(func)\n    assert_identical(expected, actual)\n"], "sample_614": ["def test_wrap_indent() -> None:\n    cases = [\n        (\"simple text\", \">> \", \">> simple text\"),\n        (\n            \"multiple\\nlines\\nof text\",\n            \"  \",\n            \"  multiple\\n  lines\\n  of text\"\n        ),\n        (\n            \"indent\\nwith\\nlength\",\n            \"---\",\n            \"---indent\\n   with\\n   length\"\n        ),\n        (\n            \"longer\\ntext\\nexample\\nwith\\nmore\\nlines\",\n            \"\",\n            \"longer\\ntext\\nexample\\nwith\\nmore\\nlines\"\n        )\n    ]\n    for text, start, expected in cases:\n        actual = formatting.wrap_indent(text, start=start)\n        assert expected == actual\n", "def test_wrap_indent() -> None:\n    text = \"This is a sample text that will be wrapped.\"\n    start = \">> \"\n    length = len(start)\n    actual = formatting.wrap_indent(text, start, length)\n    expected = \">> This is a sample text that will be wrapped.\"\n    assert actual == expected\n\n    text = \"This is a sample text\\nthat will be wrapped\\nwith multiple lines.\"\n    actual = formatting.wrap_indent(text, start, length)\n    expected = \">> This is a sample text\\n   that will be wrapped\\n   with multiple lines.\"\n    assert actual == expected\n", "def test_wrap_indent() -> None:\n    text = \"This is a test string\\nwith multiple lines\\nand indentation.\"\n    expected = \"    This is a test string\\n    with multiple lines\\n    and indentation.\"\n    actual = formatting.wrap_indent(text, start=\"    \", length=4)\n    assert actual == expected\n\n    text = \"Single line text.\"\n    expected = \">> Single line text.\"\n    actual = formatting.wrap_indent(text, start=\">> \", length=3)\n    assert actual == expected\n\n    text = \"No indentation for this one.\"\n    expected = \"No indentation for this one.\"\n    actual = formatting.wrap_indent(text)\n    assert actual == expected\n", "def test_wrap_indent() -> None:\n    text = \"This is a test string.\"\n    start = \">> \"\n    length = 4\n    expected = \">> This is a test string.\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n\n    text = \"This is a test string.\\nWith multiple lines.\"\n    expected = \">> This is a test string.\\n    With multiple lines.\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n\n    text = \"Short line\\nAnother short line\"\n    expected = \">> Short line\\n    Another short line\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n\n    text = \"A single line\"\n    expected = \">> A single line\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n", "def test_inline_dask_repr() -> None:\n    import dask.array as da\n    \n    # Create a dask array with known chunks and meta\n    array = da.from_array(np.arange(10), chunks=(5,))\n    expected_repr = \"dask.array<chunksize=(5,), meta=np.ndarray>\"\n    actual_repr = formatting.inline_dask_repr(array)\n    assert actual_repr == expected_repr\n\n    # Create a dask array without meta\n    array_no_meta = da.from_array(np.arange(10), chunks=(5,))\n    delattr(array_no_meta, '_meta')\n    expected_repr_no_meta = \"dask.array<chunksize=(5,)>\"\n    actual_repr_no_meta = formatting.inline_dask_repr(array_no_meta)\n    assert actual_repr_no_meta == expected_repr_no_meta\n"], "sample_615": ["def test_cross_product() -> None:\n    # Test cases for cross product with 3 dimensions\n    a = xr.DataArray([1, 2, 3], dims=[\"dim_0\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"dim_0\"])\n    expected = xr.DataArray([-3, 6, -3], dims=[\"dim_0\"])\n    actual = xr.cross(a, b, dim=\"dim_0\")\n    assert_identical(expected, actual)\n\n    # Test cases for cross product with 2 dimensions (return a scalar)\n    a = xr.DataArray([1, 2], dims=[\"dim_0\"])\n    b = xr.DataArray([4, 5], dims=[\"dim_0\"])\n    expected = xr.DataArray(-3)\n    actual = xr.cross(a, b, dim=\"dim_0\")\n    assert_identical(expected, actual)\n\n    # Test cases for cross product with one vector 2-dimensional, the other 3-dimensional\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])))\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    expected = xr.DataArray([12, -6, -3], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(expected, actual)\n\n    # Test cases for cross product with multiple vectors with same dimensions\n    a = xr.DataArray([[1, 2, 3], [4, 5, 6]], dims=(\"time\", \"cartesian\"), coords=dict(time=([\"time\"], [0, 1]), cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    b = xr.DataArray([[4, 5, 6], [1, 2, 3]], dims=(\"time\", \"cartesian\"), coords=dict(time=([\"time\"], [0, 1]), cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    expected = xr.DataArray([[-3, 6, -3], [3, -6, 3]], dims=(\"time", "def test_apply_ufunc_with_kwargs() -> None:\n        return x * factor\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    factor = 2\n\n    result_array = apply_ufunc(multiply_with_factor, array, kwargs={\"factor\": factor})\n    assert_identical(result_array, array * factor)\n\n    result_variable = apply_ufunc(multiply_with_factor, variable, kwargs={\"factor\": factor})\n    assert_identical(result_variable, xr.Variable(\"x\", array * factor))\n\n    result_data_array = apply_ufunc(multiply_with_factor, data_array, kwargs={\"factor\": factor})\n    assert_identical(result_data_array, xr.DataArray(xr.Variable(\"x\", array * factor), [(\"x\", -array)]))\n\n    result_dataset = apply_ufunc(multiply_with_factor, dataset, kwargs={\"factor\": factor})\n    assert_identical(result_dataset, xr.Dataset({\"y\": xr.Variable(\"x\", array * factor)}, {\"x\": -array}))\n", "def test_apply_with_broadcast_compat_data() -> None:\n        return a + b\n\n    array_a = np.array([1, 2, 3])\n    array_b = np.array([[1, 2, 3], [4, 5, 6]])\n    variable_a = xr.Variable(\"x\", array_a)\n    variable_b = xr.Variable([\"x\", \"y\"], array_b)\n    data_array_a = xr.DataArray(variable_a, [(\"x\", -array_a)])\n    data_array_b = xr.DataArray(variable_b, {\"x\": [-1, -2, -3], \"y\": [1, 2, 3]})\n    dataset_a = xr.Dataset({\"a\": variable_a}, {\"x\": -array_a})\n    dataset_b = xr.Dataset({\"b\": variable_b}, {\"x\": [-1, -2, -3], \"y\": [1, 2, 3]})\n\n    expected_result = xr.DataArray([[2, 4, 6], [5, 7, 9]], dims=[\"x\", \"y\"])\n\n    assert_identical(apply_ufunc(custom_func, variable_a, variable_b), expected_result)\n    assert_identical(apply_ufunc(custom_func, data_array_a, data_array_b), expected_result)\n    assert_identical(apply_ufunc(custom_func, dataset_a, dataset_b).to_array().squeeze(), expected_result)\n", "def test_cross_product() -> None:\n    # Test cross product with 3 dimensions\n    a = xr.DataArray([1, 2, 3], dims=[\"dim_0\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"dim_0\"])\n    expected = xr.DataArray([-3, 6, -3], dims=[\"dim_0\"])\n    actual = xr.cross(a, b, dim=\"dim_0\")\n    assert_identical(expected, actual)\n\n    # Test cross product with 2 dimensions\n    a = xr.DataArray([1, 2], dims=[\"dim_0\"])\n    b = xr.DataArray([4, 5], dims=[\"dim_0\"])\n    expected = xr.DataArray(-3)\n    actual = xr.cross(a, b, dim=\"dim_0\")\n    assert_identical(expected, actual)\n\n    # Test cross product with 3 dimensions but zeros at the last axis\n    a = xr.DataArray([1, 2, 0], dims=[\"dim_0\"])\n    b = xr.DataArray([4, 5, 0], dims=[\"dim_0\"])\n    expected = xr.DataArray([0, 0, -3], dims=[\"dim_0\"])\n    actual = xr.cross(a, b, dim=\"dim_0\")\n    assert_identical(expected, actual)\n\n    # Test cross product with one vector having dimension 2\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\"]})\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\", \"z\"]})\n    expected = xr.DataArray([12, -6, -3], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\", \"z\"]})\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(expected, actual)\n\n    # Test cross product with one vector having dimension 2 but coords in other positions\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"z\"]})\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\", \"z\"]})\n    expected = xr.DataArray([-10,", "def test_apply_ufunc_with_kwargs() -> None:\n        return a + b + offset\n\n    array = np.array([1, 2, 3])\n    offset = 5\n    expected = np.array([7, 8, 9])\n\n    result = apply_ufunc(\n        add_with_offset,\n        array,\n        array,\n        kwargs={'offset': offset}\n    )\n    assert_identical(result, expected)\n\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    expected_data_array = xr.DataArray(xr.Variable(\"x\", expected), [(\"x\", -array)])\n\n    result = apply_ufunc(\n        add_with_offset,\n        data_array,\n        data_array,\n        kwargs={'offset': offset}\n    )\n    assert_identical(result, expected_data_array)\n\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n    expected_dataset = xr.Dataset({\"y\": xr.Variable(\"x\", expected)}, {\"x\": -array})\n\n    result = apply_ufunc(\n        add_with_offset,\n        dataset,\n        dataset,\n        kwargs={'offset': offset}\n    )\n    assert_identical(result, expected_dataset)\n"], "sample_616": ["def test_apply_groupby_func() -> None:\n    data = xr.DataArray(\n        np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), dims=(\"x\", \"y\"), coords={\"x\": [\"A\", \"A\", \"B\"]}\n    )\n    grouped = data.groupby(\"x\")\n\n        return grouped_data.sum(dim=\"y\")\n\n    expected = xr.DataArray([6, 24], dims=[\"x\"], coords={\"x\": [\"A\", \"B\"]})\n    actual = apply_groupby_func(sum_group, grouped)\n    assert_identical(expected, actual)\n\n    data_a = xr.DataArray(\n        np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), dims=(\"x\", \"y\"), coords={\"x\": [\"A\", \"A\", \"B\"]}\n    )\n    data_b = xr.DataArray(\n        np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]]), dims=(\"x\", \"y\"), coords={\"x\": [\"A\", \"A\", \"B\"]}\n    )\n    grouped_a = data_a.groupby(\"x\")\n    grouped_b = data_b.groupby(\"x\")\n\n        return grouped_data_a.sum(dim=\"y\") + grouped_data_b.sum(dim=\"y\")\n\n    expected = xr.DataArray([42, 30], dims=[\"x\"], coords={\"x\": [\"A\", \"B\"]})\n    actual = apply_groupby_func(sum_groups, grouped_a, grouped_b)\n    assert_identical(expected, actual)\n", "def test_apply_variable_ufunc() -> None:\n        return variable * 2\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    expected_variable = xr.Variable(\"x\", array * 2)\n    expected_data_array = xr.DataArray(expected_variable, [(\"x\", -array)])\n    expected_dataset = xr.Dataset({\"y\": expected_variable}, {\"x\": -array})\n\n    actual_variable = apply_ufunc(multiply_by_two, variable)\n    assert_identical(actual_variable, expected_variable)\n\n    actual_data_array = apply_ufunc(multiply_by_two, data_array)\n    assert_identical(actual_data_array, expected_data_array)\n\n    actual_dataset = apply_ufunc(multiply_by_two, dataset)\n    assert_identical(actual_dataset, expected_dataset)\n", "def test_apply_ufunc_meta() -> None:\n    # Test to ensure 'meta' argument is passed and utilized correctly\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n        return x + 1\n\n    meta_array = np.array([1])\n    meta_variable = xr.Variable(\"x\", meta_array)\n    meta_data_array = xr.DataArray(meta_variable, [(\"x\", -meta_array)])\n    meta_dataset = xr.Dataset({\"y\": meta_variable}, {\"x\": -meta_array})\n\n    result_array = apply_ufunc(add_func, array, meta=meta_array)\n    assert_identical(result_array, array + 1)\n\n    result_variable = apply_ufunc(add_func, variable, meta=meta_variable)\n    assert_identical(result_variable, variable + 1)\n\n    result_data_array = apply_ufunc(add_func, data_array, meta=meta_data_array)\n    assert_identical(result_data_array, data_array + 1)\n\n    result_dataset = apply_ufunc(add_func, dataset, meta=meta_dataset)\n    assert_identical(result_dataset, dataset + 1)\n\n    # Test with dask\n    if has_dask:\n        import dask.array as da\n\n        dask_array = da.from_array(array, chunks=2)\n        dask_variable = xr.Variable(\"x\", dask_array)\n        dask_data_array = xr.DataArray(dask_variable, [(\"x\", -array)])\n        dask_dataset = xr.Dataset({\"y\": dask_variable}, {\"x\": -array})\n\n        result_dask_array = apply_ufunc(add_func, dask_array, meta=meta_array, dask=\"allowed\")\n        assert_identical(result_dask_array, dask_array + 1)\n\n        result_dask_variable = apply_ufunc(add_func, dask_variable, meta=meta_variable, dask=\"allowed\")\n        assert_identical(result_dask_variable, dask_variable + 1)\n\n        result_dask_data_array = apply_ufunc(add_func, dask_data_array, meta=meta_data_array, dask=\"allowed\")\n        assert_identical(result_dask_data_array, dask_data_array + 1)\n\n        result_dask_dataset = apply", "def test_apply_ufunc_unify_chunks() -> None:\n    ds = xr.Dataset({\"foo\": ([\"lon\", \"lat\"], np.random.rand(10, 10))})\n    ds[\"bar\"] = ds[\"foo\"]\n    chunked_ds = ds.chunk({\"lon\": 5, \"lat\": 5})\n\n        return apply_ufunc(\n            np.mean,\n            obj,\n            dask=\"parallelized\",\n            input_core_dims=[[\"lon\", \"lat\"]],\n            output_core_dims=[[]],\n            output_dtypes=[float],\n        )\n\n    unified_ds, = unify_chunks(chunked_ds)\n    assert unified_ds.foo.chunks == ((5, 5), (5, 5))\n    assert unified_ds.bar.chunks == ((5, 5), (5, 5))\n\n    mean_ds = mean_func(unified_ds)\n    assert mean_ds == mean_func(chunked_ds)\n    assert isinstance(mean_ds, xr.Dataset)\n    assert mean_ds.foo.ndim == 0\n    assert mean_ds.bar.ndim == 0\n", "def test_broadcast_compat_data_mismatched_core_dims() -> None:\n    data = np.arange(24).reshape(4, 3, 2)\n    var = xr.Variable([\"x\", \"y\", \"z\"], data)\n\n    with pytest.raises(ValueError, match=r\"unexpected dimensions\"):\n        broadcast_compat_data(var, (\"x\", \"y\"), (\"w\", \"z\"))\n    \n    with pytest.raises(ValueError, match=r\"unexpected dimensions\"):\n        broadcast_compat_data(var, (\"x\",), (\"y\", \"z\", \"w\"))\n\n    with pytest.raises(ValueError, match=r\"unexpected dimensions\"):\n        broadcast_compat_data(var, (\"w\", \"x\", \"y\"), (\"z\",))\n\n    with pytest.raises(ValueError, match=r\"unexpected dimensions\"):\n        broadcast_compat_data(var, (\"x\", \"y\", \"z\"), (\"a\", \"b\"))\n"], "sample_617": ["def test_apply_dataarray_vfunc() -> None:\n        return x * 2\n\n    data_array = xr.DataArray([1, 2, 3], dims=[\"x\"], coords={\"x\": [\"a\", \"b\", \"c\"]})\n    expected = xr.DataArray([2, 4, 6], dims=[\"x\"], coords={\"x\": [\"a\", \"b\", \"c\"]})\n\n    actual = apply_dataarray_vfunc(custom_func, data_array, signature=_UFuncSignature([[\"x\"]]))\n    assert_identical(expected, actual)\n\n    # Test with keep_attrs=\"override\"\n    data_array.attrs[\"units\"] = \"m\"\n    expected.attrs[\"units\"] = \"m\"\n    actual = apply_dataarray_vfunc(custom_func, data_array, signature=_UFuncSignature([[\"x\"]]), keep_attrs=\"override\")\n    assert_identical(expected, actual)\n\n    # Test with keep_attrs=\"drop\"\n    actual = apply_dataarray_vfunc(custom_func, data_array, signature=_UFuncSignature([[\"x\"]]), keep_attrs=\"drop\")\n    expected.attrs = {}\n    assert_identical(expected, actual)\n", "def test_apply_ufunc_with_non_numeric_data() -> None:\n    # Test apply_ufunc with non-numeric data\n    array_str = np.array([\"a\", \"b\", \"c\"])\n    variable_str = xr.Variable(\"x\", array_str)\n    data_array_str = xr.DataArray(variable_str, [(\"x\", [0, 1, 2])])\n    dataset_str = xr.Dataset({\"y\": variable_str}, {\"x\": [0, 1, 2]})\n\n        return a + b\n\n        return apply_ufunc(concatenate_strs, a, b)\n\n    assert_identical(test_concat(array_str, array_str), array_str + array_str)\n    assert_identical(test_concat(variable_str, variable_str), variable_str + variable_str)\n    assert_identical(test_concat(data_array_str, data_array_str), data_array_str + data_array_str)\n    assert_identical(test_concat(dataset_str, dataset_str), dataset_str.assign(y=dataset_str.y + dataset_str.y))\n", "def test_build_output_coords_and_indexes() -> None:\n    from xarray.core.coordinates import Coordinates\n    from xarray.core.dataarray import DataArray\n    from xarray.core.indexes import Index\n    from xarray.core.variable import Variable\n\n    coords1 = Coordinates({\"x\": Variable(\"x\", [1, 2, 3])})\n    coords2 = Coordinates({\"x\": Variable(\"x\", [3, 4, 5]), \"y\": Variable(\"y\", [5, 6, 7])})\n\n    da1 = DataArray(np.random.rand(3), coords=coords1)\n    da2 = DataArray(np.random.rand(3), coords=coords2)\n\n    sig = _UFuncSignature([[\"x\"], [\"x\"]], [[\"x\"]])\n    result_coords, result_indexes = build_output_coords_and_indexes(\n        [da1, da2], sig, exclude_dims={\"y\"}\n    )\n    \n    assert isinstance(result_coords, list)\n    assert isinstance(result_coords[0], dict)\n    assert isinstance(result_coords[0][\"x\"], Variable)\n    assert \"y\" not in result_coords[0]\n\n    assert isinstance(result_indexes, list)\n    assert isinstance(result_indexes[0], dict)\n    assert \"x\" in result_indexes[0]\n\n    assert result_coords[0][\"x\"].equals(coords1[\"x\"])\n", "def test_apply_with_exclude_dims() -> None:\n        return x + y\n\n    array1 = np.array([[1, 2], [3, 4]])\n    array2 = np.array([[5, 6], [7, 8]])\n    expected = np.array([[6, 8], [10, 12]])\n\n    variable1 = xr.Variable([\"x\", \"y\"], array1)\n    variable2 = xr.Variable([\"x\", \"y\"], array2)\n    expected_variable = xr.Variable([\"x\", \"y\"], expected)\n\n    data_array1 = xr.DataArray(variable1, {\"x\": [\"a\", \"b\"], \"y\": [-1, -2]})\n    data_array2 = xr.DataArray(variable2, {\"x\": [\"a\", \"b\"], \"y\": [-1, -2]})\n    expected_data_array = xr.DataArray(expected_variable, {\"x\": [\"a\", \"b\"], \"y\": [-1, -2]})\n\n    dataset1 = xr.Dataset({\"data\": data_array1})\n    dataset2 = xr.Dataset({\"data\": data_array2})\n    expected_dataset = xr.Dataset({\"data\": expected_data_array})\n\n    assert_identical(expected_variable, apply_ufunc(func, variable1, variable2, exclude_dims={\"x\"}))\n    assert_identical(expected_data_array, apply_ufunc(func, data_array1, data_array2, exclude_dims={\"x\"}))\n    assert_identical(expected_dataset, apply_ufunc(func, dataset1, dataset2, exclude_dims={\"x\"}))\n", "def test_apply_ufunc_with_kwargs() -> None:\n        return x**power\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    expected_variable = xr.Variable(\"x\", array**2)\n    expected_data_array = xr.DataArray(expected_variable, [(\"x\", -array)])\n    expected_dataset = xr.Dataset({\"y\": expected_variable}, {\"x\": -array})\n\n    apply_with_kwargs = functools.partial(apply_ufunc, func, kwargs={\"power\": 2})\n\n    assert_identical(expected_variable, apply_with_kwargs(variable))\n    assert_identical(expected_data_array, apply_with_kwargs(data_array))\n    assert_identical(expected_dataset, apply_with_kwargs(dataset))\n\n    # Test with groupby\n    assert_identical(expected_data_array, apply_with_kwargs(data_array.groupby(\"x\")))\n    assert_identical(expected_dataset, apply_with_kwargs(dataset.groupby(\"x\")))\n"], "sample_618": ["def test_cross_3d() -> None:\n    a = xr.DataArray([1, 2, 3], dims=\"dim_0\")\n    b = xr.DataArray([4, 5, 6], dims=\"dim_0\")\n    expected = xr.DataArray([-3, 6, -3], dims=\"dim_0\")\n    actual = xr.cross(a, b, dim=\"dim_0\")\n    assert_identical(expected, actual)\n\n    # Test with larger arrays\n    a = xr.DataArray([[1, 2, 3], [4, 5, 6]], dims=(\"time\", \"dim_0\"))\n    b = xr.DataArray([[6, 5, 4], [3, 2, 1]], dims=(\"time\", \"dim_0\"))\n    expected = xr.DataArray(\n        [[-7, 14, -7], [3, -6, 3]], dims=(\"time\", \"dim_0\"))\n    actual = xr.cross(a, b, dim=\"dim_0\")\n    assert_identical(expected, actual)\n\n    # Test with dimension of size 2\n    a = xr.DataArray([1, 2], dims=\"dim_0\")\n    b = xr.DataArray([3, 4], dims=\"dim_0\")\n    expected = xr.DataArray(-2)\n    actual = xr.cross(a, b, dim=\"dim_0\")\n    assert_identical(expected, actual)\n\n    # Test with one vector having dimension 2 and other having dimension 3\n    a = xr.DataArray([1, 2], dims=\"dim_0\")\n    b = xr.DataArray([3, 4, 5], dims=\"dim_0\")\n    expected = xr.DataArray([10, -5, -2], dims=\"dim_0\")\n    actual = xr.cross(a, b, dim=\"dim_0\")\n    assert_identical(expected, actual)\n", "def test_apply_ufunc_exclude_dims() -> None:\n        return apply_ufunc(\n            operator.mul,\n            a,\n            b,\n            input_core_dims=[[\"x\"], [\"x\"]],\n            exclude_dims={\"x\"},\n        )\n\n    array1 = xr.DataArray([1, 2, 3], dims=\"x\")\n    array2 = xr.DataArray([4, 5, 6], dims=\"x\")\n\n    expected_result = xr.DataArray([4, 10, 18], dims=\"x\")\n    actual_result = multiply_without_excluded_dims(array1, array2)\n    \n    assert_identical(expected_result, actual_result)\n\n    array1 = xr.DataArray([[1, 2], [3, 4]], dims=[\"y\", \"x\"])\n    array2 = xr.DataArray([[5, 6], [7, 8]], dims=[\"y\", \"x\"])\n\n    expected_result = xr.DataArray([[5, 12], [21, 32]], dims=[\"y\", \"x\"])\n    actual_result = multiply_without_excluded_dims(array1, array2)\n\n    assert_identical(expected_result, actual_result)\n", "def test_build_output_coords_and_indexes() -> None:\n    from xarray.core.computation import _UFuncSignature, build_output_coords_and_indexes\n    from xarray import DataArray, Variable, Index\n    from xarray.core.indexes import PandasIndex\n\n    # Setup input DataArray objects with various coordinates\n    da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [0, 1, 2], \"y\": (\"x\", [0.1, 0.2, 0.3])})\n    da2 = DataArray([4, 5, 6], dims=\"x\", coords={\"x\": [0, 1, 2], \"z\": (\"x\", [\"a\", \"b\", \"c\"])})\n\n    # Define the _UFuncSignature\n    signature = _UFuncSignature([[\"x\"], [\"x\"]], [[\"x\"]])\n\n    # Call the function under test\n    output_coords, output_indexes = build_output_coords_and_indexes(\n        [da1, da2], signature, exclude_dims=frozenset(), combine_attrs=\"override\"\n    )\n\n    # Check the output coordinates and indexes\n    expected_coords = [{\"x\": Variable(\"x\", [0, 1, 2]), \"y\": Variable(\"x\", [0.1, 0.2, 0.3]), \"z\": Variable(\"x\", [\"a\", \"b\", \"c\"])}]\n    expected_indexes = [{\"x\": PandasIndex(Index([0, 1, 2], \"x\"))}]\n    \n    assert output_coords == expected_coords\n    assert output_indexes == expected_indexes\n", "def test_apply_unified_dim_sizes_exclude() -> None:\n    a = xr.Variable((\"x\", \"y\"), np.ones((2, 3)))\n    b = xr.Variable((\"y\", \"z\"), np.ones((3, 4)))\n    c = xr.Variable((\"x\", \"z\"), np.ones((2, 4)))\n\n    # Should not raise any error as 'y' is excluded\n    result = unified_dim_sizes([a, b, c], exclude_dims={\"y\"})\n    expected = {\"x\": 2, \"z\": 4}\n    assert result == expected\n\n    # Should raise an error as 'x' is not excluded and has conflicting sizes\n    with pytest.raises(ValueError, match=r\"operands cannot be broadcast together\"):\n        unified_dim_sizes([a, b, xr.Variable((\"x\", \"y\"), np.ones((3, 3)))], exclude_dims={\"y\"})\n", "def test_cross() -> None:\n    # Test cross product for different dimensions\n    a = xr.DataArray([1, 2, 3], dims=[\"dim_0\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"dim_0\"])\n    expected = xr.DataArray([-3, 6, -3], dims=[\"dim_0\"])\n    actual = xr.cross(a, b, dim=\"dim_0\")\n    assert_identical(expected, actual)\n    \n    # Test with 2D arrays\n    a = xr.DataArray([[1, 2], [3, 4]], dims=[\"dim_0\", \"dim_1\"])\n    b = xr.DataArray([[5, 6], [7, 8]], dims=[\"dim_0\", \"dim_1\"])\n    expected = xr.DataArray([[-2, -2], [-4, -4]], dims=[\"dim_0\", \"dim_1\"])\n    actual = xr.cross(a, b, dim=\"dim_1\")\n    assert_identical(expected, actual)\n    \n    # Test with dimension of size 2\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])))\n    b = xr.DataArray([3, 4, 5], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    expected = xr.DataArray([10, -5, -2], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(expected, actual)\n    \n    # Test with dimension of size 3\n    a = xr.DataArray([1, 2, 3], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    expected = xr.DataArray([-3, 6, -3], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])))\n    actual = xr.cross(a, b, dim=\"cartesian\")\n"], "sample_619": ["def test__netcdf_to_numpy_timeunit(unit_str, expected_result) -> None:\n    assert _netcdf_to_numpy_timeunit(unit_str) == expected_result\n", "def test_convert_time_or_go_back():\n    from cftime import DatetimeNoLeap\n\n    # Test case 1: Valid conversion without going back\n    date = DatetimeNoLeap(2000, 2, 28)\n    date_type = pd.Timestamp\n    result = coding.times.convert_time_or_go_back(date, date_type)\n    expected = pd.Timestamp(\"2000-02-28\")\n    assert result == expected\n\n    # Test case 2: Invalid day, should go back one day\n    date = DatetimeNoLeap(2000, 2, 30)\n    date_type = pd.Timestamp\n    result = coding.times.convert_time_or_go_back(date, date_type)\n    expected = pd.Timestamp(\"2000-02-29\")\n    assert result == expected\n\n    # Test case 3: Still invalid, should go back two days\n    date = DatetimeNoLeap(2000, 2, 31)\n    date_type = pd.Timestamp\n    result = coding.times.convert_time_or_go_back(date, date_type)\n    expected = pd.Timestamp(\"2000-02-29\")\n    assert result == expected\n\n    # Test case 4: Valid conversion in another calendar\n    date = DatetimeNoLeap(2000, 2, 28)\n    date_type = DatetimeNoLeap\n    result = coding.times.convert_time_or_go_back(date, date_type)\n    expected = DatetimeNoLeap(2000, 2, 28)\n    assert result == expected\n\n    # Test case 5: Invalid day in another calendar, should go back one day\n    date = DatetimeNoLeap(2000, 2, 30)\n    date_type = DatetimeNoLeap\n    result = coding.times.convert_time_or_go_back(date, date_type)\n    expected = DatetimeNoLeap(2000, 2, 29)\n    assert result == expected\n\n    # Test case 6: Still invalid in another calendar, should go back two days\n    date = DatetimeNoLeap(2000, 2, 31)\n    date_type = DatetimeNoLeap\n    result = coding.times.convert_time_or_go_back(date, date_type)\n    expected = DatetimeNoLeap(2000, 2, 29)\n    assert result == expected\n", "def test_infer_time_units_from_diff() -> None:\n    # Test case to verify that _infer_time_units_from_diff correctly infers the units\n    # from an array of time deltas.\n    time_deltas = pd.to_timedelta([\"1 day\", \"2 days\", \"3 days\"])\n    expected = \"days\"\n    result = coding.times._infer_time_units_from_diff(time_deltas.to_numpy())\n    assert result == expected\n\n    time_deltas = pd.to_timedelta([\"1 hour\", \"2 hours\", \"3 hours\"])\n    expected = \"hours\"\n    result = coding.times._infer_time_units_from_diff(time_deltas.to_numpy())\n    assert result == expected\n\n    time_deltas = pd.to_timedelta([\"1 minute\", \"2 minutes\", \"3 minutes\"])\n    expected = \"minutes\"\n    result = coding.times._infer_time_units_from_diff(time_deltas.to_numpy())\n    assert result == expected\n\n    time_deltas = pd.to_timedelta([\"1 second\", \"2 seconds\", \"3 seconds\"])\n    expected = \"seconds\"\n    result = coding.times._infer_time_units_from_diff(time_deltas.to_numpy())\n    assert result == expected\n\n    # Mixed time deltas which are not evenly divisible should default to seconds\n    time_deltas = pd.to_timedelta([\"1 hour\", \"2 days\", \"3 seconds\"])\n    expected = \"seconds\"\n    result = coding.times._infer_time_units_from_diff(time_deltas.to_numpy())\n    assert result == expected\n", "def test_infer_calendar_name_invalid_input():\n    # Test for invalid input which is neither datetime64 nor object array with cftime datetimes\n    with pytest.raises(ValueError, match=\"Array does not contain datetime objects.\"):\n        coding.times.infer_calendar_name(np.array([1, 2, 3]))\n\n    with pytest.raises(ValueError, match=\"Array does not contain datetime objects.\"):\n        coding.times.infer_calendar_name(np.array([\"a\", \"b\", \"c\"]))\n\n    with pytest.raises(ValueError, match=\"Array does not contain datetime objects.\"):\n        coding.times.infer_calendar_name(np.array([pd.Timestamp('2020-01-01'), 1, 2]))\n", "def test__ensure_padded_year() -> None:\n    # Test cases where the reference date has ambiguous year formats\n    assert coding.times._ensure_padded_year(\"1-01-01\") == \"0001-01-01\"\n    assert coding.times._ensure_padded_year(\"12-1-1\") == \"0012-1-1\"\n    assert coding.times._ensure_padded_year(\"123-12-12\") == \"0123-12-12\"\n    assert coding.times._ensure_padded_year(\"1234-12-12\") == \"1234-12-12\"\n\n    with pytest.warns(SerializationWarning, match=\"Ambiguous reference date string\"):\n        assert coding.times._ensure_padded_year(\"2-3-4\") == \"0002-3-4\"\n\n    # Test cases where the reference date already has a padded year\n    assert coding.times._ensure_padded_year(\"2001-01-01\") == \"2001-01-01\"\n    assert coding.times._ensure_padded_year(\"2021-12-31\") == \"2021-12-31\"\n\n    # Test invalid reference date\n    with pytest.raises(ValueError, match=\"invalid reference date for time units\"):\n        coding.times._ensure_padded_year(\"invalid-date\")\n"], "sample_620": ["def test_concat_with_empty_objects() -> None:\n    # Test concatenating empty DataArray objects\n    da1 = DataArray([], dims=\"x\", coords={\"x\": []})\n    da2 = DataArray([], dims=\"x\", coords={\"x\": []})\n    expected = DataArray([], dims=\"x\", coords={\"x\": []})\n    actual = concat([da1, da2], dim=\"x\")\n    assert_identical(actual, expected)\n\n    # Test concatenating empty Dataset objects\n    ds1 = Dataset({}, coords={\"x\": []})\n    ds2 = Dataset({}, coords={\"x\": []})\n    expected = Dataset({}, coords={\"x\": []})\n    actual = concat([ds1, ds2], dim=\"x\")\n    assert_identical(actual, expected)\n\n    # Test concatenating a mix of empty and non-empty DataArray objects\n    da3 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [0, 1, 2]})\n    expected = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [0, 1, 2]})\n    actual = concat([da1, da3], dim=\"x\")\n    assert_identical(actual, expected)\n\n    # Test concatenating a mix of empty and non-empty Dataset objects\n    ds3 = Dataset({\"a\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2]})\n    expected = Dataset({\"a\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2]})\n    actual = concat([ds1, ds3], dim=\"x\")\n    assert_identical(actual, expected)\n", "def test_concat_with_positions() -> None:\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = Dataset({\"foo\": (\"x\", [3, 4]), \"x\": [2, 3]})\n\n    positions = [[0, 1], [3, 4]]\n    expected = Dataset({\"foo\": (\"x\", [1, 2, np.nan, 3, 4]), \"x\": [0, 1, 2, 3, 4]})\n    \n    actual = concat([ds1, ds2], dim=\"x\", positions=positions)\n    assert_identical(expected, actual)\n\n    positions = [[0, 1], [1, 2]]\n    expected_overlap = Dataset({\"foo\": (\"x\", [1, 2, 3, 4]), \"x\": [0, 1, 2, 3]})\n    actual_overlap = concat([ds1, ds2], dim=\"x\", positions=positions)\n    assert_identical(expected_overlap, actual_overlap)\n\n    positions = [[0], [1, 2, 3]]\n    ds3 = Dataset({\"foo\": (\"x\", [5, 6, 7]), \"x\": [4, 5, 6]})\n    expected_mixed = Dataset({\"foo\": (\"x\", [1, np.nan, 5, 6, 7]), \"x\": [0, 1, 4, 5, 6]})\n    actual_mixed = concat([ds1, ds3], dim=\"x\", positions=positions)\n    assert_identical(expected_mixed, actual_mixed)\n", "def test_concat_with_empty_datasets() -> None:\n    ds1 = Dataset({\"a\": (\"x\", [])}, coords={\"x\": []})\n    ds2 = Dataset({\"a\": (\"x\", [])}, coords={\"x\": []})\n\n    result = concat([ds1, ds2], dim=\"x\")\n    expected = Dataset({\"a\": (\"x\", [])}, coords={\"x\": []})\n\n    assert_identical(result, expected)\n\n    # Test with different dimensions\n    ds1 = Dataset({\"a\": (\"x\", [])}, coords={\"x\": []})\n    ds2 = Dataset({\"a\": (\"y\", [])}, coords={\"y\": []})\n\n    with pytest.raises(ValueError, match=r\"'a' is not present in all datasets\"):\n        concat([ds1, ds2], dim=\"x\")\n", "def test_concat_invalid_fill_value() -> None:\n    data = Dataset({\"foo\": (\"x\", np.random.randn(10))})\n    objs = [data.isel(x=slice(5)), data.isel(x=slice(5, None))]\n    invalid_fill_value = {\"foo\": \"invalid_type\"}\n\n    with pytest.raises(TypeError, match=r\"fill_value must be a scalar or dict-like\"):\n        concat(objs, dim=\"x\", fill_value=invalid_fill_value)\n", "def test_concat_with_positions() -> None:\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]})\n    ds2 = Dataset({\"foo\": (\"x\", [3, 4])}, coords={\"x\": [2, 3]})\n    ds3 = Dataset({\"foo\": (\"x\", [5, 6])}, coords={\"x\": [4, 5]})\n\n    actual = concat([ds1, ds2, ds3], dim=\"x\", positions=[[0, 1], [2, 3], [4, 5]])\n    expected = Dataset({\"foo\": (\"x\", [1, 2, 3, 4, 5, 6])}, coords={\"x\": [0, 1, 2, 3, 4, 5]})\n    assert_identical(actual, expected)\n\n    # Test when positions are not provided\n    actual_default_positions = concat([ds1, ds2, ds3], dim=\"x\")\n    assert_identical(actual_default_positions, expected)\n"], "sample_621": ["def test_pandas_index_isel(self) -> None:\n    pd_idx = pd.Index([1, 2, 3])\n    index = PandasIndex(pd_idx, \"x\")\n\n    # Test scalar indexer\n    result = index.isel({\"x\": 1})\n    assert result is None\n\n    # Test slice indexer\n    result = index.isel({\"x\": slice(1, 3)})\n    expected = PandasIndex(pd_idx[1:3], \"x\")\n    assert result.equals(expected)\n\n    # Test array indexer\n    result = index.isel({\"x\": np.array([0, 2])})\n    expected = PandasIndex(pd_idx[[0, 2]], \"x\")\n    assert result.equals(expected)\n\n    # Test Variable indexer\n    var_idx = Variable(\"x\", np.array([1, 2]))\n    result = index.isel({\"x\": var_idx})\n    expected = PandasIndex(pd_idx[[1, 2]], \"x\")\n    assert result.equals(expected)\n\n    # Test Variable indexer with different dimension\n    var_idx = Variable(\"y\", np.array([1, 2]))\n    result = index.isel({\"x\": var_idx})\n    assert result is None\n", "def test_PandasIndex_isel() -> None:\n    pd_idx = pd.Index([1, 2, 3, 4, 5], name=\"foo\")\n    index = PandasIndex(pd_idx, \"x\", coord_dtype=np.int32)\n\n    actual = index.isel({\"x\": slice(1, 4)})\n    expected = PandasIndex(pd_idx[1:4], \"x\", coord_dtype=np.int32)\n    assert actual.equals(expected)\n\n    actual = index.isel({\"x\": 2})\n    assert actual is None\n\n    actual = index.isel({\"x\": np.array([0, 2, 4])})\n    expected = PandasIndex(pd_idx[[0, 2, 4]], \"x\", coord_dtype=np.int32)\n    assert actual.equals(expected)\n", "def test_isel(self) -> None:\n    pd_idx = pd.Index([1, 2, 3, 4, 5], name=\"a\")\n    index = PandasIndex(pd_idx, \"x\", coord_dtype=np.int32)\n\n    # Test slicing\n    actual = index.isel({\"x\": slice(1, 4)})\n    expected = PandasIndex(pd_idx[1:4], \"x\", coord_dtype=np.int32)\n    assert actual.equals(expected)\n\n    # Test integer indexing\n    actual = index.isel({\"x\": 2})\n    assert actual is None\n\n    # Test array indexing\n    actual = index.isel({\"x\": np.array([0, 2, 4])})\n    expected = PandasIndex(pd_idx[np.array([0, 2, 4])], \"x\", coord_dtype=np.int32)\n    assert actual.equals(expected)\n\n    # Test Variable indexing\n    var = Variable(\"x\", [0, 2, 4])\n    actual = index.isel({\"x\": var})\n    expected = PandasIndex(pd_idx[np.array([0, 2, 4])], \"x\", coord_dtype=np.int32)\n    assert actual.equals(expected)\n\n    # Test Variable indexing with new dimensions\n    var_new_dim = Variable((\"y\",), [0, 2, 4])\n    actual = index.isel({\"x\": var_new_dim})\n    assert actual is None\n", "def test_roll(self) -> None:\n    index = PandasIndex([1, 2, 3, 4, 5], \"x\")\n\n    # roll forward by 2\n    actual = index.roll({\"x\": 2})\n    expected = PandasIndex([4, 5, 1, 2, 3], \"x\")\n    assert actual.equals(expected)\n\n    # roll backward by 2\n    actual = index.roll({\"x\": -2})\n    expected = PandasIndex([3, 4, 5, 1, 2], \"x\")\n    assert actual.equals(expected)\n\n    # roll by length of index should result in the same index\n    actual = index.roll({\"x\": 5})\n    expected = PandasIndex([1, 2, 3, 4, 5], \"x\")\n    assert actual.equals(expected)\n", "def test_normalize_label() -> None:\n    # Test normalization with simple array-like values\n    values = [1, 2, 3]\n    expected = np.array(values)\n    actual = normalize_label(values)\n    np.testing.assert_array_equal(actual, expected)\n\n    # Test normalization with tuple values\n    values = (1, 2, 3)\n    expected = np.array(values, dtype=object)\n    actual = normalize_label(values)\n    np.testing.assert_array_equal(actual, expected)\n\n    # Test normalization with numpy array values\n    values = np.array([1, 2, 3])\n    expected = values\n    actual = normalize_label(values)\n    np.testing.assert_array_equal(actual, expected)\n\n    # Test normalization with numpy array of dtype float\n    values = np.array([1.1, 2.2, 3.3], dtype=np.float32)\n    expected = values\n    actual = normalize_label(values)\n    np.testing.assert_array_equal(actual, expected)\n\n    # Test normalization with dtype conversion\n    values = np.array([1, 2, 3], dtype=np.float32)\n    expected = values.astype(np.float64)\n    actual = normalize_label(values, dtype=np.dtype('float64'))\n    np.testing.assert_array_equal(actual, expected)\n\n    # Test normalization with boolean indexers\n    values = np.array([True, False, True])\n    expected = values\n    actual = normalize_label(values)\n    np.testing.assert_array_equal(actual, expected)\n"], "sample_622": ["def test_maybe_encode_bools() -> None:\n    var = Variable([\"x\"], np.array([True, False, True]), {})\n    encoded_var = conventions.maybe_encode_bools(var)\n    expected_data = np.array([1, 0, 1], dtype=\"i1\")\n    expected_attrs = {\"dtype\": \"bool\"}\n    expected_var = Variable([\"x\"], expected_data, expected_attrs)\n    assert_identical(encoded_var, expected_var)\n\n    var_no_encoding = Variable([\"x\"], np.array([True, False, True]), {\"dtype\": \"bool\"})\n    encoded_var_no_encoding = conventions.maybe_encode_bools(var_no_encoding)\n    assert_identical(var_no_encoding, encoded_var_no_encoding)\n", "def test_var_as_tuple() -> None:\n    # Create a Variable\n    var = Variable((\"x\",), [1, 2, 3], {\"attr1\": \"value1\"}, {\"encoding1\": \"value1\"})\n    \n    # Get tuple representation\n    dims, data, attrs, encoding = conventions._var_as_tuple(var)\n    \n    # Check if the tuple representation is correct\n    assert dims == (\"x\",)\n    assert_array_equal(data, [1, 2, 3])\n    assert attrs == {\"attr1\": \"value1\"}\n    assert encoding == {\"encoding1\": \"value1\"}\n", "def test_encode_cf_variable_bool():\n    # Testing encoding of a boolean variable\n    var = Variable([\"x\"], np.array([True, False, True], dtype=bool))\n    encoded = conventions.encode_cf_variable(var)\n    expected = Variable([\"x\"], np.array([1, 0, 1], dtype=\"i1\"), {\"dtype\": \"bool\"})\n    assert_identical(encoded, expected)\n\n    # Ensure the encoded variable can be decoded back correctly\n    decoded = conventions.decode_cf_variable(\"x\", encoded)\n    assert_identical(decoded, var)\n", "def test_maybe_encode_bools() -> None:\n    # Test for boolean encoding\n    x = np.array([True, False, True, True], dtype=bool)\n    var = Variable([\"x\"], x)\n    encoded_var = conventions.maybe_encode_bools(var)\n    expected_data = np.array([1, 0, 1, 1], dtype=\"i1\")\n    expected_var = Variable([\"x\"], expected_data, attrs={\"dtype\": \"bool\"})\n    assert_identical(encoded_var, expected_var)\n\n    # Test for non-boolean encoding should not change the variable\n    y = np.array([1, 2, 3], dtype=int)\n    var = Variable([\"y\"], y)\n    encoded_var = conventions.maybe_encode_bools(var)\n    assert_identical(encoded_var, var)\n", "def test_maybe_encode_bools() -> None:\n    # Test maybe_encode_bools function\n    original = Variable([\"x\"], [True, False, True])\n    expected = Variable([\"x\"], [1, 0, 1], {\"dtype\": \"bool\"})\n    \n    # Apply the function maybe_encode_bools\n    encoded = conventions.maybe_encode_bools(original)\n    \n    assert_identical(encoded, expected)\n"], "sample_623": ["def test__get_default_engine_netcdf() -> None:\n    \"\"\"Test default engine selection for netCDF files.\"\"\"\n    engine_netcdf4 = _get_default_engine(\"/example.nc\")\n    assert engine_netcdf4 == \"netcdf4\"\n\n    try:\n        import netCDF4\n    except ImportError:\n        engine_scipy = _get_default_engine(\"/example.nc\")\n        assert engine_scipy == \"scipy\"\n", "def test_validate_attrs_valid_cases() -> None:\n    \"\"\"Test _validate_attrs with valid cases.\"\"\"\n    valid_attrs = {\n        \"attr1\": \"valid_string\",\n        \"attr2\": 123,\n        \"attr3\": 456.789,\n        \"attr4\": np.array([1, 2, 3]),\n        \"attr5\": [1, 2, 3],\n        \"attr6\": (1, 2, 3),\n        \"attr7\": np.bool_(True),\n        \"attr8\": np.array([True, False, True], dtype=np.bool_),\n    }\n    dataset = xr.Dataset(attrs=valid_attrs)\n    _validate_attrs(dataset, invalid_netcdf=True)  # Should not raise an error\n\n    dataset.attrs[\"attr9\"] = np.bool_(False)\n    _validate_attrs(dataset, invalid_netcdf=True)  # Should not raise an error\n\n    with pytest.raises(TypeError):\n        _validate_attrs(dataset, invalid_netcdf=False)  # Should raise an error because of np.bool_ with invalid_netcdf=False\n", "def test__get_default_engine_no_remote_access() -> None:\n    with pytest.raises(ValueError, match=\"cannot read or write netCDF files without\"):\n        _get_default_engine(\"file_without_extension\", allow_remote=False)\n", "def test_load_dataset() -> None:\n    # Create a simple dataset\n    data = xr.Dataset({\"var1\": (\"dim1\", np.arange(10))})\n\n    # Save the dataset to a temporary netCDF file\n    with xr.backends.NetCDF4DataStore.open(\"temp_test.nc\", mode=\"w\") as store:\n        data.to_netcdf(store)\n\n    # Load the dataset using the load_dataset function\n    loaded_data = load_dataset(\"temp_test.nc\")\n\n    # Check that the loaded dataset matches the original dataset\n    assert_identical(data, loaded_data)\n", "def test_open_dataset_with_default_engine() -> None:\n    # This test checks if the open_dataset function works with the default engine.\n    ds = xr.Dataset({\"a\": (\"x\", np.arange(10))})\n    path = \"test_default_engine.nc\"\n    \n    try:\n        ds.to_netcdf(path)\n        loaded_ds = xr.open_dataset(path)\n        assert_identical(ds, loaded_ds)\n    finally:\n        try:\n            os.remove(path)\n        except OSError:\n            pass\n"], "sample_624": ["def test_wrap_indent() -> None:\n    assert formatting.wrap_indent(\"hello world\", start=\">>> \") == \">>> hello world\"\n    assert formatting.wrap_indent(\"line1\\nline2\\nline3\", start=\">>> \") == \">>> line1\\n    line2\\n    line3\"\n    assert formatting.wrap_indent(\"singleline\") == \"singleline\"\n    assert formatting.wrap_indent(\"multi\\nline\", start=\">> \") == \">> multi\\n   line\"\n", "def test_wrap_indent() -> None:\n    text = \"This is a test string\\nthat spans multiple\\nlines.\"\n    start = \"Start: \"\n    length = len(start)\n    \n    # Expected output after wrap_indent\n    expected = \"Start: This is a test string\\n       that spans multiple\\n       lines.\"\n    actual = formatting.wrap_indent(text, start)\n    assert expected == actual\n    \n    # Test with different length\n    length = 10\n    expected = \"Start: This is a test string\\n          that spans multiple\\n          lines.\"\n    actual = formatting.wrap_indent(text, start, length)\n    assert expected == actual\n", "def test_format_timedelta() -> None:\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), None, \"10 days 01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), None, \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), None, \"0 days 03:00:00\"),\n        (pd.Timedelta(\"10 days\"), \"date\", \"10 days\"),\n        (pd.Timedelta(\"1 hour\"), \"time\", \"01:00:00\"),\n        (pd.Timedelta(\"NaT\"), None, \"NaT\"),\n    ]\n    for timedelta, fmt, expected in cases:\n        actual = formatting.format_timedelta(timedelta, fmt)\n        assert expected == actual\n\n    # Test with numpy.timedelta64\n    cases = [\n        (np.timedelta64(10, 'D'), None, \"10 days\"),\n        (np.timedelta64(-3, 'D'), None, \"-3 days\"),\n        (np.timedelta64(3, 'h'), None, \"0 days 03:00:00\"),\n        (np.timedelta64(10, 'D'), \"date\", \"10 days\"),\n        (np.timedelta64(1, 'h'), \"time\", \"01:00:00\"),\n    ]\n    for timedelta, fmt, expected in cases:\n        actual = formatting.format_timedelta(timedelta, fmt)\n        assert expected == actual\n", "def test_wrap_indent() -> None:\n    cases = [\n        (\"This is a test string\", \"\", 4, \"This is a test string\"),\n        (\"This is a test string\", \">\", 4, \">This is a test string\"),\n        (\"This is a\\nmultiline test\\nstring\", \"\", 4, \"This is a\\n    multiline test\\n    string\"),\n        (\"This is a\\nmultiline test\\nstring\", \">\", 4, \">This is a\\n    multiline test\\n    string\"),\n    ]\n    for text, start, length, expected in cases:\n        actual = formatting.wrap_indent(text, start, length)\n        assert actual == expected\n", "def test_inline_dask_repr() -> None:\n    import dask.array as da\n    \n    dask_arr = da.from_array(np.array([1, 2, 3]), chunks=2)\n    expected_repr = \"dask.array<chunksize=(2,)>\"\n    \n    actual_repr = formatting.inline_dask_repr(dask_arr)\n    assert actual_repr == expected_repr\n"], "sample_625": ["def test_apply_dataarray_vfunc() -> None:\n        return x**2\n\n    array = np.array([1, 2, 3])\n    data_array = xr.DataArray(array, dims=[\"x\"])\n    \n    expected = xr.DataArray(np.array([1, 4, 9]), dims=[\"x\"])\n    actual = apply_dataarray_vfunc(func, data_array, signature=_UFuncSignature([[\"x\"]]))\n    \n    assert_identical(expected, actual)\n", "def test_apply_ufunc_with_kwargs() -> None:\n    # define a function that accepts kwargs\n        return a + b + offset\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    zero_array = np.zeros_like(array)\n    zero_variable = xr.Variable(\"x\", zero_array)\n    zero_data_array = xr.DataArray(zero_variable, [(\"x\", -array)])\n    zero_dataset = xr.Dataset({\"y\": zero_variable}, {\"x\": -array})\n\n    offset = 5\n\n    assert_identical(array + offset, apply_ufunc(add_with_offset, array, zero_array, kwargs={'offset': offset}))\n    assert_identical(variable + offset, apply_ufunc(add_with_offset, variable, zero_array, kwargs={'offset': offset}))\n    assert_identical(data_array + offset, apply_ufunc(add_with_offset, data_array, zero_array, kwargs={'offset': offset}))\n    assert_identical(dataset + offset, apply_ufunc(add_with_offset, dataset, zero_array, kwargs={'offset': offset}))\n\n    # Also test with different offsets for each argument\n    offset_a = 5\n    offset_b = 3\n\n        return a + offset_a + b + offset_b\n\n    assert_identical(array + offset_a + zero_array + offset_b, apply_ufunc(add_with_offsets, array, zero_array, kwargs={'offset_a': offset_a, 'offset_b': offset_b}))\n    assert_identical(variable + offset_a + zero_variable + offset_b, apply_ufunc(add_with_offsets, variable, zero_variable, kwargs={'offset_a': offset_a, 'offset_b': offset_b}))\n    assert_identical(data_array + offset_a + zero_data_array + offset_b, apply_ufunc(add_with_offsets, data_array, zero_data_array, kwargs={'offset_a': offset_a, 'offset_b': offset_b}))\n    assert_identical(dataset + offset_a + zero_dataset + offset_b, apply_ufunc(add_with_offsets, dataset, zero_dataset, kwargs={'offset_a': offset_a, 'offset_b': offset_b}))\n", "def test__UFuncSignature_to_gufunc_string() -> None:\n    sig = _UFuncSignature([[\"a\"], [\"a\", \"b\"]], [[\"c\"]])\n    gufunc_string = sig.to_gufunc_string(exclude_dims={\"a\"})\n    expected = \"(dim0_0),(dim0_1,dim1)->(dim2)\"\n    assert gufunc_string == expected, f\"Expected {expected}, got {gufunc_string}\"\n\n    sig = _UFuncSignature([[\"x\", \"y\"], [\"y\", \"z\"]], [[\"w\"], [\"v\"]])\n    gufunc_string = sig.to_gufunc_string(exclude_dims={\"x\", \"z\"})\n    expected = \"(dim0_0,dim1),(dim1,dim2_0)->(dim3),(dim4)\"\n    assert gufunc_string == expected, f\"Expected {expected}, got {gufunc_string}\"\n", "def test_apply_groupby_func() -> None:\n        return a * b\n\n    da = xr.DataArray(\n        np.random.random((4, 3)),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": [\"a\", \"a\", \"b\", \"b\"], \"y\": [0, 1, 2]},\n    )\n\n    grouped = da.groupby(\"x\")\n    other = xr.DataArray([1, 2], dims=[\"x\"], coords={\"x\": [\"a\", \"b\"]})\n\n    expected = xr.DataArray(\n        [\n            [da[0, 0] * 1, da[0, 1] * 1, da[0, 2] * 1],\n            [da[1, 0] * 1, da[1, 1] * 1, da[1, 2] * 1],\n            [da[2, 0] * 2, da[2, 1] * 2, da[2, 2] * 2],\n            [da[3, 0] * 2, da[3, 1] * 2, da[3, 2] * 2],\n        ],\n        dims=[\"x\", \"y\"],\n        coords={\"x\": [\"a\", \"a\", \"b\", \"b\"], \"y\": [0, 1, 2]},\n    )\n\n    actual = apply_groupby_func(multiply, grouped, other)\n    xr.testing.assert_allclose(expected, actual)\n\n    grouped = da.groupby(\"y\")\n    other = xr.DataArray([1, 2, 3], dims=[\"y\"], coords={\"y\": [0, 1, 2]})\n\n    expected = xr.DataArray(\n        [\n            [da[0, 0] * 1, da[0, 1] * 2, da[0, 2] * 3],\n            [da[1, 0] * 1, da[1, 1] * 2, da[1, 2] * 3],\n            [da[2, 0] * 1, da[2, 1] * 2, da[2, 2] * 3],\n            [da[3, 0] * 1, da[3, 1] * 2, da[", "def test_apply_groupby_func() -> None:\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    coords = {\"x\": [1, 2, 1], \"group\": (\"x\", [0, 0, 1])}\n    data_array = xr.DataArray(variable, coords, dims=\"x\")\n    dataset = xr.Dataset({\"y\": variable}, coords)\n\n        return x * 2\n\n    result_da = apply_groupby_func(multiply_by_two, data_array.groupby(\"group\"))\n    result_ds = apply_groupby_func(multiply_by_two, dataset.groupby(\"group\"))\n\n    expected_da = xr.DataArray([2, 4, 6], coords=coords, dims=\"x\")\n    expected_ds = xr.Dataset({\"y\": [2, 4, 6]}, coords=coords)\n\n    assert_identical(result_da, expected_da)\n    assert_identical(result_ds, expected_ds)\n\n    with pytest.raises(ValueError, match=\"must have at least one groupby to iterate over\"):\n        apply_groupby_func(multiply_by_two, data_array)\n\n    with pytest.raises(ValueError, match=\"multiple GroupBy objects\"):\n        apply_groupby_func(multiply_by_two, data_array.groupby(\"group\"), data_array.groupby(\"x\"))\n"], "sample_626": ["def test_index_sel_result_as_tuple() -> None:\n    result = IndexSelResult(\n        dim_indexers={\"x\": 1},\n        indexes={\"x\": \"index_value\"},\n        variables={\"y\": \"var_value\"},\n        drop_coords=[\"coord1\"],\n        drop_indexes=[\"index1\"],\n        rename_dims={\"old_dim\": \"new_dim\"},\n    )\n\n    expected_tuple = (\n        {\"x\": 1},\n        {\"x\": \"index_value\"},\n        {\"y\": \"var_value\"},\n        [\"coord1\"],\n        [\"index1\"],\n        {\"old_dim\": \"new_dim\"},\n    )\n\n    assert result.as_tuple() == expected_tuple\n", "def test_merge_sel_results() -> None:\n    result1 = IndexSelResult(\n        dim_indexers={\"x\": [0, 1, 2]},\n        indexes={\"x\": CustomIndex({\"x\": 3})},\n        variables={\"var1\": xr.Variable(\"x\", [1, 2, 3])},\n        drop_coords=[\"coord1\"],\n        drop_indexes=[\"index1\"],\n        rename_dims={\"old_dim\": \"new_dim\"},\n    )\n    \n    result2 = IndexSelResult(\n        dim_indexers={\"y\": [3, 4, 5]},\n        indexes={\"y\": CustomIndex({\"y\": 3})},\n        variables={\"var2\": xr.Variable(\"y\", [4, 5, 6])},\n        drop_coords=[\"coord2\"],\n        drop_indexes=[\"index2\"],\n        rename_dims={\"old_dim2\": \"new_dim2\"},\n    )\n    \n    merged_result = merge_sel_results([result1, result2])\n    \n    assert merged_result.dim_indexers == {\"x\": [0, 1, 2], \"y\": [3, 4, 5]}\n    assert merged_result.indexes == {\"x\": CustomIndex({\"x\": 3}), \"y\": CustomIndex({\"y\": 3})}\n    assert merged_result.variables == {\"var1\": xr.Variable(\"x\", [1, 2, 3]), \"var2\": xr.Variable(\"y\", [4, 5, 6])}\n    assert merged_result.drop_coords == [\"coord1\", \"coord2\"]\n    assert merged_result.drop_indexes == [\"index1\", \"index2\"]\n    assert merged_result.rename_dims == {\"old_dim\": \"new_dim\", \"old_dim2\": \"new_dim2\"}\n\n    with pytest.raises(ValueError, match=r\".*more than one index over the following dimension.*\"):\n        merge_sel_results([result1, IndexSelResult(dim_indexers={\"x\": [6, 7, 8]})])\n", "def test_merge_sel_results() -> None:\n    # Test case with no duplicate dimensions\n    result1 = IndexSelResult(dim_indexers={\"x\": [1, 2]}, indexes={\"x\": None})\n    result2 = IndexSelResult(dim_indexers={\"y\": [3, 4]}, indexes={\"y\": None})\n    merged_result = merge_sel_results([result1, result2])\n    assert merged_result.dim_indexers == {\"x\": [1, 2], \"y\": [3, 4]}\n    assert merged_result.indexes == {\"x\": None, \"y\": None}\n\n    # Test case with duplicate dimensions raising ValueError\n    result3 = IndexSelResult(dim_indexers={\"x\": [1, 2]}, indexes={\"x\": None})\n    result4 = IndexSelResult(dim_indexers={\"x\": [3, 4]}, indexes={\"x\": None})\n    with pytest.raises(ValueError, match=\"Xarray does not support label-based selection with more than one index\"):\n        merge_sel_results([result3, result4])\n", "def test_merge_sel_results() -> None:\n    result1 = IndexSelResult(\n        dim_indexers={\"x\": slice(1, 3)},\n        indexes={\"x\": IndexVariable(\"x\", [1, 2, 3])},\n        variables={\"var1\": Variable((\"x\",), [1, 2, 3])},\n        drop_coords=[\"coord1\"],\n        drop_indexes=[\"index1\"],\n        rename_dims={\"old_dim\": \"new_dim\"},\n    )\n\n    result2 = IndexSelResult(\n        dim_indexers={\"y\": slice(0, 2)},\n        indexes={\"y\": IndexVariable(\"y\", [4, 5, 6])},\n        variables={\"var2\": Variable((\"y\",), [4, 5, 6])},\n        drop_coords=[\"coord2\"],\n        drop_indexes=[\"index2\"],\n        rename_dims={\"another_old_dim\": \"another_new_dim\"},\n    )\n\n    expected = IndexSelResult(\n        dim_indexers={\"x\": slice(1, 3), \"y\": slice(0, 2)},\n        indexes={\"x\": IndexVariable(\"x\", [1, 2, 3]), \"y\": IndexVariable(\"y\", [4, 5, 6])},\n        variables={\"var1\": Variable((\"x\",), [1, 2, 3]), \"var2\": Variable((\"y\",), [4, 5, 6])},\n        drop_coords=[\"coord1\", \"coord2\"],\n        drop_indexes=[\"index1\", \"index2\"],\n        rename_dims={\"old_dim\": \"new_dim\", \"another_old_dim\": \"another_new_dim\"},\n    )\n\n    actual = merge_sel_results([result1, result2])\n    \n    assert actual.dim_indexers == expected.dim_indexers\n    assert actual.indexes.keys() == expected.indexes.keys()\n    assert_identical(actual.variables[\"var1\"], expected.variables[\"var1\"])\n    assert_identical(actual.variables[\"var2\"], expected.variables[\"var2\"])\n    assert actual.drop_coords == expected.drop_coords\n    assert actual.drop_indexes == expected.drop_indexes\n    assert actual.rename_dims == expected.rename_dims\n", "def test_merge_sel_results() -> None:\n    res1 = IndexSelResult(\n        dim_indexers={\"x\": slice(0, 10)},\n        indexes={\"x\": CustomIndex({\"x\": 10})},\n        variables={\"var1\": xr.Variable(\"x\", np.arange(10))},\n        drop_coords=[\"coord1\"],\n        drop_indexes=[\"index1\"],\n        rename_dims={\"x\": \"x_new\"},\n    )\n\n    res2 = IndexSelResult(\n        dim_indexers={\"y\": slice(0, 5)},\n        indexes={\"y\": CustomIndex({\"y\": 5})},\n        variables={\"var2\": xr.Variable(\"y\", np.arange(5))},\n        drop_coords=[\"coord2\"],\n        drop_indexes=[\"index2\"],\n        rename_dims={\"y\": \"y_new\"},\n    )\n\n    merged = merge_sel_results([res1, res2])\n\n    assert merged.dim_indexers == {\"x\": slice(0, 10), \"y\": slice(0, 5)}\n    assert merged.indexes == {\"x\": CustomIndex({\"x\": 10}), \"y\": CustomIndex({\"y\": 5})}\n    assert \"var1\" in merged.variables\n    assert \"var2\" in merged.variables\n    assert merged.drop_coords == [\"coord1\", \"coord2\"]\n    assert merged.drop_indexes == [\"index1\", \"index2\"]\n    assert merged.rename_dims == {\"x\": \"x_new\", \"y\": \"y_new\"}\n\n    with pytest.raises(ValueError, match=\"Xarray does not support label-based selection\"):\n        res3 = IndexSelResult(\n            dim_indexers={\"x\": slice(10, 20)},\n        )\n        merge_sel_results([res1, res3])\n"], "sample_627": ["def test_concat_variable_with_different_shapes() -> None:\n    # Test concatenating variables with different shapes along a new dimension\n    ds1 = Dataset({\"a\": ((\"x\", \"y\"), [[1, 2], [3, 4]])})\n    ds2 = Dataset({\"a\": ((\"x\", \"y\"), [[5, 6, 7], [8, 9, 10]])})\n\n    # Expected result when concatenating along a new dimension\n    expected = Dataset(\n        {\"a\": ((\"z\", \"x\", \"y\"), [[[1, 2, np.nan], [3, 4, np.nan]], [[5, 6, 7], [8, 9, 10]]])},\n        coords={\"x\": [0, 1], \"y\": [0, 1, 2]}\n    )\n\n    actual = concat([ds1, ds2], dim=\"z\", fill_value=np.nan)\n    assert_identical(actual, expected)\n", "def test_concat_with_positions() -> None:\n    ds1 = Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [4, 5, 6])}, coords={\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"a\": (\"x\", [7, 8]), \"b\": (\"x\", [9, 10])}, coords={\"x\": [3, 4]})\n    ds3 = Dataset({\"a\": (\"x\", [11, 12, 13]), \"b\": (\"x\", [14, 15, 16])}, coords={\"x\": [5, 6, 7]})\n\n    expected = Dataset(\n        {\"a\": (\"x\", [1, 2, 3, 7, 8, 11, 12, 13]), \"b\": (\"x\", [4, 5, 6, 9, 10, 14, 15, 16])},\n        coords={\"x\": [0, 1, 2, 3, 4, 5, 6, 7]},\n    )\n\n    actual = concat([ds1, ds2, ds3], dim=\"x\", positions=[range(3), range(3, 5), range(5, 8)])\n    assert_identical(actual, expected)\n", "def test_concat_dimension_as_index() -> None:\n    # Test concatenating along a dimension specified as a pandas.Index\n    ds1 = Dataset({\"var1\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"var1\": (\"x\", [4, 5, 6])}, coords={\"x\": [3, 4, 5]})\n    index = pd.Index([0, 1, 2, 3, 4, 5], name=\"x\")\n    \n    expected = Dataset({\"var1\": (\"x\", [1, 2, 3, 4, 5, 6])}, coords={\"x\": [0, 1, 2, 3, 4, 5]})\n    actual = concat([ds1, ds2], dim=index)\n    assert_identical(actual, expected)\n\n    # Test with non-unique index values\n    ds1 = Dataset({\"var1\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"var1\": (\"x\", [4, 5, 6])}, coords={\"x\": [1, 2, 3]})\n    index = pd.Index([0, 1, 2, 1, 2, 3], name=\"x\")\n    \n    expected = Dataset({\"var1\": (\"x\", [1, 2, 3, 4, 5, 6])}, coords={\"x\": [0, 1, 2, 1, 2, 3]})\n    actual = concat([ds1, ds2], dim=index)\n    assert_identical(actual, expected)\n", "def test_concat_with_new_dimension_name() -> None:\n    # Test concatenation along a new dimension with a given name\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]})\n    ds2 = Dataset({\"foo\": (\"x\", [3, 4])}, coords={\"x\": [0, 1]})\n\n    expected = Dataset({\"foo\": ((\"new_dim\", \"x\"), [[1, 2], [3, 4]])}, coords={\"x\": [0, 1]})\n    actual = concat([ds1, ds2], dim=\"new_dim\")\n    \n    assert_identical(actual, expected)\n", "def test_concat_dim_as_index() -> None:\n    # Test when dim is provided as a pd.Index\n    ds1 = Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4])})\n    ds2 = Dataset({\"a\": (\"x\", [5, 6]), \"b\": (\"x\", [7, 8])})\n\n    index = pd.Index([0, 1], name=\"y\")\n    expected = Dataset({\"a\": ((\"y\", \"x\"), [[1, 2], [5, 6]]), \"b\": ((\"y\", \"x\"), [[3, 4], [7, 8]])})\n    \n    actual = concat([ds1, ds2], dim=index)\n    \n    assert_identical(actual, expected)\n    assert actual.indexes[\"y\"].equals(index)\n"], "sample_628": ["    def test_skip_words_with_special_characters(self):\n        self.checker.process_tokens(_tokenize_str(\"# $pecial &har@cter^s\"))\n        assert self.linter.release_messages() == []\n", "    def test_ignore_specific_words(self):\n        stmt = astroid.extract_node('def fff():\\n   \"\"\"bad coment\"\"\"\\n   pass')\n        with self.assertNoMessages():\n            self.checker.visit_functiondef(stmt)\n\n        stmt = astroid.extract_node('class Abc(object):\\n   \"\"\"bad coment\"\"\"\\n   pass')\n        with self.assertNoMessages():\n            self.checker.visit_classdef(stmt)\n", "    def test_ignore_specific_words_in_comments(self):\n        with self.assertNoMessages():\n            self.checker.process_tokens(_tokenize_str(\"# bad coment\"))\n        ", "    def test_skip_words_with_special_characters(self):\n        self.checker.process_tokens(_tokenize_str(\"# test@comment!\"))\n        assert self.linter.release_messages() == []\n", "    def test_ignore_words_in_config(self):\n        # Set ignore words through the configuration\n        self.checker.config.spelling_ignore_words = \"coment,msitake\"\n        \n        stmt = astroid.extract_node(\n            'def fff():\\n   \"\"\"This is a bad coment and msitake\"\"\"\\n   pass'\n        )\n        \n        with self.assertAddsMessages():\n            self.checker.visit_functiondef(stmt)\n"], "sample_629": ["def test_get_python_path():\n    # Assuming the current working directory has an __init__.py\n    current_dir = os.getcwd()\n    expected_path = current_dir\n    \n    # Create a temporary directory structure for testing\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        init_file_path = Path(tmpdirname) / \"__init__.py\"\n        init_file_path.touch()  # Create an empty __init__.py file\n        \n        # Test when file is inside a directory with __init__.py\n        subdir = Path(tmpdirname) / \"subdir\"\n        subdir.mkdir()\n        file_inside_subdir = subdir / \"module.py\"\n        file_inside_subdir.touch()\n        \n        assert get_python_path(str(file_inside_subdir)) == str(tmpdirname)\n\n        # Test when file is in a directory without __init__.py\n        subdir_without_init = Path(tmpdirname) / \"subdir_without_init\"\n        subdir_without_init.mkdir()\n        file_inside_subdir_without_init = subdir_without_init / \"module.py\"\n        file_inside_subdir_without_init.touch()\n        \n        assert get_python_path(str(file_inside_subdir_without_init)) == str(subdir_without_init)\n\n        # Test when file does not exist\n        non_existent_file = Path(tmpdirname) / \"non_existent.py\"\n        assert get_python_path(str(non_existent_file)) == str(tmpdirname)\n", "def test_get_python_path():\n    # Create a temporary directory and files to simulate the file structure\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Create __init__.py files in nested directories\n        init_path = Path(tmpdir) / \"pkg\" / \"__init__.py\"\n        init_path.parent.mkdir(parents=True, exist_ok=True)\n        init_path.touch()\n\n        nested_init_path = Path(tmpdir) / \"pkg\" / \"subpkg\" / \"__init__.py\"\n        nested_init_path.parent.mkdir(parents=True, exist_ok=True)\n        nested_init_path.touch()\n\n        # Create a regular file\n        regular_file_path = Path(tmpdir) / \"pkg\" / \"subpkg\" / \"module.py\"\n        regular_file_path.touch()\n\n        # Test cases\n        assert get_python_path(str(regular_file_path)) == str(init_path.parent)\n        assert get_python_path(str(nested_init_path)) == str(init_path.parent)\n        assert get_python_path(str(init_path)) == str(init_path.parent)\n", "def test_expand_modules():\n    files_or_modules = [\"test_module.py\", \"non_existent_module\", \"test_package\"]\n    ignore_list = [\"ignore_this.py\"]\n    ignore_list_re = [re.compile(\".*ignoreme.*\")]\n    ignore_list_paths_re = [re.compile(\".*/ignorepath/.*\")]\n    \n    # Create a temporary directory and files for testing\n    with pytest.MonkeyPatch().context() as mp:\n        mp.setattr(\"os.path.exists\", lambda x: x in [\"test_module.py\", \"test_package\"])\n        mp.setattr(\"os.path.isdir\", lambda x: x == \"test_package\")\n        mp.setattr(\"os.path.realpath\", lambda x: x)\n        mp.setattr(\"os.path.expanduser\", lambda x: x)\n        mp.setattr(\"os.path.dirname\", lambda x: \"/\".join(x.split(\"/\")[:-1]))\n        mp.setattr(\"os.getcwd\", lambda: \"/current_dir\")\n\n            if filename == \"test_module.py\":\n                return [\"test_module\"]\n            elif filename == \"test_package\":\n                return [\"test_package\"]\n            else:\n                raise ImportError\n\n        mp.setattr(\"pylint.lint.expand_modules.modutils.modpath_from_file\", mock_modpath_from_file)\n        \n            if modpath == [\"test_module\"]:\n                return \"test_module.py\"\n            elif modpath == [\"test_package\"]:\n                return \"test_package/__init__.py\"\n            else:\n                raise ImportError\n\n        mp.setattr(\"pylint.lint.expand_modules.modutils.file_from_modpath\", mock_file_from_modpath)\n        \n        result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n    \n    assert result == [\n        {\n            \"path\": \"test_module.py\",\n            \"name\": \"test_module\",\n            \"isarg\": True,\n            \"basepath\": \"test_module.py\",\n            \"basename\": \"test_module\"\n        },\n        {\n            \"path\": \"test_package/__init__.py\",\n            \"name\": \"test_package\",\n            \"isarg\": True,\n            \"basepath\": \"test_package/__init__.py\",\n            \"basename\": \"test_package\"\n        }\n    ]\n    assert errors == []\n", "def test_get_python_path():\n    # Create a temporary directory structure for testing\n    with pytest.MonkeyPatch().context() as mp:\n        temp_dir = Path(\"temp_test_dir\")\n        temp_dir.mkdir(exist_ok=True)\n        (temp_dir / \"subdir\").mkdir(exist_ok=True)\n        (temp_dir / \"__init__.py\").touch()\n        (temp_dir / \"subdir\" / \"__init__.py\").touch()\n\n        # Mock current working directory\n        mp.chdir(temp_dir)\n\n        # Test when the directory itself should be returned\n        assert get_python_path(\"subdir\") == str(temp_dir)\n\n        # Test when the parent directory should be returned\n        (temp_dir / \"subdir\" / \"__init__.py\").unlink()\n        assert get_python_path(\"subdir\") == str(temp_dir)\n\n        # Test when the current working directory should be returned\n        (temp_dir / \"__init__.py\").unlink()\n        assert get_python_path(\"subdir\") == str(temp_dir.resolve())\n\n        # Clean up temporary directory structure\n        (temp_dir / \"subdir\").rmdir()\n        temp_dir.rmdir()\n", "def test_get_python_path():\n    # Create a temporary directory structure\n    temp_dir = Path(\"temp_test_dir\")\n    temp_dir.mkdir(exist_ok=True)\n    (temp_dir / \"subdir\").mkdir(exist_ok=True)\n    (temp_dir / \"__init__.py\").touch()\n    (temp_dir / \"subdir\" / \"__init__.py\").touch()\n    (temp_dir / \"subdir\" / \"module.py\").touch()\n\n    # Test when a file is given\n    assert get_python_path(str(temp_dir / \"subdir\" / \"module.py\")) == str(temp_dir)\n\n    # Test when a directory without __init__.py is given\n    assert get_python_path(str(temp_dir / \"subdir\")) == str(temp_dir / \"subdir\")\n\n    # Test when a directory with __init__.py is given\n    assert get_python_path(str(temp_dir)) == str(temp_dir.parent)\n\n    # Cleanup\n    (temp_dir / \"subdir\" / \"module.py\").unlink()\n    (temp_dir / \"subdir\" / \"__init__.py\").unlink()\n    (temp_dir / \"__init__.py\").unlink()\n    (temp_dir / \"subdir\").rmdir()\n    temp_dir.rmdir()\n"], "sample_630": ["def test_diagram_writer_write(diagram_type, expected_edges):\n    class FakeDiagram:\n            self.title = title\n            self.TYPE = dtype\n            self.objects = []\n            self.modules_list = []\n            self.relationships = {edge: [] for edge in expected_edges}\n\n            return self.modules_list\n\n            return self.relationships[rel_type]\n\n    class FakeObject:\n            self.title = title\n            self.shape = shape\n            self.attrs = []\n            self.methods = []\n\n    config = Config()\n    writer = DotWriter(config)\n\n    # Create a fake diagram based on the parameterized type\n    diagram = FakeDiagram(\"TestDiagram\", diagram_type)\n    if diagram_type == \"class\":\n        diagram.objects.append(FakeObject(\"TestClass\"))\n\n    # Mocking the set_printer, write_classes, write_packages, and close_graph methods\n    writer.set_printer = lambda x, y: None\n    writer.write_classes = lambda x: None\n    writer.write_packages = lambda x: None\n    writer.close_graph = lambda: None\n\n    writer.write([diagram])\n\n    # Check if edges are written correctly\n    for edge in expected_edges:\n        assert diagram.get_relationships(edge) == []\n", "def test_dot_writer_class_diagram():\n    \"\"\"Test DotWriter for class diagram generation.\"\"\"\n    project = get_project(os.path.join(os.path.dirname(__file__), \"data\"))\n    linker = Linker(project)\n    CONFIG = Config()\n    handler = DiadefsHandler(CONFIG)\n    dd = DefaultDiadefGenerator(linker, handler).visit(project)\n    for diagram in dd:\n        diagram.extract_relationships()\n\n    writer = DotWriter(CONFIG)\n    writer.write_classes(dd[0])\n\n    file_name = f\"{dd[0].title.strip().replace(' ', '_')}.{CONFIG.output_format}\"\n    with open(file_name, \"r\") as f:\n        content = f.read()\n\n    assert \"digraph\" in content, \"DOT graph not generated correctly\"\n    assert any(\"label\" in line for line in content.splitlines()), \"Labels not found in DOT graph\"\n    os.remove(file_name)\n", "def test_vcgwriter_set_printer():\n    \"\"\"Test the set_printer method of VCGWriter.\"\"\"\n    config = Config()\n    writer = VCGWriter(config)\n    file_name = \"test_file.vcg\"\n    basename = \"test_basename\"\n    \n    writer.set_printer(file_name, basename)\n    \n    assert writer.graph_file.name == file_name\n    assert writer.printer is not None\n    assert writer.printer.title == basename\n    assert writer.printer.layoutalgorithm == \"dfs\"\n    assert writer.printer.late_edge_labels == \"yes\"\n    assert writer.printer.port_sharing == \"no\"\n    assert writer.printer.manhattan_edges == \"yes\"\n", "def test_dotwriter_initialization():\n    \"\"\"Test the initialization of DotWriter and VCGWriter with a Config\"\"\"\n    config = Config()\n    dot_writer = DotWriter(config)\n    assert dot_writer.config == config\n    assert dot_writer.pkg_edges == dict(arrowtail=\"none\", arrowhead=\"open\")\n    assert dot_writer.inh_edges == dict(arrowtail=\"none\", arrowhead=\"empty\")\n    assert dot_writer.imp_edges == dict(arrowtail=\"node\", arrowhead=\"empty\", style=\"dashed\")\n    assert dot_writer.association_edges == dict(fontcolor=\"green\", arrowtail=\"none\", arrowhead=\"diamond\", style=\"solid\")\n", "def test_dot_writer_initialization():\n    \"\"\"Test initialization of DotWriter\"\"\"\n    config = Config()\n    writer = DotWriter(config)\n    assert isinstance(writer, DotWriter)\n    assert writer.config == config\n    assert writer.pkg_edges == dict(arrowtail=\"none\", arrowhead=\"open\")\n    assert writer.inh_edges == dict(arrowtail=\"none\", arrowhead=\"empty\")\n    assert writer.imp_edges == dict(arrowtail=\"node\", arrowhead=\"empty\", style=\"dashed\")\n    assert writer.association_edges == dict(fontcolor=\"green\", arrowtail=\"none\", arrowhead=\"diamond\", style=\"solid\")\n"], "sample_631": ["    def test_unbalanced_tuple_unpacking(self):\n        \"\"\"Ensure proper detection of unbalanced tuple unpacking.\"\"\"\n        node = astroid.parse(\n            \"\"\"\n        a, b, c = (1, 2)\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\n                \"unbalanced-tuple-unpacking\",\n                node=node.body[0],\n                args=(\"\", 3, 2),\n            )\n        ):\n            self.walk(node)\n\n        node = astroid.parse(\n            \"\"\"\n        x, y, z = [1, 2, 3, 4]\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\n                \"unbalanced-tuple-unpacking\",\n                node=node.body[0],\n                args=(\"\", 3, 4),\n            )\n        ):\n            self.walk(node)\n", "def test_undefined_variable_in_class_scope(self):\n    node = astroid.parse(\n        \"\"\"\n        class MyClass:\n                print(undefined_var)\n        \"\"\"\n    )\n    message = Message(\"undefined-variable\", node=node.body[0].body[0].body[0], args=\"undefined_var\")\n    with self.assertAddsMessages(message):\n        self.walk(node)\n", "def test_variable_defined_and_used_in_different_scopes(self):\n    \"\"\"Ensure that variables defined in one scope and used in another are handled correctly.\"\"\"\n    module = astroid.parse(\n        \"\"\"\n            x = 1\n                return x\n            return inner()\n        \"\"\"\n    )\n    with self.assertNoMessages():\n        self.walk(module)\n\n    module = astroid.parse(\n        \"\"\"\n                return x\n            x = 1\n            return inner()\n        \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"used-before-assignment\", node=module.body[0].body[0].body[0], args=\"x\")\n    ):\n        self.walk(module)\n", "    def test_global_variable_redefined(self):\n        \"\"\"Ensure that redefining a global variable within a function is caught.\"\"\"\n        node = astroid.parse(\n            \"\"\"\n            global_var = 5\n                global global_var\n                global_var = 10\n            \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"global-variable-not-assigned\", node=node.body[1].body[0], args=\"global_var\")\n        ):\n            self.walk(node)\n", "    def test_variable_used_before_assignment(self):\n        \"\"\"Test that variables used before assignment are detected.\"\"\"\n        node = astroid.parse(\n            \"\"\"\n            print(a)\n            a = 1\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"used-before-assignment\", node=node.body[0].body[0].value, args=\"a\")\n        ):\n            self.walk(node)\n"], "sample_632": ["def test_combined_flags():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-comments\", \"--ignore-docstrings\", \"--ignore-imports\", \"--ignore-signatures\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_append_stream_with_encoding():\n    encoding = 'utf-8'\n    content = \"import os\\nimport sys\\n\"\n    streamid = \"testfile.py\"\n    sim = similar.Similar()\n    stream = StringIO(content)\n    sim.append_stream(streamid, stream, encoding=encoding)\n\n    assert len(sim.linesets) == 1\n    assert sim.linesets[0].name == streamid\n    assert sim.linesets[0]._real_lines == content.splitlines()\n", "def test_min_similarity_lines():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--duplicates\", \"2\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            \"\"\"", "def test_append_stream_unicode_decode_error():\n    content = \"print('Hello World')\\n\"\n    filename = \"test_unicode.py\"\n\n    class MockStream:\n            raise UnicodeDecodeError(\"codec\", b\"\", 1, 2, \"reason\")\n\n    mock_stream = MockStream()\n\n    sim = similar.Similar()\n    sim.append_stream(filename, mock_stream)\n\n    assert len(sim.linesets) == 0\n", "def test_similar_lines_in_multiple_files():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR1, SIMILAR2, SIMILAR3])\n    assert ex.value.code == 0\n    assert (\n        \"10 similar lines in 2 files\" in output.getvalue()\n        and \"8 similar lines in 2 files\" in output.getvalue()\n        and \"5 similar lines in 2 files\" in output.getvalue()\n    )\n"], "sample_633": ["def test_remove_successives() -> None:\n    all_couples = {\n        LineSetStartCouple(Index(11), Index(34)): CplSuccessiveLinesLimits(\n            SuccessiveLinesLimits(LineNumber(5), LineNumber(9)),\n            SuccessiveLinesLimits(LineNumber(27), LineNumber(31)),\n            effective_cmn_lines_nb=4\n        ),\n        LineSetStartCouple(Index(12), Index(35)): CplSuccessiveLinesLimits(\n            SuccessiveLinesLimits(LineNumber(6), LineNumber(10)),\n            SuccessiveLinesLimits(LineNumber(28), LineNumber(32)),\n            effective_cmn_lines_nb=4\n        ),\n        LineSetStartCouple(Index(23), Index(79)): CplSuccessiveLinesLimits(\n            SuccessiveLinesLimits(LineNumber(15), LineNumber(19)),\n            SuccessiveLinesLimits(LineNumber(45), LineNumber(49)),\n            effective_cmn_lines_nb=4\n        ),\n    }\n    remove_successives(all_couples)\n    expected_couples = {\n        LineSetStartCouple(Index(11), Index(34)): CplSuccessiveLinesLimits(\n            SuccessiveLinesLimits(LineNumber(5), LineNumber(10)),\n            SuccessiveLinesLimits(LineNumber(27), LineNumber(32)),\n            effective_cmn_lines_nb=5\n        ),\n        LineSetStartCouple(Index(23), Index(79)): CplSuccessiveLinesLimits(\n            SuccessiveLinesLimits(LineNumber(15), LineNumber(19)),\n            SuccessiveLinesLimits(LineNumber(45), LineNumber(49)),\n            effective_cmn_lines_nb=4\n        ),\n    }\n    assert all_couples == expected_couples\n", "def test_similar_lines_with_min_lines() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--duplicates\", \"2\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f\"\"\"", "def test_similar_lines_with_min_similarity_lines() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--duplicates=4\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f\"\"\"", "def test_min_similarity_lines() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--duplicates\", \"3\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f\"\"\"", "def test_ignore_comments_and_docstrings() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-comments\", \"--ignore-docstrings\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f\"\"\""], "sample_634": ["def test_expand_modules_with_ignore_list(self, files_or_modules, ignore_list, expected):\n    \"\"\"Test expand_modules with specific files in ignore_list\"\"\"\n    ignore_list_re = []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "def test__is_in_ignore_list_re_no_match() -> None:\n    patterns = [\n        re.compile(\".*enchilada.*\"),\n        re.compile(\"unittest_.*\"),\n        re.compile(\".*tests/.*\"),\n    ]\n    assert not _is_in_ignore_list_re(\"regular_file.py\", patterns)\n    assert not _is_in_ignore_list_re(\"documentation.md\", patterns)\n    assert not _is_in_ignore_list_re(\"src/scripts/script.py\", patterns)\n\n", "def test__is_in_ignore_list_re_no_match() -> None:\n    patterns = [\n        re.compile(\".*enchilada.*\"),\n        re.compile(\"unittest_.*\"),\n        re.compile(\".*tests/.*\"),\n    ]\n    assert not _is_in_ignore_list_re(\"some_random_file.py\", patterns)\n    assert not _is_in_ignore_list_re(\"documentation.md\", patterns)\n    assert not _is_in_ignore_list_re(\"src/main.py\", patterns)\n", "def test_expand_modules_with_ignores(self, files_or_modules, ignore_list, ignore_list_re, expected):\n    \"\"\"Test expand_modules with ignore-list and ignore-list-re\"\"\"\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "def test__is_in_ignore_list_re_no_match() -> None:\n    patterns = [\n        re.compile(\".*enchilada.*\"),\n        re.compile(\"unittest_.*\"),\n        re.compile(\".*tests/.*\"),\n    ]\n    assert not _is_in_ignore_list_re(\"example.py\", patterns)\n    assert not _is_in_ignore_list_re(\"src/sample_module.py\", patterns)\n    assert not _is_in_ignore_list_re(\"documentation.md\", patterns)\n\n"], "sample_635": ["    def test_google_docstring_with_extra_parameter(self) -> None:\n        \"\"\"Example of a function with extra parameter documented in\n        Google style docstring that is not present in the function signature.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''docstring ...\n\n            Args:\n                x: bla\n                y: blah blah\n                z: extra parameter not in function signature\n\n            some other stuff\n            '''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"differing-param-doc\", node=node, args=(\"z\",))\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_missing_func_params_in_epytext_docstring(self) -> None:\n        \"\"\"Example of a function with missing Epytext parameter documentation\n        in the docstring.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''docstring ...\n\n            @param x: bla\n            @type x: int\n\n            @param z: bar\n            '''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"missing-param-doc\", node=node, args=(\"y\",)),\n            MessageTest(msg_id=\"missing-type-doc\", node=node, args=(\"y\",)),\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_docstringify_recognizes_sphinx(self) -> None:\n        \"\"\"Test that `docstringify` correctly recognizes Sphinx style docstrings.\"\"\"\n        docstring = \"\"\"\n        A sample function.\n\n        :param x: The first parameter.\n        :type x: int\n        :param y: The second parameter.\n        :type y: str\n        :returns: Description of return value.\n        :rtype: bool\n        \"\"\"\n        result = docstringify(docstring, default_type=\"sphinx\")\n        assert isinstance(result, SphinxDocstring)\n        assert result.is_valid() is True\n", "    def test_missing_func_params_in_epytext_docstring(self) -> None:\n        \"\"\"Example of a function with missing Epytext style parameter\n        documentation in the docstring\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''docstring ...\n\n            @param x: bla\n\n            @param int z: bar\n            '''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"missing-param-doc\", node=node, args=(\"y\",)),\n            MessageTest(msg_id=\"missing-type-doc\", node=node, args=(\"x, y\",)),\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_finds_exceptions_in_sphinx_docstring(self) -> None:\n        \"\"\"Example of a function with exception documentation in the\n        Sphinx style docstring\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''docstring ...\n\n            :param x: bla\n            :type x: int\n            :raises ValueError: If x is zero\n            '''\n            if x == 0:\n                raise ValueError(\"x cannot be zero\")\n            return x\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.checker.visit_functiondef(node)\n"], "sample_636": ["    def test_duplicate_code_with_imports_ignored(self) -> None:\n        \"\"\"Tests ignoring import statements when checking for duplicate code.\"\"\"\n        path = join(DATA, \"raw_strings_with_imports\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-imports=y\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_ignore_comments(self) -> None:\n        \"\"\"Tests if ignoring comments works correctly.\"\"\"\n        path = join(DATA, \"raw_strings_with_comments\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-comments=y\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_with_docstrings(self) -> None:\n        \"\"\"Tests if duplicate code detection works when there are docstrings in the files.\"\"\"\n        path = join(DATA, \"raw_strings_with_docstrings\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-docstrings=no\"],\n            expected_output=expected_output,\n        )\n", "    def test_ignore_docstrings_option(self) -> None:\n        \"\"\"Tests that the ignore-docstrings option works as expected.\"\"\"\n        path = join(DATA, \"raw_strings_docstrings\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-docstrings=y\"],\n            expected_output=expected_output,\n        )\n", "def test_duplicate_code_with_import_statements(self) -> None:\n    \"\"\"Tests similarity detection with import statements ignored.\"\"\"\n    path = join(DATA, \"raw_strings_with_imports\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-imports=y\"],\n        expected_output=expected_output,\n    )\n"], "sample_637": ["    def test_mixed_case_codetag(self) -> None:\n        code = \"\"\"a = 1\n                # FiXmE mixed case\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"FiXmE mixed case\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_nonexistent_encoding_declaration(self) -> None:\n        code = \"\"\"# coding: nonexistent\n                a = 1\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"syntax-error\",\n                line=1,\n                args=\"Cannot decode using encoding 'nonexistent', bad encoding\",\n            )\n        ):\n            self.checker.process_module(_tokenize_str(code))\n", "    def test_process_module_with_ascii_encoding(self) -> None:\n        code = \"\"\"a = 1\n                # This is a comment\n                print(\"Hello, world!\")\n                \"\"\"\n        node = self.create_module_node(code, file_encoding=\"ascii\")\n        with self.assertNoMessages():\n            self.checker.process_module(node)\n", "    def test_invalid_encoding_declaration(self) -> None:\n        code = \"\"\"# coding: invalid-encoding\n                a = 1\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"syntax-error\",\n                line=1,\n                args=\"Cannot decode using encoding 'invalid-encoding', bad encoding\",\n            )\n        ):\n            self.checker.process_module(self._module_node_from_string(code))\n", "    def test_fixme_case_insensitivity(self) -> None:\n        code = \"\"\"a = 1\n                # fixme message\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"fixme message\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n"], "sample_638": ["def test_directly_supported_format(mock_writer):\n    \"\"\"Test that a directly supported format does not use Graphviz.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"dot\", TEST_DATA_DIR])\n    # Check that pyreverse actually made the call to create the diagram and we exit cleanly\n    mock_writer.DiagramWriter().write.assert_called_once()\n    assert wrapped_sysexit.value.code == 0\n", "def test_run_with_no_arguments(mock_writer, mock_diadefs_handler, mock_linker, capsys):\n    \"\"\"Test running the main function with no arguments.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    # Check that the help message is shown to the user\n    output = capsys.readouterr().out\n    assert \"Usage: %prog [options] <packages>\" in output\n    # Ensure that no further processing is done when no arguments are given\n    mock_linker.assert_not_called()\n    mock_diadefs_handler.assert_not_called()\n    mock_writer.DiagramWriter().write.assert_not_called()\n    # Ensure the program exits with a status code of 1\n    assert wrapped_sysexit.value.code == 1\n", "def test_invalid_output_format_exit_code(mock_writer, capsys):\n    \"\"\"Test that the application exits with code 1 when no arguments are provided.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    # Check that the help message is shown to the user\n    assert \"Usage: \" in capsys.readouterr().out\n    # Check that we exit with code 1\n    assert wrapped_sysexit.value.code == 1\n", "def test_default_output_format(mock_writer):\n    \"\"\"Test that the default output format is 'dot' when no format is specified.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([TEST_DATA_DIR])\n    # Check that the default format is used\n    assert main.Run([]).config.output_format == \"dot\"\n    # Check that pyreverse actually made the call to create the diagram and we exit cleanly\n    mock_writer.DiagramWriter().write.assert_called_once()\n    assert wrapped_sysexit.value.code == 0\n", "def test_run_without_arguments(mock_writer, capsys):\n    \"\"\"Test running the main script without any arguments.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    # Check that the help message is printed\n    captured = capsys.readouterr()\n    assert \"Usage: \" in captured.out\n    assert \"create UML diagrams for classes and modules in <packages>\" in captured.out\n    # Check that the exit code is 1 (error)\n    assert wrapped_sysexit.value.code == 1\n"], "sample_639": ["def test_add_message() -> None:\n    class MockLinter:\n            self.messages = []\n\n            self, msgid, line, node, args, confidence, col_offset, end_lineno, end_col_offset\n        ):\n            self.messages.append({\n                \"msgid\": msgid,\n                \"line\": line,\n                \"node\": node,\n                \"args\": args,\n                \"confidence\": confidence,\n                \"col_offset\": col_offset,\n                \"end_lineno\": end_lineno,\n                \"end_col_offset\": end_col_offset,\n            })\n\n    linter = MockLinter()\n    basic_checker = OtherBasicChecker(linter=linter)\n    \n    basic_checker.add_message(\"W0001\", line=10, args=\"Test argument\", col_offset=5)\n    \n    expected_message = {\n        \"msgid\": \"W0001\",\n        \"line\": 10,\n        \"node\": None,\n        \"args\": \"Test argument\",\n        \"confidence\": None,\n        \"col_offset\": 5,\n        \"end_lineno\": None,\n        \"end_col_offset\": None,\n    }\n    \n    assert linter.messages == [expected_message]\n", "def test_base_checker_comparison() -> None:\n    basic_checker = OtherBasicChecker()\n    different_checker = DifferentBasicChecker()\n    \n    assert basic_checker != different_checker\n    assert basic_checker > different_checker\n    assert not (different_checker > basic_checker)\n    assert hash(basic_checker) != hash(different_checker)\n", "def test_base_checker_comparison() -> None:\n    checker1 = OtherBasicChecker()\n    checker2 = DifferentBasicChecker()\n    checker3 = OtherBasicChecker()\n\n    assert checker1 != checker2\n    assert checker1 == checker3\n    assert not checker1 > checker3\n    assert checker1 > checker2\n\n    checker2.enabled = False\n    assert repr(checker2) == \"Disabled checker 'different' (responsible for 'W0002')\"\n", "def test_base_checker_comparison() -> None:\n    basic = OtherBasicChecker()\n    less_basic = LessBasicChecker()\n    different = DifferentBasicChecker()\n\n    # Test equality\n    assert basic == less_basic\n    assert basic != different\n    assert less_basic != different\n\n    # Test greater than\n    assert less_basic > different\n    assert not (basic > less_basic)\n\n    # Test hash\n    assert hash(basic) == hash(less_basic)\n    assert hash(basic) != hash(different)\n\n    # Test representation and string conversion\n    assert repr(different) == \"Checker 'different' (responsible for 'W0002')\"\n    assert str(different).startswith(\"Different checker\")\n", "def test_base_checker_equality_and_hash() -> None:\n    basic = OtherBasicChecker()\n    less_basic = LessBasicChecker()\n    different_basic = DifferentBasicChecker()\n\n    assert basic == less_basic  # Both have the same name and messages\n    assert hash(basic) == hash(less_basic)\n    \n    assert basic != different_basic  # Different name and messages\n    assert hash(basic) != hash(different_basic)\n"], "sample_640": ["def test_is_defined_in_scope() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n        x = 1\n        if True:  #@\n            y = 2\n            z = x + y\n        for a in range(5):  #@\n            b = a\n        with open(\"test.txt\") as f:  #@\n            c = f.read()\n            d = 4\n        try:  #@\n            e = 5\n        except Exception:  #@\n            f = 6\n    \"\"\"\n    )\n    assert isinstance(code, list) and len(code) == 7\n\n    if_scope = code[0]\n    assert utils.is_defined_in_scope(if_scope.body[0], \"x\", if_scope)\n    assert utils.is_defined_in_scope(if_scope.body[0], \"y\", if_scope)\n    assert not utils.is_defined_in_scope(if_scope.body[0], \"z\", if_scope)\n\n    for_scope = code[1]\n    assert utils.is_defined_in_scope(for_scope.body[0], \"a\", for_scope)\n    assert utils.is_defined_in_scope(for_scope.body[0], \"b\", for_scope)\n\n    with_scope = code[2]\n    assert utils.is_defined_in_scope(with_scope.body[0], \"f\", with_scope)\n    assert utils.is_defined_in_scope(with_scope.body[0], \"c\", with_scope)\n\n    inner_scope = code[3]\n    assert utils.is_defined_in_scope(inner_scope.body[0], \"d\", inner_scope)\n\n    try_scope = code[4]\n    assert utils.is_defined_in_scope(try_scope.body[0], \"e\", try_scope)\n    \n    except_scope = code[5]\n    assert utils.is_defined_in_scope(except_scope.body[0], \"f\", except_scope)\n", "def test_is_defined_before() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    a = 1\n    a  #@\n    b = 2\n    if True:\n        a = 3\n    b  #@\n    c = 4\n    c  #@\n    \"\"\"\n    )\n    assert utils.is_defined_before(code[0]) is True\n    assert utils.is_defined_before(code[1]) is True\n    assert utils.is_defined_before(code[2]) is True\n", "def test_is_property_setter_or_deleter() -> None:\n    node = astroid.extract_node(\n        \"\"\"\n    class MyClass:\n        @property\n            return 42\n\n        @prop.setter\n            pass\n\n        @prop.deleter\n            pass\n    \"\"\"\n    )\n    assert not utils.is_property_setter_or_deleter(node[0])\n    assert utils.is_property_setter_or_deleter(node[1])\n    assert utils.is_property_setter_or_deleter(node[2])\n", "def test_is_defined_before() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    # case 1: variable is defined before\n    x = 1\n    print(x) #@\n\n    # case 2: variable is defined after\n    print(y) #@\n    y = 2\n\n    # case 3: variable is defined in a nested scope\n    if True:\n        z = 3\n    print(z) #@\n    \"\"\"\n    )\n    assert utils.is_defined_before(code[0])\n    assert not utils.is_defined_before(code[1])\n    assert utils.is_defined_before(code[2])\n", "def test_is_defined_in_scope() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    if True:\n        x = 1  #@\n    if True:\n        y = 2  #@\n    if True:\n        nonlocal_var = 3  #@\n        nonlocal_var  #@\n        x  #@\n        z = 4\n    \"\"\"\n    )\n    assert utils.is_defined_in_scope(code[3], \"nonlocal_var\", code[2]) is False\n    assert utils.is_defined_in_scope(code[4], \"x\", code[0]) is True\n    assert utils.is_defined_in_scope(code[4], \"y\", code[1]) is False\n"], "sample_641": ["def test_load_results_with_invalid_data(tmp_path: Path, invalid_data) -> None:\n    data_file = tmp_path / \"_1.stats\"\n    with open(data_file, \"wb\") as stream:\n        pickle.dump(invalid_data, stream)\n    with pytest.warns(UserWarning, match=\"You're using an old pylint cache with invalid data following an upgrade\"):\n        result = load_results(tmp_path)\n    assert result is None\n", "def test_load_results_with_invalid_data(tmp_path: Path, invalid_data) -> None:\n    data_file = tmp_path / \"_1.stats\"\n    with open(data_file, \"wb\") as stream:\n        pickle.dump(invalid_data, stream)\n    \n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        result = load_results(tmp_path)\n        assert result is None\n        assert len(w) == 1\n        assert issubclass(w[-1].category, UserWarning)\n        assert \"You're using an old pylint cache with invalid data\" in str(w[-1].message)\n", "def test_load_results_file_not_exist(path: str) -> None:\n    assert load_results(path) is None\n\n", "def test_load_results_with_invalid_data(tmp_path: Path, invalid_data) -> None:\n    data_file = tmp_path / \"invalid_data.stats\"\n    with open(data_file, \"wb\") as stream:\n        pickle.dump(invalid_data, stream)\n    \n    result = load_results(data_file)\n    assert result is None\n", "def test_load_results_non_existent_path(path: str, pylint_home: Path) -> None:\n    result = load_results(path, pylint_home)\n    assert result is None\n"], "sample_642": ["def test_preprocess_options_errors(args: list[str], expected_exception: type[Exception], expected_message: str) -> None:\n    \"\"\"Test that _preprocess_options raises the appropriate exceptions for invalid options.\"\"\"\n    run = mock.MagicMock(spec=Run)\n    with pytest.raises(expected_exception, match=expected_message):\n        _preprocess_options(run, args)\n", "def test_preprocess_options() -> None:\n    \"\"\"Test the _preprocess_options function with various inputs.\"\"\"\n    run = Run()\n    \n    # Test --init-hook option\n    args = [\"--init-hook=import os; os.environ['TEST'] = '1'\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == []\n    assert os.environ.get('TEST') == '1'\n    \n    # Test --rcfile option\n    args = [\"--rcfile=test_rcfile\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == []\n    assert run._rcfile == \"test_rcfile\"\n    \n    # Test --output option\n    args = [\"--output=test_output\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == []\n    assert run._output == \"test_output\"\n    \n    # Test --load-plugins option\n    args = [\"--load-plugins=test_plugin\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == []\n    assert \"test_plugin\" in run._plugins\n    \n    # Test --verbose option\n    args = [\"--verbose\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == []\n    assert run.verbose\n    \n    # Test --enable-all-extensions option\n    with mock.patch(\"pylint.config.extensions\") as mock_extensions:\n        mock_extensions.__file__ = __file__\n        args = [\"--enable-all-extensions\"]\n        processed_args = config._preprocess_options(run, args)\n        assert processed_args == []\n        assert \"pylint.extensions.test_file\" in run._plugins\n    \n    # Test unrecognized option\n    args = [\"--unknown-option\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == [\"--unknown-option\"]\n    \n    # Test option with missing value\n    args = [\"--rcfile\"]\n    with pytest.raises(config.ArgumentPreprocessingError):\n        config._preprocess_options(run, args)\n    \n    # Test option that doesn't expect a value but is given one\n    args = [\"--verbose=value\"]\n    with pytest.raises(config.ArgumentPreprocessingError):\n        config._preprocess_options(run, args)\n", "def test_preprocess_options(args, expected_exception, expected_message) -> None:\n    \"\"\"Test preprocessing of options before full config parsing.\"\"\"\n    run = mock.Mock(spec=Run)\n    if expected_exception:\n        with pytest.raises(expected_exception) as exc_info:\n            _preprocess_options(run, args)\n        assert str(exc_info.value) == expected_message\n    else:\n        processed_args = _preprocess_options(run, args)\n        assert processed_args == []\n", "def test_preprocess_options() -> None:\n    \"\"\"Test the _preprocess_options function with various arguments.\"\"\"\n    run = mock.Mock(spec=Run)\n    \n    # Test case: Valid --init-hook option\n    args = [\"--init-hook=print('Hello, World!')\"]\n    processed_args = _preprocess_options(run, args)\n    run._mock.assert_called_once_with(run, \"print('Hello, World!')\")\n    assert processed_args == []\n\n    # Test case: Valid --rcfile option\n    args = [\"--rcfile=myrcfile\"]\n    processed_args = _preprocess_options(run, args)\n    run._mock.assert_called_once_with(run, \"myrcfile\")\n    assert processed_args == []\n\n    # Test case: Valid --verbose option\n    args = [\"--verbose\"]\n    processed_args = _preprocess_options(run, args)\n    run._mock.assert_called_once_with(run, None)\n    assert processed_args == []\n\n    # Test case: Invalid --rcfile option without value\n    args = [\"--rcfile\"]\n    with pytest.raises(ArgumentPreprocessingError) as excinfo:\n        _preprocess_options(run, args)\n    assert \"Option --rcfile expects a value\" in str(excinfo.value)\n\n    # Test case: Valid --enable-all-extensions option\n    args = [\"--enable-all-extensions\"]\n    processed_args = _preprocess_options(run, args)\n    run._mock.assert_called_once_with(run, None)\n    assert processed_args == []\n\n    # Test case: Unknown option\n    args = [\"--unknown-option=value\"]\n    processed_args = _preprocess_options(run, args)\n    assert processed_args == [\"--unknown-option=value\"]\n", "def test_preprocess_options(args: list[str], expected: list[str] | type[Exception]) -> None:\n    \"\"\"Test the _preprocess_options function with various arguments.\"\"\"\n    run = mock.Mock()\n    if isinstance(expected, type) and issubclass(expected, Exception):\n        with pytest.raises(expected):\n            _preprocess_options(run, args)\n    else:\n        assert _preprocess_options(run, args) == expected\n"], "sample_643": ["def test_colorize_ansi() -> None:\n    \"\"\"Test the colorize_ansi function with various styles and colors.\"\"\"\n    plain_msg = \"Hello, World!\"\n    bold_style = MessageStyle(color=None, style=(\"bold\",))\n    red_color = MessageStyle(color=\"red\", style=())\n    bold_red_style = MessageStyle(color=\"red\", style=(\"bold\",))\n\n    assert colorize_ansi(plain_msg) == plain_msg\n    assert colorize_ansi(plain_msg, bold_style) == \"\\033[1mHello, World!\\033[0m\"\n    assert colorize_ansi(plain_msg, red_color) == \"\\033[31mHello, World!\\033[0m\"\n    assert (\n        colorize_ansi(plain_msg, bold_red_style)\n        == \"\\033[1;31mHello, World!\\033[0m\"\n    )\n", "def test_colorize_ansi() -> None:\n    \"\"\"Test the colorize_ansi function with different styles and colors.\"\"\"\n    message = \"Test Message\"\n    style = MessageStyle(\"red\", (\"bold\", \"underline\"))\n    result = colorize_ansi(message, style)\n    expected_start = \"\\033[31;1;4m\"\n    expected_end = \"\\033[0m\"\n    assert result == f\"{expected_start}{message}{expected_end}\"\n\n    # Test with no styles\n    no_style = MessageStyle(None, ())\n    result = colorize_ansi(message, no_style)\n    assert result == message\n\n    # Test with deprecated parameters\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        result = colorize_ansi(message, \"blue\", \"italic\", color=\"blue\")\n        expected_start = \"\\033[34;3m\"\n        expected_end = \"\\033[0m\"\n        assert result == f\"{expected_start}{message}{expected_end}\"\n        assert len(w) == 1\n        assert issubclass(w[-1].category, DeprecationWarning)\n\n    # Test with invalid color\n    invalid_color = MessageStyle(\"invalid_color\", (\"bold\",))\n    result = colorize_ansi(message, invalid_color)\n    assert result == message\n", "def test_colorized_text_reporter() -> None:\n    \"\"\"Test the ColorizedTextReporter.\"\"\"\n    output = StringIO()\n    reporter = ColorizedTextReporter(output=output)\n\n    # Create a dummy message\n    message = Message(\n        symbol=\"line-too-long\",\n        msg_id=\"C0301\",\n        location=MessageLocationTuple(\n            abspath=\"path/to/file.py\",\n            path=\"path/to/file.py\",\n            module=\"test_module\",\n            obj=\"\",\n            line=1,\n            column=0,\n            end_lineno=None,\n            end_col_offset=None,\n        ),\n        msg=\"Line too long (80/72)\",\n        confidence=HIGH,\n    )\n\n    reporter.handle_message(message)\n    reporter._display(None)\n\n    output_value = output.getvalue().strip()\n    expected_output = (\n        \"\\033[7m************* Module test_module\\033[0m\\n\"\n        \"\\033[31;1mLine too long (80/72)\\033[0m\"\n    )\n\n    assert expected_output in output_value\n", "def test_colorized_text_reporter_output(linter: PyLinter) -> None:\n    \"\"\"Test the colorized text reporter output.\"\"\"\n    output = StringIO()\n    linter.reporter = ColorizedTextReporter(output)\n    linter.open()\n    linter.set_current_module(\"my_module\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"E1101\", line=2, args=(3, 4))\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert \"************* Module my_module\" in out_lines[0]\n    assert \"\\033[1mC0301\\033[0m\" in out_lines[1]  # Check if ANSI bold is applied\n    assert \"\\033[31mE1101\\033[0m\" in out_lines[2]  # Check if ANSI red is applied\n", "def test_colorize_ansi() -> None:\n    \"\"\"Test the colorize_ansi function.\"\"\"\n    test_msg = \"Test Message\"\n    test_style = MessageStyle(\"red\", (\"bold\", \"underline\"))\n\n    # Test with valid MessageStyle\n    result = colorize_ansi(test_msg, test_style)\n    expected = f\"{ANSI_PREFIX}31;1;4{ANSI_END}{test_msg}{ANSI_RESET}\"\n    assert result == expected\n\n    # Test with None style and color\n    result = colorize_ansi(test_msg, None)\n    assert result == test_msg\n\n    # Test with deprecated parameters\n    with warnings.catch_warnings(record=True) as cm:\n        warnings.simplefilter(\"always\")\n        result = colorize_ansi(test_msg, \"red\", \"bold\", color=\"31\")\n\n    expected = f\"{ANSI_PREFIX}31;1{ANSI_END}{test_msg}{ANSI_RESET}\"\n    assert result == expected\n    assert len(cm) == 1\n    assert issubclass(cm[0].category, DeprecationWarning)\n    assert \"In pylint 3.0, the colorize_ansi function of Text reporters will only accept a MessageStyle parameter\" in str(cm[0].message)\n"], "sample_644": ["def test_import_error(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"import_error\", REGR_DATA)\n    import_node = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"import-error\",\n        node=import_node,\n        args=\"'nonexistent_module'\",\n        confidence=UNDEFINED,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=24,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n", "def test_import_error(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"import_error\", REGR_DATA)\n        import_node = module.body[0]\n\n        msg = MessageTest(\n            msg_id=\"import-error\",\n            node=import_node,\n            args=\"import_error.module\",\n            line=1,\n            col_offset=0,\n            end_line=1,\n            end_col_offset=26,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_import(import_node)\n", "def test_shadowed_import(self) -> None:\n        module_code = \"\"\"\n        import os\n        import sys as os\n        \"\"\"\n        module = astroid.parse(module_code)\n        import_node = module.body[1]\n\n        msg = MessageTest(\n            msg_id=\"shadowed-import\",\n            node=import_node,\n            args=(\"os\", 2),\n            confidence=UNDEFINED,\n            line=2,\n            col_offset=0,\n            end_line=2,\n            end_col_offset=13,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_import(import_node)\n", "def test_import_error(self) -> None:\n    node = astroid.extract_node(\"import non_existent_module\")\n    msg = MessageTest(\n        msg_id=\"import-error\",\n        node=node,\n        args=\"'non_existent_module'\",\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=23,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(node)\n", "def test_import_self() -> None:\n    \"\"\"\n    Tests if 'import-self' message is emitted when a module imports itself.\n    \"\"\"\n    module = astroid.parse(\n        \"\"\"\n        import module\n        \"\"\",\n        module_name=\"module\"\n    )\n    import_node = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"import-self\",\n        node=import_node,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=14,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n"], "sample_645": ["def test_auto_indent_setting():\n    formatter = logging.Formatter(\n        \"%(levelname)-8s %(message)s\", datefmt=DEFAULT_LOG_DATE_FORMAT\n    )\n    formatter._style = PercentStyleMultiline(formatter._style._fmt, auto_indent=True)\n    logger = logging.getLogger(\"test_auto_indent\")\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n\n    with caplog.at_level(logging.DEBUG):\n        logger.info(\"First line\\nSecond line\\nThird line\")\n\n    assert \"First line\\n\" in caplog.text\n    assert \"Second line\\n\" in caplog.text\n    assert \"Third line\" in caplog.text\n", "def test_colored_formatter(pytester: Pytester) -> None:\n    \"\"\"Test the ColoredLevelFormatter for correct color formatting.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        from _pytest.config import create_terminal_writer\n        from _pytest.logging import ColoredLevelFormatter\n        from _pytest.terminal import TerminalReporter\n\n            logger = logging.getLogger(\"colorlogger\")\n            logger.setLevel(logging.DEBUG)\n            terminal_writer = create_terminal_writer(pytest.config)\n            formatter = ColoredLevelFormatter(terminal_writer, \"%(levelname)s: %(message)s\")\n            \n            handler = logging.StreamHandler()\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            \n            with pytest.raises(SystemExit):\n                with pytest.raises(AttributeError):\n                    logger.debug(\"Debug message\")\n                    logger.info(\"Info message\")\n                    logger.warning(\"Warning message\")\n                    logger.error(\"Error message\")\n                    logger.critical(\"Critical message\")\n\n            # This will check the colored output in the terminal reporter's buffer\n            terminal_reporter = TerminalReporter(pytest.config)\n            assert \"Debug message\" in terminal_reporter._tw.getvalue()\n            assert \"Info message\" in terminal_reporter._tw.getvalue()\n            assert \"Warning message\" in terminal_reporter._tw.getvalue()\n            assert \"Error message\" in terminal_reporter._tw.getvalue()\n            assert \"Critical message\" in terminal_reporter._tw.getvalue()\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_colored_level_formatter(pytester: Pytester) -> None:\n    \"\"\"Test the ColoredLevelFormatter with different log levels.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        from _pytest.config import Config\n        from _pytest.config import create_terminal_writer\n        from _pytest.logging import ColoredLevelFormatter\n\n            config = Config.fromdictargs({})\n            terminal_writer = create_terminal_writer(config)\n            formatter = ColoredLevelFormatter(terminal_writer, \"%(levelname)-8s %(message)s\")\n            \n            handler = logging.StreamHandler()\n            handler.setFormatter(formatter)\n            logger = logging.getLogger()\n            logger.addHandler(handler)\n            logger.setLevel(logging.DEBUG)\n            \n            caplog.set_level(logging.DEBUG)\n            \n            logger.debug(\"Debug message\")\n            logger.info(\"Info message\")\n            logger.warning(\"Warning message\")\n            logger.error(\"Error message\")\n            logger.critical(\"Critical message\")\n            \n            log_output = caplog.text\n            assert \"DEBUG    Debug message\" in log_output\n            assert \"INFO     Info message\" in log_output\n            assert \"WARNING  Warning message\" in log_output\n            assert \"ERROR    Error message\" in log_output\n            assert \"CRITICAL Critical message\" in log_output\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Test the auto-indent feature of the log formatter.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n            caplog.set_level(logging.INFO)\n            # Multi-line log message\n            logger.info(\"First line\\\\nSecond line\\\\nThird line\")\n            assert \"First line\" in caplog.text\n            assert \"Second line\" in caplog.text\n            assert \"Third line\" in caplog.text\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_auto_indent=True\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_add_color_level():\n    tw = TerminalWriter()\n    formatter = ColoredLevelFormatter(tw, DEFAULT_LOG_FORMAT)\n\n    formatter.add_color_level(logging.INFO, \"green\", \"bold\")\n    fmt = formatter._level_to_fmt_mapping[logging.INFO]\n\n    log_record = logging.LogRecord(\"test\", logging.INFO, \"test\", 10, \"info message\", None, None)\n    formatted_message = formatter.format(log_record)\n\n    assert \"info message\" in formatted_message\n    assert \"INFO\" in formatted_message  # Checking if level name is present\n"], "sample_646": ["def test_unittest_skip_with_reason(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @unittest.skip(\"skipped for a reason\")\n                pass\n\n                self.assertEqual(1, 1)\n        \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(skipped=1, passed=1)\n", "def test_error_in_setup_method(pytester: Pytester) -> None:\n    \"\"\"Test that an error in setup_method correctly fails the test and runs cleanup.\"\"\"\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n\n                self.values.append('setup')\n\n                self.values.append('setup_method')\n                assert False, \"Intentional setup_method failure\"\n\n                self.values.append('teardown_method')\n\n                pass\n\n            assert MyTestCase.values == ['setup', 'setup_method', 'teardown_method']\n        \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(failed=1)\n    reprec.assertoutcome(passed=1)\n", "def test_setup_teardown_fixtures_injection(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @classmethod\n                cls.class_var = 1\n\n                self.instance_var = 2\n\n                self.assertEqual(self.class_var, 1)\n\n                self.assertEqual(self.instance_var, 2)\n\n            @classmethod\n                cls.class_var = 0\n\n                self.instance_var = 0\n\n            assert MyTestCase.class_var == 0\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(passed=3)\n", "def test_unittest_expected_failure_for_raising_test(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @unittest.expectedFailure\n                raise ValueError(\"This is a raised exception\")\n\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(xfailed=1)\n", "def test_expected_failure_handling(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @unittest.expectedFailure\n                self.assertEqual(1, 2)\n\n            @unittest.expectedFailure\n                self.assertEqual(1, 1)\n\n                self.assertEqual(2, 2)\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 1\n    assert skipped == 1\n    reprec.assertoutcome(xfailed=1)\n"], "sample_647": ["def test_unformatted_warning(template: str, kwargs: dict, expected_message: str) -> None:\n    \"\"\"Test UnformattedWarning to ensure it formats messages correctly.\"\"\"\n    warning = UnformattedWarning(category=PytestDeprecationWarning, template=template)\n    formatted_warning = warning.format(**kwargs)\n    assert isinstance(formatted_warning, PytestDeprecationWarning)\n    assert str(formatted_warning) == expected_message\n", "def test_unformatted_warning_format() -> None:\n    \"\"\"Test the UnformattedWarning class format method.\"\"\"\n    warning = UnformattedWarning(\n        category=PytestCacheWarning,\n        template=\"Cache warning for {item}\",\n    )\n    formatted_warning = warning.format(item=\"test_item\")\n    assert isinstance(formatted_warning, PytestCacheWarning)\n    assert str(formatted_warning) == \"Cache warning for test_item\"\n", "def test_unformatted_warning_format() -> None:\n    \"\"\"Test the format method of UnformattedWarning class.\"\"\"\n    warning = UnformattedWarning(PytestWarning, \"This is a {adjective} warning\")\n    formatted_warning = warning.format(adjective=\"test\")\n    assert isinstance(formatted_warning, PytestWarning)\n    assert str(formatted_warning) == \"This is a test warning\"\n", "def test_unformatted_warning_format() -> None:\n    \"\"\"Test the UnformattedWarning class to ensure formatting works as expected.\"\"\"\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestCacheWarning,\n        template=\"Cache warning: {reason}\"\n    )\n    formatted_warning = warning.format(reason=\"cache invalidated\")\n    assert isinstance(formatted_warning, warning_types.PytestCacheWarning)\n    assert str(formatted_warning) == \"Cache warning: cache invalidated\"\n", "def test_unformatted_warning_format() -> None:\n    \"\"\"Test the UnformattedWarning class and its format method.\"\"\"\n    warning_template = \"This is a test warning with value {value}\"\n    unformatted_warning = warning_types.UnformattedWarning(warning_types.PytestWarning, warning_template)\n    formatted_warning = unformatted_warning.format(value=42)\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"This is a test warning with value 42\"\n"], "sample_648": ["def test_mark_decorator_combined_with() -> None:\n    mark1 = pytest.mark.foo\n    mark2 = pytest.mark.bar\n    combined_mark = mark1.with_args().combined_with(mark2.with_args())\n    \n    assert combined_mark.name == \"foo\"\n    assert \"bar\" in combined_mark.args[0]\n    \n    with pytest.raises(AssertionError):\n        mark1.with_args().combined_with(pytest.mark.baz.with_args())\n", "def test_mark_combined_with():\n    mark1 = pytest.mark.foo(arg1=\"val1\")\n    mark2 = pytest.mark.foo(arg2=\"val2\")\n    combined = mark1.mark.combined_with(mark2)\n\n    assert combined.name == \"foo\"\n    assert combined.kwargs[\"arg1\"] == \"val1\"\n    assert combined.kwargs[\"arg2\"] == \"val2\"\n    assert combined.args == ()\n", "def test_mark_decorator_combined_with() -> None:\n    mark1 = pytest.mark.custom1\n    mark2 = pytest.mark.custom2\n    combined_mark = mark1.with_args().combined_with(mark2.with_args())\n    \n    assert combined_mark.name == \"custom1\"\n    assert combined_mark.args == ()\n    assert combined_mark.kwargs == {}\n    assert combined_mark._param_ids_from is None\n    assert combined_mark._param_ids_generated is None\n\n    combined_mark_with_args = mark1.with_args(1, 2).combined_with(mark2.with_args(3, 4, key=\"value\"))\n    \n    assert combined_mark_with_args.name == \"custom1\"\n    assert combined_mark_with_args.args == (1, 2, 3, 4)\n    assert combined_mark_with_args.kwargs == {\"key\": \"value\"}\n    assert combined_mark_with_args._param_ids_from is None\n    assert combined_mark_with_args._param_ids_generated is None\n", "def test_mark_decorator_with_args() -> None:\n    mark = pytest.mark.foo(\"bar\")\n    assert mark.name == \"foo\"\n    assert mark.args == (\"bar\",)\n    assert mark.kwargs == {}\n\n    mark_with_args = mark.with_args(1, kwarg=\"value\")\n    assert mark_with_args.name == \"foo\"\n    assert mark_with_args.args == (\"bar\", 1)\n    assert mark_with_args.kwargs == {\"kwarg\": \"value\"}\n", "def test_mark_with_args(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.some_mark.with_args(\"arg1\", \"arg2\")\n            assert True\n    \"\"\"\n    )\n    rec = pytester.inline_run()\n    rec.assertoutcome(passed=1)\n    items = rec.getcalls(\"pytest_runtest_protocol\")\n    assert len(items) == 1\n    test_item = items[0].item\n    assert any(mark.name == \"some_mark\" for mark in test_item.iter_markers())\n    assert any(mark.args == (\"arg1\", \"arg2\") for mark in test_item.iter_markers())\n"], "sample_649": ["def test_log_capture_handler_reset_and_clear(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info(\"Initial log message\")\n            assert len(caplog.records) == 1\n            assert caplog.text == \"INFO    __main__:test_log_capture_handler_reset_and_clear.py:7 Initial log message\\\\n\"\n            \n            caplog.handler.reset()\n            assert len(caplog.records) == 0\n            assert caplog.text == \"\"\n\n            logger.info(\"Log after reset\")\n            assert len(caplog.records) == 1\n            assert caplog.text == \"INFO    __main__:test_log_capture_handler_reset_and_clear.py:13 Log after reset\\\\n\"\n            \n            caplog.handler.clear()\n            assert len(caplog.records) == 0\n            assert caplog.text == \"\"\n\n            logger.info(\"Log after clear\")\n            assert len(caplog.records) == 1\n            assert caplog.text == \"INFO    __main__:test_log_capture_handler_reset_and_clear.py:19 Log after clear\\\\n\"\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n", "def test_percent_style_multiline(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('First line\\\\nSecond line\\\\nThird line')\n            assert len(caplog.records) == 1\n            log_record = caplog.records[0]\n            formatted_message = caplog.handler.format(log_record)\n            assert 'First line\\\\n' in formatted_message\n            assert 'Second line' in formatted_message\n            assert 'Third line' in formatted_message\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n", "def test_multiline_log_messages(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.info('First line\\\\nSecond line\\\\nThird line')\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured *log call -*\",\n            \"*First line\",\n            \" Second line\",\n            \" Third line*\",\n        ]\n    )\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('This is a log message\\\\nthat spans multiple lines')\n            assert True\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\", \"--log-auto-indent=2\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO    *This is a log message\",\n            \"  that spans multiple lines\",\n        ]\n    )\n", "def test_log_format_custom(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('text with custom format')\n    \"\"\"\n    )\n    result = pytester.runpytest(\n        \"--log-level=INFO\", \"--log-format=%(asctime)s %(name)s %(levelname)s: %(message)s\"\n    )\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO    text with custom format\",\n        ]\n    )\n"], "sample_650": ["def test_log_file_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Check that log_auto_indent affects output correctly.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('first line\\\\nsecond line\\\\nthird line')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(asctime)s; %(levelname)s; %(message)s\n        log_date_format=%Y-%m-%d %H:%M:%S\n        log_auto_indent=true\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.re_match_lines(\n        [\n            r\"^[0-9-]{10} [0-9:]{8}; WARNING; first line\",\n            r\" {30}second line\",\n            r\" {30}third line\",\n        ]\n    )\n", "def test_percentstyle_multiline_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Test the PercentStyleMultiline formatter with auto-indent enabled.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n        @pytest.fixture\n            handler = logging.StreamHandler()\n            formatter = logging.Formatter('%(levelname)s: %(message)s')\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            return handler\n\n            logger.setLevel(logging.INFO)\n            caplog.set_level(logging.INFO)\n            logger.info(\"This is a single line log message.\")\n            logger.info(\"This is a multi-line log message.\\\\nIt should be indented properly.\")\n            captured_log = caplog.text\n            assert \"This is a single line log message.\" in captured_log\n            assert \"This is a multi-line log message.\" in captured_log\n            assert \"    It should be indented properly.\" in captured_log  # Check for indentation\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-format=%(levelname)s: %(message)s\", \"--log-auto-indent=True\")\n    assert result.ret == 0\n", "def test_colored_log_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Test that auto-indent works correctly with colored log output.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.info('text going to logger\\\\nwith multiple lines\\\\nand indentation')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(levelname)s %(message)s\n        log_cli=true\n        log_auto_indent=true\n        log_cli_level=INFO\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--color=yes\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*INFO text going to logger*\",\n            \"           with multiple lines\",\n            \"           and indentation\",\n        ]\n    )\n    assert '\\x1b[' in result.stdout.str()  # Ensure color codes are present\n", "def test_log_capture_fixture_basic_usage(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger(\"testlogger\")\n            with caplog.at_level(logging.INFO):\n                logger.info(\"info message\")\n                logger.error(\"error message\")\n            assert len(caplog.records) == 2\n            assert caplog.records[0].message == \"info message\"\n            assert caplog.records[1].message == \"error message\"\n            assert caplog.record_tuples == [(\"testlogger\", logging.INFO, \"info message\"),\n                                            (\"testlogger\", logging.ERROR, \"error message\")]\n            caplog.clear()\n            assert len(caplog.records) == 0\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Test that auto-indent works correctly for multiline log messages.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('First line\\\\nSecond line\\\\nThird line')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_auto_indent=true\n        log_level=WARNING\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*WARNING  test_log_auto_indent:test_log_auto_indent.py:6 First line\",\n            \"                Second line\",\n            \"                Third line\",\n        ]\n    )\n"], "sample_652": ["def test_pytest_addoption():\n    from _pytest.config import Config\n    from _pytest.config.argparsing import Parser\n\n    parser = Parser()\n    config = Config.fromdictargs({}, {})\n    pytest_addoption(parser)\n    options = parser.parse([\"--fixtures\", \"--fixtures-per-test\", \"--import-mode=append\"])\n    assert options.showfixtures is True\n    assert options.show_fixtures_per_test is True\n    assert options.importmode == \"append\"\n", "def test_pyobj_property():\n    class DummyParent:\n            self.obj = obj\n\n            if name == \"Module\":\n                return DummyParent(\"module_obj\")\n            elif name == \"Class\":\n                return DummyParent(\"class_obj\")\n            return None\n\n    class DummyNode:\n        parent = DummyParent(\"parent_obj\")\n        module = pyobj_property(\"Module\")\n        cls = pyobj_property(\"Class\")\n        instance = pyobj_property(\"Instance\")\n\n    node = DummyNode()\n    assert node.module == \"module_obj\"\n    assert node.cls == \"class_obj\"\n    assert node.instance is None\n", "def test_pyobj_property():\n    import pytest\n\n    class DummyNode:\n            self.name = name\n            self.obj = self\n\n            if cls == DummyNode:\n                return self\n            return None\n\n    class DummyClass:\n            self.getparent = DummyNode(\"test\")\n\n    node = DummyNode(\"test\")\n    assert pyobj_property(\"DummyNode\").__get__(node) == node\n\n    dummy_instance = DummyClass()\n    assert pyobj_property(\"DummyNode\").__get__(dummy_instance) == node\n", "def test_pyobj_property():\n    class MockNode:\n            self.obj = obj\n\n            return self.obj if isinstance(self.obj, cls) else None\n\n    class MockClass:\n        pass\n\n    node = MockNode(MockClass())\n    prop = pyobj_property(\"MockClass\")\n\n    class MockCollector:\n            self.node = node\n\n            return self.node.getparent(cls)\n\n            return getattr(self.node, name)\n\n    collector = MockCollector(node)\n    assert prop.fget(collector) == node.obj\n\n    node = MockNode(None)\n    collector = MockCollector(node)\n    assert prop.fget(collector) is None\n", "def test_pyobj_property():\n    class Dummy:\n            if cls == pytest.Module:\n                return DummyModule()\n            if cls == pytest.Class:\n                return DummyClass()\n            return None\n\n    class DummyModule:\n        obj = \"module_obj\"\n\n    class DummyClass:\n        obj = \"class_obj\"\n\n    module_property = pyobj_property(\"Module\")\n    class_property = pyobj_property(\"Class\")\n\n    dummy_instance = Dummy()\n\n    assert module_property.__get__(dummy_instance) == \"module_obj\"\n    assert class_property.__get__(dummy_instance) == \"class_obj\"\n"], "sample_655": ["def test_capture_io_write_bytes_and_str():\n    f = capture.CaptureIO()\n    if _PY3:\n        f.write(b\"hello\")\n        assert f.getvalue() == \"hello\"\n        f.write(\" world\")\n        assert f.getvalue() == \"hello world\"\n    else:\n        f.write(\"hello\")\n        assert f.getvalue() == \"hello\"\n        f.write(u\" world\")\n        assert f.getvalue() == \"hello world\"\n    f.close()\n", "def test_no_capture_method():\n    capman = CaptureManager(\"no\")\n    assert capman.is_globally_capturing() is False\n    capman.start_global_capturing()\n    assert capman.is_globally_capturing() is False\n    capman.stop_global_capturing()\n", "def test_capture_fixture_with_logging_and_exceptions(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n        @pytest.fixture\n            print(\"fixture setup\")\n            logging.info(\"fixture setup\")\n            raise RuntimeError(\"something went wrong\")\n            yield\n            logging.info(\"fixture teardown\")\n            print(\"fixture teardown\")\n\n            print(\"test body\")\n            logging.info(\"test body\")\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*ERROR at setup of test_capture_with_exception*\",\n            \"*RuntimeError: something went wrong*\",\n            \"*Captured stdout during setup*\",\n            \"fixture setup*\",\n            \"*Captured log call during setup*\",\n            \"*INFO*fixture setup*\",\n        ]\n    )\n    assert result.ret != 0\n", "def test_capture_global_and_fixture_disabled(capsys):\n    capmanager = CaptureManager(\"sys\")\n    capmanager.start_global_capturing()\n    print(\"global capture on\")\n    with capmanager.global_and_fixture_disabled():\n        print(\"inside disabled capture\")\n    captured = capsys.readouterr()\n    assert captured.out == \"inside disabled capture\\n\"\n    capmanager.stop_global_capturing()\n    captured = capsys.readouterr()\n    assert \"global capture on\" in captured.out\n", "def test_capture_no_method_error():\n    with pytest.raises(ValueError, match=\"unknown capturing method: 'invalid'\"):\n        CaptureManager(\"invalid\")\n"], "sample_656": ["def test_fixture_captures_stdout_stderr(testdir):\n    # Test to ensure that fixtures like capsys and capfd capture stdout and stderr correctly\n    testdir.makepyfile(\n        \"\"\"\n            print(\"stdout from capsys\")\n            sys.stderr.write(\"stderr from capsys\\\\n\")\n            out, err = capsys.readouterr()\n            assert out == \"stdout from capsys\\\\n\"\n            assert err == \"stderr from capsys\\\\n\"\n\n            print(\"stdout from capfd\")\n            sys.stderr.write(\"stderr from capfd\\\\n\")\n            out, err = capfd.readouterr()\n            assert out == \"stdout from capfd\\\\n\"\n            assert err == \"stderr from capfd\\\\n\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*2 passed*\"])\n", "def test_capture_fixture_disabled_logging(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            print(\"captured before\")\n            logging.info(\"logging before\")\n            with capsys.disabled():\n                print(\"while capture is disabled\")\n                logging.info(\"while logging is disabled\")\n            print(\"captured after\")\n            logging.info(\"logging after\")\n            out, err = capsys.readouterr()\n            assert out == \"captured before\\\\ncaptured after\\\\n\"\n            assert \"while capture is disabled\" not in out\n            assert \"logging before\" not in err\n            assert \"while logging is disabled\" not in err\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\n        \"*while capture is disabled*\",\n        \"*while logging is disabled*\",\n        \"*1 passed*\",\n    ])\n", "def test_capture_manager_repr():\n    capman = CaptureManager(\"fd\")\n    repr_string = repr(capman)\n    assert \"CaptureManager\" in repr_string\n    assert \"_method='fd'\" in repr_string\n    assert \"_global_capturing=None\" in repr_string\n    assert \"_current_item=None\" in repr_string\n", "def test_capture_with_multiple_fixtures(testdir):\n    # Test to ensure capturing works correctly with multiple fixtures and order of execution\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            print(\"setup fixture_a\")\n            yield\n            print(\"teardown fixture_a\")\n\n        @pytest.fixture\n            print(\"setup fixture_b\")\n            yield\n            print(\"teardown fixture_b\")\n\n            print(\"in test\")\n            out, err = capsys.readouterr()\n            assert \"setup fixture_a\" in out\n            assert \"setup fixture_b\" in out\n            assert \"in test\" in out\n\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*setup fixture_a*\",\n            \"*setup fixture_b*\",\n            \"*in test*\",\n            \"*teardown fixture_b*\",\n            \"*teardown fixture_a*\",\n            \"*1 passed*\",\n        ]\n    )\n", "def test_capture_sys_stdout_binary():\n    cap = capture.SysCaptureBinary(1)\n    cap.start()\n    sys.stdout.buffer.write(b'binary data')\n    out = cap.snap()\n    cap.done()\n    assert out == b'binary data'\n"], "sample_657": ["def test_mark_decorator_with_args():\n    \"\"\"Test MarkDecorator with additional args and kwargs.\"\"\"\n    mark = pytest.mark.example\n    mark_with_args = mark.with_args(1, 2, key=\"value\")\n    assert mark_with_args.args == (1, 2)\n    assert mark_with_args.kwargs == {\"key\": \"value\"}\n\n        pass\n\n    decorated_func = mark_with_args(dummy_func)\n    assert hasattr(decorated_func, \"pytestmark\")\n    assert decorated_func.pytestmark[0].args == (1, 2)\n    assert decorated_func.pytestmark[0].kwargs == {\"key\": \"value\"}\n", "def test_mark_decorator_with_args():\n    mark = pytest.mark.custom(\"arg1\", \"arg2\", kwarg1=\"value1\", kwarg2=\"value2\")\n\n    @mark\n        pass\n\n    retrieved_mark = some_function.pytestmark[0]\n    assert isinstance(retrieved_mark, Mark)\n    assert retrieved_mark.name == \"custom\"\n    assert retrieved_mark.args == (\"arg1\", \"arg2\")\n    assert retrieved_mark.kwargs == {\"kwarg1\": \"value1\", \"kwarg2\": \"value2\"}\n", "def test_markdecorator_with_args():\n    \"\"\"Test the with_args method of MarkDecorator class.\"\"\"\n    mark = pytest.mark.example\n    new_mark = mark.with_args(1, 2, key=\"value\")\n    assert isinstance(new_mark, MarkDecorator)\n    assert new_mark.args == (1, 2)\n    assert new_mark.kwargs == {\"key\": \"value\"}\n", "def test_mark_combined_with():\n    mark1 = pytest.mark.some_marker\n    mark2 = pytest.mark.some_marker\n    combined_mark = mark1.combined_with(mark2)\n    assert combined_mark.name == \"some_marker\"\n    assert combined_mark.args == ()\n    assert combined_mark.kwargs == {}\n", "def test_mark_decorator_with_args():\n    \"\"\"Test that MarkDecorator with additional arguments works correctly.\"\"\"\n\n    @pytest.mark.some_mark(arg1=\"value1\")\n        pass\n\n    mark = test_func.pytestmark[0]\n    assert mark.name == \"some_mark\"\n    assert mark.kwargs == {\"arg1\": \"value1\"}\n"], "sample_658": ["    def test_doctestmodule_import_error(self, testdir):\n        \"\"\"Test that importing a module with errors raises the appropriate outcome.\"\"\"\n        p = testdir.makepyfile(\n            faulty_module=\"\"\"\n            import non_existent_module\n            \"\"\"\n        )\n        result = testdir.runpytest(p, \"--doctest-modules\")\n        result.stdout.fnmatch_lines(\n            [\"*ERROR collecting faulty_module.py*\",\n             \"*ModuleNotFoundError: No module named 'non_existent_module'*\"]\n        )\n        result.assert_outcomes(errors=1)\n", "def test_is_mocked(self):\n    \"\"\"Test if _is_mocked correctly identifies a mock object.\"\"\"\n    class MockedObject:\n        pytest_mock_example_attribute_that_shouldnt_exist = True\n\n    class NonMockedObject:\n        pass\n\n    assert _is_mocked(MockedObject())\n    assert not _is_mocked(NonMockedObject())\n", "    def test_doctestmodule_import_error(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> import nonexistent_module\n                '''\n        \"\"\"\n        )\n        reprec = testdir.inline_run(p, \"--doctest-modules\")\n        reprec.assertoutcome(failed=1)\n", "    def test_doctestmodule_import_error_handling(self, testdir):\n        \"\"\"\n        Test to ensure that import errors are correctly handled and reported\n        when running doctests in modules.\n        \"\"\"\n        testdir.makepyfile(\n            mod_with_error=\"\"\"\n            import non_existent_module\n\n                '''\n                >>> foo()\n                'bar'\n                '''\n                return 'bar'\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines(\n            [\"*ERROR collecting mod_with_error.py*\", \"*ModuleNotFoundError*\"]\n        )\n", "    def test_doctest_failure_multiple_failures(self, testdir):\n        p = testdir.maketxtfile(\n            \"\"\"\n            >>> i = 0\n            >>> i + 1\n            2\n            >>> i + 2\n            3\n        \"\"\"\n        )\n        result = testdir.runpytest(p, \"--doctest-modules\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_doctest_failure_multiple_failures.txt:2: DocTestFailure*\",\n                \"*Expected:\",\n                \"*    2\",\n                \"*Got:\",\n                \"*    1\",\n                \"*test_doctest_failure_multiple_failures.txt:4: DocTestFailure*\",\n                \"*Expected:\",\n                \"*    3\",\n                \"*Got:\",\n                \"*    2\",\n            ]\n        )\n"], "sample_659": ["def test_traceback_filtering():\n    import _pytest._code\n    import traceback\n\n        raise ValueError(\"test error\")\n\n    try:\n        func_that_raises()\n    except ValueError:\n        exc_info = sys.exc_info()\n        tb = _pytest._code.Traceback(exc_info[2])\n        assert len(tb) > 0\n        filtered_tb = tb.filter(filter_traceback)\n        assert len(filtered_tb) == 1\n        assert isinstance(filtered_tb[0], _pytest._code.TracebackEntry)\n        assert filtered_tb[0].frame.code.name == \"func_that_raises\"\n        assert \"test error\" in str(filtered_tb[-1])\n", "    def test_frame_eval(self):\n            x = 10\n            y = 20\n            return x + y\n\n        frame = None\n        try:\n            dummy_func()\n        except:\n            frame = sys.exc_info()[2].tb_frame\n\n        code = Code(dummy_func.__code__)\n        frame = Frame(frame)\n\n        result = frame.eval(\"x + y\", x=5, y=15)\n        assert result == 20\n\n        result = frame.eval(\"x + y\", x=10, y=20)\n        assert result == 30\n", "    def test_traceback_cut(self):\n            func2()\n\n            func3()\n\n            raise ValueError(\"Test error\")\n\n        try:\n            func1()\n        except ValueError:\n            excinfo = pytest.ExceptionInfo.from_current()\n\n        tb = excinfo.traceback\n        assert len(tb) == 3\n\n        # Test cutting traceback to show only part of it\n        tb_cut = tb.cut(path=tb[1].path, lineno=tb[1].lineno)\n        assert len(tb_cut) == 2\n        assert tb_cut[0].lineno == tb[1].lineno\n        assert tb_cut[1].lineno == tb[2].lineno\n", "    def test_traceback_entry_hidden(self):\n        class HiddenFrame:\n                self.f_lineno = lineno\n                self.f_globals = f_globals\n                self.f_locals = f_locals\n                self.f_code = HiddenCode()\n\n        class HiddenCode:\n            co_filename = \"hidden.py\"\n            co_name = \"hidden\"\n\n        raw_entry = HiddenFrame(\n            lineno=10,\n            f_globals={\"__tracebackhide__\": lambda: True},\n            f_locals={\"var\": \"value\"},\n        )\n        entry = TracebackEntry(rawentry=raw_entry)\n        assert entry.ishidden()\n", "    def test_code_getargs_var(self):\n            pass\n\n        code = Code(sample_func.__code__)\n        assert code.getargs(var=True) == ('a', 'b', 'args', 'kwargs')\n        assert code.getargs(var=False) == ('a', 'b')\n"], "sample_660": ["def test_record_property_special_characters(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_property(\"key1\", \"value<>&\\\"'\")\n            record_property(\"key2\", \"another&value\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-rw\")\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    psnode = tnode.find_first_by_tag(\"properties\")\n    pnodes = psnode.find_by_tag(\"property\")\n    pnodes[0].assert_attr(name=\"key1\", value=\"value&lt;&gt;&amp;&quot;'\")\n    pnodes[1].assert_attr(name=\"key2\", value=\"another&amp;value\")\n", "def test_invalid_xml_escape_range(testdir):\n    # Test more invalid xml chars in a specific range\n    invalid = (\n        0x1F, 0x7F, 0x80, 0x9F, 0xFDD0, 0xFDEF\n    )\n\n    from _pytest.junitxml import bin_xml_escape\n\n    for i in invalid:\n        got = bin_xml_escape(chr(i)).uniobj\n        if i <= 0xFF:\n            expected = \"#x%02X\" % i\n        else:\n            expected = \"#x%04X\" % i\n        assert got == expected\n", "def test_bin_xml_escape():\n    from _pytest.junitxml import bin_xml_escape\n\n    # Test escaping of invalid XML characters\n    input_str = \"hello\\x00world\\x1B!\"\n    escaped_str = bin_xml_escape(input_str).uniobj\n    expected_str = \"hello#x00world#x1B!\"\n    assert escaped_str == expected_str\n\n    # Test escaping of valid XML characters\n    input_str = \"valid\\x09chars\\x0A!\"\n    escaped_str = bin_xml_escape(input_str).uniobj\n    expected_str = \"valid\\x09chars\\x0A!\"\n    assert escaped_str == expected_str\n", "def test_bin_xml_escape():\n    from _pytest.junitxml import bin_xml_escape\n\n    # Test escaping of valid and invalid XML characters\n    valid_input = \"Hello World!\"\n    invalid_input = \"Hello\\x00World\\x01\"\n\n    # Valid input should be escaped without changes\n    valid_output = bin_xml_escape(valid_input)\n    assert valid_output == valid_input\n\n    # Invalid input should have invalid characters replaced\n    invalid_output = bin_xml_escape(invalid_input)\n    assert invalid_output == \"Hello#x00World#x01\"\n", "def test_invalid_character_escape(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            with open(\"output.txt\", \"w\") as f:\n                f.write(chr(0x00))\n            with open(\"output.txt\", \"r\") as f:\n                content = f.read()\n            assert content == chr(0x00)\n        \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 1  # Test should fail due to invalid XML character\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(failures=1, tests=1)\n    tnode = node.find_first_by_tag(\"testcase\")\n    fnode = tnode.find_first_by_tag(\"failure\")\n    assert \"#x00\" in fnode.toxml()  # Check that the character is properly escaped\n"], "sample_661": ["def test_node_reporter_add_property(testdir):\n    from _pytest.junitxml import LogXML, _NodeReporter\n\n    logxml = LogXML(\"test.xml\", None)\n    node_reporter = _NodeReporter(\"nodeid\", logxml)\n    \n    node_reporter.add_property(\"foo\", \"bar\")\n    assert node_reporter.properties == [(\"foo\", \"bar\")]\n    \n    node = node_reporter.make_properties_node()\n    assert node.children[0].attrs == {\"name\": \"foo\", \"value\": \"bar\"}\n", "def test_bin_xml_escape():\n    from _pytest.junitxml import bin_xml_escape\n\n    assert bin_xml_escape(\"simple text\").uniobj == \"simple text\"\n    assert bin_xml_escape(\"text with < & >\").uniobj == \"text with #x3C #x26 #x3E\"\n    assert bin_xml_escape(\"newline\\nand tab\\t\").uniobj == \"newline#x0Aand tab#x09\"\n    assert bin_xml_escape(\"invalid \\x01 character\").uniobj == \"invalid #x01 character\"\n", "def test_add_stats_function():\n    log = LogXML(\"/dev/null\", None)\n\n    # Initial stats should all be zero\n    assert log.stats == {\"error\": 0, \"passed\": 0, \"failure\": 0, \"skipped\": 0}\n\n    # Add stats for different cases\n    log.add_stats(\"passed\")\n    assert log.stats[\"passed\"] == 1\n\n    log.add_stats(\"failure\")\n    assert log.stats[\"failure\"] == 1\n\n    log.add_stats(\"skipped\")\n    assert log.stats[\"skipped\"] == 1\n\n    log.add_stats(\"error\")\n    assert log.stats[\"error\"] == 1\n\n    # Add stats for passed again to ensure it increments\n    log.add_stats(\"passed\")\n    assert log.stats[\"passed\"] == 2\n", "def test_record_xml_attribute_with_xunit2(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_family = xunit2\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            record_xml_attribute(\"custom_attr\", \"custom_value\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    tnode.assert_attr(name=\"test_record\")\n    assert \"custom_attr\" not in tnode.toxml()\n    result.stdout.fnmatch_lines(\n        [\"*test_record_xml_attribute_with_xunit2.py:6:*record_xml_attribute is an experimental feature\"]\n    )\n", "def test_record_testsuite_property_with_special_characters(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"some <special> key\", \"value with & special chars\")\n\n            record_testsuite_property(\"another <key>\", \"another value\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p2_node = properties_node.find_nth_by_tag(\"property\", 1)\n    p3_node = properties_node.find_nth_by_tag(\"property\", 2)\n    p1_node.assert_attr(name=\"some <special> key\", value=\"value with & special chars\")\n    p2_node.assert_attr(name=\"another <key>\", value=\"another value\")\n"], "sample_662": ["def test_collectreport_location_property(self, testdir):\n    \"\"\"\n    Test the 'location' property of CollectReport.\n    \"\"\"\n    reprec = testdir.inline_runsource(\"def test_func(): pass\")\n    reports = reprec.getreports(\"pytest_collectreport\")\n    for rep in reports:\n        d = rep._to_json()\n        newrep = CollectReport._from_json(d)\n        assert newrep.location == (newrep.fspath, None, newrep.fspath)\n", "    def test_collectreport_custom_attributes(self, testdir):\n        \"\"\"This test verifies that custom attributes in CollectReport are serialized and deserialized correctly.\"\"\"\n        reprec = testdir.inline_runsource(\"def test_func(): pass\")\n        reports = reprec.getreports(\"pytest_collectreport\")\n        for rep in reports:\n            # Add custom attributes to the report\n            rep.custom_str = \"custom_value\"\n            rep.custom_int = 123\n            rep.custom_list = [1, 2, 3]\n            rep.custom_dict = {\"key\": \"value\"}\n\n            d = rep._to_json()\n            newrep = CollectReport._from_json(d)\n\n            # Verify that custom attributes are preserved\n            assert newrep.custom_str == rep.custom_str\n            assert newrep.custom_int == rep.custom_int\n            assert newrep.custom_list == rep.custom_list\n            assert newrep.custom_dict == rep.custom_dict\n\n            assert newrep.passed == rep.passed\n            assert newrep.failed == rep.failed\n            assert newrep.skipped == rep.skipped\n", "def test_report_with_custom_user_properties(self, testdir):\n    \"\"\"Test serialization and deserialization of reports with custom user properties.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 'custom_value'\n\n            assert custom_fixture == 'custom_value'\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    test_call_report = reports[1]\n    test_call_report.user_properties.append((\"custom_key\", \"custom_value\"))\n    data = test_call_report._to_json()\n    loaded_report = TestReport._from_json(data)\n    assert loaded_report.user_properties == [(\"custom_key\", \"custom_value\")]\n", "    def test_nodeid_serialization(self, testdir):\n        \"\"\"Test the serialization and deserialization of TestReport's nodeid attribute.\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 6\n        for rep in reports:\n            d = rep._to_json()\n            newrep = TestReport._from_json(d)\n            assert newrep.nodeid == rep.nodeid\n            assert newrep.passed == rep.passed\n            assert newrep.failed == rep.failed\n            assert newrep.skipped == rep.skipped\n            assert newrep.outcome == rep.outcome\n            assert newrep.when == rep.when\n            assert newrep.keywords == rep.keywords\n", "def test_base_report_properties():\n    \"\"\"Test the basic properties and methods of BaseReport.\"\"\"\n    report = BaseReport(\n        nodeid=\"test_node\",\n        longrepr=\"This is a long representation\",\n        outcome=\"passed\",\n        sections=[(\"Captured stdout\", \"stdout content\"), (\"Captured stderr\", \"stderr content\"), (\"Captured log\", \"log content\")]\n    )\n\n    assert report.nodeid == \"test_node\"\n    assert report.longrepr == \"This is a long representation\"\n    assert report.outcome == \"passed\"\n    \n    # Test the capstdout property\n    assert report.capstdout == \"stdout content\"\n    \n    # Test the capstderr property\n    assert report.capstderr == \"stderr content\"\n    \n    # Test the caplog property\n    assert report.caplog == \"log content\"\n\n    # Test the passed, failed and skipped properties\n    assert report.passed\n    assert not report.failed\n    assert not report.skipped\n\n    # Test the fspath property\n    report.nodeid = \"somepath/test_file.py::test_func\"\n    assert report.fspath == \"somepath/test_file.py\"\n\n    # Test head_line property\n    report.location = (\"somepath/test_file.py\", 10, \"test_func\")\n    assert report.head_line == \"test_func\"\n    \n    # Test count_towards_summary property\n    assert report.count_towards_summary\n"], "sample_663": ["def test_exit_code_enum():\n    \"\"\"Test to ensure that ExitCode enum values are correctly assigned.\"\"\"\n    assert ExitCode.OK == 0\n    assert ExitCode.TESTS_FAILED == 1\n    assert ExitCode.INTERRUPTED == 2\n    assert ExitCode.INTERNAL_ERROR == 3\n    assert ExitCode.USAGE_ERROR == 4\n    assert ExitCode.NO_TESTS_COLLECTED == 5\n", "def test_wrap_session_UsageError(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        from _pytest.main import wrap_session, Session, UsageError\n            raise UsageError(\"usage error\")\n            config = testdir.parseconfig()\n            session = Session(config)\n            assert wrap_session(config, faulty_doit) == 4\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_pytest_addoption():\n    from _pytest.config import Config, PytestPluginManager\n\n    parser = Config(PytestPluginManager())._parser\n    pytest_addoption(parser)\n    \n    options = parser.parse_args([\"--maxfail=2\", \"--strict-markers\", \"--rootdir=/path/to/root\"])\n    \n    assert options.maxfail == 2\n    assert options.strict_markers is True\n    assert options.rootdir == \"/path/to/root\"\n", "def test_pytest_addoption_defaults(testdir):\n    \"\"\"Test that default options are correctly set by pytest_addoption\"\"\"\n    config = testdir.parseconfig()\n    assert config.getini(\"norecursedirs\") == [\".*\", \"build\", \"dist\", \"CVS\", \"_darcs\", \"{arch}\", \"*.egg\", \"venv\"]\n    assert config.getini(\"testpaths\") == []\n    assert config.option.maxfail == 0\n    assert config.option.strict_markers is False\n    assert config.option.inifilename is None\n    assert config.option.continue_on_collection_errors is False\n    assert config.option.rootdir is None\n    assert config.option.collectonly is False\n    assert config.option.pyargs is False\n    assert config.option.ignore is None\n    assert config.option.ignore_glob is None\n    assert config.option.deselect is None\n    assert config.option.confcutdir is None\n    assert config.option.noconftest is False\n    assert config.option.keepduplicates is False\n    assert config.option.collect_in_virtualenv is False\n    assert config.option.basetemp is None\n", "def test_pytest_addoption_parser():\n    from _pytest.config import Config\n    from _pytest.config.argparsing import Parser\n\n    parser = Parser()\n    pytest_addoption(parser)\n    config = Config.fromdictargs({}, parser.parse([]))\n    assert config.getini(\"norecursedirs\") == [\".*\", \"build\", \"dist\", \"CVS\", \"_darcs\", \"{arch}\", \"*.egg\", \"venv\"]\n    assert config.getini(\"testpaths\") == []\n    assert config.getoption(\"maxfail\") == 0\n    assert not config.getoption(\"strict_markers\")\n    assert config.getoption(\"inifilename\") is None\n    assert not config.getoption(\"continue_on_collection_errors\")\n    assert config.getoption(\"rootdir\") is None\n    assert not config.getoption(\"collectonly\")\n    assert not config.getoption(\"pyargs\")\n    assert config.getoption(\"ignore\") is None\n    assert config.getoption(\"ignore_glob\") is None\n    assert config.getoption(\"deselect\") is None\n    assert config.getoption(\"confcutdir\") is None\n    assert not config.getoption(\"noconftest\")\n    assert not config.getoption(\"keepduplicates\")\n    assert not config.getoption(\"collect_in_virtualenv\")\n    assert config.getoption(\"basetemp\") is None\n"], "sample_664": ["def test_funcargnames_is_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            assert hasattr(request, 'funcargnames')\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*The `funcargnames` attribute was an alias for `fixturenames`, since pytest 2.3 - use the newer attribute instead.*\"\n        ]\n    )\n", "def test_funcargnames_is_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            assert hasattr(request, 'funcargnames')\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*The `funcargnames` attribute was an alias for `fixturenames`,*\",\n            \"*since pytest 2.3 - use the newer attribute instead.*\",\n        ]\n    )\n", "def test_funcargnames_deprecated_warning():\n    with pytest.warns(deprecated.PytestDeprecationWarning, match=\"The `funcargnames` attribute was an alias for `fixturenames`\"):\n        pytest.funcargnames  # Trigger the warning\n", "def test_funcargnames_is_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*The `funcargnames` attribute was an alias for `fixturenames`,*\",\n            \"*since pytest 2.3 - use the newer attribute instead.*\",\n        ]\n    )\n\n", "def test_funcargnames_deprecation(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            assert hasattr(request, 'funcargnames')\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*The `funcargnames` attribute was an alias for `fixturenames`, since pytest 2.3 - use the newer attribute instead.*\"]\n    )\n"], "sample_665": ["def test_collect_with_invalid_python_file(testdir):\n    testdir.makepyfile(\n        test_invalid_syntax=\"\"\"\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*ERROR collecting test_invalid_syntax.py*\",\n            \"*SyntaxError: invalid syntax*\",\n        ]\n    )\n    assert result.ret == ExitCode.INTERRUPTED\n", "def test_pyobj_property():\n    from pytest import Module\n\n    class MockNode:\n            self.parent = parent\n            self.obj = obj\n\n            if isinstance(self.parent, cls):\n                return self.parent\n            return None\n\n    class MockModule(Module):\n            self.obj = obj\n\n    class MockClass:\n        obj = \"test_module_obj\"\n\n    mod = MockModule(obj=\"test_module_obj\")\n    node = MockNode(parent=mod)\n\n    prop = pyobj_property(\"Module\")\n    assert prop.__get__(node) == \"test_module_obj\"\n", "    def test_pyobj_property(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            class TestClass:\n                    self.value = 42\n\n                    return self.value\n\n            @pytest.fixture\n                return TestClass()\n\n                assert test_class.get_value() == 42\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 passed in*\"])\n", "    def test_pytest_generate_tests(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"x\", [1, 2])\n                assert x in [1, 2]\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2, failed=0)\n", "def test_collect_file_matches_patterns(self, testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        python_files = test_*.py *_test.py foo_*.py\n    \"\"\"\n    )\n    testdir.makepyfile(\n        test_foo=\"def test_foo(): pass\",\n        bar_test=\"def test_bar(): pass\",\n        foo_baz=\"def test_baz(): pass\",\n        not_test=\"def test_not(): pass\",\n    )\n\n    items, _ = testdir.inline_genitems()\n    collected_files = [x.fspath.basename for x in items]\n\n    assert \"test_foo.py\" in collected_files\n    assert \"bar_test.py\" in collected_files\n    assert \"foo_baz.py\" in collected_files\n    assert \"not_test.py\" not in collected_files\n"], "sample_666": ["def test_logcapturehandler_emit():\n    handler = LogCaptureHandler()\n    logger = logging.getLogger()\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n    \n    logger.debug(\"test debug message\")\n    logger.info(\"test info message\")\n    logger.warning(\"test warning message\")\n    logger.error(\"test error message\")\n    logger.critical(\"test critical message\")\n    \n    records = handler.records\n    assert len(records) == 5\n    assert records[0].message == \"test debug message\"\n    assert records[1].message == \"test info message\"\n    assert records[2].message == \"test warning message\"\n    assert records[3].message == \"test error message\"\n    assert records[4].message == \"test critical message\"\n\n    handler.reset()\n    assert len(handler.records) == 0\n    assert handler.stream.getvalue() == \"\"\n", "def test_colored_level_formatter(caplog):\n    import logging\n    from _pytest.config import create_terminal_writer\n\n    class TerminalWriterMock:\n            # Just return the text for simplicity\n            return text\n\n    caplog.set_level(logging.DEBUG)\n\n    # Create a ColoredLevelFormatter instance with a mock terminal writer\n    formatter = ColoredLevelFormatter(\n        terminalwriter=TerminalWriterMock(),\n        fmt=\"%(levelname)-8s %(message)s\",\n    )\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger = logging.getLogger(\"testlogger\")\n    logger.addHandler(handler)\n\n    with caplog.at_level(logging.DEBUG):\n        logger.critical(\"critical message\")\n        logger.error(\"error message\")\n        logger.warning(\"warning message\")\n        logger.info(\"info message\")\n        logger.debug(\"debug message\")\n\n    assert caplog.record_tuples == [\n        (\"testlogger\", logging.CRITICAL, \"critical message\"),\n        (\"testlogger\", logging.ERROR, \"error message\"),\n        (\"testlogger\", logging.WARNING, \"warning message\"),\n        (\"testlogger\", logging.INFO, \"info message\"),\n        (\"testlogger\", logging.DEBUG, \"debug message\"),\n    ]\n", "def test_colored_level_formatter():\n    import logging\n    from _pytest.config import create_terminal_writer\n    class DummyTerminalWriter:\n            return text\n\n    logger = logging.getLogger(\"test_logger\")\n    log_message = \"This is a test log message\"\n    terminal_writer = DummyTerminalWriter()\n    formatter = ColoredLevelFormatter(terminal_writer, fmt=\"%(levelname)s: %(message)s\")\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n\n    with capture.StdCaptureFD() as cap:\n        logger.debug(log_message)\n        out, err = cap.readouterr()\n        assert \"DEBUG: This is a test log message\" in out\n\n    logger.removeHandler(handler)\n", "def test_logcapturehandler_reset():\n    handler = LogCaptureHandler()\n    logger = logging.getLogger()\n    logger.addHandler(handler)\n    logger.warning(\"initial log\")\n    assert len(handler.records) == 1\n    assert handler.records[0].getMessage() == \"initial log\"\n    \n    handler.reset()\n    assert len(handler.records) == 0\n    assert handler.stream.getvalue() == \"\"\n\n    logger.warning(\"log after reset\")\n    assert len(handler.records) == 1\n    assert handler.records[0].getMessage() == \"log after reset\"\n    logger.removeHandler(handler)\n", "def test_colored_level_formatter():\n    from _pytest.config import Config\n    from _pytest.config import create_terminal_writer\n    import logging\n\n    config = Config()\n    terminal_writer = create_terminal_writer(config)\n    formatter = ColoredLevelFormatter(terminal_writer, DEFAULT_LOG_FORMAT, DEFAULT_LOG_DATE_FORMAT)\n    record = logging.LogRecord(name=\"test\", level=logging.ERROR, pathname=None, lineno=0, msg=\"An error occurred\", args=(), exc_info=None)\n\n    formatted_message = formatter.format(record)\n    assert \"ERROR\" in formatted_message\n    assert \"An error occurred\" in formatted_message\n\n    record = logging.LogRecord(name=\"test\", level=logging.INFO, pathname=None, lineno=0, msg=\"An info message\", args=(), exc_info=None)\n    formatted_message = formatter.format(record)\n    assert \"INFO\" in formatted_message\n    assert \"An info message\" in formatted_message\n"], "sample_667": ["def test_ensure_relative_to_basetemp(tmp_path):\n    from _pytest.tmpdir import TempPathFactory\n\n    config = FakeConfig(tmp_path)\n    t = TempPathFactory.from_config(config)\n\n    with pytest.raises(ValueError, match=\"is not a normalized and relative path\"):\n        t._ensure_relative_to_basetemp(\"/absolute/path\")\n\n    valid_basename = \"relative_path\"\n    result = t._ensure_relative_to_basetemp(valid_basename)\n    assert result == valid_basename\n", "def test_tmpdir_factory_mktemp_relative_path_check(testdir):\n    mytemp = testdir.tmpdir.mkdir(\"mytemp\")\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            with pytest.raises(ValueError, match=\"is not a normalized and relative path\"):\n                tmpdir_factory.mktemp('/absolute/path', numbered=False)\n    \"\"\"\n    )\n    result = testdir.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert result.ret == 0\n", "def test_mktemp_with_non_numbered_dir(tmp_path):\n    from _pytest.tmpdir import TempPathFactory\n\n    config = FakeConfig(tmp_path)\n    t = TempPathFactory.from_config(config)\n    tmp = t.mktemp(\"non_numbered\", numbered=False)\n    assert tmp.name == \"non_numbered\"\n    assert tmp.is_dir()\n", "def test_temp_path_factory_mktemp_numbered(tmp_path_factory):\n    \"\"\"Test TempPathFactory.mktemp with numbered=True\"\"\"\n    temp_dir = tmp_path_factory.mktemp(\"testdir\", numbered=True)\n    assert temp_dir.is_dir()\n    assert re.match(r\"testdir-\\d+\", temp_dir.name)\n", "def test_mktemp_with_numbered_false(testdir):\n    \"\"\"Test the mktemp method with numbered=False to ensure it creates the correct directory.\"\"\"\n    mytemp = testdir.tmpdir.mkdir(\"mytemp\")\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            tmpdir = tmpdir_factory.mktemp('mydir', numbered=False)\n            assert tmpdir.basename == 'mydir'\n        \"\"\"\n    )\n\n    result = testdir.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert result.ret == 0\n    assert mytemp.join(\"mydir\").check()\n"], "sample_668": ["def test_fixture_positional_arguments_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture('module')\n            return 42\n\n            assert my_fixture == 42\n        \"\"\"\n    )\n    \n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\",\n        ]\n    )\n", "def test_fixture_positional_arguments_deprecation(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        \n        @pytest.fixture('module')\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.*\"\n        ]\n    )\n", "compilation error", "compilation error", "compilation error"], "sample_669": ["def test_capture_manager_repr():\n    capman = CaptureManager(\"sys\")\n    assert repr(capman) == \"<CaptureManager _method='sys' _global_capturing=None _capture_fixture=None>\"\n    capman.start_global_capturing()\n    assert \"CaptureManager\" in repr(capman)\n    capman.stop_global_capturing()\n", "def test_capture_binary_output_with_sys_capture(testdir):\n    testdir.makepyfile(\n        r\"\"\"\n        import sys\n\n            sys.stdout.buffer.write(b'hello')\n            sys.stdout.buffer.flush()\n            sys.stdout.write(\"world\\n\")\n            sys.stdout.flush()\n            out, err = capsysbinary.readouterr()\n            assert out == b'helloworld\\n'\n            assert err == b''\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--capture=sys\")\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_fd_capture_binary(tmpfile):\n    fd = tmpfile.fileno()\n    cap = capture.FDCaptureBinary(fd)\n    data = b\"hello\\xfe\\x98\\x20\"\n    os.write(fd, data)\n    cap.start()\n    os.write(fd, data)\n    s = cap.snap()\n    cap.done()\n    assert s == data\n", "def test_capturefixture_disabled_context_manager(capsys):\n    print(\"This should be captured\")\n    with capsys.disabled():\n        print(\"This should not be captured\")\n    print(\"This should be captured again\")\n    captured = capsys.readouterr()\n    assert \"This should be captured\" in captured.out\n    assert \"This should not be captured\" not in captured.out\n    assert \"This should be captured again\" in captured.out\n", "def test_capture_fixture_in_subprocess(testdir):\n    \"\"\"Test that capsys and capfd fixtures work correctly in subprocesses.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            import subprocess\n\n            # Test capsys fixture\n            subprocess.run([\"python\", \"-c\", \"print('hello from capsys')\"])\n            out, err = capsys.readouterr()\n            assert \"hello from capsys\" in out\n\n            # Test capfd fixture\n            subprocess.run([\"python\", \"-c\", \"import os; os.write(1, b'hello from capfd\\\\n')\"])\n            out, err = capfd.readouterr()\n            assert \"hello from capfd\" in out\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n"], "sample_670": ["def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_nested_parentheses() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"((true))\", matcher) is True\n    assert evaluate(\"((false))\", matcher) is False\n    assert evaluate(\"(true and (false or true))\", matcher) is True\n    assert evaluate(\"(true and (false or false))\", matcher) is False\n    assert evaluate(\"((true or false) and (true and true))\", matcher) is True\n    assert evaluate(\"(((true and false) or (not false)))\", matcher) is True\n", "def test_nested_parentheses() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"((true and false) or (true and not false))\", matcher) is True\n    assert evaluate(\"(((true)))\", matcher) is True\n    assert evaluate(\"((true and (false or true)) and not (false))\", matcher) is True\n    assert evaluate(\"((((true and false) or true) and false) or true)\", matcher) is True\n", "def test_complex_nested_expressions() -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True, \"d\": False}.__getitem__\n    assert evaluate(\"((a or b) and (c or d))\", matcher) is True\n    assert evaluate(\"((a and b) or (c and d))\", matcher) is False\n    assert evaluate(\"(a and (b or (c and d)))\", matcher) is False\n    assert evaluate(\"((a or b) and (c and (a or d)))\", matcher) is True\n    assert evaluate(\"(((a)))\", matcher) is True\n"], "sample_671": ["def test_strict_xfail_for_passing_test(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(strict=True, reason='expected to fail')\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest(p, \"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*expected to fail*\"])\n    assert result.ret == 1\n", "def test_xfail_with_custom_exception(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        class CustomException(Exception):\n            pass\n\n        @pytest.mark.xfail(raises=CustomException, reason=\"expecting CustomException\")\n            raise CustomException(\"test exception\")\n        \"\"\"\n    )\n    reports = runtestprotocol(item, log=False)\n    assert len(reports) == 3\n    callreport = reports[1]\n    assert callreport.skipped\n    assert callreport.wasxfail == \"expecting CustomException\"\n", "def test_skip_with_condition_and_reason(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"os.name == 'nt'\", reason=\"Test is not supported on Windows\")\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-rs\")\n    if sys.platform == \"win32\":\n        result.stdout.fnmatch_lines([\"*SKIP*Test is not supported on Windows*\", \"*1 skipped*\"])\n    else:\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_xfail_with_raises_and_strict(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(raises=ValueError, strict=True)\n            raise ValueError(\"expected failure\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_func*\", \"*expected failure*\"])\n    assert result.ret == 0\n", "def test_skipif_with_dynamic_condition(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import os\n\n        condition = os.name == 'posix'\n\n        @pytest.mark.skipif(condition, reason=\"Only for non-posix systems\")\n            assert True\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-rs\")\n    if os.name == 'posix':\n        result.stdout.fnmatch_lines([\"*Only for non-posix systems*\", \"*1 skipped*\"])\n    else:\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n"], "sample_672": ["def test_custom_repr_exceptions():\n    class CustomException(Exception):\n            super().__init__(message)\n\n    class CustomRepr:\n            raise CustomException(\"custom exception in repr\")\n\n    s = saferepr(CustomRepr())\n    assert \"CustomException\" in s\n    assert \"custom exception in repr\" in s\n", "def test_custom_object_with_large_list():\n    class CustomObject:\n            return \"CustomObject with large list\"\n\n    obj = CustomObject()\n    large_list = [obj] * 1000\n    result = saferepr(large_list, maxsize=50)\n    assert \"CustomObject with large list\" in result\n    assert result.startswith(\"[\")\n    assert result.endswith(\"...]\")\n", "def test_recursive_repr():\n    \"\"\"Test saferepr() with recursive data structures.\"\"\"\n\n    class Recursive:\n            self.loop = self\n\n            return \"Recursive()\"\n\n    recursive_obj = Recursive()\n    # Verifies that it handles recursion gracefully\n    assert saferepr(recursive_obj) == 'Recursive()'\n\n    recursive_list = []\n    recursive_list.append(recursive_list)\n    # Verifies that it handles recursion in lists gracefully\n    assert saferepr(recursive_list).startswith('[...]')\n\n    recursive_dict = {}\n    recursive_dict['key'] = recursive_dict\n    # Verifies that it handles recursion in dictionaries gracefully\n    assert saferepr(recursive_dict).startswith('{\\'key\\': {...}}')\n", "def test_safeformat():\n    class BrokenRepr:\n            raise ValueError(\"Broken __repr__\")\n\n    obj = {\"key\": BrokenRepr()}\n    formatted = safeformat(obj)\n    assert \"ValueError\" in formatted\n    assert \"Broken\" in formatted\n\n    obj = [BrokenRepr(), BrokenRepr()]\n    formatted = safeformat(obj)\n    assert \"ValueError\" in formatted\n    assert \"Broken\" in formatted\n\n    obj = (BrokenRepr(),)\n    formatted = safeformat(obj)\n    assert \"ValueError\" in formatted\n    assert \"Broken\" in formatted\n\n    obj = BrokenRepr()\n    formatted = safeformat(obj)\n    assert \"ValueError\" in formatted\n    assert \"Broken\" in formatted\n", "def test_safeformat():\n    class A:\n            raise ValueError(\"...\")\n\n    obj = (\"*\", A())\n    result = safeformat(obj)\n    assert \"ValueError\" in result\n    assert \"...\" in result\n"], "sample_673": ["def test_doctest_textfile_from_parent(self, testdir):\n    \"\"\"Test the DoctestTextfile.from_parent class method to ensure it\n    properly creates a DoctestTextfile instance from a parent node.\"\"\"\n    testfile = testdir.maketxtfile(\n        test_doc=\"\"\"\n        >>> x = 1\n        >>> x == 1\n        True\n    \"\"\"\n    )\n    parent = testdir.getnode(testfile)\n    doctest_textfile = DoctestTextfile.from_parent(parent=parent, fspath=testfile)\n    assert isinstance(doctest_textfile, DoctestTextfile)\n    assert doctest_textfile.fspath == testfile\n", "def test_doctest_unexpected_bdbquit(self, testdir):\n    \"\"\"Test handling of bdb.BdbQuit exception in doctests.\"\"\"\n    testdir.maketxtfile(\n        \"\"\"\n        >>> import bdb; raise bdb.BdbQuit()\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"*Exit: Quitting debugger*\"])\n", "def test_doctest_ellipsis_flag(self, testdir):\n    \"\"\"Test that the ELLIPSIS flag works correctly in doctests.\"\"\"\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        doctest_optionflags = ELLIPSIS\n    \"\"\"\n    )\n    p = testdir.makepyfile(\n        \"\"\"\n            '''\n            >>> a = \"Hello ... World\"\n            >>> print(a)\n            Hello ... World\n            '''\n    \"\"\"\n    )\n    reprec = testdir.inline_run(p, \"--doctest-modules\")\n    reprec.assertoutcome(passed=1)\n", "def test_checker_output_difference_unicode(self):\n    \"\"\"Test that the custom checker correctly handles differences in unicode output.\"\"\"\n    import doctest\n    checker = _get_checker()\n    example = doctest.Example(\"u'hello'\", \"u'hello'\")\n    result = checker.check_output(\"u'hello'\", \"u'hello'\", 0)\n    assert result is True\n    result = checker.check_output(\"u'hello'\", \"u'world'\", 0)\n    assert result is False\n", "def test_doctestmodule_with_mocked_object(self, testdir):\n    \"\"\"\n    Test that the doctests can handle mocked objects properly.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        from unittest.mock import MagicMock\n\n            '''\n            >>> mock = MagicMock()\n            >>> mock.method()\n            >>> mock.method.assert_called_once()\n            '''\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"* 1 passed *\"])\n"], "sample_674": ["def test_node_iter_markers():\n    node = nodes.Node(name=\"test_node\", parent=None, config=pytest.Config.fromdictargs({}), session=object())\n    node.add_marker(\"test_marker\")\n    markers = list(node.iter_markers())\n    assert len(markers) == 1\n    assert markers[0].name == \"test_marker\"\n", "def test_splitnode():\n    assert nodes._splitnode(\"\") == ()\n    assert nodes._splitnode(\"testing/code\") == (\"testing\", \"code\")\n    assert nodes._splitnode(\"testing/code/test_excinfo.py\") == (\"testing\", \"code\", \"test_excinfo.py\")\n    assert nodes._splitnode(\"testing/code/test_excinfo.py::TestFormattedExcinfo\") == (\"testing\", \"code\", \"test_excinfo.py\", \"TestFormattedExcinfo\")\n", "def test_node_add_marker():\n    node = nodes.Node(name=\"test_node\", parent=None, config=pytest.Config.fromdictargs({}, []), session=pytest.Session(py.path.local(), pytest.Config.fromdictargs({}, [])))\n    node.add_marker(\"test_marker\")\n    assert \"test_marker\" in node.keywords\n    assert any(marker.name == \"test_marker\" for marker in node.own_markers)\n", "def test_node_repr():\n    node = nodes.Node(name=\"testnode\", parent=None, config=pytest.Config(), session=pytest.Session())\n    assert repr(node) == \"<Node testnode>\"\n", "def test_node_add_marker():\n    node = nodes.Node(name=\"test_node\", parent=None, config=pytest.Config.fromdictargs({}), session=pytest.Session(py.path.local(), pytest.Config.fromdictargs({})), fspath=py.path.local())\n    marker = pytest.mark.skip\n    node.add_marker(marker)\n    assert node.own_markers[0].name == \"skip\"\n    assert node.keywords[\"skip\"] == marker\n"], "sample_675": ["def test_log_auto_indent_on(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('first line\\\\nsecond line\\\\nthird line')\n            assert True\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent=on\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO*first line\",\n            \"         second line\",\n            \"         third line\",\n        ]\n    )\n", "def test_log_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('text going to logger\\\\nwith multiple lines')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent=4\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured *log call -*\",\n            \"*text going to logger*\",\n            \"    with multiple lines\",\n        ]\n    )\n", "def test_log_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('text going to logger from call\\\\nwith an additional line')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent=2\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured *log call -*\",\n            \"*text going to logger from call\",\n            \"  with an additional line*\",\n        ]\n    )\n", "def test_colored_level_formatter(testdir):\n    \"\"\"\n    Test that the level names are correctly colored by ColoredLevelFormatter.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        from _pytest.logging import ColoredLevelFormatter\n        from _pytest.config import create_terminal_writer\n\n            tw = create_terminal_writer(None)\n            formatter = ColoredLevelFormatter(tw, \"%(levelname)-8s %(message)s\")\n            logger = logging.getLogger(\"test\")\n            handler = logging.StreamHandler()\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            logger.setLevel(logging.DEBUG)\n\n            with capsys.disabled():\n                logger.debug(\"debug message\")\n                logger.info(\"info message\")\n                logger.warning(\"warning message\")\n                logger.error(\"error message\")\n                logger.critical(\"critical message\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--color=yes\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*\\x1b[35mDEBUG   \\x1b[0mdebug message*\",\n            \"*\\x1b[32mINFO    \\x1b[0minfo message*\",\n            \"*\\x1b[33mWARNING \\x1b[0mwarning message*\",\n            \"*\\x1b[31mERROR   \\x1b[0merror message*\",\n            \"*\\x1b[31mCRITICAL\\x1b[0mcritical message*\",\n        ]\n    )\n", "def test_log_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('first line\\\\nsecond line\\\\nthird line', extra={'auto_indent': True})\n            logger.info('another log\\\\nwith multiple lines', extra={'auto_indent': 4})\n            assert True\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent=on\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO    *first line\",\n            \"*INFO    *    second line\",\n            \"*INFO    *    third line\",\n            \"*INFO    *another log\",\n            \"*INFO    *    with multiple lines\",\n        ]\n    )\n"], "sample_676": ["def test_warning_report_location(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import warnings\n        import pytest\n\n        @pytest.fixture\n            warnings.warn(UserWarning(\"this is a warning\"))\n            return True\n\n            assert trigger_warning\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*this is a warning*\"])\n    tr = TerminalReporter(result.config, file=result.stdout)\n    warning_report = WarningReport(\n        message=\"this is a warning\",\n        nodeid=\"test_warning_report_location.py::test_warning\",\n        fslocation=(\"test_warning_report_location.py\", 7)\n    )\n    location = warning_report.get_location(result.config)\n    assert location == \"test_warning_report_location.py:7\"\n", "def test_more_quiet_action():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--quiet\", action=MoreQuietAction, dest=\"verbose\", default=0)\n    namespace = parser.parse_args([\"--quiet\"])\n    assert namespace.verbose == -1\n    assert namespace.quiet == 1\n", "def test_warning_report_location():\n    config = pytest.Config()\n    warning_report = WarningReport(\n        message=\"Warning message\",\n        nodeid=\"test_warning.py::test_func\",\n        fslocation=(\"test_warning.py\", 10)\n    )\n    location = warning_report.get_location(config)\n    assert location == \"test_warning.py:10\"\n\n    warning_report = WarningReport(\n        message=\"Warning message\",\n        nodeid=None,\n        fslocation=(\"test_warning.py\", 20)\n    )\n    location = warning_report.get_location(config)\n    assert location == \"test_warning.py:20\"\n\n    warning_report = WarningReport(\n        message=\"Warning message\",\n        nodeid=None,\n        fslocation=\"test_warning.py\"\n    )\n    location = warning_report.get_location(config)\n    assert location == \"test_warning.py\"\n\n    warning_report = WarningReport(\n        message=\"Warning message\",\n        nodeid=None,\n        fslocation=None\n    )\n    location = warning_report.get_location(config)\n    assert location is None\n", "def test_pytest_addoption():\n    parser = pytest.Parser()\n    pytest_addoption(parser)\n    options = parser.parse_args([])\n    assert options.verbose == 0\n    assert options.color == \"auto\"\n    assert options.showcapture == \"all\"\n    assert options.tbstyle == \"auto\"\n    assert not options.fulltrace\n    assert not options.disable_warnings\n\n    options = parser.parse_args([\"-v\"])\n    assert options.verbose == 1\n\n    options = parser.parse_args([\"-vv\"])\n    assert options.verbose == 2\n\n    options = parser.parse_args([\"-q\"])\n    assert options.verbose == -1\n\n    options = parser.parse_args([\"--color\", \"yes\"])\n    assert options.color == \"yes\"\n\n    options = parser.parse_args([\"--show-capture\", \"stdout\"])\n    assert options.showcapture == \"stdout\"\n", "def test_pytest_addoption():\n    parser = pytest.Parser()\n    pytest_addoption(parser)\n    group = parser.getgroup(\"terminal reporting\")\n    assert group is not None\n    options = [opt.dest for opt in group.options]\n    expected_options = [\n        \"verbose\",\n        \"verbose\",\n        \"verbose\",\n        \"reportchars\",\n        \"disable_warnings\",\n        \"showlocals\",\n        \"tbstyle\",\n        \"showcapture\",\n        \"fulltrace\",\n        \"color\",\n    ]\n    assert sorted(options) == sorted(expected_options)\n"], "sample_677": ["def test_complex_expressions(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expression() -> None:\n    matcher = {\n        \"true\": True,\n        \"false\": False,\n        \"foo\": True,\n        \"bar\": False,\n        \"baz\": True,\n        \"qux\": False,\n    }.__getitem__\n    expr = \"true and (false or (foo and (not bar or (baz and not qux))))\"\n    expected = True  # Evaluates to: true and (false or (true and (not false or (true and not false))))\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expressions(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expression() -> None:\n    matcher = {\n        \"true\": True,\n        \"false\": False,\n        \"x\": True,\n        \"y\": False,\n        \"z\": True,\n        \"a\": False,\n        \"b\": True,\n    }.__getitem__\n    expr = \"((true and x) or (false and y)) and (z or not a) and b\"\n    assert evaluate(expr, matcher) is True\n", "def test_mixed_case_identifiers() -> None:\n    matcher = {\"True\": True, \"False\": False, \"tRuE\": True, \"FaLsE\": False}.__getitem__\n    assert evaluate(\"True\", matcher) is True\n    assert evaluate(\"False\", matcher) is False\n    assert evaluate(\"tRuE\", matcher) is True\n    assert evaluate(\"FaLsE\", matcher) is False\n    assert evaluate(\"True and FaLsE\", matcher) is False\n    assert evaluate(\"tRuE or False\", matcher) is True\n"], "sample_678": ["def test_ensure_deletable(tmp_path, monkeypatch):\n    \"\"\"Test ensure_deletable function to check if lock exists and can be removed if considered dead.\"\"\"\n    path = tmp_path / \"deletable-dir\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n\n    consider_lock_dead_if_created_before = lock_path.stat().st_mtime - 1\n\n    # Test when lock exists and is considered dead\n    assert not ensure_deletable(path, consider_lock_dead_if_created_before)\n\n    # Test when lock does not exist\n    lock_path.unlink()\n    assert ensure_deletable(path, consider_lock_dead_if_created_before)\n", "def test_ensure_extended_length_path_non_windows(tmp_path):\n    \"\"\"Ensure that ensure_extended_length_path returns the original path on non-Windows systems.\"\"\"\n    path = tmp_path / \"some_dir\"\n    path.mkdir()\n    resolved_path = ensure_extended_length_path(path)\n    assert resolved_path == path\n", "def test_create_cleanup_lock(tmp_path):\n    \"\"\"Test that a cleanup lock is created and contains the correct process ID.\"\"\"\n    path = tmp_path / \"temp\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    created_lock_path = create_cleanup_lock(path)\n    assert created_lock_path == lock_path\n    assert lock_path.is_file()\n    with open(lock_path, \"r\") as f:\n        lock_content = f.read()\n    assert lock_content == str(os.getpid())\n", "def test_ensure_deletable(tmp_path):\n    \"\"\"Ensure that the ensure_deletable function works correctly.\"\"\"\n    path = tmp_path / \"tempdir\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    lock_path.touch()  # Create the lock file\n\n    # The lock file is created right now, so it should not be deletable\n    assert not ensure_deletable(path, consider_lock_dead_if_created_before=float('inf'))\n\n    # Consider the lock as dead by setting a time far in the past\n    assert ensure_deletable(path, consider_lock_dead_if_created_before=0)\n\n    # Ensure that the lock file was deleted\n    assert not lock_path.exists()\n", "def test_create_cleanup_lock(tmp_path):\n    \"\"\"Ensure that a cleanup lock can be created and file is written correctly.\"\"\"\n    path = tmp_path / \"lock_test\"\n    path.mkdir()\n    lock_path = create_cleanup_lock(path)\n    assert lock_path.is_file()\n    with open(lock_path) as f:\n        assert f.read() == str(os.getpid())\n    # Cleanup\n    lock_path.unlink()\n"], "sample_679": ["def test_istrue_method():\n    from _pytest.nodes import Item\n    from _pytest.mark.structures import Mark\n    from ..outcomes import TEST_OUTCOME\n\n    class MockItem(Item):\n            super().__init__(name, config)\n            self.marks = []\n\n            if name:\n                return [mark for mark in self.marks if mark.name == name]\n            return self.marks\n\n    mock_item = MockItem(name=\"test_item\", config=None)\n    mock_item.marks.append(Mark(name=\"skipif\", args=(\"False\",), kwargs={\"reason\": \"skip this test\"}))\n\n    evaluator = MarkEvaluator(mock_item, \"skipif\")\n    assert evaluator.istrue() is False\n\n    mock_item.marks = []\n    mock_item.marks.append(Mark(name=\"skipif\", args=(\"True\",), kwargs={\"reason\": \"skip this test\"}))\n    evaluator = MarkEvaluator(mock_item, \"skipif\")\n    assert evaluator.istrue() is True\n\n    mock_item.marks = []\n    mock_item.marks.append(Mark(name=\"skipif\", args=(True,), kwargs={\"reason\": \"skip this test\"}))\n    evaluator = MarkEvaluator(mock_item, \"skipif\")\n    assert evaluator.istrue() is True\n\n    mock_item.marks = []\n    mock_item.marks.append(Mark(name=\"skipif\", args=(False,), kwargs={\"reason\": \"skip this test\"}))\n    evaluator = MarkEvaluator(mock_item, \"skipif\")\n    assert evaluator.istrue() is False\n\n    mock_item.marks = []\n    mock_item.marks.append(Mark(name=\"skipif\", args=(), kwargs={\"condition\": \"True\", \"reason\": \"skip this test\"}))\n    evaluator = MarkEvaluator(mock_item, \"skipif\")\n    assert evaluator.istrue() is True\n\n    mock_item.marks = []\n    mock_item.marks.append(Mark(name=\"skipif\", args=(), kwargs={\"condition\": \"False\", \"reason\": \"skip this test\"}))\n    evaluator = MarkEvaluator(mock_item, \"skipif\")\n    assert evaluator.istrue() is False\n", "def test_mark_evaluator_istrue_with_expression(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest.mark import MarkEvaluator\n        from _pytest.nodes import Item\n\n        @pytest.fixture\n            class MockItem:\n                    self.config = {}\n                    self.obj = type('', (), {})()\n                    self.obj.__globals__ = {}\n\n                    if name == \"mockmark\":\n                        return [pytest.mark.mockmark(condition=\"2+2==4\")]\n                    return []\n\n            return MockItem()\n\n            evaluator = MarkEvaluator(mock_item, \"mockmark\")\n            assert evaluator.istrue() is True\n    \"\"\"\n    )\n    result = testdir.inline_run()\n    result.assertoutcome(passed=1)\n", "def test_mark_evaluator_istrue():\n    from _pytest.mark import pytest_configure, get_empty_parameterset_mark\n    from _pytest.mark import MarkEvaluator\n    from _pytest.nodes import Node, Item\n\n    class MockConfig:\n            self._inicache = {}\n\n            return self._inicache.get(name, \"\")\n\n    class MockItem(Node):\n            super().__init__(name, None)\n            self.config = config\n            self.own_markers = []\n\n            if name:\n                return (m for m in self.own_markers if m.name == name)\n            return iter(self.own_markers)\n\n    config = MockConfig()\n    item = MockItem(name=\"test_item\", config=config)\n\n    pytest_configure(config)\n\n    # Add a marker to the item\n    item.own_markers.append(pytest.mark.skipif(\"sys.platform == 'win32'\", reason=\"skip on windows\"))\n\n    evaluator = MarkEvaluator(item, name=\"skipif\")\n    assert evaluator.istrue() == (sys.platform == \"win32\")\n", "def test_mark_evaluator_istrue(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest.mark.evaluate import MarkEvaluator\n        from _pytest.mark.structures import Mark\n\n        class TestItem(pytest.Item):\n                super().__init__(name, parent)\n                self._markers = {}\n\n                self._markers[mark.name] = mark\n\n                if name is None:\n                    return self._markers.values()\n                return [m for m in self._markers.values() if m.name == name]\n\n        @pytest.fixture\n            node = TestItem(\"test_node\", parent=None)\n            node.config = pytestconfig\n            return node\n\n            mark = Mark(name=\"custom_mark\", args=(\"2 + 2 == 4\",), kwargs={})\n            test_item.add_marker(mark)\n            evaluator = MarkEvaluator(test_item, \"custom_mark\")\n            assert evaluator.istrue() is True\n\n            mark = Mark(name=\"custom_mark\", args=(True,), kwargs={\"reason\": \"some reason\"})\n            test_item.add_marker(mark)\n            evaluator = MarkEvaluator(test_item, \"custom_mark\")\n            assert evaluator.istrue() is True\n\n            mark = Mark(name=\"custom_mark\", args=(\"invalid syntax\",), kwargs={})\n            test_item.add_marker(mark)\n            evaluator = MarkEvaluator(test_item, \"custom_mark\")\n            with pytest.raises(Exception):\n                evaluator.istrue()\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=2, failed=1)\n", "def test_mark_evaluator_istrue():\n    from _pytest.nodes import Node\n    from _pytest.mark import Mark\n\n    class MockItem(Node):\n            if name == \"foo\":\n                return [Mark(name=\"foo\", args=(\"True\",), kwargs={})]\n            return []\n\n    item = MockItem(name=\"test_item\", parent=None)\n    mark_evaluator = MarkEvaluator(item, \"foo\")\n    assert mark_evaluator.istrue() == True\n"], "sample_680": ["def test_xfail_with_raises_argument(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(raises=ZeroDivisionError, reason=\"Expecting ZeroDivisionError\")\n            1 / 0\n    \"\"\"\n    )\n    reports = runtestprotocol(item, log=False)\n    callreport = reports[1]\n    assert callreport.skipped\n    assert callreport.wasxfail == \"Expecting ZeroDivisionError\"\n", "def test_evaluate_condition_invalid_syntax(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\")\n            pass\n    \"\"\"\n    )\n    mark = item.iter_markers(name=\"skipif\")[0]\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_condition(item, mark, \"invalid syntax\")\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_evaluate_condition_with_invalid_syntax(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\", reason=\"Invalid syntax example\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert \"Error evaluating 'skipif' condition\" in excinfo.value.msg\n    assert \"invalid syntax\" in excinfo.value.msg\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_xfail_with_raises_and_no_exception(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(raises=ValueError, reason=\"expecting ValueError\")\n            pass\n    \"\"\"\n    )\n    reports = runtestprotocol(item, log=False)\n    assert len(reports) == 3\n    callreport = reports[1]\n    assert callreport.passed\n    assert callreport.wasxfail == \"expecting ValueError\"\n", "def test_evaluate_condition_syntax_error(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n"], "sample_681": ["def test_auto_indent_multiline_message(testdir):\n    \"\"\"Test auto indentation of multiline log messages.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            caplog.set_level(logging.INFO)\n            logger.info(\"This is a multiline\\\\nlog message\\\\nwith auto indentation.\")\n            assert False\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-auto-indent=2\", \"--log-level=INFO\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"INFO     *This is a multiline\",\n            \"  log message\",\n            \"  with auto indentation.\",\n        ]\n    )\n", "def test_auto_indent_true(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger()\n            logger.info(\"This is a log message\\\\nwith auto indent\")\n            assert True\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent=true\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO    This is a log message\",\n            \"*         with auto indent*\",\n        ]\n    )\n", "def test_colored_multiline_log_messages(testdir):\n    \"\"\"\n    Test that multiline log messages are correctly colored and auto-indented.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('First line\\\\nSecond line\\\\nThird line')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent=true\", \"--color=yes\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"\\x1b[32mINFO    \\x1b[0m*First line\",\n            \"         Second line\",\n            \"         Third line\",\n        ]\n    )\n", "def test_colored_multiline_log_messages(testdir):\n    \"\"\"\n    Test that multiline log messages are correctly indented and colored.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('first line\\\\nsecond line\\\\nthird line')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--color=yes\", \"--log-auto-indent=True\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"\\x1b[32mINFO    \\x1b[0m*first line\",\n            \"         second line\",\n            \"         third line\",\n        ]\n    )\n", "def test_colored_level_formatter(testdir):\n    \"\"\"\n    Test that the ColoredLevelFormatter correctly colors different log levels.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        from _pytest.logging import ColoredLevelFormatter\n        from _pytest.config import create_terminal_writer\n\n            terminal_writer = create_terminal_writer()\n            formatter = ColoredLevelFormatter(terminal_writer, \"%(levelname)s: %(message)s\")\n            logger = logging.getLogger(__name__)\n            stream_handler = logging.StreamHandler()\n            stream_handler.setFormatter(formatter)\n            logger.addHandler(stream_handler)\n            logger.setLevel(logging.DEBUG)\n\n            with capsys.disabled():\n                logger.debug(\"debug message\")\n                logger.info(\"info message\")\n                logger.warning(\"warning message\")\n                logger.error(\"error message\")\n                logger.critical(\"critical message\")\n\n            log_output = stream_handler.stream.getvalue()\n            assert \"\\x1b[35mDEBUG   \\x1b[0m: debug message\" in log_output\n            assert \"\\x1b[32mINFO    \\x1b[0m: info message\" in log_output\n            assert \"\\x1b[33mWARNING \\x1b[0m: warning message\" in log_output\n            assert \"\\x1b[31mERROR   \\x1b[0m: error message\" in log_output\n            assert \"\\x1b[31mCRITICAL\\x1b[0m: critical message\" in log_output\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--capture=no\")\n    assert result.ret == 0\n"], "sample_682": ["def test_xfail_with_raises_keyword(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(raises=ValueError, reason=\"expected ValueError\")\n            raise ValueError(\"this is a ValueError\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxs\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*XFAIL*test_func*\",\n            \"*expected ValueError*\",\n            \"*1 xfailed*\",\n        ]\n    )\n", "def test_marked_xfail_with_raises(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(raises=ZeroDivisionError)\n            1 / 0\n    \"\"\"\n    )\n    xfailed = evaluate_xfail_marks(item)\n    assert xfailed\n    assert xfailed.reason == \"\"\n    assert xfailed.run\n    assert xfailed.raises == (ZeroDivisionError,)\n", "def test_xfail_with_raises(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(raises=ValueError, reason=\"Expecting ValueError\")\n            raise ValueError(\"This is a ValueError\")\n\n        @pytest.mark.xfail(raises=ValueError, reason=\"Expecting ValueError\")\n            raise TypeError(\"This is not a ValueError\")\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*XFAIL*Expecting ValueError*\", \"*FAILED*Expecting ValueError*\"]\n    )\n    result.assert_outcomes(xfailed=1, failed=1)\n", "def test_xfail_with_raises_and_strict(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(raises=ValueError, strict=True, reason=\"Expecting ValueError\")\n            raise ValueError(\"this is a ValueError\")\n    \"\"\"\n    )\n    reports = runtestprotocol(item, log=False)\n    assert len(reports) == 3\n    callreport = reports[1]\n    assert callreport.skipped\n    assert callreport.wasxfail == \"Expecting ValueError\"\n", "def test_evaluate_condition_invalid_syntax(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n"], "sample_683": ["def test_fd_capture_invalid_fd_with_large_output(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            import os\n            fd = 1\n            os.close(fd)\n            cap = capture.FDCapture(fd)\n            cap.start()\n            large_output = \"A\" * 10000\n            os.write(fd, large_output.encode())\n            captured = cap.snap()\n            assert captured == large_output\n            cap.done()\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--capture=fd\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n", "def test_capturemanager_set_fixture():\n    \"\"\"\n    Ensure that CaptureManager.set_fixture correctly sets and handles a capture fixture,\n    and raises appropriate errors if conflicts arise.\n    \"\"\"\n    capman = CaptureManager(\"sys\")\n    request = pytest.FixtureRequest(pytest.Function(\"test_func\", lambda: None))\n    capture_fixture1 = CaptureFixture(SysCapture, request)\n    capture_fixture2 = CaptureFixture(SysCapture, request)\n    \n    # Set the first fixture and ensure it is set correctly.\n    capman.set_fixture(capture_fixture1)\n    assert capman._capture_fixture is capture_fixture1\n\n    # Attempting to set a second fixture should raise an error.\n    with pytest.raises(pytest.fixtures.FixtureLookupError):\n        capman.set_fixture(capture_fixture2)\n    \n    # Ensure that unsetting the fixture works as expected.\n    capman.unset_fixture()\n    assert capman._capture_fixture is None\n", "def test_capture_and_logging_with_different_levels(testdir):\n    \"\"\"Test capturing with different logging levels to ensure all logs are captured correctly.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n        @pytest.fixture\n            logging.basicConfig(level=logging.DEBUG)\n            yield\n            logging.shutdown()\n\n            logger.debug(\"This is a debug message\")\n            captured = capsys.readouterr()\n            assert \"This is a debug message\" in captured.err\n\n            logger.info(\"This is an info message\")\n            captured = capsys.readouterr()\n            assert \"This is an info message\" in captured.err\n\n            logger.warning(\"This is a warning message\")\n            captured = capsys.readouterr()\n            assert \"This is a warning message\" in captured.err\n\n            logger.error(\"This is an error message\")\n            captured = capsys.readouterr()\n            assert \"This is an error message\" in captured.err\n\n            logger.critical(\"This is a critical message\")\n            captured = capsys.readouterr()\n            assert \"This is a critical message\" in captured.err\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*5 passed*\"])\n", "def test_capture_manager_repr():\n    capman = CaptureManager(\"sys\")\n    repr_str = repr(capman)\n    assert \"CaptureManager\" in repr_str\n    assert \"_method='sys'\" in repr_str\n    assert \"_global_capturing=None\" in repr_str\n    assert \"_capture_fixture=None\" in repr_str\n\n    capman.start_global_capturing()\n    repr_str = repr(capman)\n    assert \"_global_capturing=<MultiCapture\" in repr_str\n\n    capman.stop_global_capturing()\n    repr_str = repr(capman)\n    assert \"_global_capturing=None\" in repr_str\n", "def test_no_capture_method():\n    capman = CaptureManager(\"no\")\n    assert not capman.is_capturing()\n    capman.start_global_capturing()\n    assert not capman.is_capturing()\n    assert capman.read_global_capture() == (\"\", \"\")\n    capman.stop_global_capturing()\n"], "sample_684": ["def test_traceback_cut() -> None:\n        raise AssertionError(\"Test\")\n\n    try:\n        func()\n    except AssertionError:\n        exci = ExceptionInfo.from_current()\n\n    tb = exci.traceback\n    cut_tb = tb.cut(path=tb[0].path)\n    assert len(cut_tb) == 1\n    assert cut_tb[0] == tb[0]\n\n    cut_tb = tb.cut(path=tb[0].path, lineno=tb[0].lineno + 1)\n    assert len(cut_tb) == 0\n\n    cut_tb = tb.cut(excludepath=py.path.local(tb[0].path).dirpath())\n    assert len(cut_tb) == 0\n", "def test_traceback_entry_hidden() -> None:\n        __tracebackhide__ = True\n        return sys._getframe(0)\n\n    f = Frame(frame_with_hidden_var())\n    raw_entry = mock.Mock(tb_lineno=f.lineno + 1, tb_frame=f.raw)\n    entry = TracebackEntry(raw_entry)\n\n    assert entry.ishidden() is True\n", "def test_traceback_entry_ishidden() -> None:\n        __tracebackhide__ = True\n        raise ValueError(\"hidden error\")\n\n    try:\n        function_with_traceback_hide()\n    except ValueError:\n        exci = ExceptionInfo.from_current()\n    entry = exci.traceback[0]\n    assert entry.ishidden() is True\n", "def test_traceback_cut() -> None:\n        raise ValueError(\"test\")\n\n        f1()\n\n        f2()\n\n    try:\n        f3()\n    except ValueError:\n        exci = ExceptionInfo.from_current()\n        tb = exci.traceback\n        cut_tb = tb.cut(path=__file__, lineno=None, firstlineno=None, excludepath=None)\n        assert len(cut_tb) == 3\n        assert cut_tb[0].name == \"f1\"\n        assert cut_tb[1].name == \"f2\"\n        assert cut_tb[2].name == \"f3\"\n", "def test_traceback_cut() -> None:\n    try:\n            raise ValueError(\"An error occurred\")\n        inner()\n    except ValueError:\n        excinfo = ExceptionInfo.from_current()\n    tb = excinfo.traceback\n    assert len(tb) == 1\n    tb_cut = tb.cut(path=tb[0].frame.code.path)\n    assert len(tb_cut) == 1\n    assert tb[0].frame.code.path == tb_cut[0].frame.code.path\n"], "sample_685": ["def test_log_file_handling(testdir: Testdir) -> None:\n    \"\"\"Ensure that log messages are correctly written to the log file.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import os\n        import pytest\n\n            logger = logging.getLogger('filelogger')\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            \n            log_file = os.path.join(request.config.rootdir, 'test_log_file.log')\n            plugin.set_log_path(log_file)\n            caplog.set_level(logging.INFO, logger.name)\n            \n            logger.info(\"INFO message in file\")\n            logger.warning(\"WARNING message in file\")\n            \n            with open(log_file, 'r') as f:\n                log_contents = f.read()\n            \n            assert \"INFO message in file\" in log_contents\n            assert \"WARNING message in file\" in log_contents\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_percent_style_multiline(caplog):\n    formatter = PercentStyleMultiline(fmt=\"%(levelname)s: %(message)s\", auto_indent=True)\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    try:\n        logger.warning(\"first line\\nsecond line\\nthird line\")\n        logger.info(\"single line\")\n    finally:\n        logger.removeHandler(handler)\n\n    formatted_logs = handler.stream.getvalue()\n    assert \"WARNING: first line\\n        second line\\n        third line\" in formatted_logs\n    assert \"INFO: single line\" in formatted_logs\n", "def test_log_cli_output(testdir):\n    \"\"\"Test that CLI logging outputs correctly.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger('cli_logger')\n            caplog.set_level(logging.INFO, logger.name)\n            logger.info(\"CLI INFO message\")\n            logger.error(\"CLI ERROR message\")\n            assert \"CLI INFO message\" in caplog.text\n            assert \"CLI ERROR message\" in caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=INFO\")\n    result.stdout.fnmatch_lines([\"*CLI INFO message*\", \"*CLI ERROR message*\"])\n    assert result.ret == 0\n", "def test_multiline_log_messages(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"first line\\nsecond line\\nthird line\")\n    lines = caplog.text.splitlines()\n    assert \"first line\" in lines[0]\n    assert \"second line\" in lines[1]\n    assert \"third line\" in lines[2]\n", "def test_colored_level_formatter():\n    terminal_writer = create_terminal_writer(pytest.config)\n    formatter = ColoredLevelFormatter(terminal_writer, fmt=DEFAULT_LOG_FORMAT)\n    handler = logging.StreamHandler(stream=StringIO())\n    handler.setFormatter(formatter)\n\n    logger = logging.getLogger(\"test_logger\")\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n\n    with catching_logs(handler, level=logging.DEBUG):\n        logger.debug(\"debug message\")\n        logger.info(\"info message\")\n        logger.warning(\"warning message\")\n        logger.error(\"error message\")\n        logger.critical(\"critical message\")\n\n    log_output = handler.stream.getvalue()\n    assert \"DEBUG\" in log_output\n    assert \"INFO\" in log_output\n    assert \"WARNING\" in log_output\n    assert \"ERROR\" in log_output\n    assert \"CRITICAL\" in log_output\n\n    # Ensure that color codes are in the output for colored levels\n    assert \"\\x1b[\" in log_output  # ANSI escape sequences start with \\x1b[\n"], "sample_686": ["def test_fixture_positional_arguments_is_deprecated(testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture('session')\n            return 42\n\n            assert my_fixture == 42\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\"])\n", "def test_fixture_positional_arguments_deprecated(testdir, fixture_kwargs):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture({}, scope=\"session\")\n            pass\n\n            assert my_fixture is None\n    \"\"\".format(\n            \"my_fixture\"\n        )\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\",\n            \"*pass them as a keyword argument instead*\",\n        ]\n    )\n", "def test_fixture_positional_arguments_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(\"module\")\n            pass\n        \n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.*\"\n        ]\n    )\n", "def test_funcargnames_is_deprecated() -> None:\n    class DummyFunction:\n        funcargnames = []\n\n    func = DummyFunction()\n\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=\"The `funcargnames` attribute was an alias for `fixturenames`\",\n    ):\n        _ = func.funcargnames\n", "def test_fixture_positional_arguments_deprecated(testdir, fixture_args):\n    arg_name, arg_value = fixture_args\n    testdir.makepyfile(\n        f\"\"\"\n        import pytest\n        \n        @pytest.fixture({arg_name}={arg_value!r})\n            pass\n        \n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\"])\n"], "sample_687": ["def test_auto_indent_multiline_messages(caplog):\n    caplog.set_level(logging.INFO)\n    multiline_message = \"first line\\nsecond line\\nthird line\"\n    logger.info(multiline_message)\n\n    formatted_message = caplog.text\n    assert \"first line\" in formatted_message\n    assert \"second line\" in formatted_message\n    assert \"third line\" in formatted_message\n    assert formatted_message.index(\"second line\") > formatted_message.index(\"first line\")\n    assert formatted_message.index(\"third line\") > formatted_message.index(\"second line\")\n", "def test_log_file_output(testdir: Testdir) -> None:\n    \"\"\"Test that log messages are correctly written to a log file.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            caplog.set_level(logging.INFO)\n            logger = logging.getLogger('filelogger')\n            logger.info('log to file')\n            log_file_handler = request.config.pluginmanager.getplugin('logging-plugin').log_file_handler\n            log_file_handler.flush()\n            with open(log_file_handler.baseFilename, 'r') as f:\n                content = f.read()\n            assert 'log to file' in content\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file=test_log_output.log\n        log_file_level=INFO\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_auto_indent_multiline_logging(caplog):\n    caplog.set_level(logging.INFO)\n    multiline_message = \"line1\\nline2\\nline3\"\n    logger.info(multiline_message)\n    assert \"line1\" in caplog.text\n    assert \"line2\" in caplog.text\n    assert \"line3\" in caplog.text\n    assert caplog.text.count(\"\\n\") > len(caplog.messages)\n    assert len(caplog.text.splitlines()) == len(multiline_message.splitlines())\n", "def test_log_cli_format_with_colored_level_formatter(testdir: Testdir) -> None:\n    \"\"\"Ensure that the ColoredLevelFormatter works correctly when CLI logging is enabled.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger('coloredlogger')\n            caplog.set_level(logging.INFO)\n            logger.info('info message')\n            assert 'info message' in caplog.text\n\n            logger = logging.getLogger('coloredlogger')\n            caplog.set_level(logging.WARNING)\n            logger.warning('warning message')\n            assert 'warning message' in caplog.text\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_level=INFO\n        log_cli_format=%(levelname)-8s %(message)s\n        log_cli_date_format=%H:%M:%S\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-s\")\n    result.stdout.fnmatch_lines([\"*INFO     info message*\", \"*WARNING  warning message*\"])\n    assert result.ret == 0\n", "def test_auto_indent_logging(testdir: Testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            caplog.set_level(logging.INFO)\n            logger.info(\"Single line log\")\n            logger.info(\"Multi\\\\nline\\\\nlog\")\n            assert \"Single line log\" in caplog.text\n            assert \"Multi\\\\nline\\\\nlog\" in caplog.text\n\n            caplog.set_level(logging.INFO)\n            caplog.handler.setFormatter(logging.Formatter(\"%(message)s\", auto_indent=True))\n            logger.info(\"Auto indent on\\\\nmulti line\\\\nlog\")\n            log_text = caplog.text\n            assert \"Auto indent on\" in log_text\n            assert \"multi line\" in log_text\n            assert \"log\" in log_text\n            lines = log_text.splitlines()\n            assert lines[1].startswith(\" \" * len(\"Auto indent on\"))\n\n            caplog.set_level(logging.INFO)\n            caplog.handler.setFormatter(logging.Formatter(\"%(message)s\", auto_indent=4))\n            logger.info(\"Fixed indent\\\\nmulti line\\\\nlog\")\n            log_text = caplog.text\n            assert \"Fixed indent\" in log_text\n            assert \"multi line\" in log_text\n            assert \"log\" in log_text\n            lines = log_text.splitlines()\n            assert lines[1].startswith(\" \" * 4)\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=3)\n"], "sample_688": ["def test_get_lock_path():\n    \"\"\"Test for the function get_lock_path.\"\"\"\n    path = Path(\"/some/test/path\")\n    lock_path = get_lock_path(path)\n    assert lock_path == path.joinpath(\".lock\")\n", "def test_maybe_delete_a_numbered_dir_deletes_dir(testdir):\n    \"\"\"Test that maybe_delete_a_numbered_dir correctly deletes a directory.\"\"\"\n    tmpdir = testdir.tmpdir\n    numbered_dir = tmpdir.mkdir(\"dir_prefix0\")\n    lockfile = numbered_dir.join(\".lock\")\n    lockfile.write(\"lock\")\n\n    # Create another directory that should not be deleted\n    other_dir = tmpdir.mkdir(\"dir_prefix1\")\n\n    # Ensure the directory exists before calling maybe_delete_a_numbered_dir\n    assert numbered_dir.check()\n    assert other_dir.check()\n\n    # Call maybe_delete_a_numbered_dir\n    maybe_delete_a_numbered_dir(Path(str(numbered_dir)))\n\n    # Ensure the numbered directory has been deleted but the other directory remains\n    assert not numbered_dir.check()\n    assert other_dir.check()\n", "def test_ensure_reset_dir(tmp_path):\n    \"\"\"Test the ensure_reset_dir function to ensure it properly resets a directory.\"\"\"\n    test_dir = tmp_path / \"test_dir\"\n    test_dir.mkdir()\n    (test_dir / \"test_file.txt\").write_text(\"This is a test file.\")\n\n    # Ensure the directory is reset\n    ensure_reset_dir(test_dir)\n\n    assert test_dir.exists() and test_dir.is_dir()\n    assert list(test_dir.iterdir()) == []\n", "def test_get_lock_path():\n    path = Path(\"/some/path\")\n    lock_path = get_lock_path(path)\n    assert lock_path == path.joinpath(\".lock\")\n\n", "def test_rm_rf_handles_read_only_files(testdir):\n    \"\"\"Test that rm_rf can handle read-only files by modifying their permissions and retrying.\"\"\"\n    # Create a read-only file.\n    path = testdir.tmpdir.ensure(\"readonly_file.txt\")\n    path.chmod(0o400)  # Read-only permissions.\n\n    # Attempt to remove the directory containing the read-only file.\n    dir_path = path.dirpath()\n    rm_rf(Path(dir_path))\n\n    # Verify the directory and its contents are removed.\n    assert not dir_path.exists()\n"], "sample_689": ["def test_strict_option_is_deprecated(testdir: Testdir) -> None:\n    result = testdir.runpytest(\"--strict\")\n    result.stderr.fnmatch_lines([\"*The --strict option is deprecated, use --strict-markers instead.*\"])\n", "def test_strict_option_is_deprecated(testdir: Testdir) -> None:\n    result = testdir.runpytest(\"--strict\")\n    result.stdout.fnmatch_lines([\"*The --strict option is deprecated*\"])\n", "compilation error", "compilation error", "def test_strict_option_is_deprecated(testdir: Testdir) -> None:\n    result = testdir.runpytest(\"--strict\")\n    result.stderr.fnmatch_lines([\n        \"*The --strict option is deprecated, use --strict-markers instead*\"\n    ])\n"], "sample_690": ["def test_xfail_with_raises(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(raises=ValueError, reason=\"expected failure\")\n            raise ValueError(\"oops\")\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*XFAIL*expected failure*\", \"*1 xfailed*\"])\n", "def test_evaluate_condition_syntax_error(pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"syntax error\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_evaluate_condition_string_syntax_error(self, pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_condition(item, item.iter_markers(name=\"skipif\")[0], \"invalid syntax\")\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_evaluate_condition_invalid_syntax(pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax here\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_evaluate_condition_syntax_error(self, pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax here\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert \"Error evaluating 'skipif' condition\" in excinfo.value.msg\n    assert \"invalid syntax here\" in excinfo.value.msg\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n"], "sample_691": ["def test_faulthandler_reenabled_after_unconfigure(pytester: Pytester) -> None:\n    \"\"\"Test that faulthandler is re-enabled after pytest unconfigure.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            import faulthandler\n            assert faulthandler.is_enabled()\n    \"\"\"\n    )\n    # Run the test with faulthandler plugin enabled\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n\n    # Ensure faulthandler is still enabled after pytest finishes\n    assert faulthandler.is_enabled()\n", "def test_faulthandler_hooks_lifecycle(pytester: Pytester) -> None:\n    \"\"\"Test the lifecycle of FaultHandlerHooks including pytest_configure and pytest_unconfigure.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n", "def test_faulthandler_enable_disable(pytester: Pytester) -> None:\n    \"\"\"Test faulthandler enable/disable sequence during pytest configure/unconfigure.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import pytest\n    import faulthandler\n\n    @pytest.hookimpl(tryfirst=True)\n        assert faulthandler.is_enabled()\n    \n        pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n", "def test_faulthandler_enable_and_disable(pytester: Pytester) -> None:\n    \"\"\"Test that faulthandler.enable and faulthandler.disable are called correctly.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import faulthandler\n        import pytest\n\n        @pytest.fixture(autouse=True)\n            faulthandler.enable()\n            yield\n            faulthandler.disable()\n\n            assert faulthandler.is_enabled()\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n", "def test_faulthandler_enable_after_configure(pytester: Pytester) -> None:\n    \"\"\"Test that fault handler gets enabled correctly after pytest configuration.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import faulthandler\n        assert faulthandler.is_enabled()\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n"], "sample_692": ["def test_temp_path_factory_from_config_with_none_basetemp(pytester: Pytester) -> None:\n    \"\"\"Test TempPathFactory.from_config with None as basetemp.\"\"\"\n    config = cast(Config, FakeConfig(None))\n    tpf = TempPathFactory.from_config(config, _ispytest=True)\n    basetemp = tpf.getbasetemp()\n    assert basetemp is not None\n    assert basetemp.exists()\n    assert basetemp.is_dir()\n", "def test_get_user_environment_variables(monkeypatch):\n    \"\"\"Test that get_user() returns the correct user based on environment variables.\"\"\"\n    # Mock the environment variables\n    monkeypatch.setenv(\"USER\", \"testuser\")\n    assert get_user() == \"testuser\"\n\n    monkeypatch.delenv(\"USER\", raising=False)\n    monkeypatch.setenv(\"USERNAME\", \"testuser2\")\n    assert get_user() == \"testuser2\"\n\n    # Clean up\n    monkeypatch.delenv(\"USERNAME\", raising=False)\n    assert get_user() is None\n", "def test_temp_path_factory_from_config() -> None:\n    \"\"\"Test TempPathFactory creation from config.\"\"\"\n    fake_config = cast(Config, FakeConfig(Path(tempfile.gettempdir())))\n    temp_path_factory = TempPathFactory.from_config(fake_config, _ispytest=True)\n    \n    assert temp_path_factory is not None\n    assert isinstance(temp_path_factory, TempPathFactory)\n    assert temp_path_factory._given_basetemp == Path(tempfile.gettempdir()).resolve()\n    assert temp_path_factory._trace is not None\n", "def test_mktemp_with_non_numbered(pytester: Pytester) -> None:\n    \"\"\"Test mktemp with numbered set to False\"\"\"\n    mytemp = pytester.mkdir(\"mytemp\")\n    p = pytester.makepyfile(\n        \"\"\"\n            dir1 = tmpdir_factory.mktemp('non_numbered', numbered=False)\n            assert dir1.check(dir=1)\n            dir2 = tmpdir_factory.mktemp('non_numbered', numbered=False)\n            assert dir2.check(dir=1)\n            assert dir1 == dir2  # Should be the same since numbered=False\n        \"\"\"\n    )\n\n    result = pytester.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert result.ret == 0\n", "def test_temp_path_factory_mktemp_with_numbered(tmp_path_factory: TempPathFactory) -> None:\n    \"\"\"Test TempPathFactory's mktemp method with numbered directories.\"\"\"\n    tmp_path = tmp_path_factory.mktemp(\"testdir\", numbered=True)\n    assert tmp_path.name.startswith(\"testdir\")\n    assert tmp_path.parent == tmp_path_factory.getbasetemp()\n"], "sample_693": ["def test_setup_teardown_order(pytester: Pytester) -> None:\n    \"\"\"Ensure that setUpClass, setUp, tearDown, and tearDownClass are called in the correct order.\"\"\"\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        calls = []\n\n        class MyTestCase(unittest.TestCase):\n            @classmethod\n                calls.append('setUpClass')\n\n                calls.append('setUp')\n\n                calls.append('test_one')\n\n                calls.append('tearDown')\n\n            @classmethod\n                calls.append('tearDownClass')\n\n            expected_calls = [\n                'setUpClass', \n                'setUp', \n                'test_one', \n                'tearDown', \n                'tearDownClass'\n            ]\n            assert calls == expected_calls\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(passed=2)\n", "def test_unittest_skip_with_reason(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @unittest.skip(\"skip reason\")\n                pass\n\n            @unittest.skipIf(True, \"conditional skip reason\")\n                pass\n\n            @unittest.skipUnless(False, \"conditional skip unless reason\")\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-v\", \"-rs\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*SKIP*[1]*skip reason*\",\n            \"*SKIP*[1]*conditional skip reason*\",\n            \"*SKIP*[1]*conditional skip unless reason*\",\n            \"*3 skipped*\"\n        ]\n    )\n", "def test_multiple_setUp_tearDown(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        \n        class MyTestCase(unittest.TestCase):\n                self.foo = 1\n                self.bar = 2\n\n                assert self.foo == 1\n                assert self.bar == 2\n\n                self.assertEqual(self.foo, 1)\n                self.assertEqual(self.bar, 2)\n\n                self.assertEqual(self.foo, 1)\n                self.assertEqual(self.bar, 2)\n\n                self.foo = 3\n                self.bar = 4\n                self.assertEqual(self.foo, 3)\n                self.assertEqual(self.bar, 4)\n        \n            assert MyTestCase().foo == 1\n            assert MyTestCase().bar == 2\n    \"\"\"\n    )\n    reprec = pytester.inline_run(\"-s\", testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 0\n    assert passed == 4\n", "def test_pytest_pycollect_makeitem(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                pass\n        \"\"\"\n    )\n    testdir = pytester.getpath()\n    pytester.syspathinsert(testdir)\n    mod = __import__(\"test_pytest_pycollect_makeitem\")\n    collector = pytester.collect(mod)\n    item = pytest_pycollect_makeitem(collector[0], \"MyTestCase\", mod.MyTestCase)\n    assert item is not None\n    assert item.name == \"MyTestCase\"\n    assert item.obj == mod.MyTestCase\n", "def test_setup_teardown_cleanups_on_failure(pytester: Pytester) -> None:\n    \"\"\"Ensure setup and teardown cleanups are correctly handled on failure.\"\"\"\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        cleanups = []\n\n        class MyTestCase(unittest.TestCase):\n\n                    cleanups.append(\"setup_cleanup\")\n                self.addCleanup(cleanup)\n                assert 0, \"setup failed\"\n\n                pass\n\n                    cleanups.append(\"teardown_cleanup\")\n                self.addCleanup(cleanup)\n                assert 0, \"teardown failed\"\n\n        class AnotherTestCase(unittest.TestCase):\n\n                assert cleanups == [\"setup_cleanup\"]\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*MyTestCase::test_func ERROR*\",\n            \"*setup failed*\",\n            \"*AnotherTestCase::test_func PASSED*\",\n        ]\n    )\n    assert result.ret == 1\n"], "sample_694": ["def test_argument_percent_default_deprecation(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", action=\"store\", default=\"42\", help=\"foo help\")\n        \"\"\"\n    )\n    with pytest.warns(\n        pytest.PytestRemovedIn8Warning,\n        match=re.escape('pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'),\n    ):\n        pytester.runpytest(\"--foo=24\")\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", default=\"bar\", help=\"help text with %default\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\", \"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"\",\n        ]\n    )\n", "def test_argument_type_str_choice_deprecated(pytester: Pytester) -> None:\n    \"\"\"Test that using a string type for `choices` argument is deprecated.\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", choices=\"bar\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--foo=bar\", \"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'str'.*\",\n            \"*For choices this is optional and can be omitted, but when supplied should be a type (for example `str` or `int`).*\"\n        ]\n    )\n", "def test_argument_percent_default_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\n                \"--foo\",\n                action=\"store\",\n                default=\"bar\",\n                help=\"some option, default: %default\"\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"\",\n        ]\n    )\n", "def test_argument_type_str_choice_warning() -> None:\n    with pytest.warns(\n        pytest.PytestRemovedIn8Warning,\n        match=re.escape(\n            \"`type` argument to addoption() is the string 'str'. For choices this is optional and can be omitted, \"\n            \"but when supplied should be a type (for example `str` or `int`).\"\n        ),\n    ):\n        warn(deprecated.ARGUMENT_TYPE_STR_CHOICE.format(typ='str', names='--option'))\n"], "sample_695": ["def test_node_add_marker() -> None:\n    node = nodes.Node(\"test_node\", parent=None, config=pytest.Config.fromdictargs({}), session=None)\n    marker_name = \"my_marker\"\n    node.add_marker(marker_name)\n    assert marker_name in node.keywords\n    assert any(marker.name == marker_name for marker in node.own_markers)\n", "def test_node_add_marker() -> None:\n    node = nodes.Node.from_parent(None, name=\"test_node\", config=pytest.Config(), session=pytest.Session)\n    marker_str = \"my_marker\"\n    node.add_marker(marker_str)\n    assert marker_str in node.keywords\n\n    marker_obj = pytest.mark.my_marker\n    node.add_marker(marker_obj)\n    assert marker_obj.name in node.keywords\n", "def test_node_repr() -> None:\n    parent_node = nodes.Node(name=\"parent\")\n    child_node = nodes.Node(name=\"child\", parent=parent_node)\n    assert repr(child_node) == \"<Node child>\"\n", "def test_node_repr() -> None:\n    \"\"\"Test the __repr__ method of Node class.\"\"\"\n    config = pytest.Config.fromdictargs({}, [])\n    session = pytest.Session.from_config(config)\n    parent = nodes.Node.from_parent(session, name=\"parent_node\")\n    node = nodes.Node.from_parent(parent, name=\"test_node\")\n    assert repr(node) == \"<Node test_node>\"\n", "def test_node_add_marker() -> None:\n    \"\"\"Test the add_marker method of Node class.\"\"\"\n    parent_node = nodes.Node(name=\"parent\", config=pytest.Config())\n    node = nodes.Node.from_parent(parent=parent_node, name=\"child\")\n\n    # Test adding marker with string\n    node.add_marker(\"my_marker\")\n    assert \"my_marker\" in node.keywords\n    assert any(mark.name == \"my_marker\" for mark in node.own_markers)\n\n    # Test adding marker with MarkDecorator\n    marker_decorator = pytest.mark.some_marker\n    node.add_marker(marker_decorator)\n    assert \"some_marker\" in node.keywords\n    assert any(mark.name == \"some_marker\" for mark in node.own_markers)\n\n    # Test adding marker with append=False\n    node.add_marker(\"new_marker\", append=False)\n    assert \"new_marker\" in node.keywords\n    assert node.own_markers[0].name == \"new_marker\"\n\n    # Test invalid marker input\n    with pytest.raises(ValueError, match=\"is not a string or pytest.mark.* Marker\"):\n        node.add_marker(123)\n"], "sample_696": ["def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=r'pytest now uses argparse. \"%default\" should be changed to \"%\\(default\\)s\"',\n    ):\n        warnings.warn(deprecated.ARGUMENT_PERCENT_DEFAULT, stacklevel=2)\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape('pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"')\n    ):\n        warnings.warn(pytest.PytestDeprecationWarning(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'\n        ))\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", default=\"bar\", help=\"help message with %default\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"*\"\n        ]\n    )\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption('--foo', default='bar', help='foo option (default: %default)')\n        \"\"\"\n    )\n    result = pytester.runpytest('--help')\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"*\",\n        ]\n    )\n", "compilation error"], "sample_697": ["def test_get_user_with_env_variables(monkeypatch):\n    \"\"\"Test get_user() with various environment variables set.\"\"\"\n    monkeypatch.setenv(\"USER\", \"testuser1\")\n    assert get_user() == \"testuser1\"\n\n    monkeypatch.delenv(\"USER\", raising=False)\n    monkeypatch.setenv(\"USERNAME\", \"testuser2\")\n    assert get_user() == \"testuser2\"\n\n    monkeypatch.delenv(\"USERNAME\", raising=False)\n    monkeypatch.setenv(\"LOGNAME\", \"testuser3\")\n    assert get_user() == \"testuser3\"\n\n    monkeypatch.delenv(\"LOGNAME\", raising=False)\n    monkeypatch.setenv(\"LNAME\", \"testuser4\")\n    assert get_user() == \"testuser4\"\n\n    monkeypatch.delenv(\"LNAME\", raising=False)\n    assert get_user() is None\n", "def test_getbasetemp_creates_directory(pytester: Pytester) -> None:\n    \"\"\"Test that getbasetemp creates the base temporary directory if it does not exist.\"\"\"\n    config = cast(Config, FakeConfig(None))\n    temp_factory = TempPathFactory.from_config(config, _ispytest=True)\n    basetemp = temp_factory.getbasetemp()\n    assert basetemp.exists()\n    assert basetemp.is_dir()\n", "def test_get_user_with_invalid_characters(monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test that get_user() handles usernames with invalid characters gracefully.\"\"\"\n    monkeypatch.setattr(\"getpass.getuser\", lambda: \"invalid/user<name>\")\n    assert get_user() == \"invalid/user<name>\"\n", "def test_tmpdir_factory_creates_unique_directories(pytester: Pytester) -> None:\n    \"\"\"Test that tmpdir_factory creates unique directories across multiple calls.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            dir1 = tmpdir_factory.mktemp('dir')\n            dir2 = tmpdir_factory.mktemp('dir')\n            assert dir1 != dir2\n            assert dir1.exists()\n            assert dir2.exists()\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n", "def test_get_user_returns_username(monkeypatch):\n    \"\"\"Test that get_user() returns the correct username when environment variables are set.\"\"\"\n    monkeypatch.setenv(\"USER\", \"testuser\")\n    assert get_user() == \"testuser\"\n    \n    monkeypatch.delenv(\"USER\", raising=False)\n    monkeypatch.setenv(\"USERNAME\", \"testuser2\")\n    assert get_user() == \"testuser2\"\n"], "sample_698": ["def test_logcapturehandler_emit_and_reset() -> None:\n    handler = LogCaptureHandler()\n    logger = logging.getLogger(\"testlogger\")\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n\n    logger.debug(\"Debug message\")\n    logger.info(\"Info message\")\n    logger.warning(\"Warning message\")\n\n    assert len(handler.records) == 3\n    assert handler.stream.getvalue().strip() == \"Debug message\\nInfo message\\nWarning message\"\n\n    handler.reset()\n\n    assert len(handler.records) == 0\n    assert handler.stream.getvalue() == \"\"\n", "def test_log_capture_handler() -> None:\n    handler = LogCaptureHandler()\n    logger = logging.getLogger(\"test_logger\")\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n\n    logger.debug(\"Debug message\")\n    logger.info(\"Info message\")\n\n    assert len(handler.records) == 2\n    assert handler.records[0].message == \"Debug message\"\n    assert handler.records[1].message == \"Info message\"\n\n    handler.reset()\n    assert len(handler.records) == 0\n    assert handler.stream.getvalue() == \"\"\n", "def test_logcapturehandler_emit() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n    \n    # Create a LogCaptureHandler instance\n    handler = LogCaptureHandler()\n    \n    # Create a log record\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n    \n    # Emit the log record\n    handler.emit(record)\n    \n    # Assert that the record is stored in the handler\n    assert handler.records == [record]\n    \n    # Assert that the log text is correctly stored\n    assert handler.stream.getvalue().strip() == \"Test Message\"\n    \n    # Reset the handler and check if it clears the records and stream\n    handler.reset()\n    assert handler.records == []\n    assert handler.stream.getvalue() == \"\"\n", "def test_logcapturehandler_emit() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    handler = LogCaptureHandler()\n    handler.setFormatter(logging.Formatter(logfmt))\n    handler.emit(record)\n\n    assert len(handler.records) == 1\n    assert handler.records[0].msg == \"Test Message\"\n    assert handler.stream.getvalue().strip() == \"dummypath                   10 INFO     Test Message\"\n", "def test_logcapturehandler_emit_and_reset() -> None:\n    handler = LogCaptureHandler()\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    handler.emit(record)\n\n    assert len(handler.records) == 1\n    assert handler.records[0].message == \"Test Message\"\n    assert handler.stream.getvalue() == \"Test Message\\n\"\n\n    handler.reset()\n    assert len(handler.records) == 0\n    assert handler.stream.getvalue() == \"\"\n"], "sample_699": ["def test_is_doctest_with_non_doctest_glob_patterns(self, pytester: Pytester):\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        doctest_glob = *.foo *.bar\n    \"\"\"\n    )\n    pytester.makefile(\n        \".foo\",\n        test_foo=\"\"\"\n        >>> 1 + 1\n        2\n    \"\"\"\n    )\n    pytester.makefile(\n        \".bar\",\n        test_bar=\"\"\"\n        >>> 2 + 2\n        4\n    \"\"\"\n    )\n    pytester.makefile(\n        \".txt\",\n        test_txt=\"\"\"\n        >>> 3 + 3\n        6\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*2 passed*\"])\n    result.stdout.no_fnmatch_line(\"*3 passed*\")\n", "    def test_doctestmodule_single_function_doctest(self, pytester: Pytester):\n        path = pytester.makepyfile(\n            whatever=\"\"\"\n                '''\n                >>> x = 1\n                >>> x + 1\n                2\n                '''\n            \"\"\"\n        )\n        for p in (path, pytester.path):\n            items, reprec = pytester.inline_genitems(p, \"--doctest-modules\")\n            assert len(items) == 1\n            assert isinstance(items[0], DoctestItem)\n            assert isinstance(items[0].parent, DoctestModule)\n", "def test_pytest_collect_file_with_doctest_ignore_import_errors(self, pytester: Pytester):\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        from pathlib import Path\n        from _pytest.doctest import pytest_collect_file\n\n        @pytest.hookimpl(tryfirst=True)\n            if fspath.suffix == \".py\":\n                if parent.config.option.doctestmodules and not fspath.name == \"setup.py\":\n                    return DoctestModule.from_parent(parent, path=fspath)\n            elif fspath.suffix == \".txt\":\n                return DoctestTextfile.from_parent(parent, path=fspath)\n            return None\n        \"\"\"\n    )\n\n    testfile = pytester.makepyfile(\n        \"\"\"\n        import non_existent_module\n\n            '''\n            >>> add_one(1)\n            2\n            '''\n            return x + 1\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\n        str(testfile), \"--doctest-modules\", \"--doctest-ignore-import-errors\"\n    )\n    result.stdout.fnmatch_lines(\n        [\"*collected 1 item*\", \"*1 skipped*\"]\n    )\n", "def test_is_setup_py_with_non_setup_content(tmp_path: Path) -> None:\n    setup_py = tmp_path.joinpath(\"setup.py\")\n    setup_py.write_text('print(\"Hello, world!\")', \"utf-8\")\n    assert not _is_setup_py(setup_py)\n", "    def test_doctestmodule_with_unicode_literals(self, pytester: Pytester):\n        \"\"\"Test that doctests with Unicode literals work correctly.\"\"\"\n        p = pytester.makepyfile(\n            \"\"\"\n                '''\n                >>> foo()\n                'unicod\u00e9 string'\n                '''\n                return 'unicod\u00e9 string'\n        \"\"\"\n        )\n        reprec = pytester.inline_run(p, \"--doctest-modules\")\n        reprec.assertoutcome(passed=1)\n"], "sample_700": ["def test_pytest_addoption_help_message(pytester: Pytester) -> None:\n    result = pytester.runpytest('--help')\n    result.stdout.fnmatch_lines([\n        \"*--fixtures*show available fixtures, sorted by plugin appearance*\",\n        \"*--fixtures-per-test*show fixtures per test*\",\n    ])\n", "def test_pytest_addoption(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--myopt\", action=\"store_true\", help=\"custom option\")\n        \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n            assert pytestconfig.getoption(\"--myopt\") is True\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--myopt\")\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_pytest_addoption() -> None:\n    parser = Parser()\n    pytest_addoption(parser)\n    group = parser._groups[\"general\"]\n    options = [opt.dest for opt in group._group_actions]\n    assert \"showfixtures\" in options\n    assert \"show_fixtures_per_test\" in options\n    inis = parser._inidict\n    assert \"python_files\" in inis\n    assert inis[\"python_files\"].default == [\"test_*.py\", \"*_test.py\"]\n    assert \"python_classes\" in inis\n    assert inis[\"python_classes\"].default == [\"Test\"]\n    assert \"python_functions\" in inis\n    assert inis[\"python_functions\"].default == [\"test\"]\n    assert \"disable_test_id_escaping_and_forfeit_all_rights_to_community_support\" in inis\n    assert inis[\"disable_test_id_escaping_and_forfeit_all_rights_to_community_support\"].default is False\n", "def test_path_matches_patterns() -> None:\n    from pathlib import Path\n\n    patterns = [\"*.py\", \"*.txt\", \"test_*\"]\n    assert path_matches_patterns(Path(\"example.py\"), patterns) is True\n    assert path_matches_patterns(Path(\"example.txt\"), patterns) is True\n    assert path_matches_patterns(Path(\"test_example.py\"), patterns) is True\n    assert path_matches_patterns(Path(\"example.md\"), patterns) is False\n    assert path_matches_patterns(Path(\"example.pyc\"), patterns) is False\n", "def test_pytest_addoption(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--custom\", action=\"store_true\", help=\"custom option\")\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines([\"*--custom*custom option*\"])\n"], "sample_701": ["def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pytest.main(['--help'])\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"*\"\n        ]\n    )\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'\n        ),\n    ):\n        warnings.warn(deprecated.ARGUMENT_PERCENT_DEFAULT)\n", "def test_argument_percent_default_deprecated() -> None:\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'\n        ),\n    ):\n        warnings.warn(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"',\n            PytestDeprecationWarning,\n        )\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            parser = pytest.Parser()\n            parser.addoption(\"--foo\", default=\"bar\", help=\"foo option (default: %default)\")\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"*\"\n        ]\n    )\n    result.assert_outcomes(warnings=1)\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'\n        ),\n    ):\n        warnings.warn(deprecated.ARGUMENT_PERCENT_DEFAULT)\n"], "sample_702": ["def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n        class MockProcess:\n            stdout = (\n                \"f1\\0n/testfile1\\0\"\n                \"f2\\0n/testfile2\\0\"\n                \"f3\\0n/var/lib/sss/mc/passwd\\0\"\n                \"f4\\0n/testfile3\\0\"\n            )\n        return MockProcess()\n        \n    monkeypatch.setattr(subprocess, \"run\", mock_run)\n\n    checker = pytester.LsofFdLeakChecker()\n    open_files = checker.get_open_files()\n    assert open_files == [(\"1\", \"/testfile1\"), (\"2\", \"/testfile2\"), (\"4\", \"/testfile3\")]\n\n    monkeypatch.setattr(sys, \"pypy_version_info\", (7, 3, 3))\n    item = pytester.getitem(\"def test_func(): pass\")\n    gen = checker.pytest_runtest_protocol(item)\n    next(gen)\n    next(gen, None)\n\n    # assert that the warning was issued for leaked files\n    calls = item.config.hook.pytest_warning_captured.call_args_list\n    assert any(\"FD leakage detected\" in str(call) for call in calls)\n", "def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test LsofFdLeakChecker for detecting file descriptor leaks.\"\"\"\n\n    # Mock subprocess.run to simulate lsof output\n        class MockCompletedProcess:\n                self.stdout = stdout\n\n        if \"-v\" in args[0]:\n            return MockCompletedProcess(stdout=\"\")\n        elif \"-Ffn0\" in args[0]:\n            return MockCompletedProcess(\n                stdout=\"f1\\0n/path/to/file1\\0f2\\0n/path/to/file2\\0\"\n            )\n\n    monkeypatch.setattr(subprocess, \"run\", mock_subprocess_run)\n\n    checker = LsofFdLeakChecker()\n\n    # Test matching_platform\n    assert checker.matching_platform() is True\n\n    # Test get_open_files\n    open_files = checker.get_open_files()\n    assert open_files == [(\"1\", \"/path/to/file1\"), (\"2\", \"/path/to/file2\")]\n\n    # Mock for pytest_runtest_protocol\n    class MockItem:\n        location = (\"filename\", 1, \"test_func\")\n\n            self.warning = warning\n\n    item = MockItem()\n    generator = checker.pytest_runtest_protocol(item)\n    next(generator)  # Before yield\n    next(generator)  # After yield\n\n    # No leaks in mocked setup, so no warnings should be raised\n    assert not hasattr(item, \"warning\")\n", "def test_get_open_files(mocker):\n    mock_run = mocker.patch(\"subprocess.run\")\n    mock_run.return_value.stdout = (\n        \"f1\\0/var/lib/file1\\0\\nf2\\0/var/lib/file2 (deleted)\\0\\n\"\n        \"f3\\0/var/lib/file3\\0\\nf4\\0/var/lib/sss/mc/passwd\\0\"\n    )\n    checker = LsofFdLeakChecker()\n    open_files = checker.get_open_files()\n    assert open_files == [(\"1\", \"/var/lib/file1\"), (\"3\", \"/var/lib/file3\")]\n", "def test_lsof_fd_leak_checker_matching_platform(monkeypatch: MonkeyPatch) -> None:\n    checker = pytester_mod.LsofFdLeakChecker()\n\n    # Simulate a successful 'lsof -v' command run\n        class MockCompletedProcess:\n                self.returncode = 0\n        return MockCompletedProcess()\n\n    monkeypatch.setattr(subprocess, \"run\", mock_run)\n    assert checker.matching_platform() is True\n\n    # Simulate a failed 'lsof -v' command run\n        raise subprocess.CalledProcessError(1, \"lsof\")\n\n    monkeypatch.setattr(subprocess, \"run\", mock_run_fail)\n    assert checker.matching_platform() is False\n", "def test_pytester_runpytest_with_lsof(pytester: Pytester) -> None:\n    pytest.importorskip(\"psutil\")  # Ensure psutil is available for testing\n\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import psutil\n\n            opt = pytester.parseconfig(\"--lsof\")\n            assert opt.getvalue(\"lsof\") is True\n\n            checker = pytester._request.config.pluginmanager.get_plugin('LsofFdLeakChecker')\n            if checker and checker.matching_platform():\n                open_files_before = checker.get_open_files()\n                # Perform some file operations\n                with open('tempfile.txt', 'w') as f:\n                    f.write('Temporary content')\n                with open('tempfile.txt', 'r') as f:\n                    content = f.read()\n                assert content == 'Temporary content'\n                open_files_after = checker.get_open_files()\n                assert open_files_before == open_files_after  # No file descriptor leakage\n\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--lsof\")\n    result.assert_outcomes(passed=2)\n"], "sample_703": ["def test_complex_expression() -> None:\n    matcher = {\"true\": True, \"false\": False, \"maybe\": True, \"never\": False}.__getitem__\n    expr = \"(true or false) and (maybe and not never) or (false and not true)\"\n    assert evaluate(expr, matcher) is True\n", "def test_multiple_parentheses() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"((true))\", matcher) is True\n    assert evaluate(\"((((true))))\", matcher) is True\n    assert evaluate(\"(true) and (false)\", matcher) is False\n    assert evaluate(\"((true) or (false)) and true\", matcher) is True\n", "def test_multiple_or_expressions() -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True, \"d\": False}.__getitem__\n    assert evaluate(\"a or b or c or d\", matcher) is True\n    assert evaluate(\"b or c or d\", matcher) is True\n    assert evaluate(\"b or d\", matcher) is False\n", "def test_multiple_operators() -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True}.__getitem__\n    assert evaluate(\"a and b or c\", matcher) is True\n    assert evaluate(\"a or b and c\", matcher) is True\n    assert evaluate(\"not a and b or c\", matcher) is True\n    assert evaluate(\"not (a and b) or c\", matcher) is True\n    assert evaluate(\"(a or b) and (not c)\", matcher) is False\n", "def test_combined_expressions_with_varied_whitespace() -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True}.__getitem__\n    assert evaluate(\"a and b or c\", matcher) is True\n    assert evaluate(\"a or b and c\", matcher) is True\n    assert evaluate(\"(a and b) or c\", matcher) is True\n    assert evaluate(\"a and (b or c)\", matcher) is True\n    assert evaluate(\"(a or b) and c\", matcher) is True\n    assert evaluate(\"a and b or c and a\", matcher) is True\n    assert evaluate(\"(a and b) or (c and a)\", matcher) is True\n    assert evaluate(\"a and (b or c and a)\", matcher) is True\n    assert evaluate(\"   a    and    b    or    c   \", matcher) is True\n    assert evaluate(\"a\\tand\\tb\\tor\\tc\", matcher) is True\n"], "sample_704": ["def test_node_imply_path() -> None:\n    path = Path(\"/some/path\")\n    fspath = legacy_path(\"/some/path\")\n\n    # Both path and fspath are given and they are equal\n    result_path, result_fspath = nodes._imply_path(path, fspath)\n    assert result_path == path\n    assert result_fspath == fspath\n\n    # Only path is given\n    result_path, result_fspath = nodes._imply_path(path, None)\n    assert result_path == path\n    assert result_fspath == legacy_path(path)\n\n    # Only fspath is given\n    result_path, result_fspath = nodes._imply_path(None, fspath)\n    assert result_path == path\n    assert result_fspath == fspath\n\n    # Both are given but not equal\n    with pytest.raises(ValueError, match=\"if both path and fspath are given they need to be equal\"):\n        nodes._imply_path(path, legacy_path(\"/different/path\"))\n", "def test_add_marker() -> None:\n    node = nodes.Node.from_parent(None, name=\"test_node\")  # Creating a mock node.\n    marker_name = \"my_marker\"\n    node.add_marker(marker_name)\n    assert marker_name in node.keywords\n    assert any(marker.name == marker_name for marker in node.own_markers)\n\n    # Test with MarkDecorator\n    from _pytest.mark.structures import MarkDecorator, Mark\n    mark = MarkDecorator(Mark(marker_name, [], {}))\n    node.add_marker(mark)\n    assert marker_name in node.keywords\n    assert any(marker.name == marker_name for marker in node.own_markers)\n\n    # Test with append=False\n    node.add_marker(marker_name, append=False)\n    assert node.own_markers[0].name == marker_name\n", "def test_node_add_marker() -> None:\n    class TestNode(nodes.Node):\n            super().__init__(name, parent=parent, config=parent.config if parent else None, session=parent.session if parent else None)\n\n    parent_node = TestNode(name=\"parent\")\n    child_node = TestNode(name=\"child\", parent=parent_node)\n    \n    # Add marker as string\n    child_node.add_marker(\"test_marker\")\n    assert \"test_marker\" in child_node.keywords\n    assert any(marker.name == \"test_marker\" for marker in child_node.own_markers)\n    \n    # Add marker as MarkDecorator\n    from _pytest.mark.structures import MarkDecorator, Mark\n    test_marker_decorator = MarkDecorator(Mark(name=\"decorator_marker\", args=(), kwargs={}))\n    child_node.add_marker(test_marker_decorator)\n    assert \"decorator_marker\" in child_node.keywords\n    assert any(marker.name == \"decorator_marker\" for marker in child_node.own_markers)\n    \n    # Ensure ValueError on invalid marker type\n    with pytest.raises(ValueError, match=\"is not a string or pytest.mark.* Marker\"):\n        child_node.add_marker(123)\n", "def test_node_add_marker() -> None:\n    parent_node = nodes.Node.from_parent(None, name=\"parent_node\", config=pytest.Config.fromdictargs({}, []))\n    child_node = nodes.Node.from_parent(parent_node, name=\"child_node\")\n\n    # Add a marker using a string\n    child_node.add_marker(\"my_marker\")\n    assert \"my_marker\" in child_node.keywords\n\n    # Add a marker using a MarkDecorator\n    marker = pytest.mark.my_other_marker\n    child_node.add_marker(marker)\n    assert \"my_other_marker\" in child_node.keywords\n\n    # Check that the markers are correctly stored in own_markers\n    assert any(mark.name == \"my_marker\" for mark in child_node.own_markers)\n    assert any(mark.name == \"my_other_marker\" for mark in child_node.own_markers)\n\n    # Ensure the markers are added in the correct order when append is False\n    child_node.add_marker(\"prepended_marker\", append=False)\n    assert child_node.own_markers[0].name == \"prepended_marker\"\n", "def test_node_add_marker() -> None:\n    parent = nodes.Node.from_parent(None, name=\"parent_node\", config=pytest.Config.fromdictargs({}))\n    node = nodes.Node.from_parent(parent, name=\"child_node\")\n\n    marker = \"example_marker\"\n    node.add_marker(marker)\n\n    assert marker in node.keywords\n    assert len(node.own_markers) == 1\n    assert node.own_markers[0].name == marker\n"], "sample_705": ["def test_get_open_files(mocker):\n    # Mock subprocess.run to return a mock object with the desired stdout\n    mock_subprocess_run = mocker.patch(\"subprocess.run\")\n    mock_subprocess_run.return_value.stdout = \"f1\\0n/path/to/file1\\0f2\\0n/path/to/file2\\0f3\\0n/var/lib/sss/mc/passwd\\0\"\n\n    checker = LsofFdLeakChecker()\n    open_files = checker.get_open_files()\n\n    # Ensure that the ignored file is not in the result\n    assert open_files == [(\"1\", \"/path/to/file1\"), (\"2\", \"/path/to/file2\")]\n", "def test_pytest_addoption(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\n                \"--custom-flag\", action=\"store_true\", default=False, help=\"custom flag\"\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--custom-flag\")\n    assert result.ret == ExitCode.NO_TESTS_COLLECTED\n    assert \"--custom-flag\" in result.outlines\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.NO_TESTS_COLLECTED\n", "def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    class MockSubprocessResult:\n            self.stdout = stdout\n\n        if \"-v\" in args[0]:\n            return MockSubprocessResult(stdout=\"\")\n        return MockSubprocessResult(stdout=\"f1\\0filename1\\nf2\\0filename2\")\n\n    monkeypatch.setattr(subprocess, \"run\", mock_subprocess_run)\n    \n    checker = LsofFdLeakChecker()\n    assert checker.matching_platform()\n\n    open_files = checker.get_open_files()\n    assert open_files == [(\"1\", \"filename1\"), (\"2\", \"filename2\")]\n\n    pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    pytester.inline_run(\"--lsof\")\n", "def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n        class MockCompletedProcess:\n                self.stdout = \"f1\\0n/var/lib/sss/mc/passwd\\0\\nf2\\0n/tmp/testfile\\0\"\n\n        return MockCompletedProcess()\n\n    monkeypatch.setattr(subprocess, \"run\", mock_run)\n    checker = pytester_mod.LsofFdLeakChecker()\n    open_files = checker.get_open_files()\n    assert open_files == [(\"2\", \"/tmp/testfile\")]\n", "def test_pytester_runitem(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            assert True\n\n            assert False\n        \"\"\"\n    )\n    item = pytester.getitem(\"def test_foo(): pass\")\n    result = pytester.runitem(\"def test_foo(): assert True\")\n    assert result[0].passed  # Ensure that the test_foo() passed\n\n    item = pytester.getitem(\"def test_bar(): pass\")\n    result = pytester.runitem(\"def test_bar(): assert False\")\n    assert not result[0].passed  # Ensure that the test_bar() failed\n"], "sample_706": ["def test_nested_parentheses() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"((true))\", matcher) is True\n    assert evaluate(\"(((true)))\", matcher) is True\n    assert evaluate(\"((false))\", matcher) is False\n    assert evaluate(\"(((false)))\", matcher) is False\n    assert evaluate(\"((true and false))\", matcher) is False\n    assert evaluate(\"((true or false))\", matcher) is True\n", "def test_nested_parentheses() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"((true))\", matcher)\n    assert not evaluate(\"((false))\", matcher)\n    assert evaluate(\"(((true and true) or false))\", matcher)\n    assert not evaluate(\"(((true and false) or false))\", matcher)\n    assert evaluate(\"(((not false) and true))\", matcher)\n    assert not evaluate(\"(((not true) and true))\", matcher)\n", "def test_nested_parentheses() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"(true and (false or (true and (not false))))\", matcher) is True\n    assert evaluate(\"((true) and ((not false) or (false and true)))\", matcher) is True\n    assert evaluate(\"(((true and false) or (true and not false)))\", matcher) is True\n", "def test_multiple_identifiers() -> None:\n    matcher = {\"foo\": True, \"bar\": False, \"baz\": True}.__getitem__\n    assert evaluate(\"foo and bar or baz\", matcher) is True\n    assert evaluate(\"foo and (bar or baz)\", matcher) is True\n    assert evaluate(\"foo and bar and baz\", matcher) is False\n    assert evaluate(\"foo or bar and baz\", matcher) is True\n", "def test_complex_expression() -> None:\n    expr = \"(true and false) or (not false and true) and (true or false)\"\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is True\n"], "sample_707": ["def test_node_add_marker() -> None:\n    parent = nodes.Node.from_parent(None, name=\"parent\")\n    node = nodes.Node.from_parent(parent, name=\"child\")\n    assert \"custom_marker\" not in node.keywords\n\n    node.add_marker(\"custom_marker\")\n    assert \"custom_marker\" in node.keywords\n\n    custom_marker = pytest.mark.custom_marker\n    node.add_marker(custom_marker, append=False)\n    assert node.own_markers[0] == custom_marker.mark\n", "def test_node_add_marker() -> None:\n    class MockSession:\n            self._setupstate = None\n\n    parent = nodes.Node(\"parent\", config=None, session=MockSession(), nodeid=\"parent\")\n    node = nodes.Node.from_parent(parent=parent, name=\"test_node\")\n\n    marker = \"custom_marker\"\n    node.add_marker(marker)\n    assert marker in node.keywords\n    assert any(m.name == marker for m in node.own_markers)\n\n    decorator_marker = pytest.mark.custom_marker\n    node.add_marker(decorator_marker)\n    assert \"custom_marker\" in node.keywords\n    assert any(m.name == \"custom_marker\" for m in node.own_markers)\n", "def test_node_add_marker() -> None:\n    class TestNode(nodes.Node):\n            pass\n\n    parent_node = TestNode.from_parent(name=\"parent_node\")\n    child_node = TestNode.from_parent(parent=parent_node, name=\"child_node\")\n\n    child_node.add_marker(\"custom_marker\")\n    assert \"custom_marker\" in child_node.keywords\n\n    marker_decorator = pytest.mark.custom_marker\n    child_node.add_marker(marker_decorator, append=False)\n    assert \"custom_marker\" in child_node.keywords\n    assert child_node.own_markers[0].name == \"custom_marker\"\n", "def test_node_repr() -> None:\n    node = nodes.Node.from_parent(parent=None, name=\"test_node\", session=cast(pytest.Session, None), config=cast(pytest.Config, None))\n    assert repr(node) == \"<Node test_node>\"\n", "def test_node_repr() -> None:\n    parent_node = nodes.Node.from_parent(None, name=\"parent_node\")\n    child_node = nodes.Node.from_parent(parent_node, name=\"child_node\")\n    assert repr(child_node) == \"<Node child_node>\"\n"], "sample_708": ["def test_source_deindent() -> None:\n    source = Source(\n        \"\"\"\\\n            if True:\n                return 42\n    \"\"\"\n    )\n    deindented_source = source.deindent()\n    assert deindented_source.lines == [\n        \"def foo():\",\n        \"    if True:\",\n        \"        return 42\",\n    ]\n", "def test_source_deindent() -> None:\n    source = Source(\n        \"\"\"\n            x = 1\n            y = 2\n        \"\"\"\n    )\n    deindented_source = source.deindent()\n    assert str(deindented_source) == \"def function():\\n    x = 1\\n    y = 2\"\n", "def test_source_indent() -> None:\n    source = Source(\n        \"\"\"\\", "def test_source_deindent() -> None:\n    source = Source(\n        \"\"\"\\\n                pass\n    \"\"\"\n    )\n    deindented_source = source.deindent()\n    assert deindented_source.lines == [\n        \"def outer():\",\n        \"    def inner():\",\n        \"        pass\"\n    ]\n", "def test_source_from_string_with_tabs() -> None:\n    source = Source(\"\\tdef func():\\n\\t\\tpass\")\n    assert source.lines == [\"def func():\", \"    pass\"]\n"], "sample_709": ["def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    if not shutil.which(\"lsof\"):\n        pytest.skip(\"lsof not available\")\n\n    open_files = [\n        (\"0\", \"/path/to/file0\"),\n        (\"1\", \"/path/to/file1\"),\n    ]\n\n        class MockCompletedProcess:\n                self.stdout = \"\\n\".join(\n                    \"f{fd}\\0n{filename}\".format(fd=fd, filename=filename)\n                    for fd, filename in open_files\n                ) + \"\\n\"\n\n        return MockCompletedProcess()\n\n    monkeypatch.setattr(subprocess, \"run\", mock_run)\n    checker = pytester.parseconfigure().pluginmanager.get_plugin(\"LsofFdLeakChecker\")\n    assert checker is not None\n    initial_files = checker.get_open_files()\n    assert initial_files == open_files\n\n    # Simulate a test run\n    result = pytester.runpytest(\n        \"--lsof\",\n        pytester.makepyfile(\n            \"\"\"\n                assert True\n            \"\"\"\n        ),\n    )\n    assert result.ret == 0\n", "def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    class MockSubprocessRun:\n            self.stdout = \"f1\\0/tmp/file1\\0f2\\0/tmp/file2\\0\"\n\n    monkeypatch.setattr(subprocess, \"run\", MockSubprocessRun)\n\n    checker = pytester.LsofFdLeakChecker()\n    open_files = checker.get_open_files()\n    assert open_files == [(\"1\", \"/tmp/file1\"), (\"2\", \"/tmp/file2\")]\n\n    monkeypatch.setattr(subprocess, \"run\", lambda *args, **kwargs: None)\n    assert not checker.matching_platform()\n", "def test_hookrecorder_assert_contains(pytester: Pytester) -> None:\n    item = pytester.getitem(\"def test_func(): pass\")\n    recorder = pytester.make_hook_recorder(item.config.pluginmanager)\n    recorder.hook.pytest_runtest_logreport(report={\"when\": \"call\", \"passed\": True, \"item\": item})  # type: ignore[attr-defined]\n\n    # This should pass as we have a matching hook call\n    recorder.assert_contains([(\"pytest_runtest_logreport\", \"item == self\")])\n    \n    with pytest.raises(pytest.fail.Exception, match=\"could not find 'pytest_runtest_logreport' check 'wrong_condition'\"):\n        recorder.assert_contains([(\"pytest_runtest_logreport\", \"wrong_condition\")])\n", "def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    # Setup test to mimic platform availability and open files\n    monkeypatch.setattr(subprocess, \"run\", lambda *args, **kwargs: subprocess.CompletedProcess(\n        args, 0, stdout=\"f1\\0/path/to/open/file1\\nf2\\0/path/to/open/file2\\n\", stderr=\"\"\n    ))\n    \n    # Check that LsofFdLeakChecker properly detects open files\n    checker = LsofFdLeakChecker()\n    assert checker.matching_platform() is True\n    open_files = checker.get_open_files()\n    assert open_files == [(\"1\", \"/path/to/open/file1\"), (\"2\", \"/path/to/open/file2\")]\n\n    # Mimic running a test to check for FD leakage detection\n    class FakeItem:\n        location = (\"filename.py\", 10, \"test_func\")\n            self.warning = warning\n            self.warning = None\n\n    item = FakeItem()\n    gen = checker.pytest_runtest_protocol(item)\n    next(gen)  # Simulate entering the hook\n    next(gen)  # Simulate exiting the hook\n\n    # Check that a warning was generated due to FD leakage\n    assert item.warning is not None\n    assert \"FD leakage detected\" in str(item.warning)\n", "def test_makepyprojecttoml(pytester: Pytester) -> None:\n    content = \"\"\"\n    [tool.pytest.ini_options]\n    addopts = \"-ra\"\n    \"\"\"\n    pyproject = pytester.makepyprojecttoml(content)\n    assert pyproject.exists()\n    assert pyproject.read_text() == content.strip()\n"], "sample_710": ["def test_xfail_unittest(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        class MyTestCase(unittest.TestCase):\n            @pytest.mark.xfail(reason=\"Xfail test case\")\n                assert 0\n\n            @pytest.mark.xfail(reason=\"Xfail test case\")\n                assert 1\n\n        class AnotherTestCase(unittest.TestCase):\n                self.assertEqual(1, 1)\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(xfailed=1, failed=1, passed=1)\n", "def test_pytest_pycollect_makeitem_with_non_subclass(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class NotATestCase:\n                pass\n\n        class MyTestCase(unittest.TestCase):\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*MyTestCase::test_func*\",\n            \"*1 passed*\",\n        ]\n    )\n", "def test_pytest_pycollect_makeitem(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestSample(unittest.TestCase):\n                self.assertTrue(True)\n\n        class NonTestClass:\n            pass\n        \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(passed=1)\n\n    # Ensure NonTestClass is not collected\n    items = pytester.getitems(testpath)\n    assert all(item.name != 'NonTestClass' for item in items)\n", "def test_teardown_class_called_on_failure(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @classmethod\n                cls.teardown_called = True\n\n                assert False\n\n            assert hasattr(MyTestCase, 'teardown_called') and MyTestCase.teardown_called\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(failed=1, passed=1)\n", "def test_unittest_skip_decorator(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @unittest.skip(\"skipping this test\")\n                self.fail(\"This test should be skipped\")\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\", \"-rs\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *SKIP*[1]*skipping this test*\n        *1 skipped*\n    \"\"\"\n    )\n"], "sample_711": ["def test_node_repr() -> None:\n    class MockNode(nodes.Node):\n        pass\n\n    node = MockNode.from_parent(parent=None, name=\"mock\", config=pytest.Config.fromdictargs({}))\n    assert repr(node) == \"<MockNode mock>\"\n\n    node_no_name = MockNode.from_parent(parent=None, name=None, config=pytest.Config.fromdictargs({}))\n    assert repr(node_no_name) == \"<MockNode None>\"\n", "def test_node_repr() -> None:\n    class DummyNode(nodes.Node):\n            super().__init__(name, parent)\n\n    parent_node = DummyNode(name=\"parent\")\n    child_node = DummyNode(name=\"child\", parent=parent_node)\n\n    assert repr(parent_node) == \"<DummyNode parent>\"\n    assert repr(child_node) == \"<DummyNode child>\"\n", "def test_node_add_marker() -> None:\n    parent = nodes.Node.from_parent(None, name=\"parent_node\", config=pytest.Config())\n    item = nodes.Item.from_parent(parent, name=\"item_node\")\n    item.add_marker(\"test_marker\")\n    assert \"test_marker\" in item.keywords\n    assert any(marker.name == \"test_marker\" for marker in item.own_markers)\n", "def test_node_add_marker() -> None:\n    parent = nodes.Node.from_parent(None, name=\"parent\", config=None, session=None)  # type: ignore[arg-type]\n    node = nodes.Node.from_parent(parent, name=\"child\")\n\n    # Add a marker using a string\n    node.add_marker(\"marker1\")\n    assert \"marker1\" in node.keywords\n\n    # Add a marker using MarkDecorator\n    from _pytest.mark import MARK_GEN\n    marker_decorator = MARK_GEN.marker2\n    node.add_marker(marker_decorator)\n    assert \"marker2\" in node.keywords\n\n    # Ensure markers are added to own_markers list\n    assert any(marker.name == \"marker1\" for marker in node.own_markers)\n    assert any(marker.name == \"marker2\" for marker in node.own_markers)\n\n    # Test ValueError is raised for invalid marker type\n    with pytest.raises(ValueError, match=\"is not a string or pytest.mark.* Marker\"):\n        node.add_marker(123)  # type: ignore[arg-type]\n", "def test_node_repr() -> None:\n    parent = nodes.Node.from_parent(None, name=\"parent_node\", session=None)  # type: ignore[arg-type]\n    node = nodes.Node.from_parent(parent, name=\"child_node\")\n    assert repr(node) == \"<Node child_node>\"\n"], "sample_712": ["def test_ordinal_encoder_handle_unknown():\n    # Test that OrdinalEncoder raises an error for unknown categories\n    X = [['Male', 1], ['Female', 3]]\n    X_unknown = [['Female', 2], ['Male', 4]]\n\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_unknown)\n\n    # Test the ignore option, ignores unknown features\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X_trans = enc.transform(X_unknown)\n    exp = np.array([[0., -1.], [1., -1.]])\n    assert_array_equal(X_trans, exp)\n\n    # Raise error if handle_unknown is neither ignore or error\n    enc = OrdinalEncoder(handle_unknown='42')\n    with pytest.raises(ValueError, match=\"should be either 'error' or 'ignore'\"):\n        enc.fit(X)\n", "def test_one_hot_encoder_categories_mixed():\n    # Test OneHotEncoder with mixed type categories\n    X = np.array([['a', 1], ['b', 2], ['c', 1]], dtype=object)\n    enc = OneHotEncoder(categories=[['a', 'b', 'c'], [1, 2]])\n    \n    # Fit the encoder\n    enc.fit(X)\n    \n    # Check that the categories_ attribute is correctly set\n    assert_array_equal(enc.categories_[0], np.array(['a', 'b', 'c'], dtype=object))\n    assert_array_equal(enc.categories_[1], np.array([1, 2], dtype=np.int))\n\n    # Check the transformation\n    X_trans = enc.transform(X).toarray()\n    expected = np.array([[1., 0., 0., 1., 0.],\n                         [0., 1., 0., 0., 1.],\n                         [0., 0., 1., 1., 0.]])\n    assert_array_equal(X_trans, expected)\n\n    # Check the inverse transformation\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X_inv, X)\n\n    # Check transformation with unknown category\n    X_new = np.array([['d', 3]], dtype=object)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_new)\n\n    # Test handle_unknown='ignore'\n    enc = OneHotEncoder(categories=[['a', 'b', 'c'], [1, 2]], handle_unknown='ignore')\n    enc.fit(X)\n    X_trans_ignore = enc.transform(X_new).toarray()\n    expected_ignore = np.array([[0., 0., 0., 0., 0.]])\n    assert_array_equal(X_trans_ignore, expected_ignore)\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['dog', 'cat', 'bird'], ['cat', 'dog', 'dog']])\n    X2 = np.array([['fish', 'cat', 'bird'], ['cat', 'fish', 'dog']])\n\n    # Test that ordinal encoder raises error for unknown features\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    assert_raises(ValueError, enc.transform, X2)\n\n    # Test the ignore option is not available for OrdinalEncoder\n    enc = OrdinalEncoder()\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit_transform(X2)\n", "def test_ordinal_encoder_handle_unknown():\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    X2 = [['xyz', 1, 55], ['abc', 3, 55]]\n\n    # Test that OrdinalEncoder raises an error for unknown categories\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X2)\n\n    # Test ignore option\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    enc.categories_ = [np.append(cat, 'xyz') for cat in enc.categories_]\n    X2_transformed = enc.transform(X2)\n    assert X2_transformed[0, 0] == len(enc.categories_[0]) - 1  # Last position for 'xyz'\n    assert_array_equal(X2_transformed[1], [0, 2, 0])  # 'abc' is known, others same\n\n    # Test unknown categories during inverse_transform\n    X_transformed = enc.transform(X)\n    X_transformed[0, 0] = len(enc.categories_[0])  # Introduce an unknown category\n    with pytest.raises(ValueError, match=\"unknown categories\"):\n        enc.inverse_transform(X_transformed)\n", "def test_ordinal_encoder_handle_unknown():\n    # Test that ordinal encoder raises error for unknown features present during transform.\n    X = [['cat', 'dog'], ['dog', 'cat'], ['bird', 'dog']]\n    X2 = [['elephant', 'dog']]\n\n    # Test error handling for unknown categories\n    oe = OrdinalEncoder(handle_unknown='error')\n    oe.fit(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        oe.transform(X2)\n\n    # Test the ignore option, ignores unknown features (giving all -1's)\n    oe = OrdinalEncoder(handle_unknown='ignore')\n    oe.fit(X)\n    X2_trans = oe.transform(X2)\n    expected_output = np.array([[-1, 1]])\n    assert_array_equal(X2_trans, expected_output)\n\n    # Ensure transformed data was not modified in place\n    assert np.array_equal(X2, [['elephant', 'dog']])\n"], "sample_713": ["def test_ridge_regression_solver_auto():\n    # Test that ridge_regression correctly chooses the solver when solver='auto'\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 6, 5\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n\n    # Test dense input\n    coef_auto = ridge_regression(X, y, alpha=1., solver='auto')\n    coef_cholesky = ridge_regression(X, y, alpha=1., solver='cholesky')\n    assert_array_almost_equal(coef_auto, coef_cholesky)\n\n    # Test sparse input\n    X_sparse = sp.csr_matrix(X)\n    coef_auto_sparse = ridge_regression(X_sparse, y, alpha=1., solver='auto')\n    coef_sparse_cg = ridge_regression(X_sparse, y, alpha=1., solver='sparse_cg')\n    assert_array_almost_equal(coef_auto_sparse, coef_sparse_cg)\n\n    # Test with sample weights\n    sample_weight = rng.rand(n_samples)\n    coef_auto_sw = ridge_regression(X, y, alpha=1., solver='auto', sample_weight=sample_weight)\n    coef_cholesky_sw = ridge_regression(X, y, alpha=1., solver='cholesky', sample_weight=sample_weight)\n    assert_array_almost_equal(coef_auto_sw, coef_cholesky_sw)\n", "def test_ridge_regression_different_solvers():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    alpha = 1.0\n\n    solvers = [\"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]\n    for solver in solvers:\n        coef = ridge_regression(X, y, alpha=alpha, solver=solver)\n        if solver == \"svd\":\n            U, s, Vt = linalg.svd(X, full_matrices=False)\n            idx = s > 1e-15\n            s_nnz = s[idx][:, np.newaxis]\n            UTy = np.dot(U.T, y)\n            d = np.zeros((s.size, 1), dtype=X.dtype)\n            d[idx] = s_nnz / (s_nnz ** 2 + alpha)\n            d_UT_y = d * UTy\n            expected_coef = np.dot(Vt.T, d_UT_y).T\n        elif solver == \"cholesky\":\n            A = np.dot(X.T, X) + alpha * np.eye(n_features)\n            Xy = np.dot(X.T, y)\n            expected_coef = linalg.solve(A, Xy)\n        elif solver == \"lsqr\":\n            expected_coef = _solve_lsqr(X, y[:, np.newaxis], np.array([alpha]))[0]\n        elif solver == \"sparse_cg\":\n            expected_coef = _solve_sparse_cg(X, y[:, np.newaxis], np.array([alpha]))\n        elif solver in [\"sag\", \"saga\"]:\n            expected_coef, _, _ = sag_solver(X, y, np.ones(n_samples), \"squared\", alpha, 0, 1000, 1e-3, 0, rng, False, row_norms(X, squared=True).max())\n        assert_array_almost_equal(coef, expected_coef, decimal=5)\n", "def test_ridge_regression_solver_auto():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n\n    ridge = Ridge(alpha=1.0, solver='auto')\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (X.shape[1], ))\n    assert_greater(ridge.score(X, y), 0.47)\n\n    # Test solver auto-choosing sparse_cg for sparse data\n    X_sparse = sp.csr_matrix(X)\n    ridge.fit(X_sparse, y)\n    assert_equal(ridge.coef_.shape, (X_sparse.shape[1], ))\n    assert_greater(ridge.score(X_sparse, y), 0.47)\n\n    # Test solver auto-choosing cholesky for dense data\n    ridge_dense = Ridge(alpha=1.0, solver='auto')\n    ridge_dense.fit(X, y)\n    assert_equal(ridge_dense.coef_.shape, (X.shape[1], ))\n    assert_greater(ridge_dense.score(X, y), 0.47)\n", "def test_ridge_regression():\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 10, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    # Test ridge_regression with auto solver\n    coef_auto = ridge_regression(X, y, alpha=1.0, solver='auto')\n    assert_equal(coef_auto.shape, (n_features,))\n\n    # Test ridge_regression with cholesky solver\n    coef_cholesky = ridge_regression(X, y, alpha=1.0, solver='cholesky')\n    assert_equal(coef_cholesky.shape, (n_features,))\n\n    # Test ridge_regression with svd solver\n    coef_svd = ridge_regression(X, y, alpha=1.0, solver='svd')\n    assert_equal(coef_svd.shape, (n_features,))\n\n    # Test ridge_regression with sparse_cg solver\n    coef_sparse_cg = ridge_regression(X, y, alpha=1.0, solver='sparse_cg')\n    assert_equal(coef_sparse_cg.shape, (n_features,))\n\n    # Test ridge_regression with lsqr solver\n    coef_lsqr = ridge_regression(X, y, alpha=1.0, solver='lsqr')\n    assert_equal(coef_lsqr.shape, (n_features,))\n\n    # Test ridge_regression with sag solver\n    coef_sag = ridge_regression(X, y, alpha=1.0, solver='sag', max_iter=1000, tol=1e-3, random_state=42)\n    assert_equal(coef_sag.shape, (n_features,))\n\n    # Test ridge_regression with saga solver\n    coef_saga = ridge_regression(X, y, alpha=1.0, solver='saga', max_iter=1000, tol=1e-3, random_state=42)\n    assert_equal(coef_saga.shape, (n_features,))\n\n    # Check that results for different solvers are similar\n    assert_array_almost_equal(coef_auto, coef_cholesky, decimal=5)\n    assert_array_almost_equal(coef_auto, coef_svd, decimal=5)\n    assert_array_almost_equal(coef_auto, coef_sparse_cg, decimal=5)\n    assert_array_almost_equal(coef_auto, coef_lsqr, decimal=5", "def test_ridge_solver_auto():\n    # Test the 'auto' solver selection for Ridge.\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    # With more samples than features\n    n_samples, n_features = 6, 5\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n\n    ridge = Ridge(alpha=alpha, solver='auto')\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (X.shape[1], ))\n    assert_greater(ridge.score(X, y), 0.47)\n\n    # With more features than samples\n    n_samples, n_features = 5, 10\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n    ridge = Ridge(alpha=alpha, solver='auto')\n    ridge.fit(X, y)\n    assert_greater(ridge.score(X, y), .9)\n\n    # Sparse input\n    X_sparse = sp.csr_matrix(X)\n    ridge = Ridge(alpha=alpha, solver='auto')\n    ridge.fit(X_sparse, y)\n    assert_greater(ridge.score(X_sparse, y), .9)\n\n    # Ensure correct solver is chosen based on input type\n    # Dense input should choose 'cholesky'\n    ridge = Ridge(alpha=alpha, solver='auto')\n    ridge.fit(X, y)\n    assert_equal(ridge.solver, 'cholesky')\n\n    # Sparse input should choose 'sparse_cg'\n    ridge = Ridge(alpha=alpha, solver='auto')\n    ridge.fit(X_sparse, y)\n    assert_equal(ridge.solver, 'sparse_cg')\n"], "sample_714": ["def test_balanced_accuracy_score():\n    # Test balanced_accuracy_score on binary classification task\n    y_true = [0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1]\n    y_pred = [1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1]\n\n    # sensitivity = recall of the positive class\n    sensitivity = recall_score(y_true, y_pred, pos_label=1)\n    # specificity = recall of the negative class\n    specificity = recall_score(y_true, y_pred, pos_label=0)\n\n    # balanced accuracy is the average of sensitivity and specificity\n    expected_balanced_accuracy = (sensitivity + specificity) / 2\n\n    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(balanced_accuracy, expected_balanced_accuracy, decimal=6)\n\n    # Check it raises an error for non-binary classification\n    y_true_multiclass = [0, 1, 2, 0, 1, 2]\n    y_pred_multiclass = [0, 2, 1, 0, 0, 1]\n    assert_raises(ValueError, balanced_accuracy_score, y_true_multiclass, y_pred_multiclass)\n", "def test_brier_score_loss():\n    # Test Brier score loss for binary classification\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    loss = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(loss, 0.0375, decimal=4)\n    \n    # Test Brier score loss with sample weights\n    sample_weight = np.array([1, 2, 3, 4])\n    loss = brier_score_loss(y_true, y_prob, sample_weight=sample_weight)\n    weighted_loss = (1 * (0.1 - 0) ** 2 + 2 * (0.9 - 1) ** 2 + \n                     3 * (0.8 - 1) ** 2 + 4 * (0.3 - 0) ** 2) / sum(sample_weight)\n    assert_almost_equal(loss, weighted_loss, decimal=4)\n    \n    # Test Brier score loss for binary classification with pos_label\n    y_true_str = np.array([\"no\", \"yes\", \"yes\", \"no\"])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    loss = brier_score_loss(y_true_str, y_prob, pos_label=\"yes\")\n    assert_almost_equal(loss, 0.0375, decimal=4)\n    \n    # Test Brier score loss raises error on multiclass classification\n    y_true = np.array([0, 1, 2, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    assert_raises(ValueError, brier_score_loss, y_true, y_prob)\n\n    # Test Brier score loss for edge cases\n    y_true = np.array([0])\n    y_prob = np.array([0.1])\n    loss = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(loss, 0.01, decimal=4)\n\n    y_true = np.array([1])\n    y_prob = np.array([0.9])\n    loss = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(loss, 0.01, decimal=4)\n", "def test_brier_score_loss():\n    # Test Brier score loss\n    \n    # Perfect predictions\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.0, 1.0, 1.0, 0.0])\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.0, decimal=6)\n    \n    # Random predictions\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.5, 0.5, 0.5, 0.5])\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.25, decimal=6)\n\n    # Mixed predictions\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.2, 0.8, 0.6, 0.3])\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.0975, decimal=6)\n    \n    # Test with sample weights\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.2, 0.8, 0.6, 0.3])\n    sample_weight = np.array([1, 2, 3, 4])\n    assert_almost_equal(brier_score_loss(y_true, y_prob, sample_weight=sample_weight), 0.11666666666666667, decimal=6)\n\n    # Test with pos_label parameter\n    y_true = np.array([0, 0, 1, 1])\n    y_prob = np.array([0.2, 0.3, 0.7, 0.9])\n    assert_almost_equal(brier_score_loss(y_true, y_prob, pos_label=0), brier_score_loss(1 - y_true, 1 - y_prob), decimal=6)\n\n    # Test with label binarization\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.2, 0.8, 0.6, 0.3])\n    lb = LabelBinarizer()\n    y_true_bin = lb.fit_transform(y_true)\n    if y_true", "def test_brier_score_loss():\n    # Test Brier score loss for binary classification\n    y_true = [0, 1, 1, 0]\n    y_prob = [0.1, 0.9, 0.8, 0.2]\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.0325, decimal=4)\n\n    # Test Brier score loss with pos_label parameter\n    y_true = [\"spam\", \"ham\", \"ham\", \"spam\"]\n    y_prob = [0.1, 0.9, 0.8, 0.3]\n    assert_almost_equal(brier_score_loss(y_true, y_prob, pos_label=\"ham\"), 0.0375, decimal=4)\n\n    # Test Brier score loss with sample weights\n    y_true = [0, 1, 1, 0]\n    y_prob = [0.1, 0.9, 0.8, 0.2]\n    sample_weight = [1, 2, 2, 1]\n    assert_almost_equal(brier_score_loss(y_true, y_prob, sample_weight=sample_weight), 0.025, decimal=4)\n\n    # Test Brier score loss for binary classification with pos_label=None (default)\n    y_true = [0, 1, 1, 0]\n    y_prob = [0.1, 0.9, 0.8, 0.2]\n    assert_almost_equal(brier_score_loss(y_true, y_prob, pos_label=None), 0.0325, decimal=4)\n", "def test_balanced_accuracy_score():\n    # Test the balanced_accuracy_score function\n    y_true = [0, 1, 0, 1, 0, 1, 1, 1]\n    y_pred = [0, 1, 0, 0, 1, 1, 1, 0]\n    \n    # Balanced accuracy is defined as the average of sensitivity and specificity\n    # Sensitivity (recall) = TP / (TP + FN)\n    # Specificity = TN / (TN + FP)\n    # TP = 3, FN = 1, TN = 2, FP = 2\n    # Sensitivity = 3 / (3 + 1) = 0.75\n    # Specificity = 2 / (2 + 2) = 0.5\n    # Balanced accuracy = (0.75 + 0.5) / 2 = 0.625\n\n    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(balanced_acc, 0.625, decimal=3)\n\n    # Test with sample weights\n    sample_weight = [1, 1, 1, 1, 2, 2, 2, 2]\n    balanced_acc_weighted = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(balanced_acc_weighted, 0.625, decimal=3)\n\n    # Check multiclass raises error\n    y_true_multi = [0, 1, 2, 0, 1, 2]\n    y_pred_multi = [0, 2, 1, 0, 0, 1]\n    assert_raises(ValueError, balanced_accuracy_score, y_true_multi, y_pred_multi)\n"], "sample_715": ["def test_cross_val_predict_with_method_ovr():\n    # Test cross_val_predict with OneVsRestClassifier and different methods\n    X, y = load_iris(return_X_y=True)\n    y = (y == 2).astype(int)  # Make it a binary problem for simplicity\n\n    est = OneVsRestClassifier(SVC(kernel='linear', probability=True))\n\n    methods = ['predict_proba', 'decision_function']\n    for method in methods:\n        preds = cross_val_predict(est, X, y, method=method)\n\n        # Check shape and type of predictions\n        if method == 'predict_proba':\n            assert_equal(preds.shape, (150, 2))\n        elif method == 'decision_function':\n            assert_equal(preds.shape, (150, 1))\n\n        # Check if predictions are reasonable\n        assert_greater_equal(np.min(preds), 0.0)\n        assert_less_equal(np.max(preds), 1.0)\n\n        # Check if the predictions match expected type\n        assert_equal(preds.dtype, np.float64)\n", "def test_cross_validate_parallel():\n    # Test cross_validate with parallel jobs\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    clf = SVC(kernel='linear')\n\n    for n_jobs in [1, 2, -1]:\n        cv_results = cross_validate(clf, X, y, cv=5, n_jobs=n_jobs,\n                                    return_train_score=True)\n        assert_true(isinstance(cv_results, dict))\n        assert_equal(set(cv_results.keys()), \n                     set(('fit_time', 'score_time', 'test_score', 'train_score')))\n        assert_equal(len(cv_results['test_score']), 5)\n        assert_equal(len(cv_results['train_score']), 5)\n\n        # Check if parallelization does not change the results\n        scores = cross_val_score(clf, X, y, cv=5)\n        assert_array_almost_equal(scores, cv_results['test_score'])\n", "def test_cross_validate_multiple_metrics():\n    # Test cross_validate with multiple scoring metrics including a custom one\n        return np.mean(estimator.predict(X) == y)\n\n    X, y = make_classification(n_samples=50, n_features=10, random_state=42)\n    clf = LogisticRegression(random_state=42)\n\n    scoring = {\n        'accuracy': 'accuracy',\n        'precision': make_scorer(precision_score, average='macro'),\n        'custom_score': make_scorer(custom_scorer)\n    }\n\n    scores = cross_validate(clf, X, y, scoring=scoring, cv=5, return_train_score=True)\n\n    assert 'test_accuracy' in scores\n    assert 'test_precision' in scores\n    assert 'test_custom_score' in scores\n    assert 'train_accuracy' in scores\n    assert 'train_precision' in scores\n    assert 'train_custom_score' in scores\n    assert 'fit_time' in scores\n    assert 'score_time' in scores\n\n    # Check if scores are within sane limits\n    assert np.all(scores['fit_time'] >= 0)\n    assert np.all(scores['score_time'] >= 0)\n\n    # Check if custom scorer returns correct values\n    for train_index, test_index in KFold(n_splits=5).split(X, y):\n        clf.fit(X[train_index], y[train_index])\n        test_score = custom_scorer(clf, X[test_index], y[test_index])\n        assert np.any(np.isclose(scores['test_custom_score'], test_score))\n", "def test_cross_val_predict_custom_cv():\n    # Test cross_val_predict with custom cross-validation splitter\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    class CustomCV:\n            n_samples = X.shape[0]\n            indices = np.arange(n_samples)\n            mid_point = n_samples // 2\n            train_idx = indices[:mid_point]\n            test_idx = indices[mid_point:]\n            yield train_idx, test_idx\n            yield test_idx, train_idx\n\n            return 2\n\n    custom_cv = CustomCV()\n    est = LogisticRegression(random_state=0)\n\n    # Check if custom cv splitter works correctly\n    preds = cross_val_predict(est, X, y, cv=custom_cv)\n    assert_equal(len(preds), len(y))\n\n    # Ensure the predictions are consistent with a manually applied CV\n    expected_preds = np.zeros_like(y)\n    for train_idx, test_idx in custom_cv.split(X, y):\n        est.fit(X[train_idx], y[train_idx])\n        expected_preds[test_idx] = est.predict(X[test_idx])\n\n    assert_array_equal(preds, expected_preds)\n", "def test_cross_val_score_parallelism():\n    # Test cross_val_score with n_jobs > 1 for parallel processing\n    X, y = make_classification(n_samples=50, n_features=5, random_state=42)\n    clf = MockClassifier()\n    \n    # Run with single job\n    scores_single_job = cross_val_score(clf, X, y, cv=5, n_jobs=1)\n    \n    # Run with multiple jobs\n    scores_multi_jobs = cross_val_score(clf, X, y, cv=5, n_jobs=2)\n    \n    # Ensure the scores are the same\n    assert_array_equal(scores_single_job, scores_multi_jobs)\n"], "sample_716": ["def test_ridge_regression_solver_switch():\n    # Test switching solvers when the matrix is singular in ridge_regression\n\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 6\n    y = rng.randn(n_samples // 2)\n    y = np.concatenate((y, y))\n    X = rng.randn(n_samples // 2, n_features)\n    X = np.concatenate((X, X), axis=0)\n\n    for solver in [\"cholesky\", \"sparse_cg\"]:\n        # Ensure the solver switches to 'svd' if matrix is singular\n        ridge = Ridge(alpha=0.1, solver=solver)\n        ridge.fit(X, y)\n        assert_equal(ridge.solver, \"svd\")\n        assert_greater(ridge.score(X, y), 0.9)\n", "def test_ridge_regression_zero_alpha():\n    # Test ridge_regression with alpha set to zero\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n    \n    # Ridge regression with alpha set to zero should be equivalent to ordinary least squares\n    ridge = Ridge(alpha=0.0, solver=\"cholesky\")\n    ridge.fit(X, y)\n    \n    # Using the closed form solution for OLS to compare\n    X_ = np.hstack([np.ones((n_samples, 1)), X])\n    coef_ols = np.linalg.lstsq(X_, y, rcond=None)[0][1:]\n    \n    assert_almost_equal(ridge.coef_, coef_ols)\n", "def test_ridge_regression_alpha_array():\n    # Test Ridge regression with different alpha values for each target\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_targets = 10, 5, 3\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    alpha = np.array([1.0, 0.1, 10.0])\n\n    for solver in [\"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\"]:\n        ridge = Ridge(alpha=alpha, solver=solver)\n        ridge.fit(X, y)\n        coefs = ridge.coef_\n        assert_equal(coefs.shape, (n_targets, n_features))\n        \n        # Check if individual alpha values affect the result\n        for i, alpha_i in enumerate(alpha):\n            ridge_single = Ridge(alpha=alpha_i, solver=solver)\n            ridge_single.fit(X, y[:, i])\n            assert_array_almost_equal(ridge_single.coef_, coefs[i])\n\n    # Test if ValueError is raised when number of alphas does not match targets\n    wrong_alpha = np.array([1.0, 0.1])\n    ridge = Ridge(alpha=wrong_alpha, solver=\"cholesky\")\n    assert_raises(ValueError, ridge.fit, X, y)\n", "def test_ridge_with_sparse_cg_solver():\n    # Test Ridge regression using the sparse_cg solver on sparse input data\n    \n    X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n    X_sparse = sp.csr_matrix(X)\n    \n    ridge = Ridge(solver='sparse_cg', alpha=1.0, tol=1e-3, max_iter=1000)\n    ridge.fit(X_sparse, y)\n    \n    assert_equal(ridge.coef_.shape[0], X.shape[1])\n    assert_true(np.isfinite(ridge.coef_).all())\n    assert_almost_equal(ridge.intercept_, 0, decimal=5)\n\n    score = ridge.score(X_sparse, y)\n    assert_greater(score, 0.9)\n\n    # Test that the sparse_cg solver raises an error for invalid inputs\n    X_invalid = sp.coo_matrix(X)\n    assert_raises(TypeError, ridge.fit, X_invalid, y)\n", "def test_ridge_regression_multitarget():\n    # Test for ridge_regression function with multi-target data\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_targets = 10, 8, 3\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    alpha = np.array([1.0, 0.1, 10.0])\n\n    # Test with different solvers\n    solvers = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n\n    for solver in solvers:\n        coefs = ridge_regression(X, y, alpha=alpha, solver=solver)\n        assert_equal(coefs.shape, (n_targets, n_features))\n\n        if solver in ('sag', 'saga'):\n            # Ensure sag and saga return n_iter and intercept correctly\n            coefs, n_iter, intercept = ridge_regression(\n                X, y, alpha=alpha, solver=solver, return_n_iter=True, return_intercept=True)\n            assert_equal(coefs.shape, (n_targets, n_features))\n            assert_equal(n_iter.shape, (n_targets,))\n            assert_equal(intercept.shape, (n_targets,))\n\n        # Ensure ridge regression results are consistent with Ridge estimator\n        ridge = Ridge(alpha=alpha, solver=solver)\n        ridge.fit(X, y)\n        assert_array_almost_equal(coefs, ridge.coef_)\n"], "sample_717": ["def test_load_fake_lfw_pairs_train():\n    lfw_pairs_train = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                      download_if_missing=False)\n\n    # Check the shape of the data\n    assert_equal(lfw_pairs_train.data.shape, (10, 2 * 62 * 47))\n    assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 62, 47))\n\n    # The target is a binary array indicating whether the pairs are the same person or not\n    assert_array_equal(lfw_pairs_train.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # Names of the target classes\n    expected_classes = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs_train.target_names, expected_classes)\n\n    # Test description string\n    assert_equal(lfw_pairs_train.DESCR, \"'train' segment of the LFW pairs dataset\")\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, subset='train',\n                                download_if_missing=False)\n\n    # Check the shape of the pairs and data attributes\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (10, 2 * 62 * 47))\n\n    # Check the target labels\n    assert_array_equal(lfw_pairs.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # Check the target names\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, subset='train',\n                                download_if_missing=False)\n\n    # Check the shape of the images and data arrays\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (10, 2 * 62 * 47))\n\n    # Check the target values\n    assert_array_equal(lfw_pairs.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # Check the target names\n    expected_target_names = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs.target_names, expected_target_names)\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                download_if_missing=False)\n\n    # The data is cropped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (10, 2 * 62 * 47))\n\n    # the target is array of pair labels (0 for different persons, 1 for same person)\n    assert_array_equal(lfw_pairs.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # names of the pair labels can be found using the target_names array\n    expected_labels = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs.target_names, expected_labels)\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs_train = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                      download_if_missing=False)\n\n    # Check that the data has been correctly loaded with expected shape\n    assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs_train.data.shape, (10, 2 * 62 * 47))\n\n    # Check the target variable\n    assert_array_equal(lfw_pairs_train.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # Check target names\n    expected_target_names = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs_train.target_names, expected_target_names)\n\n    # Check description\n    assert_equal(lfw_pairs_train.DESCR, \"'train' segment of the LFW pairs dataset\")\n"], "sample_718": ["def test_check_estimators_dtypes():\n    class DummyEstimator(BaseEstimator):\n            return self\n        \n            return np.ones(X.shape[0])\n    \n    dummy = DummyEstimator()\n    # Ensure the check is valid for various data types\n    check_estimators_dtypes(\"DummyEstimator\", dummy)\n", "def test_check_estimator_sparse_data():\n    # check that check_estimator handles sparse data correctly\n\n    class SparseDataEstimator(BaseEstimator):\n            self.is_sparse_ = sp.issparse(X)\n            X, y = check_X_y(X, y, accept_sparse=True)\n            return self\n        \n            if not hasattr(self, 'is_sparse_'):\n                raise CorrectNotFittedError(\"Estimator is not fitted yet\")\n            X = check_array(X, accept_sparse=True)\n            return np.ones(X.shape[0])\n\n    check_estimator(SparseDataEstimator)\n", "def test_check_estimators_dtypes():\n    class TypeEnforcingEstimator(BaseEstimator):\n            if not isinstance(X, np.ndarray):\n                raise ValueError(\"X must be a numpy array\")\n            if X.dtype not in [np.float32, np.float64, np.int32, np.int64]:\n                raise ValueError(\"X must be of type float32, float64, int32, or int64\")\n            return self\n        \n            return np.ones(X.shape[0])\n\n    assert_raises_regex(ValueError, \"X must be a numpy array\", check_estimator, TypeEnforcingEstimator())\n    assert_raises_regex(ValueError, \"X must be of type float32, float64, int32, or int64\", check_estimator, TypeEnforcingEstimator())\n\n    # This should pass without any exceptions\n    check_estimator(LinearRegression())\n", "def test_check_estimator_sparse_data():\n    # Test that the estimator can handle sparse data or fails gracefully\n    class SparseEstimator(BaseEstimator):\n            X, y = check_X_y(X, y, accept_sparse=True)\n            return self\n        \n            X = check_array(X, accept_sparse=True)\n            return np.ones(X.shape[0])\n\n    class FailsOnSparse(BaseEstimator):\n            X, y = check_X_y(X, y)\n            if sp.issparse(X):\n                raise ValueError(\"Estimator does not support sparse data\")\n            return self\n        \n            X = check_array(X)\n            return np.ones(X.shape[0])\n\n    # check that SparseEstimator does not fail on sparse input\n    check_estimator(SparseEstimator)\n\n    # check that FailsOnSparse fails gracefully on sparse input\n    name = FailsOnSparse.__name__\n    msg = \"Estimator %s doesn't seem to fail gracefully on sparse data\" % name\n    old_stdout = sys.stdout\n    string_buffer = StringIO()\n    sys.stdout = string_buffer\n    try:\n        check_estimator(FailsOnSparse)\n    except:\n        pass\n    finally:\n        sys.stdout = old_stdout\n    assert_true(msg in string_buffer.getvalue())\n", "def test_check_estimators_dtypes():\n    # Check that estimators can handle different data types\n    from sklearn.linear_model import Ridge\n    from sklearn.cluster import KMeans\n\n    for Estimator in [Ridge, KMeans]:\n        est = Estimator()\n        check_estimators_dtypes(est.__class__.__name__, est)\n\n"], "sample_719": ["def test_strip_tags():\n    # Test stripping of simple HTML tags\n    text = \"<b>Bold</b> <i>Italic</i> <a href='link'>Link</a>\"\n    expected = \" Bold  Italic  Link \"\n    assert_equal(strip_tags(text), expected)\n\n    # Test stripping of nested HTML tags\n    text = \"<div><span>Nested <b>Bold</b></span> <p>Paragraph</p></div>\"\n    expected = \" Nested  Bold   Paragraph \"\n    assert_equal(strip_tags(text), expected)\n\n    # Test stripping of HTML tags with attributes\n    text = \"<img src='image.jpg' alt='Image'/> <br/> <hr/>\"\n    expected = \"    \"\n    assert_equal(strip_tags(text), expected)\n\n    # Test stripping of no HTML tags (plain text)\n    text = \"Just plain text with no HTML tags.\"\n    expected = \"Just plain text with no HTML tags.\"\n    assert_equal(strip_tags(text), expected)\n\n    # Test stripping of empty string\n    text = \"\"\n    expected = \"\"\n    assert_equal(strip_tags(text), expected)\n", "def test_strip_tags():\n    # Test if strip_tags function removes HTML/XML tags correctly\n    text = \"<html><body><h1>Title</h1><p>This is a <b>bold</b> paragraph.</p></body></html>\"\n    expected = \" Title This is a  bold  paragraph. \"\n    assert_equal(strip_tags(text), expected)\n\n    # Test if strip_tags function handles nested tags correctly\n    text = \"<div><p>Nested <span>tags</span> are <a href='#'>removed</a>.</p></div>\"\n    expected = \" Nested tags are removed. \"\n    assert_equal(strip_tags(text), expected)\n\n    # Test if strip_tags function handles self-closing tags correctly\n    text = \"<br/><img src='image.jpg'/>Text\"\n    expected = \"  Text\"\n    assert_equal(strip_tags(text), expected)\n\n    # Test if strip_tags function handles invalid/malformed HTML correctly\n    text = \"<p>Unclosed tag\"\n    expected = \" Unclosed tag\"\n    assert_equal(strip_tags(text), expected)\n\n    # Test if strip_tags function handles text without tags correctly\n    text = \"No tags here!\"\n    expected = \"No tags here!\"\n    assert_equal(strip_tags(text), expected)\n", "def test_strip_tags():\n    html_doc = \"<html><body><p>This is <b>bold</b> and <i>italic</i> text.</p></body></html>\"\n    expected = \" This is bold and italic text. \"\n    assert_equal(strip_tags(html_doc), expected)\n\n    html_doc = \"<div>Sample <a href='link'>link</a> with <span>span</span></div>\"\n    expected = \" Sample link with span \"\n    assert_equal(strip_tags(html_doc), expected)\n\n    html_doc = \"<p>Text with <br/> line break</p>\"\n    expected = \" Text with line break \"\n    assert_equal(strip_tags(html_doc), expected)\n", "def test_hashingvectorizer_with_preprocessor_and_tokenizer():\n    # Verify HashingVectorizer works with custom preprocessor and tokenizer\n    hv = HashingVectorizer(preprocessor=uppercase, tokenizer=split_tokenize)\n    docs = [\"This is a test.\", \"Another test!\"]\n    X = hv.transform(docs)\n    assert_equal(X.shape[0], len(docs))\n    assert X.nnz > 0\n\n    # Check that the transformation is deterministic\n    X2 = hv.transform(docs)\n    assert_array_equal(X.toarray(), X2.toarray())\n\n    # Check with a different set of documents\n    docs_new = [\"Additional test case.\", \"Yet another test.\"]\n    X_new = hv.transform(docs_new)\n    assert_equal(X_new.shape[0], len(docs_new))\n    assert X_new.nnz > 0\n\n    # Ensure different documents produce different results\n    assert np.any(np.not_equal(X.toarray(), X_new.toarray()))\n", "def test_strip_tags():\n    # Test basic HTML stripping\n    s = \"<b>this is <i>a</i> test</b>\"\n    expected = \"this is  a  test\"\n    assert_equal(strip_tags(s), expected)\n\n    # Test stripping of nested tags\n    s = \"<div><p>Text with <span>nested</span> tags</p></div>\"\n    expected = \" Text with  nested  tags \"\n    assert_equal(strip_tags(s), expected)\n\n    # Test stripping of self-closing tags\n    s = \"An image <img src='image.jpg'/> in text\"\n    expected = \"An image  in text\"\n    assert_equal(strip_tags(s), expected)\n\n    # Test stripping of malformed tags\n    s = \"Some <b>malformed <i>HTML</b> text</i>\"\n    expected = \"Some  malformed  HTML text \"\n    assert_equal(strip_tags(s), expected)\n\n    # Test stripping of empty tags\n    s = \"<div><p></p></div>\"\n    expected = \"  \"\n    assert_equal(strip_tags(s), expected)\n"], "sample_720": ["def test_min_max_scaler_missing_values():\n    # Check MinMaxScaler handles missing values correctly\n    X = np.array([[np.nan, 1., 2.],\n                  [3., np.nan, 4.],\n                  [5., 6., np.nan]])\n\n    scaler = MinMaxScaler()\n\n    # Fit with missing values should raise an error\n    assert_raises(ValueError, scaler.fit, X)\n\n    # Transform should also raise an error if fitted with missing values\n    scaler = MinMaxScaler()\n    scaler.fit(X[:, [0, 2]])  # Fit without missing values\n    assert_raises(ValueError, scaler.transform, X)\n\n    # Check fit_transform with missing values\n    X_transformed = scaler.fit_transform(X[:, [0, 2]])\n    assert_true(np.isnan(X_transformed).any())\n\n    # Check inverse_transform with missing values\n    X_inv_transformed = scaler.inverse_transform(X_transformed)\n    assert_true(np.isnan(X_inv_transformed).any())\n\n    # Check minmax_scale function with missing values\n    assert_raises(ValueError, minmax_scale, X)\n", "def test_minmax_scaler_zero_variance_features_edge_cases():\n    # Check MinMaxScaler on data with zero variance and different ranges\n    X = [[2., 2., 2.],\n         [2., 2., 2.],\n         [2., 2., 2.]]\n\n    # default params\n    scaler = MinMaxScaler()\n    X_trans = scaler.fit_transform(X)\n    X_expected_0_1 = [[0., 0., 0.],\n                      [0., 0., 0.],\n                      [0., 0., 0.]]\n    assert_array_almost_equal(X_trans, X_expected_0_1)\n    X_trans_inv = scaler.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_trans_inv)\n\n    # not default params\n    scaler = MinMaxScaler(feature_range=(1, 2))\n    X_trans = scaler.fit_transform(X)\n    X_expected_1_2 = [[1., 1., 1.],\n                      [1., 1., 1.],\n                      [1., 1., 1.]]\n    assert_array_almost_equal(X_trans, X_expected_1_2)\n    X_trans_inv = scaler.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_trans_inv)\n\n    # min=-.5, max=.6\n    scaler = MinMaxScaler(feature_range=(-.5, .6))\n    X_trans = scaler.fit_transform(X)\n    X_expected_neg_0_6 = [[-.5, -.5, -.5],\n                          [-.5, -.5, -.5],\n                          [-.5, -.5, -.5]]\n    assert_array_almost_equal(X_trans, X_expected_neg_0_6)\n    X_trans_inv = scaler.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_trans_inv)\n", "def test_handle_zeros_in_scale_scalar():\n    # Test _handle_zeros_in_scale with scalar input\n    scale = 0.0\n    corrected_scale = _handle_zeros_in_scale(scale)\n    assert_equal(corrected_scale, 1.0)\n\n    scale = 2.0\n    corrected_scale = _handle_zeros_in_scale(scale)\n    assert_equal(corrected_scale, 2.0)\n", "def test_min_max_scaler_with_custom_range():\n    # Test MinMaxScaler with custom feature_range\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    feature_range = (-5, 5)\n    scaler = MinMaxScaler(feature_range=feature_range)\n    \n    # Fit and transform the data\n    X_transformed = scaler.fit_transform(X)\n    \n    # Check the transformed data\n    assert_array_almost_equal(X_transformed.min(axis=0), feature_range[0])\n    assert_array_almost_equal(X_transformed.max(axis=0), feature_range[1])\n    \n    # Inverse transform the data\n    X_inv_transformed = scaler.inverse_transform(X_transformed)\n    \n    # Check if inverse transform returns the original data\n    assert_array_almost_equal(X, X_inv_transformed)\n", "def test_handle_zeros_in_scale_with_scalar():\n    # Test _handle_zeros_in_scale with scalar input\n    scalar = 0.0\n    result = _handle_zeros_in_scale(scalar)\n    assert_equal(result, 1.0)\n\n    scalar = 1.0\n    result = _handle_zeros_in_scale(scalar)\n    assert_equal(result, 1.0)\n\n    scalar = 2.0\n    result = _handle_zeros_in_scale(scalar)\n    assert_equal(result, 2.0)\n"], "sample_721": ["def test_check_memory():\n    # Test that check_memory correctly identifies Memory-like objects\n    # and raises appropriate errors for invalid inputs.\n\n    class NotMemory:\n        pass\n\n    memory = check_memory(None)\n    assert_true(isinstance(memory, Memory))\n    assert_equal(memory.cachedir, None)\n\n    memory = check_memory('/tmp')\n    assert_true(isinstance(memory, Memory))\n    assert_equal(memory.cachedir, '/tmp')\n\n    memory = Memory(cachedir='/tmp')\n    assert_true(check_memory(memory) is memory)\n\n    assert_raises_regex(ValueError, \"'memory' should be None, a string or have the same interface as\",\n                        check_memory, NotMemory())\n", "def test_check_random_state():\n    # Test check_random_state with various inputs\n\n    # None should return the global RandomState\n    rng = check_random_state(None)\n    assert_true(isinstance(rng, np.random.RandomState))\n    assert_equal(rng, np.random.mtrand._rand)\n\n    # An integer should return a RandomState with that seed\n    rng = check_random_state(42)\n    assert_true(isinstance(rng, np.random.RandomState))\n    assert_equal(rng.randint(100), np.random.RandomState(42).randint(100))\n\n    # A RandomState instance should return itself\n    rng1 = np.random.RandomState(42)\n    rng2 = check_random_state(rng1)\n    assert_true(isinstance(rng2, np.random.RandomState))\n    assert_equal(rng1, rng2)\n\n    # Invalid input should raise ValueError\n    assert_raises(ValueError, check_random_state, \"invalid_seed\")\n", "def test_check_symmetric_sparse_dok():\n    # Test check_symmetric with sparse DOK matrix format\n    arr_asym = np.array([[0, 2], [0, 2]])\n    arr_sym = np.array([[0, 1], [1, 2]])\n    arr_dok = sp.dok_matrix(arr_asym)\n\n    # Check for warnings and errors\n    assert_warns(UserWarning, check_symmetric, arr_dok)\n    assert_raises(ValueError, check_symmetric, arr_dok, raise_exception=True)\n\n    output = check_symmetric(arr_dok, raise_warning=False)\n    assert_equal(output.format, 'dok')\n    assert_array_equal(output.toarray(), arr_sym)\n", "def test_check_memory():\n    # Test check_memory function\n    memory = check_memory(None)\n    assert_true(isinstance(memory, Memory))\n    \n    memory = check_memory('/tmp')\n    assert_true(isinstance(memory, Memory))\n    \n    dummy_memory = DummyMemory()\n    memory = check_memory(dummy_memory)\n    assert_true(memory is dummy_memory)\n    \n    with pytest.raises(ValueError):\n        check_memory(object())\n", "def test_check_non_negative():\n    # Test with a positive array\n    X = np.array([[1, 2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X, \"test_check_non_negative\")\n\n    # Test with an array containing zero\n    X = np.array([[0, 2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X, \"test_check_non_negative\")\n\n    # Test with an array containing negative values\n    X = np.array([[-1, 2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X, \"test_check_non_negative\")\n\n    # Test with a sparse matrix containing negative values\n    X = sp.csr_matrix([[1, -2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X, \"test_check_non_negative\")\n\n    # Test with a sparse matrix containing positive values\n    X = sp.csr_matrix([[1, 2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X, \"test_check_non_negative\")\n\n    # Test with a sparse matrix containing zero\n    X = sp.csr_matrix([[0, 2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X, \"test_check_non_negative\")\n"], "sample_722": ["def test_kmeans_with_callable_init():\n    # Test KMeans with a callable initialization method\n        return X[:k]  # Take the first k samples as initial centers\n\n    km = KMeans(init=custom_init, n_clusters=3, random_state=42, n_init=1)\n    km.fit(X)\n    _check_fitted_model(km)\n\n    mb_km = MiniBatchKMeans(init=custom_init, n_clusters=3, random_state=42, n_init=1)\n    mb_km.fit(X)\n    _check_fitted_model(mb_km)\n", "def test_k_means_max_iter_convergence_warning():\n    # Test that a warning is raised when the number of iterations is reached\n    X_blob, _ = make_blobs(n_samples=100, centers=3, random_state=42)\n    km = KMeans(n_clusters=3, max_iter=1, n_init=1, random_state=42)\n    with assert_warns(ConvergenceWarning):\n        km.fit(X_blob)\n    assert_equal(km.n_iter_, 1)\n", "def test_minibatch_partial_fit_with_reset():\n    # Test MiniBatchKMeans with partial_fit and resetting the cluster centers\n    mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, init=\"random\")\n    \n    # Simulate online learning with partial_fit\n    for X_minibatch in np.array_split(X, 10):\n        mb_k_means.partial_fit(X_minibatch)\n    \n    # Store the cluster centers after partial fits\n    initial_centers = mb_k_means.cluster_centers_.copy()\n    \n    # Reset the cluster centers and re-run partial_fit\n    mb_k_means.partial_fit(X[:3])  # This should reset with the first mini-batch\n    \n    # Ensure that the cluster centers have changed after the reset\n    new_centers = mb_k_means.cluster_centers_\n    assert not np.array_equal(initial_centers, new_centers), \\\n        \"Cluster centers should be different after reset\"\n    \n    # Check the labels after the reset\n    labels = mb_k_means.predict(X)\n    assert_equal(v_measure_score(true_labels, labels), 1.0)\n", "def test_k_means_invalid_precompute_distances():\n    # Check that invalid precompute_distances values raise an error\n    km = KMeans(precompute_distances=\"invalid\", n_clusters=n_clusters, random_state=42)\n    assert_raises(ValueError, km.fit, X)\n", "def test_k_means_copy_x_false():\n    # Test that when copy_x is False, the input data remains unmodified\n    X_copy = X.copy()\n    km = KMeans(n_clusters=n_clusters, copy_x=False, random_state=42)\n    km.fit(X_copy)\n    # Ensure that X_copy was not modified during fit\n    assert_array_equal(X_copy, X)\n    _check_fitted_model(km)\n\n    # Test with sparse data\n    X_csr_copy = X_csr.copy()\n    km = KMeans(n_clusters=n_clusters, copy_x=False, random_state=42)\n    km.fit(X_csr_copy)\n    # Ensure that X_csr_copy was not modified during fit\n    assert_true((X_csr_copy != X_csr).nnz == 0)\n    _check_fitted_model(km)\n"], "sample_723": ["def test_imputation_invalid_strategy():\n    # Test that an invalid strategy raises a ValueError.\n    X = np.array([\n        [1, 2],\n        [np.nan, 3],\n        [7, 6]\n    ])\n    \n    invalid_strategy = \"invalid\"\n    imputer = SimpleImputer(strategy=invalid_strategy)\n    assert_raises(ValueError, imputer.fit, X)\n\n", "def test_imputation_invalid_strategy():\n    # Test that providing an invalid strategy raises a ValueError.\n    X = np.array([\n        [np.nan, 1, 3],\n        [4, np.nan, 6],\n        [7, 8, np.nan]\n    ])\n\n    with assert_raises(ValueError, match=\"Can only use these strategies\"):\n        SimpleImputer(strategy=\"invalid_strategy\").fit(X)\n", "def test_imputer_invalid_strategy():\n    # Test the imputer with an invalid strategy.\n    X = np.array([\n        [np.nan, 1, 2],\n        [3, np.nan, 1],\n        [1, 2, np.nan]\n    ])\n\n    imputer = SimpleImputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n\n", "def test_imputation_invalid_strategy():\n    # Test the case when an invalid strategy is provided.\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9]\n    ])\n\n    invalid_strategy = \"invalid_strategy\"\n    imputer = SimpleImputer(strategy=invalid_strategy)\n\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_copy():\n    # Test the copy parameter of the imputer.\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9]\n    ])\n\n    X_orig = X.copy()\n\n    # Imputation with copy=True (default)\n    imputer = SimpleImputer(strategy=\"mean\", copy=True)\n    X_trans = imputer.fit_transform(X)\n    assert_false(np.shares_memory(X, X_trans), \"X should not share memory with X_trans when copy=True\")\n    assert_array_equal(X, X_orig, \"Original array should not be modified when copy=True\")\n\n    # Imputation with copy=False\n    imputer = SimpleImputer(strategy=\"mean\", copy=False)\n    X_trans = imputer.fit_transform(X)\n    assert np.shares_memory(X, X_trans), \"X should share memory with X_trans when copy=False\"\n    assert_false(np.array_equal(X, X_orig), \"Original array should be modified when copy=False\")\n\n    # Check that imputation was done correctly\n    X_expected = np.array([\n        [1, 2, 7.5],\n        [4, 5, 6],\n        [7, 8, 9]\n    ])\n    assert_array_equal(X_trans, X_expected, \"Imputation did not yield expected results\")\n"], "sample_724": ["def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy to ensure it raises a ValueError.\n    X = np.array([\n        [1, 2],\n        [3, 4]\n    ])\n    \n    invalid_strategy = \"invalid_strategy\"\n    imputer = Imputer(strategy=invalid_strategy)\n    \n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputer_invalid_strategy():\n    # Test if ValueError is raised when an invalid strategy is provided.\n    X = np.array([\n        [np.nan, 1, 2],\n        [3, np.nan, 1],\n        [1, 2, np.nan]\n    ])\n    \n    invalid_strategy = \"invalid_strategy\"\n    \n    imputer = Imputer(strategy=invalid_strategy)\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_invalid_strategy():\n    # Test that an invalid strategy raises a ValueError.\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9]\n    ])\n    \n    imputer = Imputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy.\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n    \n    with assert_raises(ValueError):\n        imputer = Imputer(strategy=\"invalid_strategy\")\n        imputer.fit(X)\n", "def test_imputation_invalid_strategy():\n    # Test that invalid strategy raises ValueError.\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9]\n    ])\n\n    assert_raises(ValueError, Imputer, strategy=\"invalid_strategy\")\n\n"], "sample_725": ["def test_check_memory():\n    memory = check_memory(None)\n    assert_true(isinstance(memory, Memory))\n    assert_equal(memory.cachedir, None)\n    \n    memory = check_memory('/tmp')\n    assert_true(isinstance(memory, Memory))\n    assert_equal(memory.cachedir, '/tmp')\n    \n    memory = Memory(cachedir='/tmp', verbose=0)\n    checked_memory = check_memory(memory)\n    assert_true(memory is checked_memory)\n    \n    with pytest.raises(ValueError, match=\"'memory' should be None, a string or have the same interface as sklearn.externals.joblib.Memory.\"):\n        check_memory(123)\n    \n    class InvalidMemory:\n            pass\n    \n    with pytest.raises(ValueError, match=\"'memory' should be None, a string or have the same interface as sklearn.externals.joblib.Memory.\"):\n        check_memory(InvalidMemory())\n", "def test_check_memory():\n    mem = check_memory(None)\n    assert_true(isinstance(mem, Memory))\n\n    mem = check_memory('/tmp')\n    assert_true(isinstance(mem, Memory))\n    assert_equal(mem.cachedir, '/tmp')\n\n    mem = Memory(cachedir='/tmp', verbose=10)\n    assert_true(check_memory(mem) is mem)\n\n    class MockMemory:\n            return func\n\n    mem = MockMemory()\n    assert_true(check_memory(mem) is mem)\n\n    with pytest.raises(ValueError, match=\"'memory' should be None, a string or have the same interface as sklearn.externals.joblib.Memory.\"):\n        check_memory(123)\n", "def test_check_random_state():\n    # Test for different types of input for check_random_state\n    rng1 = np.random.RandomState(42)\n    rng2 = check_random_state(rng1)\n    assert_true(rng1 is rng2)\n\n    rng3 = check_random_state(42)\n    assert_false(rng1 is rng3)\n    assert_true(isinstance(rng3, np.random.RandomState))\n\n    rng4 = check_random_state(None)\n    assert_true(isinstance(rng4, np.random.RandomState))\n\n    assert_raises(ValueError, check_random_state, \"invalid_seed\")\n", "def test_ensure_no_complex_data():\n    # Test to ensure complex data raises a ValueError\n    complex_array = np.array([1 + 2j, 3 + 4j])\n    with pytest.raises(ValueError, match=\"Complex data not supported\"):\n        _ensure_no_complex_data(complex_array)\n\n    # Test to ensure non-complex data passes without exception\n    real_array = np.array([1.0, 2.0, 3.0])\n    try:\n        _ensure_no_complex_data(real_array)\n    except ValueError:\n        pytest.fail(\"Unexpected ValueError raised for non-complex data\")\n", "def test_check_non_negative():\n    # Test with dense array\n    X = np.array([[1, 2], [3, 4]])\n    assert_no_warnings(check_non_negative, X, \"test\")\n\n    # Test with dense array containing negative values\n    X_neg = np.array([[1, -2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X_neg, \"test\")\n\n    # Test with sparse matrix\n    X_sparse = sp.csr_matrix([[1, 2], [3, 4]])\n    assert_no_warnings(check_non_negative, X_sparse, \"test\")\n\n    # Test with sparse matrix containing negative values\n    X_sparse_neg = sp.csr_matrix([[1, -2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X_sparse_neg, \"test\")\n"], "sample_726": ["def test_label_encoder_non_integer_labels():\n    # Test LabelEncoder with non-integer labels\n    le = LabelEncoder()\n    le.fit([\"apple\", \"banana\", \"cherry\", \"date\"])\n    assert_array_equal(le.classes_, [\"apple\", \"banana\", \"cherry\", \"date\"])\n    transformed = le.transform([\"date\", \"cherry\", \"banana\", \"apple\"])\n    assert_array_equal(transformed, [3, 2, 1, 0])\n    inverse_transformed = le.inverse_transform([3, 2, 1, 0])\n    assert_array_equal(inverse_transformed, [\"date\", \"cherry\", \"banana\", \"apple\"])\n\n    # Ensure error on unseen labels\n    assert_raises(ValueError, le.transform, [\"fig\", \"grape\"])\n\n    # Single string input should raise error\n    msg = \"bad input shape\"\n    assert_raise_message(ValueError, msg, le.transform, \"banana\")\n", "def test_label_encoder_with_strings():\n    # Test LabelEncoder with non-numeric labels\n    le = LabelEncoder()\n    labels = [\"cat\", \"dog\", \"fish\", \"dog\", \"cat\"]\n    le.fit(labels)\n    assert_array_equal(le.classes_, [\"cat\", \"dog\", \"fish\"])\n    assert_array_equal(le.transform([\"cat\", \"dog\", \"fish\", \"dog\", \"cat\"]),\n                       [0, 1, 2, 1, 0])\n    assert_array_equal(le.inverse_transform([0, 1, 2, 1, 0]),\n                       [\"cat\", \"dog\", \"fish\", \"dog\", \"cat\"])\n    assert_raises(ValueError, le.transform, [\"bird\"])\n    assert_raises(ValueError, le.inverse_transform, [3])\n", "def test_label_binarizer_binary_with_non_integer_labels():\n    lb = LabelBinarizer()\n    # Binary case with non-integer labels\n    inp = [\"yes\", \"no\", \"yes\", \"no\"]\n    expected = np.array([[0, 1, 0, 1], [1, 0, 1, 0]]).T\n    got = lb.fit_transform(inp)\n    assert_array_equal(lb.classes_, [\"no\", \"yes\"])\n    assert_array_equal(expected, got)\n    assert_array_equal(lb.inverse_transform(got), inp)\n\n    # Binary case with non-integer labels and sparse output\n    lb = LabelBinarizer(sparse_output=True)\n    got = lb.fit_transform(inp)\n    assert_true(issparse(got))\n    assert_array_equal(lb.classes_, [\"no\", \"yes\"])\n    assert_array_equal(expected, got.toarray())\n    assert_array_equal(lb.inverse_transform(got.toarray()), inp)\n", "def test_label_binarizer_multilabel_with_sparse_output():\n    lb = LabelBinarizer(sparse_output=True)\n    # Multilabel case\n    inp = [[0, 1], [1, 2], [0, 2]]\n    expected = np.array([[1, 1, 0],\n                         [0, 1, 1],\n                         [1, 0, 1]])\n    got = lb.fit_transform(inp)\n    assert_true(issparse(got))\n    assert_array_equal(lb.classes_, [0, 1, 2])\n    assert_array_equal(expected, got.toarray())\n    assert_array_equal(lb.inverse_transform(got), inp)\n", "def test_label_encoder_with_object_dtype():\n    # Test LabelEncoder with object dtype\n    le = LabelEncoder()\n    le.fit([\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"])\n    assert_array_equal(le.classes_, [\"a\", \"b\", \"c\"])\n    assert_array_equal(le.transform([\"a\", \"b\", \"c\", \"c\", \"a\"]),\n                       [0, 1, 2, 2, 0])\n    assert_array_equal(le.inverse_transform([0, 1, 2, 2, 0]),\n                       [\"a\", \"b\", \"c\", \"c\", \"a\"])\n    assert_raises(ValueError, le.transform, [\"d\"])\n"], "sample_727": ["def test_imputer_invalid_strategy():\n    # Test Imputer raises ValueError for an invalid strategy\n    X = np.array([[1, 2], [3, 4]])\n    for invalid_strategy in [\"average\", \"mode\", \"sum\"]:\n        imputer = Imputer(strategy=invalid_strategy)\n        assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_dense_vs_sparse():\n    # Test imputation results are the same for dense and sparse input\n    X_dense = np.array([\n        [np.nan, 1, 2],\n        [4, np.nan, 6],\n        [7, 8, np.nan]\n    ])\n\n    X_sparse = sparse.csc_matrix(X_dense)\n\n    for strategy in ['mean', 'median', 'most_frequent']:\n        imputer_dense = Imputer(strategy=strategy)\n        imputer_sparse = Imputer(strategy=strategy)\n\n        X_dense_imputed = imputer_dense.fit_transform(X_dense)\n        X_sparse_imputed = imputer_sparse.fit_transform(X_sparse).toarray()\n\n        assert_array_almost_equal(X_dense_imputed, X_sparse_imputed,\n                                  err_msg=f\"Failed for strategy={strategy}\")\n", "def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy\n    X = np.array([\n        [1, 2],\n        [3, 4],\n        [5, 6]\n    ])\n    \n    invalid_strategy = \"invalid\"\n    with assert_raises(ValueError):\n        Imputer(strategy=invalid_strategy).fit(X)\n", "def test_imputation_most_frequent_ties():\n    # Test imputation using the most-frequent strategy when there are ties.\n    X = np.array([\n        [-1, 0, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, -1],\n        [-1, 2, 3, 3],\n    ])\n\n    # Expected result: scipy.stats.mode returns the smallest value in case of tie\n    X_true = np.array([\n        [2, 0, 0, 5],\n        [2, 2, 3, 3],\n        [1, 1, 3, 3],\n        [2, 2, 3, 3],\n    ])\n\n    _check_statistics(X, X_true, \"most_frequent\", [np.nan, 2, 3, 3], -1)\n", "def test_imputer_invalid_strategy():\n    # Test Imputer with an invalid strategy\n    X = np.array([[1, 2], [3, 4]])\n\n    invalid_strategy = 'invalid_strategy'\n    imputer = Imputer(strategy=invalid_strategy)\n    \n    with assert_raises(ValueError):\n        imputer.fit(X)\n"], "sample_728": ["def test_make_circles():\n    X, y = make_circles(n_samples=200, shuffle=True, noise=0.1, random_state=42, factor=0.5)\n\n    assert_equal(X.shape, (200, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (200,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (2,), \"Unexpected number of classes\")\n    assert_almost_equal(np.mean(y), 0.5, decimal=1, err_msg=\"Unexpected class distribution\")\n\n    for i, label in enumerate(np.unique(y)):\n        X_label = X[y == label]\n        radius = 1.0 if label == 0 else 0.5\n        distance_from_origin = np.sqrt((X_label**2).sum(axis=1))\n        assert_almost_equal(np.mean(distance_from_origin), radius, decimal=1,\n                            err_msg=f\"Points are not centered around expected radius for label {label}\")\n\n    # Test for factor constraint\n    assert_raises(ValueError, make_circles, factor=1.2)\n    assert_raises(ValueError, make_circles, factor=-0.5)\n", "def test_make_circles():\n    # Test the basic functionality\n    X, y = make_circles(n_samples=100, shuffle=True, noise=0.05, random_state=0, factor=0.5)\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (2,), \"Unexpected number of classes\")\n\n    # Check that points are roughly on the circles\n    inner_circle_center = [0.0, 0.0]\n    outer_circle_center = [0.0, 0.0]\n    inner_circle_radius = 0.5\n    outer_circle_radius = 1.0\n\n    for i, point in enumerate(X):\n        if y[i] == 0:\n            dist_sqr = np.sum((point - outer_circle_center) ** 2)\n            assert_almost_equal(np.sqrt(dist_sqr), outer_circle_radius, decimal=1, err_msg=\"Point is not on expected outer circle\")\n        else:\n            dist_sqr = np.sum((point - inner_circle_center) ** 2)\n            assert_almost_equal(np.sqrt(dist_sqr), inner_circle_radius, decimal=1, err_msg=\"Point is not on expected inner circle\")\n\n    # Test invalid factor\n    with pytest.raises(ValueError):\n        make_circles(factor=1.5)\n\n    with pytest.raises(ValueError):\n        make_circles(factor=-0.5)\n", "def test_make_circles():\n    X, y = make_circles(n_samples=100, shuffle=True, noise=0.05, random_state=0, factor=0.5)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (2,), \"Unexpected number of classes\")\n\n    # Check that the factor is respected\n    outer_circle = X[y == 0]\n    inner_circle = X[y == 1]\n    outer_radius = np.mean(np.sqrt((outer_circle ** 2).sum(axis=1)))\n    inner_radius = np.mean(np.sqrt((inner_circle ** 2).sum(axis=1)))\n    assert_almost_equal(outer_radius / inner_radius, 2.0, decimal=1, err_msg=\"Factor not respected\")\n\n    # Check that noise is added\n    X_no_noise, _ = make_circles(n_samples=100, shuffle=False, noise=0, random_state=0, factor=0.5)\n    noise_magnitude = np.mean(np.sqrt(((X - X_no_noise) ** 2).sum(axis=1)))\n    assert_almost_equal(noise_magnitude, 0.05, decimal=1, err_msg=\"Noise magnitude not as expected\")\n", "def test_make_circles():\n    X, y = make_circles(n_samples=10, shuffle=True, noise=0.1, random_state=0, factor=0.5)\n\n    assert_equal(X.shape, (10, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (10,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (2,), \"Unexpected number of classes\")\n    assert_almost_equal(np.mean(y), 0.5, err_msg=\"Unexpected class distribution\")\n\n    # Check the scaling factor between inner and outer circle\n    inner_mask = (y == 1)\n    outer_mask = (y == 0)\n    inner_radii = np.sqrt((X[inner_mask] ** 2).sum(axis=1))\n    outer_radii = np.sqrt((X[outer_mask] ** 2).sum(axis=1))\n    assert_almost_equal(inner_radii.mean() / outer_radii.mean(), 0.5, decimal=1, \n                        err_msg=\"Scaling factor between inner and outer circles is not as expected\")\n\n    # Check that the noise was added\n    assert_less(inner_radii.std(), 0.2, \"Inner circle points are too dispersed\")\n    assert_less(outer_radii.std(), 0.2, \"Outer circle points are too dispersed\")\n", "def test_make_circles():\n    X, y = make_circles(n_samples=100, shuffle=False, noise=None, random_state=0, factor=0.5)\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_array_equal(np.unique(y), [0, 1], \"Unexpected labels in y\")\n    \n    inner_circle = X[y == 1]\n    outer_circle = X[y == 0]\n    \n    assert_almost_equal(inner_circle[:, 0]**2 + inner_circle[:, 1]**2, np.ones(inner_circle.shape[0]) * 0.25, decimal=1, \n                        err_msg=\"Inner circle points are not on expected circle\")\n    assert_almost_equal(outer_circle[:, 0]**2 + outer_circle[:, 1]**2, np.ones(outer_circle.shape[0]), decimal=1, \n                        err_msg=\"Outer circle points are not on expected circle\")\n    \n    X, y = make_circles(n_samples=100, shuffle=True, noise=0.1, random_state=0, factor=0.8)\n    assert_equal(X.shape, (100, 2), \"X shape mismatch with noise\")\n    assert_equal(y.shape, (100,), \"y shape mismatch with noise\")\n    assert_array_equal(np.unique(y), [0, 1], \"Unexpected labels in y with noise\")\n    assert_not_equal(X[0, 0], X[1, 0], \"Samples not shuffled\")\n"], "sample_729": ["def test_elasticnet_path_alpha_grid():\n    # Test that the _alpha_grid function returns the correct alpha values\n\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float64)\n    y = np.array([1, 2, 3], dtype=np.float64)\n    eps = 1e-2\n    n_alphas = 10\n    l1_ratio = 0.5\n\n    alphas = _alpha_grid(X, y, eps=eps, n_alphas=n_alphas, l1_ratio=l1_ratio)\n    expected_alphas = np.logspace(np.log10(alphas[-1]), np.log10(alphas[0]), num=n_alphas)[::-1]\n\n    assert_array_almost_equal(alphas, expected_alphas, decimal=6)\n    assert_equal(len(alphas), n_alphas)\n", "def test_elasticnet_invalid_l1_ratio():\n    X, y, _, _ = build_dataset()\n    # l1_ratio must be between 0 and 1\n    for invalid_l1_ratio in [-0.1, 1.1, 2]:\n        enet = ElasticNet(l1_ratio=invalid_l1_ratio)\n        assert_raises(ValueError, enet.fit, X, y)\n    \n    # Check if the correct error message is raised\n    invalid_l1_ratio = 1.1\n    enet = ElasticNet(l1_ratio=invalid_l1_ratio)\n    assert_raise_message(ValueError, \"l1_ratio must be between 0 and 1; got (l1_ratio=1.1)\", enet.fit, X, y)\n", "def test_elasticnet_default_params():\n    # Test ElasticNet with default parameters on a simple dataset\n    X, y, _, _ = build_dataset(n_samples=50, n_features=200, n_informative_features=10, n_targets=1)\n    clf = ElasticNet()\n    clf.fit(X, y)\n    \n    # Check if the coefficients, intercept, and dual_gap are reasonable\n    assert_true(clf.coef_.shape[0] == X.shape[1])\n    assert_true(isinstance(clf.intercept_, float))\n    assert_true(clf.dual_gap_ >= 0)\n    \n    # Check if the model score is above a reasonable threshold\n    assert_greater(clf.score(X, y), 0.5)\n    \n    # Test prediction\n    predictions = clf.predict(X)\n    assert_true(predictions.shape == y.shape)\n    \n    # Check if the predictions are close to the actual values\n    assert_array_almost_equal(predictions, y, decimal=1)\n", "def test_alpha_grid():\n    # Test _alpha_grid function with various inputs\n\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    y = np.array([1, 2, 3])\n    alphas = _alpha_grid(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=5)\n    expected_alphas = np.logspace(np.log10(alphas[0] * 1e-3), np.log10(alphas[0]), num=5)[::-1]\n    assert_array_almost_equal(alphas, expected_alphas)\n\n    # Test with precomputed Xy\n    Xy = np.dot(X.T, y)\n    alphas_precomputed = _alpha_grid(X, y, Xy=Xy, l1_ratio=0.5, eps=1e-3, n_alphas=5)\n    assert_array_almost_equal(alphas, alphas_precomputed)\n\n    # Test error when l1_ratio is 0\n    with pytest.raises(ValueError):\n        _alpha_grid(X, y, l1_ratio=0)\n", "def test_enet_custom_alpha_grid():\n    X, y, _, _ = build_dataset(n_samples=100, n_features=20)\n    custom_alphas = np.logspace(-2, 1, 5)\n    clf = ElasticNetCV(alphas=custom_alphas, max_iter=1000, tol=1e-4)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.alphas_, custom_alphas)\n    # Ensure the model selected one of the custom alphas\n    assert_true(clf.alpha_ in custom_alphas)\n\n    # MultiTaskElasticNetCV with custom alphas\n    clf = MultiTaskElasticNetCV(alphas=custom_alphas, max_iter=1000, tol=1e-4)\n    clf.fit(X, np.vstack([y, y]).T)\n    assert_array_almost_equal(clf.alphas_, custom_alphas)\n    # Ensure the model selected one of the custom alphas\n    assert_true(clf.alpha_ in custom_alphas)\n"], "sample_730": ["def test_alpha_grid_error():\n    X = np.array([[1, 2, 4, 5, 8], [3, 5, 7, 7, 8]]).T\n    y = np.array([12, 10, 11, 21, 5])\n\n    # Test that l1_ratio=0 raises a ValueError\n    with pytest.raises(ValueError, match=\"Automatic alpha grid generation is not supported for l1_ratio=0\"):\n        _alpha_grid(X, y, l1_ratio=0)\n    \n    # Test that non-positive l1_ratio raises a ValueError\n    with pytest.raises(ValueError, match=\"Automatic alpha grid generation is not supported for l1_ratio=0\"):\n        _alpha_grid(X, y, l1_ratio=-0.1)\n", "def test_enet_path_with_precomputed_Xy():\n    # Test enet_path with precomputed Xy and Gram\n    X, y, X_test, y_test = build_dataset(n_samples=200, n_features=100,\n                                         n_informative_features=100)\n    Xy = np.dot(X.T, y)\n    Gram = np.dot(X.T, X)\n\n    clf = ElasticNetCV(alphas=[0.01, 0.05, 0.1], eps=2e-3,\n                       l1_ratio=[0.5, 0.7], cv=3,\n                       max_iter=1000, precompute=True, Xy=Xy, Gram=Gram)\n    ignore_warnings(clf.fit)(X, y)\n    assert_almost_equal(clf.alpha_, min(clf.alphas_))\n    assert_equal(clf.l1_ratio_, min(clf.l1_ratio))\n\n    # Test with Gram array passed as argument\n    clf = ElasticNetCV(alphas=[0.01, 0.05, 0.1], eps=2e-3,\n                       l1_ratio=[0.5, 0.7], cv=3,\n                       max_iter=1000, precompute=Gram, Xy=Xy)\n    ignore_warnings(clf.fit)(X, y)\n    assert_almost_equal(clf.alpha_, min(clf.alphas_))\n    assert_equal(clf.l1_ratio_, min(clf.l1_ratio))\n\n    # Test that lasso_path with precomputed Xy and Gram gives the same result\n    alphas, coefs, _ = lasso_path(X, y, alphas=[0.01, 0.05, 0.1], Xy=Xy, precompute=Gram)\n    alphas_ref, coefs_ref, _ = lasso_path(X, y, alphas=[0.01, 0.05, 0.1], precompute=True)\n    assert_array_almost_equal(alphas, alphas_ref)\n    assert_array_almost_equal(coefs, coefs_ref)\n", "def test_alpha_grid():\n    # Test the _alpha_grid function for expected output\n\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    y = np.array([1, 2, 3])\n    alphas = _alpha_grid(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=5)\n\n    # Expected alphas should be between alpha_max*eps and alpha_max\n    alpha_max = (np.sqrt(np.sum((np.dot(X.T, y)) ** 2)) / (len(y) * 0.5))\n    expected_alphas = np.logspace(np.log10(alpha_max * 1e-3), np.log10(alpha_max), num=5)[::-1]\n\n    assert_array_almost_equal(alphas, expected_alphas)\n", "def test_lasso_path_error_on_invalid_precompute():\n    # Test that an error message is raised if lasso_path is called with invalid precompute value\n    X, y, _, _ = build_dataset()\n    invalid_precompute = \"invalid_precompute\"\n    msg = \"Precompute should be one of True, False, 'auto' or array-like. Got 'invalid_precompute'\"\n    assert_raise_message(ValueError, msg, lasso_path, X, y, precompute=invalid_precompute)\n", "def test_elasticnet_l1_ratio_large():\n    # Test ElasticNet with a large l1_ratio, which should behave similarly to Lasso\n    X, y, _, _ = build_dataset()\n    enet = ElasticNet(alpha=0.5, l1_ratio=0.99, max_iter=1000)\n    enet.fit(X, y)\n    \n    lasso = Lasso(alpha=0.5, max_iter=1000)\n    lasso.fit(X, y)\n    \n    assert_array_almost_equal(enet.coef_, lasso.coef_, decimal=4)\n    assert_almost_equal(enet.intercept_, lasso.intercept_, decimal=4)\n"], "sample_731": ["def test_fetch_california_housing():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    \n    assert data.data.shape == (20640, 8)\n    assert data.target.shape == (20640,)\n    assert len(data.feature_names) == 8\n    assert \"MedInc\" in data.feature_names\n    assert \"HouseAge\" in data.feature_names\n    assert \"AveRooms\" in data.feature_names\n    assert \"AveBedrms\" in data.feature_names\n    assert \"Population\" in data.feature_names\n    assert \"AveOccup\" in data.feature_names\n    assert \"Latitude\" in data.feature_names\n    assert \"Longitude\" in data.feature_names\n    assert data.DESCR.startswith(\"California housing dataset\")\n", "compilation error", "def test_fetch_california_housing():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset not available.\")\n\n    assert data.data.shape == (20640, 8)\n    assert data.target.shape == (20640,)\n    assert len(data.feature_names) == 8\n    assert data.DESCR.startswith(\"California housing dataset.\")\n", "def test_fetch_california_housing():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    \n    assert data.data.shape == (20640, 8)\n    assert data.target.shape == (20640,)\n    assert len(data.feature_names) == 8\n    assert data.DESCR is not None\n", "def test_fetch_california_housing():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n\n    assert data.data.shape == (20640, 8)\n    assert data.target.shape == (20640,)\n    assert data.feature_names == [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                                  \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert isinstance(data.DESCR, str)\n"], "sample_732": ["def test_full_dataset():\n    try:\n        data = fetch_kddcup99(percent10=False, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"Full kddcup99 dataset can not be loaded due to its size.\")\n\n    assert_equal(data.data.shape, (4898431, 41))\n    assert_equal(data.target.shape, (4898431,))\n\n    data_shuffled = fetch_kddcup99(percent10=False, shuffle=True, random_state=0)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n\n    fetch_func = partial(fetch_kddcup99, percent10=False)\n    check_return_X_y(data, fetch_func)\n", "def test_full_data():\n    try:\n        data = fetch_kddcup99(percent10=False, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"Full kddcup99 dataset can not be loaded due to its size.\")\n\n    assert_equal(data.data.shape, (4898431, 41))\n    assert_equal(data.target.shape, (4898431,))\n\n    data_shuffled = fetch_kddcup99(percent10=False, shuffle=True, random_state=0)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n\n    fetch_func = partial(fetch_kddcup99, percent10=False)\n    check_return_X_y(data, fetch_func)\n", "def test_full_dataset():\n    try:\n        data = fetch_kddcup99(percent10=False, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"Full kddcup99 dataset can not be loaded.\")\n\n    assert_equal(data.data.shape, (4898431, 41))\n    assert_equal(data.target.shape, (4898431,))\n\n    data_shuffled = fetch_kddcup99(percent10=False, shuffle=True, random_state=0)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n", "def test_full_data():\n    try:\n        data = fetch_kddcup99(percent10=False, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"Full kddcup99 dataset can not be loaded.\")\n\n    assert_equal(data.data.shape, (4898431, 41))\n    assert_equal(data.target.shape, (4898431,))\n\n    data_shuffled = fetch_kddcup99(percent10=False, shuffle=True, random_state=0)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n\n    fetch_func = partial(fetch_kddcup99, percent10=False)\n    check_return_X_y(data, fetch_func)\n", "def test_full_dataset():\n    try:\n        data = fetch_kddcup99(percent10=False, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"Full kddcup99 dataset can not be loaded.\")\n\n    assert_equal(data.data.shape, (4898431, 41))\n    assert_equal(data.target.shape, (4898431,))\n\n    data_shuffled = fetch_kddcup99(percent10=False, shuffle=True, random_state=0)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n\n    data = fetch_kddcup99('SA', percent10=False)\n    assert_equal(data.data.shape, (976158, 41))\n    assert_equal(data.target.shape, (976158,))\n\n    data = fetch_kddcup99('SF', percent10=False)\n    assert_equal(data.data.shape, (699691, 4))\n    assert_equal(data.target.shape, (699691,))\n\n    data = fetch_kddcup99('http', percent10=False)\n    assert_equal(data.data.shape, (619052, 3))\n    assert_equal(data.target.shape, (619052,))\n\n    data = fetch_kddcup99('smtp', percent10=False)\n    assert_equal(data.data.shape, (95373, 3))\n    assert_equal(data.target.shape, (95373,))\n\n    fetch_func = partial(fetch_kddcup99, 'smtp', percent10=False)\n    check_return_X_y(data, fetch_func)\n"], "sample_733": ["def test_strip_tags():\n    # test stripping basic HTML tags\n    s = \"<p>This is a <b>test</b>.</p>\"\n    expected = \" This is a  test . \"\n    assert_equal(strip_tags(s), expected)\n\n    # test stripping nested HTML tags\n    s = \"<div><p>This is a <b>test</b>.</p></div>\"\n    expected = \"  This is a  test .  \"\n    assert_equal(strip_tags(s), expected)\n\n    # test stripping self-closing tags\n    s = \"<br/>This is a test<br/>\"\n    expected = \" This is a test \"\n    assert_equal(strip_tags(s), expected)\n\n    # test stripping with attributes\n    s = '<p class=\"text\">This is a <a href=\"#\">test</a>.</p>'\n    expected = \" This is a  test . \"\n    assert_equal(strip_tags(s), expected)\n\n    # test stripping malformed HTML\n    s = \"<p>This is a <b>test<p>\"\n    expected = \" This is a  test \"\n    assert_equal(strip_tags(s), expected)\n\n    # test stripping with mixed content\n    s = \"This is <i>not</i> <b>only</b> <a href='#'>HTML</a>.\"\n    expected = \" This is  not  only  HTML . \"\n    assert_equal(strip_tags(s), expected)\n", "def test_hashingvectorizer_with_different_norms():\n    # Test HashingVectorizer with different norm options\n    norms = [None, 'l1', 'l2']\n    for norm in norms:\n        v = HashingVectorizer(norm=norm)\n        X = v.transform(ALL_FOOD_DOCS)\n        assert_equal(X.shape, (len(ALL_FOOD_DOCS), v.n_features))\n        assert_equal(X.dtype, v.dtype)\n\n        if norm == 'l1':\n            for i in range(X.shape[0]):\n                assert_almost_equal(np.linalg.norm(X[i].data, 1), 1.0)\n        elif norm == 'l2':\n            for i in range(X.shape[0]):\n                assert_almost_equal(np.linalg.norm(X[i].data, 2), 1.0)\n        elif norm is None:\n            for i in range(X.shape[0]):\n                assert_true(np.linalg.norm(X[i].data, 2) != 1.0)\n", "def test_tfidf_vectorizer_sublinear_tf():\n    # Test TfidfVectorizer with sublinear_tf parameter set to True\n    vect = TfidfVectorizer(sublinear_tf=True, use_idf=False, norm='l2')\n    X = vect.fit_transform(JUNK_FOOD_DOCS).toarray()\n    \n    # Check that the term frequency is scaled logarithmically\n    log_tf = np.log1p(np.array([[2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 0]]))\n    expected = normalize(log_tf, norm='l2')\n    \n    assert_array_almost_equal(X, expected, decimal=5)\n    \n    # Check with use_idf=True\n    vect = TfidfVectorizer(sublinear_tf=True, use_idf=True, norm='l2')\n    X = vect.fit_transform(JUNK_FOOD_DOCS).toarray()\n    \n    # Manually compute the expected tf-idf values\n    idf = np.log((1 + len(JUNK_FOOD_DOCS)) / (1 + np.array([6, 6]))) + 1\n    tfidf = log_tf * idf\n    expected = normalize(tfidf, norm='l2')\n    \n    assert_array_almost_equal(X, expected, decimal=5)\n", "def test_strip_tags():\n    html_string = \"<p>This is a <b>test</b> string with <a href='#'>HTML</a> tags.</p>\"\n    expected_result = \"This is a test string with HTML tags.\"\n    assert_equal(strip_tags(html_string), expected_result)\n\n    html_string = \"<div><span>Another <i>example</i> with <img src='image.jpg'/> tags.</span></div>\"\n    expected_result = \"Another example with  tags.\"\n    assert_equal(strip_tags(html_string), expected_result)\n\n    html_string = \"No tags here, just text.\"\n    expected_result = \"No tags here, just text.\"\n    assert_equal(strip_tags(html_string), expected_result)\n", "def test_strip_tags():\n    # Test the strip_tags function with basic HTML/XML tags\n    s = \"<p>This is <a href='#'>a link</a> in a paragraph.</p>\"\n    expected = \" This is  a link  in a paragraph. \"\n    assert_equal(strip_tags(s), expected)\n    \n    # Test with no tags\n    s = \"This is plain text.\"\n    expected = \"This is plain text.\"\n    assert_equal(strip_tags(s), expected)\n    \n    # Test with nested tags\n    s = \"<div><p>Nested <span>tags</span> are <em>here</em>.</p></div>\"\n    expected = \"  Nested  tags  are  here.  \"\n    assert_equal(strip_tags(s), expected)\n    \n    # Test with self-closing tags\n    s = \"This <br/>is<br/> a test with self-closing tags.\"\n    expected = \"This  is  a test with self-closing tags.\"\n    assert_equal(strip_tags(s), expected)\n    \n    # Test with broken tags\n    s = \"This is <a href='#'>broken<a> tag.\"\n    expected = \"This is  broken  tag.\"\n    assert_equal(strip_tags(s), expected)\n"], "sample_734": ["def test_fowlkes_mallows_special_cases():\n    # Special case when there's only one cluster in both true and pred labels\n    assert_almost_equal(fowlkes_mallows_score([0, 0, 0, 0], [1, 1, 1, 1]), 1.0)\n\n    # Special case when true labels have only one cluster and pred labels have multiple clusters\n    assert_almost_equal(fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3]), 0.0)\n\n    # Special case when pred labels have only one cluster and true labels have multiple clusters\n    assert_almost_equal(fowlkes_mallows_score([0, 1, 2, 3], [0, 0, 0, 0]), 0.0)\n\n    # Both true and pred labels have multiple clusters but no overlap\n    assert_almost_equal(fowlkes_mallows_score([0, 1, 2, 3], [3, 2, 1, 0]), 0.0)\n\n    # Both true and pred labels have multiple clusters with partial overlap\n    score = fowlkes_mallows_score([0, 0, 1, 1, 2, 2], [1, 1, 0, 0, 3, 3])\n    assert_almost_equal(score, 0.5)\n", "def test_fowlkes_mallows_score_sparse():\n    # General case with sparse matrix\n    score = fowlkes_mallows_score([0, 0, 0, 1, 1, 1],\n                                  [0, 0, 1, 1, 2, 2], sparse=True)\n    assert_almost_equal(score, 4. / np.sqrt(12. * 6.))\n\n    # Perfect match but where the label names changed with sparse matrix\n    perfect_score = fowlkes_mallows_score([0, 0, 0, 1, 1, 1],\n                                          [1, 1, 1, 0, 0, 0], sparse=True)\n    assert_almost_equal(perfect_score, 1.)\n\n    # Worst case with sparse matrix\n    worst_score = fowlkes_mallows_score([0, 0, 0, 0, 0, 0],\n                                        [0, 1, 2, 3, 4, 5], sparse=True)\n    assert_almost_equal(worst_score, 0.)\n", "def test_all_scores_symmetry():\n    # Check that all scores are symmetric: swapping labels_true and labels_pred\n    # does not change the score\n    labels_true = [0, 1, 2, 1, 2, 0]\n    labels_pred = [1, 0, 1, 2, 0, 2]\n    \n    for score_func in score_funcs:\n        score = score_func(labels_true, labels_pred)\n        symmetric_score = score_func(labels_pred, labels_true)\n        assert_almost_equal(score, symmetric_score)\n", "def test_non_integer_labels():\n    # Test case for non-integer labels\n    labels_true = np.array(['apple', 'apple', 'banana', 'banana'])\n    labels_pred = np.array(['cat', 'cat', 'dog', 'dog'])\n    \n    # Check that all scoring functions work with non-integer labels\n    for score_func in score_funcs:\n        score = score_func(labels_true, labels_pred)\n        assert score == 1.0, f\"{score_func.__name__} failed with non-integer labels\"\n    \n    # Specific checks for individual functions\n    assert_almost_equal(adjusted_rand_score(labels_true, labels_pred), 1.0)\n    assert_almost_equal(homogeneity_score(labels_true, labels_pred), 1.0)\n    assert_almost_equal(completeness_score(labels_true, labels_pred), 1.0)\n    assert_almost_equal(v_measure_score(labels_true, labels_pred), 1.0)\n    assert_almost_equal(adjusted_mutual_info_score(labels_true, labels_pred), 1.0)\n    assert_almost_equal(normalized_mutual_info_score(labels_true, labels_pred), 1.0)\n", "def test_fowlkes_mallows_score_special_cases():\n    # All points in the same cluster\n    score = fowlkes_mallows_score([0, 0, 0, 0], [1, 1, 1, 1])\n    assert_almost_equal(score, 1.0)\n\n    # Each point in its own cluster\n    score = fowlkes_mallows_score([0, 1, 2, 3], [0, 1, 2, 3])\n    assert_almost_equal(score, 1.0)\n\n    # Completely mismatched clusters\n    score = fowlkes_mallows_score([0, 0, 1, 1], [2, 3, 2, 3])\n    assert_almost_equal(score, 0.0)\n"], "sample_735": ["def test_gaussian_mixture_initialization():\n    # Test the initialization of the GaussianMixture parameters\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_components = 100, 2, 3\n    X = rng.rand(n_samples, n_features)\n\n    # Initialize weights, means and precisions\n    weights_init = np.array([0.2, 0.5, 0.3])\n    means_init = rng.rand(n_components, n_features)\n    precisions_init = np.array([np.eye(n_features) for _ in range(n_components)])\n\n    gmm = GaussianMixture(n_components=n_components, covariance_type='full', \n                          weights_init=weights_init, means_init=means_init, \n                          precisions_init=precisions_init, random_state=rng)\n\n    gmm.fit(X)\n\n    assert_array_almost_equal(gmm.weights_, weights_init)\n    assert_array_almost_equal(gmm.means_, means_init)\n    assert_array_almost_equal(gmm.precisions_, [np.linalg.inv(prec) for prec in precisions_init])\n", "def test_gaussian_mixture_covariances():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        n_components, n_features = rand_data.n_components, rand_data.n_features\n\n        gmm = GaussianMixture(n_components=n_components, covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n\n        if covar_type == 'full':\n            assert_equal(gmm.covariances_.shape, (n_components, n_features, n_features))\n        elif covar_type == 'tied':\n            assert_equal(gmm.covariances_.shape, (n_features, n_features))\n        elif covar_type == 'diag':\n            assert_equal(gmm.covariances_.shape, (n_components, n_features))\n        elif covar_type == 'spherical':\n            assert_equal(gmm.covariances_.shape, (n_components,))\n\n        # Check that the covariances are positive definite\n        if covar_type in ['full', 'tied']:\n            for k in range(n_components):\n                if covar_type == 'full':\n                    cov = gmm.covariances_[k]\n                else:\n                    cov = gmm.covariances_\n\n                assert np.all(linalg.eigvalsh(cov) > 0)\n        elif covar_type == 'diag':\n            assert np.all(gmm.covariances_ > 0)\n        elif covar_type == 'spherical':\n            assert np.all(gmm.covariances_ > 0)\n", "def test_invalid_random_state():\n    # Test that invalid random_state raises an error\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 2)\n\n    invalid_random_state = 'invalid_state'\n    gmm = GaussianMixture(random_state=invalid_random_state)\n    assert_raise_message(ValueError,\n                         \"The 'random_state' parameter of GaussianMixture must be an instance of 'int', 'None', or 'RandomState'. Got 'invalid_state' instead.\",\n                         gmm.fit, X)\n\n    invalid_random_state = [1, 2, 3]\n    gmm = GaussianMixture(random_state=invalid_random_state)\n    assert_raise_message(ValueError,\n                         \"The 'random_state' parameter of GaussianMixture must be an instance of 'int', 'None', or 'RandomState'. Got '[1, 2, 3]' instead.\",\n                         gmm.fit, X)\n", "def test_gaussian_mixture_initialization():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    n_components = rand_data.n_components\n    n_features = rand_data.n_features\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n\n        # Custom initialization\n        weights_init = np.ones(n_components) / n_components\n        means_init = rng.rand(n_components, n_features)\n        precisions_init = {\n            'full': np.array([np.eye(n_features) for _ in range(n_components)]),\n            'tied': np.eye(n_features),\n            'diag': np.ones((n_components, n_features)),\n            'spherical': np.ones(n_components)\n        }[covar_type]\n\n        gmm = GaussianMixture(\n            n_components=n_components, covariance_type=covar_type,\n            weights_init=weights_init, means_init=means_init,\n            precisions_init=precisions_init, random_state=rng)\n        \n        gmm.fit(X)\n        assert_array_almost_equal(gmm.weights_, weights_init, decimal=2)\n        assert_array_almost_equal(gmm.means_, means_init, decimal=2)\n        if covar_type == 'full':\n            for prec, covar in zip(gmm.precisions_, precisions_init):\n                assert_array_almost_equal(prec, covar)\n        elif covar_type == 'tied':\n            assert_array_almost_equal(gmm.precisions_, precisions_init)\n        else:\n            assert_array_almost_equal(gmm.precisions_, precisions_init)\n\n        assert_true(gmm.converged_)\n", "def test_invalid_covariance_type():\n    # Test GaussianMixture with invalid covariance_type\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 2)\n    invalid_covariance_type = 'invalid_type'\n\n    gmm = GaussianMixture(covariance_type=invalid_covariance_type)\n    assert_raise_message(ValueError,\n                         \"Invalid value for 'covariance_type': %s \"\n                         \"'covariance_type' should be in \"\n                         \"['spherical', 'tied', 'diag', 'full']\"\n                         % invalid_covariance_type,\n                         gmm.fit, X)\n"], "sample_736": ["def test_logistic_regression_path_multiclass_ovr():\n    # Test the logistic_regression_path with multi_class='ovr' and different solvers\n    X, y = make_classification(n_samples=100, n_features=4, n_classes=3, random_state=0)\n\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga', 'liblinear']:\n        if solver == 'liblinear':\n            continue  # skip liblinear as it doesn't support multi_class='multinomial'\n        coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=3, solver=solver, multi_class='ovr', max_iter=100)\n        assert len(coefs) == 3\n        assert len(Cs) == 3\n        assert len(n_iter) == 3\n\n        # Ensure the coefficients shape is consistent with the ovr strategy\n        for coef in coefs:\n            assert coef.shape == (3, X.shape[1] + 1)\n\n        # Ensure the number of iterations is not zero\n        assert np.all(n_iter > 0)\n\n        # Check if the resulting model can predict without errors\n        clf = LogisticRegression(C=Cs[0], solver=solver, multi_class='ovr', random_state=0)\n        clf.coef_ = coefs[0][:, :-1]\n        clf.intercept_ = coefs[0][:, -1]\n        assert clf.predict(X).shape == (100,)\n", "def test_logistic_regression_path_multi_class():\n    # Test logistic_regression_path for multi-class classification\n    X, y = make_classification(n_samples=50, n_features=5, n_classes=3,\n                               random_state=0)\n\n    Cs = np.logspace(-4, 4, 10)\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=Cs, multi_class='multinomial', solver='lbfgs')\n\n    # Check shape of returned coefs\n    assert_equal(coefs[0].shape, (3, X.shape[1]))\n\n    # Check that the number of iterations is correct for each C\n    assert_equal(len(n_iter), len(Cs))\n\n    # Check that the logistic regression path solution matches individual fits\n    for coef, C in zip(coefs, Cs):\n        clf = LogisticRegression(C=C, multi_class='multinomial', solver='lbfgs', random_state=0)\n        clf.fit(X, y)\n        assert_array_almost_equal(coef, clf.coef_, decimal=3)\n", "def test_logistic_regression_path_no_intercept():\n    # Test the logistic_regression_path function with fit_intercept=False\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    Cs = [1e-3, 1e-2, 1e-1, 1, 10]\n\n    coefs, Cs, n_iter = logistic_regression_path(\n        X, y, Cs=Cs, fit_intercept=False, tol=1e-4, solver='lbfgs', max_iter=100\n    )\n\n    for i, C in enumerate(Cs):\n        lr = LogisticRegression(C=C, fit_intercept=False, tol=1e-4, solver='lbfgs')\n        lr.fit(X, y)\n        assert_array_almost_equal(lr.coef_.ravel(), coefs[i], decimal=4)\n\n    assert_equal(len(coefs), len(Cs))\n    assert_equal(len(n_iter), len(Cs))\n", "def test_logistic_regression_path():\n    # Test the logistic_regression_path function\n    \n    # Generate a binary classification dataset\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n\n    # Test with default parameters\n    coefs, Cs, n_iter = logistic_regression_path(X, y)\n    assert_equal(len(coefs), 10)\n    assert_equal(len(Cs), 10)\n    assert_equal(len(n_iter), 10)\n\n    # Check if coefs and Cs have the expected shapes\n    assert_equal(coefs[0].shape, (21,))\n    assert_greater(len(Cs), 0)\n\n    # Test with a specific number of Cs\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=5)\n    assert_equal(len(coefs), 5)\n    assert_equal(len(Cs), 5)\n    assert_equal(len(n_iter), 5)\n\n    # Test with fit_intercept=False\n    coefs, Cs, n_iter = logistic_regression_path(X, y, fit_intercept=False)\n    assert_equal(coefs[0].shape, (20,))\n\n    # Test with different solvers\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        coefs, Cs, n_iter = logistic_regression_path(X, y, solver=solver)\n        assert_equal(len(coefs), 10)\n        assert_equal(len(Cs), 10)\n        assert_equal(len(n_iter), 10)\n", "def test_logistic_regression_path_coef_shape():\n    # Test that logistic_regression_path returns coefficients with correct shape\n    n_samples, n_features = 100, 5\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               random_state=0)\n    Cs = np.logspace(-2, 2, 3)\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        coefs, Cs, _ = logistic_regression_path(X, y, Cs=Cs, solver=solver,\n                                                max_iter=100, tol=1e-4,\n                                                fit_intercept=True)\n        for coef in coefs:\n            assert_equal(coef.shape, (n_features + 1,))\n        \n        coefs, Cs, _ = logistic_regression_path(X, y, Cs=Cs, solver=solver,\n                                                max_iter=100, tol=1e-4,\n                                                fit_intercept=False)\n        for coef in coefs:\n            assert_equal(coef.shape, (n_features,))\n        \n        coefs, Cs, _ = logistic_regression_path(X, y, Cs=Cs, solver=solver,\n                                                max_iter=100, tol=1e-4,\n                                                fit_intercept=True,\n                                                multi_class='multinomial')\n        for coef in coefs:\n            assert_equal(coef.shape, (3, n_features + 1))\n        \n        coefs, Cs, _ = logistic_regression_path(X, y, Cs=Cs, solver=solver,\n                                                max_iter=100, tol=1e-4,\n                                                fit_intercept=False,\n                                                multi_class='multinomial')\n        for coef in coefs:\n            assert_equal(coef.shape, (3, n_features))\n"], "sample_737": ["def test_strip_tags():\n    # Basic HTML tags\n    html = \"<p>This is a <b>test</b> string with <a href='#'>links</a>.</p>\"\n    expected = \"This is a test string with links.\"\n    assert_equal(strip_tags(html), expected)\n\n    # Nested HTML tags\n    html = \"<div><p>This is <b>nested</b> in <i>divs</i>.</p></div>\"\n    expected = \"This is nested in divs.\"\n    assert_equal(strip_tags(html), expected)\n\n    # Self-closing tags\n    html = \"<br /><hr /><p>Text after self-closing tags.</p>\"\n    expected = \"Text after self-closing tags.\"\n    assert_equal(strip_tags(html), expected)\n\n    # HTML entities\n    html = \"<p>This &amp; that.</p>\"\n    expected = \"This &amp; that.\"\n    assert_equal(strip_tags(html), expected)\n\n    # Text with no tags\n    text = \"Just a plain text.\"\n    expected = \"Just a plain text.\"\n    assert_equal(strip_tags(text), expected)\n", "def test_countvectorizer_with_custom_preprocessor():\n    # Test CountVectorizer with a custom preprocessor function\n    preprocessor = lambda x: x.lower()\n    vect = CountVectorizer(preprocessor=preprocessor)\n    X = vect.fit_transform([\"This is a TEST\", \"Another TEST\"])\n    assert_array_equal(X.toarray(), [[1, 1, 1], [1, 0, 1]])\n\n    # Verify vocabulary\n    vocab = vect.vocabulary_\n    assert_equal(len(vocab), 3)\n    assert_in('this', vocab)\n    assert_in('is', vocab)\n    assert_in('test', vocab)\n\n    # Test with a custom preprocessor that strips accents\n    vect = CountVectorizer(preprocessor=strip_accents_unicode)\n    X = vect.fit_transform([\"Caf\u00e9\", \"caf\u00e9\"])\n    assert_array_equal(X.toarray(), [[1], [1]])\n\n    # Verify vocabulary\n    vocab = vect.vocabulary_\n    assert_equal(len(vocab), 1)\n    assert_in('cafe', vocab)\n", "def test_strip_tags():\n    # Check stripping of simple HTML tags\n    assert_equal(strip_tags('<b>hello</b> <i>world</i>'), ' hello  world ')\n    # Check stripping of nested HTML tags\n    assert_equal(strip_tags('<div><p>hello <b>world</b></p></div>'), '  hello  world  ')\n    # Check stripping of self-closing HTML tags\n    assert_equal(strip_tags('<img src=\"image.jpg\" />'), ' ')\n    # Check stripping of tags with attributes\n    assert_equal(strip_tags('<a href=\"http://example.com\">link</a>'), ' link ')\n    # Check stripping of tags with line breaks\n    assert_equal(strip_tags('<div>\\nhello\\n</div>'), ' \\nhello\\n ')\n    # Check that only tags are stripped, not the content\n    assert_equal(strip_tags('<div>hello <b>world</b></div>'), ' hello  world ')\n", "def test_strip_tags():\n    text = \"<html><body>This is a <b>test</b> of the <a href='#'>strip_tags</a> function.</body></html>\"\n    expected = \" This is a  test  of the  strip_tags  function. \"\n    assert_equal(strip_tags(text), expected)\n\n    text = \"No tags here!\"\n    expected = \"No tags here!\"\n    assert_equal(strip_tags(text), expected)\n\n    text = \"<p>Some <em>HTML</em> with <strong>tags</strong>.</p>\"\n    expected = \" Some  HTML  with  tags .\"\n    assert_equal(strip_tags(text), expected)\n", "def test_countvectorizer_invalid_token_pattern():\n    # Test invalid token pattern raises a ValueError\n    message = \"Invalid token pattern\"\n    invalid_token_pattern = r\"[\"  # Invalid regex pattern\n\n        vect = CountVectorizer(token_pattern=invalid_token_pattern)\n        vect.fit_transform(JUNK_FOOD_DOCS)\n\n    assert_raise_message(ValueError, message, func)\n\n    # Test valid token pattern does not raise an error\n    valid_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n    vect = CountVectorizer(token_pattern=valid_token_pattern)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n    assert_equal(X.shape[0], len(JUNK_FOOD_DOCS))\n"], "sample_738": ["def test_hashing_vectorizer_custom_analyzer():\n        return s.split()\n\n    hv = HashingVectorizer(analyzer=custom_analyzer)\n    X = hv.fit_transform([\"hello world\", \"hello everyone\"])\n    \n    # Check shape\n    assert_equal(X.shape, (2, hv.n_features))\n    \n    # Check that the custom analyzer was applied correctly\n    # Since we are using a custom analyzer that just splits on spaces,\n    # we expect the words to be hashed and counted as they are.\n    analyze = hv.build_analyzer()\n    assert_equal(analyze(\"hello world\"), [\"hello\", \"world\"])\n    assert_equal(analyze(\"hello everyone\"), [\"hello\", \"everyone\"])\n    \n    # Ensure the data is correctly counted\n    assert_greater(np.max(X[0].data), 0)\n    assert_greater(np.max(X[1].data), 0)\n    assert_less_equal(np.max(X.data), 1)\n", "def test_tfidf_vectorizer_with_max_features():\n    tfidf = TfidfVectorizer(max_features=3)\n    X = tfidf.fit_transform(ALL_FOOD_DOCS).toarray()\n    feature_names = tfidf.get_feature_names()\n    \n    # Check that only 3 features are present\n    assert_equal(X.shape[1], 3)\n    \n    # Ensure that the most frequent terms are selected\n    assert_true(\"the\" in feature_names)\n    assert_true(\"pizza\" in feature_names or \"beer\" in feature_names)\n    \n    # Ensure the selected features match the top terms by frequency\n    top_terms = [\"the\", \"pizza\", \"beer\", \"burger\", \"salad\"]\n    for term in feature_names:\n        assert_in(term, top_terms)\n", "def test_hashingvectorizer_partial_fit():\n    # Test the partial_fit method of HashingVectorizer which should do nothing\n    hv = HashingVectorizer()\n    X_initial = hv.transform(JUNK_FOOD_DOCS)\n    \n    hv_partial = hv.partial_fit(JUNK_FOOD_DOCS)\n    X_partial = hv_partial.transform(JUNK_FOOD_DOCS)\n    \n    # Ensure that partial_fit doesn't modify the transformer state\n    assert_array_equal(X_initial.toarray(), X_partial.toarray())\n    assert_equal(hv, hv_partial)\n", "def test_strip_tags():\n    # Test the strip_tags function with various HTML/XML tags\n    s = \"<b>This is bold</b>\"\n    expected = \" This is bold \"\n    assert_equal(strip_tags(s), expected)\n\n    s = \"<div class='test'>Some <i>italic</i> and <u>underlined</u> text</div>\"\n    expected = \" Some  italic  and  underlined  text \"\n    assert_equal(strip_tags(s), expected)\n\n    s = \"<p>Paragraph with <a href='http://example.com'>link</a></p>\"\n    expected = \" Paragraph with  link \"\n    assert_equal(strip_tags(s), expected)\n\n    s = \"Text with no tags\"\n    expected = \"Text with no tags\"\n    assert_equal(strip_tags(s), expected)\n\n    s = \"<!-- Comment -->Visible text<!-- Another comment -->\"\n    expected = \" Visible text \"\n    assert_equal(strip_tags(s), expected)\n\n    s = \"<p>Special characters: &amp; &lt; &gt; &quot; &apos;</p>\"\n    expected = \" Special characters: &amp; &lt; &gt; &quot; &apos; \"\n    assert_equal(strip_tags(s), expected)\n\n    s = \"<p>Multiple lines\\nwith tags\\nand text</p>\"\n    expected = \" Multiple lines\\nwith tags\\nand text \"\n    assert_equal(strip_tags(s), expected)\n", "def test_strip_tags():\n    text = \"<p>This is a <b>test</b></p>\"\n    expected = \" This is a  test \"\n    assert_equal(strip_tags(text), expected)\n\n    text = \"<div>Another <a href='example.com'>example</a> text</div>\"\n    expected = \" Another  example  text \"\n    assert_equal(strip_tags(text), expected)\n\n    text = \"<html><body><h1>Title</h1><p>Paragraph</p></body></html>\"\n    expected = \"  Title  Paragraph \"\n    assert_equal(strip_tags(text), expected)\n"], "sample_739": ["def test_label_binarizer_multiclass_to_binary():\n    lb = LabelBinarizer()\n\n    # multiclass to binary with fit_transform\n    inp = [\"a\", \"b\", \"c\"]\n    expected = np.array([[1, 0, 0],\n                         [0, 1, 0],\n                         [0, 0, 1]])\n    got = lb.fit_transform(inp)\n    assert_array_equal(lb.classes_, [\"a\", \"b\", \"c\"])\n    assert_array_equal(expected, got)\n\n    # inverse transform\n    assert_array_equal(lb.inverse_transform(got), inp)\n\n    # multiclass to binary with transform\n    inp_new = [\"c\", \"a\", \"b\"]\n    expected_new = np.array([[0, 0, 1],\n                             [1, 0, 0],\n                             [0, 1, 0]])\n    got_new = lb.transform(inp_new)\n    assert_array_equal(expected_new, got_new)\n    assert_array_equal(lb.inverse_transform(got_new), inp_new)\n", "def test_multilabel_binarizer_inverse_transform_sparse():\n    # Test the inverse_transform with sparse output\n    y = [(1, 2), (3,), (1, 3)]\n    mlb = MultiLabelBinarizer(sparse_output=True)\n    Y = mlb.fit_transform(y)\n    assert_true(issparse(Y))\n    assert_equal(Y.shape, (3, 3))\n    assert_array_equal(mlb.classes_, [1, 2, 3])\n    assert_equal(mlb.inverse_transform(Y), y)\n\n    # Test the inverse_transform with sparse input having no data\n    y_empty = [(), (), ()]\n    Y_empty = mlb.transform(y_empty)\n    assert_true(issparse(Y_empty))\n    assert_equal(Y_empty.shape, (3, 3))\n    assert_array_equal(mlb.inverse_transform(Y_empty), y_empty)\n", "def test_label_encoder_unseen_labels():\n    le = LabelEncoder()\n    le.fit([1, 2, 3, 4])\n    assert_raises(ValueError, le.transform, [1, 2, 5])  # 5 is unseen\n\n    le.fit([\"cat\", \"dog\", \"mouse\"])\n    assert_raises(ValueError, le.transform, [\"cat\", \"dog\", \"elephant\"])  # 'elephant' is unseen\n\n    le.fit([0.1, 0.2, 0.3])\n    assert_raises(ValueError, le.transform, [0.1, 0.4])  # 0.4 is unseen\n", "def test_label_binarizer_with_missing_classes():\n    # Test LabelBinarizer with input labels that do not cover all classes\n    lb = LabelBinarizer()\n\n    inp = [\"cat\", \"dog\", \"dog\", \"cat\"]\n    lb.fit(inp)\n    got = lb.transform([\"cat\", \"dog\", \"bird\"])\n    expected = np.array([[1, 0],\n                         [0, 1],\n                         [0, 0]])\n    assert_array_equal(got, expected)\n\n    got_inverse = lb.inverse_transform(got)\n    expected_inverse = [\"cat\", \"dog\", None]\n    assert_array_equal(got_inverse, expected_inverse)\n\n    # Test sparse output\n    lb = LabelBinarizer(sparse_output=True)\n    lb.fit(inp)\n    got = lb.transform([\"cat\", \"dog\", \"bird\"])\n    expected = np.array([[1, 0],\n                         [0, 1],\n                         [0, 0]])\n    assert_true(issparse(got))\n    assert_array_equal(got.toarray(), expected)\n\n    got_inverse = lb.inverse_transform(got)\n    expected_inverse = [\"cat\", \"dog\", None]\n    assert_array_equal(got_inverse, expected_inverse)\n", "def test_label_encoder_with_strings():\n    # Test LabelEncoder with string labels\n    le = LabelEncoder()\n    labels = [\"dog\", \"cat\", \"cat\", \"bird\"]\n    le.fit(labels)\n    assert_array_equal(le.classes_, [\"bird\", \"cat\", \"dog\"])\n    transformed = le.transform([\"dog\", \"bird\", \"cat\"])\n    assert_array_equal(transformed, [2, 0, 1])\n    inverse_transformed = le.inverse_transform([2, 0, 1])\n    assert_array_equal(inverse_transformed, [\"dog\", \"bird\", \"cat\"])\n\n    # Test unseen label\n    assert_raises(ValueError, le.transform, [\"fish\"])\n\n    # Test empty list\n    assert_array_equal(le.transform([]), np.array([]))\n    assert_array_equal(le.inverse_transform([]), np.array([]))\n"], "sample_740": ["def test_check_random_state():\n    # Test check_random_state with various inputs\n    random_state = np.random.RandomState(42)\n    \n    # None should return the global RandomState instance used by np.random\n    rs1 = check_random_state(None)\n    assert_true(isinstance(rs1, np.random.RandomState))\n    \n    # An integer should return a new RandomState instance seeded with that integer\n    rs2 = check_random_state(42)\n    assert_true(isinstance(rs2, np.random.RandomState))\n    assert_array_equal(rs2.randint(0, 100, 10), random_state.randint(0, 100, 10))\n    \n    # A RandomState instance should return itself\n    rs3 = check_random_state(random_state)\n    assert_true(rs3 is random_state)\n    \n    # Any other input should raise a ValueError\n    assert_raises(ValueError, check_random_state, \"invalid_seed\")\n    assert_raises(ValueError, check_random_state, [1, 2, 3])\n", "def test_check_random_state():\n    # Check that check_random_state returns a valid RandomState object\n    assert_true(isinstance(check_random_state(None), np.random.RandomState))\n    assert_true(isinstance(check_random_state(np.random), np.random.RandomState))\n    assert_true(isinstance(check_random_state(42), np.random.RandomState))\n    assert_true(isinstance(check_random_state(np.random.RandomState(42)), np.random.RandomState))\n\n    # Check that invalid seeds raise a ValueError\n    invalid_seeds = [\"string\", 3.14, [1, 2, 3], (1, 2), {\"seed\": 42}]\n    for seed in invalid_seeds:\n        assert_raises_regex(ValueError, '%r cannot be used to seed a numpy.random.RandomState instance' % seed, check_random_state, seed)\n", "def test_check_non_negative():\n    X_positive = np.array([[1, 2], [3, 4]])\n    X_negative = np.array([[1, -2], [3, 4]])\n    X_mixed = np.array([[1, 2], [-3, 4]])\n\n    check_non_negative(X_positive, \"positive array\")  # Should not raise\n\n    with pytest.raises(ValueError, match=\"Negative values in data passed to negative array\"):\n        check_non_negative(X_negative, \"negative array\")\n\n    with pytest.raises(ValueError, match=\"Negative values in data passed to mixed array\"):\n        check_non_negative(X_mixed, \"mixed array\")\n\n    # Test with sparse matrices\n    X_csr_positive = sp.csr_matrix(X_positive)\n    X_csr_negative = sp.csr_matrix(X_negative)\n    X_csr_mixed = sp.csr_matrix(X_mixed)\n\n    check_non_negative(X_csr_positive, \"positive sparse array\")  # Should not raise\n\n    with pytest.raises(ValueError, match=\"Negative values in data passed to negative sparse array\"):\n        check_non_negative(X_csr_negative, \"negative sparse array\")\n\n    with pytest.raises(ValueError, match=\"Negative values in data passed to mixed sparse array\"):\n        check_non_negative(X_csr_mixed, \"mixed sparse array\")\n", "def test_check_non_negative():\n    # Test that negative values raise a ValueError\n    X = np.array([[1, 2, 3], [-1, 5, 6]])\n    assert_raises(ValueError, check_non_negative, X, \"test\")\n\n    # Test that zero and positive values pass without error\n    X = np.array([[1, 2, 3], [0, 5, 6]])\n    assert_no_warnings(check_non_negative, X, \"test\")\n\n    # Test with sparse matrix containing negative values\n    X_sparse = sp.csr_matrix([[1, 2, 3], [-1, 5, 6]])\n    assert_raises(ValueError, check_non_negative, X_sparse, \"test\")\n\n    # Test with sparse matrix containing only zero and positive values\n    X_sparse = sp.csr_matrix([[1, 2, 3], [0, 5, 6]])\n    assert_no_warnings(check_non_negative, X_sparse, \"test\")\n", "def test_check_is_fitted_multiple_attributes():\n    # Test with multiple attributes\n    class EstimatorWithMultipleAttributes:\n            self.attr1_ = None\n            self.attr2_ = None\n\n    est = EstimatorWithMultipleAttributes()\n\n    # Not fitted\n    assert_raises(NotFittedError, check_is_fitted, est, [\"attr1_\", \"attr2_\"])\n\n    # Partially fitted\n    est.attr1_ = \"attr1\"\n    assert_raises(NotFittedError, check_is_fitted, est, [\"attr1_\", \"attr2_\"])\n\n    # Fully fitted\n    est.attr2_ = \"attr2\"\n    assert_equal(None, check_is_fitted(est, [\"attr1_\", \"attr2_\"]))\n\n    # Custom error message\n    try:\n        check_is_fitted(est, [\"attr1_\", \"attr2_\", \"attr3_\"], msg=\"Custom message %(name)s\")\n    except NotFittedError as e:\n        assert_equal(str(e), \"Custom message EstimatorWithMultipleAttributes\")\n"], "sample_741": ["def test_grid_search_with_unfitted_estimator():\n    # Test that GridSearchCV raises an error if the estimator is not fitted\n    clf = LinearSVC(random_state=0)\n    grid_search = GridSearchCV(clf, {'C': [0.1, 1, 10]})\n    \n    assert_raises(NotFittedError, grid_search.predict, X)\n    assert_raises(NotFittedError, grid_search.predict_proba, X)\n    assert_raises(NotFittedError, grid_search.predict_log_proba, X)\n    assert_raises(NotFittedError, grid_search.decision_function, X)\n    assert_raises(NotFittedError, grid_search.transform, X)\n    assert_raises(NotFittedError, grid_search.inverse_transform, X)\n", "def test_parameter_sampler_with_distributions():\n    # Test that ParameterSampler samples from distributions correctly\n    param_distributions = {\n        'a': [1, 2],\n        'b': expon(scale=1.0),\n        'c': uniform(0, 1)\n    }\n    sampler = ParameterSampler(param_distributions, n_iter=10, random_state=42)\n    samples = list(sampler)\n    assert_equal(len(samples), 10)\n\n    # Check that 'a' parameter only takes values from the list\n    assert_true(all(sample['a'] in [1, 2] for sample in samples))\n\n    # Check that 'b' parameter samples from the exponential distribution\n    b_values = [sample['b'] for sample in samples]\n    assert_true(all(b >= 0 for b in b_values))\n\n    # Check that 'c' parameter samples from the uniform distribution between 0 and 1\n    c_values = [sample['c'] for sample in samples]\n    assert_true(all(0 <= c <= 1 for c in c_values))\n\n    # Ensure reproducibility with random_state\n    sampler_reproducible = ParameterSampler(param_distributions, n_iter=10, random_state=42)\n    samples_reproducible = list(sampler_reproducible)\n    assert_equal(samples, samples_reproducible)\n", "def test_parameter_sampler_with_scipy_distributions():\n    # Test ParameterSampler with scipy distributions\n    from scipy.stats import expon, randint\n\n    param_distributions = {\n        'a': randint(0, 10),\n        'b': expon(scale=1.0),\n        'c': ['x', 'y', 'z']\n    }\n    sampler = ParameterSampler(param_distributions, n_iter=5, random_state=42)\n    samples = list(sampler)\n\n    # Check length of samples\n    assert_equal(len(samples), 5)\n\n    # Check types and ranges of samples\n    for sample in samples:\n        assert_true(0 <= sample['a'] < 10)\n        assert_true(sample['b'] >= 0)\n        assert_true(sample['c'] in ['x', 'y', 'z'])\n\n    # Check deterministic behavior with same random state\n    sampler_2 = ParameterSampler(param_distributions, n_iter=5, random_state=42)\n    samples_2 = list(sampler_2)\n    assert_equal(samples, samples_2)\n", "def test_grid_search_with_custom_scorer():\n    # Custom scoring function\n        predictions = estimator.predict(X)\n        return np.mean(predictions == y)\n\n    clf = MockClassifier()\n    param_grid = {'foo_param': [1, 2, 3]}\n    grid_search = GridSearchCV(clf, param_grid, scoring=custom_scorer, refit=True)\n    grid_search.fit(X, y)\n\n    # Check that the best parameter is correctly identified\n    assert_equal(grid_search.best_estimator_.foo_param, 2)\n\n    # Ensure the custom scorer is used\n    assert_equal(grid_search.scorer_, custom_scorer)\n\n    # Check if the best score is calculated correctly\n    best_score = custom_scorer(grid_search.best_estimator_, X, y)\n    assert_almost_equal(grid_search.best_score_, best_score)\n\n    # Ensure the grid search results are consistent with the custom scoring function\n    for i, params in enumerate(grid_search.cv_results_['params']):\n        est = clone(clf).set_params(**params)\n        est.fit(X, y)\n        expected_score = custom_scorer(est, X, y)\n        assert_almost_equal(grid_search.cv_results_['mean_test_score'][i], expected_score)\n", "def test_parameter_grid_empty_dict():\n    # Test ParameterGrid with an empty dictionary should yield default params\n    param_grid = {}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 1)\n    assert_equal(list(grid), [{}])\n    assert_grid_iter_equals_getitem(grid)\n"], "sample_742": ["def test_logistic_regression_path_invalid_solver():\n    # Test logistic_regression_path with an invalid solver\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    \n    invalid_solver = \"invalid_solver\"\n    msg = (\"Logistic Regression supports only liblinear, newton-cg, \"\n           \"lbfgs, sag and saga solvers, got %s\" % invalid_solver)\n    \n    with assert_raises(ValueError) as cm:\n        logistic_regression_path(X, y, solver=invalid_solver)\n    assert_equal(str(cm.exception), msg)\n", "def test_logistic_regression_path_check_input():\n    # Test logistic_regression_path with check_input parameter\n    rng = np.random.RandomState(0)\n    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))\n    y = [1] * 100 + [-1] * 100\n    Cs = np.logspace(0, 4, 10)\n\n    # Check with check_input=True\n    coefs, Cs, _ = logistic_regression_path(X, y, Cs=Cs, fit_intercept=False, tol=1e-5, solver='lbfgs',\n                                            max_iter=1000, random_state=0, check_input=True)\n    for i, C in enumerate(Cs):\n        lr = LogisticRegression(C=C, fit_intercept=False, tol=1e-5, solver='lbfgs', random_state=0)\n        lr.fit(X, y)\n        lr_coef = lr.coef_.ravel()\n        assert_array_almost_equal(lr_coef, coefs[i], decimal=4)\n\n    # Check with check_input=False\n    coefs, Cs, _ = logistic_regression_path(X, y, Cs=Cs, fit_intercept=False, tol=1e-5, solver='lbfgs',\n                                            max_iter=1000, random_state=0, check_input=False)\n    for i, C in enumerate(Cs):\n        lr = LogisticRegression(C=C, fit_intercept=False, tol=1e-5, solver='lbfgs', random_state=0)\n        lr.fit(X, y)\n        lr_coef = lr.coef_.ravel()\n        assert_array_almost_equal(lr_coef, coefs[i], decimal=4)\n", "def test_logistic_regression_path_input_types():\n    # Test logistic_regression_path with different input types for Cs\n    X, y = make_classification(n_samples=100, n_features=5, random_state=0)\n    \n    # Case 1: Cs as integer\n    Cs_int = 5\n    coefs_int, Cs_res_int, _ = logistic_regression_path(X, y, Cs=Cs_int)\n    assert_equal(len(Cs_res_int), Cs_int)\n    \n    # Case 2: Cs as list\n    Cs_list = [1e-3, 1e-2, 1e-1, 1, 10]\n    coefs_list, Cs_res_list, _ = logistic_regression_path(X, y, Cs=Cs_list)\n    assert_array_equal(Cs_res_list, np.array(Cs_list))\n    \n    # Case 3: Cs as numpy array\n    Cs_array = np.array([1e-3, 1e-2, 1e-1, 1, 10])\n    coefs_array, Cs_res_array, _ = logistic_regression_path(X, y, Cs=Cs_array)\n    assert_array_equal(Cs_res_array, Cs_array)\n    \n    # Ensure that all results are consistent across different input types\n    for i in range(len(Cs_list)):\n        assert_array_almost_equal(coefs_list[i], coefs_array[i])\n    \n    # Verify that results with Cs as integer fall in the range of list results\n    assert_greater_equal(len(coefs_int), 2)\n    assert_array_less(coefs_int[0], coefs_list[-1])\n    assert_array_less(coefs_list[0], coefs_int[-1])\n", "def test_logistic_regression_path_multiclass():\n    # Test logistic_regression_path for multiclass classification\n    X, y = iris.data, iris.target\n    Cs = [1e-4, 1e-2, 1, 10, 1e4]\n\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=Cs, max_iter=100,\n                                                 multi_class='multinomial',\n                                                 solver='lbfgs', tol=1e-6,\n                                                 random_state=0)\n    assert_equal(len(coefs), len(Cs))\n    assert_equal(coefs[0].shape, (3, X.shape[1] + 1))\n    assert_equal(len(n_iter), len(Cs))\n    assert_array_almost_equal(n_iter, np.full(len(Cs), 100))\n\n    coefs_ovr, Cs, n_iter_ovr = logistic_regression_path(X, y, Cs=Cs, max_iter=100,\n                                                         multi_class='ovr',\n                                                         solver='lbfgs', tol=1e-6,\n                                                         random_state=0)\n    assert_equal(len(coefs_ovr), len(Cs))\n    assert_equal(coefs_ovr[0].shape, (X.shape[1] + 1,))\n    assert_equal(len(n_iter_ovr), len(Cs))\n    assert_array_almost_equal(n_iter_ovr, np.full(len(Cs), 100))\n\n    clf_multi = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=0)\n    clf_multi.fit(X, y)\n    clf_ovr = LogisticRegression(multi_class='ovr', solver='lbfgs', random_state=0)\n    clf_ovr.fit(X, y)\n    assert_array_almost_equal(coefs[-1][:, :-1], clf_multi.coef_, decimal=3)\n    assert_array_almost_equal(coefs_ovr[-1][:-1], clf_ovr.coef_[0], decimal=3)\n", "def test_logistic_regression_intercept():\n    # Test intercept fitting in LogisticRegression\n    X, y = iris.data, iris.target\n    clf = LogisticRegression(fit_intercept=True, solver='lbfgs')\n    clf.fit(X, y)\n    intercept_fitted = clf.intercept_\n    \n    clf_no_intercept = LogisticRegression(fit_intercept=False, solver='lbfgs')\n    clf_no_intercept.fit(X, y)\n    \n    # Ensure that the intercept is not zero when fit_intercept=True\n    assert not np.all(intercept_fitted == 0), \"Intercept should not be zero when fit_intercept=True\"\n    \n    # Ensure that the intercept is zero when fit_intercept=False\n    assert np.all(clf_no_intercept.intercept_ == 0), \"Intercept should be zero when fit_intercept=False\"\n    \n    # Ensure that the coefficients are not the same when intercept is fitted vs not fitted\n    assert not np.allclose(clf.coef_, clf_no_intercept.coef_), \"Coefficients should differ when intercept is fitted vs not fitted\"\n"], "sample_743": ["def test_neighbors_mismatched_fitted_data():\n    # Test that an error is raised if the query data shape does not match the fitted data shape\n    X = rng.random_sample((10, 3))\n    y = np.ones(10)\n\n    neigh = neighbors.NearestNeighbors(n_neighbors=3)\n    neigh.fit(X)\n\n    X_mismatch = rng.random_sample((10, 4))  # Mismatched shape\n\n    assert_raises(ValueError, neigh.kneighbors, X_mismatch)\n    assert_raises(ValueError, neigh.radius_neighbors, X_mismatch)\n    assert_raises(ValueError, neigh.kneighbors_graph, X_mismatch)\n    assert_raises(ValueError, neigh.radius_neighbors_graph, X_mismatch)\n\n    # Also check with supervised neighbors\n    knn = neighbors.KNeighborsClassifier(n_neighbors=3)\n    knn.fit(X, y)\n\n    assert_raises(ValueError, knn.kneighbors, X_mismatch)\n    assert_raises(ValueError, knn.radius_neighbors, X_mismatch)\n    assert_raises(ValueError, knn.kneighbors_graph, X_mismatch)\n    assert_raises(ValueError, knn.radius_neighbors_graph, X_mismatch)\n", "def test_check_weights():\n    # Test the _check_weights function to ensure it raises the appropriate errors\n    # and returns the correct values for valid inputs.\n    \n    assert_equal(_check_weights(None), None)\n    assert_equal(_check_weights('uniform'), 'uniform')\n    assert_equal(_check_weights('distance'), 'distance')\n    \n        return dist\n    \n    assert_equal(_check_weights(weight_func), weight_func)\n    \n    invalid_weights = ['invalid', 123, 1.5, [], {}]\n    for w in invalid_weights:\n        assert_raises(ValueError, _check_weights, w)\n", "def test_check_weights():\n    # Test _check_weights function\n    assert_equal(_check_weights(None), None)\n    assert_equal(_check_weights('uniform'), 'uniform')\n    assert_equal(_check_weights('distance'), 'distance')\n    assert_equal(_check_weights(_weight_func), _weight_func)\n\n    # Test invalid weights\n    assert_raises(ValueError, _check_weights, 'invalid')\n    assert_raises(ValueError, _check_weights, 123)\n", "def test_check_weights():\n    # Test the _check_weights function with different inputs\n    assert_equal(_check_weights(None), None)\n    assert_equal(_check_weights('uniform'), 'uniform')\n    assert_equal(_check_weights('distance'), 'distance')\n\n        return dist ** -2\n\n    assert_equal(_check_weights(weight_func), weight_func)\n\n    assert_raises(ValueError, _check_weights, 'invalid_weight')\n", "def test_check_algorithm_metric():\n    # Test _check_algorithm_metric method for various scenarios\n    class DummyEstimator(NeighborsBase):\n                     leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=1):\n            super().__init__(n_neighbors, radius, algorithm, leaf_size, \n                             metric, p, metric_params, n_jobs)\n            pass\n\n    # Valid scenarios\n    est = DummyEstimator(algorithm='auto', metric='minkowski', p=2)\n    est._check_algorithm_metric()\n    \n    est = DummyEstimator(algorithm='ball_tree', metric='euclidean')\n    est._check_algorithm_metric()\n    \n    est = DummyEstimator(algorithm='kd_tree', metric='manhattan')\n    est._check_algorithm_metric()\n\n    est = DummyEstimator(algorithm='brute', metric='cosine')\n    est._check_algorithm_metric()\n\n    # Invalid algorithm\n    with assert_raises(ValueError):\n        est = DummyEstimator(algorithm='invalid_algorithm')\n        est._check_algorithm_metric()\n\n    # Invalid metric for specific algorithm\n    with assert_raises(ValueError):\n        est = DummyEstimator(algorithm='kd_tree', metric='invalid_metric')\n        est._check_algorithm_metric()\n\n    # Invalid p for minkowski metric\n    with assert_raises(ValueError):\n        est = DummyEstimator(metric='minkowski', p=0.5)\n        est._check_algorithm_metric()\n\n    # Callable metric with unsupported algorithm\n    with assert_raises(ValueError):\n        est = DummyEstimator(algorithm='kd_tree', metric=lambda x, y: np.sum(x + y))\n        est._check_algorithm_metric()\n\n    # Metric params with 'p'\n    with assert_warns(SyntaxWarning):\n        est = DummyEstimator(metric_params={'p': 3})\n        est._check_algorithm_metric()\n"], "sample_744": ["def test_binarize_function():\n    # Test the standalone binarize function\n    X = np.array([[1, 0, 5], [2, 3, -1]])\n    X_bin = binarize(X, threshold=2.0, copy=True)\n    assert_equal(np.sum(X_bin == 0), 4)\n    assert_equal(np.sum(X_bin == 1), 2)\n\n    X_bin = binarize(X, threshold=2.0, copy=False)\n    assert_true(X_bin is X)\n    assert_equal(np.sum(X_bin == 0), 4)\n    assert_equal(np.sum(X_bin == 1), 2)\n\n    X_bin = binarize(X, threshold=0.0, copy=True)\n    assert_equal(np.sum(X_bin == 0), 2)\n    assert_equal(np.sum(X_bin == 1), 4)\n\n    X_sparse = sparse.csr_matrix(X)\n    X_bin_sparse = binarize(X_sparse, threshold=2.0, copy=True)\n    assert_true(sparse.isspmatrix_csr(X_bin_sparse))\n    assert_equal(np.sum(X_bin_sparse.toarray() == 0), 4)\n    assert_equal(np.sum(X_bin_sparse.toarray() == 1), 2)\n\n    X_bin_sparse = binarize(X_sparse, threshold=2.0, copy=False)\n    assert_true(X_bin_sparse is X_sparse)\n    assert_equal(np.sum(X_bin_sparse.toarray() == 0), 4)\n    assert_equal(np.sum(X_bin_sparse.toarray() == 1), 2)\n\n    X_bin_sparse = binarize(X_sparse, threshold=0.0, copy=True)\n    assert_equal(np.sum(X_bin_sparse.toarray() == 0), 2)\n    assert_equal(np.sum(X_bin_sparse.toarray() == 1), 4)\n", "def test_minmax_scale_sparse():\n    # Check minmax_scale on sparse data\n    X_sparse = sparse.csr_matrix([[0, 1, 2], [2, 0, 0], [1, 2, 4]])\n    X_dense = X_sparse.toarray()\n    \n    # default params\n    X_trans_sparse = minmax_scale(X_sparse)\n    X_trans_dense = minmax_scale(X_dense)\n    assert_array_almost_equal(X_trans_sparse.toarray(), X_trans_dense)\n    \n    # not default params: min=-1, max=1\n    X_trans_sparse = minmax_scale(X_sparse, feature_range=(-1, 1))\n    X_trans_dense = minmax_scale(X_dense, feature_range=(-1, 1))\n    assert_array_almost_equal(X_trans_sparse.toarray(), X_trans_dense)\n\n    # specific feature range\n    X_trans_sparse = minmax_scale(X_sparse, feature_range=(0, 2))\n    X_trans_dense = minmax_scale(X_dense, feature_range=(0, 2))\n    assert_array_almost_equal(X_trans_sparse.toarray(), X_trans_dense)\n", "def test_kernel_centerer_inverse_transform():\n    # Test that KernelCenterer properly handles inverse transform\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n    scaler = StandardScaler(with_std=False)\n    scaler.fit(X_fit)\n    X_fit_centered = scaler.transform(X_fit)\n    K_fit = np.dot(X_fit, X_fit.T)\n    K_pred = np.dot(X_pred, X_fit.T)\n    \n    centerer = KernelCenterer()\n    K_fit_centered = centerer.fit_transform(K_fit)\n    K_pred_centered = centerer.transform(K_pred)\n    \n    K_pred_inverse = centerer.inverse_transform(K_pred_centered)\n    \n    assert_array_almost_equal(K_pred, K_pred_inverse)\n", "def test_standard_scaler_on_sparse_csr_matrix():\n    # Test StandardScaler on sparse CSR matrices with both mean and std scaling\n    rng = np.random.RandomState(0)\n    X_dense = rng.randn(4, 5)\n    X_sparse = sparse.csr_matrix(X_dense)\n\n    scaler = StandardScaler(with_mean=True, with_std=True)\n    X_scaled = scaler.fit_transform(X_sparse)\n    \n    assert_true(sparse.issparse(X_scaled))\n    assert_true(isinstance(X_scaled, sparse.csr_matrix))\n\n    X_scaled_dense = scaler.fit_transform(X_dense)\n    assert_array_almost_equal(X_scaled.toarray(), X_scaled_dense)\n\n    # Check inverse transform\n    X_scaled_back = scaler.inverse_transform(X_scaled)\n    assert_array_almost_equal(X_scaled_back.toarray(), X_dense)\n\n    # Check that StandardScaler raises ValueError with_mean=True on sparse input\n    assert_raises(ValueError, StandardScaler(with_mean=True).fit, X_sparse)\n    assert_raises(ValueError, StandardScaler(with_mean=True).transform, X_sparse)\n\n    # Test StandardScaler without mean centering\n    scaler_no_mean = StandardScaler(with_mean=False, with_std=True)\n    X_scaled_no_mean = scaler_no_mean.fit_transform(X_sparse)\n    assert_true(sparse.issparse(X_scaled_no_mean))\n    assert_true(isinstance(X_scaled_no_mean, sparse.csr_matrix))\n\n    X_scaled_no_mean_dense = scaler_no_mean.fit_transform(X_dense)\n    assert_array_almost_equal(X_scaled_no_mean.toarray(), X_scaled_no_mean_dense)\n", "def test_minmax_scale_1d():\n    # Test the function minmax_scale with 1D input\n    X = [1., 3., 5., 0.]\n    X_scaled = minmax_scale(X)\n    assert_array_almost_equal(X_scaled, [0.2, 0.6, 1.0, 0.0])\n    assert_array_almost_equal(minmax_scale(X, feature_range=(1, 2)), [1.2, 1.6, 2.0, 1.0])\n\n    # test inplace operation\n    X = np.array([1., 3., 5., 0.])\n    X_scaled = minmax_scale(X, copy=False)\n    assert_array_almost_equal(X, [0.2, 0.6, 1.0, 0.0])\n\n    # test exception on invalid range\n    assert_raises(ValueError, minmax_scale, X, feature_range=(2, 1))\n"], "sample_745": ["def test_sparse_input():\n    X_sparse = sparse.csr_matrix([[0, 1], [2, 3], [4, 5]])\n\n    # Test that the FunctionTransformer works with sparse inputs\n    F = FunctionTransformer(np.log1p, accept_sparse=True)\n    transformed = F.transform(X_sparse)\n    assert_allclose_dense_sparse(transformed, np.log1p(X_sparse))\n\n    # Test inverse_transform with sparse input\n    F = FunctionTransformer(np.log1p, inverse_func=np.expm1, accept_sparse=True)\n    transformed = F.transform(X_sparse)\n    assert_allclose_dense_sparse(F.inverse_transform(transformed), X_sparse)\n", "def test_accept_sparse():\n    X_dense = np.array([[1, 2], [3, 4]])\n    X_sparse = sparse.csr_matrix(X_dense)\n\n    # Test with accept_sparse=True\n    transformer = FunctionTransformer(func=np.sqrt, accept_sparse=True)\n    X_transformed = transformer.transform(X_sparse)\n    assert_allclose_dense_sparse(X_transformed, np.sqrt(X_dense))\n\n    # Test with accept_sparse=False should raise an error for sparse input\n    transformer = FunctionTransformer(func=np.sqrt, accept_sparse=False)\n    with pytest.raises(TypeError):\n        transformer.transform(X_sparse)\n", "def test_inverse_func_with_kw_args():\n    X = np.array([1, 4, 9, 16]).reshape((2, 2))\n\n    # Custom inverse function that takes additional keyword arguments\n        return np.power(X, power)\n\n    F = FunctionTransformer(\n        func=np.sqrt,\n        inverse_func=custom_inverse_func, inv_kw_args=dict(power=2),\n    )\n\n    assert_array_equal(\n        F.inverse_transform(F.transform(X)),\n        np.power(np.sqrt(X), 2),\n    )\n", "def test_sparse_input_handling():\n    X_dense = np.array([[1, 0], [0, 1]], dtype=np.float64)\n    X_sparse = sparse.csr_matrix(X_dense)\n\n    # Test with validate=True and accept_sparse=False\n    transformer = FunctionTransformer(validate=True, accept_sparse=False)\n    with pytest.raises(TypeError):\n        transformer.fit_transform(X_sparse)\n\n    # Test with validate=True and accept_sparse=True\n    transformer = FunctionTransformer(validate=True, accept_sparse=True)\n    Xt = assert_no_warnings(transformer.fit_transform, X_sparse)\n    assert_allclose_dense_sparse(X_sparse, Xt)\n\n    # Test with validate=False and accept_sparse=False (should not raise error)\n    transformer = FunctionTransformer(validate=False, accept_sparse=False)\n    Xt = assert_no_warnings(transformer.fit_transform, X_sparse)\n    assert_allclose_dense_sparse(X_sparse, Xt)\n\n    # Test with validate=False and accept_sparse=True (should not raise error)\n    transformer = FunctionTransformer(validate=False, accept_sparse=True)\n    Xt = assert_no_warnings(transformer.fit_transform, X_sparse)\n    assert_allclose_dense_sparse(X_sparse, Xt)\n", "def test_sparse_input():\n    X_dense = np.array([1, 2, 3, 4]).reshape((2, 2))\n    X_sparse = sparse.csr_matrix(X_dense)\n\n    transformer = FunctionTransformer(validate=True, accept_sparse=True)\n\n    # Test that transform works with sparse input\n    Xt = transformer.fit_transform(X_sparse)\n    assert_allclose_dense_sparse(X_sparse, Xt)\n\n    # Test that inverse_transform works with sparse input\n    inverse_Xt = transformer.inverse_transform(Xt)\n    assert_allclose_dense_sparse(X_sparse, inverse_Xt)\n\n    transformer_no_sparse = FunctionTransformer(validate=True, accept_sparse=False)\n\n    # Test that transform raises an error with sparse input when accept_sparse=False\n    with pytest.raises(TypeError):\n        transformer_no_sparse.fit_transform(X_sparse)\n"], "sample_746": ["def test_zero_one_loss_binary():\n    # Test zero_one_loss for binary classification\n    y_true = [1, 0, 1, 1, 0, 0, 1, 0, 1, 1]\n    y_pred = [1, 0, 0, 1, 0, 1, 1, 0, 1, 0]\n\n    # Check if the fraction of misclassified samples is correctly computed\n    assert_equal(zero_one_loss(y_true, y_pred), 0.3)\n    assert_equal(zero_one_loss(y_true, y_pred, normalize=False), 3)\n\n    # Check with all correct predictions\n    y_pred_correct = y_true\n    assert_equal(zero_one_loss(y_true, y_pred_correct), 0)\n    assert_equal(zero_one_loss(y_true, y_pred_correct, normalize=False), 0)\n\n    # Check with all incorrect predictions\n    y_pred_incorrect = [1 - y for y in y_true]\n    assert_equal(zero_one_loss(y_true, y_pred_incorrect), 1)\n    assert_equal(zero_one_loss(y_true, y_pred_incorrect, normalize=False), 10)\n", "def test_brier_score_loss():\n    # Test Brier score loss for binary classification\n    y_true = [0, 1, 1, 0]\n    y_prob = [0.1, 0.9, 0.8, 0.3]\n    loss = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(loss, 0.0375, decimal=4)\n\n    # Test Brier score loss for binary classification with class labels\n    y_true = [\"spam\", \"ham\", \"ham\", \"spam\"]\n    y_prob = [0.1, 0.9, 0.8, 0.3]\n    loss = brier_score_loss(y_true, y_prob, pos_label=\"ham\")\n    assert_almost_equal(loss, 0.0375, decimal=4)\n\n    # Test Brier score loss for binary classification with different pos_label\n    y_true = [0, 1, 1, 0]\n    y_prob = [0.1, 0.9, 0.8, 0.3]\n    loss = brier_score_loss(y_true, 1 - np.array(y_prob), pos_label=0)\n    assert_almost_equal(loss, 0.0375, decimal=4)\n\n    # Test Brier score loss for edge cases\n    y_true = [0, 1, 1, 0]\n    y_prob = [0, 1, 1, 0]\n    loss = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(loss, 0.0, decimal=4)\n\n    y_true = [0, 1, 1, 0]\n    y_prob = [0.5, 0.5, 0.5, 0.5]\n    loss = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(loss, 0.25, decimal=4)\n", "def test_balanced_accuracy_score():\n    # Test balanced accuracy score\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 1, 0, 0]\n    \n    # Test with balanced accuracy score\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.75)\n    \n    # Test with all correct predictions\n    y_pred = [0, 1, 0, 1]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 1.0)\n    \n    # Test with all incorrect predictions\n    y_pred = [1, 0, 1, 0]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.0)\n    \n    # Test with sample weights\n    sample_weight = [0.5, 0.5, 1, 1]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred, sample_weight), 0.0)\n    \n    # Test with multiclass input should raise an error\n    y_true = [0, 1, 2, 1]\n    y_pred = [0, 1, 2, 0]\n    assert_raises(ValueError, balanced_accuracy_score, y_true, y_pred)\n", "def test_brier_score_loss():\n    # Test Brier score loss function\n    y_true = [0, 1, 1, 0]\n    y_prob = [0.1, 0.9, 0.8, 0.3]\n    loss = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(loss, 0.0375, decimal=4)\n\n    # Test with sample weights\n    sample_weight = [1, 2, 1, 2]\n    loss = brier_score_loss(y_true, y_prob, sample_weight=sample_weight)\n    assert_almost_equal(loss, 0.035, decimal=4)\n\n    # Test with pos_label parameter\n    loss = brier_score_loss(y_true, 1-np.array(y_prob), pos_label=0)\n    assert_almost_equal(loss, 0.0375, decimal=4)\n\n    # Test edge case with all probabilities being 0 or 1\n    y_true = [0, 1, 1, 0, 1, 0]\n    y_prob = [0, 1, 1, 0, 1, 0]\n    loss = brier_score_loss(y_true, y_prob)\n    assert_equal(loss, 0.0)\n\n    # Test with non-binary labels\n    y_true = [\"spam\", \"ham\", \"ham\", \"spam\"]\n    y_prob = [0.1, 0.9, 0.8, 0.3]\n    loss = brier_score_loss(y_true, y_prob, pos_label=\"ham\")\n    assert_almost_equal(loss, 0.0375, decimal=4)\n\n    # Test exception for invalid probabilities\n    y_true = [0, 1, 1, 0]\n    y_prob = [0.1, 1.1, 0.8, 0.3]\n    assert_raises(ValueError, brier_score_loss, y_true, y_prob)\n\n    y_prob = [0.1, -0.1, 0.8, 0.3]\n    assert_raises(ValueError, brier_score_loss, y_true, y_prob)\n", "def test_balanced_accuracy_score():\n    # Test balanced accuracy score\n    y_true = [0, 1, 0, 0, 1, 0]\n    y_pred = [0, 1, 0, 0, 0, 1]\n    \n    # True negatives: 2, True positives: 1, False positives: 1, False negatives: 1\n    # Sensitivity (True positive rate) = 1 / (1 + 1) = 0.5\n    # Specificity (True negative rate) = 2 / (2 + 1) = 0.666...\n    # Balanced accuracy = (Sensitivity + Specificity) / 2 = (0.5 + 0.666...) / 2 = 0.583...\n    expected_balanced_accuracy = 0.5833333333333333\n    calculated_balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(calculated_balanced_accuracy, expected_balanced_accuracy, decimal=6)\n"], "sample_747": ["def test_min_max_scaler_sparse_input():\n    # Check if MinMaxScaler works correctly with sparse inputs\n    X = sparse.csr_matrix([[1, 0, 3], [0, 1, 0], [2, 0, 0]])\n    \n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Checking the min and max values of the transformed sparse matrix\n    X_scaled_dense = X_scaled.toarray()\n    assert_array_almost_equal(X_scaled_dense.min(axis=0), [0, 0, 0])\n    assert_array_almost_equal(X_scaled_dense.max(axis=0), [1, 1, 1])\n\n    # Check inverse transform\n    X_scaled_back = scaler.inverse_transform(X_scaled)\n    assert_array_almost_equal(X_scaled_back.toarray(), X.toarray())\n\n    # Test with different feature_range\n    scaler = MinMaxScaler(feature_range=(1, 2))\n    X_scaled = scaler.fit_transform(X)\n    X_scaled_dense = X_scaled.toarray()\n    assert_array_almost_equal(X_scaled_dense.min(axis=0), [1, 1, 1])\n    assert_array_almost_equal(X_scaled_dense.max(axis=0), [2, 2, 2])\n\n    # Check inverse transform\n    X_scaled_back = scaler.inverse_transform(X_scaled)\n    assert_array_almost_equal(X_scaled_back.toarray(), X.toarray())\n", "def test_min_max_scaler_partial_fit_incorrect_input():\n    # Test that MinMaxScaler raises an error when partial_fit is given\n    # arrays with different number of features\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    X_incorrect = np.array([[7, 8], [9, 10]])\n    \n    scaler = MinMaxScaler()\n    scaler.partial_fit(X)\n    with pytest.raises(ValueError, match=\"has fewer features than during previous calls to partial_fit\"):\n        scaler.partial_fit(X_incorrect)\n", "def test_minmax_scaler_constant_feature():\n    # Test MinMaxScaler on a dataset where one feature is constant\n    X = [[0., 1., 0.5],\n         [0., 1., 0.6],\n         [0., 1., 0.3]]\n    \n    scaler = MinMaxScaler()\n    X_trans = scaler.fit_transform(X)\n    X_expected = [[0., 0., 0.5],\n                  [0., 0., 1.0],\n                  [0., 0., 0.0]]\n    assert_array_almost_equal(X_trans, X_expected)\n    X_trans_inv = scaler.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_trans_inv)\n    \n    # Test with different feature ranges\n    scaler = MinMaxScaler(feature_range=(1, 2))\n    X_trans = scaler.fit_transform(X)\n    X_expected = [[1., 1., 1.5],\n                  [1., 1., 2.0],\n                  [1., 1., 1.0]]\n    assert_array_almost_equal(X_trans, X_expected)\n    X_trans_inv = scaler.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_trans_inv)\n", "def test_maxabs_scale_axis1():\n    X = iris.data\n    X_trans = maxabs_scale(X, axis=1)\n    assert_array_almost_equal(np.max(np.abs(X_trans), axis=1), 1)\n    assert_array_almost_equal(np.min(X_trans, axis=1), -1)\n    assert_array_almost_equal(np.max(X_trans, axis=1), 1)\n", "def test_minmax_scale_function():\n    # Test the minmax_scale function on different types of input\n    X = np.array([[1., 2.], [3., 4.], [5., 6.]])\n    X_scaled = minmax_scale(X)\n    assert_array_almost_equal(X_scaled.min(axis=0), np.zeros(2))\n    assert_array_almost_equal(X_scaled.max(axis=0), np.ones(2))\n    X_inv = MinMaxScaler().fit(X).inverse_transform(X_scaled)\n    assert_array_almost_equal(X, X_inv)\n\n    # Test with different feature_range\n    X_scaled = minmax_scale(X, feature_range=(-1, 1))\n    assert_array_almost_equal(X_scaled.min(axis=0), np.array([-1, -1]))\n    assert_array_almost_equal(X_scaled.max(axis=0), np.array([1, 1]))\n    X_inv = MinMaxScaler(feature_range=(-1, 1)).fit(X).inverse_transform(X_scaled)\n    assert_array_almost_equal(X, X_inv)\n\n    # Test with 1D array\n    X_1d = np.array([1., 2., 3.])\n    X_scaled_1d = minmax_scale(X_1d)\n    assert_almost_equal(X_scaled_1d.min(), 0.0)\n    assert_almost_equal(X_scaled_1d.max(), 1.0)\n\n    # Test with axis=1\n    X_2d_axis1 = np.array([[1., 2.], [3., 4.], [5., 6.]])\n    X_scaled_axis1 = minmax_scale(X_2d_axis1, axis=1)\n    assert_array_almost_equal(X_scaled_axis1.min(axis=1), np.zeros(3))\n    assert_array_almost_equal(X_scaled_axis1.max(axis=1), np.ones(3))\n"], "sample_748": ["def test_parameter_sampler_with_distributions():\n    # Ensure that ParameterSampler can handle different types of distributions\n    param_distributions = {\n        'param1': [1, 2, 3],\n        'param2': expon(scale=10),  # exponential distribution\n        'param3': uniform(0, 1),    # uniform distribution\n    }\n    sampler = ParameterSampler(param_distributions, n_iter=10, random_state=0)\n    samples = list(sampler)\n    assert_equal(len(samples), 10)\n    for sample in samples:\n        assert_true(sample['param1'] in [1, 2, 3])\n        assert_true(0 <= sample['param2'] <= 30)  # reasonable range for exponential with scale=10\n        assert_true(0 <= sample['param3'] <= 1)\n\n    # Ensure reproducibility with fixed random state\n    sampler = ParameterSampler(param_distributions, n_iter=10, random_state=0)\n    samples2 = list(sampler)\n    assert_equal(samples, samples2)\n", "def test_parameter_sampler_with_distributions():\n    # Test ParameterSampler with distributions that require rvs\n    param_distributions = {\n        'a': expon(scale=10),  # exponential distribution\n        'b': uniform(loc=0, scale=1)  # uniform distribution\n    }\n    sampler = ParameterSampler(param_distributions, n_iter=5, random_state=42)\n    samples = list(sampler)\n    assert_equal(len(samples), 5)\n    \n    for sample in samples:\n        assert_true(0 <= sample['b'] <= 1)\n        assert_true(sample['a'] >= 0)\n\n    # Ensure deterministic output\n    expected_samples = [\n        {'a': 4.692680899768591, 'b': 0.3745401188473625},\n        {'a': 30.10121471451376, 'b': 0.9507143064099162},\n        {'a': 23.282269833209627, 'b': 0.7319939418114051},\n        {'a': 14.666494004003192, 'b': 0.5986584841970366},\n        {'a': 0.1010919875584629, 'b': 0.15601864044243652}\n    ]\n    for sample, expected in zip(samples, expected_samples):\n        assert_almost_equal(sample['a'], expected['a'])\n        assert_almost_equal(sample['b'], expected['b'])\n", "def test_parameter_sampler_with_distributions():\n    # Test ParameterSampler with actual distributions\n    from scipy.stats import expon\n\n    param_distributions = {\n        \"a\": expon(scale=2),\n        \"b\": [\"option1\", \"option2\"]\n    }\n    n_iter = 20\n    sampler = ParameterSampler(param_distributions, n_iter=n_iter, random_state=42)\n    samples = list(sampler)\n\n    assert_equal(len(samples), n_iter)\n    for sample in samples:\n        assert_true(\"a\" in sample)\n        assert_true(\"b\" in sample)\n        assert_true(0 <= sample[\"a\"] <= 10)  # expon with scale=2 should be within reasonable range\n        assert_true(sample[\"b\"] in [\"option1\", \"option2\"])\n", "def test_parameter_sampler_with_distributions():\n    # Test ParameterSampler with actual distributions\n    param_distributions = {\n        'a': [1, 2, 3],\n        'b': expon(scale=0.1),\n        'c': ['x', 'y', 'z']\n    }\n    n_iter = 10\n    sampler = ParameterSampler(param_distributions, n_iter=n_iter, random_state=0)\n    samples = list(sampler)\n\n    assert_equal(len(samples), n_iter)\n    for sample in samples:\n        assert_true(sample['a'] in [1, 2, 3])\n        assert_true(0 <= sample['b'] <= 1)\n        assert_true(sample['c'] in ['x', 'y', 'z'])\n\n    # Test reproducibility\n    sampler_repeated = ParameterSampler(param_distributions, n_iter=n_iter, random_state=0)\n    samples_repeated = list(sampler_repeated)\n    assert_equal(samples, samples_repeated)\n", "def test_grid_search_with_custom_scorer():\n    # Test that GridSearchCV can use a custom scoring function\n        return np.mean(estimator.predict(X) == y)\n    \n    X, y = make_classification(n_samples=100, n_features=4, random_state=42)\n    param_grid = {'C': [0.1, 1, 10]}\n    \n    svc = SVC(gamma='scale')\n    grid_search = GridSearchCV(svc, param_grid=param_grid, scoring=custom_scorer)\n    grid_search.fit(X, y)\n    \n    # Check that custom scorer is used\n    assert 'mean_test_score' in grid_search.cv_results_\n    assert all(grid_search.cv_results_['mean_test_score'] >= 0)\n    assert all(grid_search.cv_results_['mean_test_score'] <= 1)\n    assert grid_search.best_score_ >= 0\n    assert grid_search.best_score_ <= 1\n"], "sample_749": ["def test_column_transformer_multiple_types():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [1, 3, 5]]).T\n    X_res = np.array([[0, 2, 1], [1, 4, 3], [2, 6, 5]]).T\n\n    # Test with a mix of transformers, passthrough, and drop\n    ct = ColumnTransformer([\n        ('scale', StandardScaler(), [0]),\n        ('double', DoubleTrans(), [1]),\n        ('drop', 'drop', [2])\n    ])\n\n    X_trans = ct.fit_transform(X_array)\n    expected = np.vstack([StandardScaler().fit_transform(X_array[:, [0]]).ravel(),\n                          DoubleTrans().fit_transform(X_array[:, [1]]).ravel()]).T\n    assert_array_equal(X_trans, expected)\n\n    # Test with passthrough and drop\n    ct = ColumnTransformer([\n        ('trans1', Trans(), [0]),\n        ('trans2', 'passthrough', [1]),\n        ('trans3', 'drop', [2])\n    ])\n    X_trans = ct.fit_transform(X_array)\n    expected = X_array[:, [0, 1]]\n    assert_array_equal(X_trans, expected)\n\n    # Test with passthrough and transformer_weights\n    transformer_weights = {'trans1': 0.1, 'trans2': 10}\n    ct = ColumnTransformer([\n        ('trans1', Trans(), [0]),\n        ('trans2', 'passthrough', [1])\n    ], transformer_weights=transformer_weights)\n    X_trans = ct.fit_transform(X_array)\n    expected = np.vstack([0.1 * X_array[:, 0], 10 * X_array[:, 1]]).T\n    assert_array_equal(X_trans, expected)\n\n    # Test with string columns and boolean mask\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(X_array, columns=['first', 'second', 'third'])\n    ct = ColumnTransformer([\n        ('trans1', Trans(), 'first'),\n        ('trans2', 'passthrough', 'second')\n    ])\n    X_trans = ct.fit_transform(X_df)\n    expected = X_array[:, [0, 1]]\n    assert_array_equal(X_trans, expected)\n\n    ct = ColumnTransformer([\n        ('trans1', Trans(), '", "def test_column_transformer_fit_before_transform():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', StandardScaler(with_std=False), [1])])\n    \n    # Ensure that transform raises an error if fit is not called first\n    with pytest.raises(NotFittedError):\n        ct.transform(X_array)\n\n    # Ensure that fit_transform can be called without fit\n    X_trans = ct.fit_transform(X_array)\n    assert X_trans is not None\n    assert X_trans.shape == (3, 2)\n\n    # Ensure that transform works after fit_transform\n    X_trans2 = ct.transform(X_array)\n    assert_array_equal(X_trans, X_trans2)\n", "def test_column_transformer_mixed_data():\n    pd = pytest.importorskip('pandas')\n\n    X_array = np.array([[0, 1, 2, 'cat'], [2, 4, 6, 'dog']]).T\n    X_df = pd.DataFrame(X_array, columns=['num1', 'num2', 'num3', 'cat'])\n\n    class CatTransformer(BaseEstimator):\n            return self\n\n            mapping = {'cat': 0, 'dog': 1}\n            return np.vectorize(mapping.get)(X).reshape(-1, 1)\n\n    num_transformer = StandardScaler()\n    cat_transformer = CatTransformer()\n\n    ct = ColumnTransformer([\n        ('num', num_transformer, ['num1', 'num2', 'num3']),\n        ('cat', cat_transformer, 'cat')\n    ])\n\n    X_trans = ct.fit_transform(X_df)\n    X_expected = np.hstack([\n        num_transformer.fit_transform(X_df[['num1', 'num2', 'num3']]),\n        cat_transformer.fit_transform(X_df['cat'])\n    ])\n\n    assert_array_equal(X_trans, X_expected)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'cat'\n    assert isinstance(ct.transformers_[-1][1], CatTransformer)\n", "def test_column_transformer_with_different_data_types():\n    pd = pytest.importorskip('pandas')\n\n    # Numpy array input\n    X_array = np.array([[1, 'A', 0.5], [2, 'B', 0.3], [3, 'C', 0.2]])\n    ct = ColumnTransformer([('num', StandardScaler(), [0, 2]),\n                            ('cat', DictVectorizer(), [1])])\n    X_trans = ct.fit_transform(X_array)\n    assert X_trans.shape == (3, 5)\n\n    # Pandas DataFrame input\n    X_df = pd.DataFrame({'num1': [1, 2, 3], 'cat': ['A', 'B', 'C'], 'num2': [0.5, 0.3, 0.2]})\n    ct = ColumnTransformer([('num', StandardScaler(), ['num1', 'num2']),\n                            ('cat', DictVectorizer(), 'cat')])\n    X_trans = ct.fit_transform(X_df)\n    assert X_trans.shape == (3, 5)\n\n    # Sparse matrix input\n    X_sparse = sparse.csr_matrix(np.array([[1, 0.5], [2, 0.3], [3, 0.2]]))\n    ct = ColumnTransformer([('num', StandardScaler(), [0, 1])])\n    X_trans = ct.fit_transform(X_sparse)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 2)\n", "def test_column_transformer_with_mixed_dtypes():\n    X_array = np.array([[1, 2.5, 'a'], [4, 3.6, 'b'], [6, 1.1, 'c']])\n    \n    # Define transformers for mixed data types\n    ct = ColumnTransformer(\n        [\n            ('numeric', StandardScaler(), [0, 1]),\n            ('categorical', OneHotEncoder(), [2])\n        ]\n    )\n\n    # Expected transformation: standard scaling for numeric columns, one-hot encoding for categorical\n    expected_numeric = StandardScaler().fit_transform(X_array[:, [0, 1]].astype(float))\n    expected_categorical = OneHotEncoder().fit_transform(X_array[:, [2]]).toarray()\n    expected_result = np.hstack([expected_numeric, expected_categorical])\n\n    # Fit and transform the data\n    result = ct.fit_transform(X_array)\n\n    # Check that the transformed result matches the expected result\n    assert_allclose_dense_sparse(result, expected_result)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n"], "sample_750": ["def test_orthogonal_mp_tol_negative():\n    assert_raises(ValueError, orthogonal_mp, X, y[:, 0], tol=-0.1)\n    assert_raises(ValueError, orthogonal_mp_gram, G, Xy[:, 0], tol=-0.1)\n", "def test_default_nonzero_coefs():\n    # Test the default behavior when n_nonzero_coefs and tol are None\n    default_coef = orthogonal_mp(X, y[:, 0])\n    assert_true(np.count_nonzero(default_coef) <= max(int(0.1 * X.shape[1]), 1))\n    \n    default_coef_gram = orthogonal_mp_gram(G, Xy[:, 0])\n    assert_true(np.count_nonzero(default_coef_gram) <= max(int(0.1 * G.shape[0]), 1))\n", "def test_omp_with_intercept():\n    # Test OMP with fit_intercept=True\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs, fit_intercept=True)\n    omp.fit(X, y[:, 0])\n    assert_equal(omp.coef_.shape, (n_features,))\n    assert_true(np.count_nonzero(omp.coef_) <= n_nonzero_coefs)\n    \n    # Check intercept is not zero\n    assert_true(omp.intercept_ != 0)\n\n    # Check predictions\n    y_pred = omp.predict(X)\n    assert_equal(y_pred.shape, y[:, 0].shape)\n    assert_true(np.allclose(y_pred, y[:, 0], atol=1e-1))\n\n    # Test OrthogonalMatchingPursuitCV with fit_intercept=True\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=True, max_iter=10, cv=5)\n    ompcv.fit(X, y[:, 0])\n    assert_equal(ompcv.n_nonzero_coefs_, n_nonzero_coefs)\n    assert_true(ompcv.intercept_ != 0)\n    \n    # Check predictions\n    y_pred_cv = ompcv.predict(X)\n    assert_equal(y_pred_cv.shape, y[:, 0].shape)\n    assert_true(np.allclose(y_pred_cv, y[:, 0], atol=1e-1))\n", "def test_invalid_precompute_option():\n    assert_raises(ValueError, orthogonal_mp, X, y, precompute='invalid_option')\n", "def test_invalid_precompute_argument():\n    invalid_precompute = \"invalid\"\n    assert_raises(ValueError, orthogonal_mp, X, y, n_nonzero_coefs=5, precompute=invalid_precompute)\n    assert_raises(ValueError, orthogonal_mp, X, y, tol=1.0, precompute=invalid_precompute)\n"], "sample_751": ["def test_random_forest_classifier():\n    # Test RandomForestClassifier on the iris dataset\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(iris.data, iris.target)\n    score = clf.score(iris.data, iris.target)\n    assert score > 0.9, \"Failed with score = %f\" % score\n\n    # Test feature importances\n    importances = clf.feature_importances_\n    assert_equal(importances.shape[0], iris.data.shape[1])\n\n    # Test predictions\n    predictions = clf.predict(iris.data)\n    assert_array_equal(predictions.shape, iris.target.shape)\n\n    # Test predict_proba\n    probas = clf.predict_proba(iris.data)\n    assert_equal(probas.shape, (iris.data.shape[0], len(clf.classes_)))\n", "def test_random_forest_classifier():\n    # Check RandomForestClassifier on a toy dataset.\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n    assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)\n    assert_equal(clf.predict_proba(T).shape, (len(T), 2))\n    assert_equal(clf.predict_log_proba(T).shape, (len(T), 2))\n\n    # Check feature importances\n    clf.fit(iris.data, iris.target)\n    importances = clf.feature_importances_\n    assert_equal(importances.shape[0], iris.data.shape[1])\n    assert_greater(importances.sum(), 0)\n\n    # Check oob score\n    clf = RandomForestClassifier(n_estimators=20, oob_score=True, random_state=0)\n    clf.fit(iris.data, iris.target)\n    assert_greater(clf.oob_score_, 0.8)\n", "def test_random_forest_classifier():\n    # Check RandomForestClassifier on a toy dataset.\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n    assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)\n    assert_equal(clf.predict_proba(T).shape, (len(T), 2))\n    assert_equal(clf.decision_path(T)[0].shape, (len(T), sum([est.tree_.node_count for est in clf.estimators_])))\n\n    # Check consistency on dataset iris.\n    clf.fit(iris.data, iris.target)\n    assert_array_equal(np.unique(iris.target), clf.classes_)\n    proba = clf.predict_proba(iris.data)\n    assert_equal(proba.shape[1], len(clf.classes_))\n    assert_equal(clf.decision_function(iris.data).shape[1], len(clf.classes_))\n    assert_greater(clf.score(iris.data, iris.target), 0.9)\n\n    # Check feature importances\n    importances = clf.feature_importances_\n    assert_equal(importances.shape[0], iris.data.shape[1])\n    assert_greater(importances.sum(), 0.99)\n    assert_greater(importances.max(), 0.1)\n", "def test_random_forest_classifier():\n    # Test RandomForestClassifier on a toy dataset.\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n    assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)\n    assert_equal(clf.predict_proba(T).shape, (len(T), 2))\n    assert_equal(clf.decision_function(T).shape, (len(T),))\n    assert_greater(clf.score(X, y_class), 0.9)\n    assert_true(len(clf.estimators_) > 1)\n    assert_equal(len(set(est.random_state for est in clf.estimators_)), len(clf.estimators_))\n", "def test_random_forest_classification():\n    # Check RandomForestClassifier on a toy dataset.\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n\n    # Check feature importances\n    importances = clf.feature_importances_\n    assert_equal(importances.shape[0], len(X[0]))\n\n    # Check predict_proba\n    proba = clf.predict_proba(T)\n    assert_equal(proba.shape[1], len(set(y_class)))\n\n    # Check decision_path\n    n_samples, n_features = np.array(X).shape\n    indicator, n_nodes_ptr = clf.decision_path(X)\n    assert_equal(indicator.shape[0], n_samples)\n    assert_equal(n_nodes_ptr.shape[0], len(clf.estimators_) + 1)\n\n    # Check feature importances are consistent with their documentation\n    assert (importances >= 0).all()\n    assert (importances <= 1).all()\n    assert_almost_equal(importances.sum(), 1.0)\n\n    # Check apply method\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (n_samples, clf.n_estimators))\n\n    # Check estimator parameters\n    assert_equal(clf.n_estimators, 10)\n    assert_equal(clf.random_state, 0)\n    assert_true(clf.bootstrap)\n    assert_false(clf.oob_score)\n    assert_equal(clf.n_jobs, 1)\n    assert_false(clf.warm_start)\n    assert_is_none(clf.class_weight)\n"], "sample_752": ["def test_iforest_contamination_deprecation_warning():\n    \"\"\"Test contamination parameter deprecation warning.\"\"\"\n    X_train = iris.data\n    with pytest.warns(FutureWarning, match='default contamination parameter 0.1 will change'):\n        clf = IsolationForest(contamination=\"legacy\").fit(X_train)\n\n    assert_equal(clf._contamination, 0.1)\n    \n    with pytest.warns(None) as record:\n        clf = IsolationForest(contamination=0.2).fit(X_train)\n    assert not any(w.category == FutureWarning for w in record)\n    assert_equal(clf._contamination, 0.2)\n", "def test_iforest_contamination_auto():\n    \"\"\"Test iforest with contamination set to 'auto'\"\"\"\n    X_train = [[0, 0], [1, 1], [2, 2]]\n    clf = IsolationForest(contamination='auto').fit(X_train)\n    assert_equal(clf._contamination, 'auto')\n    assert_equal(clf.offset_, -0.5)\n    # Check if the calculated threshold is correct\n    score_samples = clf.score_samples(X_train)\n    threshold = sp.stats.scoreatpercentile(score_samples, 100 * 0.1)\n    assert_almost_equal(clf._threshold_, threshold)\n", "def test_iforest_feature_importances():\n    \"\"\"Test if feature importances are calculated correctly.\"\"\"\n    X_train = np.array([[0, 1], [1, 2], [2, 1], [1, 0], [2, 2]])\n    clf = IsolationForest(n_estimators=10, random_state=rng)\n    clf.fit(X_train)\n    \n    # Check if the sum of feature importances is 1\n    importances = clf.feature_importances_\n    assert_almost_equal(importances.sum(), 1.0, decimal=10)\n    \n    # Check if feature importances shape matches number of features\n    assert_equal(importances.shape[0], X_train.shape[1])\n    \n    # Check if feature importances are non-negative\n    assert_greater(np.min(importances), 0)\n", "def test_predict():\n    \"\"\"Test predict method on a toy dataset\"\"\"\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]\n    clf = IsolationForest(random_state=rng, contamination=0.25)\n    clf.fit(X)\n    predictions = clf.predict(X)\n    expected_predictions = np.array([1, 1, 1, 1, 1, 1, -1, -1])\n    assert_array_equal(predictions, expected_predictions)\n", "def test_iforest_contamination():\n    \"\"\"Test different contamination settings for Isolation Forest.\"\"\"\n    X = iris.data\n\n    # Test default contamination\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X)\n    scores = clf.score_samples(X)\n    decision = clf.decision_function(X)\n    assert_equal(clf.contamination, \"legacy\")\n    assert_almost_equal(clf.offset_, sp.stats.scoreatpercentile(scores, 100. * 0.1))\n\n    # Test auto contamination\n    clf = IsolationForest(random_state=rng, contamination=\"auto\")\n    clf.fit(X)\n    scores = clf.score_samples(X)\n    decision = clf.decision_function(X)\n    assert_equal(clf.contamination, \"auto\")\n    assert_equal(clf.offset_, -0.5)\n\n    # Test specific contamination value\n    contamination = 0.2\n    clf = IsolationForest(random_state=rng, contamination=contamination)\n    clf.fit(X)\n    scores = clf.score_samples(X)\n    decision = clf.decision_function(X)\n    assert_equal(clf.contamination, contamination)\n    assert_almost_equal(clf.offset_, sp.stats.scoreatpercentile(scores, 100. * contamination))\n"], "sample_753": ["def test_logistic_regression_path_input_validation():\n    # Test that logistic_regression_path raises appropriate errors for invalid inputs\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 2)\n    y = rng.randint(0, 2, 100)\n    Cs = [1e-3, 1, 1e3]\n    \n    # Check for invalid Cs (not a list or integer)\n    msg = \"Cs must be a list or integer, got <class 'str'> instead\"\n    assert_raise_message(ValueError, msg, logistic_regression_path, X, y, Cs='invalid')\n    \n    # Check for invalid pos_class in binary classification\n    msg = \"To fit OvR, use the pos_class argument\"\n    assert_raise_message(ValueError, msg, logistic_regression_path, X, y, pos_class=None, multi_class='ovr')\n    \n    # Check for invalid solver\n    msg = \"Logistic Regression supports only liblinear, newton-cg, lbfgs, sag and saga solvers, got invalid_solver\"\n    assert_raise_message(ValueError, msg, logistic_regression_path, X, y, solver='invalid_solver')\n    \n    # Check for invalid penalty with solver\n    msg = \"Solver lbfgs supports only l2 penalties, got l1 penalty.\"\n    assert_raise_message(ValueError, msg, logistic_regression_path, X, y, solver='lbfgs', penalty='l1')\n    \n    # Check for dual=True with solver not supporting it\n    msg = \"Solver lbfgs supports only dual=False, got dual=True\"\n    assert_raise_message(ValueError, msg, logistic_regression_path, X, y, solver='lbfgs', dual=True)\n    \n    # Check for invalid multi_class with solver\n    msg = \"Solver liblinear does not support a multinomial backend.\"\n    assert_raise_message(ValueError, msg, logistic_regression_path, X, y, solver='liblinear', multi_class='multinomial')\n    \n    # Check for invalid fit_intercept\n    msg = \"fit_intercept should be a boolean, got 123\"\n    assert_raise_message(ValueError, msg, logistic_regression_path, X, y, fit_intercept=123)\n    \n    # Check for invalid max_iter\n    msg = \"Maximum number of iterations for the solver must be a positive integer, got max_iter=-1\"\n    assert_raise_message(ValueError", "def test_logistic_loss():\n    # Test the logistic loss function directly\n    X, y = make_classification(n_samples=20, n_features=5, random_state=0)\n    w = np.zeros(X.shape[1])\n    alpha = 1.0\n    loss = _logistic_loss(w, X, y, alpha)\n    \n    # Manually compute the logistic loss for comparison\n    z = safe_sparse_dot(X, w)\n    yz = y * z\n    expected_loss = -np.sum(log_logistic(yz)) + 0.5 * alpha * np.dot(w, w)\n    \n    assert_almost_equal(loss, expected_loss, decimal=5)\n", "def test_logistic_regression_path_multinomial():\n    # Test the logistic_regression_path function for multinomial case\n    X, y = make_classification(n_samples=100, n_features=10, n_classes=3, random_state=0)\n    Cs = [0.01, 0.1, 1, 10, 100]\n\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=Cs, fit_intercept=True, solver=solver, multi_class='multinomial', max_iter=100)\n\n        # Check that the shape of the coefficients is correct\n        assert_equal(len(coefs), len(Cs))\n        for coef in coefs:\n            assert_equal(coef.shape, (3, X.shape[1] + 1))\n\n        # Check that the number of iterations is correct\n        assert_equal(len(n_iter), len(Cs))\n        for n in n_iter:\n            assert n > 0\n", "def test_logistic_regression_penalty():\n    # Test that LogisticRegression correctly handles different penalties\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n    \n    penalties = ['l1', 'l2']\n    solvers = ['liblinear', 'saga']\n\n    for penalty in penalties:\n        for solver in solvers:\n            clf = LogisticRegression(penalty=penalty, solver=solver, random_state=42)\n            clf.fit(X, y)\n            assert clf.penalty == penalty, \"Penalty does not match the expected value\"\n            assert clf.solver == solver, \"Solver does not match the expected value\"\n            assert clf.coef_.shape == (len(np.unique(y)), X.shape[1]), \"Coefficient shape mismatch\"\n\n    # Check that invalid penalty raises a ValueError\n    clf = LogisticRegression(penalty='invalid', solver='liblinear', random_state=42)\n    assert_raises(ValueError, clf.fit, X, y)\n", "def test_logistic_regression_path_multinomial():\n    # Test logistic_regression_path with multinomial loss\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3,\n                               n_informative=10, random_state=0)\n    Cs = np.logspace(-4, 4, 10)\n\n    coefs, Cs_used, n_iter = logistic_regression_path(\n        X, y, Cs=Cs, fit_intercept=True, tol=1e-5, solver='lbfgs',\n        multi_class='multinomial', max_iter=1000, random_state=0\n    )\n\n    # Check that the shapes of coefficients are correct\n    assert_equal(len(coefs), len(Cs))\n    assert_equal(len(Cs_used), len(Cs))\n    for coef in coefs:\n        assert_equal(coef.shape, (3, 21))  # 3 classes, 20 features + intercept\n\n    # Check that the number of iterations is as expected\n    assert_equal(n_iter.shape, (len(Cs),))\n    for n in n_iter:\n        assert n <= 1000\n\n    # Check that the loss decreases as C increases\n    losses = []\n    for coef, C in zip(coefs, Cs):\n        lr = LogisticRegression(C=C, fit_intercept=True, tol=1e-5, solver='lbfgs',\n                                multi_class='multinomial', max_iter=1000, random_state=0)\n        lr.coef_ = coef[:, :-1]\n        lr.intercept_ = coef[:, -1]\n        losses.append(log_loss(y, lr.predict_proba(X)))\n\n    assert np.all(np.diff(losses) < 0)\n"], "sample_754": ["def test_sparse_pca_zero_alpha(norm_comp):\n    # Test SparsePCA with alpha set to zero which should behave like PCA\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    spca = SparsePCA(n_components=3, alpha=0, random_state=rng,\n                     normalize_components=norm_comp)\n    spca.fit(Y)\n    pca = PCA(n_components=3)\n    pca.fit(Y)\n    \n    # Compare the components of SparsePCA with alpha=0 and PCA\n    assert_allclose(np.abs(spca.components_.dot(pca.components_.T)),\n                    np.eye(3), atol=1e-5)\n    \n    # Compare the transformed data\n    U_spca = spca.transform(Y)\n    U_pca = pca.transform(Y)\n    U_spca *= np.sign(U_spca[0, :])\n    U_pca *= np.sign(U_pca[0, :])\n    assert_allclose(U_spca, U_pca)\n", "def test_random_state(norm_comp):\n    # Test that providing different random states gives different results\n    rng1 = np.random.RandomState(0)\n    rng2 = np.random.RandomState(1)\n    \n    Y, _, _ = generate_toy_data(3, 100, (8, 8), random_state=rng1)  # generate data\n    \n    spca1 = SparsePCA(n_components=3, random_state=rng1, normalize_components=norm_comp)\n    spca1.fit(Y)\n    U1 = spca1.transform(Y)\n    \n    spca2 = SparsePCA(n_components=3, random_state=rng2, normalize_components=norm_comp)\n    spca2.fit(Y)\n    U2 = spca2.transform(Y)\n    \n    # Check if the components are different\n    assert_false(np.allclose(spca1.components_, spca2.components_))\n    # Check if the transformed data is different\n    assert_false(np.allclose(U1, U2))\n", "def test_sparsepca_tolerance(norm_comp):\n    # Test that SparsePCA stops iterating when tolerance level is reached\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    tol_values = [1e-3, 1e-4, 1e-6]\n    \n    for tol in tol_values:\n        spca = SparsePCA(n_components=8, tol=tol, random_state=rng,\n                         normalize_components=norm_comp)\n        spca.fit(X)\n        assert_true(spca.n_iter_ < spca.max_iter)\n", "def test_ridge_alpha_in_constructor(norm_comp):\n    alpha = 1\n    ridge_alpha = 0.05\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array\n    spca = SparsePCA(n_components=3, method='lars', alpha=alpha,\n                     ridge_alpha=ridge_alpha, random_state=0, \n                     normalize_components=norm_comp)\n    spca.fit(Y)\n    U = spca.transform(Y)\n    assert spca.ridge_alpha == ridge_alpha\n    assert U.shape == (10, 3)\n", "def test_sparsepca_deprecated_warning(norm_comp):\n    # Test that using ridge_alpha in transform raises the proper deprecation warning\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    spca = SparsePCA(n_components=3, random_state=rng, normalize_components=norm_comp, ridge_alpha=0.05)\n    spca.fit(Y)\n\n    warning_msg = \"The ridge_alpha parameter on transform() is deprecated since 0.19 and will be removed in 0.21.\"\n    with pytest.warns(DeprecationWarning, match=warning_msg):\n        spca.transform(Y, ridge_alpha=0.01)\n    with pytest.warns(DeprecationWarning, match=warning_msg):\n        spca.transform(Y, ridge_alpha=None)\n"], "sample_755": ["def test_davies_bouldin_score():\n    # Test Davies-Bouldin score function\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # General case with defined clusters\n    X = np.array([[0.1, 0.2], [0.2, 0.1], [1.1, 1.2], [1.2, 1.1],\n                  [2.1, 2.2], [2.2, 2.1]])\n    labels = np.array([0, 0, 1, 1, 2, 2])\n    score = davies_bouldin_score(X, labels)\n    assert score > 0.0\n\n    # Test edge case with all samples in the same cluster\n    X_same_cluster = np.array([[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]])\n    labels_same_cluster = np.array([0, 0, 0])\n    score_same_cluster = davies_bouldin_score(X_same_cluster, labels_same_cluster)\n    assert score_same_cluster == 0.0\n\n    # Test with non-numpy array input\n    X_list = [[0.1, 0.2], [0.2, 0.1], [1.1, 1.2], [1.2, 1.1],\n              [2.1, 2.2], [2.2, 2.1]]\n    labels_list = [0, 0, 1, 1, 2, 2]\n    score_list = davies_bouldin_score(X_list, labels_list)\n    assert score_list > 0.0\n\n    # Edge case with very small clusters\n    X_small_clusters = np.array([[0.1, 0.2], [1.1, 1.2], [2.1, 2.2]])\n    labels_small_clusters = np.array([0, 1, 2])\n    score_small_clusters = davies_bouldin_score(X_small_clusters, labels_small_clusters)\n    assert score_small_clusters == 0.0\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                      [0] * 5 + [1] * 5)\n\n    # General case\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3],\n                  [8, 8], [9, 9], [10, 10], [11, 11]])\n    labels = [0, 0, 0, 0, 1, 1, 1, 1]\n    pytest.approx(davies_bouldin_score(X, labels), 0.5)\n", "def test_davies_bouldin_score():\n    # Testing Davies-Bouldin score for different clustering scenarios\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Simple case where all points are in two well-separated clusters\n    X = np.array([[0, 0], [1, 1], [2, 2], [10, 10], [11, 11], [12, 12]])\n    labels = [0, 0, 0, 1, 1, 1]\n    score = davies_bouldin_score(X, labels)\n    assert score == 0.0  # Perfect separation, should result in 0 score\n\n    # Case where the clusters are not well-separated\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n    labels = [0, 0, 1, 1, 2, 2]\n    score = davies_bouldin_score(X, labels)\n    assert score > 0.0  # Non-zero score for overlapping clusters\n\n    # General case (with non numpy arrays)\n    X = [[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 + [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    score = davies_bouldin_score(X, labels)\n    assert score > 0.0  # General case should result in a positive score\n\n    # Ensure that a single-cluster case raises a ValueError\n    X = np.random.rand(10, 2)\n    labels = np.zeros(10)\n    with pytest.raises(ValueError):\n        davies_bouldin_score(X, labels)\n\n    # Ensure that the function works with sparse matrices\n    X = sp.csr_matrix([[0, 0], [1, 1], [2, 2], [10, 10], [11, 11], [", "def test_davies_bouldin_score():\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = dataset.target\n\n    # Testing with more than one cluster\n    score = davies_bouldin_score(X, y)\n    assert score > 0\n\n    # Testing with only one cluster (should raise ValueError)\n    y_one_cluster = np.zeros(X.shape[0])\n    assert_raises_regexp(ValueError,\n                         r'Number of labels is %d\\. Valid values are 2 '\n                         r'to n_samples - 1 \\(inclusive\\)' % len(np.unique(y_one_cluster)),\n                         davies_bouldin_score, X, y_one_cluster)\n\n    # Testing when all points are in different clusters (should raise ValueError)\n    y_all_different = np.arange(X.shape[0])\n    assert_raises_regexp(ValueError,\n                         r'Number of labels is %d\\. Valid values are 2 '\n                         r'to n_samples - 1 \\(inclusive\\)' % len(np.unique(y_all_different)),\n                         davies_bouldin_score, X, y_all_different)\n\n    # Testing with manually created clusters\n    X_manual = np.array([[1, 2], [1, 4], [1, 0],\n                         [10, 2], [10, 4], [10, 0]])\n    y_manual = np.array([0, 0, 0, 1, 1, 1])\n    score_manual = davies_bouldin_score(X_manual, y_manual)\n    assert score_manual == 0  # Perfect separation, thus score should be 0\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0.0 == davies_bouldin_score(np.ones((10, 2)),\n                                       [0] * 5 + [1] * 5)\n\n    # Simple case with distinct clusters\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1],\n                  [9, 9], [9, 10], [10, 9], [10, 10]])\n    labels = [0, 0, 0, 0, 1, 1, 1, 1]\n    assert davies_bouldin_score(X, labels) > 0\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(davies_bouldin_score(X, labels), 0.25)\n\n    # Test with different data distributions\n    X1 = np.random.rand(100, 2)\n    y1 = np.array([0] * 25 + [1] * 25 + [2] * 25 + [3] * 25)\n    assert davies_bouldin_score(X1, y1) > 0\n\n    X2 = np.concatenate([np.random.randn(50, 2) + [i * 5, i * 5] for i in range(3)], axis=0)\n    y2 = np.array([0] * 50 + [1] * 50 + [2] * 50)\n    assert davies_bouldin_score(X2, y2) > 0\n"], "sample_756": ["def test_fit_with_different_metrics():\n    # Test OPTICS with different distance metrics\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.4, random_state=42)\n\n    metrics = ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']\n    for metric in metrics:\n        clust = OPTICS(min_samples=10, metric=metric)\n        clust.fit(X)\n        assert clust.labels_.shape == (len(X),)\n        assert clust.labels_.dtype.kind == 'i'\n        assert clust.reachability_.shape == (len(X),)\n        assert clust.reachability_.dtype.kind == 'f'\n        assert clust.core_distances_.shape == (len(X),)\n        assert clust.core_distances_.dtype.kind == 'f'\n        assert clust.ordering_.shape == (len(X),)\n        assert clust.ordering_.dtype.kind == 'i'\n        assert set(clust.ordering_) == set(range(len(X)))\n", "def test_core_distances_computation():\n    # Test the core distances computation in _compute_core_distances_\n\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=100, centers=centers,\n                                cluster_std=0.4, random_state=0)\n    \n    clust = OPTICS(min_samples=5)\n    neighbors = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(X)\n    core_distances = clust._compute_core_distances_(X, neighbors)\n    \n    # Check if core distances are computed correctly\n    expected_core_distances = neighbors.kneighbors(X, 5)[0][:, -1]\n    assert_allclose(core_distances, expected_core_distances)\n    \n    # Check if core distances for non-core points are set to inf\n    clust_with_inf = OPTICS(min_samples=5, max_eps=0.1)\n    neighbors_with_inf = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(X)\n    core_distances_with_inf = clust_with_inf._compute_core_distances_(X, neighbors_with_inf)\n    core_distances_with_inf[core_distances_with_inf > 0.1] = np.inf\n    assert np.all(core_distances_with_inf[core_distances_with_inf != np.inf] <= 0.1)\n    assert np.all(core_distances_with_inf[core_distances_with_inf == np.inf] == np.inf)\n", "def test_core_distances_computation():\n    # Test that core distances are computed correctly\n    \n    X = np.array([[0, 0], [1, 1], [2, 2], [10, 10]])\n    nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n    \n    # Create an OPTICS instance\n    clust = OPTICS(min_samples=2)\n    core_distances = clust._compute_core_distances_(X, nbrs)\n    \n    # Compute expected core distances\n    expected_core_distances = nbrs.kneighbors(X, 2)[0][:, -1]\n\n    assert_allclose(core_distances, expected_core_distances, rtol=1e-5, atol=1e-8)\n", "def test_invalid_metric():\n    # Test an invalid metric string\n    msg = \"Metric 'invalid_metric' not valid. Use 'minkowski' or another valid metric.\"\n\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, _ = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)\n\n    # Compute OPTICS with invalid metric\n    clust = OPTICS(max_eps=1.0, min_samples=10, metric='invalid_metric')\n    \n    # Run the fit and check if it raises the correct error message\n    assert_raise_message(ValueError, msg, clust.fit, X)\n", "def test_optics_return_values():\n    # Test that the optics function returns correct core_sample_indices_ and labels_\n    \n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    core_sample_indices, labels = optics(X, min_samples=10, max_eps=0.3)\n    \n    # Check that core_sample_indices and labels are arrays\n    assert isinstance(core_sample_indices, np.ndarray)\n    assert isinstance(labels, np.ndarray)\n    \n    # Check the dimensions of the return values\n    assert core_sample_indices.ndim == 1\n    assert labels.ndim == 1\n    \n    # Check that the length of labels matches the input data\n    assert len(labels) == len(X)\n    \n    # Check that core_sample_indices is not empty\n    assert len(core_sample_indices) > 0\n\n    # Ensure all core_sample_indices are valid indices in the input data\n    assert np.all(core_sample_indices < len(X))\n    assert np.all(core_sample_indices >= 0)\n"], "sample_757": ["def test_one_hot_encoder_empty_input():\n    # Test OneHotEncoder with empty input\n    enc = OneHotEncoder(categories='auto')\n    X_empty = np.empty((0, 3))\n    \n    # fitting should work with empty input\n    enc.fit(X_empty)\n    \n    # transforming should yield an empty output with the same number of features\n    X_trans = enc.transform(X_empty)\n    assert_equal(X_trans.shape, (0, 0))\n    \n    # inverse_transform should also yield an empty output\n    X_inv = enc.inverse_transform(X_trans)\n    assert_equal(X_inv.shape, (0, 3))\n", "def test_ordinal_encoder_pandas():\n    pd = pytest.importorskip('pandas')\n\n    X_df = pd.DataFrame({'A': ['Male', 'Female', 'Female'], 'B': [1, 2, 3]})\n    enc = OrdinalEncoder()\n    X_tr = enc.fit_transform(X_df)\n\n    expected = np.array([[1., 0.],\n                         [0., 1.],\n                         [0., 2.]])\n\n    assert_array_equal(X_tr, expected)\n\n    # Testing inverse_transform\n    X_inv = enc.inverse_transform(X_tr)\n    expected_inv = np.array([['Male', 1],\n                             ['Female', 2],\n                             ['Female', 3]], dtype=object)\n\n    assert_array_equal(X_inv, expected_inv)\n\n    # Testing with different categories order\n    enc = OrdinalEncoder(categories=[['Female', 'Male'], [3, 2, 1]])\n    X_tr = enc.fit_transform(X_df)\n\n    expected = np.array([[1., 2.],\n                         [0., 1.],\n                         [0., 0.]])\n\n    assert_array_equal(X_tr, expected)\n\n    X_inv = enc.inverse_transform(X_tr)\n    assert_array_equal(X_inv, expected_inv)\n", "def test_ordinal_encoder_not_fitted():\n    X = np.array([['a'], ['b']])\n    enc = OrdinalEncoder(categories=[['a', 'b']])\n    msg = (\"This OrdinalEncoder instance is not fitted yet. \"\n           \"Call 'fit' with appropriate arguments before using this method.\")\n    with pytest.raises(NotFittedError, match=msg):\n        enc.transform(X)\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([[0, 2], [1, 0], [1, 1]])\n    X2 = np.array([[2, 1]])\n\n    # Test that ordinal encoder raises error for unknown features\n    # present during transform.\n    oe = OrdinalEncoder(categories='auto', dtype=np.float64)\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n\n    # Test the ignore option, ignores unknown features (giving all 0's)\n    oe = OrdinalEncoder(categories=[range(3)], dtype=np.float64)\n    assert_raises(ValueError, oe.fit_transform, X2)\n\n    # Raise error if handle_unknown is neither 'error' nor 'ignore'\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        oe.fit(X)\n        oe.transform(X2)\n", "def test_one_hot_encoder_invalid_handle_unknown():\n    # Test that one hot encoder raises error for invalid handle_unknown value\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])\n    invalid_values = ['raise', 'warn', 'unknown']\n\n    for value in invalid_values:\n        oh = OneHotEncoder(handle_unknown=value)\n        with pytest.raises(ValueError, match=\"handle_unknown should be either 'error' or 'ignore'\"):\n            oh.fit(X)\n"], "sample_758": ["def test_check_array_object_dtype_conversion():\n    # Check that object dtype arrays are converted to float\n    X = np.array([[1, 2, 3], [4, 5, 6]], dtype=object)\n    X_checked = check_array(X, dtype=\"numeric\")\n    assert_equal(X_checked.dtype, np.float64)\n\n    # Check that object dtype arrays with mixed types are converted to float\n    X_mixed = np.array([[1, 2, 3], [4.0, 5, 6]], dtype=object)\n    X_checked = check_array(X_mixed, dtype=\"numeric\")\n    assert_equal(X_checked.dtype, np.float64)\n\n    # Check that object dtype arrays with strings raise a ValueError\n    X_strings = np.array([[1, \"two\", 3], [4, 5, \"six\"]], dtype=object)\n    assert_raises(ValueError, check_array, X_strings, dtype=\"numeric\")\n", "def test_check_array_raise_on_ndim_exceeding_allowed():\n    # Test that check_array raises an error when ndim exceeds allowed\n\n    X = np.arange(27).reshape(3, 3, 3)\n\n    with pytest.raises(ValueError, match=\"Found array with dim 3. Estimator expected <= 2.\"):\n        check_array(X, allow_nd=False)\n\n    # Test with a 4D array\n    X_4d = np.arange(81).reshape(3, 3, 3, 3)\n\n    with pytest.raises(ValueError, match=\"Found array with dim 4. Estimator expected <= 2.\"):\n        check_array(X_4d, allow_nd=False)\n\n    # Ensure it doesn't raise when allow_nd=True\n    X_checked = check_array(X_4d, allow_nd=True)\n    assert_array_equal(X_checked, X_4d)\n", "def test_check_symmetric_sparse_matrix():\n    # Test check_symmetric with different types of sparse matrices\n    arr_asym = np.array([[0, 2], [0, 2]])\n    arr_sym = np.array([[0, 1], [1, 2]])\n\n    for sparse_format in [sp.csr_matrix, sp.csc_matrix, sp.coo_matrix, sp.lil_matrix, sp.dok_matrix, sp.bsr_matrix]:\n        sp_asym = sparse_format(arr_asym)\n        sp_sym = sparse_format(arr_sym)\n\n        assert_warns(UserWarning, check_symmetric, sp_asym)\n        assert_raises(ValueError, check_symmetric, sp_asym, raise_exception=True)\n        output = check_symmetric(sp_asym, raise_warning=False)\n        assert_array_equal(output.toarray(), sp_sym.toarray())\n        assert_equal(output.format, sp_asym.format)\n", "def test_check_is_fitted_custom_message():\n    class CustomEstimator:\n            self.is_fitted_ = False\n\n            self.is_fitted_ = True\n\n    est = CustomEstimator()\n\n    try:\n        assert_raises(NotFittedError, check_is_fitted, est, \"is_fitted_\", \n                      msg=\"Custom message: Estimator not fitted yet.\")\n    except NotFittedError as e:\n        assert_equal(str(e), \"Custom message: Estimator not fitted yet.\")\n\n    est.fit(*make_blobs())\n    assert_equal(None, check_is_fitted(est, \"is_fitted_\"))\n", "def test_check_array_large_sparse_deprecation():\n    # Test deprecation warning for accept_large_sparse\n    X_csr = sp.rand(10, 10, format='csr', dtype=np.float32)\n    \n    with pytest.warns(FutureWarning, match=\"accept_large_sparse parameter is deprecated\"):\n        check_array(X_csr, accept_sparse=True, accept_large_sparse=True)\n    \n    with pytest.warns(FutureWarning, match=\"accept_large_sparse parameter is deprecated\"):\n        check_array(X_csr, accept_sparse=True, accept_large_sparse=False)\n"], "sample_759": ["def test_ordinal_encoder_handle_unknown():\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])\n    X2 = np.array([[4, 1, 1]])\n\n    # Test that ordinal encoder raises error for unknown features\n    # present during transform.\n    oe = OrdinalEncoder(categories='auto')\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n\n    # Ensure that the transformed data is not modified in place\n    X2_transformed = X2.copy()\n    assert_raises(ValueError, oe.transform, X2_transformed)\n    assert_array_equal(X2, X2_transformed)\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['a', 'b'], ['c', 'd']], dtype=object)\n    X_test = np.array([['a', 'e'], ['f', 'd']], dtype=object)\n\n    # Test that OrdinalEncoder raises error for unknown categories\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_test)\n\n    # Test the ignore option, which should raise an error since it's not implemented\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    with pytest.raises(ValueError, match=\"handle_unknown should be either 'error' or 'ignore', got 'ignore'.\"):\n        enc.fit(X)\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['cat1', 1], ['cat2', 2], ['cat3', 3]], dtype=object)\n    X2 = np.array([['cat4', 4]], dtype=object)\n\n    # Test that OrdinalEncoder raises error for unknown categories during transform\n    ord_enc = OrdinalEncoder()\n    ord_enc.fit(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        ord_enc.transform(X2)\n\n    # Test with handle_unknown set to 'ignore' (which OrdinalEncoder doesn't support, should raise an error on init)\n    with pytest.raises(TypeError):\n        ord_enc = OrdinalEncoder(handle_unknown='ignore')\n", "def test_ordinal_encoder_unseen_categories():\n    X_train = [['cat', 1], ['dog', 2], ['mouse', 3]]\n    X_test = [['cat', 1], ['dog', 4], ['elephant', 2]]\n    \n    enc = OrdinalEncoder()\n    enc.fit(X_train)\n    \n    # Test that unseen categories during transform raise an error\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_test)\n\n    # Test the ignore option, unseen categories will be encoded as NaN\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    X_trans = enc.fit_transform(X_train)\n    assert_array_equal(X_trans, [[0, 0], [1, 1], [2, 2]])\n    \n    X_test_trans = enc.transform(X_test)\n    expected = np.array([[0, 0], [1, np.nan], [np.nan, 1]])\n    assert_array_equal(X_test_trans, expected)\n", "def test_ordinal_encoder_handle_unknown():\n    X = [['a', 1], ['b', 2]]\n    X2 = [['a', 3], ['c', 2]]\n\n    # Test that ordinal encoder raises error for unknown features\n    # present during transform.\n    enc = OrdinalEncoder(handle_unknown='error')\n    enc.fit(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X2)\n\n    # Test the ignore option, ignores unknown features (giving -1 for unknown)\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X2_trans = enc.transform(X2)\n    assert_array_equal(X2_trans, np.array([[0., -1.], [-1., 1.]]))\n\n    # Raise error if handle_unknown is neither ignore nor error\n    enc = OrdinalEncoder(handle_unknown='unknown_option')\n    with pytest.raises(ValueError, match=\"handle_unknown should be either 'error' or 'ignore'\"):\n        enc.fit(X)\n"], "sample_760": ["def test_get_scorer():\n    # Test the get_scorer function.\n    assert get_scorer('accuracy') == SCORERS['accuracy']\n    assert get_scorer('f1') == SCORERS['f1']\n    assert get_scorer('r2') == SCORERS['r2']\n\n    with pytest.raises(ValueError, match=\"'invalid_score' is not a valid scoring value\"):\n        get_scorer('invalid_score')\n    \n    custom_scorer = make_scorer(accuracy_score)\n    assert get_scorer(custom_scorer) == custom_scorer\n", "def test_scorer_factory_args():\n    # Test the _factory_args method for different scorers\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_mm, y_mm)\n\n    # Test for _PredictScorer\n    scorer = make_scorer(accuracy_score)\n    assert_equal(scorer._factory_args(), \"\")\n    \n    # Test for _ProbaScorer\n    proba_scorer = make_scorer(log_loss, needs_proba=True)\n    assert_equal(proba_scorer._factory_args(), \", needs_proba=True\")\n    \n    # Test for _ThresholdScorer\n    threshold_scorer = make_scorer(roc_auc_score, needs_threshold=True)\n    assert_equal(threshold_scorer._factory_args(), \", needs_threshold=True\")\n    \n    # Check if they are callable and return expected results\n    assert isinstance(scorer(clf, X_mm, y_mm), float)\n    assert isinstance(proba_scorer(clf, X_mm, y_mm), float)\n    assert isinstance(threshold_scorer(clf, X_mm, y_mm), float)\n", "def test_make_scorer_greater_is_better():\n    # Test make_scorer with greater_is_better=False\n        return np.sum(np.abs(y_true - y_pred))\n\n    reg = DecisionTreeRegressor()\n    X, y = make_blobs(random_state=0, centers=2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    reg.fit(X_train, y_train)\n\n    scorer = make_scorer(custom_score, greater_is_better=False)\n    score1 = scorer(reg, X_test, y_test)\n    score2 = -custom_score(y_test, reg.predict(X_test))\n    assert_almost_equal(score1, score2)\n", "def test_get_scorer():\n    # Test get_scorer function with valid and invalid inputs.\n\n    # Valid inputs\n    scorer = get_scorer('accuracy')\n    assert isinstance(scorer, _PredictScorer)\n    assert_almost_equal(scorer(EstimatorWithFitAndPredict().fit([[1], [2], [3]], [1, 1, 0]), [[1], [2], [3]], [1, 1, 0]), 1.0)\n\n    scorer = get_scorer('roc_auc')\n    assert isinstance(scorer, _ThresholdScorer)\n\n    scorer = get_scorer('neg_log_loss')\n    assert isinstance(scorer, _ProbaScorer)\n\n    # Invalid input\n    with pytest.raises(ValueError, match=\"is not a valid scoring value\"):\n        get_scorer('invalid_scorer')\n", "def test_get_scorer():\n    # Test that get_scorer retrieves the correct scorer\n    assert get_scorer('accuracy') == accuracy_scorer\n    assert get_scorer('r2') == r2_scorer\n    assert get_scorer('neg_mean_squared_error') == neg_mean_squared_error_scorer\n\n    # Test that get_scorer raises ValueError for invalid scorer name\n    with pytest.raises(ValueError, match=\"is not a valid scoring value\"):\n        get_scorer('invalid_scorer')\n\n    # Test that get_scorer returns the same callable if a callable is passed\n        return 1.0\n\n    assert get_scorer(custom_scorer) == custom_scorer\n"], "sample_761": ["def test_simple_imputer_invalid_fill_value():\n    # Test invalid fill_value for SimpleImputer\n    X = np.array([\n        [np.nan, 1, 2],\n        [3, np.nan, 6],\n        [7, 8, np.nan]\n    ])\n\n    with pytest.raises(ValueError, match=\"'fill_value'=[a-zA-Z0-9_]* is invalid\"):\n        imputer = SimpleImputer(strategy=\"constant\", fill_value={})\n        imputer.fit_transform(X)\n    \n    with pytest.raises(ValueError, match=\"'fill_value'=[a-zA-Z0-9_]* is invalid\"):\n        imputer = SimpleImputer(strategy=\"constant\", fill_value=[1, 2, 3])\n        imputer.fit_transform(X)\n", "def test_imputer_mixed_types():\n    # Test imputation on mixed types with strategy \"most_frequent\"\n    X = np.array([\n        [np.nan, 1, 'a'],\n        [2, np.nan, 'b'],\n        [2, 3, np.nan],\n        [np.nan, 3, 'a']\n    ], dtype=object)\n\n    X_true = np.array([\n        [2, 1, 'a'],\n        [2, 3, 'b'],\n        [2, 3, 'a'],\n        [2, 3, 'a']\n    ], dtype=object)\n\n    imputer = SimpleImputer(strategy=\"most_frequent\")\n    X_trans = imputer.fit_transform(X)\n\n    assert_array_equal(X_trans, X_true)\n", "def test_simple_imputer_invalid_fill_value():\n    # Test SimpleImputer raises an error when fill_value is invalid for numeric data\n    X = np.array([\n        [np.nan, 1, 2],\n        [3, np.nan, 4],\n        [5, 6, np.nan]\n    ])\n\n    # fill_value is a string, which is invalid for numeric data\n    with pytest.raises(ValueError, match=\"'fill_value'='invalid' is invalid\"):\n        imputer = SimpleImputer(strategy=\"constant\", fill_value=\"invalid\")\n        imputer.fit_transform(X)\n", "def test_imputation_with_different_fill_values():\n    # Test the imputation with different fill values and strategies\n    X = np.array([\n        [np.nan, 2, np.nan, 0],\n        [4, np.nan, 6, np.nan],\n        [10, 5, np.nan, 1],\n        [np.nan, 3, 7, np.nan]\n    ])\n    \n    # Test with numerical fill value\n    imputer = SimpleImputer(strategy='constant', fill_value=99)\n    X_trans = imputer.fit_transform(X)\n    X_expected = np.array([\n        [99, 2, 99, 0],\n        [4, 99, 6, 99],\n        [10, 5, 99, 1],\n        [99, 3, 7, 99]\n    ])\n    assert_array_equal(X_trans, X_expected)\n\n    # Test with string fill value\n    X_str = np.array([\n        [None, 'b', None, 'e'],\n        ['a', None, 'c', None],\n        ['d', 'e', None, 'f'],\n        [None, 'g', 'h', None]\n    ], dtype=object)\n    imputer = SimpleImputer(strategy='constant', fill_value='missing')\n    X_trans = imputer.fit_transform(X_str)\n    X_expected = np.array([\n        ['missing', 'b', 'missing', 'e'],\n        ['a', 'missing', 'c', 'missing'],\n        ['d', 'e', 'missing', 'f'],\n        ['missing', 'g', 'h', 'missing']\n    ], dtype=object)\n    assert_array_equal(X_trans, X_expected)\n\n    # Test with numpy array fill value\n    X = np.array([\n        [np.nan, 2, np.nan],\n        [4, np.nan, 6],\n        [10, 5, np.nan]\n    ])\n    imputer = SimpleImputer(strategy='constant', fill_value=np.array([1, 2, 3]))\n    X_trans = imputer.fit_transform(X)\n    X_expected = np.array([\n        [1, 2, 3],\n        [4, 2, 6],\n        [10, 5, 3]\n    ])\n    assert_array_equal(X_trans, X_expected)\n", "def test_simple_imputer_invalid_fill_value():\n    # Test SimpleImputer raises error with invalid fill_value for numeric data\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9]\n    ])\n\n    with pytest.raises(ValueError, match=\"Expected a numerical value\"):\n        imputer = SimpleImputer(strategy=\"constant\", fill_value=\"invalid\")\n        imputer.fit_transform(X)\n"], "sample_762": ["def test_get_indices():\n    # Tests the get_indices method of BiclusterMixin\n\n    class MyBicluster(BaseEstimator, BiclusterMixin):\n            self.rows_ = np.array([[1, 0, 1], [0, 1, 1]])\n            self.columns_ = np.array([[1, 1], [0, 1]])\n\n    bicluster = MyBicluster()\n    row_ind, col_ind = bicluster.get_indices(0)\n    assert_array_equal(row_ind, [0, 2])\n    assert_array_equal(col_ind, [0, 1])\n\n    row_ind, col_ind = bicluster.get_indices(1)\n    assert_array_equal(row_ind, [1, 2])\n    assert_array_equal(col_ind, [1])\n", "def test_first_and_last_element():\n    # Test _first_and_last_element with different array types\n\n    # Numpy array\n    arr = np.array([1, 2, 3, 4, 5])\n    first, last = _first_and_last_element(arr)\n    assert_equal(first, 1)\n    assert_equal(last, 5)\n\n    # Sparse matrix (CSR)\n    arr = sp.csr_matrix([[1, 0], [0, 2]])\n    first, last = _first_and_last_element(arr)\n    assert_equal(first, 1)\n    assert_equal(last, 2)\n\n    # Sparse matrix (DOK)\n    arr = sp.dok_matrix((5, 5), dtype=np.int32)\n    arr[0, 0] = 1\n    arr[4, 4] = 5\n    first, last = _first_and_last_element(arr)\n    assert_equal(first, 1)\n    assert_equal(last, 5)\n\n    # Sparse matrix (COO)\n    arr = sp.coo_matrix([[1, 0], [0, 2]])\n    first, last = _first_and_last_element(arr)\n    assert_equal(first, 1)\n    assert_equal(last, 2)\n", "def test_get_indices():\n    # Test the get_indices method of BiclusterMixin\n    class MockBicluster(BiclusterMixin):\n            self.rows_ = rows\n            self.columns_ = columns\n\n    rows = np.array([[1, 0, 0], [0, 1, 1]])\n    columns = np.array([[0, 1], [1, 0]])\n\n    bicluster = MockBicluster(rows, columns)\n\n    row_ind, col_ind = bicluster.get_indices(0)\n    assert_array_equal(row_ind, np.array([0]))\n    assert_array_equal(col_ind, np.array([1]))\n\n    row_ind, col_ind = bicluster.get_indices(1)\n    assert_array_equal(row_ind, np.array([1]))\n    assert_array_equal(col_ind, np.array([0]))\n", "def test_first_and_last_element():\n    # Test _first_and_last_element with numpy array\n    arr = np.array([1, 2, 3, 4, 5])\n    assert_equal(_first_and_last_element(arr), (1, 5))\n\n    # Test _first_and_last_element with sparse matrix\n    sparse_arr = sp.csr_matrix(arr)\n    assert_equal(_first_and_last_element(sparse_arr), (1, 5))\n\n    # Test _first_and_last_element with dok_matrix (sparse matrix without .data attribute)\n    dok_arr = sp.dok_matrix(arr)\n    assert_equal(_first_and_last_element(dok_arr), (1, 5))\n\n    # Test _first_and_last_element with empty array\n    empty_arr = np.array([])\n    assert_raises(IndexError, _first_and_last_element, empty_arr)\n\n    # Test _first_and_last_element with empty sparse matrix\n    empty_sparse_arr = sp.csr_matrix(empty_arr)\n    assert_raises(IndexError, _first_and_last_element, empty_sparse_arr)\n\n    # Test _first_and_last_element with single element array\n    single_element_arr = np.array([42])\n    assert_equal(_first_and_last_element(single_element_arr), (42, 42))\n\n    # Test _first_and_last_element with single element sparse matrix\n    single_element_sparse_arr = sp.csr_matrix(single_element_arr)\n    assert_equal(_first_and_last_element(single_element_sparse_arr), (42, 42))\n", "def test_first_and_last_element():\n    # Test _first_and_last_element with numpy arrays\n    arr = np.array([1, 2, 3, 4, 5])\n    first, last = _first_and_last_element(arr)\n    assert_equal(first, 1)\n    assert_equal(last, 5)\n\n    # Test _first_and_last_element with sparse csr matrix\n    arr = sp.csr_matrix(np.array([1, 2, 3, 4, 5]))\n    first, last = _first_and_last_element(arr)\n    assert_equal(first, 1)\n    assert_equal(last, 5)\n\n    # Test _first_and_last_element with sparse dok matrix\n    arr = sp.dok_matrix((5, 5), dtype=np.int32)\n    arr[0, 0] = 1\n    arr[4, 4] = 5\n    first, last = _first_and_last_element(arr)\n    assert_equal(first, 1)\n    assert_equal(last, 5)\n"], "sample_763": ["def test_check_array_nan_values():\n    # Test that check_array raises the appropriate error when input contains NaN values\n    X = np.array([[1, 2], [3, np.nan]])\n    msg = 'Input contains NaN, infinity or a value too large for dtype(\\'float64\\').'\n    with pytest.raises(ValueError, match=msg):\n        check_array(X, force_all_finite=True)\n\n    # Test that check_array allows NaN values when force_all_finite is 'allow-nan'\n    X_checked = check_array(X, force_all_finite='allow-nan')\n    assert_allclose_dense_sparse(X, X_checked)\n\n    # Test that check_array raises an error when input contains NaN and force_all_finite is False\n    with pytest.raises(ValueError, match=msg):\n        check_array(X, force_all_finite=True)\n\n    # Test with sparse matrix containing NaN\n    X_sparse = sp.csr_matrix(X)\n    with pytest.raises(ValueError, match=msg):\n        check_array(X_sparse, force_all_finite=True)\n\n    # Test that check_array allows NaN values in sparse matrix when force_all_finite is 'allow-nan'\n    X_checked = check_array(X_sparse, force_all_finite='allow-nan', accept_sparse=True)\n    assert_allclose_dense_sparse(X_sparse, X_checked)\n", "def test_check_array_allow_nd():\n    X = np.arange(27).reshape(3, 3, 3)\n    # Allow nd=True should not raise an error\n    X_checked = check_array(X, allow_nd=True)\n    assert_array_equal(X, X_checked)\n    # Allow nd=False should raise an error for 3D array\n    assert_raises(ValueError, check_array, X, allow_nd=False)\n", "def test_check_symmetric_handles_nans():\n    # Test that check_symmetric handles NaNs correctly\n\n    arr_sym_nan = np.array([[np.nan, 1], [1, np.nan]])\n    arr_asym_nan = np.array([[np.nan, 2], [0, np.nan]])\n    arr_sym = np.array([[0, 1], [1, 2]])\n\n    test_arrays = {'dense': arr_asym_nan,\n                   'dok': sp.dok_matrix(arr_asym_nan),\n                   'csr': sp.csr_matrix(arr_asym_nan),\n                   'csc': sp.csc_matrix(arr_asym_nan),\n                   'coo': sp.coo_matrix(arr_asym_nan),\n                   'lil': sp.lil_matrix(arr_asym_nan),\n                   'bsr': sp.bsr_matrix(arr_asym_nan)}\n\n    # Check for warnings and errors with NaNs\n    for arr_format, arr in test_arrays.items():\n        assert_warns(UserWarning, check_symmetric, arr)\n        assert_raises(ValueError, check_symmetric, arr, raise_exception=True)\n\n        output = check_symmetric(arr, raise_warning=False)\n        if sp.issparse(output):\n            assert_equal(output.format, arr_format)\n            assert_array_equal(output.toarray(), arr_sym_nan)\n        else:\n            assert_array_equal(output, arr_sym_nan)\n\n    # Check that symmetric array with NaNs does not raise\n    output = check_symmetric(arr_sym_nan, raise_warning=True)\n    assert_array_equal(output, arr_sym_nan)\n", "def test_check_array_incorrect_dtype():\n    # Test that check_array raises an error for incompatible dtypes\n    X = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.int64)\n    incompatible_dtype = np.complex64\n\n    # Check error message for incompatible dtype\n    msg = \"Complex data not supported\"\n    assert_raises_regex(ValueError, msg, check_array, X, dtype=incompatible_dtype)\n\n    # Check warning message when converting object dtype to float\n    X_obj = X.astype(object)\n    assert_warns_message(DataConversionWarning,\n                         \"Data with input dtype object were all converted to float64\",\n                         check_array, X_obj, dtype=np.float64, warn_on_dtype=True)\n", "def test_check_symmetric_sparse_formats():\n    # Test that sparse matrices in different formats are correctly checked for symmetry.\n    arr_asym = np.array([[0, 2], [0, 2]])\n    arr_sym = np.array([[0, 1], [1, 2]])\n\n    # Define the sparse matrix formats to test\n    sparse_formats = [\n        sp.csr_matrix, sp.csc_matrix, sp.coo_matrix, sp.lil_matrix,\n        sp.bsr_matrix, sp.dok_matrix, sp.dia_matrix\n    ]\n\n    for fmt in sparse_formats:\n        arr_sparse_asym = fmt(arr_asym)\n        arr_sparse_sym = fmt(arr_sym)\n        # Test that non-symmetric matrices raise a warning\n        assert_warns(UserWarning, check_symmetric, arr_sparse_asym)\n        # Test that symmetric matrices do not raise any warnings or exceptions\n        assert_no_warnings(check_symmetric, arr_sparse_sym)\n        # Test symmetrization of non-symmetric matrices\n        symmetrized = check_symmetric(arr_sparse_asym, raise_warning=False)\n        assert_allclose_dense_sparse(symmetrized, 0.5 * (arr_asym + arr_asym.T))\n"], "sample_764": ["def test_column_transformer_with_custom_weights():\n    X_array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).T\n    transformer_weights = {'trans1': 0.5, 'trans2': 2}\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', StandardScaler(with_std=False), [1])],\n                           transformer_weights=transformer_weights)\n    \n    X_trans = ct.fit_transform(X_array)\n    assert_allclose_dense_sparse(X_trans[:, 0], 0.5 * StandardScaler().fit_transform(X_array[:, [0]]))\n    assert_allclose_dense_sparse(X_trans[:, 1], 2 * StandardScaler(with_std=False).fit_transform(X_array[:, [1]]))\n    \n    # Ensure transformer_weights attribute is correctly set\n    assert ct.transformer_weights == transformer_weights\n", "def test_column_transformer_remainder_no_cols():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    # When no columns are specified for the remainder transformer\n    ct = ColumnTransformer([('trans1', Trans(), [0])], remainder=DoubleTrans())\n    X_res_first = X_array[:, 0].reshape(-1, 1)\n    X_res_none = np.zeros((X_array.shape[0], 0))  # no columns left to transform\n    \n    assert_array_equal(ct.fit_transform(X_array), X_res_first)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n\n    # Dropping all columns should result in an empty array\n    ct = ColumnTransformer([('trans1', 'drop', [0])], remainder=DoubleTrans())\n    assert_array_equal(ct.fit_transform(X_array), X_res_none)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_none)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_with_callable_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first = np.array([[0], [1], [2]])\n\n        return [0]\n\n    ct = ColumnTransformer([('trans1', Trans(), [1])], remainder=func)\n    assert_array_equal(ct.fit_transform(X_array), np.hstack([np.array([[2], [4], [6]]), X_res_first]))\n    assert_array_equal(ct.fit(X_array).transform(X_array), np.hstack([np.array([[2], [4], [6]]), X_res_first]))\n    assert callable(ct.remainder)\n    assert ct.transformers_[-1][2] == [0]\n\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n\n        return ['first']\n\n    ct = ColumnTransformer([('trans1', Trans(), ['second'])], remainder=func_df)\n    assert_array_equal(ct.fit_transform(X_df), np.hstack([np.array([[2], [4], [6]]), X_res_first]))\n    assert_array_equal(ct.fit(X_df).transform(X_df), np.hstack([np.array([[2], [4], [6]]), X_res_first]))\n    assert callable(ct.remainder)\n    assert ct.transformers_[-1][2] == ['first']\n", "def test_column_transformer_get_feature_names_remainder():\n    # Test get_feature_names when remainder is specified\n    X = np.array([[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}],\n                  [{'c': 5}, {'c': 6}]], dtype=object).T\n\n    # Test when remainder is 'drop'\n    ct = ColumnTransformer(\n        [('col0', DictVectorizer(), 0)],\n        remainder='drop'\n    )\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['col0__a', 'col0__b'])\n\n    # Test when remainder is 'passthrough' (should raise NotImplementedError)\n    ct = ColumnTransformer(\n        [('col0', DictVectorizer(), 0)],\n        remainder='passthrough'\n    )\n    ct.fit(X)\n    assert_raise_message(\n        NotImplementedError, 'get_feature_names is not yet supported',\n        ct.get_feature_names)\n\n    # Test when remainder is a transformer (e.g., StandardScaler)\n    ct = ColumnTransformer(\n        [('col0', DictVectorizer(), 0)],\n        remainder=StandardScaler()\n    )\n    ct.fit(X)\n    assert_raise_message(\n        AttributeError,\n        \"Transformer remainder (type StandardScaler) does not provide get_feature_names\",\n        ct.get_feature_names)\n", "def test_column_transformer_with_dict_vectorizer():\n    # test column transformer with DictVectorizer\n    X = np.array([[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}],\n                  [{'c': 5}, {'c': 6}]], dtype=object).T\n\n    ct = ColumnTransformer([('col0', DictVectorizer(), 0),\n                            ('col1', DictVectorizer(), 1)])\n    X_trans = ct.fit_transform(X)\n    expected = np.array([\n        [1., 2., 0., 0., 5.],\n        [3., 4., 0., 0., 6.]\n    ])\n    assert_array_equal(X_trans, expected)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n\n    # test with remainder='passthrough'\n    ct = ColumnTransformer([('col0', DictVectorizer(), 0)],\n                           remainder='passthrough')\n    X_trans = ct.fit_transform(X)\n    expected = np.array([\n        [1., 2., {'c': 5}],\n        [3., 4., {'c': 6}]\n    ], dtype=object)\n    assert_array_equal(X_trans, expected)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'passthrough'\n    assert ct.transformers_[-1][2] == [1]\n\n    # test with remainder='drop'\n    ct = ColumnTransformer([('col1', DictVectorizer(), 1)],\n                           remainder='drop')\n    X_trans = ct.fit_transform(X)\n    expected = np.array([\n        [5.],\n        [6.]\n    ])\n    assert_array_equal(X_trans, expected)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'drop'\n    assert ct.transformers_[-1][2] == [0]\n"], "sample_765": ["def test_cohen_kappa_score_edge_cases():\n    # Test edge cases for cohen_kappa_score\n\n    # Perfect agreement\n    y1 = [1, 1, 1, 1]\n    y2 = [1, 1, 1, 1]\n    assert_almost_equal(cohen_kappa_score(y1, y2), 1.0)\n\n    # Complete disagreement\n    y1 = [0, 0, 1, 1]\n    y2 = [1, 1, 0, 0]\n    assert_almost_equal(cohen_kappa_score(y1, y2), -1.0)\n\n    # No agreement and uniform distribution\n    y1 = [0, 1, 0, 1]\n    y2 = [1, 0, 1, 0]\n    assert_almost_equal(cohen_kappa_score(y1, y2), -1.0)\n\n    # Mixed agreement\n    y1 = [0, 1, 0, 1]\n    y2 = [0, 1, 1, 0]\n    assert_almost_equal(cohen_kappa_score(y1, y2), 0.0)\n", "def test_cohen_kappa_score_multiclass():\n    # Test Cohen's kappa for multiclass classification\n    y_true = [0, 1, 2, 2, 1, 0]\n    y_pred = [0, 2, 2, 2, 0, 1]\n    \n    # Expected confusion matrix:\n    # [[1, 1, 0],\n    #  [1, 0, 1],\n    #  [0, 0, 2]]\n    cm = confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, np.array([[1, 1, 0], [1, 0, 1], [0, 0, 2]]))\n    \n    kappa = cohen_kappa_score(y_true, y_pred)\n    assert_almost_equal(kappa, -0.142857, decimal=6)\n    \n    # Test with sample weights\n    sample_weight = [1, 2, 3, 4, 5, 6]\n    kappa_weighted = cohen_kappa_score(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(kappa_weighted, -0.0526315, decimal=6)\n    \n    # Test with different labels order\n    labels = [2, 0, 1]\n    kappa_with_labels = cohen_kappa_score(y_true, y_pred, labels=labels)\n    assert_almost_equal(kappa_with_labels, -0.142857, decimal=6)\n    \n    # Test with different weighting schemes\n    kappa_linear = cohen_kappa_score(y_true, y_pred, weights=\"linear\")\n    kappa_quadratic = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    assert_almost_equal(kappa_linear, -0.071428, decimal=6)\n    assert_almost_equal(kappa_quadratic, -0.034965, decimal=6)\n", "def test_log_loss_edge_cases():\n    # Test log_loss with edge cases\n    \n    # Case where y_true contains only one unique label\n    y_true = [1, 1, 1, 1]\n    y_pred = [[0.1, 0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6]]\n    with pytest.raises(ValueError, match=\"y_true contains only one label\"):\n        log_loss(y_true, y_pred)\n    \n    # Case where y_pred probabilities are exactly 0 or 1\n    y_true = [0, 1, 0, 1]\n    y_pred = [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n    loss = log_loss(y_true, y_pred)\n    assert loss == 0.0, f\"Expected 0.0, got {loss}\"\n\n    # Case where y_pred contains probabilities exactly at the boundary (eps)\n    y_true = [0, 1, 0, 1]\n    eps = 1e-15\n    y_pred = [[1-eps, eps], [eps, 1-eps], [1-eps, eps], [eps, 1-eps]]\n    loss = log_loss(y_true, y_pred)\n    assert_almost_equal(loss, -np.log(eps), decimal=6)\n", "def test_fbeta_score_zero_beta():\n    # Test that fbeta_score function raises ValueError for beta=0.\n    y_true, y_pred, _ = make_prediction(binary=True)\n    assert_raises(ValueError, fbeta_score, y_true, y_pred, beta=0)\n", "def test_cohen_kappa_score_multiclass():\n    # Test cohen_kappa_score with multiclass classification\n    y1 = np.array([0, 1, 2, 1, 0, 2, 1, 2, 0, 1])\n    y2 = np.array([0, 2, 1, 1, 0, 2, 1, 2, 1, 1])\n    expected_kappa = (cohen_kappa_score(y1, y2))\n    \n    assert_almost_equal(expected_kappa, 0.5396825396825397, decimal=4)\n\n    # Test cohen_kappa_score with missing labels\n    y1 = np.array([0, 1, 2, 1, 0, 2, 1, 2, 0, 1])\n    y2 = np.array([0, 2, 1, 1, 0, 2, 1, 2, 1, 1])\n    labels = [0, 1, 2, 3]\n    expected_kappa = (cohen_kappa_score(y1, y2, labels=labels))\n    \n    assert_almost_equal(expected_kappa, 0.5396825396825397, decimal=4)\n"], "sample_766": ["def test_sparse_encode_threshold():\n    # Test for the threshold algorithm with positive and negative constraints\n    n_components = 5\n    rng = np.random.RandomState(0)\n    dictionary = rng.randn(n_components, n_features)\n\n    X_threshold = rng.randn(n_samples, n_features)\n    code = sparse_encode(X_threshold, dictionary, algorithm='threshold', alpha=0.5)\n    assert_equal(code.shape, (n_samples, n_components))\n    assert (code < 0).any()\n\n    code_positive = sparse_encode(X_threshold, dictionary, algorithm='threshold', alpha=0.5, positive=True)\n    assert_equal(code_positive.shape, (n_samples, n_components))\n    assert (code_positive >= 0).all()\n", "def test_sparse_encode_threshold():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    X_ = rng.randn(n_samples, n_features)\n\n    # Test the 'threshold' algorithm\n    regularization = 0.5\n    code = sparse_encode(X_, V, algorithm='threshold', alpha=regularization)\n    assert code.shape == (n_samples, n_components)\n    assert (np.abs(code) < regularization).all() or (code == 0).all()\n", "def test_update_dict_shapes():\n    n_components = 5\n    n_features = 8\n    n_samples = 10\n    rng = np.random.RandomState(0)\n    dictionary = rng.randn(n_features, n_components)\n    Y = rng.randn(n_features, n_samples)\n    code = rng.randn(n_components, n_samples)\n    updated_dict = _update_dict(dictionary, Y, code)\n    assert_equal(updated_dict.shape, (n_features, n_components))\n\n", "def test_sparse_encode_dictionary_shape_mismatch():\n    rng = np.random.RandomState(0)\n    n_components = 5\n    X_ = rng.randn(10, 8)\n    dictionary = rng.randn(n_components, 7)  # intentionally wrong shape\n    err_msg = \"Dictionary and X have different numbers of features:\"\n    with pytest.raises(ValueError, match=err_msg):\n        sparse_encode(X_, dictionary, algorithm='lasso_lars')\n", "def test_sparse_encode_unavailable_algorithm():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    err_msg = 'Sparse coding method must be \"lasso_lars\" \"lasso_cd\", \"lasso\", \"threshold\" or \"omp\", got unknown_algorithm.'\n    with pytest.raises(ValueError, match=err_msg):\n        sparse_encode(X, V, algorithm=\"unknown_algorithm\")\n"], "sample_767": ["def test_column_transformer_with_mixed_data_types():\n    # Test with mixed data types in numpy array and pandas DataFrame\n    pd = pytest.importorskip('pandas')\n    \n    X_array = np.array([[1.5, 2, 'a'], [3.5, 4, 'b'], [5.5, 6, 'c']])\n    X_df = pd.DataFrame(X_array, columns=['numeric', 'integer', 'string'])\n    \n    # Expected results\n    X_res_numeric = np.array([[1.5], [3.5], [5.5]])\n    X_res_integer = np.array([[2], [4], [6]])\n    X_res_string = np.array([['a'], ['b'], ['c']])\n    \n    # Test with numpy array\n    ct = ColumnTransformer([('numeric', Trans(), [0]), \n                            ('integer', Trans(), [1]), \n                            ('string', Trans(), [2])])\n    assert_array_equal(ct.fit_transform(X_array), np.hstack([X_res_numeric, X_res_integer, X_res_string]))\n    assert_array_equal(ct.fit(X_array).transform(X_array), np.hstack([X_res_numeric, X_res_integer, X_res_string]))\n    \n    # Test with pandas DataFrame\n    ct = ColumnTransformer([('numeric', Trans(), 'numeric'), \n                            ('integer', Trans(), 'integer'), \n                            ('string', Trans(), 'string')])\n    assert_array_equal(ct.fit_transform(X_df), np.hstack([X_res_numeric, X_res_integer, X_res_string]))\n    assert_array_equal(ct.fit(X_df).transform(X_df), np.hstack([X_res_numeric, X_res_integer, X_res_string]))\n", "def test_column_transformer_different_dtypes():\n    X_array = np.array([[0, 1.0, 'cat'], [2, 4.5, 'dog'], [8, 6.3, 'cat']]).T\n    # Column 0: integer, Column 1: float, Column 2: string\n\n    # Expected results after applying StandardScaler to integer and float columns,\n    # and OneHotEncoder to string column\n    expected_result = np.array([\n        [-1.06904497, -1.22624196, 1, 0],\n        [-0.26726124, 0.05693087, 0, 1],\n        [1.33630621, 1.16931109, 1, 0]\n    ])\n\n    ct = ColumnTransformer([\n        ('scale_int', StandardScaler(), [0]),\n        ('scale_float', StandardScaler(), [1]),\n        ('encode_str', OneHotEncoder(), [2]),\n    ])\n\n    # Fit and transform the data\n    assert_array_almost_equal(ct.fit_transform(X_array.T), expected_result)\n    assert_array_almost_equal(ct.fit(X_array.T).transform(X_array.T), expected_result)\n", "def test_column_transformer_mixed_sparse_dense_output():\n    # Test for correct mixed sparse and dense output handling\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_sparse = sparse.csr_matrix(X_array)\n\n    col_trans = ColumnTransformer([('dense_trans', StandardScaler(), [0]),\n                                   ('sparse_trans', SparseMatrixTrans(), [1])],\n                                  sparse_threshold=0.5)\n    X_trans = col_trans.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert_equal(X_trans.shape, (3, 4))\n    \n    col_trans = ColumnTransformer([('dense_trans', StandardScaler(), [0]),\n                                   ('sparse_trans', SparseMatrixTrans(), [1])],\n                                  sparse_threshold=1.0)\n    X_trans = col_trans.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert_equal(X_trans.shape, (3, 4))\n\n    col_trans = ColumnTransformer([('dense_trans', StandardScaler(), [0]),\n                                   ('sparse_trans', SparseMatrixTrans(), [1])],\n                                  sparse_threshold=0.0)\n    X_trans = col_trans.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n    assert_equal(X_trans.shape, (3, 4))\n    \n    # Test with dense input and sparse transformer output\n    col_trans = ColumnTransformer([('dense_trans', StandardScaler(), [0]),\n                                   ('sparse_trans', SparseMatrixTrans(), [1])],\n                                  sparse_threshold=0.5)\n    X_trans = col_trans.fit_transform(X_sparse)\n    assert sparse.issparse(X_trans)\n    assert_equal(X_trans.shape, (3, 4))\n\n    col_trans = ColumnTransformer([('dense_trans', StandardScaler(), [0]),\n                                   ('sparse_trans', SparseMatrixTrans(), [1])],\n                                  sparse_threshold=1.0)\n    X_trans = col_trans.fit_transform(X_sparse)\n    assert sparse.issparse(X_trans)\n    assert_equal(X_trans.shape, (3, 4))\n\n    col_trans = ColumnTransformer([('dense_trans', StandardScaler(), [0]),\n                                   ('sparse_trans', SparseMatrixTrans(), [1])],\n                                  sparse_threshold=0.0)\n    X_trans = col_trans.fit_transform(X_sparse)\n    assert not sparse.issparse(X_trans)\n    assert_equal(X_trans.shape, (3, 4))\n", "def test_column_transformer_mixed_dtypes():\n    # Test with mixed data types (numeric, string, boolean)\n    pd = pytest.importorskip('pandas')\n\n    df = pd.DataFrame({\n        'num': [0, 1, 2],\n        'cat': ['a', 'b', 'c'],\n        'bool': [True, False, True]\n    })\n\n    ct = ColumnTransformer(\n        [('num', StandardScaler(), ['num']),\n         ('cat', OneHotEncoder(), ['cat']),\n         ('bool', 'passthrough', ['bool'])],\n        remainder='drop'\n    )\n\n    result = ct.fit_transform(df)\n    expected_result = np.hstack([\n        StandardScaler().fit_transform(df[['num']]),\n        OneHotEncoder().fit_transform(df[['cat']]).toarray(),\n        df[['bool']].values\n    ])\n\n    assert_array_equal(result, expected_result)\n\n    # Ensure proper handling of boolean columns\n    bool_df = pd.DataFrame({\n        'num': [0, 1, 2],\n        'bool1': [True, False, True],\n        'bool2': [False, True, False]\n    })\n\n    ct = ColumnTransformer(\n        [('num', StandardScaler(), ['num']),\n         ('bool', 'passthrough', ['bool1', 'bool2'])],\n        remainder='drop'\n    )\n\n    result = ct.fit_transform(bool_df)\n    expected_result = np.hstack([\n        StandardScaler().fit_transform(bool_df[['num']]),\n        bool_df[['bool1', 'bool2']].values\n    ])\n\n    assert_array_equal(result, expected_result)\n", "def test_column_transformer_mixed_dtypes():\n    # Test ColumnTransformer with mixed datatypes including numerical, categorical, and boolean\n    X = np.array([\n        [1, 'a', True],\n        [0, 'b', False]\n    ])\n    \n    column_transformer = ColumnTransformer(\n        transformers=[\n            ('num', StandardScaler(), [0]),\n            ('cat', OneHotEncoder(), [1]),\n            ('bool', FunctionTransformer(lambda x: x.astype(int)), [2])\n        ]\n    )\n    \n    X_trans = column_transformer.fit_transform(X)\n    expected = np.array([\n        [1., 0., 1., 0., 1.],\n        [-1., 1., 0., 1., 0.]\n    ])\n    \n    assert_array_equal(X_trans, expected)\n"], "sample_768": ["def test_leave_one_out():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n\n    loo = LeaveOneOut()\n\n    splits = list(loo.split(X, y))\n    assert_equal(len(splits), len(X))  # Should have as many splits as samples\n\n    for train_index, test_index in splits:\n        assert_equal(len(test_index), 1)\n        assert_equal(len(train_index), len(X) - 1)\n        # Ensure the test index is excluded from the training indices\n        assert not np.any(np.in1d(test_index, train_index))\n\n    # Test get_n_splits function\n    assert_equal(loo.get_n_splits(X), len(X))\n    \n    # Test if the repr works without any errors\n    assert_equal(repr(loo), \"LeaveOneOut()\")\n", "def test_leave_one_out():\n    # Test LeaveOneOut cross-validator with a small dataset\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 2])\n    loo = LeaveOneOut()\n    splits = list(loo.split(X, y))\n    expected_splits = [([1, 2], [0]), ([0, 2], [1]), ([0, 1], [2])]\n\n    # Check if the number of splits is correct\n    assert_equal(loo.get_n_splits(X, y), len(X))\n\n    # Check if splits are correct\n    for (train, test), (expected_train, expected_test) in zip(splits, expected_splits):\n        assert_array_equal(train, expected_train)\n        assert_array_equal(test, expected_test)\n\n    # Check if the splits work with 1D data\n    X_1d = np.array([1, 2, 3])\n    splits_1d = list(loo.split(X_1d, y))\n    for (train, test), (expected_train, expected_test) in zip(splits_1d, expected_splits):\n        assert_array_equal(train, expected_train)\n        assert_array_equal(test, expected_test)\n\n    # Check if ValueError is raised for None X parameter\n    assert_raises(ValueError, loo.get_n_splits, None, y)\n", "def test_leave_one_out_split():\n    # Test LeaveOneOut split\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n    splits = list(loo.split(X, y))\n\n    # Expecting 4 splits for 4 samples\n    assert_equal(len(splits), 4)\n\n    # Check the splits\n    expected_splits = [\n        (np.array([1, 2, 3]), np.array([0])),\n        (np.array([0, 2, 3]), np.array([1])),\n        (np.array([0, 1, 3]), np.array([2])),\n        (np.array([0, 1, 2]), np.array([3]))\n    ]\n\n    for split, expected_split in zip(splits, expected_splits):\n        train, test = split\n        expected_train, expected_test = expected_split\n        assert_array_equal(train, expected_train)\n        assert_array_equal(test, expected_test)\n\n    # Check if the number of splits is correct\n    assert_equal(loo.get_n_splits(X), 4)\n\n    # Check the repr\n    assert_equal(repr(loo), \"LeaveOneOut()\")\n", "def test_time_series_split_with_min_samples():\n    # Test TimeSeriesSplit with a minimum number of samples\n    X = np.array([[i] for i in range(5)])\n\n    # Should fail if there are less samples than required for the number of splits\n    assert_raises_regexp(ValueError, \"Cannot have number of folds.*greater\",\n                         next, TimeSeriesSplit(n_splits=6).split(X))\n    assert_raises_regexp(ValueError, \"Cannot have number of folds.*greater\",\n                         next, TimeSeriesSplit(n_splits=5).split(X))\n\n    # Test for the minimum number of samples that works\n    tscv = TimeSeriesSplit(n_splits=4)\n    splits = list(tscv.split(X))\n    assert_equal(len(splits), 4)\n    expected_splits = [\n        (np.array([0]), np.array([1])),\n        (np.array([0, 1]), np.array([2])),\n        (np.array([0, 1, 2]), np.array([3])),\n        (np.array([0, 1, 2, 3]), np.array([4]))\n    ]\n    for (train, test), (exp_train, exp_test) in zip(splits, expected_splits):\n        assert_array_equal(train, exp_train)\n        assert_array_equal(test, exp_test)\n", "def test_leave_one_out_split():\n    # Test LeaveOneOut split to ensure it returns the correct number of splits\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    loo = LeaveOneOut()\n    splits = list(loo.split(X))\n    assert_equal(len(splits), 4)\n    for train_index, test_index in splits:\n        assert_equal(len(test_index), 1)\n        assert_equal(len(train_index), 3)\n"], "sample_769": ["def test_multilabel_accuracy_score_with_weight():\n    # Test accuracy_score with sample_weight for multilabel case\n    y_true = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]])\n    y_pred = np.array([[0, 1, 1], [1, 1, 0], [1, 0, 0]])\n    sample_weight = np.array([1, 2, 3])\n\n    # Without weight\n    assert_equal(accuracy_score(y_true, y_pred), 2 / 3)\n\n    # With weight\n    expected_score = (1 * 1 + 0 * 2 + 1 * 3) / 6\n    assert_equal(accuracy_score(y_true, y_pred, sample_weight=sample_weight), expected_score)\n", "def test_log_loss_multilabel():\n    # Test log loss for multilabel classification task\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[0.9, 0.1, 0.8], [0.2, 0.7, 0.1], [0.8, 0.9, 0.2]])\n    loss = log_loss(y_true, y_pred, labels=[0, 1])\n    \n    # Expected log loss\n    expected_loss = -np.mean(\n        [np.log(0.9), np.log(0.1), np.log(0.8), np.log(0.8), np.log(0.7), np.log(0.9)]\n    )\n    assert_almost_equal(loss, expected_loss, decimal=6)\n\n    # Test log loss for multilabel classification task with missing labels\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[0.9, 0.1, 0.8], [0.2, 0.7, 0.1], [0.8, 0.9, 0.2]])\n    loss = log_loss(y_true, y_pred, labels=[0, 1, 2])\n\n    # Expected log loss when providing all labels\n    expected_loss = -np.mean(\n        [np.log(0.9), np.log(0.1), np.log(0.8), np.log(0.2), np.log(0.7), np.log(0.1), np.log(0.8), np.log(0.9), np.log(0.2)]\n    )\n    assert_almost_equal(loss, expected_loss, decimal=6)\n", "def test_multilabel_confusion_matrix_multilabel_with_sample_weight():\n    # Test multilabel confusion matrix with sample weights in multilabel-indicator case\n    y_true = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]])\n    y_pred = np.array([[0, 0, 1], [1, 0, 1], [1, 0, 1]])\n    sample_weight = np.array([1, 2, 3])\n    cm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n\n    expected_cm = np.array([[[3, 0], [0, 3]],\n                            [[3, 3], [0, 3]],\n                            [[0, 3], [1, 2]]])\n    assert_array_equal(cm, expected_cm)\n\n    # Check that the sample weight affects the counts correctly\n    sample_weight = np.array([1, 1, 1])\n    cm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    expected_cm = np.array([[[2, 1], [0, 2]],\n                            [[2, 1], [1, 1]],\n                            [[1, 2], [0, 2]]])\n    assert_array_equal(cm, expected_cm)\n", "def test_multilabel_confusion_matrix_unseen_labels():\n    # Test multilabel confusion matrix with unseen labels in y_pred\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[0, 0, 1], [1, 0, 1], [0, 0, 1]])\n\n    # Expected confusion matrix when unseen labels are present in y_pred\n    expected_cm = [[[1, 0], [1, 1]],\n                   [[2, 0], [1, 0]],\n                   [[0, 2], [1, 0]]]\n\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, expected_cm)\n\n    # Test with labels argument including unseen labels\n    labels = [0, 1, 2, 3]\n    expected_cm_with_labels = [[[1, 0], [1, 1]],\n                               [[2, 0], [1, 0]],\n                               [[0, 2], [1, 0]],\n                               [[3, 0], [0, 0]]]\n\n    cm_with_labels = multilabel_confusion_matrix(y_true, y_pred, labels=labels)\n    assert_array_equal(cm_with_labels, expected_cm_with_labels)\n", "def test_precision_recall_fscore_support_with_weighted_samples():\n    y_true = np.array([0, 1, 0, 1])\n    y_pred = np.array([0, 0, 0, 1])\n    sample_weight = np.array([0.2, 0.8, 0.5, 0.5])\n\n    precision, recall, fscore, support = precision_recall_fscore_support(\n        y_true, y_pred, average=None, sample_weight=sample_weight)\n\n    # Calculate expected values manually\n    tp = np.array([0.7, 0.5])\n    fp = np.array([0.5, 0.0])\n    fn = np.array([0.0, 0.8])\n\n    expected_precision = tp / (tp + fp)\n    expected_recall = tp / (tp + fn)\n    expected_fscore = 2 * expected_precision * expected_recall / (expected_precision + expected_recall)\n\n    assert_array_almost_equal(precision, expected_precision)\n    assert_array_almost_equal(recall, expected_recall)\n    assert_array_almost_equal(fscore, expected_fscore)\n    assert_array_equal(support, [0.7, 1.3])\n"], "sample_770": ["def test_davies_bouldin_score():\n    # Assert the value is 0. when all samples are equals\n    X = np.ones((10, 2))\n    labels = [0] * 5 + [1] * 5\n    assert_equal(0., davies_bouldin_score(X, labels))\n\n    # General case\n    X = np.array([[1, 2], [2, 3], [3, 4], [8, 7], [7, 8], [8, 9]])\n    labels = np.array([0, 0, 0, 1, 1, 1])\n    assert_greater(davies_bouldin_score(X, labels), 0)\n\n    # Check raise when there is only one cluster\n    assert_raises_on_only_one_label(davies_bouldin_score)\n    \n    # Check raise when all points are in different clusters\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n\n    # Expecting a valid Davies-Bouldin score for well-separated clusters\n    db_score = davies_bouldin_score(X, labels)\n    assert db_score > 0.0\n\n    # Edge case where all samples are equal\n    assert_equal(0.0, davies_bouldin_score(np.ones((10, 2)), [0] * 5 + [1] * 5))\n\n    # Edge case where clusters are very close to each other\n    X_close = ([[0, 0], [0.1, 0.1]] * 5 + [[0.2, 0.2], [0.3, 0.3]] * 5)\n    labels_close = [0] * 10 + [1] * 10\n    db_score_close = davies_bouldin_score(X_close, labels_close)\n    assert db_score_close > 0.0\n    assert db_score_close < db_score\n", "def test_davies_bouldin_score():\n    # Test Davies-Bouldin score computation\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                          [0] * 5 + [1] * 5))\n\n    # General case\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    score = davies_bouldin_score(X, labels)\n    assert_greater(score, 0)\n\n    # Test non-numpy input\n    X_list = [[0, 0], [1, 1], [0, 0], [1, 1], [2, 2], [3, 3], [2, 2], [3, 3]]\n    labels_list = [0, 0, 0, 0, 1, 1, 1, 1]\n    score_list = davies_bouldin_score(X_list, labels_list)\n    assert_greater(score_list, 0)\n    assert_equal(score, score_list)\n", "def test_davies_bouldin_score():\n    # Tests the Davies-Bouldin score.\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = dataset.target\n\n    # General case\n    score = davies_bouldin_score(X, y)\n    assert_greater(score, 0)\n\n    # Edge case: when all samples are in one cluster, score should be 0\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)), [0] * 10))\n\n    # Edge case: when each sample is its own cluster, score should be 0\n    assert_equal(0., davies_bouldin_score(np.eye(10), np.arange(10)))\n\n    # Test for non-numpy arrays\n    X_list = X.tolist()\n    y_list = y.tolist()\n    assert_equal(davies_bouldin_score(X_list, y_list), score)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0 when all samples are identical\n    X = np.ones((10, 2))\n    labels = [0] * 5 + [1] * 5\n    assert_equal(0.0, davies_bouldin_score(X, labels))\n\n    # General case\n    X = np.array([[1, 2], [1, 4], [1, 0],\n                  [4, 2], [4, 4], [4, 0]])\n    labels = [0, 0, 0, 1, 1, 1]\n    score = davies_bouldin_score(X, labels)\n    assert_greater(score, 0)\n    \n    # Testing with non-numpy arrays\n    X_list = [[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]\n    labels_list = [0, 0, 0, 1, 1, 1]\n    score_list = davies_bouldin_score(X_list, labels_list)\n    pytest.approx(score, score_list)\n"], "sample_771": ["def test_minmax_scale_1d_edge_cases():\n    # Test minmax_scale with 1D input with edge cases (constant and negative values)\n    X_constant = np.array([5, 5, 5, 5])\n    X_negative = np.array([-5, -1, -3, -7])\n    \n    # Test minmax_scale on constant values\n    X_constant_scaled = minmax_scale(X_constant)\n    assert_array_almost_equal(X_constant_scaled, np.zeros_like(X_constant))\n\n    # Test minmax_scale on negative values\n    X_negative_scaled = minmax_scale(X_negative)\n    assert_array_almost_equal(X_negative_scaled, (X_negative + 7) / 6)\n    \n    # Ensure inverse_transform retrieves the original data\n    scaler_constant = MinMaxScaler().fit(X_constant.reshape(-1, 1))\n    assert_array_almost_equal(scaler_constant.inverse_transform(X_constant_scaled.reshape(-1, 1)).ravel(), X_constant)\n\n    scaler_negative = MinMaxScaler().fit(X_negative.reshape(-1, 1))\n    assert_array_almost_equal(scaler_negative.inverse_transform(X_negative_scaled.reshape(-1, 1)).ravel(), X_negative)\n", "def test_scale_handle_zeros_in_scale():\n    # Test the handling of zeros in the scale function\n    X = np.array([[0., 0., 2.],\n                  [0., 0., 4.],\n                  [0., 0., 6.]])\n\n    # Standard scaling\n    X_scaled = scale(X)\n    assert_array_almost_equal(X_scaled[:, 0], np.zeros(3))\n    assert_array_almost_equal(X_scaled[:, 1], np.zeros(3))\n    assert_array_almost_equal(X_scaled[:, 2], [-1.22474487, 0., 1.22474487])\n\n    # Scaling without mean\n    X_scaled = scale(X, with_mean=False)\n    assert_array_almost_equal(X_scaled[:, 0], np.zeros(3))\n    assert_array_almost_equal(X_scaled[:, 1], np.zeros(3))\n    assert_array_almost_equal(X_scaled[:, 2], [0.33333333, 0.66666667, 1.])\n\n    # Scaling without std\n    X_scaled = scale(X, with_std=False)\n    assert_array_almost_equal(X_scaled[:, 0], np.zeros(3))\n    assert_array_almost_equal(X_scaled[:, 1], np.zeros(3))\n    assert_array_almost_equal(X_scaled[:, 2], [-2., 0., 2.])\n\n    # Scaling without mean and std\n    X_scaled = scale(X, with_mean=False, with_std=False)\n    assert_array_almost_equal(X_scaled[:, 0], np.zeros(3))\n    assert_array_almost_equal(X_scaled[:, 1], np.zeros(3))\n    assert_array_almost_equal(X_scaled[:, 2], [2., 4., 6.])\n", "def test_scale_nan_handling():\n    # Test scale function's handling of NaN values\n    X_with_nan = np.array([[np.nan, 1, 2], [3, 4, np.nan], [7, 8, 9]])\n    \n    X_scaled = scale(X_with_nan)\n    assert np.isnan(X_scaled).any()  # NaNs should remain in the scaled data\n\n    # Check that other values are scaled correctly\n    X_scaled_no_nan = X_scaled[~np.isnan(X_scaled)]\n    X_no_nan = X_with_nan[~np.isnan(X_with_nan)]\n    assert_array_almost_equal(X_scaled_no_nan.mean(), 0, decimal=6)\n    assert_array_almost_equal(X_scaled_no_nan.std(), 1, decimal=6)\n\n    # Check that scale function doesn't modify original array\n    X_copy = X_with_nan.copy()\n    scale(X_with_nan)\n    assert_array_equal(X_with_nan, X_copy)\n\n    # Check inverse_transform handling of NaNs\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_with_nan)\n    X_inv_trans = scaler.inverse_transform(X_scaled)\n    assert np.isnan(X_inv_trans).any()\n    assert_array_almost_equal(X_inv_trans[~np.isnan(X_inv_trans)], \n                              X_with_nan[~np.isnan(X_with_nan)])\n", "def test_add_dummy_feature_empty():\n    # Test add_dummy_feature with an empty array\n    X = np.array([])\n    X = X.reshape(0, 0)\n    X_with_dummy = add_dummy_feature(X)\n    assert_array_equal(X_with_dummy, np.ones((0, 1)))\n\n    # Test add_dummy_feature with empty sparse array\n    X_sparse = sparse.csr_matrix(X)\n    X_sparse_with_dummy = add_dummy_feature(X_sparse)\n    assert sparse.issparse(X_sparse_with_dummy)\n    assert_array_equal(X_sparse_with_dummy.toarray(), np.ones((0, 1)))\n\n    X_sparse = sparse.csc_matrix(X)\n    X_sparse_with_dummy = add_dummy_feature(X_sparse)\n    assert sparse.issparse(X_sparse_with_dummy)\n    assert_array_equal(X_sparse_with_dummy.toarray(), np.ones((0, 1)))\n", "def test_scale_sparse_with_nan():\n    # Test the scale function with sparse input containing NaNs\n    rng = np.random.RandomState(42)\n    X = rng.randn(10, 5)\n    X[5, 2] = np.nan\n    X[2, 3] = np.nan\n    X_csr = sparse.csr_matrix(X)\n    \n    # Testing with mean centering, which should raise an error\n    assert_raises(ValueError, scale, X_csr, with_mean=True)\n    \n    # Testing without mean centering\n    X_scaled = scale(X_csr, with_mean=False)\n    assert not np.any(np.isnan(X_scaled.data))\n\n    # Testing axis 1, which should raise an error\n    assert_raises(ValueError, scale, X_csr, with_mean=False, axis=1)\n\n    # Verify that the presence of NaNs does not affect the scaling of non-NaN elements\n    X_dense = X_csr.toarray()\n    X_dense_scaled = scale(X_dense, with_mean=False)\n    X_csr_scaled = scale(X_csr, with_mean=False)\n    assert_array_almost_equal(X_dense_scaled, X_csr_scaled.toarray())\n"], "sample_772": ["def check_estimator_params(name):\n    # Test that estimator parameters are correctly set\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    params = {\n        'n_estimators': 10,\n        'max_depth': 5,\n        'min_samples_split': 4,\n        'min_samples_leaf': 2,\n        'min_weight_fraction_leaf': 0.1,\n        'bootstrap': True,\n        'oob_score': True,\n        'n_jobs': 2,\n        'random_state': 42,\n        'verbose': 1,\n        'warm_start': True\n    }\n\n    est = ForestEstimator(**params)\n    for param, value in params.items():\n        assert_equal(getattr(est, param), value)\n", "def test_random_forest_classifier_with_sample_weights():\n    # Test RandomForestClassifier with different sample weights\n    X, y = datasets.make_classification(n_samples=100, n_features=4, random_state=42)\n    sample_weight = np.random.rand(100)\n    \n    # Train without sample weights\n    clf_no_weights = RandomForestClassifier(random_state=42)\n    clf_no_weights.fit(X, y)\n    \n    # Train with sample weights\n    clf_with_weights = RandomForestClassifier(random_state=42)\n    clf_with_weights.fit(X, y, sample_weight=sample_weight)\n    \n    # Check if the predictions are different\n    predictions_no_weights = clf_no_weights.predict(X)\n    predictions_with_weights = clf_with_weights.predict(X)\n    \n    assert not np.array_equal(predictions_no_weights, predictions_with_weights), \"Predictions with and without sample weights should differ.\"\n\n    # Check if the feature importances are different\n    importances_no_weights = clf_no_weights.feature_importances_\n    importances_with_weights = clf_with_weights.feature_importances_\n    \n    assert not np.allclose(importances_no_weights, importances_with_weights), \"Feature importances with and without sample weights should differ.\"\n", "def test_random_forest_classifier_feature_importances():\n    # Test the feature_importances_ property of RandomForestClassifier\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(iris.data, iris.target)\n    \n    importances = clf.feature_importances_\n    assert_equal(importances.shape[0], iris.data.shape[1])\n    assert_greater(importances.sum(), 0.0)\n    assert_array_almost_equal(importances, clf.feature_importances_)\n", "def test_random_trees_embedding_fit_transform(name):\n    # Check that fit_transform works correctly with RandomTreesEmbedding\n    ForestTransformer = FOREST_TRANSFORMERS[name]\n    \n    X, y = datasets.make_classification(n_samples=50, n_features=10, \n                                        random_state=0)\n    transformer = ForestTransformer(n_estimators=10, max_depth=2, \n                                    random_state=0)\n    \n    X_transformed = transformer.fit_transform(X, y)\n    assert X_transformed.shape[0] == X.shape[0]\n    assert isinstance(X_transformed, csr_matrix if transformer.sparse_output else np.ndarray)\n    \n    transformer.sparse_output = False\n    X_transformed_dense = transformer.fit_transform(X, y)\n    assert X_transformed_dense.shape == (X.shape[0], X_transformed.shape[1])\n    assert isinstance(X_transformed_dense, np.ndarray)\n    assert_array_almost_equal(X_transformed_dense, X_transformed.toarray())\n\n    # Check that transform works correctly after fit_transform\n    X_transformed_test = transformer.transform(X)\n    assert_array_almost_equal(X_transformed_dense, X_transformed_test)\n", "def test_feature_importances(name):\n    # Check that feature importances sum to one and are non-negative\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    X, y = datasets.make_classification(n_samples=100, n_features=4,\n                                        n_informative=2, n_redundant=0,\n                                        random_state=0)\n\n    est = ForestEstimator(n_estimators=10, random_state=0)\n    est.fit(X, y)\n\n    importances = est.feature_importances_\n    assert_array_almost_equal(importances.sum(), 1.0,\n                              err_msg=\"Feature importances do not sum to 1 for {0}\".format(name))\n    assert np.all(importances >= 0.0), \"Negative feature importance for {0}\".format(name)\n"], "sample_773": ["def test_logistic_regression_no_penalty(solver):\n    # Test that LogisticRegression with penalty='none' can be fitted\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    clf = LogisticRegression(penalty='none', solver=solver, max_iter=200, random_state=0)\n    clf.fit(X, y)\n    pred = clf.predict(X)\n    assert clf.coef_.shape == (len(np.unique(y)), X.shape[1])\n    assert clf.intercept_.shape == (len(np.unique(y)),)\n    assert np.all(np.isfinite(clf.coef_))\n    assert np.all(np.isfinite(clf.intercept_))\n    assert_greater(np.mean(pred == y), 0.7)\n", "def test_multinomial_loss_and_grad(solver):\n    # Test that _multinomial_loss_grad returns correct values\n    X, y = make_classification(n_samples=100, n_features=5, n_classes=3,\n                               n_informative=3, random_state=0)\n    X = scale(X)\n    lbin = LabelBinarizer()\n    Y_multi = lbin.fit_transform(y)\n    if Y_multi.shape[1] == 1:\n        Y_multi = np.hstack([1 - Y_multi, Y_multi])\n    \n    classes = np.unique(y)\n    n_classes = classes.size\n    n_features = X.shape[1]\n    w = np.zeros((n_classes, n_features + 1), dtype=X.dtype)\n    \n    sample_weight = np.ones(X.shape[0], dtype=X.dtype)\n    alpha = 1.0\n    \n    loss, grad, p = _multinomial_loss_grad(w.ravel(), X, Y_multi, alpha, sample_weight)\n    expected_loss, expected_p, _ = _multinomial_loss(w.ravel(), X, Y_multi, alpha, sample_weight)\n    \n    assert_almost_equal(loss, expected_loss)\n    assert_array_almost_equal(p, expected_p)\n    \n    # Check gradient with finite differences\n    approx_grad = optimize.approx_fprime(\n        w.ravel(), lambda w: _multinomial_loss(w, X, Y_multi, alpha, sample_weight)[0], 1e-5\n    )\n    assert_array_almost_equal(grad, approx_grad, decimal=2)\n", "def test_logistic_regression_path_cs():\n    # Test the logistic_regression_path returns coefs for each C correctly\n    X, y = make_classification(n_samples=200, n_features=20, random_state=0)\n\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        Cs = np.logspace(-4, 4, 5)\n        coefs, Cs, _ = _logistic_regression_path(X, y, Cs=Cs, solver=solver, max_iter=1000, tol=1e-5)\n        for i, C in enumerate(Cs):\n            lr = LogisticRegression(C=C, solver=solver, max_iter=1000, tol=1e-5, random_state=0)\n            lr.fit(X, y)\n            assert_array_almost_equal(lr.coef_, coefs[i], decimal=4, err_msg=\"with solver = %s\" % solver)\n", "def test_logistic_regression_path_cs_int():\n    # Test that passing an integer for Cs in _logistic_regression_path works and\n    # the output is consistent with the expected number of regularization parameters.\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    num_cs = 5\n\n    coefs, Cs, n_iter = _logistic_regression_path(X, y, Cs=num_cs, solver='lbfgs')\n    \n    assert len(Cs) == num_cs\n    assert coefs.shape[0] == num_cs\n    assert len(n_iter) == num_cs\n", "def test_logistic_regression_no_penalty(penalty, solver):\n    # Test logistic regression with penalty='none'\n    X, y = make_classification(n_samples=100, n_features=5, random_state=0)\n    \n    if penalty == 'none' and solver == 'liblinear':\n        # liblinear does not support penalty='none'\n        clf = LogisticRegression(penalty=penalty, solver=solver)\n        msg = \"penalty='none' is not supported for the liblinear solver\"\n        assert_raise_message(ValueError, msg, clf.fit, X, y)\n    else:\n        clf = LogisticRegression(penalty=penalty, solver=solver, random_state=0)\n        clf.fit(X, y)\n        assert clf.coef_.shape == (len(np.unique(y)), X.shape[1])\n        assert clf.intercept_.shape == (len(np.unique(y)),)\n"], "sample_774": ["def test_one_hot_encoder_missing_values():\n    # Test OneHotEncoder's handling of missing values in different data types\n    X = [['Male', 1, np.nan], ['Female', 3, 'missing'], ['Female', 2, None]]\n    enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    \n    # Fit the encoder and transform the data\n    X_trans = enc.fit_transform(X)\n    assert_equal(X_trans.shape, (3, 6))\n\n    # Expected transformation result\n    expected = np.array([\n        [0., 1., 1., 0., 0., 0.], \n        [1., 0., 0., 1., 0., 0.], \n        [1., 0., 0., 0., 1., 0.]\n    ])\n\n    # Check the transformed result\n    assert_array_equal(X_trans, expected)\n\n    # Test inverse transform with missing values\n    X_inv = enc.inverse_transform(X_trans)\n    expected_inv = np.array([\n        ['Male', 1, None],\n        ['Female', 3, None],\n        ['Female', 2, None]\n    ], dtype=object)\n    assert_array_equal(X_inv, expected_inv)\n\n    # Check feature names with missing values\n    feature_names = enc.get_feature_names(['gender', 'number', 'status'])\n    expected_feature_names = np.array([\n        'gender_Female', 'gender_Male', \n        'number_1', 'number_3', 'number_2', \n        'status_missing'\n    ])\n    assert_array_equal(feature_names, expected_feature_names)\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])\n    X2 = np.array([[4, 1, 1]])\n\n    # Test that OrdinalEncoder raises error for unknown features present during transform.\n    oe = OrdinalEncoder(categories='auto')\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n    \n    # Test the ignore option, ignores unknown features (giving all 0's)\n    oe = OrdinalEncoder(categories='auto', handle_unknown='ignore')\n    X2_passed = X2.copy()\n    assert_array_equal(oe.fit_transform(X), [[0, 1, 0], [1, 0, 1], [1, 0, 0]])\n    assert_array_equal(oe.transform(X2_passed), [[-1, -1, -1]])\n    # ensure transformed data was not modified in place\n    assert_allclose(X2, X2_passed)\n\n    # Raise error if handle_unknown is neither ignore or error.\n    assert_raises(ValueError, OrdinalEncoder(categories='auto', handle_unknown='invalid').fit, X)\n", "def test_ordinal_encoder_inverse_with_unknowns():\n    # Test the inverse_transform method with unknown categories\n    X = [['a', 'x'], ['b', 'y'], ['c', 'z']]\n    enc = OrdinalEncoder()\n    X_trans = enc.fit_transform(X)\n    # Introduce an unknown category in the transformed data\n    X_trans[0, 1] = 10  # 10 is not a valid category index for the second feature\n\n    expected_inverse = np.array([['a', None], ['b', 'y'], ['c', 'z']], dtype=object)\n    assert_array_equal(enc.inverse_transform(X_trans), expected_inverse)\n\n    # Test with multiple unknowns\n    X_trans[1, 0] = 10  # 10 is not a valid category index for the first feature\n    expected_inverse = np.array([['a', None], [None, 'y'], ['c', 'z']], dtype=object)\n    assert_array_equal(enc.inverse_transform(X_trans), expected_inverse)\n", "def test_one_hot_encoder_with_nan():\n    # Test OneHotEncoder with missing values\n    X = [['Male', 1], ['Female', np.nan], ['Female', 2]]\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X_trans = enc.fit_transform(X).toarray()\n    \n    # Check the shape of the transformed array\n    assert_equal(X_trans.shape, (3, 4))\n    \n    # Check the transformed result\n    expected_result = [[0., 1., 1., 0.],\n                       [1., 0., 0., 0.],\n                       [1., 0., 0., 1.]]\n    assert_array_equal(X_trans, expected_result)\n    \n    # Test inverse transform\n    X_inv = enc.inverse_transform(X_trans)\n    expected_inverse = np.array([['Male', 1],\n                                 ['Female', None],\n                                 ['Female', 2]], dtype=object)\n    assert_array_equal(X_inv, expected_inverse)\n    \n    # Check that an error is raised when handle_unknown is 'error'\n    enc = OneHotEncoder(handle_unknown='error')\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_deprecated_attributes():\n    # Test accessing deprecated attributes\n    X = [['a', 1], ['b', 2]]\n    enc = OneHotEncoder()\n    with ignore_warnings(category=DeprecationWarning):\n        enc.fit(X)\n\n    with pytest.warns(DeprecationWarning, match=\"The `active_features_` attribute was deprecated\"):\n        _ = enc.active_features_\n    \n    with pytest.warns(DeprecationWarning, match=\"The `feature_indices_` attribute was deprecated\"):\n        _ = enc.feature_indices_\n    \n    with pytest.warns(DeprecationWarning, match=\"The `n_values_` attribute was deprecated\"):\n        _ = enc.n_values_\n"], "sample_775": ["def test_custom_object():\n    # Test pretty-printing of a custom estimator object\n    class CustomEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n            return {\"param1\": self.param1, \"param2\": self.param2}\n\n    custom_estimator = CustomEstimator(param1=10, param2=20)\n    expected = \"\"\"", "def test_custom_estimator():\n    # Custom estimator to test pretty printing of user-defined estimators\n    class CustomEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n            return {\"param1\": self.param1, \"param2\": self.param2}\n\n    custom_estimator = CustomEstimator(param1=10, param2=20)\n    expected = \"\"\"", "def test_estimator_with_nested_params():\n    # Test pretty printing of an estimator with nested parameters\n    class CustomEstimator(BaseEstimator):\n            self.nested_param = nested_param\n\n            return {'nested_param': self.nested_param}\n\n    class NestedParam:\n            return \"NestedParam(sub_param=42)\"\n\n    estimator = CustomEstimator(nested_param=NestedParam())\n    expected = \"\"\"CustomEstimator(nested_param=NestedParam(sub_param=42))\"\"\"\n    assert estimator.__repr__() == expected\n", "def test_custom_compact_dict_representation():\n    # Test custom compact representation of dictionaries\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    \n    # A simple dictionary\n    simple_dict = {'a': 1, 'b': 2, 'c': 3}\n    expected = \"\"\"", "def test_format_params_or_dict_items():\n    # Test _format_params_or_dict_items method with compact=True for dict\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    params = {'param1': 'value1', 'param2': 'value2', 'param3': 'value3'}\n    stream = []\n    pp._format_params_or_dict_items(params.items(), stream.append, 0, 0, {}, 0, True)\n    expected = \"param1: 'value1', param2: 'value2', param3: 'value3'\"\n    assert ''.join(stream) == expected\n\n    # Test _format_params_or_dict_items method with compact=False for dict\n    pp = _EstimatorPrettyPrinter(compact=False, indent=1, indent_at_name=True)\n    stream = []\n    pp._format_params_or_dict_items(params.items(), stream.append, 0, 0, {}, 0, True)\n    expected = \"param1: 'value1',\\n param2: 'value2',\\n param3: 'value3'\"\n    assert ''.join(stream) == expected\n\n    # Test _format_params_or_dict_items method with compact=True for params\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    params = {'param1': 'value1', 'param2': 'value2', 'param3': 'value3'}\n    stream = []\n    pp._format_params_or_dict_items(params.items(), stream.append, 0, 0, {}, 0, False)\n    expected = \"param1='value1', param2='value2', param3='value3'\"\n    assert ''.join(stream) == expected\n\n    # Test _format_params_or_dict_items method with compact=False for params\n    pp = _EstimatorPrettyPrinter(compact=False, indent=1, indent_at_name=True)\n    stream = []\n    pp._format_params_or_dict_items(params.items(), stream.append, 0, 0, {}, 0, False)\n    expected = \"param1='value1',\\n param2='value2',\\n param3='value3'\"\n    assert ''.join(stream) == expected\n"], "sample_776": ["def test_lars_path_max_iter():\n    # Test that lars_path respects the max_iter parameter\n    max_iter = 3\n    alphas_, active, coef_path_, n_iter = linear_model.lars_path(\n        diabetes.data, diabetes.target, method=\"lar\", max_iter=max_iter, return_n_iter=True)\n    \n    assert n_iter <= max_iter  # Check that number of iterations does not exceed max_iter\n    \n    max_iter = 10\n    alphas_, active, coef_path_, n_iter = linear_model.lars_path(\n        diabetes.data, diabetes.target, method=\"lar\", max_iter=max_iter, return_n_iter=True)\n    \n    assert n_iter <= max_iter  # Check that number of iterations does not exceed max_iter\n", "def test_lars_path_max_iter():\n    # Test that lars_path respects the max_iter parameter and stops early\n    max_iter = 5\n    alphas_, active, coef_path_ = linear_model.lars_path(\n        diabetes.data, diabetes.target, method=\"lar\", max_iter=max_iter)\n\n    # Assert that the number of iterations is less than or equal to max_iter\n    assert len(alphas_) <= max_iter + 1\n    assert len(active) <= max_iter\n\n    # Also ensure the coefficients path has the expected shape\n    assert coef_path_.shape[1] == len(alphas_)\n\n    # Check if lars_path returns early when max_iter is set to a small value\n    max_iter = 1\n    alphas_, active, coef_path_ = linear_model.lars_path(\n        diabetes.data, diabetes.target, method=\"lasso\", max_iter=max_iter)\n\n    assert len(alphas_) <= max_iter + 1\n    assert len(active) <= max_iter\n    assert coef_path_.shape[1] == len(alphas_)\n", "def test_lars_path_return_n_iter():\n    # Test that the ``return_n_iter=True`` option returns the correct number of iterations\n\n    X, y = diabetes.data, diabetes.target\n    alphas, active, coefs, n_iter = linear_model.lars_path(X, y, method=\"lar\", return_n_iter=True)\n    \n    assert isinstance(n_iter, int), \"n_iter should be an integer\"\n    assert n_iter > 0, \"n_iter should be greater than 0\"\n    assert n_iter == len(alphas) - 1, \"n_iter should be equal to the number of alphas minus 1\"\n", "def test_lars_path_positive():\n    # Test the lars_path function with positive=True for Lasso method\n    X, y = diabetes.data, diabetes.target\n    \n    # Run LARS with positive=True\n    alphas_pos, active_pos, coefs_pos = linear_model.lars_path(X, y, method='lasso', positive=True)\n    \n    # Ensure all coefficients are non-negative\n    assert np.all(coefs_pos >= 0)\n    \n    # Run LARS with positive=False for comparison\n    alphas_neg, active_neg, coefs_neg = linear_model.lars_path(X, y, method='lasso', positive=False)\n    \n    # Ensure at least one coefficient is negative\n    assert np.any(coefs_neg < 0)\n\n    # Verify that both methods return the same active variables\n    assert_array_almost_equal(active_pos, active_neg)\n    \n    # Ensure that the maximum alpha values are identical\n    assert_equal(alphas_pos[0], alphas_neg[0])\n", "def test_lars_path_no_early_stopping():\n    # Test that the lars_path function correctly runs to the maximum number of iterations\n    # when alpha_min is set to a very small value ensuring no early stopping\n    X = diabetes.data\n    y = diabetes.target\n    max_iter = 5\n    alphas_, active, coef_path_, n_iter_ = linear_model.lars_path(\n        X, y, method=\"lar\", max_iter=max_iter, alpha_min=-1.0, return_n_iter=True\n    )\n    \n    # Check that the number of iterations run equals the maximum number of iterations specified\n    assert_equal(n_iter_, max_iter)\n    \n    # Check that the final coefficients are finite\n    assert np.isfinite(coef_path_).all()\n"], "sample_777": ["def test_quantile_estimator():\n    # Check QuantileEstimator functionality.\n    X = np.random.rand(100, 2)\n    y = np.random.rand(100)\n    alpha = 0.75\n\n    estimator = QuantileEstimator(alpha=alpha)\n    estimator.fit(X, y)\n    predictions = estimator.predict(X)\n\n    assert_equal(predictions.shape, (100,))\n    assert_almost_equal(np.percentile(y, alpha * 100), predictions[0])\n", "def test_mean_estimator():\n    # Test that MeanEstimator returns the mean of the training targets.\n    est = MeanEstimator()\n    X_train = np.array([[1, 2], [3, 4], [5, 6]])\n    y_train = np.array([2, 3, 5])\n\n    est.fit(X_train, y_train)\n    y_pred = est.predict(X_train)\n    \n    assert_array_almost_equal(y_pred, np.array([3.33333333, 3.33333333, 3.33333333]))\n    \n    # Test with sample weights\n    sample_weight = np.array([0.2, 0.3, 0.5])\n    est.fit(X_train, y_train, sample_weight=sample_weight)\n    y_pred = est.predict(X_train)\n    \n    weighted_mean = np.average(y_train, weights=sample_weight)\n    assert_array_almost_equal(y_pred, np.array([weighted_mean, weighted_mean, weighted_mean]))\n", "def test_zero_estimator_error_handling():\n    # Ensure ZeroEstimator raises the appropriate error when misconfigured.\n    zero_estimator = ZeroEstimator()\n    clf = GradientBoostingClassifier(n_estimators=10, init=zero_estimator)\n    assert_raises(NotFittedError, zero_estimator.predict, X)\n    assert_raises(NotFittedError, zero_estimator.predict_proba, X)\n    \n    zero_estimator.fit(X, y)\n    assert_raises(NotFittedError, zero_estimator.predict, [])\n    assert_raises(NotFittedError, zero_estimator.predict_proba, [])\n", "def test_zero_estimator_clf_predictions():\n    # Test ZeroEstimator predictions for classification\n    est = ZeroEstimator()\n    est.fit(X, y)\n    y_pred = est.predict(X)\n    assert_array_equal(y_pred, np.zeros((len(X), 1)))\n    y_proba = est.predict_proba(X)\n    assert_array_equal(y_proba, np.zeros((len(X), 1)))\n\n    # Test if ZeroEstimator raises NotFittedError when not fitted\n    est_unfitted = ZeroEstimator()\n    with pytest.raises(NotFittedError):\n        est_unfitted.predict(X)\n    with pytest.raises(NotFittedError):\n        est_unfitted.predict_proba(X)\n", "def test_custom_loss_function():\n    # Check that a custom loss function can be used with GradientBoostingRegressor\n    class CustomLossFunction(_gb_losses.RegressionLossFunction):\n            super().__init__(n_classes)\n        \n            return DummyRegressor(strategy=\"mean\")\n        \n            return np.mean(np.abs(y - pred))\n        \n            return y - pred\n\n            terminal_region = np.where(terminal_regions == leaf)[0]\n            tree.value[leaf, 0, 0] = np.mean(residual.take(terminal_region, axis=0))\n\n    X, y = datasets.make_regression(n_samples=100, n_features=4, noise=0.1, random_state=42)\n    clf = GradientBoostingRegressor(loss=CustomLossFunction(1), n_estimators=50, random_state=42)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    mse = mean_squared_error(y, y_pred)\n    assert mse < 0.5, \"Custom loss function failed with mse %.4f\" % mse\n"], "sample_778": ["def test_invalid_beta_loss():\n    # Test that an error is raised if an invalid beta_loss is provided.\n    A = np.abs(np.random.randn(6, 4))\n    msg = \"Invalid beta_loss parameter: got 'invalid' instead of one\"\n    assert_raise_message(ValueError, msg, non_negative_factorization, A, beta_loss='invalid')\n\n    msg = \"Invalid beta_loss parameter: got None instead of one\"\n    assert_raise_message(ValueError, msg, non_negative_factorization, A, beta_loss=None)\n\n    msg = \"Invalid beta_loss parameter: got [] instead of one\"\n    assert_raise_message(ValueError, msg, non_negative_factorization, A, beta_loss=[])\n\n    msg = \"Invalid beta_loss parameter: got {} instead of one\"\n    assert_raise_message(ValueError, msg, non_negative_factorization, A, beta_loss={})\n", "def test_solver_parameters():\n    # Test that invalid solver parameters raise appropriate errors\n    A = np.ones((2, 2))\n    msg = \"Invalid solver parameter 'invalid_solver'\"\n    assert_raise_message(ValueError, msg, NMF(solver='invalid_solver').fit, A)\n    \n    # Test valid solver parameters do not raise errors\n    for solver in ['cd', 'mu']:\n        try:\n            NMF(solver=solver).fit(A)\n        except ValueError as e:\n            pytest.fail(f\"Unexpected ValueError raised: {e}\")\n", "def test_check_init():\n    # Test that _check_init raises the appropriate errors\n    rng = np.random.mtrand.RandomState(42)\n    A = np.abs(rng.randn(10, 5))\n\n    # Check shape mismatch\n    msg = \"Array with wrong shape passed to NMF initialization\"\n    assert_raise_message(ValueError, msg, nmf._check_init, A, (5, 10), \"NMF initialization\")\n\n    # Check non-negative values\n    A_negative = A.copy()\n    A_negative[0, 0] = -1\n    msg = \"Negative values in data passed to NMF initialization\"\n    assert_raise_message(ValueError, msg, nmf._check_init, A_negative, A.shape, \"NMF initialization\")\n\n    # Check array full of zeros\n    A_zeros = np.zeros((10, 5))\n    msg = \"Array passed to NMF initialization is full of zeros.\"\n    assert_raise_message(ValueError, msg, nmf._check_init, A_zeros, A.shape, \"NMF initialization\")\n", "def test_non_negative_factorization_init_warning():\n    # Test the FutureWarning for the default value of init in non_negative_factorization\n    rng = np.random.mtrand.RandomState(42)\n    A = np.abs(rng.randn(10, 10))\n\n    msg = (\"The default value of init will change from \"\n           \"random to None in 0.23 to make it consistent \"\n           \"with decomposition.NMF.\")\n    with pytest.warns(FutureWarning, match=msg):\n        non_negative_factorization(A, n_components=5)\n", "def test_initialize_nmf_random_seed():\n    # Test that the random seed produces the same initialization\n    rng = np.random.mtrand.RandomState(42)\n    data = np.abs(rng.randn(10, 10))\n    \n    for init in ('random', 'nndsvd', 'nndsvda', 'nndsvdar'):\n        W1, H1 = nmf._initialize_nmf(data, 10, init=init, random_state=0)\n        W2, H2 = nmf._initialize_nmf(data, 10, init=init, random_state=0)\n        assert_array_almost_equal(W1, W2)\n        assert_array_almost_equal(H1, H2)\n\n    # Test with different random seeds\n    W1, H1 = nmf._initialize_nmf(data, 10, init='random', random_state=0)\n    W2, H2 = nmf._initialize_nmf(data, 10, init='random', random_state=1)\n    assert not np.allclose(W1, W2)\n    assert not np.allclose(H1, H2)\n"], "sample_779": ["def test_check_estimators_dtypes():\n    # Check if estimator can handle different data types\n    class EstimatorWithDtypeSupport(BaseEstimator):\n            X, y = check_X_y(X, y)\n            return self\n\n            X = check_array(X)\n            return np.ones(X.shape[0])\n\n    # Creating estimator instance\n    estimator = EstimatorWithDtypeSupport()\n\n    # Running the test\n    check_estimators_dtypes(estimator.__class__.__name__, estimator)\n", "def test_check_estimators_partial_fit_n_features():\n    # check that estimators with partial_fit raise an error when number of features changes\n    from sklearn.linear_model import SGDClassifier\n\n    # SGDClassifier supports partial_fit\n    estimator = SGDClassifier()\n    check_estimators_partial_fit_n_features(estimator.__class__.__name__, estimator)\n\n    # Custom estimator to test partial_fit behavior\n    class PartialFitEstimator(BaseEstimator):\n            self.coef_ = None\n\n            X, y = check_X_y(X, y)\n            if self.coef_ is None:\n                self.coef_ = np.zeros(X.shape[1])\n            elif X.shape[1] != self.coef_.shape[0]:\n                raise ValueError(\"Number of features does not match.\")\n            return self\n\n            return self.partial_fit(X, y)\n\n    custom_estimator = PartialFitEstimator()\n    check_estimators_partial_fit_n_features(custom_estimator.__class__.__name__, custom_estimator)\n", "def test_check_class_weight_balanced():\n    # Test that class_weight=\"balanced\" improves the fit of classifiers\n    \n    from sklearn.datasets import make_classification\n    from sklearn.linear_model import LogisticRegression\n    \n    # Generate a binary classification dataset with imbalanced classes\n    X, y = make_classification(n_samples=1000, n_features=20, \n                               n_informative=2, n_redundant=10, \n                               n_clusters_per_class=1, weights=[0.99, 0.01], \n                               flip_y=0, random_state=0)\n    \n    # Split into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, \n                                                        random_state=0)\n    \n    # Train a LogisticRegression classifier with default class_weight\n    clf = LogisticRegression()\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    f1_default = f1_score(y_test, y_pred, average='weighted')\n    \n    # Train a LogisticRegression classifier with class_weight=\"balanced\"\n    clf_balanced = LogisticRegression(class_weight='balanced')\n    clf_balanced.fit(X_train, y_train)\n    y_pred_balanced = clf_balanced.predict(X_test)\n    f1_balanced = f1_score(y_test, y_pred_balanced, average='weighted')\n    \n    # Assert that the balanced classifier has a better F1 score\n    assert_greater(f1_balanced, f1_default, \n                   \"Class weight 'balanced' did not improve F1 score\")\n    \n    # Also run the check_class_weight_balanced_classifiers utility function\n    check_class_weight_balanced_classifiers(\"LogisticRegression\", \n                                            LogisticRegression, X_train, \n                                            y_train, X_test, y_test, None)\n", "def test_check_estimators_dtypes():\n    class EstimatorWithDtypes(BaseEstimator):\n            X, y = check_X_y(X, y, dtype=np.float64, ensure_2d=True)\n            return self\n\n            X = check_array(X, dtype=np.float64, ensure_2d=True)\n            return np.ones(X.shape[0])\n\n    check_estimators_dtypes(\"EstimatorWithDtypes\", EstimatorWithDtypes())\n", "def test_check_class_weight():\n    # Check that class_weight parameter works correctly with classifiers\n\n    # Check with balanced class weights\n    X, y = make_blobs(centers=2, random_state=0, n_samples=30)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    weights = {0: 0.1, 1: 0.9}\n\n    classifiers = [SVC(), RandomForestClassifier(), SGDClassifier()]\n\n    for classifier in classifiers:\n        # Test with custom class weights\n        classifier.set_params(class_weight=weights)\n        classifier.fit(X_train, y_train)\n        y_pred = classifier.predict(X_test)\n        assert_greater(np.mean(y_pred == y_test), 0.7)\n\n        # Test with 'balanced' class weights\n        check_class_weight_balanced_classifiers(\n            classifier.__class__.__name__, classifier, X_train, y_train,\n            X_test, y_test, weights\n        )\n\n    # Check that BadBalancedWeightsClassifier fails the test\n    msg = \"Classifier BadBalancedWeightsClassifier is not computing class_weight=balanced properly\"\n    assert_raises_regex(AssertionError, msg, check_class_weight_balanced_linear_classifier,\n                        \"BadBalancedWeightsClassifier\", BadBalancedWeightsClassifier)\n"], "sample_780": ["def test_lda_check_params():\n    # Test that _check_params raises the appropriate errors\n    with pytest.raises(ValueError, match=\"Invalid 'n_components' parameter\"):\n        LatentDirichletAllocation(n_components=0)._check_params()\n    with pytest.raises(ValueError, match=\"Invalid 'total_samples' parameter\"):\n        LatentDirichletAllocation(total_samples=0)._check_params()\n    with pytest.raises(ValueError, match=\"Invalid 'learning_offset' parameter\"):\n        LatentDirichletAllocation(learning_offset=-1)._check_params()\n    with pytest.raises(ValueError, match=\"Invalid 'learning_method' parameter\"):\n        LatentDirichletAllocation(learning_method='invalid_method')._check_params()\n", "def test_lda_doc_topic_prior():\n    # Test LDA with different doc_topic_prior values\n    rng = np.random.RandomState(0)\n    n_components, X = _build_sparse_mtx()\n    priors = [None, 0.1, 1.0, 10.0]\n    for prior in priors:\n        lda = LatentDirichletAllocation(n_components=n_components,\n                                        doc_topic_prior=prior,\n                                        random_state=rng)\n        lda.fit(X)\n        assert lda.doc_topic_prior_ == (1. / n_components if prior is None else prior)\n", "def test_lda_max_iter():\n    # Test that LDA respects the max_iter parameter\n    rng = np.random.RandomState(0)\n    n_components, X = _build_sparse_mtx()\n    max_iter = 5\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    max_iter=max_iter,\n                                    learning_method='batch',\n                                    random_state=rng)\n    lda.fit(X)\n    assert_equal(lda.n_iter_, max_iter)\n\n    # Verify that increasing max_iter allows more iterations\n    max_iter = 10\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    max_iter=max_iter,\n                                    learning_method='batch',\n                                    random_state=rng)\n    lda.fit(X)\n    assert_equal(lda.n_iter_, max_iter)\n", "def test_lda_check_params():\n    # Test the _check_params method directly by initializing with invalid parameters\n    invalid_params = [\n        {\"n_components\": -1},\n        {\"learning_method\": \"invalid_method\"},\n        {\"total_samples\": -1},\n        {\"learning_offset\": -10},\n    ]\n\n    for params in invalid_params:\n        lda = LatentDirichletAllocation(**params)\n        with pytest.raises(ValueError, match=r\"^Invalid '.*' parameter\"):\n            lda._check_params()\n", "def test_lda_check_params():\n    # Test the private method _check_params\n    lda = LatentDirichletAllocation(n_components=-1)\n    with pytest.raises(ValueError, match=\"Invalid 'n_components' parameter\"):\n        lda._check_params()\n\n    lda = LatentDirichletAllocation(total_samples=0)\n    with pytest.raises(ValueError, match=\"Invalid 'total_samples' parameter\"):\n        lda._check_params()\n\n    lda = LatentDirichletAllocation(learning_offset=-1)\n    with pytest.raises(ValueError, match=\"Invalid 'learning_offset' parameter\"):\n        lda._check_params()\n\n    lda = LatentDirichletAllocation(learning_method='unknown')\n    with pytest.raises(ValueError, match=\"Invalid 'learning_method' parameter\"):\n        lda._check_params()\n"], "sample_781": ["def test_random_trees_embedding_fit_transform():\n    # Test the fit_transform method of RandomTreesEmbedding\n    X, y = datasets.make_classification(n_samples=100, n_features=10, random_state=0)\n    \n    # Create RandomTreesEmbedding instance with sparse_output=True\n    rte_sparse = RandomTreesEmbedding(n_estimators=10, random_state=0, sparse_output=True)\n    X_transformed_sparse = rte_sparse.fit_transform(X)\n    \n    # Create RandomTreesEmbedding instance with sparse_output=False\n    rte_dense = RandomTreesEmbedding(n_estimators=10, random_state=0, sparse_output=False)\n    X_transformed_dense = rte_dense.fit_transform(X)\n    \n    # Check that the dense transformation is a dense numpy array\n    assert isinstance(X_transformed_dense, np.ndarray)\n    \n    # Check that the sparse transformation is a sparse matrix\n    assert isinstance(X_transformed_sparse, csr_matrix)\n    \n    # Check that the shapes of the transformed data are as expected\n    assert X_transformed_sparse.shape == X_transformed_dense.shape\n    \n    # Check that the transformed data from sparse and dense are equivalent\n    assert_array_almost_equal(X_transformed_sparse.toarray(), X_transformed_dense)\n", "def test_apply_returns_leaf_indices(name):\n    # Test that apply method returns correct leaf indices\n    X, y = iris.data, iris.target\n    ForestEstimator = FOREST_ESTIMATORS[name]\n\n    est = ForestEstimator(n_estimators=10, random_state=0)\n    est.fit(X, y)\n    leaves = est.apply(X)\n\n    # Check shape of leaves is correct\n    assert leaves.shape == (X.shape[0], est.n_estimators)\n\n    # Check that the leaves returned are indeed leaves in the trees\n    for estimator, leaf_indices in zip(est.estimators_, leaves.T):\n        tree = estimator.tree_\n        assert np.all(tree.children_left[leaf_indices] == -1)\n        assert np.all(tree.children_right[leaf_indices] == -1)\n", "def test_oob_prediction_shape(name):\n    # Test that the shape of oob_prediction_ matches the expectations\n    X, y = datasets.make_classification(n_samples=100, n_features=4, random_state=0)\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    \n    if name in FOREST_CLASSIFIERS:\n        est = ForestEstimator(n_estimators=20, oob_score=True, bootstrap=True, random_state=0)\n        est.fit(X, y)\n        assert hasattr(est, \"oob_decision_function_\")\n        assert est.oob_decision_function_.shape == (X.shape[0], len(np.unique(y)))\n        \n    elif name in FOREST_REGRESSORS:\n        est = ForestEstimator(n_estimators=20, oob_score=True, bootstrap=True, random_state=0)\n        est.fit(X, y)\n        assert hasattr(est, \"oob_prediction_\")\n        assert est.oob_prediction_.shape == y.shape\n", "def test_feature_importances_zero_when_single_node_trees():\n    # Check if feature_importances_ are all zeros when trees are single node\n    X, y = make_classification(n_samples=10, n_informative=2, n_features=4, random_state=0)\n    clf = RandomForestClassifier(n_estimators=5, max_depth=1, random_state=0)\n    clf.fit(X, y)\n    assert_array_equal(clf.feature_importances_, np.zeros(X.shape[1]))\n", "def test_parallel_build_trees(name):\n    # Test _parallel_build_trees function.\n    X, y = hastie_X, hastie_y\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    forest = ForestEstimator(n_estimators=10, bootstrap=True, random_state=0)\n    forest.fit(X, y)\n\n    # Extract a single tree and fit it using _parallel_build_trees\n    tree = forest.estimators_[0]\n    X_sample, y_sample = X[:10], y[:10]\n    sample_weight = np.ones_like(y_sample)\n\n    tree_fitted = _parallel_build_trees(tree, forest, X_sample, y_sample, sample_weight, 0, 1)\n    \n    # Validate that the tree has been fitted correctly\n    assert hasattr(tree_fitted, 'tree_')\n    assert tree_fitted.tree_.node_count > 1\n"], "sample_782": ["def test_column_transformer_with_custom_weights():\n    X_array = np.array([[1, 2, 3], [4, 5, 6]]).T\n    custom_transformer = StandardScaler()\n\n    ct = ColumnTransformer([('trans1', custom_transformer, [0]),\n                            ('trans2', custom_transformer, [1])],\n                           transformer_weights={'trans1': 2, 'trans2': 0.5})\n\n    X_trans = ct.fit_transform(X_array)\n    expected_trans1 = 2 * custom_transformer.fit_transform(X_array[:, [0]])\n    expected_trans2 = 0.5 * custom_transformer.fit_transform(X_array[:, [1]])\n    expected_X_trans = np.hstack([expected_trans1, expected_trans2])\n\n    assert_array_equal(X_trans, expected_X_trans)\n    assert_array_equal(ct.fit(X_array).transform(X_array), expected_X_trans)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n", "def test_column_transformer_with_no_transformers():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([])\n\n    # Ensure that fit and transform do not alter the input data\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n\n    # Ensure that fit and transform do not alter the input data for pandas DataFrame\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n    assert_array_equal(ct.fit_transform(X_df), X_df.values)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_df.values)\n", "def test_column_transformer_with_mixed_types():\n    # Test ColumnTransformer with mixed data types (numeric, string, boolean)\n    X_mixed = np.array([\n        [1, 'a', True],\n        [2, 'b', False],\n        [3, 'c', True]\n    ], dtype=object)\n\n    expected_output = np.array([\n        [-1.22474487, 1, 0, 1],\n        [0., 0, 1, 0],\n        [1.22474487, 0, 0, 1]\n    ])\n\n    ct = ColumnTransformer([\n        ('scaler', StandardScaler(), [0]),\n        ('onehot', OneHotEncoder(), [1]),\n        ('passthrough', 'passthrough', [2])\n    ])\n\n    result = ct.fit_transform(X_mixed)\n    assert result.shape == expected_output.shape\n    assert_allclose_dense_sparse(result, expected_output)\n\n    assert_array_equal(ct.named_transformers_.scaler.mean_, [2])\n    assert ct.named_transformers_.onehot.categories_[0].tolist() == ['a', 'b', 'c']\n    assert ct.transformers_[-1][0] == 'passthrough'\n    assert ct.transformers_[-1][1] == 'passthrough'\n    assert_array_equal(ct.transformers_[-1][2], [2])\n", "def test_column_transformer_remainder_identity_transformer():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    class IdentityTransformer(BaseEstimator, TransformerMixin):\n            return self\n\n            return X\n\n    # Using IdentityTransformer as remainder\n    ct = ColumnTransformer([('trans1', Trans(), [0])], remainder=IdentityTransformer())\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], IdentityTransformer)\n    assert_array_equal(ct.transformers_[-1][2], [1])\n", "def test_column_transformer_with_pipeline():\n    from sklearn.pipeline import Pipeline\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res = np.array([[0, 1, 2], [1, 2, 3]]).T\n\n    pipeline = Pipeline([('scale', StandardScaler())])\n\n    ct = ColumnTransformer([('trans', pipeline, [0, 1])])\n    assert_array_equal(ct.fit_transform(X_array), StandardScaler().fit_transform(X_array))\n    assert_array_equal(ct.fit(X_array).transform(X_array), StandardScaler().fit_transform(X_array))\n\n    pipeline = Pipeline([('double', DoubleTrans())])\n    ct = ColumnTransformer([('trans', pipeline, [0, 1])])\n    assert_array_equal(ct.fit_transform(X_array), 2 * X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), 2 * X_array)\n"], "sample_783": ["def test_imputation_with_sparse_matrix_constant_strategy():\n    # Check imputation with sparse matrix using constant strategy\n    X = sparse.csr_matrix([\n        [np.nan, 1.1, 0, np.nan],\n        [1.2, np.nan, 1.3, np.nan],\n        [0, 0, np.nan, np.nan],\n        [1.4, 1.5, 0, np.nan]\n    ])\n\n    X_true = sparse.csr_matrix([\n        [-1, 1.1, 0, -1],\n        [1.2, -1, 1.3, -1],\n        [0, 0, -1, -1],\n        [1.4, 1.5, 0, -1]\n    ])\n\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=-1)\n    X_trans = imputer.fit_transform(X)\n\n    assert_allclose_dense_sparse(X_trans, X_true)\n", "def test_missing_indicator_fit_transform():\n    # Test fit_transform method of MissingIndicator\n    X = np.array([[np.nan, 1, 3],\n                  [4, 0, np.nan],\n                  [8, 1, 0]])\n    indicator = MissingIndicator()\n    X_ind = indicator.fit_transform(X)\n    assert_array_equal(X_ind, np.array([[True, False], [False, True], [False, False]]))\n", "def test_imputation_median_all_missing_column():\n    # Test if all missing values in a column are handled correctly\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, np.nan],\n        [7, 8, np.nan]\n    ])\n\n    # Expected behavior: the column with all missing values should be removed\n    X_true_mean = np.array([\n        [1, 2],\n        [4, 5],\n        [7, 8]\n    ])\n\n    X_true_median = np.array([\n        [1, 2],\n        [4, 5],\n        [7, 8]\n    ])\n\n    imputer_mean = SimpleImputer(strategy='mean')\n    X_trans_mean = imputer_mean.fit_transform(X)\n    assert_array_equal(X_trans_mean, X_true_mean)\n\n    imputer_median = SimpleImputer(strategy='median')\n    X_trans_median = imputer_median.fit_transform(X)\n    assert_array_equal(X_trans_median, X_true_median)\n", "def test_imputation_all_missing_column():\n    # Test that columns with all missing values are discarded when strategy is not \"constant\"\n    X = np.array([\n        [np.nan, 2, 3, np.nan],\n        [np.nan, 5, 6, np.nan],\n        [np.nan, 8, 9, np.nan]\n    ])\n\n    strategies = [\"mean\", \"median\", \"most_frequent\"]\n\n    for strategy in strategies:\n        imputer = SimpleImputer(strategy=strategy)\n        X_trans = imputer.fit_transform(X)\n\n        # Ensure the column with all missing values is discarded\n        assert X_trans.shape[1] == 2\n        # Ensure the correct columns are kept\n        assert_array_equal(X_trans[:, 0], np.array([2, 5, 8]))\n        assert_array_equal(X_trans[:, 1], np.array([3, 6, 9]))\n\n    # Test that column with all missing values is kept when strategy is \"constant\"\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=-1)\n    X_trans = imputer.fit_transform(X)\n\n    # Ensure all columns are kept\n    assert X_trans.shape[1] == 4\n    # Ensure missing values are filled with the constant\n    assert_array_equal(X_trans[:, 0], np.array([-1, -1, -1]))\n    assert_array_equal(X_trans[:, 3], np.array([-1, -1, -1]))\n", "def test_missing_indicator_all_features(marker):\n    # Test MissingIndicator with features='all' parameter\n    X = np.array([\n        [marker, \"a\", \"b\"],\n        [\"c\", marker, \"d\"],\n        [\"e\", \"f\", marker],\n        [\"g\", \"h\", \"i\"]\n    ], dtype=object)\n\n    X_true = np.array([\n        [True, False, False],\n        [False, True, False],\n        [False, False, True],\n        [False, False, False]\n    ], dtype=bool)\n\n    indicator = MissingIndicator(missing_values=marker, features='all', sparse=False)\n    X_trans = indicator.fit_transform(X)\n\n    assert_array_equal(X_trans, X_true)\n    assert_array_equal(indicator.features_, np.array([0, 1, 2]))\n\n    indicator.set_params(sparse=True)\n    X_trans_sparse = indicator.fit_transform(X)\n    assert isinstance(X_trans_sparse, sparse.csc_matrix)\n    assert_array_equal(X_trans_sparse.toarray(), X_true)\n"], "sample_784": ["def test_calibration_no_sample_weight_provided():\n    \"\"\"Test calibration without providing sample weights\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6, random_state=42)\n    X_train, y_train = X[:n_samples], y[:n_samples]\n    X_test, y_test = X[n_samples:], y[n_samples:]\n\n    base_estimators = [LinearSVC(random_state=42), MultinomialNB()]\n\n    for base_estimator in base_estimators:\n        base_estimator.fit(X_train, y_train)\n        for method in ['sigmoid', 'isotonic']:\n            calibrated_clf = CalibratedClassifierCV(base_estimator, method=method, cv=3)\n            calibrated_clf.fit(X_train, y_train)\n            y_prob = calibrated_clf.predict_proba(X_test)\n            y_pred = calibrated_clf.predict(X_test)\n\n            # Check that predictions sum to 1\n            assert_array_almost_equal(np.sum(y_prob, axis=1), np.ones(len(X_test)))\n\n            # Check that Brier score has improved after calibration\n            prob_pos_clf = base_estimator.predict_proba(X_test)[:, 1] if hasattr(base_estimator, 'predict_proba') else expit(base_estimator.decision_function(X_test))\n            prob_pos_calibrated = y_prob[:, 1]\n            assert_greater(brier_score_loss(y_test, prob_pos_clf), brier_score_loss(y_test, prob_pos_calibrated))\n", "def test_calibration_curve_all_zero_or_one():\n    \"\"\"Check calibration_curve for edge cases where y_pred is all zeros or all ones\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    \n    # Case when y_pred is all zeros\n    y_pred_all_zeros = np.array([0, 0, 0, 0, 0, 0])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred_all_zeros, n_bins=2)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 1)\n    assert_almost_equal(prob_true, [0.5])\n    assert_almost_equal(prob_pred, [0.0])\n    \n    # Case when y_pred is all ones\n    y_pred_all_ones = np.array([1, 1, 1, 1, 1, 1])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred_all_ones, n_bins=2)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 1)\n    assert_almost_equal(prob_true, [0.5])\n    assert_almost_equal(prob_pred, [1.0])\n", "def test_calibrated_classifier_default_linear_svc():\n    \"\"\"Test that CalibratedClassifierCV defaults to LinearSVC when base_estimator is None\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=n_samples, n_features=6,\n                               random_state=42)\n\n    X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]\n    X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]\n\n    # CalibratedClassifierCV without base_estimator should default to LinearSVC\n    cal_clf = CalibratedClassifierCV(method='sigmoid', cv=2)\n    cal_clf.fit(X_train, y_train)\n    probas = cal_clf.predict_proba(X_test)\n    assert_array_almost_equal(np.sum(probas, axis=1), np.ones(len(X_test)))\n\n    # Check that log-loss of calibrated classifier is smaller than\n    # log-loss of uncalibrated LinearSVC turned probabilities via Platt scaling\n    clf = LinearSVC(random_state=42)\n    clf.fit(X_train, y_train)\n    uncalibrated_probas = expit(clf.decision_function(X_test))\n    uncalibrated_log_loss = log_loss(y_test, uncalibrated_probas)\n    calibrated_log_loss = log_loss(y_test, probas)\n    assert_greater(uncalibrated_log_loss, calibrated_log_loss)\n", "def test_calibrated_classifier_init():\n    \"\"\"Test initialization of CalibratedClassifierCV with different parameters\"\"\"\n    # Test default initialization\n    clf = CalibratedClassifierCV()\n    assert clf.base_estimator is None\n    assert clf.method == 'sigmoid'\n    assert clf.cv == 'warn'\n\n    # Test initialization with custom parameters\n    base_estimator = RandomForestClassifier(n_estimators=10)\n    clf = CalibratedClassifierCV(base_estimator=base_estimator, method='isotonic', cv=5)\n    assert clf.base_estimator == base_estimator\n    assert clf.method == 'isotonic'\n    assert clf.cv == 5\n\n    # Test invalid method\n    with pytest.raises(ValueError):\n        CalibratedClassifierCV(method='invalid_method')\n", "def test_calibration_classifier_with_weights():\n    \"\"\"Test calibration with classifier that inherently supports sample weights.\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6, random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=len(y))\n\n    X_train, y_train, sw_train = X[:n_samples], y[:n_samples], sample_weight[:n_samples]\n    X_test, y_test = X[n_samples:], y[n_samples:]\n\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X_train, y_train, sample_weight=sw_train)\n    prob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n    for method in ['isotonic', 'sigmoid']:\n        pc_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        pc_clf.fit(X_train, y_train, sample_weight=sw_train)\n        prob_pos_pc_clf = pc_clf.predict_proba(X_test)[:, 1]\n\n        # Check that brier score has improved after calibration\n        assert_greater(brier_score_loss(y_test, prob_pos_clf), brier_score_loss(y_test, prob_pos_pc_clf))\n\n        # Check invariance against relabeling [0, 1] -> [1, 2]\n        pc_clf.fit(X_train, y_train + 1, sample_weight=sw_train)\n        prob_pos_pc_clf_relabeled = pc_clf.predict_proba(X_test)[:, 1]\n        assert_array_almost_equal(prob_pos_pc_clf, prob_pos_pc_clf_relabeled)\n\n        # Check invariance against relabeling [0, 1] -> [-1, 1]\n        pc_clf.fit(X_train, 2 * y_train - 1, sample_weight=sw_train)\n        prob_pos_pc_clf_relabeled = pc_clf.predict_proba(X_test)[:, 1]\n        assert_array_almost_equal(prob_pos_pc_clf, prob_pos_pc_clf_relabeled)\n"], "sample_785": ["def test_leave_p_out():\n    # Test LeavePOut with p > 1\n    X = np.arange(10).reshape((5, 2))\n    y = np.array([1, 2, 3, 4, 5])\n    lpo = LeavePOut(p=2)\n    \n    expected_splits = [\n        (np.array([2, 3, 4]), np.array([0, 1])),\n        (np.array([1, 3, 4]), np.array([0, 2])),\n        (np.array([1, 2, 4]), np.array([0, 3])),\n        (np.array([1, 2, 3]), np.array([0, 4])),\n        (np.array([0, 3, 4]), np.array([1, 2])),\n        (np.array([0, 2, 4]), np.array([1, 3])),\n        (np.array([0, 2, 3]), np.array([1, 4])),\n        (np.array([0, 1, 4]), np.array([2, 3])),\n        (np.array([0, 1, 3]), np.array([2, 4])),\n        (np.array([0, 1, 2]), np.array([3, 4]))\n    ]\n    \n    splits = list(lpo.split(X, y))\n    \n    for i, (train, test) in enumerate(splits):\n        expected_train, expected_test = expected_splits[i]\n        assert_array_equal(train, expected_train)\n        assert_array_equal(test, expected_test)\n        \n    # Check that the number of splits is correct\n    assert_equal(lpo.get_n_splits(X, y), len(expected_splits))\n", "def test_leave_one_out_split():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n    splits = list(loo.split(X, y))\n\n    assert_equal(len(splits), 4)\n    assert_equal(len(splits[0][0]), 3)\n    assert_equal(len(splits[0][1]), 1)\n\n    for train_index, test_index in splits:\n        assert_equal(len(test_index), 1)\n        assert_equal(len(train_index), 3)\n        assert not set(train_index).intersection(test_index)\n", "def test_leave_one_out():\n    # Test LeaveOneOut\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n    \n    splits = list(loo.split(X, y))\n    expected_splits = [\n        (np.array([1, 2, 3]), np.array([0])),\n        (np.array([0, 2, 3]), np.array([1])),\n        (np.array([0, 1, 3]), np.array([2])),\n        (np.array([0, 1, 2]), np.array([3]))\n    ]\n    assert_equal(len(splits), len(expected_splits))\n    for (train, test), (expected_train, expected_test) in zip(splits, expected_splits):\n        assert_array_equal(train, expected_train)\n        assert_array_equal(test, expected_test)\n    \n    assert_equal(loo.get_n_splits(X, y), 4)\n    assert_equal(loo.get_n_splits(X), 4)\n    assert_equal(repr(loo), \"LeaveOneOut()\")\n", "def test_leave_one_group_out():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14]])\n    y = np.array([1, 2, 3, 4, 5, 6, 7])\n    groups = np.array([1, 1, 2, 2, 3, 3, 3])\n    \n    logo = LeaveOneGroupOut()\n    \n    expected_splits = [\n        (np.array([2, 3, 4, 5, 6]), np.array([0, 1])),\n        (np.array([0, 1, 4, 5, 6]), np.array([2, 3])),\n        (np.array([0, 1, 2, 3]), np.array([4, 5, 6]))\n    ]\n    \n    splits = list(logo.split(X, y, groups))\n    assert_equal(len(splits), 3)\n    \n    for i, (train, test) in enumerate(splits):\n        assert_array_equal(train, expected_splits[i][0])\n        assert_array_equal(test, expected_splits[i][1])\n    \n    # Test if get_n_splits works correctly\n    assert_equal(logo.get_n_splits(X, y, groups), 3)\n    \n    # Test if the repr works without any errors\n    assert_equal(repr(logo), 'LeaveOneGroupOut()')\n    \n    # Test that train, test indices returned are integers\n    for train, test in splits:\n        assert_equal(np.asarray(train).dtype.kind, 'i')\n        assert_equal(np.asarray(test).dtype.kind, 'i')\n", "def test_leave_p_out():\n    # Ensure LeavePOut splits data correctly\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([0, 1, 2, 3, 4])\n    lpo = LeavePOut(p=2)\n    expected_splits = [\n        (np.array([2, 3, 4]), np.array([0, 1])),\n        (np.array([1, 3, 4]), np.array([0, 2])),\n        (np.array([1, 2, 4]), np.array([0, 3])),\n        (np.array([1, 2, 3]), np.array([0, 4])),\n        (np.array([0, 3, 4]), np.array([1, 2])),\n        (np.array([0, 2, 4]), np.array([1, 3])),\n        (np.array([0, 2, 3]), np.array([1, 4])),\n        (np.array([0, 1, 4]), np.array([2, 3])),\n        (np.array([0, 1, 3]), np.array([2, 4])),\n        (np.array([0, 1, 2]), np.array([3, 4]))\n    ]\n    \n    for i, (train_index, test_index) in enumerate(lpo.split(X)):\n        np.testing.assert_array_equal(train_index, expected_splits[i][0])\n        np.testing.assert_array_equal(test_index, expected_splits[i][1])\n\n    # Check get_n_splits returns the correct number of splits\n    assert_equal(lpo.get_n_splits(X), len(expected_splits))\n\n    # Check that a ValueError is raised when p is greater than the number of samples\n    with pytest.raises(ValueError, match=\"The 'X' parameter should not be None.\"):\n        lpo.get_n_splits(None)\n"], "sample_786": ["def test_constant_feature():\n    # Test when a feature has constant value\n    X_constant = np.array([[5], [5], [5], [5]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        est.fit(X_constant)\n        assert len(w) == 1\n        assert issubclass(w[0].category, UserWarning)\n        assert \"Feature 0 is constant and will be replaced with 0.\" in str(w[0].message)\n    Xt = est.transform(X_constant)\n    assert_array_equal(Xt, np.zeros(X_constant.shape))\n", "def test_invalid_dtype():\n    X_str = [['a', 'b'], ['c', 'd'], ['e', 'f']]\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    assert_raises(ValueError, est.fit, X_str)\n\n    X_mixed = [[1, 'b'], [2, 'c'], [3, 'd']]\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    assert_raises(ValueError, est.fit, X_mixed)\n", "def test_fit_with_nan_values():\n    X_nan = [[np.nan, 1.5, -4, -1],\n             [-1, 2.5, -3, -0.5],\n             [0, 3.5, -2, 0.5],\n             [1, 4.5, -1, 2]]\n    \n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    assert_raises(ValueError, est.fit, X_nan)\n", "def test_fit_transform_with_constant_feature():\n    X_const = np.array([[1, 2, 3],\n                        [1, 2, 3],\n                        [1, 2, 3],\n                        [1, 2, 3]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    assert_warns_message(UserWarning,\n                         \"Feature 0 is constant and will be replaced \"\n                         \"with 0.\", est.fit, X_const)\n    Xt = est.transform(X_const)\n    expected = np.array([[0, 0, 0],\n                         [0, 0, 0],\n                         [0, 0, 0],\n                         [0, 0, 0]])\n    assert_array_equal(Xt, expected)\n", "def test_single_feature():\n    X = np.array([[0.1], [0.4], [0.7], [1.0]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    est.fit(X)\n    Xt = est.transform(X)\n    expected = np.array([[0], [0], [1], [2]])\n    assert_array_equal(Xt, expected)\n\n    # Test inverse transform\n    Xinv = est.inverse_transform(Xt)\n    expected_inv = np.array([[0.2], [0.2], [0.6], [0.9]])\n    assert np.allclose(Xinv, expected_inv, atol=0.1)\n"], "sample_787": ["def test_confusion_matrix_multiclass_with_labels():\n    y_true = [0, 1, 2, 2, 1]\n    y_pred = [0, 0, 2, 1, 1]\n\n    # Test confusion matrix with specified labels\n    labels = [0, 1, 2]\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    expected_cm = np.array([[1, 1, 0],\n                            [0, 1, 1],\n                            [0, 1, 1]])\n    assert_array_equal(cm, expected_cm)\n\n    # Test confusion matrix with different order of labels\n    labels = [2, 0, 1]\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    expected_cm = np.array([[1, 0, 1],\n                            [0, 1, 1],\n                            [1, 1, 0]])\n    assert_array_equal(cm, expected_cm)\n\n    # Test confusion matrix with a label not in y_true or y_pred\n    labels = [0, 1, 2, 3]\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    expected_cm = np.array([[1, 1, 0, 0],\n                            [0, 1, 1, 0],\n                            [0, 1, 1, 0],\n                            [0, 0, 0, 0]])\n    assert_array_equal(cm, expected_cm)\n", "def test_zero_one_loss():\n    y_true = [0, 1, 2, 3]\n    y_pred = [0, 2, 1, 3]\n    \n    # Case when normalize=True (default)\n    loss_normalized = zero_one_loss(y_true, y_pred)\n    assert_almost_equal(loss_normalized, 0.5)\n\n    # Case when normalize=False\n    loss_unnormalized = zero_one_loss(y_true, y_pred, normalize=False)\n    assert_equal(loss_unnormalized, 2)\n\n    # Case with sample_weight\n    sample_weight = [1, 2, 1, 1]\n    weighted_loss = zero_one_loss(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(weighted_loss, 0.4)\n\n    # Case with sample_weight and normalize=False\n    weighted_loss_unnormalized = zero_one_loss(y_true, y_pred, normalize=False, sample_weight=sample_weight)\n    assert_equal(weighted_loss_unnormalized, 2)\n", "def test_confusion_matrix_multiclass():\n    # Test confusion matrix - multi-class case\n    y_true = [1, 0, 1, 1, 2, 2, 0, 1, 2, 0]\n    y_pred = [1, 0, 0, 1, 2, 1, 0, 2, 2, 1]\n\n    cm = confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, [[2, 1, 0],\n                            [1, 2, 1],\n                            [0, 1, 2]])\n\n    # Test confusion matrix with different class orders\n    labels = [2, 1, 0]\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    assert_array_equal(cm, [[2, 1, 0],\n                            [1, 2, 1],\n                            [0, 1, 2]])\n\n    # Test confusion matrix with some classes missing in y_pred\n    labels = [0, 1, 2, 3]\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    assert_array_equal(cm, [[2, 1, 0, 0],\n                            [1, 2, 1, 0],\n                            [0, 1, 2, 0],\n                            [0, 0, 0, 0]])\n\n    # Test sample weight with confusion matrix\n    sample_weight = np.array([1, 2, 1, 1, 1, 3, 2, 1, 1, 1])\n    cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    assert_array_equal(cm, [[4, 2, 0],\n                            [1, 2, 1],\n                            [0, 1, 5]])\n", "def test_zero_one_loss_multiclass():\n    # Test zero-one loss for multiclass classification\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 2]\n\n    # Check zero-one loss for default normalize=True\n    loss = zero_one_loss(y_true, y_pred)\n    expected_loss = 1 - accuracy_score(y_true, y_pred)\n    assert_almost_equal(loss, expected_loss)\n\n    # Check zero-one loss for normalize=False\n    loss = zero_one_loss(y_true, y_pred, normalize=False)\n    expected_loss = len(y_true) - accuracy_score(y_true, y_pred, normalize=False)\n    assert_almost_equal(loss, expected_loss)\n\n    # Check zero-one loss for sample weights\n    sample_weight = [1, 1, 1, 1, 1, 1]\n    loss = zero_one_loss(y_true, y_pred, sample_weight=sample_weight)\n    expected_loss = 1 - accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(loss, expected_loss)\n\n    # Check zero-one loss for different sample weights\n    sample_weight = [0.5, 0.5, 1, 1, 1, 1]\n    loss = zero_one_loss(y_true, y_pred, sample_weight=sample_weight)\n    expected_loss = 1 - accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(loss, expected_loss)\n", "def test_multilabel_confusion_matrix_samplewise():\n    # Test multilabel confusion matrix with samplewise=True\n    y_true = np.array([[0, 1, 0], [1, 1, 0], [1, 0, 1]])\n    y_pred = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 1]])\n    cm = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    assert_array_equal(cm, [[[1, 1], [0, 1]],\n                            [[1, 1], [0, 1]],\n                            [[0, 1], [1, 1]]])\n\n    # Check with sample_weight\n    sample_weight = np.array([1, 2, 3])\n    cm_weighted = multilabel_confusion_matrix(y_true, y_pred, samplewise=True, sample_weight=sample_weight)\n    assert_array_equal(cm_weighted, [[[1, 1], [0, 1]],\n                                     [[2, 2], [0, 2]],\n                                     [[0, 3], [1, 2]]])\n\n    # Check with labels parameter\n    cm_labels = multilabel_confusion_matrix(y_true, y_pred, samplewise=True, labels=[1, 0])\n    assert_array_equal(cm_labels, [[[1, 0], [1, 1]],\n                                   [[1, 1], [1, 0]],\n                                   [[1, 1], [0, 1]]])\n"], "sample_788": ["def test_constant_feature_behavior():\n    X = np.array([[1, 2, 3],\n                  [1, 5, 6],\n                  [1, 8, 9]])\n    est = KBinsDiscretizer(strategy='uniform', n_bins=3, encode='ordinal')\n    msg = \"Feature 0 is constant and will be replaced with 0.\"\n    assert_warns_message(UserWarning, msg, est.fit, X)\n    assert_array_equal(est.transform(X)[:, 0], np.zeros(X.shape[0]))\n\n    # Test with different encoding\n    est = KBinsDiscretizer(strategy='uniform', n_bins=3, encode='onehot-dense')\n    assert_warns_message(UserWarning, msg, est.fit, X)\n    Xt = est.transform(X)\n    assert_array_equal(Xt[:, 0:2], np.zeros((X.shape[0], 2)))\n\n    est = KBinsDiscretizer(strategy='uniform', n_bins=3, encode='onehot')\n    assert_warns_message(UserWarning, msg, est.fit, X)\n    Xt_sparse = est.transform(X)\n    assert sp.issparse(Xt_sparse)\n    assert_array_equal(Xt_sparse.toarray()[:, 0:2], np.zeros((X.shape[0], 2)))\n", "def test_single_feature():\n    X_single_feature = np.array([[0], [1], [2], [3], [4]])\n    est = KBinsDiscretizer(n_bins=2, encode='ordinal', strategy='uniform')\n    Xt = est.fit_transform(X_single_feature)\n    assert_array_equal(Xt, np.array([[0], [0], [0], [1], [1]]))\n    Xinv = est.inverse_transform(Xt)\n    assert_array_almost_equal(Xinv, np.array([[0.], [1.], [2.], [3.], [4.]]))\n\n    est = KBinsDiscretizer(n_bins=3, encode='onehot-dense', strategy='kmeans')\n    Xt = est.fit_transform(X_single_feature)\n    assert Xt.shape == (5, 3)\n    Xinv = est.inverse_transform(Xt)\n    assert Xinv.shape == (5, 1)\n\n    est = KBinsDiscretizer(n_bins=4, encode='onehot', strategy='quantile')\n    Xt = est.fit_transform(X_single_feature)\n    assert Xt.shape == (5, 4)\n    Xinv = est.inverse_transform(Xt)\n    assert Xinv.shape == (5, 1)\n", "def test_transform_after_inverse_transform():\n    kbd = KBinsDiscretizer(n_bins=3, strategy='uniform', encode='ordinal')\n    Xt = kbd.fit_transform(X)\n    Xinv = kbd.inverse_transform(Xt)\n    Xt_retransformed = kbd.transform(Xinv)\n    assert_array_equal(Xt, Xt_retransformed)\n", "def test_transform_outside_bounds():\n    X_train = np.array([[0, 1], [1, 3], [2, 5], [3, 7]])\n    kbd = KBinsDiscretizer(n_bins=[3, 4], strategy='uniform', encode='ordinal')\n    kbd.fit(X_train)\n\n    # Test data with values outside the bounds of the training data\n    X_test = np.array([[-1, 0], [4, 8]])\n    Xt = kbd.transform(X_test)\n    expected_Xt = np.array([[0, 0], [2, 3]])\n    assert_array_equal(Xt, expected_Xt)\n", "def test_kmeans_bin_edges():\n    from sklearn.cluster import KMeans\n    \n    X = np.array([[1, 2, 6, 8],\n                  [1, 4, 7, 9],\n                  [1, 3, 5, 10]])\n    \n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')\n    est.fit(X)\n    \n    # Check if KMeans centers are correctly computed\n    for jj, column in enumerate(X.T):\n        km = KMeans(n_clusters=3, init='k-means++', n_init=1)\n        km.fit(column[:, None])\n        centers = km.cluster_centers_[:, 0]\n        centers.sort()\n        \n        # Compute expected bin edges\n        expected_bin_edges = (centers[1:] + centers[:-1]) * 0.5\n        expected_bin_edges = np.r_[column.min(), expected_bin_edges, column.max()]\n        \n        np.testing.assert_array_almost_equal(expected_bin_edges, est.bin_edges_[jj])\n\n    # Ensure transformation is consistent with bin edges\n    Xt = est.transform(X)\n    for jj in range(X.shape[1]):\n        expected_Xt = np.digitize(X[:, jj], est.bin_edges_[jj][1:-1], right=True)\n        np.testing.assert_array_equal(expected_Xt, Xt[:, jj])\n"], "sample_789": ["def test_adaboost_classifier_default_base_estimator():\n    # Check that AdaBoostClassifier uses DecisionTreeClassifier(max_depth=1)\n    # by default and performs correctly on a toy dataset.\n    clf = AdaBoostClassifier(random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n    assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)\n    assert_equal(clf.predict_proba(T).shape, (len(T), 2))\n    assert_equal(clf.decision_function(T).shape, (len(T),))\n    assert isinstance(clf.base_estimator_, DecisionTreeClassifier)\n    assert_equal(clf.base_estimator_.max_depth, 1)\n", "def test_adaboost_classifier_with_different_base_estimators():\n    \"\"\"\n    Test AdaBoostClassifier with different types of base estimators to ensure\n    it works as expected with different estimators.\n    \"\"\"\n\n    # Test with DecisionTreeClassifier with different parameters\n    base_estimator = DecisionTreeClassifier(max_depth=3)\n    clf = AdaBoostClassifier(base_estimator=base_estimator, random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n\n    # Test with SVC with probability=True to use SAMME.R algorithm\n    base_estimator = SVC(probability=True, gamma='scale')\n    clf = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME.R', random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n\n    # Test with Logistic Regression\n    from sklearn.linear_model import LogisticRegression\n    base_estimator = LogisticRegression()\n    clf = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME.R', random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n\n    # Test with KNeighborsClassifier\n    from sklearn.neighbors import KNeighborsClassifier\n    base_estimator = KNeighborsClassifier()\n    clf = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME', random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n", "def test_adaboost_classifier_feature_importances():\n    # Test feature importances for AdaBoostClassifier\n\n    X, y = datasets.make_classification(n_samples=100, n_features=20, \n                                        n_informative=5, n_redundant=0, \n                                        n_repeated=0, random_state=0)\n\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg, random_state=0)\n        clf.fit(X, y)\n\n        # Ensure that the sum of feature importances is 1.0\n        feature_importances = clf.feature_importances_\n        assert_equal(len(feature_importances), X.shape[1])\n        assert_array_almost_equal(np.sum(feature_importances), 1.0)\n\n        # Check if the most important features have non-zero importances\n        important_features = feature_importances[:5]\n        assert_greater(np.sum(important_features), 0.0)\n\n        # Ensure that the importances are greater for the first 5 features\n        # since they are the informative ones\n        unimportant_features = feature_importances[5:]\n        assert_greater(np.min(important_features), np.max(unimportant_features))\n", "def test_adaboost_classifier_staged_score():\n    # Test the staged_score method of AdaBoostClassifier on the iris dataset.\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg, random_state=0)\n        clf.fit(iris.data, iris.target)\n\n        staged_scores = list(clf.staged_score(iris.data, iris.target))\n        final_score = clf.score(iris.data, iris.target)\n\n        assert_equal(len(staged_scores), clf.n_estimators)\n        assert_array_almost_equal(staged_scores[-1], final_score)\n", "def test_early_stopping():\n    # Check that early stopping works as expected\n    X, y = datasets.make_classification(n_samples=100, n_features=20, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n    clf.fit(X_train, y_train)\n    score_early_stop = clf.score(X_test, y_test)\n\n    clf = AdaBoostClassifier(n_estimators=300, random_state=42)\n    clf.fit(X_train, y_train)\n    score_no_stop = clf.score(X_test, y_test)\n\n    assert score_early_stop == score_no_stop, \"Scores should be the same due to early stopping\"\n\n    reg = AdaBoostRegressor(n_estimators=100, random_state=42)\n    reg.fit(X_train, y_train)\n    score_early_stop = reg.score(X_test, y_test)\n\n    reg = AdaBoostRegressor(n_estimators=300, random_state=42)\n    reg.fit(X_train, y_train)\n    score_no_stop = reg.score(X_test, y_test)\n\n    assert score_early_stop == score_no_stop, \"Scores should be the same due to early stopping\"\n"], "sample_790": ["def test_kernel_pca_fit_transform_with_params():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n\n    # Test with different kernel parameters\n    kpca = KernelPCA(n_components=3, kernel='poly', gamma=0.5, degree=3, coef0=1)\n    X_transformed = kpca.fit_transform(X_fit)\n    assert_equal(X_transformed.shape, (5, 3))\n\n    kpca = KernelPCA(n_components=2, kernel='rbf', gamma=0.1)\n    X_transformed = kpca.fit_transform(X_fit)\n    assert_equal(X_transformed.shape, (5, 2))\n\n    kpca = KernelPCA(n_components=4, kernel='sigmoid', gamma=0.2, coef0=0)\n    X_transformed = kpca.fit_transform(X_fit)\n    assert_equal(X_transformed.shape, (5, 4))\n", "def test_kernel_pca_invalid_gamma():\n    # Test to check if an error is raised for invalid gamma value\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    \n    kpca = KernelPCA(kernel=\"rbf\", gamma=-1)  # invalid gamma value\n    with pytest.raises(ValueError, match=\"The 'gamma' parameter of KernelPCA must be a float in the range\"):\n        kpca.fit(X_fit)\n", "def test_kernel_pca_rbf_kernel():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    # Check if KernelPCA with rbf kernel produces consistent results\n    gamma = 15\n    kpca = KernelPCA(kernel=\"rbf\", gamma=gamma)\n    K_fit = rbf_kernel(X_fit, gamma=gamma)\n    K_pred = rbf_kernel(X_pred, X_fit, gamma=gamma)\n    \n    X_kpca = kpca.fit(X_fit).transform(X_pred)\n    kpca_precomputed = KernelPCA(kernel=\"precomputed\")\n    X_kpca_precomputed = kpca_precomputed.fit(K_fit).transform(K_pred)\n    \n    assert_array_almost_equal(np.abs(X_kpca), np.abs(X_kpca_precomputed))\n", "def test_kernel_pca_invalid_eigen_solver():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    invalid_solver = \"invalid_solver\"\n    kpca = KernelPCA(eigen_solver=invalid_solver)\n    with pytest.raises(ValueError, match=\"Unknown eigen_solver\"):\n        kpca.fit(X_fit)\n", "def test_kernel_pca_with_custom_callable_kernel():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n        if Y is None:\n            Y = X\n        return np.dot(X, Y.T)\n\n    kpca = KernelPCA(n_components=3, kernel=custom_kernel, fit_inverse_transform=True)\n    X_fit_transformed = kpca.fit_transform(X_fit)\n    X_pred_transformed = kpca.transform(X_pred)\n    X_pred_inverse_transformed = kpca.inverse_transform(X_pred_transformed)\n\n    assert_equal(X_fit_transformed.shape, (5, 3))\n    assert_equal(X_pred_transformed.shape, (2, 3))\n    assert_equal(X_pred_inverse_transformed.shape, (2, 4))\n    assert_array_almost_equal(kpca.inverse_transform(kpca.transform(X_fit)), X_fit)\n"], "sample_791": ["def test_ordinal_encoder_invalid_params():\n    enc = OrdinalEncoder(categories='auto')\n    assert_raises_regex(\n        ValueError,\n        \"Shape mismatch: if n_values is an array, it has to be of shape\",\n        enc.fit, [['Male', 'Tall'], ['Female', 'Short']])\n\n    enc = OrdinalEncoder(categories=[['Male', 'Female'], ['Tall']])\n    assert_raises_regex(\n        ValueError,\n        \"Shape mismatch: if n_values is an array, it has to be of shape\",\n        enc.fit, [['Male', 'Tall'], ['Female', 'Short']])\n\n    enc = OrdinalEncoder(categories=[['Male', 'Female']])\n    assert_raises_regex(\n        ValueError,\n        \"Shape mismatch: if n_values is an array, it has to be of shape\",\n        enc.fit, [['Male', 'Tall'], ['Female', 'Short']])\n", "def test_ordinal_encoder_fit():\n    # Check that fit method of OrdinalEncoder works correctly\n    X = np.array([['Male', 'a'], ['Female', 'b'], ['Male', 'a']], dtype=object)\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    \n    expected_categories = [np.array(['Female', 'Male'], dtype=object), \n                           np.array(['a', 'b'], dtype=object)]\n    for result, expected in zip(enc.categories_, expected_categories):\n        assert_array_equal(result, expected)\n\n    # Check that transform method of OrdinalEncoder works correctly after fit\n    X_trans = enc.transform(X)\n    expected_trans = np.array([[1., 0.], [0., 1.], [1., 0.]])\n    assert_array_equal(X_trans, expected_trans)\n\n    # Check that inverse_transform method of OrdinalEncoder works correctly after fit\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X_inv, X)\n", "def test_ordinal_encoder_handle_unknown():\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    X2 = [['xyz', 2, 55], ['ghi', 1, 55]]\n\n    enc = OrdinalEncoder(handle_unknown='error')\n    enc.fit(X)\n    assert_raises(ValueError, enc.transform, X2)\n\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X2_trans = enc.transform(X2)\n    assert_array_equal(X2_trans, np.full(X2_trans.shape, np.nan))\n", "def test_ordinal_encoder_specified_categories_mixed_columns():\n    # multiple columns\n    X = np.array([['a', 'b'], [0, 2], [10, 20]], dtype=object).T\n    enc = OrdinalEncoder(categories=[['a', 'b', 'c'], [0, 1, 2], [10, 20, 30]])\n    exp = np.array([[0., 0., 0.],\n                    [1., 2., 1.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['a', 'b', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n    assert enc.categories_[1].tolist() == [0, 1, 2]\n    assert np.issubdtype(enc.categories_[1].dtype, np.integer)\n    assert enc.categories_[2].tolist() == [10, 20, 30]\n    assert np.issubdtype(enc.categories_[2].dtype, np.integer)\n\n    # when specifying categories manually, unknown categories should raise error\n    X2 = np.array([['a', 'd'], [0, 3], [10, 40]], dtype=object).T\n    enc = OrdinalEncoder(categories=[['a', 'b', 'c'], [0, 1, 2], [10, 20, 30]])\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit(X2)\n", "def test_ordinal_encoder_unknown_category():\n    # Test for unknown category handling in OrdinalEncoder\n    X = [['cat', 1], ['dog', 2]]\n    X_test = [['cat', 1], ['dog', 2], ['fish', 3]]\n    \n    enc = OrdinalEncoder(categories='auto')\n    enc.fit(X)\n    \n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_test)\n    \n    enc = OrdinalEncoder(categories='auto', dtype='int64')\n    enc.fit(X)\n    \n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_test)\n    \n    enc = OrdinalEncoder(categories='auto', dtype='int64')\n    enc.fit(X)\n    X_transformed = enc.transform(X)\n    expected = np.array([[0, 0], [1, 1]], dtype='int64')\n    assert_array_equal(X_transformed, expected)\n"], "sample_792": ["def test_gnb_var_smoothing():\n    \"\"\"Test the effect of var_smoothing parameter in GaussianNB.\"\"\"\n    clf = GaussianNB(var_smoothing=1e-1)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n\n    clf = GaussianNB(var_smoothing=1e-5)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n", "def test_cnb_partial_fit():\n    # Test the partial fit method for ComplementNB\n    clf = ComplementNB().partial_fit(X2[:3], y2[:3], classes=np.unique(y2))\n    clf.partial_fit(X2[3:], y2[3:])\n    \n    y_pred = clf.predict(X2)\n    assert_array_equal(y_pred, y2)\n    \n    y_pred_proba = clf.predict_proba(X2)\n    y_pred_log_proba = clf.predict_log_proba(X2)\n    assert_array_almost_equal(np.log(y_pred_proba), y_pred_log_proba, 8)\n\n    clf2 = ComplementNB()\n    clf2.partial_fit(X2[:2], y2[:2], classes=np.unique(y2))\n    clf2.partial_fit(X2[2:4], y2[2:4])\n    clf2.partial_fit(X2[4:], y2[4:])\n    \n    assert_array_equal(clf.predict(X2), clf2.predict(X2))\n    assert_array_almost_equal(clf.predict_proba(X2), clf2.predict_proba(X2))\n    assert_array_almost_equal(clf.predict_log_proba(X2), clf2.predict_log_proba(X2))\n", "def test_gnb_partial_fit_different_calls():\n    \"\"\"Test GaussianNB partial_fit with different calls and class orders.\"\"\"\n    clf = GaussianNB()\n    # First call to partial_fit with class order [1, 2]\n    clf.partial_fit(X, y, classes=[1, 2])\n    assert_array_almost_equal(clf.class_prior_, np.array([0.5, 0.5]))\n    # Second call to partial_fit with same class order\n    clf.partial_fit(X, y)\n    assert_array_almost_equal(clf.class_prior_, np.array([0.5, 0.5]))\n\n    # Create new classifier and call partial_fit with different class order\n    clf_new = GaussianNB()\n    clf_new.partial_fit(X, y, classes=[2, 1])\n    assert_array_almost_equal(clf_new.class_prior_, np.array([0.5, 0.5]))\n\n    # Verify that predictions remain consistent\n    assert_array_equal(clf.predict(X), clf_new.predict(X))\n", "def test_gnb_incremental_fit():\n    # Test that incremental fitting converges to the same result as batch fitting\n\n    X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.5, random_state=0)\n\n    clf_batch = GaussianNB().fit(X_train, y_train)\n    y_pred_batch = clf_batch.predict(X_test)\n\n    clf_incremental = GaussianNB()\n    clf_incremental.partial_fit(X_train[:5], y_train[:5], classes=np.unique(y_train))\n    clf_incremental.partial_fit(X_train[5:], y_train[5:])\n    y_pred_incremental = clf_incremental.predict(X_test)\n\n    assert_array_equal(y_pred_batch, y_pred_incremental)\n    assert_array_almost_equal(clf_batch.theta_, clf_incremental.theta_)\n    assert_array_almost_equal(clf_batch.sigma_, clf_incremental.sigma_)\n    assert_array_almost_equal(clf_batch.class_prior_, clf_incremental.class_prior_)\n", "def test_gnb_var_smoothing():\n    \"\"\"Test the effect of the var_smoothing parameter on GaussianNB\"\"\"\n    # Create a simple dataset\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n    y = np.array([0, 0, 1, 1, 1])\n\n    # Fit GaussianNB with default var_smoothing\n    clf_default = GaussianNB().fit(X, y)\n    # Fit GaussianNB with a larger var_smoothing\n    clf_large_smoothing = GaussianNB(var_smoothing=1e-2).fit(X, y)\n    \n    # Ensure that the class prior probabilities are the same\n    assert_array_almost_equal(clf_default.class_prior_, clf_large_smoothing.class_prior_)\n    \n    # Check if the variance with larger var_smoothing is indeed larger\n    assert np.all(clf_large_smoothing.sigma_ >= clf_default.sigma_)\n    \n    # Predict probabilities with both models and check if they are different\n    y_pred_proba_default = clf_default.predict_proba(X)\n    y_pred_proba_large_smoothing = clf_large_smoothing.predict_proba(X)\n    assert not np.allclose(y_pred_proba_default, y_pred_proba_large_smoothing)\n"], "sample_793": ["def test_iforest_random_state():\n    \"\"\"Check Isolation Forest random_state parameter.\"\"\"\n    X = iris.data\n\n    # Check that random_state produces consistent results\n    clf1 = IsolationForest(random_state=42).fit(X)\n    clf2 = IsolationForest(random_state=42).fit(X)\n    assert_array_equal(clf1.predict(X), clf2.predict(X))\n\n    # Check that different random_states produce different results\n    clf3 = IsolationForest(random_state=43).fit(X)\n    assert_raises(AssertionError, assert_array_equal, clf1.predict(X), clf3.predict(X))\n\n    # Check that random_state=None produces different results\n    clf4 = IsolationForest(random_state=None).fit(X)\n    assert_raises(AssertionError, assert_array_equal, clf1.predict(X), clf4.predict(X))\n", "def test_iforest_random_state():\n    \"\"\"Test if random_state produces consistent results.\"\"\"\n    X = iris.data\n\n    # Test if results are consistent when random_state is set\n    clf1 = IsolationForest(random_state=42).fit(X)\n    clf2 = IsolationForest(random_state=42).fit(X)\n\n    assert_array_equal(clf1.predict(X), clf2.predict(X))\n\n    # Test if results are different when random_state is different\n    clf3 = IsolationForest(random_state=43).fit(X)\n    assert np.any(clf1.predict(X) != clf3.predict(X))\n", "def test_predict_inliers_outliers():\n    \"\"\"Test predict method for clear inliers and outliers.\"\"\"\n    X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    X_inliers = np.array([[0.1, 0.1], [0.2, 0.2]])\n    X_outliers = np.array([[5, 5], [6, 6]])\n\n    clf = IsolationForest(contamination=0.1, random_state=rng).fit(X_train)\n    inlier_preds = clf.predict(X_inliers)\n    outlier_preds = clf.predict(X_outliers)\n\n    assert_array_equal(inlier_preds, [1, 1])\n    assert_array_equal(outlier_preds, [-1, -1])\n", "def test_fit_predict():\n    \"\"\"Test fit and predict methods of IsolationForest.\"\"\"\n    X_train = [[-1.1], [0.3], [0.5], [100]]\n    X_test = [[0.1], [0], [90]]\n    y_train = [1, 1, 1, -1]  # Only the last point is an outlier\n\n    # Fit the model\n    clf = IsolationForest(contamination=0.25, random_state=rng)\n    clf.fit(X_train)\n\n    # Predict on the training data\n    y_pred_train = clf.predict(X_train)\n    assert_array_equal(y_pred_train, y_train)\n\n    # Predict on the testing data\n    y_pred_test = clf.predict(X_test)\n    assert_array_equal(y_pred_test, [1, 1, -1])  # Last point should be an outlier\n", "def test_fit_method_with_sample_weight():\n    \"\"\"Test the fit method when sample_weight is provided.\"\"\"\n    rng = check_random_state(42)\n    X_train = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    sample_weight = rng.rand(len(X_train))\n    \n    clf = IsolationForest().fit(X_train, sample_weight=sample_weight)\n    assert hasattr(clf, 'estimators_')\n    assert hasattr(clf, 'estimators_samples_')\n    assert hasattr(clf, 'max_samples_')\n    assert_equal(len(clf.estimators_), clf.n_estimators)\n    assert_equal(len(clf.estimators_samples_), clf.n_estimators)\n"], "sample_794": ["def test_ridge_intercept_with_sparse_data():\n    # Test intercept handling with sparse data.\n    X, y = make_regression(n_samples=200, n_features=5, noise=0.1)\n    X_sparse = sp.csr_matrix(X)\n    y_sparse = y\n\n    for solver in ['sag', 'sparse_cg']:\n        ridge_dense = Ridge(alpha=1.0, fit_intercept=True, solver=solver)\n        ridge_sparse = Ridge(alpha=1.0, fit_intercept=True, solver=solver)\n\n        ridge_dense.fit(X, y)\n        ridge_sparse.fit(X_sparse, y_sparse)\n\n        assert_almost_equal(ridge_dense.intercept_, ridge_sparse.intercept_, decimal=5)\n        assert_array_almost_equal(ridge_dense.coef_, ridge_sparse.coef_, decimal=5)\n\n    # Test with return_intercept=True\n    ridge_sparse_cg = Ridge(alpha=1.0, fit_intercept=True, solver='sparse_cg')\n    coef, intercept = ridge_regression(X_sparse, y_sparse, alpha=1.0, solver='sparse_cg', return_intercept=True)\n\n    ridge_sparse_cg.fit(X_sparse, y_sparse)\n    assert_array_almost_equal(ridge_sparse_cg.coef_, coef, decimal=5)\n    assert_almost_equal(ridge_sparse_cg.intercept_, intercept, decimal=5)\n", "def test_ridge_solvers_consistency():\n    # Test that different solvers give similar results.\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 100, 10\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    alphas = [0.1, 1.0, 10.0]\n    solvers = ['svd', 'cholesky', 'sparse_cg', 'lsqr', 'sag', 'saga']\n\n    results = {}\n    for solver in solvers:\n        ridge = Ridge(alpha=1.0, solver=solver, random_state=42)\n        ridge.fit(X, y)\n        results[solver] = ridge.coef_\n\n    for solver1, solver2 in product(solvers, repeat=2):\n        assert_allclose(results[solver1], results[solver2], rtol=1e-5, atol=1e-5,\n                        err_msg=f\"Solvers {solver1} and {solver2} give different results.\")\n", "def test_ridge_regression_with_regularization():\n    # Test the ridge regression with different regularization strengths\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 10, 8\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    \n    for alpha in [0.1, 1.0, 10.0, 100.0]:\n        ridge = Ridge(alpha=alpha, solver=\"cholesky\")\n        ridge.fit(X, y)\n        \n        # Check that coefficients are non-zero (due to regularization)\n        assert np.any(ridge.coef_ != 0), \"Coefficients should not be zero with regularization\"\n        \n        # Check that the intercept is non-zero if fit_intercept is True\n        if ridge.fit_intercept:\n            assert ridge.intercept_ != 0, \"Intercept should not be zero with fit_intercept=True\"\n\n        # Check that the Ridge regression with regularization has a lower score than without\n        ridge_no_reg = Ridge(alpha=0.0, solver=\"cholesky\")\n        ridge_no_reg.fit(X, y)\n        assert ridge.score(X, y) <= ridge_no_reg.score(X, y), \"Regularization should not improve the score\"\n", "def test_ridge_regression_multiple_targets():\n    # Test Ridge regression with multiple target variables\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_targets = 10, 5, 3\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    alpha = 1.0\n\n    for solver in [\"svd\", \"sparse_cg\", \"cholesky\", \"lsqr\", \"sag\", \"saga\"]:\n        ridge = Ridge(alpha=alpha, solver=solver)\n        ridge.fit(X, y)\n        assert_equal(ridge.coef_.shape, (n_targets, n_features))\n        y_pred = ridge.predict(X)\n        assert_equal(y_pred.shape, (n_samples, n_targets))\n        score = ridge.score(X, y)\n        assert_greater(score, 0.5)\n", "def test_ridge_solver_auto():\n    # Test 'auto' solver selection and its consistency with manual solver selection.\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 10, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    alpha = 1.0\n\n    # Test with more samples than features, expect cholesky to be chosen\n    ridge_auto = Ridge(alpha=alpha, solver='auto')\n    ridge_cholesky = Ridge(alpha=alpha, solver='cholesky')\n    ridge_auto.fit(X, y)\n    ridge_cholesky.fit(X, y)\n    assert_array_almost_equal(ridge_auto.coef_, ridge_cholesky.coef_)\n\n    # Test with more features than samples, expect svd to be chosen\n    X = rng.randn(n_features, n_samples)\n    y = rng.randn(n_features)\n    ridge_auto.fit(X, y)\n    ridge_svd = Ridge(alpha=alpha, solver='svd')\n    ridge_svd.fit(X, y)\n    assert_array_almost_equal(ridge_auto.coef_, ridge_svd.coef_)\n\n    # Test with sparse input, expect sparse_cg to be chosen\n    X_sparse = sp.csr_matrix(X)\n    ridge_auto.fit(X_sparse, y)\n    ridge_sparse_cg = Ridge(alpha=alpha, solver='sparse_cg')\n    ridge_sparse_cg.fit(X_sparse, y)\n    assert_array_almost_equal(ridge_auto.coef_, ridge_sparse_cg.coef_)\n"], "sample_795": ["def test_check_sample_weights_invariance():\n    class SampleWeightInvarianceClassifier(BaseEstimator, ClassifierMixin):\n            self.classes_ = np.unique(y)\n            self.mean_ = np.average(X, weights=sample_weight, axis=0)\n            return self\n\n            return np.full(X.shape[0], self.classes_[0])\n\n    check_sample_weights_invariance(\"SampleWeightInvarianceClassifier\", SampleWeightInvarianceClassifier())\n", "def test_check_estimators_sparse_data():\n    # check that the estimator can handle sparse data formats\n    from sklearn.linear_model import Ridge\n\n    # Create a sparse matrix\n    rng = np.random.RandomState(0)\n    X = rng.rand(40, 10)\n    X[X < .8] = 0\n    X_csr = sp.csr_matrix(X)\n    y = (4 * rng.rand(40)).astype(np.int)\n\n    # Use Ridge estimator for testing\n    estimator = Ridge()\n    \n    try:\n        check_estimator(estimator)\n    except Exception as e:\n        if 'sparse' not in repr(e).lower():\n            raise AssertionError(\"Estimator doesn't seem to fail gracefully on sparse data: \"\n                                 \"error message should explicitly state that sparse input \"\n                                 \"is not supported if this is not the case.\")\n", "def test_check_class_weight_balanced_classifiers():\n    # Tests that check_class_weight_balanced_classifiers works on a class with\n    # `class_weight` parameter set to 'balanced'\n\n    # Generate a dataset\n    X, y = make_blobs(n_samples=100, centers=2, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n\n    # Define class weights\n    weights = {0: 0.1, 1: 0.9}\n\n    # Test on various classifiers\n    classifiers = [SGDClassifier, RandomForestClassifier, SVC]\n\n    for clf in classifiers:\n        check_class_weight_balanced_classifiers(clf.__name__, clf, X_train, y_train, X_test, y_test, weights)\n", "def test_check_class_weight_balanced_classifiers():\n    # check class_weight=\"balanced\" improves f1 score\n    from sklearn.datasets import make_classification\n\n    X, y = make_classification(n_classes=3, class_sep=2, weights=[0.1, 0.2, 0.7],\n                               n_informative=3, n_redundant=0, flip_y=0,\n                               n_features=20, n_clusters_per_class=1,\n                               n_samples=1000, random_state=10)\n    \n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n                                                        random_state=42)\n\n    # Test with a classifier that supports class_weight\n    clf = RandomForestClassifier()\n    weights = {0: 1.0, 1: 1.0, 2: 1.0}\n    check_class_weight_balanced_classifiers(clf.__class__.__name__, clf, \n                                            X_train, y_train, \n                                            X_test, y_test, weights)\n", "def test_check_estimators_partial_fit_n_features():\n    # check that partial_fit raises an error when the number of features changes\n    msg = \"The estimator test does not raise an error when the number of features changes between calls to partial_fit.\"\n    from sklearn.linear_model import PassiveAggressiveClassifier\n\n    assert_raises_regex(ValueError, msg, check_estimators_partial_fit_n_features, \"PassiveAggressiveClassifier\", PassiveAggressiveClassifier())\n\n    from sklearn.naive_bayes import MultinomialNB\n    assert_raises_regex(ValueError, msg, check_estimators_partial_fit_n_features, \"MultinomialNB\", MultinomialNB())\n"], "sample_796": ["def test_huber_invalid_epsilon():\n    # Test that HuberRegressor raises an error for invalid epsilon values\n    X, y = make_regression_with_outliers()\n    with pytest.raises(ValueError, match=\"epsilon should be greater than or equal to 1.0\"):\n        huber = HuberRegressor(epsilon=0.5)\n        huber.fit(X, y)\n", "def test_huber_fit_intercept_false():\n    # Test HuberRegressor with fit_intercept set to False\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=False, epsilon=1.35, alpha=0.01)\n    huber.fit(X, y)\n\n    # Check that intercept is set to 0\n    assert huber.intercept_ == 0.0\n\n    # Fit a LinearRegression model for comparison\n    lr = LinearRegression(fit_intercept=False)\n    lr.fit(X, y)\n\n    # Compare coefficients\n    assert_almost_equal(huber.coef_, lr.coef_, 3)\n", "def test_huber_intercept_only():\n    # Test HuberRegressor with fit_intercept=True and no features (only intercept)\n    X = np.ones((10, 0))  # no feature, only intercept\n    y = np.arange(10)\n    huber = HuberRegressor(fit_intercept=True, alpha=0.01, max_iter=100)\n    huber.fit(X, y)\n    assert huber.coef_.shape[0] == 0  # no coefficients\n    assert_almost_equal(huber.intercept_, 4.5)  # mean of y\n", "def test_huber_fit_intercept_false():\n    # Test HuberRegressor with fit_intercept=False\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=False, alpha=0.01, max_iter=100)\n    huber.fit(X, y)\n    assert huber.intercept_ == 0.0\n    assert huber.coef_.shape == (X.shape[1],)\n\n    # Ensure that the model without intercept fits reasonably well\n    linear_loss = np.dot(X, huber.coef_) - y\n    mask = np.abs(linear_loss) < huber.epsilon * huber.scale_\n    huber_score = huber.score(X[mask], y[mask])\n    assert huber_score > 0.5  # Expect a reasonable R^2 score\n", "def test_huber_convergence_warning():\n    X, y = make_regression_with_outliers()\n    # Test that the model raises a convergence warning when it does not converge\n    with pytest.raises(ValueError, match=\"HuberRegressor convergence failed\"):\n        huber = HuberRegressor(max_iter=1, tol=1e-20)  # Use extreme tol to force non-convergence\n        huber.fit(X, y)\n"], "sample_797": ["def test_handle_zeros_in_scale_scalar():\n    # Check the function's behavior with scalar input\n    scale = 0.0\n    result = _handle_zeros_in_scale(scale)\n    assert result == 1.0\n\n    scale = 5.0\n    result = _handle_zeros_in_scale(scale)\n    assert result == 5.0\n", "def test_handle_zeros_in_scale_scalar():\n    # Test handling of scalar zero in _handle_zeros_in_scale\n    zero_scalar = 0.0\n    non_zero_scalar = 5.0\n\n    result = _handle_zeros_in_scale(zero_scalar)\n    assert_equal(result, 1.0)\n\n    result_non_zero = _handle_zeros_in_scale(non_zero_scalar)\n    assert_equal(result_non_zero, non_zero_scalar)\n", "def test_handle_zeros_in_scale_scalar():\n    # Test _handle_zeros_in_scale for scalar input\n    scalar = 0.0\n    result = _handle_zeros_in_scale(scalar)\n    assert result == 1.0\n\n    scalar = 5.0\n    result = _handle_zeros_in_scale(scalar)\n    assert result == 5.0\n", "def test_standard_scaler_with_nan():\n    # Test StandardScaler with NaN values\n    X = np.array([[np.nan, 1, 2], [3, 4, np.nan], [1, np.nan, 5]])\n    scaler = StandardScaler()\n\n    # Check if NaN values do not affect the computation of mean and std\n    X_scaled = scaler.fit_transform(X)\n    assert_array_almost_equal(scaler.mean_, [2, 2.5, 3.5])\n    assert_array_almost_equal(scaler.scale_, [1, 1.5, 1.5])\n\n    # Check if NaN values are ignored during the transformation\n    assert_array_almost_equal(X_scaled, [[np.nan, -1, -1], [1, 1, np.nan], [-1, np.nan, 1]])\n\n    # Check inverse transformation\n    X_scaled_back = scaler.inverse_transform(X_scaled)\n    assert_array_almost_equal(X_scaled_back, [[np.nan, 1, 2], [3, 4, np.nan], [1, np.nan, 5]])\n\n    # Check if NaN values are preserved during partial_fit\n    scaler = StandardScaler()\n    for batch in [X[:2], X[2:]]:\n        scaler.partial_fit(batch)\n    X_scaled = scaler.transform(X)\n    assert_array_almost_equal(scaler.mean_, [2, 2.5, 3.5])\n    assert_array_almost_equal(scaler.scale_, [1, 1.5, 1.5])\n    assert_array_almost_equal(X_scaled, [[np.nan, -1, -1], [1, 1, np.nan], [-1, np.nan, 1]])\n", "def test_kernel_centerer_sparse_input():\n    # Test KernelCenterer with sparse input\n    rng = np.random.RandomState(0)\n    X_dense = rng.random_sample((5, 4))\n    X_sparse = sparse.csr_matrix(X_dense)\n    \n    scaler = StandardScaler(with_std=False)\n    scaler.fit(X_dense)\n    X_dense_centered = scaler.transform(X_dense)\n    K_dense = np.dot(X_dense, X_dense.T)\n    \n    centerer = KernelCenterer()\n    K_dense_centered = centerer.fit_transform(K_dense)\n    \n    K_sparse = X_sparse * X_sparse.T\n    K_sparse_centered = centerer.fit_transform(K_sparse)\n    \n    assert isinstance(K_sparse_centered, sparse.csr_matrix)\n    assert_allclose_dense_sparse(K_dense_centered, K_sparse_centered)\n    \n    X_pred_dense = rng.random_sample((2, 4))\n    K_pred_dense = np.dot(X_pred_dense, X_dense.T)\n    X_pred_sparse = sparse.csr_matrix(X_pred_dense)\n    K_pred_sparse = X_pred_sparse * X_sparse.T\n    \n    K_pred_dense_centered = centerer.transform(K_pred_dense)\n    K_pred_sparse_centered = centerer.transform(K_pred_sparse)\n    \n    assert isinstance(K_pred_sparse_centered, sparse.csr_matrix)\n    assert_allclose_dense_sparse(K_pred_dense_centered, K_pred_sparse_centered)\n"], "sample_798": ["def test_ridge_regression_invalid_alpha():\n    # Test if invalid alpha values raise appropriate errors\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    \n    # Test for negative alpha\n    assert_raises(ValueError, ridge_regression, X, y, alpha=-0.1)\n\n    # Test for non-numeric alpha\n    assert_raises(TypeError, ridge_regression, X, y, alpha='invalid_alpha')\n\n    # Test for complex number alpha\n    assert_raises(TypeError, ridge_regression, X, y, alpha=complex(1, 1))\n", "def test_ridge_regression_solver_auto():\n    # Test that the 'auto' solver chooses the appropriate solver\n    # based on the input data characteristics\n\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    # Test with more samples than features (dense matrix)\n    n_samples, n_features = 10, 5\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n    ridge_auto = Ridge(alpha=alpha, solver='auto')\n    ridge_auto.fit(X, y)\n    assert_greater(ridge_auto.score(X, y), 0.5)\n\n    # Test with more features than samples (dense matrix)\n    n_samples, n_features = 5, 10\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n    ridge_auto.fit(X, y)\n    assert_greater(ridge_auto.score(X, y), 0.5)\n\n    # Test with sparse matrix\n    X_sparse = sp.csr_matrix(rng.randn(n_samples, n_features))\n    y = rng.randn(n_samples)\n    ridge_auto.fit(X_sparse, y)\n    assert_greater(ridge_auto.score(X_sparse, y), 0.5)\n\n    # Check that 'auto' solver chooses 'cholesky' for dense data\n    if not sparse.issparse(X):\n        assert ridge_auto.solver == 'cholesky'\n\n    # Check that 'auto' solver chooses 'sparse_cg' for sparse data\n    if sparse.issparse(X_sparse):\n        assert ridge_auto.solver == 'sparse_cg'\n", "def test_ridge_solver_switch():\n    X = np.array([[1, 0], [0, 1]])\n    y = np.array([1, 2])\n\n    # Test that 'auto' solver correctly switches to 'sag' with return_intercept\n    ridge = Ridge(solver='auto', return_intercept=True)\n    ridge.fit(X, y)\n    assert_equal(ridge.solver, 'sag')\n\n    # Test that 'auto' solver correctly switches to 'cholesky' for dense data\n    ridge = Ridge(solver='auto')\n    ridge.fit(X, y)\n    assert_equal(ridge.solver, 'cholesky')\n\n    # Test that 'auto' solver correctly switches to 'sparse_cg' for sparse data\n    X_sparse = sp.csr_matrix(X)\n    ridge = Ridge(solver='auto')\n    ridge.fit(X_sparse, y)\n    assert_equal(ridge.solver, 'sparse_cg')\n", "def test_ridge_intercept_no_intercept():\n    # Test Ridge without intercept calculation\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 10, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    ridge = Ridge(fit_intercept=False)\n    ridge.fit(X, y)\n    intercept = ridge.intercept_\n    assert_equal(intercept, 0.0)\n\n    # Test Ridge with explicit intercept\n    ridge = Ridge(fit_intercept=True)\n    ridge.fit(X, y)\n    intercept = ridge.intercept_\n    assert_not_equal(intercept, 0.0)\n", "def test_ridge_intercept_handling():\n    # Check if Ridge correctly handles intercept fitting with and without normalization\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 100, 10\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples) + 10  # Adding a non-zero intercept\n\n    # Case 1: fit_intercept=True, normalize=False\n    ridge = Ridge(fit_intercept=True, normalize=False)\n    ridge.fit(X, y)\n    assert_almost_equal(ridge.intercept_, 10, decimal=1)\n\n    # Case 2: fit_intercept=True, normalize=True\n    ridge = Ridge(fit_intercept=True, normalize=True)\n    ridge.fit(X, y)\n    assert_almost_equal(ridge.intercept_, 10, decimal=1)\n\n    # Case 3: fit_intercept=False, normalize=False\n    ridge = Ridge(fit_intercept=False, normalize=False)\n    ridge.fit(X, y)\n    assert_almost_equal(ridge.intercept_, 0, decimal=1)\n\n    # Case 4: fit_intercept=False, normalize=True (normalize should have no effect)\n    ridge = Ridge(fit_intercept=False, normalize=True)\n    ridge.fit(X, y)\n    assert_almost_equal(ridge.intercept_, 0, decimal=1)\n"], "sample_799": ["def test_permutation_test_score_with_groups():\n    # Check if permutation_test_score works correctly with group labels\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n    groups = np.repeat([1, 2, 3, 4], 25)\n    clf = LogisticRegression(max_iter=1000)\n\n    score, permutation_scores, pvalue = permutation_test_score(\n        clf, X, y, groups=groups, cv=GroupKFold(n_splits=4), n_permutations=100, random_state=42\n    )\n\n    assert_greater(score, 0.5)\n    assert_almost_equal(pvalue, 0.01, 2)\n    assert len(permutation_scores) == 100\n    assert permutation_scores.mean() < score\n\n    # Check if FitFailedWarning is raised correctly when an estimator fails\n    failing_clf = FailingClassifier(FailingClassifier.FAILING_PARAMETER)\n    with pytest.raises(ValueError):\n        permutation_test_score(failing_clf, X, y, groups=groups, cv=GroupKFold(n_splits=4), n_permutations=100, random_state=42)\n", "def test_cross_val_score_with_custom_metric():\n    # Custom scoring function\n        return np.mean(y_true == y_pred)\n\n    X, y = make_classification(n_samples=50, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n\n    # Using the custom scoring function directly\n    scores = cross_val_score(clf, X, y, scoring=make_scorer(custom_metric))\n    assert len(scores) == 5  # Default CV is 5-fold\n    assert scores.mean() > 0.9  # Expect a high accuracy for this simple dataset\n\n    # Using a dictionary with custom scoring function\n    scores_dict = cross_validate(clf, X, y, scoring={'custom': make_scorer(custom_metric)}, return_train_score=True)\n    assert 'train_custom' in scores_dict\n    assert 'test_custom' in scores_dict\n    assert len(scores_dict['train_custom']) == 5\n    assert len(scores_dict['test_custom']) == 5\n    assert scores_dict['test_custom'].mean() > 0.9\n", "def test_cross_val_predict_sparse_input():\n    # Test cross_val_predict with sparse input\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    X_sparse = coo_matrix(X)\n    clf = LogisticRegression(random_state=0)\n    \n    # Sparse input with predict method\n    preds_dense = cross_val_predict(clf, X, y, method='predict', cv=5)\n    preds_sparse = cross_val_predict(clf, X_sparse, y, method='predict', cv=5)\n    assert_array_equal(preds_dense, preds_sparse)\n\n    # Sparse input with decision_function method\n    preds_dense = cross_val_predict(clf, X, y, method='decision_function', cv=5)\n    preds_sparse = cross_val_predict(clf, X_sparse, y, method='decision_function', cv=5)\n    assert_array_almost_equal(preds_dense, preds_sparse)\n    \n    # Sparse input with predict_proba method\n    preds_dense = cross_val_predict(clf, X, y, method='predict_proba', cv=5)\n    preds_sparse = cross_val_predict(clf, X_sparse, y, method='predict_proba', cv=5)\n    assert_array_almost_equal(preds_dense, preds_sparse)\n    \n    # Sparse input with predict_log_proba method\n    preds_dense = cross_val_predict(clf, X, y, method='predict_log_proba', cv=5)\n    preds_sparse = cross_val_predict(clf, X_sparse, y, method='predict_log_proba', cv=5)\n    assert_array_almost_equal(preds_dense, preds_sparse)\n", "def test_cross_val_predict_unfitted_estimator():\n    # Test to ensure that cross_val_predict raises an error when the estimator\n    # is not fitted before calling decision_function, predict_proba, and predict_log_proba\n\n    X, y = make_classification(n_samples=50, n_features=2, random_state=0)\n    est = LogisticRegression()\n\n    methods = ['decision_function', 'predict_proba', 'predict_log_proba']\n    for method in methods:\n        assert_raises(NotFittedError, cross_val_predict, est, X, y, method=method, cv=3)\n", "def test_cross_validate_return_train_score():\n    # Verify that return_train_score parameter works as expected\n    X, y = make_classification(n_samples=100, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    cv = KFold(n_splits=5)\n\n    # Test with return_train_score=True\n    scores_with_train = cross_validate(clf, X, y, cv=cv, scoring='accuracy',\n                                       return_train_score=True)\n    assert 'train_score' in scores_with_train\n    assert len(scores_with_train['train_score']) == 5\n\n    # Test with return_train_score=False\n    scores_without_train = cross_validate(clf, X, y, cv=cv, scoring='accuracy',\n                                          return_train_score=False)\n    assert 'train_score' not in scores_without_train\n"], "sample_800": ["def test_check_non_transformer_estimators_n_iter():\n    from sklearn.linear_model import Ridge, LogisticRegression, SGDRegressor\n    from sklearn.decomposition import PCA\n\n    # Check estimator with n_iter_\n    check_non_transformer_estimators_n_iter(\"Ridge\", Ridge())\n    check_non_transformer_estimators_n_iter(\"SGDRegressor\", SGDRegressor())\n    check_non_transformer_estimators_n_iter(\"LogisticRegression\", LogisticRegression(max_iter=10))\n\n    # Check transformer with n_iter_\n    check_transformer_n_iter(\"PCA\", PCA())\n", "def test_check_estimator_sparse_data():\n    # Check if the estimator can handle sparse data correctly\n    from sklearn.linear_model import LogisticRegression\n\n    class SparseCompatibleEstimator(LogisticRegression):\n            if sp.issparse(X):\n                self.sparse_input_ = True\n            else:\n                self.sparse_input_ = False\n            return super().fit(X, y)\n\n            if sp.issparse(X) != self.sparse_input_:\n                raise ValueError(\"Input format mismatch: trained with sparse={} but got sparse={}\".format(\n                    self.sparse_input_, sp.issparse(X)))\n            return super().predict(X)\n\n    # Check the custom estimator\n    check_estimator(SparseCompatibleEstimator())\n", "def test_check_transformer_general():\n    # Test that check_transformer_general works correctly on a simple transformer\n    from sklearn.preprocessing import StandardScaler\n    check_estimator(StandardScaler())\n    check_transformer_general(\"StandardScaler\", StandardScaler())\n", "def test_check_class_weight_balanced_classifiers():\n    # Test that balanced class weights improve the score for imbalanced datasets\n    X_train = np.array([[0.8], [1], [1.2], [1.4], [1.6], [1.8], [2]])\n    y_train = np.array([0, 0, 0, 1, 1, 1, 1])\n    X_test = np.array([[0.9], [1.1], [1.3], [1.5], [1.7], [1.9]])\n    y_test = np.array([0, 0, 0, 1, 1, 1])\n    weights = np.array([0.25, 0.25, 0.25, 0.25, 0.25, 0.25])\n\n    classifiers = [\n        LogisticRegression(),\n        DecisionTreeClassifier(),\n        RandomForestClassifier()\n    ]\n\n    for clf in classifiers:\n        check_class_weight_balanced_classifiers(clf.__class__.__name__, clf, X_train, y_train, X_test, y_test, weights)\n", "def test_check_estimators_dtypes():\n    # check that all estimators can handle different data types\n    from sklearn.linear_model import Ridge\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.cluster import KMeans\n\n    estimators = [Ridge, RandomForestRegressor, KMeans]\n\n    for estimator in estimators:\n        # create an instance of the estimator\n        est = estimator()\n        # set random state if applicable\n        set_random_state(est)\n\n        # check the estimator for different data types\n        check_estimators_dtypes(est.__class__.__name__, est)\n"], "sample_801": ["def test_clone():\n    # Test the clone function\n    lr = LogisticRegression(C=1.0, max_iter=200)\n    lr_clone = clone(lr)\n    assert lr.__class__ == lr_clone.__class__\n    assert lr.get_params() == lr_clone.get_params()\n    assert lr is not lr_clone\n\n    # Test cloning a list of estimators\n    estimators = [LogisticRegression(C=1.0), StandardScaler(), PCA(n_components=2)]\n    cloned_estimators = clone(estimators)\n    assert len(estimators) == len(cloned_estimators)\n    for est, cloned_est in zip(estimators, cloned_estimators):\n        assert est.__class__ == cloned_est.__class__\n        assert est.get_params() == cloned_est.get_params()\n        assert est is not cloned_est\n\n    # Test safe=False with non-estimator\n    non_estimator = {\"key\": \"value\"}\n    cloned_non_estimator = clone(non_estimator, safe=False)\n    assert non_estimator == cloned_non_estimator\n    assert non_estimator is not cloned_non_estimator\n\n    # Test clone with nested estimators\n    pipeline = make_pipeline(StandardScaler(), LogisticRegression(C=0.1))\n    cloned_pipeline = clone(pipeline)\n    assert pipeline.__class__ == cloned_pipeline.__class__\n    assert pipeline.get_params() == cloned_pipeline.get_params()\n    assert pipeline is not cloned_pipeline\n", "def test_clone():\n    # Test the clone function\n    lr = LogisticRegression(C=1.0, max_iter=200)\n    lr_cloned = clone(lr)\n    assert lr.get_params() == lr_cloned.get_params()\n    assert lr.__class__ == lr_cloned.__class__\n\n    # Test with nested estimators\n    pipeline = Pipeline([('scaler', StandardScaler()), ('clf', lr)])\n    pipeline_cloned = clone(pipeline)\n    assert pipeline.get_params() == pipeline_cloned.get_params()\n    assert pipeline.steps[0][1].get_params() == pipeline_cloned.steps[0][1].get_params()\n    assert pipeline.steps[1][1].get_params() == pipeline_cloned.steps[1][1].get_params()\n\n    # Test safe=False\n    obj = {'a': 1, 'b': [1, 2, 3]}\n    obj_cloned = clone(obj, safe=False)\n    assert obj == obj_cloned\n    assert obj is not obj_cloned\n", "def test_clone_estimator():\n    # Test the clone function\n    class MyEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n    est = MyEstimator(param1=5)\n    est_clone = clone(est)\n    assert est.param1 == est_clone.param1\n    assert est.param2 == est_clone.param2\n    assert est is not est_clone\n\n    # Test cloning with nested estimators\n    nested_est = MyEstimator(param1=MyEstimator(param1=3), param2=4)\n    nested_est_clone = clone(nested_est)\n    assert nested_est.param1.param1 == nested_est_clone.param1.param1\n    assert nested_est.param2 == nested_est_clone.param2\n    assert nested_est is not nested_est_clone\n    assert nested_est.param1 is not nested_est_clone.param1\n\n    # Test cloning with lists of estimators\n    list_est = [MyEstimator(param1=i) for i in range(3)]\n    list_est_clone = clone(list_est)\n    for e, ec in zip(list_est, list_est_clone):\n        assert e.param1 == ec.param1\n        assert e.param2 == ec.param2\n        assert e is not ec\n\n    # Test safe=False cloning on non-estimators\n    non_estimator = {\"key\": \"value\"}\n    non_estimator_clone = clone(non_estimator, safe=False)\n    assert non_estimator == non_estimator_clone\n    assert non_estimator is not non_estimator_clone\n", "def test_clone_estimator():\n    # Ensure that the clone function correctly clones an estimator\n    lr = LogisticRegression(C=10, max_iter=200)\n    lr_cloned = clone(lr)\n    \n    assert lr.get_params() == lr_cloned.get_params()\n    assert lr is not lr_cloned  # Ensure it's a different object\n    \n    # Ensure that modifying the clone does not affect the original\n    lr_cloned.C = 20\n    assert lr.C == 10\n    assert lr_cloned.C == 20\n    \n    # Test with nested estimators\n    pipeline = Pipeline(steps=[('scaler', StandardScaler()), ('clf', LogisticRegression(C=5))])\n    pipeline_cloned = clone(pipeline)\n    \n    assert pipeline.get_params() == pipeline_cloned.get_params()\n    assert pipeline is not pipeline_cloned\n    assert pipeline.steps[1][1] is not pipeline_cloned.steps[1][1]\n", "def test_clone_estimator():\n    # Test the `clone` function with different types of input\n\n    # Case 1: Cloning a single estimator\n    lr = LogisticRegression(C=2.0)\n    lr_cloned = clone(lr)\n    assert lr.get_params() == lr_cloned.get_params()\n    assert lr is not lr_cloned\n\n    # Case 2: Cloning a list of estimators\n    lr_list = [LogisticRegression(C=1.0), LogisticRegression(C=2.0)]\n    lr_list_cloned = clone(lr_list)\n    assert len(lr_list) == len(lr_list_cloned)\n    for original, cloned in zip(lr_list, lr_list_cloned):\n        assert original.get_params() == cloned.get_params()\n        assert original is not cloned\n\n    # Case 3: Cloning a non-estimator object with safe=True should raise a TypeError\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        try:\n            clone(object(), safe=True)\n        except TypeError:\n            pass\n        else:\n            assert False, \"TypeError not raised for non-estimator object with safe=True\"\n\n    # Case 4: Cloning a non-estimator object with safe=False should fallback to deepcopy\n    non_estimator = {\"key\": \"value\"}\n    non_estimator_cloned = clone(non_estimator, safe=False)\n    assert non_estimator == non_estimator_cloned\n    assert non_estimator is not non_estimator_cloned\n\n    # Case 5: Cloning an estimator with nested estimators\n    pipeline = make_pipeline(StandardScaler(), LogisticRegression(C=999))\n    pipeline_cloned = clone(pipeline)\n    assert pipeline.get_params() == pipeline_cloned.get_params()\n    assert pipeline is not pipeline_cloned\n    assert pipeline.steps[0][1] is not pipeline_cloned.steps[0][1]\n    assert pipeline.steps[1][1] is not pipeline_cloned.steps[1][1]\n"], "sample_802": ["def test_pipeline_with_dropped_steps():\n    # Test pipeline with dropped steps using 'drop' and None\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    # Define a pipeline with dropped steps\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n    pca = PCA(n_components=2, svd_solver='full')\n    select = SelectKBest(f_classif, k=1)\n    pipe = Pipeline([\n        ('pca', 'drop'),\n        ('select', select),\n        ('svc', clf)\n    ])\n    # Ensure the dropped step is correctly skipped\n    pipe.fit(X, y)\n    assert_array_equal(pipe.predict(X), clf.fit(select.fit_transform(X, y), y).predict(select.transform(X)))\n    pipe.set_params(pca=None)\n    pipe.fit(X, y)\n    assert_array_equal(pipe.predict(X), clf.fit(select.fit_transform(X, y), y).predict(select.transform(X)))\n", "def test_pipeline_memory_with_slicing():\n    # Test that slicing a pipeline with memory keeps the memory object\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        \n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', transf), ('svc', clf)], memory=memory)\n        pipe.fit(X, y)\n        sub_pipe = pipe[:1]\n        assert sub_pipe.memory is not None\n        assert sub_pipe.memory.location == cachedir\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_get_params_deep():\n    # Test the get_params method with deep=True and deep=False\n    clf = SVC()\n    filter1 = SelectKBest(f_classif)\n    pipe = Pipeline([('anova', filter1), ('svc', clf)])\n\n    params_deep = pipe.get_params(deep=True)\n    params_shallow = pipe.get_params(deep=False)\n\n    # Ensure deep gets params from subobjects\n    assert 'anova__k' in params_deep\n    assert 'svc__C' in params_deep\n    assert 'anova__k' not in params_shallow\n    assert 'svc__C' not in params_shallow\n\n    # Ensure shallow only gets params from the pipeline itself\n    assert 'steps' in params_shallow\n    assert 'memory' in params_shallow\n    assert 'steps' in params_deep\n    assert 'memory' in params_deep\n\n    # Check that the parameters match expected values\n    assert params_deep['anova__k'] == 10\n    assert params_deep['svc__C'] == 1.0\n    assert params_shallow['steps'] == [('anova', filter1), ('svc', clf)]\n    assert params_shallow['memory'] is None\n", "def test_pipeline_memory_with_partial_fit():\n    # Test that memory is used in a pipeline with a partially fitted estimator\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n\n        # Create a pipeline with a transformer and a classifier supporting partial_fit\n        clf = SGDClassifier(max_iter=1, tol=None, random_state=0, warm_start=True)\n        transf = DummyTransf()\n        cached_pipe = Pipeline([('transf', transf), ('sgd', clf)], memory=memory)\n\n        # First partial fit\n        cached_pipe.named_steps['sgd'].partial_fit(X, y, classes=np.unique(y))\n        ts = cached_pipe.named_steps['transf'].timestamp_\n\n        # Check that timestamp remains the same after the second partial fit\n        cached_pipe.named_steps['sgd'].partial_fit(X, y)\n        assert_equal(ts, cached_pipe.named_steps['transf'].timestamp_)\n\n        # Check that predictions are consistent\n        predictions_1 = cached_pipe.predict(X)\n        cached_pipe.named_steps['sgd'].partial_fit(X, y)\n        predictions_2 = cached_pipe.predict(X)\n        assert_array_equal(predictions_1, predictions_2)\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_memory_transform_only():\n    # Test that pipeline with memory works with a transformer only (no final estimator)\n    iris = load_iris()\n    X = iris.data\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        \n        transf = DummyTransf()\n        cached_pipe = Pipeline([('transf', transf)], memory=memory)\n\n        # Memoize the transformer at the first fit\n        Xt = cached_pipe.fit_transform(X)\n        ts = cached_pipe.named_steps['transf'].timestamp_\n        assert_array_equal(transf.fit_transform(X), Xt)\n\n        # Fit again to ensure transformer is loaded from cache\n        Xt2 = cached_pipe.fit_transform(X)\n        assert_array_equal(Xt, Xt2)\n        assert_equal(ts, cached_pipe.named_steps['transf'].timestamp_)\n    finally:\n        shutil.rmtree(cachedir)\n"], "sample_803": ["def test_average_precision_score_uninterpolated():\n    # Test average precision score with uninterpolated precision-recall curve\n    y_true = [0, 1, 1, 0, 1]\n    y_score = [0.1, 0.4, 0.35, 0.8, 0.7]\n    ap_score = average_precision_score(y_true, y_score)\n\n    # Compute manually\n    precision, recall, _ = precision_recall_curve(y_true, y_score)\n    expected_ap_score = -np.sum(np.diff(recall) * np.array(precision)[:-1])\n\n    assert_almost_equal(ap_score, expected_ap_score, decimal=5)\n\n    # Edge case where y_true has no positive samples\n    y_true = [0, 0, 0, 0]\n    y_score = [0.1, 0.4, 0.35, 0.8]\n    assert_raises(ValueError, average_precision_score, y_true, y_score)\n\n    # Edge case where y_true has no negative samples\n    y_true = [1, 1, 1, 1]\n    y_score = [0.1, 0.4, 0.35, 0.8]\n    assert_raises(ValueError, average_precision_score, y_true, y_score)\n", "def test_average_precision_score_single_label():\n    # Test average_precision_score with single label in y_true\n    y_true = [1]\n    y_pred = [0.5]\n    assert_almost_equal(average_precision_score(y_true, y_pred), 1.0)\n    \n    y_true = [0]\n    y_pred = [0.5]\n    assert_almost_equal(average_precision_score(y_true, y_pred), 0.0)\n    \n    y_true = [0, 1]\n    y_pred = [0.5, 0.5]\n    assert_almost_equal(average_precision_score(y_true, y_pred), 0.5)\n", "def test_label_binarize_exception():\n    # Test label_binarize raises exception for non-binary labels in roc_auc_score\n    y_true = np.array([0, 2, 1, 2])\n    y_score = np.array([0.5, 0.6, 0.7, 0.8])\n    error_message = \"Data is not binary and pos_label is not specified\"\n    assert_raise_message(ValueError, error_message, _binary_clf_curve, y_true, y_score)\n", "def test_auc_with_memmap():\n    # Test AUC computation with a numpy memmap instance\n    x = np.array([0, 1, 2, 3, 4])\n    y = np.array([0, 1, 2, 3, 4])\n    with open('test_memmap.dat', 'w+b') as f:\n        memmap_x = np.memmap(f, dtype='float64', mode='w+', shape=x.shape)\n        memmap_y = np.memmap(f, dtype='float64', mode='w+', shape=y.shape)\n        memmap_x[:] = x[:]\n        memmap_y[:] = y[:]\n        assert_array_almost_equal(auc(memmap_x, memmap_y), 8.0)\n", "def test_average_precision_score_binary_edge_cases():\n    # Test average_precision_score on edge cases for binary classification\n    y_true = [0, 1, 1, 0]\n    y_scores = [0.5, 0.5, 0.5, 0.5]\n    ap_score = average_precision_score(y_true, y_scores)\n    assert_almost_equal(ap_score, 0.5)\n    \n    y_true = [1, 0, 0, 1, 1]\n    y_scores = [0.1, 0.1, 0.4, 0.4, 0.4]\n    ap_score = average_precision_score(y_true, y_scores)\n    assert_almost_equal(ap_score, 0.53333333)\n\n    # Check when y_score has only one unique value and y_true has both classes\n    y_true = [0, 1, 0, 1, 0]\n    y_scores = [0.5, 0.5, 0.5, 0.5, 0.5]\n    ap_score = average_precision_score(y_true, y_scores)\n    assert_almost_equal(ap_score, 0.4)\n"], "sample_804": ["def test_ordinal_encoder_handle_unknown():\n    # Test that ordinal encoder raises error for unknown features\n    # present during transform.\n    X = [['Male', 1], ['Female', 3]]\n    X2 = [['Female', 2], ['Unknown', 3]]\n\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    msg = \"Found unknown categories\"\n    with pytest.raises(ValueError, match=msg):\n        enc.transform(X2)\n\n    # Test the ignore option, ignores unknown features (giving all NaNs)\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    X2_transformed = enc.transform(X2)\n    expected_transformed = np.array([[0., np.nan], [np.nan, 1.]])\n    assert_array_equal(X2_transformed, expected_transformed)\n", "def test_ordinal_encoder_inverse_with_missing_values():\n    # Test the inverse_transform method of OrdinalEncoder with missing values.\n    X = [['Male', 1, 'girl', None, 3],\n         ['Female', None, 'boy', 1, 10],\n         ['Male', 51, 'girl', 12, None],\n         ['Male', 91, None, 21, 30]]\n    \n    enc = OrdinalEncoder()\n    X_trans = enc.fit_transform(X)\n    \n    # Manually fill missing values with a placeholder for comparison\n    X_filled = [['Male', 1, 'girl', '', 3],\n                ['Female', '', 'boy', 1, 10],\n                ['Male', 51, 'girl', 12, ''],\n                ['Male', 91, '', 21, 30]]\n    \n    exp = np.array(X_filled, dtype=object)\n    assert_array_equal(enc.inverse_transform(X_trans), exp)\n", "def test_one_hot_encoder_inverse_handle_unknown():\n    # Test inverse_transform with handle_unknown='ignore'\n    X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X_tr = enc.fit_transform(X)\n    \n    # Adding a new category 'xyz' which was not present during fitting\n    X_new = [['xyz', 2, 55], ['abc', 1, 55]]\n    X_tr_new = enc.transform(X_new)\n    \n    exp = np.array(X_new, dtype=object)\n    exp[0, 0] = None  # 'xyz' should be replaced with None\n    \n    assert_array_equal(enc.inverse_transform(X_tr_new), exp)\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['a', 'b'], ['c', 'd']])\n    X2 = np.array([['e', 'f']])\n\n    # Test that ordinal encoder raises error for unknown features\n    # present during transform.\n    oe = OrdinalEncoder(handle_unknown='error')\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n\n    # Test the ignore option, ignores unknown features (giving -1)\n    oe = OrdinalEncoder(handle_unknown='ignore')\n    oe.fit(X)\n    X2_transformed = oe.transform(X2)\n    expected = np.array([[-1, -1], [-1, -1]], dtype=np.float64)\n    assert_array_equal(X2_transformed, expected)\n\n    # Raise error if handle_unknown is neither ignore or error.\n    oe = OrdinalEncoder(handle_unknown='unknown_option')\n    assert_raises(ValueError, oe.fit, X)\n", "def test_ordinal_encoder_drop():\n    X = [['Male', 1], ['Female', 3], ['Male', 2]]\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    assert_array_equal(enc.categories_[0], ['Female', 'Male'])\n    assert_array_equal(enc.categories_[1], [1, 2, 3])\n    \n    # Test transformation\n    X_trans = enc.transform(X)\n    expected = np.array([[1., 0.], [0., 2.], [1., 1.]])\n    assert_array_equal(X_trans, expected)\n    \n    # Test inverse transformation\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X_inv, X)\n    \n    # Test handling of unknown categories during transform with 'ignore'\n    X_unknown = [['Female', 2], ['Male', 4]]\n    enc_ignore = OrdinalEncoder(categories=[['Female', 'Male'], [1, 2, 3]], handle_unknown='ignore')\n    enc_ignore.fit(X)\n    X_trans_ignore = enc_ignore.transform(X_unknown)\n    expected_ignore = np.array([[0., 1.], [1., -1.]])\n    assert_array_equal(X_trans_ignore, expected_ignore)\n    \n    # Test handling of unknown categories during transform with 'error'\n    enc_error = OrdinalEncoder(categories=[['Female', 'Male'], [1, 2, 3]], handle_unknown='error')\n    enc_error.fit(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc_error.transform(X_unknown)\n"], "sample_805": ["def test_mean_gamma_deviance():\n    # Test mean_gamma_deviance with positive values\n    y_true = [2, 0.5, 1, 4]\n    y_pred = [0.5, 0.5, 2., 2.]\n    assert_almost_equal(mean_gamma_deviance(y_true, y_pred), 1.0568, decimal=3)\n\n    # Test mean_gamma_deviance with sample weights\n    sample_weight = [1, 2, 1, 1]\n    assert_almost_equal(mean_gamma_deviance(y_true, y_pred, sample_weight=sample_weight), 0.8661, decimal=3)\n\n    # Test mean_gamma_deviance with invalid values (negative y_true)\n    y_true_invalid = [-1, 0.5, 1, 4]\n    with pytest.raises(ValueError, match=\"strictly positive y_true and y_pred\"):\n        mean_gamma_deviance(y_true_invalid, y_pred)\n\n    # Test mean_gamma_deviance with invalid values (non-positive y_pred)\n    y_pred_invalid = [0, 0.5, 2., 2.]\n    with pytest.raises(ValueError, match=\"strictly positive y_true and y_pred\"):\n        mean_gamma_deviance(y_true, y_pred_invalid)\n", "def test_max_error_multioutput_exception():\n    y_true = np.array([[1, 0, 0], [0, 1, 1], [1, 1, 0]])\n    y_pred = np.array([[0, 0, 0], [1, 0, 1], [0, 0, 0]])\n\n    with pytest.raises(ValueError, match=\"Multioutput not supported in max_error\"):\n        max_error(y_true, y_pred)\n", "def test_max_error():\n    y_true = [3, 2, 7, 1]\n    y_pred = [4, 2, 7, 1]\n    assert_almost_equal(max_error(y_true, y_pred), 1)\n    \n    y_true = [1, 10, 5]\n    y_pred = [2, 5, 9]\n    assert_almost_equal(max_error(y_true, y_pred), 5)\n\n    y_true = [0, 0, 0]\n    y_pred = [0, 0, 0]\n    assert_almost_equal(max_error(y_true, y_pred), 0)\n\n    # Check multioutput case\n    y_true = [[1, 2], [2, 3], [3, 4]]\n    y_pred = [[2, 2], [2, 1], [1, 6]]\n    with pytest.raises(ValueError, match=\"Multioutput not supported in max_error\"):\n        max_error(y_true, y_pred)\n", "def test_mean_gamma_deviance():\n    y_true = [2, 0.5, 1, 4]\n    y_pred = [0.5, 0.5, 2., 2.]\n    assert_almost_equal(mean_gamma_deviance(y_true, y_pred), 1.056, decimal=3)\n\n    y_true = [1, 2, 3, 4]\n    y_pred = [2, 2, 3, 4]\n    assert_almost_equal(mean_gamma_deviance(y_true, y_pred), 0.153, decimal=3)\n\n    with pytest.raises(ValueError, match=\"can only be used on strictly positive y_true and y_pred.\"):\n        mean_gamma_deviance([0, 2, 3], [1, 2, 3])\n\n    with pytest.raises(ValueError, match=\"can only be used on strictly positive y_true and y_pred.\"):\n        mean_gamma_deviance([1, 2, 3], [1, 0, 3])\n", "def test_mean_poisson_deviance():\n    # Basic test for mean_poisson_deviance\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n    expected_result = mean_tweedie_deviance(y_true, y_pred, p=1)\n    assert_almost_equal(mean_poisson_deviance(y_true, y_pred), expected_result)\n    \n    # Test with sample weights\n    sample_weight = np.array([1, 2, 1, 0.5])\n    expected_result = mean_tweedie_deviance(y_true, y_pred, p=1, sample_weight=sample_weight)\n    assert_almost_equal(mean_poisson_deviance(y_true, y_pred, sample_weight=sample_weight), expected_result)\n    \n    # Test exception for negative values in y_true\n    assert_raises_regex(ValueError, \"non-negative y_true and strictly positive y_pred\",\n                        mean_poisson_deviance, [-1, 0, 1, 2], [0.5, 0.5, 2., 2.])\n    \n    # Test exception for non-positive values in y_pred\n    assert_raises_regex(ValueError, \"non-negative y_true and strictly positive y_pred\",\n                        mean_poisson_deviance, [2, 0, 1, 4], [0, 0.5, 2., 2.])\n"], "sample_806": ["def test_loss_functions_deprecation():\n    # Test that loss functions raise a deprecation warning\n    with pytest.deprecated_call():\n        _ = LeastSquaresError(1)\n        \n    with pytest.deprecated_call():\n        _ = LeastAbsoluteError(1)\n        \n    with pytest.deprecated_call():\n        _ = HuberLossFunction(1)\n        \n    with pytest.deprecated_call():\n        _ = QuantileLossFunction(1)\n        \n    with pytest.deprecated_call():\n        _ = BinomialDeviance(2)\n        \n    with pytest.deprecated_call():\n        _ = MultinomialDeviance(3)\n        \n    with pytest.deprecated_call():\n        _ = ExponentialLoss(2)\n", "def test_zero_estimator():\n    # Test ZeroEstimator for both classifier and regressor\n    X, y = datasets.make_classification(n_samples=100, random_state=1)\n    zr = ZeroEstimator()\n    assert_raises(NotFittedError, zr.predict, X)\n    zr.fit(X, y)\n    preds = zr.predict(X)\n    assert np.all(preds == 0)\n\n    X, y = datasets.make_regression(n_samples=100, random_state=1)\n    zr = ZeroEstimator()\n    assert_raises(NotFittedError, zr.predict, X)\n    zr.fit(X, y)\n    preds = zr.predict(X)\n    assert np.all(preds == 0)\n\n    zr.fit(X, y)\n    preds_proba = zr.predict_proba(X)\n    assert np.all(preds_proba == 0)\n", "def test_zero_estimator():\n    # Test if ZeroEstimator works as expected\n    X = [[0], [1], [2], [3], [4]]\n    y_classification = [0, 1, 0, 1, 0]\n    y_regression = [0.5, 1.5, 2.5, 3.5, 4.5]\n\n    # Test ZeroEstimator for classification\n    zero_estimator_clf = ZeroEstimator()\n    zero_estimator_clf.fit(X, y_classification)\n    y_pred_clf = zero_estimator_clf.predict(X)\n    assert_array_equal(y_pred_clf, np.zeros((len(X), 1)))\n\n    y_pred_proba_clf = zero_estimator_clf.predict_proba(X)\n    assert_array_equal(y_pred_proba_clf, np.zeros((len(X), 1)))\n\n    # Test ZeroEstimator for regression\n    zero_estimator_reg = ZeroEstimator()\n    zero_estimator_reg.fit(X, y_regression)\n    y_pred_reg = zero_estimator_reg.predict(X)\n    assert_array_equal(y_pred_reg, np.zeros((len(X), 1)))\n", "def test_verbose_reporter_initialization():\n    # Test VerboseReporter initialization and update methods\n    from io import StringIO\n    import sys\n\n    old_stdout = sys.stdout\n    sys.stdout = StringIO()\n    \n    # Create a mock estimator\n    class MockEstimator:\n        n_estimators = 10\n        subsample = 0.8\n        train_score_ = np.random.rand(n_estimators)\n        oob_improvement_ = np.random.rand(n_estimators)\n\n    est = MockEstimator()\n    verbose_reporter = VerboseReporter(verbose=1)\n    verbose_reporter.init(est, begin_at_stage=0)\n\n    for i in range(est.n_estimators):\n        verbose_reporter.update(i, est)\n    \n    verbose_output = sys.stdout\n    sys.stdout = old_stdout\n\n    verbose_output.seek(0)\n    header = verbose_output.readline().rstrip()\n    true_header = ' '.join(['%10s'] + ['%16s'] * 3) % (\n        'Iter', 'Train Loss', 'OOB Improve', 'Remaining Time')\n    assert_equal(true_header, header)\n\n    n_lines = sum(1 for l in verbose_output.readlines())\n    assert_equal(n_lines, est.n_estimators)\n", "def test_mean_estimator():\n    # Check if MeanEstimator provides correct mean predictions.\n    X_train = np.array([[1, 2], [3, 4], [5, 6]])\n    y_train = np.array([2, 4, 6])\n    mean_est = MeanEstimator()\n    mean_est.fit(X_train, y_train)\n    \n    X_test = np.array([[7, 8], [9, 10]])\n    y_pred = mean_est.predict(X_test)\n    \n    assert_array_almost_equal(y_pred, np.array([[4.0], [4.0]]))\n    check_is_fitted(mean_est, 'mean')\n"], "sample_807": ["def test_calibration_with_random_forest():\n    \"\"\"Test calibration with RandomForestClassifier on binary classification\"\"\"\n    X, y = make_classification(n_samples=200, n_features=5, random_state=42)\n    X_train, X_test, y_train, y_test = X[:150], X[150:], y[:150], y[150:]\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train, y_train)\n    prob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n    for method in ['isotonic', 'sigmoid']:\n        pc_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n        pc_clf.fit(X_train, y_train)\n        prob_pos_pc_clf = pc_clf.predict_proba(X_test)[:, 1]\n\n        # Check that brier score has improved after calibration\n        assert_greater(brier_score_loss(y_test, prob_pos_clf),\n                       brier_score_loss(y_test, prob_pos_pc_clf))\n\n        # Check that log-loss of calibrated classifier is smaller than\n        # log-loss of uncalibrated classifier\n        uncalibrated_log_loss = log_loss(y_test, prob_pos_clf)\n        calibrated_log_loss = log_loss(y_test, prob_pos_pc_clf)\n        assert_greater_equal(uncalibrated_log_loss, calibrated_log_loss)\n", "def test_calibration_with_sparse_matrix():\n    \"\"\"Test calibration with sparse matrices\"\"\"\n    X, y = make_classification(n_samples=200, n_features=10, random_state=42)\n    X_train, y_train = X[:100], y[:100]\n    X_test, y_test = X[100:], y[100:]\n\n    # Convert to sparse matrix\n    X_train_sparse = sparse.csr_matrix(X_train)\n    X_test_sparse = sparse.csr_matrix(X_test)\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train_sparse, y_train)\n    prob_pos_clf = clf.predict_proba(X_test_sparse)[:, 1]\n\n    for method in ['isotonic', 'sigmoid']:\n        pc_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n        pc_clf.fit(X_train_sparse, y_train)\n        prob_pos_pc_clf = pc_clf.predict_proba(X_test_sparse)[:, 1]\n\n        # Check that brier score has improved after calibration\n        assert_greater(brier_score_loss(y_test, prob_pos_clf),\n                       brier_score_loss(y_test, prob_pos_pc_clf))\n", "def test_calibration_no_base_estimator():\n    \"\"\"Test calibration when no base estimator is provided\"\"\"\n    X, y = make_classification(n_samples=100, n_features=6, random_state=42)\n    X_train, y_train = X[:80], y[:80]\n    X_test, y_test = X[80:], y[80:]\n\n    # Test with default base estimator (LinearSVC)\n    for method in ['isotonic', 'sigmoid']:\n        clf = CalibratedClassifierCV(method=method, cv=2)\n        clf.fit(X_train, y_train)\n        probas = clf.predict_proba(X_test)\n        assert_array_almost_equal(np.sum(probas, axis=1), np.ones(len(X_test)))\n\n        # Check that the predictions are not all the same and within valid probability range\n        assert np.any(np.diff(probas, axis=0)), \"Predictions are all the same\"\n        assert np.all(probas >= 0) and np.all(probas <= 1), \"Probabilities are out of range\"\n", "def test_calibration_invalid_estimator():\n    \"\"\"Test that calibration raises an error with invalid base_estimator\"\"\"\n    X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n    invalid_base_estimator = \"this_is_not_an_estimator\"\n    with pytest.raises(TypeError):\n        CalibratedClassifierCV(base_estimator=invalid_base_estimator, method='sigmoid').fit(X, y)\n\n    class InvalidEstimator:\n        pass\n\n    invalid_base_estimator = InvalidEstimator()\n    with pytest.raises(TypeError):\n        CalibratedClassifierCV(base_estimator=invalid_base_estimator, method='sigmoid').fit(X, y)\n", "def test_calibration_single_class():\n    \"\"\"Test calibration when the training set contains a single class\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=n_samples, n_features=6, n_classes=1,\n                               random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=y.size)\n\n    X_train, y_train, sw_train = X[:n_samples // 2], y[:n_samples // 2], sample_weight[:n_samples // 2]\n    X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]\n\n    clf = MultinomialNB()\n    clf.fit(X_train, y_train, sw_train)\n    prob_pos_clf = clf.predict_proba(X_test)[:, 0]\n\n    for method in ['isotonic', 'sigmoid']:\n        pc_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        with pytest.raises(ValueError, match=\"Training data only contains one class\"):\n            pc_clf.fit(X_train, y_train, sample_weight=sw_train)\n"], "sample_808": ["def test_iforest_random_state():\n    \"\"\"Check that IsolationForest produces consistent results with a fixed random_state.\"\"\"\n    X_train = np.array([[0, 1], [1, 2], [2, 1], [2, 2], [3, 1]])\n    \n    clf1 = IsolationForest(random_state=42).fit(X_train)\n    pred1 = clf1.predict(X_train)\n    decision_func1 = clf1.decision_function(X_train)\n    \n    clf2 = IsolationForest(random_state=42).fit(X_train)\n    pred2 = clf2.predict(X_train)\n    decision_func2 = clf2.decision_function(X_train)\n    \n    assert_array_equal(pred1, pred2)\n    assert_array_almost_equal(decision_func1, decision_func2)\n\n    clf3 = IsolationForest(random_state=24).fit(X_train)\n    pred3 = clf3.predict(X_train)\n    decision_func3 = clf3.decision_function(X_train)\n    \n    assert not np.array_equal(pred1, pred3)\n    assert not np.allclose(decision_func1, decision_func3)\n", "def test_iforest_fit_predict():\n    \"\"\"Test fit and predict methods of IsolationForest.\"\"\"\n    rng = check_random_state(0)\n    X = rng.randn(100, 4)\n\n    # Fit the model\n    clf = IsolationForest(contamination=0.1, random_state=rng)\n    clf.fit(X)\n\n    # Check if the model is fitted\n    check_is_fitted(clf, [\"estimators_\", \"max_samples_\", \"offset_\"])\n\n    # Predict on training data\n    y_pred = clf.predict(X)\n    assert y_pred.shape[0] == X.shape[0]\n\n    # Check if predict returns valid output\n    assert np.all(np.isin(y_pred, [-1, 1]))\n\n    # Test with different contamination parameter\n    clf = IsolationForest(contamination=\"auto\", random_state=rng)\n    clf.fit(X)\n    y_pred = clf.predict(X)\n    assert y_pred.shape[0] == X.shape[0]\n    assert np.all(np.isin(y_pred, [-1, 1]))\n", "def test_iforest_fit_with_sample_weight():\n    \"\"\"Test fitting IsolationForest with sample weights.\"\"\"\n    X_train = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n    sample_weight = np.array([1, 1, 1, 1, 5])\n\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X_train, sample_weight=sample_weight)\n    decision_scores = clf.decision_function(X_train)\n\n    # Assert that the sample weights affect the decision function\n    assert_greater(np.min(decision_scores[:4]), np.max(decision_scores[4:]))\n\n    # Ensure predictions reflect the weights\n    predictions = clf.predict(X_train)\n    assert_array_equal(predictions[:4], [1, 1, 1, 1])\n    assert_array_equal(predictions[4:], [-1])\n", "def test_iforest_random_state():\n    \"\"\"Check random state consistency in IsolationForest\"\"\"\n    X = iris.data\n\n    # Fit with a given random_state and store predictions\n    clf1 = IsolationForest(random_state=42).fit(X)\n    pred1 = clf1.predict(X)\n    scores1 = clf1.decision_function(X)\n\n    # Fit with the same random_state and ensure predictions are consistent\n    clf2 = IsolationForest(random_state=42).fit(X)\n    pred2 = clf2.predict(X)\n    scores2 = clf2.decision_function(X)\n\n    assert_array_equal(pred1, pred2)\n    assert_array_almost_equal(scores1, scores2)\n\n    # Fit with a different random_state and ensure predictions are different\n    clf3 = IsolationForest(random_state=0).fit(X)\n    pred3 = clf3.predict(X)\n    scores3 = clf3.decision_function(X)\n\n    assert not np.array_equal(pred1, pred3)\n    assert not np.allclose(scores1, scores3)\n", "def test_iforest_random_state():\n    \"\"\"Test IsolationForest with different random states.\"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n                                                        boston.target,\n                                                        random_state=rng)\n\n    clf1 = IsolationForest(random_state=0).fit(X_train)\n    clf2 = IsolationForest(random_state=0).fit(X_train)\n    clf3 = IsolationForest(random_state=42).fit(X_train)\n\n    # Same random state should yield identical models\n    assert_array_equal(clf1.predict(X_test), clf2.predict(X_test))\n\n    # Different random states should yield different models\n    with pytest.raises(AssertionError):\n        assert_array_equal(clf1.predict(X_test), clf3.predict(X_test))\n"], "sample_809": ["def test_mutual_info_classif_sparse():\n    # Test mutual_info_classif with sparse input.\n    X_dense = np.array([[0, 0, 0],\n                        [1, 1, 0],\n                        [2, 0, 1],\n                        [2, 0, 1],\n                        [2, 0, 1]])\n    y = np.array([0, 1, 2, 2, 1])\n    X_sparse = csr_matrix(X_dense)\n\n    mi_dense = mutual_info_classif(X_dense, y, discrete_features=True)\n    mi_sparse = mutual_info_classif(X_sparse, y, discrete_features=True)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_regression_sparse():\n    # Test mutual_info_regression with sparse input.\n    rng = check_random_state(0)\n    X_dense = rng.rand(100, 3)\n    X_sparse = csr_matrix(X_dense)\n    y = rng.rand(100)\n\n    mi_dense = mutual_info_regression(X_dense, y, random_state=0)\n    mi_sparse = mutual_info_regression(X_sparse, y, random_state=0)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_estimate_mi_sparse_input():\n    # Generate sparse data and a continuous target variable\n    rng = check_random_state(0)\n    X_dense = rng.rand(100, 3)\n    X_sparse = csr_matrix(X_dense)\n    y = rng.rand(100)\n\n    # Test that the function can handle sparse input correctly\n    mi_dense = mutual_info_regression(X_dense, y, discrete_features=False, random_state=0)\n    mi_sparse = mutual_info_regression(X_sparse, y, discrete_features=False, random_state=0)\n    assert_array_equal(mi_dense, mi_sparse)\n\n    # Test with discrete features in sparse input\n    X_dense[:, 2] = X_dense[:, 2] > 0.5\n    X_sparse = csr_matrix(X_dense)\n    mi_dense = mutual_info_classif(X_dense, y > 0.5, discrete_features=[2], random_state=0)\n    mi_sparse = mutual_info_classif(X_sparse, y > 0.5, discrete_features=[2], random_state=0)\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_regression_sparse():\n    # Test mutual info regression with sparse input matrix\n    rng = check_random_state(0)\n    X_dense = rng.rand(100, 4)\n    X_sparse = csr_matrix(X_dense)\n    y = 0.5 * X_dense[:, 0] + 0.3 * X_dense[:, 1] + rng.rand(100)\n\n    mi_dense = mutual_info_regression(X_dense, y, random_state=0)\n    mi_sparse = mutual_info_regression(X_sparse, y, random_state=0)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_compute_mi_cc_zero_correlation():\n    # Test with two continuous variables that have zero correlation.\n    # In this case, the mutual information should be close to zero.\n\n    rng = check_random_state(0)\n    n_samples = 1000\n\n    # Generate two independent random variables\n    x = rng.normal(size=n_samples)\n    y = rng.normal(size=n_samples)\n\n    for n_neighbors in [3, 5, 7]:\n        I_computed = _compute_mi(x, y, False, False, n_neighbors)\n        assert_almost_equal(I_computed, 0, 1)\n"], "sample_810": ["def test_pipeline_partial_fit():\n    # Test partial_fit method of the pipeline\n    class Incremental(BaseEstimator):\n            self.partial_fit_called = False\n\n            self.partial_fit_called = True\n            return self\n\n            return X\n\n            pass\n\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    clf = SVC()\n    incr = Incremental()\n    pipe = Pipeline([('incr', incr), ('svc', clf)])\n\n    # Check that we can't call partial_fit on pipeline with non-incremental last estimator\n    assert_raises_regex(AttributeError, \"'SVC' object has no attribute 'partial_fit'\", pipe.partial_fit, X, y)\n\n    # Check that we can call partial_fit on pipeline with incremental step\n    clf2 = DummyRegressor()\n    pipe2 = Pipeline([('incr', incr), ('dummy', clf2)])\n    pipe2.partial_fit(X, y)\n    assert pipe2.named_steps['incr'].partial_fit_called\n\n    # Check that we can call partial_fit on pipeline with incremental last estimator\n    clf3 = Incremental()\n    pipe3 = Pipeline([('incr', incr), ('incr2', clf3)])\n    pipe3.partial_fit(X, y)\n    assert pipe3.named_steps['incr'].partial_fit_called\n    assert pipe3.named_steps['incr2'].partial_fit_called\n", "def test_pipeline_with_none_transformer():\n    # Test that a None transformer works as expected within a pipeline\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n    pipeline = Pipeline([('transformer', None), ('svc', clf)])\n    \n    # Test fitting the pipeline\n    pipeline.fit(X, y)\n    # Test predictions and scoring\n    assert_array_equal(pipeline.predict(X), clf.predict(X))\n    assert_array_equal(pipeline.predict_proba(X), clf.predict_proba(X))\n    assert_array_equal(pipeline.predict_log_proba(X), clf.predict_log_proba(X))\n    assert pipeline.score(X, y) == clf.score(X, y)\n    # Test that the named steps are as expected\n    assert pipeline.named_steps['transformer'] is None\n    assert pipeline.named_steps['svc'] is clf\n", "def test_pipeline_get_params_deep():\n    # Test the get_params method with deep=True and deep=False\n    clf = SVC()\n    filter1 = SelectKBest(f_classif)\n    pipe = Pipeline([('anova', filter1), ('svc', clf)])\n\n    # Check deep=True\n    params_deep = pipe.get_params(deep=True)\n    assert 'anova__k' in params_deep\n    assert 'svc__C' in params_deep\n    assert 'svc' in params_deep\n    assert 'anova' in params_deep\n\n    # Check deep=False\n    params_shallow = pipe.get_params(deep=False)\n    assert 'anova__k' not in params_shallow\n    assert 'svc__C' not in params_shallow\n    assert 'svc' in params_shallow\n    assert 'anova' in params_shallow\n", "def test_pipeline_memory_with_partial_fit():\n    # Test pipeline with memory and a final estimator that supports partial_fit\n    class DummyPartialFitEstimator(BaseEstimator):\n            self.partial_fit_called = False\n\n            self.partial_fit_called = True\n            return self\n\n            # This will fail if fit is called instead of partial_fit\n            assert False, \"fit should not be called\"\n            return self\n\n            return np.zeros(X.shape[0])\n\n    class DummyTransform(Transf):\n            return self\n\n            return X\n\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n\n        X = np.array([[1, 2, 3], [4, 5, 6]])\n        y = np.array([0, 1])\n\n        pipe = Pipeline([\n            ('transform', DummyTransform()),\n            ('estimator', DummyPartialFitEstimator())\n        ], memory=memory)\n\n        pipe.partial_fit(X, y)\n        assert pipe.named_steps['estimator'].partial_fit_called\n        assert not hasattr(pipe.named_steps['transform'], 'timestamp_')\n\n        # Check that cached transformer works as expected\n        pipe.partial_fit(X, y)\n        assert pipe.named_steps['estimator'].partial_fit_called\n        assert not hasattr(pipe.named_steps['transform'], 'timestamp_')\n\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_len():\n    # Test the __len__ method of the pipeline.\n    clf = SVC()\n    filter1 = SelectKBest(f_classif)\n    pipe = Pipeline([('anova', filter1), ('svc', clf)])\n    assert_equal(len(pipe), 2)\n\n    # Test with three steps\n    scaler = StandardScaler()\n    pipe = Pipeline([('scaler', scaler), ('anova', filter1), ('svc', clf)])\n    assert_equal(len(pipe), 3)\n\n    # Test with a single step\n    pipe = Pipeline([('svc', clf)])\n    assert_equal(len(pipe), 1)\n"], "sample_811": ["def test_rbf_kernel_edge_cases():\n    # Test rbf_kernel with edge case inputs\n    rng = np.random.RandomState(0)\n    \n    # Test with empty array\n    X = np.empty((0, 4))\n    Y = np.empty((0, 4))\n    K = rbf_kernel(X, Y)\n    assert_array_almost_equal(K, np.empty((0, 0)))\n\n    # Test with very large values\n    X = np.array([[1e20, 1e20], [1e20, 1e20]])\n    Y = np.array([[1e20, 1e20], [1e20, 1e20]])\n    K = rbf_kernel(X, Y)\n    assert_array_almost_equal(K, np.ones((2, 2)))\n\n    # Test with very small values\n    X = np.array([[1e-20, 1e-20], [1e-20, 1e-20]])\n    Y = np.array([[1e-20, 1e-20], [1e-20, 1e-20]])\n    K = rbf_kernel(X, Y)\n    assert_array_almost_equal(K, np.ones((2, 2)))\n\n    # Test with mixed small and large values\n    X = np.array([[1e-20, 1e20], [1e20, 1e-20]])\n    Y = np.array([[1e20, 1e-20], [1e-20, 1e20]])\n    K = rbf_kernel(X, Y)\n    expected = np.exp(-2 * (1e40))\n    assert_array_almost_equal(K, np.array([[expected, 1], [1, expected]]))\n", "def test_manhattan_distances():\n    # Check the pairwise Manhattan distances computation\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((2, 4))\n\n    # Dense arrays\n    D = manhattan_distances(X, Y)\n    expected = cdist(X, Y, 'cityblock')\n    assert_array_almost_equal(D, expected)\n\n    # Sparse matrices\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    D_sparse = manhattan_distances(X_sparse, Y_sparse)\n    assert_array_almost_equal(D_sparse, expected)\n\n    # Component-wise distances\n    D_comp = manhattan_distances(X, Y, sum_over_features=False)\n    expected_comp = np.abs(X[:, np.newaxis, :] - Y[np.newaxis, :, :]).reshape(-1, X.shape[1])\n    assert_array_almost_equal(D_comp, expected_comp)\n\n    # Test errors for sum_over_features=False with sparse matrices\n    assert_raises(TypeError, manhattan_distances, X_sparse, Y_sparse, sum_over_features=False)\n\n    # Ensure the function handles edge cases correctly\n    X_edge = np.array([[0, 1], [1, 1]])\n    Y_edge = np.array([[1, 0], [2, 2]])\n    D_edge = manhattan_distances(X_edge, Y_edge)\n    expected_edge = cdist(X_edge, Y_edge, 'cityblock')\n    assert_array_almost_equal(D_edge, expected_edge)\n", "def test_check_pairwise_arrays_precomputed():\n    # Test precomputed metric with correct and incorrect shapes\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((5, 4))\n    \n    # Correct shape\n    X_precomputed = pairwise_distances(X)\n    Y_precomputed = pairwise_distances(Y, X)\n    \n    XA_checked, XB_checked = check_pairwise_arrays(X_precomputed, None, precomputed=True)\n    assert_array_equal(X_precomputed, XA_checked)\n    assert_array_equal(X_precomputed, XB_checked)\n    \n    XA_checked, XB_checked = check_pairwise_arrays(X_precomputed, Y_precomputed, precomputed=True)\n    assert_array_equal(X_precomputed, XA_checked)\n    assert_array_equal(Y_precomputed, XB_checked)\n    \n    # Incorrect shapes\n    Y_precomputed_wrong = rng.random_sample((6, 5))\n    assert_raises(ValueError, check_pairwise_arrays, X_precomputed, Y_precomputed_wrong, precomputed=True)\n    \n    Y_precomputed_wrong = rng.random_sample((5, 6))\n    assert_raises(ValueError, check_pairwise_arrays, X_precomputed, Y_precomputed_wrong, precomputed=True)\n", "def test_check_pairwise_precomputed_shape():\n    # Test the check_pairwise_arrays function with precomputed distances\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((5, 4))\n    \n    # When precomputed=True, the second dimension of X should match the first dimension of Y\n    X_precomputed = pairwise_distances(X, Y)\n    assert_raises(ValueError, check_pairwise_arrays, X_precomputed, Y, precomputed=True)\n    \n    # Valid precomputed case\n    Y_precomputed = pairwise_distances(Y)\n    X_checked, Y_checked = check_pairwise_arrays(Y_precomputed, Y_precomputed, precomputed=True)\n    assert_array_equal(X_checked, Y_precomputed)\n    assert_array_equal(Y_checked, Y_precomputed)\n    \n    # When precomputed=False, X and Y should have the same number of features\n    X_wrong_shape = np.resize(np.arange(30), (5, 6))\n    assert_raises(ValueError, check_pairwise_arrays, X, X_wrong_shape)\n    \n    # Valid non-precomputed case\n    X_checked, Y_checked = check_pairwise_arrays(X, Y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(Y_checked, Y)\n", "def test_pairwise_distances_custom_metric():\n    # Test pairwise_distances with a custom metric\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n\n        return np.sum(np.abs(x - y) ** 3) ** (1/3)\n\n    # Test with custom metric\n    S = pairwise_distances(X, Y, metric=custom_metric)\n    S2 = np.array([[custom_metric(x, y) for y in Y] for x in X])\n    assert_array_almost_equal(S, S2)\n\n    # Test with custom metric and Y=None\n    S = pairwise_distances(X, metric=custom_metric)\n    S2 = np.array([[custom_metric(x, y) for y in X] for x in X])\n    assert_array_almost_equal(S, S2)\n\n    # Test with sparse X and Y\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    assert_raises(TypeError, pairwise_distances, X_sparse, Y_sparse, metric=custom_metric)\n\n    # Test with tuples as X and Y\n    X_tuples = tuple([tuple(row) for row in X])\n    Y_tuples = tuple([tuple(row) for row in Y])\n    S = pairwise_distances(X_tuples, Y_tuples, metric=custom_metric)\n    assert_array_almost_equal(S, S2)\n\n    # Test with precomputed metric\n    D = pairwise_distances(X, metric=custom_metric)\n    assert_raises(ValueError, pairwise_distances, D, metric=\"precomputed\")\n\n    # Test that a value error is raised if the metric is not callable\n    assert_raises(ValueError, pairwise_distances, X, Y, metric=123)\n"], "sample_812": ["def test_dict_compact_formatting():\n    # Test that dicts are formatted correctly with the compact parameter\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n\n    params = {\"param1\": 1, \"param2\": 2, \"param3\": 3}\n    expected = \"{'param1': 1, 'param2': 2, 'param3': 3}\"\n    assert pp.pformat(params) == expected\n\n    params = {\"param1\": 1, \"param2\": {\"subparam1\": 1, \"subparam2\": 2}, \"param3\": 3}\n    expected = \"{'param1': 1, 'param2': {'subparam1': 1, 'subparam2': 2}, 'param3': 3}\"\n    assert pp.pformat(params) == expected\n\n    # Test with nested dicts and lists\n    params = {\n        \"param1\": [1, 2, 3],\n        \"param2\": {\"subparam1\": [1, 2], \"subparam2\": {\"subsubparam\": 1}},\n        \"param3\": 3\n    }\n    expected = (\"{'param1': [1, 2, 3], 'param2': {'subparam1': [1, 2], \"\n                \"'subparam2': {'subsubparam': 1}}, 'param3': 3}\")\n    assert pp.pformat(params) == expected\n\n    # Test with more items than n_max_elements_to_show\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=2)\n    params = {\"param1\": 1, \"param2\": 2, \"param3\": 3}\n    expected = \"{'param1': 1, 'param2': 2, ...}\"\n    assert pp.pformat(params) == expected\n", "def test_custom_key_val_tuple():\n    # Test custom KeyValTuple and KeyValTupleParam rendering\n    class CustomEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n    \n    est = CustomEstimator()\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    \n    expected = \"\"\"", "def test_empty_estimator():\n    # Test the _EstimatorPrettyPrinter with an empty custom estimator\n    class EmptyEstimator(BaseEstimator):\n            pass\n\n    empty_estimator = EmptyEstimator()\n    expected = \"EmptyEstimator()\"\n    assert empty_estimator.__repr__() == expected\n\n    # With changed_only set to True, it should still print the same as no parameters\n    set_config(print_changed_only=True)\n    assert empty_estimator.__repr__() == expected\n    set_config(print_changed_only=False)\n", "def test_large_number_of_parameters():\n    # Test to ensure the pretty printer handles large number of parameters correctly\n    large_params = {f'param{i}': i for i in range(50)}\n    \n    class LargeParamEstimator(BaseEstimator):\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n\n    estimator = LargeParamEstimator(**large_params)\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=40)\n    \n    expected = \"\"\"", "def test_compact_dict():\n    # Test compact printing of dictionaries\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n\n    # No compact\n    dict_no_compact = {i: i for i in range(5)}\n    expected_no_compact = r\"\"\""], "sample_813": ["def test_ard_regression_coef_pruning():\n    # Test that ARDRegression prunes coefficients correctly\n    n_samples, n_features = 100, 10\n    rng = np.random.RandomState(0)\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    # Fit the model\n    clf = ARDRegression(threshold_lambda=1e+2, n_iter=1000)\n    clf.fit(X, y)\n    \n    # Check that some coefficients have been pruned\n    assert np.sum(clf.lambda_ < clf.threshold_lambda) < n_features, (\n        \"No coefficients were pruned\"\n    )\n\n    # Check that the pruned coefficients are effectively zero\n    assert_array_almost_equal(clf.coef_[clf.lambda_ >= clf.threshold_lambda], 0, decimal=6)\n", "def test_bayesian_ridge_normalize():\n    # Test BayesianRidge with normalize parameter\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n    y = np.array([1, 2, 3, 4, 5])\n    \n    # Without normalization\n    clf_no_norm = BayesianRidge(normalize=False)\n    clf_no_norm.fit(X, y)\n    pred_no_norm = clf_no_norm.predict(X)\n\n    # With normalization\n    clf_norm = BayesianRidge(normalize=True)\n    clf_norm.fit(X, y)\n    pred_norm = clf_norm.predict(X)\n\n    # Check that both predictions are almost equal\n    assert_array_almost_equal(pred_no_norm, pred_norm, decimal=5)\n", "def test_bayesian_ridge_predict_with_std():\n    # Test BayesianRidge predict method with return_std=True\n    X = np.array([[1, 2], [2, 4], [3, 6], [4, 8], [5, 10]])\n    y = np.array([1, 2, 3, 4, 5])\n    clf = BayesianRidge(compute_score=True)\n    clf.fit(X, y)\n\n    y_mean, y_std = clf.predict(X, return_std=True)\n\n    # Check that the mean prediction is approximately equal to y\n    assert_array_almost_equal(y_mean, y, decimal=2)\n\n    # Check that the standard deviation is positive and has the right shape\n    assert y_std.shape == y.shape\n    assert np.all(y_std > 0)\n", "def test_bayesian_ridge_with_sample_weights():\n    # Test BayesianRidge with varying sample weights\n    X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n    y = np.array([1, 2, 3, 4, 5])\n    sample_weight = np.array([1, 1, 1, 10, 10])\n\n    # Fit BayesianRidge with sample weights\n    clf = BayesianRidge()\n    clf.fit(X, y, sample_weight=sample_weight)\n    \n    # Check that the coefficients are as expected\n    expected_coefs = np.array([0.999, 0.999])\n    assert_array_almost_equal(clf.coef_, expected_coefs, decimal=2)\n\n    # Check that the intercept is as expected\n    expected_intercept = 0.0\n    assert_almost_equal(clf.intercept_, expected_intercept, decimal=2)\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with fit_intercept set to False\n    X = np.array([[0, 0], [1, 1], [2, 2]])\n    y = np.array([0, 1, 2])\n    clf_no_intercept = BayesianRidge(fit_intercept=False)\n    clf_no_intercept.fit(X, y)\n    assert_almost_equal(clf_no_intercept.intercept_, 0.0)\n    assert_array_almost_equal(clf_no_intercept.predict(X), y)\n    \n    # Test BayesianRidge with fit_intercept set to True\n    clf_with_intercept = BayesianRidge(fit_intercept=True)\n    clf_with_intercept.fit(X, y)\n    assert clf_with_intercept.intercept_ != 0.0\n    assert_array_almost_equal(clf_with_intercept.predict(X), y, decimal=1)\n"], "sample_814": ["def test_warm_start_with_partial_fit():\n    # Test if warm start with partial fitting works as expected.\n    X, y = datasets.make_hastie_10_2(n_samples=200, random_state=1)\n    est = GradientBoostingClassifier(n_estimators=100, max_depth=1, warm_start=True)\n    est.fit(X[:100], y[:100])\n    \n    # Partially fit on the remaining data\n    est.set_params(n_estimators=200)\n    est.fit(X[100:], y[100:])\n    \n    assert est.n_estimators == 200\n    assert len(est.estimators_) == 200\n\n    # Check if predictions are reasonable\n    assert est.score(X, y) > 0.8\n", "def test_mean_estimator():\n    # Check if the MeanEstimator works as expected.\n    estimator = MeanEstimator()\n    X = np.array([[0], [0], [0], [0], [0]])\n    y = np.array([1, 2, 3, 4, 5])\n    \n    # Test fit\n    estimator.fit(X, y)\n    assert_almost_equal(estimator.mean, 3.0)\n\n    # Test predict\n    predictions = estimator.predict(X)\n    assert_array_equal(predictions, np.full((5, 1), 3.0))\n", "def test_zero_estimator_classification():\n    # Test ZeroEstimator in classification tasks.\n    clf = GradientBoostingClassifier(n_estimators=10, init=ZeroEstimator())\n    clf.fit(X, y)\n    y_pred = clf.predict(T)\n    assert len(y_pred) == len(T)\n    assert_array_equal(y_pred, true_result)\n\n    # Test ZeroEstimator in multi-class classification\n    clf = GradientBoostingClassifier(n_estimators=10, init=ZeroEstimator())\n    clf.fit(iris.data, iris.target)\n    y_pred = clf.predict(iris.data)\n    assert len(y_pred) == len(iris.target)\n    assert_array_equal(np.unique(y_pred), np.unique(iris.target))\n", "def test_zero_estimator_raises_not_fitted_error():\n    # Test that using ZeroEstimator before fitting raises NotFittedError.\n    zero_est = ZeroEstimator()\n    X_sample = np.array([[1, 2], [3, 4]])\n    \n    # Expecting NotFittedError since ZeroEstimator is not fitted yet\n    with pytest.raises(NotFittedError):\n        zero_est.predict(X_sample)\n", "def test_gradient_boosting_loss_function_updates():\n    # Test if gradient boosting loss function updates terminal regions correctly\n    from sklearn.tree import DecisionTreeRegressor\n    X, y = datasets.make_regression(n_samples=100, n_features=4, noise=0.1, random_state=42)\n    \n    for loss in ['ls', 'lad', 'huber', 'quantile']:\n        gbr = GradientBoostingRegressor(loss=loss, n_estimators=10, max_depth=2, random_state=42)\n        gbr.fit(X, y)\n        \n        raw_predictions = np.zeros((X.shape[0], 1), dtype=np.float64)\n        sample_weight = np.ones(X.shape[0])\n        sample_mask = np.ones(X.shape[0], dtype=bool)\n        residual = y - raw_predictions.ravel()\n        \n        tree = DecisionTreeRegressor(max_depth=2)\n        tree.fit(X, residual)\n        terminal_regions = tree.apply(X)\n        \n        # update terminal regions\n        gbr.loss_.update_terminal_regions(tree.tree_, X, y, residual, raw_predictions,\n                                          sample_weight, sample_mask, gbr.learning_rate, 0)\n        \n        assert np.any(tree.tree_.value != 0), f\"Loss function {loss} did not update terminal regions\"\n"], "sample_815": ["def test_zero_one_loss():\n    # Test Zero-one loss\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 0, 0, 1]\n\n    # normalize=True\n    loss = zero_one_loss(y_true, y_pred, normalize=True)\n    assert_almost_equal(loss, 1/4)\n\n    # normalize=False\n    loss = zero_one_loss(y_true, y_pred, normalize=False)\n    assert_equal(loss, 1)\n\n    # with sample weights\n    sample_weight = [1, 2, 3, 4]\n    loss = zero_one_loss(y_true, y_pred, normalize=True, sample_weight=sample_weight)\n    assert_almost_equal(loss, 2/(1+2+3+4))\n\n    loss = zero_one_loss(y_true, y_pred, normalize=False, sample_weight=sample_weight)\n    assert_equal(loss, 2)\n\n    # multilabel case with binary label indicators\n    y_true = np.array([[0, 1], [1, 1]])\n    y_pred = np.array([[1, 1], [1, 0]])\n\n    # normalize=True\n    loss = zero_one_loss(y_true, y_pred, normalize=True)\n    assert_almost_equal(loss, 0.5)\n\n    # normalize=False\n    loss = zero_one_loss(y_true, y_pred, normalize=False)\n    assert_equal(loss, 1)\n\n    # with sample weights\n    sample_weight = np.array([2, 3])\n    loss = zero_one_loss(y_true, y_pred, normalize=True, sample_weight=sample_weight)\n    assert_almost_equal(loss, 3/(2+3))\n\n    loss = zero_one_loss(y_true, y_pred, normalize=False, sample_weight=sample_weight)\n    assert_equal(loss, 3)\n", "def test_cohen_kappa_score_edge_cases():\n    # Edge case when all labels are the same\n    y_true = [1, 1, 1, 1]\n    y_pred = [1, 1, 1, 1]\n    assert_almost_equal(cohen_kappa_score(y_true, y_pred), 1.0)\n\n    # Edge case when all predictions are incorrect\n    y_true = [0, 0, 0, 0]\n    y_pred = [1, 1, 1, 1]\n    assert_almost_equal(cohen_kappa_score(y_true, y_pred), -1.0)\n\n    # Edge case when there are no agreements\n    y_true = [0, 1, 2, 3]\n    y_pred = [3, 2, 1, 0]\n    assert_almost_equal(cohen_kappa_score(y_true, y_pred), -0.333, decimal=3)\n\n    # Edge case with one class missing in predictions\n    y_true = [0, 1, 2, 2]\n    y_pred = [0, 1, 1, 1]\n    assert_almost_equal(cohen_kappa_score(y_true, y_pred), 0.153, decimal=3)\n\n    # Edge case with one class missing in true values\n    y_true = [0, 0, 0, 0]\n    y_pred = [0, 0, 1, 1]\n    assert_almost_equal(cohen_kappa_score(y_true, y_pred), 0.0)\n", "def test_fbeta_score_binary():\n    # Test F-beta score for binary classification task\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # detailed measures for each class\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average=None)\n    assert_array_almost_equal(p, [0.73, 0.85], 2)\n    assert_array_almost_equal(r, [0.88, 0.68], 2)\n    assert_array_almost_equal(f, [0.80, 0.76], 2)\n    assert_array_equal(s, [25, 25])\n\n    # individual scoring function that can be used for grid search\n    for beta in [0.5, 1, 2]:\n        fs = assert_no_warnings(fbeta_score, y_true, y_pred, beta=beta, average='binary')\n        beta2 = beta ** 2\n        expected_fbeta = (1 + beta2) * p[1] * r[1] / (beta2 * p[1] + r[1])\n        assert_almost_equal(fs, expected_fbeta, 2)\n", "def test_classification_report_multilabel():\n    # Test classification report for multilabel classification task\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0], [0, 0, 1]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [1, 0, 0], [0, 0, 1]])\n\n    expected_report = \"\"\"\\\n              precision    recall  f1-score   support\n\n           0       1.00      0.50      0.67         2\n           1       0.50      1.00      0.67         2\n           2       0.50      1.00      0.67         2\n\n   micro avg       0.67      0.67      0.67         6\n   macro avg       0.67      0.83      0.67         6", "def test_cohen_kappa_multilabel():\n    # Test cohen_kappa_score in multilabel setting\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [1, 1, 0]])\n\n    # Binarize the labels\n    lb = LabelBinarizer()\n    y_true_bin = lb.fit_transform(y_true)\n    y_pred_bin = lb.transform(y_pred)\n\n    # Calculate the Cohen's kappa score\n    kappa = cohen_kappa_score(y_true_bin.flatten(), y_pred_bin.flatten())\n    expected_kappa = 0.42857142857142855\n\n    assert_almost_equal(kappa, expected_kappa, decimal=5)\n\n    # Test with different labels\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 1]])\n    y_pred = np.array([[1, 0, 0], [0, 0, 1], [1, 1, 1]])\n    y_true_bin = lb.fit_transform(y_true)\n    y_pred_bin = lb.transform(y_pred)\n\n    kappa = cohen_kappa_score(y_true_bin.flatten(), y_pred_bin.flatten())\n    expected_kappa = 0.5000000000000001\n\n    assert_almost_equal(kappa, expected_kappa, decimal=5)\n"], "sample_816": ["def test_strip_accents_unicode_speed():\n    # Check that strip_accents_unicode is significantly slower than strip_accents_ascii\n    import time\n\n    text = \"\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u00f9\u00fa\u00fb\u00fc\u00fd\" * 1000\n\n    start_time = time.time()\n    strip_accents_unicode(text)\n    unicode_time = time.time() - start_time\n\n    start_time = time.time()\n    strip_accents_ascii(text)\n    ascii_time = time.time() - start_time\n\n    # Ensure unicode method is at least 10 times slower than ascii method\n    assert unicode_time > 10 * ascii_time\n", "def test_invalid_strip_accents():\n    # Test invalid values for strip_accents parameter\n    invalid_values = ['invalid', 123, ['ascii']]\n    for invalid_value in invalid_values:\n        vectorizer = CountVectorizer(strip_accents=invalid_value)\n        with pytest.raises(ValueError, match=f'Invalid value for \"strip_accents\": {invalid_value}'):\n            vectorizer.build_preprocessor()\n\n    # Test valid callable for strip_accents\n        return s.replace('\u00e9', 'e').replace('\u00ea', 'e')\n    \n    vectorizer = CountVectorizer(strip_accents=custom_strip_accents)\n    preprocessor = vectorizer.build_preprocessor()\n    assert preprocessor(\"clich\u00e9 \u00e9touff\u00e9e\") == \"cliche etouffee\"\n", "def test_tfidf_transformer_handle_infinities():\n    # Test that TfidfTransformer can handle infinities in input data\n    X = np.array([[1, 1, np.inf], [1, np.nan, 1], [np.inf, 1, 1]])\n    transformer = TfidfTransformer()\n    \n    with pytest.raises(ValueError, match=\"Input contains NaN, infinity or a value too large\"):\n        transformer.fit_transform(X)\n", "def test_strip_tags():\n    # Basic HTML tags\n    s = \"<div>Hello <b>world</b></div>\"\n    expected = \" Hello  world \"\n    assert_equal(strip_tags(s), expected)\n\n    # Nested HTML tags\n    s = \"<div><p>This is a <a href='link'>link</a></p></div>\"\n    expected = \" This is a  link \"\n    assert_equal(strip_tags(s), expected)\n\n    # XML tags\n    s = \"<note><to>Tove</to><from>Jani</from><heading>Reminder</heading><body>Don't forget me this weekend!</body></note>\"\n    expected = \" Tove Jani Reminder Don't forget me this weekend! \"\n    assert_equal(strip_tags(s), expected)\n\n    # No tags\n    s = \"Just a plain text.\"\n    expected = \"Just a plain text.\"\n    assert_equal(strip_tags(s), expected)\n\n    # Empty string\n    s = \"\"\n    expected = \"\"\n    assert_equal(strip_tags(s), expected)\n\n    # HTML comments\n    s = \"<!-- This is a comment -->Hello <!-- another comment -->world\"\n    expected = \" Hello  world\"\n    assert_equal(strip_tags(s), expected)\n", "def test_vectorizer_with_custom_tokenizer():\n    custom_tokenizer = lambda x: re.findall(r'\\b\\w{4}\\b', x)\n\n    # Test CountVectorizer with custom tokenizer\n    cv = CountVectorizer(tokenizer=custom_tokenizer)\n    X = cv.fit_transform([\"This is a test\", \"Another test here\"])\n    expected_feature_names = [\"this\", \"test\", \"here\"]\n    assert_array_equal(cv.get_feature_names(), expected_feature_names)\n    assert_array_equal(X.toarray(), [[1, 1, 0], [0, 1, 1]])\n\n    # Test TfidfVectorizer with custom tokenizer\n    tv = TfidfVectorizer(tokenizer=custom_tokenizer)\n    X_tfidf = tv.fit_transform([\"This is a test\", \"Another test here\"])\n    expected_feature_names_tfidf = [\"this\", \"test\", \"here\"]\n    assert_array_equal(tv.get_feature_names(), expected_feature_names_tfidf)\n    assert_array_almost_equal(X_tfidf.toarray(), [[0.70710678, 0.70710678, 0], [0, 0.70710678, 0.70710678]])\n\n    # Test HashingVectorizer with custom tokenizer\n    hv = HashingVectorizer(tokenizer=custom_tokenizer, n_features=2**4)\n    X_hash = hv.transform([\"This is a test\", \"Another test here\"])\n    assert_equal(X_hash.shape, (2, 16))\n"], "sample_817": ["def test_zero_variance_with_single_sample():\n    # Test VarianceThreshold with single sample input, should raise ValueError.\n    assert_raises(ValueError, VarianceThreshold().fit, [[0, 1, 2, 3, 4]])\n", "def test_all_features_removed():\n    # Test VarianceThreshold when all features have variance below the threshold\n    X = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]  # All features have zero variance\n    for sparse_format in [csr_matrix, csc_matrix, bsr_matrix]:\n        with pytest.raises(ValueError, match=r\"No feature in X meets the variance threshold\"):\n            VarianceThreshold(threshold=0.1).fit(sparse_format(X))\n    with pytest.raises(ValueError, match=r\"No feature in X meets the variance threshold\"):\n        VarianceThreshold(threshold=0.1).fit(X)\n", "def test_negative_threshold():\n    # Test VarianceThreshold with a negative threshold.\n    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n        sel = VarianceThreshold(threshold=-0.1).fit(X)\n        assert_array_equal([0, 1, 2, 3, 4], sel.get_support(indices=True))\n", "def test_negative_threshold():\n    # Test VarianceThreshold with negative threshold (should raise ValueError).\n    with pytest.raises(ValueError, match=\"No feature in X meets the variance threshold\"):\n        VarianceThreshold(threshold=-0.1).fit(data)\n", "def test_variances_attribute():\n    # Test that the `variances_` attribute is correctly computed.\n    sel = VarianceThreshold().fit(data)\n    expected_variances = np.var(data, axis=0)\n    assert_array_equal(expected_variances, sel.variances_)\n\n    # Test with threshold 0 and sparse input\n    sparse_data = csr_matrix(data)\n    sel_sparse = VarianceThreshold().fit(sparse_data)\n    _, sparse_variances = mean_variance_axis(sparse_data, axis=0)\n    assert_array_equal(sparse_variances, sel_sparse.variances_)\n"], "sample_818": ["def test_spectral_clustering_precomputed_affinity():\n    # Test that SpectralClustering works with a precomputed affinity matrix\n    X, y = make_blobs(n_samples=100, random_state=42, centers=3, cluster_std=1.0)\n    D = pairwise_distances(X)  # Distance matrix\n    S = np.exp(-D ** 2 / (2. * 1.0 ** 2))  # RBF (Gaussian) Kernel\n\n    model = SpectralClustering(n_clusters=3, affinity='precomputed', random_state=42)\n    labels = model.fit(S).labels_\n\n    assert adjusted_rand_score(y, labels) > 0.8  # Check the clustering quality\n    assert (X.shape[0],) == labels.shape  # Check shape of output\n", "def test_spectral_clustering_precomputed_affinity():\n    # Test that SpectralClustering works with precomputed affinity matrix\n    X, y = make_blobs(n_samples=100, centers=3, random_state=42)\n    affinity_matrix = rbf_kernel(X, gamma=1.0)\n    \n    model = SpectralClustering(n_clusters=3, affinity='precomputed', random_state=0)\n    labels = model.fit_predict(affinity_matrix)\n    \n    assert labels.shape == (100,)\n    assert len(np.unique(labels)) == 3\n    assert model.affinity_matrix_.shape == (100, 100)\n    \n    # Check if the clustering is reasonably close to the true labels\n    assert adjusted_rand_score(y, labels) > 0.8\n", "def test_spectral_clustering_precomputed_affinity():\n    # Test SpectralClustering with a precomputed affinity matrix\n    X, y = make_blobs(n_samples=30, centers=[[1, 1], [-1, -1], [1, -1]], random_state=42)\n    \n    # Use RBF kernel to create an affinity matrix\n    affinity_matrix = rbf_kernel(X, gamma=0.1)\n    \n    model = SpectralClustering(n_clusters=3, affinity='precomputed', random_state=42)\n    labels = model.fit_predict(affinity_matrix)\n    \n    assert len(np.unique(labels)) == 3\n    assert adjusted_rand_score(y, labels) > 0.8\n", "def test_spectral_clustering_precomputed_affinity():\n    # Test SpectralClustering with precomputed affinity matrix\n    centers = np.array([\n        [1, 1], [1, 2], [1, 3],\n        [10, 10], [10, 11], [10, 12]\n    ])\n    X, true_labels = make_blobs(n_samples=100, centers=centers, cluster_std=0.1, random_state=42)\n    D = pairwise_distances(X)\n    S = np.exp(- D ** 2 / (2. * 1.0 ** 2))  # RBF kernel\n\n    model = SpectralClustering(n_clusters=3, affinity='precomputed', random_state=0)\n    labels = model.fit(S).labels_\n    assert adjusted_rand_score(true_labels, labels) > 0.8\n\n    model_copy = pickle.loads(pickle.dumps(model))\n    assert model_copy.n_clusters == model.n_clusters\n    assert_array_equal(model_copy.labels_, model.labels_)\n", "def test_spectral_clustering_precomputed_affinity():\n    # Test spectral clustering with precomputed affinity matrix\n    centers = np.array([\n        [1, 1],\n        [2, 2],\n        [1, 2],\n        [8, 8],\n        [9, 9],\n        [8, 9],\n    ])\n    X, true_labels = make_blobs(n_samples=300, centers=centers, cluster_std=0.1, random_state=42)\n    S = pairwise_distances(X)  # Distance matrix\n    S = np.max(S) - S  # Similarity matrix\n\n    model = SpectralClustering(n_clusters=6, affinity='precomputed', random_state=42)\n    labels = model.fit(S).labels_\n    \n    assert adjusted_rand_score(true_labels, labels) > 0.8\n\n    model_copy = pickle.loads(pickle.dumps(model))\n    assert model_copy.n_clusters == model.n_clusters\n    assert model_copy.affinity == model.affinity\n    assert_array_equal(model_copy.labels_, model.labels_)\n"], "sample_819": ["def test_voting_regressor_transform():\n    \"\"\"Check transform method of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n\n    ereg1 = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2)]).fit(X, y)\n    transformed = ereg1.transform(X)\n\n    assert_array_equal(transformed.shape, (6, 2))\n    assert_array_almost_equal(transformed[:, 0], reg1.predict(X), decimal=2)\n    assert_array_almost_equal(transformed[:, 1], reg2.predict(X), decimal=2)\n\n    # Test with weights\n    ereg2 = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2)], weights=[1, 2])\n    ereg2.fit(X, y)\n    transformed_weighted = ereg2.transform(X)\n\n    assert_array_equal(transformed_weighted.shape, (6, 2))\n    assert_array_almost_equal(transformed_weighted[:, 0], reg1.predict(X), decimal=2)\n    assert_array_almost_equal(transformed_weighted[:, 1], reg2.predict(X), decimal=2)\n", "def test_voting_regressor_predict():\n    \"\"\"Test the prediction of VotingRegressor on a toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    \n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n    \n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2)])\n    ereg.fit(X, y)\n    pred = ereg.predict(X)\n    \n    # Expected predictions using individual regressors\n    pred_reg1 = reg1.fit(X, y).predict(X)\n    pred_reg2 = reg2.fit(X, y).predict(X)\n    \n    # Weighted average of the predictions\n    expected_pred = np.average([pred_reg1, pred_reg2], axis=0)\n    \n    assert_array_almost_equal(pred, expected_pred, decimal=2)\n", "def test_voting_regressor():\n    \"\"\"Test VotingRegressor on a toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=123)\n    reg3 = DummyRegressor(strategy='mean')\n    \n    X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n    y = np.dot(X, np.array([1, 2])) + 3\n\n    ereg1 = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2), ('dr', reg3)])\n    ereg2 = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)], weights=[2, 1])\n\n    ereg1.fit(X, y)\n    ereg2.fit(X, y)\n\n    pred1 = ereg1.predict(X)\n    pred2 = ereg2.predict(X)\n\n    assert pred1.shape == (4,)\n    assert pred2.shape == (4,)\n    assert_almost_equal(np.mean(pred1), np.mean(y), decimal=1)\n    assert_almost_equal(np.mean(pred2), np.mean(y), decimal=1)\n", "def test_voting_regressor_on_toy_dataset():\n    \"\"\"Check VotingRegressor predictions on a toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16]])\n    y = np.array([2, 3, 4, 5])\n\n    ereg = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)])\n    ereg.fit(X, y)\n    predictions = ereg.predict(X)\n\n    reg1_pred = reg1.fit(X, y).predict(X)\n    reg2_pred = reg2.fit(X, y).predict(X)\n    expected_pred = np.mean([reg1_pred, reg2_pred], axis=0)\n\n    assert_array_almost_equal(predictions, expected_pred, decimal=2)\n", "def test_voting_regressor():\n    \"\"\"Check basic functionality of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    X = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n    y = np.array([2, 4, 6, 8])\n\n    ereg = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)])\n    ereg.fit(X, y)\n    predictions = ereg.predict(X)\n\n    reg1_pred = reg1.fit(X, y).predict(X)\n    reg2_pred = reg2.fit(X, y).predict(X)\n    avg_pred = np.mean(np.vstack([reg1_pred, reg2_pred]), axis=0)\n\n    assert_array_almost_equal(predictions, avg_pred, decimal=1)\n\n    # Test transform method\n    transformed_preds = ereg.transform(X)\n    assert_array_almost_equal(transformed_preds[:, 0], reg1_pred, decimal=1)\n    assert_array_almost_equal(transformed_preds[:, 1], reg2_pred, decimal=1)\n"], "sample_820": ["def test_voting_regressor():\n    \"\"\"Check VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    reg3 = DummyRegressor(strategy='mean')\n\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16]])\n    y = np.array([2, 6, 12, 20])\n\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dummy', reg3)])\n    ereg.fit(X, y)\n\n    pred = ereg.predict(X)\n    expected_pred = np.mean([reg1.fit(X, y).predict(X),\n                             reg2.fit(X, y).predict(X),\n                             reg3.fit(X, y).predict(X)], axis=0)\n    assert_array_almost_equal(pred, expected_pred, decimal=2)\n\n    transformed = ereg.transform(X)\n    expected_transformed = np.column_stack([reg1.predict(X),\n                                            reg2.predict(X),\n                                            reg3.predict(X)])\n    assert_array_almost_equal(transformed, expected_transformed, decimal=2)\n", "def test_set_params_with_none_estimator():\n    \"\"\"Check setting parameters with None estimator\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard', weights=[1, 1, 1])\n\n    eclf.set_params(rf=None)\n    assert 'rf' not in eclf.named_estimators\n    assert eclf.estimators[1][1] is None\n    assert eclf.get_params()['rf'] is None\n\n    eclf.fit(X, y)\n    assert 'rf' not in eclf.named_estimators_\n    assert len(eclf.estimators_) == 2\n    assert all(isinstance(est, (LogisticRegression, GaussianNB))\n               for est in eclf.estimators_)\n    assert 'rf' not in eclf.named_estimators_\n", "def test_predict_transform_consistency():\n    \"\"\"Check consistency between predict and transform methods for VotingClassifier.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard').fit(X, y)\n\n    X_test = np.array([[-1.1, -1.5],\n                       [-1.2, -1.4],\n                       [-3.4, -2.2],\n                       [1.1, 1.2],\n                       [2.1, 1.4],\n                       [3.1, 2.3]])\n\n    y_pred = eclf.predict(X_test)\n    y_transform = eclf.transform(X_test)\n\n    # In 'hard' voting, transform method should return class labels from each classifier\n    assert_array_equal(y_pred, np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=1, arr=y_transform))\n", "def test_voting_regressor():\n    \"\"\"Check VotingRegressor basic functionality.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    reg3 = DummyRegressor(strategy='mean')\n\n    X, y = datasets.make_regression(n_samples=20, n_features=2, random_state=1)\n\n    ereg = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2), ('dr', reg3)])\n    ereg.fit(X, y)\n    pred = ereg.predict(X)\n\n    # Assert that the predictions are arrays and have the same length as input\n    assert isinstance(pred, np.ndarray)\n    assert len(pred) == len(y)\n\n    # Assert that the estimator names are accessible\n    assert 'lr' in ereg.named_estimators\n    assert 'rf' in ereg.named_estimators\n    assert 'dr' in ereg.named_estimators\n\n    # Assert that predictions are close to the actual target values\n    assert_almost_equal(pred, y, decimal=1)\n\n    # Test the transform method\n    transformed = ereg.transform(X)\n    assert transformed.shape == (len(X), 3)\n    assert isinstance(transformed, np.ndarray)\n", "def test_voting_regressor_transform():\n    \"\"\"Check transform method of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    X = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n    y = np.array([1, 2, 3, 4])\n\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2)]).fit(X, y)\n\n    # Ensure the transform method returns predictions for each regressor\n    transform_result = ereg.transform(X)\n    assert transform_result.shape == (4, 2)\n    assert_array_almost_equal(transform_result[:, 0], reg1.predict(X))\n    assert_array_almost_equal(transform_result[:, 1], reg2.predict(X))\n"], "sample_821": ["def test_affinity_propagation_damping_bounds():\n    # Test for damping factor bounds\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S)\n\n    # Test for damping too low\n    with pytest.raises(ValueError, match='damping must be >= 0.5 and < 1'):\n        affinity_propagation(S, preference=preference, damping=0.4)\n\n    # Test for damping too high\n    with pytest.raises(ValueError, match='damping must be >= 0.5 and < 1'):\n        affinity_propagation(S, preference=preference, damping=1.0)\n", "def test_affinity_propagation_damping_values():\n    # Test various damping values within the valid range\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n\n    # Test damping values at the boundary\n    for damping in [0.5, 0.75, 0.99]:\n        cluster_centers_indices, labels = affinity_propagation(\n            S, preference=preference, damping=damping)\n        n_clusters_ = len(cluster_centers_indices)\n        assert_equal(n_clusters, n_clusters_)\n\n    # Test damping value out of the valid range should raise ValueError\n    for damping in [0.4, 1.0]:\n        assert_raises(ValueError, affinity_propagation, S, preference=preference, damping=damping)\n", "def test_affinity_propagation_no_samples():\n    # Test AffinityPropagation with an empty dataset\n    X_empty = np.array([]).reshape(0, 2)  # no samples, 2 features\n    \n    af = AffinityPropagation()\n    assert_raises(ValueError, af.fit, X_empty)\n", "def test_affinity_propagation_damping_range():\n    # Test that damping outside the valid range raises a ValueError\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S)\n\n    # Damping less than 0.5\n    with pytest.raises(ValueError, match=\"damping must be >= 0.5 and < 1\"):\n        affinity_propagation(S, preference=preference, damping=0.4)\n\n    # Damping equal to 1\n    with pytest.raises(ValueError, match=\"damping must be >= 0.5 and < 1\"):\n        affinity_propagation(S, preference=preference, damping=1)\n\n    # Damping greater than 1\n    with pytest.raises(ValueError, match=\"damping must be >= 0.5 and < 1\"):\n        affinity_propagation(S, preference=preference, damping=1.1)\n", "def test_affinity_propagation_damping_value_error():\n    # Test that providing an invalid damping value raises a ValueError\n    S = -euclidean_distances(X, squared=True)\n    invalid_damping_values = [0.4, 1.0, -0.1, 1.1]\n    for damping in invalid_damping_values:\n        assert_raises(ValueError, affinity_propagation, S, damping=damping)\n"], "sample_822": ["def test_invalid_shape_pairwise_distances():\n    # Ensure an error is raised when the shape is invalid for precomputed metric.\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[1, 2]])\n    D = np.array([[1, 0], [0, 1], [1, 1]])\n\n    # Invalid shape when using precomputed metric\n    assert_raises(ValueError, pairwise_distances, D, Y, metric='precomputed')\n    assert_raises(ValueError, pairwise_distances, X, D, metric='precomputed')\n\n    # Valid shape for precomputed metric\n    D_valid = np.array([[1, 0], [0, 1]])\n    result = pairwise_distances(D_valid, metric='precomputed')\n    assert_array_almost_equal(result, D_valid)\n", "def test_pairwise_distances_argmin_min_custom_metric():\n    # Custom distance metric\n        return np.sum(np.abs(x - y) ** 3) ** (1 / 3)\n\n    X = np.array([[0, 1], [1, 1], [2, 2]])\n    Y = np.array([[3, 3], [1, 0], [4, 4]])\n\n    expected_idx = [1, 1, 1]\n    expected_vals = [1.44224957, 0.44224957, 1.44224957]\n\n    idx, vals = pairwise_distances_argmin_min(X, Y, metric=custom_metric)\n\n    assert_array_almost_equal(idx, expected_idx)\n    assert_array_almost_equal(vals, expected_vals)\n", "def test_additive_chi2_kernel():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((10, 4))\n    K = additive_chi2_kernel(X, Y)\n    \n    # Check that the additive chi2 kernel gives expected results\n    for i, x in enumerate(X):\n        for j, y in enumerate(Y):\n            chi2_add = -np.sum((x - y) ** 2 / (x + y))\n            assert_almost_equal(K[i, j], chi2_add)\n    \n    # Check diagonal is zero for data with itself\n    K_self = additive_chi2_kernel(X)\n    assert_array_equal(np.diag(K_self), 0)\n    \n    # Check that float32 is preserved\n    X = rng.random_sample((5, 4)).astype(np.float32)\n    Y = rng.random_sample((10, 4)).astype(np.float32)\n    K = additive_chi2_kernel(X, Y)\n    assert_equal(K.dtype, np.float32)\n    \n    # Check integer type gets converted\n    X = rng.random_sample((5, 4)).astype(np.int32)\n    Y = rng.random_sample((10, 4)).astype(np.int32)\n    K = additive_chi2_kernel(X, Y)\n    assert_equal(K.dtype, np.float)\n    \n    # Check negative input raises error\n    X_neg = X.copy()\n    X_neg[0, 0] = -1\n    assert_raises(ValueError, additive_chi2_kernel, X_neg, Y)\n    \n    # Check different n_features in X and Y raises error\n    assert_raises(ValueError, additive_chi2_kernel, X, Y[:, :3])\n    \n    # Check sparse matrices raise error\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    assert_raises(ValueError, additive_chi2_kernel, X_sparse, Y_sparse)\n", "def test_cosine_distances_sparse():\n    # Check the pairwise Cosine distances computation with sparse input\n    rng = np.random.RandomState(1337)\n    X = np.abs(rng.rand(10, 5))\n    X_sparse = csr_matrix(X)\n    \n    D_dense = cosine_distances(X)\n    D_sparse = cosine_distances(X_sparse)\n    \n    assert_array_almost_equal(D_dense, D_sparse.todense())\n    \n    # Check that the diagonal elements are 0\n    assert_array_almost_equal(D_sparse.diagonal(), np.zeros(D_sparse.shape[0]))\n    \n    # Check that non-diagonal elements are in [0, 2]\n    assert np.all(D_sparse.data >= 0)\n    assert np.all(D_sparse.data <= 2)\n", "def test_pairwise_distances_chunked_multiple_batches():\n    # Ensure pairwise_distances_chunked works correctly with multiple batches\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((100, 4))\n    Y = rng.random_sample((50, 4))\n    \n        return D_chunk\n    \n    # Using a very small working memory to force multiple batches\n    working_memory = 1e-6\n    \n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func, working_memory=working_memory)\n    chunks = list(gen)\n    assert len(chunks) > 1, \"Expected multiple chunks due to small working memory\"\n    \n    # Reconstruct the full distance matrix from chunks and compare with full computation\n    reconstructed_distances = np.vstack(chunks)\n    full_distances = pairwise_distances(X, Y)\n    assert_array_almost_equal(reconstructed_distances, full_distances)\n"], "sample_823": ["def test_return_float_dtype():\n    # Test the _return_float_dtype function\n    X = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    Y = np.array([[5, 6], [7, 8]], dtype=np.float32)\n    X_checked, Y_checked, dtype = _return_float_dtype(X, Y)\n    assert X_checked.dtype == np.float32\n    assert Y_checked.dtype == np.float32\n    assert dtype == np.float32\n\n    X = np.array([[1, 2], [3, 4]], dtype=np.float64)\n    Y = np.array([[5, 6], [7, 8]], dtype=np.float64)\n    X_checked, Y_checked, dtype = _return_float_dtype(X, Y)\n    assert X_checked.dtype == np.float64\n    assert Y_checked.dtype == np.float64\n    assert dtype == np.float64\n\n    # Check the case when Y is None\n    X = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    X_checked, Y_checked, dtype = _return_float_dtype(X, None)\n    assert X_checked.dtype == np.float32\n    assert Y_checked.dtype == np.float32\n    assert dtype == np.float32\n\n    # Check the case when X and Y have different dtypes\n    X = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    Y = np.array([[5, 6], [7, 8]], dtype=np.float64)\n    X_checked, Y_checked, dtype = _return_float_dtype(X, Y)\n    assert X_checked.dtype == np.float32\n    assert Y_checked.dtype == np.float64\n    assert dtype == np.float64\n\n    X = np.array([[1, 2], [3, 4]], dtype=np.float64)\n    Y = np.array([[5, 6], [7, 8]], dtype=np.float32)\n    X_checked, Y_checked, dtype = _return_float_dtype(X, Y)\n    assert X_checked.dtype == np.float64\n    assert Y_checked.dtype == np.float32\n    assert dtype == np.float64\n", "def test_check_pairwise_arrays_with_precomputed():\n    # Test `check_pairwise_arrays` with precomputed distance matrices.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 5))\n\n    # Valid case where X is a precomputed distance matrix\n    XA_checked, XB_checked = check_pairwise_arrays(X, X, precomputed=True)\n    assert_array_equal(XA_checked, X)\n    assert_array_equal(XB_checked, X)\n\n    # Invalid case where the second dimension does not match the first\n    X_invalid = rng.random_sample((5, 3))\n    assert_raises(ValueError, check_pairwise_arrays, X, X_invalid, precomputed=True)\n", "def test_additive_chi2_kernel():\n    # Test the additive chi-squared kernel.\n\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((10, 4))\n\n    K_add = additive_chi2_kernel(X, Y)\n\n    for i, x in enumerate(X):\n        for j, y in enumerate(Y):\n            chi2 = -np.sum((x - y) ** 2 / (x + y))\n            assert_almost_equal(K_add[i, j], chi2)\n\n    # check diagonal is correct for data with itself\n    K_self = additive_chi2_kernel(X)\n    for i in range(len(X)):\n        assert_almost_equal(K_self[i, i], 0)\n\n    # test negative input\n    assert_raises(ValueError, additive_chi2_kernel, [[0, -1]])\n    assert_raises(ValueError, additive_chi2_kernel, [[0, -1]], [[-1, -1]])\n    assert_raises(ValueError, additive_chi2_kernel, [[0, 1]], [[-1, -1]])\n\n    # different n_features in X and Y\n    assert_raises(ValueError, additive_chi2_kernel, [[0, 1]], [[.2, .2, .6]])\n\n    # sparse matrices\n    assert_raises(ValueError, additive_chi2_kernel, csr_matrix(X), csr_matrix(Y))\n", "def test_paired_cosine_distances():\n    # Check the paired cosine distances computation\n    X = [[0, 1], [1, 1]]\n    Y = [[1, 0], [1, 1]]\n    D = paired_cosine_distances(X, Y)\n    expected = [1.0, 0.0]\n    assert_array_almost_equal(D, expected)\n\n    # Check with sparse matrices\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    D_sparse = paired_cosine_distances(X_sparse, Y_sparse)\n    assert_array_almost_equal(D_sparse, expected)\n\n    # Check with normalized input\n    X_normalized = normalize(X, copy=True)\n    Y_normalized = normalize(Y, copy=True)\n    D_normalized = paired_cosine_distances(X_normalized, Y_normalized)\n    assert_array_almost_equal(D_normalized, expected)\n\n    # Check with mismatched shapes\n    Y_mismatched = [[1, 0, 1], [1, 1, 1]]\n    assert_raises(ValueError, paired_cosine_distances, X, Y_mismatched)\n", "def test_paired_cosine_distances():\n    # Check the paired cosine distances computation\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((5, 4))\n\n    S = paired_distances(X, Y, metric='cosine')\n    S2 = paired_cosine_distances(X, Y)\n    assert_array_almost_equal(S, S2)\n\n    S_sparse = paired_cosine_distances(csr_matrix(X), csr_matrix(Y))\n    assert_array_almost_equal(S, S_sparse)\n\n    normalized_X = normalize(X, copy=True)\n    normalized_Y = normalize(Y, copy=True)\n    expected = np.array([np.dot(normalized_X[i], normalized_Y[i]) for i in range(len(X))])\n    assert_array_almost_equal(1 - expected, S)\n"], "sample_824": ["def test_manhattan_distances():\n    # Check the Manhattan distances computation\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n\n    # Manhattan distance (L1) between X and Y\n    D = manhattan_distances(X, Y)\n    D2 = distance.cdist(X, Y, 'cityblock')\n    assert_array_almost_equal(D, D2)\n\n    # Manhattan distance (L1) between X and itself\n    D = manhattan_distances(X, X)\n    D2 = distance.cdist(X, X, 'cityblock')\n    assert_array_almost_equal(D, D2)\n\n    # Test with sparse X and Y\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    D_sparse = manhattan_distances(X_sparse, Y_sparse)\n    assert_array_almost_equal(D_sparse, D2)\n", "def test_invalid_dtype_in_check_pairwise_arrays():\n    # Ensure an error is raised if an invalid dtype is provided.\n    XA = np.resize(np.arange(40), (5, 8)).astype(np.int32)\n    XB = np.resize(np.arange(32), (4, 8)).astype(np.int32)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB, dtype=np.complex128)\n\n    # Ensure an error is raised if dtype mismatch occurs and float32 is expected\n    XA = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n    XB = np.resize(np.arange(40), (5, 8)).astype(np.float64)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB, dtype=np.float32)\n", "def test_pairwise_distances_argmin_min_sparse():\n    # Test pairwise_distances_argmin_min with sparse input\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 5))\n    Y = rng.random_sample((15, 5))\n    X[X < 0.7] = 0\n    Y[Y < 0.7] = 0\n\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n\n    expected_idx, expected_vals = pairwise_distances_argmin_min(X, Y, metric=\"euclidean\")\n    \n    idx, vals = pairwise_distances_argmin_min(X_sparse, Y_sparse, metric=\"euclidean\")\n    assert_array_almost_equal(idx, expected_idx)\n    assert_array_almost_equal(vals, expected_vals)\n\n    idx2 = pairwise_distances_argmin(X_sparse, Y_sparse, metric=\"euclidean\")\n    assert_array_almost_equal(idx2, expected_idx)\n", "def test_pairwise_distances_argmin_min_exceptions():\n    # Test pairwise_distances_argmin_min for exceptions\n\n    X = [[0], [1], [2]]\n    Y = [[0, 1], [1, 2]]\n    \n    # Test dimension mismatch error\n    assert_raises(ValueError, pairwise_distances_argmin_min, X, Y)\n    \n    X = [[0], [1]]\n    Y = [[0, 1], [1, 2]]\n    \n    # Test invalid metric\n    assert_raises(ValueError, pairwise_distances_argmin_min, X, Y, metric=\"invalid_metric\")\n    \n    # Test invalid axis\n    assert_raises(ValueError, pairwise_distances_argmin_min, X, Y, axis=2)\n", "def test_return_float_dtype():\n    # Test the _return_float_dtype function for different input types and values\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4)).astype(np.float32)\n    Y = rng.random_sample((5, 4)).astype(np.float32)\n    \n    X_checked, Y_checked, dtype = _return_float_dtype(X, Y)\n    assert dtype == np.float32\n    assert_array_almost_equal(X, X_checked)\n    assert_array_almost_equal(Y, Y_checked)\n    \n    X = rng.random_sample((5, 4)).astype(np.float)\n    Y = rng.random_sample((5, 4)).astype(np.float)\n    \n    X_checked, Y_checked, dtype = _return_float_dtype(X, Y)\n    assert dtype == np.float\n    assert_array_almost_equal(X, X_checked)\n    assert_array_almost_equal(Y, Y_checked)\n    \n    X = rng.random_sample((5, 4)).astype(np.int)\n    Y = rng.random_sample((5, 4)).astype(np.float)\n    \n    X_checked, Y_checked, dtype = _return_float_dtype(X, Y)\n    assert dtype == np.float\n    assert_array_almost_equal(X.astype(np.float), X_checked)\n    assert_array_almost_equal(Y, Y_checked)\n    \n    X = [[1, 2], [3, 4]]\n    X_checked, Y_checked, dtype = _return_float_dtype(X, None)\n    assert dtype == np.float\n    assert_array_almost_equal(np.array(X, dtype=np.float), X_checked)\n    assert Y_checked is None\n"], "sample_825": ["def test_pls_transform_return_type():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    n_components = 2\n    pls = pls_.PLSCanonical(n_components=n_components)\n    pls.fit(X, Y)\n    \n    # Test transform return type when Y is provided\n    X_transformed, Y_transformed = pls.transform(X, Y)\n    assert isinstance(X_transformed, np.ndarray)\n    assert isinstance(Y_transformed, np.ndarray)\n    \n    # Test transform return type when Y is not provided\n    X_transformed_only = pls.transform(X)\n    assert isinstance(X_transformed_only, np.ndarray)\n    \n    # Check shapes\n    assert X_transformed.shape == (X.shape[0], n_components)\n    assert Y_transformed.shape == (Y.shape[0], n_components)\n    assert X_transformed_only.shape == (X.shape[0], n_components)\n", "def test_pls_transform_return_types():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls = pls_.PLSRegression(n_components=2)\n    pls.fit(X, Y)\n    \n    # Test transform without Y\n    X_scores = pls.transform(X)\n    assert isinstance(X_scores, np.ndarray), \"X_scores should be a numpy array\"\n    assert X_scores.shape == (X.shape[0], pls.n_components), \"X_scores shape mismatch\"\n\n    # Test transform with Y\n    X_scores, Y_scores = pls.transform(X, Y)\n    assert isinstance(X_scores, np.ndarray), \"X_scores should be a numpy array\"\n    assert isinstance(Y_scores, np.ndarray), \"Y_scores should be a numpy array\"\n    assert X_scores.shape == (X.shape[0], pls.n_components), \"X_scores shape mismatch\"\n    assert Y_scores.shape == (Y.shape[0], pls.n_components), \"Y_scores shape mismatch\"\n", "def test_pls_algorithm_validation():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    # Test invalid algorithm parameter\n    invalid_algorithm = 'invalid_algo'\n    for clf_class in [pls_.PLSCanonical, pls_.PLSRegression]:\n        clf = clf_class(n_components=2, algorithm=invalid_algorithm)\n        assert_raise_message(ValueError, \"Got algorithm %s when only 'svd' and 'nipals' are known\" % invalid_algorithm, clf.fit, X, Y)\n\n    # Test invalid mode parameter for PLSCanonical\n    invalid_mode = 'C'\n    pls_canonical = pls_.PLSCanonical(n_components=2, mode=invalid_mode)\n    assert_raise_message(ValueError, \"Incompatible configuration: mode B is not implemented with svd algorithm\", pls_canonical.fit, X, Y)\n\n    # Test invalid deflation mode parameter for _PLS\n    invalid_deflation_mode = 'invalid_deflation'\n    for clf_class in [pls_.PLSCanonical, pls_.PLSRegression]:\n        clf = clf_class(n_components=2)\n        clf.deflation_mode = invalid_deflation_mode\n        assert_raise_message(ValueError, \"The deflation mode is unknown\", clf.fit, X, Y)\n", "def test_pls_transform_inverse():\n    # Test inverse transform functionality to ensure we can revert scores back to original data\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls = pls_.PLSCanonical(n_components=2)\n    pls.fit(X, Y)\n    X_scores = pls.transform(X)\n    X_reconstructed = np.dot(X_scores, pls.x_loadings_.T) * pls.x_std_ + pls.x_mean_\n    \n    # Check that the inverse transformation gives back the original data (up to a tolerance due to potential numerical instability)\n    assert_array_almost_equal(X, X_reconstructed, decimal=4, err_msg=\"Reconstructed X does not match original X\")\n\n    Y_scores = pls.transform(X, Y)[1]\n    Y_reconstructed = np.dot(Y_scores, pls.y_loadings_.T) * pls.y_std_ + pls.y_mean_\n    \n    assert_array_almost_equal(Y, Y_reconstructed, decimal=4, err_msg=\"Reconstructed Y does not match original Y\")\n", "def test_pls_invalid_mode():\n    # Test that an invalid mode raises a ValueError\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    for invalid_mode in [\"C\", \"D\", \"invalid\"]:\n        pls_invalid_mode = pls_.PLSCanonical(n_components=2, mode=invalid_mode)\n        assert_raise_message(ValueError, 'Got algorithm svd when only',\n                             pls_invalid_mode.fit, X, Y)\n"], "sample_826": ["def test_one_hot_encoder_drop_first():\n    # Test OneHotEncoder with drop='first'\n    X = [['a', 1], ['b', 2], ['a', 1], ['b', 3]]\n    enc = OneHotEncoder(drop='first')\n    X_trans = enc.fit_transform(X).toarray()\n    \n    # Expected transformation: dropping the first category in each feature\n    expected = np.array([[0., 1., 1.],\n                         [1., 0., 0.],\n                         [0., 1., 1.],\n                         [1., 0., 0.]])\n    \n    assert_array_equal(X_trans, expected)\n    assert_array_equal(enc.drop_idx_, [0, 0])\n    \n    # Test inverse transform\n    X_inv = enc.inverse_transform(X_trans)\n    expected_inverse = np.array([['a', 1],\n                                 ['b', 2],\n                                 ['a', 1],\n                                 ['b', None]], dtype=object)\n    \n    assert_array_equal(X_inv, expected_inverse)\n", "def test_one_hot_encoder_handle_unknown_drop_combined():\n    X = [['c', 2, 'a'],\n         ['b', 2, 'b']]\n\n    # Test that handle_unknown='ignore' with drop='first' raises an error\n    ohe = OneHotEncoder(handle_unknown='ignore', drop='first')\n    assert_raises_regex(\n        ValueError,\n        \"`handle_unknown` must be 'error' when the drop parameter is specified\",\n        ohe.fit, X)\n\n    # Test that handle_unknown='ignore' with drop specific categories raises an error\n    ohe = OneHotEncoder(handle_unknown='ignore', drop=['c', 2, 'a'])\n    assert_raises_regex(\n        ValueError,\n        \"`handle_unknown` must be 'error' when the drop parameter is specified\",\n        ohe.fit, X)\n\n    # Valid case with handle_unknown='error' and drop='first'\n    ohe = OneHotEncoder(handle_unknown='error', drop='first')\n    X_trans = ohe.fit_transform(X).toarray()\n    expected = np.array([[0, 1, 1], [1, 0, 0]])\n    assert_array_equal(X_trans, expected)\n", "def test_one_hot_encoder_drop_first():\n    # Test drop='first' parameter\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(drop='first')\n    X_trans = enc.fit_transform(X).toarray()\n    expected = np.array([[1., 0., 0., 1.],\n                         [0., 0., 1., 0.],\n                         [0., 1., 0., 1.]])\n    assert_array_equal(X_trans, expected)\n\n    # Check categories and drop_idx_\n    expected_categories = [np.array(['Female', 'Male'], dtype=object), np.array([1, 2, 3], dtype=object)]\n    expected_drop_idx = np.array([0, 0])\n    assert_array_equal(enc.categories_, expected_categories)\n    assert_array_equal(enc.drop_idx_, expected_drop_idx)\n\n    # Test inverse_transform\n    X_inv = enc.inverse_transform(X_trans)\n    expected_inv = np.array(X, dtype=object)\n    assert_array_equal(X_inv, expected_inv)\n", "def test_ordinal_encoder_handle_unknown():\n    # Test that OrdinalEncoder raises error for unknown features during transform.\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])\n    X2 = np.array([[4, 1, 1]])\n\n    # Test that OrdinalEncoder raises error for unknown features during transform.\n    oe = OrdinalEncoder()\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n", "def test_ordinal_encoder_unknown_categories():\n    X = [['abc', 2], ['def', 3]]\n    enc = OrdinalEncoder(categories=[['abc', 'def'], [1, 2, 3]])\n    enc.fit(X)\n\n    # unknown category during transform should raise an error\n    X_unknown = [['abc', 4], ['def', 2]]\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_unknown)\n\n    # setting handle_unknown='ignore' should set unknown category to -1\n    enc = OrdinalEncoder(categories=[['abc', 'def'], [1, 2, 3]], handle_unknown='ignore')\n    X_trans = enc.fit(X).transform(X_unknown)\n    expected_output = np.array([[0, -1], [1, 1]], dtype=enc.dtype)\n    assert_array_equal(X_trans, expected_output)\n"], "sample_827": ["def test_inplace_swap_row_csc():\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csc = sp.csc_matrix(X)\n\n    # Swap first and last rows\n    inplace_swap_row_csc(X_csc, 0, -1)\n    X[[0, -1]] = X[[-1, 0]]\n    assert_array_equal(X_csc.toarray(), X)\n\n    # Swap middle rows\n    inplace_swap_row_csc(X_csc, 1, 3)\n    X[[1, 3]] = X[[3, 1]]\n    assert_array_equal(X_csc.toarray(), X)\n\n    # Test invalid row indices\n    assert_raises(IndexError, inplace_swap_row_csc, X_csc, 0, 5)\n    assert_raises(IndexError, inplace_swap_row_csc, X_csc, -6, 2)\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc, np.array([1]), 2)\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc, 1, np.array([2]))\n", "def test_inplace_swap_row_csc():\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csc = sp.csc_matrix(X)\n\n    swap = linalg.get_blas_funcs(('swap',), (X,))\n    swap = swap[0]\n    X[[0, -1]] = swap(X[[0, -1]])\n    inplace_swap_row_csc(X_csc, 0, -1)\n    assert_array_equal(X, X_csc.toarray())\n\n    X[[2, 3]] = swap(X[[2, 3]])\n    inplace_swap_row_csc(X_csc, 2, 3)\n    assert_array_equal(X, X_csc.toarray())\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc, np.array([0]), 1)\n\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float32)\n    X_csc = sp.csc_matrix(X)\n    swap = linalg.get_blas_funcs(('swap',), (X,))\n    swap = swap[0]\n    X[[0, -1]] = swap(X[[0, -1]])\n    inplace_swap_row_csc(X_csc, 0, -1)\n    assert_array_equal(X, X_csc.toarray())\n    X[[2, 3]] = swap(X[[2, 3]])\n    inplace_swap_row_csc(X_csc, 2, 3)\n    assert_array_equal(X, X_csc.toarray())\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc, np.array([0]), 1)\n", "def test_inplace_csr_column_scale():\n    # Test inplace_csr_column_scale function\n    X = sp.csr_matrix([[1, 2, 0], [4, 0, 6], [7, 8, 9]], dtype=np.float64)\n    scale = np.array([1, 0.5, 2], dtype=np.float64)\n\n    expected_X = np.array([[1, 1, 0], [4, 0, 12], [7, 4, 18]], dtype=np.float64)\n\n    inplace_csr_column_scale(X, scale)\n    assert_array_almost_equal(X.toarray(), expected_X)\n\n    # Test with float32 dtype\n    X = sp.csr_matrix([[1, 2, 0], [4, 0, 6], [7, 8, 9]], dtype=np.float32)\n    scale = np.array([1, 0.5, 2], dtype=np.float32)\n    expected_X = np.array([[1, 1, 0], [4, 0, 12], [7, 4, 18]], dtype=np.float32)\n\n    inplace_csr_column_scale(X, scale)\n    assert_array_almost_equal(X.toarray(), expected_X)\n\n    # Test with mismatched scale\n    X = sp.csr_matrix([[1, 2, 0], [4, 0, 6], [7, 8, 9]], dtype=np.float64)\n    scale = np.array([1, 0.5], dtype=np.float64)\n    with pytest.raises(AssertionError):\n        inplace_csr_column_scale(X, scale)\n", "def test_inplace_swap_row_csc():\n    X = np.array([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9],\n                  [10, 11, 12]], dtype=np.float64)\n    X_csc = sp.csc_matrix(X)\n\n    # Swap rows 0 and 3\n    inplace_swap_row_csc(X_csc, 0, 3)\n    X[[0, 3]] = X[[3, 0]]\n    assert_array_equal(X, X_csc.toarray())\n\n    # Swap rows 1 and 2\n    inplace_swap_row_csc(X_csc, 1, 2)\n    X[[1, 2]] = X[[2, 1]]\n    assert_array_equal(X, X_csc.toarray())\n\n    # Test negative index swap\n    inplace_swap_row_csc(X_csc, -1, 0)\n    X[[3, 0]] = X[[0, 3]]\n    assert_array_equal(X, X_csc.toarray())\n\n    # Test invalid type for row indices\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc, np.array([0]), 1)\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc, 0, np.array([1]))\n", "def test_inplace_csr_column_scale():\n    rng = np.random.RandomState(0)\n    X = sp.rand(100, 200, 0.05, format='csr')\n    X_orig = X.copy()\n    XA = X.toarray()\n    scale = rng.rand(200)\n    XA *= scale\n\n    inplace_csr_column_scale(X, scale)\n    assert_array_almost_equal(XA, X.toarray())\n    assert_raises(AssertionError, assert_array_almost_equal, X_orig.toarray(), X.toarray())\n\n    X = X.astype(np.float32)\n    scale = scale.astype(np.float32)\n    X_orig = X.copy()\n    XA = X.toarray()\n    XA *= scale\n\n    inplace_csr_column_scale(X, scale)\n    assert_array_almost_equal(XA, X.toarray())\n    assert_raises(AssertionError, assert_array_almost_equal, X_orig.toarray(), X.toarray())\n"], "sample_828": ["def test_check_pairwise_arrays_dtype():\n    # Ensure check_pairwise_arrays respects dtype argument.\n    rng = np.random.RandomState(0)\n    XA = rng.random_sample((5, 4))\n    XB = rng.random_sample((5, 4))\n\n    # Default behavior (uses _return_float_dtype)\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert XA_checked.dtype == np.float\n    assert XB_checked.dtype == np.float\n\n    # Specified float32 dtype\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB, dtype=np.float32)\n    assert XA_checked.dtype == np.float32\n    assert XB_checked.dtype == np.float32\n\n    # Specified float64 dtype\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB, dtype=np.float64)\n    assert XA_checked.dtype == np.float64\n    assert XB_checked.dtype == np.float64\n\n    # Specified int dtype (should upcast to float)\n    XA_checked, XB_checked = check_pairwise_arrays(XA.astype(np.int), XB.astype(np.int), dtype=np.int)\n    assert XA_checked.dtype == np.float\n    assert XB_checked.dtype == np.float\n\n    # Ensure error raised on invalid dtype\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB, dtype=\"invalid_dtype\")\n", "def test_rbf_kernel_known_values():\n    # Test the RBF kernel with known values\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n    gamma = 0.1\n\n    # Manually compute the RBF kernel\n    expected_K = np.exp(-gamma * np.array([\n        [32, 72],\n        [8, 32]\n    ]))\n\n    # Use the rbf_kernel function\n    K = rbf_kernel(X, Y, gamma=gamma)\n    assert_array_almost_equal(K, expected_K)\n", "def test_additive_chi2_kernel():\n    # Test the additive chi-squared kernel.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((10, 4))\n    \n    # Test valid input\n    K = additive_chi2_kernel(X, Y)\n    assert K.shape == (5, 10)\n\n    # Test diagonal for same input\n    K_diag = additive_chi2_kernel(X)\n    assert K_diag.shape == (5, 5)\n    assert np.allclose(np.diag(K_diag), 0)\n\n    # Test invalid input: negative values\n    X_neg = X.copy()\n    X_neg[0, 0] = -1\n    with pytest.raises(ValueError, match=\"X contains negative values\"):\n        additive_chi2_kernel(X_neg, Y)\n\n    # Test invalid input: sparse matrices\n    with pytest.raises(ValueError, match=\"additive_chi2 does not support sparse matrices\"):\n        additive_chi2_kernel(csr_matrix(X), csr_matrix(Y))\n", "def test_check_return_float_dtype():\n    # Ensure _return_float_dtype returns expected dtype\n    # when both X and Y are float32, the result should be float32\n    X = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    Y = np.array([[5, 6], [7, 8]], dtype=np.float32)\n    _, _, dtype = _return_float_dtype(X, Y)\n    assert dtype == np.float32\n\n    # when X is float32 and Y is float64, the result should be float\n    X = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    Y = np.array([[5, 6], [7, 8]], dtype=np.float64)\n    _, _, dtype = _return_float_dtype(X, Y)\n    assert dtype == np.float\n\n    # when X is float64 and Y is None, the result should be float\n    X = np.array([[1, 2], [3, 4]], dtype=np.float64)\n    _, _, dtype = _return_float_dtype(X, None)\n    assert dtype == np.float\n\n    # when both X and Y are int, the result should be float\n    X = np.array([[1, 2], [3, 4]], dtype=int)\n    Y = np.array([[5, 6], [7, 8]], dtype=int)\n    _, _, dtype = _return_float_dtype(X, Y)\n    assert dtype == np.float\n\n    # when X is not an array and Y is None, it should convert X to an array\n    X = [[1, 2], [3, 4]]\n    _, _, dtype = _return_float_dtype(X, None)\n    assert dtype == np.float\n", "def test_check_pairwise_arrays_precomputed():\n    # Ensure that precomputed distances are correctly validated.\n    rng = np.random.RandomState(0)\n    XA = rng.random_sample((5, 5))\n    \n    # Check correct precomputed shape\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XA, precomputed=True)\n    assert_array_equal(XA, XA_checked)\n    assert_array_equal(XA, XB_checked)\n    \n    # Check incorrect precomputed shape (n_samples_a != n_features)\n    XA_incorrect = rng.random_sample((5, 4))\n    assert_raises(ValueError, check_pairwise_arrays, XA, XA_incorrect, precomputed=True)\n    \n    # Check non-negative validation for precomputed distances\n    XA_negative = np.full((5, 5), -1)\n    assert_raises(ValueError, check_pairwise_arrays, XA_negative, XA_negative, precomputed=True)\n\n    # Ensure that dtype is preserved when precomputed distances are provided\n    XA_float32 = XA.astype(np.float32)\n    XA_checked, XB_checked = check_pairwise_arrays(XA_float32, XA_float32, precomputed=True)\n    assert_equal(XA_checked.dtype, np.float32)\n    assert_equal(XB_checked.dtype, np.float32)\n"], "sample_829": ["def test_incremental_pca_copy_param():\n    # Test that the copy parameter works as expected.\n    X = iris.data\n    X_copy = X.copy()\n\n    ipca = IncrementalPCA(n_components=2, copy=False)\n    ipca.fit(X)\n    # X should be modified\n    assert not np.array_equal(X, X_copy)\n\n    X = X_copy.copy()\n    ipca = IncrementalPCA(n_components=2, copy=True)\n    ipca.fit(X)\n    # X should not be modified\n    assert np.array_equal(X, X_copy)\n", "def test_incremental_pca_fit_with_different_batch_sizes():\n    # Test the IncrementalPCA fitting with varying batch sizes.\n    rng = np.random.RandomState(1999)\n    X = rng.randn(200, 10)  # generate random data\n\n    n_components = 5\n    batch_sizes = [5, 10, 20, 50, 100]\n\n    for batch_size in batch_sizes:\n        ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n        ipca.fit(X)\n        assert ipca.components_.shape == (n_components, X.shape[1])\n        assert ipca.explained_variance_.shape == (n_components,)\n        assert ipca.explained_variance_ratio_.shape == (n_components,)\n        assert ipca.singular_values_.shape == (n_components,)\n\n    # Test with a batch size larger than the number of samples\n    large_batch_size = X.shape[0] + 10\n    ipca = IncrementalPCA(n_components=n_components, batch_size=large_batch_size)\n    ipca.fit(X)\n    assert ipca.components_.shape == (n_components, X.shape[1])\n    assert ipca.explained_variance_.shape == (n_components,)\n    assert ipca.explained_variance_ratio_.shape == (n_components,)\n    assert ipca.singular_values_.shape == (n_components,)\n", "def test_fit_with_different_dtypes():\n    # Test that IncrementalPCA works with different data types.\n    rng = np.random.RandomState(1999)\n    X = rng.randn(100, 5).astype(np.float64)\n    ipca = IncrementalPCA(n_components=2, batch_size=10)\n    ipca.fit(X)\n    assert ipca.components_.dtype == np.float64\n\n    X = rng.randn(100, 5).astype(np.float32)\n    ipca = IncrementalPCA(n_components=2, batch_size=10)\n    ipca.fit(X)\n    assert ipca.components_.dtype == np.float32\n\n    X = rng.randn(100, 5).astype(np.float64)\n    ipca = IncrementalPCA(n_components=2, batch_size=10)\n    ipca.partial_fit(X)\n    assert ipca.components_.dtype == np.float64\n\n    X = rng.randn(100, 5).astype(np.float32)\n    ipca = IncrementalPCA(n_components=2, batch_size=10)\n    ipca.partial_fit(X)\n    assert ipca.components_.dtype == np.float32\n", "def test_incremental_pca_mean_variance():\n    # Test that IncrementalPCA correctly computes the mean and variance\n    rng = np.random.RandomState(1999)\n    n_samples = 100\n    n_features = 10\n    X = rng.randn(n_samples, n_features)\n\n    ipca = IncrementalPCA(n_components=5, batch_size=20)\n    ipca.fit(X)\n\n    # Compute mean and variance using numpy directly for comparison\n    expected_mean = np.mean(X, axis=0)\n    expected_variance = np.var(X, axis=0, ddof=1)\n\n    # Compare the mean and variance computed by IncrementalPCA\n    assert_array_almost_equal(ipca.mean_, expected_mean, decimal=6)\n    assert_array_almost_equal(ipca.var_, expected_variance, decimal=6)\n", "def test_incremental_pca_copy_param():\n    # Test that the copy parameter works correctly.\n    X = datasets.load_iris().data\n    X_copy = X.copy()\n    \n    # Check that X is not altered when copy=True\n    ipca = IncrementalPCA(n_components=2, batch_size=10, copy=True)\n    ipca.fit(X)\n    assert np.array_equal(X, X_copy), \"X should not be modified when copy=True\"\n    \n    # Check that X is altered when copy=False\n    ipca = IncrementalPCA(n_components=2, batch_size=10, copy=False)\n    ipca.fit(X)\n    assert not np.array_equal(X, X_copy), \"X should be modified when copy=False\"\n"], "sample_830": ["def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n\n    assert 'System:' in captured.out\n    assert 'python' in captured.out\n    assert 'executable' in captured.out\n    assert 'machine' in captured.out\n\n    assert 'BLAS:' in captured.out\n    assert 'macros' in captured.out\n    assert 'lib_dirs' in captured.out\n    assert 'cblas_libs' in captured.out\n\n    assert 'Python deps:' in captured.out\n    assert 'pip' in captured.out\n    assert 'setuptools' in captured.out\n    assert 'sklearn' in captured.out\n    assert 'numpy' in captured.out\n    assert 'scipy' in captured.out\n    assert 'Cython' in captured.out\n    assert 'pandas' in captured.out\n    assert 'matplotlib' in captured.out\n", "def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n\n    assert 'System:' in captured.out\n    assert 'BLAS:' in captured.out\n    assert 'Python deps:' in captured.out\n\n    sys_info = _get_sys_info()\n    for key in sys_info:\n        assert key in captured.out\n\n    deps_info = _get_deps_info()\n    for key in deps_info:\n        assert key in captured.out\n", "def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n\n    assert 'System:' in captured.out\n    assert 'BLAS:' in captured.out\n    assert 'Python deps:' in captured.out\n", "def test_show_versions(capsys):\n    show_versions()\n    \n    captured = capsys.readouterr()\n    assert 'System:' in captured.out\n    assert 'BLAS:' in captured.out\n    assert 'Python deps:' in captured.out\n\n    sys_info_keys = ['python', 'executable', 'machine']\n    for key in sys_info_keys:\n        assert key in captured.out\n\n    deps_info_keys = ['pip', 'setuptools', 'sklearn', 'numpy', 'scipy', 'Cython', 'pandas', 'matplotlib']\n    for key in deps_info_keys:\n        assert key in captured.out\n", "def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n    \n    assert 'System:' in captured.out\n    assert 'BLAS:' in captured.out\n    assert 'Python deps:' in captured.out\n    assert 'python' in captured.out\n    assert 'executable' in captured.out\n    assert 'machine' in captured.out\n    assert 'pip' in captured.out\n    assert 'setuptools' in captured.out\n    assert 'sklearn' in captured.out\n    assert 'numpy' in captured.out\n    assert 'scipy' in captured.out\n    assert 'Cython' in captured.out\n    assert 'pandas' in captured.out\n    assert 'matplotlib' in captured.out\n"], "sample_831": ["def test_color_brew():\n    # Test the _color_brew function for generating colors\n    n_colors = 5\n    colors = _color_brew(n_colors)\n    assert_equal(len(colors), n_colors)\n    for color in colors:\n        assert_equal(len(color), 3)\n        assert all(0 <= c <= 255 for c in color), f\"Color {color} is out of bounds\"\n\n    # Test edge cases\n    colors = _color_brew(1)\n    assert_equal(len(colors), 1)\n    assert_equal(len(colors[0]), 3)\n    assert all(0 <= c <= 255 for c in colors[0]), f\"Color {colors[0]} is out of bounds\"\n\n    colors = _color_brew(0)\n    assert_equal(len(colors), 0)\n", "def test_plot_tree_regressor(pyplot):\n    # mostly smoke tests\n    # Check correctness of plot_tree for DecisionTreeRegressor\n    reg = DecisionTreeRegressor(max_depth=3,\n                                min_samples_split=2,\n                                criterion=\"mse\",\n                                random_state=2)\n    reg.fit(X, y)\n\n    # Test export code\n    feature_names = ['feature 1', 'feature 2']\n    nodes = plot_tree(reg, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"feature 1 <= 0.0\\nmse = 1.0\\n\"\n                                   \"samples = 6\\nvalue = 0.0\")\n    assert nodes[1].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = -1.0\"\n    assert nodes[2].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = 1.0\"\n", "def test_plot_tree_proportion(pyplot):\n    # Test plot_tree with proportion=True\n    clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n    clf.fit(X, y)\n\n    # Test with proportion=True\n    nodes = plot_tree(clf, proportion=True)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"X[1] <= 0.0\\ngini = 0.5\\nsamples = 100.0%\\nvalue = [0.5, 0.5]\")\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 50.0%\\nvalue = [1.0, 0.0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 50.0%\\nvalue = [0.0, 1.0]\"\n\n    # Test with different feature names\n    nodes = plot_tree(clf, feature_names=[\"feat1\", \"feat2\"], proportion=True)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"feat2 <= 0.0\\ngini = 0.5\\nsamples = 100.0%\\nvalue = [0.5, 0.5]\")\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 50.0%\\nvalue = [1.0, 0.0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 50.0%\\nvalue = [0.0, 1.0]\"\n", "def test_plot_tree_with_various_parameters(pyplot):\n    # Check the plot_tree function with various parameters\n    clf = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=2)\n    clf.fit(X, y)\n\n    feature_names = ['feature0', 'feature1']\n    class_names = ['class1', 'class2']\n\n    # Test with filled and rounded parameters\n    nodes = plot_tree(clf, feature_names=feature_names, class_names=class_names,\n                      filled=True, rounded=True, impurity=False, node_ids=True)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"feature0 <= 0.0\\nsamples = 6\\nvalue = [3, 3]\\nclass = class1\")\n    assert nodes[1].get_text() == \"samples = 3\\nvalue = [3, 0]\\nclass = class1\"\n    assert nodes[2].get_text() == \"samples = 3\\nvalue = [0, 3]\\nclass = class2\"\n\n    # Test with rotated tree\n    nodes = plot_tree(clf, feature_names=feature_names, class_names=class_names,\n                      rotate=True, proportion=True, precision=2)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"feature0 <= 0.00\\nsamples = 100.00%\\nvalue = [0.50, 0.50]\\nclass = class1\")\n    assert nodes[1].get_text() == \"samples = 50.00%\\nvalue = [1.00, 0.00]\\nclass = class1\"\n    assert nodes[2].get_text() == \"samples = 50.00%\\nvalue = [0.00, 1.00]\\nclass = class2\"\n\n    # Test with fontsize parameter\n    nodes = plot_tree(clf, feature_names=feature_names, class_names=class_names,\n                      fontsize=10)\n    for node in nodes:\n        assert node.get_fontsize() == 10\n", "def test_plot_tree():\n    # Test plot_tree function\n    clf = DecisionTreeClassifier(max_depth=3, random_state=2)\n    clf.fit(X, y)\n\n    import matplotlib.pyplot as plt\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    plot_tree(clf, ax=ax, filled=True, feature_names=[\"feature1\", \"feature2\"], \n              class_names=[\"class1\", \"class2\"], precision=2, proportion=True, \n              rounded=True, fontsize=10)\n\n    # Check the number of annotations\n    annotations = [child for child in ax.get_children() if isinstance(child, plt.Annotation)]\n    assert len(annotations) == 3\n\n    # Check the content of the root node annotation\n    assert annotations[0].get_text() == (\"feature1 <= 0.0\\n\"\n                                         \"gini = 0.5\\n\"\n                                         \"samples = 100.0%\\n\"\n                                         \"value = [0.5, 0.5]\\n\"\n                                         \"class = class1\")\n\n    # Check the content of the first child node annotation\n    assert annotations[1].get_text() == (\"gini = 0.0\\n\"\n                                         \"samples = 50.0%\\n\"\n                                         \"value = [1.0, 0.0]\\n\"\n                                         \"class = class1\")\n\n    # Check the content of the second child node annotation\n    assert annotations[2].get_text() == (\"gini = 0.0\\n\"\n                                         \"samples = 50.0%\\n\"\n                                         \"value = [0.0, 1.0]\\n\"\n                                         \"class = class2\")\n\n    plt.close(fig)\n"], "sample_832": ["def test_bayesian_ridge_predict_std():\n    # Test that the standard deviation of the predictions is correctly computed\n    X = np.array([[1], [2], [3], [4], [5]])\n    y = np.array([1, 2, 3, 4, 5])\n    \n    clf = BayesianRidge()\n    clf.fit(X, y)\n    \n    y_mean, y_std = clf.predict(X, return_std=True)\n    \n    # Check that the standard deviation is non-negative\n    assert np.all(y_std >= 0)\n    \n    # Check that the standard deviation is relatively small since the target is linear\n    assert np.all(y_std < 1)\n", "def test_ard_lambda_pruning():\n    # Test ARDRegression pruning mechanism for weights with high precision\n    X = np.array([[1, 0], [0, 1], [1, 1]])\n    y = np.array([1, 2, 3])\n    \n    # Setting a low threshold for lambda to force pruning\n    clf = ARDRegression(threshold_lambda=1.0)\n    clf.fit(X, y)\n\n    # Check if lambda values greater than threshold are pruned\n    assert np.sum(clf.lambda_ < 1.0) == np.sum(clf.coef_ != 0)\n    assert np.sum(clf.lambda_ >= 1.0) == np.sum(clf.coef_ == 0)\n\n    # Ensure that pruned weights are set to zero\n    assert_array_almost_equal(clf.coef_[clf.lambda_ >= 1.0], 0)\n\n    # Ensure that non-pruned weights are not set to zero\n    assert np.all(clf.coef_[clf.lambda_ < 1.0] != 0)\n", "def test_ard_pruning():\n    # Test that ARDRegression correctly prunes irrelevant features\n    X = np.array([[1, 0, 0], [2, 0, 0], [3, 0, 0], [4, 0, 0]])\n    y = np.array([1, 2, 3, 4])\n    clf = ARDRegression(threshold_lambda=1e+3)\n    clf.fit(X, y)\n\n    # Only the first feature should remain after pruning\n    assert clf.coef_[0] != 0\n    assert clf.coef_[1] == 0\n    assert clf.coef_[2] == 0\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with fit_intercept=True and fit_intercept=False\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n\n    # With intercept\n    clf_with_intercept = BayesianRidge(fit_intercept=True)\n    clf_with_intercept.fit(X, y)\n    y_pred_with_intercept = clf_with_intercept.predict(X)\n    \n    # Without intercept\n    clf_without_intercept = BayesianRidge(fit_intercept=False)\n    clf_without_intercept.fit(X, y)\n    y_pred_without_intercept = clf_without_intercept.predict(X)\n\n    # Check that predictions are different\n    assert np.any(np.abs(y_pred_with_intercept - y_pred_without_intercept) > 1e-3)\n\n    # Check that the intercept is not zero when fit_intercept=True\n    assert clf_with_intercept.intercept_ != 0.0\n\n    # Check that the intercept is zero when fit_intercept=False\n    assert clf_without_intercept.intercept_ == 0.0\n", "def test_bayesian_ridge_convergence_warning():\n    # Test that a warning is raised if the algorithm does not converge\n    from sklearn.exceptions import ConvergenceWarning\n    X, y = diabetes.data, diabetes.target\n    clf = BayesianRidge(n_iter=1, tol=1e-10)  # Force non-convergence\n    with pytest.warns(ConvergenceWarning):\n        clf.fit(X, y)\n"], "sample_833": ["def test_logistic_regression_path_ovr():\n    # Test logistic_regression_path function with OvR setup\n    X, y = make_classification(n_samples=50, n_features=5, random_state=0)\n    Cs = [0.1, 1, 10]\n    coefs, Cs_, n_iter = _logistic_regression_path(\n        X, y, Cs=Cs, fit_intercept=True, solver='lbfgs', multi_class='ovr',\n        random_state=0\n    )\n    \n    # Check that the returned coefs, Cs, and n_iter have the expected shapes\n    assert coefs.shape == (len(Cs), X.shape[1] + 1)\n    assert_array_almost_equal(Cs, Cs_)\n    assert n_iter.shape == (len(Cs),)\n    \n    # Fit a LogisticRegression model for each C and compare coefficients\n    for i, C in enumerate(Cs):\n        lr = LogisticRegression(C=C, fit_intercept=True, solver='lbfgs', multi_class='ovr', random_state=0)\n        lr.fit(X, y)\n        lr_coef = np.concatenate([lr.coef_.ravel(), lr.intercept_])\n        assert_array_almost_equal(lr_coef, coefs[i], decimal=4)\n\n    # Test convergence warning\n    with pytest.warns(ConvergenceWarning):\n        _logistic_regression_path(X, y, Cs=[0.1], max_iter=1, tol=1e-20, solver='lbfgs', multi_class='ovr', random_state=0)\n", "def test_logistic_regression_path_multinomial():\n    # Test the logistic_regression_path function with multinomial loss\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3,\n                               n_informative=15, random_state=0)\n    Cs = np.logspace(-4, 4, 3)\n    coefs, Cs, n_iter = _logistic_regression_path(X, y, Cs=Cs,\n                                                  solver='lbfgs',\n                                                  multi_class='multinomial',\n                                                  random_state=0)\n    assert coefs.shape == (len(Cs), 3, X.shape[1])\n    assert n_iter.shape == (len(Cs),)\n    assert np.all(n_iter > 0)\n", "def test_logistic_regression_path_multinomial():\n    # Test the logistic_regression_path function with multinomial loss\n    X, y = make_classification(n_samples=100, n_features=5, n_classes=3, n_informative=3, random_state=0)\n    Cs = [1, 10, 100]\n\n    # Use logistic_regression_path directly\n    coefs, Cs_out, n_iter = _logistic_regression_path(X, y, Cs=Cs, fit_intercept=True, tol=1e-4, \n                                                      solver='lbfgs', multi_class='multinomial', max_iter=1000)\n\n    assert len(coefs) == len(Cs)\n    assert np.allclose(Cs_out, Cs)\n    assert n_iter.shape == (len(Cs),)\n\n    # Ensure the coefficients for different Cs are not identical\n    assert not np.allclose(coefs[0], coefs[1])\n    assert not np.allclose(coefs[1], coefs[2])\n", "def test_logistic_regression_path_check_solver():\n    # Test that _check_solver function raises appropriate ValueErrors\n    # based on solver, penalty, and dual inputs.\n\n    # Invalid solver\n    with pytest.raises(ValueError, match=\"Logistic Regression supports only solvers\"):\n        _check_solver('invalid_solver', 'l2', False)\n\n    # Invalid penalty\n    with pytest.raises(ValueError, match=\"Logistic Regression supports only penalties\"):\n        _check_solver('lbfgs', 'invalid_penalty', False)\n\n    # Incompatible solver and penalty\n    with pytest.raises(ValueError, match=\"Solver lbfgs supports only 'l2' or 'none' penalties\"):\n        _check_solver('lbfgs', 'l1', False)\n\n    # Solver 'lbfgs' with dual=True\n    with pytest.raises(ValueError, match=\"Solver lbfgs supports only dual=False\"):\n        _check_solver('lbfgs', 'l2', True)\n\n    # Solver 'liblinear' with penalty='none'\n    with pytest.raises(ValueError, match=\"penalty='none' is not supported for the liblinear solver\"):\n        _check_solver('liblinear', 'none', False)\n\n    # Solver 'liblinear' with penalty='elasticnet'\n    with pytest.raises(ValueError, match=\"Only 'saga' solver supports elasticnet penalty\"):\n        _check_solver('liblinear', 'elasticnet', False)\n", "def test_logistic_regression_path_multiclass():\n    # Test _logistic_regression_path with multi_class='multinomial'\n    X, y = make_classification(n_samples=100, n_classes=3, n_informative=5,\n                               random_state=0)\n    Cs = [0.1, 1.0, 10.0]\n    coefs, Cs_, n_iter = _logistic_regression_path(X, y, Cs=Cs,\n                                                   multi_class='multinomial',\n                                                   solver='lbfgs', max_iter=100)\n    assert_array_equal(Cs, Cs_)\n    assert coefs.shape[0] == len(Cs)\n    assert coefs.shape[1] == len(np.unique(y))\n    assert coefs.shape[2] == X.shape[1] + 1\n    assert n_iter.shape[0] == len(Cs)\n"], "sample_834": ["def test_fit_transform():\n    \"\"\"Test the fit_transform method\n\n    Ensure that the fit_transform method returns the same result as calling\n    fit followed by transform.\n    \"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    \n    nca = NeighborhoodComponentsAnalysis()\n    X_fit_transform = nca.fit_transform(X, y)\n    \n    nca = NeighborhoodComponentsAnalysis()\n    nca.fit(X, y)\n    X_separate = nca.transform(X)\n    \n    assert_array_almost_equal(X_fit_transform, X_separate)\n", "def test_different_random_states():\n    \"\"\"Test the effect of different random states on the initial transformation.\"\"\"\n    X, y = make_classification(n_samples=50, n_features=5, n_classes=3, random_state=0)\n\n    random_state_1 = 0\n    random_state_2 = 42\n\n    nca_1 = NeighborhoodComponentsAnalysis(init='random', random_state=random_state_1)\n    nca_2 = NeighborhoodComponentsAnalysis(init='random', random_state=random_state_2)\n\n    nca_1.fit(X, y)\n    nca_2.fit(X, y)\n\n    assert not np.array_equal(nca_1.components_, nca_2.components_), \\\n        \"Components should be different for different random states\"\n\n    # Check if the results are consistent when using the same random state\n    nca_3 = NeighborhoodComponentsAnalysis(init='random', random_state=random_state_1)\n    nca_3.fit(X, y)\n\n    assert_array_almost_equal(nca_1.components_, nca_3.components_), \\\n        \"Components should be the same for the same random state\"\n", "def test_transform_without_fit():\n    # Test calling transform before fitting raises an error\n    X = np.arange(12).reshape(4, 3)\n    nca = NeighborhoodComponentsAnalysis()\n    \n    with pytest.raises(ValueError, match=\"This NeighborhoodComponentsAnalysis instance is not fitted yet.\"):\n        nca.transform(X)\n", "def test_fit_transform():\n    \"\"\"Test the fit_transform method and ensure it provides the same output\n    as calling fit followed by transform separately.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=5, n_classes=3, \n                               n_informative=3, n_redundant=0, random_state=42)\n    nca = NeighborhoodComponentsAnalysis(n_components=3, random_state=42)\n    \n    # Apply fit_transform\n    X_t = nca.fit_transform(X, y)\n    \n    # Apply fit followed by transform\n    nca.fit(X, y)\n    X_t_separate = nca.transform(X)\n    \n    assert_array_almost_equal(X_t, X_t_separate)\n", "def test_transform_not_fitted_error():\n    \"\"\"Test that transform method raises NotFittedError if fit is not called.\"\"\"\n    X = np.array([[0, 0], [0, 1], [2, 0], [2, 1]])\n    nca = NeighborhoodComponentsAnalysis()\n    with pytest.raises(RuntimeError, match=\"This NeighborhoodComponentsAnalysis instance is not fitted yet\"):\n        nca.transform(X)\n"], "sample_835": ["def test_base_estimator_no_sample_weight():\n    \"\"\"\n    Test that a base estimator without support for sample_weight raises a ValueError.\n    \"\"\"\n    class NoSampleWeightEstimator(BaseEstimator):\n            pass\n        \n            return np.zeros(X.shape[0])\n    \n    clf = AdaBoostClassifier(base_estimator=NoSampleWeightEstimator())\n    with pytest.raises(ValueError, match=\"doesn't support sample_weight\"):\n        clf.fit(X, y_class)\n\n    reg = AdaBoostRegressor(base_estimator=NoSampleWeightEstimator())\n    with pytest.raises(ValueError, match=\"doesn't support sample_weight\"):\n        reg.fit(X, y_regr)\n", "def test_adaboost_classifier_with_custom_loss():\n    \"\"\"\n    Test AdaBoostClassifier with a custom loss function.\n    This ensures that the classifier can handle custom loss functions and\n    the weights update accordingly.\n    \"\"\"\n    class CustomLossEstimator(BaseEstimator):\n            self.classes_, y = np.unique(y, return_inverse=True)\n            self.n_classes_ = len(self.classes_)\n            self.sample_weight = sample_weight\n\n            return np.zeros(X.shape[0], dtype=int)\n\n            proba = np.ones((X.shape[0], self.n_classes_)) / self.n_classes_\n            return proba\n\n    class CustomLossAdaBoostClassifier(AdaBoostClassifier):\n            estimator = self._make_estimator(random_state=random_state)\n            estimator.fit(X, y, sample_weight=sample_weight)\n            estimator_error = np.mean(np.average((estimator.predict(X) != y), weights=sample_weight, axis=0))\n            if estimator_error <= 0:\n                return sample_weight, 1., 0.\n            estimator_weight = self.learning_rate * (np.log((1 - estimator_error) / estimator_error) + np.log(self.n_classes_ - 1.))\n            if not iboost == self.n_estimators - 1:\n                sample_weight *= np.exp(estimator_weight * (estimator.predict(X) != y))\n            return sample_weight, estimator_weight, estimator_error\n\n    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n    clf = CustomLossAdaBoostClassifier(base_estimator=CustomLossEstimator(), n_estimators=10, random_state=0)\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    assert clf.score(X_test, y_test) > 0.9\n    assert len(clf.estimators_) == 10\n    assert_array_equal(predictions, clf.staged_predict(X_test)[-1])\n", "def test_adaboost_classifier_feature_importances():\n    # Test feature importances for AdaBoostClassifier.\n    X, y = datasets.make_classification(n_samples=2000, n_features=10,\n                                        n_informative=3, n_redundant=0,\n                                        n_repeated=0, shuffle=False, random_state=1)\n\n    clf = AdaBoostClassifier()\n    clf.fit(X, y)\n    \n    importances = clf.feature_importances_\n    assert importances.shape[0] == 10\n    assert (importances[:3] >= importances[3:]).all()\n\n    # Test feature importances for AdaBoostRegressor\n    X, y = datasets.make_regression(n_samples=2000, n_features=10,\n                                    n_informative=3, noise=0.0, random_state=1)\n\n    regr = AdaBoostRegressor()\n    regr.fit(X, y)\n\n    importances = regr.feature_importances_\n    assert importances.shape[0] == 10\n    assert (importances[:3] >= importances[3:]).all()\n", "def test_adaboost_classifier_learning_rate():\n    # Check that AdaBoostClassifier raises ValueError for non-positive learning_rate\n    clf = AdaBoostClassifier(learning_rate=0)\n    with pytest.raises(ValueError, match=\"learning_rate must be greater than zero\"):\n        clf.fit(X, y_class)\n    \n    clf = AdaBoostClassifier(learning_rate=-1)\n    with pytest.raises(ValueError, match=\"learning_rate must be greater than zero\"):\n        clf.fit(X, y_class)\n", "def test_adaboost_classifier_with_custom_base_estimator():\n    \"\"\"\n    Test AdaBoostClassifier with a custom base estimator that doesn't support\n    sample weights to ensure it raises an appropriate error.\n    \"\"\"\n    class NoSampleWeightEstimator(BaseEstimator):\n            pass\n\n            return np.zeros(X.shape[0])\n\n    clf = AdaBoostClassifier(base_estimator=NoSampleWeightEstimator(), algorithm='SAMME')\n    with pytest.raises(ValueError, match=\"doesn't support sample_weight\"):\n        clf.fit(X, y_class)\n"], "sample_836": ["def test_ovr_decision_function():\n    n_samples = 5\n    n_classes = 3\n    n_classifiers = int(n_classes * (n_classes - 1) / 2)\n    \n    predictions = np.random.randint(0, 2, size=(n_samples, n_classifiers))\n    confidences = np.random.randn(n_samples, n_classifiers)\n\n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n\n    assert decision_function.shape == (n_samples, n_classes)\n    assert np.all(decision_function >= -1/3) and np.all(decision_function <= 1/3 + n_classes - 1)\n    \n    # Check tie-breaking behavior\n    confidences_zero = np.zeros_like(confidences)\n    decision_function_zero = _ovr_decision_function(predictions, confidences_zero, n_classes)\n    assert np.all(decision_function_zero == np.sum(predictions == np.arange(n_classes)[None, :, None], axis=-1))\n", "def test_ovr_decision_function():\n    # Test _ovr_decision_function with a simple example\n    predictions = np.array([[0, 1, 1], \n                            [1, 0, 0], \n                            [0, 0, 1]])\n    confidences = np.array([[0.1, 0.3, 0.2], \n                            [0.5, 0.4, 0.6], \n                            [0.2, 0.1, 0.3]])\n    n_classes = 3\n\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    expected_decision = np.array([[1.03333333, 1.1, -0.13333333],\n                                  [1.06666667, -0.1, 1.03333333],\n                                  [0.96666667, 1.03333333, 1.0]])\n    \n    assert_array_almost_equal(decision, expected_decision)\n\n    # Test with another set of predictions and confidences\n    predictions = np.array([[1, 0, 1], \n                            [0, 1, 0], \n                            [1, 0, 1]])\n    confidences = np.array([[0.5, 0.2, 0.3], \n                            [0.1, 0.4, 0.6], \n                            [0.7, 0.8, 0.9]])\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    expected_decision = np.array([[-0.03333333, 1.06666667, 2.03333333],\n                                  [2.1, 0.06666667, -0.03333333],\n                                  [-0.03333333, 1.06666667, 2.03333333]])\n    \n    assert_array_almost_equal(decision, expected_decision)\n", "def test_ovr_decision_function():\n    # Test a basic case with 3 classes\n    predictions = np.array([[0, 0, 1],\n                            [1, 0, 1],\n                            [1, 1, 0]])\n    confidences = np.array([[0.5, 0.2, 0.8],\n                            [0.6, 0.1, 0.7],\n                            [0.4, 0.3, 0.9]])\n\n    n_classes = 3\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n\n    expected_votes = np.array([[2, 1, 1],\n                               [1, 2, 1],\n                               [1, 1, 2]])\n    expected_sum_of_confidences = np.array([[-0.7,  0.5,  0.2],\n                                            [-0.8,  0.7,  0.1],\n                                            [-0.7,  0.6,  0.1]])\n    expected_transformed_confidences = expected_sum_of_confidences / (3 * (np.abs(expected_sum_of_confidences) + 1))\n    expected_decision = expected_votes + expected_transformed_confidences\n\n    assert_array_almost_equal(decision, expected_decision)\n\n    # Test with 4 classes\n    predictions = np.array([[0, 0, 1, 1, 0, 2],\n                            [1, 1, 0, 2, 2, 1],\n                            [1, 2, 0, 0, 1, 2]])\n    confidences = np.array([[0.5, 0.2, 0.8, 0.9, 0.4, 0.7],\n                            [0.6, 0.1, 0.7, 0.3, 0.5, 0.2],\n                            [0.4, 0.3, 0.9, 0.6, 0.8, 0.1]])\n\n    n_classes = 4\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n\n    expected_votes = np.array([[3, 1, 1, 1],\n                               [1, 3, ", "def test_ovr_decision_function():\n    # Test _ovr_decision_function with some example data\n    predictions = np.array([\n        [0, 1, 0],\n        [1, 0, 1],\n        [0, 1, 1],\n        [1, 0, 0]\n    ])\n    confidences = np.array([\n        [0.1, 0.4, 0.3],\n        [0.7, 0.2, 0.5],\n        [0.3, 0.8, 0.6],\n        [0.6, 0.1, 0.4]\n    ])\n    n_classes = 3\n\n    expected_decision_function = np.array([\n        [-0.1, 2.3, 0.43333333],\n        [1.1, -0.9, 1.23333333],\n        [-0.7, 2.93333333, 2.06666667],\n        [1.3, -0.73333333, -0.43333333]\n    ])\n\n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n    assert_array_almost_equal(decision_function, expected_decision_function)\n", "def test_ovr_decision_function():\n    # Test the _ovr_decision_function with synthetic data\n    predictions = np.array([[0, 1, 0],\n                            [1, 1, 0],\n                            [0, 0, 1],\n                            [1, 0, 1]])\n    confidences = np.array([[0.1, 0.4, 0.3],\n                            [0.5, 0.2, 0.6],\n                            [0.3, 0.4, 0.7],\n                            [0.8, 0.1, 0.2]])\n    n_classes = 3\n\n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n\n    expected_votes = np.array([[2, 1, 0],\n                               [0, 2, 1],\n                               [2, 0, 1],\n                               [1, 1, 1]])\n    expected_confidences = np.array([[0.1/3.3, 0.4/3.4, 0.3/3.3],\n                                     [0.5/3.5, 0.2/3.2, 0.6/3.6],\n                                     [0.3/3.3, 0.4/3.4, 0.7/3.7],\n                                     [0.8/3.8, 0.1/3.1, 0.2/3.2]])\n\n    expected_sum_of_confidences = np.array([[2 + 0.1/3.3, 1 + 0.4/3.4, 0 + 0.3/3.3],\n                                            [0 + 0.5/3.5, 2 + 0.2/3.2, 1 + 0.6/3.6],\n                                            [2 + 0.3/3.3, 0 + 0.4/3.4, 1 + 0.7/3.7],\n                                            [1 + 0.8/3.8, 1 + 0.1/3.1, 1 + 0.2/3.2]])\n\n    assert_array_almost_equal(decision_function, expected_sum_of_confidences)\n"], "sample_837": ["def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n    \n    # Check for some expected output segments in the captured output\n    assert 'System:' in captured.out\n    assert 'BLAS:' in captured.out\n    assert 'Python deps:' in captured.out\n    assert 'python' in captured.out\n    assert 'executable' in captured.out\n    assert 'machine' in captured.out\n    assert 'pip' in captured.out\n    assert 'setuptools' in captured.out\n    assert 'sklearn' in captured.out\n    assert 'numpy' in captured.out\n    assert 'scipy' in captured.out\n    assert 'Cython' in captured.out\n    assert 'pandas' in captured.out\n    assert 'matplotlib' in captured.out\n    assert 'joblib' in captured.out\n", "def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n\n    assert \"System:\" in captured.out\n    assert \"BLAS:\" in captured.out\n    assert \"Python deps:\" in captured.out\n\n    sys_info = _get_sys_info()\n    for k in sys_info:\n        assert k in captured.out\n\n    deps_info = _get_deps_info()\n    for k in deps_info:\n        assert k in captured.out\n\n    from sklearn.utils._build_utils import get_blas_info\n    cblas_libs, blas_dict = get_blas_info()\n    for k, v in blas_dict.items():\n        if isinstance(v, list) and k == \"define_macros\":\n            for item in v:\n                key, val = item\n                assert f\"{key}={val}\" in captured.out\n        else:\n            assert k in captured.out\n", "def test_show_versions(capsys):\n    show_versions()\n\n    captured = capsys.readouterr()\n    output = captured.out\n\n    assert \"System:\" in output\n    assert \"BLAS:\" in output\n    assert \"Python deps:\" in output\n\n    sys_info = _get_sys_info()\n    for key in sys_info:\n        assert key in output\n\n    blas_info = _get_blas_info()\n    for key in blas_info:\n        assert key in output\n\n    deps_info = _get_deps_info()\n    for key in deps_info:\n        assert key in output\n", "def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n\n    assert 'System:' in captured.out\n    assert 'BLAS:' in captured.out\n    assert 'Python deps:' in captured.out\n\n    sys_info_keys = ['python', 'executable', 'machine']\n    for key in sys_info_keys:\n        assert key in captured.out\n\n    deps_info_keys = ['pip', 'setuptools', 'sklearn', 'numpy', 'scipy', 'Cython', 'pandas', 'matplotlib', 'joblib']\n    for key in deps_info_keys:\n        assert key in captured.out\n", "def test_show_versions(capsys):\n    show_versions()\n    captured = capsys.readouterr()\n\n    assert 'System:' in captured.out\n    assert 'BLAS:' in captured.out\n    assert 'Python deps:' in captured.out\n    assert 'python' in captured.out\n    assert 'executable' in captured.out\n    assert 'machine' in captured.out\n    assert 'pip' in captured.out\n    assert 'setuptools' in captured.out\n    assert 'sklearn' in captured.out\n    assert 'numpy' in captured.out\n    assert 'scipy' in captured.out\n    assert 'Cython' in captured.out\n    assert 'pandas' in captured.out\n    assert 'matplotlib' in captured.out\n    assert 'joblib' in captured.out\n"], "sample_838": ["def test_column_transformer_with_mixed_dtypes():\n    # Test that ColumnTransformer can handle mixed data types\n    X = np.array([[1, 'a', 2.5],\n                  [2, 'b', 3.5],\n                  [3, 'c', 4.5]], dtype=object)\n    \n    ct = ColumnTransformer(\n        [('num', StandardScaler(), [0, 2]),\n         ('cat', OneHotEncoder(), [1])]\n    )\n\n    transformed = ct.fit_transform(X)\n    expected_transformed = np.array([[-1.22474487, -1.22474487, 1., 0., 0.],\n                                     [0., 0., 0., 1., 0.],\n                                     [1.22474487, 1.22474487, 0., 0., 1.]])\n    \n    assert_allclose_dense_sparse(transformed, expected_transformed)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n", "def test_column_transformer_with_mixed_types():\n    X = np.array([[1, 2, 'a'], [3, 4, 'b'], [5, 6, 'c']])\n\n    column_transformer = ColumnTransformer(\n        [\n            ('num', StandardScaler(), [0, 1]),\n            ('cat', OneHotEncoder(), [2])\n        ],\n        remainder='passthrough'\n    )\n\n    X_trans = column_transformer.fit_transform(X)\n\n    expected_output = np.array([\n        [-1.22474487, -1.22474487, 1., 0., 0., 1, 2, 'a'],\n        [0., 0., 0., 1., 0., 3, 4, 'b'],\n        [1.22474487, 1.22474487, 0., 0., 1., 5, 6, 'c']\n    ], dtype=object)\n\n    assert_array_equal(X_trans, expected_output)\n    assert len(column_transformer.transformers_) == 2\n    assert column_transformer.transformers_[-1][0] == 'remainder'\n    assert column_transformer.transformers_[-1][1] == 'passthrough'\n    assert_array_equal(column_transformer.transformers_[-1][2], [])\n", "def test_column_transformer_transformer_weights_with_remainder():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_second = np.array([2, 4, 6]).reshape(-1, 1)\n    X_res_both = np.hstack([X_res_first * 0.1, X_res_second * 10, X_res_second])\n\n    transformer_weights = {'trans1': .1, 'trans2': 10}\n\n    # Define the ColumnTransformer with a remainder and transformer_weights\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           remainder='passthrough',\n                           transformer_weights=transformer_weights)\n    \n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 3  # including remainder\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'passthrough'\n    assert_array_equal(ct.transformers_[-1][2], [2])\n\n    # Check that transformer weights are correctly applied\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           remainder=StandardScaler(),\n                           transformer_weights=transformer_weights)\n    X_trans = ct.fit_transform(X_array)\n    assert ct.transformer_weights == transformer_weights\n    assert ct.named_transformers_.trans1.transform(X_array[:, [0]])[:, 0].mean() * 0.1 == X_trans[:, 0].mean()\n    assert ct.named_transformers_.trans2.transform(X_array[:, [1]])[:, 0].mean() * 10 == X_trans[:, 1].mean()\n", "def test_column_transformer_with_non_transformer():\n    # Ensure that using an invalid transformer raises an appropriate error\n    class InvalidTransformer:\n            return self\n\n            return X\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('invalid', InvalidTransformer(), [0])])\n    assert_raise_message(TypeError, \"All estimators should implement fit\",\n                         ct.fit, X_array)\n", "def test_column_transformer_with_pipeline():\n    from sklearn.pipeline import Pipeline\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_both = X_array\n\n    pipeline = Pipeline([\n        ('trans', Trans())\n    ])\n\n    ct = ColumnTransformer([('pipe', pipeline, [0])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array), X_res_first)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)\n\n    ct = ColumnTransformer([('pipe', pipeline, [0, 1])])\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 1\n"], "sample_839": ["def test_strip_tags():\n    text = \"<b>Bold</b> and <i>italic</i> text with <a href='url'>link</a>.\"\n    expected = \" Bold  and  italic  text with  link .\"\n    assert strip_tags(text) == expected\n\n    text = \"No tags here!\"\n    expected = \"No tags here!\"\n    assert strip_tags(text) == expected\n\n    text = \"<p>Paragraph with <br> line break.</p>\"\n    expected = \" Paragraph with   line break. \"\n    assert strip_tags(text) == expected\n", "def test_strip_tags():\n    # Test basic HTML tags stripping\n    s = \"<b>bold</b> <i>italic</i> <u>underline</u>\"\n    expected = \" bold  italic  underline \"\n    assert strip_tags(s) == expected\n\n    # Test nested HTML tags stripping\n    s = \"<div><p>paragraph</p><span>span</span></div>\"\n    expected = \" paragraph span \"\n    assert strip_tags(s) == expected\n\n    # Test HTML tags with attributes stripping\n    s = '<a href=\"http://example.com\">link</a>'\n    expected = \" link \"\n    assert strip_tags(s) == expected\n\n    # Test no HTML tags present\n    s = \"no tags here\"\n    expected = \"no tags here\"\n    assert strip_tags(s) == expected\n\n    # Test empty string\n    s = \"\"\n    expected = \"\"\n    assert strip_tags(s) == expected\n\n    # Test string with only HTML tags\n    s = \"<div></div><p></p>\"\n    expected = \"  \"\n    assert strip_tags(s) == expected\n", "def test_strip_tags():\n    # check basic HTML stripping\n    html = \"<p>This is a <strong>test</strong> of <a href='http://example.com'>strip_tags</a> function.</p>\"\n    expected = \" This is a test of strip_tags function. \"\n    assert strip_tags(html) == expected\n\n    # check nested tags stripping\n    html = \"<div><p>This is <span>nested <em>HTML</em></span> tags</p></div>\"\n    expected = \" This is nested HTML tags \"\n    assert strip_tags(html) == expected\n\n    # check self-closing tags\n    html = \"<div>This is a test<br/> with self-closing tags</div>\"\n    expected = \" This is a test  with self-closing tags \"\n    assert strip_tags(html) == expected\n\n    # check no tags\n    html = \"This is a plain text with no tags.\"\n    expected = \"This is a plain text with no tags.\"\n    assert strip_tags(html) == expected\n\n    # check tags with attributes\n    html = \"<img src='image.jpg' alt='test image'>This is a test</img>\"\n    expected = \" This is a test \"\n    assert strip_tags(html) == expected\n\n    # check unicode characters within tags\n    html = \"<p>Unicode test: &#9731; &#x1F600;</p>\"\n    expected = \" Unicode test: \u2603 \ud83d\ude00 \"\n    assert strip_tags(html) == expected\n", "def test_strip_tags():\n    # Test strip_tags function with various HTML and XML tags\n    text = \"<p>This is a <b>test</b> string with <a href='#'>HTML</a> tags.</p>\"\n    expected = \" This is a  test  string with  HTML  tags. \"\n    assert strip_tags(text) == expected\n\n    text = \"<div><p>Another <span>test</span> with <br>different tags.</p></div>\"\n    expected = \" Another  test  with  different tags. \"\n    assert strip_tags(text) == expected\n\n    text = \"<html><body><h1>Header</h1><p>Paragraph.</p></body></html>\"\n    expected = \" Header  Paragraph. \"\n    assert strip_tags(text) == expected\n\n    text = \"<html><body><h1>Header</h1><p>Paragraph with <a href='#'>link</a>.</p></body></html>\"\n    expected = \" Header  Paragraph with  link . \"\n    assert strip_tags(text) == expected\n\n    text = \"<tag1><tag2>Text with nested <tag3>tags</tag3></tag2></tag1>\"\n    expected = \" Text with nested  tags \"\n    assert strip_tags(text) == expected\n\n    text = \"No tags\"\n    expected = \"No tags\"\n    assert strip_tags(text) == expected\n", "def test_countvectorizer_with_unicode():\n    # Test CountVectorizer with input containing unicode characters\n    data = [\"\u4f60\u597d\u4e16\u754c\", \"\u3053\u3093\u306b\u3061\u306f\u4e16\u754c\", \"\uc548\ub155\ud558\uc138\uc694 \uc138\uacc4\"]\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(data)\n    \n    # Check shape of the transformed data\n    assert X.shape == (3, 6)  # 3 documents, 6 unique tokens\n\n    # Verify that the correct tokens are extracted\n    feature_names = vectorizer.get_feature_names()\n    expected_feature_names = ['\u3053\u3093\u306b\u3061\u306f\u4e16\u754c', '\uc138\uacc4', '\u4f60\u597d\u4e16\u754c', '\uc548\ub155\ud558\uc138\uc694']\n    assert set(feature_names) == set(expected_feature_names)\n\n    # Check that the transformation result is as expected\n    assert X[0, feature_names.index('\u4f60\u597d\u4e16\u754c')] == 1\n    assert X[1, feature_names.index('\u3053\u3093\u306b\u3061\u306f\u4e16\u754c')] == 1\n    assert X[2, feature_names.index('\uc548\ub155\ud558\uc138\uc694')] == 1\n    assert X[2, feature_names.index('\uc138\uacc4')] == 1\n"], "sample_840": ["def test_pls_transform_consistency():\n    # Ensure that transform method is consistent with fit_transform\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(), pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        X_transform, Y_transform = clf.transform(X, Y)\n        X_fit_transform, Y_fit_transform = clf.fit_transform(X, Y)\n        \n        assert_array_almost_equal(X_transform, X_fit_transform, decimal=6,\n                                  err_msg=\"Transform and fit_transform X outputs differ\")\n        assert_array_almost_equal(Y_transform, Y_fit_transform, decimal=6,\n                                  err_msg=\"Transform and fit_transform Y outputs differ\")\n", "def test_pls_transform_and_predict():\n    # Test the transform and predict methods of PLSRegression and PLSCanonical\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    for clf_class in [pls_.PLSRegression, pls_.PLSCanonical]:\n        clf = clf_class(n_components=2)\n        clf.fit(X, Y)\n        \n        # Test transform method\n        X_transformed, Y_transformed = clf.transform(X, Y)\n        assert X_transformed.shape == (X.shape[0], 2)\n        assert Y_transformed.shape == (Y.shape[0], 2)\n\n        # Test predict method\n        Y_pred = clf.predict(X)\n        assert Y_pred.shape == Y.shape\n\n        # Check if Y_pred follows the same pattern as the original Y\n        assert np.corrcoef(Y.flatten(), Y_pred.flatten())[0, 1] > 0.9\n", "def test_pls_transform():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    pls_ca = pls_.PLSCanonical(n_components=2)\n    pls_ca.fit(X, Y)\n\n    # Check that transform raises an error if not fitted\n    pls_unfitted = pls_.PLSCanonical(n_components=2)\n    with pytest.raises(ValueError, match=\"This PLSCanonical instance is not fitted yet\"):\n        pls_unfitted.transform(X)\n\n    # Check that transform returns correct dimensions\n    X_transformed, Y_transformed = pls_ca.transform(X, Y)\n    assert X_transformed.shape == (X.shape[0], 2)\n    assert Y_transformed.shape == (Y.shape[0], 2)\n\n    # Check that transform works with only X\n    X_transformed_only = pls_ca.transform(X)\n    assert X_transformed_only.shape == (X.shape[0], 2)\n\n    # Check that transformed X and Y are approximately equal to the scores\n    assert_array_almost_equal(X_transformed, pls_ca.x_scores_, decimal=5,\n                              err_msg=\"Transformed X not approximately equal to x_scores\")\n    assert_array_almost_equal(Y_transformed, pls_ca.y_scores_, decimal=5,\n                              err_msg=\"Transformed Y not approximately equal to y_scores\")\n", "def test_pls_with_single_component():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    \n    # Test PLSCanonical with a single component\n    pls_ca = pls_.PLSCanonical(n_components=1)\n    pls_ca.fit(X, Y)\n    \n    # Check that the number of components is 1\n    assert pls_ca.x_scores_.shape[1] == 1\n    assert pls_ca.y_scores_.shape[1] == 1\n    assert pls_ca.x_weights_.shape[1] == 1\n    assert pls_ca.y_weights_.shape[1] == 1\n    assert pls_ca.x_loadings_.shape[1] == 1\n    assert pls_ca.y_loadings_.shape[1] == 1\n    \n    # Check that the scores and loadings are not empty\n    assert pls_ca.x_scores_.shape[0] == X.shape[0]\n    assert pls_ca.y_scores_.shape[0] == Y.shape[0]\n    assert pls_ca.x_loadings_.shape[0] == X.shape[1]\n    assert pls_ca.y_loadings_.shape[0] == Y.shape[1]\n    \n    # Check that the transformation is consistent\n    X_transformed = pls_ca.transform(X)\n    assert X_transformed.shape[1] == 1\n    assert_array_almost_equal(X_transformed, pls_ca.x_scores_)\n    \n    # Check prediction\n    Y_pred = pls_ca.predict(X)\n    assert Y_pred.shape == Y.shape\n    assert np.any(Y_pred != 0)\n", "def test_center_scale_xy():\n    # Test the _center_scale_xy function for correct centering and scaling\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    Y = np.array([[2, 3], [5, 6], [8, 9]])\n\n    # Case 1: Scaling is True\n    X_scaled, Y_scaled, x_mean, y_mean, x_std, y_std = _center_scale_xy(X.copy(), Y.copy(), scale=True)\n    assert_array_almost_equal(X_scaled.mean(axis=0), np.zeros(X.shape[1]), err_msg=\"X not correctly centered\")\n    assert_array_almost_equal(Y_scaled.mean(axis=0), np.zeros(Y.shape[1]), err_msg=\"Y not correctly centered\")\n    assert_array_almost_equal(X_scaled.std(axis=0, ddof=1), np.ones(X.shape[1]), err_msg=\"X not correctly scaled\")\n    assert_array_almost_equal(Y_scaled.std(axis=0, ddof=1), np.ones(Y.shape[1]), err_msg=\"Y not correctly scaled\")\n    \n    # Case 2: Scaling is False\n    X_scaled, Y_scaled, x_mean, y_mean, x_std, y_std = _center_scale_xy(X.copy(), Y.copy(), scale=False)\n    assert_array_almost_equal(X_scaled.mean(axis=0), np.zeros(X.shape[1]), err_msg=\"X not correctly centered\")\n    assert_array_almost_equal(Y_scaled.mean(axis=0), np.zeros(Y.shape[1]), err_msg=\"Y not correctly centered\")\n    assert_array_almost_equal(x_std, np.ones(X.shape[1]), err_msg=\"X std not correctly set to ones\")\n    assert_array_almost_equal(y_std, np.ones(Y.shape[1]), err_msg=\"Y std not correctly set to ones\")\n"], "sample_841": ["def test_ridge_multitarget_sparse():\n    # Test Ridge on sparse input with multiple targets\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 8, 5\n    n_targets = 3\n    X = sp.csr_matrix(rng.randn(n_samples, n_features))\n    y = rng.randn(n_samples, n_targets)\n    \n    ridge = Ridge(alpha=1.0)\n    ridge.fit(X, y)\n    assert ridge.coef_.shape == (n_targets, n_features)\n    assert ridge.intercept_.shape == (n_targets,)\n    \n    y_pred = ridge.predict(X)\n    assert y_pred.shape == (n_samples, n_targets)\n    assert ridge.score(X, y) > 0.5\n", "def test_ridge_fit_intercept_sparse_auto_solver():\n    # Test auto solver with sparse input and fit_intercept=True\n    X, y = _make_sparse_offset_regression(n_features=20, random_state=0)\n    X_csr = sp.csr_matrix(X)\n\n    ridge_auto = Ridge(alpha=1., solver='auto', fit_intercept=True)\n    dense_ridge = Ridge(alpha=1., solver='sparse_cg', fit_intercept=True)\n\n    dense_ridge.fit(X, y)\n    with pytest.warns(None) as record:\n        ridge_auto.fit(X_csr, y)\n    assert len(record) == 0\n    assert np.allclose(dense_ridge.intercept_, ridge_auto.intercept_)\n    assert np.allclose(dense_ridge.coef_, ridge_auto.coef_)\n", "def test_ridge_solver_switching():\n    # Test that Ridge switches solvers correctly based on the input type\n    X, y = make_regression(n_samples=100, n_features=20, random_state=0)\n    sparse_X = sp.csr_matrix(X)\n\n    # Test with dense input\n    ridge_dense_auto = Ridge(solver='auto')\n    ridge_dense_auto.fit(X, y)\n    assert ridge_dense_auto.solver == 'cholesky'\n\n    # Test with sparse input\n    ridge_sparse_auto = Ridge(solver='auto')\n    ridge_sparse_auto.fit(sparse_X, y)\n    assert ridge_sparse_auto.solver == 'sparse_cg'\n\n    # Test that sag solver is chosen when return_intercept is True\n    ridge_dense_sag = Ridge(solver='auto', return_intercept=True)\n    ridge_dense_sag.fit(X, y)\n    assert ridge_dense_sag.solver == 'sag'\n\n    ridge_sparse_sag = Ridge(solver='auto', return_intercept=True)\n    with pytest.raises(ValueError, match=\"In Ridge, only 'sag' solver can directly fit the intercept\"):\n        ridge_sparse_sag.fit(sparse_X, y)\n\n    # Test that manually setting solver works as expected\n    ridge_dense_manual = Ridge(solver='svd')\n    ridge_dense_manual.fit(X, y)\n    assert ridge_dense_manual.solver == 'svd'\n\n    # Should raise an error if solver is not supported for sparse input\n    ridge_sparse_manual = Ridge(solver='svd')\n    with pytest.raises(TypeError):\n        ridge_sparse_manual.fit(sparse_X, y)\n", "def test_ridge_solver_auto():\n    # Test Ridge solver='auto' chooses the appropriate solver\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    # Dense data\n    X_dense = rng.randn(6, 5)\n    y_dense = rng.randn(6)\n\n    ridge_auto_dense = Ridge(alpha=alpha, solver='auto')\n    ridge_auto_dense.fit(X_dense, y_dense)\n    assert ridge_auto_dense.coef_.shape == (X_dense.shape[1], )\n\n    # Sparse data\n    X_sparse = sp.csr_matrix(X_dense)\n    y_sparse = y_dense\n\n    ridge_auto_sparse = Ridge(alpha=alpha, solver='auto')\n    ridge_auto_sparse.fit(X_sparse, y_sparse)\n    assert ridge_auto_sparse.coef_.shape == (X_sparse.shape[1], )\n\n    # Ensure the same results as with explicit solver choice\n    ridge_sag = Ridge(alpha=alpha, solver='sag')\n    ridge_sag.fit(X_dense, y_dense)\n    assert_allclose(ridge_auto_dense.coef_, ridge_sag.coef_, rtol=1e-5)\n    assert_allclose(ridge_auto_dense.intercept_, ridge_sag.intercept_, rtol=1e-5)\n\n    ridge_sparse_cg = Ridge(alpha=alpha, solver='sparse_cg')\n    ridge_sparse_cg.fit(X_sparse, y_sparse)\n    assert_allclose(ridge_auto_sparse.coef_, ridge_sparse_cg.coef_, rtol=1e-5)\n    assert_allclose(ridge_auto_sparse.intercept_, ridge_sparse_cg.intercept_, rtol=1e-5)\n", "def test_ridge_regression_solver_switch():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    # Test solver switch when sample_weight is provided and solver is 'auto'\n    sample_weight = np.ones(n_samples)\n    ridge_auto = Ridge(alpha=1.0, solver='auto')\n    ridge_auto.fit(X, y, sample_weight=sample_weight)\n    assert ridge_auto.solver == 'cholesky'\n\n    # Test solver switch when return_intercept is True and solver is 'auto'\n    ridge_auto_intercept = Ridge(alpha=1.0, solver='auto', fit_intercept=True)\n    ridge_auto_intercept.fit(X, y)\n    assert ridge_auto_intercept.solver == 'sag'\n"], "sample_842": ["def test_clone_non_estimator():\n    # Test cloning of a non-estimator object with safe=False\n    original_list = [1, 2, 3]\n    cloned_list = clone(original_list, safe=False)\n    assert cloned_list == original_list\n    assert id(cloned_list) != id(original_list)\n\n    original_set = {1, 2, 3}\n    cloned_set = clone(original_set, safe=False)\n    assert cloned_set == original_set\n    assert id(cloned_set) != id(original_set)\n\n    original_tuple = (1, 2, 3)\n    cloned_tuple = clone(original_tuple, safe=False)\n    assert cloned_tuple == original_tuple\n    assert id(cloned_tuple) != id(original_tuple)\n\n    # Test cloning of a non-estimator object with safe=True\n    with pytest.raises(TypeError):\n        clone(original_list, safe=True)\n", "def test_clone_non_estimator_safe_false():\n    # Test that clone works with non-estimator when safe is False\n    non_estimator = [1, 2, 3]\n    cloned_non_estimator = clone(non_estimator, safe=False)\n    assert cloned_non_estimator == non_estimator\n    assert id(cloned_non_estimator) != id(non_estimator)\n", "def test_base_estimator_repr():\n    # Test the __repr__ method of BaseEstimator for various scenarios.\n    class MyEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n    estimator = MyEstimator(param1=5, param2=\"test\")\n    repr_str = repr(estimator)\n    \n    assert \"MyEstimator\" in repr_str\n    assert \"param1=5\" in repr_str\n    assert \"param2='test'\" in repr_str\n\n    # Test with a long parameter list\n    class LongParamsEstimator(BaseEstimator):\n            for i, param in enumerate(params):\n                setattr(self, f'param{i}', param)\n\n    long_params = LongParamsEstimator(*range(50))\n    repr_str = repr(long_params)\n    assert len(repr_str) < 1000  # Should not be excessively long\n\n    # Ensure ellipsis is used for long representations\n    assert \"...\" in repr_str\n", "def test_base_estimator_get_set_params():\n    # Define a dummy estimator class for testing\n    class DummyEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n    est = DummyEstimator(param1=10, param2=20)\n    \n    # Test get_params\n    params = est.get_params()\n    assert params == {'param1': 10, 'param2': 20}\n\n    # Test set_params\n    est.set_params(param1=30, param2=40)\n    assert est.param1 == 30\n    assert est.param2 == 40\n\n    # Test invalid parameter in set_params\n    with pytest.raises(ValueError):\n        est.set_params(param3=50)\n", "def test_base_estimator_get_params():\n    # Test BaseEstimator get_params method\n    class MyEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n    estimator = MyEstimator(param1=5, param2=10)\n    params = estimator.get_params()\n    assert params == {\"param1\": 5, \"param2\": 10}\n"], "sample_843": ["def test_kernel_bounds(kernel):\n    # Check that the bounds property returns log-transformed bounds correctly.\n    bounds = kernel.bounds\n    index = 0\n    for hyperparameter in kernel.hyperparameters:\n        if not hyperparameter.fixed:\n            size = hyperparameter.n_elements\n            if size > 1:  # anisotropic kernels\n                expected_bounds = np.log(hyperparameter.bounds)\n                assert_array_almost_equal(bounds[index:index + size], expected_bounds)\n                index += size\n            else:\n                expected_bounds = np.log(hyperparameter.bounds)\n                assert_almost_equal(bounds[index], expected_bounds)\n                index += 1\n", "def test_kernel_arithmetic(kernel):\n    # Test arithmetic operations on kernels\n    kernel_sum = kernel + kernel\n    kernel_product = kernel * kernel\n    kernel_exponentiation = kernel ** 2\n\n    # Check that the sum of a kernel with itself equals doubling the kernel\n    K_sum = kernel_sum(X)\n    K_double = 2 * kernel(X)\n    assert_almost_equal(K_sum, K_double, 5)\n\n    # Check that the product of a kernel with itself equals squaring the kernel\n    K_product = kernel_product(X)\n    K_squared = kernel(X) ** 2\n    assert_almost_equal(K_product, K_squared, 5)\n\n    # Check that exponentiating a kernel to power 2 equals squaring the kernel\n    K_exponentiation = kernel_exponentiation(X)\n    assert_almost_equal(K_exponentiation, K_squared, 5)\n\n    # Test diag method consistency with arithmetic operations\n    K_sum_diag = kernel_sum.diag(X)\n    K_double_diag = 2 * kernel.diag(X)\n    assert_almost_equal(K_sum_diag, K_double_diag, 5)\n\n    K_product_diag = kernel_product.diag(X)\n    K_squared_diag = kernel.diag(X) ** 2\n    assert_almost_equal(K_product_diag, K_squared_diag, 5)\n\n    K_exponentiation_diag = kernel_exponentiation.diag(X)\n    assert_almost_equal(K_exponentiation_diag, K_squared_diag, 5)\n", "def test_clone_with_theta():\n    # Test the clone_with_theta method for a kernel.\n    kernel = RBF(length_scale=2.0)\n    theta = np.log([3.0])\n    \n    cloned_kernel = kernel.clone_with_theta(theta)\n    assert_almost_equal(cloned_kernel.length_scale, 3.0)\n    assert_almost_equal(cloned_kernel.theta, theta)\n    \n    # Test for a compound kernel\n    compound_kernel = kernel + WhiteKernel(noise_level=1.0)\n    theta = np.log([3.0, 2.0])\n    \n    cloned_compound_kernel = compound_kernel.clone_with_theta(theta)\n    assert_almost_equal(cloned_compound_kernel.k1.length_scale, 3.0)\n    assert_almost_equal(cloned_compound_kernel.k2.noise_level, 2.0)\n    assert_almost_equal(cloned_compound_kernel.theta, theta)\n", "def test_kernel_set_params_invalid_key():\n    kernel = RBF(length_scale=1.0)\n    with pytest.raises(ValueError, match=\"Invalid parameter\"):\n        kernel.set_params(non_existent_param=0.5)\n", "def test_kernel_bounds(kernel):\n    # Check that kernel bounds are correctly set and retrieved.\n    bounds = kernel.bounds\n    for hyperparameter, bound in zip(kernel.hyperparameters, bounds):\n        if hyperparameter.fixed:\n            continue\n        assert_almost_equal(bound, np.log(hyperparameter.bounds))\n\n    # Test setting and getting bounds\n    new_bounds = np.log([(1e-4, 1e4)] * len(kernel.hyperparameters))\n    for i, hyperparameter in enumerate(kernel.hyperparameters):\n        if hyperparameter.fixed:\n            continue\n        params = kernel.get_params()\n        params[hyperparameter.name + \"_bounds\"] = (1e-4, 1e4)\n        kernel.set_params(**params)\n        assert_almost_equal(kernel.bounds[i], new_bounds[i])\n"], "sample_844": ["def test_metric_params():\n    # Test the usage of metric_params in OPTICS\n    X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=0)\n    metric_params = {'p': 3}\n\n    clust = OPTICS(min_samples=5, metric='minkowski', p=3, metric_params=metric_params).fit(X)\n    assert clust.p == 3\n    assert clust.metric_params == metric_params\n\n    # Check if using a different metric affects the results\n    clust2 = OPTICS(min_samples=5, metric='minkowski', p=2).fit(X)\n    assert not np.array_equal(clust.labels_, clust2.labels_)\n", "def test_invalid_metric():\n    # Test an invalid metric parameter to ensure proper error handling\n    X, labels_true = make_blobs(n_samples=750, centers=3, cluster_std=0.4, random_state=0)\n    clust = OPTICS(max_eps=5.0, min_samples=10, metric='invalid_metric')\n\n    msg = \"Metric 'invalid_metric' not valid\"\n    with pytest.raises(ValueError, match=msg):\n        clust.fit(X)\n", "def test_fit_precomputed():\n    # Test the fit method with precomputed distance matrix\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=50, centers=centers, cluster_std=0.4, random_state=0)\n    distance_matrix = pairwise_distances(X)\n\n    clust = OPTICS(max_eps=0.5, min_samples=3, cluster_method='xi', metric='precomputed')\n    clust.fit(distance_matrix)\n    \n    # Check if labels have been assigned\n    assert clust.labels_.shape == (len(X),)\n    assert clust.labels_.dtype.kind == 'i'\n    \n    # Check if other attributes are correctly assigned\n    assert clust.reachability_.shape == (len(X),)\n    assert clust.reachability_.dtype.kind == 'f'\n    assert clust.core_distances_.shape == (len(X),)\n    assert clust.core_distances_.dtype.kind == 'f'\n    assert clust.ordering_.shape == (len(X),)\n    assert clust.ordering_.dtype.kind == 'i'\n    assert set(clust.ordering_) == set(range(len(X)))\n", "def test_core_distances_computation():\n    # Test _compute_core_distances_ with a simple dataset\n    X_test = np.array([[0, 0], [0, 1], [1, 1], [1, 0]])\n    neighbors = NearestNeighbors(n_neighbors=2, algorithm='brute', metric='euclidean').fit(X_test)\n    core_distances = _compute_core_distances_(X_test, neighbors, min_samples=2, working_memory=None)\n    \n    # Expected core distances for each point with min_samples=2\n    expected_core_distances = np.array([1, 1, 1, 1])\n    \n    assert_array_equal(core_distances, expected_core_distances)\n", "def test_metric_params():\n    # Test metric_params parameter for custom metric\n        return np.sqrt(np.sum((x - y) ** 2))\n\n    # Generate sample data\n    X, _ = make_blobs(n_samples=50, centers=2, cluster_std=0.5, random_state=42)\n    \n    # Compute OPTICS with custom metric\n    clust = OPTICS(min_samples=3, metric=custom_metric, metric_params={}).fit(X)\n\n    # Ensure labels are assigned correctly\n    assert len(clust.labels_) == 50\n    assert len(np.unique(clust.labels_)) <= 2  # At most 2 clusters\n"], "sample_845": ["def test_countvectorizer_with_unicode_stop_words():\n    # Test CountVectorizer with stop words containing non-ASCII characters\n    stop_words = ['c\u2019\u00e9tait', 'tr\u00e8s']\n    vectorizer = CountVectorizer(stop_words=stop_words)\n    text = [\"J'ai mang\u00e9 du kangourou ce midi, c'\u00e9tait pas tr\u00e8s bon.\"]\n    X = vectorizer.fit_transform(text)\n    feature_names = vectorizer.get_feature_names()\n    \n    # Ensure that the stop words have been removed from the feature names\n    for stop_word in stop_words:\n        assert strip_accents_unicode(stop_word.lower()) not in feature_names\n\n    # Ensure that other words are still present\n    expected_words = ['ai', 'mange', 'du', 'kangourou', 'ce', 'midi', 'pas', 'bon']\n    assert_array_equal(feature_names, expected_words)\n", "def test_strip_tags():\n    # Simple HTML tags\n    html_text = \"<p>This is a <b>bold</b> test.</p>\"\n    expected = \" This is a  bold  test. \"\n    assert strip_tags(html_text) == expected\n\n    # HTML with attributes\n    html_text = '<a href=\"http://example.com\">link</a>'\n    expected = \" link \"\n    assert strip_tags(html_text) == expected\n\n    # Nested HTML tags\n    html_text = \"<div><p>Nested <span>tags</span></p></div>\"\n    expected = \"  Nested  tags   \"\n    assert strip_tags(html_text) == expected\n\n    # HTML with comments\n    html_text = \"<!-- This is a comment --><p>Text</p>\"\n    expected = \" Text \"\n    assert strip_tags(html_text) == expected\n\n    # HTML with special characters\n    html_text = \"<p>This &amp; that</p>\"\n    expected = \" This &amp; that \"\n    assert strip_tags(html_text) == expected\n", "def test_countvectorizer_invalid_vocabulary():\n    # Test CountVectorizer with invalid vocabulary types\n    invalid_vocab = 12345\n    vectorizer = CountVectorizer(vocabulary=invalid_vocab)\n    assert_raises(TypeError, vectorizer.fit, ALL_FOOD_DOCS)\n\n    invalid_vocab = {\"valid\": 1, \"also_valid\": \"not_an_int\"}\n    vectorizer = CountVectorizer(vocabulary=invalid_vocab)\n    assert_raises(ValueError, vectorizer.fit, ALL_FOOD_DOCS)\n\n    invalid_vocab = [\"valid\", 123, \"also_valid\"]\n    vectorizer = CountVectorizer(vocabulary=invalid_vocab)\n    assert_raises(TypeError, vectorizer.fit, ALL_FOOD_DOCS)\n", "def test_strip_tags():\n    text = \"<p>This is a <strong>test</strong> with <a href='#'>HTML</a> tags.</p>\"\n    expected = \" This is a  test  with  HTML  tags. \"\n    assert strip_tags(text) == expected\n\n    text = \"No HTML tags here!\"\n    expected = \"No HTML tags here!\"\n    assert strip_tags(text) == expected\n\n    text = \"<div><span>Nested <b>HTML</b> tags</span></div>\"\n    expected = \" Nested  HTML  tags \"\n    assert strip_tags(text) == expected\n\n    text = \"<p>This is a <strong>test</strong> with <a href='#'>HTML</a> tags and special characters &amp; entities.</p>\"\n    expected = \" This is a  test  with  HTML  tags and special characters &amp; entities. \"\n    assert strip_tags(text) == expected\n", "def test_strip_tags():\n    # Test to ensure that the strip_tags function works correctly\n    assert strip_tags(\"<b>This is a test</b>\") == \" This is a test \"\n    assert strip_tags(\"<div><p>Another test</p></div>\") == \"  Another test  \"\n    assert strip_tags(\"<div><p>Tags <should> be stripped.</p></div>\") == \"  Tags  be stripped.  \"\n    assert strip_tags(\"No tags here!\") == \"No tags here!\"\n"], "sample_846": ["def test_column_transformer_custom_transformer():\n    class CustomTransformer(BaseEstimator, TransformerMixin):\n            self.mean_ = np.mean(X, axis=0)\n            return self\n\n            return X - self.mean_\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res = X_array - np.mean(X_array, axis=0)\n\n    ct = ColumnTransformer([('custom', CustomTransformer(), [0, 1])])\n    assert_array_equal(ct.fit_transform(X_array), X_res)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res)\n    assert len(ct.transformers_) == 1\n    assert isinstance(ct.transformers_[0][1], CustomTransformer)\n\n    # Test with a DataFrame\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n\n    ct = ColumnTransformer([('custom', CustomTransformer(), ['first', 'second'])])\n    assert_array_equal(ct.fit_transform(X_df), X_res)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res)\n    assert len(ct.transformers_) == 1\n    assert isinstance(ct.transformers_[0][1], CustomTransformer)\n\n    # Ensure transformer without mean is fitted correctly\n    custom_trans = CustomTransformer()\n    ct = ColumnTransformer([('custom', custom_trans, ['first', 'second'])])\n    ct.fit(X_df)\n    assert hasattr(custom_trans, 'mean_')\n", "def test_column_transformer_with_mixed_types():\n    pd = pytest.importorskip('pandas')\n\n    # Mixed data types\n    X_array = np.array([[0, 1.1, 'a'],\n                        [2, 2.2, 'b'],\n                        [1, 3.3, 'c']])\n    X_df = pd.DataFrame(X_array, columns=['int', 'float', 'str'])\n\n    # Expected result\n    expected_result = np.array([\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [1.0, 0.5, 0.0, 1.0, 0.0],\n        [0.5, 1.0, 0.0, 0.0, 1.0]\n    ])\n\n    # Column transformer with mixed types\n    ct = ColumnTransformer([\n        ('int_trans', StandardScaler(), ['int']),\n        ('float_trans', StandardScaler(), ['float']),\n        ('str_trans', OneHotEncoder(), ['str'])\n    ])\n\n    # Fit and transform\n    result = ct.fit_transform(X_df)\n    assert_array_equal(result, expected_result)\n\n    # Ensure the transformers are correctly set up\n    assert isinstance(ct.named_transformers_.int_trans, StandardScaler)\n    assert isinstance(ct.named_transformers_.float_trans, StandardScaler)\n    assert isinstance(ct.named_transformers_.str_trans, OneHotEncoder)\n", "def test_column_transformer_drop_passthrough_combination():\n    # Test a combination of 'drop' and 'passthrough' transformers\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n\n    # Drop the first column, passthrough the second, and double the third\n    ct = ColumnTransformer([\n        ('trans1', 'drop', [0]),\n        ('trans2', 'passthrough', [1]),\n        ('trans3', DoubleTrans(), [2])\n    ], remainder='drop')\n\n    X_trans = ct.fit_transform(X_array)\n    expected_output = np.array([[1, 4],\n                                [4, 12],\n                                [6, 8]])\n\n    assert_array_equal(X_trans, expected_output)\n    assert len(ct.transformers_) == 3\n    assert ct.transformers_[0][1] == 'drop'\n    assert ct.transformers_[1][1] == 'passthrough'\n    assert isinstance(ct.transformers_[2][1], DoubleTrans)\n", "def test_column_transformer_custom_transformer():\n    # Custom transformer to test fit, transform, and fit_transform methods\n    class CustomTransformer(BaseEstimator, TransformerMixin):\n            self.mean_ = np.mean(X, axis=0)\n            return self\n\n            return X - self.mean_\n\n    X_array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    expected_output = X_array - np.mean(X_array, axis=0)\n\n    ct = ColumnTransformer([('custom', CustomTransformer(), [0, 1, 2])])\n\n    # Test fit_transform\n    transformed = ct.fit_transform(X_array)\n    assert_array_equal(transformed, expected_output)\n\n    # Test separate fit and transform\n    ct.fit(X_array)\n    transformed = ct.transform(X_array)\n    assert_array_equal(transformed, expected_output)\n\n    # Check if the custom transformer has been fitted\n    assert hasattr(ct.named_transformers_['custom'], 'mean_')\n    assert_array_equal(ct.named_transformers_['custom'].mean_, np.mean(X_array, axis=0))\n\n    # Test with remainder\n    ct_remainder = ColumnTransformer([('custom', CustomTransformer(), [0, 1])], remainder='passthrough')\n    transformed_remainder = ct_remainder.fit_transform(X_array)\n    expected_output_remainder = np.hstack([X_array[:, :2] - np.mean(X_array[:, :2], axis=0).reshape(1, -1), X_array[:, 2:]])\n    assert_array_equal(transformed_remainder, expected_output_remainder)\n", "def test_column_transformer_with_callable_transformer():\n    # Test to ensure that transformer which is a callable function works correctly\n    X_array = np.array([[1, 2], [3, 4], [5, 6]])\n\n        return X * 2\n\n    ct = ColumnTransformer([('double', transformer_func, [0, 1])], remainder='drop')\n    \n    expected_output = np.array([[2, 4], [6, 8], [10, 12]])\n    \n    assert_array_equal(ct.fit_transform(X_array), expected_output)\n    assert_array_equal(ct.fit(X_array).transform(X_array), expected_output)\n\n    # Test with callable function as remainder\n    ct = ColumnTransformer([('double', transformer_func, [0])], remainder=transformer_func)\n    \n    expected_output = np.array([[2, 4], [6, 8], [10, 12]])\n    \n    assert_array_equal(ct.fit_transform(X_array), expected_output)\n    assert_array_equal(ct.fit(X_array).transform(X_array), expected_output)\n"], "sample_847": ["def test_multi_task_elastic_net_alpha_warning():\n    # Test that an appropriate warning is raised when alpha is set to 0\n    X, y, _, _ = build_dataset(n_features=50, n_targets=2)\n    clf = MultiTaskElasticNet(alpha=0)\n    assert_warns(UserWarning, clf.fit, X, y)\n", "def test_lasso_path_sparse_input():\n    # Test that lasso_path works with sparse input data\n\n    # Sparse data\n    X = sparse.csr_matrix([[0, 1, 0], [1, 0, 1], [2, 2, 2]])\n    y = np.array([1, 2, 3])\n\n    alphas, coefs, dual_gaps = lasso_path(X, y, alphas=[5., 1., .5])\n    \n    # Expected results calculated manually or from trusted implementation\n    expected_coefs = np.array([[0., 0., 0.],\n                               [0., 1., 0.],\n                               [0., 1., 0.]])\n\n    assert_array_almost_equal(coefs.T, expected_coefs, decimal=2)\n    assert dual_gaps[-1] < 1e-5  # Ensure that the optimization converged\n", "def test_lasso_path_negative_l1_ratio():\n    X, y, _, _ = build_dataset()\n\n    # Test that passing a negative l1_ratio raises a ValueError\n    msg = \"l1_ratio must be between 0 and 1; got (l1_ratio=-0.1)\"\n    assert_raise_message(ValueError, msg, lasso_path, X, y, l1_ratio=-0.1)\n\n    # Test that passing l1_ratio greater than 1 raises a ValueError\n    msg = \"l1_ratio must be between 0 and 1; got (l1_ratio=1.1)\"\n    assert_raise_message(ValueError, msg, lasso_path, X, y, l1_ratio=1.1)\n", "def test_lasso_path_with_various_parameters():\n    # Test lasso_path function with different parameter values to ensure proper functionality\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n    \n    # Test with default parameters\n    alphas, coefs, gaps = lasso_path(X, y)\n    assert len(alphas) == 100  # Default n_alphas\n    assert coefs.shape == (20, 100)\n    assert gaps.shape == (100,)\n\n    # Test with custom alphas\n    custom_alphas = np.logspace(-3, 1, 50)\n    alphas, coefs, gaps = lasso_path(X, y, alphas=custom_alphas)\n    assert_array_almost_equal(alphas, custom_alphas)\n    assert coefs.shape == (20, 50)\n    assert gaps.shape == (50,)\n\n    # Test with positive parameter\n    alphas, coefs, gaps = lasso_path(X, y, positive=True)\n    assert (coefs >= 0).all()\n\n    # Test with return_n_iter parameter\n    alphas, coefs, gaps, n_iters = lasso_path(X, y, return_n_iter=True)\n    assert len(n_iters) == 100\n\n    # Test with sparse input data\n    X_sparse = sparse.csr_matrix(X)\n    alphas, coefs, gaps = lasso_path(X_sparse, y)\n    assert len(alphas) == 100\n    assert coefs.shape == (20, 100)\n    assert gaps.shape == (100,)\n", "def test_alpha_grid():\n    # Test the internal _alpha_grid function\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    y = np.array([1, 2, 3])\n    \n    alphas = _alpha_grid(X, y, l1_ratio=0.5, fit_intercept=True, eps=1e-3, n_alphas=100, normalize=False, copy_X=True)\n    assert len(alphas) == 100\n    assert alphas[0] > alphas[-1]\n\n    alphas = _alpha_grid(X, y, l1_ratio=0.8, fit_intercept=False, eps=1e-2, n_alphas=50, normalize=True, copy_X=False)\n    assert len(alphas) == 50\n    assert alphas[0] > alphas[-1]\n\n    with pytest.raises(ValueError, match=\"Automatic alpha grid generation is not supported for l1_ratio=0\"):\n        _alpha_grid(X, y, l1_ratio=0, fit_intercept=True, eps=1e-3, n_alphas=100, normalize=False, copy_X=True)\n"], "sample_848": ["def test_multi_output_regressor_invalid_estimator():\n    X, y = datasets.make_regression(n_targets=3)\n\n    class InvalidEstimator:\n            pass\n\n    invalid_estimator = MultiOutputRegressor(InvalidEstimator())\n\n    # Test if fit raises ValueError when using an invalid estimator without predict method\n    assert_raises(ValueError, invalid_estimator.fit, X, y)\n\n    # Test if predict raises NotFittedError when fit is not called\n    assert_raises(NotFittedError, invalid_estimator.predict, X)\n", "def test_classifier_chain_order_validation():\n    # Test to ensure ClassifierChain raises error with invalid order\n    X, Y = generate_multilabel_dataset_with_correlations()\n    invalid_order = [0, 2, 2, 3]  # Duplicate order index\n    classifier_chain = ClassifierChain(LogisticRegression(), order=invalid_order)\n    assert_raises(ValueError, classifier_chain.fit, X, Y)\n    \n    invalid_order = [0, 1, 2, 4]  # Out of bounds index\n    classifier_chain = ClassifierChain(LogisticRegression(), order=invalid_order)\n    assert_raises(ValueError, classifier_chain.fit, X, Y)\n\n    invalid_order = 'invalid'  # Invalid order type\n    classifier_chain = ClassifierChain(LogisticRegression(), order=invalid_order)\n    assert_raises(ValueError, classifier_chain.fit, X, Y)\n", "def test_regressor_chain_with_random_order_and_crossval():\n    # Fit RegressorChain with random order and cross-validation\n    X, Y = generate_multilabel_dataset_with_correlations()\n\n    chain = RegressorChain(Ridge(), order='random', random_state=42, cv=3)\n    chain.fit(X, Y)\n    Y_pred = chain.predict(X)\n    \n    assert Y_pred.shape == Y.shape\n    assert len(set(chain.order_)) == Y.shape[1]\n    assert all(isinstance(estimator, Ridge) for estimator in chain.estimators_)\n\n    # Compare with the same chain but with a fixed order\n    chain_fixed = RegressorChain(Ridge(), order=chain.order_, cv=3)\n    chain_fixed.fit(X, Y)\n    Y_pred_fixed = chain_fixed.predict(X)\n\n    assert_array_equal(chain_fixed.order_, chain.order_)\n    assert_array_almost_equal(Y_pred, Y_pred_fixed)\n", "def test_multi_output_regressor_predict_no_fit():\n    # Test predict method raises NotFittedError if called before fit\n    X, y = datasets.make_regression(n_targets=3)\n    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    with pytest.raises(NotFittedError):\n        rgr.predict(X)\n", "def test_multi_target_classifier_chain_with_random_order():\n    # Test if ClassifierChain with random order fits and predicts correctly\n    X, Y = generate_multilabel_dataset_with_correlations()\n    random_state = 42\n\n    chain = ClassifierChain(LogisticRegression(), order='random', random_state=random_state)\n    chain.fit(X, Y)\n    \n    # Check if the order is randomized\n    assert list(chain.order_) != list(range(Y.shape[1]))\n    \n    Y_pred = chain.predict(X)\n    assert Y_pred.shape == Y.shape\n\n    # Check if the order is consistent\n    new_chain = ClassifierChain(LogisticRegression(), order=chain.order_)\n    new_chain.fit(X, Y)\n    Y_new_pred = new_chain.predict(X)\n    assert_array_equal(Y_pred, Y_new_pred)\n"], "sample_849": ["def test_leave_p_out():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    p = 2\n    lpo = LeavePOut(p)\n    \n    # Ensure the number of splits is correct\n    assert lpo.get_n_splits(X) == comb(len(X), p, exact=True)\n    \n    splits = list(lpo.split(X, y))\n    assert len(splits) == comb(len(X), p, exact=True)\n\n    # Check all possible combinations are present\n    expected_splits = [\n        (np.array([2, 3]), np.array([0, 1])),\n        (np.array([1, 3]), np.array([0, 2])),\n        (np.array([1, 2]), np.array([0, 3])),\n        (np.array([0, 3]), np.array([1, 2])),\n        (np.array([0, 2]), np.array([1, 3])),\n        (np.array([0, 1]), np.array([2, 3]))\n    ]\n    \n    for split in splits:\n        assert split in expected_splits\n\n    # Edge case where p is greater than or equal to the number of samples\n    with pytest.raises(ValueError, match=\"p=4 must be strictly less than the number of samples=4\"):\n        LeavePOut(4).split(X, y)\n    with pytest.raises(ValueError, match=\"p=5 must be strictly less than the number of samples=4\"):\n        LeavePOut(5).split(X, y)\n", "def test_leave_one_out():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n    splits = list(loo.split(X, y))\n\n    expected_splits = [\n        (np.array([1, 2, 3]), np.array([0])),\n        (np.array([0, 2, 3]), np.array([1])),\n        (np.array([0, 1, 3]), np.array([2])),\n        (np.array([0, 1, 2]), np.array([3]))\n    ]\n\n    for (train_idx, test_idx), (expected_train, expected_test) in zip(splits, expected_splits):\n        assert_array_equal(train_idx, expected_train)\n        assert_array_equal(test_idx, expected_test)\n\n    # Test get_n_splits\n    n_splits = loo.get_n_splits(X)\n    assert n_splits == len(X)\n\n    # Test that split method raises ValueError when n_samples <= 1\n    X_single = np.array([[1, 2]])\n    with pytest.raises(ValueError, match=\"Cannot perform LeaveOneOut with n_samples=1\"):\n        list(loo.split(X_single))\n", "def test_leave_p_out_edge_cases():\n    # Test LeavePOut with p=1 should be equivalent to LeaveOneOut\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    lpo = LeavePOut(p=1)\n    loo = LeaveOneOut()\n\n    lpo_splits = list(lpo.split(X, y))\n    loo_splits = list(loo.split(X, y))\n\n    assert len(lpo_splits) == len(loo_splits)\n    for (train_lpo, test_lpo), (train_loo, test_loo) in zip(lpo_splits, loo_splits):\n        assert_array_equal(train_lpo, train_loo)\n        assert_array_equal(test_lpo, test_loo)\n\n    # Test LeavePOut with p greater than the number of samples should raise an error\n    with pytest.raises(ValueError, match=\"p=4 must be strictly less than the number of samples=3\"):\n        lpo = LeavePOut(p=4)\n        next(lpo.split(X, y))\n\n    # Test LeavePOut with p equal to the number of samples should raise an error\n    with pytest.raises(ValueError, match=\"p=3 must be strictly less than the number of samples=3\"):\n        lpo = LeavePOut(p=3)\n        next(lpo.split(X, y))\n\n    # Test LeavePOut with p=2\n    lpo = LeavePOut(p=2)\n    splits = list(lpo.split(X, y))\n    assert len(splits) == comb(len(X), 2)\n    expected_splits = [\n        (np.array([2]), np.array([0, 1])),\n        (np.array([1]), np.array([0, 2])),\n        (np.array([0]), np.array([1, 2]))\n    ]\n    for (train, test), (exp_train, exp_test) in zip(splits, expected_splits):\n        assert_array_equal(train, exp_train)\n        assert_array_equal(test, exp_test)\n", "def test_leave_p_out_indices():\n    # Check all indices are returned in the test sets\n    X1 = np.ones(18)\n    lpo = LeavePOut(p=3)\n    check_cv_coverage(lpo, X1, y=None, groups=None, expected_n_splits=816)  # 18 choose 3 is 816\n\n    # Check all indices are returned in the test sets even when different p values are used\n    for p in [1, 2, 4]:\n        lpo = LeavePOut(p=p)\n        expected_n_splits = int(comb(18, p))\n        check_cv_coverage(lpo, X1, y=None, groups=None, expected_n_splits=expected_n_splits)\n\n    # Check if get_n_splits returns the correct number of splits\n    assert LeavePOut(p=3).get_n_splits(X1) == 816\n", "def test_leave_p_out_empty_trainset():\n    cv = LeavePOut(p=1)\n    X, y = [[1]], [0]  # 1 sample\n    with pytest.raises(\n            ValueError,\n            match='Cannot perform LeavePOut with n_samples=1.'):\n        next(cv.split(X, y))\n\n    cv = LeavePOut(p=2)\n    X, y = [[1], [2]], [0, 1]  # 2 samples\n    with pytest.raises(\n            ValueError,\n            match='p=2 must be strictly less than the number of samples=2'):\n        next(cv.split(X, y))\n\n    cv = LeavePOut(p=3)\n    X, y = [[1], [2], [3]], [0, 1, 2]  # 3 samples\n    with pytest.raises(\n            ValueError,\n            match='p=3 must be strictly less than the number of samples=3'):\n        next(cv.split(X, y))\n"], "sample_850": ["def test_rbf_sampler_random_state():\n    # Test the RBFSampler's random_state functionality for reproducibility\n    gamma = 1.0\n    n_components = 500\n    random_state = 42\n\n    rbf_transform_1 = RBFSampler(gamma=gamma, n_components=n_components, random_state=random_state)\n    X_trans_1 = rbf_transform_1.fit_transform(X)\n    Y_trans_1 = rbf_transform_1.transform(Y)\n\n    rbf_transform_2 = RBFSampler(gamma=gamma, n_components=n_components, random_state=random_state)\n    X_trans_2 = rbf_transform_2.fit_transform(X)\n    Y_trans_2 = rbf_transform_2.transform(Y)\n\n    # Check that the two transformations are identical\n    assert_array_almost_equal(X_trans_1, X_trans_2)\n    assert_array_almost_equal(Y_trans_1, Y_trans_2)\n\n    # Check that using a different random_state produces different results\n    rbf_transform_3 = RBFSampler(gamma=gamma, n_components=n_components, random_state=random_state + 1)\n    X_trans_3 = rbf_transform_3.fit_transform(X)\n    Y_trans_3 = rbf_transform_3.transform(Y)\n\n    assert not np.allclose(X_trans_1, X_trans_3)\n    assert not np.allclose(Y_trans_1, Y_trans_3)\n", "def test_rbf_sampler_random_state():\n    # Test that RBFSampler produces consistent results with a fixed random state\n    gamma = 5.0\n    random_state = 42\n    rbf_transform_1 = RBFSampler(gamma=gamma, n_components=200, random_state=random_state)\n    rbf_transform_2 = RBFSampler(gamma=gamma, n_components=200, random_state=random_state)\n\n    X_trans_1 = rbf_transform_1.fit_transform(X)\n    X_trans_2 = rbf_transform_2.fit_transform(X)\n\n    assert_array_almost_equal(X_trans_1, X_trans_2)\n\n    # Test that a different random state produces different results\n    rbf_transform_3 = RBFSampler(gamma=gamma, n_components=200, random_state=random_state + 1)\n    X_trans_3 = rbf_transform_3.fit_transform(X)\n\n    with pytest.raises(AssertionError):\n        assert_array_almost_equal(X_trans_1, X_trans_3)\n", "def test_rbf_sampler_random_state():\n    # Test RBFSampler with different random states to ensure reproducibility\n    gamma = 10.\n    \n    # Test with int random_state\n    rbf_transform_1 = RBFSampler(gamma=gamma, n_components=100, random_state=42)\n    X_trans_1 = rbf_transform_1.fit_transform(X)\n    rbf_transform_2 = RBFSampler(gamma=gamma, n_components=100, random_state=42)\n    X_trans_2 = rbf_transform_2.fit_transform(X)\n    assert_array_almost_equal(X_trans_1, X_trans_2)\n    \n    # Test with RandomState instance\n    random_state_instance = np.random.RandomState(42)\n    rbf_transform_3 = RBFSampler(gamma=gamma, n_components=100, random_state=random_state_instance)\n    X_trans_3 = rbf_transform_3.fit_transform(X)\n    random_state_instance = np.random.RandomState(42)  # Reset to ensure reproducibility\n    rbf_transform_4 = RBFSampler(gamma=gamma, n_components=100, random_state=random_state_instance)\n    X_trans_4 = rbf_transform_4.fit_transform(X)\n    assert_array_almost_equal(X_trans_3, X_trans_4)\n    \n    # Test with None random_state (should produce different results)\n    rbf_transform_5 = RBFSampler(gamma=gamma, n_components=100, random_state=None)\n    X_trans_5 = rbf_transform_5.fit_transform(X)\n    rbf_transform_6 = RBFSampler(gamma=gamma, n_components=100, random_state=None)\n    X_trans_6 = rbf_transform_6.fit_transform(X)\n    with pytest.raises(AssertionError):\n        assert_array_almost_equal(X_trans_5, X_trans_6)\n", "def test_rbf_sampler_sparse():\n    # Test RBFSampler on sparse input data\n    gamma = 10.\n    kernel = rbf_kernel(X, Y, gamma=gamma)\n\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n\n    # approximate kernel mapping\n    rbf_transform = RBFSampler(gamma=gamma, n_components=1000, random_state=42)\n    X_trans = rbf_transform.fit_transform(X_sparse)\n    Y_trans = rbf_transform.transform(Y_sparse)\n    kernel_approx = np.dot(X_trans, Y_trans.T)\n\n    error = kernel - kernel_approx\n    assert np.abs(np.mean(error)) <= 0.01  # close to unbiased\n    np.abs(error, out=error)\n    assert np.max(error) <= 0.1  # nothing too far off\n    assert np.mean(error) <= 0.05  # mean is fairly close\n\n    # Ensure the transformed sparse matrices are dense arrays\n    assert isinstance(X_trans, np.ndarray)\n    assert isinstance(Y_trans, np.ndarray)\n", "def test_rbf_sampler_with_sparse_matrix():\n    # test that RBFSampler approximates kernel on sparse random data\n    # generate sparse data\n    rng = np.random.RandomState(0)\n    X_sparse = csr_matrix(rng.random_sample(size=(300, 50)))\n    Y_sparse = csr_matrix(rng.random_sample(size=(300, 50)))\n    X_sparse /= X_sparse.sum(axis=1)\n    Y_sparse /= Y_sparse.sum(axis=1)\n\n    # compute exact kernel\n    gamma = 10.\n    kernel = rbf_kernel(X_sparse.toarray(), Y_sparse.toarray(), gamma=gamma)\n\n    # approximate kernel mapping\n    rbf_transform = RBFSampler(gamma=gamma, n_components=1000, random_state=42)\n    X_trans = rbf_transform.fit_transform(X_sparse)\n    Y_trans = rbf_transform.transform(Y_sparse)\n    kernel_approx = np.dot(X_trans, Y_trans.T)\n\n    error = kernel - kernel_approx\n    assert np.abs(np.mean(error)) <= 0.01  # close to unbiased\n    np.abs(error, out=error)\n    assert np.max(error) <= 0.1  # nothing too far off\n    assert np.mean(error) <= 0.05  # mean is fairly close\n"], "sample_851": ["def test_regression_metrics_individual_cases(metric, y_true, y_pred, expected):\n    assert_almost_equal(metric(y_true, y_pred), expected, decimal=2)\n", "def test_metrics_non_numeric_input(metric):\n    y_true = [\"a\", \"b\", \"c\"]\n    y_pred = [1, 2, 3]\n    err_msg = \"Input contains non-numeric values.\"\n    with pytest.raises(ValueError, match=err_msg):\n        metric(y_true, y_pred)\n", "def test_max_error():\n    y_true = [3, 2, 7, 1]\n    y_pred = [4, 2, 7, 1]\n    error = max_error(y_true, y_pred)\n    assert_almost_equal(error, 1)\n\n    y_true = np.array([1.5, 2.5, 3.5])\n    y_pred = np.array([1.0, 2.0, 3.0])\n    error = max_error(y_true, y_pred)\n    assert_almost_equal(error, 0.5)\n\n    y_true = [0]\n    y_pred = [0]\n    error = max_error(y_true, y_pred)\n    assert_almost_equal(error, 0)\n\n    err_msg = \"Multioutput not supported in max_error\"\n    with pytest.raises(ValueError, match=err_msg):\n        max_error([[1, 2], [3, 4]], [[1, 2], [3, 4]])\n", "def test_max_error():\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2, 8]\n    assert_almost_equal(max_error(y_true, y_pred), 1.0)\n\n    # Test for multioutput scenario\n    y_true_multi = [[3, -0.5, 2, 7], [1, 2, -1, 4]]\n    y_pred_multi = [[2.5, 0.0, 2, 8], [1, 2, 1, 4]]\n    with pytest.raises(ValueError, match=\"Multioutput not supported in max_error\"):\n        max_error(y_true_multi, y_pred_multi)\n", "def test_max_error():\n    y_true = [1, 2, 3, 4]\n    y_pred = [1, 2, 3, 6]\n    assert_almost_equal(max_error(y_true, y_pred), 2)\n\n    y_true = [1.5, 2.5, 3.5, 4.5]\n    y_pred = [1.5, 2.5, 3.5, 4.8]\n    assert_almost_equal(max_error(y_true, y_pred), 0.3)\n\n    err_msg = \"Multioutput not supported in max_error\"\n    with pytest.raises(ValueError, match=err_msg):\n        max_error([[1, 2], [3, 4]], [[1, 2], [3, 5]])\n"], "sample_852": ["def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3, random_state=0)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n    assert np.all(np.bincount(y) == np.array([33, 33, 34])), \"Unexpected distribution of classes\"\n\n    # Test with different covariance\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3, cov=2.0, random_state=0)\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n    assert np.all(np.bincount(y) == np.array([33, 33, 34])), \"Unexpected distribution of classes\"\n\n    # Test with different mean\n    mean = [1, 1]\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3, mean=mean, random_state=0)\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n    assert np.all(np.bincount(y) == np.array([33, 33, 34])), \"Unexpected distribution of classes\"\n    assert_array_almost_equal(np.mean(X, axis=0), mean, decimal=0, err_msg=\"Mean is not correct\")\n", "def test_make_circles():\n    X, y = make_circles(n_samples=100, shuffle=False, noise=None, random_state=0, factor=0.8)\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (2,), \"Unexpected number of classes\"\n    assert_almost_equal(np.mean(y), 0.5, err_msg=\"Class balance mismatch\")\n\n    # Test with noise\n    X_noise, y_noise = make_circles(n_samples=100, shuffle=False, noise=0.1, random_state=0, factor=0.8)\n    assert np.std(X - X_noise) > 0, \"Noise not added correctly\"\n\n    # Test with shuffle\n    X_shuffled, y_shuffled = make_circles(n_samples=100, shuffle=True, noise=None, random_state=0, factor=0.8)\n    assert not np.array_equal(X, X_shuffled), \"Data not shuffled\"\n    assert not np.array_equal(y, y_shuffled), \"Labels not shuffled\"\n\n    # Test invalid factor\n    with pytest.raises(ValueError):\n        make_circles(n_samples=100, shuffle=False, noise=None, random_state=0, factor=1.1)\n", "def test_make_circles():\n    # Test default parameters\n    X, y = make_circles()\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (2,), \"Unexpected number of classes\"\n\n    # Test custom parameters\n    X, y = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=0)\n    assert X.shape == (200, 2), \"X shape mismatch\"\n    assert y.shape == (200,), \"y shape mismatch\"\n    assert np.unique(y).shape == (2,), \"Unexpected number of classes\"\n    assert_almost_equal(np.mean(np.linalg.norm(X[y == 0], axis=1)), 1.0, decimal=1, err_msg=\"Outer circle radius mismatch\")\n    assert_almost_equal(np.mean(np.linalg.norm(X[y == 1], axis=1)), 0.5, decimal=1, err_msg=\"Inner circle radius mismatch\")\n\n    # Test factor boundary conditions\n    with pytest.raises(ValueError):\n        make_circles(factor=1.0)\n    with pytest.raises(ValueError):\n        make_circles(factor=0.0)\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3,\n                                   random_state=0)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n\n    # Test for the mean and covariance values\n    mean = np.zeros(2)\n    cov = np.identity(2)\n    X, y = make_gaussian_quantiles(mean=mean, cov=cov, n_samples=100, \n                                   n_features=2, n_classes=3, random_state=0)\n    assert_array_almost_equal(np.mean(X, axis=0), mean, decimal=1, \n                              err_msg=\"Mean mismatch\")\n    assert_array_almost_equal(np.cov(X.T), cov, decimal=1, \n                              err_msg=\"Covariance mismatch\")\n", "def test_make_circles():\n    X, y = make_circles(n_samples=100, shuffle=True, noise=0.1, random_state=42, factor=0.5)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (2,), \"Unexpected number of classes\"\n    assert_almost_equal(np.mean(y), 0.5, decimal=1, err_msg=\"Classes not balanced\")\n    assert_almost_equal(np.var(X, axis=0).mean(), 0.5, decimal=1, err_msg=\"Unexpected variance in features\")\n\n    with pytest.raises(ValueError, match=\"'factor' has to be between 0 and 1.\"):\n        make_circles(n_samples=100, factor=1.5)\n"], "sample_853": ["def test_transform_target_regressor_default_transformer():\n    # Check that the default transformer used is an identity function when\n    # no transformer, func, or inverse_func are provided.\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression())\n    y_pred = regr.fit(X, y).predict(X)\n    \n    # Check that the predicted values match the regressor's predictions\n    assert_allclose(y_pred, regr.regressor_.predict(X))\n    \n    # Check that the transformer is a FunctionTransformer with identity function\n    assert isinstance(regr.transformer_, FunctionTransformer)\n    assert regr.transformer_.func is None\n    assert regr.transformer_.inverse_func is None\n    \n    # Check that the transform and inverse_transform functions are identity\n    y_transformed = regr.transformer_.transform(y.reshape(-1, 1)).squeeze()\n    assert_allclose(y, y_transformed)\n    y_inverse_transformed = regr.transformer_.inverse_transform(y_transformed.reshape(-1, 1)).squeeze()\n    assert_allclose(y, y_inverse_transformed)\n", "def test_transform_target_regressor_default_identity_transformer():\n    # Test when no transformer or functions are provided, the default identity\n    # transformer is used.\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression())\n    y_pred = regr.fit(X, y).predict(X)\n    assert y.shape == y_pred.shape\n    assert_allclose(y, y_pred)\n    # check the transformer output\n    y_tran = regr.transformer_.transform(y.reshape(-1, 1)).squeeze()\n    assert_allclose(y, y_tran)\n    assert_allclose(y, regr.transformer_.inverse_transform(y_tran.reshape(-1, 1)).squeeze())\n    # check the regressor output\n    lr = LinearRegression().fit(X, y)\n    assert_allclose(regr.regressor_.coef_.ravel(), lr.coef_.ravel())\n", "def test_transform_target_regressor_default_transformer():\n    # Test the default identity transformer\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression())\n    y_pred = regr.fit(X, y).predict(X)\n    # check the transformer output (should be identity)\n    y_tran = regr.transformer_.transform(y.reshape(-1, 1)).squeeze()\n    assert_allclose(y, y_tran)\n    assert_allclose(y, regr.transformer_.inverse_transform(y_tran.reshape(-1, 1)).squeeze())\n    assert y.shape == y_pred.shape\n    assert_allclose(y_pred, regr.regressor_.predict(X))\n    # check the regressor output\n    lr = LinearRegression().fit(X, y)\n    assert_allclose(regr.regressor_.coef_.ravel(), lr.coef_.ravel())\n", "def test_transform_target_regressor_default_transformer():\n    # Test TransformedTargetRegressor with default transformer (identity)\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression())\n    y_pred = regr.fit(X, y).predict(X)\n    assert y.shape == y_pred.shape\n    # check the transformer output (should be identity)\n    y_tran = regr.transformer_.transform(y.reshape(-1, 1)).squeeze()\n    assert_allclose(y, y_tran)\n    assert_allclose(y, regr.transformer_.inverse_transform(\n        y_tran.reshape(-1, 1)).squeeze())\n    # check the regressor output\n    lr = LinearRegression().fit(X, y)\n    assert_allclose(regr.regressor_.coef_.ravel(), lr.coef_.ravel())\n", "def test_transform_target_regressor_identity_transformer():\n    # Test the default behavior when no transformer or functions are specified\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression())\n    y_pred = regr.fit(X, y).predict(X)\n    \n    # Check that the transformer used is an identity transformer\n    assert isinstance(regr.transformer_, FunctionTransformer)\n    assert regr.transformer_.func is None\n    assert regr.transformer_.inverse_func is None\n    \n    # Check that the predictions match those of the base regressor without any transformation\n    lr = LinearRegression().fit(X, y)\n    y_lr_pred = lr.predict(X)\n    assert_allclose(y_pred, y_lr_pred)\n"], "sample_854": ["def test_coef_attribute_error():\n    # Test that trying to access coef_ attribute when kernel is not linear raises an AttributeError\n    clf = svm.SVC(kernel='rbf').fit(X, Y)\n    with pytest.raises(AttributeError):\n        _ = clf.coef_\n\n    clf = svm.NuSVC(kernel='poly').fit(X, Y)\n    with pytest.raises(AttributeError):\n        _ = clf.coef_\n\n    clf = svm.SVR(kernel='sigmoid').fit(X, Y)\n    with pytest.raises(AttributeError):\n        _ = clf.coef_\n\n    clf = svm.NuSVR(kernel='precomputed').fit(np.dot(X, np.array(X).T), Y)\n    with pytest.raises(AttributeError):\n        _ = clf.coef_\n", "def test_libsvm_linear_kernel():\n    # Test libsvm with linear kernel\n    clf = svm.SVC(kernel='linear')\n    clf.fit(X, Y)\n    \n    # Check if the coefficients and intercept are calculated correctly\n    expected_coef = np.array([[-0.25, 0.25]])\n    expected_intercept = np.array([0.])\n    \n    assert_array_almost_equal(clf.coef_, expected_coef, decimal=3)\n    assert_array_almost_equal(clf.intercept_, expected_intercept, decimal=3)\n\n    # Test prediction\n    pred = clf.predict(T)\n    assert_array_equal(pred, true_result)\n    \n    # Check decision function\n    expected_decision_function = np.dot(T, clf.coef_.T) + clf.intercept_\n    assert_array_almost_equal(clf.decision_function(T), expected_decision_function, decimal=3)\n", "def test_libsvm_one_vs_one_coef():\n    # Test the function _one_vs_one_coef for generating primal coefficients\n    dual_coef = np.array([\n        [0.5, -0.5],\n        [-0.3, 0.3]\n    ])\n    n_support = [1, 1, 1]\n    support_vectors = np.array([\n        [1, 2],\n        [3, 4],\n        [5, 6]\n    ])\n\n    expected_coef = [\n        np.array([2.5, 3.5]),\n        np.array([4.3, 5.3]),\n        np.array([2.2, 3.2])\n    ]\n\n    coef = _one_vs_one_coef(dual_coef, n_support, support_vectors)\n\n    for c, ec in zip(coef, expected_coef):\n        assert_array_almost_equal(c, ec, decimal=2)\n", "def test_libsvm_gamma_validation():\n    # Test if the gamma parameter validation works correctly in BaseLibSVM\n    class TestSVC(BaseLibSVM):\n            self._impl = 'c_svc'\n            super().__init__(**kwargs)\n        \n            return super().fit(X, y, sample_weight)\n\n    # Test invalid gamma values\n    with pytest.raises(ValueError, match=\"The gamma value of 0.0 is invalid\"):\n        TestSVC(kernel='rbf', degree=3, gamma=0, coef0=0.0, tol=1e-3, C=1.0, nu=0.5, epsilon=0.1,\n                shrinking=True, probability=False, cache_size=200, class_weight=None, verbose=False,\n                max_iter=-1, random_state=None).fit(X, Y)\n\n    with pytest.raises(ValueError, match=\"When 'gamma' is a string, it should be either 'scale' or 'auto'\"):\n        TestSVC(kernel='rbf', degree=3, gamma='invalid', coef0=0.0, tol=1e-3, C=1.0, nu=0.5, epsilon=0.1,\n                shrinking=True, probability=False, cache_size=200, class_weight=None, verbose=False,\n                max_iter=-1, random_state=None).fit(X, Y)\n", "def test_libsvm_fit():\n    # Test the fit method of BaseLibSVM with different solver types\n    class TestSVM(BaseLibSVM):\n                     tol=1e-3, C=1.0, nu=0.5, epsilon=0.1, shrinking=True,\n                     probability=False, cache_size=200, class_weight=None,\n                     verbose=False, max_iter=-1, random_state=None):\n            self._impl = 'c_svc'\n            super().__init__(kernel, degree, gamma, coef0, tol, C, nu, epsilon,\n                             shrinking, probability, cache_size, class_weight,\n                             verbose, max_iter, random_state)\n\n    X_train = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y_train = [1, 1, 1, 2, 2, 2]\n    clf = TestSVM()\n    clf.fit(X_train, y_train)\n\n    assert hasattr(clf, \"support_\")\n    assert hasattr(clf, \"support_vectors_\")\n    assert hasattr(clf, \"_n_support\")\n    assert hasattr(clf, \"dual_coef_\")\n    assert hasattr(clf, \"intercept_\")\n    assert hasattr(clf, \"probA_\")\n    assert hasattr(clf, \"probB_\")\n    assert hasattr(clf, \"fit_status_\")\n    assert_array_equal(clf.support_, [1, 3])\n    assert_array_equal(clf.support_vectors_, np.array(X_train)[clf.support_])\n    assert_array_equal(clf.intercept_, [0.])\n    assert clf.fit_status_ == 0\n\n    with pytest.raises(ValueError):\n        clf.fit([[0, 0], [1, 1]], [0])\n\n    with pytest.raises(ValueError):\n        clf.fit([[0, 0], [1, 1]], [0, 0])\n\n    with pytest.raises(ValueError):\n        clf.fit([[0, 0], [1, 1]], [0, 1], sample_weight=[1, 0, 0])\n\n    with pytest.raises(ValueError):\n        clf.fit(np.array([[0, 0], [1, 1]]), np.array([0, "], "sample_855": ["def test_dummy_classifier_sparse_and_dense_equivalence():\n    X = [[0]] * 5  # ignored\n    y_dense = np.array([1, 2, 1, 1, 2])\n    y_sparse = sp.csc_matrix(y_dense).T  # make y sparse\n\n    for strategy in [\"stratified\", \"most_frequent\", \"prior\", \"uniform\", \"constant\"]:\n        if strategy == \"constant\":\n            constant_value = 1\n        else:\n            constant_value = None\n\n        clf_dense = DummyClassifier(strategy=strategy, random_state=0, constant=constant_value)\n        clf_sparse = DummyClassifier(strategy=strategy, random_state=0, constant=constant_value)\n\n        clf_dense.fit(X, y_dense)\n        clf_sparse.fit(X, y_sparse)\n\n        assert_array_equal(clf_dense.predict(X), clf_sparse.predict(X))\n\n        if strategy != \"uniform\":\n            assert_array_equal(clf_dense.predict_proba(X), clf_sparse.predict_proba(X))\n            assert_array_equal(clf_dense.predict_log_proba(X), clf_sparse.predict_log_proba(X))\n", "def test_dummy_classifier_predict_log_proba():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = [1, 2, 1, 1]\n\n    clf = DummyClassifier(strategy=\"most_frequent\", random_state=0)\n    clf.fit(X, y)\n    log_proba = clf.predict_log_proba(X)\n    expected_log_proba = np.log(clf.predict_proba(X))\n    assert_array_almost_equal(log_proba, expected_log_proba)\n", "def test_dummy_regressor_invalid_strategy():\n    X = [[0]] * 5\n    y = [1, 2, 4, 6, 8]\n\n    # Test invalid strategy for DummyRegressor\n    est = DummyRegressor(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, est.fit, X, y)\n", "def test_invalid_parameters_for_classifier():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = [1, 2, 1, 1]\n\n    # Test invalid strategy\n    clf = DummyClassifier(strategy=\"invalid_strategy\")\n    with pytest.raises(ValueError, match=\"Unknown strategy type: invalid_strategy\"):\n        clf.fit(X, y)\n\n    # Test constant strategy without constant value\n    clf = DummyClassifier(strategy=\"constant\")\n    with pytest.raises(ValueError, match=\"Constant target value has to be specified\"):\n        clf.fit(X, y)\n\n    # Test constant strategy with invalid constant value\n    clf = DummyClassifier(strategy=\"constant\", constant=[1, 2])\n    with pytest.raises(ValueError, match=\"Constant target value should have shape\"):\n        clf.fit(X, y)\n\n    # Test constant strategy with constant value not in training data\n    clf = DummyClassifier(strategy=\"constant\", constant=3)\n    with pytest.raises(ValueError, match=\"The constant target value must be present in the training data\"):\n        clf.fit(X, y)\n", "def test_dummy_classifier_with_different_strategies():\n    X = [[0]] * 6  # ignored\n    y = [0, 1, 2, 3, 4, 5]\n\n    strategies = [\"most_frequent\", \"stratified\", \"uniform\", \"prior\", \"constant\"]\n\n    for strategy in strategies:\n        clf = DummyClassifier(strategy=strategy, random_state=42, constant=1)\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n        assert y_pred is not None  # Ensure that predictions are made\n        assert len(y_pred) == len(X)  # Ensure that the number of predictions matches input size\n\n        if strategy == \"constant\":\n            assert_array_equal(y_pred, np.ones(len(X)))  # Check constant prediction\n\n        _check_predict_proba(clf, X, y)  # Check probability predictions\n\n    # Special case for constant strategy with a string constant\n    y = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\"]\n    clf = DummyClassifier(strategy=\"constant\", random_state=42, constant=\"banana\")\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, [\"banana\"] * len(X))  # Check constant prediction with string\n    _check_predict_proba(clf, X, y)\n"], "sample_856": ["def test_kfold_with_shuffle():\n    X = np.arange(20).reshape((10, 2))\n    y = np.array([0, 1] * 5)\n    \n    # Check shuffling with different random states produces different splits\n    kf1 = KFold(n_splits=2, shuffle=True, random_state=42)\n    kf2 = KFold(n_splits=2, shuffle=True, random_state=43)\n    splits1 = list(kf1.split(X))\n    splits2 = list(kf2.split(X))\n\n    assert splits1 != splits2, \"Shuffling with different random states should produce different splits\"\n    \n    # Check reproducibility with same random state\n    kf3 = KFold(n_splits=2, shuffle=True, random_state=42)\n    splits3 = list(kf3.split(X))\n    \n    assert splits1 == splits3, \"Shuffling with same random state should produce same splits\"\n    \n    # Check that the shuffled splits still cover all samples\n    all_test_indices = np.concatenate([test for _, test in splits1])\n    assert len(all_test_indices) == len(X), \"All samples should be covered in the splits\"\n    assert len(np.unique(all_test_indices)) == len(X), \"Each sample should appear exactly once in the splits\"\n", "def test_leave_one_out_split():\n    # Test the LeaveOneOut split functionality\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    loo = LeaveOneOut()\n\n    expected_splits = [\n        (np.array([1, 2, 3]), np.array([0])),\n        (np.array([0, 2, 3]), np.array([1])),\n        (np.array([0, 1, 3]), np.array([2])),\n        (np.array([0, 1, 2]), np.array([3])),\n    ]\n\n    for i, (train_index, test_index) in enumerate(loo.split(X)):\n        expected_train_index, expected_test_index = expected_splits[i]\n        assert_array_equal(train_index, expected_train_index)\n        assert_array_equal(test_index, expected_test_index)\n        assert_array_equal(X[train_index], X[expected_train_index])\n        assert_array_equal(X[test_index], X[expected_test_index])\n        assert_array_equal(y[train_index], y[expected_train_index])\n        assert_array_equal(y[test_index], y[expected_test_index])\n\n    # Check if get_n_splits returns the number of samples\n    assert loo.get_n_splits(X) == len(X)\n", "def test_time_series_split_max_train_size():\n    X = np.arange(20).reshape(-1, 1)\n    y = np.arange(20)\n\n    # Test max_train_size with different values\n    tscv = TimeSeriesSplit(n_splits=3, max_train_size=5)\n    splits = list(tscv.split(X))\n    assert len(splits) == 3\n    assert_array_equal(splits[0][0], np.array([0, 1, 2, 3, 4]))\n    assert_array_equal(splits[0][1], np.array([5, 6]))\n    assert_array_equal(splits[1][0], np.array([0, 1, 2, 3, 4, 5, 6]))\n    assert_array_equal(splits[1][1], np.array([7, 8, 9]))\n    assert_array_equal(splits[2][0], np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]))\n    assert_array_equal(splits[2][1], np.array([13, 14, 15, 16, 17]))\n\n    # Test max_train_size with max_train_size less than the number of training samples in the first split\n    tscv = TimeSeriesSplit(n_splits=3, max_train_size=3)\n    splits = list(tscv.split(X))\n    assert len(splits) == 3\n    assert_array_equal(splits[0][0], np.array([0, 1, 2]))\n    assert_array_equal(splits[0][1], np.array([3, 4]))\n    assert_array_equal(splits[1][0], np.array([0, 1, 2, 3, 4]))\n    assert_array_equal(splits[1][1], np.array([5, 6, 7, 8, 9]))\n    assert_array_equal(splits[2][0], np.array([4, 5, 6, 7, 8, 9, 10, 11, 12]))\n    assert_array_equal(splits[2][1], np.array([13, 14, 15, 16, 17]))\n\n    # Ensure splits respect max_train_size when max_train_size is greater than the number of training samples in the last split\n", "def test_predefinedsplit_with_various_splits():\n    # Test PredefinedSplit with various split scenarios\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([0, 1, 0, 1, 0])\n    \n    # Case 1: All samples are in one fold\n    ps = PredefinedSplit([0, 0, 0, 0, 0])\n    splits = list(ps.split(X, y))\n    assert len(splits) == 1\n    assert_array_equal(splits[0][0], [])  # No training samples\n    assert_array_equal(splits[0][1], np.arange(5))  # All samples in the test set\n\n    # Case 2: Each sample is its own fold\n    ps = PredefinedSplit([0, 1, 2, 3, 4])\n    splits = list(ps.split(X, y))\n    assert len(splits) == 5\n    for i, (train_idx, test_idx) in enumerate(splits):\n        assert_array_equal(train_idx, np.delete(np.arange(5), i))\n        assert_array_equal(test_idx, [i])\n\n    # Case 3: Some samples are in a fold, some are not\n    ps = PredefinedSplit([0, 0, -1, 1, 1])\n    splits = list(ps.split(X, y))\n    assert len(splits) == 2\n    assert_array_equal(splits[0][0], [2, 3, 4])\n    assert_array_equal(splits[0][1], [0, 1])\n    assert_array_equal(splits[1][0], [0, 1, 2])\n    assert_array_equal(splits[1][1], [3, 4])\n\n    # Check get_n_splits return value\n    assert ps.get_n_splits() == 2\n", "def test_get_n_splits_with_none_y_and_groups():\n    # Ensuring get_n_splits raises an error when y and groups are None\n    loo = LeaveOneOut()\n    lpo = LeavePOut(p=2)\n    kf = KFold(n_splits=2)\n    skf = StratifiedKFold(n_splits=2)\n    lolo = LeaveOneGroupOut()\n    lopo = LeavePGroupsOut(n_groups=2)\n    ss = ShuffleSplit(random_state=0)\n    ps = PredefinedSplit([1, 1, 2, 2])\n\n    for cv in [loo, lpo, kf, skf, lolo, lopo, ss, ps]:\n        with pytest.raises(ValueError, match=\"The 'X' parameter should not be None.\"):\n            cv.get_n_splits(None)\n\n    # Ensuring get_n_splits raises an error when groups are None for Group-based splitters\n    for cv in [lolo, lopo]:\n        with pytest.raises(ValueError, match=\"The 'groups' parameter should not be None.\"):\n            cv.get_n_splits(X=np.ones(10))\n"], "sample_857": ["def test_predict_log_proba():\n    # Test predict_log_proba method for classifiers.\n    for name, Tree in CLF_TREES.items():\n        clf = Tree(random_state=0)\n        clf.fit(iris.data, iris.target)\n        log_proba = clf.predict_log_proba(iris.data)\n        proba = clf.predict_proba(iris.data)\n        assert_array_almost_equal(np.exp(log_proba), proba, err_msg=\"Failed with {0}\".format(name))\n\n        # Test with max_depth set\n        clf = Tree(max_depth=3, random_state=0)\n        clf.fit(iris.data, iris.target)\n        log_proba = clf.predict_log_proba(iris.data)\n        proba = clf.predict_proba(iris.data)\n        assert_array_almost_equal(np.exp(log_proba), proba, err_msg=\"Failed with {0}\".format(name))\n", "def test_get_depth_n_leaves_unfitted_tree():\n    # Ensure that calling get_depth and get_n_leaves on an unfitted tree raises NotFittedError\n    for name, TreeEstimator in ALL_TREES.items():\n        est = TreeEstimator()\n        with pytest.raises(NotFittedError):\n            est.get_depth()\n        with pytest.raises(NotFittedError):\n            est.get_n_leaves()\n", "def test_zero_split_impurity():\n    # Check that no split occurs when min_impurity_split is set to a very high value\n    high_impurity_split = 10.0\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n    y = np.array([0, 1, 1, 0, 1])\n    \n    for name, TreeEstimator in ALL_TREES.items():\n        if \"ExtraTree\" not in name:  # Exclude ExtraTree since they involve randomness\n            with ignore_warnings(category=DeprecationWarning):\n                est = TreeEstimator(min_impurity_split=high_impurity_split, random_state=0)\n                est.fit(X, y)\n                # The tree should only have one node (the root)\n                assert est.tree_.node_count == 1\n                assert est.tree_.children_left[0] == TREE_LEAF\n                assert est.tree_.children_right[0] == TREE_LEAF\n", "def test_max_features_auto_regressor():\n    # Test `max_features='auto'` behavior for regressors\n    reg = DecisionTreeRegressor(max_features=\"auto\", random_state=0)\n    reg.fit(boston.data, boston.target)\n    # `max_features='auto'` should use all features for regression\n    assert reg.max_features_ == boston.data.shape[1]\n\n    reg = ExtraTreeRegressor(max_features=\"auto\", random_state=0)\n    reg.fit(boston.data, boston.target)\n    assert reg.max_features_ == boston.data.shape[1]\n", "def test_fit_with_ccp_alpha():\n    # Check fitting with non-zero ccp_alpha\n\n    # DecisionTreeClassifier\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=0.1)\n    clf.fit(iris.data, iris.target)\n    score = accuracy_score(clf.predict(iris.data), iris.target)\n    assert score > 0.5, (\n        \"Failed with DecisionTreeClassifier and score = {0}\".format(score)\n    )\n\n    # ExtraTreeClassifier\n    clf = ExtraTreeClassifier(random_state=0, ccp_alpha=0.1)\n    clf.fit(iris.data, iris.target)\n    score = accuracy_score(clf.predict(iris.data), iris.target)\n    assert score > 0.5, (\n        \"Failed with ExtraTreeClassifier and score = {0}\".format(score)\n    )\n\n    # DecisionTreeRegressor\n    reg = DecisionTreeRegressor(random_state=0, ccp_alpha=0.1)\n    reg.fit(boston.data, boston.target)\n    score = mean_squared_error(boston.target, reg.predict(boston.data))\n    assert score < 10, (\n        \"Failed with DecisionTreeRegressor and score = {0}\".format(score)\n    )\n\n    # ExtraTreeRegressor\n    reg = ExtraTreeRegressor(random_state=0, ccp_alpha=0.1)\n    reg.fit(boston.data, boston.target)\n    score = mean_squared_error(boston.target, reg.predict(boston.data))\n    assert score < 10, (\n        \"Failed with ExtraTreeRegressor and score = {0}\".format(score)\n    )\n"], "sample_858": ["def test_voting_regressor_sample_weight():\n    \"\"\"Check sample_weight parameter of VotingRegressor.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    reg3 = DecisionTreeRegressor(random_state=123)\n    \n    sample_weight = np.random.RandomState(123).uniform(size=(len(y_r),))\n\n    ereg1 = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2), ('dt', reg3)])\n    ereg1.fit(X_r, y_r, sample_weight=sample_weight)\n    reg1.fit(X_r, y_r, sample_weight=sample_weight)\n    reg2.fit(X_r, y_r, sample_weight=sample_weight)\n    reg3.fit(X_r, y_r, sample_weight=sample_weight)\n    \n    assert_array_almost_equal(ereg1.predict(X_r), np.average(\n        np.vstack([reg1.predict(X_r), reg2.predict(X_r), reg3.predict(X_r)]), \n        axis=0, weights=ereg1._weights_not_none))\n    \n    ereg2 = VotingRegressor(estimators=[('lr', reg1)], weights=[1])\n    ereg2.fit(X_r, y_r, sample_weight=sample_weight)\n    assert_array_almost_equal(ereg2.predict(X_r), reg1.predict(X_r))\n    \n    # Check that error is raised if sample_weight is not supported by one of the estimators\n    reg4 = DummyRegressor()\n    ereg3 = VotingRegressor(estimators=[('lr', reg1), ('dummy', reg4)], weights=[1, 1])\n    msg = ('Underlying estimator DummyRegressor does not support sample weights.')\n    with pytest.raises(TypeError, match=msg):\n        ereg3.fit(X_r, y_r, sample_weight=sample_weight)\n    \n    # Check that _parallel_fit_estimator raises the right error if not linked to sample_weight\n    class RegressorErrorFit(RegressorMixin, BaseEstimator):\n            raise TypeError('Error unrelated to sample_weight.')\n    reg = RegressorErrorFit()\n    with pytest.raises(TypeError, match='Error unrelated to sample_weight'):\n        reg.fit(X_r, y_r, sample_weight=sample_weight)\n", "def test_voting_regressor_transform():\n    \"\"\"Check transform method of VotingRegressor on boston dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    X_r_train, X_r_test, y_r_train, y_r_test = train_test_split(X_r, y_r, test_size=0.25, random_state=123)\n\n    ereg1 = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2)],\n        weights=[1, 1]).fit(X_r_train, y_r_train)\n\n    assert ereg1.transform(X_r_test).shape == (len(X_r_test), 2)\n    preds = ereg1.transform(X_r_test)\n    assert_array_almost_equal(preds[:, 0], reg1.fit(X_r_train, y_r_train).predict(X_r_test))\n    assert_array_almost_equal(preds[:, 1], reg2.fit(X_r_train, y_r_train).predict(X_r_test))\n", "def test_voting_regressor_predict():\n    \"\"\"Check predicted values for VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    reg3 = DummyRegressor(strategy='mean')\n\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16]])\n    y = np.array([1, 4, 9, 16])\n\n    assert_array_almost_equal(reg1.fit(X, y).predict(X), [1, 4, 9, 16], decimal=1)\n    assert_array_almost_equal(reg2.fit(X, y).predict(X), [1, 4, 9, 16], decimal=1)\n    assert_array_almost_equal(reg3.fit(X, y).predict(X), [7.5, 7.5, 7.5, 7.5], decimal=1)\n\n    ereg = VotingRegressor(estimators=[\n                            ('lr', reg1), ('rf', reg2), ('mean', reg3)],\n                            weights=[2, 1, 1])\n    assert_array_almost_equal(ereg.fit(X, y).predict(X), [1.5, 4, 9, 16], decimal=1)\n", "def test_voting_regressor_on_toy_problem():\n    \"\"\"Manually check predicted regression values for toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=123)\n    reg3 = DummyRegressor(strategy='mean')\n    \n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([2, 4, 6, 8, 10])\n    \n    reg1.fit(X, y)\n    reg2.fit(X, y)\n    reg3.fit(X, y)\n    \n    eclf = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dr', reg3)], weights=[1, 1, 1])\n    eclf.fit(X, y)\n    \n    y_pred = eclf.predict(X)\n    expected_pred = np.mean(np.vstack([reg1.predict(X), reg2.predict(X), reg3.predict(X)]), axis=0)\n    \n    assert_array_almost_equal(y_pred, expected_pred, decimal=2)\n", "def test_voting_regressor_with_different_weights():\n    \"\"\"Check weighted average regression with different weights.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    reg3 = DecisionTreeRegressor(random_state=123)\n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2), ('dt', reg3)], weights=[0.2, 0.3, 0.5])\n\n    X_r_train, X_r_test, y_r_train, y_r_test = train_test_split(X_r, y_r, test_size=0.25, random_state=123)\n\n    reg1_pred = reg1.fit(X_r_train, y_r_train).predict(X_r_test)\n    reg2_pred = reg2.fit(X_r_train, y_r_train).predict(X_r_test)\n    reg3_pred = reg3.fit(X_r_train, y_r_train).predict(X_r_test)\n    ereg_pred = ereg.fit(X_r_train, y_r_train).predict(X_r_test)\n\n    avg = np.average(np.asarray([reg1_pred, reg2_pred, reg3_pred]), axis=0, weights=[0.2, 0.3, 0.5])\n    assert_almost_equal(ereg_pred, avg, decimal=2)\n\n    # Check behavior when weights are all set to zero\n    ereg_zero_weights = VotingRegressor([('lr', reg1), ('rf', reg2), ('dt', reg3)], weights=[0, 0, 0])\n    msg = 'All weights are zero.'\n    assert_raise_message(ValueError, msg, ereg_zero_weights.fit, X_r_train, y_r_train)\n"], "sample_859": ["def test_elastic_net_intercept():\n    # Test that ElasticNet correctly handles intercept fitting and centering\n    X = [[0], [0], [1]]\n    y = [1, 2, 3]\n    \n    clf = ElasticNet(alpha=0.1, fit_intercept=True)\n    clf.fit(X, y)\n    pred = clf.predict([[1], [2], [3]])\n    assert_almost_equal(clf.intercept_, 2.0)\n    assert_array_almost_equal(pred, [2.25, 2.5, 2.75], decimal=2)\n\n    clf = ElasticNet(alpha=0.1, fit_intercept=False)\n    clf.fit(X, y)\n    pred = clf.predict([[1], [2], [3]])\n    assert_almost_equal(clf.intercept_, 0.0)\n    assert_array_almost_equal(pred, [0.0, 0.0, 0.0], decimal=2)\n", "def test_alpha_grid():\n    # Test the _alpha_grid function with different inputs\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float64)\n    y = np.array([1, 2, 3], dtype=np.float64)\n    l1_ratio = 0.5\n    eps = 1e-3\n    n_alphas = 5\n    \n    # Check alpha grid computation\n    alphas = _alpha_grid(X, y, l1_ratio=l1_ratio, eps=eps, n_alphas=n_alphas)\n    assert alphas.shape[0] == n_alphas\n    assert alphas[0] > alphas[-1]\n    \n    # Check if the error is raised for l1_ratio = 0\n    with pytest.raises(ValueError, match=\"Automatic alpha grid generation is not supported\"):\n        _alpha_grid(X, y, l1_ratio=0, eps=eps, n_alphas=n_alphas)\n", "def test_alpha_grid():\n    # Test the _alpha_grid function to ensure it generates correct alpha values\n    X, y, _, _ = build_dataset(n_samples=50, n_features=10)\n    alphas = _alpha_grid(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100)\n    assert len(alphas) == 100, \"The length of alphas should be 100\"\n    assert np.all(np.diff(alphas) <= 0), \"Alphas should be in decreasing order\"\n\n    # Test l1_ratio=1 (Lasso)\n    alphas = _alpha_grid(X, y, l1_ratio=1.0, eps=1e-3, n_alphas=50)\n    assert len(alphas) == 50, \"The length of alphas should be 50\"\n\n    # Test ValueError for l1_ratio=0\n    with pytest.raises(ValueError, match=\"l1_ratio=0. Please supply a grid\"):\n        _alpha_grid(X, y, l1_ratio=0.0, eps=1e-3, n_alphas=100)\n", "def test_alpha_grid_negative_l1_ratio():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    \n    with pytest.raises(ValueError, match=\"l1_ratio=0\"):\n        _alpha_grid(X, y, l1_ratio=0)\n    \n    with pytest.raises(ValueError, match=\"l1_ratio=0\"):\n        _alpha_grid(X, y, l1_ratio=0)\n", "def test_enet_intercept():\n    # Test ElasticNet with fit_intercept=True and fit_intercept=False\n    X = np.array([[-1.], [0.], [1.]])\n    Y = np.array([-2, 0, 2])  # a line with an intercept\n    T = np.array([[2.], [3.], [4.]])  # test sample\n\n    # fit_intercept=True\n    clf = ElasticNet(alpha=0.5, l1_ratio=0.5, fit_intercept=True)\n    clf.fit(X, Y)\n    pred = clf.predict(T)\n    assert_array_almost_equal(clf.coef_, [0.8], 2)\n    assert_almost_equal(clf.intercept_, 0.4, 2)\n    assert_array_almost_equal(pred, [2.0, 2.8, 3.6], 2)\n    assert_almost_equal(clf.dual_gap_, 0, 4)\n\n    # fit_intercept=False\n    clf = ElasticNet(alpha=0.5, l1_ratio=0.5, fit_intercept=False)\n    clf.fit(X, Y)\n    pred = clf.predict(T)\n    assert_array_almost_equal(clf.coef_, [1.3333], 3)\n    assert_almost_equal(clf.intercept_, 0, 3)\n    assert_array_almost_equal(pred, [2.6667, 4.0000, 5.3333], 3)\n    assert_almost_equal(clf.dual_gap_, 0, 4)\n"], "sample_860": ["def test_check_random_state():\n    # Test that check_random_state works correctly with various inputs\n\n    # When seed is None, it should return the global RandomState singleton\n    assert check_random_state(None) is np.random.mtrand._rand\n\n    # When seed is an integer, it should return a new RandomState instance\n    seed = 42\n    random_state = check_random_state(seed)\n    assert isinstance(random_state, np.random.RandomState)\n    assert random_state.randint(0, 100) == np.random.RandomState(seed).randint(0, 100)\n\n    # When seed is already a RandomState instance, it should return the same instance\n    rs_instance = np.random.RandomState(seed)\n    assert check_random_state(rs_instance) is rs_instance\n\n    # When seed is an invalid type, it should raise a ValueError\n    for invalid_seed in [\"invalid\", 3.14, [1, 2, 3]]:\n        with pytest.raises(ValueError, match=f\"{repr(invalid_seed)} cannot be used to seed a numpy.random.RandomState instance\"):\n            check_random_state(invalid_seed)\n", "def test_check_array_with_sparse_matrix_formats():\n    # Test check_array with different sparse matrix formats\n    sparse_formats = ['csr', 'csc', 'coo', 'bsr', 'lil', 'dok', 'dia']\n    X = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n    for fmt in sparse_formats:\n        sp_matrix = getattr(sp, fmt + '_matrix')(X)\n        X_checked = check_array(sp_matrix, accept_sparse=sparse_formats)\n        assert sp.issparse(X_checked)\n        assert X_checked.format == fmt\n        assert_allclose_dense_sparse(X_checked, sp_matrix)\n\n    # Test exception for invalid sparse format\n    with pytest.raises(ValueError, match=\"Parameter 'accept_sparse' should be a string,\"):\n        check_array(X, accept_sparse=['invalid_format'])\n", "def test_check_symmetric_sparse_formats():\n    # Check that different sparse formats are handled correctly\n    array_sym = np.array([[1, 2, 3], [2, 5, 6], [3, 6, 9]])\n    sparse_formats = ['csr', 'csc', 'coo', 'lil', 'dok', 'bsr', 'dia']\n\n    for fmt in sparse_formats:\n        sparse_matrix = getattr(sp, fmt + '_matrix')(array_sym)\n        assert_array_equal(check_symmetric(sparse_matrix).toarray(), array_sym)\n\n    # Test with non-symmetric matrix and ensure it is symmetrized\n    array_asym = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    symmetrized_array = 0.5 * (array_asym + array_asym.T)\n    \n    for fmt in sparse_formats:\n        sparse_matrix = getattr(sp, fmt + '_matrix')(array_asym)\n        sym_matrix = check_symmetric(sparse_matrix, raise_warning=False)\n        assert_array_equal(sym_matrix.toarray(), symmetrized_array)\n", "def test_check_memory_with_invalid_version():\n    # Test check_memory with joblib version less than 0.12\n    import joblib\n    from distutils.version import LooseVersion\n    import joblib.memory\n\n    if LooseVersion(joblib.__version__) >= '0.12':\n        pytest.skip(\"This test is only applicable for joblib versions < 0.12\")\n\n    memory = check_memory(\"cache_directory\")\n    assert memory.cachedir == 'cache_directory'\n    assert isinstance(memory, joblib.memory.Memory)\n\n    memory = check_memory(None)\n    assert memory.cachedir is None\n    assert isinstance(memory, joblib.memory.Memory)\n", "def test_check_memory_deprecation_warning():\n    # Test that check_memory emits deprecation warning for 'cachedir' attribute\n    with pytest.warns(DeprecationWarning, match=\"The 'cachedir' attribute is deprecated\"):\n        memory = check_memory(\"cache_directory\")\n        assert memory.cachedir == os.path.join('cache_directory', 'joblib')\n    \n    with pytest.warns(DeprecationWarning, match=\"The 'cachedir' attribute is deprecated\"):\n        memory = check_memory(None)\n        assert memory.cachedir is None\n\n    # Test that check_memory does not emit warning for valid joblib.Memory instance\n    with pytest.warns(None) as record:\n        dummy = DummyMemory()\n        memory = check_memory(dummy)\n        assert memory is dummy\n        assert len(record) == 0\n"], "sample_861": ["def test_parameter_sampler_error():\n    # Test for invalid input types in ParameterSampler\n\n    # non-dict input\n    with pytest.raises(TypeError, match=r'Parameter distribution is not a dict or a list \\(0\\)'):\n        ParameterSampler(0, n_iter=10)\n\n    # list containing non-dict elements\n    with pytest.raises(TypeError, match=r'Parameter distribution is not a dict \\(0\\)'):\n        ParameterSampler([{'foo': [0]}, 0], n_iter=10)\n\n    # dict with non-iterable/non-distribution values\n    with pytest.raises(TypeError, match=r'Parameter value is not iterable or distribution \\(key=\\'foo\\', value=0\\)'):\n        ParameterSampler({'foo': 0}, n_iter=10)\n\n    # list of dicts with invalid values\n    with pytest.raises(TypeError, match=r'Parameter value is not iterable or distribution \\(key=\\'foo\\', value=0\\)'):\n        ParameterSampler([{'foo': [1]}, {'foo': 0}], n_iter=10)\n", "def test_parameter_sampler_invalid_distribution():\n    # Check that ParameterSampler raises error on invalid distribution\n    with pytest.raises(TypeError, match='Parameter value is not iterable or distribution'):\n        ParameterSampler({'a': [1, 2], 'b': object()}, n_iter=10)\n    with pytest.raises(TypeError, match='Parameter value is not iterable or distribution'):\n        ParameterSampler({'a': object()}, n_iter=10)\n", "def test_parameter_sampler_with_distributions():\n    # Test ParameterSampler with various distributions\n    param_distributions = {\n        'a': [1, 2, 3],\n        'b': expon(),\n        'c': uniform(0, 1),\n        'd': bernoulli(0.5)\n    }\n    sampler = ParameterSampler(param_distributions, n_iter=20, random_state=0)\n    samples = list(sampler)\n    assert len(samples) == 20\n\n    for sample in samples:\n        assert sample['a'] in [1, 2, 3]\n        assert sample['b'] >= 0\n        assert 0 <= sample['c'] <= 1\n        assert sample['d'] in [0, 1]\n\n    if sp_version >= (0, 16):\n        # Ensure reproducibility with scipy >= 0.16\n        sampler = ParameterSampler(param_distributions, n_iter=20, random_state=0)\n        samples_reproducible = list(sampler)\n        assert samples == samples_reproducible\n", "def test_parameter_grid_non_iterable_value():\n    # Test that a TypeError is raised if the parameter grid value is not iterable\n    param_grid = {\"foo\": 1}  # Non-iterable value\n    with pytest.raises(TypeError, match=\"Parameter grid value is not iterable\"):\n        ParameterGrid(param_grid)\n", "def test_parameter_sampler_with_distributions():\n    # Test ParameterSampler with scipy distributions\n    from scipy.stats import norm, randint\n\n    param_distributions = {\n        'C': norm(loc=0, scale=1),\n        'kernel': ['linear', 'rbf'],\n        'degree': randint(1, 5)\n    }\n    n_iter = 10\n    sampler = ParameterSampler(param_distributions, n_iter=n_iter, random_state=42)\n    samples = list(sampler)\n    \n    assert len(samples) == n_iter\n    for sample in samples:\n        assert -3 < sample['C'] < 3  # reasonable bounds for standard normal distribution\n        assert sample['kernel'] in ['linear', 'rbf']\n        assert 1 <= sample['degree'] < 5\n\n    # Check for reproducibility\n    sampler_2 = ParameterSampler(param_distributions, n_iter=n_iter, random_state=42)\n    samples_2 = list(sampler_2)\n    assert samples == samples_2\n"], "sample_862": ["def test_preprocess_lowercase():\n    doc = \"This is a Test Document with Mixed CASE.\"\n    processed_doc = _preprocess(doc, lower=True)\n    assert processed_doc == \"this is a test document with mixed case.\"\n\n    doc = \"This is a Test Document with Mixed CASE.\"\n    processed_doc = _preprocess(doc, lower=False)\n    assert processed_doc == doc\n", "def test_vectorizer_preprocessor_not_callable():\n    # Ensure ValueError is raised if preprocessor is not callable\n    message = \"preprocessor should be a callable\"\n    with pytest.raises(ValueError, match=message):\n        CountVectorizer(preprocessor=\"not_callable\").fit_transform([\"test\"])\n    with pytest.raises(ValueError, match=message):\n        TfidfVectorizer(preprocessor=\"not_callable\").fit_transform([\"test\"])\n", "def test_strip_tags():\n    # Test with simple HTML tags\n    text = \"<p>This is a <b>test</b>.</p>\"\n    expected = \" This is a  test  . \"\n    assert strip_tags(text) == expected\n\n    # Test with nested HTML tags\n    text = \"<div><p>This is <span>a test</span>.</p></div>\"\n    expected = \"  This is  a test  .  \"\n    assert strip_tags(text) == expected\n\n    # Test with self-closing tags\n    text = \"This is a test<br/> with self-closing tags.\"\n    expected = \"This is a test  with self-closing tags.\"\n    assert strip_tags(text) == expected\n\n    # Test with attributes in tags\n    text = '<a href=\"https://example.com\">This is a link</a>'\n    expected = \" This is a link \"\n    assert strip_tags(text) == expected\n\n    # Test with malformed tags\n    text = \"<p>This is a test with a malformed tag</span>\"\n    expected = \" This is a test with a malformed tag \"\n    assert strip_tags(text) == expected\n\n    # Test with no tags\n    text = \"This is a plain text.\"\n    expected = \"This is a plain text.\"\n    assert strip_tags(text) == expected\n\n    # Test with empty string\n    text = \"\"\n    expected = \"\"\n    assert strip_tags(text) == expected\n", "def test_hashing_vectorizer_different_ngram_ranges():\n    corpus = [\"this is a test\", \"another test document\"]\n    hv1 = HashingVectorizer(ngram_range=(1, 1), binary=True)\n    hv2 = HashingVectorizer(ngram_range=(2, 2), binary=True)\n    hv3 = HashingVectorizer(ngram_range=(1, 2), binary=True)\n\n    X1 = hv1.fit_transform(corpus)\n    X2 = hv2.fit_transform(corpus)\n    X3 = hv3.fit_transform(corpus)\n\n    assert X1.shape == (2, hv1.n_features)\n    assert X2.shape == (2, hv2.n_features)\n    assert X3.shape == (2, hv3.n_features)\n\n    # Ensure different ngram ranges produce different hash values\n    assert np.any(X1.data != X2.data)\n    assert np.any(X1.data != X3.data)\n    assert np.any(X2.data != X3.data)\n\n    # Check if binary parameter is respected\n    assert np.all(X1.data == 1)\n    assert np.all(X2.data == 1)\n    assert np.all(X3.data == 1)\n", "def test_strip_accents_with_diacritics():\n    # Test to ensure strip_accents functions correctly handle diacritics\n    a = 'n\u01d0 h\u01ceo, sh\u00ecji\u00e8!'\n    expected_unicode = 'ni hao, shijie!'\n    assert strip_accents_unicode(a) == expected_unicode\n    expected_ascii = 'ni hao, shijie!'\n    assert strip_accents_ascii(a) == expected_ascii\n\n    a = 'Dvo\u0159\u00e1k'\n    expected_unicode = 'Dvorak'\n    assert strip_accents_unicode(a) == expected_unicode\n    expected_ascii = 'Dvorak'\n    assert strip_accents_ascii(a) == expected_ascii\n\n    a = 'fa\u00e7ade'\n    expected_unicode = 'facade'\n    assert strip_accents_unicode(a) == expected_unicode\n    expected_ascii = 'facade'\n    assert strip_accents_ascii(a) == expected_ascii\n"], "sample_863": ["def test_pipeline_get_params():\n    # Test the get_params method of the pipeline.\n    clf = SVC()\n    filter1 = SelectKBest(f_classif)\n    pipe = Pipeline([('anova', filter1), ('svc', clf)])\n\n    params = pipe.get_params()\n    assert 'anova__k' in params\n    assert 'svc__C' in params\n    assert 'svc' in params\n    assert 'anova' in params\n\n    # Ensure deep parameter fetching\n    assert params['svc'] is clf\n    assert params['anova'] is filter1\n    assert 'anova__score_func' in params\n    assert params['anova__score_func'] is f_classif\n\n    # Ensure we can get parameters for single sub-object\n    assert pipe.get_params(deep=False)['svc'] is clf\n    assert 'svc__C' not in pipe.get_params(deep=False)\n", "def test_pipeline_memory_clearing():\n    # Test that the memory cache is cleared correctly when re-fitting the pipeline\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib.__version__) < LooseVersion('0.12'):\n            memory = joblib.Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = joblib.Memory(location=cachedir, verbose=10)\n\n        # Define the pipeline with a transformer and a classifier\n        transf = DummyTransf()\n        clf = SVC(probability=True, random_state=0)\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)], memory=memory)\n\n        # Fit the pipeline twice, first to cache, second to ensure cache is used\n        cached_pipe.fit(X, y)\n        ts_first_fit = cached_pipe.named_steps['transf'].timestamp_\n        cached_pipe.fit(X, y)\n        ts_second_fit = cached_pipe.named_steps['transf'].timestamp_\n\n        # Ensure that the timestamp remains the same, indicating cache usage\n        assert ts_first_fit == ts_second_fit\n\n        # Re-fit with new data to ensure the cache is cleared and not reused\n        X_new = X + 0.1\n        cached_pipe.fit(X_new, y)\n        ts_third_fit = cached_pipe.named_steps['transf'].timestamp_\n\n        # Ensure that the timestamp changes, indicating cache clearing and refitting\n        assert ts_third_fit != ts_second_fit\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_with_mixed_transformers():\n    # Test a pipeline with mixed types of transformers\n    X = iris.data\n    y = iris.target\n\n    transf1 = StandardScaler()\n    transf2 = PCA(n_components=2, svd_solver='full')\n    clf = LogisticRegression()\n\n    pipe = Pipeline([\n        ('scaler', transf1),\n        ('pca', transf2),\n        ('classifier', clf)\n    ])\n\n    # Fit and transform the data\n    pipe.fit(X, y)\n    transformed_X = pipe.transform(X)\n    assert transformed_X.shape == (X.shape[0], 2)\n\n    # Ensure that all steps are correctly applied\n    assert pipe.named_steps['scaler'].mean_ is not None\n    assert pipe.named_steps['pca'].components_ is not None\n    assert hasattr(pipe.named_steps['classifier'], 'coef_')\n\n    # Test predictions\n    predictions = pipe.predict(X)\n    assert predictions.shape == (X.shape[0],)\n\n    # Test score\n    score = pipe.score(X, y)\n    assert isinstance(score, float)\n", "def test_pipeline_named_steps_access():\n    # Test accessing steps via named_steps attribute\n    transf = Transf()\n    clf = LogisticRegression()\n    pipeline = Pipeline([('transf', transf), ('clf', clf)])\n    \n    # Accessing steps\n    assert pipeline.named_steps['transf'] is transf\n    assert pipeline.named_steps['clf'] is clf\n    \n    # Modifying steps\n    new_transf = Transf()\n    pipeline.named_steps['transf'] = new_transf\n    assert pipeline.named_steps['transf'] is new_transf\n    \n    # Testing that named_steps can be used to access parameters\n    assert pipeline.named_steps['clf'].get_params()['penalty'] == 'l2'\n    pipeline.named_steps['clf'].set_params(C=0.5)\n    assert pipeline.named_steps['clf'].get_params()['C'] == 0.5\n", "def test_pipeline_transformer_only():\n    # Test pipeline with only transformers and no final estimator\n    X = iris.data\n    y = iris.target\n    pca = PCA(n_components=2, svd_solver='full')\n    scaler = StandardScaler()\n    pipeline = Pipeline([('scaler', scaler), ('pca', pca)])\n\n    # Test fit, transform, and fit_transform\n    pipeline.fit(X)\n    X_trans = pipeline.transform(X)\n    X_fit_trans = pipeline.fit_transform(X)\n    assert_array_almost_equal(X_trans, X_fit_trans)\n    assert X_trans.shape == (X.shape[0], 2)\n\n    # Test inverse_transform\n    X_inv_trans = pipeline.inverse_transform(X_trans)\n    assert X_inv_trans.shape == X.shape\n    assert_array_almost_equal(X, scaler.inverse_transform(pca.inverse_transform(X_trans)))\n\n    # Test get_params and set_params\n    params = pipeline.get_params()\n    assert 'scaler__with_mean' in params\n    pipeline.set_params(scaler__with_mean=False)\n    assert not pipeline.named_steps['scaler'].with_mean\n\n    # Test slicing pipeline\n    sub_pipeline = pipeline[:1]\n    assert isinstance(sub_pipeline, Pipeline)\n    assert len(sub_pipeline.steps) == 1\n    assert sub_pipeline.steps[0][0] == 'scaler'\n    X_sub_trans = sub_pipeline.transform(X)\n    assert_array_almost_equal(X_sub_trans, scaler.fit_transform(X))\n\n    # Test indexing pipeline\n    assert pipeline[0] == scaler\n    assert pipeline['scaler'] == scaler\n    assert pipeline[1] == pca\n    assert pipeline['pca'] == pca\n    assert_raises(IndexError, lambda: pipeline[2])\n    assert_raises(KeyError, lambda: pipeline['nonexistent'])\n"], "sample_864": ["def test_meanshift_no_points_within_bandwidth():\n    # Test case where no points are within the bandwidth of the initial seed\n    X = np.array([[1, 1], [2, 2], [3, 3]])\n    ms = MeanShift(bandwidth=0.1, seeds=[[0, 0]])\n    with pytest.raises(ValueError, match=\"No point was within bandwidth=0.1\"):\n        ms.fit(X)\n", "def test_meanshift_with_anisotropic_data():\n    # Test MeanShift on anisotropic data\n    rng = np.random.RandomState(42)\n    X = np.dot(rng.randn(300, 2), rng.randn(2, 2))\n\n    bandwidth = estimate_bandwidth(X, quantile=0.2)\n    ms = MeanShift(bandwidth=bandwidth)\n    ms.fit(X)\n    labels = ms.labels_\n\n    assert len(np.unique(labels)) > 1\n    assert ms.cluster_centers_.shape[1] == 2\n", "def test_mean_shift_with_explicit_seeds():\n    # Test MeanShift algorithm with explicit seeds\n    seeds = np.array([[10, 10], [9, 9], [11, 11]])\n    ms = MeanShift(bandwidth=1.5, seeds=seeds)\n    labels = ms.fit(X).labels_\n    labels_unique = np.unique(labels)\n    n_clusters_ = len(labels_unique)\n    assert n_clusters_ == 3\n    assert_array_equal(labels_unique, [0, 1, 2])\n\n    cluster_centers, labels_mean_shift = mean_shift(X, bandwidth=1.5, seeds=seeds)\n    labels_mean_shift_unique = np.unique(labels_mean_shift)\n    n_clusters_mean_shift = len(labels_mean_shift_unique)\n    assert n_clusters_mean_shift == 3\n    assert_array_equal(labels_mean_shift_unique, [0, 1, 2])\n", "def test_zero_bandwidth():\n    # Test MeanShift with bandwidth set to zero\n    ms = MeanShift(bandwidth=0)\n    msg = (r\"bandwidth needs to be greater than zero or None,\"\n           r\" got 0\\.000000\")\n    with pytest.raises(ValueError, match=msg):\n        ms.fit(X)\n", "def test_mean_shift_with_seeds():\n    # Test MeanShift with explicit seeds\n    seeds = np.array([[10, 10], [9, 9], [11, 11]])\n    ms = MeanShift(bandwidth=1.2, seeds=seeds)\n    labels = ms.fit(X).labels_\n    labels_unique = np.unique(labels)\n    n_clusters_ = len(labels_unique)\n    assert n_clusters_ == 3  # We expect 3 clusters since we provided 3 seeds\n\n    cluster_centers, labels_mean_shift = mean_shift(X, seeds=seeds)\n    labels_mean_shift_unique = np.unique(labels_mean_shift)\n    n_clusters_mean_shift = len(labels_mean_shift_unique)\n    assert n_clusters_mean_shift == 3\n"], "sample_865": ["def test_prune_tree_regression_specific_case():\n    # Test pruning on a specific regression case to ensure proper pruning logic\n    X = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]\n    y = [1, 1, 2, 2, 3, 3, 4, 4, 5, 5]\n    estimator = DecisionTreeRegressor(random_state=0, ccp_alpha=0.1)\n    estimator.fit(X, y)\n    \n    # Ensure pruning has occurred\n    assert estimator.tree_.node_count < len(X), \"Tree was not pruned as expected\"\n    \n    # Test predictions before and after pruning for a known value\n    y_pred_before_pruning = estimator.predict([[6.5]])\n    estimator.set_params(ccp_alpha=0.5).fit(X, y)\n    y_pred_after_pruning = estimator.predict([[6.5]])\n    \n    assert y_pred_before_pruning != y_pred_after_pruning, \"Pruning did not change predictions as expected\"\n", "def test_predict_log_proba():\n    # Check predict_log_proba on a small toy dataset.\n    X, y = datasets.make_classification(n_samples=100, n_features=4,\n                                        random_state=0)\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n    \n    log_proba = clf.predict_log_proba(X)\n    proba = clf.predict_proba(X)\n    \n    assert_array_almost_equal(np.log(proba), log_proba, err_msg=\"Log probabilities do not match with log of probabilities\")\n    assert log_proba.shape == (X.shape[0], len(clf.classes_)), \"Shape mismatch in log probabilities\"\n", "def test_prune_tree_zero_impurity():\n    # Test pruning on a tree with zero impurity nodes.\n    X_zero_impurity = np.array([[0], [0], [0], [1], [1], [1]])\n    y_zero_impurity = np.array([0, 0, 0, 1, 1, 1])\n\n    for name, TreeClassifier in CLF_TREES.items():\n        clf = TreeClassifier(random_state=0, ccp_alpha=0.1)\n        clf.fit(X_zero_impurity, y_zero_impurity)\n\n        # Check if the tree is pruned correctly\n        assert clf.tree_.node_count == 1, (\n            f\"Failed to prune tree to a single node with {name}. \"\n            f\"Node count: {clf.tree_.node_count}\"\n        )\n\n        # Check if the pruned tree predicts correctly\n        y_pred = clf.predict(X_zero_impurity)\n        assert_array_equal(y_pred, y_zero_impurity, \n                           f\"Failed to predict correctly with {name} after pruning.\")\n", "def test_min_samples_split_greater_than_sample_size():\n    # Test case where min_samples_split is greater than the number of samples\n    X = [[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]]\n    y = [0, 1, 0, 1, 0]\n\n    for name, TreeEstimator in ALL_TREES.items():\n        est = TreeEstimator(min_samples_split=10, random_state=0)\n        est.fit(X, y)\n        assert est.tree_.max_depth == 0, (\n            f\"Failed with {name}, tree depth should be 0 when \"\n            f\"min_samples_split > n_samples\")\n", "def test_max_features_invalid_string():\n    # Test invalid string for max_features parameter raises ValueError\n    for name, TreeEstimator in ALL_TREES.items():\n        with pytest.raises(ValueError, match='Invalid value for max_features'):\n            TreeEstimator(max_features=\"invalid\").fit(X, y)\n"], "sample_866": ["def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input data\n    X_sparse = csr_matrix(X)\n    \n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_dense = af.fit_predict(X)\n    labels_sparse = af.fit_predict(X_sparse)\n    \n    assert_array_equal(labels_dense, labels_sparse)\n", "def test_affinity_propagation_damping_value_error():\n    # Test ValueError when damping is outside the valid range\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S)\n\n    with pytest.raises(ValueError, match=r'damping must be >= 0.5 and < 1'):\n        affinity_propagation(S, preference=preference, damping=0.4)\n\n    with pytest.raises(ValueError, match=r'damping must be >= 0.5 and < 1'):\n        affinity_propagation(S, preference=preference, damping=1)\n", "def test_affinity_propagation_damping():\n    # Test AffinityPropagation with different damping values\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n\n    # Test with damping close to lower bound\n    af = AffinityPropagation(preference=preference, damping=0.5)\n    labels_low_damping = af.fit(X).labels_\n    assert np.unique(labels_low_damping).size == n_clusters\n\n    # Test with damping close to upper bound\n    af = AffinityPropagation(preference=preference, damping=0.99)\n    labels_high_damping = af.fit(X).labels_\n    assert np.unique(labels_high_damping).size == n_clusters\n\n    # Test invalid damping value\n    with pytest.raises(ValueError):\n        AffinityPropagation(preference=preference, damping=0.4).fit(X)\n    with pytest.raises(ValueError):\n        AffinityPropagation(preference=preference, damping=1.0).fit(X)\n", "def test_affinity_propagation_damping_values():\n    # Test different damping values within the valid range\n    damping_values = [0.5, 0.7, 0.9]\n    preference = np.median(-euclidean_distances(X, squared=True)) * 10\n\n    for damping in damping_values:\n        af = AffinityPropagation(preference=preference, damping=damping, affinity=\"euclidean\")\n        labels = af.fit_predict(X)\n        unique_labels = np.unique(labels)\n        assert len(unique_labels) == n_clusters\n\n    # Test invalid damping values\n    with pytest.raises(ValueError):\n        AffinityPropagation(damping=0.4).fit(X)\n    with pytest.raises(ValueError):\n        AffinityPropagation(damping=1.0).fit(X)\n", "def test_affinity_propagation_invalid_affinity():\n    # Test for invalid affinity input\n    with pytest.raises(ValueError, match=\"Affinity must be 'precomputed' or 'euclidean'. Got 'invalid' instead\"):\n        af = AffinityPropagation(affinity=\"invalid\")\n        af.fit(X)\n"], "sample_867": ["def test_random_search_with_distributions():\n    # Test that RandomizedSearchCV correctly samples from distributions\n    param_distributions = {\n        'a': randint(0, 4),\n        'b': uniform(0, 1)\n    }\n    search = RandomizedSearchCV(MockClassifier(), param_distributions, n_iter=10, random_state=42)\n    search.fit(X, y)\n    \n    # Check that the parameter space was sampled correctly\n    assert len(search.cv_results_['params']) == 10\n    for params in search.cv_results_['params']:\n        assert 'a' in params and 0 <= params['a'] < 4\n        assert 'b' in params and 0 <= params['b'] <= 1\n\n    # Check that the best estimator is correctly set\n    assert hasattr(search, 'best_estimator_')\n", "def test_parameter_sampler_with_distributions():\n    # Test ParameterSampler with continuous and discrete distributions\n    param_distributions = {\n        'C': uniform(0, 1),  # Continuous distribution\n        'kernel': ['linear', 'rbf', 'poly'],  # Discrete distribution\n        'degree': randint(1, 5)  # Discrete integer distribution\n    }\n\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=10, random_state=42)\n    samples = list(sampler)\n\n    # Check if the right number of samples are generated\n    assert len(samples) == 10\n\n    # Check if sampled parameters are in the expected range\n    for sample in samples:\n        assert 0 <= sample['C'] <= 1\n        assert sample['kernel'] in ['linear', 'rbf', 'poly']\n        assert 1 <= sample['degree'] < 5\n\n    # Test that repeated calls with the same random_state yield identical results\n    sampler1 = ParameterSampler(param_distributions=param_distributions,\n                                n_iter=10, random_state=42)\n    sampler2 = ParameterSampler(param_distributions=param_distributions,\n                                n_iter=10, random_state=42)\n    assert [x for x in sampler1] == [x for x in sampler2]\n", "def test_parameter_grid_getitem():\n    # Test __getitem__ method for ParameterGrid\n    param_grid = {'a': [1, 2], 'b': [True, False]}\n    grid = ParameterGrid(param_grid)\n\n    expected_grid = [\n        {'a': 1, 'b': True},\n        {'a': 1, 'b': False},\n        {'a': 2, 'b': True},\n        {'a': 2, 'b': False}\n    ]\n    \n    for i in range(len(grid)):\n        assert grid[i] == expected_grid[i]\n\n    # Test out of range index\n    with pytest.raises(IndexError, match='ParameterGrid index out of range'):\n        grid[len(grid)]\n", "def test_parameter_sampler_validation():\n    # Test ParameterSampler for validation of param_distributions input\n    with pytest.raises(TypeError, match=r'Parameter distribution is not a dict or a list'):\n        ParameterSampler(0, n_iter=10)\n    \n    with pytest.raises(TypeError, match=r'Parameter distribution is not a dict'):\n        ParameterSampler([{'a': [1, 2]}, 0], n_iter=10)\n\n    with pytest.raises(TypeError, match=r'Parameter value is not iterable or distribution'):\n        ParameterSampler({'a': 0}, n_iter=10)\n\n    with pytest.raises(TypeError, match=r'Parameter value is not iterable or distribution'):\n        ParameterSampler({'a': None}, n_iter=10)\n", "def test_parameters_with_non_iterable_values():\n    # Test that an error is raised when a parameter value is not iterable\n    clf = MockClassifier()\n    \n    param_grid = {'foo_param': [1, 2, 3], 'bar_param': 'not_iterable'}\n    param_sampler = {'foo_param': [1, 2, 3], 'bar_param': 42}\n\n    grid_search = GridSearchCV(clf, param_grid, cv=3)\n    assert_raises(TypeError, grid_search.fit, X, y)\n    \n    random_search = RandomizedSearchCV(clf, param_sampler, n_iter=10, cv=3)\n    assert_raises(TypeError, random_search.fit, X, y)\n"], "sample_868": ["def test_empty_labels(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == pytest.approx(1.0)\n", "def test_empty_labels(metric_name):\n    # Test to ensure that metrics handle empty labels correctly\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == pytest.approx(1.0)\n\n    # Test for partial emptiness\n    assert metric([0, 0, 1, 1], []) == pytest.approx(0.0)\n    assert metric([], [0, 0, 1, 1]) == pytest.approx(0.0)\n", "def test_empty_input(metric):\n    # testing metric for empty input\n    assert metric([], []) == 1.0\n", "def test_no_cluster_single_class(metric):\n    # case where all points belong to a single class (no clustering)\n    labels_true = [0, 0, 0, 0, 0]\n    labels_pred = [0, 0, 0, 0, 0]\n    assert metric(labels_true, labels_pred) == pytest.approx(1.0)\n\n    # case where all points belong to different classes (trivial clustering)\n    labels_true = [0, 1, 2, 3, 4]\n    labels_pred = [0, 1, 2, 3, 4]\n    assert metric(labels_true, labels_pred) == pytest.approx(1.0)\n\n    # case where true labels are single class but predictions are different\n    labels_true = [0, 0, 0, 0, 0]\n    labels_pred = [0, 1, 2, 3, 4]\n    if metric.__name__ in NORMALIZED_METRICS:\n        assert metric(labels_true, labels_pred) == pytest.approx(0.0)\n    else:\n        assert metric(labels_true, labels_pred) >= 0.0\n", "def test_zero_length_labels(metric):\n    # Test that all supervised metrics handle zero-length input properly\n    y_true = []\n    y_pred = []\n    assert metric(y_true, y_pred) == pytest.approx(1.0)\n"], "sample_869": ["def test_multiclass_multilabel_confusion_matrix():\n    # Test confusion matrix - multi-class multi-label case\n    y_true = [[0, 1, 2], [1, 2, 3], [0, 1, 3]]\n    y_pred = [[0, 2, 3], [1, 2, 3], [0, 1, 2]]\n\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, [[[1, 1], [0, 1]],\n                            [[1, 0], [1, 1]],\n                            [[1, 0], [0, 2]],\n                            [[2, 0], [0, 1]]])\n\n    # test with explicit labels\n    cm = multilabel_confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n    assert_array_equal(cm, [[[1, 1], [0, 1]],\n                            [[1, 0], [1, 1]],\n                            [[1, 0], [0, 2]],\n                            [[2, 0], [0, 1]]])\n\n    # test with a super set of present labels\n    cm = multilabel_confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3, 4])\n    assert_array_equal(cm, [[[1, 1], [0, 1]],\n                            [[1, 0], [1, 1]],\n                            [[1, 0], [0, 2]],\n                            [[2, 0], [0, 1]],\n                            [[3, 0], [0, 0]]])\n\n    # test error when none of the specified labels are in y_true\n    with pytest.raises(ValueError):\n        multilabel_confusion_matrix(y_true, y_pred, labels=[4, 5])\n", "def test_fbeta_score_multiclass():\n    # Test F-beta score for multiclass classification task\n    y_true, y_pred, _ = make_prediction(binary=False)\n\n    # compute scores with default labels introspection\n    beta = 0.5\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average=None)\n    assert_array_almost_equal(p, [0.83, 0.33, 0.42], 2)\n    assert_array_almost_equal(r, [0.79, 0.09, 0.90], 2)\n    assert_array_almost_equal(f, [0.81, 0.15, 0.57], 2)\n    assert_array_equal(s, [24, 31, 20])\n\n    # averaging tests\n    ps = precision_score(y_true, y_pred, pos_label=1, average='micro')\n    assert_array_almost_equal(ps, 0.53, 2)\n\n    rs = recall_score(y_true, y_pred, average='micro')\n    assert_array_almost_equal(rs, 0.53, 2)\n\n    fs = f1_score(y_true, y_pred, average='micro')\n    assert_array_almost_equal(fs, 0.53, 2)\n\n    ps = precision_score(y_true, y_pred, average='macro')\n    assert_array_almost_equal(ps, 0.53, 2)\n\n    rs = recall_score(y_true, y_pred, average='macro')\n    assert_array_almost_equal(rs, 0.60, 2)\n\n    fs = f1_score(y_true, y_pred, average='macro')\n    assert_array_almost_equal(fs, 0.51, 2)\n\n    ps = precision_score(y_true, y_pred, average='weighted')\n    assert_array_almost_equal(ps, 0.51, 2)\n\n    rs = recall_score(y_true, y_pred, average='weighted')\n    assert_array_almost_equal(rs, 0.53, 2)\n\n    fs = f1_score(y_true, y_pred, average='weighted')\n    assert_array_almost_equal(fs, 0.47, 2)\n\n    fbs = fbeta_score(y_true, y_pred, beta=beta, average='micro')\n    assert_array_almost_equal(fbs, (1 + beta**2) * ps * rs / (beta**2 *", "def test_multiclass_classification_report_dict_output():\n    # Test performance report with dictionary output for multiclass classification\n    iris = datasets.load_iris()\n    y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)\n\n    # Expected report dictionary\n    expected_report = {\n        'setosa': {'precision': 0.8260869565217391, 'recall': 0.7916666666666666, 'f1-score': 0.8085106382978724, 'support': 24},\n        'versicolor': {'precision': 0.3333333333333333, 'recall': 0.0967741935483871, 'f1-score': 0.15, 'support': 31},\n        'virginica': {'precision': 0.4186046511627907, 'recall': 0.9, 'f1-score': 0.5714285714285714, 'support': 20},\n        'accuracy': 0.5333333333333333,\n        'macro avg': {'precision': 0.526008313672621, 'recall': 0.596146953405018, 'f1-score': 0.5099797365754813, 'support': 75},\n        'weighted avg': {'precision': 0.5137535108414785, 'recall': 0.5333333333333333, 'f1-score': 0.47310435663627154, 'support': 75}\n    }\n\n    report = classification_report(y_true, y_pred, labels=np.arange(len(iris.target_names)), target_names=iris.target_names, output_dict=True)\n\n    # assert the 2 dicts are equal\n    assert report.keys() == expected_report.keys()\n    for key in expected_report:\n        if key == 'accuracy':\n            assert isinstance(report[key], float)\n            assert report[key] == expected_report[key]\n        else:\n            assert report[key].keys() == expected_report[key].keys()\n            for metric in expected_report[key]:\n                assert_almost_equal(expected_report[key][metric], report[key][metric])\n\n    assert type(expected_report['setosa']['precision']) == float\n    assert type(expected_report['macro avg']['precision']) == float\n    assert type(expected_report['setosa']['support']) == int\n    assert type(expected_report['macro avg']['support'])", "def test_weighted_sum():\n    sample_score = np.array([1, 2, 3, 4])\n    sample_weight = np.array([0.1, 0.2, 0.3, 0.4])\n\n    # Test without normalization\n    result = _weighted_sum(sample_score, sample_weight, normalize=False)\n    assert_almost_equal(result, 1*0.1 + 2*0.2 + 3*0.3 + 4*0.4)\n\n    # Test with normalization\n    result = _weighted_sum(sample_score, sample_weight, normalize=True)\n    assert_almost_equal(result, (1*0.1 + 2*0.2 + 3*0.3 + 4*0.4) / np.sum(sample_weight))\n\n    # Test without sample_weight\n    result = _weighted_sum(sample_score, None, normalize=False)\n    assert_almost_equal(result, np.sum(sample_score))\n\n    result = _weighted_sum(sample_score, None, normalize=True)\n    assert_almost_equal(result, np.mean(sample_score))\n", "def test_confusion_matrix_no_labels_provided():\n    # Test confusion matrix when no labels are provided\n    y_true = [1, 2, 3, 3]\n    y_pred = [1, 3, 3, 1]\n    cm = confusion_matrix(y_true, y_pred)\n    # Confusion matrix should include all unique labels from y_true and y_pred\n    expected_cm = np.array([[1, 0, 0], [0, 0, 1], [1, 1, 1]])\n    assert_array_equal(cm, expected_cm)\n\n    y_true = [0, 1, 1, 2, 2, 2]\n    y_pred = [0, 0, 1, 1, 2, 2]\n    cm = confusion_matrix(y_true, y_pred)\n    expected_cm = np.array([[1, 1, 0],\n                            [0, 1, 0],\n                            [0, 1, 2]])\n    assert_array_equal(cm, expected_cm)\n\n    # Test confusion matrix with a single class and no predictions for another class\n    y_true = [0, 0, 0, 0, 0, 0]\n    y_pred = [0, 0, 0, 0, 0, 0]\n    cm = confusion_matrix(y_true, y_pred)\n    expected_cm = np.array([[6]])\n    assert_array_equal(cm, expected_cm)\n\n    # Test confusion matrix with all predictions being the same but true labels being different\n    y_true = [0, 1, 2, 3]\n    y_pred = [0, 0, 0, 0]\n    cm = confusion_matrix(y_true, y_pred)\n    expected_cm = np.array([[1, 0, 0, 0],\n                            [1, 0, 0, 0],\n                            [1, 0, 0, 0],\n                            [1, 0, 0, 0]])\n    assert_array_equal(cm, expected_cm)\n"], "sample_870": ["def test_predict_unfitted_model():\n    \"\"\"Test that prediction with an unfitted model does not break and returns expected shapes.\"\"\"\n    kernel = RBF(length_scale=1.0)\n    gpr = GaussianProcessRegressor(kernel=kernel)\n    \n    X_new = np.atleast_2d([1.0, 2.0, 3.0]).T\n    y_mean, y_std = gpr.predict(X_new, return_std=True)\n    _, y_cov = gpr.predict(X_new, return_cov=True)\n\n    assert y_mean.shape == (3,)\n    assert y_std.shape == (3,)\n    assert y_cov.shape == (3, 3)\n    assert np.allclose(y_mean, 0)  # Mean should be zero for unfitted model\n    assert np.allclose(np.diag(y_cov), y_std**2)  # Std and Cov should be consistent\n", "def test_custom_kernel_diag(kernel):\n    \"\"\"Test that the diag method of a custom kernel doesn't mutate the input matrix X.\"\"\"\n    class CustomKernel(C):\n            return X[:, 0]\n\n    X_test = np.array([[1, 2], [3, 4], [5, 6]])\n    X_test_copy = X_test.copy()\n    custom_kernel = CustomKernel()\n    \n    gpr = GaussianProcessRegressor(kernel=custom_kernel).fit(X, y)\n    gpr.predict(X_test, return_std=True)\n    \n    assert np.array_equal(X_test, X_test_copy), \"The input matrix X was mutated by the diag method of the custom kernel.\"\n", "def test_kernel_clone():\n    \"\"\"Check that the kernel is properly cloned during the fitting process and \n    that the cloned kernel parameters do not interfere with the original kernel.\n    \"\"\"\n    kernel = RBF(length_scale=1.0)\n    gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=2)\n    \n    # fit the model with the original kernel\n    gpr.fit(X, y)\n    cloned_kernel = gpr.kernel_\n\n    # ensure that the original kernel parameters are not modified\n    assert kernel.theta != cloned_kernel.theta\n\n    # ensure that the cloned kernel parameters are within bounds\n    bounds = gpr.kernel_.bounds\n    max_ = np.finfo(cloned_kernel.theta.dtype).max\n    tiny = 1e-10\n    bounds[~np.isfinite(bounds[:, 1]), 1] = max_\n\n    assert_array_less(bounds[:, 0], cloned_kernel.theta + tiny)\n    assert_array_less(cloned_kernel.theta, bounds[:, 1] + tiny)\n", "def test_invalid_kernel_type():\n    \"\"\"Check that an error is raised when an invalid kernel type is provided.\"\"\"\n    class InvalidKernel:\n        pass\n\n    invalid_kernel = InvalidKernel()\n    with pytest.raises(TypeError, match=\"kernel must be an instance of Kernel or None\"):\n        GaussianProcessRegressor(kernel=invalid_kernel).fit(X, y)\n", "def test_kernel_none_default():\n    # Test that GPR uses the default kernel when no kernel is provided.\n    gpr = GaussianProcessRegressor()\n    gpr.fit(X, y)\n    \n    # Default kernel is ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n    default_kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n    \n    assert gpr.kernel_.theta == default_kernel.theta\n    assert gpr.kernel_.bounds == default_kernel.bounds\n"], "sample_871": ["def test_silhouette_samples_custom_metric():\n    # Test silhouette_samples with a custom metric\n    X = np.array([[0.0], [1.0], [2.0], [3.0], [4.0], [5.0]])\n    labels = np.array([0, 0, 1, 1, 2, 2])\n\n        return np.abs(x - y)\n\n    expected_silhouettes = np.array([-1.0, -1.0, 1.0, 1.0, 1.0, 1.0])\n    \n    sil_samples = silhouette_samples(X, labels, metric=custom_metric)\n    assert_array_equal(sil_samples, expected_silhouettes)\n", "def test_calinski_harabasz_non_numpy_input():\n    # Test calinski_harabasz_score with non-numpy inputs\n    X = [[0, 0], [1, 1], [0, 0], [1, 1], [0, 0], [1, 1]]\n    labels = [0, 0, 1, 1, 2, 2]\n    score = calinski_harabasz_score(X, labels)\n    assert score > 0\n\n    # Test with a list of lists input\n    X_list = [\n        [0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5],\n        [6, 6], [7, 7], [8, 8], [9, 9]\n    ]\n    labels_list = [0, 0, 0, 1, 1, 1, 2, 2, 2, 2]\n    score = calinski_harabasz_score(X_list, labels_list)\n    assert score > 0\n", "def test_calinski_harabasz_score_single_cluster():\n    # Test when all points belong to a single cluster\n    X = np.array([[0, 0], [0, 0], [0, 0], [0, 0]])\n    labels = [0, 0, 0, 0]\n    with pytest.raises(ValueError, match=\"Number of labels is 1\"):\n        calinski_harabasz_score(X, labels)\n", "def test_silhouette_score_with_callable_metric():\n    # Test silhouette_score with a custom metric\n        return np.sqrt(np.sum((x - y) ** 2))\n\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = dataset.target\n\n    score_callable = silhouette_score(X, y, metric=custom_metric)\n    assert score_callable > 0\n\n    # Test with sampling\n    score_callable_sample = silhouette_score(\n        X, y, metric=custom_metric, sample_size=int(X.shape[0] / 2), random_state=0\n    )\n    assert score_callable_sample > 0\n    pytest.approx(score_callable, score_callable_sample)\n", "def test_silhouette_samples_different_metrics():\n    # Test the silhouette_samples function with different distance metrics\n    X, labels = datasets.make_blobs(n_samples=30, centers=3, cluster_std=1.0, random_state=0)\n\n    metrics = [\"cityblock\", \"cosine\", \"euclidean\", \"l1\", \"l2\", \"manhattan\"]\n\n    for metric in metrics:\n        result = silhouette_samples(X, labels, metric=metric)\n        assert result.shape == (30,)\n        assert np.all(result >= -1) and np.all(result <= 1)\n"], "sample_872": ["def test_auc_memmap():\n    # Test AUC with numpy memmap to ensure compatibility\n    x = np.array([0, 0.5, 1])\n    y = np.array([0, 0.5, 1])\n    \n    with tempfile.NamedTemporaryFile() as tmp:\n        memmap_x = np.memmap(tmp.name, dtype='float64', mode='w+', shape=x.shape)\n        memmap_y = np.memmap(tmp.name, dtype='float64', mode='w+', shape=y.shape)\n        \n        memmap_x[:] = x[:]\n        memmap_y[:] = y[:]\n        \n        assert_array_almost_equal(auc(memmap_x, memmap_y), 0.5)\n", "def test_auc_single_point_raises_error():\n    # Test that auc raises an error when x has only one point\n    x = [0]\n    y = [0]\n    with pytest.raises(ValueError, match=\"At least 2 points are needed to compute area under curve\"):\n        auc(x, y)\n", "def test_auc_memmap():\n    # Test AUC computation with numpy memmap\n    x = np.array([0, 0.5, 1])\n    y = np.array([0, 0.5, 1])\n    \n    # Create a temporary file to store the memmap\n    filename = 'temp_memmap.dat'\n    memmap_y = np.memmap(filename, dtype='float32', mode='w+', shape=y.shape)\n    memmap_y[:] = y[:]\n    \n    try:\n        assert_array_almost_equal(auc(x, memmap_y), 0.5)\n    finally:\n        # Clean up the temporary file\n        import os\n        os.remove(filename)\n", "def test_roc_auc_score_multilabel():\n    # Test ROC AUC score for multilabel classification\n\n    # Perfect prediction\n    y_true = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n    y_score = np.array([[0.9, 0.1], [0.2, 0.8], [0.7, 0.9], [0.1, 0.2]])\n    roc_auc = roc_auc_score(y_true, y_score, average='macro')\n    assert roc_auc == 1.0\n\n    # Worst prediction\n    y_score = np.array([[0.1, 0.9], [0.8, 0.2], [0.3, 0.1], [0.6, 0.5]])\n    roc_auc = roc_auc_score(y_true, y_score, average='macro')\n    assert roc_auc == 0.0\n\n    # Random prediction\n    y_score = np.random.rand(4, 2)\n    roc_auc_macro = roc_auc_score(y_true, y_score, average='macro')\n    roc_auc_micro = roc_auc_score(y_true, y_score, average='micro')\n    assert 0.0 <= roc_auc_macro <= 1.0\n    assert 0.0 <= roc_auc_micro <= 1.0\n\n    # Test with sample weights\n    sample_weight = np.array([0.2, 0.5, 0.3, 0.1])\n    roc_auc = roc_auc_score(y_true, y_score, average='macro', sample_weight=sample_weight)\n    assert 0.0 <= roc_auc <= 1.0\n\n    # Test with missing labels\n    y_true = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n    y_score = np.array([[0.9, 0.1], [0.2, 0.8], [0.7, 0.9], [0.1, 0.2]])\n    labels = [0, 1, 2]\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_score, average='macro', labels=labels)\n\n    # Test with different averaging methods\n    roc_auc_samples = roc_auc_score(y_true, y_score, average='", "def test_average_precision_zero_division():\n    # Test average_precision_score does not result in ZeroDivisionError\n    y_true = [0, 1, 1, 1]\n    y_score = [0.6, 0.5, 0.4, 0.7]\n    \n    # Ensure no zero division occurs\n    score = average_precision_score(y_true, y_score)\n    assert score != float('inf')\n    assert score != float('-inf')\n    assert not np.isnan(score)\n    assert score > 0\n\n    # Another edge case with a single positive and multiple negatives\n    y_true = [0, 0, 1, 0]\n    y_score = [0.5, 0.5, 0.5, 0.5]\n    \n    score = average_precision_score(y_true, y_score)\n    assert score != float('inf')\n    assert score != float('-inf')\n    assert not np.isnan(score)\n    assert score > 0\n"], "sample_873": ["def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(feature_names))\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(None))\n\n    # Check with input_features that does not match feature_names_in_\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out([\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\"])\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    \n    # Test with default input_features (None)\n    feature_names_out = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out)\n    \n    # Test with provided input_features\n    feature_names_out_with_input = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_with_input)\n    \n    # Test with mismatched input_features length\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=feature_names[:-1])\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    # Check default feature names if input_features is None\n    sel.fit(X, y)\n    sel.feature_names_in_ = None  # Unset feature_names_in_\n    default_feature_names = [f\"x{i}\" for i in range(X.shape[1])]\n    feature_names_out = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    # Check feature names with input_features argument\n    custom_feature_names = [\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\", \"f8\", \"f9\", \"f10\"]\n    feature_names_out = sel.get_feature_names_out(custom_feature_names)\n    assert_array_equal(custom_feature_names[::2], feature_names_out)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    # Check with generated feature names\n    sel = StepSelector()\n    sel.fit(np.empty((2, 10)))\n    feature_names_out = sel.get_feature_names_out()\n    assert_array_equal([f\"x{i}\" for i in range(0, 10, 2)], feature_names_out)\n\n    # Check with input features not matching feature_names_in_\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out([\"x0\", \"x1\", \"x2\", \"x3\", \"x4\"])\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    # Check default names when input_features is None and feature_names_in_ is not defined\n    sel2 = StepSelector()\n    sel2.fit(X, y)\n    default_feature_names_out = sel2.get_feature_names_out()\n    expected_default_feature_names = np.array([\"x0\", \"x2\", \"x4\", \"x6\", \"x8\"])\n    assert_array_equal(expected_default_feature_names, default_feature_names_out)\n\n    # Check error when input_features does not match feature_names_in_\n    sel3 = StepSelector()\n    sel3.fit(X, y)\n    with pytest.raises(ValueError):\n        sel3.get_feature_names_out([\"x0\", \"x1\"])\n"], "sample_874": ["def test_set_params():\n    sel = StepSelector()\n    sel.set_params(step=3)\n    assert sel.step == 3\n    sel.set_params(step=1)\n    assert sel.step == 1\n    with pytest.raises(ValueError):\n        sel.set_params(nonexistent_param=42)\n", "def test_clone_step_selector():\n    sel = StepSelector(step=3)\n    cloned_sel = clone(sel)\n    assert sel.get_params() == cloned_sel.get_params()\n    assert type(cloned_sel) is StepSelector\n    assert cloned_sel.step == 3\n\n    # Check if cloned selector works as expected\n    Xt_actual = cloned_sel.fit(X, y).transform(X)\n    expected_support = [True, False, False] * 3 + [True]\n    assert_array_equal(Xt[:, :4], Xt_actual[:, :4])\n    assert_array_equal(expected_support, cloned_sel.get_support())\n", "def test_clone_step_selector():\n    sel = StepSelector(step=3)\n    sel_cloned = clone(sel)\n    assert sel.step == sel_cloned.step\n\n    Xt_actual = sel.fit(X, y).transform(X)\n    Xt_cloned_actual = sel_cloned.fit(X, y).transform(X)\n    assert_array_equal(Xt_actual, Xt_cloned_actual)\n", "def test_clone():\n    sel = StepSelector()\n    sel_cloned = clone(sel)\n    assert sel.get_params() == sel_cloned.get_params()\n    assert type(sel) == type(sel_cloned)\n    assert sel is not sel_cloned\n\n    # Ensure cloned estimator has not been fitted\n    with pytest.raises(AttributeError):\n        sel_cloned.transform(X)\n", "def test_set_params():\n    sel = StepSelector()\n    sel.set_params(step=3)\n    assert sel.step == 3\n\n    sel = StepSelector(step=4)\n    sel.set_params(step=5)\n    assert sel.step == 5\n\n    with pytest.raises(ValueError):\n        sel.set_params(non_existent_param=1)\n\n    with pytest.raises(ValueError):\n        sel.set_params(step=\"invalid_value\")\n"], "sample_875": ["def test_jaccard_score_multilabel_unbalanced():\n    # Test Jaccard score for a multilabel case with unbalanced labels\n    y_true = np.array([[1, 1, 0, 0], [0, 1, 1, 0], [1, 0, 1, 1]])\n    y_pred = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [1, 0, 0, 1]])\n\n    # Verify Jaccard score with different averaging methods\n    assert_almost_equal(jaccard_score(y_true, y_pred, average=\"samples\"), 0.5)\n    assert_almost_equal(jaccard_score(y_true, y_pred, average=\"micro\"), 0.5)\n    assert_almost_equal(jaccard_score(y_true, y_pred, average=\"macro\"), 0.41666667)\n    assert_almost_equal(jaccard_score(y_true, y_pred, average=\"weighted\"), 0.5)\n\n    # Verify Jaccard score without averaging\n    assert_array_almost_equal(jaccard_score(y_true, y_pred, average=None), [1, 0.5, 0, 1])\n", "def test_confusion_matrix_multiclass_unordered_labels():\n    # Test confusion matrix - multi-class case with unordered labels\n    y_true = [2, 0, 2, 2, 0, 1, 1, 2]\n    y_pred = [0, 0, 2, 2, 0, 2, 1, 1]\n\n    # compute confusion matrix with default label ordering\n    cm_default = confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm_default, [[2, 0, 0], [0, 1, 1], [1, 1, 2]])\n\n    # compute confusion matrix with explicitly provided unordered labels\n    cm_unordered = confusion_matrix(y_true, y_pred, labels=[1, 0, 2])\n    assert_array_equal(cm_unordered, [[1, 0, 1], [2, 2, 0], [1, 0, 2]])\n\n    # test with missing label in y_pred\n    cm_missing = confusion_matrix(y_true, y_pred, labels=[0, 2])\n    assert_array_equal(cm_missing, [[2, 0], [1, 3]])\n", "def test_multilabel_confusion_matrix_with_class_imbalance():\n    # Test multilabel confusion matrix with class imbalance\n    y_true = np.array([[1, 0, 1], [1, 0, 0], [0, 1, 1], [1, 1, 1]])\n    y_pred = np.array([[1, 0, 1], [0, 0, 0], [0, 0, 1], [1, 1, 1]])\n    expected_cm = [\n        [[2, 0], [1, 1]],\n        [[2, 1], [0, 1]],\n        [[1, 1], [0, 2]],\n    ]\n    \n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, expected_cm)\n\n    # Test with sample_weight\n    sample_weight = np.array([2, 1, 3, 1])\n    expected_cm_weighted = [\n        [[5, 0], [1, 2]],\n        [[5, 1], [0, 2]],\n        [[2, 1], [1, 4]],\n    ]\n    \n    cm_weighted = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    assert_array_equal(cm_weighted, expected_cm_weighted)\n", "def test_multilabel_confusion_matrix_with_sample_weight():\n    # Test multilabel confusion matrix - multilabel-indicator case with sample_weight\n    from scipy.sparse import csc_matrix, csr_matrix\n\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]])\n    y_true_csr = csr_matrix(y_true)\n    y_pred_csr = csr_matrix(y_pred)\n    y_true_csc = csc_matrix(y_true)\n    y_pred_csc = csc_matrix(y_pred)\n    sample_weight = np.array([2, 1, 3])\n\n    # cross test different types\n    real_cm = [[[2, 0], [1, 2]], [[2, 0], [1, 2]], [[0, 3], [1, 0]]]\n    trues = [y_true, y_true_csr, y_true_csc]\n    preds = [y_pred, y_pred_csr, y_pred_csc]\n\n    for y_true_tmp in trues:\n        for y_pred_tmp in preds:\n            cm = multilabel_confusion_matrix(y_true_tmp, y_pred_tmp, sample_weight=sample_weight)\n            assert_array_equal(cm, real_cm)\n", "def test_multilabel_accuracy_score_with_sparse_matrix():\n    # Test accuracy_score with sparse matrix input\n    from scipy.sparse import csr_matrix\n\n    # Dense label indicator matrix format\n    y1 = np.array([[0, 1, 1], [1, 0, 1]])\n    y2 = np.array([[0, 0, 1], [1, 0, 1]])\n\n    # Convert to sparse matrices\n    y1_sparse = csr_matrix(y1)\n    y2_sparse = csr_matrix(y2)\n\n    assert accuracy_score(y1_sparse, y2_sparse) == 0.5\n    assert accuracy_score(y1_sparse, y1_sparse) == 1\n    assert accuracy_score(y2_sparse, y2_sparse) == 1\n    assert accuracy_score(y2_sparse, np.logical_not(y2_sparse).astype(int)) == 0\n    assert accuracy_score(y1_sparse, np.logical_not(y1_sparse).astype(int)) == 0\n    assert accuracy_score(y1_sparse, csr_matrix(np.zeros(y1_sparse.shape))) == 0\n    assert accuracy_score(y2_sparse, csr_matrix(np.zeros(y1_sparse.shape))) == 0\n"], "sample_876": ["def test_early_stopping_validation_fraction():\n    # Test that validation_fraction is used correctly in early stopping\n    X = [[3, 2], [1, 6], [5, 6], [-2, -4], [3, 5], [6, 7], [8, 9], [1, -1]]\n    y = [0, 1, 1, 0, 1, 0, 1, 0]\n\n    mlp = MLPClassifier(\n        solver=\"sgd\", max_iter=50, early_stopping=True, validation_fraction=0.25\n    )\n    mlp.fit(X, y)\n\n    # Ensure that validation_fraction is correctly set and used\n    n_samples = len(X)\n    n_validation_samples = int(n_samples * mlp.validation_fraction)\n    assert n_validation_samples == 2  # 25% of 8 samples is 2\n\n    # Check if validation scores have been recorded\n    assert mlp.validation_scores_ is not None\n    assert len(mlp.validation_scores_) > 0\n\n    # Check if validation_fraction is in the expected range\n    assert 0 < mlp.validation_fraction < 1\n", "def test_initialization_parameters():\n    # Test that initializing MLPClassifier and MLPRegressor with different parameters works correctly\n    hidden_layer_sizes = (50, 30, 10)\n    alpha = 0.01\n    learning_rate_init = 0.05\n    max_iter = 100\n    random_state = 42\n    tol = 1e-4\n    activation = \"tanh\"\n    solver = \"adam\"\n\n    clf = MLPClassifier(\n        hidden_layer_sizes=hidden_layer_sizes,\n        alpha=alpha,\n        learning_rate_init=learning_rate_init,\n        max_iter=max_iter,\n        random_state=random_state,\n        tol=tol,\n        activation=activation,\n        solver=solver,\n    )\n\n    assert clf.hidden_layer_sizes == hidden_layer_sizes\n    assert clf.alpha == alpha\n    assert clf.learning_rate_init == learning_rate_init\n    assert clf.max_iter == max_iter\n    assert clf.random_state == random_state\n    assert clf.tol == tol\n    assert clf.activation == activation\n    assert clf.solver == solver\n\n    reg = MLPRegressor(\n        hidden_layer_sizes=hidden_layer_sizes,\n        alpha=alpha,\n        learning_rate_init=learning_rate_init,\n        max_iter=max_iter,\n        random_state=random_state,\n        tol=tol,\n        activation=activation,\n        solver=solver,\n    )\n\n    assert reg.hidden_layer_sizes == hidden_layer_sizes\n    assert reg.alpha == alpha\n    assert reg.learning_rate_init == learning_rate_init\n    assert reg.max_iter == max_iter\n    assert reg.random_state == random_state\n    assert reg.tol == tol\n    assert reg.activation == activation\n    assert reg.solver == solver\n\n    # Check the initialized attributes\n    clf.fit(X_iris, y_iris)\n    assert clf.n_layers_ == len(hidden_layer_sizes) + 2  # input layer + hidden layers + output layer\n    assert len(clf.coefs_) == len(hidden_layer_sizes) + 1\n    assert len(clf.intercepts_) == len(hidden_layer_sizes) + 1\n\n    reg.fit(X_reg, y_reg)\n    assert reg.n_layers_ == len(hidden_layer_sizes) + 2\n    assert len(reg.coefs_) == len(hidden_layer_sizes) + 1\n    assert len(reg.intercepts_) == len(hidden_layer_sizes) + 1\n", "def test_predict_proba_multiclass_edge_cases():\n    # Test predict_proba with edge cases in multiclass settings\n    X = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n    y = np.array([0, 1, 2])\n\n    clf = MLPClassifier(hidden_layer_sizes=5, activation=\"logistic\", random_state=1)\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y)\n    \n    # Check probabilities sum to 1 for each sample\n    y_proba = clf.predict_proba(X)\n    assert np.allclose(y_proba.sum(axis=1), 1)\n    \n    # Check predict_log_proba returns log of predict_proba\n    y_log_proba = clf.predict_log_proba(X)\n    assert_allclose(y_log_proba, np.log(y_proba))\n\n    # Check predict_proba for single sample input\n    single_sample = np.array([[0.1, 0.2]])\n    y_proba_single = clf.predict_proba(single_sample)\n    assert y_proba_single.shape == (1, 3)\n    assert np.allclose(y_proba_single.sum(axis=1), 1)\n\n    # Check predict_log_proba for single sample input\n    y_log_proba_single = clf.predict_log_proba(single_sample)\n    assert_allclose(y_log_proba_single, np.log(y_proba_single))\n", "def test_mlp_custom_activation():\n    # Test MLP with a custom activation function\n    class CustomActivation:\n            return np.maximum(0, x)  # ReLU\n\n            return np.where(x > 0, 1, 0)  # ReLU derivative\n\n    ACTIVATIONS['custom'] = CustomActivation()\n    DERIVATIVES['custom'] = CustomActivation().derivative\n\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n\n    mlp = MLPClassifier(\n        hidden_layer_sizes=(5,),\n        activation='custom',\n        max_iter=100,\n        random_state=1\n    )\n\n    with ignore_warnings(category=ConvergenceWarning):\n        mlp.fit(X, y)\n\n    y_pred = mlp.predict(X)\n    assert y_pred.shape == (50,)\n    assert accuracy_score(y, y_pred) > 0.9\n\n    # Cleanup the custom activation to not affect other tests\n    del ACTIVATIONS['custom']\n    del DERIVATIVES['custom']\n", "def test_backward_pass():\n    # Test the backward pass to ensure that gradients are computed correctly\n    # and backpropagation is functioning properly.\n    \n    X = np.array([[0.5, 0.3], [0.6, 0.7], [0.8, 0.5], [0.2, 0.9]])\n    y = np.array([0, 1, 1, 0])\n    \n    mlp = MLPClassifier(\n        solver=\"sgd\",\n        learning_rate_init=0.1,\n        alpha=0.1,\n        activation=\"logistic\",\n        random_state=1,\n        max_iter=1,\n        hidden_layer_sizes=2,\n        momentum=0,\n    )\n    \n    # Fit the model to initialize parameters\n    mlp.fit(X, y)\n    \n    # Manually compute gradients via backpropagation\n    layer_units = [X.shape[1]] + [mlp.hidden_layer_sizes] + [mlp.n_outputs_]\n    activations = [X] + [None] * (len(layer_units) - 1)\n    deltas = [None] * (len(activations) - 1)\n    coef_grads = [np.empty((n_fan_in_, n_fan_out_), dtype=X.dtype)\n                  for n_fan_in_, n_fan_out_ in zip(layer_units[:-1], layer_units[1:])]\n    intercept_grads = [np.empty(n_fan_out_, dtype=X.dtype) for n_fan_out_ in layer_units[1:]]\n\n    mlp._backprop(X, mlp._label_binarizer.transform(y), activations, deltas, coef_grads, intercept_grads)\n    \n    # Ensure that gradients are not zero or NaN (indicative of proper gradient computation)\n    for coef_grad, intercept_grad in zip(coef_grads, intercept_grads):\n        assert np.all(np.isfinite(coef_grad))\n        assert np.all(np.isfinite(intercept_grad))\n        assert np.any(coef_grad != 0)\n        assert np.any(intercept_grad != 0)\n"], "sample_877": ["def test_isotonic_regression_y_min_y_max():\n    # Test for isotonic regression with both y_min and y_max set\n    x = np.array([1, 2, 3, 4, 5, 6, 7])\n    y = np.array([10, 5, 6, 8, 7, 10, 9])\n    y_min = 6\n    y_max = 8\n\n    # Create model and fit_transform\n    ir = IsotonicRegression(y_min=y_min, y_max=y_max)\n    y_transformed = ir.fit_transform(x, y)\n\n    assert np.all(y_transformed >= y_min)\n    assert np.all(y_transformed <= y_max)\n", "def test_isotonic_regression_with_nan_values():\n    # Check IsotonicRegression with NaN values in the input\n    x = [0, 1, 2, np.nan, 4, 5]\n    y = [5, 6, 7, 8, 9, 10]\n\n    # Check that it raises an error when there are NaNs in the input\n    ir = IsotonicRegression()\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        ir.fit(x, y)\n\n    # Similarly for isotonic_regression function\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        isotonic_regression(y, sample_weight=x)\n", "def test_isotonic_regression_increasing_false():\n    y = np.array([1, 2, 3, 4, 5])\n    y_ = np.array([5, 4, 3, 2, 1])\n    x = np.arange(len(y))\n    \n    ir = IsotonicRegression(increasing=False)\n    y_transformed = ir.fit_transform(x, y)\n    \n    assert_array_equal(y_, y_transformed)\n", "def test_isotonic_regression_y_min_y_max_boundaries():\n    # Test to ensure y_min and y_max boundaries are respected\n    X = np.array([1, 2, 3, 4, 5])\n    y = np.array([1, 2, 3, 4, 5])\n    y_min = 2\n    y_max = 4\n\n    ir = IsotonicRegression(y_min=y_min, y_max=y_max)\n    y_transformed = ir.fit_transform(X, y)\n\n    assert np.all(y_transformed >= y_min)\n    assert np.all(y_transformed <= y_max)\n", "def test_isotonic_regression_y_min_y_max_clipping():\n    # Test the clipping behavior of y_min and y_max during isotonic regression\n    y = np.array([3, 7, 5, 9, 8, 7, 10])\n    y_min = 6\n    y_max = 9\n    y_expected = np.array([6, 6, 6, 8, 8, 8, 9])\n    \n    y_transformed = isotonic_regression(y, y_min=y_min, y_max=y_max)\n    assert_array_equal(y_transformed, y_expected)\n    \n    ir = IsotonicRegression(y_min=y_min, y_max=y_max)\n    ir.fit(np.arange(len(y)), y)\n    y_pred = ir.predict(np.arange(len(y)))\n    assert_array_equal(y_pred, y_expected)\n"], "sample_878": ["def test_column_transformer_callable_remainder():\n    \"\"\"Test the interaction between callable remainder and column transformer\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_fit_array = np.array([[0, 1, 2], [2, 4, 6], [3, 6, 9]]).T\n    X_fit_df = pd.DataFrame(X_fit_array, columns=[\"first\", \"second\", \"third\"])\n\n    X_trans_array = np.array([[2, 4, 6], [0, 1, 2], [3, 6, 9]]).T\n    X_trans_df = pd.DataFrame(X_trans_array, columns=[\"second\", \"first\", \"third\"])\n\n        return X[:, 2:]\n\n    tf = ColumnTransformer([(\"bycol\", Trans(), [0, 1])], remainder=remainder_callable)\n\n    tf.fit(X_fit_df)\n    X_fit_trans = tf.transform(X_fit_df)\n\n    # Changing the order still works\n    X_trans = tf.transform(X_trans_df)\n    assert_allclose(X_trans, X_fit_trans)\n\n    # extra columns are ignored\n    X_extended_df = X_fit_df.copy()\n    X_extended_df[\"fourth\"] = [4, 8, 12]\n    X_trans = tf.transform(X_extended_df)\n    assert_allclose(X_trans, X_fit_trans)\n", "def test_column_transformer_mixed_dtypes():\n    # Test ColumnTransformer with mixed data types\n    pd = pytest.importorskip(\"pandas\")\n    X_df = pd.DataFrame({\n        'num1': [1, 2, 3],\n        'num2': [4.0, 5.1, 6.2],\n        'cat1': ['a', 'b', 'a'],\n        'cat2': pd.Categorical(['cat', 'dog', 'cat']),\n        'text': ['apple', 'banana', 'cherry']\n    })\n    \n    ct = ColumnTransformer(\n        transformers=[\n            ('num', StandardScaler(), ['num1', 'num2']),\n            ('cat', OneHotEncoder(), ['cat1', 'cat2']),\n            ('txt', FunctionTransformer(lambda x: x), ['text'])\n        ],\n        remainder='passthrough'\n    )\n    \n    X_trans = ct.fit_transform(X_df)\n    \n    expected_transformed_shape = (3, 8)  # 2 scaled + 3 one-hot + 3 passthrough (2 numeric + 1 text)\n    assert X_trans.shape == expected_transformed_shape\n    \n    # Check if all columns are present in the transformed output\n    transformed_columns = ct.get_feature_names_out(input_features=X_df.columns)\n    expected_columns = [\n        'num__num1', 'num__num2',\n        'cat__cat1_a', 'cat__cat1_b', 'cat__cat2_cat', 'cat__cat2_dog',\n        'txt__text', 'remainder__num1', 'remainder__num2'\n    ]\n    assert all(col in transformed_columns for col in expected_columns)\n", "def test_column_transformer_with_hidden_params():\n    # Test ColumnTransformer with hidden parameter constraints\n    class HiddenTrans(TransformerMixin, BaseEstimator):\n            return self\n\n            return X + 1\n\n    X_array = np.array([[1, 2], [3, 4], [5, 6]])\n\n    hidden_transformer = HiddenTrans()\n    ct = ColumnTransformer(\n        [(\"hidden\", hidden_transformer, [0])],\n        remainder=\"passthrough\",\n        verbose_feature_names_out=True,\n    )\n\n    # Ensure it does not raise an error\n    X_trans = ct.fit_transform(X_array)\n    expected_result = np.array([[2, 2], [4, 4], [6, 6]])\n    assert_array_equal(X_trans, expected_result)\n\n    # Check the hidden transformer was correctly stored\n    assert isinstance(ct.named_transformers_[\"hidden\"], HiddenTrans)\n\n    # Check parameter constraints\n    params = ct.get_params()\n    assert \"hidden__verbose\" not in params  # Hidden parameter should not appear\n", "def test_column_transformer_invalid_remainder_estimator():\n    # Test that an invalid estimator for the remainder parameter raises an error\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    class InvalidEstimator:\n            return self\n\n            return X\n\n    remainder = InvalidEstimator()\n    ct = ColumnTransformer([(\"trans\", Trans(), [0])], remainder=remainder)\n    msg = \"All estimators should implement fit and transform\"\n    with pytest.raises(TypeError, match=msg):\n        ct.fit(X_array)\n", "def test_column_transformer_with_callable_remainder():\n    \"\"\"Test ColumnTransformer with callable remainder.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    \n    # DataFrame setup\n    df = pd.DataFrame(\n        {\n            \"col1\": [1, 2, 3],\n            \"col2\": [4, 5, 6],\n            \"col3\": [7, 8, 9],\n        }\n    )\n    \n        return [\"col2\", \"col3\"]\n    \n    # Transformer setup\n    ct = ColumnTransformer(\n        [(\"col1_transform\", Trans(), [\"col1\"])],\n        remainder=remainder_selector\n    )\n    \n    # Expected result\n    expected_result = np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])\n    \n    # Fit and transform\n    transformed = ct.fit_transform(df)\n    \n    # Assertions\n    assert_array_equal(transformed, expected_result)\n    assert ct.transformers_[-1][0] == \"remainder\"\n    assert callable(ct.transformers_[-1][2])\n    assert_array_equal(ct.transformers_[-1][2](df), [\"col2\", \"col3\"])\n"], "sample_879": ["def test_ordinal_encoder_drop_first():\n    \"\"\"Check OrdinalEncoder handles drop='first' correctly.\"\"\"\n\n    X = [[\"a\", \"cat\"], [\"b\", \"dog\"], [\"b\", \"cat\"], [\"c\", \"mouse\"]]\n    categories = [[\"a\", \"b\", \"c\"], [\"cat\", \"dog\", \"mouse\"]]\n    oe = OrdinalEncoder(categories=categories, drop='first')\n    \n    X_trans = oe.fit_transform(X)\n    \n    # Since 'a' and 'cat' are the first categories in their respective features, they should not be included\n    expected_X_trans = np.array([[0, 0], [1, 1], [1, 0], [2, 2]])\n    assert_array_equal(X_trans, expected_X_trans)\n    \n    # Check inverse transform\n    X_inv = oe.inverse_transform(X_trans)\n    expected_X_inv = np.array([['a', 'cat'], ['b', 'dog'], ['b', 'cat'], ['c', 'mouse']], dtype=object)\n    assert_array_equal(X_inv, expected_X_inv)\n", "def test_one_hot_encoder_min_frequency():\n    \"\"\"Check OneHotEncoder with min_frequency parameter works as expected.\"\"\"\n    X = [[\"dog\"], [\"cat\"], [\"cat\"], [\"dog\"], [\"dog\"], [\"fish\"]]\n    \n    # Use min_frequency as an integer\n    ohe = OneHotEncoder(min_frequency=2, sparse_output=False)\n    X_trans = ohe.fit_transform(X)\n    expected = np.array([[1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 0]])\n    assert_allclose(X_trans, expected)\n    assert_array_equal(ohe.infrequent_categories_, [[\"fish\"]])\n    \n    # Use min_frequency as a float\n    ohe = OneHotEncoder(min_frequency=0.3, sparse_output=False)\n    X_trans = ohe.fit_transform(X)\n    expected = np.array([[1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 0]])\n    assert_allclose(X_trans, expected)\n    assert_array_equal(ohe.infrequent_categories_, [[\"fish\"]])\n    \n    # Check if handle_unknown='infrequent_if_exist' works with min_frequency\n    ohe = OneHotEncoder(min_frequency=2, handle_unknown=\"infrequent_if_exist\", sparse_output=False)\n    X_trans = ohe.fit_transform(X)\n    X_test = [[\"dog\"], [\"fish\"]]\n    X_test_trans = ohe.transform(X_test)\n    expected = np.array([[1, 0], [0, 1]])\n    assert_allclose(X_test_trans, expected)\n    assert_array_equal(ohe.infrequent_categories_, [[\"fish\"]])\n", "def test_one_hot_encoder_feature_name_combiner_infrequent_categories():\n    \"\"\"Test that feature name combiner works correctly with infrequent categories.\"\"\"\n        return f\"{feature}_custom_{category}\"\n\n    X = [[\"a\"], [\"b\"], [\"c\"], [\"a\"], [\"a\"], [\"b\"], [\"d\"], [\"e\"], [\"e\"]]\n    ohe = OneHotEncoder(\n        max_categories=3,\n        handle_unknown=\"infrequent_if_exist\",\n        feature_name_combiner=custom_combiner,\n        sparse_output=False,\n    ).fit(X)\n\n    # Check infrequent categories are combined correctly\n    assert_array_equal(ohe.infrequent_categories_, [[\"d\", \"e\"]])\n\n    # Check feature names\n    feature_names = ohe.get_feature_names_out([\"feature\"])\n    expected_feature_names = [\"feature_custom_a\", \"feature_custom_b\", \"feature_custom_c\", \"feature_custom_infrequent_sklearn\"]\n    assert_array_equal(feature_names, expected_feature_names)\n\n    X_test = [[\"a\"], [\"b\"], [\"d\"], [\"e\"]]\n    X_trans = ohe.transform(X_test)\n    expected = np.array([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 0, 1],\n        [0, 0, 0, 1]\n    ])\n    assert_allclose(X_trans, expected)\n", "def test_one_hot_encoder_drop_infrequent_category():\n    \"\"\"Check dropping an infrequent category works as expected.\"\"\"\n    X = np.array([[\"dog\"], [\"cat\"], [\"cat\"], [\"bird\"], [\"cat\"], [\"dog\"], [\"bird\"]])\n    \n    ohe = OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"infrequent_if_exist\", min_frequency=2)\n    X_trans = ohe.fit_transform(X)\n    \n    # 'bird' is infrequent and should be combined to 'infrequent_sklearn' and dropped\n    expected_trans = np.array([[1, 0], [0, 1], [0, 1], [0, 0], [0, 1], [1, 0], [0, 0]])\n    assert_allclose(X_trans, expected_trans)\n    \n    # Check inverse_transform maps dropped category correctly\n    X_inv = ohe.inverse_transform(X_trans)\n    expected_inv = np.array([['dog'], ['cat'], ['cat'], ['dog'], ['cat'], ['dog'], ['dog']], dtype=object)\n    assert_array_equal(X_inv, expected_inv)\n", "def test_one_hot_encoder_inverse_transform_with_infrequent():\n    \"\"\"Test inverse_transform for OneHotEncoder with infrequent categories.\"\"\"\n    X = [[\"apple\", 2], [\"banana\", 3], [\"apple\", 3], [\"banana\", 2], [\"cherry\", 3], [\"cherry\", 3]]\n    enc = OneHotEncoder(handle_unknown=\"infrequent_if_exist\", sparse_output=False, min_frequency=2)\n    X_trans = enc.fit_transform(X)\n    \n    # \"cherry\" should be an infrequent category\n    expected_inverse = np.array([[\"apple\", 2], [\"banana\", 3], [\"apple\", 3], [\"banana\", 2], [\"infrequent_sklearn\", 3], [\"infrequent_sklearn\", 3]], dtype=object)\n    assert_array_equal(enc.inverse_transform(X_trans), expected_inverse)\n"], "sample_880": ["def test_ovr_decision_function():\n    # Define number of classes and generate random predictions and confidences\n    n_classes = 3\n    n_samples = 5\n    np.random.seed(0)\n    \n    predictions = np.random.randint(0, 2, size=(n_samples, n_classes * (n_classes - 1) // 2))\n    confidences = np.random.uniform(-1, 1, size=(n_samples, n_classes * (n_classes - 1) // 2))\n    \n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n\n    assert decision_function.shape == (n_samples, n_classes)\n    # Simple sanity checks to verify that the output values are within the expected range\n    assert np.all(decision_function >= -1)\n    assert np.all(decision_function <= n_classes)\n", "def test_ovr_decision_function():\n    # Test the One-vs-Rest decision function with mock predictions and confidences\n    n_classes = 3\n    predictions = np.array([\n        [0, 1, 0],\n        [1, 2, 1],\n        [2, 0, 2],\n        [0, 2, 1],\n    ])\n    confidences = np.array([\n        [1.0, -0.5, 2.0],\n        [-1.0, 2.0, -0.5],\n        [0.5, -1.0, 1.0],\n        [1.5, 0.5, -1.0],\n    ])\n\n    expected_ovr = np.array([\n        [3.0, -0.167, -2.833],\n        [-0.833, 1.5, -0.667],\n        [-1.167, 1.0, 1.167],\n        [2.167, -0.167, -2.0],\n    ])\n\n    ovr_scores = _ovr_decision_function(predictions, confidences, n_classes)\n    assert_allclose(ovr_scores, expected_ovr, atol=1e-3)\n", "def test_ovr_decision_function():\n    # Test _ovr_decision_function with a simple example\n    predictions = np.array([[0, 1], [1, 0], [0, 0]])\n    confidences = np.array([[0.8, 0.1], [0.4, 0.3], [0.9, 0.2]])\n    n_classes = 3\n\n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n    \n    expected_votes = np.array([[2, 1, 0], [1, 2, 0], [3, 0, 0]])\n    expected_sum_of_confidences = np.array([[0.8, -0.1, -0.9], [-0.4, 0.3, -0.7], [0.9, -0.2, -0.7]])\n    expected_transformed_confidences = expected_sum_of_confidences / (3 * (np.abs(expected_sum_of_confidences) + 1))\n    expected_decision_function = expected_votes + expected_transformed_confidences\n\n    assert_allclose(decision_function, expected_decision_function, atol=1e-6)\n\n    # Test with no predictions\n    predictions = np.zeros((0, 0))\n    confidences = np.zeros((0, 0))\n    n_classes = 0\n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n    assert decision_function.shape == (0, 0)\n", "def test_ovr_decision_function():\n    # Test _ovr_decision_function with a small example\n    predictions = np.array([\n        [0, 1, 1],\n        [1, 0, 0],\n        [1, 1, 0],\n        [0, 0, 1],\n    ])\n    \n    confidences = np.array([\n        [0.8, -0.6, 0.7],\n        [-0.5, 0.9, -0.4],\n        [0.2, 0.3, -0.7],\n        [0.6, -0.8, 0.4],\n    ])\n    \n    n_classes = 3\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    \n    # Expected decision values are calculated manually\n    expected_decision = np.array([\n        [1.26666667, -0.13333333, 1.16666667],\n        [0.3, 1.43333333, -1.36666667],\n        [0.76666667, 1.1, -0.86666667],\n        [1.26666667, -0.53333333, 1.56666667],\n    ])\n    \n    assert_allclose(decision, expected_decision, rtol=1e-6)\n", "def test_ovr_decision_function():\n    predictions = np.array([\n        [0, 1, 0, 1, 0, 1],\n        [1, 0, 1, 0, 1, 0],\n        [0, 1, 0, 1, 0, 1],\n        [1, 0, 1, 0, 1, 0]\n    ])\n\n    confidences = np.array([\n        [0.2, 0.8, 0.3, 0.7, 0.6, 0.4],\n        [0.8, 0.2, 0.7, 0.3, 0.4, 0.6],\n        [0.3, 0.7, 0.2, 0.8, 0.5, 0.5],\n        [0.7, 0.3, 0.8, 0.2, 0.5, 0.5]\n    ])\n\n    n_classes = 3\n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n    \n    expected_votes = np.array([\n        [2, 2, 2],\n        [2, 2, 2],\n        [2, 2, 2],\n        [2, 2, 2]\n    ])\n    \n    expected_sum_of_confidences = np.array([\n        [-0.7, 2.5, -1.8],\n        [2.5, -0.7, -1.8],\n        [-0.7, 2.5, -1.8],\n        [2.5, -0.7, -1.8]\n    ])\n\n    expected_transformed_confidences = expected_sum_of_confidences / (3 * (np.abs(expected_sum_of_confidences) + 1))\n    expected_decision_function = expected_votes + expected_transformed_confidences\n\n    assert_allclose(decision_function, expected_decision_function)\n"], "sample_881": ["def test_auc_memmap():\n    # Test auc with numpy memmap input\n    filename = 'test_memmap.dat'\n    x = np.array([0, 0.5, 1], dtype=np.float64)\n    y = np.array([0, 0.5, 1], dtype=np.float64)\n    memmap_x = np.memmap(filename, dtype='float64', mode='w+', shape=(3,))\n    memmap_y = np.memmap(filename, dtype='float64', mode='w+', shape=(3,))\n    memmap_x[:] = x[:]\n    memmap_y[:] = y[:]\n    assert_array_almost_equal(auc(memmap_x, memmap_y), 0.5)\n", "def test_auc_edge_cases():\n    # Test edge cases for the auc function\n    \n    # Case: Very small difference in x\n    x = [0, 1e-10]\n    y = [0, 1]\n    assert_array_almost_equal(auc(x, y), 1e-10 / 2)\n\n    # Case: Very small difference in y\n    x = [0, 1]\n    y = [0, 1e-10]\n    assert_array_almost_equal(auc(x, y), 1 / 2 * 1e-10)\n\n    # Case: High precision floats\n    x = [0, 1]\n    y = [0, np.pi]\n    assert_array_almost_equal(auc(x, y), np.pi / 2)\n\n    # Case: Inf values in x\n    x = [0, np.inf]\n    y = [0, 1]\n    with pytest.raises(ValueError, match=\"x is neither increasing nor decreasing\"):\n        auc(x, y)\n\n    # Case: Inf values in y\n    x = [0, 1]\n    y = [0, np.inf]\n    assert_array_almost_equal(auc(x, y), np.inf)\n", "def test_label_ranking_loss_zero_relevant_labels():\n    # Test that label_ranking_loss returns 0 when there are zero relevant labels.\n    y_true = np.array([[0, 0, 0], [0, 0, 0]])\n    y_score = np.array([[0.25, 0.5, 0.75], [0.1, 0.2, 0.3]])\n    result = label_ranking_loss(y_true, y_score)\n    assert result == 0.0\n\n    # Test that label_ranking_loss handles cases with all relevant labels.\n    y_true_all_relevant = np.array([[1, 1, 1], [1, 1, 1]])\n    result_all_relevant = label_ranking_loss(y_true_all_relevant, y_score)\n    assert result_all_relevant == 0.0\n", "def test_auc_with_memmap():\n    # Create a memmap array for x and y\n    x = np.memmap('x.mmap', dtype='float32', mode='w+', shape=(3,))\n    y = np.memmap('y.mmap', dtype='float32', mode='w+', shape=(3,))\n    \n    # Assign values to x and y\n    x[:] = [0, 0.5, 1]\n    y[:] = [0, 0.5, 1]\n\n    # Compute auc using the memmap arrays\n    result = auc(x, y)\n\n    # Verify that the result is as expected\n    assert_almost_equal(result, 0.5)\n\n    # Clean up the memmap files\n    import os\n    os.remove('x.mmap')\n    os.remove('y.mmap')\n", "def test_roc_auc_score_multilabel():\n    # Test roc_auc_score for multilabel-indicator format.\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 0]])\n    y_score = np.array([[0.8, 0.1, 0.9], [0.4, 0.6, 0.3], [0.7, 0.2, 0.1]])\n\n    # Macro average\n    macro_roc_auc = roc_auc_score(y_true, y_score, average='macro')\n    expected_macro = (0.75 + 1.0 + 0.5) / 3  # Average of AUCs for each label\n    assert_almost_equal(macro_roc_auc, expected_macro)\n\n    # Weighted average\n    weighted_roc_auc = roc_auc_score(y_true, y_score, average='weighted')\n    expected_weighted = (0.75 * 2/3) + (1.0 * 1/3) + (0.5 * 1/3)\n    assert_almost_equal(weighted_roc_auc, expected_weighted)\n\n    # Samples average\n    samples_roc_auc = roc_auc_score(y_true, y_score, average='samples')\n    assert samples_roc_auc == pytest.approx(1.0 / 3 * (0.75 + 0.5 + 1.0))\n\n    # Micro average\n    micro_roc_auc = roc_auc_score(y_true, y_score, average='micro')\n    assert micro_roc_auc == pytest.approx(0.75)\n"], "sample_882": ["def test_mlp_classifier_loss_curve():\n    # Test that the loss curve is being properly recorded during training\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n\n    clf = MLPClassifier(hidden_layer_sizes=5, activation=\"relu\", random_state=1, max_iter=50)\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y)\n\n    # Check that the loss curve has recorded the loss at each iteration\n    assert len(clf.loss_curve_) == clf.n_iter_\n    # Check that the loss decreases over iterations\n    assert all(x >= y for x, y in zip(clf.loss_curve_, clf.loss_curve_[1:]))\n", "def test_activation_functions():\n    # Test that each activation function works as expected with the MLPClassifier.\n    X_train, y_train = X_digits_binary[:150], y_digits_binary[:150]\n    X_test, y_test = X_digits_binary[150:], y_digits_binary[150:]\n\n    for activation in ACTIVATION_TYPES:\n        mlp = MLPClassifier(\n            solver=\"adam\",\n            hidden_layer_sizes=(50,),\n            max_iter=200,\n            random_state=1,\n            activation=activation,\n        )\n        with ignore_warnings(category=ConvergenceWarning):\n            mlp.fit(X_train, y_train)\n        \n        y_pred = mlp.predict(X_test)\n        score = mlp.score(X_test, y_test)\n        assert score > 0.8, f\"Failed with activation {activation}\"\n        assert y_pred.shape == y_test.shape\n        assert np.all(np.isin(y_pred, mlp.classes_))\n\n        if activation != \"identity\":\n            y_proba = mlp.predict_proba(X_test)\n            assert y_proba.shape == (X_test.shape[0], len(mlp.classes_))\n            assert np.allclose(np.sum(y_proba, axis=1), 1)\n", "def test_forward_pass():\n    # Test the forward pass computations.\n    X = X_digits_binary[:10]\n    y = y_digits_binary[:10]\n\n    mlp = MLPClassifier(\n        hidden_layer_sizes=(5,),\n        activation='relu',\n        solver='adam',\n        max_iter=1,\n        random_state=42\n    )\n    mlp.fit(X, y)\n\n    activations = [X] + [None] * (mlp.n_layers_ - 1)\n    forward_pass_activations = mlp._forward_pass(activations)\n\n    # Check the shape of each layer's activation\n    assert forward_pass_activations[0].shape == X.shape\n    assert forward_pass_activations[-1].shape == (X.shape[0], mlp.n_outputs_)\n    for i in range(1, mlp.n_layers_ - 1):\n        assert forward_pass_activations[i].shape == (X.shape[0], mlp.hidden_layer_sizes[0])\n", "def test_hidden_layer_sizes_validation():\n    \"\"\"Test validation of hidden_layer_sizes parameter.\"\"\"\n    X, y = X_digits_binary[:10], y_digits_binary[:10]\n\n    # Invalid type for hidden_layer_sizes\n    with pytest.raises(ValueError, match=\"hidden_layer_sizes must be > 0, got\"):\n        MLPClassifier(hidden_layer_sizes=-5).fit(X, y)\n\n    # Invalid values in hidden_layer_sizes array\n    with pytest.raises(ValueError, match=\"hidden_layer_sizes must be > 0, got\"):\n        MLPClassifier(hidden_layer_sizes=(10, -5, 20)).fit(X, y)\n\n    # Single positive integer for hidden_layer_sizes\n    clf = MLPClassifier(hidden_layer_sizes=5)\n    clf.fit(X, y)\n    assert len(clf.hidden_layer_sizes) == 1\n    assert clf.hidden_layer_sizes[0] == 5\n\n    # Array of positive integers for hidden_layer_sizes\n    clf = MLPClassifier(hidden_layer_sizes=(10, 20, 30))\n    clf.fit(X, y)\n    assert len(clf.hidden_layer_sizes) == 3\n    assert clf.hidden_layer_sizes == [10, 20, 30]\n", "def test_activation_functions():\n    # Test that all activation functions work as expected.\n\n    X_train = X_digits_binary[:100]\n    y_train = y_digits_binary[:100]\n    X_test = X_digits_binary[100:]\n    y_test = y_digits_binary[100:]\n\n    for activation in ACTIVATION_TYPES:\n        mlp = MLPClassifier(\n            solver=\"adam\",\n            hidden_layer_sizes=(10, 5),\n            activation=activation,\n            random_state=1,\n            max_iter=200,\n        )\n        with ignore_warnings(category=ConvergenceWarning):\n            mlp.fit(X_train, y_train)\n        score = mlp.score(X_test, y_test)\n        assert score > 0.80, f\"Failed with activation {activation}\"\n"], "sample_883": ["def test_bayesian_ridge_parameter_constraints():\n    # Test that BayesianRidge and ARDRegression raise appropriate errors for invalid parameter values\n    invalid_parameters = [\n        {\"max_iter\": 0},  # Invalid as it should be >= 1\n        {\"tol\": -1e-3},  # Invalid as it should be >= 0\n        {\"alpha_1\": -1e-6},  # Invalid as it should be > 0\n        {\"alpha_2\": -1e-6},  # Invalid as it should be > 0\n        {\"lambda_1\": -1e-6},  # Invalid as it should be > 0\n        {\"lambda_2\": -1e-6},  # Invalid as it should be > 0\n        {\"alpha_init\": -1},  # Invalid as it should be > 0 or None\n        {\"lambda_init\": -1},  # Invalid as it should be > 0 or None\n        {\"compute_score\": \"not_a_boolean\"},  # Invalid as it should be a boolean\n        {\"fit_intercept\": \"not_a_boolean\"},  # Invalid as it should be a boolean\n        {\"copy_X\": \"not_a_boolean\"},  # Invalid as it should be a boolean\n        {\"verbose\": \"not_a_verbose_option\"},  # Invalid as it should be a verbose option\n        {\"n_iter\": 0},  # Invalid as it should be >= 1\n    ]\n\n    for params in invalid_parameters:\n        with pytest.raises(ValueError):\n            BayesianRidge(**params)\n        with pytest.raises(ValueError):\n            ARDRegression(**params)\n", "def test_bayesian_ridge_max_iter():\n    \"\"\"Check that max_iter parameter works correctly and stops the fitting process after given iterations.\"\"\"\n    X, y = diabetes.data, diabetes.target\n\n    max_iter = 5\n    clf = BayesianRidge(max_iter=max_iter)\n    clf.fit(X, y)\n\n    assert clf.n_iter_ == max_iter\n", "def test_bayesian_ridge_fit_intercept():\n    # Test BayesianRidge with and without intercept\n    X = np.array([[0], [1], [2], [3]])\n    y = np.array([0, 1, 2, 3])\n\n    # Without intercept\n    model_no_intercept = BayesianRidge(fit_intercept=False)\n    model_no_intercept.fit(X, y)\n    assert_almost_equal(model_no_intercept.intercept_, 0.0)\n    assert_array_almost_equal(model_no_intercept.predict(X), y)\n\n    # With intercept\n    model_with_intercept = BayesianRidge(fit_intercept=True)\n    model_with_intercept.fit(X, y)\n    assert_almost_equal(model_with_intercept.intercept_, 0.0)\n    assert_array_almost_equal(model_with_intercept.predict(X), y)\n", "def test_bayesian_ridge_negative_tolerance():\n    \"\"\"Check that BayesianRidge raises an error for negative tolerance.\"\"\"\n    X, y = diabetes.data, diabetes.target\n\n    with pytest.raises(ValueError, match=r\"Tolerance should be positive\"):\n        clf = BayesianRidge(tol=-1e-3)\n        clf.fit(X, y)\n", "def test_bayesian_ridge_default_parameters():\n    # Test that default parameters produce a reasonable model\n    X, y = diabetes.data, diabetes.target\n    clf = BayesianRidge()\n    clf.fit(X, y)\n    \n    # Check that the coefficients are not all zero\n    assert np.any(clf.coef_ != 0)\n    \n    # Check that alpha_ and lambda_ are positive\n    assert clf.alpha_ > 0\n    assert clf.lambda_ > 0\n\n    # Check that the number of iterations is within a reasonable range\n    assert clf.n_iter_ > 0\n    assert clf.n_iter_ <= 300\n\n    # Check that the shape of sigma_ is correct\n    assert clf.sigma_.shape == (X.shape[1], X.shape[1])\n"], "sample_884": ["def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        n_features = MockClass2().n_features_\n    assert n_features == 10\n", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        assert MockClass2().n_features_ == 10\n", "def test_deprecated_property():\n    mock_instance = MockClass2()\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        assert mock_instance.n_features_ == 10\n", "def test_deprecated_property():\n    instance = MockClass2()\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        value = instance.n_features_\n    assert value == 10\n", "def test_property_deprecation():\n    mock_instance = MockClass2()\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        assert mock_instance.n_features_ == 10\n"], "sample_885": ["def test_hidden_constraint_with_callable():\n    \"\"\"Check that hidden constraints work with callable constraints.\"\"\"\n\n    @validate_params({\"param\": [Hidden(callable), str]})\n        pass\n\n    # Both callable and str are valid params\n    f(\"string\")\n    f(lambda x: x + 1)\n\n    with pytest.raises(\n        InvalidParameterError, match=\"The 'param' parameter\"\n    ) as exc_info:\n        f(param=123)\n\n    # The callable option is not exposed in the error message\n    err_msg = str(exc_info.value)\n    assert \"an instance of 'str'\" in err_msg\n    assert \"a callable\" not in err_msg\n", "def test_is_not_satisfied_by(constraint_declaration, value, expected):\n    \"\"\"Sanity check for the is_satisfied_by method for invalid values\"\"\"\n    constraint = make_constraint(constraint_declaration)\n    assert constraint.is_satisfied_by(value) == expected\n", "def test_multiple_constraints():\n    \"\"\"Check that validate_params works with multiple constraints on a single parameter.\"\"\"\n\n    @validate_params({\n        \"param\": [Interval(Integral, 0, 10, closed=\"both\"), StrOptions({\"option1\", \"option2\"})]\n    })\n        pass\n\n    # param can be an integer within the specified interval\n    f(5)\n    f(10)\n\n    # param can be a string from the specified options\n    f(\"option1\")\n    f(\"option2\")\n\n    # invalid cases\n    with pytest.raises(InvalidParameterError, match=\"The 'param' parameter\"):\n        f(11)  # out of interval\n\n    with pytest.raises(InvalidParameterError, match=\"The 'param' parameter\"):\n        f(\"option3\")  # not in StrOptions\n", "def test_hidden_options():\n    \"\"\"Check that internal Options constraints are not exposed in the error message.\"\"\"\n\n    @validate_params({\"param\": [Hidden(Options(int, {1, 2, 3})), Options(str, {\"a\", \"b\"})]})\n        pass\n\n    # \"a\" and \"b\" are valid params\n    f(\"a\")\n    f(\"b\")\n\n    with pytest.raises(\n        InvalidParameterError, match=\"The 'param' parameter\"\n    ) as exc_info:\n        f(param=\"wrong\")\n\n    # the integer options are not exposed in the error message\n    err_msg = str(exc_info.value)\n    assert \"an int among {1, 2, 3}\" not in err_msg\n    assert \"a str among {'a', 'b'}\" in err_msg\n", "def test_hidden_constraint_with_multiple_hidden_constraints():\n    \"\"\"Check that multiple Hidden constraints are not exposed in the error message.\"\"\"\n\n    @validate_params({\"param\": [Hidden(list), Hidden(dict), int]})\n        pass\n\n    # list, dict, and int are valid params\n    f([1, 2, 3])\n    f({\"a\": 1, \"b\": 2})\n    f(10)\n\n    with pytest.raises(\n        InvalidParameterError, match=\"The 'param' parameter\"\n    ) as exc_info:\n        f(param=\"bad\")\n\n    # Only int should be exposed in the error message\n    err_msg = str(exc_info.value)\n    assert \"an instance of 'int'\" in err_msg\n    assert \"an instance of 'list'\" not in err_msg\n    assert \"an instance of 'dict'\" not in err_msg\n"], "sample_886": ["def test__wrap_data_with_container_default():\n    \"\"\"Check _wrap_data_with_container with default configuration.\"\"\"\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().fit(X)\n\n    # Default configuration should return data_to_wrap unchanged\n    est.set_output(transform=\"default\")\n    result = _wrap_data_with_container(\"transform\", X, X, est)\n    assert result is X\n\n    # If not configured, return data_to_wrap unchanged\n    est._sklearn_output_config = {}\n    result = _wrap_data_with_container(\"transform\", X, X, est)\n    assert result is X\n", "def test_set_output_with_sparse_data():\n    \"\"\"Check that _SetOutputMixin raises an error with sparse input.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    class EstimatorWithSparseInput(_SetOutputMixin):\n            self.n_features_in_ = X.shape[1]\n            return self\n\n            return csr_matrix(X)\n\n            return np.asarray([f\"X{i}\" for i in range(self.n_features_in_)], dtype=object)\n\n    X = csr_matrix([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSparseInput().fit(X.toarray())\n    est.set_output(transform=\"pandas\")\n\n    msg = \"Pandas output does not support sparse data.\"\n    with pytest.raises(ValueError, match=msg):\n        est.transform(X)\n", "def test__wrap_data_with_container_default():\n    \"\"\"Check _wrap_data_with_container for default config.\"\"\"\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().fit(X)\n    \n    # Ensure global config is set to 'default'\n    with config_context(transform_output=\"default\"):\n        output = _wrap_data_with_container(\"transform\", X, X, est)\n        assert isinstance(output, np.ndarray)\n        assert_array_equal(output, X)\n\n    # Set estimator-specific config to 'default'\n    est.set_output(transform=\"default\")\n    output = _wrap_data_with_container(\"transform\", X, X, est)\n    assert isinstance(output, np.ndarray)\n    assert_array_equal(output, X)\n", "def test__wrap_data_with_container():\n    \"\"\"Check _wrap_data_with_container works as expected.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    \n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    original_input = pd.DataFrame(X, columns=[\"a\", \"b\", \"c\"])\n\n    est = EstimatorWithSetOutput().fit(X)\n    _safe_set_output(est, transform=\"pandas\")\n    wrapped_data = _wrap_data_with_container(\"transform\", X, original_input, est)\n    assert isinstance(wrapped_data, pd.DataFrame)\n    assert_array_equal(wrapped_data.columns, [\"X0\", \"X1\", \"X2\"])\n\n    _safe_set_output(est, transform=\"default\")\n    wrapped_data = _wrap_data_with_container(\"transform\", X, original_input, est)\n    assert isinstance(wrapped_data, np.ndarray)\n    assert_array_equal(wrapped_data, X)\n\n    est.set_output(transform=\"pandas\")\n    wrapped_data = _wrap_data_with_container(\"transform\", X, original_input, est)\n    assert isinstance(wrapped_data, pd.DataFrame)\n    assert_array_equal(wrapped_data.columns, [\"X0\", \"X1\", \"X2\"])\n", "def test__wrap_data_with_container():\n    \"\"\"Check _wrap_data_with_container for various configurations.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    \n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    original_input = pd.DataFrame(X)\n    est = EstimatorWithSetOutput().fit(X)\n\n    # Case 1: Default output configuration\n    est.set_output(transform=\"default\")\n    output = _wrap_data_with_container(\"transform\", X, original_input, est)\n    assert isinstance(output, np.ndarray)\n\n    # Case 2: Pandas output configuration\n    est.set_output(transform=\"pandas\")\n    output = _wrap_data_with_container(\"transform\", X, original_input, est)\n    assert isinstance(output, pd.DataFrame)\n    assert_array_equal(output.columns, est.get_feature_names_out())\n\n    # Case 3: Estimator not configured for auto-wrapping\n    class EstimatorWithoutAutoWrap(_SetOutputMixin):\n            return X\n\n            return [\"f0\", \"f1\", \"f2\"]\n\n    est_no_wrap = EstimatorWithoutAutoWrap().fit(X)\n    output = _wrap_data_with_container(\"transform\", X, original_input, est_no_wrap)\n    assert isinstance(output, np.ndarray)\n"], "sample_887": ["def test_calibrated_classifier_cv_predict_proba_not_fitted():\n    # Check that calling `predict_proba` on an unfitted model raises an error\n    X, y = make_classification(n_samples=50, n_features=5, random_state=42)\n    clf = LinearSVC()\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3)\n\n    with pytest.raises(NotFittedError, match=\"This CalibratedClassifierCV instance is not fitted yet.\"):\n        cal_clf.predict_proba(X)\n", "def test_calibrated_classifier_deprecation_base_estimator_method():\n    \"\"\"Check that the deprecation warning is raised when using `base_estimator` instead of `estimator` with a different method.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n\n    # Using base_estimator with method 'isotonic'\n    clf = MultinomialNB()\n    calibrated_classifier = CalibratedClassifierCV(base_estimator=clf, method=\"isotonic\")\n\n    warn_msg = \"`base_estimator` was renamed to `estimator`\"\n    with pytest.warns(FutureWarning, match=warn_msg):\n        calibrated_classifier.fit(X, y)\n\n    # Ensure the deprecation warning is still raised with method 'sigmoid'\n    calibrated_classifier = CalibratedClassifierCV(base_estimator=clf, method=\"sigmoid\")\n    with pytest.warns(FutureWarning, match=warn_msg):\n        calibrated_classifier.fit(X, y)\n", "def test_calibrated_classifier_cv_check_sample_weight_invariance(method, ensemble):\n    \"\"\"Test that CalibratedClassifierCV results are invariant to sample weights\n    set to zero, which effectively removes samples from training.\"\"\"\n    X, y = load_iris(return_X_y=True)\n    X = StandardScaler().fit_transform(X)\n    X, y = X[y != 2], y[y != 2]  # binary classification\n\n    sample_weight = np.ones_like(y)\n    sample_weight[::2] = 0\n\n    estimator = LogisticRegression()\n    calibrated_clf_with_sw = CalibratedClassifierCV(\n        estimator, method=method, ensemble=ensemble, cv=2\n    )\n    calibrated_clf_without_sw = clone(calibrated_clf_with_sw)\n\n    calibrated_clf_with_sw.fit(X, y, sample_weight=sample_weight)\n    calibrated_clf_without_sw.fit(X[1::2], y[1::2])\n\n    for est_with_sw, est_without_sw in zip(\n        calibrated_clf_with_sw.calibrated_classifiers_,\n        calibrated_clf_without_sw.calibrated_classifiers_,\n    ):\n        assert_allclose(\n            est_with_sw.estimator.coef_, est_without_sw.estimator.coef_\n        )\n\n    prob_with_sw = calibrated_clf_with_sw.predict_proba(X)\n    prob_without_sw = calibrated_clf_without_sw.predict_proba(X)\n    assert_allclose(prob_with_sw, prob_without_sw)\n", "def test_calibrated_classifier_cv_parallel_fit_params(data):\n    \"\"\"Test that fit parameters are correctly passed to the base estimator\n    when using parallel fitting in CalibratedClassifierCV.\"\"\"\n    X, y = data\n    fit_params = {\"custom_param\": np.ones_like(y)}\n\n    class EstimatorWithCustomParam(LogisticRegression):\n            assert custom_param is not None\n            return super().fit(X, y, sample_weight=sample_weight)\n\n    clf = EstimatorWithCustomParam()\n    cal_clf = CalibratedClassifierCV(clf, n_jobs=2, cv=3)\n    cal_clf.fit(X, y, **fit_params)\n", "def test_calibration_with_pipeline_transformations():\n    \"\"\"Test that CalibratedClassifierCV works with pipelines that include \n    transformations.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n    \n    # Create a pipeline with transformations and a classifier\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('classifier', LogisticRegression())\n    ])\n    \n    # Instantiate the CalibratedClassifierCV with the pipeline\n    cal_clf = CalibratedClassifierCV(pipeline, cv=3, method='sigmoid', ensemble=True)\n    \n    # Fit the calibrated classifier\n    cal_clf.fit(X, y)\n    \n    # Make predictions\n    probas = cal_clf.predict_proba(X)\n    \n    # Check that the probabilities sum to 1\n    assert np.allclose(probas.sum(axis=1), np.ones(probas.shape[0]))\n    \n    # Check that the predictions are reasonable\n    assert probas.min() >= 0\n    assert probas.max() <= 1\n"], "sample_888": ["def test_iforest_with_different_feature_subsample_sizes():\n    \"\"\"Test Isolation Forest with different feature subsample sizes.\"\"\"\n    X, _ = make_classification(n_samples=100, n_features=20, random_state=0)\n\n    # Test with max_features as float less than 1\n    clf = IsolationForest(max_features=0.5)\n    clf.fit(X)\n    assert len(clf.estimators_features_[0]) == int(0.5 * X.shape[1])\n    \n    # Test with max_features as integer less than number of features\n    clf = IsolationForest(max_features=10)\n    clf.fit(X)\n    assert len(clf.estimators_features_[0]) == 10\n\n    # Test with max_features as integer equal to number of features\n    clf = IsolationForest(max_features=20)\n    clf.fit(X)\n    assert len(clf.estimators_features_[0]) == 20\n", "def test_iforest_predict():\n    \"\"\"Test the predict method of IsolationForest\"\"\"\n    X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    X_test_inliers = np.array([[2, 3], [4, 5], [6, 7]])\n    X_test_outliers = np.array([[100, 200], [200, 300]])\n\n    clf = IsolationForest(contamination=0.2, random_state=42)\n    clf.fit(X_train)\n    \n    # Predict inliers\n    predictions_inliers = clf.predict(X_test_inliers)\n    assert_array_equal(predictions_inliers, np.ones(len(X_test_inliers), dtype=int))\n\n    # Predict outliers\n    predictions_outliers = clf.predict(X_test_outliers)\n    assert_array_equal(predictions_outliers, -np.ones(len(X_test_outliers), dtype=int))\n", "def test_iforest_with_random_state():\n    \"\"\"Test if Isolation Forest produces consistent results with a fixed random_state.\"\"\"\n    X_train = np.array([[0, 1], [1, 2], [2, 1], [1, 1], [0, 0], [2, 2]])\n    X_test = np.array([[0.5, 1.5], [1.5, 1.5], [1, 0.5]])\n\n    random_state = 42\n    clf1 = IsolationForest(random_state=random_state)\n    clf2 = IsolationForest(random_state=random_state)\n\n    clf1.fit(X_train)\n    clf2.fit(X_train)\n\n    pred1 = clf1.predict(X_test)\n    pred2 = clf2.predict(X_test)\n\n    assert_array_equal(pred1, pred2)\n", "def test_iforest_with_different_random_state():\n    \"\"\"Test IsolationForest with different random_state values.\"\"\"\n    X = iris.data\n    \n    clf1 = IsolationForest(random_state=42).fit(X)\n    clf2 = IsolationForest(random_state=42).fit(X)\n    clf3 = IsolationForest(random_state=1).fit(X)\n    \n    # Ensure that using the same random_state results in the same predictions\n    assert_array_equal(clf1.predict(X), clf2.predict(X))\n    \n    # Ensure that using a different random_state results in different predictions\n    assert not np.array_equal(clf1.predict(X), clf3.predict(X))\n", "def test_iforest_invalid_params():\n    \"\"\"Test that Invalid parameters raise appropriate exceptions.\"\"\"\n    X = iris.data\n\n    # Test invalid n_estimators\n    with pytest.raises(ValueError, match=\"n_estimators must be an integer\"):\n        IsolationForest(n_estimators=0).fit(X)\n    with pytest.raises(ValueError, match=\"n_estimators must be an integer\"):\n        IsolationForest(n_estimators=-10).fit(X)\n    with pytest.raises(ValueError, match=\"n_estimators must be an integer\"):\n        IsolationForest(n_estimators=1.5).fit(X)\n\n    # Test invalid max_samples\n    with pytest.raises(ValueError, match=\"max_samples must be 'auto', an int\"):\n        IsolationForest(max_samples=0).fit(X)\n    with pytest.raises(ValueError, match=\"max_samples must be 'auto', an int\"):\n        IsolationForest(max_samples=-1).fit(X)\n    with pytest.raises(ValueError, match=\"max_samples must be 'auto', an int\"):\n        IsolationForest(max_samples=1.5).fit(X)\n\n    # Test invalid contamination\n    with pytest.raises(ValueError, match=\"contamination must be in the range\"):\n        IsolationForest(contamination=0).fit(X)\n    with pytest.raises(ValueError, match=\"contamination must be in the range\"):\n        IsolationForest(contamination=0.6).fit(X)\n    with pytest.raises(ValueError, match=\"contamination must be in the range\"):\n        IsolationForest(contamination=-0.1).fit(X)\n\n    # Test invalid max_features\n    with pytest.raises(ValueError, match=\"max_features must be an int or float\"):\n        IsolationForest(max_features=0).fit(X)\n    with pytest.raises(ValueError, match=\"max_features must be an int or float\"):\n        IsolationForest(max_features=-1).fit(X)\n    with pytest.raises(ValueError, match=\"max_features must be an int or float\"):\n        IsolationForest(max_features=1.5).fit(X)\n\n    # Test invalid bootstrap\n    with pytest.raises(ValueError, match=\"bootstrap must be a boolean\"):\n        IsolationForest(bootstrap=\"invalid\").fit(X)\n\n    # Test invalid n_jobs\n    with pytest.raises(ValueError, match=\"n_jobs must be an integer\"):\n        IsolationForest(n_jobs=\"invalid\").fit(X)\n    with pytest.raises(ValueError, match=\"n_jobs must be an integer\"):\n        IsolationForest(n_jobs=1.5).fit(X)\n\n    # Test invalid random"], "sample_889": ["def test_calibrated_classifier_cv_predict_proba_shape(method):\n    \"\"\"Check the shape of the output from `predict_proba` matches the number of classes.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3, random_state=42)\n    clf = RandomForestClassifier(random_state=42)\n\n    cal_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n    cal_clf.fit(X, y)\n    probas = cal_clf.predict_proba(X)\n\n    assert probas.shape == (X.shape[0], len(np.unique(y))), (\n        f\"Expected shape {(X.shape[0], len(np.unique(y)))}, got {probas.shape}\"\n    )\n", "def test_calibration_invalid_estimator(method, ensemble):\n    \"\"\"Test CalibratedClassifierCV with invalid estimators.\"\"\"\n    class InvalidEstimator:\n        \"\"\"An invalid estimator that does not implement fit, predict_proba, or decision_function.\"\"\"\n        pass\n\n    X, y = make_classification(n_samples=100, n_features=4, random_state=42)\n    with pytest.raises(ValueError, match=\"The estimator should implement 'fit' and 'predict_proba' or 'fit' and 'decision_function' methods\"):\n        CalibratedClassifierCV(estimator=InvalidEstimator(), method=method, ensemble=ensemble).fit(X, y)\n", "def test_calibrated_classifier_cv_parameter_constraints():\n    \"\"\"Test that the parameter constraints are enforced for CalibratedClassifierCV.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n\n    invalid_estimators = [\n        \"invalid_string\",\n        123,\n        {\"fit\": lambda: None, \"predict\": lambda: None},\n    ]\n\n    for invalid_estimator in invalid_estimators:\n        with pytest.raises(ValueError, match=\"Invalid parameter 'estimator'\"):\n            CalibratedClassifierCV(estimator=invalid_estimator).fit(X, y)\n\n    invalid_methods = [\"invalid_method\", 123]\n    for invalid_method in invalid_methods:\n        with pytest.raises(ValueError, match=\"Invalid parameter 'method'\"):\n            CalibratedClassifierCV(estimator=LogisticRegression(), method=invalid_method).fit(X, y)\n\n    invalid_cv = [\"invalid_cv\", 123.45]\n    for invalid_cv_value in invalid_cv:\n        with pytest.raises(ValueError, match=\"Invalid parameter 'cv'\"):\n            CalibratedClassifierCV(estimator=LogisticRegression(), cv=invalid_cv_value).fit(X, y)\n\n    invalid_n_jobs = [\"invalid_n_jobs\", 123.45]\n    for invalid_n_jobs_value in invalid_n_jobs:\n        with pytest.raises(ValueError, match=\"Invalid parameter 'n_jobs'\"):\n            CalibratedClassifierCV(estimator=LogisticRegression(), n_jobs=invalid_n_jobs_value).fit(X, y)\n\n    invalid_ensemble = [\"invalid_ensemble\", 123]\n    for invalid_ensemble_value in invalid_ensemble:\n        with pytest.raises(ValueError, match=\"Invalid parameter 'ensemble'\"):\n            CalibratedClassifierCV(estimator=LogisticRegression(), ensemble=invalid_ensemble_value).fit(X, y)\n\n    invalid_base_estimators = [\n        \"invalid_base_estimator\",\n        123,\n        {\"fit\": lambda: None, \"predict\": lambda: None},\n    ]\n    for invalid_base_estimator in invalid_base_estimators:\n        with pytest.raises(ValueError, match=\"Invalid parameter 'base_estimator'\"):\n            CalibratedClassifierCV(base_estimator=invalid_base_estimator).fit(X, y)\n", "def test_calibrated_classifier_cv_more_tags():\n    \"\"\"Test the _more_tags method for CalibratedClassifierCV.\"\"\"\n    calibrated_classifier = CalibratedClassifierCV(estimator=LogisticRegression())\n    tags = calibrated_classifier._more_tags()\n    assert \"_xfail_checks\" in tags\n    assert \"check_sample_weights_invariance\" in tags[\"_xfail_checks\"]\n    assert (\n        tags[\"_xfail_checks\"][\"check_sample_weights_invariance\"]\n        == \"Due to the cross-validation and sample ordering, removing a sample\"\n           \" is not strictly equal to putting is weight to zero. Specific unit\"\n           \" tests are added for CalibratedClassifierCV specifically.\"\n    )\n", "def test_calibrated_classifier_cv_predict_proba(data, method, ensemble):\n    \"\"\"Test the predict_proba method of CalibratedClassifierCV.\"\"\"\n    X, y = data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n    # Train a base estimator and a calibrated classifier\n    clf = LogisticRegression()\n    cal_clf = CalibratedClassifierCV(clf, method=method, ensemble=ensemble)\n    cal_clf.fit(X_train, y_train)\n\n    # Predict probabilities for the test set\n    probas = cal_clf.predict_proba(X_test)\n\n    # Check that the predicted probabilities sum to 1\n    assert_allclose(np.sum(probas, axis=1), np.ones(probas.shape[0]))\n\n    # Check that the shape of the predicted probabilities is correct\n    assert probas.shape == (X_test.shape[0], len(cal_clf.classes_))\n\n    # Check that the predicted probabilities are non-negative and do not exceed 1\n    assert np.all(probas >= 0) and np.all(probas <= 1)\n"], "sample_890": ["def test_invalid_estimator():\n    \"\"\"Check that SequentialFeatureSelector raises an error if an invalid estimator is passed.\"\"\"\n    X, y = make_regression(n_features=10, random_state=0)\n\n    class InvalidEstimator:\n            pass\n\n    sfs = SequentialFeatureSelector(\n        InvalidEstimator(),\n        n_features_to_select=2,\n    )\n\n    with pytest.raises(TypeError, match=\"Estimator should implement 'fit' method\"):\n        sfs.fit(X, y)\n", "def test_invalid_estimator():\n    \"\"\"Check that SequentialFeatureSelector raises a ValueError for invalid estimator\"\"\"\n    X, y = make_regression(n_features=10, random_state=0)\n    invalid_estimator = \"not_an_estimator\"\n\n    with pytest.raises(ValueError, match=\"estimator should be an estimator implementing 'fit'\"):\n        SequentialFeatureSelector(\n            invalid_estimator, \n            n_features_to_select=\"auto\"\n        ).fit(X, y)\n", "def test_n_jobs_parallel():\n    \"\"\"Check that the n_jobs parameter is respected and allows parallel computation\"\"\"\n    from sklearn.utils.testing import assert_run_parallel\n\n    X, y = make_regression(n_features=20, random_state=0)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=5,\n        direction='forward',\n        n_jobs=2,\n        cv=5\n    )\n\n    with assert_run_parallel(n_jobs=2):\n        sfs.fit(X, y)\n", "def test_custom_scorer():\n    \"\"\"Ensure that a custom scoring function works as expected.\"\"\"\n\n        return -np.mean((estimator.predict(X) - y) ** 2)  # negative MSE for minimization\n\n    X, y = make_regression(n_features=5, random_state=0)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=3,\n        scoring=custom_scorer,\n        cv=3,\n    )\n    sfs.fit(X, y)\n\n    assert sfs.transform(X).shape[1] == 3\n", "def test_invalid_cv_object():\n    \"\"\"Test that an invalid cv object raises an error.\"\"\"\n    X, y = make_classification(n_features=10, n_samples=100, random_state=0)\n    invalid_cv = \"invalid_cv_object\"\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=5,\n        cv=invalid_cv,\n    )\n\n    with pytest.raises(ValueError, match=\"Expected cv as an integer, cross-validation object (from sklearn.model_selection) or an iterable yielding train, test splits. Got\"):\n        sfs.fit(X, y)\n"], "sample_891": ["def test_auc_memmap():\n    # Test Area Under Curve (AUC) computation with numpy memmap\n    x = np.array([0, 0.5, 1])\n    y = np.array([0, 0.5, 1])\n    \n    # Create a temporary file for memmap\n    with open('tempfile.dat', 'wb') as f:\n        f.write(x.tobytes())\n        f.write(y.tobytes())\n\n    x_memmap = np.memmap('tempfile.dat', dtype='float64', mode='r', shape=(3,))\n    y_memmap = np.memmap('tempfile.dat', dtype='float64', mode='r', shape=(3,), offset=x.nbytes)\n    \n    assert_array_almost_equal(auc(x_memmap, y_memmap), 0.5)\n\n    # Clean up temporary file\n    import os\n    os.remove('tempfile.dat')\n", "def test_auc_custom(y_true, y_score, expected_auc):\n    assert_almost_equal(auc(y_true, y_score), expected_auc)\n", "def test_roc_auc_score_multilabel():\n    # Test roc_auc_score for multilabel-indicator format\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_score = np.array([[0.8, 0.1, 0.6], [0.2, 0.7, 0.4], [0.6, 0.9, 0.3]])\n    \n    # Calculate ROC AUC scores for each label\n    roc_auc_0 = roc_auc_score(y_true[:, 0], y_score[:, 0])\n    roc_auc_1 = roc_auc_score(y_true[:, 1], y_score[:, 1])\n    roc_auc_2 = roc_auc_score(y_true[:, 2], y_score[:, 2])\n    \n    # Test 'macro' average\n    expected_macro = (roc_auc_0 + roc_auc_1 + roc_auc_2) / 3\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='macro'), expected_macro)\n    \n    # Test 'weighted' average\n    support = np.sum(y_true, axis=0)\n    expected_weighted = (roc_auc_0 * support[0] + roc_auc_1 * support[1] + roc_auc_2 * support[2]) / support.sum()\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='weighted'), expected_weighted)\n    \n    # Test 'samples' average\n    roc_auc_samples = [roc_auc_score(y_true[i], y_score[i]) for i in range(len(y_true))]\n    expected_samples = np.mean(roc_auc_samples)\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='samples'), expected_samples)\n    \n    # Test 'micro' average\n    y_true_ravel = y_true.ravel()\n    y_score_ravel = y_score.ravel()\n    expected_micro = roc_auc_score(y_true_ravel, y_score_ravel)\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='micro'), expected_micro)\n", "def test_auc_memmap():\n    # Test auc with numpy.memmap input\n    x = np.memmap('x.mmap', dtype='float32', mode='w+', shape=(3,))\n    y = np.memmap('y.mmap', dtype='float32', mode='w+', shape=(3,))\n    x[:] = [0, 0.5, 1]\n    y[:] = [0, 0.5, 1]\n    \n    result = auc(x, y)\n    assert result == pytest.approx(0.5)\n\n    # Clean up\n    import os\n    os.remove('x.mmap')\n    os.remove('y.mmap')\n", "def test_average_precision_score_no_positives():\n    # Test average_precision_score when there are no positive samples in y_true\n    y_true = np.array([0, 0, 0, 0])\n    y_score = np.array([0.1, 0.2, 0.3, 0.4])\n    \n    # Check the warning for no positive class\n    expected_message = \"No positive class found in y_true, recall is set to one for all thresholds.\"\n    with pytest.warns(UserWarning, match=expected_message):\n        precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n    \n    assert_array_almost_equal(precision, [0.0, 0.0, 0.0, 1.0])\n    assert_array_almost_equal(recall, [1.0, 1.0, 1.0, 0.0])\n    assert_array_almost_equal(thresholds, [0.1, 0.2, 0.3, 0.4])\n    \n    # Check the score\n    with pytest.warns(UserWarning, match=expected_message):\n        avg_precision = average_precision_score(y_true, y_score)\n    assert avg_precision == 0.0\n"], "sample_892": ["def test_adaboost_regressor_staged_predict_consistency():\n    # Test the consistency of staged predictions for AdaBoostRegressor.\n    rng = np.random.RandomState(0)\n    X, y = datasets.make_regression(n_samples=50, n_features=4, noise=0.1, random_state=rng)\n    reg = AdaBoostRegressor(n_estimators=10, random_state=rng)\n    reg.fit(X, y)\n    \n    staged_preds = list(reg.staged_predict(X))\n    final_pred = reg.predict(X)\n    \n    # The last staged prediction should be the same as the final prediction\n    assert_array_almost_equal(staged_preds[-1], final_pred)\n    \n    # The length of staged predictions should be equal to the number of estimators\n    assert len(staged_preds) == reg.n_estimators\n", "def test_adaboost_classifier_multiclass():\n    # Check AdaBoostClassifier on a multi-class classification problem\n    X, y = datasets.make_classification(\n        n_samples=1000, n_features=20, n_informative=10, n_classes=3, random_state=0\n    )\n\n    clf = AdaBoostClassifier(n_estimators=50, random_state=0)\n    clf.fit(X, y)\n\n    # Check predict\n    y_pred = clf.predict(X)\n    assert_array_equal(np.unique(y_pred), np.array([0, 1, 2]))\n\n    # Check predict_proba\n    y_proba = clf.predict_proba(X)\n    assert y_proba.shape == (1000, 3)\n    assert_array_almost_equal(np.sum(y_proba, axis=1), np.ones(1000))\n\n    # Check decision_function\n    y_decision = clf.decision_function(X)\n    assert y_decision.shape == (1000, 3)\n    assert_array_equal(np.argmax(y_decision, axis=1), y_pred)\n\n    # Check score\n    score = clf.score(X, y)\n    assert score > 0.8\n", "def test_adaboostclassifier_feature_importances():\n    \"\"\"Check if feature importances are computed correctly for AdaBoostClassifier.\"\"\"\n    X, y = datasets.make_classification(\n        n_samples=1000,\n        n_features=10,\n        n_informative=5,\n        n_redundant=2,\n        random_state=42\n    )\n\n    clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n\n    assert importances.shape[0] == 10\n    assert np.sum(importances) == pytest.approx(1.0)\n    assert np.all(importances >= 0)\n", "def test_adaboost_classifier_with_different_estimators():\n    # Test AdaBoostClassifier with different base estimators\n\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.naive_bayes import GaussianNB\n\n    X, y = datasets.make_classification(\n        n_samples=100, n_features=4, n_informative=2, n_redundant=0, random_state=0\n    )\n\n    # Test with KNeighborsClassifier\n    clf_knn = AdaBoostClassifier(estimator=KNeighborsClassifier(), random_state=0)\n    clf_knn.fit(X, y)\n    assert clf_knn.score(X, y) > 0.9, \"Failed with KNeighborsClassifier\"\n\n    # Test with GaussianNB\n    clf_gnb = AdaBoostClassifier(estimator=GaussianNB(), random_state=0)\n    clf_gnb.fit(X, y)\n    assert clf_gnb.score(X, y) > 0.9, \"Failed with GaussianNB\"\n\n    # Ensure that the classes_ attribute is properly set\n    assert_array_equal(clf_knn.classes_, clf_gnb.classes_)\n", "def test_adaboost_classifier_fit_with_custom_estimator():\n    \"\"\"Check that AdaBoostClassifier works with a custom base estimator.\"\"\"\n    \n    class CustomEstimator(BaseEstimator):\n            self.param = param\n        \n            self.classes_ = np.unique(y)\n            return self\n        \n            return np.full(X.shape[0], self.classes_[0])\n        \n            proba = np.zeros((X.shape[0], len(self.classes_)))\n            proba[:, 0] = 1.0\n            return proba\n    \n    X, y = datasets.make_classification(n_samples=100, n_features=4, random_state=42)\n    custom_estimator = CustomEstimator(param=42)\n    clf = AdaBoostClassifier(estimator=custom_estimator, n_estimators=5, random_state=42)\n    \n    clf.fit(X, y)\n    assert clf.estimator_.param == 42\n    assert len(clf.estimators_) == 5\n    assert clf.score(X, y) > 0.5\n"], "sample_893": ["def test_color_brew():\n    # Test the _color_brew function for different numbers of colors\n    colors_1 = _color_brew(1)\n    assert len(colors_1) == 1\n    assert colors_1 == [[255, 191, 191]]\n\n    colors_2 = _color_brew(2)\n    assert len(colors_2) == 2\n    assert colors_2 == [[255, 191, 191], [191, 255, 223]]\n\n    colors_3 = _color_brew(3)\n    assert len(colors_3) == 3\n    assert colors_3 == [[255, 191, 191], [191, 255, 223], [191, 223, 255]]\n\n    colors_10 = _color_brew(10)\n    assert len(colors_10) == 10\n    assert colors_10[0] == [255, 191, 191]\n    assert colors_10[1] == [255, 223, 191]\n    assert colors_10[2] == [255, 255, 191]\n    assert colors_10[3] == [223, 255, 191]\n    assert colors_10[4] == [191, 255, 191]\n    assert colors_10[5] == [191, 255, 223]\n    assert colors_10[6] == [191, 255, 255]\n    assert colors_10[7] == [191, 223, 255]\n    assert colors_10[8] == [191, 191, 255]\n    assert colors_10[9] == [223, 191, 255]\n", "def test_plot_tree_regression(pyplot):\n    # mostly smoke tests\n    # Check correctness of plot_tree for regression tree\n    reg = DecisionTreeRegressor(max_depth=3, min_samples_split=2, criterion=\"squared_error\", random_state=2)\n    reg.fit(X, y)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(reg, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\nsquared_error = 1.0\\nsamples = 6\\nvalue = 0.0\"\n    )\n    assert nodes[1].get_text() == \"squared_error = 0.0\\nsamples = 3\\nvalue = -1.0\"\n    assert nodes[2].get_text() == \"squared_error = 0.0\\nsamples = 3\\nvalue = 1.0\"\n", "def test_plot_tree_with_different_parameters(pyplot):\n    # Check plot_tree with different parameter combinations\n    clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n    clf.fit(X, y)\n\n    # Test plot with impurity=False\n    nodes = plot_tree(clf, impurity=False)\n    assert len(nodes) == 3\n    assert \"gini\" not in nodes[0].get_text()\n    assert \"gini\" not in nodes[1].get_text()\n    assert \"gini\" not in nodes[2].get_text()\n\n    # Test plot with filled=True\n    nodes = plot_tree(clf, filled=True)\n    assert len(nodes) == 3\n    assert nodes[0].get_bbox_patch().get_facecolor() != nodes[1].get_bbox_patch().get_facecolor()\n\n    # Test plot with rounded=True\n    nodes = plot_tree(clf, rounded=True)\n    assert len(nodes) == 3\n    assert \"boxstyle\" in nodes[0].get_bbox_patch().get_path_effects()[0].get_path().properties()\n    \n    # Test plot with node_ids=True\n    nodes = plot_tree(clf, node_ids=True)\n    assert len(nodes) == 3\n    assert \"node 0\" in nodes[0].get_text()\n    assert \"node 1\" in nodes[1].get_text()\n    assert \"node 2\" in nodes[2].get_text()\n\n    # Test plot with proportion=True\n    nodes = plot_tree(clf, proportion=True)\n    assert len(nodes) == 3\n    assert \"%\" in nodes[0].get_text()\n    assert \"%\" in nodes[1].get_text()\n    assert \"%\" in nodes[2].get_text()\n\n    # Test plot with different fontsize\n    nodes = plot_tree(clf, fontsize=14)\n    assert len(nodes) == 3\n    assert nodes[0].get_fontsize() == 14\n    assert nodes[1].get_fontsize() == 14\n    assert nodes[2].get_fontsize() == 14\n", "def test_plot_tree_regression(pyplot):\n    # mostly smoke tests\n    # Check correctness of plot_tree for DecisionTreeRegressor\n    reg = DecisionTreeRegressor(\n        max_depth=3, min_samples_split=2, criterion=\"squared_error\", random_state=2\n    )\n    reg.fit(X, y)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(reg, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\nsquared_error = 1.0\\nsamples = 6\\nvalue = 0.0\"\n    )\n    assert nodes[1].get_text() == \"squared_error = 0.0\\nsamples = 3\\nvalue = -1.0\"\n    assert nodes[2].get_text() == \"squared_error = 0.0\\nsamples = 3\\nvalue = 1.0\"\n", "def test_export_text_multioutput_classifier():\n    # Test export_text for multi-output classifier\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    X_multi = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [2, 2]]\n    y_multi = [[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1]]\n    clf.fit(X_multi, y_multi)\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- value: [0.00, 1.00]\n    |--- feature_1 >  0.00\n    |   |--- value: [1.00, 0.00]\n    \"\"\").lstrip()\n    assert export_text(clf) == expected_report\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- weights: [[3.00, 0.00], [0.00, 3.00]] value: [0.00, 1.00]\n    |--- feature_1 >  0.00\n    |   |--- weights: [[0.00, 4.00], [4.00, 0.00]] value: [1.00, 0.00]\n    \"\"\").lstrip()\n    assert export_text(clf, show_weights=True) == expected_report\n"], "sample_894": ["def test_max_samples_fraction(name):\n    \"\"\"Test if the `max_samples` parameter works correctly when set as a fraction.\"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_large, y_large, random_state=0, stratify=y_large\n    )\n    \n    max_samples_fraction = 0.5\n    n_samples = X_train.shape[0]\n    expected_n_samples_bootstrap = int(max_samples_fraction * n_samples)\n    \n    est = FOREST_ESTIMATORS[name](\n        bootstrap=True,\n        max_samples=max_samples_fraction,\n        random_state=0\n    )\n    est.fit(X_train, y_train)\n    \n    # Check if the bootstrap samples size is as expected\n    for tree in est.estimators_:\n        sample_indices = _generate_sample_indices(\n            tree.random_state, n_samples, expected_n_samples_bootstrap\n        )\n        assert len(sample_indices) == expected_n_samples_bootstrap\n        \n    # Check if the model predictions are valid\n    if name in FOREST_CLASSIFIERS:\n        y_pred = est.predict(X_test)\n        accuracy = (y_pred == y_test).mean()\n        assert accuracy > 0.5, f\"Failed with accuracy {accuracy} for {name}\"\n    elif name in FOREST_REGRESSORS:\n        y_pred = est.predict(X_test)\n        mse = mean_squared_error(y_test, y_pred)\n        assert mse < 2.0, f\"Failed with MSE {mse} for {name}\"\n", "def test_random_forest_classifier_warm_start():\n    # Check that RandomForestClassifier with warm start grows a forest incrementally\n    X, y = make_classification(n_samples=100, n_features=4, random_state=0)\n    clf = RandomForestClassifier(n_estimators=5, warm_start=True, random_state=0)\n\n    clf.fit(X, y)\n    initial_n_estimators = len(clf)\n    \n    clf.set_params(n_estimators=10)\n    clf.fit(X, y)\n    assert len(clf) == 10\n\n    # Check that predictions with warm start are consistent with a new fit\n    clf_new = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf_new.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), clf_new.predict(X))\n\n    # Check that feature importances with warm start are consistent with a new fit\n    assert_array_almost_equal(clf.feature_importances_, clf_new.feature_importances_)\n", "def test_random_trees_embedding_fit_transform():\n    # Test RandomTreesEmbedding's fit_transform method\n    X, y = datasets.make_classification(n_samples=100, n_features=4, random_state=0)\n    \n    # Create the RandomTreesEmbedding with sparse_output=False\n    hasher = RandomTreesEmbedding(n_estimators=10, sparse_output=False, random_state=0)\n    X_transformed_dense = hasher.fit_transform(X)\n    \n    # Assert that type is ndarray, not scipy.sparse.csr_matrix\n    assert isinstance(X_transformed_dense, np.ndarray)\n    \n    # Create the RandomTreesEmbedding with sparse_output=True\n    hasher_sparse = RandomTreesEmbedding(n_estimators=10, sparse_output=True, random_state=0)\n    X_transformed_sparse = hasher_sparse.fit_transform(X)\n    \n    # Assert that type is sparse matrix\n    assert isinstance(X_transformed_sparse, csr_matrix)\n    \n    # Ensure the outputs are the same regardless of the sparse_output setting\n    assert_array_equal(X_transformed_sparse.toarray(), X_transformed_dense)\n    \n    # Ensure the fit_transform method returns the same result as calling fit and then transform\n    hasher_fit = RandomTreesEmbedding(n_estimators=10, sparse_output=False, random_state=0)\n    hasher_fit.fit(X)\n    X_transformed_fit = hasher_fit.transform(X)\n    assert_array_equal(X_transformed_fit, X_transformed_dense)\n", "def test_feature_importances_sum_to_one(name):\n    # Test that feature importances sum to one\n    ForestEstimator = FOREST_CLASSIFIERS_REGRESSORS[name]\n    X, y = datasets.make_classification(n_samples=100, n_features=4, random_state=42)\n    \n    est = ForestEstimator(n_estimators=10, random_state=0)\n    est.fit(X, y)\n    feature_importances = est.feature_importances_\n    \n    assert np.isclose(np.sum(feature_importances), 1.0), (\n        f\"Feature importances do not sum to one: {feature_importances}\"\n    )\n", "def test_random_forest_max_samples_invalid_type():\n    \"\"\"Check that RandomForest raises an error with invalid `max_samples` type.\"\"\"\n    est = RandomForestClassifier(bootstrap=True, max_samples=\"invalid_type\")\n    with pytest.raises(ValueError, match=\"Invalid type for `max_samples` parameter\"):\n        est.fit(X, y)\n\n    est = RandomForestRegressor(bootstrap=True, max_samples=complex(1, 1))\n    with pytest.raises(ValueError, match=\"Invalid type for `max_samples` parameter\"):\n        est.fit(X, y)\n"], "sample_895": ["def test_column_transformer_with_callable_remainder():\n    \"\"\"Test ColumnTransformer with a callable remainder that selects columns based on condition.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    # DataFrame with multiple types of columns\n    df = pd.DataFrame({\n        'num1': [1, 2, 3, 4],\n        'num2': [5, 6, 7, 8],\n        'cat': ['a', 'b', 'a', 'b'],\n        'text': ['foo', 'bar', 'foo', 'bar'],\n    })\n\n    # Callable remainder that selects numeric columns\n        return X.select_dtypes(include=np.number).columns\n\n    # Define ColumnTransformer\n    ct = ColumnTransformer(\n        transformers=[\n            ('cat', OneHotEncoder(), ['cat']),\n        ],\n        remainder=numeric_columns\n    )\n\n    # Expected transformed output\n    expected = np.array([\n        [1, 0, 1, 5],\n        [0, 1, 2, 6],\n        [1, 0, 3, 7],\n        [0, 1, 4, 8]\n    ])\n\n    # Check fit_transform output\n    result = ct.fit_transform(df)\n    assert_array_equal(result, expected)\n\n    # Check transform output\n    result = ct.transform(df)\n    assert_array_equal(result, expected)\n", "def test_column_transformer_with_none_transformer():\n    # Test ColumnTransformer with None as a transformer\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([(\"trans1\", None, [0]), (\"trans2\", Trans(), [1])], remainder=\"passthrough\")\n    with pytest.raises(TypeError, match=\"All estimators should implement fit and transform\"):\n        ct.fit(X_array)\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", None, [1])], remainder=\"drop\")\n    with pytest.raises(TypeError, match=\"All estimators should implement fit and transform\"):\n        ct.fit(X_array)\n", "def test_column_transformer_with_callable_that_raises():\n    \"\"\"Test that ColumnTransformer correctly propagates an error from a callable column specifier.\"\"\"\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n        raise ValueError(\"Intentional error from callable\")\n\n    ct = ColumnTransformer([(\"trans\", Trans(), faulty_callable)], remainder=\"drop\")\n\n    with pytest.raises(ValueError, match=\"Intentional error from callable\"):\n        ct.fit_transform(X_array)\n\n    with pytest.raises(ValueError, match=\"Intentional error from callable\"):\n        ct.fit(X_array).transform(X_array)\n", "def test_column_transformer_set_output_with_callable():\n    \"\"\"Test the set_output method when a callable transformer is used.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n        return X * 2\n\n    X = pd.DataFrame({\n        \"a\": [1, 2, 3],\n        \"b\": [4, 5, 6]\n    })\n\n    multiply_transformer = FunctionTransformer(multiply_by_two, feature_names_out=\"one-to-one\")\n\n    ct = ColumnTransformer(\n        [\n            (\"mult_a\", multiply_transformer, [\"a\"]),\n            (\"mult_b\", multiply_transformer, [\"b\"])\n        ]\n    )\n\n    ct.set_output(transform=\"pandas\")\n\n    X_transformed = ct.fit_transform(X)\n    expected_columns = [\"mult_a__a\", \"mult_b__b\"]\n    assert isinstance(X_transformed, pd.DataFrame)\n    assert_array_equal(X_transformed.columns, expected_columns)\n    assert_array_equal(X_transformed[\"mult_a__a\"], X[\"a\"] * 2)\n    assert_array_equal(X_transformed[\"mult_b__b\"], X[\"b\"] * 2)\n", "def test_column_transformer_callable_remainder():\n    # Test that callable remainder works as expected\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    X_res_both = X_array.copy()\n\n    # second and third columns are doubled when remainder is callable DoubleTrans\n    X_res_both[:, 1:3] *= 2\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [0])],\n        remainder=lambda x: DoubleTrans().fit_transform(x)\n    )\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == \"remainder\"\n    assert isinstance(ct.transformers_[-1][1], FunctionTransformer)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n"], "sample_896": ["def test_nmf_inverse_transform_sparse():\n    # Test that NMF.inverse_transform works with sparse input data\n    random_state = np.random.RandomState(0)\n    A = np.abs(random_state.randn(6, 4))\n    A_sparse = sp.csr_matrix(A)\n    m = NMF(\n        solver=\"cd\",\n        n_components=4,\n        init=\"random\",\n        random_state=0,\n        max_iter=1000,\n    )\n    ft = m.fit_transform(A_sparse)\n    A_new = m.inverse_transform(ft)\n    assert_array_almost_equal(A_sparse.toarray(), A_new, decimal=2)\n", "def test_special_sparse_dot_with_nonzero_X():\n    # Test _special_sparse_dot function with non-zero elements in X\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_components = 10, 8, 5\n    W = np.abs(rng.randn(n_samples, n_components))\n    H = np.abs(rng.randn(n_components, n_features))\n\n    # Creating a sparse matrix X with specific non-zero elements\n    X = sp.csr_matrix((n_samples, n_features))\n    X[0, 0] = 1.0\n    X[1, 2] = 2.0\n    X[3, 5] = 3.0\n    X[4, 7] = 4.0\n    X.eliminate_zeros()\n\n    WH_safe = nmf._special_sparse_dot(W, H, X)\n    WH = np.dot(W, H)\n\n    # Test that WH_safe only has non-zero values where X is non-zero\n    ii, jj = X.nonzero()\n    WH_safe_data = np.asarray(WH_safe[ii, jj]).ravel()\n    assert_array_almost_equal(WH_safe_data, WH[ii, jj], decimal=10)\n\n    # Test that WH_safe and X have the same sparse structure\n    assert_array_equal(WH_safe.indices, X.indices)\n    assert_array_equal(WH_safe.indptr, X.indptr)\n    assert_array_equal(WH_safe.shape, X.shape)\n", "def test_nmf_solver_mu_with_custom_init():\n    # Check that NMF with solver='mu' works correctly with custom initialization\n    rng = np.random.RandomState(42)\n    X = np.abs(rng.randn(6, 5))\n    n_components = 3\n    avg = np.sqrt(X.mean() / n_components)\n    W_init = np.abs(avg * rng.randn(6, n_components))\n    H_init = np.abs(avg * rng.randn(n_components, 5))\n\n    nmf = NMF(\n        n_components=n_components,\n        init=\"custom\",\n        solver=\"mu\",\n        random_state=0,\n        max_iter=100,\n        tol=1e-4,\n    )\n\n    W = nmf.fit_transform(X, W=W_init, H=H_init)\n    H = nmf.components_\n    X_reconstructed = np.dot(W, H)\n    reconstruction_err = linalg.norm(X - X_reconstructed, 'fro')\n\n    assert not ((W < 0).any() or (H < 0).any())\n    assert reconstruction_err < 0.1\n", "def test_invalid_init_value():\n    # Test that an error is raised for invalid init parameter\n    A = np.ones((2, 2))\n    msg = \"Invalid init parameter: got 'invalid' instead of one of\"\n    with pytest.raises(ValueError, match=msg):\n        NMF(n_components=2, init=\"invalid\").fit(A)\n\n    with pytest.raises(ValueError, match=msg):\n        MiniBatchNMF(n_components=2, init=\"invalid\").fit(A)\n", "def test_beta_divergence_exceptions():\n    # Test that _beta_divergence raises appropriate exceptions for invalid inputs\n    W = np.array([[1, 2], [3, 4]])\n    H = np.array([[1, 2], [3, 4]])\n    \n    # Invalid beta_loss string\n    with pytest.raises(ValueError, match=\"beta_loss should be one of\"):\n        nmf._beta_divergence(W, W, H, \"invalid_loss\")\n\n    # Invalid beta_loss type\n    with pytest.raises(TypeError, match=\"beta_loss should be either a float or one of\"):\n        nmf._beta_divergence(W, W, H, {\"beta\": 2})\n    \n    # Sparse matrix with invalid shape\n    X = sp.csr_matrix(W)\n    with pytest.raises(ValueError, match=\"Array with wrong first dimension passed\"):\n        nmf._beta_divergence(X[:1], W, H, 2)\n        \n    # Square root with beta not equal to 2\n    with pytest.raises(ValueError, match=\"Square root can only be applied when beta is 2\"):\n        nmf._beta_divergence(W, W, H, 1, square_root=True)\n\n"], "sample_897": ["def test_plot_partial_dependence_with_custom_subsample_limit(pyplot, clf_diabetes, diabetes):\n    # Test partial dependence plot with custom subsample limit\n    grid_resolution = 25\n    disp = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [\"age\", (\"age\", \"bmi\")],\n        grid_resolution=grid_resolution,\n        feature_names=diabetes.feature_names,\n        subsample=2,\n        random_state=42,\n    )\n    \n    # Check that the correct number of ICE lines are plotted\n    assert disp.lines_.shape == (1, 2, 3)\n    assert len(disp.lines_[0, 0]) == 2  # Only 2 ICE lines should be plotted due to subsample limit\n    assert len(disp.lines_[0, 1]) == 2  # Only 2 ICE lines should be plotted due to subsample limit\n    \n    # Check that the axes labels are correct\n    assert disp.axes_[0, 0].get_xlabel() == \"age\"\n    assert disp.axes_[0, 0].get_ylabel() == \"Partial dependence\"\n    assert disp.axes_[0, 1].get_xlabel() == \"age\"\n    assert disp.axes_[0, 1].get_ylabel() == \"bmi\"\n\n    # Check that the deciles lines are correct\n    assert disp.deciles_vlines_[0, 0] is not None\n    assert disp.deciles_hlines_[0, 1] is not None\n\n    # Check that contours are not plotted for 1-way plots\n    assert disp.contours_[0, 0] is None\n    assert disp.contours_[0, 1] is None\n", "def test_partial_dependence_plot_custom_subsample():\n    # Test that the custom subsample is correctly applied and sampled predictions\n    # match the expected number of samples.\n    X, y = make_friedman1()\n    clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n    grid_resolution = 25\n    subsample = 15\n    features = [0, 1]\n\n    disp = PartialDependenceDisplay.from_estimator(\n        clf,\n        X,\n        features,\n        grid_resolution=grid_resolution,\n        subsample=subsample,\n        random_state=42,\n        kind=\"individual\"\n    )\n    \n    # Check that the number of ICE lines is equal to the subsample size\n    for i in range(len(features)):\n        ice_lines = disp.lines_[0, i]\n        assert len(ice_lines) == subsample\n", "def test_plot_partial_dependence_invalid_grid_resolution(pyplot, clf_diabetes, diabetes):\n    # Test that an error is raised when grid_resolution is less than 1.\n    err_msg = \"Expected grid_resolution >= 1, got 0\"\n    with pytest.raises(ValueError, match=err_msg):\n        PartialDependenceDisplay.from_estimator(\n            clf_diabetes,\n            diabetes.data,\n            [0, 2],\n            grid_resolution=0,\n            feature_names=diabetes.feature_names,\n        )\n\n    err_msg = \"Expected grid_resolution >= 1, got -5\"\n    with pytest.raises(ValueError, match=err_msg):\n        PartialDependenceDisplay.from_estimator(\n            clf_diabetes,\n            diabetes.data,\n            [0, 2],\n            grid_resolution=-5,\n            feature_names=diabetes.feature_names,\n        )\n", "def test_plot_partial_dependence_subsample_integer(pyplot, clf_diabetes, diabetes):\n    # Test that the subsample parameter works correctly with integer input\n    grid_resolution = 25\n    feature_names = diabetes.feature_names\n    subsample_size = 10\n\n    disp = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [\"age\", \"bmi\"],\n        kind=\"individual\",\n        grid_resolution=grid_resolution,\n        feature_names=feature_names,\n        subsample=subsample_size,\n        random_state=42,\n    )\n\n    # Ensure that the number of ICE lines plotted matches the subsample size\n    assert disp.lines_.shape == (1, 2, subsample_size)\n    assert len(disp.lines_[0, 0]) == subsample_size\n    assert len(disp.lines_[0, 1]) == subsample_size\n\n    for ax in disp.axes_.ravel():\n        lines = ax.get_lines()\n        assert len(lines) == subsample_size\n", "def test_plot_partial_dependence_is_categorical(pyplot, clf_diabetes, diabetes):\n    # Check that the is_categorical attribute is properly set and used\n\n    # Define categorical feature indices\n    categorical_features = [0, 1]\n\n    # Create the PartialDependenceDisplay with categorical features\n    disp = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        features=[0, 1, (0, 2)],\n        grid_resolution=10,\n        feature_names=diabetes.feature_names,\n        categorical_features=categorical_features,\n    )\n\n    # Check if is_categorical attribute is set correctly\n    assert disp.is_categorical == [(True,), (True,), (True, False)]\n\n    # Ensure no errors are raised during plotting\n    disp.plot()\n    fig = pyplot.gcf()\n    assert disp.figure_ is fig\n    assert disp.bars_.shape == (1, 3)\n    assert disp.contours_.shape == (1, 3)\n\n    # Check that categorical feature plots are bar plots\n    assert disp.bars_[0, 0] is not None\n    assert disp.bars_[0, 1] is not None\n    assert disp.bars_[0, 2] is None  # 0 is categorical, 2 is not\n    assert disp.contours_[0, 2] is not None\n\n    # Check that legends and axis labels are correct\n    assert disp.axes_[0, 0].get_xlabel() == \"age\"\n    assert disp.axes_[0, 1].get_xlabel() == \"sex\"\n    assert disp.axes_[0, 2].get_xlabel() == \"age\"\n    assert disp.axes_[0, 2].get_ylabel() == \"bmi\"\n"], "sample_898": ["def test_auc():\n    # Test AUC with various input formats and values\n    x = np.array([0, 1, 2, 3])\n    y_increasing = np.array([0, 0.5, 0.75, 1.0])\n    y_decreasing = np.array([1.0, 0.75, 0.5, 0.0])\n\n    # Basic AUC calculation with monotonic increasing y\n    result = auc(x, y_increasing)\n    expected = 0.875\n    assert_almost_equal(result, expected, err_msg=\"AUC calculation failed for increasing y\")\n\n    # Basic AUC calculation with monotonic decreasing y\n    result = auc(x, y_decreasing)\n    expected = 0.875\n    assert_almost_equal(result, expected, err_msg=\"AUC calculation failed for decreasing y\")\n\n    # x not monotonic should raise ValueError\n    x_non_monotonic = np.array([0, 2, 1, 3])\n    assert_raises(ValueError, auc, x_non_monotonic, y_increasing)\n\n    # AUC with reorder set to True should warn and still compute correctly\n    x_tied = np.array([0, 2, 2, 3])\n    y_tied = np.array([0, 0.5, 0.75, 1.0])\n    with warnings.catch_warnings(record=True) as w:\n        result = auc(x_tied, y_tied, reorder=True)\n        expected = 0.875\n        assert len(w) > 0 and issubclass(w[-1].category, DeprecationWarning)\n        assert_almost_equal(result, expected, err_msg=\"AUC calculation with reorder failed\")\n\n    # AUC with only 1 point should raise ValueError\n    x_single = np.array([0])\n    y_single = np.array([1])\n    assert_raises(ValueError, auc, x_single, y_single)\n", "def test_auc_with_different_directions():\n    # Test AUC computation with monotonic increasing and decreasing x\n    x_inc = np.array([0, 1, 2, 3])\n    y = np.array([0, 0.5, 0.75, 1])\n    \n    x_dec = np.array([3, 2, 1, 0])\n    \n    # Monotonic increasing x\n    assert_almost_equal(auc(x_inc, y), 1.875, err_msg=\"AUC with increasing x failed\")\n    \n    # Monotonic decreasing x\n    assert_almost_equal(auc(x_dec, y[::-1]), 1.875, err_msg=\"AUC with decreasing x failed\")\n\n    # Test error when x is neither increasing nor decreasing\n    x_random = np.array([0, 2, 1, 3])\n    assert_raises(ValueError, auc, x_random, y)\n", "def test_auc():\n    # Test the `auc` function with various scenarios\n\n    # Test with increasing x and y\n    x = np.array([0, 1, 2])\n    y = np.array([0, 1, 2])\n    assert_almost_equal(auc(x, y), 2.0)\n\n    # Test with decreasing x and y\n    x = np.array([2, 1, 0])\n    y = np.array([2, 1, 0])\n    assert_almost_equal(auc(x, y), -2.0)\n\n    # Test with unordered x and y with reorder=False\n    x = np.array([0, 2, 1])\n    y = np.array([0, 2, 1])\n    assert_raises(ValueError, auc, x, y, reorder=False)\n\n    # Test with unordered x and y with reorder=True\n    x = np.array([0, 2, 1])\n    y = np.array([0, 2, 1])\n    assert_almost_equal(auc(x, y, reorder=True), 1.5)\n\n    # Test with monotonic increasing x and y\n    x = np.array([0, 1, 2, 3])\n    y = np.array([0, 1, 2, 3])\n    assert_almost_equal(auc(x, y), 4.5)\n\n    # Test with less than 2 points\n    x = np.array([0])\n    y = np.array([0])\n    assert_raises(ValueError, auc, x, y)\n", "def test_roc_auc_score_with_partial_auc():\n    # Test the partial AUC computation with max_fpr\n    y_true = np.array([0, 0, 1, 1])\n    y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n\n    # Full AUC\n    full_auc = roc_auc_score(y_true, y_scores)\n    assert_almost_equal(full_auc, 0.75, decimal=2)\n\n    # Partial AUC with max_fpr=0.5\n    partial_auc = roc_auc_score(y_true, y_scores, max_fpr=0.5)\n    assert_almost_equal(partial_auc, 0.5, decimal=2)\n\n    # Partial AUC with max_fpr=0.8\n    partial_auc_0_8 = roc_auc_score(y_true, y_scores, max_fpr=0.8)\n    assert_almost_equal(partial_auc_0_8, 0.625, decimal=2)\n\n    # Check invalid max_fpr values\n    assert_raises(ValueError, roc_auc_score, y_true, y_scores, max_fpr=1.1)\n    assert_raises(ValueError, roc_auc_score, y_true, y_scores, max_fpr=-0.1)\n", "def test_auc():\n    # Test AUC score with some specific cases\n    y_true = np.array([0, 0, 1, 1])\n    y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n    assert_almost_equal(roc_auc_score(y_true, y_scores), 0.75)\n\n    # Test AUC with all positive class\n    y_true_all_pos = np.array([1, 1, 1, 1])\n    y_scores_all_pos = np.array([0.1, 0.4, 0.35, 0.8])\n    assert_raise_message(ValueError, \"Only one class present in y_true.\",\n                         roc_auc_score, y_true_all_pos, y_scores_all_pos)\n\n    # Test AUC with all negative class\n    y_true_all_neg = np.array([0, 0, 0, 0])\n    y_scores_all_neg = np.array([0.1, 0.4, 0.35, 0.8])\n    assert_raise_message(ValueError, \"Only one class present in y_true.\",\n                         roc_auc_score, y_true_all_neg, y_scores_all_neg)\n\n    # Test AUC with random scores and labels\n    random_state = check_random_state(0)\n    y_true_rand = random_state.randint(0, 2, size=100)\n    y_scores_rand = random_state.random_sample(100)\n    roc_auc_rand = roc_auc_score(y_true_rand, y_scores_rand)\n    assert_greater(roc_auc_rand, 0.5)\n\n    # Test AUC with tied scores\n    y_true_tied = np.array([0, 0, 1, 1])\n    y_scores_tied = np.array([0.5, 0.5, 0.5, 0.5])\n    assert_almost_equal(roc_auc_score(y_true_tied, y_scores_tied), 0.5)\n"], "sample_899": ["def test_set_checking_parameters():\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.cluster import KMeans\n    from sklearn.decomposition import PCA\n    from sklearn.linear_model import Ridge\n    \n    # Test for various estimators\n    estimators = [\n        LogisticRegression(),\n        KMeans(),\n        PCA(),\n        Ridge()\n    ]\n    \n    for estimator in estimators:\n        params = estimator.get_params()\n        set_checking_parameters(estimator)\n        new_params = estimator.get_params()\n        \n        for param, value in new_params.items():\n            if param in params and params[param] != value:\n                assert params[param] != value, (\n                    f\"Parameter '{param}' should be changed by set_checking_parameters \"\n                    f\"from {params[param]} to {value} for {type(estimator).__name__}\"\n                )\n            elif param not in params:\n                assert param in [\"max_iter\", \"n_iter\", \"n_init\", \"n_estimators\", \"max_trials\"], (\n                    f\"Unexpected parameter '{param}' set by set_checking_parameters \"\n                    f\"for {type(estimator).__name__}\"\n                )\n", "def test_check_estimators_nan_inf():\n    class EstimatorWithNaNInfHandling(BaseEstimator):\n            X, y = check_X_y(X, y, force_all_finite=True)\n            return self\n\n            X = check_array(X, force_all_finite=True)\n            return np.ones(X.shape[0])\n\n    class EstimatorWithoutNaNInfHandling(BaseEstimator):\n            return self\n\n            return np.ones(X.shape[0])\n\n    # This estimator should pass as it handles NaN and Inf\n    check_estimator(EstimatorWithNaNInfHandling())\n\n    # This estimator should fail as it doesn't handle NaN and Inf\n    msg = \"Estimator doesn't check for NaN and inf in fit.\"\n    assert_raises_regex(AssertionError, msg, check_estimator, EstimatorWithoutNaNInfHandling())\n", "def test_check_estimators_dtypes():\n    # check that check_estimators_dtypes works correctly for various data types\n    from sklearn.linear_model import Ridge\n    \n    class DummyEstimator(BaseEstimator):\n            X = check_array(X)\n            y = check_array(y, ensure_2d=False)\n            self.coef_ = np.ones(X.shape[1])\n            return self\n\n            X = check_array(X)\n            return np.dot(X, self.coef_)\n\n    # DummyEstimator should pass the dtype check\n    check_estimators_dtypes(\"DummyEstimator\", DummyEstimator())\n\n    # Ridge should pass the dtype check\n    check_estimators_dtypes(\"Ridge\", Ridge())\n", "def test_check_estimators_nan_inf():\n    # check that estimators raise an error when input contains NaN or inf values\n    from sklearn.linear_model import LogisticRegression\n\n    class EstimatorWithNanInfHandling(BaseEstimator):\n            if not np.isfinite(X).all():\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\")\n            return self\n\n            if not np.isfinite(X).all():\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\")\n            return np.zeros(X.shape[0])\n\n    # Check custom estimator\n    check_estimator(EstimatorWithNanInfHandling)\n\n    # Check a known estimator\n    check_estimator(LogisticRegression)\n", "def test_check_classifiers_classes():\n    # Check if classifiers can properly handle various class formats\n\n    class SimpleClassifier(BaseEstimator, ClassifierMixin):\n            self.classes_ = np.unique(y)\n            return self\n\n            return np.random.choice(self.classes_, X.shape[0])\n\n    check_classifiers_classes(\"SimpleClassifier\", SimpleClassifier())\n\n    # Check with actual classifiers\n    check_classifiers_classes(\"RandomForestClassifier\", RandomForestClassifier())\n    check_classifiers_classes(\"SGDClassifier\", SGDClassifier())\n    check_classifiers_classes(\"AdaBoostClassifier\", AdaBoostClassifier())\n"], "sample_900": ["def test_invalid_hidden_layer_sizes():\n    # Test that invalid hidden_layer_sizes raise a ValueError.\n    X = [[0.6, 0.8, 0.7], [0.2, 0.4, 0.6]]\n    y = [0, 1]\n    invalid_sizes = [(-1,), (0,), (100, -100), (-100, 100)]\n\n    for sizes in invalid_sizes:\n        clf = MLPClassifier(hidden_layer_sizes=sizes)\n        assert_raises(ValueError, clf.fit, X, y)\n", "def test_activation_functions():\n    # Test that all activation functions can be set and produce outputs\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n\n    for activation in ACTIVATION_TYPES:\n        clf = MLPClassifier(hidden_layer_sizes=5, activation=activation, random_state=1)\n        with ignore_warnings(category=ConvergenceWarning):\n            clf.fit(X, y)\n        y_pred = clf.predict(X)\n        assert y_pred.shape == y.shape\n", "def test_invalid_activation_function():\n    # Test that providing an invalid activation function raises a ValueError.\n    X = X_digits_multi[:50]\n    y = y_digits_multi[:50]\n\n    clf = MLPClassifier(hidden_layer_sizes=5, activation='invalid_activation', random_state=1)\n    with pytest.raises(ValueError, match=\"The activation 'invalid_activation' is not supported.\"):\n        clf.fit(X, y)\n", "def test_activation_functions():\n    # Test that each activation function produces valid outputs.\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n    \n    for activation in ACTIVATION_TYPES:\n        clf = MLPClassifier(hidden_layer_sizes=(5,), activation=activation,\n                            random_state=1, max_iter=100)\n        with ignore_warnings(category=ConvergenceWarning):\n            clf.fit(X, y)\n        output = clf.predict(X)\n        assert output.shape == y.shape\n        assert np.all(output >= 0)  # Since these are class labels\n", "def test_custom_initialization():\n    # Test that custom initialization of weights and biases works correctly.\n    X = X_digits_binary[:100]\n    y = y_digits_binary[:100]\n\n    # Custom initialization\n    init_coefs_0 = np.array([[0.01, 0.02], [0.03, 0.01], [0.05, 0]])\n    init_coefs_1 = np.array([[0.01], [0.02]])\n    init_intercepts_0 = np.array([0.01, 0.01])\n    init_intercepts_1 = np.array([0.1])\n\n    mlp = MLPClassifier(hidden_layer_sizes=(2,), random_state=1, max_iter=1)\n    mlp.coefs_ = [init_coefs_0, init_coefs_1]\n    mlp.intercepts_ = [init_intercepts_0, init_intercepts_1]\n    \n    mlp.fit(X, y)\n\n    # Check if weights and biases have been updated from initial values\n    assert not np.array_equal(mlp.coefs_[0], init_coefs_0)\n    assert not np.array_equal(mlp.coefs_[1], init_coefs_1)\n    assert not np.array_equal(mlp.intercepts_[0], init_intercepts_0)\n    assert not np.array_equal(mlp.intercepts_[1], init_intercepts_1)\n"], "sample_901": ["def test_k_means_pp_init():\n    # Test the k-means++ initialization method\n    n_clusters = 3\n    X, _ = make_blobs(random_state=42)\n    \n    init_centers = _k_init(X, n_clusters=n_clusters, x_squared_norms=row_norms(X, squared=True), random_state=42)\n\n    # Check that the number of initialized centers is correct\n    assert init_centers.shape == (n_clusters, X.shape[1])\n\n    # Check that the initialized centers are distinct\n    assert np.unique(init_centers, axis=0).shape[0] == n_clusters\n\n    # Check that the initialized centers are within the bounds of the data\n    assert np.all(init_centers >= X.min(axis=0)) and np.all(init_centers <= X.max(axis=0))\n", "def test_k_means_invalid_precompute_distances():\n    # Test to check handling of invalid precompute_distances values\n    invalid_values = [123, 'invalid', 1.5, {}, []]\n    for value in invalid_values:\n        km = KMeans(precompute_distances=value, n_clusters=3)\n        with pytest.raises(ValueError, match=\"precompute_distances should be 'auto' or True/False\"):\n            km.fit(X)\n", "def test_k_means_cluster_centers_after_fit():\n    # Test that the cluster centers do not change after calling fit twice\n    km = KMeans(n_clusters=n_clusters, random_state=42)\n    km.fit(X)\n    centers_first_fit = km.cluster_centers_.copy()\n    \n    km.fit(X)\n    centers_second_fit = km.cluster_centers_\n    \n    assert_array_almost_equal(centers_first_fit, centers_second_fit)\n", "def test_kmeans_different_scales(estimator, X):\n    # Test KMeans and MiniBatchKMeans with data in different scales\n    kmeans = estimator(n_clusters=2, random_state=42).fit(X)\n    labels = kmeans.labels_\n    assert len(set(labels)) == 2  # should form two distinct clusters\n    inertia = kmeans.inertia_\n    assert inertia > 0\n    cluster_centers = kmeans.cluster_centers_\n    assert cluster_centers.shape == (2, X.shape[1])\n", "def test_k_means_single_cluster():\n    # Test case to ensure k_means works correctly with a single cluster\n    rng = np.random.RandomState(42)\n    X_single_cluster = rng.normal(size=(50, 2))  # Generate data points\n    \n    # Fit KMeans with a single cluster\n    km = KMeans(n_clusters=1, random_state=42)\n    km.fit(X_single_cluster)\n    \n    # Check that all labels are the same\n    assert len(np.unique(km.labels_)) == 1\n    assert km.n_iter_ > 0\n    \n    # Check that the cluster center is the mean of the data points\n    expected_center = np.mean(X_single_cluster, axis=0)\n    assert_array_almost_equal(km.cluster_centers_, expected_center.reshape(1, -1))\n    assert km.inertia_ > 0.0\n"], "sample_902": ["def test_pipeline_fit_transform_with_fit_params():\n    # Test fit_transform with passing fit parameters to the final estimator\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    y = np.array([1, 0])\n    class CustomTransformer(BaseEstimator, TransformerMixin):\n            self.fit_params_ = fit_params\n            return self\n        \n            return X * 2\n\n    class CustomEstimator(BaseEstimator):\n            self.fit_params_ = fit_params\n            return self\n        \n            self.fit(X, y, **fit_params)\n            return X * 3\n\n    custom_transformer = CustomTransformer()\n    custom_estimator = CustomEstimator()\n    pipe = Pipeline([\n        ('custom_transformer', custom_transformer),\n        ('custom_estimator', custom_estimator)\n    ])\n\n    X_transformed = pipe.fit_transform(X, y, custom_estimator__param=42)\n    assert_array_equal(X_transformed, X * 3)\n    assert_equal(pipe.named_steps['custom_estimator'].fit_params_['param'], 42)\n", "def test_pipeline_memory_with_different_transformer():\n    # Test the pipeline caching with a different transformer to ensure cache is not used incorrectly\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(cachedir=cachedir, verbose=10)\n        # Use DummyTransf initially\n        transf1 = DummyTransf()\n        clf = SVC(probability=True, random_state=0)\n        cached_pipe = Pipeline([('transf', transf1), ('svc', clf)], memory=memory)\n        cached_pipe.fit(X, y)\n        ts1 = cached_pipe.named_steps['transf'].timestamp_\n\n        # Change to a different transformer\n        transf2 = StandardScaler()\n        cached_pipe.set_params(transf=transf2)\n        cached_pipe.fit(X, y)\n        ts2 = cached_pipe.named_steps['transf'].timestamp_\n\n        # Check that the timestamp has changed, indicating the cache was not used\n        assert ts1 != ts2\n\n        # Ensure the pipeline still produces correct predictions\n        assert_array_equal(clf.predict(X), cached_pipe.predict(X))\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_with_none_steps():\n    # Test the pipeline with None steps in between\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    \n    # Create pipeline with None steps\n    pipeline = Pipeline([('m1', Mult(2)), ('none', None), ('m3', Mult(3))])\n\n    # Ensure pipeline works correctly\n    X_transformed = pipeline.fit_transform(X, y)\n    expected_output = np.array([[1*2*3, 2*2*3], [3*2*3, 4*2*3], [5*2*3, 6*2*3]])\n    assert_array_equal(X_transformed, expected_output)\n\n    # Ensure prediction works correctly\n    y_pred = pipeline.fit(X, y).predict(X)\n    expected_pred = np.array([12, 36, 60])\n    assert_array_equal(y_pred, expected_pred)\n\n    # Ensure inverse_transform works correctly\n    X_inv_transformed = pipeline.inverse_transform(X_transformed)\n    expected_inv_output = X\n    assert_array_equal(X_inv_transformed, expected_inv_output)\n", "def test_pipeline_memory_cache_eviction():\n    # Test that the cache eviction works when the pipeline is updated\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(cachedir=cachedir, verbose=10)\n        # Initial pipeline with Transformer + SVC\n        clf = SVC(probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', transf), ('svc', clf)], memory=memory)\n        pipe.fit(X, y)\n        \n        # Store the timestamp of the transformer\n        ts_initial = pipe.named_steps['transf'].timestamp_\n\n        # Modify the pipeline by changing a parameter in the transformer\n        pipe.set_params(transf__a=1)\n        pipe.fit(X, y)\n\n        # Store the new timestamp of the transformer\n        ts_modified = pipe.named_steps['transf'].timestamp_\n\n        # Check that the cache was evicted and the transformer was refitted\n        assert ts_initial != ts_modified\n        \n        # Check that the results are still identical after modifying the parameter\n        clf2 = SVC(probability=True, random_state=0)\n        transf2 = DummyTransf()\n        pipe2 = Pipeline([('transf', transf2), ('svc', clf2)])\n        pipe2.set_params(transf__a=1)\n        pipe2.fit(X, y)\n        \n        assert_array_equal(pipe.predict(X), pipe2.predict(X))\n        assert_array_equal(pipe.predict_proba(X), pipe2.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X), pipe2.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), pipe2.score(X, y))\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_with_none_step():\n    # Test pipeline with None as a step in the middle\n    X = np.array([[1, 2]])\n    y = np.array([1])\n    mult2 = Mult(mult=2)\n    pipeline = Pipeline([('mult2', mult2), ('none_step', None), ('mult2_again', mult2)])\n\n    # Test fit_transform\n    exp = 2 * 2\n    assert_array_equal([[exp]], pipeline.fit_transform(X, y))\n    assert_array_equal([exp], pipeline.fit(X).predict(X))\n    assert_array_equal(X, pipeline.inverse_transform([[exp]]))\n\n    # Test predict_proba\n    assert_raises(AttributeError, getattr, pipeline, 'predict_proba')\n\n    # Test score\n    assert_equal(pipeline.score(X), 4)\n\n    # Test with None at the beginning\n    pipeline = Pipeline([('none_step', None), ('mult2', mult2)])\n    exp = 2\n    assert_array_equal([[exp]], pipeline.fit_transform(X, y))\n    assert_array_equal([exp], pipeline.fit(X).predict(X))\n    assert_array_equal(X, pipeline.inverse_transform([[exp]]))\n\n    # Test with None at the end\n    pipeline = Pipeline([('mult2', mult2), ('none_step', None)])\n    exp = 2\n    assert_array_equal([[exp]], pipeline.fit_transform(X, y))\n    assert_array_equal([exp], pipeline.fit(X).predict(X))\n    assert_array_equal(X, pipeline.inverse_transform([[exp]]))\n"], "sample_903": ["def test_joint_probabilities():\n    # Test if joint probabilities are correctly calculated.\n    random_state = check_random_state(0)\n    distances = random_state.randn(10, 2).astype(np.float32)\n    distances = np.abs(distances.dot(distances.T))\n    np.fill_diagonal(distances, 0.0)\n    desired_perplexity = 30.0\n\n    P = _joint_probabilities(distances, desired_perplexity, verbose=0)\n    assert np.all(np.isfinite(P)), \"All probabilities should be finite\"\n    assert np.all(P >= 0), \"All probabilities should be non-negative\"\n    assert np.all(P <= 1), \"All probabilities should be <= 1\"\n    assert_almost_equal(P.sum(), 1.0, decimal=5)\n\n    # Test nearest neighbors joint probabilities\n    n_samples, k = 10, 5\n    distances_nn = distances[:, :k]\n    neighbors = np.tile(np.arange(k), (n_samples, 1))\n    P_nn = _joint_probabilities_nn(distances_nn, neighbors, desired_perplexity, verbose=0)\n    assert np.all(np.isfinite(P_nn.data)), \"All probabilities should be finite in sparse matrix\"\n    assert np.all(P_nn.data >= 0), \"All probabilities should be non-negative in sparse matrix\"\n    assert np.all(P_nn.data <= 1), \"All probabilities should be <= 1 in sparse matrix\"\n    assert_almost_equal(P_nn.sum(), 1.0, decimal=5)\n", "def test_trustworthiness_with_different_perplexity():\n    # Test trustworthiness for different perplexity values\n    random_state = check_random_state(0)\n    X = random_state.randn(100, 2)\n\n    for perplexity in [5, 30, 50]:\n        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=0)\n        X_embedded = tsne.fit_transform(X)\n        t = trustworthiness(X, X_embedded, n_neighbors=5)\n        assert_greater(t, 0.8, msg='Trustworthiness={:0.3f} < 0.8 for perplexity={}'.format(t, perplexity))\n", "def test_trustworthiness_with_sparse_matrix():\n    # Test trustworthiness with sparse input matrix\n    random_state = check_random_state(0)\n    X, _ = make_blobs(n_samples=100, n_features=2, random_state=random_state)\n    X[X < 0.5] = 0  # Sparsify the matrix\n    X_sparse = sp.csr_matrix(X)\n    \n    tsne = TSNE(n_components=2, perplexity=30, learning_rate=100.0,\n                random_state=0, method='exact')\n    X_embedded = tsne.fit_transform(X_sparse)\n    t = trustworthiness(X_sparse.toarray(), X_embedded, n_neighbors=5)\n    assert_greater(t, 0.8, msg=\"Trustworthiness={:0.3f} < 0.8 with sparse input\".format(t))\n", "def test_metric_callable():\n    # Test that a custom metric callable can be used with TSNE\n        return np.sum(np.abs(x - y))\n\n    tsne = TSNE(metric=custom_metric, method='exact')\n    X = np.array([[0.0, 0.0], [1.0, 1.0], [0.0, 1.0], [1.0, 0.0]])\n    X_embedded = tsne.fit_transform(X)\n    assert_equal(X_embedded.shape, (4, 2))\n\n    # Check that the callable is used correctly\n    D = pairwise_distances(X, metric=custom_metric)\n    tsne_precomputed = TSNE(metric=\"precomputed\", method='exact')\n    X_embedded_precomputed = tsne_precomputed.fit_transform(D)\n    assert_array_almost_equal(X_embedded, X_embedded_precomputed, decimal=5)\n", "def test_trustworthiness_edge_cases():\n    # Test trustworthiness with edge cases.\n\n    # Case 1: single sample\n    X = np.array([[0.0, 0.0]])\n    X_embedded = np.array([[0.0, 0.0]])\n    assert_equal(trustworthiness(X, X_embedded, n_neighbors=1), 1.0)\n\n    # Case 2: two samples, identical\n    X = np.array([[0.0, 0.0], [0.0, 0.0]])\n    X_embedded = np.array([[0.0, 0.0], [0.0, 0.0]])\n    assert_equal(trustworthiness(X, X_embedded, n_neighbors=1), 1.0)\n\n    # Case 3: two samples, different\n    X = np.array([[0.0, 0.0], [1.0, 1.0]])\n    X_embedded = np.array([[0.0, 0.0], [1.0, 1.0]])\n    assert_equal(trustworthiness(X, X_embedded, n_neighbors=1), 1.0)\n\n    # Case 4: three samples, one sample outlier\n    X = np.array([[0.0, 0.0], [1.0, 1.0], [10.0, 10.0]])\n    X_embedded = np.array([[0.0, 0.0], [1.0, 1.0], [10.0, 10.0]])\n    assert_equal(trustworthiness(X, X_embedded, n_neighbors=2), 1.0)\n\n    # Case 5: non-euclidean metric\n    X = np.array([[0.0, 0.0], [1.0, 1.0], [0.0, 1.0]])\n    X_embedded = np.array([[0.0, 0.0], [1.0, 1.0], [0.0, 1.0]])\n    assert_almost_equal(trustworthiness(X, X_embedded, metric='manhattan'), 1.0)\n"], "sample_904": ["def test_envvar():\n    text = (\".. envvar:: ENV_VAR\\n\"\n            \"   :noindex:\\n\"\n            \"   Description of the environment variable.\\n\")\n    domain = app.env.get_domain('std')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"ENV_VAR\"],\n                                                    [desc_addname, ()])],\n                                  [desc_content, nodes.paragraph, \"Description of the environment variable.\"])]))\n    assert_node(doctree[0], addnodes.index,\n                entries=[])\n\n    objects = list(domain.get_objects())\n    assert ('ENV_VAR', 'ENV_VAR', 'envvar', 'index', 'envvar-ENV_VAR', 0) in objects\n\n    # Cross-referencing the environment variable\n    refnode = domain.resolve_xref(app.env, 'index', app.builder, 'envvar', 'ENV_VAR',\n                                  pending_xref(), nodes.paragraph())\n    assert_node(refnode, nodes.reference, refid=\"envvar-ENV_VAR\")\n", "def test_token_xrefs():\n    text = \"`token1` `~token2` `prefix:token3` `~prefix:token4`\"\n    result_nodes = token_xrefs(text)\n    \n    assert len(result_nodes) == 8\n    \n    assert isinstance(result_nodes[0], pending_xref)\n    assert result_nodes[0]['reftarget'] == 'token1'\n    assert_node(result_nodes[0], [pending_xref, nodes.literal, \"token1\"])\n    \n    assert isinstance(result_nodes[2], pending_xref)\n    assert result_nodes[2]['reftarget'] == 'token2'\n    assert_node(result_nodes[2], [pending_xref, nodes.literal, \"token2\"])\n    \n    assert isinstance(result_nodes[4], pending_xref)\n    assert result_nodes[4]['reftarget'] == 'prefix:token3'\n    assert_node(result_nodes[4], [pending_xref, nodes.literal, \"prefix:token3\"])\n    \n    assert isinstance(result_nodes[6], pending_xref)\n    assert result_nodes[6]['reftarget'] == 'prefix:token4'\n    assert_node(result_nodes[6], [pending_xref, nodes.literal, \"token4\"])\n", "def test_generic_object_handle_signature():\n    env = mock.Mock(domaindata={})\n    env.app.registry.enumerable_nodes = {}\n    domain = StandardDomain(env)\n\n    # Create a GenericObject instance\n    obj = GenericObject()\n    obj.env = env\n    obj.state = mock.Mock(document=mock.Mock(ids={}, nameids={}))\n    obj.objtype = 'generic'\n\n    # Test without parse_node\n    signode = desc_signature()\n    name = obj.handle_signature('test_signature', signode)\n    assert name == 'test_signature'\n    assert_node(signode, [addnodes.desc_name, 'test_signature'])\n\n    # Test with parse_node\n        signode += addnodes.desc_name('parsed_' + sig, 'parsed_' + sig)\n        return 'parsed_' + sig\n\n    obj.parse_node = mock_parse_node\n    signode = desc_signature()\n    name = obj.handle_signature('test_signature', signode)\n    assert name == 'parsed_test_signature'\n    assert_node(signode, [addnodes.desc_name, 'parsed_test_signature'])\n", "def test_envvar():\n    text = (\".. envvar:: VAR1\\n\"\n            \"\\n\"\n            \".. envvar:: VAR2\\n\")\n    domain = app.env.get_domain('std')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, ([desc, ([desc_signature, \"VAR1\"], [desc_content, ()])],\n                          [desc, ([desc_signature, \"VAR2\"], [desc_content, ()])]))\n    assert ('envvar', 'VAR1') in domain.objects\n    assert ('envvar', 'VAR2') in domain.objects\n    assert domain.objects[('envvar', 'VAR1')] == ('index', 'envvar-VAR1')\n    assert domain.objects[('envvar', 'VAR2')] == ('index', 'envvar-VAR2')\n", "def test_env_var_role():\n    env = mock.Mock(domaindata={})\n    env.app.registry.enumerable_nodes = {}\n    domain = StandardDomain(env)\n\n    # testing handle_signature for EnvVar\n    directive = EnvVar('envvar', [], None, None, None, None, None, None, None)\n    signode = desc_signature('', '')\n    result = directive.handle_signature(\"ENV_VAR\", signode)\n    assert result == \"ENV_VAR\"\n    assert_node(signode, [addnodes.desc_name, \"ENV_VAR\"])\n\n    # testing add_target_and_index for EnvVar\n    directive.env = env\n    directive.state = mock.Mock(document=mock.Mock(ids={}, note_explicit_target=mock.Mock()))\n    directive.objtype = \"envvar\"\n    directive.state.document.note_explicit_target = mock.Mock()\n    signode = desc_signature('', '', ids=['test_id'])\n    directive.add_target_and_index(\"ENV_VAR\", \"ENV_VAR\", signode)\n    assert 'envvar-ENV_VAR' in signode['ids']\n    assert directive.state.document.note_explicit_target.called\n\n    # testing result_nodes for EnvVarXRefRole\n    role = EnvVarXRefRole()\n    document = nodes.document('', '')\n    env = mock.Mock(new_serialno=lambda x: 1)\n    env.get_domain = lambda x: domain\n    node = pending_xref('', refdomain='std', reftarget='ENV_VAR')\n    result, messages = role.result_nodes(document, env, node, True)\n    assert len(result) == 3\n    assert isinstance(result[0], addnodes.index)\n    assert_node(result[0], addnodes.index, entries=[\n        ('single', 'ENV_VAR', 'index-1', '', None),\n        ('single', 'environment variable; ENV_VAR', 'index-1', '', None)\n    ])\n    assert_node(result[1], nodes.target, ids=['index-1'])\n    assert_node(result[2], pending_xref, reftarget='ENV_VAR')\n"], "sample_905": ["def test_getall():\n    # Test with a module having __all__ attribute\n    module = types.ModuleType('test_module')\n    module.__all__ = ['foo', 'bar', 'baz']\n    assert inspect.getall(module) == ['foo', 'bar', 'baz']\n\n    # Test with a module not having __all__ attribute\n    module = types.ModuleType('test_module_no_all')\n    assert inspect.getall(module) is None\n\n    # Test with an invalid __all__ attribute\n    module.__all__ = 'invalid_all'\n    with pytest.raises(ValueError):\n        inspect.getall(module)\n\n", "def test_ispartial():\n        pass\n\n    partial_func = functools.partial(func, 1)\n    assert inspect.ispartial(partial_func) is True\n\n    class MyClass:\n            pass\n\n    partial_method = functools.partialmethod(MyClass.method, 1)\n    assert inspect.ispartial(partial_method) is True\n\n    assert inspect.ispartial(func) is False\n    assert inspect.ispartial(MyClass.method) is False\n", "def test_getannotations():\n    class Foo:\n        x: int\n        y: str\n\n    annotations = inspect.getannotations(Foo)\n    assert annotations == {'x': int, 'y': str}\n\n    class Bar:\n        pass\n\n    annotations = inspect.getannotations(Bar)\n    assert annotations == {}\n\n    # Test with invalid __annotations__\n    class Baz:\n        __annotations__ = \"invalid\"\n\n    annotations = inspect.getannotations(Baz)\n    assert annotations == {}\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(foo) is True\n\n        pass\n\n    assert inspect.is_singledispatch_function(bar) is False\n\n", "def test_getall():\n    class ModuleWithAll:\n        __all__ = ['a', 'b', 'c']\n\n    class ModuleWithoutAll:\n        pass\n\n    class ModuleWithInvalidAll:\n        __all__ = ['a', 'b', 123]\n\n    assert inspect.getall(ModuleWithAll) == ['a', 'b', 'c']\n    assert inspect.getall(ModuleWithoutAll) is None\n\n    with pytest.raises(ValueError):\n        inspect.getall(ModuleWithInvalidAll)\n"], "sample_906": ["def test_c_ast_basic_literals():\n    # Test basic literals (boolean, integer, character, string)\n    class Config:\n        c_id_attributes = []\n        c_paren_attributes = []\n\n        parser = DefinitionParser(literal, location=None, config=Config())\n        return parser._parse_literal()\n\n    bool_true = parse_literal('true')\n    bool_false = parse_literal('false')\n    int_literal = parse_literal('42')\n    char_literal = parse_literal(r\"'a'\")\n    string_literal = parse_literal(r'\"Hello, World!\"')\n\n    assert str(bool_true) == 'true'\n    assert str(bool_false) == 'false'\n    assert str(int_literal) == '42'\n    assert str(char_literal) == \"'a'\"\n    assert str(string_literal) == '\"Hello, World!\"'\n\n    # Check the display strings\n    assert bool_true.get_display_string() == 'true'\n    assert bool_false.get_display_string() == 'false'\n    assert int_literal.get_display_string() == '42'\n    assert char_literal.get_display_string() == \"'a'\"\n    assert string_literal.get_display_string() == '\"Hello, World!\"'\n", "def test_c_domain_basic_functionality():\n    from sphinx.domains.c import CDomain, CFunctionObject, CMemberObject\n\n    # Test CDomain instantiation\n    domain = CDomain(env=None)\n    assert domain.name == 'c'\n    assert domain.label == 'C'\n\n    # Test CFunctionObject instantiation\n    function_obj = CFunctionObject()\n    assert function_obj.object_type == 'function'\n\n    # Test CMemberObject instantiation\n    member_obj = CMemberObject()\n    assert member_obj.object_type == 'member'\n", "def test_c_ast_identifier():\n    id1 = ASTIdentifier(\"example\")\n    id2 = ASTIdentifier(\"example\")\n    id3 = ASTIdentifier(\"different\")\n\n    # Test __eq__\n    assert id1 == id2\n    assert id1 != id3\n\n    # Test is_anon\n    anon_id = ASTIdentifier(\"@anon\")\n    non_anon_id = ASTIdentifier(\"nonAnon\")\n    assert anon_id.is_anon() is True\n    assert non_anon_id.is_anon() is False\n\n    # Test __str__\n    assert str(id1) == \"example\"\n    assert str(anon_id) == \"@anon\"\n\n    # Test get_display_string\n    assert id1.get_display_string() == \"example\"\n    assert anon_id.get_display_string() == \"[anonymous]\"\n\n    # Test describe_signature with different modes\n    signode = addnodes.desc_signature()\n    env = None\n    symbol = None\n\n    id1.describe_signature(signode, 'markType', env, '', symbol)\n    assert_node(signode, addnodes.desc_sig_name, ids=[\"example\"])\n\n    id1.describe_signature(signode, 'lastIsName', env, '', symbol)\n    assert_node(signode, addnodes.desc_name, ids=[\"example\"])\n\n    id1.describe_signature(signode, 'noneIsName', env, '', symbol)\n    assert_node(signode, addnodes.desc_sig_name, ids=[\"example\"])\n\n    with pytest.raises(Exception, match=\"Unknown description mode: invalidMode\"):\n        id1.describe_signature(signode, 'invalidMode', env, '', symbol)\n", "def test_domain_c_ast_basic_parsing():\n        class Config:\n            c_id_attributes = [\"id_attr\"]\n            c_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(input_str, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_declaration(objectType, objectType)\n        parser.assert_end()\n        return str(ast)\n    \n    # Test basic parsing of C declarations\n    assert parse_declaration(\"function\", \"void func(int a)\") == \"void func(int a)\"\n    assert parse_declaration(\"member\", \"int memberVar\") == \"int memberVar\"\n    assert parse_declaration(\"macro\", \"#define MAX 100\") == \"MAX\"\n    assert parse_declaration(\"struct\", \"struct MyStruct\") == \"MyStruct\"\n    assert parse_declaration(\"union\", \"union MyUnion\") == \"MyUnion\"\n    assert parse_declaration(\"enum\", \"enum MyEnum\") == \"MyEnum\"\n    assert parse_declaration(\"enumerator\", \"ENUM_VAL = 10\") == \"ENUM_VAL = 10\"\n    assert parse_declaration(\"type\", \"typedef int MyInt\") == \"typedef int MyInt\"\n", "def test_c_expression_parsing():\n        class Config:\n            c_id_attributes = []\n            c_paren_attributes = []\n\n        parser = DefinitionParser(expr, location=None, config=Config())\n        ast = parser.parse_expression()\n        ast_str = str(ast)\n        display_str = ast.get_display_string()\n\n        assert ast_str == expected_ast_str, f\"Expected AST str: {expected_ast_str}, got: {ast_str}\"\n        assert display_str == expected_display_str, f\"Expected display str: {expected_display_str}, got: {display_str}\"\n\n    check_expr_parsing('42', '42', '42')\n    check_expr_parsing('3.14', '3.14', '3.14')\n    check_expr_parsing('\"hello\"', '\"hello\"', '\"hello\"')\n    check_expr_parsing('\\'a\\'', \"'a'\", \"'a'\")\n    check_expr_parsing('x + y', 'x + y', 'x + y')\n    check_expr_parsing('a * (b + c)', 'a * (b + c)', 'a * (b + c)')\n    check_expr_parsing('!flag', '!flag', '!flag')\n    check_expr_parsing('sizeof(int)', 'sizeof(int)', 'sizeof(int)')\n    check_expr_parsing('alignof(double)', 'alignof(double)', 'alignof(double)')\n"], "sample_907": ["def test_domain_cpp_new_expressions():\n    check('function', 'void* p = new int', {2: '1p'}, output='void *p = new int')\n    check('function', 'void* p = new (std::nothrow) int', {2: '1p'}, \n          output='void *p = new (std::nothrow) int')\n    check('function', 'void* p = new int[42]', {2: '1p'}, output='void *p = new int[42]')\n    check('function', 'void* p = new int()', {2: '1p'}, output='void *p = new int()')\n    check('function', 'void* p = new int{42}', {2: '1p'}, output='void *p = new int{42}')\n    check('function', 'void* p = new int[42]{1, 2, 3}', {2: '1p'}, output='void *p = new int[42]{1, 2, 3}')\n    check('function', 'void* p = ::new int', {2: '1p'}, output='void *p = ::new int')\n    check('function', 'void* p = ::new int[42]', {2: '1p'}, output='void *p = ::new int[42]')\n    check('function', 'void* p = ::new int()', {2: '1p'}, output='void *p = ::new int()')\n    check('function', 'void* p = ::new int{42}', {2: '1p'}, output='void *p = ::new int{42}')\n", "def test_domain_cpp_ast_trailing_requires_clauses():\n    check('function', 'template<typename T> void f() requires A::B::C', {4: 'I0EIQ1AIN1B1CE1fv'})\n    check('function', 'template<typename T> void f() requires (A && B) || C', {4: 'I0EIQooaa1A1B1CE1fv'})\n    check('function', 'template<typename T> void f() requires A, typename = std::enable_if_t<!B>', {4: 'I0EIQaa1A1_NSt9enable_ifIXne1BEvEE1fv'})\n    check('function', 'template<typename T> void f() requires A, requires B', {4: 'I0EIQaa1A1BE1fv'})\n", "def test_domain_cpp_ast_template_instantiations():\n    check('type', 'template<> {key}int A::x<int>', {2: 'IEN1AIiE1xIiE'}, key='using')\n    check('member', 'template<> int A::x<int>', {2: 'IEN1AIiE1xIiE'})\n    check('function', 'template<> void A::f<int>(int)', {2: 'IEN1AIiE1fIiEEi'})\n    check('class', 'template<> class A::B<int>', {2: 'IEN1AIiE1BIiEE'})\n    check('concept', 'template<> {key}A::C<int>', {2: 'IEN1AIiE1C'})\n    check('enum', 'template<> enum A::E<int>', {2: 'IEN1AIiE1E'})\n    check('enumerator', 'template<> {key}A::E<int>::e = 42', {2: 'IEN1AIiE1E1e'})\n", "def test_domain_cpp_ast_decltype_auto():\n    check('function', 'decltype(auto) f()', {1: 'f', 2: '1fv'})\n    check('function', 'decltype(auto) f(int x)', {1: 'f__i', 2: '1fi'})\n    check('function', 'decltype(auto) f(int x, int y)', {1: 'f__i.i', 2: '1fii'})\n    check('member', 'decltype(auto) var', {1: 'var', 2: '3var'})\n    check('type', '{key}decltype(auto) var = some_expr', {2: '3var'}, key='using')\n", "def test_domain_cpp_ast_pointer_to_member_with_array():\n    check('function', 'void f(int C::*[5])', {2: '1fA5_M1Ci'})\n    check('function', 'void f(int C::*[5][10])', {2: '1fA10_A5_M1Ci'})\n    check('function', 'void f(int C::*const[5])', {2: '1fA5_KM1Ci'})\n    check('function', 'void f(int C::*volatile[5])', {2: '1fA5_VM1Ci'})\n    check('function', 'void f(int C::*const volatile[5])', {2: '1fA5_VKM1Ci'},\n          output='void f(int C::*volatile const[5])')\n    check('function', 'void f(int C::*volatile const[5][10])', {2: '1fA10_A5_VKM1Ci'})\n"], "sample_908": ["def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a, b=2, *args, c=3, d, **kwargs): pass\"\n    expected = \"a, b=2, *args, c=3, d, **kwargs\"\n    module = ast.parse(source)\n    func_def = module.body[0]\n    assert ast.unparse_arguments(func_def.args) == expected\n", "def test_unparse_arguments():\n    source = \"(a, b=2, *args, c=3, **kwargs)\"\n    module = ast.parse(\"def f\" + source + \": pass\")\n    expected = \"a, b=2, *args, c=3, **kwargs\"\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a, b=2, *args, c, d=4, **kwargs): pass\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == \"a, b=2, *args, c, d=4, **kwargs\"\n", "def test_unparse_arguments():\n    source = \"def func(a, b, c=3, *args, d, e=5, **kwargs): pass\"\n    expected = \"a, b, c=3, *args, d, e=5, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n"], "sample_909": ["    def test_directive_sections(self):\n        docstring = \"\"\"\\", "    def test_google_returns_section_multiple_fields(self):\n        docstring = \"\"\"\\", "    def test_custom_section_parsing(self):\n        config = Config(napoleon_custom_sections=['CustomSection', ('AnotherSection', 'custom')])\n        docstring = \"\"\"\n        CustomSection:\n            This is a custom section.\n\n        AnotherSection:\n            This is another custom section.\n        \"\"\"\n        expected = \"\"\"\\", "    def test_custom_section_with_admonition(self):\n        docstring = \"\"\"\\", "    def test_custom_sections(self):\n        docstring = \"\"\"\\"], "sample_910": ["def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        logger.warning('message2')\n        assert 'prefix: WARNING: message1' in strip_escseq(warning.getvalue())\n        assert 'prefix: WARNING: message2' in strip_escseq(warning.getvalue())\n\n    logger.warning('message3')\n    assert 'WARNING: message3' in strip_escseq(warning.getvalue())\n    assert 'prefix: WARNING: message3' not in strip_escseq(warning.getvalue())\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings(\"PREFIX:\"):\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'PREFIX: WARNING: message1' in strip_escseq(warning.getvalue())\n    assert 'PREFIX: WARNING: message2' in strip_escseq(warning.getvalue())\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings('PREFIX'):\n        logger.warning('message1')\n        logger.info('message2')\n        logger.error('message3')\n\n    assert 'PREFIX WARNING: message1' in strip_escseq(warning.getvalue())\n    assert 'message2' in status.getvalue()  # info logs are not prefixed\n    assert 'PREFIX WARNING: message3' in strip_escseq(warning.getvalue())\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'prefix: WARNING: message1' in strip_escseq(warning.getvalue())\n    assert 'prefix: WARNING: message2' in strip_escseq(warning.getvalue())\n\n    # ensure prefix is removed after context\n    logger.warning('message3')\n    assert 'WARNING: message3' in strip_escseq(warning.getvalue())\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'prefix: WARNING: message1' in strip_escseq(warning.getvalue())\n    assert 'prefix: WARNING: message2' in strip_escseq(warning.getvalue())\n\n    # ensure prefix is not applied outside the context\n    logger.warning('message3')\n    assert 'prefix: WARNING: message3' not in strip_escseq(warning.getvalue())\n"], "sample_911": ["def test_invalid_definitions():\n    with pytest.raises(DefinitionError):\n        parse('class', 'class *A')  # Invalid class definition with pointer\n    with pytest.raises(DefinitionError):\n        parse('function', 'void 42f()')  # Function name starting with a number\n    with pytest.raises(DefinitionError):\n        parse('member', 'const int 123abc')  # Member name starting with a number\n    with pytest.raises(DefinitionError):\n        parse('type', 'template<int T> T()')  # Invalid type template with parentheses\n    with pytest.raises(DefinitionError):\n        parse('enum', 'enum class A : bool')  # Invalid enum underlying type\n    with pytest.raises(DefinitionError):\n        parse('concept', 'template<int T> concept Foo = T')  # Invalid concept definition with non-type template\n", "def test_parse_expression_fallback():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(expr, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = True\n        try:\n            ast = parser.parse_expression()\n            result = str(ast)\n        except DefinitionError as e:\n            result = str(e)\n        assert result == expected_result, f\"Expected {expected_result} but got {result}\"\n        assert isinstance(ast, ASTFallbackExpr)\n        assert ast.get_id(version=2) == expected_value\n\n    fallback_check('(1 + 2) * (3 / 4)', '((1 + 2) * (3 / 4))', '((1 + 2) * (3 / 4))')\n    fallback_check('a + b * c', '(a + (b * c))', '(a + (b * c))')\n    fallback_check('A<B, C>', 'A<B, C>', 'A<B, C>')\n", "def test_template_specializations():\n    # Explicit template specialization\n    check('function', 'template<> void f<int>(int)', {2: 'IE1fIiEi'})\n    check('class', 'template<> class A<int>', {2: 'IE1AIiE'})\n    check('member', 'template<> int A<int>::a', {2: 'IEN1AIiE1aE'})\n    check('type', 'template<> using T = A<int>', {2: 'IE1T'})\n\n    # Partial template specialization\n    check('class', 'template<typename T> class A<T*>', {2: 'I0EI1AI1TE'})\n    check('class', 'template<typename T, typename U> class B<T, U*>', {2: 'I00EI1BI1TI1UE'})\n    check('function', 'template<typename T> void f(T*)', {2: 'I0E1fPT1TE'})\n    check('type', 'template<typename T> using U = A<T*>', {2: 'I0E1U'})\n", "def test_alias_object():\n    check('type', 'using MyAlias = int', {2: '7MyAlias'})\n    check('type', 'using MyPtr = int*', {2: '6MyPtr'})\n    check('type', 'using MyConstPtr = const int*', {2: '10MyConstPtr'})\n    check('type', 'using MyRef = int&', {2: '5MyRef'})\n    check('type', 'using MyArray = int[10]', {2: '8MyArray'})\n    check('type', 'using MyFuncPtr = void (*)(int)', {2: '10MyFuncPtr'})\n    check('type', 'using MyTemplateAlias = std::vector<int>', {2: '16MyTemplateAlias'})\n    check('type', 'using namespace std', {2: '5std'})\n\n    # Aliases with templates\n    check('type', 'template<typename T> using MyTemplate = std::vector<T>', {2: 'I0E9MyTemplate'})\n    check('type', 'template<typename T> using MyPtrTemplate = T*', {2: 'I0E13MyPtrTemplate'})\n    check('type', 'template<typename T> using MyRefTemplate = T&', {2: 'I0E13MyRefTemplate'})\n    check('type', 'template<typename T> using MyArrayTemplate = T[10]', {2: 'I0E16MyArrayTemplate'})\n    check('type', 'template<typename T> using MyFuncPtrTemplate = void (*)(T)', {2: 'I0E18MyFuncPtrTemplate'})\n\n    # Aliases with nested templates\n    check('type', 'template<typename T, typename U> using MyNestedTemplate = std::pair<T, U>', {2: 'I00E17MyNestedTemplate'})\n", "def test_duplicated_symbols():\n    rootSymbol = Symbol(None, None, None, None, None, None)\n\n    # First symbol\n    decl1 = parse(\"class\", \"A\")\n    symbol1 = rootSymbol.add_declaration(decl1, docname=\"TestDoc1\")\n\n    # Duplicate symbol, different docname\n    decl2 = parse(\"class\", \"A\")\n    with pytest.raises(_DuplicateSymbolError):\n        symbol2 = rootSymbol.add_declaration(decl2, docname=\"TestDoc2\")\n\n    # Check that the original symbol remains unchanged\n    assert symbol1.declaration.get_newest_id() == decl1.get_newest_id()\n    assert symbol1.docname == \"TestDoc1\"\n\n    # Duplicate symbol, same docname\n    with pytest.raises(_DuplicateSymbolError):\n        symbol3 = rootSymbol.add_declaration(decl2, docname=\"TestDoc1\")\n\n    # Adding different symbol\n    decl3 = parse(\"class\", \"B\")\n    symbol4 = rootSymbol.add_declaration(decl3, docname=\"TestDoc1\")\n    assert symbol4.declaration.get_newest_id() == decl3.get_newest_id()\n    assert symbol4.docname == \"TestDoc1\"\n"], "sample_912": ["def test_pyclass_signature(app):\n    text = (\".. py:class:: MyClass\\n\"\n            \"   :module: mymodule\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_addname, \"mymodule.\"],\n                                                    [desc_name, \"MyClass\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"class\",\n                domain=\"py\", objtype=\"class\", noindex=False)\n\n    assert 'mymodule.MyClass' in domain.objects\n    assert domain.objects['mymodule.MyClass'] == ('index', 'mymodule.MyClass', 'class')\n", "def test_pyxrefrole_process_link():\n    env = Mock()\n    env.ref_context = {'py:module': 'test_module', 'py:class': 'TestClass'}\n    role = PyXRefRole()\n    \n    # Test with explicit title\n    title, target = role.process_link(env, Mock(), True, 'test_title', 'test_target')\n    assert title == 'test_title'\n    assert target == 'test_target'\n    \n    # Test without explicit title\n    title, target = role.process_link(env, Mock(), False, '~test_target', '.test_target')\n    assert title == 'test_target'\n    assert target == 'test_target'\n    \n    # Test with non-explicit title and nested class context\n    title, target = role.process_link(env, Mock(), False, '~test_module.TestClass.subclass', '.test_module.TestClass.subclass')\n    assert title == 'subclass'\n    assert target == 'test_module.TestClass.subclass'\n", "def test_pyfunction_with_module_option(app):\n    text = (\".. py:module:: testmodule\\n\"\n            \".. py:function:: func1\\n\"\n            \".. py:function:: func2\\n\"\n            \"   :module: testmodule.submodule\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    \n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"testmodule.\"],\n                                                    [desc_name, \"func1\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])],\n                          nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"testmodule.submodule.\"],\n                                                    [desc_name, \"func2\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    \n    assert 'testmodule.func1' in domain.objects\n    assert domain.objects['testmodule.func1'] == ('index', 'testmodule.func1', 'function')\n    assert 'testmodule.submodule.func2' in domain.objects\n    assert domain.objects['testmodule.submodule.func2'] == ('index', 'testmodule.submodule.func2', 'function')\n", "def test_pyfield_make_xref():\n    field = PyField(None)\n    rolename = 'class'\n    domain = 'py'\n    target = 'None'\n    innernode = nodes.emphasis\n    contnode = None\n    env = Mock(BuildEnvironment)\n\n    node = field.make_xref(rolename, domain, target, innernode, contnode, env)\n    assert_node(node, [pending_xref, pending_xref, 'None'])\n    assert node[0]['reftype'] == 'obj'\n", "def test_parse_arglist():\n    # Test simple argument list\n    arglist = 'a, b, c'\n    expected = 'a, b, c'\n    signode = desc_signature('', '')\n    _pseudo_parse_arglist(signode, arglist)\n    assert signode.astext() == expected\n\n    # Test argument list with optional arguments\n    arglist = 'a, [b, c]'\n    expected = 'a, [b, c]'\n    signode = desc_signature('', '')\n    _pseudo_parse_arglist(signode, arglist)\n    assert signode.astext() == expected\n\n    # Test argument list with mixed optional and non-optional arguments\n    arglist = 'a, [b], c'\n    expected = 'a, [b], c'\n    signode = desc_signature('', '')\n    _pseudo_parse_arglist(signode, arglist)\n    assert signode.astext() == expected\n\n    # Test argument list with nested optional arguments\n    arglist = 'a, [b, [c, d]]'\n    expected = 'a, [b, [c, d]]'\n    signode = desc_signature('', '')\n    _pseudo_parse_arglist(signode, arglist)\n    assert signode.astext() == expected\n\n    # Test argument list with nested optional arguments and non-optional arguments\n    arglist = 'a, [b, [c, d]], e'\n    expected = 'a, [b, [c, d]], e'\n    signode = desc_signature('', '')\n    _pseudo_parse_arglist(signode, arglist)\n    assert signode.astext() == expected\n\n    # Test argument list with empty optional arguments\n    arglist = 'a, [], b'\n    expected = 'a, [], b'\n    signode = desc_signature('', '')\n    _pseudo_parse_arglist(signode, arglist)\n    assert signode.astext() == expected\n\n    # Test argument list with incorrect nesting\n    arglist = 'a, [b, [c], d]'\n    expected = 'a, [b, [c], d]'\n    signode = desc_signature('', '')\n    _pseudo_parse_arglist(signode, arglist)\n    assert signode.astext() == expected\n\n    # Test argument list with complex nesting\n    arglist = 'a, [b, [c, [d, e], f]], g'\n    expected = 'a, [b,"], "sample_913": ["def test_pymodule_signature(app):\n    text = (\".. py:module:: example.module\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :synopsis: This is a test module\\n\"\n            \"   :noindex:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, [])\n    assert 'example.module' not in domain.modules\n\n    text = (\".. py:module:: example.module\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :synopsis: This is a test module\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, [nodes.target])\n    assert 'example.module' in domain.modules\n    assert domain.modules['example.module'] == ('index', 'module-example.module', \n                                                'This is a test module', 'Unix', False)\n", "def test_parse_annotation_invalid_syntax():\n    doctree = _parse_annotation(\"Invalid[Syntax\")\n    assert_node(doctree, ([pending_xref, \"Invalid[Syntax\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"Invalid[Syntax\")\n", "def test_parse_arglist_with_var_args():\n    sig = 'func(a, b, *args, **kwargs)'\n    signode = addnodes.desc_signature(sig, '')\n    parsed_params = _parse_arglist('a, b, *args, **kwargs')\n    signode += parsed_params\n    assert_node(parsed_params, addnodes.desc_parameterlist,\n                ([addnodes.desc_parameter, ([addnodes.desc_sig_name, \"a\"])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_name, \"b\"])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_operator, \"*\"],\n                                            [addnodes.desc_sig_name, \"args\"])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_operator, \"**\"],\n                                            [addnodes.desc_sig_name, \"kwargs\"])]))\n", "def test_pyfunction_with_module(app):\n    text = (\".. py:function:: func1\\n\"\n            \"   :module: mod1\\n\"\n            \"\\n\"\n            \".. py:function:: func2\\n\"\n            \"   :module: mod2\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"mod1.\"],\n                                                    [desc_name, \"func1\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"mod2.\"],\n                                                    [desc_name, \"func2\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n\n    assert 'mod1.func1' in domain.objects\n    assert domain.objects['mod1.func1'] == ('index', 'mod1.func1', 'function')\n    assert 'mod2.func2' in domain.objects\n    assert domain.objects['mod2.func2'] == ('index', 'mod2.func2', 'function')\n", "def test_pymodulelevel(app):\n    text = \".. py:function:: func\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert 'func' in domain.objects\n    assert domain.objects['func'] == ('index', 'func', 'function')\n"], "sample_914": ["def test_unparse_arguments():\n    code = \"\"\"", "def test_unparse_arguments():\n    source = \"def func(a, b=2, *args, c, d=4, **kwargs): pass\"\n    module = ast.parse(source)\n    func_node = module.body[0]\n    assert ast.unparse_arguments(func_node.args) == \"a, b=2, *args, c, d=4, **kwargs\"\n", "def test_unparse_arguments():\n    source = \"def func(a, b: int, c=3, d: str = 'default', *args, kw1, kw2='kw_default', **kwargs): pass\"\n    module = ast.parse(source)\n    func_def = module.body[0]\n    expected = \"a, b: int, c=3, d: str = 'default', *args, kw1, kw2 = 'kw_default', **kwargs\"\n    assert ast.unparse_arguments(func_def.args) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a, b=2, *args, c=3, **kwargs): pass\"\n    module = ast.parse(source)\n    args = module.body[0].args\n    expected = \"a, b=2, *args, c=3, **kwargs\"\n    assert ast.unparse_arguments(args) == expected\n", "def test_unparse_arguments():\n    source = \"\"\"def func(a, b=2, *args, c, d=4, **kwargs): pass\"\"\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == \"a, b = 2, *args, c, d = 4, **kwargs\"\n"], "sample_915": ["def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @func.register(int)\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(_) is False\n", "def test_getdoc_inherited_decorated_method():\n    class Foo:\n            \"\"\"docstring.\"\"\"\n\n    class Bar(Foo):\n        @functools.wraps(Foo.meth)\n            pass\n\n    doc = inspect.getdoc(Bar.meth, cls=Bar, name='meth')\n    assert doc == \"docstring.\"\n", "def test_ispartial():\n        pass\n\n    p = functools.partial(func, 1)\n    assert inspect.ispartial(p) is True\n    assert inspect.ispartial(func) is False\n\n    class Foo:\n            pass\n\n    p_method = functools.partialmethod(Foo.method, 1)\n    assert inspect.ispartial(p_method) is True\n    assert inspect.ispartial(Foo.method) is False\n", "def test_isabstractmethod():\n    class AbstractClass:\n        @staticmethod\n            pass\n\n        @classmethod\n            pass\n\n        @property\n            return \"property\"\n\n        @staticmethod\n            raise NotImplementedError\n\n        @classmethod\n            raise NotImplementedError\n\n        @property\n            raise NotImplementedError\n\n    assert inspect.isabstractmethod(AbstractClass.abstract_staticmethod) is True\n    assert inspect.isabstractmethod(AbstractClass.abstract_classmethod) is True\n    assert inspect.isabstractmethod(AbstractClass.abstract_property) is True\n    assert inspect.isabstractmethod(AbstractClass.concrete_method) is False\n    assert inspect.isabstractmethod(AbstractClass.concrete_classmethod) is False\n    assert inspect.isabstractmethod(AbstractClass.concrete_property) is False\n", "def test_isabstractmethod():\n    class AbstractBase:\n            pass\n\n            pass\n\n        @staticmethod\n            pass\n\n        @classmethod\n            pass\n\n        @property\n            return \"property\"\n\n        @staticmethod\n            pass\n\n    class Derived(AbstractBase):\n            return \"implemented\"\n\n        @property\n            return \"overridden property\"\n\n    # Not an abstract method\n    assert inspect.isabstractmethod(AbstractBase.implemented_method) is False\n    assert inspect.isabstractmethod(Derived().implemented_method) is False\n\n    # Static method\n    assert inspect.isabstractmethod(AbstractBase.static_method) is False\n    assert inspect.isabstractmethod(Derived().static_method) is False\n\n    # Class method\n    assert inspect.isabstractmethod(AbstractBase.class_method) is False\n    assert inspect.isabstractmethod(Derived().class_method) is False\n\n    # Property\n    assert inspect.isabstractmethod(AbstractBase.some_property) is False\n    assert inspect.isabstractmethod(Derived().some_property) is False\n\n    # Other static method\n    assert inspect.isabstractmethod(AbstractBase.some_other_method) is False\n    assert inspect.isabstractmethod(Derived().some_other_method) is False\n\n    from abc import ABC, abstractmethod\n\n    class AbstractDerived(ABC):\n        @abstractmethod\n            pass\n\n    assert inspect.isabstractmethod(AbstractDerived.another_method) is True\n"], "sample_916": ["def test_macro_definitions():\n    check('macro', 'MY_MACRO', {1: 'MY_MACRO', 2: '7MY_MACRO'})\n    check('macro', 'MY_MACRO(x)', {1: 'MY_MACRO__x', 2: '7MY_MACRO1xE'})\n    check('macro', 'MY_MACRO(x, y)', {1: 'MY_MACRO__x.y', 2: '7MY_MACRO2xyE'})\n    check('macro', 'MY_MACRO(...)', {1: 'MY_MACRO__Dp', 2: '7MY_MACRODpE'})\n    check('macro', 'MY_MACRO(x, ...)', {1: 'MY_MACRO__xDp', 2: '7MY_MACRO1xDpE'})\n    check('macro', 'MY_MACRO(x, y, ...)', {1: 'MY_MACRO__x.yDp', 2: '7MY_MACRO2xyDpE'})\n", "def test_ast_identifier():\n    ident = ASTIdentifier(\"testIdent\")\n    assert ident.identifier == \"testIdent\"\n    assert not ident.is_anon()\n    assert str(ident) == \"testIdent\"\n    assert ident.get_display_string() == \"testIdent\"\n\n    anon_ident = ASTIdentifier(\"@anonIdent\")\n    assert anon_ident.is_anon()\n    assert anon_ident.get_display_string() == \"[anonymous]\"\n\n    signode = addnodes.desc_signature()\n    anon_ident.describe_signature(signode, 'markType', None, '', None)\n    assert isinstance(signode[0], nodes.strong)\n    assert signode[0].astext() == \"[anonymous]\"\n\n    ident.describe_signature(signode, 'markType', None, '', None)\n    assert isinstance(signode[-1], nodes.Text)\n    assert signode[-1].astext() == \"testIdent\"\n", "def test_c_macro():\n    # Test simple macros\n    check('macro', 'MAX(a, b)', {2: \"3MAX\"})\n    check('macro', 'MIN(a, b)', {2: \"3MIN\"})\n    check('macro', 'SQUARE(x)', {2: \"6SQUARE\"})\n    \n    # Test macros with more complex expressions\n    check('macro', 'ABS(x) ((x) < 0 ? -(x) : (x))', {2: \"3ABS\"})\n    check('macro', 'CLAMP(x, lower, upper) ((x) < (lower) ? (lower) : (x) > (upper) ? (upper) : (x))', {2: \"5CLAMP\"})\n\n    # Test variadic macros\n    check('macro', 'LOG(fmt, ...) fprintf(stderr, fmt, __VA_ARGS__)', {2: \"3LOG\"})\n", "def test_c_declaration_parsing():\n        class Config:\n            c_id_attributes = [\"id_attr\"]\n            c_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(string, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_declaration(name, name)\n        parser.assert_end()\n        return ast\n\n        ast = c_parse(name, input)\n        res = str(ast)\n        assert res == expected_output, f\"Expected '{expected_output}', but got '{res}'\"\n    \n    # Test basic type declarations\n    c_check(\"type\", \"int a\", \"int a\")\n    c_check(\"type\", \"float b\", \"float b\")\n    c_check(\"type\", \"double c\", \"double c\")\n\n    # Test pointer declarations\n    c_check(\"type\", \"int *a\", \"int *a\")\n    c_check(\"type\", \"const char *b\", \"const char *b\")\n\n    # Test function declarations\n    c_check(\"function\", \"void f()\", \"void f()\")\n    c_check(\"function\", \"int add(int a, int b)\", \"int add(int a, int b)\")\n    \n    # Test struct declarations\n    c_check(\"struct\", \"struct Point\", \"Point\")\n    c_check(\"struct\", \"struct Point { int x; int y; }\", \"Point\")\n    \n    # Test enum declarations\n    c_check(\"enum\", \"enum Color { RED, GREEN, BLUE }\", \"Color\")\n    c_check(\"enumerator\", \"RED\", \"RED\")\n    c_check(\"enumerator\", \"GREEN = 1\", \"GREEN = 1\")\n    \n    # Test macro declarations\n    c_check(\"macro\", \"#define MAX(a, b) ((a) > (b) ? (a) : (b))\", \"MAX(a, b)\")\n    \n    # Test union declarations\n    c_check(\"union\", \"union Data { int i; float f; }\", \"Data\")\n", "def test_ast_identifier():\n    identifier = ASTIdentifier(\"test_identifier\")\n    assert str(identifier) == \"test_identifier\"\n    assert identifier.get_display_string() == \"test_identifier\"\n    assert identifier.is_anon() == False\n\n    anon_identifier = ASTIdentifier(\"@anon_identifier\")\n    assert str(anon_identifier) == \"@anon_identifier\"\n    assert anon_identifier.get_display_string() == \"[anonymous]\"\n    assert anon_identifier.is_anon() == True\n\n    signode = addnodes.desc_signature('', '')\n    env = None  # Mock or provide a valid environment if necessary\n    symbol = None  # Mock or provide a valid symbol if necessary\n    identifier.describe_signature(signode, 'markType', env, '', symbol)\n    assert len(signode) > 0\n"], "sample_917": ["def test_template_param_with_default_type():\n    check('class', 'template<typename T, typename U = int> A', {2: 'I01iE1A'})\n    check('class', 'template<template<typename V> class T, typename U = T<int>> B',\n          {2: 'II0E01T1iE1B'})\n    check('function', 'template<typename T = double> void f()', {2: 'I0dE1fv', 4: 'I0dE1fvv'})\n    check('type', 'template<typename T = std::vector<int>> using Alias = T', {2: 'I0NSt6vectorIiEEE5Alias'})\n", "def test_fundamental_type_id_generation():\n    # additional test to improve coverage on fundamental type id generation\n\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(type_name, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_type(False)\n        res = ast.get_id(version=2)\n        assert res == expected_id, f\"Expected id: {expected_id}, but got: {res}\"\n\n    fundamental_types = {\n        'std::nullptr_t': 'NSt9nullptr_tE',\n        'char': 'c',\n        'int': 'i',\n        'double': 'd',\n        'float': 'f',\n        'bool': 'b',\n        'void': 'v',\n        'wchar_t': 'w',\n        'char16_t': 'Ds',\n        'char32_t': 'Di',\n        'signed char': 'a',\n        'unsigned char': 'h',\n        'short': 's',\n        'unsigned short': 't',\n        'unsigned': 'j',\n        'long': 'l',\n        'unsigned long': 'm',\n        'long long': 'x',\n        'unsigned long long': 'y'\n    }\n\n    for type_name, expected_id in fundamental_types.items():\n        check_fundamental_type(type_name, expected_id)\n", "def test_describe_signature():\n    # Ensure the describe_signature method works correctly for various AST nodes\n    from docutils import nodes\n\n    class MockEnvironment:\n        temp_data = {'cpp:parent_symbol': Symbol(None, None, None, None, None, None)}\n        config = type('Config', (object,), {'cpp_id_attributes': [], 'cpp_paren_attributes': []})\n\n    env = MockEnvironment()\n\n        signode = addnodes.desc_signature('sig', '')\n        declaration.describe_signature(signode, 'markName', env, {})\n        result = str(signode[0])\n        assert result == expected, f\"Expected '{expected}', but got '{result}'\"\n\n    # Function\n    ast = parse(\"function\", \"void func(int a, float b)\")\n    check_signature(ast, 'void func(int a, float b)')\n\n    # Type\n    ast = parse(\"type\", \"int MyType\")\n    check_signature(ast, 'int MyType')\n\n    # Member\n    ast = parse(\"member\", \"int memberVar\")\n    check_signature(ast, 'int memberVar')\n\n    # Class\n    ast = parse(\"class\", \"MyClass\")\n    check_signature(ast, 'MyClass')\n\n    # Enum\n    ast = parse(\"enum\", \"MyEnum\")\n    check_signature(ast, 'MyEnum')\n\n    # Enumerator\n    ast = parse(\"enumerator\", \"MyEnumerator = 1\")\n    check_signature(ast, 'MyEnumerator = 1')\n\n    # Concept\n    ast = parse(\"concept\", \"template<typename T> Concept\")\n    check_signature(ast, 'template<typename T> Concept')\n\n    # Union\n    ast = parse(\"union\", \"MyUnion\")\n    check_signature(ast, 'MyUnion')\n", "def test_parsing_template_parameter_list():\n    # Function to test parsing of template parameter list with different configurations\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(params_str, location=None, config=Config())\n        params = parser._parse_template_parameter_list()\n        parser.assert_end()\n        assert str(params) == expected, f\"Expected: {expected}, but got: {str(params)}\"\n\n    parse_template_params(\"template<typename T>\", \"template<typename T> \")\n    parse_template_params(\"template<int I, typename T>\", \"template<int I, typename T> \")\n    parse_template_params(\"template<template<typename> typename U>\", \"template<template<typename> typename U> \")\n    parse_template_params(\"template<template<int> typename... Ts>\", \"template<template<int> typename... Ts> \")\n    parse_template_params(\"template<typename T = int>\", \"template<typename T = int> \")\n    parse_template_params(\"template<int I = 0>\", \"template<int I = 0> \")\n    parse_template_params(\"template<template<int> typename U = Default<int>>\", \"template<template<int> typename U = Default<int>> \")\n\n    # Test with pack expansion and default arguments\n    parse_template_params(\"template<typename... Ts>\", \"template<typename... Ts> \")\n    parse_template_params(\"template<typename T, typename... Ts>\", \"template<typename T, typename... Ts> \")\n    parse_template_params(\"template<template<typename> typename... Us>\", \"template<template<typename> typename... Us> \")\n\n    with pytest.raises(DefinitionError):\n        parse_template_params(\"template<>\", \"\")\n    with pytest.raises(DefinitionError):\n        parse_template_params(\"template<int I, typename>\", \"\")\n", "def test_invalid_declarations():\n    invalid_declarations = [\n        ('function', 'void 123func()'),\n        ('function', 'int main(int argc, char* argv['),\n        ('function', 'template<int 123T> void func()'),\n        ('type', 'int 123var'),\n        ('class', 'class 123Class'),\n        ('member', 'int 123member'),\n        ('enum', 'enum 123Enum'),\n        ('enumerator', '123EnumVal')\n    ]\n    for decl_type, decl in invalid_declarations:\n        with pytest.raises(DefinitionError):\n            parse(decl_type, decl)\n"], "sample_918": ["def test_pyfunction_signature_with_annotations_and_defaults(app):\n    text = (\".. py:function:: process_item(item: str, quantity: int = 1, price: float = 0.0) -> dict\\n\"\n            \"   :module: store\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"function \"],\n                                                    [desc_addname, \"store.\"],\n                                                    [desc_name, \"process_item\"],\n                                                    [desc_parameterlist,\n                                                     ([desc_parameter, ([desc_sig_name, \"item\"],\n                                                                        [desc_sig_punctuation, \":\"],\n                                                                        \" \",\n                                                                        [nodes.inline, pending_xref, \"str\"])],\n                                                      [desc_parameter, ([desc_sig_name, \"quantity\"],\n                                                                        [desc_sig_punctuation, \":\"],\n                                                                        \" \",\n                                                                        [nodes.inline, pending_xref, \"int\"],\n                                                                        \" \",\n                                                                        [desc_sig_operator, \"=\"],\n                                                                        \" \",\n                                                                        [nodes.inline, \"1\"])],\n                                                      [desc_parameter, ([desc_sig_name, \"price\"],\n                                                                        [desc_sig_punctuation, \":\"],\n                                                                        \" \",\n                                                                        [nodes.inline, pending_xref, \"float\"],\n                                                                        \" \",\n                                                                        [desc_sig_operator, \"=\"],\n                                                                        \" \",\n                                                                        [nodes.inline, \"0.0\"])])],\n                                                    [desc_returns, pending_xref, \"dict\"])],\n                                  [desc_content, ()])]))\n\n    domain = app.env.get_domain('py')\n    assert 'store.process_item' in domain.objects\n    assert domain.objects['store.process_item'] == ('index', 'store.process_item', 'function')\n", "def test_parse_annotation_error_handling():\n    # Test handling of invalid syntax in annotations\n    doctree = _parse_annotation(\"Invalid[Type\")\n    assert_node(doctree, ([pending_xref, \"Invalid[Type\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"Invalid[Type\")\n\n    # Test handling of unsupported syntax in annotations\n    doctree = _parse_annotation(\"Unsupported{Type}\")\n    assert_node(doctree, ([pending_xref, \"Unsupported{Type\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"Unsupported{Type\")\n\n    # Test handling of empty annotation\n    doctree = _parse_annotation(\"\")\n    assert_node(doctree, ([pending_xref, \"\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"\")\n", "def test_parse_annotation_edge_cases():\n    # Edge case: Empty annotation string\n    doctree = _parse_annotation(\"\")\n    assert_node(doctree, [])\n\n    # Edge case: Single punctuation\n    doctree = _parse_annotation(\"[\")\n    assert_node(doctree, ([addnodes.desc_sig_punctuation, \"[\"],))\n\n    # Edge case: Nested structures\n    doctree = _parse_annotation(\"Dict[str, List[Tuple[int, Union[str, None]]]]\")\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"None\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_parse_annotation_error_handling():\n    # Test handling of invalid annotations gracefully\n    doctree = _parse_annotation(\"Invalid[Annotation\")\n    assert_node(doctree, ([pending_xref, \"Invalid[Annotation\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"Invalid[Annotation\")\n\n    doctree = _parse_annotation(\"Tuple[int, str\")\n    assert_node(doctree, ([pending_xref, \"Tuple[int, str\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"Tuple[int, str\")\n\n    # Test handling of unexpected syntax gracefully\n    doctree = _parse_annotation(\"123\")\n    assert_node(doctree, ([pending_xref, \"123\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"123\")\n", "def test_type_to_xref():\n    from sphinx.domains.python import type_to_xref\n\n    xref_node = type_to_xref('int')\n    assert_node(xref_node, pending_xref, reftarget='int', refdomain='py', reftype='class')\n\n    xref_node = type_to_xref('None')\n    assert_node(xref_node, pending_xref, reftarget='None', refdomain='py', reftype='obj')\n\n    xref_node = type_to_xref('MyClass')\n    assert_node(xref_node, pending_xref, reftarget='MyClass', refdomain='py', reftype='class')\n"], "sample_919": ["def test_member_function_default_arguments():\n    check('function', 'void f(int a, float b = 3.14f, char c = \\'x\\')',\n          {1: 'f__i.f.c', 2: '1fifc'})\n    check('function', 'void g(const std::string& s, int n = 10, bool flag = true)',\n          {1: 'g__std::stringCR.i.b', 2: '1gRKNSt6stringEiKb'})\n    check('function', 'void h(double d, double e = 2.718, double pi = 3.14159)',\n          {1: 'h__d.d.d', 2: '1hddd'})\n", "def test_invalid_definitions():\n    with pytest.raises(DefinitionError):\n        parse('class', 'class 123Invalid')  # invalid class name\n    with pytest.raises(DefinitionError):\n        parse('function', 'void 123InvalidFunc()')  # invalid function name\n    with pytest.raises(DefinitionError):\n        parse('type', 'using 123InvalidType = int')  # invalid type alias name\n    with pytest.raises(DefinitionError):\n        parse('member', 'int 123InvalidMember')  # invalid member name\n    with pytest.raises(DefinitionError):\n        parse('concept', 'template<typename T> concept 123InvalidConcept = true')  # invalid concept name\n    with pytest.raises(DefinitionError):\n        parse('enumerator', '123InvalidEnumerator')  # invalid enumerator name\n", "def test_define_symbol():\n    class MockDocument:\n            self.ids = set()\n\n            self.ids.update(signode['ids'])\n\n    root_symbol = Symbol(None, None, None, None, None, None)\n    docname = \"test_doc\"\n\n    # Add a symbol without any declaration\n    name = ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"TestSymbol\"), None)], [False], False)\n    symbol = root_symbol.add_name(name)\n    assert symbol is not None\n    assert symbol.identOrOp.identifier == \"TestSymbol\"\n    assert symbol.declaration is None\n\n    # Add a declaration to the existing symbol\n    decl = ASTDeclaration('class', 'class', None, None, None, name, None)\n    symbol.add_declaration(decl, docname)\n    assert symbol.declaration == decl\n    assert symbol.docname == docname\n\n    # Add another symbol with declaration\n    name2 = ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"AnotherSymbol\"), None)], [False], False)\n    decl2 = ASTDeclaration('class', 'class', None, None, None, name2, None)\n    symbol2 = root_symbol.add_declaration(decl2, docname)\n    assert symbol2 is not None\n    assert symbol2.identOrOp.identifier == \"AnotherSymbol\"\n    assert symbol2.declaration == decl2\n    assert symbol2.docname == docname\n\n    # Test symbol ID generation\n    ids = []\n    for i in range(1, _max_id + 1):\n        try:\n            id = decl.get_id(version=i)\n            ids.append(id)\n        except NoOldIdError:\n            ids.append(None)\n    assert ids == [\"1TestSymbol\", \"1TestSymbol\", \"1TestSymbol\", \"1TestSymbol\"]\n\n    # Test adding symbol to a document\n    signode = addnodes.desc_signature(\"TestSymbol\", '')\n    mock_doc = MockDocument()\n    decl.describe_signature(signode, 'lastIsName', mock_doc, options={})\n    assert signode['ids'] == [\"1TestSymbol\"]\n    assert mock_doc.ids == {\"1TestSymbol\"}\n", "def test_parse_nested_name():\n        parser = DefinitionParser(name, location=None, config=None)\n        ast = parser._parse_nested_name()\n        assert str(ast) == expected\n\n    check('A::B::C', 'A::B::C')\n    check('A<B,C>::D', 'A<B,C>::D')\n    check('A<B<C,D>,E<F>>', 'A<B<C,D>,E<F>>')\n    check('::A::B', '::A::B')\n    check('A<B,C...>::D', 'A<B,C...>::D')\n\n    with pytest.raises(DefinitionError):\n        check('A::', 'A::')\n    with pytest.raises(DefinitionError):\n        check('A<', 'A<')\n", "def test_template_function_specializations():\n    check('function', 'template<> void f<int>(int)', {2: 'IE1fIiEi'})\n    check('function', 'template<> void f<double>(double)', {2: 'IE1fIdEd'})\n    check('function', 'template<> void f<char>(char)', {2: 'IE1fIcEc'})\n    check('function', 'template<> void f<std::string>(std::string)', {2: 'IE1fINSt6stringEENSt6stringEE'})\n    check('function', 'template<> void f<A>(A)', {2: 'IE1fI1AE1A'})\n    check('function', 'template<> void f<int, double>(int, double)', {2: 'IE1fIidEid'})\n    check('function', 'template<> void f<int, char>(int, char)', {2: 'IE1fIicEic'})\n    check('function', 'template<> void f<std::string, A>(std::string, A)', {2: 'IE1fINSt6stringE1AE1NSt6stringE1A'})\n    check('function', 'template<> void f<std::vector<int>>(std::vector<int>)', {2: 'IE1fINSt6vectorIiEEE1NSt6vectorIiEEE'})\n"], "sample_920": ["    def test_returns_section_with_no_type(self):\n        config = Config()\n        docstring = \"\"\"\n        Example function.\n\n        Returns:\n            Description of return value without specifying type.\n        \"\"\"\n        actual = str(GoogleDocstring(docstring, config=config, app=None, what='function', name='example_func', obj=None))\n        expected = \"\"\"\n        Example function.\n\n        :returns: Description of return value without specifying type.\n        \"\"\"\n        self.assertEqual(dedent(expected), actual)\n", "    def test_custom_section(self):\n        config = Config(napoleon_custom_sections=['Custom Section'])\n        docstring = \"\"\"\\", "    def test_custom_sections_with_formatting(self):\n        docstring = \"\"\"\\", "    def test_custom_sections_with_admonition(self):\n        docstring = \"\"\"\\", "    def test_extended_description(self):\n        config = Config()\n        docstring = \"\"\"\n        A function with an extended description.\n\n        This description spans multiple lines\n        and includes detailed explanation of the function\n        and its behavior.\n\n        Args:\n            arg1 (str): The first argument.\n            arg2 (int): The second argument.\n        \n        Returns:\n            bool: True if successful, False otherwise.\n\n        Raises:\n            ValueError: If an error occurs.\n        \"\"\"\n        actual = str(GoogleDocstring(docstring, config=config, app=None,\n                     what='function', name='test_function', obj=None))\n        expected = \"\"\"\n        A function with an extended description.\n\n        This description spans multiple lines\n        and includes detailed explanation of the function\n        and its behavior.\n\n        :param arg1: The first argument.\n        :type arg1: str\n        :param arg2: The second argument.\n        :type arg2: int\n        :returns: True if successful, False otherwise.\n        :rtype: bool\n        :raises ValueError: If an error occurs.\n        \"\"\"\n        self.assertEqual(dedent(expected).strip(), actual.strip())\n"], "sample_921": ["def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func.register(int)(lambda x: x)) is True\n    assert inspect.is_singledispatch_function(lambda x: x) is False\n\n", "def test_unwrap_all():\n        @functools.wraps(f)\n            return f(*args, **kwargs)\n        return wrapped\n\n    @decorator\n        return x\n\n    partial_func = functools.partial(my_function, 1)\n    \n    assert inspect.unwrap_all(partial_func) is my_function\n    assert inspect.unwrap_all(my_function) is my_function\n    assert inspect.unwrap_all(partial_func, stop=lambda o: o == my_function) is my_function\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return \"default\"\n\n    @foo.register(int)\n        return \"int\"\n\n    assert inspect.is_singledispatch_function(foo) is True\n    assert inspect.is_singledispatch_function(foo.dispatch(int)) is False\n\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return \"default\"\n\n    @func.register(int)\n        return \"int\"\n\n    @func.register(str)\n        return \"str\"\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func.dispatch(int)) is False\n    assert inspect.is_singledispatch_function(func.dispatch(str)) is False\n", "def test_getargspec():\n        pass\n\n        pass\n\n        pass\n\n    # Test for simple function\n    spec = inspect.getargspec(simple_func)\n    assert spec.args == ['a', 'b', 'c', 'd']\n    assert spec.varargs == 'args'\n    assert spec.varkw == 'kwargs'\n    assert spec.defaults == (1, 2)\n    assert spec.kwonlyargs == []\n    assert spec.kwonlydefaults is None\n    assert spec.annotations == {}\n\n    # Test for function with annotations\n    spec = inspect.getargspec(annotated_func)\n    assert spec.args == ['x', 'y']\n    assert spec.varargs is None\n    assert spec.varkw is None\n    assert spec.defaults == ('default',)\n    assert spec.kwonlyargs == []\n    assert spec.kwonlydefaults is None\n    assert spec.annotations == {'x': int, 'y': str, 'return': None}\n\n    # Test for bound method\n    class MyClass:\n            pass\n\n    bound_method = MyClass().method\n    spec = inspect.getargspec(bound_method)\n    assert spec.args == ['x', 'y']\n    assert spec.varargs is None\n    assert spec.varkw is None\n    assert spec.defaults == (10,)\n    assert spec.kwonlyargs == []\n    assert spec.kwonlydefaults is None\n    assert spec.annotations == {}\n"], "sample_922": ["def test_type_to_xref():\n    from sphinx.domains.python import type_to_xref\n\n    # Test with various input types\n    xref = type_to_xref('int')\n    assert isinstance(xref, addnodes.pending_xref)\n    assert xref['reftarget'] == 'int'\n    assert xref['reftype'] == 'class'\n\n    xref = type_to_xref('str')\n    assert isinstance(xref, addnodes.pending_xref)\n    assert xref['reftarget'] == 'str'\n    assert xref['reftype'] == 'class'\n\n    xref = type_to_xref('None')\n    assert isinstance(xref, addnodes.pending_xref)\n    assert xref['reftarget'] == 'None'\n    assert xref['reftype'] == 'obj'\n\n    xref = type_to_xref('custom_type')\n    assert isinstance(xref, addnodes.pending_xref)\n    assert xref['reftarget'] == 'custom_type'\n    assert xref['reftype'] == 'class'\n", "def test_pyfunction_with_all_options(app):\n    text = (\".. py:function:: decorated_func(arg1: int, arg2: str = 'default') -> None\\n\"\n            \"   :async:\\n\"\n            \"   :module: test_module\\n\"\n            \"   :annotation: This is a test function\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"async \"],\n                                                    [desc_addname, \"test_module.\"],\n                                                    [desc_name, \"decorated_func\"],\n                                                    [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"arg1\"],\n                                                                                            [desc_sig_punctuation, \":\"],\n                                                                                            \" \",\n                                                                                            [pending_xref, \"int\"])],\n                                                                          [desc_parameter, ([desc_sig_name, \"arg2\"],\n                                                                                            [desc_sig_punctuation, \":\"],\n                                                                                            \" \",\n                                                                                            [pending_xref, \"str\"],\n                                                                                            [desc_sig_operator, \"=\"],\n                                                                                            [nodes.inline, \"'default'\"])])],\n                                                    [desc_returns, pending_xref, \"None\"],\n                                                    [desc_annotation, \" This is a test function\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert 'test_module.decorated_func' in domain.objects\n    assert domain.objects['test_module.decorated_func'] == ('index', 'test_module.decorated_func', 'function')\n", "def test_pymodule_signature(app):\n    text = (\".. py:module:: example.module\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :synopsis: Example module\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, ([nodes.target, ()],\n                          [addnodes.index, ()]))\n    assert 'example.module' in domain.modules\n    assert domain.modules['example.module'].docname == 'index'\n    assert domain.modules['example.module'].platform == 'Unix'\n    assert domain.modules['example.module'].synopsis == 'Example module'\n    assert domain.modules['example.module'].deprecated is True\n", "def test_parse_arglist():\n    \"\"\"Test _parse_arglist function.\"\"\"\n        signode = addnodes.desc_signature('', '')\n        signode += _parse_arglist(arglist)\n        return signode\n\n    signode = parse_arglist('a, b, c')\n    assert_node(signode[0], addnodes.desc_parameterlist,\n                ([addnodes.desc_parameter, ([addnodes.desc_sig_name, 'a'])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_name, 'b'])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_name, 'c'])]))\n\n    signode = parse_arglist('a: int, b: str, c: float')\n    assert_node(signode[0], addnodes.desc_parameterlist,\n                ([addnodes.desc_parameter, ([addnodes.desc_sig_name, 'a'],\n                                            [addnodes.desc_sig_punctuation, ':'],\n                                            ' ',\n                                            [pending_xref, 'int'])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_name, 'b'],\n                                            [addnodes.desc_sig_punctuation, ':'],\n                                            ' ',\n                                            [pending_xref, 'str'])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_name, 'c'],\n                                            [addnodes.desc_sig_punctuation, ':'],\n                                            ' ',\n                                            [pending_xref, 'float'])]))\n\n    signode = parse_arglist('a: List[int], b: Tuple[int, str]')\n    assert_node(signode[0], addnodes.desc_parameterlist,\n                ([addnodes.desc_parameter, ([addnodes.desc_sig_name, 'a'],\n                                            [addnodes.desc_sig_punctuation, ':'],\n                                            ' ',\n                                            [pending_xref, 'List'],\n                                            [addnodes.desc_sig_punctuation, '['],\n                                            [pending_xref, 'int'],\n                                            [addnodes.desc_sig_punctuation, ']'])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_name, 'b'],\n                                            [addnodes.desc_sig_punctuation, ':'],\n                                            ' ',\n                                            [pending_xref, 'Tuple'],\n                                            [addnodes.desc_sig_punctuation, '['],\n                                            [pending_xref, 'int'],\n                                            [addnodes.desc_sig_punctuation, ', '],\n                                            [pending_xref, 'str'],\n                                            [addnodes.desc_sig_punctuation, ']'])]))\n", "def test_parse_annotation_with_complex_nested_types():\n    doctree = _parse_annotation(\"List[Dict[str, Union[int, List[str]]]]\")\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n"], "sample_923": ["def test_expression_parsing():\n    # This test aims to cover more cases for expressions parsing\n\n    # Test various types of literals\n    literals = [\n        ('42', 'L42E'),\n        ('42u', 'L42uE'),\n        ('42U', 'L42UE'),\n        ('42l', 'L42lE'),\n        ('42L', 'L42LE'),\n        ('3.14', 'L3.14E'),\n        ('3.14f', 'L3.14fE'),\n        ('3.14F', 'L3.14FE'),\n        ('3.14l', 'L3.14lE'),\n        ('3.14L', 'L3.14LE'),\n        ('\\'a\\'', 'c97E'),\n        ('\"string\"', 'LA7_KcE'),\n        ('true', 'L1E'),\n        ('false', 'L0E')\n    ]\n    for literal, id in literals:\n        exprCheck(literal, id)\n\n    # Test unary expressions\n    unary_expressions = [\n        ('++i', 'ppi'),\n        ('--i', 'mmi'),\n        ('+i', 'psi'),\n        ('-i', 'ngi'),\n        ('!i', 'nti'),\n        ('not i', 'nti'),\n        ('~i', 'coi'),\n        ('compl i', 'coi')\n    ]\n    for expr, id in unary_expressions:\n        exprCheck(expr, id)\n\n    # Test binary expressions\n    binary_expressions = [\n        ('a + b', 'pl1a1b'),\n        ('a - b', 'mi1a1b'),\n        ('a * b', 'ml1a1b'),\n        ('a / b', 'dv1a1b'),\n        ('a % b', 'rm1a1b'),\n        ('a & b', 'an1a1b'),\n        ('a | b', 'or1a1b'),\n        ('a ^ b', 'eo1a1b'),\n        ('a << b', 'ls1a1b'),\n        ('a >> b', 'rs1a1b'),\n        ('a && b', 'aa1a1b'),\n        ('a || b', 'oo1a1b'),\n        ('a < b', 'lt1a1b'),\n        ('a > b', 'gt1a1b'),\n        ('a <= b',", "def test_symbol_lookup():\n    root = Symbol(None, None, None, None)\n    a = Symbol(root, ASTIdentifier('a'), None, None)\n    b = Symbol(a, ASTIdentifier('b'), None, None)\n    c = Symbol(b, ASTIdentifier('c'), None, None)\n    d = Symbol(c, ASTIdentifier('d'), None, None)\n\n    assert root.find_identifier(ASTIdentifier('a'), matchSelf=False, recurseInAnon=False) == a\n    assert a.find_identifier(ASTIdentifier('b'), matchSelf=False, recurseInAnon=False) == b\n    assert b.find_identifier(ASTIdentifier('c'), matchSelf=False, recurseInAnon=False) == c\n    assert c.find_identifier(ASTIdentifier('d'), matchSelf=False, recurseInAnon=False) == d\n\n    assert root.find_identifier(ASTIdentifier('b'), matchSelf=True, recurseInAnon=False) is None\n    assert a.find_identifier(ASTIdentifier('c'), matchSelf=True, recurseInAnon=False) is None\n    assert b.find_identifier(ASTIdentifier('d'), matchSelf=True, recurseInAnon=False) is None\n", "def test_c_domain_object_definitions():\n        env = {\n            'temp_data': {},\n            'domaindata': {'c': {'root_symbol': Symbol(None, None, None, None)}},\n            'docname': 'test_doc',\n            'config': {},\n        }\n        env = type('Env', (object,), env)()\n        obj = CDomain(env)\n        obj = obj.directives[objtype](env, 'test', [sig], {}, None, None, 'Test', None)\n        signode = addnodes.desc_signature(sig, '')\n        try:\n            ast = obj.handle_signature(sig, signode)\n        except Exception as e:\n            print(f\"Error parsing signature '{sig}' for object type '{objtype}':\", e)\n            raise\n        assert expected_id == ast.get_newest_id()\n        if expected_display_name:\n            assert signode.astext() == expected_display_name\n\n    parse_and_check('function', 'void my_function(int)', 'c.my_function')\n    parse_and_check('struct', 'MyStruct', 'c.MyStruct')\n    parse_and_check('union', 'MyUnion', 'c.MyUnion')\n    parse_and_check('enum', 'MyEnum', 'c.MyEnum')\n    parse_and_check('enumerator', 'MyEnumerator', 'c.MyEnumerator')\n    parse_and_check('type', 'MyType', 'c.MyType')\n    parse_and_check('macro', 'MY_MACRO', 'c.MY_MACRO')\n", "def test_ast_identifier():\n    # Test initialization of ASTIdentifier with a valid identifier\n    identifier = \"valid_identifier\"\n    ast_id = cppDomain.ASTIdentifier(identifier)\n    assert ast_id.identifier == identifier\n\n    # Test is_anon method\n    anon_identifier = \"@anon\"\n    non_anon_identifier = \"non_anon\"\n    ast_anon_id = cppDomain.ASTIdentifier(anon_identifier)\n    ast_non_anon_id = cppDomain.ASTIdentifier(non_anon_identifier)\n    assert ast_anon_id.is_anon() is True\n    assert ast_non_anon_id.is_anon() is False\n\n    # Test __str__ method\n    assert str(ast_id) == identifier\n\n    # Test get_display_string method\n    assert ast_anon_id.get_display_string() == \"[anonymous]\"\n    assert ast_non_anon_id.get_display_string() == non_anon_identifier\n", "def test_c_basic_expressions():\n    # Boolean literals\n    assert str(ASTBooleanLiteral(True)) == 'true'\n    assert str(ASTBooleanLiteral(False)) == 'false'\n\n    # Number literals\n    assert str(ASTNumberLiteral(\"42\")) == '42'\n    assert str(ASTNumberLiteral(\"3.14\")) == '3.14'\n\n    # Character literals\n    assert str(ASTCharLiteral(\"\", \"a\")) == \"'a'\"\n    assert str(ASTCharLiteral(\"L\", \"a\")) == \"L'a'\"\n\n    # String literals\n    assert str(ASTStringLiteral('\"Hello\"')) == '\"Hello\"'\n\n    # Paren expressions\n    expr = ASTParenExpr(ASTNumberLiteral(\"42\"))\n    assert str(expr) == \"(42)\"\n\n    # Id expressions\n    name = ASTNestedName([ASTIdentifier(\"foo\")], False)\n    id_expr = ASTIdExpression(name)\n    assert str(id_expr) == \"foo\"\n\n    # Unary operations\n    unary_expr = ASTUnaryOpExpr(\"-\", ASTNumberLiteral(\"42\"))\n    assert str(unary_expr) == \"-42\"\n\n    # Binary operations\n    bin_expr = ASTBinOpExpr([ASTNumberLiteral(\"2\"), ASTNumberLiteral(\"3\")], [\"+\"])\n    assert str(bin_expr) == \"2 + 3\"\n\n    # Assignment expressions\n    assign_expr = ASTAssignmentExpr([ASTIdExpression(name), ASTNumberLiteral(\"42\")], [\"=\"])\n    assert str(assign_expr) == \"foo = 42\"\n"], "sample_924": ["def test_nested_member_functions():\n    check('function', 'void A::B::f()', {1: 'A::B::f', 2: 'N1A1B1fEv'})\n    check('function', 'int A::B::g(int)', {1: 'A::B::g__i', 2: 'N1A1B1gEi'})\n    check('function', 'A::B::C &A::B::h(A::B::C &)', \n          {1: 'A::B::h__A::B::C&R', 2: 'N1A1B1hERN1B1CE'})\n    check('function', 'A::B::D A::B::i(A::B::D)', \n          {1: 'A::B::i__A::B::D', 2: 'N1A1B1iE1D'})\n    check('function', 'A::B::E *A::B::j(A::B::E *)', \n          {1: 'A::B::j__A::B::E*', 2: 'N1A1B1jEP1E'})\n", "def test_function_pointer_to_member():\n    check('function', 'void f(int C::* pm)', {2: '1fM1Ci'})\n    check('function', 'void f(void (C::* pm)())', {2: '1fM1CFvvE'})\n    check('function', 'void f(void (C::* pm)(int, float))', {2: '1fM1CFifE'})\n    check('function', 'void f(int (C::* pm)[5])', {2: '1fM1CA5_i'})\n    check('function', 'int (C::* f())(int, float)', {2: '1fM1CiFifE'})\n    check('function', 'void f(int (C::* pm)() const)', {2: '1fM1CKiFvvE'})\n", "def test_duplicate_declaration():\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    parser1 = DefinitionParser(\"class A\", location=None, config=None)\n    parser1.allowFallbackExpressionParsing = False\n    ast1 = parser1.parse_declaration('class', 'class')\n    parser1.assert_end()\n    symbol1 = rootSymbol.add_declaration(ast1, docname=\"TestDoc1\")\n\n    parser2 = DefinitionParser(\"class A\", location=None, config=None)\n    parser2.allowFallbackExpressionParsing = False\n    ast2 = parser2.parse_declaration('class', 'class')\n    parser2.assert_end()\n\n    with pytest.raises(_DuplicateSymbolError):\n        rootSymbol.add_declaration(ast2, docname=\"TestDoc2\")\n", "def test_trailing_return_type_parsing():\n    check('function', 'auto f() -> int', {2: '1fv', 4: '1fiv'})\n    check('function', 'auto g() -> decltype(auto)', {2: '1gv', 4: '1gdv'})\n    check('function', 'auto h() -> std::vector<int>', {2: '1hv', 4: '1hNSt6vectorIiEEv'})\n    check('function', 'auto i() -> decltype(a + b)', {2: '1iv', 4: '1idtepl1a1bEv'})\n    check('function', 'auto j() -> decltype((a))', {2: '1jv', 4: '1jdv'})\n    check('function', 'auto k() -> decltype(N::T::f<int>())', {2: '1kv', 4: '1kdN1N1T1fIiEEv'})\n", "def test_template_specializations():\n    # Test template specializations for functions\n    check('function', 'template<> void A<int>::foo()', {2: 'IEN1AIiE3fooEv', 4: 'IEN1AIiE3fooEvv'})\n    check('function', 'template<> void A<int>::B<int>::bar()', {2: 'IEIEN1AIiE1BIiE3barEv', 4: 'IEIEN1AIiE1BIiE3barEvv'})\n    check('function', 'template<> void A<int>::B<int>::C<int>::baz()', {2: 'IEIEIEN1AIiE1BIiE1CIiE3bazEv', 4: 'IEIEIEN1AIiE1BIiE1CIiE3bazEvv'})\n    \n    # Test template specializations for classes\n    check('class', 'template<> class A<int>', {2: 'IEN1AIiE'}, output='template<> {key}A<int>')\n    check('class', 'template<> class A<int>::B<int>', {2: 'IEN1AIiE1BIiE'}, output='template<> {key}A<int>::B<int>')\n    check('class', 'template<> class A<int>::B<int>::C<int>', {2: 'IEIEN1AIiE1BIiE1CIiE'}, output='template<> {key}A<int>::B<int>::C<int>')\n    \n    # Test template specializations for members\n    check('member', 'template<> int A<int>::a', {2: 'IEN1AIiE1aE'})\n    check('member', 'template<> int A<int>::B<int>::b', {2: 'IEIEN1AIiE1BIiE1bE'})\n    check('member', 'template<> int A<int>::B<int>::C<int>::c', {2: 'IEIEIEN1AIiE1BIiE1CIiE1cE'})\n    \n    # Test template specializations for type aliases\n    check('type', 'template<> using A = B<int>', {2: 'IEN1AIiE'}, key='using')\n    check('type', 'template<> using A<int>::B = C<int>', {2: 'IEN1AIiE1BIiE'}, key='using')\n    check('type', '"], "sample_925": ["def test_mock_with_multiple_modules():\n    modnames = ['sphinx.unknown1', 'sphinx.unknown2']\n    submodule1 = modnames[0] + '.submodule'\n    submodule2 = modnames[1] + '.submodule'\n\n    for modname in modnames:\n        assert modname not in sys.modules\n        with pytest.raises(ImportError):\n            import_module(modname)\n\n    with mock(modnames):\n        for modname in modnames:\n            import_module(modname)\n            assert modname in sys.modules\n            assert isinstance(sys.modules[modname], _MockModule)\n\n        # submodules are also mocked\n        for submodule in [submodule1, submodule2]:\n            import_module(submodule)\n            assert submodule in sys.modules\n            assert isinstance(sys.modules[submodule], _MockModule)\n\n    for modname in modnames:\n        assert modname not in sys.modules\n        with pytest.raises(ImportError):\n            import_module(modname)\n", "def test_MockObject_magic_methods():\n    mock = _MockObject()\n\n    # __len__ method\n    assert len(mock) == 0\n\n    # __contains__ method\n    assert 'some_key' not in mock\n\n    # __iter__ method\n    assert list(iter(mock)) == []\n\n    # __getitem__ method\n    assert isinstance(mock['some_key'], _MockObject)\n\n    # __call__ method\n    result = mock('arg1', 'arg2')\n    assert result is mock\n\n        pass\n\n    # __call__ method when used as decorator\n    assert mock(sample_func) is sample_func\n\n    # __mro_entries__ method\n    assert mock.__mro_entries__(()) == (_MockObject,)\n", "def test_mock_context_manager():\n    modname = 'sphinx.mocked'\n    another_modname = 'sphinx.another_mocked'\n    assert modname not in sys.modules\n    assert another_modname not in sys.modules\n\n    with mock([modname, another_modname]):\n        import_module(modname)\n        import_module(another_modname)\n        assert modname in sys.modules\n        assert another_modname in sys.modules\n        assert isinstance(sys.modules[modname], _MockModule)\n        assert isinstance(sys.modules[another_modname], _MockModule)\n\n        # Ensure nested mocking works\n        with mock(['sphinx.nested_mocked']):\n            nested_modname = 'sphinx.nested_mocked'\n            import_module(nested_modname)\n            assert nested_modname in sys.modules\n            assert isinstance(sys.modules[nested_modname], _MockModule)\n\n        assert 'sphinx.nested_mocked' not in sys.modules\n\n    assert modname not in sys.modules\n    assert another_modname not in sys.modules\n    with pytest.raises(ImportError):\n        import_module(modname)\n    with pytest.raises(ImportError):\n        import_module(another_modname)\n", "def test_MockLoader():\n    modname = 'sphinx.mockedmodule'\n    finder = MockFinder([modname])\n    loader = MockLoader(finder)\n    \n    spec = loader.create_module(ModuleSpec(modname, loader))\n    assert isinstance(spec, _MockModule)\n    assert spec.__name__ == modname\n\n    module = _MockModule(modname)\n    loader.exec_module(module)  # should not raise any exception\n", "def test_MockLoader_and_MockFinder():\n    modname = 'sphinx.mockedmodule'\n    submodule = modname + '.submodule'\n\n    finder = MockFinder([modname])\n    assert finder.find_spec(modname, []) is not None\n    assert finder.find_spec(submodule, []) is not None\n    assert finder.find_spec('nonmockedmodule', []) is None\n\n    loader = MockLoader(finder)\n    spec = loader.create_module(ModuleSpec(modname, loader))\n    assert isinstance(spec, _MockModule)\n    assert spec.__name__ == modname\n    assert modname in finder.mocked_modules\n\n    # Ensure that exec_module does nothing\n    module = _MockModule(modname)\n    loader.exec_module(module)  # should not raise any exception\n"], "sample_926": ["def test_c_variable_definitions():\n    check('member', 'int var', {1: 'var__i', 2: '3var'})\n    check('member', 'const int var', {1: 'var__iC', 2: '3var'})\n    check('member', 'volatile int var', {1: 'var__iV', 2: '3var'})\n    check('member', 'const volatile int var', {1: 'var__iVC', 2: '3var'})\n    check('member', 'int *var', {1: 'var__iP', 2: '3var'})\n    check('member', 'int *const var', {1: 'var__iPC', 2: '3var'})\n    check('member', 'int *volatile const var', {1: 'var__iPVC', 2: '3var'})\n    check('member', 'int *volatile const *var', {1: 'var__iPVCP', 2: '3var'})\n    check('member', 'int &var', {1: 'var__iR', 2: '3var'})\n    check('member', 'int var[]', {1: 'var__iA', 2: '3var'})\n    check('member', 'int var[10]', {1: 'var__iA10', 2: '3var'})\n    check('member', 'std::vector<int> var', {1: 'var__std::vector:i:', 2: '3var'})\n    check('member', 'std::vector<int> var[10]', {1: 'var__std::vector:i:A10', 2: '3var'})\n    check('member', 'std::vector<int> *var', {1: 'var__std::vector:i:P', 2: '3var'})\n    check('member', 'std::vector<int> &var', {1: 'var__std::vector:i:R', 2: '3var'})\n", "def test_c_domain_symbol_lookup():\n    # Tests for the Symbol class and its lookup methods\n    root = Symbol(None, None, None, None)\n\n    # Add a nested name and check lookup\n    nested_name = ASTNestedName([ASTIdentifier('A'), ASTIdentifier('B')], rooted=False)\n    decl = ASTDeclaration('type', 'type', ASTTypeWithInit(ASTType(ASTDeclSpecs('type', ASTDeclSpecsSimple(None, None, False, False, False, False, []), ASTDeclSpecsSimple(None, None, False, False, False, False, []), ASTTrailingTypeSpecFundamental('int')), ASTDeclaratorNameParam(nested_name, [], None)), None), False)\n    symbol = root.add_declaration(decl, 'test_doc')\n\n    # Lookup by identifier\n    found_symbol = root.find_identifier(ASTIdentifier('A'), matchSelf=False, recurseInAnon=False, searchInSiblings=False)\n    assert found_symbol is not None\n    assert found_symbol.ident == ASTIdentifier('A')\n\n    # Nested lookup\n    found_nested_symbol = found_symbol.find_identifier(ASTIdentifier('B'), matchSelf=False, recurseInAnon=False, searchInSiblings=False)\n    assert found_nested_symbol is not None\n    assert found_nested_symbol.ident == ASTIdentifier('B')\n\n    # Lookup with fully qualified name\n    fully_qualified_name = ASTNestedName([ASTIdentifier('A'), ASTIdentifier('B')], rooted=True)\n    found_symbol = root.find_declaration(fully_qualified_name, 'type', matchSelf=True, recurseInAnon=False)\n    assert found_symbol is not None\n    assert found_symbol.ident == ASTIdentifier('B')\n", "def test_ast_identifier():\n    ident = ASTIdentifier(\"test_ident\")\n    assert ident.identifier == \"test_ident\"\n    assert str(ident) == \"test_ident\"\n    assert ident.get_display_string() == \"test_ident\"\n    assert not ident.is_anon()\n\n    ident_anon = ASTIdentifier(\"@anon_ident\")\n    assert ident_anon.is_anon()\n    assert ident_anon.get_display_string() == \"[anonymous]\"\n", "def test_c_basic_function_parsing():\n        parser = DefinitionParser(signature, location=None, config=None)\n        ast = parser.parse_declaration('function', 'function')\n        parser.assert_end()\n        assert str(ast) == expected_output\n\n    check_function_signature('void my_function(int a, float b)',\n                             'void my_function(int a, float b)')\n    check_function_signature('int compute_sum(int a, int b)',\n                             'int compute_sum(int a, int b)')\n    check_function_signature('double multiply(double x, double y)',\n                             'double multiply(double x, double y)')\n", "def test_c_domain_struct_signature():\n    text = \"struct my_struct { int a; float b; };\"\n    class Config:\n        c_id_attributes = [\"id_attr\"]\n        c_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser(text, location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    try:\n        ast = parser.parse_declaration(\"struct\", \"struct\")\n        parser.assert_end()\n        res = str(ast)\n        expected = \"my_struct\"\n        assert res == expected, f\"Expected: {expected}, but got: {res}\"\n    except DefinitionError as e:\n        pytest.fail(f\"Parsing failed with error: {e}\")\n"], "sample_927": ["def test_noexcept_exprs():\n    check(\"function\", \"void f() noexcept\", {1: \"f\", 2: \"1fv\"})\n    check(\"function\", \"void f() noexcept(true)\", {1: \"f\", 2: \"1fv\"})\n    check(\"function\", \"void f() noexcept(false)\", {1: \"f\", 2: \"1fv\"})\n    check(\"function\", \"void f() noexcept(1 + 1 == 2)\", {1: \"f\", 2: \"1fv\"})\n    check(\"function\", \"void f() noexcept(std::is_nothrow_copy_constructible<T>::value)\", \n          {1: \"f\", 2: \"1fv\"})\n", "def test_operator_overloads():\n    # Test different operator overloads with various signatures\n    check('function', 'bool operator==(const MyClass &rhs) const', \n          {1: \"eq-operator__MyClassCR\", 2: \"NK1eqlERK7MyClass\"})\n    check('function', 'bool operator!=(const MyClass &rhs) const', \n          {1: \"neq-operator__MyClassCR\", 2: \"NK1neqlERK7MyClass\"})\n    check('function', 'MyClass& operator=(const MyClass &rhs)', \n          {1: \"assign-operator__MyClassCR\", 2: \"1aSERK7MyClass\"})\n    check('function', 'MyClass operator+(const MyClass &rhs) const', \n          {1: \"add-operator__MyClassCR\", 2: \"NK1plERK7MyClass\"})\n    check('function', 'bool operator<(const MyClass &rhs) const', \n          {1: \"lt-operator__MyClassCR\", 2: \"NK1ltERK7MyClass\"})\n    check('function', 'bool operator>(const MyClass &rhs) const', \n          {1: \"gt-operator__MyClassCR\", 2: \"NK1gtERK7MyClass\"})\n    check('function', 'bool operator<=(const MyClass &rhs) const', \n          {1: \"lte-operator__MyClassCR\", 2: \"NK1leERK7MyClass\"})\n    check('function', 'bool operator>=(const MyClass &rhs) const', \n          {1: \"gte-operator__MyClassCR\", 2: \"NK1geERK7MyClass\"})\n", "def test_expression_parsing():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser(\"5 + 10 * (3 - 1)\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    ast = parser.parse_expression()\n    assert str(ast) == \"5 + 10 * (3 - 1)\"\n    assert ast.get_display_string() == \"5 + 10 * (3 - 1)\"\n\n    parser = DefinitionParser(\"std::vector<int>{1, 2, 3}\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    ast = parser.parse_expression()\n    assert str(ast) == \"std::vector<int>{1, 2, 3}\"\n    assert ast.get_display_string() == \"std::vector<int>{1, 2, 3}\"\n\n    parser = DefinitionParser(\"f(a, b, c)\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    ast = parser.parse_expression()\n    assert str(ast) == \"f(a, b, c)\"\n    assert ast.get_display_string() == \"f(a, b, c)\"\n\n    parser = DefinitionParser(\"a.b->c[2]\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    ast = parser.parse_expression()\n    assert str(ast) == \"a.b->c[2]\"\n    assert ast.get_display_string() == \"a.b->c[2]\"\n\n    parser = DefinitionParser(\"!a && b || c\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    ast = parser.parse_expression()\n    assert str(ast) == \"!a && b || c\"\n    assert ast.get_display_string() == \"!a && b || c\"\n", "def test_parse_string_literal():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(literal, location=None, config=Config())\n        return parser._parse_string()\n\n    assert parse_string_literal('\"hello\"') == '\"hello\"'\n    assert parse_string_literal('\"he\\\\\"llo\"') == '\"he\\\\\"llo\"'\n    assert parse_string_literal('\"he\\\\nllo\"') == '\"he\\\\nllo\"'\n    assert parse_string_literal('\"he\\\\tllo\"') == '\"he\\\\tllo\"'\n    assert parse_string_literal('\"he\\\\\\\\llo\"') == '\"he\\\\\\\\llo\"'\n\n    with pytest.raises(DefinitionError):\n        parse_string_literal('\"hello')\n", "def test_template_specializations():\n    check('function', 'template<> void A<int>::f()', {2: 'IEN1AIiE1fEv', 4: 'IEN1AIiE1fEvv'})\n    check('function', 'template<> template<> void A<int>::B<int>::f()', \n          {2: 'IEIEN1AIiE1BIiE1fEv', 4: 'IEIEN1AIiE1BIiE1fEvv'})\n    check('member', 'template<> int A<int>::a', {2: 'IEN1AIiE1aE'})\n    check('member', 'template int A<int>::a', {2: 'IEN1AIiE1aE'},\n          output='template<> int A<int>::a')  # same as above\n    check('member', 'template<> template<> int A<int>::B<int>::b', \n          {2: 'IEIEN1AIiE1BIiE1bE'})\n    check('member', 'template int A<int>::B<int>::b', {2: 'IEIEN1AIiE1BIiE1bE'},\n          output='template<> template<> int A<int>::B<int>::b')  # same as above\n"], "sample_928": ["def test_heading():\n    env = Environment()\n    env.language = 'en'\n    \n    # Test level 1 heading\n    assert heading(env, 'Heading Level 1', 1) == 'Heading Level 1\\n=============='\n    \n    # Test level 2 heading\n    assert heading(env, 'Heading Level 2', 2) == 'Heading Level 2\\n--------------'\n    \n    # Test level 3 heading\n    assert heading(env, 'Heading Level 3', 3) == 'Heading Level 3\\n~~~~~~~~~~~~~~'\n    \n    # Test assertion for invalid level\n    try:\n        heading(env, 'Heading Level 4', 4)\n    except AssertionError:\n        assert True\n    else:\n        assert False\n", "def test_heading():\n    env = Environment()\n    env.language = 'en'\n    assert heading(env, 'Title', 1) == 'Title\\n====='\n    assert heading(env, 'Section', 2) == 'Section\\n-------'\n    assert heading(env, 'Subsection', 3) == 'Subsection\\n~~~~~~~~~~'\n", "def test_heading():\n    env = Environment()\n    env.language = 'ja'\n\n    result = heading(env, 'Heading Level 1', level=1)\n    assert result == 'Heading Level 1\\n================'\n\n    result = heading(env, 'Heading Level 2', level=2)\n    assert result == 'Heading Level 2\\n----------------'\n\n    result = heading(env, 'Heading Level 3', level=3)\n    assert result == 'Heading Level 3\\n~~~~~~~~~~~~~~~~'\n    \n    # Test with different language setting\n    env.language = 'en'\n    result = heading(env, 'Heading Level 1', level=1)\n    assert result == 'Heading Level 1\\n==============='\n", "def test_heading():\n    env = Environment()\n    env.language = 'ja'\n    assert heading(env, 'Title', 1) == 'Title\\n====='\n    assert heading(env, 'Subtitle', 2) == 'Subtitle\\n--------'\n    assert heading(env, 'Section', 3) == 'Section\\n~~~~~~~'\n    env.language = 'en'\n    assert heading(env, 'Title', 1) == 'Title\\n====='\n    assert heading(env, 'Subtitle', 2) == 'Subtitle\\n--------'\n    assert heading(env, 'Section', 3) == 'Section\\n~~~~~~~'\n", "def test_heading():\n    env = Environment()\n    env.language = \"en\"\n\n    heading1 = heading(env, \"Heading Level 1\", 1)\n    assert heading1 == \"Heading Level 1\\n===============\"\n\n    heading2 = heading(env, \"Heading Level 2\", 2)\n    assert heading2 == \"Heading Level 2\\n--------------\"\n\n    heading3 = heading(env, \"Heading Level 3\", 3)\n    assert heading3 == \"Heading Level 3\\n~~~~~~~~~~~~~~\"\n\n    # Test with a wide character language\n    env.language = \"ja\"\n    heading1_ja = heading(env, \"\u898b\u51fa\u3057\u30ec\u30d9\u30eb1\", 1)\n    assert heading1_ja == \"\u898b\u51fa\u3057\u30ec\u30d9\u30eb1\\n\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\uff1d\"\n"], "sample_929": ["def test_type_to_xref():\n    env = Mock()\n    env.ref_context = {'py:module': 'mod', 'py:class': 'Cls'}\n    \n    xref_node = type_to_xref('int', env)\n    assert isinstance(xref_node, pending_xref)\n    assert_node(xref_node, refdomain='py', reftype='class', reftarget='int', **{'py:module': 'mod', 'py:class': 'Cls'})\n    \n    xref_node = type_to_xref('None', env)\n    assert isinstance(xref_node, pending_xref)\n    assert_node(xref_node, refdomain='py', reftype='obj', reftarget='None', **{'py:module': 'mod', 'py:class': 'Cls'})\n    \n    xref_node = type_to_xref('str')\n    assert isinstance(xref_node, pending_xref)\n    assert_node(xref_node, refdomain='py', reftype='class', reftarget='str')\n", "def test_type_to_xref():\n    env = Mock(ref_context={'py:module': 'mymodule', 'py:class': 'MyClass'})\n    node = type_to_xref('int', env)\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    assert_node(node[0], nodes.Text, text=\"int\")\n\n    node = type_to_xref('None', env)\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n    assert_node(node[0], nodes.Text, text=\"None\")\n\n    node = type_to_xref('MyType', env)\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"MyType\", **{'py:module': 'mymodule', 'py:class': 'MyClass'})\n    assert_node(node[0], nodes.Text, text=\"MyType\")\n\n    node = type_to_xref('MyModule.MyType', env)\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"MyModule.MyType\", **{'py:module': 'mymodule', 'py:class': 'MyClass'})\n    assert_node(node[0], nodes.Text, text=\"MyModule.MyType\")\n", "def test_pymodule_directive(app):\n    domain = app.env.get_domain('py')\n    text = \".. py:module:: example\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index))\n    assert 'example' in domain.modules\n    assert domain.modules['example'] == ('index', 'module-example', '', '', False)\n", "def test_pymodule(app):\n    text = (\".. py:module:: mymodule\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :synopsis: Example module\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index))\n    module_entry = domain.modules['mymodule']\n    assert module_entry.docname == 'index'\n    assert module_entry.synopsis == 'Example module'\n    assert module_entry.platform == 'Unix'\n    assert module_entry.deprecated is True\n", "def test_typeto_xref():\n    # Test with a 'None' type\n    env = Mock(ref_context={})\n    node = type_to_xref('None', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n\n    # Test with a class type\n    node = type_to_xref('int', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='int')\n\n    # Test with a nested type\n    env = Mock(ref_context={'py:module': 'mymodule', 'py:class': 'MyClass'})\n    node = type_to_xref('MyClass.InnerClass', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='MyClass.InnerClass')\n    assert node['py:module'] == 'mymodule'\n    assert node['py:class'] == 'MyClass'\n"], "sample_930": ["def test_create_index_with_invalid_entry(app):\n    text = (\".. index:: unknown: foo; bar\\n\"\n            \".. index:: pair: valid; entry\\n\")\n    restructuredtext.parse(app, text)\n    with pytest.warns(UserWarning, match=r'unknown index entry type'):\n        index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 1\n    assert index[0] == ('V', [('valid', [[], [('entry', [('', '#index-1')])], None])])\n", "def test_create_index_with_special_characters(app):\n    text = (\".. index:: single: foo\\n\"\n            \".. index:: single: foo (bar)\\n\"\n            \".. index:: single: foo (baz)\\n\"\n            \".. index:: single: foo (qux)\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    assert len(index) == 1\n    assert index[0] == ('F', [('foo', [[], [('(bar)', [('', '#index-1')]),\n                                            ('(baz)', [('', '#index-2')]),\n                                            ('(qux)', [('', '#index-3')])], None])])\n", "def test_create_mixed_index_entries(app):\n    text = (\".. index:: single: sphinx\\n\"\n            \".. index:: pair: Python; language\\n\"\n            \".. index:: triple: HTML; CSS; JavaScript\\n\"\n            \".. index:: see: docutils; documentation tool\\n\"\n            \".. index:: seealso: API; Application Programming Interface\\n\"\n            \".. index:: foo; bar\\n\"\n            \".. index:: foo; baz\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    \n    assert len(index) == 6\n    assert index[0] == ('A', [('API', [[], [('see also Application Programming Interface', [])], None])])\n    assert index[1] == ('D', [('docutils', [[], [('see documentation tool', [])], None])])\n    assert index[2] == ('F', [('foo', [[], [('bar', [('', '#index-5')]),\n                                             ('baz', [('', '#index-6')])], None])])\n    assert index[3] == ('H', [('HTML', [[], [('CSS JavaScript', [('', '#index-2')])], None])])\n    assert index[4] == ('J', [('JavaScript', [[], [('HTML CSS', [('', '#index-2')])], None])])\n    assert index[5] == ('P', [('Python', [[], [('language', [('', '#index-1')])], None]),\n                              ('sphinx', [[('', '#index-0')], [], None])])\n", "def test_create_index_with_category_key(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: single: Python\\n\"\n            \".. index:: single: pip\\n\"\n            \".. index:: single: Sphinx\\n\"\n            \".. index:: single: \u0415\u043b\u044c\\n\"\n            \".. index:: single: \u0451\u043b\u043a\u0430\\n\"\n            \".. index:: single: \u200f\u05ea\u05d9\u05e8\u05d1\u05e2\u200e\\n\"\n            \".. index:: single: 9-symbol\\n\"\n            \".. index:: single: &-symbol\\n\"\n            \".. index:: single: \u00a3100\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=False)\n    \n    assert len(index) == 6\n    assert index[0] == ('Symbols', [('&-symbol', [[('', '#index-9')], [], None]),\n                                    ('9-symbol', [[('', '#index-8')], [], None]),\n                                    ('\u00a3100', [[('', '#index-10')], [], None])])\n    assert index[1] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[2] == ('P', [('pip', [[('', '#index-2')], [], None]),\n                              ('Python', [[('', '#index-1')], [], None])])\n    assert index[3] == ('S', [('Sphinx', [[('', '#index-3')], [], None])])\n    assert index[4] == ('\u0415', [('\u0451\u043b\u043a\u0430', [[('', '#index-6')], [], None]),\n                              ('\u0415\u043b\u044c', [[('', '#index-5')], [], None])])\n    assert index[5] == ('\u05ea', [('\u200f\u05ea\u05d9\u05e8\u05d1\u05e2\u200e', [[('', '#index-7')], [], None])])\n", "def test_create_mixed_index(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: triple: foo; bar; baz\\n\"\n            \".. index:: see: Sphinx; documentation tool\\n\"\n            \".. index:: seealso: reST; markup\\n\"\n            \".. index:: !important; topic\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 7\n    assert index[0] == ('B', [('bar', [[], [('baz, foo', [('', '#index-2')])], None]),\n                              ('baz', [[], [('foo bar', [('', '#index-2')])], None])])\n    assert index[1] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[2] == ('F', [('foo', [[], [('bar baz', [('', '#index-2')])], None])])\n    assert index[3] == ('I', [('interpreter', [[], [('Python', [('', '#index-1')])], None])])\n    assert index[4] == ('P', [('Python', [[], [('interpreter', [('', '#index-1')])], None])])\n    assert index[5] == ('R', [('reST', [[], [('see also markup', [])], None])])\n    assert index[6] == ('S', [('Sphinx', [[], [('see documentation tool', [])], None])])\n"], "sample_931": ["def test_pyfunction_with_annotations(app):\n    text = (\".. py:function:: annotated_func(a: int, b: str = 'default') -> bool\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"annotated_func\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"bool\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"a\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"int\"])],\n                                      [desc_parameter, ([desc_sig_name, \"b\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"str\"],\n                                                        \" \",\n                                                        [desc_sig_operator, \"=\"],\n                                                        \" \",\n                                                        [nodes.inline, \"'default'\"])])])\n", "def test_type_to_xref(app):\n    from sphinx.environment import BuildEnvironment\n\n    env = BuildEnvironment(app.srcdir, app.confdir)\n    env.ref_context['py:module'] = 'mod'\n    env.ref_context['py:class'] = 'MyClass'\n\n    # Check for 'None' type\n    xref = type_to_xref('None', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n    assert xref[0].astext() == 'None'\n\n    # Check for a class type\n    xref = type_to_xref('int', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='int')\n    assert xref[0].astext() == 'int'\n\n    # Check for a type with module and class context\n    xref = type_to_xref('List[int]', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='List')\n    assert xref[0].astext() == 'List'\n", "def test_pymodule_directive(app, status, warning):\n    \"\"\"Test the PyModule directive.\"\"\"\n    text = (\".. py:module:: sphinx.domains.python\\n\"\n            \".. py:module:: sphinx.environment\\n\"\n            \".. py:module:: sphinx.directives\\n\")\n    restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    modules = domain.data['modules']\n\n    assert 'sphinx.domains.python' in modules\n    assert 'sphinx.environment' in modules\n    assert 'sphinx.directives' in modules\n\n    assert modules['sphinx.domains.python'][0] == 'index'\n    assert modules['sphinx.environment'][0] == 'index'\n    assert modules['sphinx.directives'][0] == 'index'\n", "def test_pyobject_handle_signature():\n    # Test handling signature for a function with prefix\n    env = Mock(ref_context={'py:module': 'module1', 'py:class': 'Class'})\n    directive = PyFunction('py:function', [], {'module': 'module1'}, {}, Mock(), env)\n    signode = desc_signature()\n    fullname, prefix = directive.handle_signature(\"func(a, b)\", signode)\n    assert fullname == \"Class.func\"\n    assert prefix == \"Class\"\n    assert_node(signode, ([desc_addname, \"Class.\"],\n                          [desc_name, \"func\"],\n                          [desc_parameterlist, ([desc_parameter, \"a\"],\n                                                [desc_parameter, \"b\"])]))\n\n    # Test handling signature for a class with no prefix\n    directive = PyClasslike('py:class', [], {'module': 'module2'}, {}, Mock(), env)\n    signode = desc_signature()\n    fullname, prefix = directive.handle_signature(\"MyClass\", signode)\n    assert fullname == \"MyClass\"\n    assert prefix == \"\"\n    assert_node(signode, ([desc_annotation, \"class \"],\n                          [desc_name, \"MyClass\"]))\n\n    # Test handling signature for a method with nested classes\n    env.ref_context['py:class'] = 'OuterClass.InnerClass'\n    directive = PyMethod('py:method', [], {'module': 'module3'}, {}, Mock(), env)\n    signode = desc_signature()\n    fullname, prefix = directive.handle_signature(\"method()\", signode)\n    assert fullname == \"OuterClass.InnerClass.method\"\n    assert prefix == \"OuterClass.InnerClass\"\n    assert_node(signode, ([desc_addname, \"OuterClass.InnerClass.\"],\n                          [desc_name, \"method\"],\n                          [desc_parameterlist, ()]))\n", "def test_parsing_module_directive():\n    text = (\".. py:module:: test_module\\n\"\n            \".. py:class:: TestClass\\n\"\n            \"\\n\"\n            \"   .. py:method:: test_method(param1: int, param2: str) -> None\\n\"\n            \"      :async:\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: test_attr\\n\"\n            \"      :type: float\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n\n    # Test module directive\n    assert 'test_module' in domain.modules\n    assert domain.modules['test_module'].docname == 'index'\n\n    # Test class directive\n    assert 'test_module.TestClass' in domain.objects\n    assert domain.objects['test_module.TestClass'][2] == 'class'\n\n    # Test method directive\n    assert 'test_module.TestClass.test_method' in domain.objects\n    assert domain.objects['test_module.TestClass.test_method'][2] == 'method'\n\n    # Test attribute directive\n    assert 'test_module.TestClass.test_attr' in domain.objects\n    assert domain.objects['test_module.TestClass.test_attr'][2] == 'attribute'\n"], "sample_932": ["def test_symbol_lookup():\n    # Test symbol lookup and merging\n\n    rootSymbol = Symbol(None, None, None, None, None, None)\n\n    # Creating some symbols\n    classA = ASTIdentifier(\"A\")\n    classB = ASTIdentifier(\"B\")\n    symbolA = Symbol(parent=rootSymbol, identOrOp=classA, templateParams=None, templateArgs=None, declaration=None, docname=\"docA\")\n    symbolB = Symbol(parent=rootSymbol, identOrOp=classB, templateParams=None, templateArgs=None, declaration=None, docname=\"docB\")\n\n    # Adding declarations\n    declA = parse('class', 'A')\n    declB = parse('class', 'B')\n    symbolA._fill_empty(declA, \"docA\")\n    symbolB._fill_empty(declB, \"docB\")\n\n    # Checking initial lookup\n    foundA = rootSymbol.find_identifier(ASTIdentifier(\"A\"), matchSelf=True, recurseInAnon=False, searchInSiblings=False)\n    foundB = rootSymbol.find_identifier(ASTIdentifier(\"B\"), matchSelf=True, recurseInAnon=False, searchInSiblings=False)\n    assert foundA is not None\n    assert foundB is not None\n    assert foundA.docname == \"docA\"\n    assert foundB.docname == \"docB\"\n\n    # Merging symbols\n    otherRootSymbol = Symbol(None, None, None, None, None, None)\n    symbolA_other = Symbol(parent=otherRootSymbol, identOrOp=classA, templateParams=None, templateArgs=None, declaration=None, docname=\"docA_other\")\n    symbolA_other._fill_empty(declA, \"docA_other\")\n\n    rootSymbol.merge_with(otherRootSymbol, [\"docA_other\"], None)\n\n    # Checking merged lookup\n    foundA_merged = rootSymbol.find_identifier(ASTIdentifier(\"A\"), matchSelf=True, recurseInAnon=False, searchInSiblings=False)\n    assert foundA_merged is not None\n    assert foundA_merged.docname == \"docA\"\n\n    # Test qualified name lookup\n    nestedName = ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"A\"), None)], [False], rooted=False)\n    foundQualified = rootSymbol.find_name(nestedName, [], 'class', templateShorthand=False, matchSelf=True, recurseInAnon=False, searchInSiblings=False)\n    assert foundQualified[0][0] is not None\n   ", "def test_template_function_specializations():\n    check('function', 'template<> void f<int>()', {2: 'IE1fIiEv'})\n    check('function', 'template<> void f<int, double>()', {2: 'IE1fIidEv'})\n    check('function', 'template<> void f<int, double, char>()', {2: 'IE1fIidcEv'})\n    check('function', 'template<> void f<int, double, char, long>()', {2: 'IE1fIidclEv'})\n    check('function', 'template<> void f<A<B<C>>>()', {2: 'IE1fI1AI1BI1C1CEEv'})\n    check('function', 'template<> void f<A<B<C>, D<E>>>()', {2: 'IE1fI1AI1BI1C1CE1DI1EEE'})\n    check('function', 'template<> void f<std::vector<int>, std::map<int, std::string>>()', \n          {2: 'IE1fINSt6vectorIiEEINSt3mapIiNSt6stringEEEEE'})\n", "def test_braced_init_list():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(string, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser._parse_braced_init_list()\n        res = str(ast)\n        return res\n\n    assert parse_braced_init('{}') == '{}'\n    assert parse_braced_init('{42}') == '{42}'\n    assert parse_braced_init('{42, 43}') == '{42, 43}'\n    assert parse_braced_init('{{1, 2}, {3, 4}}') == '{{1, 2}, {3, 4}}'\n    assert parse_braced_init('{{1, 2}, {3, 4},}') == '{{1, 2}, {3, 4},}'\n    assert parse_braced_init('{...}') == '{...}'\n", "def test_alias_transform():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    \n    env = Mock()\n    env.temp_data = {}\n    env.domaindata = {\"cpp\": {\"root_symbol\": Symbol(None, None, None, None, None, None)}}\n    env.config = Config()\n    \n    alias = AliasNode(\"f()\", env)\n    alias2 = AliasNode(\"void f()\", env)\n    alias3 = AliasNode(\"T f()\", env)\n    \n    transform = AliasTransform(env)\n    \n    node1 = nodes.paragraph()\n    node1 += alias\n    \n    node2 = nodes.paragraph()\n    node2 += alias2\n    \n    node3 = nodes.paragraph()\n    node3 += alias3\n    \n    document = new_document('test data')\n    document += node1\n    document += node2\n    document += node3\n    \n    transform.document = document\n    transform.apply()\n    \n    assert len(document.children[0]) == 1\n    assert len(document.children[1]) == 1\n    assert len(document.children[2]) == 1\n    \n    assert isinstance(document.children[0][0], addnodes.desc_signature)\n    assert isinstance(document.children[1][0], addnodes.desc_signature)\n    assert isinstance(document.children[2][0], addnodes.desc_signature)\n", "def test_template_specializations():\n    check('class', 'template<> {key}A<int>', {2: 'IE1AIiEE'})\n    check('class', 'template<int N> {key}A<N>', {2: 'I_iE1AINiEE'})\n    check('class', 'template<int N> {key}A<42>', {2: 'I_iE1AILi42EE'})\n    check('class', 'template<int N> {key}A<N + 1>', {2: 'I_iE1AIL1pNiEE'})\n    check('class', 'template<int N> {key}A<(N)>', {2: 'I_iE1AIL1NiEE'})\n    check('class', 'template<int ...Ns> {key}A<Ns...>', {2: 'I_DpiE1AIDpNiEE'})\n    check('function', 'template<> void f<int>()', {2: 'IE1fIiEEv', 4: 'IE1fIiEEvv'})\n    check('function', 'template<int N> void f<N>()', {2: 'I_iE1fILiNiEEv', 4: 'I_iE1fILiNiEEvv'})\n    check('function', 'template<int N> void f<(N)>()', {2: 'I_iE1fIL1NiEEv', 4: 'I_iE1fIL1NiEEvv'})\n    check('function', 'template<int ...Ns> void f<Ns...>()', {2: 'I_DpiE1fIDpNiEEv', 4: 'I_DpiE1fIDpNiEEvv'})\n    check('member', 'template<> int A<int>::a', {2: 'IEN1AIiE1aE'})\n    check('member', 'template<int N> int A<N>::a', {2: 'IEN1AINiE1aE'})\n    check('member', 'template<int N> int A<N + 1>::a', {2: 'IEN1AINL1pNiE1aE'})\n"], "sample_933": ["def test_gettext_should_write_function(app, tmpdir):\n    # Test should_write function by creating temporary files\n    filepath = tmpdir / 'temp.pot'\n    \n    # Initial content to write\n    content = POHEADER % {\n        'project': 'Test Project',\n        'version': '1.0',\n        'ctime': '2023-01-01 00:00+0000',\n    }\n    \n    # First write should return True because file does not exist\n    assert should_write(filepath, content) is True\n\n    # Write content to file\n    filepath.write_text(content)\n\n    # Second write with same content should return False\n    assert should_write(filepath, content) is False\n\n    # Modify content and check if should_write returns True\n    new_content = content.replace('Test Project', 'New Project')\n    assert should_write(filepath, new_content) is True\n", "def test_gettext_with_uuids(app):\n    # Ensure gettext builder generates UUIDs for messages when gettext_uuid is True\n    app.builder.build_all()\n\n    catalog_path = app.outdir / 'index_entries.pot'\n    assert catalog_path.isfile(), 'Expected .pot file not found'\n\n    catalog = catalog_path.read_text()\n    uuid_pattern = re.compile(r'#\\s*UUID:\\s*[\\da-fA-F-]+\\s*$', re.MULTILINE)\n\n    # Check that UUIDs are present in the catalog\n    uuids = uuid_pattern.findall(catalog)\n    assert len(uuids) > 0, \"Expected UUIDs not found in the .pot file\"\n\n    # Ensure that each msgid has a corresponding UUID\n    msgid_pattern = re.compile(r'msgid\\s*\".*\"$', re.MULTILINE)\n    msgids = msgid_pattern.findall(catalog)\n    assert len(msgids) == len(uuids), \"Mismatch between msgids and UUIDs count\"\n", "def test_gettext_should_write():\n    content_new = (\n        'msgid \"\"\\n'\n        'msgstr \"\"\\n'\n        '\"POT-Creation-Date: 2023-01-01 00:00+0000\\\\n\"\\n'\n        '\"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\\\n\"\\n'\n    )\n    \n    content_old = (\n        'msgid \"\"\\n'\n        'msgstr \"\"\\n'\n        '\"POT-Creation-Date: 2022-01-01 00:00+0000\\\\n\"\\n'\n        '\"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\\\n\"\\n'\n    )\n\n    # Create a temporary file for the old content\n    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding='utf-8') as tmp:\n        tmp.write(content_old)\n        tmp_filename = tmp.name\n\n    try:\n        # Test should_write function\n        assert should_write(tmp_filename, content_new) == True\n        assert should_write(tmp_filename, content_old) == False\n    finally:\n        os.remove(tmp_filename)\n", "def test_should_write_function(app):\n    app.builder.build_all()\n\n    # Prepare a test case where the content should be written\n    new_content = 'msgid \"New Message\"\\nmsgstr \"New Message Translation\"\\n'\n    filepath = app.outdir / 'test_should_write.pot'\n    if filepath.exists():\n        filepath.unlink()  # Ensure the file does not exist before testing\n\n    assert should_write(filepath, new_content) is True\n\n    # Simulate a case where the content should not be written\n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write(new_content)\n\n    assert should_write(filepath, new_content) is False\n\n    # Simulate a case where only the header changes and content should be written\n    modified_content = (\n        'msgid \"\"\\nmsgstr \"\"\\n\"POT-Creation-Date: 2023-10-01 00:00+0000\"\\n'\n        '\"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\"\\n'\n        'msgid \"New Message\"\\nmsgstr \"New Message Translation\"\\n'\n    )\n    assert should_write(filepath, modified_content) is True\n\n    # Simulate a case where the body changes and content should be written\n    modified_content = (\n        'msgid \"\"\\nmsgstr \"\"\\n\"POT-Creation-Date: 2023-10-01 00:00+0000\"\\n'\n        '\"PO-Revision-Date: 2023-10-01 00:00+0000\"\\n'\n        'msgid \"New Message\"\\nmsgstr \"Changed Message Translation\"\\n'\n    )\n    assert should_write(filepath, modified_content) is True\n", "def test_gettext_should_write(app, tempdir):\n    tempdir.ensure_dir(\"output\")\n    existing_file = tempdir / \"output\" / \"existing.pot\"\n    new_file = tempdir / \"output\" / \"new.pot\"\n\n    existing_content = POHEADER % {\n        'project': 'TestProject',\n        'version': '1.0',\n        'copyright': '2023 TestAuthor',\n        'ctime': datetime.now(ltz).strftime('%Y-%m-%d %H:%M%z')\n    }\n    new_content = POHEADER % {\n        'project': 'TestProject',\n        'version': '1.1',\n        'copyright': '2023 TestAuthor',\n        'ctime': datetime.now(ltz).strftime('%Y-%m-%d %H:%M%z')\n    }\n\n    existing_file.write_text(existing_content, encoding='utf-8')\n\n    assert not should_write(str(existing_file), existing_content)\n    assert should_write(str(existing_file), new_content)\n    assert should_write(str(new_file), new_content)\n"], "sample_934": ["def test_c_symbol_tree():\n    root = Symbol(None, None, None, None)\n    symbol_A = Symbol(root, ASTIdentifier('A'), None, None)\n    symbol_B = Symbol(symbol_A, ASTIdentifier('B'), None, None)\n    symbol_C = Symbol(symbol_A, ASTIdentifier('C'), None, None)\n\n    assert symbol_A.parent == root\n    assert symbol_B.parent == symbol_A\n    assert symbol_C.parent == symbol_A\n    assert symbol_B in symbol_A._children\n    assert symbol_C in symbol_A._children\n\n    symbol_A.remove()\n    assert symbol_A.parent is None\n    assert symbol_B.parent == symbol_A\n    assert symbol_C.parent == symbol_A\n    assert symbol_B in symbol_A._children\n    assert symbol_C in symbol_A._children\n", "def test_ast_identifier():\n    identifier1 = ASTIdentifier('test')\n    identifier2 = ASTIdentifier('test')\n    identifier3 = ASTIdentifier('different')\n\n    assert identifier1 == identifier2\n    assert identifier1 != identifier3\n    assert not identifier1.is_anon()\n    assert str(identifier1) == 'test'\n    assert identifier1.get_display_string() == 'test'\n\n    anonymous_identifier = ASTIdentifier('@anon')\n    assert anonymous_identifier.is_anon()\n    assert str(anonymous_identifier) == '@anon'\n    assert anonymous_identifier.get_display_string() == '[anonymous]'\n", "def test_ast_identifier():\n    id1 = ASTIdentifier('test')\n    id2 = ASTIdentifier('test')\n    id3 = ASTIdentifier('other')\n\n    assert id1 == id2\n    assert id1 != id3\n    assert id1.is_anon() is False\n    anon_id = ASTIdentifier('@anon')\n    assert anon_id.is_anon() is True\n\n    assert str(id1) == 'test'\n    assert id1.get_display_string() == 'test'\n    assert anon_id.get_display_string() == '[anonymous]'\n\n    signode = addnodes.desc_signature('', '')\n    env = None\n    symbol = None\n    id1.describe_signature(signode, 'markType', env, '', symbol)\n    assert_node(signode, [addnodes.pending_xref, '[anonymous]'])\n", "def test_c_fundamental_types():\n    _fundamental_types = [\n        'void', '_Bool', 'bool', 'char', 'int', 'float', 'double',\n        '__int64', 'signed char', 'unsigned char', 'short', 'unsigned short',\n        'long', 'unsigned long', 'long long', 'unsigned long long',\n        'wchar_t', 'char16_t', 'char32_t'\n    ]\n    for t in _fundamental_types:\n            id = t.replace(\" \", \"-\").replace(\"long\", \"l\").replace(\"int\", \"i\")\n            id = id.replace(\"bool\", \"b\").replace(\"char\", \"c\")\n            id = id.replace(\"wc_t\", \"wchar_t\").replace(\"c16_t\", \"char16_t\")\n            id = id.replace(\"c32_t\", \"char32_t\")\n            return \"f__%s\" % id\n\n            id = t.replace(\" \", \"\")\n            return \"1f%s\" % id\n        check(\"function\", \"void f(%s arg)\" % t, {1: makeIdV1(), 2: makeIdV2()})\n", "def test_c_alias_transform():\n    from docutils import nodes\n    from sphinx.transforms.post_transforms import ReferencesResolver\n    from sphinx.util.nodes import make_refnode\n    from sphinx.application import Sphinx\n\n    class DummyEnv:\n        config = {\n            'extensions': ['sphinx.ext.autodoc'],\n            'autodoc_default_options': {},\n        }\n        temp_data = {}\n        ref_context = {}\n        domains = {\n            'c': CDomain(env=None)\n        }\n        docname = 'dummy_doc'\n        filename = 'dummy.rst'\n    \n    class DummyBuilder:\n            return \"dummy_uri\"\n    \n    class DummyDocument:\n        settings = {\n            'env': DummyEnv()\n        }\n\n    alias = AliasNode(\"int foo\", env=DummyEnv())\n    alias.document = DummyDocument()\n    alias.source = \"dummy_source\"\n    \n    transform = AliasTransform(alias.document)\n    transform.apply()\n    \n    assert isinstance(alias[0], addnodes.desc_signature)\n    assert alias[0][0].astext() == 'int foo'\n    \n    alias = AliasNode(\"void bar()\", env=DummyEnv())\n    alias.document = DummyDocument()\n    alias.source = \"dummy_source\"\n    \n    transform = AliasTransform(alias.document)\n    transform.apply()\n    \n    assert isinstance(alias[0], addnodes.desc_signature)\n    assert alias[0][0].astext() == 'void bar()'\n"], "sample_935": ["def test_template_specializations():\n    # Test explicit specializations for various C++ constructs\n    check('function', 'template<> void A<int>::f()', {2: 'IE1AINiE1fEv', 4: 'IE1AINiE1fEvv'})\n    check('function', 'template<> int A<int>::B<int>::g()', {2: 'IEIEN1AINiE1BIiE1gEv', 4: 'IEIEN1AINiE1BIiE1gEvv'})\n    check('class', 'template<> {key}A<int>', {2: 'IE1AINiEE', 4: 'IE1AINiEE'})\n    check('class', 'template<> struct A<int>', {2: 'IE1AINiEE', 4: 'IE1AINiEE'}, output='template<> {key}A<int>')\n    check('member', 'template<> int A<int>::a', {2: 'IEN1AINiE1aE', 4: 'IEN1AINiE1aE'})\n    check('type', 'template<> using A<int>::type = B<int>', {2: 'IEN1AINiE4typeE', 4: 'IEN1AINiE4typeE'}, key='using')\n    check('enum', 'template<> enum A<int>', {2: 'IE1AINiEE', 4: 'IE1AINiEE'}, output='template<> {key}A<int>')\n    check('enumerator', 'template<> enum A<int> {key}B', {2: 'IE1AINiE1BE', 4: 'IE1AINiE1BE'})\n", "def test_template_with_constraints(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"template_with_constraints\")\n    assert len(ws) == 0\n\n        match = re.search(r'<div class=\"cpp-class-signature\">(.+?)</div>', text, re.DOTALL)\n        assert match, \"Expected cpp-class-signature not found\"\n        return match.group(1).strip()\n\n    f = 'template_with_constraints.html'\n    t = (app.outdir / f).read_text()\n    \n    signature = extract_signature(t)\n    expected_signature = 'template&lt;typename T&gt; requires Concept&lt;T&gt; class ConstrainedClass'\n    \n    assert signature == expected_signature, f\"Expected '{expected_signature}' but found '{signature}'\"\n\n    ws = filter_warnings(warning, \"template_with_constraints\")\n    assert len(ws) == 0\n", "def test_template_alias():\n    check('type', 'template<typename T> {key}alias = T', {2: 'I0E5alias'}, key='using')\n    check('type', 'template<typename T> {key}A<T>::alias = T', {2: 'I0E1AI1TE5alias'}, key='using')\n    check('type', 'template<typename T, typename U> {key}alias = std::pair<T, U>', {2: 'I00E5alias'}, key='using')\n    check('type', 'template<typename T, typename U> {key}A<T>::alias = std::pair<T, U>', {2: 'I00E1AI1TE5alias'}, key='using')\n", "def test_namespace_definitions():\n    check('namespace', 'NS', {2: '2NS'}, output='{key}NS')\n    check('namespace', 'NS::SubNS', {2: 'N2NS5SubNSE'}, output='{key}NS::SubNS')\n\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n\n    parser = DefinitionParser('NS', location=None, config=Config())\n    ast = parser.parse_namespace_object()\n    assert str(ast.nestedName) == 'NS'\n    assert ast.templatePrefix is None\n\n    parser = DefinitionParser('NS::SubNS', location=None, config=Config())\n    ast = parser.parse_namespace_object()\n    assert str(ast.nestedName) == 'NS::SubNS'\n    assert ast.templatePrefix is None\n\n    parser = DefinitionParser('template<> NS::SubNS', location=None, config=Config())\n    ast = parser.parse_namespace_object()\n    assert str(ast.nestedName) == 'NS::SubNS'\n    assert ast.templatePrefix is not None\n", "def test_duplicate_symbol_error():\n    with pytest.raises(cppDomain._DuplicateSymbolError) as excinfo:\n        root = Symbol(None, None, None, None, None, None)\n        decl1 = parse('class', 'class Test {}')\n        symbol1 = root.add_declaration(decl1, docname=\"doc1\")\n        decl2 = parse('class', 'class Test {}')\n        symbol2 = root.add_declaration(decl2, docname=\"doc2\")\n    assert str(excinfo.value.symbol) == symbol1\n    assert str(excinfo.value.declaration) == decl2\n"], "sample_936": ["def test_stringify_broken_type():\n    assert stringify(BrokenType) == \"test_util_typing.BrokenType\"\n", "def test_stringify_broken_type():\n    broken_type_instance = BrokenType()\n    assert stringify(broken_type_instance) == \"test_util_typing.BrokenType\"\n", "def test_stringify_broken_type():\n    assert stringify(BrokenType) == \"test_util_typing.BrokenType\"\n", "def test_stringify_broken_type():\n    assert stringify(BrokenType) == \"test_util_typing.BrokenType\"\n", "def test_stringify_forward_references():\n    ForwardRef = typing.ForwardRef if sys.version_info >= (3, 7) else _ForwardRef\n\n    assert stringify(ForwardRef('int')) == \"int\"\n    assert stringify(ForwardRef('List[int]')) == \"List[int]\"\n    assert stringify(ForwardRef('Dict[str, float]')) == \"Dict[str, float]\"\n    assert stringify(ForwardRef('Tuple[str, ...]')) == \"Tuple[str, ...]\"\n"], "sample_937": ["def test_unparse_function_def():\n    source = \"\"\"", "def test_unparse_FunctionDef():\n    source = \"\"\"", "def test_unparse_function_def():\n    source = \"def func(a, b=2, *args, c, d=4, **kwargs): pass\"\n    expected = \"def func(a, b=2, *args, c, d=4, **kwargs): pass\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_parse_with_type_comments():\n    source = '''", "def test_parse_with_type_comments():\n    source = \"a: int = 1  # type: int\"\n    module = ast.parse(source)\n    assert isinstance(module, ast.Module)\n    assert len(module.body) == 1\n    assign = module.body[0]\n    assert isinstance(assign, ast.Assign)\n    assert assign.targets[0].id == 'a'\n    assert isinstance(assign.value, ast.Constant)\n    assert assign.value.value == 1\n    assert assign.type_comment == 'int'\n"], "sample_938": ["def test_rubric(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert ('.sp\\n'\n            '.RS 0\\n'\n            '.B Rubric Title\\n'\n            '.RE\\n' in content)\n", "def test_no_man_pages_config(app, status, warning):\n    app.builder.build_all()\n    assert 'no \"man_pages\" config value found; no manual pages will be written' in warning.getvalue()\n", "def test_custom_man_page_config(app, status, warning):\n    app.builder.build_all()\n    assert (app.outdir / 'custom.5').exists()\n\n    content = (app.outdir / 'custom.5').read_text()\n    assert r'Custom Project' in content\n    assert r'Custom Author' in content\n", "def test_warning_for_unknown_document(app, status, warning):\n    app.config.man_pages = [('unknown_doc', 'title', 'description', 'author', 1)]\n    app.builder.build_all()\n\n    assert 'WARNING: \"man_pages\" config value references unknown document unknown_doc' in warning.getvalue()\n", "def test_no_man_pages_config(app, status, warning):\n    app.config.man_pages = []\n    app.builder.init()\n    assert \"no \\\"man_pages\\\" config value found; no manual pages will be written\" in warning.getvalue()\n"], "sample_939": ["def test_unparse_generic_visit():\n    class CustomNode(ast.AST):\n        _fields = ()\n\n    custom_node = CustomNode()\n    with pytest.raises(NotImplementedError, match='Unable to parse CustomNode object'):\n        ast.unparse(custom_node)\n", "def test_unparse_function_def():\n    source = \"def foo(x, y=1, *args, z, **kwargs): pass\"\n    expected = \"def foo(x, y=1, *args, z, **kwargs): pass\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_parse_with_invalid_code():\n    invalid_code = \"def incomplete_function(\"\n    with pytest.raises(SyntaxError):\n        ast.parse(invalid_code)\n", "def test_unparse_with_arguments():\n    source = \"def func(a, b=2, *args, c, d=4, **kwargs): pass\"\n    expected = \"def func(a, b=2, *args, c, d=4, **kwargs): pass\"\n    module = ast.parse(source)\n    func_def = module.body[0]\n    assert ast.unparse(func_def.args, source) == \"a, b=2, *args, c, d=4, **kwargs\"\n", "compilation error"], "sample_940": ["def test_isenumclass():\n    class MyEnum(enum.Enum):\n        ONE = 1\n        TWO = 2\n\n    class NotEnum:\n        pass\n\n    assert inspect.isenumclass(MyEnum) is True\n    assert inspect.isenumclass(NotEnum) is False\n    assert inspect.isenumclass(1) is False\n", "def test_evaluate_signature():\n    from typing import List, Dict, Any\n\n    sig = inspect.signature(lambda x: None)\n    evaluated_sig = inspect.evaluate_signature(sig, globalns={\"List\": List, \"Dict\": Dict, \"Any\": Any})\n    assert isinstance(evaluated_sig, inspect.Signature)\n    assert evaluated_sig.parameters == sig.parameters\n    assert evaluated_sig.return_annotation == sig.return_annotation\n\n        pass\n\n    sig = inspect.signature(func)\n    evaluated_sig = inspect.evaluate_signature(sig, globalns={\"List\": List, \"Dict\": Dict, \"Any\": Any})\n    assert stringify_signature(evaluated_sig) == \"(x: List[int], y: Dict[str, Any]) -> Any\"\n", "def test_ispartial():\n        return a + b\n\n    partial_func = functools.partial(foo, 1)\n    assert inspect.ispartial(partial_func) is True\n\n    non_partial_func = foo\n    assert inspect.ispartial(non_partial_func) is False\n\n    class Foo:\n        @functools.partialmethod\n            return a + b\n\n    assert inspect.ispartial(Foo().meth) is True\n    assert inspect.ispartial(Foo.meth) is True\n", "def test_ispartial():\n        pass\n\n    p_func = functools.partial(func, 1)\n    assert inspect.ispartial(p_func) is True\n    assert inspect.ispartial(func) is False\n\n    class Foo:\n            pass\n\n    p_method = functools.partial(Foo().method, 1)\n    assert inspect.ispartial(p_method) is True\n    assert inspect.ispartial(Foo().method) is False\n", "def test_signature_from_str_varargs_kwargs():\n    signature = '(a, b, *args, **kwargs)'\n    sig = inspect.signature_from_str(signature)\n    assert list(sig.parameters.keys()) == ['a', 'b', 'args', 'kwargs']\n    assert sig.parameters['a'].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters['b'].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters['args'].kind == Parameter.VAR_POSITIONAL\n    assert sig.parameters['kwargs'].kind == Parameter.VAR_KEYWORD\n"], "sample_941": ["def test_stringify_forward_reference():\n    from typing import ForwardRef\n    forward_ref = ForwardRef(\"MyClass1\")\n    assert stringify(forward_ref) == \"MyClass1\"\n", "def test_restify_ellipsis():\n    assert restify(Ellipsis) == \"...\"\n", "def test_restify_forward_reference():\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef\n    else:\n        from sphinx.util.typing import ForwardRef\n\n    assert restify(ForwardRef(\"MyClass1\")) == \":class:`MyClass1`\"\n    assert restify(ForwardRef(\"MyClass2\")) == \":class:`MyClass2`\"\n", "def test_stringify_forward_reference():\n    from typing import ForwardRef  # type: ignore\n    forward_ref_type = ForwardRef(\"MyClass1\")\n    assert stringify(forward_ref_type) == \"MyClass1\"\n", "def test_get_type_hints():\n        pass\n\n    class MyClass:\n        x: int\n        y: str\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': None}\n    assert get_type_hints(MyClass) == {'x': int, 'y': str}\n"], "sample_942": ["def test_type_to_xref():\n    env = Mock()\n    env.ref_context = {'py:module': 'module', 'py:class': 'Class'}\n\n    # Test with 'None'\n    xref_node = type_to_xref('None', env)\n    assert_node(xref_node, [nodes.Text, \"None\"])\n    assert xref_node['reftype'] == 'obj'\n    assert xref_node['py:module'] == 'module'\n    assert xref_node['py:class'] == 'Class'\n    \n    # Test with a class\n    xref_node = type_to_xref('str', env)\n    assert_node(xref_node, [nodes.Text, \"str\"])\n    assert xref_node['reftype'] == 'class'\n    assert xref_node['py:module'] == 'module'\n    assert xref_node['py:class'] == 'Class'\n\n    # Test without env\n    xref_node = type_to_xref('int')\n    assert_node(xref_node, [nodes.Text, \"int\"])\n    assert xref_node['reftype'] == 'class'\n    assert 'py:module' not in xref_node\n    assert 'py:class' not in xref_node\n", "def test_pymodule_directive(app):\n    text = (\".. py:module:: example.module\\n\"\n            \".. py:function:: example_function\\n\"\n            \".. py:class:: ExampleClass\\n\"\n            \".. py:exception:: ExampleException\\n\"\n            \".. py:attribute:: example_attribute\\n\"\n            \".. py:data:: example_data\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    \n    # Check that the module is correctly registered\n    assert 'example.module' in domain.modules\n    assert domain.modules['example.module'].docname == 'index'\n    \n    # Check that other objects are correctly registered under the module\n    assert 'example.module.example_function' in domain.objects\n    assert domain.objects['example.module.example_function'] == ('index', 'example.module.example_function', 'function', False)\n    \n    assert 'example.module.ExampleClass' in domain.objects\n    assert domain.objects['example.module.ExampleClass'] == ('index', 'example.module.ExampleClass', 'class', False)\n    \n    assert 'example.module.ExampleException' in domain.objects\n    assert domain.objects['example.module.ExampleException'] == ('index', 'example.module.ExampleException', 'exception', False)\n    \n    assert 'example.module.example_attribute' in domain.objects\n    assert domain.objects['example.module.example_attribute'] == ('index', 'example.module.example_attribute', 'attribute', False)\n    \n    assert 'example.module.example_data' in domain.objects\n    assert domain.objects['example.module.example_data'] == ('index', 'example.module.example_data', 'data', False)\n    \n    # Verify that the restructured text is parsed correctly\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_addname, \"example.module.\"],\n                                                    [desc_name, \"ExampleClass\"])],\n                                  desc_content)]))\n", "def test_pydecorator_function_signature(app):\n    text = (\".. py:decorator:: decorator_func\\n\"\n            \".. py:decorator:: another_decorator\\n\"\n            \"   :async:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_name, \"decorator_func\"])],\n                                  desc_content)],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"async \"],\n                                                    [desc_addname, \"@\"],\n                                                    [desc_name, \"another_decorator\"])],\n                                  desc_content)]))\n    \n    assert 'decorator_func' in domain.objects\n    assert domain.objects['decorator_func'] == ('index', 'decorator_func', 'function', False)\n    assert 'another_decorator' in domain.objects\n    assert domain.objects['another_decorator'] == ('index', 'another_decorator', 'function', False)\n", "def test_pymodule_directive():\n    text = (\".. py:module:: example\\n\"\n            \".. py:function:: func1\\n\"\n            \".. py:data:: data1\\n\"\n            \".. py:class:: Class1\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"example.\"],\n                                                    [desc_name, \"func1\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])],\n                          nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"example.\"],\n                                                    [desc_name, \"data1\"])],\n                                  [desc_content, ()])],\n                          nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_addname, \"example.\"],\n                                                    [desc_name, \"Class1\"])],\n                                  [desc_content, ()])]))\n\n    assert 'example.func1' in domain.objects\n    assert domain.objects['example.func1'] == ('index', 'example.func1', 'function', False)\n    assert 'example.data1' in domain.objects\n    assert domain.objects['example.data1'] == ('index', 'example.data1', 'data', False)\n    assert 'example.Class1' in domain.objects\n    assert domain.objects['example.Class1'] == ('index', 'example.Class1', 'class', False)\n", "def test_type_to_xref():\n    env = Mock()\n    env.ref_context = {'py:module': 'mymodule', 'py:class': 'MyClass'}\n\n    # Test with None type\n    xref_node = type_to_xref('None', env)\n    assert isinstance(xref_node, pending_xref)\n    assert xref_node['reftype'] == 'obj'\n    assert xref_node['refdomain'] == 'py'\n    assert xref_node['reftarget'] == 'None'\n    assert xref_node['py:module'] == 'mymodule'\n    assert xref_node['py:class'] == 'MyClass'\n    assert xref_node[0].astext() == 'None'\n\n    # Test with a class type\n    xref_node = type_to_xref('int', env)\n    assert isinstance(xref_node, pending_xref)\n    assert xref_node['reftype'] == 'class'\n    assert xref_node['refdomain'] == 'py'\n    assert xref_node['reftarget'] == 'int'\n    assert xref_node['py:module'] == 'mymodule'\n    assert xref_node['py:class'] == 'MyClass'\n    assert xref_node[0].astext() == 'int'\n\n    # Test without environment\n    xref_node = type_to_xref('str')\n    assert isinstance(xref_node, pending_xref)\n    assert xref_node['reftype'] == 'class'\n    assert xref_node['refdomain'] == 'py'\n    assert xref_node['reftarget'] == 'str'\n    assert xref_node[0].astext() == 'str'\n"], "sample_943": ["def test_missing_subpackage_in_toc(apidoc):\n    \"\"\"Ensure subpackage is skipped if excluded, and ensure TOC consistency.\"\"\"\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.rst').isfile()\n    assert (outdir / 'a.b.rst').isfile()\n    assert not (outdir / 'a.b.c.rst').isfile()  # skipped because of exclude\n    assert (outdir / 'a.b.e.rst').isfile()\n\n    toc = extract_toc(outdir / 'a.rst')\n    assert 'a.b' in toc\n    assert 'a.b.e' in toc\n    assert 'a.b.c' not in toc  # ensure excluded subpackage is not in TOC\n", "def test_inheritance_option(make_app, apidoc):\n    \"\"\"Test to ensure that the '--inheritance' option is respected.\"\"\"\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'inheritance.rst').isfile()\n\n    with open(outdir / 'inheritance.rst') as f:\n        rst = f.read()\n        assert \":show-inheritance:\" in rst\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n\n    builddir = outdir / '_build' / 'text'\n    assert (builddir / 'inheritance.txt').isfile()\n\n    with open(builddir / 'inheritance.txt') as f:\n        txt = f.read()\n        assert \":show-inheritance:\" in txt\n", "def test_follow_links(make_app, apidoc, tempdir):\n    \"\"\"\n    Test the --follow-links option to ensure symbolic links are followed.\n    \"\"\"\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'index.rst').isfile()\n    \n    # Create a symbolic link\n    (tempdir / 'linked').makedirs()\n    (tempdir / 'linked' / 'linked_example.py').write_text('')\n    os.symlink(tempdir / 'linked', tempdir / 'symlink_to_linked')\n    \n    apidoc_main(['--follow-links', '-o', outdir, tempdir / 'symlink_to_linked'])\n    \n    assert (outdir / 'symlink_to_linked.rst').isfile()\n    assert (outdir / 'symlink_to_linked.linked_example.rst').isfile()\n    \n    with open(outdir / 'symlink_to_linked.linked_example.rst') as f:\n        rst = f.read()\n        assert \"automodule:: symlink_to_linked.linked_example\" in rst\n    \n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n", "def test_excludes_with_implicit_namespaces(make_app, apidoc):\n    \"\"\"Test excludes with implicit namespaces enabled.\"\"\"\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.b.c.rst').isfile()\n    assert (outdir / 'a.b.e.rst').isfile()\n    assert (outdir / 'a.b.x.rst').isfile()  # 'b/x/__init__.py' exists\n    assert not (outdir / 'a.b.x.y.rst').isfile()  # excluded\n\n    with open(outdir / 'a.b.c.rst') as f:\n        rst = f.read()\n        assert \"automodule:: a.b.c.d\\n\" in rst\n        assert \"automodule:: a.b.c\\n\" in rst\n\n    with open(outdir / 'a.b.e.rst') as f:\n        rst = f.read()\n        assert \"automodule:: a.b.e.f\\n\" in rst\n\n    with open(outdir / 'a.b.x.rst') as f:\n        rst = f.read()\n        assert \"automodule:: a.b.x\\n\" in rst\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n\n    builddir = outdir / '_build' / 'text'\n    assert (builddir / 'a.b.c.txt').isfile()\n    assert (builddir / 'a.b.e.txt').isfile()\n    assert (builddir / 'a.b.x.txt').isfile()\n\n    with open(builddir / 'a.b.c.txt') as f:\n        txt = f.read()\n        assert \"a.b.c package\\n\" in txt\n\n    with open(builddir / 'a.b.e.txt') as f:\n        txt = f.read()\n        assert \"a.b.e.f module\\n\" in txt\n\n    with open(builddir / 'a.b.x.txt') as f:\n        txt = f.read()\n        assert \"a.b.x namespace\\n\" in txt\n", "def test_exclude_private_modules(make_app, apidoc):\n    \"\"\"Test to ensure private modules are excluded unless --private option is specified\"\"\"\n    outdir = apidoc.outdir\n\n    # Check that the files are created\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'public_module.rst').isfile()\n    assert (outdir / '_private_module.rst').isfile()\n\n    # Without --private option\n    apidoc_main(['-o', str(outdir), str(apidoc.coderoot)])\n    assert (outdir / 'public_module.rst').isfile()\n    assert not (outdir / '_private_module.rst').exists()\n\n    # With --private option\n    apidoc_main(['--private', '-f', '-o', str(outdir), str(apidoc.coderoot)])\n    assert (outdir / 'public_module.rst').isfile()\n    assert (outdir / '_private_module.rst').exists()\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n\n    builddir = outdir / '_build' / 'text'\n    assert (builddir / 'public_module.txt').isfile()\n    assert (builddir / '_private_module.txt').isfile()\n"], "sample_944": ["def test_get_type_hints():\n        pass\n\n    class MyClass:\n        attr: int\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(MyClass) == {'attr': int}\n\n    # Testing with invalid type hints\n        pass\n\n    assert get_type_hints(invalid_func) == {'a': 'NonExistentType', 'return': None}\n\n    # Testing with ForwardRef\n    class MyClassWithForwardRef:\n            pass\n\n    hints = get_type_hints(MyClassWithForwardRef.method, {'MyClassWithForwardRef': MyClassWithForwardRef})\n    assert hints == {'param': MyClassWithForwardRef, 'return': MyClassWithForwardRef}\n", "def test_get_type_hints():\n        return True\n\n    class TestClass:\n        attr: int\n\n            return str(a)\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(TestClass) == {'attr': int}\n    assert get_type_hints(TestClass.method) == {'a': float, 'return': str}\n", "def test_get_type_hints():\n        return True\n\n    class SampleClass:\n            return {}\n\n    # Test with a simple function\n    assert get_type_hints(sample_function) == {'a': int, 'b': str, 'return': bool}\n\n    # Test with a class method\n    assert get_type_hints(SampleClass.method, {'List': List, 'Dict': Dict}) == {'x': List[int], 'return': Dict[str, int]}\n\n    # Test with a class itself\n    assert get_type_hints(SampleClass) == {}\n\n    # Test with global and local namespaces\n    globalns = {'List': List, 'Dict': Dict}\n    localns = {'int': str, 'str': int}\n    assert get_type_hints(SampleClass.method, globalns, localns) == {'x': List[int], 'return': Dict[str, int]}\n", "def test_stringify_type_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n", "def test_restify_invalid_builtin_classes():\n    assert restify(Struct) == \":class:`struct.Struct`\"\n    assert restify(TracebackType) == \":class:`types.TracebackType`\"\n"], "sample_945": ["def test_parse_arglist_with_keywords_and_varargs():\n    \"\"\"Test _parse_arglist with a mix of keywords, varargs, and kwargs.\"\"\"\n    from sphinx.domains.python import _parse_arglist\n\n    env = Mock()\n    arglist = 'a, b=1, *args, c, **kwargs'\n    paramlist = _parse_arglist(arglist, env)\n\n    assert_node(paramlist, addnodes.desc_parameterlist)\n    assert_node(paramlist[0], addnodes.desc_parameter, ([addnodes.desc_sig_name, \"a\"]))\n    assert_node(paramlist[1], addnodes.desc_parameter, ([addnodes.desc_sig_name, \"b\"],\n                                                        [addnodes.desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"1\"]))\n    assert_node(paramlist[2], addnodes.desc_parameter, ([addnodes.desc_sig_operator, \"*\"],\n                                                        [addnodes.desc_sig_name, \"args\"]))\n    assert_node(paramlist[3], addnodes.desc_parameter, ([addnodes.desc_sig_name, \"c\"]))\n    assert_node(paramlist[4], addnodes.desc_parameter, ([addnodes.desc_sig_operator, \"**\"],\n                                                        [addnodes.desc_sig_name, \"kwargs\"]))\n", "def test_type_to_xref():\n    env = Mock()\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = False\n    env.ref_context = {'py:module': 'test_module', 'py:class': 'TestClass'}\n\n    # Test with \"None\" type\n    node = type_to_xref('None', env)\n    assert_node(node, [pending_xref_condition, 'None'])\n    assert node['reftype'] == 'obj'\n\n    # Test with a regular class type\n    node = type_to_xref('int', env)\n    assert_node(node, [nodes.Text, 'int'])\n    assert node['reftype'] == 'class'\n    assert node['refdomain'] == 'py'\n    assert node['reftarget'] == 'int'\n    assert node['py:module'] == 'test_module'\n    assert node['py:class'] == 'TestClass'\n\n    # Test with unqualified type names enabled\n    env.config.python_use_unqualified_type_names = True\n    node = type_to_xref('test_module.TestClass.InnerClass', env)\n    assert_node(node, [pending_xref_condition, 'InnerClass'])\n    assert node['reftype'] == 'class'\n    assert node['refdomain'] == 'py'\n    assert node['reftarget'] == 'test_module.TestClass.InnerClass'\n    assert node['py:module'] == 'test_module'\n    assert node['py:class'] == 'TestClass'\n", "def test_pymodule_with_options(app):\n    text = (\".. py:module:: module_a.submodule\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :synopsis: This is a test module\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, ([nodes.target, {'ids': ['module-module_a.submodule']}],\n                          addnodes.index))\n    assert 'module_a.submodule' in domain.modules\n    assert domain.modules['module_a.submodule'] == ('index', 'module-module_a.submodule', \n                                                    'This is a test module', 'Unix', True)\n", "def test_type_to_xref():\n    env = Mock()\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = False\n    env.ref_context = {'py:module': 'example_module', 'py:class': 'ExampleClass'}\n    \n    # Test with standard type\n    xref_node = type_to_xref('int', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='class', reftarget='int')\n    assert_node(xref_node[0], nodes.Text, \"int\")\n\n    # Test with None type\n    xref_node = type_to_xref('None', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n    assert_node(xref_node[0], nodes.Text, \"None\")\n\n    # Test with fully qualified type name\n    env.config.python_use_unqualified_type_names = True\n    xref_node = type_to_xref('example_module.ExampleClass', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='class', reftarget='example_module.ExampleClass')\n    assert_node(xref_node[0], pending_xref_condition, condition='resolved')\n    assert_node(xref_node[1], pending_xref_condition, condition='*')\n", "def test_pymodule_directive(app):\n    text = (\".. py:module:: example_module\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :synopsis: An example module\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index))\n    assert 'example_module' in domain.modules\n    module_entry = domain.modules['example_module']\n    assert module_entry.synopsis == 'An example module'\n    assert module_entry.platform == 'Unix'\n    assert module_entry.deprecated is True\n"], "sample_946": ["def test_pyobject_handle_signature():\n    env = Mock(ref_context={}, config=Mock(add_module_names=True))\n    obj = PyObject(env, Mock(state=Mock(document=Mock(ids={}))), Mock())\n    signode = desc_signature('dummy', '')\n\n    # Test with module and class\n    env.ref_context = {'py:module': 'test_module', 'py:class': 'TestClass'}\n    obj.names = [('TestClass.method', 'method')]\n    fullname, prefix = obj.handle_signature('method(self, arg1, arg2)', signode)\n    assert fullname == 'TestClass.method'\n    assert prefix == ''\n    assert_node(signode, ([desc_addname, ''],\n                          [desc_name, 'method'],\n                          [desc_parameterlist, ([desc_parameter, 'self'],\n                                                [desc_parameter, 'arg1'],\n                                                [desc_parameter, 'arg2'])]))\n\n    # Test without module and class\n    env.ref_context = {}\n    obj.names = [('method', 'method')]\n    fullname, prefix = obj.handle_signature('method(arg1, arg2)', signode)\n    assert fullname == 'method'\n    assert prefix == ''\n    assert_node(signode, ([desc_name, 'method'],\n                          [desc_parameterlist, ([desc_parameter, 'arg1'],\n                                                [desc_parameter, 'arg2'])]))\n\n    # Test with return annotation\n    env.ref_context = {}\n    obj.names = [('method', 'method')]\n    fullname, prefix = obj.handle_signature('method(arg1, arg2) -> str', signode)\n    assert fullname == 'method'\n    assert prefix == ''\n    assert_node(signode, ([desc_name, 'method'],\n                          [desc_parameterlist, ([desc_parameter, 'arg1'],\n                                                [desc_parameter, 'arg2'])],\n                          [desc_returns, ([pending_xref, 'str'])]))\n\n    # Test with prefix\n    env.ref_context = {}\n    obj.names = [('TestClass.method', 'TestClass.method')]\n    fullname, prefix = obj.handle_signature('TestClass.method(arg1, arg2)', signode)\n    assert fullname == 'TestClass.method'\n    assert prefix == 'TestClass'\n    assert_node(signode, ([desc_addname, 'TestClass.'],\n                          [desc_name, 'method'],\n                          [desc_parameterlist, ([desc_parameter, 'arg1'],\n                                                [desc_parameter, 'arg2'])]))\n", "def test_pyclass_signature_with_nested_classes(app):\n    text = (\".. py:class:: OuterClass\\n\"\n            \"\\n\"\n            \"   .. py:class:: OuterClass.InnerClass\\n\"\n            \"   .. py:class:: OuterClass.InnerClass.InnerInnerClass\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"OuterClass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n\n    # OuterClass\n    assert_node(doctree[0], addnodes.index,\n                entries=[('single', 'OuterClass (built-in class)', 'OuterClass', '', None)])\n    assert 'OuterClass' in domain.objects\n    assert domain.objects['OuterClass'] == ('index', 'OuterClass', 'class', False)\n\n    # OuterClass.InnerClass\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'OuterClass.InnerClass (built-in class)', 'OuterClass.InnerClass', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"OuterClass.InnerClass\"])],\n                                   [desc_content, ()]))\n    assert 'OuterClass.InnerClass' in domain.objects\n    assert domain.objects['OuterClass.InnerClass'] == ('index', 'OuterClass.InnerClass', 'class', False)\n\n    # OuterClass.InnerClass.InnerInnerClass\n    assert_node(doctree[1][1][2], addnodes.index,\n                entries=[('single', 'OuterClass.InnerClass.InnerInnerClass (built-in class)', 'OuterClass.InnerClass.InnerInnerClass', '', None)])\n    assert_node(doctree[1][1][3], ([desc_signature, ([desc_name, \"OuterClass.InnerClass.InnerInnerClass\"])],\n                                   [desc_content, ()]))\n    assert 'OuterClass.InnerClass.InnerInnerClass' in domain.objects\n    assert domain.objects['OuterClass.InnerClass.InnerInnerClass'] == ('index', 'OuterClass.InnerClass.InnerInnerClass', 'class', False)\n", "def test_pyfunction_in_class(app):\n    text = (\".. py:class:: MyClass\\n\"\n            \"\\n\"\n            \"   .. py:function:: my_function(arg1: int, arg2: str) -> None\\n\"\n            \"      :module: mymodule\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"MyClass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'my_function() (in module mymodule)', 'mymodule.my_function', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_addname, \"@\"],\n                                                     [desc_name, \"my_function\"],\n                                                     [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"arg1\"],\n                                                                                            [desc_sig_punctuation, \":\"],\n                                                                                            \" \",\n                                                                                            [pending_xref, \"int\"])],\n                                                                           [desc_parameter, ([desc_sig_name, \"arg2\"],\n                                                                                            [desc_sig_punctuation, \":\"],\n                                                                                            \" \",\n                                                                                            [pending_xref, \"str\"])])],\n                                                     [desc_returns, pending_xref, \"None\"])],\n                                    [desc_content, ()]))\n    assert 'mymodule.my_function' in domain.objects\n    assert domain.objects['mymodule.my_function'] == ('index', 'mymodule.my_function', 'function', False)\n", "def test_pyclass_with_nested_classes(app):\n    text = (\".. py:class:: OuterClass\\n\"\n            \"\\n\"\n            \"   .. py:class:: InnerClass\\n\"\n            \"\\n\"\n            \"      .. py:method:: inner_method()\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"OuterClass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                                            [desc_name, \"OuterClass.InnerClass\"])],\n                                                          [desc_content, (addnodes.index,\n                                                                          [desc, ([desc_signature, ([desc_name, \"inner_method\"],\n                                                                                                    [desc_parameterlist, ()])],\n                                                                                  [desc_content, ()])])])])])])]))\n    assert 'OuterClass' in domain.objects\n    assert domain.objects['OuterClass'] == ('index', 'OuterClass', 'class', False)\n    assert 'OuterClass.InnerClass' in domain.objects\n    assert domain.objects['OuterClass.InnerClass'] == ('index', 'OuterClass.InnerClass', 'class', False)\n    assert 'OuterClass.InnerClass.inner_method' in domain.objects\n    assert domain.objects['OuterClass.InnerClass.inner_method'] == ('index', 'OuterClass.InnerClass.inner_method', 'method', False)\n", "def test_pyfunction_with_kwargs(app):\n    text = \".. py:function:: hello(name: str, **kwargs) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [nodes.inline, pending_xref, \"str\"])],\n                                      [desc_parameter, ([desc_sig_operator, \"**\"],\n                                                        [desc_sig_name, \"kwargs\"])])])\n"], "sample_947": ["def test_alias_definitions():\n    text = (\".. c:alias:: AType\\n\"\n            \".. c:alias:: AnotherType\\n\"\n            \"   :maxdepth: 2\\n\"\n            \"   :noroot:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, [addnodes.desc, AliasNode, addnodes.desc, AliasNode])\n    assert_node(doctree[0], addnodes.desc, domain=\"c\", objtype=\"alias\", noindex=True)\n    assert_node(doctree[2], addnodes.desc, domain=\"c\", objtype=\"alias\", noindex=True)\n    assert_node(doctree[1], AliasNode, sig=\"AType\", aliasOptions={'maxdepth': 1, 'noroot': False})\n    assert_node(doctree[3], AliasNode, sig=\"AnotherType\", aliasOptions={'maxdepth': 2, 'noroot': True})\n\n    entries = extract_role_links(app, \"index.html\")\n    assert entries == [('c.AType', 'AType', 'AType'),\n                       ('c.AnotherType', 'AnotherType', 'AnotherType')]\n", "def test_c_enum():\n    text = \"\"\"", "def test_enum_with_initializers():\n    check('enum', 'enum color { RED = 1, GREEN, BLUE = 5, YELLOW }', {1: 'color'})\n    check('enum', 'enum state { ON = 1, OFF = 0 }', {1: 'state'})\n    check('enum', 'enum mixed { FIRST = 10, SECOND, THIRD = 20 }', {1: 'mixed'})\n    check('enum', 'enum fruits { APPLE, ORANGE = 3, BANANA }', {1: 'fruits'})\n", "def test_parse_paren_expr():\n    class Config:\n        c_id_attributes = [\"id_attr\"]\n        c_paren_attributes = [\"paren_attr\"]\n\n    parser = DefinitionParser(\"(5 + 3)\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    ast = parser._parse_paren_expression()\n    assert str(ast) == \"(5 + 3)\"\n\n    parser = DefinitionParser(\"(a + b)\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    ast = parser._parse_paren_expression()\n    assert str(ast) == \"(a + b)\"\n\n    parser = DefinitionParser(\"(x * (y + z))\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    ast = parser._parse_paren_expression()\n    assert str(ast) == \"(x * (y + z))\"\n\n    parser = DefinitionParser(\"(((a)))\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    ast = parser._parse_paren_expression()\n    assert str(ast) == \"(((a)))\"\n\n    parser = DefinitionParser(\"((5 + 3) * 2)\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    ast = parser._parse_paren_expression()\n    assert str(ast) == \"((5 + 3) * 2)\"\n", "def test_cnamespace_directives():\n    # Test the 'namespace', 'namespace-push', and 'namespace-pop' directives\n    text = \"\"\""], "sample_948": ["def test_paren_expr_list():\n    # Test paren expressions and braced initializers in various contexts\n    check('function', 'void f(int (a))', {1: \"f__a\", 2: \"1f1i\"})\n    check('function', 'void f(int ((a)))', {1: \"f__a\", 2: \"1f1i\"})\n    check('function', 'void f(int (a[5]))', {1: \"f__aA\", 2: \"1fA5_i\"})\n    check('function', 'void f(int (a)[5])', {1: \"f__aA\", 2: \"1fA5_i\"})\n    check('function', 'void f(int ((a))[5])', {1: \"f__aA\", 2: \"1fA5_i\"})\n    check('function', 'void f(int (*a)[5])', {1: \"f__aPA\", 2: \"1fPA5_i\"})\n    check('function', 'void f(int (&a)[5])', {1: \"f__aRA\", 2: \"1fRA5_i\"})\n    check('function', 'void f(int (*a)(int, double))', {1: \"f__aPFidE\", 2: \"1fP1aPFidE\"})\n    check('function', 'void f(int (*&a)(int, double))', {1: \"f__aRFidE\", 2: \"1fRP1aPFidE\"})\n\n    # Test braced initializers in expressions\n    exprCheck('{}', 'ilE')\n    exprCheck('{1, 2, 3}', 'ilL1EL2EL3EE')\n    exprCheck('{{}}', 'ililEE')\n    exprCheck('{{1, 2}, {3, 4}}', 'ililL1EL2EEilL3EL4EEE')\n    exprCheck('{1, 2, 3,}', 'ilL1EL2EL3EE')\n    exprCheck('{{1, 2}, {3, 4},}', 'ililL1EL2EEilL3EL4EEE')\n    exprCheck('{1, 2, args...}', 'ilL1EL2Esp4argsE')\n", "def test_unary_expression_parsing():\n        ids = {'1': 'v__T', '2': '1v'}\n        check('member', 'T v = {}'.format(expr), ids)\n    \n    unaryCheck('++x', 'pp1x')\n    unaryCheck('--x', 'mm1x')\n    unaryCheck('*x', 'de1x')\n    unaryCheck('&x', 'ad1x')\n    unaryCheck('+x', 'ps1x')\n    unaryCheck('-x', 'ng1x')\n    unaryCheck('!x', 'nt1x')\n    unaryCheck('not x', 'nt1x')\n    unaryCheck('~x', 'co1x')\n    unaryCheck('compl x', 'co1x')\n", "def test_trailing_type_spec():\n    # Test fundamental types with trailing type specifiers\n    check('member', 'int const a', {1: 'a__iC', 2: '1a'})\n    check('member', 'int volatile a', {1: 'a__iV', 2: '1a'})\n    check('member', 'int const volatile a', {1: 'a__iVC', 2: '1a'}, output='int volatile const a')\n    check('member', 'int volatile const a', {1: 'a__iVC', 2: '1a'})\n\n    # Test nested names with trailing type specifiers\n    check('member', 'A::B const a', {1: 'a__A::BCR', 2: '1a'})\n    check('member', 'A::B volatile a', {1: 'a__A::BVR', 2: '1a'})\n    check('member', 'A::B const volatile a', {1: 'a__A::BVCR', 2: '1a'}, output='A::B volatile const a')\n    check('member', 'A::B volatile const a', {1: 'a__A::BVCR', 2: '1a'})\n\n    # Test decltype with trailing type specifier\n    check('member', 'decltype(auto) const a', {1: 'a__DcC', 2: '1a'})\n    check('member', 'decltype(auto) volatile a', {1: 'a__DcV', 2: '1a'})\n    check('member', 'decltype(auto) const volatile a', {1: 'a__DcVC', 2: '1a'}, output='decltype(auto) volatile const a')\n    check('member', 'decltype(auto) volatile const a', {1: 'a__DcVC', 2: '1a'})\n", "def test_operators_with_templates():\n    check('function', 'template<typename T> void operator*()', {2: 'I0Emlv'})\n    check('function', 'template<typename T> void operator/()', {2: 'I0Edvv'})\n    check('function', 'template<typename T> void operator%()', {2: 'I0Ermv'})\n    check('function', 'template<typename T> void operator+()', {2: 'I0Eplv'})\n    check('function', 'template<typename T> void operator-()', {2: 'I0Emiv'})\n    check('function', 'template<typename T> void operator&()', {2: 'I0Eanv'})\n    check('function', 'template<typename T> void operator|()', {2: 'I0Eorv'})\n    check('function', 'template<typename T> void operator^()', {2: 'I0Eeov'})\n    check('function', 'template<typename T> void operator==()', {2: 'I0Eeqv'})\n    check('function', 'template<typename T> void operator!=()', {2: 'I0Enev'})\n    check('function', 'template<typename T> void operator<=()', {2: 'I0Elev'})\n    check('function', 'template<typename T> void operator>=()', {2: 'I0Egev'})\n    check('function', 'template<typename T> void operator<()', {2: 'I0Eltv'})\n    check('function', 'template<typename T> void operator>()', {2: 'I0Egtv'})\n    check('function', 'template<typename T> void operator<<()', {2: 'I0Elsv'})\n    check('function', 'template<typename T> void operator>>()', {2: 'I0Ersv'})\n    check('function', 'template<typename T> void operator&&()', {2: 'I0Eaav'})\n    check('function', 'template<typename T> void operator||()', {2: 'I0Eoov'})\n    check('function', 'template<typename T> void operator!()', {2: 'I0Entv'})\n    check('function', 'template<typename T> void operator~()', {2: 'I0Ecov'})\n    check('function', 'template<typename T> void operator+=()', {2: 'I0EpLv'})\n    check('function', 'template<typename T> void operator-=", "def test_unresolved_reference():\n    text = (\".. cpp:function:: void f()\\n\"\n            \".. cpp:function:: void g()\\n\"\n            \".. cpp:var:: int h\\n\"\n            \".. cpp:var:: int unresolved\\n\")\n\n    doctree = restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n\n    assert len(ws) == 1\n    assert \"index.rst:4: WARNING: cpp:identifier reference target not found: unresolved\" in ws[0]\n\n    assert_node(doctree, [addnodes.desc, addnodes.desc, addnodes.desc, addnodes.desc])\n    assert_node(doctree[0], addnodes.desc, desc_signature=\"void f()\")\n    assert_node(doctree[1], addnodes.desc, desc_signature=\"void g()\")\n    assert_node(doctree[2], addnodes.desc, desc_signature=\"int h\")\n    assert_node(doctree[3], addnodes.desc, desc_signature=\"int unresolved\")\n"], "sample_949": ["def test_missing_document_reference(app, status, warning):\n    app.builder.build_all()\n    warnings = warning.getvalue()\n    assert 'WARNING: \"man_pages\" config value references unknown document' in warnings\n", "def test_unknown_document_reference(app, status, warning):\n    app.build()\n    warning_text = warning.getvalue()\n    assert 'WARNING: \"man_pages\" config value references unknown document' in warning_text\n", "def test_man_show_urls(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n\n    # Check that URLs are shown in the content\n    assert 'http://example.com' in content\n    assert 'https://sphinx-doc.org' in content\n", "def test_no_man_pages_config(app, status, warning):\n    app.build()\n    warning_text = warning.getvalue()\n    assert 'no \"man_pages\" config value found; no manual pages will be written' in warning_text\n", "def test_custom_man_pages_config(app, status, warning):\n    app.build()\n    assert (app.outdir / 'sample.5').exists()\n    \n    content = (app.outdir / 'sample.5').read_text()\n    assert '.TH \"sample\" \"5\"' in content  # check the title header with section\n    assert '.SH \"Sample Description\"' in content  # check the description\n    assert '.SH \"Author Name\"' in content  # check the author\n"], "sample_950": ["def test_pyfunction_with_positional_only(app):\n    text = \".. py:function:: func(a, /, b, *, c)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    desc_parameterlist)],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                      [desc_parameter, desc_sig_operator, \"/\"],\n                                      [desc_parameter, desc_sig_name, \"b\"],\n                                      [desc_parameter, desc_sig_operator, \"*\"],\n                                      [desc_parameter, desc_sig_name, \"c\"])])\n", "def test_pymodule_signature(app):\n    text = (\".. py:module:: example\\n\"\n            \"   :synopsis: This is an example module.\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"module \"],\n                                                    [desc_name, \"example\"])],\n                                  desc_content)]))\n    \n    assert 'example' in domain.modules\n    assert domain.modules['example'] == ('index', 'module-example', 'This is an example module.', 'Unix', True)\n", "def test_parse_annotation_with_ellipsis(app):\n    doctree = _parse_annotation(\"Tuple[int, ...]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [desc_sig_punctuation, \"...\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"List[...]\", app.env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [desc_sig_punctuation, \"...\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"Dict[str, ...]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [desc_sig_punctuation, \"...\"],\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_type_to_xref():\n    env = Mock(spec=BuildEnvironment)\n    env.config.python_use_unqualified_type_names = False\n    env.ref_context = {'py:module': 'example', 'py:class': 'MyClass'}\n    \n    node = type_to_xref(\"MyClass\", env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='MyClass', **{'py:module': 'example', 'py:class': 'MyClass'})\n    assert_node(node[0], nodes.Text, 'MyClass')\n    \n    env.config.python_use_unqualified_type_names = True\n    node = type_to_xref(\"example.MyClass\", env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='example.MyClass', **{'py:module': 'example', 'py:class': 'MyClass'})\n    assert_node(node[0], pending_xref_condition, condition='resolved')\n    assert_node(node[1], pending_xref_condition, condition='*')\n\n    node = type_to_xref(\"None\", env)\n    assert_node(node, pending_xref, refdomain='py', reftype='obj', reftarget='None', **{'py:module': 'example', 'py:class': 'MyClass'})\n    assert_node(node[0], nodes.Text, 'None')\n", "def test_parse_annotation_nested_generics(app):\n    doctree = _parse_annotation(\"Dict[str, List[Tuple[int, str]]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n"], "sample_951": ["def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return \"default\"\n\n    @func.register(int)\n        return \"int\"\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func.__call__) is False  # Ensure __call__ is not detected as singledispatch\n\n        return \"not singledispatch\"\n\n    assert inspect.is_singledispatch_function(non_singledispatch_func) is False\n", "def test_getargspec():\n        \"\"\"Test function for getargspec.\"\"\"\n        return a + b\n\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        argspec = inspect.getargspec(test_func)\n        assert argspec.args == ['a', 'b']\n        assert argspec.varargs == 'args'\n        assert argspec.varkw == 'kwargs'\n        assert argspec.defaults == (1,)\n        assert argspec.kwonlyargs == []\n        assert argspec.kwdefaults is None\n        assert argspec.annotations == {}\n\n        assert len(w) == 1\n        assert issubclass(w[-1].category, RemovedInSphinx50Warning)\n        assert \"sphinx.ext.inspect.getargspec() is deprecated\" in str(w[-1].message)\n", "def test_getargspec():\n        pass\n\n    spec = inspect.getargspec(func)\n    assert spec.args == ['a', 'b', 'c', 'd']\n    assert spec.varargs == 'e'\n    assert spec.varkw == 'f'\n    assert spec.defaults == (1, 2)\n    assert spec.kwonlyargs == []\n    assert spec.kwdefaults is None\n    assert spec.annotations == {}\n\n        pass\n\n    spec = inspect.getargspec(annotated_func)\n    assert spec.args == ['a', 'b', 'c']\n    assert spec.varargs is None\n    assert spec.varkw is None\n    assert spec.defaults == (True,)\n    assert spec.kwonlyargs == []\n    assert spec.kwdefaults is None\n    assert spec.annotations == {'a': int, 'b': str, 'c': bool, 'return': None}\n", "def test_getall():\n    class FooModule:\n        __all__ = ['foo', 'bar']\n\n    class BarModule:\n        __all__ = ['foo', 123]  # Invalid __all__ containing a non-string item\n\n    class BazModule:\n        pass  # No __all__ attribute\n\n    foo_module = FooModule()\n    bar_module = BarModule()\n    baz_module = BazModule()\n\n    assert inspect.getall(foo_module) == ['foo', 'bar']\n    assert inspect.getall(baz_module) is None\n\n    with pytest.raises(ValueError):\n        inspect.getall(bar_module)\n", "def test_getall():\n    module_with_all = types.ModuleType(\"module_with_all\")\n    module_with_all.__all__ = [\"a\", \"b\", \"c\"]\n    assert inspect.getall(module_with_all) == [\"a\", \"b\", \"c\"]\n\n    module_without_all = types.ModuleType(\"module_without_all\")\n    assert inspect.getall(module_without_all) is None\n\n    module_invalid_all = types.ModuleType(\"module_invalid_all\")\n    module_invalid_all.__all__ = \"not a list or tuple\"\n    with pytest.raises(ValueError):\n        inspect.getall(module_invalid_all)\n"], "sample_952": ["def test_object_description_custom_class():\n    class CustomClass:\n            return \"<CustomClass instance>\"\n    \n    instance = CustomClass()\n    description = inspect.object_description(instance)\n    assert description == \"<CustomClass instance>\"\n", "def test_getmro():\n    class Foo:\n        pass\n\n    class Bar(Foo):\n        pass\n\n    class Baz(Bar):\n        pass\n\n    assert inspect.getmro(Baz) == (Baz, Bar, Foo, object)\n    assert inspect.getmro(Bar) == (Bar, Foo, object)\n    assert inspect.getmro(Foo) == (Foo, object)\n    assert inspect.getmro(object) == (object,)\n", "def test_unwrap_all():\n        pass\n\n        @functools.wraps(f)\n            return f(*args, **kwargs)\n        return wrapped\n\n    wrapped_func = wrapper(func)\n    assert inspect.unwrap_all(wrapped_func) is func\n\n    class PartialClass:\n            pass\n\n    partial_method = functools.partial(PartialClass().method)\n    assert inspect.unwrap_all(partial_method) is PartialClass().method\n\n    assert inspect.unwrap_all(PartialClass.method, stop=lambda x: isinstance(x, type)) is PartialClass.method\n", "def test_getargspec():\n        pass\n\n    spec = inspect.getargspec(sample_function)\n    assert spec.args == ['a', 'b', 'c', 'd']\n    assert spec.varargs == 'args'\n    assert spec.varkw == 'kwargs'\n    assert spec.defaults == (2,)\n    assert spec.kwonlyargs == ['c', 'd']\n    assert spec.kwdefaults == {'d': 4}\n    assert spec.annotations == {}\n", "def test_getargspec():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b', 'c', 'd', 'e']\n    assert argspec.varargs == 'args'\n    assert argspec.varkw == 'kwargs'\n    assert argspec.defaults == (2, 3, 5)\n    assert argspec.kwonlyargs == ['d', 'e']\n    assert argspec.kwdefaults == {'e': 5}\n    assert argspec.annotations == {'c': int, 'return': None}\n"], "sample_953": ["def test_invalid_path():\n    answers = {\n        'Root path': '/invalid/path/for/testing',\n        'Project name': 'Invalid Path Test',\n        'Author name': 'Test Author',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    with pytest.raises(ValidationError):\n        d['path'] = qs.do_prompt('Root path for the documentation', validator=qs.is_path)\n", "def test_is_path():\n    # Test for valid path\n    valid_path = tempdir\n    assert qs.is_path(valid_path) == valid_path\n\n    # Test for invalid path\n    with pytest.raises(qs.ValidationError):\n        qs.is_path(\"invalid_path\")\n", "def test_invalid_path():\n    invalid_path = \"/invalid/path\"\n    qs.term_input = mock_input({'Root path for the documentation': invalid_path})\n    d = {}\n    with pytest.raises(SystemExit):\n        qs.ask_user(d)\n\n    answers = {\n        'Root path for the documentation': tempdir,\n        'Separate source and build': 'y',\n        'Name prefix for templates': '.',\n        'Project name': 'STASI\u2122',\n        'Author name': 'Wolfgang Sch\u00e4uble & G\\'Beckstein',\n        'Project version': '2.0',\n        'Project release': '2.0.1',\n        'Project language': 'de',\n        'Source file suffix': '.txt',\n        'Name of your master document': 'contents',\n        'autodoc': 'y',\n        'doctest': 'yes',\n        'intersphinx': 'no',\n        'todo': 'y',\n        'coverage': 'no',\n        'imgmath': 'N',\n        'mathjax': 'no',\n        'ifconfig': 'no',\n        'viewcode': 'no',\n        'githubpages': 'no',\n        'Create Makefile': 'no',\n        'Create Windows command file': 'no',\n        'Do you want to use the epub builder': 'yes',\n    }\n    qs.term_input = mock_input(answers, needanswer=True)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d)\n\n    conffile = tempdir / 'source' / 'conf.py'\n    assert conffile.isfile()\n    ns = {}\n    exec(conffile.read_text(), ns)\n    assert ns['extensions'] == [\n        'sphinx.ext.autodoc', 'sphinx.ext.doctest', 'sphinx.ext.todo'\n    ]\n    assert ns['templates_path'] == ['.templates']\n    assert ns['source_suffix'] == '.txt'\n    assert ns['root_doc'] == 'contents'\n    assert ns['project'] == 'STASI\u2122'\n    assert ns['copyright'] == '%s, Wolfgang Sch\u00e4uble & G\\'Beckstein' % \\\n        time.strftime('%Y')\n    assert ns['version'] == '2.0'\n    assert ns['release'] == '2.0.1'\n    assert ns['todo_include_todos'] is True\n    assert ns['html_static_path'] == ['.static']\n\n    assert (tempdir / 'build').isdir", "def test_invalid_path():\n    invalid_path = '/this/path/does/not/exist'\n    qs.term_input = mock_input({\n        'Root path for the documentation': invalid_path\n    })\n    d = {}\n    with pytest.raises(SystemExit):\n        qs.ask_user(d)\n", "def test_invalid_path():\n    qs.term_input = mock_input({'Root path for the documentation': '/invalid/path'})\n    d = {}\n    with pytest.raises(SystemExit):\n        qs.ask_user(d)\n\n"], "sample_954": ["def test_nested_inline_transform(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'nested-inline.1').read_text()\n\n    # Check that nested inline nodes are flattened\n    assert '<strong>foo=</strong><emphasis>1</emphasis><strong>&bar=</strong><emphasis>2</emphasis>' in content\n\n    # Ensure no nested inline structure is present\n    assert '<strong><emphasis>' not in content\n", "def test_nested_inline_transform(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # Check if nested inline nodes are correctly flattened\n    assert '<strong>foo=</strong><emphasis>1</emphasis>' not in content\n    assert '<strong>&bar=</strong><emphasis>2</emphasis>' not in content\n\n    # Instead of the above, ensure it is flattened properly\n    assert '\\\\fBfoo=\\\\fP\\\\fI1\\\\fP' in content\n    assert '\\\\fB&bar=\\\\fP\\\\fI2\\\\fP' in content\n", "def test_nested_inline_transform(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # check the transformation of nested inline nodes\n    assert r'\\fBfoo=\\fP\\fI1\\fP' in content\n    assert r'\\fB&bar=\\fP\\fI2\\fP' in content\n\n    # Ensure no nested inline nodes remain\n    assert '<strong>foo=<emphasis>1</emphasis>&bar=<emphasis>2</emphasis></strong>' not in content\n", "def test_nested_inline_transform(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # Before: <strong>foo=<emphasis>1</emphasis>&bar=<emphasis>2</emphasis></strong>\n    # After: <strong>foo=</strong><emphasis>1</emphasis><strong>&bar=</strong><emphasis>2</emphasis>\n    assert '.B foo=' in content\n    assert '\\\\fI1\\\\fP' in content\n    assert '.B &bar=' in content\n    assert '\\\\fI2\\\\fP' in content\n", "def test_domain_specific_object_descriptions(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # check desc nodes\n    assert '.SH NAME' in content\n    assert '.B test_function()' in content\n    assert '.B param1' in content\n    assert ' -> ' in content\n    assert '.B return_type' in content\n\n    # check desc_signature\n    assert '.B test_function_signature' in content\n    assert '.B param1' in content\n\n    # check desc_signature_line\n    assert '.SH SYNOPSIS' in content\n    assert '.B test_function_signature_line' in content\n\n    # check desc_content\n    assert '.SH DESCRIPTION' in content\n    assert '.B test_function_content' in content\n\n    # check desc_inline\n    assert '.B test_inline' in content\n"], "sample_955": ["def test_unparse_with_default_arguments():\n    source = \"def func(a, b=2, c: int = 3): pass\"\n    expected = \"def func(a, b=2, c: int = 3): pass\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args, source) == \"a, b=2, c: int = 3\"\n", "def test_unparse_function_def():\n    source = \"def foo(x, y=2, z: int = 3) -> int: pass\"\n    expected = \"foo(x, y=2, z: int = 3)\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_unparse_FunctionDef():\n    source = \"def func(a, b=2, *args, **kwargs): pass\"\n    module = ast.parse(source)\n    expected = \"def func(a, b=2, *args, **kwargs): ...\"\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_unparse_with_arguments():\n    source = \"def func(a, b, c=3, *args, d, e=5, **kwargs): pass\"\n    expected = \"def func(a, b, c=3, *args, d, e=5, **kwargs): pass\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_parse_with_syntax_error():\n    code = \"a + \"  # Incomplete expression, should cause SyntaxError\n    result = ast.parse(code)\n    assert isinstance(result, ast.Module)\n    assert len(result.body) == 1\n    assert isinstance(result.body[0], ast.Expr)\n    assert isinstance(result.body[0].value, ast.BinOp)\n    assert isinstance(result.body[0].value.left, ast.Name)\n    assert result.body[0].value.left.id == \"a\"\n    assert result.body[0].value.op is None\n    assert result.body[0].value.right is None\n"], "sample_956": ["def test_fetch_inventory_file_not_found(app, status, warning):\n    \"\"\"Test fetch_inventory when the inventory file is not found\"\"\"\n    intersphinx_setup(app)\n    \n    with pytest.raises(Exception) as excinfo:\n        fetch_inventory(app, 'http://hostname/', 'http://hostname/invalid_inventory.inv')\n    \n    assert 'intersphinx inventory' in str(excinfo.value)\n    assert 'not fetchable' in str(excinfo.value)\n", "def test_fetch_inventory_local_file(tempdir, app, status, warning):\n    \"\"\"Test fetch_inventory for local file path\"\"\"\n    inv_file = tempdir / 'local_inventory'\n    inv_file.write_bytes(inventory_v2)\n    intersphinx_setup(app)\n\n    # fetch inventory from local file\n    inv_data = fetch_inventory(app, 'http://localhost/', str(inv_file))\n    assert inv_data is not None\n    assert 'py:module' in inv_data\n", "def test_fetch_inventory_local_file(tempdir, app, status, warning):\n    \"\"\"Test fetch_inventory with a local file\"\"\"\n    inv_file = tempdir / INVENTORY_FILENAME\n    inv_file.write_bytes(inventory_v2)\n\n    intersphinx_setup(app)\n    invdata = fetch_inventory(app, '', str(inv_file))\n    \n    assert invdata is not None\n    assert 'py:module' in invdata\n    assert invdata['py:module']['module2'] == ('foo', '2.0', 'https://docs.python.org/foo.html#module-module2', '-')\n", "def test_fetch_inventory_local_file(app, tempdir):\n    \"\"\"Test fetch_inventory with a local file\"\"\"\n    inv_file = tempdir / INVENTORY_FILENAME\n    inv_file.write_bytes(inventory_v2)\n    intersphinx_setup(app)\n\n    with mock.patch(\"builtins.open\", mock.mock_open(read_data=inv_file.read_bytes())) as mock_file:\n        inv_data = fetch_inventory(app, 'http://hostname/', str(inv_file))\n        mock_file.assert_called_once_with(str(inv_file), 'rb')\n        assert inv_data is not None\n", "def test_create_element_from_result():\n    \"\"\"Test _create_element_from_result\"\"\"\n    domain = mock.Mock()\n    domain.name = 'py'\n    node = addnodes.pending_xref('')\n    contnode = nodes.TextElement('example', 'example')\n    inv_name = 'inv_name'\n    data = ('project', '1.0', 'https://example.com/docs', 'displayname')\n\n    # Create the element\n    element = _create_element_from_result(domain, inv_name, data, node, contnode)\n\n    # Check the attributes of the created element\n    assert isinstance(element, nodes.reference)\n    assert element['internal'] is False\n    assert element['refuri'] == 'https://example.com/docs'\n    assert element['reftitle'] == '(in project v1.0)'\n    assert element.astext() == 'example'\n"], "sample_957": ["def test_stringify_broken_type_hints():\n    assert stringify(BrokenType) == 'BrokenType'\n", "def test_get_type_hints():\n        return bool(a and b)\n\n    class SampleClass:\n            return str(x + (y or 0))\n\n    hints = get_type_hints(sample_function)\n    assert hints == {'a': int, 'b': str, 'return': bool}\n\n    hints = get_type_hints(SampleClass.method)\n    assert hints == {'x': float, 'y': Optional[int], 'return': str}\n\n    # Test with a function that has no annotations\n        return x + y\n\n    hints = get_type_hints(no_annotations)\n    assert hints == {}\n", "def test_get_type_hints():\n        return True\n\n    class MyClass:\n        attr: int\n\n            pass\n\n    # Testing functions\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n\n    # Testing methods\n    assert get_type_hints(MyClass.method) == {'param': str, 'return': None}\n\n    # Testing class attributes\n    assert get_type_hints(MyClass) == {'attr': int}\n\n    # Testing invalid object\n    assert get_type_hints(\"invalid_object\") == {}\n\n    # Testing with globalns and localns\n    globalns = {'int': float}\n    localns = {'str': list}\n    assert get_type_hints(func, globalns, localns) == {'a': float, 'b': list, 'return': bool}\n", "def test_restify_newtype():\n    assert restify(MyInt) == \":class:`MyInt`\"\n", "def test_get_type_hints():\n        return True\n\n    class MyClass:\n            return None\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(MyClass.method) == {'self': MyClass, 'x': List[int], 'y': Dict[str, float], 'return': Optional[str]}\n"], "sample_958": ["def test_domain_c_ast_identifier():\n    from sphinx.domains.c import ASTIdentifier\n\n    # Test creating an identifier\n    identifier = ASTIdentifier(\"test_id\")\n    assert identifier.identifier == \"test_id\"\n\n    # Test equality\n    identifier2 = ASTIdentifier(\"test_id\")\n    assert identifier == identifier2\n\n    identifier3 = ASTIdentifier(\"different_id\")\n    assert identifier != identifier3\n\n    # Test is_anon method\n    anonymous_identifier = ASTIdentifier(\"@anon_id\")\n    assert anonymous_identifier.is_anon() is True\n    assert identifier.is_anon() is False\n\n    # Test __str__ method\n    assert str(identifier) == \"test_id\"\n    assert str(anonymous_identifier) == \"@anon_id\"\n\n    # Test get_display_string method\n    assert identifier.get_display_string() == \"test_id\"\n    assert anonymous_identifier.get_display_string() == \"[anonymous]\"\n\n    # Test describe_signature method\n    from docutils import nodes\n    from sphinx import addnodes\n    from sphinx.util.cfamily import StringifyTransform, verify_description_mode\n\n    class MockSymbol:\n            return \"mock_key\"\n\n    signode = addnodes.desc_signature()\n    identifier.describe_signature(signode, 'markType', None, '', MockSymbol())\n    assert len(signode) == 1\n    assert isinstance(signode[0], addnodes.pending_xref)\n    assert signode[0][0].astext() == \"test_id\"\n\n    signode = addnodes.desc_signature()\n    anonymous_identifier.describe_signature(signode, 'markType', None, '', MockSymbol())\n    assert len(signode) == 1\n    assert isinstance(signode[0], nodes.Text)\n    assert signode[0].astext() == \"[anonymous]\"\n\n    signode = addnodes.desc_signature()\n    identifier.describe_signature(signode, 'lastIsName', None, '', MockSymbol())\n    assert len(signode) == 1\n    assert isinstance(signode[0], addnodes.desc_name)\n    assert signode[0][0].astext() == \"test_id\"\n", "def test_domain_cpp_ast_builtin_types():\n    check('type', 'int', {1: 'int', 2: 'i'})\n    check('type', 'unsigned int', {1: 'unsigned-int', 2: 'j'})\n    check('type', 'signed int', {1: 'signed-int', 2: 'i'})\n    check('type', 'short', {1: 'short', 2: 's'})\n    check('type', 'unsigned short', {1: 'unsigned-short', 2: 't'})\n    check('type', 'signed short', {1: 'signed-short', 2: 's'})\n    check('type', 'long', {1: 'long', 2: 'l'})\n    check('type', 'unsigned long', {1: 'unsigned-long', 2: 'm'})\n    check('type', 'signed long', {1: 'signed-long', 2: 'l'})\n    check('type', 'long long', {1: 'long-long', 2: 'x'})\n    check('type', 'unsigned long long', {1: 'unsigned-long-long', 2: 'y'})\n    check('type', 'signed long long', {1: 'signed-long-long', 2: 'x'})\n    check('type', 'char', {1: 'char', 2: 'c'})\n    check('type', 'unsigned char', {1: 'unsigned-char', 2: 'h'})\n    check('type', 'signed char', {1: 'signed-char', 2: 'a'})\n    check('type', 'wchar_t', {1: 'wchar_t', 2: 'w'})\n    check('type', 'char16_t', {1: 'char16_t', 2: 'Ds'})\n    check('type', 'char32_t', {1: 'char32_t', 2: 'Di'})\n    check('type', 'float', {1: 'float', 2: 'f'})\n    check('type', 'double', {1: 'double', 2: 'd'})\n    check('type', 'long double', {1: 'long-double', 2: 'e'})\n    check('type', 'bool', {1: 'bool', 2: 'b'})\n", "def test_domain_c_ast_parsing():\n        class Config:\n            c_id_attributes = [\"id_attr\"]\n            c_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(decl, location=None, config=Config())\n        return parser.parse_declaration('member', 'member')\n\n    # Fundamental types\n    decl = \"int a\"\n    ast = parse_c_declaration(decl)\n    assert str(ast) == \"int a\"\n\n    # Pointer types\n    decl = \"int *b\"\n    ast = parse_c_declaration(decl)\n    assert str(ast) == \"int *b\"\n\n    # Array types\n    decl = \"int c[10]\"\n    ast = parse_c_declaration(decl)\n    assert str(ast) == \"int c[10]\"\n\n    # Function pointers\n    decl = \"int (*d)(float, double)\"\n    ast = parse_c_declaration(decl)\n    assert str(ast) == \"int (*d)(float, double)\"\n\n    # Struct declaration\n    decl = \"struct MyStruct { int x; float y; }\"\n    ast = parse_c_declaration(decl)\n    assert str(ast) == \"struct MyStruct { int x; float y; }\"\n\n    # Typedef\n    decl = \"typedef int MyInt\"\n    ast = parse_c_declaration(decl)\n    assert str(ast) == \"typedef int MyInt\"\n\n    # Function declaration\n    decl = \"void func(int a, float b)\"\n    ast = parse_c_declaration(decl)\n    assert str(ast) == \"void func(int a, float b)\"\n\n    # Enum declaration\n    decl = \"enum MyEnum { RED, GREEN, BLUE }\"\n    ast = parse_c_declaration(decl)\n    assert str(ast) == \"enum MyEnum { RED, GREEN, BLUE }\"\n\n    # Union declaration\n    decl = \"union MyUnion { int a; float b; }\"\n    ast = parse_c_declaration(decl)\n    assert str(ast) == \"union MyUnion { int a; float b; }\"\n\n    # Macro definition\n    decl = \"#define MY_MACRO(a, b) ((a) + (b))\"\n    ast = parse_c_declaration(decl)\n    assert str(ast) == \"#define MY_MACRO(a, b) ((a) + (b))\"\n", "def test_domain_c_symbol_lookup():\n    rootSymbol = Symbol(None, None, None, None, None)\n    symbolA = Symbol(rootSymbol, ASTIdentifier('A'), None, None, None)\n    symbolB = Symbol(symbolA, ASTIdentifier('B'), None, None, None)\n    symbolC = Symbol(symbolB, ASTIdentifier('C'), None, None, None)\n\n    # Add symbols for function parameters\n    decl_param = ASTDeclaration('functionParam', None, ASTFunctionParameter(ASTTypeWithInit(ASTType(\n        ASTDeclSpecs('function', ASTDeclSpecsSimple(None, None, False, False, False, False, []), \n        ASTDeclSpecsSimple(None, None, False, False, False, False, []), ASTTrailingTypeSpecFundamental('int')),\n        ASTDeclaratorNameParam(ASTNestedName([ASTIdentifier('param')], False), [], None)\n    ), None)), False)\n\n    symbolA.add_declaration(decl_param, 'docname', 1)\n\n    # Check lookup for existing symbol\n    result = rootSymbol.find_declaration(ASTNestedName([ASTIdentifier('A'), ASTIdentifier('B')], False), 'functionParam', True, False)\n    assert result is symbolB\n\n    # Check lookup for non-existing symbol\n    result = rootSymbol.find_declaration(ASTNestedName([ASTIdentifier('A'), ASTIdentifier('D')], False), 'functionParam', True, False)\n    assert result is None\n", "def test_domain_c_ast_identifier():\n    id1 = ASTIdentifier(\"test\")\n    id2 = ASTIdentifier(\"test\")\n    id3 = ASTIdentifier(\"test_diff\")\n    \n    assert id1 == id2\n    assert id1 != id3\n    assert str(id1) == \"test\"\n    assert id1.get_display_string() == \"test\"\n    \n    anon_id = ASTIdentifier(\"@anon\")\n    assert anon_id.is_anon()\n    assert anon_id.get_display_string() == \"[anonymous]\"\n    \n    signode = addnodes.desc_signature()\n    env = None\n    symbol = Symbol(None, id1, None, None, None)\n    id1.describe_signature(signode, 'markType', env, 'prefix_', symbol)\n    assert_node(signode, [addnodes.pending_xref])\n    assert_node(signode[0], addnodes.pending_xref, refdomain='c', reftype='identifier', reftarget='prefix_test')\n    assert_node(signode[0][0], addnodes.desc_sig_name, text=\"test\")\n"], "sample_959": ["def test_domain_cpp_ast_function_pointer_to_member():\n    check('function', 'void f(int A::* p)', {2: '1fM1Ai'})\n    check('function', 'void f(int (A::* p)(float, double))', {2: '1fM1AFifdE'})\n    check('function', 'void f(int (A::* p)(float, double) const)', {2: '1fM1AKFifdE'})\n    check('function', 'void f(int (A::* p)(float, double) volatile)', {2: '1fM1AVFifdE'})\n    check('function', 'void f(int (A::* p)(float, double) const volatile)', {2: '1fM1AVKFifdE'})\n", "def test_domain_cpp_parse_operator_cast():\n    # Test for parsing the operator cast expressions\n    check('function', 'operator int()', {1: 'castto-i-operator', 2: 'cviv'})\n    check('function', 'operator float*()', {1: 'castto-fP-operator', 2: 'cvPfiv'})\n    check('function', 'operator void*()', {1: 'castto-vP-operator', 2: 'cvPvv'})\n    check('function', 'operator const std::string&() const',\n          {1: 'castto-std::stringCR-operatorC', 2: 'NKcvRKNSt6stringEEv'})\n    check('function', 'operator const std::vector<int>&() const',\n          {1: 'castto-std::vector:i:CR-operatorC', 2: 'NKcvRKNSt6vectorIiEEEvv'})\n", "def test_domain_cpp_ast_template_specializations():\n    # Check basic template specialization\n    check('class', 'template<> {key}A<int>', {2: 'IE1AIiE'}, output='template<> {key}A<int>')\n\n    # Check partial template specialization\n    check('class', 'template<typename T> {key}A<int, T>', {2: 'I0EI1AIi1TE'}, output='template<> {key}A<int, T>')\n    check('class', 'template<typename T, typename U> {key}A<T, U>', {2: 'I00EI1AI1T1UE'}, output='template<> {key}A<T, U>')\n\n    # Check specialization with default parameters\n    check('class', 'template<> {key}A<int, double = 3.14>', {2: 'IE1AIidE'}, output='template<> {key}A<int, double = 3.14>')\n    check('class', 'template<> {key}A<int, double = 3.14, bool = true>', {2: 'IE1AIidbE'}, output='template<> {key}A<int, double = 3.14, bool = true>')\n\n    # Check member specialization\n    check('member', 'template<> int A<int>::a', {2: 'IEN1AIiE1aE'}, output='template<> int A<int>::a')\n    check('member', 'template<> static int A<int>::a', {2: 'IEN1AIiE1aE'}, output='template<> static int A<int>::a')\n\n    # Check function specialization\n    check('function', 'template<> void f<int>(int)', {2: 'IE1fIiEi', 4: 'IE1fIiEiv'}, output='template<> void f<int>(int)')\n    check('function', 'template<> void f<double>(double)', {2: 'IE1fIdEd', 4: 'IE1fIdEdv'}, output='template<> void f<double>(double)')\n", "def test_domain_cpp_ast_cast_expressions():\n        castExpr = '{0}<{1}>({2})'.format(cast, typ, expr)\n        ids = 'IE1CIA{}{}{}E'.format(cast, typ, expr)\n        idDict = {2: ids.format(), 3: ids.format(id)}\n        if id4 is not None:\n            idDict[4] = ids.format(id4)\n        check('class', 'template<> {key}C<a[%s]>' % castExpr, idDict)\n\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(castExpr, location=None,\n                                  config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_expression()\n        res = str(ast)\n        if res != castExpr:\n            print(\"\")\n            print(\"Input:    \", castExpr)\n            print(\"Result:   \", res)\n            raise DefinitionError(\"\")\n        displayString = ast.get_display_string()\n        if res != displayString:\n            print(\"\")\n            print(\"Input:    \", castExpr)\n            print(\"Result:   \", res)\n            print(\"Display:  \", displayString)\n            raise DefinitionError(\"\")\n\n    castCheck('dynamic_cast', 'T&', 'i', 'dcR1T1i')\n    castCheck('static_cast', 'T&', 'i', 'scR1T1i')\n    castCheck('reinterpret_cast', 'T&', 'i', 'rcR1T1i')\n    castCheck('const_cast', 'T&', 'i', 'ccR1T1i')\n", "def test_domain_cpp_ast_explicit_instantiation():\n    # explicit instantiation of a template\n    check('function', 'template void f<int>();', {2: 'I_E1fIiEEv'})\n    check('function', 'template void f<double>();', {2: 'I_E1fIdEEv'})\n    check('function', 'template void f<int, double>();', {2: 'I_E1fIidEEv'})\n\n    # explicit instantiation of a member function\n    check('function', 'template void A<int>::f();', {2: 'IEN1AIiE1fEv'})\n    check('function', 'template void A<int>::B<double>::g();', {2: 'IEIEN1AIiE1BIiE1gEv'})\n\n    # explicit instantiation of a variable template\n    check('member', 'template int var<int>;', {2: 'I_E3varIiEE'})\n    check('member', 'template double var<double>;', {2: 'I_E3varIdEE'})\n\n    # explicit instantiation of a class template\n    check('class', 'template class A<int>;', {2: 'I_E1AIiE'})\n    check('class', 'template class B<double>;', {2: 'I_E1BIiE'})\n"], "sample_960": ["def test_parse_annotation_syntax_error(app):\n    # Test _parse_annotation for a syntactically incorrect annotation\n    doctree = _parse_annotation(\"invalid[syntax\", app.env)\n    assert_node(doctree, [pending_xref, \"invalid[syntax\"])\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"invalid[syntax\")\n", "def test_type_to_xref():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    # Case: type string 'None' which should use 'obj' role\n    node = type_to_xref('None', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n    assert_node(node[0], nodes.Text, text='None')\n\n    # Case: regular type string\n    node = type_to_xref('int', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='int')\n    assert_node(node[0], nodes.Text, text='int')\n\n    # Case: qualified type name\n    node = type_to_xref('typing.List', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='typing.List')\n    assert_node(node[0], nodes.Text, text='typing.List')\n\n    # Case: unqualified type names enabled\n    env.config.python_use_unqualified_type_names = True\n    node = type_to_xref('typing.List', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='typing.List')\n    assert_node(node[0], pending_xref_condition, condition='resolved')\n    assert_node(node[1], pending_xref_condition, condition='*')\n    assert_node(node[0][0], nodes.Text, text='List')\n    assert_node(node[1][0], nodes.Text, text='typing.List')\n", "def test_pyfield_make_xref():\n    field = PyField('returntype', label=_('Return type'), has_arg=False,\n                    names=('rtype',), bodyrolename='class')\n    env = Mock()\n    field.make_xref('class', 'py', 'int', nodes.emphasis, nodes.Text('int'), env)\n    \n    result = field.make_xref('class', 'py', 'int', nodes.emphasis, nodes.Text('int'), env)\n    assert isinstance(result, pending_xref)\n    assert result['reftype'] == 'class'\n    assert result['reftarget'] == 'int'\n    assert result.astext() == 'int'\n", "def test_pyxrefmixin_make_xref():\n    env = Mock()\n    env.ref_context = {'py:module': 'test_module', 'py:class': 'TestClass'}\n    \n    class TestPyXrefMixin(PyXrefMixin):\n            return super().make_xref(rolename, domain, target, innernode, contnode, env, inliner, location)\n        \n    mixin = TestPyXrefMixin()\n    \n    refnode = mixin.make_xref('class', 'py', 'test_target', env=env)\n    assert refnode['refspecific'] == True\n    assert refnode['py:module'] == 'test_module'\n    assert refnode['py:class'] == 'TestClass'\n    assert refnode['reftarget'] == 'test_target'\n    assert isinstance(refnode[0], pending_xref_condition)\n    \n    refnode = mixin.make_xref('class', 'py', '.test_target', env=env)\n    assert refnode['reftarget'] == 'test_target'\n    assert isinstance(refnode[0], nodes.Text)\n    assert refnode[0].astext() == 'test_target'\n    \n    refnode = mixin.make_xref('class', 'py', '~test.target', env=env)\n    assert refnode['reftarget'] == 'test.target'\n    assert isinstance(refnode[0], nodes.Text)\n    assert refnode[0].astext() == 'target'\n", "def test_pyfunction_with_various_annotations(app):\n    text = (\".. py:function:: mixed_annotations(a: int | None, b: List[str], c: Tuple[int, ...], \"\n            \"d: Dict[str, int], e: Optional[float] = None) -> bool\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"mixed_annotations\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"bool\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"a\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, ([pending_xref, \"int\"],\n                                                                         \" \",\n                                                                         [desc_sig_punctuation, \"|\"],\n                                                                         \" \",\n                                                                         [pending_xref, \"None\"])])],\n                                      [desc_parameter, ([desc_sig_name, \"b\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, ([pending_xref, \"List\"],\n                                                                         [desc_sig_punctuation, \"[\"],\n                                                                         [pending_xref, \"str\"],\n                                                                         [desc_sig_punctuation, \"]\"])])],\n                                      [desc_parameter, ([desc_sig_name, \"c\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, ([pending_xref, \"Tuple\"],\n                                                                         [desc_sig_punctuation, \"[\"],\n                                                                         [pending_xref, \"int\"],\n                                                                         [desc_sig_punctuation, \", \"],\n                                                                         [desc_sig_punctuation, \"...\"],\n                                                                         [desc_sig_punctuation, \"]\"])])],\n                                      [desc_parameter, ([desc_sig_name, \"d\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, ([pending_xref, \"Dict\"],\n                                                                         [desc_sig_punctuation, \"[\"],\n                                                                         [pending_xref, \"str\"],\n                                                                         [desc_sig_punctuation, \", \"],\n                                                                         [pending_xref, \"int\"],\n                                                                         [desc_sig_punctuation, \"]\"])])],\n                                      [desc_parameter, ([desc_sig_name, \"e\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig"], "sample_961": ["def test_pyfunction_with_annotations(app):\n    text = (\".. py:function:: hello(name: str, age: int = 21, active: bool = True) -> str\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"str\"])],\n                                      [desc_parameter, ([desc_sig_name, \"age\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"int\"],\n                                                        \" \",\n                                                        [desc_sig_operator, \"=\"],\n                                                        \" \",\n                                                        [nodes.inline, \"21\"])],\n                                      [desc_parameter, ([desc_sig_name, \"active\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"bool\"],\n                                                        \" \",\n                                                        [desc_sig_operator, \"=\"],\n                                                        \" \",\n                                                        [nodes.inline, \"True\"])])])\n", "def test_parse_annotation_invalid_syntax(app):\n    # Invalid syntax for type annotation should return it as a plain text node\n    doctree = _parse_annotation(\"Invalid[syntax\", app.env)\n    assert_node(doctree, [nodes.Text, \"Invalid[syntax\"])\n\n    doctree = _parse_annotation(\"Unclosed[Bracket\", app.env)\n    assert_node(doctree, [nodes.Text, \"Unclosed[Bracket\"])\n\n    doctree = _parse_annotation(\"Mismatched[Bracket]\", app.env)\n    assert_node(doctree, [nodes.Text, \"Mismatched[Bracket]\"])\n", "def test_pyfunction_return_annotation(app):\n    text = \".. py:function:: hello(name: str) -> Tuple[int, str]\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, ([pending_xref, \"Tuple\"],\n                                                                    [desc_sig_punctuation, \"[\"],\n                                                                    [pending_xref, \"int\"],\n                                                                    [desc_sig_punctuation, \", \"],\n                                                                    [pending_xref, \"str\"],\n                                                                    [desc_sig_punctuation, \"]\"])])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][2],\n                ([pending_xref, \"Tuple\"],\n                 [desc_sig_punctuation, \"[\"],\n                 [pending_xref, \"int\"],\n                 [desc_sig_punctuation, \", \"],\n                 [pending_xref, \"str\"],\n                 [desc_sig_punctuation, \"]\"]))\n", "def test_type_to_xref():\n    env = Mock()\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = False\n    env.ref_context = {'py:module': 'module', 'py:class': 'Class'}\n\n    # Test with a simple class reference\n    xref = type_to_xref(\"int\", env)\n    assert_node(xref, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    assert_node(xref[0], nodes.Text, \"int\")\n\n    # Test with a None type reference\n    xref = type_to_xref(\"None\", env)\n    assert_node(xref, pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n    assert_node(xref[0], nodes.Text, \"None\")\n\n    # Test with unqualified type names enabled\n    env.config.python_use_unqualified_type_names = True\n    xref = type_to_xref(\"module.Class\", env)\n    assert_node(xref, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"module.Class\")\n    assert_node(xref[0], pending_xref_condition, condition='resolved')\n    assert_node(xref[0][0], nodes.Text, \"Class\")\n    assert_node(xref[1], pending_xref_condition, condition='*')\n    assert_node(xref[1][0], nodes.Text, \"module.Class\")\n\n    # Test without env\n    xref = type_to_xref(\"List[int]\")\n    assert_node(xref, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"List[int]\")\n    assert_node(xref[0], nodes.Text, \"List[int]\")\n", "def test_pymodule_signature(app):\n    text = (\".. py:module:: example.module\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :synopsis: Example module for testing\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index))\n    \n    assert 'example.module' in domain.modules\n    assert domain.modules['example.module'] == ('index', 'module-example.module', \n                                                'Example module for testing', 'Unix', True)\n"], "sample_962": ["def test_ismock():\n    from sphinx.ext.autodoc.mock import _MockObject, _MockModule, ismock\n\n    mock_obj = _MockObject()\n    mock_module = _MockModule('mock_module')\n    \n    assert ismock(mock_obj) is True\n    assert ismock(mock_module) is True\n    assert ismock(\"not a mock\") is False\n    assert ismock(123) is False\n    assert ismock(None) is False\n\n    class RealClass:\n        pass\n\n    real_instance = RealClass()\n    assert ismock(real_instance) is False\n", "def test_mockobject_methods():\n    mock_obj = _MockObject()\n    assert mock_obj.__len__() == 0\n    assert 'key' not in mock_obj\n    assert list(mock_obj.__iter__()) == []\n    assert mock_obj.__getitem__('key').__name__ == 'key'\n    assert mock_obj.__getattr__('attribute').__name__ == 'attribute'\n    assert isinstance(mock_obj.__call__(), _MockObject)\n    assert repr(mock_obj) == '_MockObject'\n", "def test_ismock():\n    from sphinx.ext.autodoc.mock import _MockObject, _MockModule, ismock\n\n    mock_obj = _MockObject()\n    mock_module = _MockModule(\"mock_module\")\n    non_mock_obj = MyClass1()\n\n    assert ismock(mock_obj) is True\n    assert ismock(mock_module) is True\n    assert ismock(non_mock_obj) is False\n", "def test_mock_object_behavior():\n    # Test the basic behavior of _MockObject\n    mock_obj = _MockObject()\n    assert len(mock_obj) == 0\n    assert 'key' not in mock_obj\n    assert list(iter(mock_obj)) == []\n    assert repr(mock_obj) == \"_MockObject\"\n\n    # Test __getitem__ method\n    item = mock_obj['item']\n    assert isinstance(item, _MockObject)\n    assert item.__name__ == 'item'\n\n    # Test __getattr__ method\n    attr = mock_obj.some_attr\n    assert isinstance(attr, _MockObject)\n    assert attr.__name__ == 'some_attr'\n\n    # Test __call__ method\n    call_obj = mock_obj()\n    assert isinstance(call_obj, _MockObject)\n\n    # Test __mro_entries__ method\n    assert mock_obj.__mro_entries__(()) == (_MockObject,)\n\n", "def test_mock_object_behaviors():\n    mock_obj = _MockObject()\n\n    # Test __len__\n    assert len(mock_obj) == 0\n\n    # Test __contains__\n    assert 'key' not in mock_obj\n\n    # Test __iter__\n    assert list(iter(mock_obj)) == []\n\n    # Test __mro_entries__\n    assert mock_obj.__mro_entries__(()) == (_MockObject,)\n\n    # Test __getitem__\n    item = mock_obj['key']\n    assert isinstance(item, _MockObject)\n    assert item.__name__ == 'key'\n\n    # Test __getattr__\n    attr = mock_obj.some_attribute\n    assert isinstance(attr, _MockObject)\n    assert attr.__name__ == 'some_attribute'\n\n    # Test __call__\n    called_obj = mock_obj()\n    assert isinstance(called_obj, _MockObject)\n\n    # Test __repr__\n    assert repr(mock_obj) == '_MockObject'\n"], "sample_963": ["def test_restify_invalid_builtin_classes():\n    assert restify(Struct) == \":py:class:`struct.Struct`\"\n    assert restify(TracebackType) == \":py:class:`types.TracebackType`\"\n", "def test_restify_custom_forward_ref():\n    class MyClass:\n        pass\n\n    forward_ref = ForwardRef(\"MyClass\")\n\n    if sys.version_info >= (3, 7):\n        assert restify(forward_ref) == \":py:class:`MyClass`\"\n    else:\n        assert restify(forward_ref) == \":py:class:`~MyClass`\"\n", "def test_get_type_hints():\n    from sphinx.util.typing import get_type_hints\n    from sphinx.util.inspect import safe_getattr\n\n    class TestClass:\n        attr: int\n            pass\n\n        pass\n\n    global_ns = globals()\n    local_ns = locals()\n\n    obj = TestClass()\n    assert get_type_hints(obj) == safe_getattr(obj, '__annotations__', {})\n    assert get_type_hints(obj.method) == {'param': str, 'return': bool}\n    assert get_type_hints(broken_method, global_ns, local_ns) == {'param': 'UnknownType', 'return': bool}\n", "def test_stringify_forward_ref():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"MyClass1\")) == \"MyClass1\"\n", "def test_stringify_broken_type_hints():\n    assert stringify(BrokenType) == \"tests.test_util_typing.BrokenType\"\n"], "sample_964": ["def test_pyfunction_with_keyword_only_arguments(app):\n    text = \".. py:function:: hello(*, a, b=1)\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist)],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, nodes.inline, \"*\"],\n                                      [desc_parameter, ([desc_sig_name, \"a\"])],\n                                      [desc_parameter, ([desc_sig_name, \"b\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"1\"])])])\n\n    assert 'hello' in domain.objects\n    assert domain.objects['hello'] == ('index', 'hello', 'function', False)\n", "def test_pyxrefmixin_make_xref():\n    env = Mock()\n    env.ref_context = {'py:module': 'testmod', 'py:class': 'TestClass'}\n    env.config.python_use_unqualified_type_names = False\n\n    mixin = PyXrefMixin()\n    mixin.make_xref = Mock()\n    mixin.make_xref.return_value = pending_xref('', nodes.Text('TestClass'), refdomain='py', reftype='class', reftarget='TestClass')\n\n    node = mixin.make_xref('class', 'py', 'TestClass', nodes.emphasis, nodes.Text('TestClass'), env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='TestClass')\n    assert node['refspecific'] is True\n    assert node['py:module'] == 'testmod'\n    assert node['py:class'] == 'TestClass'\n\n    mixin.make_xref.return_value = pending_xref('', nodes.Text('TestClass'), refdomain='py', reftype='obj', reftarget='TestClass')\n    node = mixin.make_xref('obj', 'py', 'None', nodes.emphasis, nodes.Text('None'), env)\n    assert_node(node, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n\n    env.config.python_use_unqualified_type_names = True\n    mixin.make_xref.return_value = pending_xref('', nodes.Text('TestClass'), refdomain='py', reftype='class', reftarget='testmod.TestClass')\n    node = mixin.make_xref('class', 'py', 'testmod.TestClass', nodes.emphasis, nodes.Text('TestClass'), env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='testmod.TestClass')\n    assert node[0][0].astext() == 'TestClass'\n", "def test_pyfunction_signature_no_return(app):\n    text = \".. py:function:: hello(name: str)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist)],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      desc_sig_space,\n                                                      [nodes.inline, pending_xref, \"str\"])])\n", "def test_pycurrentmodule_directive(app):\n    text = (\".. py:currentmodule:: example\\n\"\n            \".. py:function:: foo()\\n\"\n            \".. py:currentmodule:: example.submodule\\n\"\n            \".. py:function:: bar()\\n\"\n            \".. py:currentmodule:: None\\n\"\n            \".. py:function:: baz()\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, ([nodes.target, addnodes.index, addnodes.index, desc], \n                          [nodes.target, addnodes.index, addnodes.index, desc],\n                          [nodes.target, addnodes.index, addnodes.index, desc]))\n                          \n    assert 'example.foo' in domain.objects\n    assert domain.objects['example.foo'] == ('index', 'example.foo', 'function', False)\n\n    assert 'example.submodule.bar' in domain.objects\n    assert domain.objects['example.submodule.bar'] == ('index', 'example.submodule.bar', 'function', False)\n\n    assert 'baz' in domain.objects\n    assert domain.objects['baz'] == ('index', 'baz', 'function', False)\n", "def test_type_to_xref():\n    env = Mock()\n    env.ref_context = {'py:module': 'mymodule', 'py:class': 'MyClass'}\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    # Test with 'None'\n    node = type_to_xref('None', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n    assert_node(node[0], nodes.Text, 'None')\n\n    # Test with a fully-qualified class name\n    node = type_to_xref('mymodule.MyClass', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='mymodule.MyClass')\n    assert_node(node[0], nodes.Text, 'mymodule.MyClass')\n\n    # Test with unqualified class name when python_use_unqualified_type_names is True\n    env.config.python_use_unqualified_type_names = True\n    node = type_to_xref('mymodule.MyClass', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='mymodule.MyClass')\n    assert_node(node[0], pending_xref_condition, condition='resolved')\n    assert_node(node[0][0], nodes.Text, 'MyClass')\n    assert_node(node[1], pending_xref_condition, condition='*')\n    assert_node(node[1][0], nodes.Text, 'mymodule.MyClass')\n\n    # Test with plain class name\n    node = type_to_xref('MyClass', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='MyClass')\n    assert_node(node[0], pending_xref_condition, condition='resolved')\n    assert_node(node[0][0], nodes.Text, 'MyClass')\n    assert_node(node[1], pending_xref_condition, condition='*')\n    assert_node(node[1][0], nodes.Text, 'MyClass')\n"], "sample_965": ["def test_getannotations():\n    class Foo:\n        __annotations__ = {'a': int, 'b': str}\n    \n    class Bar:\n        pass\n\n    obj_with_annotations = Foo()\n    obj_without_annotations = Bar()\n    \n    annotations = inspect.getannotations(obj_with_annotations)\n    assert annotations == {'a': int, 'b': str}\n    \n    annotations = inspect.getannotations(obj_without_annotations)\n    assert annotations == {}\n    \n    obj_with_invalid_annotations = Foo()\n    obj_with_invalid_annotations.__annotations__ = 'invalid'\n    \n    annotations = inspect.getannotations(obj_with_invalid_annotations)\n    assert annotations == {}\n", "def test_getargspec():\n        pass\n\n    fullargspec = inspect.getargspec(sample_function)\n    assert fullargspec.args == ['a', 'b', 'c', 'd']\n    assert fullargspec.varargs == 'e'\n    assert fullargspec.varkw == 'f'\n    assert fullargspec.defaults == (1, 2)\n    assert fullargspec.kwonlyargs == []\n    assert fullargspec.kwdefaults is None\n    assert fullargspec.annotations == {}\n\n        pass\n\n    fullargspec = inspect.getargspec(annotated_function)\n    assert fullargspec.args == ['a', 'b', 'c']\n    assert fullargspec.varargs is None\n    assert fullargspec.varkw is None\n    assert fullargspec.defaults == (1.0,)\n    assert fullargspec.kwonlyargs == []\n    assert fullargspec.kwdefaults is None\n    assert fullargspec.annotations == {'a': int, 'b': str, 'c': float, 'return': None}\n", "def test_isNewType():\n    NewType = typing.NewType('NewType', int)\n    assert inspect.isNewType(NewType) is True\n    assert inspect.isNewType(int) is False\n    assert inspect.isNewType('not a NewType') is False\n", "def test_getargspec():\n        \"\"\"Test function\"\"\"\n        pass\n\n    # Test a normal function\n    spec = inspect.getargspec(func)\n    assert spec.args == ['a', 'b', 'c', 'd']\n    assert spec.varargs == 'args'\n    assert spec.varkw == 'kwargs'\n    assert spec.defaults == (1, 2)\n    assert spec.kwonlyargs == []\n    assert spec.kwdefaults is None\n    assert spec.annotations == {}\n\n    # Test a bound method\n    class Foo:\n            \"\"\"Test method\"\"\"\n            pass\n\n    bound_method = Foo().method\n    spec = inspect.getargspec(bound_method)\n    assert spec.args == ['self', 'x', 'y']\n    assert spec.varargs is None\n    assert spec.varkw is None\n    assert spec.defaults == (10,)\n    assert spec.kwonlyargs == []\n    assert spec.kwdefaults is None\n    assert spec.annotations == {}\n\n    # Test a function with annotations\n        pass\n\n    spec = inspect.getargspec(annotated_func)\n    assert spec.args == ['a', 'b']\n    assert spec.varargs == 'args'\n    assert spec.varkw == 'kwargs'\n    assert spec.defaults == ('default',)\n    assert spec.kwonlyargs == []\n    assert spec.kwdefaults is None\n    assert spec.annotations == {'a': int, 'b': str, 'args': list, 'kwargs': dict, 'return': None}\n\n    # Test a deprecated function warning\n    with pytest.warns(inspect.RemovedInSphinx50Warning):\n        inspect.getargspec(func)\n", "def test_getmro():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    assert inspect.getmro(C) == (C, B, A, object)\n    assert inspect.getmro(B) == (B, A, object)\n    assert inspect.getmro(A) == (A, object)\n    assert inspect.getmro(object) == (object,)\n"], "sample_966": ["def test_parse_annotation_subscript(app):\n    doctree = _parse_annotation(\"Dict[str, List[int]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"Union[Optional[int], str]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Optional\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_pyvariable_signature(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:variable:: my_var\\n\"\n            \"   :type: int\\n\"\n            \"   :value: 42\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"example.\"],\n                                                    [desc_name, \"my_var\"],\n                                                    [desc_annotation, ([desc_sig_punctuation, ':'],\n                                                                       desc_sig_space,\n                                                                       [pending_xref, \"int\"])],\n                                                    [desc_annotation, (desc_sig_space,\n                                                                       [desc_sig_punctuation, '='],\n                                                                       desc_sig_space,\n                                                                       \"42\")]\n                                                    )],\n                                  [desc_content, ()])]))\n    assert_node(doctree[3][0][2], pending_xref, **{\"py:module\": \"example\"})\n    assert 'example.my_var' in domain.objects\n    assert domain.objects['example.my_var'] == ('index', 'example.my_var', 'data', False)\n", "def test_parse_annotation_with_subscript(app):\n    # Test parsing of subscripted types\n    doctree = _parse_annotation(\"List[Dict[str, int]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n                          \n    # Test parsing of complex nested subscripted types\n    doctree = _parse_annotation(\"Dict[str, List[Tuple[int, Union[str, None]]]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"None\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_pyfunction_with_default_arguments(app):\n    text = (\".. py:function:: func(a, b=True, c='default', d=3.14)\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    desc_parameterlist)],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\", domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1], [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"a\"])],\n                                                        [desc_parameter, ([desc_sig_name, \"b\"],\n                                                                          [desc_sig_operator, \"=\"],\n                                                                          [nodes.inline, \"True\"])],\n                                                        [desc_parameter, ([desc_sig_name, \"c\"],\n                                                                          [desc_sig_operator, \"=\"],\n                                                                          [nodes.inline, \"'default'\"])],\n                                                        [desc_parameter, ([desc_sig_name, \"d\"],\n                                                                          [desc_sig_operator, \"=\"],\n                                                                          [nodes.inline, \"3.14\"])])])\n    assert 'func' in domain.objects\n    assert domain.objects['func'] == ('index', 'func', 'function', False)\n", "def test_parse_annotation_with_subscript(app):\n    doctree = _parse_annotation(\"Dict[str, List[int]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n"], "sample_967": ["def test_mathjax_path_not_set(app, status, warning):\n    with pytest.raises(ExtensionError, match='mathjax_path config value must be set for the mathjax extension to work'):\n        app.builder.build_all()\n", "def test_custom_mathjax_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<script defer=\"defer\" src=\"custom/path/to/mathjax.js\">'\n            '</script>' in content)\n", "def test_mathjax_inline_delimiters(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<span class=\"math notranslate nohighlight\">\\\\(E=mc\\^2\\\\)</span>')\n    assert re.search(html, content, re.S)\n", "def test_mathjax_path_not_set(app, status, warning):\n    with pytest.raises(ExtensionError, match='mathjax_path config value must be set for the mathjax extension to work'):\n        app.builder.build_all()\n", "def test_mathjax_inline_config(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<span class=\"math notranslate nohighlight\">'\n            r'\\(E=mc\\^2\\)</span>')\n    assert re.search(html, content, re.S)\n"], "sample_968": ["def test_type_to_xref():\n    env = Mock()\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = True\n    env.ref_context = {'py:module': 'module', 'py:class': 'Class'}\n\n    xref = type_to_xref('int', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='int', refspecific=False)\n    assert_node(xref[0], pending_xref_condition, condition='resolved')\n    assert_node(xref[1], pending_xref_condition, condition='*')\n\n    xref = type_to_xref('~module.Class', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='module.Class', refspecific=False)\n    assert_node(xref[0], pending_xref_condition, condition='resolved')\n    assert_node(xref[1], pending_xref_condition, condition='*')\n\n    xref = type_to_xref('.Class', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='Class', refspecific=True)\n    assert_node(xref[0], pending_xref_condition, condition='resolved')\n    assert_node(xref[1], pending_xref_condition, condition='*')\n\n    env.config.python_use_unqualified_type_names = False\n    xref = type_to_xref('int', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='int', refspecific=False)\n    assert_node(xref[0], nodes.Text, 'int')\n", "def test_parse_annotation_with_env_none():\n    # Test the case when env parameter is None\n    with pytest.warns(RemovedInSphinx50Warning, match=\"The env parameter for _parse_annotation becomes required now.\"):\n        doctree = _parse_annotation(\"int\", None)\n        assert_node(doctree, ([pending_xref, \"int\"],))\n        assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n\n    with pytest.warns(RemovedInSphinx50Warning, match=\"The env parameter for _parse_annotation becomes required now.\"):\n        doctree = _parse_annotation(\"List[int]\", None)\n        assert_node(doctree, ([pending_xref, \"List\"],\n                              [desc_sig_punctuation, \"[\"],\n                              [pending_xref, \"int\"],\n                              [desc_sig_punctuation, \"]\"]))\n\n    with pytest.warns(RemovedInSphinx50Warning, match=\"The env parameter for _parse_annotation becomes required now.\"):\n        doctree = _parse_annotation(\"Tuple[int, int]\", None)\n        assert_node(doctree, ([pending_xref, \"Tuple\"],\n                              [desc_sig_punctuation, \"[\"],\n                              [pending_xref, \"int\"],\n                              [desc_sig_punctuation, \",\"],\n                              desc_sig_space,\n                              [pending_xref, \"int\"],\n                              [desc_sig_punctuation, \"]\"]))\n", "def test_type_to_xref():\n    env = Mock()\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = False\n    env.ref_context = {'py:module': 'mod', 'py:class': 'Class'}\n    \n    # Test with target 'None'\n    xref = type_to_xref('None', env)\n    assert isinstance(xref, addnodes.pending_xref)\n    assert xref['reftype'] == 'obj'\n    assert xref['reftarget'] == 'None'\n    assert xref['py:module'] == 'mod'\n    assert xref['py:class'] == 'Class'\n    assert xref['refspecific'] is False\n    \n    # Test with target starting with '.'\n    xref = type_to_xref('.InnerClass', env)\n    assert xref['reftarget'] == 'InnerClass'\n    assert xref['refspecific'] is True\n    assert isinstance(xref[0], pending_xref_condition)\n    assert xref[0].astext() == 'InnerClass'\n\n    # Test with target starting with '~'\n    xref = type_to_xref('~mod.Class', env)\n    assert xref['reftarget'] == 'mod.Class'\n    assert xref['refspecific'] is False\n    assert isinstance(xref[0], nodes.Text)\n    assert xref[0].astext() == 'Class'\n    \n    # Test with normal target\n    xref = type_to_xref('mod.Class.InnerClass', env)\n    assert xref['reftarget'] == 'mod.Class.InnerClass'\n    assert xref['refspecific'] is False\n    assert isinstance(xref[0], nodes.Text)\n    assert xref[0].astext() == 'mod.Class.InnerClass'\n    \n    # Test with unqualified type names\n    env.config.python_use_unqualified_type_names = True\n    xref = type_to_xref('mod.Class.InnerClass', env)\n    assert xref['reftarget'] == 'mod.Class.InnerClass'\n    assert xref['refspecific'] is False\n    assert isinstance(xref[0], pending_xref_condition)\n    assert xref[0].astext() == 'InnerClass'\n", "def test_pyvariable_signature(app):\n    text = (\".. py:variable:: MY_VAR\\n\"\n            \"   :type: str\\n\"\n            \"   :value: 'Hello World'\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"MY_VAR\"],\n                                                    [desc_annotation, ([desc_sig_punctuation, ':'],\n                                                                       desc_sig_space,\n                                                                       [pending_xref, \"str\"])],\n                                                    [desc_annotation, (desc_sig_space,\n                                                                       [desc_sig_punctuation, '='],\n                                                                       desc_sig_space,\n                                                                       nodes.Text(\"'Hello World'\")])\n                                                    )],\n                                  [desc_content, ()])]))\n    assert 'MY_VAR' in domain.objects\n    assert domain.objects['MY_VAR'] == ('index', 'MY_VAR', 'data', False)\n", "def test_type_to_xref():\n    env = Mock()\n    env.ref_context = {'py:module': 'example.module', 'py:class': 'ExampleClass'}\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    refnode = type_to_xref('int', env)\n    assert_node(refnode, refdomain='py', reftype='class', reftarget='int', refspecific=False)\n    assert_node(refnode[0], nodes.Text, text='int')\n\n    refnode = type_to_xref('example.module.ExampleClass', env)\n    assert_node(refnode, refdomain='py', reftype='class', reftarget='example.module.ExampleClass', refspecific=False)\n    assert_node(refnode[0], nodes.Text, text='example.module.ExampleClass')\n\n    refnode = type_to_xref('~example.module.ExampleClass', env)\n    assert_node(refnode, refdomain='py', reftype='class', reftarget='example.module.ExampleClass', refspecific=False)\n    assert_node(refnode[0], nodes.Text, text='ExampleClass')\n\n    refnode = type_to_xref('.ExampleClass', env)\n    assert_node(refnode, refdomain='py', reftype='class', reftarget='ExampleClass', refspecific=True)\n    assert_node(refnode[0], nodes.Text, text='ExampleClass')\n\n    env.config.python_use_unqualified_type_names = True\n    refnode = type_to_xref('example.module.ExampleClass', env)\n    assert_node(refnode, refdomain='py', reftype='class', reftarget='example.module.ExampleClass', refspecific=False)\n    assert_node(refnode[0][0], pending_xref_condition, condition='resolved', text='ExampleClass')\n    assert_node(refnode[0][1], pending_xref_condition, condition='*', text='example.module.ExampleClass')\n\n    refnode = type_to_xref('None', env)\n    assert_node(refnode, refdomain='py', reftype='obj', reftarget='None', refspecific=False)\n    assert_node(refnode[0], nodes.Text, text='None')\n"], "sample_969": ["def test_get_type_hints():\n    from sphinx.util.inspect import safe_getattr\n\n        pass\n\n        pass\n\n    assert get_type_hints(func) == {'x': int, 'y': str, 'return': bool}\n    assert get_type_hints(func_with_forward_ref, globals(), locals()) == {\n        'x': MyClass1,\n        'y': MyClass2,\n        'return': MyClass1\n    }\n\n    class TestClass:\n            pass\n\n    assert get_type_hints(TestClass.method) == {'x': int, 'y': str, 'return': bool}\n    \n    # Test with class annotations\n    class AnnotatedClass:\n        x: int\n        y: str\n    \n    assert get_type_hints(AnnotatedClass) == {'x': int, 'y': str}\n\n    # Test with invalid annotations\n    class InvalidAnnotatedClass:\n        x: 'InvalidType'\n    \n    assert get_type_hints(InvalidAnnotatedClass) == {'x': 'InvalidType'}\n\n    # Test with exceptions\n    class ExceptionAnnotatedClass:\n        x = 1\n\n        @property\n            raise AttributeError\n\n    assert get_type_hints(ExceptionAnnotatedClass) == {}\n", "def test_restify_invalid_builtin_class():\n    class CustomStruct(Struct):\n        pass\n\n    class CustomTracebackType(TracebackType):\n        pass\n\n    assert restify(CustomStruct) == \":py:class:`struct.Struct`\"\n    assert restify(CustomTracebackType) == \":py:class:`types.TracebackType`\"\n", "def test_get_type_hints():\n        return str(a) == b\n\n    class MyClass:\n        attr: int\n\n            return x * 2\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(MyClass) == {'attr': int}\n    assert get_type_hints(MyClass.method) == {'self': MyClass, 'x': float, 'return': float}\n\n    # Test with broken ForwardRef\n    globalns = {'MyClass': MyClass}\n    localns = {}\n    assert get_type_hints('MyClass', globalns, localns) == {}\n\n    # Test with module having invalid __annotations__\n    class InvalidAnnotationsClass:\n        __annotations__ = None\n\n    assert get_type_hints(InvalidAnnotationsClass) == {}\n", "def test_get_type_hints():\n        pass\n\n    class SampleClass:\n        attr: int\n\n            pass\n\n    assert get_type_hints(sample_func) == {'a': int, 'b': str, 'return': None}\n    assert get_type_hints(SampleClass) == {'attr': int}\n    assert get_type_hints(SampleClass.method) == {'self': SampleClass, 'x': int, 'return': str}\n\n    # Test with ForwardRef\n    if sys.version_info >= (3, 7):\n        forward_ref_func = Callable[['SampleClass'], None]\n        assert get_type_hints(forward_ref_func, {'SampleClass': SampleClass}) == {'return': None, 'args': (SampleClass,)}\n", "def test_restify_type_GenericAlias():\n    from typing import Generic\n    class CustomList(Generic[T]):\n        pass\n    assert restify(CustomList[int]) == \":py:class:`tests.test_util_typing.CustomList`\\\\ [:py:class:`int`]\"\n"], "sample_970": ["def test_getorigbases():\n    class Foo:\n        pass\n\n    class Bar(Foo):\n        pass\n\n    class Baz(Bar):\n        __orig_bases__ = (Bar,)\n\n    class Qux:\n        __orig_bases__ = (Foo, Bar)\n\n    assert inspect.getorigbases(Foo) is None\n    assert inspect.getorigbases(Bar) is None\n    assert inspect.getorigbases(Baz) == (Bar,)\n    assert inspect.getorigbases(Qux) == (Foo, Bar)\n", "def test_getall():\n    mod = types.ModuleType('testmodule')\n    mod.__all__ = ['foo', 'bar', 'baz']\n    assert inspect.getall(mod) == ['foo', 'bar', 'baz']\n\n    mod.__all__ = ('foo', 'bar', 'baz')\n    assert inspect.getall(mod) == ['foo', 'bar', 'baz']\n\n    mod.__all__ = 'foobar'\n    with pytest.raises(ValueError):\n        inspect.getall(mod)\n\n    del mod.__all__\n    assert inspect.getall(mod) is None\n", "def test_getmro():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    class D:\n        pass\n\n    assert inspect.getmro(A) == (A, object)\n    assert inspect.getmro(B) == (B, A, object)\n    assert inspect.getmro(C) == (C, B, A, object)\n    assert inspect.getmro(D) == (D, object)\n    assert inspect.getmro(1) == tuple()  # Non-class object\n", "def test_isNewType():\n    if sys.version_info >= (3, 10):\n        # NewType is directly available in typing module for Python 3.10+\n        from typing import NewType\n        UserId = NewType('UserId', int)\n        assert inspect.isNewType(UserId) is True\n        assert inspect.isNewType(int) is False\n        assert inspect.isNewType(\"some string\") is False\n    else:\n        # Manually create NewType-like objects for older Python versions\n        from typing import NewType\n        UserId = NewType('UserId', int)\n        assert inspect.isNewType(UserId) is True\n        assert inspect.isNewType(int) is False\n        assert inspect.isNewType(\"some string\") is False\n", "def test_isNewType():\n    if sys.version_info >= (3, 10):\n        MyType = typing.NewType('MyType', int)\n        assert inspect.isNewType(MyType) is True\n        assert inspect.isNewType(int) is False\n    else:\n            return type('NewType', (), {'__module__': 'typing', '__qualname__': 'NewType.<locals>.new_type'})\n        \n        MyType = mock_new_type('MyType', int)\n        assert inspect.isNewType(MyType) is True\n        assert inspect.isNewType(int) is False\n"], "sample_971": ["def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'prefix: WARNING: message1' in warning.getvalue()\n    assert 'prefix: WARNING: message2' in warning.getvalue()\n\n    # Test nested prefixed_warnings context managers\n    with prefixed_warnings(\"outer:\"):\n        with prefixed_warnings(\"inner:\"):\n            logger.warning('message3')\n        logger.warning('message4')\n\n    assert 'inner: WARNING: message3' in warning.getvalue()\n    assert 'outer: WARNING: message4' in warning.getvalue()\n\n    # Ensure no prefix outside of context manager\n    logger.warning('message5')\n    assert 'WARNING: message5' in warning.getvalue()\n    assert 'prefix: WARNING: message5' not in warning.getvalue()\n    assert 'outer: WARNING: message5' not in warning.getvalue()\n    assert 'inner: WARNING: message5' not in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        assert 'prefix: WARNING: message1' in warning.getvalue()\n\n    logger.warning('message2')\n    assert 'WARNING: message2' in warning.getvalue()\n    assert 'prefix: WARNING: message2' not in warning.getvalue()\n\n    with prefixed_warnings(\"new_prefix:\"):\n        logger.warning('message3')\n        assert 'new_prefix: WARNING: message3' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        logger.warning('message2')\n        \n    assert 'prefix: WARNING: message1' in warning.getvalue()\n    assert 'prefix: WARNING: message2' in warning.getvalue()\n\n    warning.truncate(0)\n    with logging.prefixed_warnings(\"new_prefix:\"):\n        logger.warning('message3')\n        \n    assert 'new_prefix: WARNING: message3' in warning.getvalue()\n", "def test_memory_handler(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    memhandler = logging.MemoryHandler()\n    logger.addHandler(memhandler)\n\n    logger.warning('message1')\n    logger.error('message2')\n    logger.critical('message3')\n\n    assert len(memhandler.buffer) == 3\n    assert memhandler.buffer[0].getMessage() == 'message1'\n    assert memhandler.buffer[1].getMessage() == 'message2'\n    assert memhandler.buffer[2].getMessage() == 'message3'\n\n    memhandler.clear()\n    assert len(memhandler.buffer) == 0\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        logger.error('message2')\n        logger.critical('message3')\n    assert 'prefix: WARNING: message1' in warning.getvalue()\n    assert 'prefix: ERROR: message2' in warning.getvalue()\n    assert 'prefix: CRITICAL: message3' in warning.getvalue()\n\n    # Ensure prefix does not affect subsequent logs\n    logger.warning('message4')\n    assert 'prefix: WARNING: message4' not in warning.getvalue()\n    assert 'WARNING: message4' in warning.getvalue()\n"], "sample_972": ["def test_get_type_hints():\n    from sphinx.util.typing import get_type_hints\n    from sphinx.util.inspect import safe_getattr\n    \n        pass\n    \n    class MyClass:\n            pass\n        \n    assert get_type_hints(func_with_hints) == {'a': int, 'b': str, 'return': None}\n    assert get_type_hints(MyClass.method_with_hints, MyClass.__dict__) == {'x': List[int], 'return': Dict[str, int]}\n    \n    # Test for broken class\n    class BrokenClass:\n        __annotations__ = 'broken'\n    \n    assert get_type_hints(BrokenClass) == {}\n    assert get_type_hints('not_a_class') == {}\n", "def test_get_type_hints(obj, expected):\n    from sphinx.util.typing import get_type_hints\n\n    assert get_type_hints(obj) == expected\n", "def test_get_type_hints():\n    class Sample:\n        attr: int\n\n            pass\n\n    sample_instance = Sample()\n\n    # Test getting type hints for class and method\n    assert get_type_hints(Sample) == {'attr': int}\n    assert get_type_hints(Sample.method) == {'param': str, 'return': bool}\n    assert get_type_hints(sample_instance.method) == {'param': str, 'return': bool}\n\n    # Test when object has no annotations\n    class NoAnnotations:\n            pass\n\n    no_annotations_instance = NoAnnotations()\n    assert get_type_hints(NoAnnotations) == {}\n    assert get_type_hints(NoAnnotations.no_annotations_method) == {}\n    assert get_type_hints(no_annotations_instance.no_annotations_method) == {}\n\n    # Test with invalid object\n    assert get_type_hints(\"invalid_object\") == {}\n", "def test_get_type_hints():\n    from sphinx.util.typing import get_type_hints\n\n        pass\n\n    class SampleClass:\n        attr: int\n\n            return str(a)\n\n    assert get_type_hints(sample_function) == {'a': int, 'b': str, 'return': None}\n    assert get_type_hints(SampleClass) == {'attr': int}\n    assert get_type_hints(SampleClass.method) == {'a': int, 'return': str}\n", "def test_get_type_hints():\n    from sphinx.util.typing import get_type_hints\n\n        pass\n\n    class TestClass:\n        attr: Optional[int]\n\n            pass\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': None}\n    assert get_type_hints(TestClass) == {'attr': Optional[int]}\n    assert get_type_hints(TestClass.method) == {'x': List[str], 'return': Dict[str, int]}\n"], "sample_973": ["def test_isabstractmethod():\n    class AbstractBase:\n        @staticmethod\n        @abc.abstractmethod\n            pass\n\n        @classmethod\n        @abc.abstractmethod\n            pass\n\n        @abc.abstractmethod\n            pass\n\n    class ConcreteClass(AbstractBase):\n            pass\n\n            pass\n\n            pass\n\n    assert inspect.isabstractmethod(AbstractBase.abstract_static_method) is True\n    assert inspect.isabstractmethod(AbstractBase.abstract_class_method) is True\n    assert inspect.isabstractmethod(AbstractBase.abstract_instance_method) is True\n\n    concrete = ConcreteClass()\n    assert inspect.isabstractmethod(concrete.abstract_static_method) is False\n    assert inspect.isabstractmethod(concrete.abstract_class_method) is False\n    assert inspect.isabstractmethod(concrete.abstract_instance_method) is False\n", "def test_isabstractmethod():\n    class Foo:\n        @staticmethod\n            pass\n\n        @classmethod\n            pass\n\n            pass\n\n        @property\n            pass\n\n        @property\n            raise NotImplementedError\n        abstractprop.__isabstractmethod__ = True\n\n        @staticmethod\n            raise NotImplementedError\n        abstractstaticmeth.__isabstractmethod__ = True\n\n        @classmethod\n            raise NotImplementedError\n        abstractclassmeth.__isabstractmethod__ = True\n\n            raise NotImplementedError\n        abstractmeth.__isabstractmethod__ = True\n\n    assert inspect.isabstractmethod(Foo.staticmeth) is False\n    assert inspect.isabstractmethod(Foo.classmeth) is False\n    assert inspect.isabstractmethod(Foo.regularmeth) is False\n    assert inspect.isabstractmethod(Foo.prop) is False\n\n    assert inspect.isabstractmethod(Foo.abstractprop) is True\n    assert inspect.isabstractmethod(Foo.abstractstaticmeth) is True\n    assert inspect.isabstractmethod(Foo.abstractclassmeth) is True\n    assert inspect.isabstractmethod(Foo.abstractmeth) is True\n", "def test_getall():\n    import os\n    import collections\n\n    # Test with a module that has __all__\n    module_with_all = collections\n    assert inspect.getall(module_with_all) is not None\n\n    # Test with a module that does not have __all__\n    module_without_all = os\n    assert inspect.getall(module_without_all) is None\n\n    # Test with an invalid __all__\n    class InvalidAll:\n        __all__ = [1, 2, 3]\n\n    with pytest.raises(ValueError):\n        inspect.getall(InvalidAll)\n\n    # Test with a valid __all__\n    class ValidAll:\n        __all__ = ['a', 'b', 'c']\n\n    assert inspect.getall(ValidAll) == ['a', 'b', 'c']\n", "def test_getargspec():\n        return a + b + c\n\n    fullargspec = inspect.getargspec(sample_function)\n\n    assert fullargspec.args == ['a', 'b', 'c']\n    assert fullargspec.varargs == 'args'\n    assert fullargspec.varkw == 'kwargs'\n    assert fullargspec.defaults == (3,)\n    assert fullargspec.kwonlyargs == []\n    assert fullargspec.kwdefaults is None\n    assert fullargspec.annotations == {}\n\n        return str(x) == y\n\n    fullargspec = inspect.getargspec(annotated_function)\n    \n    assert fullargspec.args == ['x', 'y']\n    assert fullargspec.varargs is None\n    assert fullargspec.varkw is None\n    assert fullargspec.defaults == ('default',)\n    assert fullargspec.kwonlyargs == []\n    assert fullargspec.kwdefaults is None\n    assert fullargspec.annotations == {'x': int, 'y': str, 'return': bool}\n", "def test_isroutine():\n    from target.functions import builtin_func, func, partial_builtin_func, partial_func\n    from target.methods import Base\n\n    assert inspect.isroutine(func) is True                     # function\n    assert inspect.isroutine(partial_func) is True             # partial-ed function\n    assert inspect.isroutine(builtin_func) is True             # builtin function\n    assert inspect.isroutine(partial_builtin_func) is True     # partial-ed builtin function\n    assert inspect.isroutine(Base.meth) is True                # method of class\n    assert inspect.isroutine(Base.partialmeth) is True         # partial-ed method of class\n    assert inspect.isroutine(Base().meth) is False             # method of instance\n"], "sample_974": ["def test_ccode_print_method():\n    class CustomFunc(Abs):\n            return \"custom_abs(%s)\" % printer._print(self.args[0])\n    assert ccode(CustomFunc(x)) == \"custom_abs(x)\"\n", "def test_ccode_AugmentedAssignment():\n    expr = aug_assign(x, '*', y + z)\n    assert ccode(expr) == 'x *= y + z;'\n    expr = aug_assign(x, '-', sin(y))\n    assert ccode(expr) == 'x -= sin(y);'\n    expr = aug_assign(x, '/', x**2 + y)\n    assert ccode(expr) == 'x /= pow(x, 2) + y;'\n", "def test_ccode_Exp1():\n    expr = exp(1)\n    assert ccode(expr) == 'M_E'\n    expr = exp(x)\n    assert ccode(expr) == 'exp(x)'\n    expr = exp(x + y)\n    assert ccode(expr) == 'exp(x + y)'\n", "def test_ccode_AugmentedAssignment():\n    expr = aug_assign(x, '-', y + z)\n    assert ccode(expr) == 'x -= y + z;'\n    expr = aug_assign(x, '*', y - z)\n    assert ccode(expr) == 'x *= y - z;'\n    expr = aug_assign(x, '/', y * z)\n    assert ccode(expr) == 'x /= y*z;'\n", "def test_ccode_For():\n    i = Idx('i', Range(0, 10))\n    j = Idx('j', Range(0, 5))\n    k = Idx('k', Range(0, 3))\n    x = IndexedBase('x', shape=(10, 5, 3))\n    y = IndexedBase('y', shape=(10, 5, 3))\n    \n    expr = For(i, Range(0, 10), For(j, Range(0, 5), For(k, Range(0, 3), Assignment(x[i, j, k], y[i, j, k] + 1))))\n    expected_code = (\n        'for (int i=0; i<10; i++){\\n'\n        '   for (int j=0; j<5; j++){\\n'\n        '      for (int k=0; k<3; k++){\\n'\n        '         x[i*5*3 + j*3 + k] = y[i*5*3 + j*3 + k] + 1;\\n'\n        '      }\\n'\n        '   }\\n'\n        '}'\n    )\n    assert ccode(expr) == expected_code\n"], "sample_975": ["def test_nsolve_logarithmic():\n    # Test for solving logarithmic equations using nsolve.\n    x = Symbol('x')\n    ans = nsolve(log(x) - 1, x, 2)\n    assert abs(ans - exp(1)) < 1e-15\n", "def test_unrad():\n    from sympy.abc import a, b, c, x, y, z\n    from sympy import Rational, root, sqrt, solve\n\n    # Test cases where unrad successfully removes radicals\n    eq, cov = unrad(sqrt(x)*x**Rational(1, 3) + 2)\n    assert eq == x**5 - 64\n    assert cov == []\n\n    eq, cov = unrad(sqrt(x) + root(x + 1, 3))\n    assert eq == x**3 - x**2 - 2*x - 1\n    assert cov == []\n\n    eq, cov = unrad(sqrt(x) + root(x, 3) - 2)\n    assert eq == _p**3 + _p**2 - 2\n    assert cov == [_p, _p**6 - x]\n\n    eq, cov = unrad(x**Rational(1, 3) + y**Rational(1, 4) - 1, x, y)\n    assert eq == y + _p**3 - 1\n    assert cov == [_p, _p**4 - x]\n\n    eq, cov = unrad(x**Rational(1, 2) + y**Rational(1, 2) - 1, x, y)\n    assert eq == x + y - 1\n    assert cov == []\n\n    # Test cases where unrad returns None indicating no radicals to remove\n    assert unrad(x**2 + y**2 - 1) is None\n    assert unrad(a*x**2 + b*x + c) is None\n\n    # Test cases where unrad raises NotImplementedError\n    raises(NotImplementedError, lambda: unrad(sqrt(x) + sqrt(y) + z))\n    raises(NotImplementedError, lambda: unrad(sqrt(x + y) + sqrt(x - y)))\n", "def test_checksol():\n    from sympy.abc import x, y\n\n    # True solution\n    assert checksol(x**2 - 1, x, 1) == True\n    assert checksol(x**2 - y**2, {x: 1, y: 1}) == True\n\n    # False solution\n    assert checksol(x**2 - 1, x, 2) == False\n    assert checksol(x**2 - y**2, {x: 2, y: 3}) == False\n\n    # None (cannot conclude)\n    assert checksol(x**2 - y**2, {x: y}) == None\n\n    # Test minimal flag\n    assert checksol(x**2 - y**2, {x: y}, minimal=True) == None\n\n    # Test force flag\n    assert checksol(x**2 - y**2, {x: y}, force=True) == None\n\n    # Test simplify flag\n    assert checksol(x**2 - y**2, {x: y}, simplify=True) == None\n\n    # Test numerical flag\n    assert checksol(x**2 - y**2, {x: y}, numerical=True) == None\n\n    # checksol using an empty dictionary\n    assert checksol(x**2 + x - x*(x + 1), {}) == True\n", "def test_check_assumptions():\n    x = Symbol('x', real=True)\n    y = Symbol('y', positive=True)\n    assert check_assumptions(x + y, real=True, positive=None) is True\n    assert check_assumptions(x - y, real=True, positive=None) is None\n    assert check_assumptions(x - y, real=True, positive=False) is True\n    assert check_assumptions(x, real=True) is True\n    assert check_assumptions(y, positive=True) is True\n    assert check_assumptions(y - 1, positive=True) is False\n    assert check_assumptions(y + 1, positive=True) is True\n    assert check_assumptions(2 * x + y, real=True, positive=None) is True\n    assert check_assumptions(2 * x - y, real=True, positive=None) is None\n", "def test_checksol():\n    x, y = symbols('x y')\n    assert checksol(x**4 - 1, x, 1) is True\n    assert checksol(x**4 - 1, x, 0) is False\n    assert checksol(x**2 + y**2 - 5**2, {x: 3, y: 4}) is True\n    assert checksol(x**2 + x - x*(x + 1), {}) is True\n    assert checksol(sqrt(x) - 2, x, 4) is True\n    assert checksol(sqrt(x) - 2, x, 3) is False\n"], "sample_976": ["def test_symbol_sanitize():\n    from sympy.core.logic import fuzzy_bool\n    assumptions = {'commutative': True, 'positive': 1, 'real': None}\n    Symbol._sanitize(assumptions)\n    assert assumptions == {'commutative': True, 'positive': True}\n    assumptions = {'commutative': False}\n    Symbol._sanitize(assumptions)\n    assert assumptions == {'commutative': False}\n    raises(ValueError, lambda: Symbol._sanitize({'commutative': None}))\n    assumptions = {'bounded': True}\n    Symbol._sanitize(assumptions)\n    assert assumptions == {'finite': True}\n", "def test_Wild_exclude():\n    x = Symbol(\"x\")\n    y = Symbol(\"y\")\n    z = Symbol(\"z\")\n    w = Symbol(\"w\")\n\n    a = Wild(\"a\", exclude=[x, y])\n    b = Wild(\"b\", exclude=[z])\n\n    assert x.match(a) is None\n    assert y.match(a) is None\n    assert z.match(a) == {a: z}\n    assert w.match(a) == {a: w}\n\n    assert x.match(b) == {b: x}\n    assert y.match(b) == {b: y}\n    assert z.match(b) is None\n    assert w.match(b) == {b: w}\n", "def test_Wild_exclude():\n    x = Symbol(\"x\")\n    y = Symbol(\"y\")\n    z = Symbol(\"z\")\n    w = Wild('w', exclude=[x, y])\n\n    assert w.matches(x) is None\n    assert w.matches(y) is None\n    assert w.matches(z) == {w: z}\n\n    w = Wild('w', exclude=[x])\n    assert w.matches(x) is None\n    assert w.matches(y) == {w: y}\n    assert w.matches(z) == {w: z}\n\n    w = Wild('w', exclude=[y])\n    assert w.matches(x) == {w: x}\n    assert w.matches(y) is None\n    assert w.matches(z) == {w: z}\n\n    w = Wild('w', exclude=[x, y, z])\n    assert w.matches(x) is None\n    assert w.matches(y) is None\n    assert w.matches(z) is None\n", "def test_Symbol_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y', integer=True, negative=True)\n    z = Symbol('z', complex=True, zero=True)\n\n    assert x.is_real is True\n    assert x.is_positive is True\n    assert x.is_integer is False\n    assert x.is_complex is False\n\n    assert y.is_integer is True\n    assert y.is_negative is True\n    assert y.is_real is True\n    assert y.is_positive is False\n\n    assert z.is_complex is True\n    assert z.is_zero is True\n    assert z.is_real is False\n    assert z.is_positive is False\n    assert z.is_negative is False\n\n    raises(ValueError, lambda: Symbol('w', commutative=None))\n", "def test_Symbol_sanitize():\n    assumptions = {'commutative': None}\n    raises(ValueError, lambda: Symbol._sanitize(assumptions))\n    \n    assumptions = {'commutative': True, 'finite': 1, 'zero': 0}\n    Symbol._sanitize(assumptions)\n    assert assumptions == {'commutative': True, 'finite': True, 'zero': False}\n\n    assumptions = {'commutative': True, 'bounded': True, 'unbounded': False}\n    with raises(SymPyDeprecationWarning):\n        Symbol._sanitize(assumptions)\n    assert assumptions == {'commutative': True, 'finite': True, 'infinite': False}\n\n    assumptions = {'commutative': True, 'infinitesimal': True}\n    with raises(SymPyDeprecationWarning):\n        Symbol._sanitize(assumptions)\n    assert assumptions == {'commutative': True, 'zero': True}\n"], "sample_977": ["def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(x*y, (x, 0, 5), (y, 0, 3))) == \"Hold[Sum[x*y, {x, 0, 5}, {y, 0, 3}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x, (x, 1, 10))) == \"Hold[Sum[x, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2, (x, 0, 5))) == \"Hold[Sum[x^2, {x, 0, 5}]]\"\n    assert mcode(Sum(x*y, (x, 0, 5), (y, 0, 2))) == \"Hold[Sum[x*y, {x, 0, 5}, {y, 0, 2}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x, (x, 1, 10))) == \"Hold[Sum[x, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(sin(x), (x, 1, 10))) == \"Hold[Sum[Sin[x], {x, 1, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x, (x, 1, 10))) == \"Hold[Sum[x, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2, (x, 0, y))) == \"Hold[Sum[x^2, {x, 0, y}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x, (x, 1, 10))) == \"Hold[Sum[x, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2, (x, 0, 5))) == \"Hold[Sum[x^2, {x, 0, 5}]]\"\n    assert mcode(Sum(x*y, (x, 1, 3), (y, 1, 4))) == \"Hold[Sum[x*y, {x, 1, 3}, {y, 1, 4}]]\"\n"], "sample_978": ["def test_bspline_basis_invalid_degree():\n    try:\n        bspline_basis(-1, range(5), 0, x)\n    except ValueError as e:\n        assert str(e) == 'degree must be non-negative: 0'\n    \n    try:\n        bspline_basis(3, [0, 1, 2], 0, x)\n    except ValueError as e:\n        assert str(e) == 'n + d + 1 must not exceed len(knots) - 1'\n", "def test_invalid_degree():\n    knots = range(5)\n    try:\n        bspline_basis(-1, knots, 0, x)\n        assert False, \"Expected ValueError for negative degree\"\n    except ValueError as e:\n        assert str(e) == 'degree must be non-negative: 0'\n    \n    try:\n        bspline_basis(5, knots, 0, x)\n        assert False, \"Expected ValueError for degree exceeding knot intervals\"\n    except ValueError as e:\n        assert str(e) == 'n + d + 1 must not exceed len(knots) - 1'\n", "def test_invalid_degree():\n    d = -1\n    knots = range(5)\n    try:\n        bspline_basis_set(d, knots, x)\n    except ValueError as e:\n        assert str(e) == 'degree must be non-negative: -1'\n    else:\n        assert False, \"Expected ValueError for negative degree\"\n", "def test_invalid_input():\n    knots = range(5)\n    try:\n        bspline_basis(-1, knots, 0, x)\n    except ValueError as e:\n        assert str(e) == 'degree must be non-negative: 0'\n    else:\n        assert False, \"Expected ValueError for negative degree\"\n\n    try:\n        bspline_basis(2, knots, 4, x)\n    except ValueError as e:\n        assert str(e) == 'n + d + 1 must not exceed len(knots) - 1'\n    else:\n        assert False, \"Expected ValueError for invalid n\"\n", "def test_bspline_basis_invalid_degree():\n    d = -1\n    knots = range(5)\n    try:\n        bspline_basis_set(d, knots, x)\n    except ValueError as e:\n        assert str(e) == 'degree must be non-negative: 0'\n\n    d = 0\n    knots = [0]\n    try:\n        bspline_basis_set(d, knots, x)\n    except ValueError as e:\n        assert str(e) == 'n + d + 1 must not exceed len(knots) - 1'\n"], "sample_979": ["def test_MatrixSymbol_as_coeff_mmul():\n    A = MatrixSymbol('A', n, m)\n    coeff, matmul = A.as_coeff_mmul()\n    assert coeff == 1\n    assert matmul == MatMul(A)\n", "def test_MatrixElement_equality():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    assert (A[0, 0] == B[0, 0]) == Eq(A[0, 0], B[0, 0])\n    assert (A[0, 0] == A[0, 0]) == Eq(A[0, 0], A[0, 0])\n    assert (A[0, 0] != B[0, 0]) == (Ne(A[0, 0], B[0, 0]))\n    assert (A[0, 0] != A[0, 0]) == (Ne(A[0, 0], A[0, 0]))\n", "def test_MatrixSymbol_subs_shape():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    assert A.subs({n: 3, m: 4}).shape == (3, 4)\n    assert (A*B).subs({n: 3, m: 4, l: 2}).shape == (3, 2)\n", "def test_matrix_symbol_subs_shape():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', n, m)\n\n    assert A.subs(n, 3).shape == (3, m)\n    assert B.subs(l, 4).shape == (m, 4)\n    assert C.subs(m, 2).shape == (n, 2)\n\n    assert (A*B).subs(n, 3).subs(m, 2).subs(l, 4).shape == (3, 4)\n", "def test_as_explicit():\n    A = MatrixSymbol('A', 3, 3)\n    explicit_A = A.as_explicit()\n    assert explicit_A.shape == (3, 3)\n    assert explicit_A == Matrix([\n        [A[0, 0], A[0, 1], A[0, 2]],\n        [A[1, 0], A[1, 1], A[1, 2]],\n        [A[2, 0], A[2, 1], A[2, 2]]\n    ])\n"], "sample_980": ["def test_af_functions():\n    # Test _af_invert\n    assert _af_invert([1, 2, 0, 3]) == [2, 0, 1, 3]\n    assert _af_invert([3, 2, 1, 0]) == [3, 2, 1, 0]\n    assert _af_invert([0, 1, 2, 3]) == [0, 1, 2, 3]\n\n    # Test _af_pow\n    assert _af_pow([2, 0, 3, 1], 4) == [0, 1, 2, 3]\n    assert _af_pow([1, 0, 2, 3], 2) == [0, 1, 2, 3]\n    assert _af_pow([1, 0, 2, 3], 3) == [1, 0, 2, 3]\n    assert _af_pow([1, 2, 0, 3], -1) == [2, 0, 1, 3]\n\n    # Test _af_commutes_with\n    assert _af_commutes_with([1, 0, 2], [0, 1, 2]) is True\n    assert _af_commutes_with([1, 0, 2], [2, 1, 0]) is False\n    assert _af_commutes_with([0, 2, 1], [1, 0, 2]) is False\n\n    # Test _merge (internal function, so create a wrapper for testing)\n        temp = arr[:]\n        return _merge(arr, temp, left, mid, right)\n\n    assert merge_wrapper([1, 3, 5, 2, 4, 6], 0, 3, 5) == 3\n    assert merge_wrapper([1, 2, 3, 4, 5, 6], 0, 3, 5) == 0\n    assert merge_wrapper([6, 5, 4, 3, 2, 1], 0, 3, 5) == 9\n", "def test_af_functions():\n    # Test _af_rmul\n    assert _af_rmul([1, 0, 2], [0, 2, 1]) == [1, 2, 0]\n    assert _af_rmul([2, 0, 1, 3], [3, 1, 0, 2]) == [3, 0, 1, 2]\n\n    # Test _af_rmuln\n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [2, 0, 1]) == [0, 1, 2]\n    assert _af_rmuln([2, 0, 1], [1, 2, 0], [0, 2, 1]) == [1, 2, 0]\n    assert _af_rmuln([2, 0, 1], [1, 2, 0], [0, 2, 1], [2, 0, 1]) == [2, 0, 1]\n    assert _af_rmuln([2, 0, 1], [1, 2, 0], [0, 2, 1], [2, 0, 1], [0, 2, 1]) == [1, 2, 0]\n\n    # Test _af_parity\n    assert _af_parity([0, 1, 2, 3]) == 0\n    assert _af_parity([3, 2, 0, 1]) == 1\n    assert _af_parity([1, 0, 3, 2]) == 1\n\n    # Test _af_invert\n    assert _af_invert([1, 2, 0, 3]) == [2, 0, 1, 3]\n    assert _af_invert([3, 2, 0, 1]) == [2, 3, 1, 0]\n\n    # Test _af_pow\n    assert _af_pow([2, 0, 3, 1], 4) == [0, 1, 2, 3]\n    assert _af_pow([1, 0, 2], 3) == [0, 1", "def test__af_rmul():\n    assert _af_rmul([1, 0, 2], [0, 2, 1]) == [1, 2, 0]\n    assert _af_rmul([0, 1, 2], [2, 1, 0]) == [2, 1, 0]\n    assert _af_rmul([2, 0, 1], [1, 2, 0]) == [0, 1, 2]\n    assert _af_rmul([3, 2, 1, 0], [0, 1, 2, 3]) == [3, 2, 1, 0]\n", "def test_af_invert():\n    # Test the _af_invert function\n    perm = [1, 2, 0, 3]\n    expected_inverted = [2, 0, 1, 3]\n    assert _af_invert(perm) == expected_inverted\n\n    # Verify that inverting the inverted permutation returns the original permutation\n    double_inverted = _af_invert(expected_inverted)\n    assert double_inverted == perm\n\n    # Test with an identity permutation\n    identity_perm = list(range(5))\n    assert _af_invert(identity_perm) == identity_perm\n\n    # Test with an empty permutation\n    empty_perm = []\n    assert _af_invert(empty_perm) == empty_perm\n\n    # Test with a single element permutation\n    single_elem_perm = [0]\n    assert _af_invert(single_elem_perm) == single_elem_perm\n\n    # Test with a larger permutation\n    large_perm = [3, 4, 1, 0, 2]\n    expected_large_inverted = [3, 2, 4, 0, 1]\n    assert _af_invert(large_perm) == expected_large_inverted\n", "def test_af_invert():\n    assert _af_invert([1, 0, 2, 3]) == [1, 0, 2, 3]\n    assert _af_invert([2, 0, 1]) == [1, 2, 0]\n    assert _af_invert([3, 2, 1, 0]) == [3, 2, 1, 0]\n    assert _af_invert([0]) == [0]\n    assert _af_invert([]) == []\n\n    p = Permutation([3, 0, 2, 1])\n    assert _af_invert(p.array_form) == [1, 3, 2, 0]\n    assert _af_rmul(_af_invert(p.array_form), p.array_form) == list(range(p.size))\n"], "sample_981": ["def test_af_functions():\n    # test _af_rmul\n    a = [0, 1, 2, 3, 4]\n    b = [4, 3, 2, 1, 0]\n    assert _af_rmul(a, b) == [4, 3, 2, 1, 0]\n\n    # test _af_rmuln\n    c = [1, 0, 2]\n    d = [2, 1, 0]\n    e = [0, 2, 1]\n    assert _af_rmuln(a, b, c) == [0, 3, 4, 1, 2]\n    assert _af_rmuln(a, b, c, d) == [2, 3, 4, 1, 0]\n    assert _af_rmuln(a, b, c, d, e) == [0, 3, 4, 2, 1]\n\n    # test _af_parity\n    assert _af_parity([0, 1, 2, 3]) == 0\n    assert _af_parity([3, 2, 1, 0]) == 1\n\n    # test _af_invert\n    assert _af_invert([1, 2, 0, 3]) == [2, 0, 1, 3]\n\n    # test _af_pow\n    f = [2, 0, 3, 1]\n    assert _af_pow(f, 4) == [0, 1, 2, 3]\n    assert _af_pow(f, 0) == [0, 1, 2, 3]\n    assert _af_pow(f, -1) == [1, 3, 0, 2]\n\n    # test _af_commutes_with\n    g = [0, 1, 2, 3]\n    h = [3, 2, 1, 0]\n    assert _af_commutes_with(g, h) == False\n    assert _af_commutes_with(g, g) == True\n", "def test_af_rmuln():\n    # Test cases for _af_rmuln function\n    assert _af_rmuln([1, 0, 2], [0, 2, 1]) == [1, 2, 0]\n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [2, 1, 0]) == [1, 0, 2]\n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [2, 1, 0], [1, 2, 0]) == [0, 2, 1]\n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [2, 1, 0], [1, 2, 0], [1, 0, 2]) == [2, 1, 0]\n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [2, 1, 0], [1, 2, 0], [1, 0, 2], [0, 2, 1]) == [1, 2, 0]\n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [2, 1, 0], [1, 2, 0], [1, 0, 2], [0, 2, 1], [2, 0, 1]) == [1, 0, 2]\n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [2, 1, 0], [1, 2, 0], [1, 0, 2], [0, 2, 1], [2, 0, 1], [1, 2, 0]) == [0, 2, 1]\n    assert _af_rmuln([1, 0, 2]) == [1, 0, 2]\n    assert _af_rmuln([1, 0, 2], [0, 2, 1]) == [1, ", "def test_af_functions():\n    # Test _af_rmul with various input lengths\n    a, b = [1, 0, 2], [0, 2, 1]\n    assert _af_rmul(a, b) == [1, 2, 0]\n    assert _af_rmul([2, 1], [1, 0]) == [1, 2]\n\n    # Test _af_rmuln with multiple array forms\n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [1, 2, 0]) == [2, 0, 1]\n    assert _af_rmuln([2, 0, 1], [1, 2, 0], [0, 1, 2], [2, 0, 1]) == [0, 2, 1]\n    assert _af_rmuln([1, 2], [2, 1]) == [2, 1]\n\n    # Test _af_invert for inversion of permutations\n    assert _af_invert([1, 2, 0]) == [2, 0, 1]\n    assert _af_invert([3, 1, 0, 2]) == [2, 1, 3, 0]\n\n    # Test _af_pow for powers of permutations\n    assert _af_pow([1, 0, 2], 0) == [0, 1, 2]\n    assert _af_pow([1, 0, 2], 1) == [1, 0, 2]\n    assert _af_pow([1, 0, 2], 2) == [0, 1, 2]\n    assert _af_pow([1, 0, 2, 3], -1) == [1, 0, 2, 3]\n    assert _af_pow([1, 0, 2, 3], 3) == [1, 0, 2, 3]\n\n    # Test _af_parity for parity of permutations\n    assert _af_parity([0, 1, 2, 3]) == 0\n    assert _af_parity([3, 2, 0, 1]) == 1\n    assert _af_parity", "def test_af_rmul():\n    a = [1, 0, 2]\n    b = [0, 2, 1]\n    # Test _af_rmul\n    assert _af_rmul(a, b) == [1, 2, 0]\n    assert _af_rmul([2, 0, 1], [1, 0, 2]) == [0, 2, 1]\n    assert _af_rmul([0, 1, 2], [2, 0, 1]) == [2, 1, 0]\n    # Test edge cases\n    assert _af_rmul([], []) == []\n    assert _af_rmul([0], [0]) == [0]\n    assert _af_rmul([1, 0], [1, 0]) == [0, 1]\n", "def test_af_commutes_with():\n    a = [1, 0, 2]\n    b = [0, 2, 1]\n    c = [2, 0, 1]\n\n    assert _af_commutes_with(a, b) is False\n    assert _af_commutes_with(a, c) is False\n    assert _af_commutes_with(a, a) is True\n    assert _af_commutes_with(b, b) is True\n    assert _af_commutes_with(c, c) is True\n\n    p = Permutation(a)\n    q = Permutation(b)\n    r = Permutation(c)\n\n    assert p.commutes_with(q) is False\n    assert p.commutes_with(r) is False\n    assert p.commutes_with(p) is True\n    assert q.commutes_with(q) is True\n    assert r.commutes_with(r) is True\n"], "sample_982": ["def test_pollard_rho():\n    assert pollard_rho(10403, seed=1) == 101  # 10403 = 101 * 103\n    assert pollard_rho(10403, seed=2) == 103  # 10403 = 101 * 103\n    assert pollard_rho(19, seed=1) is None  # 19 is prime\n    assert pollard_rho(2**64 + 1, seed=1234) == 274177  # 2**64 + 1 = 274177 * 67280421310721\n    assert pollard_rho(2**64 + 1, seed=5678) == 67280421310721  # 2**64 + 1 = 274177 * 67280421310721\n    assert pollard_rho(2**31 - 1, seed=1) is None  # 2**31 - 1 is prime\n    raises(ValueError, lambda: pollard_rho(4))\n    raises(ValueError, lambda: pollard_rho(2))\n    raises(ValueError, lambda: pollard_rho(3))\n", "def test_pollard_rho():\n    assert pollard_rho(10403, seed=1) in {101, 103}\n    assert pollard_rho(10403, seed=2) in {101, 103}\n    assert pollard_rho(600851475143, seed=42) in {71, 839, 1471, 6857}\n    assert pollard_rho(600851475143, seed=50, retries=2) in {71, 839, 1471, 6857}\n    assert pollard_rho(19, seed=1) is None\n    raises(ValueError, lambda: pollard_rho(4))\n    \n    # Test with a custom generator function F\n    F = lambda x: (x**2 + 1) % 10403\n    assert pollard_rho(10403, F=F, seed=1) in {101, 103}\n    assert pollard_rho(10403, F=F, seed=2) in {101, 103}\n", "def test_pollard_rho():\n    n = 10403\n    factor = pollard_rho(n)\n    assert factor in {101, 103}\n    n = 9999991\n    factor = pollard_rho(n)\n    assert factor is None\n    n = 2**31 - 1\n    factor = pollard_rho(n)\n    assert factor is None\n    n = 2**61 - 1\n    factor = pollard_rho(n)\n    assert factor is None\n    n = 10403\n    factor = pollard_rho(n, seed=1234)\n    assert factor in {101, 103}\n    n = 10403\n    factor = pollard_rho(n, s=3, retries=0)\n    assert factor in {101, 103}\n", "def test_pollard_rho():\n    assert pollard_rho(8051) == 97  # 8051 = 97 * 83\n    assert pollard_rho(10403, seed=2) == 101  # 10403 = 101 * 103\n    assert pollard_rho(2**64 + 15) == 274177  # 2**64 + 15 = 274177 * 67280421310721\n    assert pollard_rho(3*7*11*13*17*19*23*29, seed=5) == 19  # 3*7*11*13*17*19*23*29\n    assert pollard_rho(10007*20011, seed=3) == 10007  # 10007 * 20011\n    assert pollard_rho(19) is None  # 19 is prime\n    raises(ValueError, lambda: pollard_rho(4))\n    raises(ValueError, lambda: pollard_rho(1))\n    raises(ValueError, lambda: pollard_rho(0))\n", "def test_pollard_rho():\n    assert pollard_rho(10403) == 101  # 10403 = 101 * 103\n    assert pollard_rho(2251799813685249) == 2147483647  # 2251799813685249 = 2147483647 * 1048577\n    assert pollard_rho(10403, seed=42) == 101  # test with different seed\n    assert pollard_rho(2**61 - 1) is None  # this is a prime number\n    assert pollard_rho(2**61 - 1, retries=3) is None  # additional retries\n    raises(ValueError, lambda: pollard_rho(3))  # n should be greater than 4\n    raises(ValueError, lambda: pollard_rho(4))  # n should be greater than 4\n"], "sample_983": ["def test_determinant_of_larger_matrix():\n    # Larger matrix determinant test\n    A = SparseMatrix([\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12],\n        [13, 14, 15, 16]\n    ])\n    assert A.det() == 0\n\n    B = SparseMatrix([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]\n    ])\n    assert B.det() == 1\n\n    C = SparseMatrix([\n        [2, 0, 0, 0],\n        [0, 2, 0, 0],\n        [0, 0, 2, 0],\n        [0, 0, 0, 2]\n    ])\n    assert C.det() == 16\n\n    D = SparseMatrix([\n        [2, 1, 3, 1],\n        [4, 1, 6, 2],\n        [3, 1, 0, 5],\n        [1, 0, 2, 0]\n    ])\n    assert D.det() == -57\n", "def test_cholesky_sparse():\n    A = SparseMatrix([[4, 12, -16], [12, 37, -43], [-16, -43, 98]])\n    L = A.cholesky()\n    assert L == SparseMatrix([\n        [2, 0, 0],\n        [6, 1, 0],\n        [-8, 5, 3]])\n    assert L * L.T == A\n\n    raises(ValueError, lambda: SparseMatrix([[1, 2], [3, 4]]).cholesky())\n    raises(ValueError, lambda: SparseMatrix([[1, 2], [2, -4]]).cholesky())\n\n    B = SparseMatrix([[6, 15], [15, 55]])\n    assert B.cholesky() == SparseMatrix([\n        [6, 0],\n        [2.5, 5]])\n    assert B.cholesky() * B.cholesky().T == B\n", "def test_applyfunc():\n    m = SparseMatrix(3, 3, lambda i, j: i * j)\n    assert m.applyfunc(lambda x: x + 1) == SparseMatrix(3, 3, lambda i, j: i * j + 1)\n    assert m.applyfunc(lambda x: x * 2) == SparseMatrix(3, 3, lambda i, j: i * j * 2)\n    assert m.applyfunc(lambda x: x ** 2) == SparseMatrix(3, 3, lambda i, j: (i * j) ** 2)\n    assert m.applyfunc(lambda x: 0) == SparseMatrix(3, 3, lambda i, j: 0)\n    assert m.applyfunc(abs) == m\n", "def test_transpose_symmetry():\n    # Ensure transpose of transpose returns the original matrix\n    a = SparseMatrix(((1, 2), (3, 4), (5, 6)))\n    assert a.T.T == a\n\n    # Check transpose of a square matrix\n    b = SparseMatrix(((1, 2), (3, 4)))\n    assert b.T == SparseMatrix(((1, 3), (2, 4)))\n\n    # Ensure transpose of a zero matrix remains zero\n    c = SparseMatrix.zeros(3, 3)\n    assert c.T == SparseMatrix.zeros(3, 3)\n\n    # Ensure transpose of a non-square matrix\n    d = SparseMatrix(((1, 2, 3), (4, 5, 6)))\n    assert d.T == SparseMatrix(((1, 4), (2, 5), (3, 6)))\n\n    # Ensure transpose of a sparse matrix with only one non-zero element\n    e = SparseMatrix(3, 3, {(0, 1): 5})\n    assert e.T == SparseMatrix(3, 3, {(1, 0): 5})\n", "def test_scalar_multiply():\n    a = SparseMatrix((\n        (1, 0, 2),\n        (0, 3, 0),\n        (4, 0, 5)\n    ))\n    b = a.scalar_multiply(2)\n    assert b == SparseMatrix((\n        (2, 0, 4),\n        (0, 6, 0),\n        (8, 0, 10)\n    ))\n    c = a.scalar_multiply(0)\n    assert c == SparseMatrix((\n        (0, 0, 0),\n        (0, 0, 0),\n        (0, 0, 0)\n    ))\n"], "sample_984": ["def test_BlockMatrix():\n    from sympy.matrices import Matrix, BlockMatrix\n    M1 = Matrix([[1, 2], [3, 4]])\n    M2 = Matrix([[5, 6], [7, 8]])\n    BM = BlockMatrix([[M1, M2], [M2, M1]])\n    assert str(BM) == \"Matrix([[1, 2, 5, 6], [3, 4, 7, 8], [5, 6, 1, 2], [7, 8, 3, 4]])\"\n", "def test_Rational_integration():\n    r = Rational(2, 3)\n    p = sstr(r)\n    assert p == \"2/3\"\n    \n    # Test Rational with sympy_integers setting\n    p_sympy_integers = sstr(r, sympy_integers=True)\n    assert p_sympy_integers == \"S(2)/3\"\n    \n    # Confirm integration with sympy_integers setting\n    assert sstr(r*x, sympy_integers=True) == \"S(2)/3*x\"\n", "def test_BlockMatrix():\n    from sympy import BlockMatrix, Matrix\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6]])\n    C = Matrix([[7], [8]])\n    D = Matrix([[9]])\n    BM = BlockMatrix([[A, B], [C, D]])\n    assert str(BM) == 'Matrix([\\n[1, 2, 5, 6],\\n[3, 4, 7, 8],\\n[9]])'\n    BM_single = BlockMatrix([[A]])\n    assert str(BM_single) == 'Matrix([\\n[1, 2],\\n[3, 4]])'\n", "def test_MatrixElement():\n    from sympy.matrices import MatrixSymbol\n    X = MatrixSymbol('X', 3, 3)\n    assert str(X[1, 2]) == 'X[1, 2]'\n    assert str(X[0, 0]) == 'X[0, 0]'\n    assert str(X[x, y]) == 'X[x, y]'\n", "def test_HadamardProduct():\n    from sympy.matrices.expressions import HadamardProduct\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = HadamardProduct(A, B)\n    assert str(expr) == \"A.*B\"\n"], "sample_985": ["def test_root_functions():\n    from sympy import sqrt, cbrt, root, real_root\n    from sympy.abc import x, y\n\n    assert sqrt(4) == 2\n    assert sqrt(x**2) == sqrt(x**2)\n    assert sqrt(y**2).subs(y, 3) == 3\n\n    assert cbrt(8) == 2\n    assert cbrt(x**3) == (x**3)**(Rational(1, 3))\n    assert cbrt(y**3).subs(y, 3) == 3\n\n    assert root(16, 4) == 2\n    assert root(x, 3) == x**(Rational(1, 3))\n    assert root(x, -Rational(2, 3)) == x**(-Rational(3, 2))\n    assert root(-2, 3, 2) == -(-1)**(Rational(2, 3))*2**(Rational(1, 3))\n\n    assert real_root(-8, 3) == -2\n    assert real_root(root(-8, 3)) == -2\n    assert real_root(root(-8, 3, 2)) == -2*(-1)**(Rational(2, 3))\n", "def test_sqrt():\n    from sympy import sqrt, Symbol, Eq\n    x = Symbol('x')\n    y = Symbol('y', positive=True)\n\n    # Basic functionality\n    assert sqrt(x) == Pow(x, S.Half)\n    assert sqrt(x)**2 == x\n\n    # sqrt(x**2) does not simplify to x\n    assert sqrt(x**2) == Pow(x**2, S.Half)\n    \n    # sqrt(x**2) when x is positive\n    assert sqrt(y**2) == y\n    \n    # Force simplification with powdenest\n    from sympy import powdenest\n    assert powdenest(sqrt(x**2), force=True) == x\n    \n    # Check principal square root and branches\n    assert Eq(sqrt(x**2), x).subs(x, -1) == False\n    \n    # Use rootof to get both branches\n    from sympy import rootof\n    assert [rootof(x**2 - 3, i) for i in (0, 1)] == [-sqrt(3), sqrt(3)]\n", "def test_sqrt():\n    from sympy.abc import x, y\n\n    assert sqrt(4) == 2\n    assert sqrt(9) == 3\n    assert sqrt(x) == Pow(x, S.Half)\n    assert sqrt(x**2) == sqrt(x**2)\n    assert sqrt(x**2).subs(x, 3) == 3\n    assert sqrt(x**2).subs(x, -3) == 3\n    assert sqrt(x**2).subs(x, 0) == 0\n\n    # Test with positive symbol\n    y = symbols('y', positive=True)\n    assert sqrt(y**2) == y\n\n    # Test with powdenest\n    from sympy import powdenest\n    assert powdenest(sqrt(x**2), force=True) == x\n\n    # Test with rootof\n    from sympy import rootof\n    assert [rootof(x**2 - 3, i) for i in (0, 1)] == [-sqrt(3), sqrt(3)]\n", "def test_minmax_properties():\n    x, y, z = symbols('x y z')\n    p = symbols('p', positive=True)\n    n = symbols('n', negative=True)\n\n    # Testing Max properties\n    assert Max(p, -2).is_positive\n    assert Max(n, -2).is_nonpositive\n    assert Max(-p, -2).is_nonpositive\n    assert Max(p, 0).is_nonnegative\n    assert Max(p, n).is_real\n    assert Max(n, -oo).is_negative\n    assert Max(oo, -oo) == oo\n\n    # Testing Min properties\n    assert Min(p, -2).is_nonpositive\n    assert Min(n, -2).is_negative\n    assert Min(-p, -2).is_negative\n    assert Min(p, 0).is_nonnegative\n    assert Min(p, n).is_real\n    assert Min(n, -oo) == -oo\n    assert Min(oo, -oo) == -oo\n\n    # Further testing mixed properties\n    assert Max(p, n).is_real\n    assert Min(p, n).is_real\n    assert Max(p, n, -oo).is_real\n    assert Min(p, n, oo).is_real\n\n    # Testing fdiff for Max\n    assert Max(x, y).fdiff(1) == Heaviside(x - y)\n    assert Max(x, y).fdiff(2) == Heaviside(y - x)\n    assert Max(x, y, z).fdiff(1) == Heaviside(x - Max(y, z))\n    assert Max(x, y, z).fdiff(2) == Heaviside(y - Max(x, z))\n    assert Max(x, y, z).fdiff(3) == Heaviside(z - Max(x, y))\n\n    # Testing fdiff for Min\n    assert Min(x, y).fdiff(1) == Heaviside(y - x)\n    assert Min(x, y).fdiff(2) == Heaviside(x - y)\n    assert Min(x, y, z).fdiff(1) == Heaviside(Min(y, z) - x)\n    assert Min(x, y, z).fdiff(2) == Heaviside(Min(x, z) - y)\n    assert Min(x, y, z).fdiff(3) == Heaviside", "def test_minmax_functions():\n    from sympy import Symbol, Rational, Eq, rootof\n    x = Symbol('x')\n    y = Symbol('y', positive=True)\n\n    # Test sqrt\n    assert sqrt(x) == Pow(x, S.Half)\n    assert sqrt(x**2) == Pow(x**2, S.Half)\n    assert sqrt(y**2) == y\n    assert powdenest(sqrt(x**2), force=True) == x\n\n    # Test cbrt\n    assert cbrt(x) == Pow(x, Rational(1, 3))\n    assert cbrt(x**3) == Pow(x**3, Rational(1, 3))\n    assert cbrt(y**3) == y\n\n    # Test root\n    assert root(x, 2) == sqrt(x)\n    assert root(x, 3) == cbrt(x)\n    assert root(x, -Rational(2, 3)) == Pow(x, -Rational(3, 2))\n    assert root(-2, 3, 2) == -(-1)**Rational(2, 3) * Pow(2, Rational(1, 3))\n\n    # Test real_root\n    assert real_root(-8, 3) == -2\n    assert real_root(root(-8, 3)) == -2\n    assert real_root(root(-8, 3, 2)) == -2 * Pow(-1, Rational(2, 3))\n\n    # Test Min and Max\n    assert Max(x, -2).subs(x, 3) == 3\n    assert Max(x, -2).subs(x, -3) == -2\n    assert Max(x, y) == Max(x, y)\n    assert Max(y, 5) == y\n    assert Max(1, x, oo) == oo\n\n    assert Min(x, -2).subs(x, 3) == -2\n    assert Min(x, -2).subs(x, -3) == -3\n    assert Min(x, y) == Min(x, y)\n    assert Min(y, 5) == 5\n    assert Min(1, x, -oo) == -oo\n\n    # Test MinMaxBase properties\n    min_expr = Min(x, y, 2)\n    assert min_expr._eval_is_positive() == fuzzy_and(arg.is_positive for"], "sample_986": ["def test_evalf_hypergeometric_series():\n    from sympy import Sum, factorial\n    expr = Sum(1/factorial(n), (n, 0, oo))\n    assert NS(expr, 15) == '2.71828182845905'  # Expected value of e\n", "def test_evalf_sum_issue():\n    # From issue 8411\n    s = Sum(1/(n**2 + n), (n, 1, oo))\n    assert s.evalf() == s.doit().evalf()\n    # Additional test for Sum of harmonic series, which should diverge\n    raises(ValueError, lambda: Sum(1/n, (n, 1, oo)).evalf())\n", "def test_evalf_special_functions():\n    from sympy import bernoulli\n    assert NS(bernoulli(10), 15) == '0.075757575757576'\n    assert NS(bernoulli(15), 15) == '-0.0306122448979592'\n    assert NS(bernoulli(30), 15) == '0.000171046666666667'\n", "def test_evalf_special_functions():\n    from sympy.functions.combinatorial.numbers import bernoulli\n    assert NS(bernoulli(10), 15) == '5.00000000000000'\n    assert NS(bernoulli(15), 15) == '-1.00000000000000'\n    assert NS(bernoulli(20), 15) == '2.00000000000000'\n    assert NS(bernoulli(30), 15) == '-1.00000000000000'\n    assert NS(bernoulli(40), 15) == '2.00000000000000'\n", "def test_evalf_trig_special_cases():\n    assert NS('sin(pi/2)', 15) == '1.00000000000000'\n    assert NS('cos(pi/2)', 15) == '0.000000000000000'\n    assert NS('tan(pi/4)', 15) == '1.00000000000000'\n    assert NS('sin(3*pi/2)', 15) == '-1.00000000000000'\n    assert NS('cos(2*pi)', 15) == '1.00000000000000'\n    assert NS('tan(0)', 15) == '0.000000000000000'\n    assert NS('sin(pi)', 15) == '0.000000000000000'\n"], "sample_987": ["def test_evalf_abs():\n    from sympy import Abs, I\n\n    # Test for real values\n    assert NS(Abs(-5), 10) == '5.0000000000'\n    assert NS(Abs(10), 10) == '10.0000000000'\n    assert NS(Abs(-3.5), 10) == '3.5000000000'\n    \n    # Test for complex values\n    assert NS(Abs(3 + 4*I), 10) == '5.0000000000'\n    assert NS(Abs(-3 - 4*I), 10) == '5.0000000000'\n    assert NS(Abs(1 - I), 10) == '1.414213562'\n    \n    # Test for symbolic expressions\n    expr = Abs(x**2 - 4)\n    assert expr.evalf(subs={x: 3}) == 5.0\n    assert expr.evalf(subs={x: 2}) == 0.0\n    assert expr.evalf(subs={x: -1}) == 3.0\n", "def test_evalf_special_functions():\n    from sympy import bernoulli, binomial\n    \n    # Test Bernoulli numbers\n    assert NS(bernoulli(4), 15) == '0.166666666666667'\n    assert NS(bernoulli(10), 15) == '-0.0757575757575761'\n    \n    # Test binomial coefficient\n    assert NS(binomial(30, 15), 15) == '155117520'\n    assert NS(binomial(100, 50), 15) == '1.00891344545564e+29'\n    \n    # Test with higher precision\n    assert NS(bernoulli(20), 25) == '-529.124242424242424242424242'\n    assert NS(binomial(50, 25), 25) == '1.264514848936376075210359e+14'\n", "def test_evalf_special_functions():\n    from sympy import bernoulli\n    assert NS(bernoulli(4), 15) == '0.166666666666667'\n    assert NS(bernoulli(10), 15) == '-0.0757575757575760'\n    assert NS(bernoulli(20), 15) == '0.0811511599841901'\n    assert NS(bernoulli(30), 15) == '-0.173842904906588'\n", "def test_evalf_abs():\n    assert NS(Abs(-E)) == '2.71828182845905'\n    assert NS(Abs(3 + 4*I)) == '5.00000000000000'\n    assert NS(Abs(x), subs={x: -5}) == '5'\n    assert NS(Abs(x + I*y), subs={x: 3, y: 4}) == '5.00000000000000'\n", "def test_evalf_custom_function():\n    class CustomFunction(Function):\n        @classmethod\n            if arg.is_Number:\n                return arg + 1\n\n    f = CustomFunction(x)\n    assert NS(f, subs={x: 2}) == '3.00000000000000'\n    assert NS(f, subs={x: pi}) == '4.14159265358979'\n    assert NS(f.evalf(subs={x: 2})) == '3.00000000000000'\n    assert NS(f.evalf(subs={x: pi})) == '4.14159265358979'\n"], "sample_988": ["def test_number_comparisons():\n    assert S.Zero < S.One\n    assert S.One > S.Zero\n    assert S.NegativeOne < S.Zero\n    assert S.Half < S.One\n    assert S.One > S.Half\n    assert S.Infinity > S.One\n    assert S.NegativeInfinity < S.One\n    assert S.NaN != S.One\n    assert S.NaN != S.NaN\n\n    assert (S.Zero).is_Number\n    assert (S.One).is_Number\n    assert (S.NegativeOne).is_Number\n    assert (S.Half).is_Number\n    assert (S.Infinity).is_Number\n    assert (S.NegativeInfinity).is_Number\n    assert (S.NaN).is_Number\n\n    x = Symbol('x')\n    assert (S.Zero + x).is_Add\n    assert (S.One * x).is_Mul\n    assert (S.Half / x).is_Mul\n", "def test_mpf_norm():\n    from mpmath.libmp.libmpf import mpf\n    # Testing mpf normalization with mantissa values\n    assert mpf_norm((1, 0, 0, 0), 53) == _mpf_zero  # Normalized zero\n    assert mpf_norm((1, 1, 0, 1), 53) == (1, 1, 0, 1)  # Already normalized\n    # Test with non-zero mantissa\n    assert mpf_norm((1, 0, 0, 1), 53) == (1, 0, 0, 1)  # Non-zero mantissa\n", "def test_mpf_norm():\n    from mpmath.libmp.libmpf import prec_to_dps, _normalize\n    prec = 53\n    mpf = (0, 1, -1, 1)\n    assert mpf_norm(mpf, prec) == _normalize(mpf, prec, rnd)\n    mpf = (0, 0, 0, 0)\n    assert mpf_norm(mpf, prec) == _mpf_zero\n    mpf = (1, 0, 1, 0)\n    assert mpf_norm(mpf, prec) == (1, 0, 1, 0)\n    mpf = (0, 1, 0, 0)\n    assert mpf_norm(mpf, prec) == _normalize(mpf, prec, rnd)\n", "def test_mpf_norm():\n    from sympy.core.numbers import mpf_norm\n    from mpmath import mpf\n    assert mpf_norm((0, 0, 0, 0), 53) == (0, 0, 0, 0)\n    assert mpf_norm((0, 0, 0, 1), 53) == (0, 0, 0, 1)\n    assert mpf_norm((1, 1, 0, 1), 53) == (1, 1, 0, 1)\n    assert mpf_norm((0, 0, 0, 1), 53) == (0, 0, 0, 1)\n    assert mpf_norm((0, 0, 0, 0), 53) == (0, 0, 0, 0)\n", "def test_comp():\n    assert comp(3, 3) is True\n    assert comp(3.0, 3) is True\n    assert comp(3.0, '3.0') is True\n    assert comp(3, 3, 0.1) is True\n    assert comp(3, 3.1, 0.1) is True\n    assert comp(3, 3.2, 0.1) is False\n    assert comp(3, '3.0', '') is True\n    assert comp(Float(3.0), '3.0') is True\n    raises(ValueError, lambda: comp('3.0', 3))\n    raises(ValueError, lambda: comp(3, '3.0'))\n    raises(ValueError, lambda: comp(Float(3.0), 3))\n    raises(ValueError, lambda: comp(3, '3.0', 0.1))\n    raises(ValueError, lambda: comp(Float(3.0), '3.0', 0.1))\n    assert comp(3, 3.1, 0.1) is True\n    assert comp(Float(3.0), Float(3.1), 0.1) is True\n    assert comp(Float(3.0), Float(3.2), 0.1) is False\n    assert comp(Float(3.0), Float(3.0), 0.0) is True\n"], "sample_989": ["def test_mpf_norm_zero():\n    assert mpf_norm((1, 0, 1, 0), 10) == _mpf_zero\n    assert mpf_norm((1, 0, 1, 1), 10) == (1, 0, 1, 1)\n", "def test_mpf_norm_special_cases():\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)  # Zero edge case\n    assert mpf_norm((1, 0, 1, 0), 10) == _mpf_zero  # Mantissa is zero, but not bc\n    assert mpf_norm((1, 0, 1, 1), 10) == (1, 0, 1, 1)  # Special case with non-zero bc\n    assert mpf_norm((0, 1, -1, 1), 10) == (0, 1, -1, 1)  # Negative exponent with non-zero mantissa\n    assert mpf_norm((1, 2**10, 1, 11), 10) == (1, 2**10, 1, 11)  # Large mantissa with normalization\n    assert mpf_norm((1, 0, 0, 0), 10) == _mpf_zero  # Normalizing an already zero value\n", "def test_mpf_norm_large_numbers():\n    large_number = (0, 1, 1000000000, 1000000000)\n    assert mpf_norm(large_number, 100) == large_number  # should normalize correctly\n\n    large_negative_number = (1, 1, 1000000000, 1000000000)\n    assert mpf_norm(large_negative_number, 100) == large_negative_number  # should normalize correctly\n\n    zero_mantissa = (0, 0, 1000000000, 1000000000)\n    assert mpf_norm(zero_mantissa, 100) == _mpf_zero  # should normalize to zero\n", "def test_mpf_norm_exceptions():\n    from mpmath.libmp.libmpf import fnan, finf, fninf\n\n    # Ensure mpf_norm returns correct special values\n    assert mpf_norm(fnan, 53) == fnan\n    assert mpf_norm(finf, 53) == finf\n    assert mpf_norm(fninf, 53) == fninf\n\n    # Check that invalid inputs raise appropriate exceptions\n    invalid_mpf = (0, 1, 0, 0)  # This is not a valid mpf tuple\n    raises(ValueError, lambda: mpf_norm(invalid_mpf, 53))\n", "def test_mpf_norm_nonzero_mantissa():\n    # Ensure mpf_norm handles non-zero mantissa correctly\n    assert mpf_norm((1, 123456789, 5, 0), 10) == (1, 123456789, 5, 0)\n    assert mpf_norm((0, 987654321, -5, 0), 10) == (0, 987654321, -5, 0)\n"], "sample_990": ["def test_rewrite_hyperbolics_as_exp():\n    x = Symbol('x')\n    expr = sinh(x) + cosh(x) + tanh(x) + coth(x)\n    rewritten_expr = _rewrite_hyperbolics_as_exp(expr)\n    expected_expr = (exp(x) - exp(-x)) / 2 + (exp(x) + exp(-x)) / 2 + (exp(x) - exp(-x)) / (exp(x) + exp(-x)) + (exp(x) + exp(-x)) / (exp(x) - exp(-x))\n    assert rewritten_expr == expected_expr\n", "def test_hyperbolic_function_class():\n    from sympy.core.function import ArgumentIndexError\n    from sympy import symbols\n\n    x = symbols('x')\n    \n    # Test fdiff\n    assert sinh(x).fdiff() == cosh(x)\n    assert cosh(x).fdiff() == sinh(x)\n    assert tanh(x).fdiff() == 1 - tanh(x)**2\n    assert coth(x).fdiff() == -1/sinh(x)**2\n    assert csch(x).fdiff() == -coth(x)*csch(x)\n    assert sech(x).fdiff() == -tanh(x)*sech(x)\n    \n    raises(ArgumentIndexError, lambda: sinh(x).fdiff(2))\n    raises(ArgumentIndexError, lambda: cosh(x).fdiff(2))\n    raises(ArgumentIndexError, lambda: tanh(x).fdiff(2))\n    raises(ArgumentIndexError, lambda: coth(x).fdiff(2))\n    raises(ArgumentIndexError, lambda: csch(x).fdiff(2))\n    raises(ArgumentIndexError, lambda: sech(x).fdiff(2))\n\n    # Test as_real_imag\n    assert sinh(x).as_real_imag() == (sinh(x), 0)\n    assert cosh(x).as_real_imag() == (cosh(x), 0)\n    assert tanh(x).as_real_imag() == (tanh(x), 0)\n    assert coth(x).as_real_imag() == (coth(x), 0)\n    assert csch(x).as_real_imag() == (csch(x), 0)\n    assert sech(x).as_real_imag() == (sech(x), 0)\n", "def test_atanh_expansion():\n    x, y = symbols('x, y')\n    assert atanh(x + y).expand(trig=True) == (atanh(x) + atanh(y)).simplify()\n    assert atanh(2*x).expand(trig=True) == (2 * atanh(x) / (1 + x**2)).simplify()\n    assert atanh(3*x).expand(trig=True) == (atanh(x) + atanh(2*x)).simplify()\n", "def test_hyperbolic_trig_expansions():\n    x, y = symbols('x,y')\n    \n    # Testing tanh expansion with trigonometric identities\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x) / (1 + tanh(x)**2)\n    \n    # Testing coth expansion with trigonometric identities\n    assert coth(x+y).expand(trig=True) == (coth(x)*coth(y) + 1) / (coth(y) + coth(x))\n    assert coth(2*x).expand(trig=True) == coth(x)**2 + 1 / (2*coth(x))\n\n    # Testing csch expansion with trigonometric identities\n    assert csch(x+y).expand(trig=True) == csch(x)*csch(y) / (coth(x) + coth(y))\n    assert csch(2*x).expand(trig=True) == 2*csch(x)*coth(x) / (1 + coth(x)**2)\n    \n    # Testing sech expansion with trigonometric identities\n    assert sech(x+y).expand(trig=True) == sech(x)*sech(y) / (1 + tanh(x)*tanh(y))\n    assert sech(2*x).expand(trig=True) == 2*sech(x)**2 / (1 + sech(x)**2)\n", "def test_atanh_series_expansion():\n    x = Symbol('x')\n    assert atanh(x).series(x, 0, 5) == x + x**3/3 + x**5/5 + O(x**7)\n"], "sample_991": ["def test_product_with_exponential_terms():\n    # Test products involving exponential functions\n    from sympy.abc import x, n\n    assert product(exp(x), (x, 1, n)) == exp(Sum(x, (x, 1, n)).doit())\n    assert product(exp(x), (x, 1, 5)).doit() == exp(15)\n    assert product(exp(-x), (x, 1, n)) == exp(-Sum(x, (x, 1, n)).doit())\n    assert product(exp(-x), (x, 1, 5)).doit() == exp(-15)\n", "def test_doit_with_deep_hint():\n    # Test the behavior of doit with deep hint set to True and False\n\n    # Deep hint set to True (default)\n    p = Product(k**2, (k, 1, n))\n    assert p.doit(deep=True) == factorial(n)**2\n\n    # Deep hint set to False\n    p = Product(k**2, (k, 1, n))\n    assert p.doit(deep=False) == powsimp(factorial(n)**2)\n\n    # Nested Products with deep hint\n    p_nested = Product(Product(k, (k, 1, n)), (n, 1, m))\n    assert p_nested.doit(deep=True) == factorial(m)\n    assert p_nested.doit(deep=False) == Product(factorial(n), (n, 1, m))\n\n    # Deep hint with unsimplified expressions\n    p_unsimplified = Product(k**(1 + 1), (k, 1, n))\n    assert p_unsimplified.doit(deep=True) == factorial(n)**2\n    assert p_unsimplified.doit(deep=False) == powsimp(factorial(n)**2)\n", "def test_reverse_order_with_nested_products():\n    x, y, z, a, b, c, d = symbols('x, y, z, a, b, c, d', integer=True)\n\n    # Test reversing order on nested products\n    P1 = Product(Product(x*y, (x, a, b)), (y, c, d))\n    P1_reversed = Product(Product(1/(x*y), (x, b + 1, a - 1)), (y, d + 1, c - 1))\n    assert P1.reverse_order(x, y) == P1_reversed\n\n    P2 = Product(Product(x*y*z, (x, a, b), (y, c, d)), (z, a, b))\n    P2_reversed = Product(Product(1/(x*y*z), (x, b + 1, a - 1), (y, d + 1, c - 1)), (z, b + 1, a - 1))\n    assert P2.reverse_order(x, y, z) == P2_reversed\n\n    P3 = Product(Product(x*y*z, (x, a, b), (y, c, d), (z, m, n)), (k, 1, 3))\n    P3_reversed = Product(Product(1/(x*y*z), (x, b + 1, a - 1), (y, d + 1, c - 1), (z, n + 1, m - 1)), (k, 4, -1))\n    assert P3.reverse_order(x, y, z, k) == P3_reversed\n", "def test_expr_with_deltas():\n    from sympy.functions import KroneckerDelta\n    i = symbols('i', integer=True)\n\n    # Testing with KroneckerDelta\n    expr1 = KroneckerDelta(i, 2)\n    prod1 = Product(expr1, (i, 1, 5)).doit()\n    assert prod1 == 0\n\n    expr2 = 1 + KroneckerDelta(i, 2)\n    prod2 = Product(expr2, (i, 1, 5)).doit()\n    assert prod2 == 2\n\n    # Testing with mixed terms including KroneckerDelta\n    expr3 = i * KroneckerDelta(i, 3)\n    prod3 = Product(expr3, (i, 1, 5)).doit()\n    assert prod3 == 3\n\n    expr4 = (i + 1) * KroneckerDelta(i, 3)\n    prod4 = Product(expr4, (i, 1, 5)).doit()\n    assert prod4 == 4\n\n    # Testing with non-integer range where KroneckerDelta should return zero\n    expr5 = KroneckerDelta(i, 2.5)\n    prod5 = Product(expr5, (i, 1, 5)).doit()\n    assert prod5 == 0\n", "def test_product_rewrite_as_sum():\n    from sympy import Sum, log, exp\n    f, n = symbols('f n')\n    P = Product(f, (n, 1, 5))\n    S = Sum(log(f), (n, 1, 5))\n    assert P.rewrite(Sum) == exp(S)\n"], "sample_992": ["def test_MpmathPrinter():\n    p = MpmathPrinter()\n    expr = acos(x)\n    assert 'mpmath' not in p.module_imports\n    assert p.doprint(expr) == 'mpmath.acos(x)'\n    assert 'mpmath' in p.module_imports\n    assert p.doprint(pi) == 'mpmath.pi'\n    assert p.module_imports == {'mpmath': {'acos', 'pi'}}\n\n    # Test printing of Float\n    from sympy import Float\n    assert p.doprint(Float('1.23')) == 'mpmath.mpf((1, 2300000000000001, -52, 53))'\n\n    # Test printing of uppergamma and lowergamma\n    from sympy.functions.special.gamma_functions import uppergamma, lowergamma\n    assert p.doprint(uppergamma(x, y)) == 'mpmath.gammainc(x, y, mpmath.inf)'\n    assert p.doprint(lowergamma(x, y)) == 'mpmath.gammainc(x, 0, y)'\n\n    # Test printing of log2 and log1p\n    from sympy.functions.elementary.exponential import log2, log1p\n    assert p.doprint(log2(x)) == 'mpmath.log(x)/mpmath.log(2)'\n    assert p.doprint(log1p(x)) == 'mpmath.log(x+1)'\n", "def test_MpmathPrinter():\n    p = MpmathPrinter()\n    expr = acos(x)\n    assert 'mpmath' not in p.module_imports\n    assert p.doprint(expr) == 'mpmath.acos(x)'\n    assert 'mpmath' in p.module_imports\n    assert not any(m.startswith('numpy') for m in p.module_imports)\n    assert p.doprint(pi) == 'mpmath.pi'\n    assert 'mpmath' in p.module_imports\n    assert p.doprint(pi + x) == 'mpmath.pi + x'\n    assert p.doprint(Mod(x, 2)) == 'x % 2'\n    assert p.doprint(Assignment(x, 2)) == 'x = 2'\n", "def test_NumPyPrinter():\n    p = NumPyPrinter()\n    expr = acos(x)\n    assert 'numpy' not in p.module_imports\n    assert p.doprint(expr) == 'numpy.arccos(x)'\n    assert 'numpy' in p.module_imports\n    assert p.doprint(x**0.5) == 'numpy.sqrt(x)'\n    assert p.doprint(x % 2) == 'numpy.mod(x, 2)'\n    assert p.doprint(And(x, y)) == 'numpy.logical_and.reduce((x, y))'\n    assert p.doprint(Or(x, y)) == 'numpy.logical_or.reduce((x, y))'\n    assert p.doprint(Not(x)) == 'numpy.logical_not(x)'\n    assert p.doprint(Re(x)) == 'numpy.real(x)'\n    assert p.doprint(Im(x)) == 'numpy.imag(x)'\n    assert p.doprint(arg(x)) == 'numpy.angle(x)'\n    assert p.doprint(Min(x, y, z)) == 'numpy.amin((x, y, z))'\n    assert p.doprint(Max(x, y, z)) == 'numpy.amax((x, y, z))'\n    assert p.doprint(Piecewise((x, x > 0), (y, True))) == 'numpy.select([x > 0, True], [x, y], default=numpy.nan)'\n    assert p.doprint(Mod(x, 2)) == 'numpy.mod(x, 2)'\n    assert p.doprint(Sum(x, (x, 1, 5))) == 'builtins.sum(x for x in range(1, 6))'\n", "def test_MpmathPrinter():\n    p = MpmathPrinter()\n    assert not p.module_imports\n    assert p.doprint(pi) == 'mpmath.pi'\n    assert 'mpmath' in p.module_imports\n    expr = x + acos(x)\n    assert p.doprint(expr) == 'x + mpmath.acos(x)'\n    expr = x + symbols('log1p')(x)\n    assert p.doprint(expr) == 'x + mpmath.log(x+1)'\n    expr = symbols('uppergamma')(x, y)\n    assert p.doprint(expr) == 'mpmath.gammainc(x, y, mpmath.inf)'\n    expr = symbols('lowergamma')(x, y)\n    assert p.doprint(expr) == 'mpmath.gammainc(x, 0, y)'\n", "def test_MpmathPrinter():\n    p = MpmathPrinter()\n    expr = acos(x)\n    assert 'mpmath' not in p.module_imports\n    assert p.doprint(expr) == 'mpmath.acos(x)'\n    assert 'mpmath' in p.module_imports\n\n    from sympy import uppergamma, lowergamma, log1p, log2, Float\n    assert p.doprint(uppergamma(x, y)) == 'mpmath.gammainc(x, y, mpmath.inf)'\n    assert p.doprint(lowergamma(x, y)) == 'mpmath.gammainc(x, 0, y)'\n    assert p.doprint(log1p(x)) == 'mpmath.log(x+1)'\n    assert p.doprint(log2(x)) == 'mpmath.log(x)/mpmath.log(2)'\n    assert p.doprint(Float('1.23')) == 'mpmath.mpf((1, 2305843009213693952, -51, 53))'\n"], "sample_993": ["def test_FreeGroup_is_abelian():\n    G, a, b, c = free_group(\"a, b, c\")\n    assert G.is_abelian == False\n\n    H, d = free_group(\"d\")\n    assert H.is_abelian == True\n\n    assert free_group(\"\")[0].is_abelian == True\n", "def test_FreeGroupElm_cyclic_reduction():\n    w1 = x**2*y**-1*x**-1\n    w2 = y*x**-1*y**-1\n    assert w1.cyclic_reduction() == x*y**-1\n    assert w2.cyclic_reduction() == y*x**-1\n    assert w1.cyclic_reduction(removed=True) == (x*y**-1, x)\n    assert w2.cyclic_reduction(removed=True) == (y*x**-1, y)\n", "def test_FreeGroup_cyclic_conjugates():\n    w = x*y*x*y*x\n    assert w.cyclic_conjugates() == {x*y*x**2*y, x**2*y*x*y, y*x*y*x**2, y*x**2*y*x, x*y*x*y*x}\n    s = x*y*x**2*y*x\n    assert s.cyclic_conjugates() == {x**2*y*x**2*y, y*x**2*y*x**2, x*y*x**2*y*x}\n", "def test_FreeGroupElm_cyclic_operations():\n    w = x*y*x*y*x\n    w_cyclic_conjugates = w.cyclic_conjugates()\n    assert len(w_cyclic_conjugates) == 5\n    assert w in w_cyclic_conjugates\n    assert x*y*x**2*y in w_cyclic_conjugates\n    assert y*x**2*y*x in w_cyclic_conjugates\n\n    w1 = x**2*y**5\n    w2 = x*y**5*x\n    assert w1.is_cyclic_conjugate(w2) == True\n    w3 = x**-1*y**5*x**-1\n    assert w3.is_cyclic_conjugate(w2) == False\n\n    assert (x**2*y**-1*x**-1).is_cyclically_reduced() == False\n    assert (y*x**2*y**2).is_cyclically_reduced() == True\n\n    assert (x**2*y**2*x**-1).identity_cyclic_reduction() == x*y**2\n    assert (x**-3*y**-1*x**5).identity_cyclic_reduction() == x**2*y**-1\n\n    assert (x**2*y**2*x**-1).cyclic_reduction() == x*y**2\n    assert (x**-3*y**-1*x**5).cyclic_reduction() == y**-1*x**2\n    assert (x**-3*y**-1*x**5).cyclic_reduction(removed=True) == (y**-1*x**2, x**-3)\n", "def test_FreeGroupElm_power_of():\n    assert (x*y)**2 == (x*y)*(x*y)\n    assert (x**2*y**2*z).power_of(x*y*z) == False\n    assert (x**4*y**4).power_of(x*y) == True\n    assert (x**-4*y**-4).power_of(x**-1*y**-1) == True\n    assert (x*y*x*y).power_of((x*y)) == True\n    assert (x*y*x*y).power_of((y*x)) == False\n"], "sample_994": ["def test_Integer_conversion():\n    # Test conversion of Integer to various types\n    i = Integer(3)\n    assert int(i) == 3\n    assert float(i) == 3.0\n    assert complex(i) == 3 + 0j\n\n    # Test conversion of negative Integer to various types\n    i_neg = Integer(-5)\n    assert int(i_neg) == -5\n    assert float(i_neg) == -5.0\n    assert complex(i_neg) == -5 + 0j\n\n    # Test conversion of large Integer to float and complex\n    i_large = Integer(10**20)\n    assert float(i_large) == 10**20\n    assert complex(i_large) == 10**20 + 0j\n", "def test_mpf_norm_edge_cases():\n    # Test edge cases for mpf_norm\n    assert mpf_norm((0, 0, 1, 0), 53) == _mpf_zero  # Zero mantissa but non-zero exp\n    assert mpf_norm((0, 0, -1, 0), 53) == _mpf_zero  # Negative exponent but zero mantissa\n    assert mpf_norm((1, 0, 0, 1), 53) == _mpf_zero  # Negative zero edge case\n    assert mpf_norm((0, 1, 1, 53), 53) == (0, 1, 1, 53)  # Normal positive number\n    assert mpf_norm((1, 1, -1, 53), 53) == (1, 1, -1, 53)  # Normal negative number\n", "def test_issue_12345():\n    # Test for a combination of rational and float arithmetic\n    r = Rational(3, 7)\n    f = Float(2.5)\n    assert r + f == Float(3, 7) + 2.5\n    assert r - f == Float(3, 7) - 2.5\n    assert r * f == Float(3, 7) * 2.5\n    assert r / f == Float(3, 7) / 2.5\n    assert f + r == 2.5 + Float(3, 7)\n    assert f - r == 2.5 - Float(3, 7)\n    assert f * r == 2.5 * Float(3, 7)\n    assert f / r == 2.5 / Float(3, 7)\n", "def test_mpf_norm_issue_6639():\n    assert mpf_norm((1, 0, 1, 0), 53) == _mpf_zero\n    assert mpf_norm((0, 0, 0, 0), 53) == _mpf_zero\n    assert mpf_norm((_mpf_inf[0], 0, 0, 0), 53) == _mpf_inf\n    assert mpf_norm((_mpf_ninf[0], 0, 0, 0), 53) == _mpf_ninf\n    assert mpf_norm((_mpf_nan[0], 0, 0, 0), 53) == _mpf_nan\n", "def test_as_integer_ratio():\n    assert _as_integer_ratio(2.5) == (5, 2)\n    assert _as_integer_ratio(0.25) == (1, 4)\n    assert _as_integer_ratio(-2.5) == (-5, 2)\n    assert _as_integer_ratio(0.0) == (0, 1)\n    raises(ValueError, lambda: _as_integer_ratio(float('inf')))\n    raises(ValueError, lambda: _as_integer_ratio(float('-inf')))\n    raises(ValueError, lambda: _as_integer_ratio(float('nan')))\n"], "sample_995": ["def test_mpf_norm_inf_nan():\n    assert mpf_norm((0, 1, 0, 0), 10) == _mpf_inf\n    assert mpf_norm((0, -1, 0, 0), 10) == _mpf_ninf\n    assert mpf_norm((0, 0, 1, 0), 10) == _mpf_nan\n", "def test_mpf_norm_inf_nan():\n    from mpmath.libmp.libmpf import fnan, finf, fninf, fzero\n    assert mpf_norm(fnan, 10) == fnan\n    assert mpf_norm(finf, 10) == finf\n    assert mpf_norm(fninf, 10) == fninf\n    assert mpf_norm(fzero, 10) == fzero\n", "def test_mpf_norm_special_cases():\n    # Check special case handling of zero, inf, and nan mantissa by mpf_norm\n    from sympy import Float\n    from mpmath.libmp.libmpf import _normalize as mpf_normalize, finf, fninf, fnan\n\n    # zero mantissa with normal precision\n    mpf_zero = (0, 0, 0, 0)\n    assert mpf_norm(mpf_zero, 53) == (0, 0, 0, 0)\n    assert Float._new(mpf_zero, 53)._mpf_ == (0, 0, 0, 0)\n\n    # zero mantissa with non-zero exponent and non-zero bits count\n    mpf_inf_like = (0, 0, 12345, 10)\n    assert mpf_norm(mpf_inf_like, 53) == finf\n    assert Float._new(mpf_inf_like, 53)._mpf_ == finf\n\n    mpf_ninf_like = (1, 0, 12345, 10)\n    assert mpf_norm(mpf_ninf_like, 53) == fninf\n    assert Float._new(mpf_ninf_like, 53)._mpf_ == fninf\n\n    mpf_nan_like = (0, 0, 0, 1)\n    assert mpf_norm(mpf_nan_like, 53) == fnan\n    assert Float._new(mpf_nan_like, 53)._mpf_ == fnan\n", "def test_mpf_norm_inf_nan():\n    assert mpf_norm((_mpf_inf, 1, 0, 1), 10) == _mpf_inf\n    assert mpf_norm((_mpf_ninf, 1, 0, 1), 10) == _mpf_ninf\n    assert mpf_norm((_mpf_nan, 1, 0, 1), 10) == _mpf_nan\n\n    inf_mpf = Float('inf')._mpf_\n    assert mpf_norm(inf_mpf, 10) == _mpf_inf\n\n    ninf_mpf = Float('-inf')._mpf_\n    assert mpf_norm(ninf_mpf, 10) == _mpf_ninf\n\n    nan_mpf = Float('nan')._mpf_\n    assert mpf_norm(nan_mpf, 10) == _mpf_nan\n", "def test_mpf_norm_special_cases():\n    assert mpf_norm((0, 0, 1, 0), 10) == (0, 0, 0, 0)  # edge case where mantissa is zero and exponent is positive\n    assert mpf_norm((1, 1, 1, 1), 10) == mpf('0.5')._mpf_  # positive normalized mpf value\n    assert mpf_norm((1, 1, -1, 1), 10) == mpf('2')._mpf_  # negative normalized mpf value\n    assert mpf_norm((0, 1, -2, 10), 10) == mpf('0.25')._mpf_  # normalized mpf with precision 10\n    assert mpf_norm((0, 1, 0, 10), 10) == mpf('1')._mpf_  # normalized mpf with exponent zero\n"], "sample_996": ["def test_product_with_non_integer_limits():\n    # Test products with non-integer limits\n    assert product(2, (k, 1, 3.5)) == 2**(3.5 - 1 + 1)\n    assert product(k, (k, 1, 3.5)) == factorial(3) * 3.5\n    assert product(k**2, (k, 1, 3.5)) == factorial(3)**2 * 3.5**2\n    \n    # Test products with symbolic non-integer limits\n    a = Symbol('a', real=True)\n    b = Symbol('b', real=True)\n    assert product(2, (k, a, b)) == 2**(b - a + 1)\n    assert product(k, (k, a, b)) == Product(k, (k, a, b)).doit()\n    assert product(k**2, (k, a, b)) == Product(k**2, (k, a, b)).doit()\n", "def test_product_with_substitution():\n    # Test Product with substitution\n    i, j, k = symbols('i j k', integer=True)\n    P = Product(i**2, (i, 1, j))\n    assert P.subs(j, 5).doit() == Product(i**2, (i, 1, 5)).doit()\n    assert P.subs(j, k).doit() == Product(i**2, (i, 1, k)).doit()\n    P = Product(i**2, (i, 1, j)).doit()\n    assert P.subs(j, 5) == Product(i**2, (i, 1, 5)).doit()\n    assert P.subs(j, k) == Product(i**2, (i, 1, k)).doit()\n    assert Product(i**2, (i, 1, j)).subs(i, k) == Product(k**2, (k, 1, j))\n    assert Product(i**2, (i, 1, j)).subs(j, k + 1) == Product(i**2, (i, 1, k + 1))\n", "def test_commutative_property():\n    # Test the commutative property of Product\n    A, B = symbols(\"A B\", commutative=True)\n\n    p1 = Product(A * B, (k, 1, 5)).doit()\n    p2 = Product(B * A, (k, 1, 5)).doit()\n    assert p1 == p2\n\n    # Test non-commutative property\n    A, B = symbols(\"A B\", commutative=False)\n\n    p1 = Product(A * B, (k, 1, 5)).doit()\n    p2 = Product(B * A, (k, 1, 5)).doit()\n    assert p1 != p2\n\n    # Test with symbolic limits and functions\n    f = Function('f', commutative=True)\n    p1 = Product(A * f(k), (k, 1, n)).doit()\n    p2 = Product(f(k) * A, (k, 1, n)).doit()\n    assert p1 == p2\n\n    # Non-commutative function\n    f = Function('f', commutative=False)\n    p1 = Product(A * f(k), (k, 1, n)).doit()\n    p2 = Product(f(k) * A, (k, 1, n)).doit()\n    assert p1 != p2\n", "def test_eval_adjoint_conjugate_transpose():\n    A, B = symbols(\"A B\", commutative=False)\n    p = Product(A*B**k, (k, 1, 3))\n    \n    # Test adjoint\n    assert p.adjoint() == Product(B.adjoint()**k * A.adjoint(), (k, 1, 3))\n\n    # Test conjugate\n    assert p.conjugate() == Product((A*B**k).conjugate(), (k, 1, 3))\n\n    # Test transpose\n    assert p.transpose() == Product((A*B**k).transpose(), (k, 1, 3))\n", "def test_product_with_negative_limit():\n    assert product(2, (k, n, a)) == 2**(a - n + 1)\n    assert product(k, (k, n, -1)) == factorial(-1 - n + 1)\n    assert product(k**2, (k, -3, -1)) == factorial(-1 + 1)**2\n    assert product(k**2, (k, -3, -1)).doit() == 36\n    assert product(k + 1, (k, -5, -2)) == rf(1 - 5, -2 + 5 + 1)\n    assert product(cos(k), (k, -5, -1)) == cos(-5)*cos(-4)*cos(-3)*cos(-2)*cos(-1)\n    assert product(1 + 1/k, (k, -3, -1)) == rf(2, -1 + 3 + 1)/factorial(-1 + 3 + 1)\n"], "sample_997": ["def test_function_exponentiation():\n    transformations = standard_transformations + (implicit_multiplication_application,)\n    x = Symbol('x')\n    assert parse_expr(\"sin**2(x)\", transformations=transformations) == sin(x)**2\n    assert parse_expr(\"cos**4(x)\", transformations=transformations, local_dict={'cos': Function('cos')}) == Function('cos')(x)**4\n    assert parse_expr(\"tan**3(theta)\", transformations=transformations, local_dict={'theta': Symbol('theta'), 'tan': Function('tan')}) == Function('tan')(Symbol('theta'))**3\n", "def test_auto_number():\n    inputs = {\n        '123': Integer(123),\n        '0.456': Float('0.456'),\n        '1.23e-4': Float('1.23e-4'),\n        '5.67j': 5.67 * I,\n        '0b1010': Integer(10),\n        '0o12': Integer(10),\n        '0x1A': Integer(26),\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text) == result\n", "def test_auto_symbol():\n    local_dict = {}\n    global_dict = {}\n    assert parse_expr('a + b', local_dict=local_dict, global_dict=global_dict) == Symbol('a') + Symbol('b')\n    assert parse_expr('a + 2', local_dict=local_dict, global_dict=global_dict) == Symbol('a') + 2\n    assert parse_expr('2 + b', local_dict=local_dict, global_dict=global_dict) == 2 + Symbol('b')\n\n    local_dict['a'] = Integer(5)\n    assert parse_expr('a + b', local_dict=local_dict, global_dict=global_dict) == 5 + Symbol('b')\n    assert parse_expr('a + 2', local_dict=local_dict, global_dict=global_dict) == 5 + 2\n    assert parse_expr('2 + b', local_dict=local_dict, global_dict=global_dict) == 2 + Symbol('b')\n\n    global_dict['b'] = Integer(3)\n    assert parse_expr('a + b', local_dict=local_dict, global_dict=global_dict) == 5 + 3\n    assert parse_expr('a + 2', local_dict=local_dict, global_dict=global_dict) == 5 + 2\n    assert parse_expr('2 + b', local_dict=local_dict, global_dict=global_dict) == 2 + 3\n", "def test_lambda_notation():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"lambda x: x + 1\", transformations=transformations) == Lambda(x, x + 1)\n    assert parse_expr(\"lambda x, y: x + y\", transformations=transformations) == Lambda((x, y), x + y)\n    raises(TokenError, lambda: parse_expr(\"lambda *args: args\", transformations=transformations))\n    raises(TokenError, lambda: parse_expr(\"lambda **kwargs: kwargs\", transformations=transformations))\n", "def test_function_exponentiation():\n    transformations = standard_transformations + (function_exponentiation,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"sin**2(x)\", transformations=transformations) == sin(x)**2\n    assert parse_expr(\"exp**3(x)\", transformations=transformations) == exp(x)**3\n    assert parse_expr(\"cos**4(x + y)\", transformations=transformations, local_dict={'cos': Function('cos')}) == Function('cos')(x + y)**4\n"], "sample_998": ["def test_latex_NDimArray_nested():\n    x, y, z, w = symbols(\"x y z w\")\n\n    # Test nested NDimArray structures\n\n    M1 = MutableDenseNDimArray([1, 2])\n    M2 = MutableDenseNDimArray([3, 4])\n    M_nested = MutableDenseNDimArray([[M1, M2], [M2, M1]])\n\n    expected_latex = r\"\"\"\\left[\\begin{matrix}\"\"\"\\\n                     r\"\"\"\\left[\\begin{matrix}1 & 2\\end{matrix}\\right] & \"\"\"\\\n                     r\"\"\"\\left[\\begin{matrix}3 & 4\\end{matrix}\\right]\\\\\"\"\"\\\n                     r\"\"\"\\left[\\begin{matrix}3 & 4\\end{matrix}\\right] & \"\"\"\\\n                     r\"\"\"\\left[\\begin{matrix}1 & 2\\end{matrix}\\right]\"\"\"\\\n                     r\"\"\"\\end{matrix}\\right]\"\"\"\n\n    assert latex(M_nested) == expected_latex\n", "def test_latex_wildcard():\n    # Ensure that wildcards are printed correctly in LaTeX\n    w = Wild('w')\n    e = Wild('e', exclude=[x])\n    assert latex(w) == r\"w\"\n    assert latex(e) == r\"e\"\n    expr = w*x + e\n    assert latex(expr) == r\"w x + e\"\n    expr2 = e**2 + w**2\n    assert latex(expr2) == r\"e^{2} + w^{2}\"\n", "def test_latex_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == \"1 + 2 i + 3 j + 4 k\"\n    q = Quaternion(x, y, z, t)\n    assert latex(q) == \"x + y i + z j + t k\"\n    q = Quaternion(a + b, c - d, e*f, g/h)\n    assert latex(q) == r\"\\left(a + b\\right) + \\left(c - d\\right) i + e f j + \\frac{g}{h} k\"\n", "def test_latex_unknown_function():\n    # Test LaTeX rendering for unknown functions\n    unknown_func = Function('unknown')\n    assert latex(unknown_func(x)) == r'\\operatorname{unknown}{\\left (x \\right )}'\n    assert latex(unknown_func(x, y)) == r'\\operatorname{unknown}{\\left (x,y \\right )}'\n    assert latex(unknown_func(x**2)) == r'\\operatorname{unknown}{\\left (x^{2} \\right )}'\n    assert latex(unknown_func(x**2, y**3)) == r'\\operatorname{unknown}{\\left (x^{2},y^{3} \\right )}'\n", "def test_latex_parenthesize():\n    # Test the parenthesize method for various levels and strictness\n    from sympy.printing.latex import LatexPrinter\n\n    p = LatexPrinter()\n\n    # Simple cases without strictness\n    assert p.parenthesize(x + 1, 10) == r\"\\left(x + 1\\right)\"\n    assert p.parenthesize(x + 1, 10, strict=False) == r\"\\left(x + 1\\right)\"\n    assert p.parenthesize(x, 10) == \"x\"\n    assert p.parenthesize(x, 10, strict=False) == \"x\"\n\n    # Cases with strictness\n    assert p.parenthesize(x + 1, 10, strict=True) == r\"\\left(x + 1\\right)\"\n    assert p.parenthesize(x, 10, strict=True) == \"x\"\n\n    # Test with precedence levels\n    assert p.parenthesize(x * y, 1) == r\"\\left(x y\\right)\"\n    assert p.parenthesize(x * y, 1, strict=False) == r\"\\left(x y\\right)\"\n    assert p.parenthesize(x * y, 2) == \"x y\"\n    assert p.parenthesize(x * y, 2, strict=False) == \"x y\"\n\n    # Test with nested expressions\n    assert p.parenthesize(x + y * z, 10) == r\"\\left(x + y z\\right)\"\n    assert p.parenthesize(x * (y + z), 1) == r\"\\left(x \\left(y + z\\right)\\right)\"\n    assert p.parenthesize(x * (y + z), 2) == r\"x \\left(y + z\\right)\"\n    assert p.parenthesize(x * (y + z), 2, strict=False) == r\"x \\left(y + z\\right)\"\n"], "sample_999": ["def test_latex_assumptions():\n    x_positive = Symbol('x', positive=True)\n    x_nonnegative = Symbol('x', nonnegative=True)\n    assert latex(x_positive) == 'x'\n    assert latex(x_nonnegative) == 'x'\n    assert latex(Abs(x_positive)) == r'\\left|{x}\\right|'\n    assert latex(Abs(x_nonnegative)) == r'\\left|{x}\\right|'\n    assert latex(Abs(x_positive**-1)) == r'\\left|{x^{-1}}\\right|'\n    assert latex(Abs(x_nonnegative**-1)) == r'\\left|{x^{-1}}\\right|'\n    assert latex(x_positive + x_nonnegative) == 'x + x'\n    assert latex(sqrt(x_positive)) == r'\\sqrt{x}'\n    assert latex(sqrt(x_nonnegative)) == r'\\sqrt{x}'\n    assert latex(1 / x_positive) == r'\\frac{1}{x}'\n    assert latex(1 / x_nonnegative) == r'\\frac{1}{x}'\n", "def test_latex_undefined_function():\n    f = UndefinedFunction('f')\n    g = UndefinedFunction('g')\n    h = UndefinedFunction('h')\n    assert latex(f(x)) == r'f{\\left (x \\right )}'\n    assert latex(g(x, y)) == r'g{\\left (x, y \\right )}'\n    assert latex(h(x, y, z)) == r'h{\\left (x, y, z \\right )}'\n    assert latex(f) == r'f'\n    assert latex(g) == r'g'\n    assert latex(h) == r'h'\n", "def test_latex_undefined_functions():\n    f = UndefinedFunction('f')\n    g = UndefinedFunction('g')\n    h = UndefinedFunction('h')\n    assert latex(f(x)) == r'f{\\left (x \\right )}'\n    assert latex(g(x, y)) == r'g{\\left (x,y \\right )}'\n    assert latex(h(x, y, z)) == r'h{\\left (x,y,z \\right )}'\n    assert latex(f) == r'f'\n    assert latex(g) == r'g'\n    assert latex(h) == r'h'\n    assert latex(f(x) + g(y)) == r'f{\\left (x \\right )} + g{\\left (y \\right )}'\n    assert latex(g(x, y) * h(y, z)) == r'g{\\left (x,y \\right )} h{\\left (y,z \\right )}'\n    assert latex(h(x, y, z)**2) == r'h^{2}{\\left (x,y,z \\right )}'\n", "def test_latex_mixed_expressions():\n    # mixed expressions involving Add, Mul, and Pow\n    assert latex(x + y * z) == \"x + y z\"\n    assert latex(x * y + z) == \"x y + z\"\n    assert latex(x * (y + z)) == \"x \\left(y + z\\right)\"\n    assert latex((x + y) * z) == \"\\left(x + y\\right) z\"\n    assert latex(x + y * z**2) == \"x + y z^{2}\"\n    assert latex((x + y) * z**2) == \"\\left(x + y\\right) z^{2}\"\n    assert latex((x + y) / z) == \"\\frac{x + y}{z}\"\n    assert latex(x / (y + z)) == \"\\frac{x}{y + z}\"\n    assert latex((x + y) / (z + w)) == \"\\frac{x + y}{z + w}\"\n    assert latex(x**2 + 2*x*y + y**2) == \"x^{2} + 2 x y + y^{2}\"\n    assert latex((x + 1)**2) == \"\\left(x + 1\\right)^{2}\"\n    assert latex((x + y + z)**3) == \"\\left(x + y + z\\right)^{3}\"\n    assert latex((x**2 + y**2)**Rational(1, 2)) == \"\\sqrt{x^{2} + y^{2}}\"\n    assert latex((x**3 + y**3)**Rational(1, 3)) == \"\\sqrt[3]{x^{3} + y^{3}}\"\n", "def test_latex_noncommutative_symbols():\n    # Test that noncommutative symbols are printed correctly\n    A, B = symbols('A B', commutative=False)\n    expr1 = A*B + B*A\n    expr2 = A**2 + B**2\n    assert latex(expr1) == r\"A B + B A\"\n    assert latex(expr2) == r\"A^{2} + B^{2}\"\n\n    # Check expressions involving multiple noncommutative symbols\n    expr3 = A*B**2 + B*A**2\n    expr4 = (A*B + B*A)**2\n    assert latex(expr3) == r\"A B^{2} + B A^{2}\"\n    assert latex(expr4) == r\"\\left(A B + B A\\right)^{2}\"\n"], "sample_1000": ["def test_custom_user_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"custom_scalar_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n        'custom_fcn(x) + custom_scalar_fcn(x) + custom_mat_fcn([1 x])'\n", "def test_octave_code_with_user_functions():\n    custom_functions = {\n        \"f\": \"custom_f\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_matrix_g\"),\n              (lambda x: not x.is_Matrix, \"custom_scalar_g\")]\n    }\n    f = Function('f')\n    g = Function('g')\n    expr1 = f(x) + g(x) + g(Matrix([[x, y]]))\n    assert mcode(expr1, user_functions=custom_functions) == \"custom_f(x) + custom_scalar_g(x) + custom_matrix_g([x y])\"\n    expr2 = f(x**2) * g(y) + f(Matrix([[x, y]]))\n    assert mcode(expr2, user_functions=custom_functions) == \"custom_f(x.^2)*custom_scalar_g(y) + custom_f([x y])\"\n", "def test_custom_user_functions():\n    from sympy import Function, cos\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"custom_scalar_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n        'custom_fcn(x) + custom_scalar_fcn(x) + custom_mat_fcn([1 x])'\n    assert mcode(cos(f(x)), user_functions={\"f\": \"custom_fcn\"}) == 'cos(custom_fcn(x))'\n", "def test_UserDefinedFunctions():\n    f = Function('f')\n    g = Function('g')\n    assert mcode(f(x), user_functions={f: \"user_fcn\"}) == \"user_fcn(x)\"\n    custom_funcs = {g: [(lambda x: x.is_Matrix, \"matrix_fcn\"), (lambda x: not x.is_Matrix, \"scalar_fcn\")]}\n    assert mcode(g(x), user_functions=custom_funcs) == \"scalar_fcn(x)\"\n    mat = Matrix([[1, x]])\n    assert mcode(g(mat), user_functions=custom_funcs) == \"matrix_fcn([1 x])\"\n", "def test_Lambda():\n    f = Lambda(x, x**2 + 2*x + 1)\n    assert mcode(f) == '@(x) x.^2 + 2*x + 1'\n    assert mcode(f(y)) == '(y).^2 + 2*y + 1'\n"], "sample_1001": ["def test_latex_parenthesize():\n    from sympy.abc import a, b, c\n    from sympy.printing.latex import LatexPrinter\n\n    printer = LatexPrinter()\n\n    # Testing precedence and parenthesis insertion\n    assert printer.parenthesize(a + b, PRECEDENCE[\"Add\"] + 1) == r\"\\left(a + b\\right)\"\n    assert printer.parenthesize(a * b, PRECEDENCE[\"Mul\"] + 1) == r\"\\left(a b\\right)\"\n    assert printer.parenthesize(a ** b, PRECEDENCE[\"Pow\"] + 1) == r\"\\left(a^{b}\\right)\"\n    assert printer.parenthesize(a + b, PRECEDENCE[\"Add\"]) == r\"a + b\"\n    assert printer.parenthesize(a * b, PRECEDENCE[\"Mul\"]) == r\"a b\"\n    assert printer.parenthesize(a ** b, PRECEDENCE[\"Pow\"]) == r\"a^{b}\"\n    assert printer.parenthesize(a + b, PRECEDENCE[\"Add\"], strict=True) == r\"\\left(a + b\\right)\"\n    assert printer.parenthesize(a * b, PRECEDENCE[\"Mul\"], strict=True) == r\"\\left(a b\\right)\"\n    assert printer.parenthesize(a ** b, PRECEDENCE[\"Pow\"], strict=True) == r\"\\left(a^{b}\\right)\"\n\n    # Testing strictness in parenthesis insertion\n    assert printer.parenthesize(a + b * c, PRECEDENCE[\"Add\"], strict=True) == r\"a + b c\"\n    assert printer.parenthesize(a * b + c, PRECEDENCE[\"Mul\"], strict=True) == r\"a b + c\"\n    assert printer.parenthesize(a ** (b + c), PRECEDENCE[\"Pow\"], strict=True) == r\"a^{b + c}\"\n", "def test_latex_Modulus():\n    assert latex(Mod(x, y)) == r'x\\bmod{y}'\n    assert latex(Mod(x + 1, y - 1)) == r'\\left(x + 1\\right)\\bmod{y - 1}'\n    assert latex(2 * Mod(x, y)) == r'2 \\left(x\\bmod{y}\\right)'\n    assert latex(Mod(x, y) + 1) == r'\\left(x\\bmod{y}\\right) + 1'\n    assert latex(Mod(3*x + 2, 2*x + 1)) == r'\\left(3 x + 2\\right)\\bmod{2 x + 1}'\n", "def test_latex_MatrixSymbol_determinant():\n    from sympy import MatrixSymbol, Determinant\n    A = MatrixSymbol('A', 3, 3)\n    assert latex(Determinant(A)) == r\"\\left|A\\right|\"\n    assert latex(Determinant(2*A)) == r\"\\left|2 A\\right|\"\n    assert latex(Determinant(A + 2*eye(3))) == r\"\\left|A + 2 I\\right|\"\n", "def test_latex_parenthesize():\n    assert latex(Add(x, y), order='none') == 'x + y'\n    assert latex(Add(Add(x, y), z), order='none') == 'x + y + z'\n    assert latex(Add(Add(x, y), Mul(2, z)), order='none') == 'x + y + 2 z'\n    assert latex(Mul(Add(x, y), z), order='none') == r'\\left(x + y\\right) z'\n    assert latex(Mul(Add(x, y), Mul(2, z)), order='none') == r'\\left(x + y\\right) 2 z'\n    assert latex(Pow(Add(x, y), z), order='none') == r'\\left(x + y\\right)^{z}'\n    assert latex(Pow(Add(x, y), Pow(z, 2)), order='none') == r'\\left(x + y\\right)^{z^{2}}'\n    assert latex(Pow(Add(x, y), Add(z, 2)), order='none') == r'\\left(x + y\\right)^{z + 2}'\n    assert latex(Pow(Add(x, y), Mul(2, z)), order='none') == r'\\left(x + y\\right)^{2 z}'\n    assert latex(Pow(Mul(x, y), z), order='none') == r'x^{z} y^{z}'\n    assert latex(Pow(Mul(x, y), Add(z, 2)), order='none') == r'\\left(x y\\right)^{z + 2}'\n    assert latex(Pow(Mul(x, y), Mul(2, z)), order='none') == r'\\left(x y\\right)^{2 z}'\n    assert latex(Pow(Add(Mul(2, x), y), z), order='none') == r'\\left(2 x + y\\right)^{z}'\n    assert latex(Pow(Add(Mul(2, x), y), Add(z, 2)), order='none') == r'\\left(2 x + y\\right)^{z + 2}'\n    assert latex(Pow(Add(Mul(2, x), y), Mul(2, z)), order='none') == r'\\left(2 x + y\\right)^{2 z}'\n    assert latex(Pow(Add(Mul(2, x), Mul(3,", "def test_latex_tuples():\n    assert latex((x, y, z)) == r\"\\left ( x,\\quad y,\\quad z\\right )\"\n    assert latex((x, (y, z))) == r\"\\left ( x,\\quad \\left ( y,\\quad z\\right )\\right )\"\n    assert latex(((x, y), z)) == r\"\\left ( \\left ( x,\\quad y\\right ),\\quad z\\right )\"\n"], "sample_1002": ["def test_mpf_to_int():\n    assert int(Float('0.5', 15)) == 0\n    assert int(Float('1.5', 15)) == 1\n    assert int(Float('-1.5', 15)) == -1\n    assert int(Float('2.999', 15)) == 2\n    assert int(Float('-2.999', 15)) == -2\n    assert int(Float('123456789.987654321', 50)) == 123456789\n    assert int(Float('-123456789.987654321', 50)) == -123456789\n    raises(ValueError, lambda: int(Float('nan')))\n    raises(ValueError, lambda: int(Float('inf')))\n    raises(ValueError, lambda: int(Float('-inf')))\n", "def test_mpf_norm_cases():\n    # Test cases for mpf_norm function\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((0, 1, 0, 1), 10) == (0, 1, 0, 1)\n    assert mpf_norm((1, 0, 1, 0), 10) == _mpf_zero\n    assert mpf_norm((1, 0, 1, 1), 10) == (1, 0, 1, 1)\n    assert mpf_norm((1, 1, 1, 1), 10) == (1, 1, 1, 1)\n    assert mpf_norm((0, 1, 1, 0), 10) == (0, 1, 1, 0)\n    assert mpf_norm((0, 1, 2, 0), 10) == (0, 1, 2, 0)\n", "def test_issue_11192():\n    assert Rational(11, 3) % 3 == Rational(2, 3)\n    assert Rational(11, 3) % 1 == Rational(2, 3)\n    assert 11 % Rational(3, 1) == 2\n    assert 11 % Rational(3, 2) == Rational(1, 2)\n    assert Rational(17, 2) % Rational(2, 3) == Rational(1, 6)\n    assert Rational(-17, 2) % Rational(2, 3) == Rational(1, 2)\n", "def test_issue_17422():\n    # Test for issue 17422\n    assert comp(Float('1.23456789', ''), '1.23456789')\n    assert comp(Float('1.23456789', ''), Float('1.23456789', ''))\n    assert comp(Float('1.23456789', ''), Float('1.234567890', '')) is False\n    assert comp(Float('1.234567890', ''), '1.23456789') is False\n    assert comp(Float('1.234567890', ''), Float('1.23456789', '')) is False\n", "def test_comp_function():\n    assert comp(1, 1.0000000001) is False\n    assert comp(1, 1.00000000001) is True\n    assert comp(1, 1.0000000002, tol=1e-10) is False\n    assert comp(1, 1.0000000002, tol=1e-9) is True\n    assert comp(1, 1.0000000002, tol='') is False\n    assert comp(1, \"1.0000000002\") is False\n    assert comp(1.0000000001, \"1.0000000002\") is False\n    assert comp(1.0000000001, \"1.0000000001\") is True\n    assert comp(1.0000000001, \"1.0000000002\", tol='') is False\n    raises(ValueError, lambda: comp(1.0000000001, \"string\"))\n    raises(ValueError, lambda: comp(1, \"1.0000000002\", tol='invalid'))\n"], "sample_1003": ["def test_Gen_preprocess():\n    assert Gen.preprocess(x) == x\n    assert Gen.preprocess(1) == 1\n    \n    raises(OptionError, lambda: Gen.preprocess('invalid'))\n", "def test_Method_postprocess():\n    opt = {'method': 'groebner'}\n    Method.postprocess(opt)\n\n    assert opt == {'method': 'groebner'}\n", "def test_Method_postprocess():\n    opt = {'method': 'test_method'}\n    Method.postprocess(opt)\n\n    assert opt == {'method': 'test_method'}\n", "def test_Method_postprocess():\n    opt = {'method': 'groebner'}\n    Method.postprocess(opt)\n\n    assert opt == {'method': 'groebner'}\n", "def test_Method_postprocess():\n    opt = {'method': 'default'}\n    Method.postprocess(opt)\n\n    assert opt == {'method': 'default'}\n"], "sample_1004": ["def test_invalid_condition():\n    raises(ValueError, lambda: ConditionSet(x + 1, x + 1 < 1, S.Integers))\n    raises(ValueError, lambda: ConditionSet(x + 1, x < 1, S.Integers))\n    raises(ValueError, lambda: ConditionSet(x, x < 1, {x + 1, y}))\n", "def test_condition_simplification():\n    # Test if the ConditionSet simplifies when the condition is always true\n    condition = x < 10\n    base_set = FiniteSet(1, 2, 3)\n    cond_set = ConditionSet(x, condition, base_set)\n    assert cond_set == base_set\n\n    # Test if the ConditionSet simplifies when the condition is always false\n    condition = x > 10\n    base_set = FiniteSet(1, 2, 3)\n    cond_set = ConditionSet(x, condition, base_set)\n    assert cond_set == EmptySet()\n\n    # Test if the ConditionSet simplifies with mixed conditions\n    condition = Or(x < 0, x > 2)\n    base_set = FiniteSet(-1, 0, 1, 2, 3)\n    cond_set = ConditionSet(x, condition, base_set)\n    assert cond_set == FiniteSet(-1, 3)\n", "def test_CondSet_with_set_base():\n    assert ConditionSet(x, x < 2, {1, 2, 3}) == FiniteSet(1)\n    assert ConditionSet(x, x % 2 == 0, {1, 2, 3, 4, 5}) == FiniteSet(2, 4)\n    assert ConditionSet(x, x > 3, {1, 2, 3, 4, 5}) == FiniteSet(4, 5)\n    assert ConditionSet(x, x < 1, {1, 2, 3}) == EmptySet()\n", "def test_CondSet_no_base_set():\n    assert ConditionSet(x, x < 1).base_set == S.UniversalSet\n    cs = ConditionSet(x, x < 1)\n    assert 0 in cs\n    assert 2 not in cs\n    assert cs.contains(0) == S.true\n    assert cs.contains(2) == S.false\n", "def test_contains():\n    assert ConditionSet(x, x < 5, Interval(0, 10)).contains(3) == And(Lambda(x, x < 5)(3), Interval(0, 10).contains(3))\n    assert ConditionSet(x, x < 5, Interval(0, 10)).contains(6) == And(Lambda(x, x < 5)(6), Interval(0, 10).contains(6))\n    assert ConditionSet(x, x > 5, FiniteSet(1, 2, 3, 4, 5, 6)).contains(6) == And(Lambda(x, x > 5)(6), FiniteSet(1, 2, 3, 4, 5, 6).contains(6))\n    assert ConditionSet(x, x > 5, FiniteSet(1, 2, 3, 4, 5)).contains(6) == And(Lambda(x, x > 5)(6), FiniteSet(1, 2, 3, 4, 5).contains(6))\n"], "sample_1005": ["def test_latex_custom_functions():\n    class CustomFunction(Function):\n        @classmethod\n            return None  # this prevents evaluation\n\n            return r'\\operatorname{CustomFunc}\\left(%s\\right)' % printer._print(self.args[0])\n\n    f = CustomFunction(x)\n    assert latex(f) == r'\\operatorname{CustomFunc}\\left(x\\right)'\n    \n    g = CustomFunction(x + y)\n    assert latex(g) == r'\\operatorname{CustomFunc}\\left(x + y\\right)'\n    \n    h = CustomFunction(x * y)\n    assert latex(h) == r'\\operatorname{CustomFunc}\\left(x y\\right)'\n    \n    i = CustomFunction(sin(x))\n    assert latex(i) == r'\\operatorname{CustomFunc}\\left(\\sin{\\left (x \\right )}\\right)'\n", "def test_latex_Gradient():\n    assert latex(Gradient(x**2 + y**2)) == r\"\\nabla\\cdot \\left(x^{2} + y^{2}\\right)\"\n    assert latex(Gradient(sin(x) + cos(y))) == r\"\\nabla\\cdot \\left(\\sin{\\left (x \\right )} + \\cos{\\left (y \\right )}\\right)\"\n    assert latex(Gradient(2*x*y + 3*y*z + 4*z*x)) == r\"\\nabla\\cdot \\left(2 x y + 3 y z + 4 x z\\right)\"\n", "def test_latex_special_function_classes():\n    from sympy.functions.special.gamma_functions import gamma\n    from sympy.functions.special.beta_functions import beta\n    from sympy.functions.special.delta_functions import DiracDelta\n    from sympy.functions.special.error_functions import Chi\n    from sympy.functions.special.tensor_functions import KroneckerDelta\n\n    assert latex(gamma(x)) == r\"\\Gamma\\left(x\\right)\"\n    assert latex(beta(x, y)) == r\"\\operatorname{B}\\left(x, y\\right)\"\n    assert latex(DiracDelta(x)) == r\"\\delta\\left(x\\right)\"\n    assert latex(Chi(x)) == r\"\\operatorname{Chi}\\left(x\\right)\"\n    assert latex(KroneckerDelta(x, y)) == r\"\\delta_{x y}\"\n\n    # Ensure they render correctly in complex expressions\n    assert latex(gamma(x) + beta(x, y)) == r\"\\Gamma\\left(x\\right) + \\operatorname{B}\\left(x, y\\right)\"\n    assert latex(DiracDelta(x) * Chi(x)) == r\"\\delta\\left(x\\right) \\operatorname{Chi}\\left(x\\right)\"\n    assert latex(KroneckerDelta(x, y) + 1) == r\"\\delta_{x y} + 1\"\n", "def test_latex_derivative_with_subs():\n    from sympy.abc import f\n    expr = diff(f(x), x, evaluate=False).subs(f(x), x**2 + sin(x))\n    assert latex(expr) == r\"\\frac{d}{d x} \\left(x^{2} + \\sin{\\left (x \\right )}\\right)\"\n    expr = diff(f(x)*g(y), x, evaluate=False).subs(f(x), x**2 + sin(x))\n    assert latex(expr) == r\"\\frac{\\partial}{\\partial x} \\left(\\left(x^{2} + \\sin{\\left (x \\right )}\\right) g{\\left (y \\right )}\\right)\"\n", "def test_latex_Limit_with_directions():\n    expr = Limit(x**2, x, 0, dir='-')\n    assert latex(expr) == r\"\\lim_{x \\to 0^-} x^{2}\"\n    expr = Limit(x**2, x, 0, dir='+')\n    assert latex(expr) == r\"\\lim_{x \\to 0^+} x^{2}\"\n    expr = Limit(x**2, x, 0, dir='+-')\n    assert latex(expr) == r\"\\lim_{x \\to 0} x^{2}\"\n    expr = Limit(x**2, x, oo)\n    assert latex(expr) == r\"\\lim_{x \\to \\infty} x^{2}\"\n    expr = Limit(x**2, x, -oo)\n    assert latex(expr) == r\"\\lim_{x \\to -\\infty} x^{2}\"\n"], "sample_1006": ["def test_subfactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(n).is_even is None\n    assert subfactorial(n).is_odd is None\n\n    assert subfactorial(n + 2).is_integer is True\n    assert subfactorial(n + 1).is_even is True\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, positive=True)\n\n    assert subfactorial(-1) == 1\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(6) == 265\n    assert subfactorial(7) == 1854\n    assert subfactorial(n).func == subfactorial\n    assert subfactorial(n + 1).func == subfactorial\n    assert subfactorial(oo) == oo\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n    assert subfactorial(n).rewrite(factorial) == subfactorial(n)\n    assert subfactorial(n).rewrite(gamma) == subfactorial(n)\n    assert subfactorial(n).rewrite(Product) == subfactorial(n)\n    assert subfactorial(n).rewrite(rf) == subfactorial(n)\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer is True\n    assert subfactorial(k).is_nonnegative is True\n    assert subfactorial(k).is_even is None\n    assert subfactorial(k).is_odd is None\n    assert subfactorial(n).is_even is None\n    assert subfactorial(n).is_odd is None\n\n    assert subfactorial(n).diff(n) == subfactorial(n) - subfactorial(n - 1)\n    assert subfactorial(n).series(n, 0, 3) == 1 - n + n**2/2 + O(n**3)\n", "def test_subfactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    r = Symbol('r', integer=False)\n    s = Symbol('s', integer=False, negative=True)\n    t = Symbol('t', nonnegative=True)\n    u = Symbol('u', noninteger=True)\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(5) == 44\n    assert subfactorial(10) == 1334961\n\n    assert subfactorial(x).is_integer is None\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer\n    assert subfactorial(r).is_integer is None\n\n    assert subfactorial(n).is_positive is None\n    assert subfactorial(k).is_positive is None\n\n    assert subfactorial(x).is_real is None\n    assert subfactorial(n).is_real is None\n    assert subfactorial(k).is_real is True\n    assert subfactorial(r).is_real is None\n    assert subfactorial(s).is_real is None\n    assert subfactorial(t).is_real is True\n    assert subfactorial(u).is_real is None\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(4).is_even is True\n    assert subfactorial(5).is_odd is True\n", "def test_subfactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    r = Symbol('r', integer=False)\n    s = Symbol('s', integer=False, negative=True)\n    t = Symbol('t', nonnegative=True)\n    u = Symbol('u', noninteger=True)\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(6) == 265\n    assert subfactorial(7) == 1854\n    assert subfactorial(8) == 14833\n\n    assert subfactorial(x).is_integer is None\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer\n    assert subfactorial(r).is_integer is None\n\n    assert subfactorial(n).is_positive is None\n    assert subfactorial(k).is_positive\n\n    assert subfactorial(x).is_real is None\n    assert subfactorial(n).is_real is None\n    assert subfactorial(k).is_real is True\n    assert subfactorial(r).is_real is None\n    assert subfactorial(s).is_real is True\n    assert subfactorial(t).is_real is True\n    assert subfactorial(u).is_real is True\n\n    assert subfactorial(x).is_even is None\n    assert subfactorial(n).is_even is None\n    assert subfactorial(k).is_even is None\n    assert subfactorial(k + 2).is_even is None\n\n    assert subfactorial(oo) == oo\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n\n    assert subfactorial(n).diff(n) == subfactorial(n) * polygamma(0, n) - subfactorial(n - 1)\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    x = Symbol('x')\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(6) == 265\n    assert subfactorial(7) == 1854\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer is True\n\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(k).is_nonnegative is True\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(k).is_even is None\n    assert subfactorial(4).is_even is True\n    assert subfactorial(5).is_even is False\n\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(k).is_odd is None\n    assert subfactorial(4).is_odd is False\n    assert subfactorial(5).is_odd is True\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n    assert subfactorial(3).rewrite(uppergamma) == uppergamma(4, -1)/S.Exp1\n    assert subfactorial(x).rewrite(uppergamma) == uppergamma(x + 1, -1)/S.Exp1\n"], "sample_1007": ["def test_subfactorial():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n    \n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(10) == 1334961\n    \n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n    \n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer\n    \n    assert subfactorial(n).is_even is None\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(k).is_even is None\n    assert subfactorial(k).is_odd is None\n    \n    assert subfactorial(n).is_positive is None\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(k).is_positive\n    assert subfactorial(k).is_nonnegative\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n\n    assert subfactorial(-1) == 0\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer is True\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(k).is_nonnegative is True\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n    assert subfactorial(n).rewrite(factorial) == simplify((factorial(n) / S.Exp1).round())\n\n    assert subfactorial(n).diff(n) == (n - 1) * (subfactorial(n - 1) + subfactorial(n - 2))\n    assert subfactorial(n).series(n, 0, 3) == 1 - n + n**2/2 + O(n**3)\n", "def test_subfactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    r = Symbol('r', integer=False)\n    u = Symbol('u', noninteger=True)\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(6) == 265\n    assert subfactorial(n).func == subfactorial\n    assert subfactorial(n + 1).func == subfactorial\n\n    assert subfactorial(x).is_integer is None\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer\n    assert subfactorial(r).is_integer is None\n\n    assert subfactorial(n).is_positive is None\n    assert subfactorial(k).is_positive\n\n    assert subfactorial(x).is_real is None\n    assert subfactorial(n).is_real is None\n    assert subfactorial(k).is_real is True\n    assert subfactorial(r).is_real is None\n    assert subfactorial(u).is_real is None\n\n    assert subfactorial(x).is_even is None\n    assert subfactorial(n).is_even is None\n    assert subfactorial(k).is_even is None\n    assert subfactorial(k + 1).is_even is True\n    assert subfactorial(k + 2).is_even is None\n\n    assert subfactorial(x).is_odd is None\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(k).is_odd is None\n    assert subfactorial(k + 1).is_odd is None\n    assert subfactorial(k + 2).is_odd is True\n\n    assert subfactorial(n + 1).rewrite(uppergamma) == uppergamma(n + 2, -1) / S.Exp1\n    assert subfactorial(n + 2).rewrite(uppergamma) == uppergamma(n + 3, -1) / S.Exp1\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(n).is_even is None\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(n).is_nonnegative is None\n\n    m = Symbol('m', integer=True, nonnegative=True)\n    assert subfactorial(m).is_integer is True\n    assert subfactorial(m).is_nonnegative is True\n    assert subfactorial(2 * m + 1).is_even is True\n    assert subfactorial(2 * m).is_odd is True\n\n    assert subfactorial(oo) == oo\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(6) == 265\n    assert subfactorial(10) == 1334961\n    assert subfactorial(n).func == subfactorial\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(n).is_even is None\n    assert subfactorial(n).is_odd is None\n\n    assert subfactorial(oo) == oo\n    assert subfactorial(zoo) == zoo\n    assert subfactorial(nan) == nan\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n    assert subfactorial(5).rewrite(uppergamma) == uppergamma(6, -1) / S.Exp1\n"], "sample_1008": ["def test_ang_acc():\n    q1, q2, q3, q4, q5 = dynamicsymbols('q1 q2 q3 q4 q5')\n    q1d, q2d, q3d, q4d, q5d = dynamicsymbols('q1 q2 q3 q4 q5', 1)\n    q1dd, q2dd, q3dd, q4dd, q5dd = dynamicsymbols('q1 q2 q3 q4 q5', 2)\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n    C = B.orientnew('C', 'Axis', [q3, B.y])\n    D = N.orientnew('D', 'Axis', [q4, N.y])\n    A.set_ang_acc(N, q1dd*N.z)\n    assert A.ang_acc_in(N) == q1dd*N.z\n    assert B.ang_acc_in(N) == q1dd*N.z + q2dd*B.x\n    assert C.ang_acc_in(N) == q1dd*N.z + q2dd*B.x + q3dd*C.y\n    D.set_ang_acc(N, q5dd*D.x)\n    assert D.ang_acc_in(N) == q5dd*D.x\n    assert N.ang_acc_in(A) == -q1dd*N.z\n    assert N.ang_acc_in(B) == -q1dd*N.z - q2dd*B.x\n    assert N.ang_acc_in(C) == -q1dd*N.z - q2dd*B.x - q3dd*C.y\n    assert N.ang_acc_in(D) == -q5dd*D.x\n    D.set_ang_acc(N, 0)\n    assert D.ang_acc_in(N) == 0\n", "def test_coordinate_sym_equality():\n    \"\"\"Tests the equality and inequality of CoordinateSym instances.\"\"\"\n    A = ReferenceFrame('A')\n    B = ReferenceFrame('B')\n    \n    coord1_A = CoordinateSym('Ax', A, 0)\n    coord2_A = CoordinateSym('Ax', A, 0)\n    coord3_A = CoordinateSym('Ay', A, 1)\n    coord1_B = CoordinateSym('Bx', B, 0)\n\n    # Same frame and index should be equal\n    assert coord1_A == coord2_A\n    \n    # Different index should not be equal\n    assert coord1_A != coord3_A\n    \n    # Different frame should not be equal\n    assert coord1_A != coord1_B\n\n    # Hash tests\n    assert hash(coord1_A) == hash(coord2_A)\n    assert hash(coord1_A) != hash(coord3_A)\n    assert hash(coord1_A) != hash(coord1_B)\n", "def test_reference_frame_initialization():\n    \"\"\"Tests the initialization of ReferenceFrame with various parameters.\"\"\"\n    # Test default initialization\n    N = ReferenceFrame('N')\n    assert N.name == 'N'\n    assert N.indices == ['x', 'y', 'z']\n    assert N.latex_vecs == [r\"\\mathbf{\\hat{n}_x}\", r\"\\mathbf{\\hat{n}_y}\", r\"\\mathbf{\\hat{n}_z}\"]\n    assert N.str_vecs == ['N.x', 'N.y', 'N.z']\n\n    # Test initialization with custom indices\n    O = ReferenceFrame('O', indices=('a', 'b', 'c'))\n    assert O.indices == ['a', 'b', 'c']\n    assert O.latex_vecs == [r\"\\mathbf{\\hat{o}_a}\", r\"\\mathbf{\\hat{o}_b}\", r\"\\mathbf{\\hat{o}_c}\"]\n    assert O.str_vecs == [\"O['a']\", \"O['b']\", \"O['c']\"]\n\n    # Test initialization with custom LaTeX basis vectors\n    P = ReferenceFrame('P', latexs=('A1', 'A2', 'A3'))\n    assert P.latex_vecs == ['A1', 'A2', 'A3']\n\n    # Test initialization with custom variable names\n    Q = ReferenceFrame('Q', variables=['Q1', 'Q2', 'Q3'])\n    assert Q.varlist[0].name == 'Q1'\n    assert Q.varlist[1].name == 'Q2'\n    assert Q.varlist[2].name == 'Q3'\n\n    # Test invalid inputs for name\n    try:\n        ReferenceFrame(123)\n    except TypeError as e:\n        assert str(e) == 'Need to supply a valid name'\n\n    # Test invalid inputs for indices\n    try:\n        ReferenceFrame('R', indices='abc')\n    except TypeError as e:\n        assert str(e) == 'Supply the indices as a list'\n    try:\n        ReferenceFrame('S', indices=['a', 'b'])\n    except ValueError as e:\n        assert str(e) == 'Supply 3 indices'\n    try:\n        ReferenceFrame('T', indices=['a', 'b', 3])\n    except TypeError as e:\n        assert str(e) == 'Indices must be strings'\n\n    # Test invalid inputs for LaTeX basis vectors\n    try:\n       ", "def test_coordinate_sym_equality():\n    \"\"\"Tests the equality and inequality of CoordinateSym objects.\"\"\"\n    A = ReferenceFrame('A')\n    B = ReferenceFrame('B')\n    coord_sym_A0 = CoordinateSym('Ax', A, 0)\n    coord_sym_A1 = CoordinateSym('Ay', A, 1)\n    coord_sym_B0 = CoordinateSym('Bx', B, 0)\n\n    assert coord_sym_A0 == A[0]\n    assert coord_sym_A1 == A[1]\n    assert coord_sym_B0 == B[0]\n    assert coord_sym_A0 != coord_sym_A1\n    assert coord_sym_A0 != coord_sym_B0\n    assert coord_sym_A1 != coord_sym_B0\n\n    # Ensure equality with the same frame and index, even if instantiated separately\n    assert coord_sym_A0 == CoordinateSym('Ax', A, 0)\n    assert coord_sym_B0 == CoordinateSym('Bx', B, 0)\n", "def test_invalid_indices():\n    \"\"\"Test for invalid indices during CoordinateSym creation\"\"\"\n    A = ReferenceFrame('A')\n    try:\n        CoordinateSym('Ax', A, -1)\n    except ValueError as e:\n        assert str(e) == \"Invalid index specified\"\n    \n    try:\n        CoordinateSym('Ax', A, 3)\n    except ValueError as e:\n        assert str(e) == \"Invalid index specified\"\n\n    # Valid indices should not raise any exception\n    try:\n        CoordinateSym('Ax', A, 0)\n        CoordinateSym('Ax', A, 1)\n        CoordinateSym('Ax', A, 2)\n    except ValueError:\n        assert False, \"Valid indices raised an exception\"\n"], "sample_1009": ["def test_Vector_operations():\n    q1, q2 = symbols('q1 q2')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    v1 = q1 * N.x + q2 * N.y\n    v2 = -q1 * A.x + q2 * A.y\n\n    # Test vector addition\n    v3 = v1 + v2\n    assert isinstance(v3, Vector)\n    assert dot(v3, N.x) == q1 - q1 * cos(q1)\n    assert dot(v3, N.y) == q2 + q2 * cos(q1)\n    assert dot(v3, N.z) == -q1 * sin(q1)\n\n    # Test vector subtraction\n    v4 = v1 - v2\n    assert isinstance(v4, Vector)\n    assert dot(v4, N.x) == q1 + q1 * cos(q1)\n    assert dot(v4, N.y) == q2 - q2 * cos(q1)\n    assert dot(v4, N.z) == q1 * sin(q1)\n\n    # Test vector negation\n    v5 = -v1\n    assert isinstance(v5, Vector)\n    assert dot(v5, N.x) == -q1\n    assert dot(v5, N.y) == -q2\n    assert dot(v5, N.z) == 0\n\n    # Test vector scalar multiplication\n    v6 = 2 * v1\n    assert isinstance(v6, Vector)\n    assert dot(v6, N.x) == 2 * q1\n    assert dot(v6, N.y) == 2 * q2\n    assert dot(v6, N.z) == 0\n\n    # Test vector scalar division\n    v7 = v1 / 2\n    assert isinstance(v7, Vector)\n    assert dot(v7, N.x) == q1 / 2\n    assert dot(v7, N.y) == q2 / 2\n    assert dot(v7, N.z) == 0\n\n    # Test dot product\n    v8 = v1 & v2\n    assert v8 == -q1**2 * cos(q1) + q2**2 * cos(q1)\n\n    # Test cross product\n    v9 = v1 ^ v2\n    assert isinstance(v9", "def test_Vector_operations():\n    # Test vector addition and subtraction\n    v1 = x * A.x + y * A.y\n    v2 = z * A.x + x * A.y\n\n    v_add = v1 + v2\n    v_sub = v1 - v2\n\n    assert v_add == (x + z) * A.x + (y + x) * A.y\n    assert v_sub == (x - z) * A.x + (y - x) * A.y\n\n    # Test vector negation\n    v_neg = -v1\n    assert v_neg == -x * A.x - y * A.y\n\n    # Test scalar multiplication and division\n    v_mul = 2 * v1\n    v_div = v1 / 2\n\n    assert v_mul == 2 * x * A.x + 2 * y * A.y\n    assert v_div == (x / 2) * A.x + (y / 2) * A.y\n\n    # Test dot product\n    v3 = x * A.x + y * A.y + z * A.z\n    assert (v3 & A.x) == x\n    assert (v3 & A.y) == y\n    assert (v3 & A.z) == z\n\n    # Test cross product\n    assert (A.x ^ A.y) == A.z\n    assert (A.y ^ A.z) == A.x\n    assert (A.z ^ A.x) == A.y\n\n    # Test magnitude and normalization\n    v4 = 3 * A.x + 4 * A.y\n    assert v4.magnitude() == 5\n    assert v4.normalize() == (3/5) * A.x + (4/5) * A.y\n\n    # Test apply function\n    v5 = x * A.x + y * A.y + z * A.z\n    v5_applied = v5.applyfunc(lambda a: a**2)\n    assert v5_applied == (x**2) * A.x + (y**2) * A.y + (z**2) * A.z\n\n    # Test substitution\n    v6 = x * A.x + y * A.y + z * A.z\n    v6_subs = v6.subs({x: 1, y: 2, z: 3})\n    assert v6_subs", "def test_Vector_operations():\n    u1, u2 = dynamicsymbols('u1 u2')\n    v1 = u1 * A.x + u2 * A.y\n    v2 = 2 * A.x + 3 * A.y\n    \n    # Test scalar multiplication\n    v3 = v1 * 2\n    assert isinstance(v3, Vector)\n    assert v3 == 2*u1*A.x + 2*u2*A.y\n    \n    # Test scalar division\n    v4 = v2 / 2\n    assert isinstance(v4, Vector)\n    assert v4 == A.x + 1.5*A.y\n    \n    # Test negation\n    v5 = -v1\n    assert isinstance(v5, Vector)\n    assert v5 == -u1*A.x - u2*A.y\n    \n    # Test addition of zero vector\n    v_zero = Vector(0)\n    assert v1 + v_zero == v1\n    assert v1 - v_zero == v1\n    \n    # Test cross product\n    v6 = u1 * A.y + u2 * A.z\n    v7 = v1 ^ v6\n    assert isinstance(v7, Vector)\n    assert v7 == -u1*u2*A.x\n\n    # Test outer product\n    v8 = v1 | v2\n    assert isinstance(v8, Dyadic)\n    assert str(v8) == '2*u1*(A.x|A.x) + 3*u1*(A.x|A.y) + 2*u2*(A.y|A.x) + 3*u2*(A.y|A.y)'\n", "def test_Vector_operations():\n    # Test scalar multiplication\n    v = x * A.x + y * A.y + z * A.z\n    scalar = 3\n    v_scaled = v * scalar\n    assert dot(v_scaled, A.x) == 3 * x\n    assert dot(v_scaled, A.y) == 3 * y\n    assert dot(v_scaled, A.z) == 3 * z\n\n    # Test negation\n    v_neg = -v\n    assert dot(v_neg, A.x) == -x\n    assert dot(v_neg, A.y) == -y\n    assert dot(v_neg, A.z) == -z\n\n    # Test division\n    v_div = v / 2\n    assert dot(v_div, A.x) == x / 2\n    assert dot(v_div, A.y) == y / 2\n    assert dot(v_div, A.z) == z / 2\n\n    # Test outer product\n    v1 = x * A.x + y * A.y\n    v2 = y * A.x - x * A.y\n    d = v1 | v2\n    assert str(d) == \"(x**2 - y**2)*(A.x|A.x) + 2*x*y*(A.x|A.y)\"\n\n    # Test cross product\n    v_cross = v1 ^ v2\n    assert v_cross == (x**2 + y**2) * A.z\n\n    # Test magnitude\n    assert v1.magnitude() == sqrt(x**2 + y**2)\n\n    # Test normalization\n    v_normalized = v1.normalize()\n    assert v_normalized.magnitude() == 1\n\n    # Test applyfunc\n    v_applied = v.applyfunc(lambda a: a + 1)\n    assert dot(v_applied, A.x) == x + 1\n    assert dot(v_applied, A.y) == y + 1\n    assert dot(v_applied, A.z) == z + 1\n\n    # Test __eq__ and __ne__\n    v_same = x * A.x + y * A.y + z * A.z\n    assert v == v_same\n    assert not (v != v_same)\n", "def test_Vector_operations():\n    # Test negation\n    v1 = x*A.x + y*A.y + z*A.z\n    v_neg = -v1\n    assert dot(v_neg, A.x) == -x\n    assert dot(v_neg, A.y) == -y\n    assert dot(v_neg, A.z) == -z\n\n    # Test scalar multiplication\n    v2 = 2 * v1\n    assert dot(v2, A.x) == 2*x\n    assert dot(v2, A.y) == 2*y\n    assert dot(v2, A.z) == 2*z\n\n    # Test scalar division\n    v3 = v1 / 2\n    assert dot(v3, A.x) == x/2\n    assert dot(v3, A.y) == y/2\n    assert dot(v3, A.z) == z/2\n\n    # Test outer product\n    v4 = A.x | A.y\n    from sympy.physics.vector.dyadic import Dyadic\n    assert isinstance(v4, Dyadic)\n\n    # Test cross product\n    v5 = A.x ^ A.y\n    assert v5 == A.z\n\n    # Test magnitude and normalization\n    v6 = x*A.x + y*A.y + z*A.z\n    assert v6.magnitude() == sqrt(x**2 + y**2 + z**2)\n    v6_normalized = v6.normalize()\n    assert v6_normalized.magnitude() == 1\n\n    # Test applyfunc\n    v7 = v6.applyfunc(lambda t: t**2)\n    assert dot(v7, A.x) == x**2\n    assert dot(v7, A.y) == y**2\n    assert dot(v7, A.z) == z**2\n\n    # Test substitutions\n    s = symbols('s')\n    v8 = s*A.x\n    assert v8.subs(s, 2) == 2*A.x\n\n    # Test pretty printing and latex printing\n    assert v6._pretty() is not None\n    assert v6._latex() is not None\n"], "sample_1010": ["def test_latex_AccumulateBounds():\n    a, b, x = symbols('a b x')\n    assert latex(AccumBounds(a, b)) == r\"\\langle a, b\\rangle\"\n    assert latex(AccumBounds(a + 1, b + 1)) == r\"\\langle a + 1, b + 1\\rangle\"\n    assert latex(AccumBounds(sin(x), cos(x))) == r\"\\langle \\sin{\\left (x \\right )}, \\cos{\\left (x \\right )}\\rangle\"\n    assert latex(AccumBounds(0, oo)) == r\"\\langle 0, \\infty\\rangle\"\n    assert latex(AccumBounds(-oo, 1)) == r\"\\langle -\\infty, 1\\rangle\"\n", "def test_latex_long_frac_ratio():\n    assert latex((2*Integral(x, x)/3), long_frac_ratio=2) == r\"\\frac{2 x \\, dx}{3}\"\n    assert latex((2*Integral(x, x)/3), long_frac_ratio=0) == r\"\\frac{2}{3} \\int x\\, dx\"\n", "def test_latex_ProductSet_with_symbols():\n    from sympy.sets.sets import ProductSet\n    a, b, c = symbols('a b c')\n    ps1 = ProductSet(Interval(a, b), Interval(b, c))\n    ps2 = ProductSet(Interval(a, b), FiniteSet(c))\n    ps3 = ProductSet(FiniteSet(a), FiniteSet(b), FiniteSet(c))\n\n    assert latex(ps1) == r\"\\left[a, b\\right] \\times \\left[b, c\\right]\"\n    assert latex(ps2) == r\"\\left[a, b\\right] \\times \\left\\{c\\right\\}\"\n    assert latex(ps3) == r\"\\left\\{a\\right\\} \\times \\left\\{b\\right\\} \\times \\left\\{c\\right\\}\"\n", "def test_latex_modifiers_combination():\n    assert latex(symbols(\"xVecDot_tilde\")) == r\"\\tilde{\\dot{\\vec{x}}}\"\n    assert latex(symbols(\"yPrimeHat__bm\")) == r\"\\boldsymbol{\\hat{y}}'\"\n    assert latex(symbols(\"zbar\")) == r\"\\bar{z}\"\n    assert latex(symbols(\"aGrave\")) == r\"\\grave{a}\"\n    assert latex(symbols(\"bAcute\")) == r\"\\acute{b}\"\n    assert latex(symbols(\"cCheck\")) == r\"\\check{c}\"\n    assert latex(symbols(\"dBreve\")) == r\"\\breve{d}\"\n    assert latex(symbols(\"eMathring\")) == r\"\\mathring{e}\"\n    assert latex(symbols(\"xVecDotBar_tilde\")) == r\"\\tilde{\\bar{\\dot{\\vec{x}}}}\"\n    assert latex(symbols(\"yPrimeHat__bm_norm\")) == r\"\\left\\|{\\boldsymbol{\\hat{y}}'}\\right\\|\"\n", "def test_latex_precedence():\n    # Testing to ensure that parentheses are correctly placed for precedence\n    expr = x**(y + z)\n    assert latex(expr) == r\"x^{y + z}\"\n    \n    expr = x**y + z\n    assert latex(expr) == r\"x^{y} + z\"\n    \n    expr = x*(y + z)\n    assert latex(expr) == r\"x \\left(y + z\\right)\"\n    \n    expr = x/y + z\n    assert latex(expr) == r\"\\frac{x}{y} + z\"\n    \n    expr = x + y*z\n    assert latex(expr) == r\"x + y z\"\n    \n    expr = (x + y)*z\n    assert latex(expr) == r\"\\left(x + y\\right) z\"\n"], "sample_1011": ["def test_octave_code_with_assignment():\n    assert mcode(sin(x), assign_to=\"s\") == \"s = sin(x);\"\n    assert mcode(cos(x), assign_to=\"c\") == \"c = cos(x);\"\n    assert mcode(tan(x), assign_to=\"t\") == \"t = tan(x);\"\n    assert mcode(cot(x), assign_to=\"ct\") == \"ct = cot(x);\"\n    assert mcode(sec(x), assign_to=\"sc\") == \"sc = sec(x);\"\n    assert mcode(csc(x), assign_to=\"cs\") == \"cs = csc(x);\"\n    assert mcode(x + y, assign_to=\"sum\") == \"sum = x + y;\"\n    assert mcode(x * y, assign_to=\"prod\") == \"prod = x.*y;\"\n    assert mcode(x ** y, assign_to=\"pow\") == \"pow = x.^y;\"\n    assert mcode(x / y, assign_to=\"div\") == \"div = x./y;\"\n", "def test_operators():\n    assert mcode(x & y | z) == \"x & y | z\"\n    assert mcode(~(x | y)) == \"~(x | y)\"\n    assert mcode((x & y) | (z & x)) == \"x & y | z & x\"\n    assert mcode((x | y) & (z | x)) == \"(x | y) & (x | z)\"\n    assert mcode(2 * (x | y) + 3 * (z & x)) == \"2*(x | y) + 3*(x & z)\"\n", "def test_user_defined_functions():\n    from sympy import Function\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_f\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_g_mat\"),\n              (lambda x: not x.is_Matrix, \"custom_g\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n        'custom_f(x) + custom_g(x) + custom_g_mat([1 x])'\n", "def test_matrix_operations():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[2, 0], [1, 3]])\n    assert mcode(A + B) == \"[3 2; 4 7]\"\n    assert mcode(A - B) == \"[-1 2; 2 1]\"\n    assert mcode(A * B) == \"[4 6; 10 12]\"\n    assert mcode(A / 2) == \"[1/2 1; 3/2 2]\"\n    assert mcode(A ** 2) == \"[7 10; 15 22]\"\n", "def test_user_functions():\n    f = Function('f')\n    g = Function('g')\n    h = Function('h')\n\n    # Custom function mappings\n    user_functions = {\n        'f': 'my_octave_function',\n        'g': [(lambda x: x.is_Matrix, 'custom_matrix_function'),\n              (lambda x: not x.is_Matrix, 'custom_scalar_function')]\n    }\n\n    assert mcode(f(x), user_functions=user_functions) == 'my_octave_function(x)'\n    assert mcode(g(x), user_functions=user_functions) == 'custom_scalar_function(x)'\n    assert mcode(g(Matrix([[x, y]])), user_functions=user_functions) == 'custom_matrix_function([x y])'\n\n    # No mapping provided, should fallback to default function name\n    assert mcode(h(x), user_functions=user_functions) == 'h(x)'\n\n    # Edge case: empty user_functions dictionary\n    assert mcode(f(x), user_functions={}) == 'f(x)'\n    assert mcode(g(x), user_functions={}) == 'g(x)'\n"], "sample_1012": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x > 6))) == 'sympy.Piecewise((1, Eq(x, 0)), (2, Gt(x, 6)))'\n    assert p.doprint(Rational(1, 2)) == 'sympy.Rational(1, 2)'\n    assert p.doprint(pi) == 'sympy.pi'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert p.doprint(And(x > 0, y < 0)) == 'sympy.And(x > 0, y < 0)'\n    assert p.doprint(Or(x > 0, y < 0)) == 'sympy.Or(x > 0, y < 0)'\n    assert p.doprint(Piecewise((1, x < 0), (2, x > 0), (3, True))) == 'sympy.Piecewise((1, x < 0), (2, x > 0), (3, True))'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Rational(1, 2)) == 'sympy.Rational(1, 2)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    expr = pi + x\n    assert p.doprint(expr) == 'sympy.pi + x'\n    assert p.doprint(sign(x)) == '(0.0 if x == 0 else sympy.sign(x))'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(pi) == 'sympy.Pi'\n    assert p.doprint(Assignment(x, 2)) == 'x = 2'\n    assert p.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6))) == 'sympy.Piecewise((1, Eq(x, 0)), (2, x > 6))'\n    assert p.doprint(Piecewise((2, Le(x, 0)),\n                        (3, Gt(x, 0)), evaluate=False)) == 'sympy.Piecewise((2, Le(x, 0)), (3, Gt(x, 0)), evaluate=False)'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Mod(x, 2)) == 'x % 2'\n    assert p.doprint(And(x, y)) == 'x and y'\n    assert p.doprint(Or(x, y)) == 'x or y'\n    assert p.doprint(Assignment(x, 2)) == 'x = 2'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x>6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert p.doprint(Piecewise((2, Le(x, 0)), (3, Gt(x, 0)), evaluate=False)) == '((2) if (x <= 0) else (3) if (x > 0) else None)'\n    assert p.doprint(none) == 'None'\n"], "sample_1013": ["def test_implemented_function_multiple_calls():\n    # Test calling the implemented function multiple times to ensure consistency\n    f = implemented_function(\"f\", lambda x: 2 * x)\n    l = lambdify(x, f(x), modules=\"sympy\")\n    assert l(1) == 2\n    assert l(2) == 4\n    assert l(3) == 6\n\n    # Ensure the implemented function can be nested within another\n    g = implemented_function(\"g\", lambda x: x + 1)\n    l_nested = lambdify(x, f(g(x)), modules=\"sympy\")\n    assert l_nested(1) == 4\n    assert l_nested(2) == 6\n    assert l_nested(3) == 8\n\n    # Ensure lambdified function can handle lists of inputs\n    l_list = lambdify([x, y], [f(x), g(y)], modules=\"sympy\")\n    assert l_list(2, 3) == [4, 4]\n    assert l_list(4, 5) == [8, 6]\n", "def test_lambdastr():\n    # Test lambdastr function to ensure it generates correct lambda string\n    from sympy.abc import a, b, c\n\n    # Test single argument\n    assert lambdastr(a, a**2) == 'lambda a: (a**2)'\n\n    # Test multiple arguments\n    assert lambdastr((a, b), a + b) == 'lambda a,b: (a + b)'\n\n    # Test nested arguments\n    assert lambdastr((a, (b, c)), a + b + c) == 'lambda _0,_1: (lambda a,b,c: (a + b + c))(_0,_1[0],_1[1])'\n\n    # Test complex expression\n    assert lambdastr((a, b), a*b + sin(a) - cos(b)) == 'lambda a,b: (a*b + sin(a) - cos(b))'\n", "def test_lambdastr():\n    expr = x**2 + y**2\n    lstr = lambdastr((x, y), expr)\n    assert lstr == 'lambda x,y: ((x**2) + (y**2))'\n    \n    expr = sin(x) * cos(y)\n    lstr = lambdastr((x, y), expr)\n    assert lstr == 'lambda x,y: (sin(x) * cos(y))'\n    \n    expr = x + y + z\n    lstr = lambdastr((x, (y, z)), expr)\n    assert lstr == 'lambda _0,_1: (lambda x,y,z: (x + y + z))(_0,_1[0],_1[1])'\n    \n    expr = Matrix([[x, y], [z, 1+z]])\n    lstr = lambdastr((x, y, z), expr)\n    assert lstr == 'lambda x,y,z: (Matrix([[x, y], [z, (1 + z)]]))'\n", "def test_custom_namespace():\n    # Custom namespace with a mix of math and custom function\n    custom_namespace = {\"custom_sin\": lambda x: 2 * math.sin(x)}\n    f = lambdify(x, sin(x) + custom_namespace[\"custom_sin\"](x), {\"sin\": math.sin, **custom_namespace})\n    assert f(math.pi/2) == math.sin(math.pi/2) + 2 * math.sin(math.pi/2)\n", "def test_lambdastr_basic():\n    from sympy.abc import a, b, c\n    assert lambdastr(a, a**2) == \"lambda a: (a**2)\"\n    assert lambdastr((a, b), a + b) == \"lambda a,b: (a + b)\"\n    assert lambdastr((a, (b, c)), a + b + c) == \"lambda _0,_1: (lambda a,b,c: (a + b + c))(_0,_1[0],_1[1])\"\n"], "sample_1014": ["def test_setitem():\n    mutable_array = MutableDenseNDimArray([0, 1, 2, 3], (2, 2))\n    assert mutable_array[0, 0] == 0\n    assert mutable_array[1, 1] == 3\n\n    mutable_array[0, 0] = 5\n    assert mutable_array[0, 0] == 5\n\n    mutable_array[1, 1] = 7\n    assert mutable_array[1, 1] == 7\n\n    mutable_array[0, 1] = Symbol('x')\n    assert mutable_array[0, 1] == Symbol('x')\n\n    raises(TypeError, lambda: ImmutableDenseNDimArray([0, 1, 2, 3], (2, 2))[0, 0] = 5)\n    raises(TypeError, lambda: ImmutableDenseNDimArray([0, 1, 2, 3], (2, 2))._array.__setitem__(0, 5))\n", "def test_as_mutable_and_as_immutable():\n    imm_array = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    mut_array = imm_array.as_mutable()\n    assert isinstance(mut_array, MutableDenseNDimArray)\n    assert mut_array.tolist() == imm_array.tolist()\n\n    mut_array[0, 0] = 10\n    assert mut_array[0, 0] == 10\n    assert imm_array[0, 0] == 1  # immutability check\n\n    new_imm_array = mut_array.as_immutable()\n    assert isinstance(new_imm_array, ImmutableDenseNDimArray)\n    assert new_imm_array.tolist() == mut_array.tolist()\n\n    raises(TypeError, lambda: new_imm_array.__setitem__((0, 0), 20))\n", "def test_immutability():\n    # Verify that ImmutableDenseNDimArray cannot be modified\n    a = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    raises(TypeError, lambda: a.__setitem__((0, 0), 10))\n\n    # Verify that as_mutable returns a MutableDenseNDimArray\n    b = a.as_mutable()\n    assert isinstance(b, MutableDenseNDimArray)\n\n    # Check that we can modify the returned MutableDenseNDimArray\n    b[0, 0] = 10\n    assert b[0, 0] == 10\n\n    # Ensure that original ImmutableDenseNDimArray has not been modified\n    assert a[0, 0] == 1\n", "def test_as_mutable():\n    dense_array = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    mutable_array = dense_array.as_mutable()\n    assert isinstance(mutable_array, MutableDenseNDimArray)\n    mutable_array[0, 0] = 10\n    assert mutable_array[0, 0] == 10\n    assert dense_array[0, 0] == 1  # Ensure immutability of the original array\n\n    sparse_array = ImmutableSparseNDimArray([1, 2, 3, 4], (2, 2))\n    mutable_sparse_array = sparse_array.as_mutable()\n    assert isinstance(mutable_sparse_array, MutableDenseNDimArray)\n    mutable_sparse_array[0, 0] = 10\n    assert mutable_sparse_array[0, 0] == 10\n    assert sparse_array[0, 0] == 1  # Ensure immutability of the original array\n", "def test_zeros():\n    zero_array_1d = ImmutableDenseNDimArray.zeros(5)\n    assert zero_array_1d.tolist() == [0, 0, 0, 0, 0]\n    assert zero_array_1d.shape == (5,)\n    assert zero_array_1d.rank() == 1\n\n    zero_array_2d = ImmutableDenseNDimArray.zeros(3, 4)\n    assert zero_array_2d.tolist() == [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n    assert zero_array_2d.shape == (3, 4)\n    assert zero_array_2d.rank() == 2\n\n    zero_array_3d = ImmutableDenseNDimArray.zeros(2, 2, 2)\n    assert zero_array_3d.tolist() == [[[0, 0], [0, 0]], [[0, 0], [0, 0]]]\n    assert zero_array_3d.shape == (2, 2, 2)\n    assert zero_array_3d.rank() == 3\n\n    zero_array_immutable = zero_array_3d.as_immutable()\n    assert isinstance(zero_array_immutable, ImmutableDenseNDimArray)\n    assert zero_array_immutable.tolist() == zero_array_3d.tolist()\n\n    zero_array_mutable = zero_array_3d.as_mutable()\n    assert isinstance(zero_array_mutable, MutableDenseNDimArray)\n    assert zero_array_mutable.tolist() == zero_array_3d.tolist()\n"], "sample_1015": ["def test_ccode_print_NumberSymbol():\n    assert ccode(S.Pi) == \"M_PI\"\n    assert ccode(S.Exp1) == \"M_E\"\n    assert ccode(S(1)/S.Pi) == \"M_1_PI\"\n    assert ccode(2/sqrt(S.Pi)) == \"M_2_SQRTPI\"\n    assert ccode(sqrt(2)) == \"M_SQRT2\"\n    assert ccode(1/sqrt(2)) == \"M_SQRT1_2\"\n", "def test_ccode_NaN():\n    assert ccode(nan, standard='c99') == 'NAN'\n    assert ccode(float('nan'), standard='c99') == 'NAN'\n    assert ccode(nan, standard='c89') == 'NAN'\n    assert ccode(float('nan'), standard='c89') == 'NAN'\n", "def test_ccode_Mul():\n    assert ccode(2*x*y) == \"2*x*y\"\n    assert ccode(-2*x*y) == \"-2*x*y\"\n    assert ccode(x*y*z) == \"x*y*z\"\n    assert ccode(-x*y*z) == \"-x*y*z\"\n    assert ccode((x + y)*z) == \"(x + y)*z\"\n    assert ccode((x - y)*z) == \"(x - y)*z\"\n    assert ccode((x + y)*(z - 1)) == \"(x + y)*(z - 1)\"\n    assert ccode(2*(x + y)) == \"2*(x + y)\"\n    assert ccode(2*(x - y)) == \"2*(x - y)\"\n    assert ccode(-2*(x + y)) == \"-2*(x + y)\"\n    assert ccode(-2*(x - y)) == \"-2*(x - y)\"\n", "def test_ccode_Print():\n    printf = Print('Hello %s\\\\n', [x])\n    assert ccode(printf) == 'printf(\"Hello %s\\\\n\", x);'\n    \n    # Test with multiple arguments\n    printf = Print('Sum of %d and %d is %d\\\\n', [x, y, x + y])\n    assert ccode(printf) == 'printf(\"Sum of %d and %d is %d\\\\n\", x, y, x + y);'\n    \n    # Test with float arguments\n    printf = Print('Value of x: %f\\\\n', [x])\n    assert ccode(printf) == 'printf(\"Value of x: %f\\\\n\", x);'\n", "def test_ccode_NaN():\n    assert ccode(nan, standard='C99') == 'NAN'\n    assert ccode(float('nan'), standard='C99') == 'NAN'\n    assert ccode(nan, standard='C89') == 'HUGE_VAL'\n    assert ccode(float('nan'), standard='C89') == 'HUGE_VAL'\n"], "sample_1016": ["def test_Assignment():\n    from sympy.codegen.ast import Assignment\n    assert mcode(Assignment(x, y)) == \"x = y;\"\n    assert mcode(Assignment(x, sin(y))) == \"x = sin(y);\"\n    assert mcode(Assignment(MatrixSymbol('A', 2, 2), Matrix([[1, 2], [3, 4]]))) == \"A = [1 2; 3 4];\"\n    assert mcode(Assignment(MatrixSymbol('A', 1, 1), Matrix([[pi]]))) == \"A = pi;\"\n", "def test_Assignment():\n    a = Symbol('a')\n    expr = Assignment(a, x + y)\n    assert mcode(expr) == \"a = x + y;\"\n    expr = Assignment(a, Piecewise((x, x < 1), (x**2, True)))\n    assert mcode(expr) == \"a = ((x < 1).*(x) + (~(x < 1)).*(x.^2));\"\n    expr = Assignment(a, Piecewise((x, x < 1), (x**2, x < 2), (x**3, True)))\n    assert mcode(expr, inline=False) == (\n        \"if (x < 1)\\n\"\n        \"  a = x;\\n\"\n        \"elseif (x < 2)\\n\"\n        \"  a = x.^2;\\n\"\n        \"else\\n\"\n        \"  a = x.^3;\\n\"\n        \"end\")\n", "def test_user_functions():\n    custom_functions = {\n        \"myfunc\": \"custom_octave_func\",\n        \"otherfunc\": [(lambda x: x.is_Matrix, \"custom_matrix_func\"),\n                      (lambda x: not x.is_Matrix, \"custom_scalar_func\")]\n    }\n    myfunc = Function('myfunc')\n    otherfunc = Function('otherfunc')\n    mat = Matrix([[x, y], [z, 1]])\n    \n    assert mcode(myfunc(x), user_functions=custom_functions) == \"custom_octave_func(x)\"\n    assert mcode(otherfunc(x), user_functions=custom_functions) == \"custom_scalar_func(x)\"\n    assert mcode(otherfunc(mat), user_functions=custom_functions) == \"custom_matrix_func([x y; z 1])\"\n", "def test_user_defined_functions():\n    from sympy import Function\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_f\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_mat_g\"),\n              (lambda x: not x.is_Matrix, \"custom_g\")]\n    }\n    assert mcode(f(x), user_functions=custom_functions) == \"custom_f(x)\"\n    assert mcode(g(x), user_functions=custom_functions) == \"custom_g(x)\"\n    mat = Matrix([[1, x]])\n    assert mcode(g(mat), user_functions=custom_functions) == \"custom_mat_g([1 x])\"\n", "def test_Indexing_and_Slicing():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    v = MatrixSymbol('v', 3, 1)\n    h = MatrixSymbol('h', 1, 3)\n\n    assert mcode(A[1, 1]) == \"A(2, 2)\"\n    assert mcode(A[0:2, 0:2]) == \"A(1:2, 1:2)\"\n    assert mcode(A[:, 1]) == \"A(:, 2)\"\n    assert mcode(A[1, :]) == \"A(2, :)\"\n    assert mcode(A[1:3, 1]) == \"A(2:3, 2)\"\n    assert mcode(A[1, 0:3]) == \"A(2, 1:3)\"\n    assert mcode(A[0:2, 1:3]) == \"A(1:2, 2:3)\"\n    assert mcode(A[0, 0]**2 + A[0, 1] + A[0, 2]) == \"A(1, 1).^2 + A(1, 2) + A(1, 3)\"\n\n    expr = A[0:2, 0:2] + B[1:3, 1:3]\n    assert mcode(expr) == \"A(1:2, 1:2) + B(2:3, 2:3)\"\n\n    expr = h[0, 0] * v[0, 0] + h[0, 1] * v[1, 0] + h[0, 2] * v[2, 0]\n    assert mcode(expr) == \"h(1, 1)*v(1, 1) + h(1, 2)*v(2, 1) + h(1, 3)*v(3, 1)\"\n"], "sample_1017": ["def test_simplify_logic():\n    # Test simplify_logic function with various forms\n    assert simplify_logic(And(Or(A, B), Or(A, C)), 'cnf') == And(Or(A, B), Or(A, C))\n    assert simplify_logic(And(Or(A, B), Or(A, C)), 'dnf') == Or(And(A, A), And(A, C, B))\n\n    assert simplify_logic(Or(And(A, B), And(A, C)), 'cnf') == And(Or(A, A), Or(A, B, C))\n    assert simplify_logic(Or(And(A, B), And(A, C)), 'dnf') == Or(And(A, B), And(A, C))\n\n    assert simplify_logic(Implies(A, Or(B, C)), 'cnf') == Or(Not(A), Or(B, C))\n    assert simplify_logic(Implies(A, Or(B, C)), 'dnf') == Or(Not(A), Or(B, C))\n\n    assert simplify_logic(Equivalent(A, And(B, C)), 'cnf') == And(Or(Not(A), B), Or(Not(A), C), Or(Not(B), Not(C), A))\n    assert simplify_logic(Equivalent(A, And(B, C)), 'dnf') == Or(And(Not(A), Not(B), Not(C)), And(A, B, C))\n\n    assert simplify_logic(Not(A & B), 'cnf') == Or(Not(A), Not(B))\n    assert simplify_logic(Not(A & B), 'dnf') == Or(Not(A), Not(B))\n\n    assert simplify_logic(Not(A | B), 'cnf') == And(Not(A), Not(B))\n    assert simplify_logic(Not(A | B), 'dnf') == And(Not(A), Not(B))\n\n    # Test simplify_logic with more complex boolean expressions\n    expr = And(Or(A, B), Or(Not(A), C), Or(B, Not(C)))\n    assert simplify_logic(expr, 'cnf') == And(Or(A, B), Or(Not(A), C), Or(B, Not(C)))\n    assert simplify_logic(expr, 'dnf') == Or(And(A, B, Not(C)), And(A, B, C), And(Not(A), B, Not(C)), And(Not(A), B, C))\n    \n    expr = Or(And(A, B, C), And(Not(A), Not(B), Not(C)))\n    assert simplify", "def test_BooleanAtom_noop():\n    \"\"\"Test that BooleanAtom operations raise the correct TypeError\"\"\"\n\n    for atom in [true, false]:\n        for op in ['+', '-', '*', '/', '**', '%']:\n            raises(TypeError, lambda: eval(f'atom {op} 1'))\n            raises(TypeError, lambda: eval(f'1 {op} atom'))\n        raises(TypeError, lambda: atom ** 1)\n        raises(TypeError, lambda: 1 ** atom)\n        raises(TypeError, lambda: atom < 1)\n        raises(TypeError, lambda: atom <= 1)\n        raises(TypeError, lambda: atom > 1)\n        raises(TypeError, lambda: atom >= 1)\n        raises(TypeError, lambda: 1 < atom)\n        raises(TypeError, lambda: 1 <= atom)\n        raises(TypeError, lambda: 1 > atom)\n        raises(TypeError, lambda: 1 >= atom)\n", "def test_boolean_atoms():\n    assert isinstance(true, BooleanTrue)\n    assert isinstance(false, BooleanFalse)\n    assert BooleanTrue() is true\n    assert BooleanFalse() is false\n    assert true.canonical is true\n    assert false.canonical is false\n    assert true.simplify() is true\n    assert false.simplify() is false\n    assert true.expand() is true\n    assert false.expand() is false\n    assert true.as_set() == S.UniversalSet\n    assert false.as_set() == S.EmptySet\n    assert hash(true) == hash(BooleanTrue())\n    assert hash(false) == hash(BooleanFalse())\n", "def test_to_cnf_dnf_not_implemented():\n    # Test NotImplementedError for to_cnf and to_dnf with multivariate expressions\n    x, y = symbols('x y', real=True)\n    raises(NotImplementedError, lambda: (x > 1).to_cnf())\n    raises(NotImplementedError, lambda: (x > 1).to_dnf())\n    raises(NotImplementedError, lambda: And(x > 1, y > 2).to_cnf())\n    raises(NotImplementedError, lambda: And(x > 1, y > 2).to_dnf())\n", "def test_distribute_or_over_and_with_multiple_terms():\n    expr = And(Or(A, B), Or(C, D), Or(E, F))\n    expected = Or(And(A, C, E), And(A, C, F), And(A, D, E), And(A, D, F),\n                  And(B, C, E), And(B, C, F), And(B, D, E), And(B, D, F))\n    assert distribute_or_over_and(expr) == expected\n"], "sample_1018": ["def test_fcode_While():\n    x = symbols('x')\n    body = [Assignment(x, x + 1)]\n    condition = x < 10\n    f = While(condition, body)\n    sol = fcode(f)\n    assert sol == (\"do while (x < 10)\\n\"\n                   \"   x = x + 1\\n\"\n                   \"end do\")\n", "def test_fcode_functions_floats():\n    x, y = symbols('x,y')\n    assert fcode(sin(Float('1.5'))) == \"      sin(1.50000000000000d0)\"\n    assert fcode(cos(Float('2.5'))) == \"      cos(2.50000000000000d0)\"\n    assert fcode(atan2(Float('1.5'), y)) == \"      atan2(1.50000000000000d0, y)\"\n    assert fcode(log(Float('2.5'))) == \"      log(2.50000000000000d0)\"\n    assert fcode(exp(Float('1.5'))) == \"      exp(1.50000000000000d0)\"\n    assert fcode(gamma(Float('1.5')), user_functions={\"gamma\": \"mygamma\"}) == \"      mygamma(1.50000000000000d0)\"\n    assert fcode(conjugate(Float('2.5'))) == \"      conjg(2.50000000000000d0)\"\n    assert fcode(sqrt(Float('1.5'))) == '      sqrt(1.50000000000000d0)'\n    assert fcode(factorial(Integer(5))) == \"      fct(5)\"\n", "def test_fcode_AugmentedAssignment():\n    x = symbols('x')\n    y = symbols('y')\n    # Test augmented assignment for addition\n    assert fcode(aug_assign(x, '+', y), source_format='free') == 'x = x + y'\n    # Test augmented assignment for subtraction\n    assert fcode(aug_assign(x, '-', y), source_format='free') == 'x = x - y'\n    # Test augmented assignment for multiplication\n    assert fcode(aug_assign(x, '*', y), source_format='free') == 'x = x*y'\n    # Test augmented assignment for division\n    assert fcode(aug_assign(x, '/', y), source_format='free') == 'x = x/y'\n    # Test augmented assignment for power\n    assert fcode(aug_assign(x, '**', y), source_format='free') == 'x = x**y'\n", "def test_fcode_functions_with_parameters():\n    x, y = symbols('x y')\n    f = Function('f')(x, y)\n    assert fcode(f, user_functions={'f': 'custom_function'}) == \"      custom_function(x, y)\"\n    g = Function('g')(x)\n    assert fcode(g, user_functions={'g': 'another_function'}) == \"      another_function(x)\"\n    h = Function('h')(x, y)\n    assert fcode(h, user_functions={'h': 'complex_function'}) == \"      complex_function(x, y)\"\n", "def test_fcode_MatrixElement():\n    A = MatrixSymbol('A', 3, 3)\n    assert fcode(A[1, 2]) == \"      A(2, 3)\"\n    assert fcode(A[0, 0] + A[1, 1]) == \"      A(1, 1) + A(2, 2)\"\n    assert fcode(A[2, 2] * 2) == \"      2*A(3, 3)\"\n    assert fcode(A[1, 0] - A[0, 1]) == \"      A(2, 1) - A(1, 2)\"\n"], "sample_1019": ["def test__monotonic_sign():\n    nn = Dummy(integer=True, nonnegative=True)\n    p = Dummy(integer=True, positive=True)\n    p2 = Dummy(integer=True, positive=True)\n    \n    assert _monotonic_sign(nn + 1) == 1\n    assert _monotonic_sign(p - 1) == Dummy('nneg', nonnegative=True)\n    assert _monotonic_sign(nn*p + 1) == 1\n    assert _monotonic_sign(p2*p + 1) == 2\n    assert _monotonic_sign(nn - 1) is None\n    assert _monotonic_sign(-nn) == Dummy('npos', nonpositive=True)\n    assert _monotonic_sign(p**2 + 1) == 1\n    assert _monotonic_sign(p**3 + p**2) == Dummy('pos', positive=True)\n    assert _monotonic_sign(p**3 - p**2) == Dummy('npos', nonpositive=True)\n    assert _monotonic_sign(nn**3 + nn) == Dummy('nneg', nonnegative=True)\n", "def test__monotonic_sign():\n    nn = Dummy(integer=True, nonnegative=True)\n    p = Dummy(integer=True, positive=True)\n    p2 = Dummy(integer=True, positive=True)\n\n    assert _monotonic_sign(nn + 1) == S.One\n    assert _monotonic_sign(p - 1) == Dummy('nneg', nonnegative=True)\n    assert _monotonic_sign(nn*p + 1) == S.One\n    assert _monotonic_sign(p2*p + 1) == S(2)\n    assert _monotonic_sign(nn - 1) is None\n", "def test__monotonic_sign():\n    nn = Dummy(integer=True, nonnegative=True)\n    p = Dummy(integer=True, positive=True)\n    p2 = Dummy(integer=True, positive=True)\n    assert _monotonic_sign(nn + 1) == 1\n    assert _monotonic_sign(p - 1) == Dummy('nneg', nonnegative=True)\n    assert _monotonic_sign(nn*p + 1) == 1\n    assert _monotonic_sign(p2*p + 1) == 2\n    assert _monotonic_sign(nn - 1) is None\n    assert _monotonic_sign(nn**2 + nn + 1) == 1\n    assert _monotonic_sign(p**3 - p**2 - p - 1) is None\n    assert _monotonic_sign(p**3 + p**2 + p + 1) == Dummy('pos', positive=True)\n    assert _monotonic_sign(p2**3 + p2**2 - p2 - 1) == Dummy('npos', nonpositive=True)\n    assert _monotonic_sign(nn**3 - nn**2 + nn - 1) is None\n    assert _monotonic_sign(1/(nn + 1)) == Dummy('pos', positive=True)\n    assert _monotonic_sign(nn/(nn + 1)) == Dummy('nneg', nonnegative=True)\n    assert _monotonic_sign(nn/(nn + 1) + 1) == Dummy('pos', positive=True)\n    assert _monotonic_sign((nn - 1)/(nn + 1)) == Dummy('nneg', nonnegative=True)\n    assert _monotonic_sign(1/(p + 1)) == Dummy('pos', positive=True)\n    assert _monotonic_sign(p/(p + 1)) == Dummy('nneg', nonnegative=True)\n    assert _monotonic_sign((p - 1)/(p + 1)) == Dummy('npos', nonpositive=True)\n", "def test__monotonic_sign():\n    nn = Dummy(integer=True, nonnegative=True)\n    p = Dummy(integer=True, positive=True)\n    p2 = Dummy(integer=True, positive=True)\n\n    assert _monotonic_sign(nn + 1) == 1\n    assert _monotonic_sign(p - 1) == Dummy('nneg', nonnegative=True)\n    assert _monotonic_sign(nn * p + 1) == 1\n    assert _monotonic_sign(p2 * p + 1) == 2\n    assert _monotonic_sign(nn - 1) is None  # could be negative, zero, or positive\n    assert _monotonic_sign(p**2 - p) == 0  # non-negative polynomial\n    assert _monotonic_sign(nn**2 + p) == _eps  # polynomial with variable\n    assert _monotonic_sign(nn + p**2) == _eps  # non-negative polynomial\n    assert _monotonic_sign(-nn**2) == 0  # negative polynomial\n    assert _monotonic_sign(-p**2) == Dummy('npos', nonpositive=True)\n", "def test__monotonic_sign():\n    nn = Dummy(integer=True, nonnegative=True)\n    p = Dummy(integer=True, positive=True)\n    p2 = Dummy(integer=True, positive=True)\n    assert _monotonic_sign(nn + 1) == 1\n    assert _monotonic_sign(p - 1) == Dummy('nneg', nonnegative=True)\n    assert _monotonic_sign(nn*p + 1) == 1\n    assert _monotonic_sign(p2*p + 1) == 2\n    assert _monotonic_sign(nn - 1) is None\n"], "sample_1020": ["def test_Sum():\n    assert mcode(Sum(x, (x, 1, 10))) == \"Hold[Sum[x, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2, (x, 0, 5))) == \"Hold[Sum[x^2, {x, 0, 5}]]\"\n    assert mcode(Sum(x*y, (x, 1, 3), (y, 1, 2))) == \"Hold[Sum[x*y, {x, 1, 3}, {y, 1, 2}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(sin(x), (x, 0, 2*pi))) == \"Hold[Sum[Sin[x], {x, 0, 2*Pi}]]\"\n    assert mcode(Sum(x**2 + y**2, (x, 1, 5), (y, 1, 5))) == \"Hold[Sum[x^2 + y^2, {x, 1, 5}, {y, 1, 5}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(sin(x), (x, 0, 2*pi))) == \"Hold[Sum[Sin[x], {x, 0, 2*Pi}]]\"\n    assert mcode(Sum(x*y, (x, 0, 5), (y, 1, 10))) == \"Hold[Sum[x*y, {x, 0, 5}, {y, 1, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x, (x, 1, 10))) == \"Hold[Sum[x, {x, 1, 10}]]\"\n    assert mcode(Sum(x*y, (x, 1, 10), (y, 1, 5))) == \"Hold[Sum[x*y, {x, 1, 10}, {y, 1, 5}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x, (x, 1, 10))) == \"Hold[Sum[x, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2, (x, 1, 5))) == \"Hold[Sum[x^2, {x, 1, 5}]]\"\n    assert mcode(Sum(x*y, (x, 1, 5), (y, 1, 5))) == \"Hold[Sum[x*y, {x, 1, 5}, {y, 1, 5}]]\"\n    assert mcode(Sum(f(x), (x, 0, oo))) == \"Hold[Sum[f[x], {x, 0, Infinity}]]\"\n"], "sample_1021": ["def test_quaternion_subtraction():\n    q = Quaternion(x, y, z, w)\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1 - q2 == Quaternion(-4, -4, -4, -4)\n    assert q - q1 == Quaternion(x - 1, y - 2, z - 3, w - 4)\n    assert q1 - 5 == Quaternion(-4, -3, -2, -1)\n    assert q1 - (2 + 3*I) == Quaternion(1 - 2 - 3*I, 2, 3, 4)\n", "def test_quaternion_addition_subtraction():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    q3 = Quaternion(0, 0, 0, 0)\n    q4 = Quaternion(-1, -2, -3, -4)\n\n    assert q1 + q2 == Quaternion(6, 8, 10, 12)\n    assert q1 - q2 == Quaternion(-4, -4, -4, -4)\n    assert q1 + 5 == Quaternion(6, 2, 3, 4)\n    assert q1 - 5 == Quaternion(-4, 2, 3, 4)\n    assert q1 + q3 == q1\n    assert q1 - q3 == q1\n    assert q1 + q4 == q3\n    assert q1 - q4 == Quaternion(2, 4, 6, 8)\n\n    q5 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    assert q5 + Quaternion(1, 1, 1, 1) == Quaternion(4 + 4*I, 3 + 5*I, 1, 8 + 8*I)\n    assert q5 - Quaternion(1, 1, 1, 1) == Quaternion(2 + 4*I, 1 + 5*I, -1, 6 + 8*I)\n", "def test_quaternion_exceptions():\n    # Ensure non-commutative symbols raise an exception\n    c = symbols(\"c\", commutative=False)\n    raises(ValueError, lambda: Quaternion(c, 0, 0, 0))\n\n    # Ensure invalid rotation matrix raises an exception (non-orthogonal matrix)\n    M_invalid = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    raises(ValueError, lambda: Quaternion.from_rotation_matrix(M_invalid))\n\n    # Ensure quaternion with zero norm raises an exception for inverse operation\n    q_zero = Quaternion(0, 0, 0, 0)\n    raises(ValueError, lambda: q_zero.inverse())\n", "def test_quaternion_subtraction():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1 - q2 == Quaternion(-4, -4, -4, -4)\n    assert q2 - q1 == Quaternion(4, 4, 4, 4)\n    assert q1 - 1 == Quaternion(0, 2, 3, 4)\n    assert q2 - 3 == Quaternion(2, 6, 7, 8)\n    assert q1 - I == Quaternion(1, 1, 3, 4)\n    assert q2 - I == Quaternion(5, 5, 7, 8)\n    \n    q3 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    assert q3 - (2 + 3*I) == Quaternion(1 + I, 2 + 5*I, 0, 7 + 8*I)\n    assert q3 - 3 == Quaternion(0 + 4*I, 2 + 5*I, 0, 7 + 8*I)\n", "def test_quaternion_exceptions():\n    # Test ValueError for non-commutative arguments\n    non_commutative_sym = symbols(\"non_commutative_sym\", commutative=False)\n    raises(ValueError, lambda: Quaternion(non_commutative_sym, 0, 0, 0))\n\n    # Test ValueError for zero norm quaternion inverse\n    q_zero_norm = Quaternion(0, 0, 0, 0)\n    raises(ValueError, lambda: q_zero_norm.inverse())\n\n    # Test __copysign with edge case of y being zero\n    assert Quaternion.__copysign(5, 0) == 0\n    assert Quaternion.__copysign(-5, 0) == 0\n"], "sample_1022": ["def test_factorial_notation():\n    cases = {\n        '5!': 'factorial(5)',\n        'x!': 'factorial(x)',\n        'y!!': 'factorial2(y)',\n        '10!!': 'factorial2(10)',\n        '(2 + 3)!': 'factorial(2 + 3)',\n        '(x + 1)!': 'factorial(x + 1)',\n        '(x + 2)!!': 'factorial2(x + 2)'\n    }\n    transformations = standard_transformations + (convert_xor,)\n    for case, expected in cases.items():\n        result = parse_expr(case, transformations=transformations)\n        expected_result = parse_expr(expected, transformations=transformations)\n        assert(result == expected_result), f\"{result} != {expected_result}\"\n\n    # Ensure error is raised for invalid factorial notation\n    invalid_cases = ['x!!!', '5!!!']\n    for case in invalid_cases:\n        raises(TokenError, lambda: parse_expr(case, transformations=transformations))\n", "def test_factorial_notation():\n    cases = {\n        '5!': 'factorial(5)',\n        '5!!': 'factorial2(5)',\n        'x!': 'factorial(x)',\n        'x!!': 'factorial2(x)',\n        '5!!!': TokenError,  # Should raise TokenError for more than 2 factorials\n    }\n    transformations = standard_transformations + (convert_xor,)\n    for case, expected in cases.items():\n        if expected == TokenError:\n            raises(TokenError, lambda: parse_expr(case, transformations=transformations))\n        else:\n            implicit = parse_expr(case, transformations=transformations)\n            normal = parse_expr(expected, transformations=transformations)\n            assert(implicit == normal), (implicit, normal)\n\n    # Test with local_dict and global_dict\n    local_dict = {'x': sympy.Symbol('x')}\n    cases_with_dicts = {\n        'x!': 'factorial(x)',\n        'x!!': 'factorial2(x)',\n    }\n    for case, expected in cases_with_dicts.items():\n        implicit = parse_expr(case, local_dict=local_dict, transformations=transformations)\n        normal = parse_expr(expected, local_dict=local_dict, transformations=transformations)\n        assert(implicit == normal), (implicit, normal)\n", "def test_lambda_notation():\n    cases = {\n        'lambda x: x + 1': 'Lambda(x, x + 1)',\n        'lambda x, y: x + y': 'Lambda((x, y), x + y)',\n        'lambda x: x**2': 'Lambda(x, x**2)'\n    }\n    transformations = standard_transformations\n    for case in cases:\n        parsed_expr = parse_expr(case, transformations=transformations)\n        expected_expr = parse_expr(cases[case], transformations=transformations)\n        assert(parsed_expr == expected_expr), (parsed_expr, expected_expr)\n\n    # Test invalid lambda usage\n    invalid_lambda = ['lambda', 'lambda x']\n    for case in invalid_lambda:\n        raises(SyntaxError, lambda: parse_expr(case, transformations=transformations))\n", "def test_custom_split_symbols():\n        # Custom rule: Only split symbols that start with 'a' and are not 'alpha'\n        if symbol.startswith('a') and symbol != 'alpha':\n            return _token_splittable(symbol)\n        return False\n\n    transformations = standard_transformations + (split_symbols_custom(can_split), implicit_multiplication)\n    cases = {\n        'abc': 'a*b*c',  # Should split\n        'alpha': 'alpha',  # Should not split\n        'apple': 'a*p*p*l*e',  # Should split\n        'beta': 'b*e*t*a',  # Should split (since it doesn't start with 'a')\n    }\n\n    for case, expected in cases.items():\n        assert parse_expr(case, transformations=transformations) == parse_expr(expected)\n\n    # Test with implicit multiplication and application\n    transformations2 = transformations + (implicit_application,)\n    assert parse_expr('a b c', transformations=transformations2) == parse_expr('a*b*c')\n    assert parse_expr('apple pie', transformations=transformations2) == parse_expr('a*p*p*l*e*p*i*e')\n", "def test_repeated_decimals():\n    transformations = standard_transformations + (repeated_decimals, auto_number)\n    cases = {\n        '0.1[3]': 'Rational(1, 9)',\n        '0.[142857]': 'Rational(1, 7)',\n        '3.0[6]': '3 + Rational(2, 3)',\n        '.1[6]': 'Rational(1, 6)',\n        '1.[9]': '2'\n    }\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) ==\n               parse_expr(expected))\n"], "sample_1023": ["def test_totientrange():\n    from sympy.ntheory.generate import sieve\n    sieve._reset()\n    assert list(sieve.totientrange(10, 20)) == [4, 10, 4, 12, 6, 8, 8, 16, 6, 18]\n    assert list(sieve.totientrange(30, 40)) == [8, 16, 6, 20, 12, 18, 12, 32, 18, 24]\n    assert list(sieve.totientrange(50, 60)) == [20, 16, 40, 18, 30, 16, 24, 40, 36, 22]\n    assert list(sieve.totientrange(5, 5)) == []\n    assert list(sieve.totientrange(1, 10)) == [1, 1, 2, 2, 4, 2, 6, 4, 6]\n", "def test_sieve_extend():\n    from sympy.ntheory.generate import sieve\n    sieve._reset()\n    sieve.extend(50)\n    assert sieve._list[-1] == 47\n    assert len(sieve._list) == 15\n    sieve.extend(100)\n    assert sieve._list[-1] == 97\n    assert len(sieve._list) == 25\n    sieve.extend(150)\n    assert sieve._list[-1] == 149\n    assert len(sieve._list) == 35\n    sieve.extend(200)\n    assert sieve._list[-1] == 199\n    assert len(sieve._list) == 46\n    sieve.extend(300)\n    assert sieve._list[-1] == 293\n    assert len(sieve._list) == 62\n", "def test_prime_sieve():\n    from sympy.ntheory.generate import sieve\n    sieve._reset()\n\n    assert sieve[1] == 2\n    assert sieve[5] == 11\n    assert sieve[10] == 29\n\n    sieve.extend(100)\n    assert 97 in sieve\n    assert 100 not in sieve\n    assert sieve[25] == 97\n\n    sieve.extend_to_no(30)\n    assert len(sieve._list) >= 30\n    assert sieve[30] == 113\n\n    assert list(sieve.primerange(10, 20)) == [11, 13, 17, 19]\n\n    assert list(sieve.totientrange(10, 20)) == [4, 6, 4, 12, 6, 8, 8, 16, 8, 18]\n    assert list(sieve.mobiusrange(10, 20)) == [1, -1, 0, -1, 1, 1, 0, -1, -1, 0]\n\n    assert 25 not in sieve\n    sieve.extend(25)\n    assert 25 not in sieve\n    assert 23 in sieve\n    assert 29 in sieve\n\n    assert sieve.__contains__(23)\n    assert not sieve.__contains__(24)\n    assert sieve.__contains__(29)\n\n    assert sieve.search(20) == (8, 9)\n    assert sieve.search(23) == (9, 9)\n    assert sieve.search(25) == (9, 10)\n", "def test_sieve_extend():\n    from sympy.ntheory.generate import sieve\n    sieve._reset()\n    assert sieve._list.tolist() == [2, 3, 5, 7, 11, 13]\n    sieve.extend(50)\n    assert sieve._list.tolist() == [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n    sieve.extend(100)\n    assert sieve._list.tolist() == [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n    sieve.extend(200)\n    assert sieve._list.tolist() == [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199]\n    raises(ValueError, lambda: sieve.extend(-10))\n", "def test_sieve():\n    sieve._reset()\n    assert repr(sieve) == \"<prime sieve (6): 2, 3, 5, ... 11, 13\\n\" \\\n                          \"totient sieve (6): 0, 1, 1, ... 2, 4\\n\" \\\n                          \"mobius sieve (6): 0, 1, -1, ... 0, -1>\"\n    sieve.extend(30)\n    assert sieve._list == _array('l', [2, 3, 5, 7, 11, 13, 17, 19, 23, 29])\n    assert 25 not in sieve\n    assert 29 in sieve\n    assert sieve.search(29) == (10, 10)\n    assert sieve.search(30) == (10, 11)\n    assert sieve[10] == 29\n    assert sieve[1:5] == _array('l', [2, 3, 5, 7])\n    sieve.extend_to_no(15)\n    assert sieve._list[14] == 47\n    sieve._reset(prime=True)\n    assert sieve._list == _array('l', [2, 3, 5, 7, 11, 13])\n    sieve._reset(totient=True)\n    assert sieve._tlist == _array('l', [0, 1, 1, 2, 2, 4])\n    sieve._reset(mobius=True)\n    assert sieve._mlist == _array('l', [0, 1, -1, -1, 0, -1])\n"], "sample_1024": ["def test_issue_12345():\n    # Testing corner cases for igcd and ilcm functions\n\n    # Testing igcd for large numbers\n    large_num = 10**100\n    assert igcd(large_num, large_num + 1) == 1\n    assert igcd(large_num, large_num) == large_num\n\n    # Testing ilcm for large numbers\n    assert ilcm(large_num, large_num + 1) == large_num * (large_num + 1)\n    assert ilcm(large_num, large_num) == large_num\n\n    # Testing igcd with negative large numbers\n    assert igcd(-large_num, large_num + 1) == 1\n    assert igcd(-large_num, -large_num) == large_num\n\n    # Testing ilcm with negative large numbers\n    assert ilcm(-large_num, large_num + 1) == large_num * (large_num + 1)\n    assert ilcm(-large_num, -large_num) == large_num\n", "def test_mpf_norm_inf_nan():\n    assert mpf_norm((_mpf_inf), 10) == _mpf_inf\n    assert mpf_norm((_mpf_ninf), 10) == _mpf_ninf\n    assert mpf_norm((_mpf_nan), 10) == _mpf_nan\n", "def test_igcd2_large_numbers():\n    # Test igcd2 with very large numbers\n    a = 2**1000 * 3**500\n    b = 2**999 * 3**501\n    assert igcd2(a, b) == 2**999 * 3**500\n", "def test_as_integer_ratio():\n    assert Rational(3, 4).as_integer_ratio() == (3, 4)\n    assert Rational(-3, 4).as_integer_ratio() == (-3, 4)\n    assert Rational(3, -4).as_integer_ratio() == (-3, 4)\n    assert Rational(-3, -4).as_integer_ratio() == (3, 4)\n    assert Rational(5).as_integer_ratio() == (5, 1)\n    assert Integer(5).as_integer_ratio() == (5, 1)\n    assert Float(0.125).as_integer_ratio() == (1, 8)\n    assert Float(-0.125).as_integer_ratio() == (-1, 8)\n    assert Float(3.25).as_integer_ratio() == (13, 4)\n    assert Float(-3.25).as_integer_ratio() == (-13, 4)\n    assert Float(0.1).as_integer_ratio() == (3602879701896397, 36028797018963968)\n    assert Float(-0.1).as_integer_ratio() == (-3602879701896397, 36028797018963968)\n    assert Float(1.1).as_integer_ratio() == (2476979795053773, 2251799813685248)\n    assert Float(-1.1).as_integer_ratio() == (-2476979795053773, 2251799813685248)\n", "def test_mpf_norm():\n    # Check that mpf_norm correctly handles special cases\n    assert mpf_norm((1, 0, 0, 0), 10) == (0, 0, 0, 0)  # zero\n    assert mpf_norm((0, 1, 0, 1), 10) == (0, 1, 0, 1)  # one\n    assert mpf_norm((1, 1, 0, 1), 10) == (1, 1, 0, 1)  # -one\n    assert mpf_norm((1, 0, 1, 0), 10) == (0, 0, 0, 0)  # zero with sign\n    assert mpf_norm((0, 0, 10, 2), 10) == (0, 0, 10, 2)  # small positive\n    assert mpf_norm((1, 0, 10, 2), 10) == (1, 0, 10, 2)  # small negative\n\n    # Check that mpf_norm handles infinity and nan correctly\n    assert mpf_norm((0, 0, 1, -1), 10) == _mpf_inf  # positive infinity\n    assert mpf_norm((1, 0, 1, -1), 10) == _mpf_ninf  # negative infinity\n    assert mpf_norm((0, 0, 1, -2), 10) == _mpf_nan  # nan\n\n    # Test with Float numbers\n    assert Float._new((1, 0, 1, 0), 10)._mpf_ == mpf('0')._mpf_\n    assert Float._new((0, 1, 0, 1), 10)._mpf_ == mpf('1')._mpf_\n    assert Float._new((1, 1, 0, 1), 10)._mpf_ == mpf('-1')._mpf_\n"], "sample_1025": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.doprint(And(x, y)) == 'sympy.And(x, y)'\n    assert p.doprint(Or(x, y)) == 'sympy.Or(x, y)'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6))) == 'sympy.Piecewise((1, Eq(x, 0)), (2, x > 6))'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n", "def test_SymPyPrinter():\n    from sympy import sin, cos\n    p = SymPyPrinter()\n    assert p.doprint(sin(x) + cos(x)) == 'sympy.sin(x) + sympy.cos(x)'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.doprint(Rational(1, 3)) == 'sympy.Rational(1, 3)'\n", "def test_NaN_Infinity():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(zoo) == \"float('nan')\"\n    assert prntr.doprint(oo) == \"float('inf')\"\n    assert prntr.doprint(-oo) == \"float('-inf')\"\n    assert prntr.doprint(Mod(x, 2)) == 'x % 2'\n", "def test_SymPyPrinter():\n    from sympy import sin, cos, Function\n    from sympy.printing.pycode import SymPyPrinter\n\n    class CustomFunction(Function):\n        pass\n\n    p = SymPyPrinter()\n\n    # Test standard sympy functions\n    assert p.doprint(sin(x)) == 'sympy.sin(x)'\n    assert p.doprint(cos(x)) == 'sympy.cos(x)'\n\n    # Test custom function\n    custom_func = CustomFunction(x)\n    assert p.doprint(custom_func) == 'sympy.CustomFunction(x)'\n\n    # Ensure module imports are correct\n    assert p.module_imports == {'sympy': {'sin', 'cos', 'CustomFunction'}}\n"], "sample_1026": ["def test_lambdify_with_custom_function():\n    # Custom function with special implementation\n        return a * b + 1\n\n    f_sym = Function('f')\n    f_impl = implemented_function(f_sym, custom_func)\n    \n    # Test lambdify with the custom function\n    f = lambdify((x, y), f_impl(x, y), modules='sympy')\n    assert f(2, 3) == 7  # 2 * 3 + 1 = 7\n    assert f(4, 5) == 21  # 4 * 5 + 1 = 21\n\n    # Test lambdify with a combination of custom function and regular operations\n    expr = f_impl(x, y) + x - y\n    g = lambdify((x, y), expr, modules='sympy')\n    assert g(2, 3) == 6  # (2 * 3 + 1) + 2 - 3 = 6\n    assert g(4, 5) == 20  # (4 * 5 + 1) + 4 - 5 = 20\n", "def test_custom_function_in_namespace():\n    # Custom function to be used in the namespace\n        return a * b + b\n\n    # Use the custom function in lambdify\n    expr = custom_func(x, y)\n    f = lambdify((x, y), expr, modules={\"custom_func\": custom_func})\n\n    # Test the custom function\n    assert f(2, 3) == custom_func(2, 3)\n    assert f(0, 5) == custom_func(0, 5)\n    assert f(-1, -1) == custom_func(-1, -1)\n\n    # Test with different inputs\n    assert f(4.5, 1.5) == custom_func(4.5, 1.5)\n    assert f(-2.5, 2.5) == custom_func(-2.5, 2.5)\n", "def test_issue_15847():\n    # test lambdify with multiple functions with different implementations\n    f1 = implemented_function(\"f\", lambda x: x + 1)\n    f2 = implemented_function(\"f\", lambda x: x - 1)\n    \n    l1 = lambdify(x, f1(x))\n    l2 = lambdify(x, f2(x))\n    \n    assert l1(5) == 6\n    assert l2(5) == 4\n\n    # Ensure that each implementation is unique and doesn't interfere with the other\n    assert l1(0) == 1\n    assert l2(0) == -1\n", "def test_issue_15000():\n    # Test to ensure proper handling of functions with no arguments\n    f = lambdify([], sin(pi/2), modules=\"sympy\")\n    assert f() == 1\n\n    g = lambdify([], exp(0), modules=\"math\")\n    assert g() == 1.0\n\n    h = lambdify([], tan(pi/4), modules=\"numpy\")\n    assert h() == 1.0\n", "def test_lambdify_with_dictionaries():\n    custom_functions = {\n        'custom_sin': lambda x: math.sin(x) + 1,\n        'custom_cos': lambda x: math.cos(x) - 1\n    }\n    expr = symbols('custom_sin')(x) + symbols('custom_cos')(x)\n    f = lambdify(x, expr, modules=[custom_functions])\n    assert f(0) == 0  # sin(0) + 1 + cos(0) - 1 = 1 + 0 = 0\n    assert f(math.pi/2) == 1  # sin(pi/2) + 1 + cos(pi/2) - 1 = 2 + 0 - 1 = 1\n"], "sample_1027": ["def test_Poly_unify_with_modulus():\n    f = Poly(x, x, modulus=3)\n    g = Poly(y, y, modulus=3)\n    h = Poly(y, y, modulus=5)\n\n    assert f._unify(g)[2:] == (DMP([FF(3)(1), FF(3)(0)], FF(3)), DMP([FF(3)(1)], FF(3)))\n    assert f._unify(h)[2:] == (DMP([FF(5)(1), FF(5)(0)], FF(5)), DMP([FF(5)(1)], FF(5)))\n\n    i = Poly(x + 1, x, modulus=3)\n    j = Poly(x + 2, x, modulus=3)\n    k = Poly(x + 2, x, modulus=5)\n\n    assert i._unify(j)[2:] == (DMP([FF(3)(1), FF(3)(1)], FF(3)), DMP([FF(3)(1), FF(3)(2)], FF(3)))\n    assert i._unify(k)[2:] == (DMP([FF(5)(1), FF(5)(1)], FF(5)), DMP([FF(5)(1), FF(5)(2)], FF(5)))\n\n    l = Poly(x + 1, x, y, modulus=3)\n    m = Poly(x + 2, x, y, modulus=3)\n    n = Poly(x + 2, x, y, modulus=5)\n\n    assert l._unify(m)[2:] == (DMP([[FF(3)(1)], [FF(3)(1)]], FF(3)), DMP([[FF(3)(1)], [FF(3)(2)]], FF(3)))\n    assert l._unify(n)[2:] == (DMP([[FF(5)(1)], [FF(5)(1)]], FF(5)), DMP([[FF(5)(1)], [FF(5)(2)]], FF(5)))\n", "def test_Poly___call__():\n    f = Poly(2*x*y + 3*x + y + 2*z)\n    \n    assert f(2) == Poly(5*y + 2*z + 6)\n    assert f(2, 5) == Poly(2*z + 31)\n    assert f(2, 5, 7) == 45\n    assert f(x, y) == Poly(2*x*y + 3*x + y + 2*z, x, y, z)\n    assert f(x, 2) == Poly(2*x*2 + 3*x + 2 + 2*z, x, z)\n    assert f(2, y, z) == Poly(4*y + 2*z + 6, y, z)\n", "def test_Poly_per():\n    # Basic tests for the Poly.per method\n    f = Poly(x**2 + 1, x)\n    rep = DMP.from_list([1, 0, 1], 1, ZZ)\n\n    assert f.per(rep) == Poly(x**2 + 1, x)\n\n    # Test with different generators\n    f = Poly(x**2 + y, x, y)\n    rep = DMP.from_list([[1], [1, 0]], 1, ZZ)\n\n    assert f.per(rep) == Poly(x**2 + y, x, y)\n\n    # Test with removal of generator\n    f = Poly(x**2 + 1, x, y)\n    rep = DMP.from_list([1, 0, 1], 1, ZZ)\n\n    assert f.per(rep, remove=1) == Poly(x**2 + 1, x)\n\n    # Test with composite domain\n    f = Poly(x**2 + y, x, y, domain=ZZ[y])\n    rep = DMP.from_list([[1], [1, 0]], 1, ZZ[y])\n\n    assert f.per(rep) == Poly(x**2 + y, x, domain=ZZ[y])\n\n    # Test that it raises an error with invalid input\n    raises(ValueError, lambda: f.per(rep, remove=2))\n    raises(ValueError, lambda: f.per(rep, gens=[x, z]))\n", "def test_Poly_eval_subs():\n    f = Poly(x**3 - 3*x**2 + 2*x + 1, x)\n    assert f.eval(x, 2) == 3\n    assert f.eval(x, 0) == 1\n    assert f.eval(x, -1) == -3\n\n    g = Poly(x**2*y + x*y**2 + x*y + 1, x, y)\n    assert g.eval({x: 1, y: 2}) == 11\n    assert g.eval({x: -1, y: 2}) == 3\n    assert g.eval({x: 1, y: -2}) == -1\n\n    h = Poly(x**2 + y, x, y)\n    assert h.eval({y: 3}) == Poly(x**2 + 3, x)\n    assert h.eval({x: 2}) == Poly(y + 4, y)\n    assert h.eval({x: 1, y: 2}) == 3\n    assert h.eval({x: 2}) == Poly(y + 4, y)\n    assert h.eval({y: 0}) == Poly(x**2, x)\n    assert h.eval({x: 0}) == Poly(y, y)\n", "def test_Poly_has_only_gens_univariate():\n    assert Poly(x**2 + 2*x + 1, x).has_only_gens(x) is True\n    assert Poly(x**2 + 2*x + 1, x).has_only_gens(y) is False\n    assert Poly(x**2 + 2*x + 1, x).has_only_gens(x, y) is False\n    assert Poly(x + 1, x, domain='ZZ[y]').has_only_gens(x) is True\n    assert Poly(x + y, x, domain='ZZ[y]').has_only_gens(x) is False\n    assert Poly(x + y, x, domain='ZZ[y]').has_only_gens(y) is False\n    raises(GeneratorsError, lambda: Poly(x**2 + 2*x + 1, x).has_only_gens(y, z))\n"], "sample_1028": ["def test_Mod_eval():\n    # Test cases for the eval method of the Mod class to ensure proper evaluation\n    assert Mod.eval(5, 2) == 1\n    assert Mod.eval(5, 3) == 2\n    assert Mod.eval(5, 5) == 0\n    assert Mod.eval(5, 1) == 0\n    assert Mod.eval(-5, 3) == 1\n    assert Mod.eval(5, -3) == -1\n    assert Mod.eval(-5, -3) == -2\n    assert Mod.eval(0, 3) == 0\n    assert Mod.eval(5, 0) == None  # Should raise ZeroDivisionError, but eval returns None\n    assert Mod.eval(oo, 3) == nan\n    assert Mod.eval(5, oo) == nan\n    assert Mod.eval(-oo, 3) == nan\n    assert Mod.eval(5, -oo) == nan\n    assert Mod.eval(nan, 3) == nan\n    assert Mod.eval(5, nan) == nan\n    assert Mod.eval(nan, nan) == nan\n    assert Mod.eval(4.5, 2.1).evalf() == 0.3  # Float modulo\n    assert Mod.eval(x + 1, x).subs(x, 2) == 1  # Symbolic modulo\n", "def test_Mod_zero_division():\n    assert Mod(0, 1) == 0\n    assert Mod(0, -1) == 0\n    with raises(ZeroDivisionError):\n        Mod(1, 0)\n    with raises(ZeroDivisionError):\n        Mod(-1, 0)\n", "def test_Mod_eval():\n    assert Mod(0, 5) == 0\n    assert Mod(5, 0) == nan\n    assert Mod(5, 2) == 1\n    assert Mod(-5, 2) == 1\n    assert Mod(5, -2) == -1\n    assert Mod(-5, -2) == -2\n    assert Mod(5*x, 2*x) == 1*x\n    assert Mod(-5*x, 2*x) == 1*x\n    assert Mod(5*x, -2*x) == -1*x\n    assert Mod(-5*x, -2*x) == -2*x\n    assert Mod(5, 2*y) == 1\n    assert Mod(-5, 2*y) == 1\n    assert Mod(5, -2*y) == -1\n    assert Mod(-5, -2*y) == -2\n", "def test_mod_zero_division():\n    # Test to ensure ZeroDivisionError is raised\n    raises(ZeroDivisionError, lambda: Mod(5, 0))\n    raises(ZeroDivisionError, lambda: Mod(0, 0))\n    raises(ZeroDivisionError, lambda: Mod(x, 0))\n    raises(ZeroDivisionError, lambda: Mod(5, S.Zero))\n    raises(ZeroDivisionError, lambda: Mod(x, S.Zero))\n", "def test_Mod_with_floats():\n    # Test cases for Mod function with floating point numbers\n    assert Mod(3.5, 1.2) == 1.1\n    assert Mod(-3.5, 1.2) == 0.9\n    assert Mod(3.5, -1.2) == -0.9\n    assert Mod(-3.5, -1.2) == -1.1\n    assert Mod(5.5, 2.3) == 0.9\n    assert Mod(-5.5, 2.3) == 1.4\n    assert Mod(5.5, -2.3) == -1.4\n    assert Mod(-5.5, -2.3) == -0.9\n"], "sample_1029": ["def test_AtomicExpr():\n    sT(S.ImaginaryUnit, \"I\")\n    sT(S.Exp1, \"E\")\n    sT(S.Pi, \"pi\")\n    sT(S.GoldenRatio, \"GoldenRatio\")\n", "def test_AtomicExpr():\n    from sympy.core.expr import AtomicExpr\n    class MyAtomicExpr(AtomicExpr):\n            self.name = name\n            return self.name\n    expr = MyAtomicExpr(\"a\")\n    sT(expr, \"a\")\n", "def test_NumberSymbol():\n    from sympy import pi, E, oo, zoo, nan, Catalan, EulerGamma, GoldenRatio, TribonacciConstant\n    sT(pi, 'pi')\n    sT(E, 'E')\n    sT(oo, 'oo')\n    sT(zoo, 'zoo')\n    sT(nan, 'nan')\n    sT(Catalan, 'Catalan')\n    sT(EulerGamma, 'EulerGamma')\n    sT(GoldenRatio, 'GoldenRatio')\n    sT(TribonacciConstant, 'TribonacciConstant')\n", "def test_NaN():\n    sT(S.NaN, \"nan\")\n    sT(Float('nan'), \"Float('nan', precision=53)\")\n", "def test_MonogenicFiniteExtension():\n    from sympy.polys.domains import QQ\n    from sympy.polys.polytools import Poly\n    ext = MonogenicFiniteExtension(Poly(x**3 + x + 1, x, domain=QQ))\n    sT(ext, \"FiniteExtension(Poly(x**3 + x + 1, x, domain='QQ'))\")\n"], "sample_1030": ["def test_are_coplanar():\n    from sympy.geometry import Point3D, Line3D, Plane\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    d = Plane(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 0))\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    assert are_coplanar(a, b, c) == False\n    assert are_coplanar(d, p1, p2) == False\n    assert are_coplanar(a, d) == True\n    assert are_coplanar(p1, p1.translate(1, 1, 1), p1.translate(2, 2, 2)) == False\n", "def test_closest_points():\n    points = [Point2D(0, 0), Point2D(3, 4), Point2D(6, 8), Point2D(3, 0)]\n    assert closest_points(*points) == {(Point2D(0, 0), Point2D(3, 0))}\n    points = [Point2D(1, 2), Point2D(4, 6), Point2D(-1, -1), Point2D(0, 0)]\n    assert closest_points(*points) == {(Point2D(-1, -1), Point2D(0, 0))}\n    raises(ValueError, lambda: closest_points(Point2D(1, 2)))\n    raises(ValueError, lambda: closest_points(Point2D(1, 2), \"not a point\"))\n", "def test_are_coplanar():\n    from sympy.geometry import Point3D, Line3D, Plane\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n    p3 = Point3D(2, 2, 2)\n    p4 = Point3D(1, 0, 0)\n    line1 = Line3D(p1, p2)\n    line2 = Line3D(p2, p3)\n    plane1 = Plane(p1, p2, p3)\n    plane2 = Plane(p1, p2, p4)\n    assert are_coplanar(p1, p2, p3) is True\n    assert are_coplanar(line1, line2) is True\n    assert are_coplanar(line1, plane1) is True\n    assert are_coplanar(p1, p2, p4) is False\n    assert are_coplanar(p1, p2, p3, line1) is True\n    assert are_coplanar(p1, p2, p4, line1) is False\n    assert are_coplanar(plane1, plane2) is True\n", "def test_find():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    eq = x + y**2 + z\n    assert find('x', eq) == x\n    assert find(y, eq) == y\n    raises(ValueError, lambda: find('a', eq))\n    raises(ValueError, lambda: find('y', x + y + y))\n", "def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n    # Test with 3 collinear points\n    a, b, c = Point3D(1, 1, 1), Point3D(2, 2, 2), Point3D(3, 3, 3)\n    assert not are_coplanar(a, b, c)\n    # Test with 4 points, 3 of which are collinear\n    d = Point3D(1, 2, 3)\n    assert not are_coplanar(a, b, c, d)\n    # Test with points forming a plane\n    e = Point3D(0, 0, 0)\n    assert are_coplanar(a, b, c, e)\n    # Test with lines that are coplanar\n    f = Line3D(a, b)\n    g = Line3D(c, d)\n    assert not are_coplanar(f, g)\n    # Test with a plane\n    plane = Plane(a, b, e)\n    assert are_coplanar(plane, a, b, e)\n    assert not are_coplanar(plane, a, b, d)\n    # Test with mixed entities\n    assert are_coplanar(plane, f, g, e)\n"], "sample_1031": ["def test_quantity_creation():\n    # Test creation of various quantities and their attributes\n    meter = Quantity(\"meter\")\n    meter.set_dimension(length)\n    meter.set_scale_factor(One)\n    assert meter.name == \"meter\"\n    assert meter.dimension == length\n    assert meter.scale_factor == One\n\n    kilogram = Quantity(\"kilogram\")\n    kilogram.set_dimension(mass)\n    kilogram.set_scale_factor(One)\n    assert kilogram.name == \"kilogram\"\n    assert kilogram.dimension == mass\n    assert kilogram.scale_factor == One\n\n    second = Quantity(\"second\")\n    second.set_dimension(time)\n    second.set_scale_factor(One)\n    assert second.name == \"second\"\n    assert second.dimension == time\n    assert second.scale_factor == One\n", "def test_quantity_definitions():\n    units = [\n        (percent, One, Rational(1, 100)),\n        (permille, One, Rational(1, 1000)),\n        (radian, One, One),\n        (degree, One, pi / 180),\n        (steradian, One, One),\n        (angular_mil, One, 2 * pi / 6400),\n        (meter, length, One),\n        (kilogram, mass, One),\n        (second, time, One),\n        (ampere, current, One),\n        (kelvin, temperature, One),\n        (mole, amount_of_substance, One),\n        (candela, luminous_intensity, One),\n        (gram, mass, kilogram / kilo),\n        (milligram, mass, milli * gram),\n        (microgram, mass, micro * gram),\n        (newton, force, kilogram * meter / second**2),\n        (joule, energy, newton * meter),\n        (watt, power, joule / second),\n        (pascal, pressure, newton / meter**2),\n        (hertz, frequency, One),\n    ]\n\n    for unit, dim, scale in units:\n        assert unit.dimension == dim\n        assert unit.scale_factor == scale\n", "def test_quantity_properties():\n    q = Quantity(\"test_quantity\")\n    q.set_dimension(length)\n    q.set_scale_factor(10 * m)\n\n    assert q.name == \"test_quantity\"\n    assert q.dimension == length\n    assert q.scale_factor == 10 * m\n    assert str(q) == \"test_quantity\"\n    assert repr(q) == \"Quantity(test_quantity)\"\n", "def test_quantity_scale_factor():\n    # Testing the scale factors of defined quantities\n    assert percent.scale_factor == Rational(1, 100)\n    assert permille.scale_factor == Rational(1, 1000)\n    assert radian.scale_factor == One\n    assert degree.scale_factor == pi/180\n    assert steradian.scale_factor == One\n    assert angular_mil.scale_factor == 2*pi/6400\n    assert meter.scale_factor == One\n    assert kilogram.scale_factor == One\n    assert second.scale_factor == One\n    assert ampere.scale_factor == One\n    assert kelvin.scale_factor == One\n    assert mole.scale_factor == One\n    assert candela.scale_factor == One\n    assert gram.scale_factor == kilogram/kilo\n    assert milligram.scale_factor == milli*gram\n    assert microgram.scale_factor == micro*gram\n    assert newton.scale_factor == kilogram*meter/second**2\n    assert joule.scale_factor == newton*meter\n    assert watt.scale_factor == joule/second\n    assert pascal.scale_factor == newton/meter**2\n    assert hertz.scale_factor == One\n    assert volt.scale_factor == joule/coulomb\n    assert ohm.scale_factor == volt/ampere\n    assert siemens.scale_factor == ampere/volt\n    assert farad.scale_factor == coulomb/volt\n    assert henry.scale_factor == volt*second/ampere\n    assert tesla.scale_factor == volt*second/meter**2\n    assert weber.scale_factor == joule/ampere\n    assert dioptre.scale_factor == 1/meter\n    assert lux.scale_factor == steradian*candela/meter**2\n    assert katal.scale_factor == mol/second\n    assert gray.scale_factor == meter**2/second**2\n    assert becquerel.scale_factor == 1/second\n    assert liter.scale_factor == meter**3 / 1000\n    assert deciliter.scale_factor == liter / 10\n    assert centiliter.scale_factor == liter / 100\n    assert milliliter.scale_factor == liter / 1000\n    assert millisecond.scale_factor == milli*second\n    assert microsecond.scale_factor == micro*second\n    assert nanosecond.scale_factor == nano*second\n    assert picosecond.scale_factor == pico*second\n    assert minute.scale_factor == 60*second\n    assert hour", "def test_quantity_set_scale_factor():\n    # Test if setting scale factor works correctly\n    test_quantity = Quantity(\"test_quantity\")\n    test_quantity.set_dimension(length)\n    test_quantity.set_scale_factor(100 * m)\n\n    assert test_quantity.scale_factor == 100 * m\n    assert test_quantity.dimension == length\n\n    # Test resetting the scale factor\n    test_quantity.set_scale_factor(200 * m)\n    assert test_quantity.scale_factor == 200 * m\n"], "sample_1032": ["def test_identity_function():\n    from sympy import Id, Symbol\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert Id(x) == x\n    assert Id(y) == y\n    assert Id(x + y) == x + y\n    assert Id(2 * x) == 2 * x\n    assert Id(x**2) == x**2\n    assert Id(x).subs(x, y) == y\n\n    # Check that Id is indeed a singleton\n    assert Id is S.IdentityFunction\n", "def test_cbrt():\n    from sympy.abc import x\n    assert cbrt(8) == 2\n    assert cbrt(-8) == -2\n    assert cbrt(27) == 3\n    assert cbrt(-27) == -3\n    assert cbrt(x**3) == x\n    assert cbrt(-x**3) == -x\n    assert cbrt(8.0) == 2.0\n    assert cbrt(-8.0) == -2.0\n    assert cbrt(x).diff(x) == x**(-Rational(2, 3))/3\n    assert cbrt(x**6).rewrite(Pow) == x**2\n    raises(ValueError, lambda: cbrt(-x).subs(x, 0))\n", "def test_sqrt_cbrt():\n    from sympy.abc import x, y\n    from sympy import Symbol, sqrt, cbrt, Rational\n\n    # Testing sqrt function\n    assert sqrt(4) == 2\n    assert sqrt(2)**2 == 2\n    assert sqrt(x**2) == sqrt(x**2)\n    assert sqrt(x**2).subs(x, 2) == 2\n    assert sqrt(y**2).subs(y, 3) == 3\n    assert sqrt(-1) == sqrt(-1)  # complex result\n\n    # Testing cbrt function\n    assert cbrt(27) == 3\n    assert cbrt(8) == 2\n    assert cbrt(64) == 4\n    assert cbrt(x**3) == cbrt(x**3)\n    assert cbrt(x**3).subs(x, 2) == 2\n    assert cbrt(-8) == cbrt(-8)\n    assert cbrt(-8).evalf() == -2\n\n    # Edge cases\n    assert sqrt(0) == 0\n    assert cbrt(0) == 0\n    assert sqrt(oo) == oo\n    assert cbrt(oo) == oo\n    assert sqrt(-oo) == sqrt(-oo)\n    assert cbrt(-oo) == cbrt(-oo)\n    \n    # Simplifications\n    x = Symbol('x', positive=True)\n    assert sqrt(x**2) == x\n    assert cbrt(x**3) == x\n\n    x = Symbol('x', negative=True)\n    assert sqrt(x**2) == -x\n    assert cbrt(x**3) == x\n", "def test_issue_13005():\n    from sympy.abc import a, b, c, d\n    assert Max(a, Min(a, b, c)) == a\n    assert Max(a, Min(a, b)) == a\n    assert Min(a, Max(a, b, c)) == a\n    assert Min(a, Max(a, b)) == a\n", "def test_identity_function():\n    from sympy import Id, Symbol\n    x = Symbol('x')\n    y = Symbol('y', positive=True)\n    assert Id(x) == x\n    assert Id(y) == y\n    assert Id(x + y) == x + y\n    assert Id(x*y) == x*y\n    assert Id(x**2) == x**2\n    assert Id(x**2).subs(x, 3) == 9\n    assert Id(x + 1).subs(x, 2) == 3\n    assert Id(x).subs(x, 0) == 0\n    assert Id(x*y).subs({x: 2, y: 3}) == 6\n    assert Id(2) == 2\n    assert Id(0) == 0\n    assert Id(-3) == -3\n\n    # Test with lambdify\n    f = lambdify(x, Id(x))\n    assert f(3) == 3\n    assert f(0) == 0\n    assert f(-5) == -5\n"], "sample_1033": ["def test_unevaluated_Add():\n    from sympy.abc import a, b\n    from sympy import Add, Integer\n\n    # Test that Numbers are combined and sorted properly\n    res = _unevaluated_Add(*[Integer(1), a, Integer(2)])\n    assert res.args == (3, a)\n\n    # Test commutativity with unevaluated Add\n    res = _unevaluated_Add(a + 1, a + 2)\n    assert res == a + a + 3\n\n    # Test when nested Adds are present\n    res = _unevaluated_Add(a + (b + 1), a + 1)\n    assert res == a + a + b + 2\n\n    # Test when a Number and symbol are combined\n    res = _unevaluated_Add(2*a, 3*a, Integer(5))\n    assert res == 5*a + 5\n\n    # Test preservation of order for non-commutative terms\n    from sympy import symbols\n    A, B = symbols('A B', commutative=False)\n    res = _unevaluated_Add(A, B, A)\n    assert res.args == (A, A, B)\n", "def test_unevaluated_Add():\n    from sympy.abc import a, b, c\n    from sympy.core.add import _unevaluated_Add as uAdd\n    assert uAdd(a + b, c) == a + b + c\n    assert uAdd(a, uAdd(b, c)) == a + b + c\n    assert uAdd(a, b, c, 2) == a + b + c + 2\n    assert uAdd(3, 1) == 4\n    assert uAdd(a, 0, b) == a + b\n    assert uAdd(a + b, c + 2) == a + b + c + 2\n    assert isinstance(uAdd(a, b, 2), Add)\n    assert uAdd(1.0, 2) == 3.0\n    assert uAdd(a + 1, a + 2) == a + a + 3\n", "def test_as_coeff_add():\n    expr1 = 7 + 3*x\n    expr2 = 7*x\n    expr3 = 2*x + 3 + x\n\n    assert expr1.as_coeff_add() == (7, (3*x,))\n    assert expr2.as_coeff_add() == (0, (7*x,))\n    assert expr3.as_coeff_add() == (3, (2*x + x,))\n", "def test_as_coefficients_dict():\n    x, y = symbols('x y')\n    assert (3*x + 4*y).as_coefficients_dict() == {x: 3, y: 4}\n    assert (5*x**2 + 7*x*y).as_coefficients_dict() == {x**2: 5, x*y: 7}\n    assert (3.5*x + 2.5*y).as_coefficients_dict() == {x: 3.5, y: 2.5}\n    assert (x + x).as_coefficients_dict() == {x: 2}\n    assert (3*x + 0*y).as_coefficients_dict() == {x: 3, y: 0}\n    assert (0*x + 0*y).as_coefficients_dict() == {x: 0, y: 0}\n    assert (2*x + 3*y + 4).as_coefficients_dict() == {1: 4, x: 2, y: 3}\n    assert (x + y + 1).as_coefficients_dict() == {1: 1, x: 1, y: 1}\n    assert (x + y + 1).as_coefficients_dict()[z] == 0  # undefined symbol z\n", "def test_unevaluated_Add():\n    from sympy.abc import a, b, x, y\n    from sympy import Add, S, Rational\n\n    # Test the _unevaluated_Add function directly\n    a = _unevaluated_Add(*[S(1.0), x, S(2)])\n    assert a.args[0] == 3.0\n    assert a.args[1] == x\n\n    # Test nested Adds\n    a = _unevaluated_Add(x + 1, x + 2)\n    assert a == Add(x + x + 3, evaluate=False)\n\n    # Ensure sorting of arguments\n    a = _unevaluated_Add(x + y, y + x)\n    opts = (Add(x + y, y + x, evaluate=False), Add(y + x, x + y, evaluate=False))\n    assert a in opts and a == _unevaluated_Add(x + y, y + x)\n\n    # Ensure handling of non-Add and Number types\n    a = _unevaluated_Add(a * b, Rational(1, 2))\n    assert a.args[0] == Rational(1, 2)\n    assert a.args[1] == a * b\n"], "sample_1034": ["def test_apply_grover():\n    numqubits = 3\n    oracle = lambda qubits: qubits == IntQubit(5, nqubits=numqubits)\n    result = qapply(apply_grover(oracle, numqubits))\n    expected = IntQubit(5, nqubits=numqubits)\n    assert result == expected\n\n    numqubits = 2\n    oracle = lambda qubits: qubits == IntQubit(3, nqubits=numqubits)\n    result = qapply(apply_grover(oracle, numqubits))\n    expected = IntQubit(3, nqubits=numqubits)\n    assert result == expected\n\n    numqubits = 1\n    oracle = lambda qubits: qubits == IntQubit(0, nqubits=numqubits)\n    result = qapply(apply_grover(oracle, numqubits))\n    expected = IntQubit(0, nqubits=numqubits)\n    assert result == expected\n\n    try:\n        apply_grover(oracle, 0)\n    except QuantumError as e:\n        assert str(e) == \"Grover's algorithm needs nqubits > 0, received 0 qubits\"\n", "def test_apply_grover():\n    nqubits = 2\n    f = lambda qubits: qubits == IntQubit(2)\n    result = qapply(apply_grover(f, nqubits))\n    assert result == IntQubit(2, nqubits=nqubits)\n\n    nqubits = 3\n    f = lambda qubits: qubits == IntQubit(5)\n    result = qapply(apply_grover(f, nqubits))\n    assert result == IntQubit(5, nqubits=nqubits)\n\n    nqubits = 1\n    f = lambda qubits: qubits == IntQubit(1)\n    result = qapply(apply_grover(f, nqubits))\n    assert result == IntQubit(1, nqubits=nqubits)\n", "def test_apply_grover():\n    numqubits = 2\n    f = return_one_on_two\n    result = qapply(apply_grover(f, numqubits))\n    assert result == IntQubit(2, nqubits=numqubits)\n\n    numqubits = 3\n    f = lambda qubits: qubits == IntQubit(5, nqubits=3)\n    result = qapply(apply_grover(f, numqubits))\n    assert result == IntQubit(5, nqubits=numqubits)\n\n    numqubits = 1\n    f = lambda qubits: qubits == IntQubit(1, nqubits=1)\n    result = qapply(apply_grover(f, numqubits))\n    assert result == IntQubit(1, nqubits=numqubits)\n\n    # Test with invalid number of qubits\n    try:\n        apply_grover(f, 0)\n    except QuantumError as e:\n        assert str(e) == \"Grover's algorithm needs nqubits > 0, received 0 qubits\"\n", "def test_apply_grover():\n    nqubits = 2\n    f = lambda qubits: qubits == IntQubit(2)\n    result = qapply(apply_grover(f, nqubits))\n    expected = IntQubit(2, nqubits=nqubits)\n    assert result == expected\n\n    nqubits = 3\n    f = lambda qubits: qubits == IntQubit(5)\n    result = qapply(apply_grover(f, nqubits))\n    expected = IntQubit(5, nqubits=nqubits)\n    assert result == expected\n", "def test_apply_grover():\n    numqubits = 3\n    f = lambda qubits: qubits == IntQubit(5, nqubits=numqubits)\n    result = qapply(apply_grover(f, numqubits))\n    expected = IntQubit(5, nqubits=numqubits)\n    assert result == expected\n"], "sample_1035": ["def test_qubit_flip():\n    # Test flipping specific bits in a Qubit\n    q = Qubit(0, 1, 0, 1)\n    flipped_q = q.flip(1)  # Flip the second bit from the right\n    assert flipped_q == Qubit(0, 1, 1, 1)\n\n    flipped_q = q.flip(0, 1, 2, 3)  # Flip all bits\n    assert flipped_q == Qubit(1, 0, 1, 0)\n", "def test_qubit_initialization():\n    q = Qubit(0, 1, 0, 1)\n    assert q.qubit_values == (0, 1, 0, 1)\n    assert q.nqubits == 4\n    assert len(q) == 4\n    assert q.flip(1) == Qubit(0, 1, 1, 1)\n    assert q[0] == 1\n    assert q[3] == 0\n", "def test_qubit_flip():\n    q = Qubit(0, 1, 0)\n    flipped_q = q.flip(1)\n    assert flipped_q == Qubit(0, 0, 0)\n    flipped_q = q.flip(0, 2)\n    assert flipped_q == Qubit(1, 1, 1)\n", "def test_qubit_flip():\n    q = Qubit(0, 1, 0, 1)\n    flipped_q = q.flip(1)\n    expected_q = Qubit(0, 1, 1, 1)\n    assert flipped_q == expected_q\n\n    flipped_q = q.flip(1, 3)\n    expected_q = Qubit(1, 1, 1, 1)\n    assert flipped_q == expected_q\n\n    flipped_q = q.flip(0, 2)\n    expected_q = Qubit(1, 1, 0, 0)\n    assert flipped_q == expected_q\n", "def test_qubit_flip():\n    q = IntQubit(5, nqubits=3)  # |101>\n    flipped_q = q.flip(0)  # Should flip the least significant qubit: |100>\n    assert flipped_q == IntQubit(4, nqubits=3)\n\n    flipped_q = q.flip(1)  # Should flip the middle qubit: |111>\n    assert flipped_q == IntQubit(7, nqubits=3)\n\n    flipped_q = q.flip(0, 1, 2)  # Should flip all qubits: |010>\n    assert flipped_q == IntQubit(2, nqubits=3)\n"], "sample_1036": ["def test_as_coefficients_dict():\n    x, y, z = symbols('x y z')\n    expr = 3*x*y*z\n    result = expr.as_coefficients_dict()\n    assert result == {x*y*z: 3}\n    assert result[x] == 0\n", "def test_unevaluated_mul():\n    from sympy import sqrt, Rational\n    from sympy.abc import a, b, c\n    from sympy.core.mul import _unevaluated_Mul as uMul\n\n    # Test number collection and flattening Muls\n    expr = uMul(a, b, c)\n    assert expr.args == (a, b, c)\n\n    expr = uMul(2, a, 3)\n    assert expr.args == (6, a)\n\n    expr = uMul(a, 2, 3, b)\n    assert expr.args == (6, a, b)\n\n    # Test that two unevaluated Muls with the same arguments compare as equal\n    expr1 = uMul(sqrt(2), sqrt(3))\n    expr2 = uMul(sqrt(3), sqrt(2))\n    assert expr1 == expr2\n\n    expr = uMul(Rational(1, 2), uMul(a, b))\n    assert expr == uMul(Rational(1, 2), a, b)\n\n    # Test that the order is preserved for non-commutative args\n    A, B, C = symbols('A B C', commutative=False)\n    expr = uMul(A, B, C, 2, 3)\n    assert expr.args == (6, A, B, C)\n", "def test_as_coeff_mul():\n    a, b, c = symbols('a b c')\n    expr = Mul(3, a, b, c)\n    assert expr.as_coeff_mul() == (3, (a, b, c))\n    assert expr.as_coeff_mul(a) == (1, (3, a, b, c))\n    assert expr.as_coeff_mul(b) == (1, (3, a, b, c))\n    assert expr.as_coeff_mul(c) == (1, (3, a, b, c))\n    \n    expr = Mul(-2, a, b)\n    assert expr.as_coeff_mul() == (-2, (a, b))\n    assert expr.as_coeff_mul(a) == (1, (-2, a, b))\n    assert expr.as_coeff_mul(b) == (1, (-2, a, b))\n", "def test_unevaluated_Mul():\n    from sympy import S, sqrt\n    from sympy.abc import x\n    from sympy.core.mul import _unevaluated_Mul as uMul\n\n    # Basic test cases for _unevaluated_Mul\n    a = uMul(*[S(3.0), x, S(2)])\n    assert a.args[0] == 6.00000000000000\n    assert a.args[1] == x\n\n    m1 = uMul(sqrt(2), sqrt(3))\n    m2 = uMul(sqrt(3), sqrt(2))\n    assert m1 == m2\n\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m1 == uMul(u)\n    assert m1 != Mul(*m1.args)\n\n    # Check with non-commutative arguments\n    a = Symbol('a', commutative=False)\n    b = Symbol('b', commutative=False)\n    m = uMul(a, b, x)\n    assert m.args == (x, Mul(a, b, evaluate=False))\n\n    # Check with complex number multiplication\n    assert uMul(S(2), I) == Mul(2*I, evaluate=False)\n    assert uMul(I, S(2)) == Mul(2*I, evaluate=False)\n", "def test_unevaluated_mul():\n    from sympy import S, sqrt\n    from sympy.abc import x, y\n\n    # Test with different types of numbers and symbols\n    a = _unevaluated_Mul(*[S(3.0), x, S(2)])\n    assert a.args[0] == 6.00000000000000\n    assert a.args[1] == x\n\n    # Test with commutative numbers\n    m = _unevaluated_Mul(sqrt(2), sqrt(3))\n    assert m == _unevaluated_Mul(sqrt(3), sqrt(2))\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m == _unevaluated_Mul(u)\n    assert m != Mul(*m.args)\n\n    # Test with non-commutative numbers\n    nc_mul = _unevaluated_Mul(x, y, evaluate=False)\n    assert nc_mul.args == (x, y)\n    assert _unevaluated_Mul(x, y, x) == Mul(x, y, x, evaluate=False)\n"], "sample_1037": ["def test_MatMul_inversion():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, n)\n    D = MatrixSymbol('D', n, m)\n\n    # Test inversion of a single matrix\n    assert (A*B).inverse() == (B**-1) * (A**-1)\n    \n    # Test inversion of a product of matrices\n    assert (A*B*C).inverse() == C**-1 * B**-1 * A**-1\n    assert (A*B*C*D).inverse() == D**-1 * C**-1 * B**-1 * A**-1\n\n    # Test inversion of square matrices\n    E = MatrixSymbol('E', m, m)\n    F = MatrixSymbol('F', m, m)\n    assert (E*F).inverse() == F**-1 * E**-1\n", "def test_as_coeff_matrices():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, k)\n    expr = 2 * A * B * C\n\n    coeff, matrices = expr.as_coeff_matrices()\n    assert coeff == 2\n    assert matrices == [A, B, C]\n\n    expr = A * B * C\n    coeff, matrices = expr.as_coeff_matrices()\n    assert coeff == 1\n    assert matrices == [A, B, C]\n\n    expr = 3 * (A * B * C)\n    coeff, matrices = expr.as_coeff_matrices()\n    assert coeff == 3\n    assert matrices == [A, B, C]\n\n    expr = MatMul(A, B, C)\n    coeff, matrices = expr.as_coeff_matrices()\n    assert coeff == 1\n    assert matrices == [A, B, C]\n\n    expr = MatMul(4, A, B, C)\n    coeff, matrices = expr.as_coeff_matrices()\n    assert coeff == 4\n    assert matrices == [A, B, C]\n\n    expr = MatMul(3, 4, A, B, C)\n    coeff, matrices = expr.as_coeff_matrices()\n    assert coeff == 12\n    assert matrices == [A, B, C]\n", "def test_MatMul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    \n    # Test MatMul with identity matrix\n    I = Identity(2)\n    assert MatMul(A, I).doit() == A\n    assert MatMul(I, A).doit() == A\n    \n    # Test MatMul with zero matrix\n    Z = ZeroMatrix(2, 2)\n    assert MatMul(A, Z).doit() == Z\n    assert MatMul(Z, A).doit() == Z\n    \n    # Test MatMul with scalar multiplication\n    assert MatMul(2, A).doit() == 2 * A\n    assert MatMul(A, 2).doit() == 2 * A\n    \n    # Test MatMul with multiple matrix multiplications\n    assert MatMul(A, B, C).doit() == MatMul(A, B, C)\n    \n    # Test MatMul with scalar and multiple matrix multiplications\n    assert MatMul(3, A, B).doit() == 3 * MatMul(A, B)\n    assert MatMul(A, 3, B).doit() == 3 * MatMul(A, B)\n    assert MatMul(A, B, 3).doit() == 3 * MatMul(A, B)\n    \n    # Test MatMul with transposed matrix\n    assert MatMul(A, transpose(B)).doit() == MatMul(A, Transpose(B).doit())\n    assert MatMul(transpose(A), B).doit() == MatMul(Transpose(A).doit(), B)\n    \n    # Test MatMul with deep doit\n    D = Matrix([[1, 2], [3, 4]])\n    E = Matrix([[5, 6], [7, 8]])\n    assert MatMul(D, E).doit(deep=True) == D * E\n    \n    # Test MatMul with shallow doit\n    assert MatMul(D, E).doit(deep=False) == MatMul(D, E)\n", "def test_MatMul_doit():\n    from sympy import eye\n\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n\n    # Testing doit() method on MatMul\n    expr = MatMul(A, B, C)\n    assert expr.doit() == MatMul(A, B, C)\n\n    # Testing with scalar multiplication\n    expr = MatMul(2, A, B)\n    assert expr.doit() == 2 * MatMul(A, B)\n\n    # Testing with Identity\n    expr = MatMul(A, Identity(2))\n    assert expr.doit() == A\n\n    # Testing with ZeroMatrix\n    expr = MatMul(A, ZeroMatrix(2, 2))\n    assert expr.doit() == ZeroMatrix(2, 2)\n\n    # Testing with explicit matrices\n    A_explicit = Matrix([[1, 2], [3, 4]])\n    B_explicit = Matrix([[2, 0], [0, 2]])\n    expr = MatMul(A_explicit, B_explicit)\n    assert expr.doit() == Matrix([[2, 4], [6, 8]])\n\n    # Testing deep doit\n    expr = MatMul(A, MatMul(B, C))\n    assert expr.doit(deep=True) == MatMul(A, B, C)\n", "def test_refine_MatMul():\n    from sympy.assumptions import assuming, Q\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    expr = X * X.T\n    assert refine(expr) == expr  # No assumptions, no refinement\n\n    with assuming(Q.orthogonal(X)):\n        assert refine(expr) == Identity(2)  # Orthogonal matrix refinement\n\n    expr2 = X * Y.T\n    with assuming(Q.orthogonal(X)):\n        assert refine(expr2) == expr2  # Y is not orthogonal, no refinement\n\n    with assuming(Q.orthogonal(X), Q.orthogonal(Y)):\n        assert refine(expr2) == X * Y.T  # Both orthogonal, but not adjoint to each other, no refinement\n\n    expr3 = X * X.conjugate()\n    with assuming(Q.unitary(X)):\n        assert refine(expr3) == Identity(2)  # Unitary matrix refinement\n\n    expr4 = X * X\n    with assuming(Q.orthogonal(X)):\n        assert refine(expr4) == expr4  # Orthogonal assumption, but not adjoint, no refinement\n"], "sample_1038": ["def test_MatrixExpr_as_explicit():\n    A = MatrixSymbol('A', 2, 2)\n    A_explicit = A.as_explicit()\n    assert isinstance(A_explicit, ImmutableMatrix)\n    assert A_explicit.shape == (2, 2)\n    assert A_explicit[0, 0] == A[0, 0]\n    assert A_explicit[0, 1] == A[0, 1]\n    assert A_explicit[1, 0] == A[1, 0]\n    assert A_explicit[1, 1] == A[1, 1]\n", "def test_MatrixExpr_as_real_imag():\n    A = MatrixSymbol('A', 2, 2)\n    A_real_imag = A.as_real_imag()\n    assert isinstance(A_real_imag, tuple)\n    assert len(A_real_imag) == 2\n    A_real, A_imag = A_real_imag\n    assert A_real.shape == A.shape\n    assert A_imag.shape == A.shape\n    assert A_real == (S(1)/2) * (A + A._eval_conjugate())\n    assert A_imag == (A - A._eval_conjugate()) / (2 * S.ImaginaryUnit)\n", "def test_matrix_transpose():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', n, n)\n\n    assert transpose(A).shape == (m, n)\n    assert transpose(A + B.T) == transpose(A) + B\n    assert transpose(A * B).shape == (l, n)\n    assert transpose(C * A) == transpose(A) * transpose(C)\n    assert transpose(transpose(A)) == A\n\n    Z = ZeroMatrix(n, m)\n    assert transpose(Z) == ZeroMatrix(m, n)\n\n    I = Identity(n)\n    assert transpose(I) == I\n", "def test_MatrixExpr_as_explicit():\n    A = MatrixSymbol('A', 2, 2)\n    assert A.as_explicit() == ImmutableMatrix([[A[0, 0], A[0, 1]], [A[1, 0], A[1, 1]]])\n\n    B = MatrixSymbol('B', 3, 2)\n    assert B.as_explicit() == ImmutableMatrix([[B[0, 0], B[0, 1]], [B[1, 0], B[1, 1]], [B[2, 0], B[2, 1]]])\n", "def test_matrix_expr_neg():\n    A = MatrixSymbol('A', n, m)\n    neg_A = -A\n    assert isinstance(neg_A, MatMul)\n    assert neg_A.args == (S.NegativeOne, A)\n    assert neg_A.shape == A.shape\n    assert -(-A) == A\n"], "sample_1039": ["def test_doprint():\n    # Test cases for MathMLContentPrinter and MathMLPresentationPrinter doprint method\n    expr = x + y\n    content_result = mp.doprint(expr)\n    presentation_result = mpp.doprint(expr)\n    \n    expected_content = '<apply><plus/><ci>x</ci><ci>y</ci></apply>'\n    expected_presentation = '<mrow><mi>x</mi><mo>+</mo><mi>y</mi></mrow>'\n    \n    assert content_result == expected_content\n    assert presentation_result == expected_presentation\n\n    expr = x * y\n    content_result = mp.doprint(expr)\n    presentation_result = mpp.doprint(expr)\n    \n    expected_content = '<apply><times/><ci>x</ci><ci>y</ci></apply>'\n    expected_presentation = '<mrow><mi>x</mi><mo>&InvisibleTimes;</mo><mi>y</mi></mrow>'\n    \n    assert content_result == expected_content\n    assert presentation_result == expected_presentation\n\n    expr = sin(x)\n    content_result = mp.doprint(expr)\n    presentation_result = mpp.doprint(expr)\n    \n    expected_content = '<apply><sin/><ci>x</ci></apply>'\n    expected_presentation = '<mrow><mi>sin</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    \n    assert content_result == expected_content\n    assert presentation_result == expected_presentation\n", "def test_apply_patch():\n    # Test if the apply_patch method correctly patches the xml.dom.minidom module\n    mp.apply_patch()\n    from xml.dom.minidom import Element, Text\n    \n    assert Element.writexml.__code__ is not mp._Element_writexml_old.__code__\n    assert Text.writexml.__code__ is not mp._Text_writexml_old.__code__\n\n    mp.restore_patch()\n    assert Element.writexml.__code__ is mp._Element_writexml_old.__code__\n    assert Text.writexml.__code__ is mp._Text_writexml_old.__code__\n\n    mpp.apply_patch()\n    assert Element.writexml.__code__ is not mpp._Element_writexml_old.__code__\n    assert Text.writexml.__code__ is not mpp._Text_writexml_old.__code__\n\n    mpp.restore_patch()\n    assert Element.writexml.__code__ is mpp._Element_writexml_old.__code__\n    assert Text.writexml.__code__ is mpp._Text_writexml_old.__code__\n", "def test_presentation_mathml_fractions():\n    mml_1 = mpp._print(Rational(3, 4))\n    assert mml_1.nodeName == 'mfrac'\n    assert len(mml_1.childNodes) == 2\n    assert mml_1.childNodes[0].nodeName == 'mn'\n    assert mml_1.childNodes[0].childNodes[0].nodeValue == '3'\n    assert mml_1.childNodes[1].nodeName == 'mn'\n    assert mml_1.childNodes[1].childNodes[0].nodeValue == '4'\n\n    mml_2 = mpp._print(Float(1.5))\n    assert mml_2.nodeName == 'mn'\n    assert mml_2.childNodes[0].nodeValue == '1.5'\n\n    mml_3 = mpp._print(x/2)\n    assert mml_3.nodeName == 'mfrac'\n    assert mml_3.childNodes[0].nodeName == 'mi'\n    assert mml_3.childNodes[0].childNodes[0].nodeValue == 'x'\n    assert mml_3.childNodes[1].nodeName == 'mn'\n    assert mml_3.childNodes[1].childNodes[0].nodeValue == '2'\n", "def test_content_mathml_patch_restore():\n    mp.apply_patch()\n    # Check if the patch is applied correctly\n    doc = mp.dom.createElement('apply')\n    doc.appendChild(mp.dom.createElement('plus'))\n    doc.appendChild(mp.dom.createElement('ci'))\n    doc.childNodes[1].appendChild(mp.dom.createTextNode('x'))\n    doc.appendChild(mp.dom.createElement('cn'))\n    doc.childNodes[2].appendChild(mp.dom.createTextNode('1'))\n    prettyxml_patched = doc.toprettyxml()\n    assert prettyxml_patched == '<apply>\\n\\t<plus/>\\n\\t<ci>x</ci>\\n\\t<cn>1</cn>\\n</apply>\\n'\n\n    mp.restore_patch()\n    # Check if the patch is restored correctly\n    prettyxml_restored = doc.toprettyxml()\n    assert prettyxml_restored == prettyxml_patched  # Should be the same as patched\n", "def test_print_function():\n    f = Symbol('f')\n    assert mpp.doprint(f(x)) == '<mrow><mi>f</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(f(x)) == '<apply><ci>f</ci><ci>x</ci></apply>'\n"], "sample_1040": ["def test_apply_patch_restore_patch():\n    # Apply and restore patch\n    mp.apply_patch()\n    mpp.apply_patch()\n    \n    # Test the patched functionality\n    doc = mp.dom.createElement('test')\n    txt = mp.dom.createTextNode('This is a test')\n    doc.appendChild(txt)\n    assert doc.toxml() == '<test>This is a test</test>'\n    \n    doc_p = mpp.dom.createElement('test')\n    txt_p = mpp.dom.createTextNode('This is a test for presentation')\n    doc_p.appendChild(txt_p)\n    assert doc_p.toxml() == '<test>This is a test for presentation</test>'\n\n    # Restore original functionality\n    mp.restore_patch()\n    mpp.restore_patch()\n\n    # Test the original functionality\n    doc = mp.dom.createElement('test')\n    txt = mp.dom.createTextNode('This is a test')\n    doc.appendChild(txt)\n    assert doc.toxml() == '<test>This is a test</test>'\n    \n    doc_p = mpp.dom.createElement('test')\n    txt_p = mpp.dom.createTextNode('This is a test for presentation')\n    doc_p.appendChild(txt_p)\n    assert doc_p.toxml() == '<test>This is a test for presentation</test>'\n", "def test_print_infinity():\n    pos_inf = S.Infinity\n    neg_inf = S.NegativeInfinity\n\n    # Testing content MathML for positive infinity\n    mml_pos_inf = mp._print(pos_inf)\n    assert mml_pos_inf.nodeName == 'infinity'\n    assert not mml_pos_inf.hasChildNodes()\n\n    # Testing content MathML for negative infinity\n    mml_neg_inf = mp._print(neg_inf)\n    assert mml_neg_inf.nodeName == 'apply'\n    assert mml_neg_inf.childNodes[0].nodeName == 'minus'\n    assert mml_neg_inf.childNodes[1].nodeName == 'infinity'\n\n    # Testing presentation MathML for positive infinity\n    mml_pos_inf_pres = mpp._print(pos_inf)\n    assert mml_pos_inf_pres.nodeName == 'mi'\n    assert mml_pos_inf_pres.childNodes[0].nodeValue == '&#x221E;'\n\n    # Testing presentation MathML for negative infinity\n    mml_neg_inf_pres = mpp._print(neg_inf)\n    assert mml_neg_inf_pres.nodeName == 'mrow'\n    assert mml_neg_inf_pres.childNodes[0].nodeName == 'mo'\n    assert mml_neg_inf_pres.childNodes[0].childNodes[0].nodeValue == '-'\n    assert mml_neg_inf_pres.childNodes[1].nodeName == 'mi'\n    assert mml_neg_inf_pres.childNodes[1].childNodes[0].nodeValue == '&#x221E;'\n", "def test_mathml_functions_with_args():\n    f = Function('f')\n    g = Function('g')\n    \n    mml_1 = mp._print(f(x, y))\n    assert mml_1.nodeName == 'apply'\n    assert mml_1.childNodes[0].nodeName == 'ci'\n    assert mml_1.childNodes[1].nodeName == 'ci'\n    assert mml_1.childNodes[2].nodeName == 'ci'\n    assert mml_1.childNodes[1].childNodes[0].nodeValue == 'x'\n    assert mml_1.childNodes[2].childNodes[0].nodeValue == 'y'\n\n    mml_2 = mp._print(g(x**2, y**3))\n    assert mml_2.nodeName == 'apply'\n    assert mml_2.childNodes[0].nodeName == 'ci'\n    assert mml_2.childNodes[1].nodeName == 'apply'\n    assert mml_2.childNodes[1].childNodes[0].nodeName == 'power'\n    assert mml_2.childNodes[1].childNodes[1].childNodes[0].nodeValue == 'x'\n    assert mml_2.childNodes[1].childNodes[2].childNodes[0].nodeValue == '2'\n    assert mml_2.childNodes[2].nodeName == 'apply'\n    assert mml_2.childNodes[2].childNodes[0].nodeName == 'power'\n    assert mml_2.childNodes[2].childNodes[1].childNodes[0].nodeValue == 'y'\n    assert mml_2.childNodes[2].childNodes[2].childNodes[0].nodeValue == '3'\n", "def test_mathml_presentation_greek():\n    mml = mpp._print(Symbol('Alpha'))\n    assert mml.nodeName == 'mi'\n    assert mml.childNodes[0].nodeValue == u'\\N{GREEK CAPITAL LETTER ALPHA}'\n\n    assert mpp.doprint(Symbol('Beta')) == '<mi>&#914;</mi>'\n    assert mpp.doprint(Symbol('Gamma')) == '<mi>&#915;</mi>'\n    assert mpp.doprint(Symbol('Delta')) == '<mi>&#916;</mi>'\n    assert mpp.doprint(Symbol('Epsilon')) == '<mi>&#917;</mi>'\n    assert mpp.doprint(Symbol('Zeta')) == '<mi>&#918;</mi>'\n    assert mpp.doprint(Symbol('Eta')) == '<mi>&#919;</mi>'\n    assert mpp.doprint(Symbol('Theta')) == '<mi>&#920;</mi>'\n    assert mpp.doprint(Symbol('Iota')) == '<mi>&#921;</mi>'\n    assert mpp.doprint(Symbol('Kappa')) == '<mi>&#922;</mi>'\n    assert mpp.doprint(Symbol('Lambda')) == '<mi>&#923;</mi>'\n    assert mpp.doprint(Symbol('Mu')) == '<mi>&#924;</mi>'\n    assert mpp.doprint(Symbol('Nu')) == '<mi>&#925;</mi>'\n    assert mpp.doprint(Symbol('Xi')) == '<mi>&#926;</mi>'\n    assert mpp.doprint(Symbol('Omicron')) == '<mi>&#927;</mi>'\n    assert mpp.doprint(Symbol('Pi')) == '<mi>&#928;</mi>'\n    assert mpp.doprint(Symbol('Rho')) == '<mi>&#929;</mi>'\n    assert mpp.doprint(Symbol('Sigma')) == '<mi>&#931;</mi>'\n    assert mpp.doprint(Symbol('Tau')) == '<mi>&#932;</mi>'\n    assert mpp.doprint(Symbol('Upsilon')) == '<mi>&#933;</mi>'\n    assert mpp.doprint(Symbol('Phi')) == '<mi>&#934;</mi>'\n    assert mpp.doprint(Symbol('Chi')) == '<mi>&#935;</mi>'\n    assert mpp.doprint(Symbol('Psi')) == '<mi>&#936;</mi>'\n    assert mpp.dopr", "def test_content_mathml_exponents():\n    expr = x**y\n    mml = mp._print(expr)\n    assert mml.nodeName == 'apply'\n    assert mml.childNodes[0].nodeName == 'power'\n    assert mml.childNodes[1].nodeName == 'ci'\n    assert mml.childNodes[1].childNodes[0].nodeValue == 'x'\n    assert mml.childNodes[2].nodeName == 'ci'\n    assert mml.childNodes[2].childNodes[0].nodeValue == 'y'\n"], "sample_1041": ["def test_MatrixExpr_properties():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n\n    # Test MatrixExpr properties\n    assert A.is_Matrix\n    assert A.is_MatrixExpr\n    assert A.is_Identity is None\n    assert not A.is_Inverse\n    assert not A.is_Transpose\n    assert not A.is_ZeroMatrix\n    assert not A.is_MatAdd\n    assert not A.is_MatMul\n    assert not A.is_commutative\n    assert not A.is_number\n    assert not A.is_symbol\n\n    # Test properties of other matrix types\n    I = Identity(n)\n    assert I.is_Identity\n    assert I.is_square\n\n    Z = ZeroMatrix(n, m)\n    assert Z.is_ZeroMatrix\n    assert not Z.is_square\n", "def test_MatrixExpr_from_index_summation():\n    i, j, k, l, N = symbols('i j k l N')\n    A = MatrixSymbol('A', N, N)\n    B = MatrixSymbol('B', N, N)\n    expr = Sum(A[i, j]*B[j, k], (j, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B\n\n    expr = Sum(A[j, i]*B[j, k], (j, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == A.T*B\n\n    expr = Sum(A[i, i], (i, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == trace(A)\n\n    expr = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, N-1), (k, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B.T*A.T\n\n    expr = Sum(A[i, j]*B[j, k]*KroneckerDelta(i, k), (j, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B\n\n    expr = Sum(A[i, j]*B[j, k], (j, 0, N-1), (k, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B.T\n", "def test_matrixexpr_as_real_imag():\n    A = MatrixSymbol('A', n, m)\n    real, imag = A.as_real_imag()\n    assert real == (S(1)/2)*(A + A._eval_conjugate())\n    assert imag == (A - A._eval_conjugate())/(2*S.ImaginaryUnit)\n", "def test_conjugate():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, n)\n    Z = ZeroMatrix(n, m)\n    I = Identity(n)\n\n    assert A.conjugate() == conjugate(A)\n    assert (A + B).conjugate() == A.conjugate() + B.conjugate()\n    assert (A * B).conjugate() == A.conjugate() * B.conjugate()\n    assert Z.conjugate() == Z\n    assert I.conjugate() == I\n", "def test_matrixexpr_as_explicit():\n    A = MatrixSymbol('A', 2, 2)\n    expr = A + Identity(2)\n    explicit_expr = expr.as_explicit()\n    expected = Matrix([[A[0,0] + 1, A[0,1]], [A[1,0], A[1,1] + 1]])\n    \n    assert explicit_expr == expected\n\n    B = MatrixSymbol('B', 3, 3)\n    explicit_B = B.as_explicit()\n    expected_B = Matrix([[B[0,0], B[0,1], B[0,2]], [B[1,0], B[1,1], B[1,2]], [B[2,0], B[2,1], B[2,2]]])\n    \n    assert explicit_B == expected_B\n"], "sample_1042": ["def test_IndexedBase_offset():\n    i = symbols('i', integer=True)\n    o = symbols('o', integer=True)\n    A = IndexedBase('A', offset=o)\n    assert A.offset == o\n    assert A[i].subs(o, 3) == IndexedBase('A', offset=3)[i]\n", "def test_IndexedBase_strides_and_offset():\n    i, j = symbols('i j', integer=True)\n    l, m, n, o = symbols('l m n o', integer=True)\n    A = IndexedBase('A', strides=(l, m, n), offset=o)\n    B = IndexedBase('B', strides='C')\n    C = IndexedBase('C', strides='F')\n\n    assert A.strides == (l, m, n)\n    assert A.offset == o\n\n    assert B.strides == 'C'\n    assert B.offset == 0\n\n    assert C.strides == 'F'\n    assert C.offset == 0\n\n    a_i_j_k = A[i, j, 0]\n    b_i_j = B[i, j]\n    c_i_j = C[i, j]\n\n    from sympy.printing import ccode\n    assert ccode(a_i_j_k) == 'A[l*i + m*j + n*0 + o]'\n    assert ccode(b_i_j) == 'B[i][j]'\n    assert ccode(c_i_j) == 'C[i + j*1]'\n", "def test_IndexedBase_offset_strides():\n    i, j = symbols('i j', integer=True)\n    o, p = symbols('o p', integer=True)\n    a = IndexedBase('a', shape=(o, p), offset=1, strides=(2, 3))\n    assert a.shape == Tuple(o, p)\n    assert a.offset == 1\n    assert a.strides == (2, 3)\n    assert a[1, 2].subs(a, {1: 2}) == IndexedBase('a', shape=(o, p), offset=2, strides=(2, 3))[1, 2]\n", "def test_IndexedBase_offset_and_strides():\n    i, j, k = symbols('i j k', integer=True)\n    a, b = symbols('a b')\n    m, n, o = symbols('m n o', integer=True)\n\n    A = IndexedBase('A', shape=(m, n), offset=o, strides=(a, b))\n    assert A.offset == o\n    assert A.strides == (a, b)\n    assert A.shape == Tuple(m, n)\n    assert A[i, j].shape == Tuple(m, n)\n    assert A[i, j].offset == o\n    assert A[i, j].strides == (a, b)\n\n    # Test default offset and strides\n    B = IndexedBase('B', shape=(m, n))\n    assert B.offset == S.Zero\n    assert B.strides is None\n\n    # Test code generation printing\n    from sympy.printing import ccode\n    assert ccode(A[i, j]) == 'A[a*i + b*j + o]'\n    assert ccode(B[i, j]) == 'B[i, j]'\n", "def test_IndexedBase_strides():\n    i, j = symbols('i j', integer=True)\n    m, n, o, p = symbols('m n o p', integer=True)\n    A = IndexedBase('A', shape=(m, n), strides=(o, p))\n    assert A.strides == Tuple(o, p)\n    assert A.shape == Tuple(m, n)\n    B = IndexedBase('B', strides='C')\n    assert B.strides == 'C'\n    C = IndexedBase('C', strides='F')\n    assert C.strides == 'F'\n"], "sample_1043": ["def test_user_functions():\n    user_functions = {\n        \"custom_func\": \"CustomFunc\"\n    }\n    assert mcode(Function('custom_func')(x, y), user_functions=user_functions) == \"CustomFunc[x, y]\"\n    user_functions = {\n        \"h\": \"H\"\n    }\n    h = Function('h')\n    assert mcode(h(x), user_functions=user_functions) == \"H[x]\"\n", "def test_user_defined_functions():\n    settings = {'user_functions': {'foo': 'FooFunc', 'bar': 'BarFunc'}}\n    assert mcode(Function('foo')(x), **settings) == 'FooFunc[x]'\n    assert mcode(Function('bar')(x, y), **settings) == 'BarFunc[x, y]'\n    assert mcode(Function('foo')(sin(x)), **settings) == 'FooFunc[Sin[x]]'\n    assert mcode(Function('baz')(x), **settings) == 'baz[x]'  # unknown function\n", "def test_custom_function():\n    custom_funcs = {\n        'myfunc': [(lambda x: True, 'MyFunc')]\n    }\n    assert mcode(Function('myfunc')(x, y), user_functions=custom_funcs) == \"MyFunc[x, y]\"\n    assert mcode(Function('myfunc')(x**2 + y, z), user_functions=custom_funcs) == \"MyFunc[x^2 + y, z]\"\n", "def test_user_defined_function():\n    custom_func = Function('custom_func')\n    settings = {'user_functions': {'custom_func': 'CustomFunc'}}\n    assert mcode(custom_func(x, y), **settings) == \"CustomFunc[x, y]\"\n    assert mcode(custom_func(x**2, sin(y)), **settings) == \"CustomFunc[x^2, Sin[y]]\"\n", "def test_user_functions():\n    user_func = {'foo': 'FooFunction', 'bar': 'BarFunction'}\n    expr1 = Function('foo')(x, y)\n    expr2 = Function('bar')(z)\n    assert mcode(expr1, user_functions=user_func) == \"FooFunction[x, y]\"\n    assert mcode(expr2, user_functions=user_func) == \"BarFunction[z]\"\n    # Testing if known_functions are preserved\n    assert mcode(sin(x), user_functions=user_func) == \"Sin[x]\"\n"], "sample_1044": ["def test_integer_nthroot():\n    from sympy import integer_nthroot, S\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(256, 4) == (4, True)\n    assert integer_nthroot(257, 4) == (4, False)\n    assert integer_nthroot(1, 1) == (1, True)\n    assert integer_nthroot(0, 1) == (0, True)\n    assert integer_nthroot(0, 2) == (0, True)\n    assert integer_nthroot(1, 2) == (1, True)\n    assert integer_nthroot(10**20, 10) == (10**2, True)\n    assert integer_nthroot(10**20 + 1, 10) == (10**2, False)\n    raises(ValueError, lambda: integer_nthroot(-1, 2))\n    raises(ValueError, lambda: integer_nthroot(1, 0))\n", "def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(1, 1) == (1, True)\n    assert integer_nthroot(0, 1) == (0, True)\n    assert integer_nthroot(10**12, 2) == (10**6, True)\n    assert integer_nthroot(10**12 + 1, 2) == (10**6, False)\n    raises(ValueError, lambda: integer_nthroot(-1, 2))\n    raises(ValueError, lambda: integer_nthroot(16, 0))\n", "def test_integer_nthroot():\n    from sympy import integer_nthroot, S\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(1, 1) == (1, True)\n    assert integer_nthroot(0, 1) == (0, True)\n    assert integer_nthroot(0, 2) == (0, True)\n    assert integer_nthroot(1, 0) == (1, True)\n    assert integer_nthroot(1024, 10) == (2, False)\n    raises(ValueError, lambda: integer_nthroot(-1, 2))\n    raises(ValueError, lambda: integer_nthroot(1, -2))\n", "def test_integer_nthroot():\n    # Test for positive perfect square\n    assert integer_nthroot(16, 2) == (4, True)\n    # Test for positive non-perfect square\n    assert integer_nthroot(26, 2) == (5, False)\n    # Test for a large number\n    assert integer_nthroot(17984395633462800708566937239551, 2) == (134101317, False)\n    # Test for positive perfect cube\n    assert integer_nthroot(27, 3) == (3, True)\n    # Test for positive non-perfect cube\n    assert integer_nthroot(28, 3) == (3, False)\n    # Test for negative number\n    raises(ValueError, lambda: integer_nthroot(-1, 2))\n    # Test for zero\n    assert integer_nthroot(0, 2) == (0, True)\n    # Test for one\n    assert integer_nthroot(1, 2) == (1, True)\n    # Test for n being one\n    assert integer_nthroot(10, 1) == (10, True)\n    # Test for n greater than y\n    assert integer_nthroot(5, 6) == (1, False)\n", "def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(1, 1) == (1, True)\n    assert integer_nthroot(0, 1) == (0, True)\n    assert integer_nthroot(1, 2) == (1, True)\n    assert integer_nthroot(0, 2) == (0, True)\n    assert integer_nthroot(8, 3) == (2, True)\n    assert integer_nthroot(9, 3) == (2, False)\n    assert integer_nthroot(123456789, 3) == (497, False)\n    assert integer_nthroot(123456789, 10) == (2, False)\n\n    raises(ValueError, lambda: integer_nthroot(-1, 2))\n    raises(ValueError, lambda: integer_nthroot(1, 0))\n    raises(ValueError, lambda: integer_nthroot(-1, -1))\n"], "sample_1045": ["def test_mpf_norm():\n    # Test the mpf_norm function for different cases\n    assert mpf_norm((0, 0, 0, 0), 53) == (0, long(0), 0, 0)\n    assert mpf_norm((1, 0, 1, 0), 53) == (0, long(0), 0, 0)\n    assert mpf_norm((1, 1, 1, 1), 53) == (1, long(1), 1, 1)\n    assert mpf_norm((0, 1, 0, 1), 53) == (0, long(1), 0, 1)\n    assert mpf_norm((0, 1, 0, 2), 53) == (0, long(1), 0, 2)\n    assert mpf_norm((0, 1, 0, 3), 53) == (0, long(1), 0, 3)\n    assert mpf_norm((0, 1, 0, 53), 53) == (0, long(1), 0, 53)\n    assert mpf_norm((0, 1, 0, 100), 53) == (0, long(1), 0, 100)\n", "def test_igcd_lehmer_large_numbers():\n    # Test the igcd_lehmer with very large numbers\n    a = 10**1000 + 123456789\n    b = 10**1000 + 987654321\n    assert igcd_lehmer(a, b) == 1\n    assert igcd_lehmer(a * 123, b * 123) == 123\n", "def test_mod_inverse_extreme_values():\n    # Test for large prime values\n    assert mod_inverse(123456789123456789, 987654321987654321) == 818181818181818181\n\n    # Test for non-prime modulus\n    assert mod_inverse(123456789123456789, 987654321987654322) == 548387096774193548\n    \n    # Test for values with large difference\n    assert mod_inverse(10**100 + 12345, 10**50 + 67890) == 804123711340206185\n\n    # Test for values resulting in direct inversion\n    assert mod_inverse(1, 10**10) == 1\n    assert mod_inverse(10**9 + 7, 10**9 + 9) == 666666666\n    \n    # Test for cases where inverse does not exist\n    raises(ValueError, lambda: mod_inverse(6, 12))\n    raises(ValueError, lambda: mod_inverse(10**20, 2*10**20))\n\n    # Test with symbolic input\n    x = Symbol('x')\n    raises(TypeError, lambda: mod_inverse(3, x))\n", "def test_mpf_norm_special_cases():\n    from mpmath.libmp.libmpf import fnan, finf, fninf\n\n    # Check normalization of NaN, +inf, -inf\n    assert mpf_norm(fnan, 53) == fnan\n    assert mpf_norm(finf, 53) == finf\n    assert mpf_norm(fninf, 53) == fninf\n\n    # Check normalization of zero\n    assert mpf_norm((0, 0, 0, 0), 53) == (0, 0, 0, 0)\n    \n    # Check normalization of large exponent values\n    assert mpf_norm((0, 1, 1000, 1), 53) == (0, 1, 1000, 1)\n    assert mpf_norm((0, 1, -1000, 1), 53) == (0, 1, -1000, 1)\n\n    # Check normalization of negative mantissa values\n    assert mpf_norm((1, 1, 0, 1), 53) == (1, 1, 0, 1)\n    \n    # Check normalization of very small numbers\n    assert mpf_norm((0, 1, -1000, 1), 53) == (0, 1, -1000, 1)\n    \n    # Check normalization of very large numbers\n    assert mpf_norm((0, 1, 1000, 1), 53) == (0, 1, 1000, 1)\n\n    # Check normalization where mantissa is zero but exponent is not zero\n    assert mpf_norm((0, 0, 1000, 0), 53) == (0, 0, 0, 0)\n    assert mpf_norm((0, 0, -1000, 0), 53) == (0, 0, 0, 0)\n", "def test_mpf_norm_edge_cases():\n    assert mpf_norm((_mpf_zero[0], _mpf_zero[1], _mpf_zero[2], _mpf_zero[3]), 10) == _mpf_zero\n    assert mpf_norm((_mpf_inf[0], _mpf_inf[1], _mpf_inf[2], _mpf_inf[3]), 10) == _mpf_inf\n    assert mpf_norm((_mpf_ninf[0], _mpf_ninf[1], _mpf_ninf[2], _mpf_ninf[3]), 10) == _mpf_ninf\n    assert mpf_norm((_mpf_nan[0], _mpf_nan[1], _mpf_nan[2], _mpf_nan[3]), 10) == _mpf_nan\n"], "sample_1046": ["def test_tensor_element():\n    Lorentz = TensorIndexType(\"Lorentz\")\n    i, j, k = tensor_indices(\"i j k\", Lorentz)\n    A = tensorhead(\"A\", [Lorentz, Lorentz], [[1], [1]])\n    B = tensorhead(\"B\", [Lorentz, Lorentz], [[1], [1]])\n\n    expr = A(i, j) + B(i, j)\n    te = TensorElement(expr, {i: 2})\n\n    assert te.get_free_indices() == [j]\n    assert te.get_indices() == [j]\n\n    repl = {A(i, j): [[1, 2], [3, 4]], B(i, j): [[5, 6], [7, 8]]}\n    result = te.replace_with_arrays(repl)\n    expected = Array([[1, 2], [3, 4]])[2, :] + Array([[5, 6], [7, 8]])[2, :]\n\n    assert (result == expected).all()\n", "def test_tensor_creation_invalid_indices():\n    L = TensorIndexType(\"L\")\n    i0, i1, i2 = tensor_indices('i0:3', L)\n    A = tensorhead(\"A\", [L, L], [[1, 1]])\n\n    raises(TypeError, lambda: A(i0, 3.5))  # Non-integer index\n    raises(ValueError, lambda: A(i0, Symbol(\"x\")))  # Symbol not matching tensor index type\n    raises(ValueError, lambda: A(i0, L2))  # Wrong tensor index type\n", "def test_index_structure_from_indices():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    m0, m1, m2, m3 = tensor_indices('m0,m1,m2,m3', Lorentz)\n    \n    # Test with both free and dummy indices\n    idx_structure = _IndexStructure.from_indices(m0, m1, -m1, m3)\n    assert idx_structure.free == [(m0, 0), (m3, 3)]\n    assert idx_structure.dum == [(1, 2)]\n    assert idx_structure.index_types == [Lorentz, Lorentz, Lorentz, Lorentz]\n\n    # Test with only free indices\n    idx_structure = _IndexStructure.from_indices(m0, m1, m2, m3)\n    assert idx_structure.free == [(m0, 0), (m1, 1), (m2, 2), (m3, 3)]\n    assert idx_structure.dum == []\n    assert idx_structure.index_types == [Lorentz, Lorentz, Lorentz, Lorentz]\n\n    # Test with only dummy indices\n    idx_structure = _IndexStructure.from_indices(m0, -m0, m1, -m1)\n    assert idx_structure.free == []\n    assert idx_structure.dum == [(0, 1), (2, 3)]\n    assert idx_structure.index_types == [Lorentz, Lorentz, Lorentz, Lorentz]\n\n    # Test with no indices\n    idx_structure = _IndexStructure.from_indices()\n    assert idx_structure.free == []\n    assert idx_structure.dum == []\n    assert idx_structure.index_types == []\n\n    # Test sorting of dummy indices\n    idx_structure = _IndexStructure.from_indices(m1, -m1, m0, -m0)\n    assert idx_structure.free == []\n    assert idx_structure.dum == [(2, 3), (0, 1)]\n    assert idx_structure.index_types == [Lorentz, Lorentz, Lorentz, Lorentz]\n", "def test_tensor_data_lazy_evaluator():\n    from sympy.tensor.tensor import _TensorDataLazyEvaluator, TensorHead, TensorIndexType, TensorIndex\n    from sympy.tensor.array import MutableDenseNDimArray\n    import numpy as np\n\n    # Create dummy tensor data\n    L = TensorIndexType(\"Lorentz\", dim=4)\n    i0, i1 = tensor_indices('i0 i1', L)\n\n    A = tensorhead(\"A\", [L], [[1]])\n    B = tensorhead(\"B\", [L], [[1]])\n\n    data_a = MutableDenseNDimArray([1, 2, 3, 4])\n    data_b = MutableDenseNDimArray([4, 3, 2, 1])\n\n    evaluator = _TensorDataLazyEvaluator()\n    evaluator[A] = data_a\n    evaluator[B] = data_b\n\n    # Test __getitem__\n    assert evaluator[A] == data_a\n    assert evaluator[B] == data_b\n\n    # Test _get\n    assert evaluator._get(A) == data_a\n    assert evaluator._get(B) == data_b\n\n    # Test exception when retrieving non-existing data\n    C = tensorhead(\"C\", [L], [[1]])\n    assert evaluator[C] is None\n\n    # Test data contraction with tensor product\n    C = tensorhead(\"C\", [L, L], [[1], [1]])\n    data_c = MutableDenseNDimArray(np.outer(data_a, data_b).tolist())\n    evaluator[C] = data_c\n\n    assert evaluator[C] == data_c\n\n    # Test tensor contraction via data_contract_dum\n    contracted_data = evaluator.data_contract_dum([data_c], [(0, 1)], 2)\n    assert contracted_data == sum(a * b for a, b in zip(data_a, data_b))\n\n    # Test inverse and inverse transpose matrix\n    D = tensorhead(\"D\", [L, L], [[1], [1]])\n    matrix_data = MutableDenseNDimArray(np.array([[4, 7], [2, 6]], dtype=float).tolist())\n    inverse_matrix_data = evaluator.inverse_matrix(matrix_data)\n    inverse_transpose_matrix_data = evaluator.inverse_transpose_matrix(matrix_data)\n\n    evaluator[D] = matrix_data\n\n    assert np.allclose(inverse_matrix_data.tomatrix().tolist(), np.linalg.inv(matrix_data.tomatrix()).tolist())\n    assert np.allclose(inverse_transpose_matrix", "def test_index_replacement():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    a, b, c, d = tensor_indices('a, b, c, d', Lorentz)\n    A = tensorhead('A', [Lorentz], [[1]])\n    B = tensorhead('B', [Lorentz], [[1]])\n    C = tensorhead('C', [Lorentz], [[1]])\n    \n    # Test with simple index replacement\n    expr = A(a)\n    repl = {a: b}\n    assert expr.substitute_indices(*repl.items()) == A(b)\n\n    # Test with multiple index replacement\n    expr = A(a) * B(b)\n    repl = {a: c, b: d}\n    assert expr.substitute_indices(*repl.items()) == A(c) * B(d)\n\n    # Test with swapping indices\n    expr = A(a) * B(b)\n    repl = {a: b, b: a}\n    assert expr.substitute_indices(*repl.items()) == A(b) * B(a)\n\n    # Test with more complicated expression\n    expr = A(a) * B(b) + C(c)\n    repl = {a: d, b: a, c: b}\n    assert expr.substitute_indices(*repl.items()) == A(d) * B(a) + C(b)\n\n    # Test raising exception for incompatible indices\n    expr = A(a) * B(b)\n    repl = {a: a, b: a}\n    raises(ValueError, lambda: expr.substitute_indices(*repl.items()))\n"], "sample_1047": ["def test_issue_11567():\n    x = Symbol('x', nonzero=True)\n    assert (1/(x + 1)).is_nonzero is True\n    assert (1/(x - x)).is_nonzero is False\n", "def test_symbol_nonnegative():\n    x = Symbol('x', nonnegative=True)\n    assert x.is_positive is None\n    assert x.is_nonpositive is None\n    assert x.is_negative is False\n    assert x.is_nonnegative is True\n    assert x.is_zero is None\n    assert x.is_nonzero is None\n\n", "def test_symbol_rational():\n    x = Symbol('x', rational=True)\n    assert x.is_rational is True\n    assert x.is_real is True\n    assert x.is_finite is True\n    assert x.is_integer is None\n    assert x.is_algebraic is True\n\n    x = Symbol('x', rational=False)\n    assert x.is_rational is False\n    assert x.is_real is None\n    assert x.is_finite is None\n    assert x.is_integer is None\n    assert x.is_algebraic is None\n", "def test_symbol_transcendental():\n    x = Symbol('x', transcendental=True)\n    assert x.is_transcendental is True\n    assert x.is_algebraic is False\n    assert x.is_complex is True\n    assert x.is_integer is False\n    assert x.is_rational is False\n    assert x.is_real is None\n    assert x.is_positive is None\n    assert x.is_negative is None\n    assert x.is_zero is None\n    assert x.is_nonzero is None\n    assert x.is_irrational is None\n    assert x.is_finite is None\n    assert x.is_infinite is None\n    assert x.is_commutative is True\n    assert x.is_prime is None\n    assert x.is_composite is None\n", "def test_symbol_nonzero():\n    x = Symbol('x', nonzero=True)\n    assert x.is_zero is False\n    assert x.is_nonzero is True\n    assert x.is_real is None\n    assert x.is_imaginary is None\n    assert x.is_positive is None\n    assert x.is_negative is None\n    assert x.is_finite is None\n    assert x.is_infinite is None\n"], "sample_1048": ["def test_parabola_axis_of_symmetry():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    p2 = Parabola(Point(3, 7), Line(Point(7, 6), Point(3, 6)))\n    p3 = Parabola(Point(6, 0), Line(Point(4, 0), Point(4, 9)))\n\n    assert p1.axis_of_symmetry == Line(Point(0, 0), Point(0, 1))\n    assert p2.axis_of_symmetry == Line(Point(3, 7), Point(4, 7))\n    assert p3.axis_of_symmetry == Line(Point(6, 0), Point(6, 1))\n", "def test_parabola_intersections():\n    # Test intersection with a line\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    l1 = Line(Point(1, -2), Point(-1, -2))\n    assert p1.intersection(l1) == [Point2D(-4, -2), Point2D(4, -2)]\n    \n    # Test intersection with another parabola\n    p2 = Parabola(Point(0, 0), Line(Point(6, 9), Point(8, 9)))\n    assert p1.intersection(p2) == [Point2D(0, 0)]\n    \n    # Test intersection with an ellipse\n    e1 = Ellipse(Point(0, 0), 10, 5)\n    assert p1.intersection(e1) == [Point2D(-10, 0), Point2D(10, 0)]\n    \n    # Test intersection with a segment\n    s1 = Segment2D(Point(-5, -1), Point(5, -1))\n    assert p1.intersection(s1) == [Point2D(-4, -1), Point2D(4, -1)]\n    \n    # Test intersection with a ray\n    r1 = Ray2D(Point(-4, 2), Point(4, 2))\n    assert p1.intersection(r1) == [Point2D(-4, 2), Point2D(4, 2)]\n    \n    # Test intersection with a point\n    pt = Point2D(0, 0)\n    assert p1.intersection(pt) == [pt]\n    \n    # Test intersection with a 3D linear entity\n    l3d = Line(Point(0, 0, 0), Point(1, 1, 1))\n    raises(TypeError, lambda: p1.intersection(l3d))\n    \n    # Test intersection with an unsupported type\n    raises(TypeError, lambda: p1.intersection(\"unsupported_type\"))\n", "def test_axis_of_symmetry():\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n    \n    pa1 = Parabola(p1, d1)\n    pa2 = Parabola(p2, d2)\n    \n    assert pa1.axis_of_symmetry == Line(Point(0, 0), Point(0, 1))\n    assert pa2.axis_of_symmetry == Line(Point(3, 7), Point(3, 8))\n", "def test_parabola_axis_of_symmetry():\n    p1 = Point(0, 0)\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n\n    pa1 = Parabola(p1, d1)\n    pa2 = Parabola(p1, d2)\n\n    # Test axis of symmetry for vertical directrix\n    assert pa1.axis_of_symmetry == Line(Point(0, 0), Point(0, 1))\n\n    # Test axis of symmetry for horizontal directrix\n    assert pa2.axis_of_symmetry == Line(Point(0, 0), Point(1, 0))\n", "def test_parabola_axis_of_symmetry():\n    # Test horizontal directrix\n    focus = Point(0, 0)\n    directrix = Line(Point(1, -2), Point(-1, -2))\n    parabola = Parabola(focus, directrix)\n    assert parabola.axis_of_symmetry == Line(Point(0, 0), Point(0, 1))\n\n    # Test vertical directrix\n    directrix = Line(Point(3, 1), Point(3, 2))\n    parabola = Parabola(focus, directrix)\n    assert parabola.axis_of_symmetry == Line(Point(0, 0), Point(1, 0))\n\n    # Test symbolic axis of symmetry\n    a = symbols('a')\n    focus = Point(a, 0)\n    directrix = Line(Point(a + 2, 1), Point(a + 2, 2))\n    parabola = Parabola(focus, directrix)\n    assert parabola.axis_of_symmetry == Line(Point(a, 0), Point(a, 1))\n\n    # Test axis of symmetry using different focus\n    focus = Point(3, 5)\n    directrix = Line(Point(1, 1), Point(5, 1))\n    parabola = Parabola(focus, directrix)\n    assert parabola.axis_of_symmetry == Line(Point(3, 5), Point(3, 6))\n"], "sample_1049": ["def test_plane_parameter_value():\n    x, y, z, u, v = symbols('x y z u v', real=True)\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n    p3 = Point3D(1, 2, 3)\n    pl = Plane(p1, p2, p3)\n\n    pt_on_circle = pl.arbitrary_point(u).subs(u, pi/4)\n    param_value = pl.parameter_value(pt_on_circle, u)\n    assert param_value == {u: pi/4}\n\n    pt_outside_circle = pl.p1 + (pt_on_circle - pl.p1) * 2\n    param_value_outside = pl.parameter_value(pt_outside_circle, u)\n    assert param_value_outside == {u: pi/4}\n\n    pt_arbitrary = pl.arbitrary_point(u, v).subs({u: 1, v: 2})\n    param_value_arbitrary = pl.parameter_value(pt_arbitrary, u, v)\n    assert param_value_arbitrary == {u: 1, v: 2}\n\n    raises(ValueError, lambda: pl.parameter_value(Point3D(10, 10, 10), u))\n    raises(ValueError, lambda: pl.parameter_value(pt_on_circle, u, pi))\n", "def test_plane_are_concurrent():\n    # Test for Plane.are_concurrent method\n    p1 = Plane(Point3D(1, 0, 0), normal_vector=(1, -1, 1))\n    p2 = Plane(Point3D(0, -2, 0), normal_vector=(3, 1, 1))\n    p3 = Plane(Point3D(0, -1, 0), normal_vector=(5, -1, 9))\n    p4 = Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))\n    p5 = Plane(Point3D(2, 2, 2), normal_vector=(2, 2, 2))\n    p6 = Plane(Point3D(3, 3, 3), normal_vector=(-1, -1, -1))\n\n    assert Plane.are_concurrent(p1, p2) is True\n    assert Plane.are_concurrent(p1, p2, p3) is False\n    assert Plane.are_concurrent(p4, p5) is True\n    assert Plane.are_concurrent(p4, p5, p6) is True\n    assert Plane.are_concurrent(p1, p2, p4) is False\n", "def test_plane_parallel_plane():\n    p1 = Point3D(1, 2, 3)\n    normal_vector = (2, 3, 4)\n    plane1 = Plane(p1, normal_vector=normal_vector)\n    point = Point3D(4, 5, 6)\n    \n    parallel_plane = plane1.parallel_plane(point)\n    \n    assert parallel_plane.p1 == point\n    assert parallel_plane.normal_vector == normal_vector\n    assert parallel_plane.is_parallel(plane1)\n    assert not parallel_plane.is_perpendicular(plane1)\n\n    # Edge case: the point provided is the same as the defining point of the original plane\n    parallel_plane_same_point = plane1.parallel_plane(p1)\n    assert parallel_plane_same_point.p1 == p1\n    assert parallel_plane_same_point.normal_vector == normal_vector\n    assert parallel_plane_same_point.is_parallel(plane1)\n    assert not parallel_plane_same_point.is_perpendicular(plane1)\n", "def test_plane_random_point():\n    # Ensure that a random point lies on the plane\n    pl3 = Plane(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(1, 2, 3))\n    pt = pl3.random_point()\n    assert pt in pl3\n", "def test_plane_parameter_value():\n    p1 = Point3D(1, 1, 1)\n    normal_vector = (1, 1, 1)\n    plane = Plane(p1, normal_vector=normal_vector)\n\n    # Test with single parameter\n    param = symbols('t', real=True)\n    pt_on_circle = plane.arbitrary_point(param).subs(param, pi/4)\n    assert plane.parameter_value(pt_on_circle, param) == {param: pi/4}\n\n    # Test with two parameters\n    u, v = symbols('u v', real=True)\n    pt_on_plane = plane.arbitrary_point(u, v).subs({u: sqrt(2), v: sqrt(3)})\n    assert plane.parameter_value(pt_on_plane, u, v) == {u: sqrt(2), v: sqrt(3)}\n\n    # Test when the point is the defining point of the plane\n    assert plane.parameter_value(p1, param) == p1\n    assert plane.parameter_value(p1, u, v) == p1\n\n    # Test with a point not on the plane\n    p_not_on_plane = Point3D(2, 2, 4)\n    raises(ValueError, lambda: plane.parameter_value(p_not_on_plane, param))\n    raises(ValueError, lambda: plane.parameter_value(p_not_on_plane, u, v))\n"], "sample_1050": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.doprint(sign(x)) == '(0.0 if x == 0 else sympy.copysign(1, x))'\n    assert p.doprint(Piecewise((1, Eq(x, 0)),\n                               (2, x > 6))) == 'sympy.Piecewise((1, sympy.Eq(x, 0)), (2, x > 6))'\n", "def test_AbstractPythonCodePrinter():\n    prntr = AbstractPythonCodePrinter()\n    assert prntr.doprint(S.Pi) == 'math.pi'\n    assert prntr.doprint(S.Exp1) == 'math.e'\n    assert prntr.doprint(S.Infinity) == \"float('inf')\"\n    assert prntr.doprint(S.NegativeInfinity) == \"float('-inf')\"\n    assert prntr.doprint(S.ComplexInfinity) == \"float('nan')\"\n    assert prntr.doprint(S.NaN) == \"float('nan')\"\n    assert prntr.doprint(Sqrt(x)) == 'math.sqrt(x)'\n    assert prntr.doprint(cos(x)) == 'math.cos(x)'\n    assert prntr.doprint(Abs(x)) == 'abs(x)'\n    assert prntr.doprint(Mod(x, 3)) == 'x % 3'\n", "def test_SymPyPrinter():\n    prntr = SymPyPrinter()\n    assert prntr.doprint(acos(x)) == 'sympy.acos(x)'\n    assert prntr.doprint(sign(x)) == 'sympy.sign(x)'\n    assert prntr.doprint(Mod(x, 2)) == 'x % 2'\n    assert prntr.doprint(And(x, y)) == 'x and y'\n    assert prntr.doprint(Or(x, y)) == 'x or y'\n    assert prntr.doprint(pi) == 'sympy.pi'\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)), (2, x > 6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert prntr.doprint(sign(x)) == '(0.0 if x == 0 else sympy.copysign(1, x))'\n", "def test_AbstractPythonCodePrinter():\n    prntr = AbstractPythonCodePrinter()\n    assert prntr._declare_number_const('a', 5) == \"a = 5\"\n    assert prntr._module_format('math.sin') == 'math.sin'\n    assert prntr._module_format('numpy.sin') == 'numpy.sin'\n    assert prntr._get_statement('a = 5') == 'a = 5'\n    assert prntr._get_comment('This is a comment') == '  # This is a comment'\n    assert prntr._expand_fold_binary_op('add', [x, y, z]) == 'add(add(x, y), z)'\n    assert prntr._expand_reduce_binary_op('add', [x, y, z, a]) == 'add(add(x, y), add(z, a))'\n    assert prntr._get_einsum_string([2, 2], [(0, 2), (1, 3)]) == ('abcd,cdab', ['a', 'b'], ['c', 'd'])\n\n    expr = Piecewise((1, x > 0), (0, x <= 0))\n    assert prntr._print_Piecewise(expr) == '((1) if (x > 0) else (0) if (x <= 0) else None)'\n    assert prntr._print_NaN(expr) == \"float('nan')\"\n    assert prntr._print_Infinity(expr) == \"float('inf')\"\n    assert prntr._print_NegativeInfinity(expr) == \"float('-inf')\"\n    assert prntr._print_ComplexInfinity(expr) == \"float('nan')\"\n    assert prntr._print_Mod(Mod(x, 2)) == 'x % 2'\n    assert prntr._print_Relational(Eq(x, y)) == '(x == y)'\n    assert prntr._print_ITE(expr) == '((1) if (x > 0) else (0) if (x <= 0) else None)'\n    assert prntr._print_Sum(x + y) == '(builtins.sum(x + y for x in range(0, 1)))'\n    assert prntr._print_ImaginaryUnit(expr) == '1j'\n    assert prntr._print_MatrixBase(MatrixSymbol('", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    expr = Piecewise((1, Eq(x, 0)), (2, x > 6))\n    assert p.doprint(expr) == 'sympy.Piecewise((1, Eq(x, 0)), (2, x > 6))'\n    assert p.doprint(p[0, 1]) == 'p[0, 1]'\n"], "sample_1051": ["def test_custom_style():\n    custom_styles = [(Basic, {'color': 'green', 'shape': 'box'}),\n                     (Expr,  {'color': 'red'})]\n    assert dotnode(x, styles=custom_styles, repeat=False) == \\\n            '\"Symbol(\\'x\\')\" [\"color\"=\"red\", \"label\"=\"x\", \"shape\"=\"box\"];'\n    assert dotnode(x+2, styles=custom_styles, repeat=False) == \\\n            '\"Add(Integer(2), Symbol(\\'x\\'))\" [\"color\"=\"red\", \"label\"=\"Add\", \"shape\"=\"box\"];'\n    \n    text = dotprint(x+2, styles=custom_styles, repeat=False)\n    assert '\"Symbol(\\'x\\')\" [\"color\"=\"red\", \"label\"=\"x\", \"shape\"=\"box\"];' in text\n    assert '\"Add(Integer(2), Symbol(\\'x\\'))\" [\"color\"=\"red\", \"label\"=\"Add\", \"shape\"=\"box\"];' in text\n", "def test_custom_styles():\n    custom_styles = [\n        (Basic, {'color': 'red', 'shape': 'box'}),\n        (Expr, {'color': 'green'}),\n        (Symbol, {'color': 'yellow', 'shape': 'diamond'})\n    ]\n    assert dotnode(x, styles=custom_styles, repeat=False) ==\\\n        '\"Symbol(\\'x\\')\" [\"color\"=\"yellow\", \"label\"=\"x\", \"shape\"=\"diamond\"];'\n    assert dotnode(x + 1, styles=custom_styles, repeat=False) ==\\\n        '\"Add(Integer(1), Symbol(\\'x\\'))\" [\"color\"=\"green\", \"label\"=\"Add\", \"shape\"=\"box\"];'\n    assert dotnode(x**2, styles=custom_styles, repeat=False) ==\\\n        '\"Pow(Symbol(\\'x\\'), Integer(2))\" [\"color\"=\"green\", \"label\"=\"Pow\", \"shape\"=\"box\"];'\n", "def test_custom_styles():\n    custom_styles = [(Symbol, {'color': 'green', 'shape': 'box'}),\n                     (Integer, {'color': 'red', 'shape': 'diamond'})]\n    assert dotnode(x, styles=custom_styles, repeat=False) == \\\n        '\"Symbol(\\'x\\')\" [\"color\"=\"green\", \"label\"=\"x\", \"shape\"=\"box\"];'\n    assert dotnode(Integer(2), styles=custom_styles, repeat=False) == \\\n        '\"Integer(2)\" [\"color\"=\"red\", \"label\"=\"2\", \"shape\"=\"diamond\"];'\n    assert dotnode(x + 2, styles=custom_styles, repeat=False) == \\\n        '\"Add(Integer(2), Symbol(\\'x\\'))\" [\"color\"=\"black\", \"label\"=\"Add\", \"shape\"=\"ellipse\"];'\n", "def test_custom_styles():\n    custom_styles = [(Basic, {'color': 'green', 'shape': 'box'}),\n                     (Expr,  {'color': 'red'})]\n    assert dotnode(x, styles=custom_styles, repeat=False) == \\\n           '\"Symbol(\\'x\\')\" [\"color\"=\"red\", \"label\"=\"x\", \"shape\"=\"box\"];'\n    assert dotnode(x + 1, styles=custom_styles, repeat=False) == \\\n           '\"Add(Integer(1), Symbol(\\'x\\'))\" [\"color\"=\"red\", \"label\"=\"Add\", \"shape\"=\"box\"];'\n    assert dotnode(Float(3.14), styles=custom_styles, repeat=False) == \\\n           '\"Float(\\'3.14\\', precision=53)\" [\"color\"=\"red\", \"label\"=\"3.14\", \"shape\"=\"box\"];'\n", "def test_custom_styles():\n    custom_styles = [(Basic, {'color': 'red', 'shape': 'box'}),\n                     (Expr, {'color': 'green'})]\n    # Testing custom styles with Basic object\n    assert styleof(Basic(1), custom_styles) == {'color': 'red', 'shape': 'box'}\n\n    # Testing custom styles with expression\n    assert styleof(x + 1, custom_styles) == {'color': 'green', 'shape': 'box'}\n"], "sample_1052": ["def test_julia_codegen():\n    x, y = symbols('x y')\n    expr = x + y\n    name_expr = (\"test\", expr)\n    result = codegen(name_expr, \"julia\", \"file\", header=False, empty=False)\n    expected = [\n        (\"file.jl\",\n        \"#\\n\"\n        \"#   Code generated with sympy {}\\n\"\n        \"#\\n\"\n        \"#   See http://www.sympy.org/ for more information.\\n\"\n        \"#\\n\"\n        \"#   This file is part of 'project'\\n\"\n        \"#\\n\"\n        \"function test(x, y)\\n\"\n        \"    test_result = x + y\\n\"\n        \"    return test_result\\n\"\n        \"end\\n\".format(sympy_version))\n    ]\n    assert result == expected\n", "def test_routine_variables_property():\n    x, y, z = symbols('x y z')\n    expr = (x + y) * z\n    routine = make_routine(\"test\", expr)\n    assert set(routine.variables) == {x, y, z, Symbol('test_result')}\n\n    # Test with local variables\n    exprs = [Eq(Symbol('a'), x + y), Eq(Symbol('b'), y - z)]\n    routine = make_routine(\"test\", exprs)\n    assert set(routine.variables) == {x, y, z, Symbol('a'), Symbol('b')}\n\n    # Test with global variables\n    routine = make_routine(\"test\", expr, global_vars=[z])\n    assert set(routine.variables) == {x, y, z, Symbol('test_result')}\n", "def test_rust_codegen():\n    x, y, z = symbols('x,y,z')\n    expr = (x + y)*z\n    routine = make_routine(\"test\", expr)\n    code_gen = RustCodeGen()\n    source = get_string(code_gen.dump_rs, [routine])\n    expected = (\n        \"fn test(x: f64, y: f64, z: f64) -> f64 {\\n\"\n        \"   let test_result = z*(x + y);\\n\"\n        \"   test_result\\n\"\n        \"}\\n\"\n    )\n    assert source == expected\n", "def test_rust_codegen():\n    from sympy import sin, cos\n    x, y, z = symbols('x,y,z')\n    expr1 = sin(x) + cos(y)\n    expr2 = (x + y)*z\n    routine = make_routine(\"test_rust\", [expr1, expr2])\n    code_gen = RustCodeGen()\n    source = get_string(code_gen.dump_rs, [routine])\n    expected = (\n        \"fn test_rust(x: f64, y: f64, z: f64) -> (f64, f64) {\\n\"\n        \"    let test_rust_result_0 = x + y;\\n\"\n        \"    let test_rust_result_1 = sin(x) + cos(y);\\n\"\n        \"    (test_rust_result_1, test_rust_result_0)\\n\"\n        \"}\\n\"\n    )\n    assert source == expected\n", "def test_rust_codegen():\n    x, y, z = symbols('x,y,z')\n    expr = (x + y) * z\n    routine = make_routine(\"test\", expr)\n    code_gen = RustCodeGen()\n    source = get_string(code_gen.dump_rs, [routine])\n    expected = (\n        \"fn test(x: f64, y: f64, z: f64) -> f64 {\\n\"\n        \"   let test_result;\\n\"\n        \"   test_result = z*(x + y);\\n\"\n        \"   test_result\\n\"\n        \"}\\n\"\n    )\n    assert source == expected\n"], "sample_1053": ["def test_mpf_norm_special_cases():\n    # Special case: mantissa is zero but bc is non-zero, indicating special values\n    mpf_inf = (0, 0, 0, 1)  # This should represent +inf\n    assert mpf_norm(mpf_inf, 53) == _mpf_inf\n\n    mpf_ninf = (1, 0, 0, 1)  # This should represent -inf\n    assert mpf_norm(mpf_ninf, 53) == _mpf_ninf\n\n    mpf_nan = (0, 0, 0, 2)  # This should represent NaN\n    assert mpf_norm(mpf_nan, 53) == _mpf_nan\n", "def test_issue_12345():\n    # Test comp function with different tolerance levels\n    assert comp(1.0001, 1.0002, tol=0.0002)\n    assert not comp(1.0001, 1.0003, tol=0.0001)\n    assert comp(1.0001, 1.0001, tol=0.0001)\n    \n    # Test mpf_norm with tuples having mantissa zero but different exponents\n    assert mpf_norm((1, 0, 10, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((0, 0, 1, 1), 10) == (0, 0, 0, 0)\n    \n    # Test mod_inverse with zero cases\n    assert mod_inverse(S(5), S(1)) == 0\n    assert mod_inverse(S(7), S(-1)) == -1\n    \n    # Test mod_inverse with large integers\n    assert mod_inverse(2**60 + 1, 2**62 + 7) == 576460752303423489\n    assert mod_inverse(2**60 + 1, -2**62 - 7) == -576460752303423489\n", "def test_mpf_norm():\n    assert mpf_norm(_mpf_zero, 10) == _mpf_zero\n    assert mpf_norm(_mpf_inf, 10) == _mpf_inf\n    assert mpf_norm(_mpf_ninf, 10) == _mpf_ninf\n    assert mpf_norm(_mpf_nan, 10) == _mpf_nan\n    assert mpf_norm((1, 0, 1, 0), 10) == _mpf_zero\n    assert mpf_norm((1, 0, 1, 0), 53) == _mpf_zero\n    assert mpf_norm((0, 1, 10, 1), 10) == (0, MPZ(1), 10, 1)\n    assert mpf_norm((0, MPZ(1), 10, 1), 10) == (0, MPZ(1), 10, 1)\n    assert mpf_norm((1, MPZ(1), 10, 1), 10) == (1, MPZ(1), 10, 1)\n    assert mpf_norm((0, 1, 10, 0), 10) == _mpf_zero\n    assert mpf_norm((1, 1, 10, 0), 10) == _mpf_zero\n    assert mpf_norm((1, 1, 10, 1), 10) == (1, MPZ(1), 10, 1)\n    assert mpf_norm((0, MPZ(0), 0, 0), 10) == _mpf_zero\n    assert mpf_norm((1, MPZ(0), 0, 0), 10) == _mpf_zero\n    assert mpf_norm((0, MPZ(1), 0, 0), 10) == (0, MPZ(1), 0, 0)\n    assert mpf_norm((1, MPZ(1), 0, 0), 10) == (1, MPZ(1), 0, 0)\n", "def test_mpf_norm():\n    # Test positive and negative finite values\n    assert mpf_norm((0, 12345, 3, 7), 10) == (0, 12345, 3, 7)\n    assert mpf_norm((1, 12345, 3, 7), 10) == (1, 12345, 3, 7)\n    \n    # Test zero mantissa with non-zero exponent and sign\n    assert mpf_norm((0, 0, 3, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((1, 0, 3, 0), 10) == (0, 0, 0, 0)\n    \n    # Test normalization of large mantissa\n    assert mpf_norm((0, 1234567890, 3, 10), 20) == (0, 1234567890, 3, 10)\n    \n    # Test normalization of small mantissa\n    assert mpf_norm((0, 1, -100, 1), 5) == (0, 1, -100, 1)\n    \n    # Test normalization for special values\n    from mpmath.libmp.libmpf import finf, fninf, fnan\n    assert mpf_norm(finf, 10) == finf\n    assert mpf_norm(fninf, 10) == fninf\n    assert mpf_norm(fnan, 10) == fnan\n", "def test_mpf_norm_special_cases():\n    # Test special cases for mpf_norm function\n    assert mpf_norm(_mpf_inf, 10) == _mpf_inf\n    assert mpf_norm(_mpf_ninf, 10) == _mpf_ninf\n    assert mpf_norm(_mpf_nan, 10) == _mpf_nan\n\n    # Test with a valid mpf tuple\n    mpf_tuple = (0, 1, 1, 1)\n    assert mpf_norm(mpf_tuple, 10) == mpf_tuple\n"], "sample_1054": ["def test_Range_contains():\n    assert 5 in Range(10)\n    assert -5 not in Range(10)\n    assert 10 not in Range(10)\n    assert 0 in Range(0)\n    assert 0 in Range(1, 10, 2)\n    assert 2 in Range(1, 10, 2)\n    assert 3 not in Range(1, 10, 2)\n    assert -2 in Range(-10, 0, 2)\n    assert -1 not in Range(-10, 0, 2)\n    assert 2 in Range(-10, 10, 4)\n    assert -2 in Range(-10, 10, 4)\n    assert 1 not in Range(-10, 10, 4)\n    assert 2 in Range(2, 10, 2)\n    assert 1 not in Range(2, 10, 2)\n    assert 2 in Range(2, -10, -2)\n    assert -2 in Range(2, -10, -2)\n    assert 1 not in Range(2, -10, -2)\n    assert -2 in Range(-2, 10, 4)\n    assert 0 in Range(-2, 10, 4)\n    assert 4 in Range(-2, 10, 4)\n    assert 1 not in Range(-2, 10, 4)\n    assert -1 in Range(-2, 10, 1)\n    assert -1 not in Range(2, 10, -1)\n    assert 2 not in Range(2, 10, -2)\n    assert -2 not in Range(2, 10, -2)\n", "def test_ComplexRegion_polar_contains_edge_cases():\n    r = Interval(0, 1)\n    theta = Interval(0, 2 * S.Pi)\n\n    c = ComplexRegion(r * theta, polar=True)\n\n    # Test edge cases for ComplexRegion in polar form\n    assert 0 in c\n    assert 1 in c\n    assert -1 not in c\n    assert I in c\n    assert -I in c\n    assert 0.5 + 0.5*I in c\n    assert 1 + I not in c\n    assert 0.7071067811865475 + 0.7071067811865475*I in c  # cos(pi/4) + I*sin(pi/4)\n    assert -0.7071067811865475 - 0.7071067811865475*I in c  # cos(5*pi/4) + I*sin(5*pi/4)\n\n    # Test boundary conditions\n    assert 1 * exp(I * 2 * S.Pi) in c  # boundary theta = 2*pi\n    assert 1 * exp(I * S.Pi) in c  # boundary theta = pi\n    assert 1 * exp(I * S.Pi/2) in c  # boundary theta = pi/2\n    assert 1 * exp(I * 3 * S.Pi/2) in c  # boundary theta = 3*pi/2\n    assert 0 in c  # origin\n\n    # Test values out of range\n    assert 1.5 + 0.5*I not in c\n    assert -0.5 + 1.5*I not in c\n", "def test_Range_contains():\n    assert 5 in Range(0, 10)\n    assert -5 not in Range(0, 10)\n    assert 0 in Range(0, 10)\n    assert 9 in Range(0, 10)\n    assert 10 not in Range(0, 10)\n    assert 5 in Range(0, 10, 2)\n    assert 6 not in Range(0, 10, 2)\n    assert 8 in Range(0, 10, 2)\n    assert 2 in Range(-10, 10)\n    assert -10 in Range(-10, 10)\n    assert -11 not in Range(-10, 10)\n    assert 10 not in Range(-10, 10)\n    assert -10 in Range(-10, 10, 2)\n    assert -9 not in Range(-10, 10, 2)\n    assert 8 in Range(-10, 10, 2)\n    assert 9 not in Range(-10, 10, 2)\n    assert 0 in Range(0, oo, 1)\n    assert oo not in Range(0, oo, 1)\n    assert 1000000 in Range(0, oo, 100)\n", "def test_complexregion_real_imaginary():\n    # Rectangular form\n    c1 = ComplexRegion(Interval(1, 3)*Interval(4, 6))\n    assert c1.a_interval == Interval(1, 3)\n    assert c1.b_interval == Interval(4, 6)\n    assert (2 + 5*I) in c1\n    assert (1 + 4*I) in c1\n    assert (3 + 6*I) in c1\n    assert (1 + 7*I) not in c1\n    assert (4 + 5*I) not in c1\n\n    # Polar form\n    c2 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    assert c2.a_interval == Interval(0, 1)\n    assert c2.b_interval == Interval(0, 2*S.Pi)\n    assert (0.5 + 0.5*I) in c2\n    assert (1 + 0*I) in c2\n    assert (0 + 1*I) in c2\n    assert (1 + 1*I) not in c2\n    assert (1 - 1*I) not in c2\n", "def test_integers_intersection():\n    Z = S.Integers\n    assert Z.intersect(S.Naturals) == S.Naturals\n    assert Z.intersect(S.Naturals0) == S.Naturals0\n    assert Z.intersect(S.Reals) == S.Integers\n    assert Z.intersect(Interval(0, 10)) == Range(0, 11)\n    assert Z.intersect(Interval(-10, 0)) == Range(-10, 1)\n    assert Z.intersect(FiniteSet(1, 2, 3.5, 4)) == FiniteSet(1, 2, 4)\n"], "sample_1055": ["def test_encipher_decipher_bifid():\n    key = \"goldbug\"\n    symbols = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    msg = \"HELLO\"\n    ciphertext = encipher_bifid(msg, key, symbols)\n    assert decipher_bifid(ciphertext, key, symbols) == msg\n\n    key = \"secret\"\n    symbols = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    msg = \"SECRET MESSAGE\"\n    ciphertext = encipher_bifid(msg, key, symbols)\n    assert decipher_bifid(ciphertext, key, symbols) == msg.replace(\" \", \"\")\n\n    key = \"keywithnumbers123\"\n    symbols = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n    msg = \"NUMBERS123\"\n    ciphertext = encipher_bifid(msg, key, symbols)\n    assert decipher_bifid(ciphertext, key, symbols) == msg\n", "def test_bifid10_cipher():\n    key = 'goldbug'\n    alp = bifid10\n    msg = 'meet me at 8pm'\n    enc = encipher_bifid(msg, key, alp)\n    assert enc == 'QFRQNRQOCY5'\n    dec = decipher_bifid(enc, key, alp)\n    assert dec == 'MEETMEAT8PM'\n    key = 'secret'\n    msg = 'encryption test 123'\n    enc = encipher_bifid(msg, key, alp)\n    assert enc == 'XOJZTXJZMYCTK1A2A3'\n    dec = decipher_bifid(enc, key, alp)\n    assert dec == 'ENCRYPTIONTEST123'\n", "def test_decipher_bifid():\n    key = \"gold bug\"\n    alp = AZ().replace('J', '')\n    msg = AZ(\"meet me on monday!\")\n    ct = encipher_bifid(msg, key, alp)\n    assert ct == \"IEILHHFSTSFQYE\"\n    assert decipher_bifid(ct, key, alp) == \"MEETMEONMONDAY\"\n    \n    key = \"\"\n    msg = \"hello world!\"\n    ct = encipher_bifid(msg, key)\n    assert ct == \"bmtwmg-bIo*w\"\n    assert decipher_bifid(ct, key) == \"hello world!\"\n    \n    key = \"gold bug\"\n    msg = \"hello world!\"\n    ct = encipher_bifid(msg, key)\n    assert ct == \"hg2sfuei7t}w\"\n    assert decipher_bifid(ct, key) == \"heldo~wor6d!\"\n\n    # Ensure changing key gives different decryption\n    assert decipher_bifid(ct, 'gold pug') != \"hello world!\"\n", "def test_check_and_join_with_invalid_symbols():\n    raises(ValueError, lambda: check_and_join('hello', 'aeiou'))\n    raises(ValueError, lambda: check_and_join('12345', '135', filter=False))\n    assert check_and_join('hello world', 'helo wrd', filter=True) == 'hello world'\n    assert check_and_join('12345', '135', filter=True) == '135'\n", "def test_elgamal_public_key():\n    pri = elgamal_private_key(digit=100)\n    pub = elgamal_public_key(pri)\n    p, r, e = pub\n    assert isprime(p)\n    assert is_primitive_root(r, p)\n    assert isinstance(e, int)\n    assert 1 < e < p\n"], "sample_1056": ["def test_boolean_ops():\n    from sympy import And, Or, Not\n\n    assert lambdarepr(And(x, y)) == \"((x) and (y))\"\n    assert lambdarepr(Or(x, y)) == \"((x) or (y))\"\n    assert lambdarepr(Not(x)) == \"(not (x))\"\n    assert lambdarepr(And(x, y, z)) == \"((x) and (y) and (z))\"\n    assert lambdarepr(Or(x, y, z)) == \"((x) or (y) or (z))\"\n", "def test_boolean_operations():\n    from sympy import And, Or, Not\n    assert lambdarepr(And(x, y, z)) == \"((x) and (y) and (z))\"\n    assert lambdarepr(Or(x, y, z)) == \"((x) or (y) or (z))\"\n    assert lambdarepr(Not(x)) == \"(not (x))\"\n", "def test_logical_operations():\n    from sympy import And, Or, Not, true, false\n\n    expr = And(x > 1, y < 2)\n    l = lambdarepr(expr)\n    eval(\"lambda x, y: \" + l)\n    assert l == \"(((x > 1)) and ((y < 2)))\"\n\n    expr = Or(x > 1, y < 2)\n    l = lambdarepr(expr)\n    eval(\"lambda x, y: \" + l)\n    assert l == \"(((x > 1)) or ((y < 2)))\"\n\n    expr = Not(x > 1)\n    l = lambdarepr(expr)\n    eval(\"lambda x: \" + l)\n    assert l == \"(not ((x > 1)))\"\n\n    expr = true\n    l = lambdarepr(expr)\n    eval(\"lambda: \" + l)\n    assert l == \"True\"\n\n    expr = false\n    l = lambdarepr(expr)\n    eval(\"lambda: \" + l)\n    assert l == \"False\"\n", "def test_boolean_logic():\n    assert lambdarepr(x & y) == \"((x) and (y))\"\n    assert lambdarepr(x | y) == \"((x) or (y))\"\n    assert lambdarepr(~x) == \"(not (x))\"\n    assert lambdarepr(True) == \"True\"\n    assert lambdarepr(False) == \"False\"\n", "def test_boolean_expressions():\n    from sympy import And, Or, Not, true, false\n    assert lambdarepr(And(x > 1, y < 2)) == \"((x > 1) and (y < 2))\"\n    assert lambdarepr(Or(x > 1, y < 2)) == \"((x > 1) or (y < 2))\"\n    assert lambdarepr(Not(x > 1)) == \"(not (x > 1))\"\n    assert lambdarepr(true) == \"True\"\n    assert lambdarepr(false) == \"False\"\n"], "sample_1057": ["def test_render_as_module():\n    code_content = Print(\"Hello World\")\n    result = render_as_module(code_content, standard='python3')\n    expected_import = 'from sympy.codegen.ast import Print'\n    expected_code = 'Print(\"Hello World\")'\n    assert expected_import in result\n    assert expected_code in result\n", "def test_render_as_module():\n    content = Print(\"Hello, World!\")\n    result = render_as_module(content)\n    assert \"import sympy\" in result\n    assert \"print('Hello, World!')\" in result\n", "def test_render_as_module():\n    from sympy import symbols\n    x, y = symbols('x y')\n    content = Print(x + y)\n    result = render_as_module(content)\n    \n    # Check if the result contains the expected import statements and the printed content\n    assert 'from sympy import symbols' in result\n    assert 'from sympy.codegen.ast import Print' in result\n    assert 'Print(x + y)' in result\n", "def test_render_as_module():\n    code = Print(\"Hello, World!\")\n    rendered_module = render_as_module(code)\n    \n    # Check if the import statement is correct\n    assert \"from sympy.codegen.ast import Print\" in rendered_module\n    \n    # Check if the code content is correct\n    assert \"Print('Hello, World!')\" in rendered_module\n", "def test_render_as_module():\n    from sympy import symbols\n    x, y = symbols('x y')\n    content = Print(x + y)\n    result = render_as_module(content)\n    assert 'from sympy import symbols' in result\n    assert 'x + y' in result\n"], "sample_1058": ["def test_print_FunctionDefinition():\n    from sympy.codegen.ast import FunctionDefinition, Variable, Return\n    from sympy import symbols\n\n    x, y = symbols('x y')\n    f_def = FunctionDefinition('foo', [Variable(x), Variable(y)], [Return(x + y)])\n    prntr = PythonCodePrinter()\n\n    expected_output = \"def foo(x, y):\\n    return x + y\"\n    assert prntr.doprint(f_def) == expected_output\n", "def test_AbstractPythonCodePrinter_indentation():\n    prntr = PythonCodePrinter()\n    code_str = \"if x > 1:\\nprint(x)\\nelse:\\nprint(-x)\"\n    expected_output = \"    if x > 1:\\n        print(x)\\n    else:\\n        print(-x)\"\n    assert prntr._indent_codestring(code_str) == expected_output\n", "def test_operators():\n    prntr = PythonCodePrinter()\n    \n    assert prntr._print_Not(x) == 'not x'\n    assert prntr._print_Mod(x % y) == 'x % y'\n    assert prntr._print_Relational(Eq(x, y)) == '(x == y)'\n    assert prntr._print_Relational(Le(x, y)) == '(x <= y)'\n    assert prntr._print_Relational(Gt(x, y)) == '(x > y)'\n    assert prntr._print_ITE(Piecewise((1, Eq(x, 0)), (2, True))) == '((1) if (x == 0) else (2))'\n    assert prntr._print_Sum(Assignment(x, 2)) == 'builtins.sum(x = 2 for i in range(0, 1))'\n    assert prntr._print_ImaginaryUnit(S(1j)) == '1j'\n    assert prntr._print_Half(Rational(1, 2)) == '1/2'\n", "def test_function_definitions():\n    from sympy.codegen.ast import FunctionDefinition, Return, Variable\n    var_x = Variable(symbols('x'), dtype='int')\n    func_def = FunctionDefinition('my_function', [var_x], [Return(var_x)])\n    prntr = PythonCodePrinter()\n    expected_output = \"def my_function(x):\\n    return x\"\n    assert prntr.doprint(func_def) == expected_output\n", "def test_AbstractPythonCodePrinter_methods():\n    prntr = AbstractPythonCodePrinter()\n\n    # Test _declare_number_const\n    assert prntr._declare_number_const(\"CONSTANT\", 3.14) == \"CONSTANT = 3.14\"\n\n    # Test _module_format\n    assert prntr._module_format(\"math.sin\") == \"math.sin\"\n    assert prntr._module_format(\"math.sin\", register=False) == \"sin\"\n    assert prntr.module_imports == {'math': {'sin'}}\n\n    # Test _expand_fold_binary_op\n    assert prntr._expand_fold_binary_op(\"add\", [x, y, z]) == \"add(add(x, y), z)\"\n\n    # Test _expand_reduce_binary_op\n    assert prntr._expand_reduce_binary_op(\"add\", [x, y, z, 1]) == \"add(add(x, y), add(z, 1))\"\n\n    # Test _get_einsum_string\n    contraction_string, letters_free, letters_dum = prntr._get_einsum_string([2, 2], [(0, 1), (2, 3)])\n    assert contraction_string == \"ii,jj\"\n    assert letters_free == []\n    assert letters_dum == [\"i\", \"j\"]\n\n    # Test _print_FunctionDefinition\n    from sympy.codegen.ast import FunctionDefinition, Variable\n    fd = FunctionDefinition(\"f\", [Variable(x)], [Assignment(x, y)])\n    assert prntr._print_FunctionDefinition(fd) == \"def f(x):\\n    x = y\"\n\n    # Test _print_While\n    from sympy.codegen.ast import While\n    whl = While(x < 5, [Assignment(x, x + 1)])\n    assert prntr._print_While(whl) == \"while (x < 5):\\n    x = x + 1\"\n\n    # Test _print_Declaration\n    from sympy.codegen.ast import Declaration\n    decl = Declaration(Variable(x, value=y))\n    assert prntr._print_Declaration(decl) == \"x = y\"\n\n    # Test _print_Return\n    from sympy.codegen.ast import Return\n    ret = Return(x)\n    assert prntr._print_Return(ret) == \"return x\"\n\n    # Test _print_Print\n    from sympy.codegen.ast import Print\n    prnt = Print"], "sample_1059": ["def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n\n    assert assoc_laguerre(0, a, x) == 1\n    assert assoc_laguerre(1, a, x) == a - x + 1\n    assert assoc_laguerre(2, a, x) == (a**2 + 2*a + x**2 - 4*a*x + 2*x + 2)/2\n    assert assoc_laguerre(3, a, x) == (a**3 + 3*a**2 + 3*a*x**2 + 6*a**2*x - \n                                       9*a*x - x**3 + 3*x**2 - 6*x + 6)/6\n\n    X = assoc_laguerre(n, a, x)\n    assert isinstance(X, assoc_laguerre)\n\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(n, a, 0) == binomial(n + a, a)\n    assert assoc_laguerre(n, a, oo) == (-1)**n * oo\n    assert assoc_laguerre(n, a, -oo) == oo\n\n    assert conjugate(assoc_laguerre(n, a, x)) == assoc_laguerre(n, conjugate(a), conjugate(x))\n\n    _k = Dummy('k')\n\n    assert assoc_laguerre(n, a, x).rewrite(\"polynomial\").dummy_eq(\n        Sum(x**_k*RisingFactorial(-n, _k)/(factorial(_k)*gamma(_k + a + 1)), (_k, 0, n))*gamma(n + a + 1)/factorial(n))\n    \n    assert diff(assoc_laguerre(n, a, x), x) == -assoc_laguerre(n - 1, a + 1, x)\n    assert diff(assoc_laguerre(n, a, x), a).dummy_eq(Sum(assoc_laguerre(_k, a, x)/(-a + n), (_k, 0, n - 1)))\n\n    raises(ArgumentIndexError, lambda: assoc_laguerre(n, a, x).fdiff(1))\n    raises(ArgumentIndexError,", "def test_jacobi_conjugate_and_subs():\n    from sympy.abc import n, a, b, x\n    # Testing conjugate property\n    assert conjugate(jacobi(n, a, b, x)) == jacobi(n, conjugate(a), conjugate(b), conjugate(x))\n\n    # Testing subs method for jacobi\n    expr = jacobi(n, a, b, x)\n    assert expr.subs({n: 2, a: 1, b: 1, x: 0.5}) == jacobi(2, 1, 1, 0.5)\n    assert expr.subs({n: 3, a: 2, b: 2, x: 0.3}) == jacobi(3, 2, 2, 0.3)\n    assert expr.subs({n: 1, a: 0, b: 0, x: 1}) == legendre(1, 1)\n", "def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n\n    # Test specific values\n    assert assoc_laguerre(0, a, x) == 1\n    assert assoc_laguerre(1, a, x) == a - x + 1\n    assert assoc_laguerre(2, a, x) == (a**2/2 - x*a + a + x**2/2 - 2*x + 1).expand()\n\n    # Symbolic tests\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(n, a, 0) == binomial(a + n, a)\n\n    X = assoc_laguerre(n, a, x)\n    assert isinstance(X, assoc_laguerre)\n\n    # Check symbolic evaluations for edge cases\n    assert assoc_laguerre(n, a, oo) == (-1)**n * oo\n    assert assoc_laguerre(n, a, -oo) == oo\n\n    # Test conjugate\n    assert conjugate(assoc_laguerre(n, a, x)) == assoc_laguerre(n, conjugate(a), conjugate(x))\n\n    # Test differentiation\n    _k = Dummy('k')\n    assert diff(assoc_laguerre(n, a, x), x) == -assoc_laguerre(n - 1, a + 1, x)\n    assert diff(assoc_laguerre(n, a, x), a).dummy_eq(Sum(assoc_laguerre(_k, a, x)/(n - a), (_k, 0, n - 1)))\n    assert diff(assoc_laguerre(n, a, x), n) == Derivative(assoc_laguerre(n, a, x), n)\n\n    # Test rewrite as polynomial\n    assert assoc_laguerre(n, a, x).rewrite(\"polynomial\").dummy_eq(\n        gamma(n + a + 1) / factorial(n) * Sum(\n            RisingFactorial(-n, _k) / (gamma(_k + a + 1) * factorial(_k)) * x**_k, (_k, 0, n)\n        )\n    )\n\n    raises(ArgumentIndexError, lambda: assoc_laguerre(n, a,", "def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n\n    # Basic tests\n    assert assoc_laguerre(0, a, x) == 1\n    assert assoc_laguerre(1, a, x) == a - x + 1\n    assert assoc_laguerre(2, a, x).expand() == (a**2/2 + 3*a/2 + x**2/2 - 2*a*x - 2*x + 1).expand()\n    assert assoc_laguerre(3, a, x).expand() == (a**3/6 + a**2 + 11*a/6 + x**3/6 - x**2*(a/2 + 3/2) - x*(-a**2/2 - 5*a/2 - 3) + 1).expand()\n\n    # Special values\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(n, a, 0) == binomial(n + a, n)\n    assert assoc_laguerre(n, a, oo) == (-1)**n * oo\n    assert assoc_laguerre(n, a, -oo) == oo\n\n    assert conjugate(assoc_laguerre(n, a, x)) == assoc_laguerre(n, conjugate(a), conjugate(x))\n\n    # Derivatives\n    assert diff(assoc_laguerre(n, a, x), x) == -assoc_laguerre(n - 1, a + 1, x)\n    assert diff(assoc_laguerre(n, a, x), a).dummy_eq(Sum(assoc_laguerre(_k, a, x)/(-a + n), (_k, 0, n - 1)))\n    assert diff(assoc_laguerre(n, a, x), n) == Derivative(assoc_laguerre(n, a, x), n)\n\n    _k = Dummy('k')\n    assert assoc_laguerre(n, a, x).rewrite(\"polynomial\").dummy_eq(Sum(x**_k*RisingFactorial(-n, _k)/(gamma(_k + a + 1)*factorial(_k)), (_k, 0, n)) * gamma(n", "def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    alpha = Symbol(\"alpha\")\n\n    assert assoc_laguerre(0, alpha, x) == 1\n    assert assoc_laguerre(1, alpha, x) == alpha - x + 1\n    assert assoc_laguerre(2, alpha, x) == (alpha**2/2 + 3*alpha/2 + x**2/2 - (alpha + 2)*x + 1).expand()\n    assert assoc_laguerre(3, alpha, x) == (alpha**3/6 + alpha**2 + 11*alpha/6 - x**3/6 + (alpha/2 + 3/2)*x**2 - (alpha**2/2 + 5*alpha/2 + 3)*x + 1).expand()\n\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(n, alpha, 0) == binomial(n + alpha, alpha)\n\n    X = assoc_laguerre(n, alpha, x)\n    assert isinstance(X, assoc_laguerre)\n\n    assert assoc_laguerre(n, alpha, oo) == (-1)**n * oo\n    assert assoc_laguerre(n, alpha, -oo) == oo\n\n    assert conjugate(assoc_laguerre(n, alpha, x)) == assoc_laguerre(n, conjugate(alpha), conjugate(x))\n\n    _k = Dummy('k')\n\n    assert assoc_laguerre(n, alpha, x).rewrite(\"polynomial\").dummy_eq(\n        Sum(x**_k * RisingFactorial(-n, _k) / (gamma(_k + alpha + 1) * factorial(_k)), (_k, 0, n)) * gamma(n + alpha + 1) / factorial(n)\n    )\n\n    assert diff(assoc_laguerre(n, alpha, x), x) == -assoc_laguerre(n - 1, alpha + 1, x)\n    assert diff(assoc_laguerre(n, alpha, x), alpha).dummy_eq(\n        Sum(assoc_laguerre(_k, alpha, x) / (n - alpha), (_k, 0, n - 1))\n    )\n\n    raises(ValueError, lambda: assoc"], "sample_1060": ["def test_function_definition():\n    from sympy.codegen.ast import FunctionDefinition, Variable\n    prntr = PythonCodePrinter()\n    f = FunctionDefinition('func', [Variable(x), Variable(y)], [Assignment(z, x + y), Assignment(x, x - y)])\n    expected = (\n        'def func(x, y):\\n'\n        '    z = x + y\\n'\n        '    x = x - y'\n    )\n    assert prntr.doprint(f) == expected\n", "def test_print_functions_and_constants():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(abs(x)) == 'abs(x)'\n    assert prntr.doprint(x + y) == 'x + y'\n    assert prntr.doprint(acos(x)) == 'math.acos(x)'\n    assert prntr.doprint(sign(x)) == '(0.0 if x == 0 else math.copysign(1, x))'\n    assert prntr.doprint(pi) == 'math.pi'\n    assert prntr.doprint(S.Infinity) == \"float('inf')\"\n    assert prntr.doprint(S.NegativeInfinity) == \"float('-inf')\"\n    assert prntr.doprint(S.NaN) == \"float('nan')\"\n    assert prntr.doprint(Mod(x, 2)) == 'x % 2'\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)), (2, x>6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n", "def test_CustomFunctionsAndConstants():\n    custom_functions = {'custom_func': 'custom_module.custom_func'}\n    custom_constants = {'custom_const': 'custom_module.custom_const'}\n    prntr = PythonCodePrinter(settings={'user_functions': custom_functions, 'user_constants': custom_constants})\n    \n    class CustomFunc(Expr):\n            self._args = (arg,)\n        \n        @property\n            return self._args\n\n            return 'custom_func'\n    \n    class CustomConst(Expr):\n            return 'custom_const'\n\n    assert prntr.doprint(CustomFunc(x)) == 'custom_module.custom_func(x)'\n    assert prntr.doprint(CustomConst()) == 'custom_module.custom_const'\n    assert 'custom_module' in prntr.module_imports\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    assert p.doprint(Mod(x, 2)) == 'x % 2'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Identity(3)) == 'sympy.eye(3)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    assert p.doprint(Identity(3)) == 'sympy.eye(3)'\n    assert 'sympy' in p.module_imports\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert 'sympy' in p.module_imports\n"], "sample_1061": ["def test_isqrt_edge_cases():\n    # Check edge case where n is zero\n    assert isqrt(0) == 0\n    # Check edge case where n is one\n    assert isqrt(1) == 1\n    # Check edge case where n is a large perfect square\n    assert isqrt(10**12) == 10**6\n    # Check edge case where n is a large non-perfect square\n    assert isqrt(10**12 + 1) == 10**6\n    # Check edge case where n is a large non-perfect square less than a perfect square\n    assert isqrt(10**12 - 1) == 10**6 - 1\n", "def test_integer_nthroot_edge_cases():\n    # Test very large integers\n    large_number = 2**1000\n    assert integer_nthroot(large_number, 10) == (2**100, True)\n    assert integer_nthroot(large_number + 1, 10) == (2**100, False)\n    assert integer_nthroot(large_number - 1, 10) == (2**100 - 1, False)\n\n    # Test perfect squares\n    for i in range(1000):\n        n = i**2\n        assert integer_nthroot(n, 2) == (i, True)\n        assert integer_nthroot(n + 1, 2) == (i, False)\n        assert integer_nthroot(n - 1, 2) == (i - 1, False)\n\n    # Test perfect cubes\n    for i in range(100):\n        n = i**3\n        assert integer_nthroot(n, 3) == (i, True)\n        assert integer_nthroot(n + 1, 3) == (i, False)\n        assert integer_nthroot(n - 1, 3) == (i - 1, False)\n\n    # Test nth roots where n > value\n    assert integer_nthroot(2, 10) == (1, False)\n    assert integer_nthroot(1, 10) == (1, True)\n    assert integer_nthroot(0, 10) == (0, True)\n\n    # Test negative numbers for odd roots\n    assert integer_nthroot(-8, 3) == (-2, True)\n    assert integer_nthroot(-9, 3) == (-2, False)\n    assert integer_nthroot(-7, 3) == (-2, False)\n\n    # Test invalid inputs\n    raises(ValueError, lambda: integer_nthroot(-8, 2))\n    raises(ValueError, lambda: integer_nthroot(8, -2))\n    raises(ValueError, lambda: integer_nthroot(-8, 0))\n    raises(ValueError, lambda: integer_nthroot(0, 0))\n", "def test_integer_nthroot_corner_cases():\n    # Testing for base cases\n    assert integer_nthroot(0, 1) == (0, True)\n    assert integer_nthroot(1, 1) == (1, True)\n    assert integer_nthroot(1, 2) == (1, True)\n    \n    # Testing for n = 0 and n = -1\n    raises(ValueError, lambda: integer_nthroot(10, 0))\n    raises(ValueError, lambda: integer_nthroot(10, -1))\n    \n    # Testing for y = -1 and y = -10\n    raises(ValueError, lambda: integer_nthroot(-1, 2))\n    raises(ValueError, lambda: integer_nthroot(-10, 3))\n    \n    # Testing large values\n    large_value = 10**100\n    assert integer_nthroot(large_value, 10) == (10**10, True)\n    assert integer_nthroot(large_value + 1, 10) == (10**10, False)\n    \n    # Testing perfect squares and cubes\n    assert integer_nthroot(256, 4) == (4, True)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(1000000, 6) == (10, True)\n    \n    # Testing non-perfect squares and cubes\n    assert integer_nthroot(255, 4) == (3, False)\n    assert integer_nthroot(26, 3) == (2, False)\n    assert integer_nthroot(999999, 6) == (9, False)\n", "def test_integer_nthroot_edge_cases():\n    # Edge cases for integer_nthroot function\n    assert integer_nthroot(0, 1) == (0, True)\n    assert integer_nthroot(1, 0) == (1, True)\n    assert integer_nthroot(0, 2) == (0, True)\n    assert integer_nthroot(1, 2) == (1, True)\n    assert integer_nthroot(2, 2) == (1, False)\n    assert integer_nthroot(2, 3) == (1, False)\n    assert integer_nthroot(10**18, 18) == (10, True)\n    assert integer_nthroot(10**18 + 1, 18) == (10, False)\n", "def test_pow_class():\n    from sympy import Pow, Rational, Symbol, oo, nan\n\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test base and exponent properties\n    p = Pow(x, y)\n    assert p.base == x\n    assert p.exp == y\n\n    # Test class key\n    assert Pow.class_key() == (3, 2, 'Pow')\n\n    # Test commutativity\n    assert Pow(x, y).is_commutative == (x.is_commutative and y.is_commutative)\n    assert Pow(2, 3).is_commutative is True\n    assert Pow(2, x).is_commutative == x.is_commutative\n\n    # Test _eval_is_integer\n    assert Pow(2, 3)._eval_is_integer() is True\n    assert Pow(x, 3)._eval_is_integer() is None\n    assert Pow(2, x)._eval_is_integer() is None\n\n    # Test _eval_is_zero\n    assert Pow(0, 3)._eval_is_zero() is True\n    assert Pow(2, 3)._eval_is_zero() is False\n    assert Pow(2, x)._eval_is_zero() is None\n\n    # Test _eval_is_extended_real\n    assert Pow(2, 3)._eval_is_extended_real() is True\n    assert Pow(-2, 3)._eval_is_extended_real() is True\n    assert Pow(2, -3)._eval_is_extended_real() is True\n    assert Pow(-2, -3)._eval_is_extended_real() is True\n    assert Pow(-2, Rational(1, 2))._eval_is_extended_real() is False\n\n    # Test _eval_is_rational\n    assert Pow(Rational(1, 2), 2)._eval_is_rational() is True\n    assert Pow(2, Rational(1, 2))._eval_is_rational() is False\n\n    # Test _eval_is_algebraic\n    assert Pow(2, 3)._eval_is_algebraic() is True\n    assert Pow(2, Rational(1, 2))._eval_is_algebraic() is True\n    assert Pow(2, Symbol('a'))._eval_is_algebraic() is None\n\n    # Test _eval_is_extended_positive\n    assert Pow(2, 3)._eval_is_extended_positive() is"], "sample_1062": ["def test_TR22():\n    assert TR22(tan(x)**2) == sec(x)**2 - 1\n    assert TR22(cot(x)**2) == csc(x)**2 - 1\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n    assert TR22(cot(x)**4) == (csc(x)**2 - 1)**2\n    assert TR22(tan(x)**6) == (sec(x)**2 - 1)**3\n    assert TR22(cot(x)**6) == (csc(x)**2 - 1)**3\n", "def test_TR0():\n    # test simplification of rational polynomials\n    assert TR0((x + x**2)/(x)) == x + 1\n    assert TR0((x**2 + 2*x + 1)/(x + 1)) == x + 1\n    assert TR0((sin(x) + sin(x)*cos(x))/(sin(x))) == 1 + cos(x)\n    assert TR0((sin(x)*cos(x) + sin(x)**2)/(sin(x))) == cos(x) + sin(x)\n    assert TR0((cos(x)**2 + sin(x)**2)/(cos(x)**2)) == 1 + (sin(x)/cos(x))**2\n    assert TR0((cos(x)**2 - sin(x)**2)/(cos(x)**2)) == 1 - (sin(x)/cos(x))**2\n", "def test_TR22():\n    assert TR22(tan(x)**2) == sec(x)**2 - 1\n    assert TR22(cot(x)**2) == csc(x)**2 - 1\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n    assert TR22(cot(x)**4) == (csc(x)**2 - 1)**2\n    assert TR22(tan(x)**6) == (sec(x)**2 - 1)**3\n    assert TR22(cot(x)**6) == (csc(x)**2 - 1)**3\n", "def test_TR22():\n    assert TR22(tan(x)**2 + 1) == sec(x)**2\n    assert TR22(cot(x)**2 + 1) == csc(x)**2\n    assert TR22(tan(x)**4 + 2*tan(x)**2 + 1) == sec(x)**4\n    assert TR22(cot(x)**4 + 2*cot(x)**2 + 1) == csc(x)**4\n    assert TR22(tan(x)**2 + tan(x)**4) == tan(x)**4 + sec(x)**2 - 1\n    assert TR22(cot(x)**2 + cot(x)**4) == cot(x)**4 + csc(x)**2 - 1\n", "def test_TR22():\n    assert TR22(tan(x)**2) == sec(x)**2 - 1\n    assert TR22(cot(x)**2) == csc(x)**2 - 1\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n    assert TR22(cot(x)**4) == (csc(x)**2 - 1)**2\n    assert TR22(1/tan(x)**2) == cot(x)**2\n    assert TR22(1/cot(x)**2) == tan(x)**2\n"], "sample_1063": ["def test_lambdastr():\n        return f\"custom({expr})\"\n\n    # Test with a simple expression\n    expr = x + y\n    result = lambdastr((x, y), expr, printer=custom_printer)\n    assert result == 'lambda x,y: (custom(x + y))'\n\n    # Test with nested tuple arguments\n    expr = x + y + z\n    result = lambdastr((x, (y, z)), expr, printer=custom_printer)\n    assert result == 'lambda _0,_1: (lambda x,y,z: custom(x + y + z))(_0,_1[0],_1[1])'\n\n    # Test with a matrix\n    mat = Matrix([[x, y], [z, x + y]])\n    result = lambdastr((x, y, z), mat, printer=custom_printer)\n    expected = 'lambda x,y,z: (custom(Matrix([[x, y], [z, x + y]])))'\n    assert result == expected\n", "def test_lambda_with_undefined_function():\n    from sympy import Function\n    U = Function('U')\n    f = lambdify(x, U(x))\n    assert f(2).__class__.__name__ == 'UndefinedFunction'\n    assert f(2).args == (2,)\n    assert f(2).func == U\n", "def test_tensorflow_custom_functions():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n\n    custom_square = lambda x: x ** 2\n    custom_add = lambda x, y: x + y\n\n    # Test single argument custom function\n    f1 = lambdify(x, custom_square(x), modules={'custom_square': custom_square, 'tensorflow'})\n    with tensorflow.compat.v1.Session() as s:\n        a = tensorflow.constant(3, dtype=tensorflow.float32)\n        assert f1(a).eval(session=s) == 9\n\n    # Test multiple argument custom function\n    f2 = lambdify((x, y), custom_add(x, y), modules={'custom_add': custom_add, 'tensorflow'})\n    with tensorflow.compat.v1.Session() as s:\n        a = tensorflow.constant(3, dtype=tensorflow.float32)\n        b = tensorflow.constant(4, dtype=tensorflow.float32)\n        assert f2(a, b).eval(session=s) == 7\n\n    # Test custom function within a larger expression\n    expr = custom_add(custom_square(x), custom_square(y))\n    f3 = lambdify((x, y), expr, modules={'custom_square': custom_square, 'custom_add': custom_add, 'tensorflow'})\n    with tensorflow.compat.v1.Session() as s:\n        a = tensorflow.constant(3, dtype=tensorflow.float32)\n        b = tensorflow.constant(4, dtype=tensorflow.float32)\n        assert f3(a, b).eval(session=s) == 25\n", "def test_import_reload():\n    # Ensure that reloading a module properly resets the namespace\n    _import(\"math\", reload=True)\n    _import(\"numpy\", reload=True)\n    f_math = lambdify(x, sin(x), \"math\")\n    f_numpy = lambdify(x, sin(x), \"numpy\")\n    assert f_math(math.pi) == 0\n    assert numpy.allclose(f_numpy(numpy.pi), numpy.sin(numpy.pi))\n\n    # Check that changes in one namespace do not affect another\n    MATH[\"sin\"] = lambda x: \"math sin\"\n    NUMPY[\"sin\"] = lambda x: \"numpy sin\"\n    f_math = lambdify(x, sin(x), \"math\")\n    f_numpy = lambdify(x, sin(x), \"numpy\")\n    assert f_math(0) == \"math sin\"\n    assert f_numpy(0) == \"numpy sin\"\n\n    # Reload to reset to original functions\n    _import(\"math\", reload=True)\n    _import(\"numpy\", reload=True)\n    f_math = lambdify(x, sin(x), \"math\")\n    f_numpy = lambdify(x, sin(x), \"numpy\")\n    assert f_math(math.pi) == 0\n    assert numpy.allclose(f_numpy(numpy.pi), numpy.sin(numpy.pi))\n", "def test_lambdify_nested_expressions():\n    # Test lambdify with nested expressions\n    f = lambdify([x, y], sin(cos(x + y)))\n    assert abs(f(0, 0) - sin(cos(0))) < 1e-12\n    assert abs(f(pi, pi) - sin(cos(2*pi))) < 1e-12\n\n    expr = x**2 + y**2 + z**2\n    nested_expr = sqrt(expr)\n    f = lambdify([x, y, z], nested_expr)\n    assert f(1, 2, 2) == 3\n    assert f(3, 4, 12) == 13\n\n    expr = Piecewise((x + 1, x > 0), (y + 2, y > 0), (z + 3, True))\n    f = lambdify([x, y, z], expr)\n    assert f(1, -1, -1) == 2\n    assert f(-1, 1, -1) == 3\n    assert f(-1, -1, 1) == 4\n\n    expr = Max(x, Min(y, z))\n    f = lambdify([x, y, z], expr)\n    assert f(1, 2, 3) == 2\n    assert f(3, 2, 1) == 3\n"], "sample_1064": ["def test_tensorflow_piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    from sympy import Piecewise\n\n    expr = Piecewise((x**2, x < 1), (x, True))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        \"tensorflow.where(tensorflow.math.less(x, 1), tensorflow.math.pow(x, 2), x)\"\n    assert tensorflow_code(expr, tensorflow_version='0.12') == \\\n        \"tensorflow.select(tensorflow.math.less(x, 1), tensorflow.math.pow(x, 2), x)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random() * 2 - 1)\n\n    expr = Piecewise((sin(x), x < 1), (cos(x), x < 2), (tan(x), True))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        \"tensorflow.where(tensorflow.math.less(x, 1), tensorflow.math.sin(x), \" \\\n        \"tensorflow.where(tensorflow.math.less(x, 2), tensorflow.math.cos(x), tensorflow.math.tan(x)))\"\n    assert tensorflow_code(expr, tensorflow_version='0.12') == \\\n        \"tensorflow.select(tensorflow.math.less(x, 1), tensorflow.math.sin(x), \" \\\n        \"tensorflow.select(tensorflow.math.less(x, 2), tensorflow.math.cos(x), tensorflow.math.tan(x)))\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random() * 3 - 1)\n", "def test_tensorflow_derivative():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Derivative(sin(x), x)\n    assert tensorflow_code(expr) == \"tensorflow.gradients(tensorflow.math.sin(x), x)[0]\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n\n    expr = Derivative(x**2 + y**2, x)\n    assert tensorflow_code(expr) == \"tensorflow.gradients(tensorflow.math.add(tensorflow.math.pow(x, 2), tensorflow.math.pow(y, 2)), x)[0]\"\n    _compare_tensorflow_scalar((x, y), expr, rng=lambda: random.random())\n\n    expr = Derivative(sin(x) * cos(y), x)\n    assert tensorflow_code(expr) == \"tensorflow.gradients(tensorflow.math.multiply(tensorflow.math.sin(x), tensorflow.math.cos(y)), x)[0]\"\n    _compare_tensorflow_scalar((x, y), expr, rng=lambda: random.random())\n\n    expr = Derivative(x**2 * y + y**2 * x, x)\n    assert tensorflow_code(expr) == \"tensorflow.gradients(tensorflow.math.add(tensorflow.math.multiply(tensorflow.math.pow(x, 2), y), tensorflow.math.multiply(tensorflow.math.pow(y, 2), x)), x)[0]\"\n    _compare_tensorflow_scalar((x, y), expr, rng=lambda: random.random())\n", "def test_tensorflow_derivative():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Derivative(sin(x), x)\n    assert tensorflow_code(expr) == \"tensorflow.gradients(tensorflow.math.sin(x), x)[0]\"\n    _compare_tensorflow_scalar((x,), expr)\n\n    expr = Derivative(sin(x) + cos(y), x)\n    assert tensorflow_code(expr) == \"tensorflow.gradients(tensorflow.math.add(tensorflow.math.sin(x), tensorflow.math.cos(y)), x)[0]\"\n    _compare_tensorflow_scalar((x, y), expr)\n\n    # Test with multiple variables\n    try:\n        expr = Derivative(sin(x), (x, y))\n        tensorflow_code(expr)\n    except NotImplementedError as e:\n        assert str(e) == \"derivation by multiple variables is not supported\"\n\n    # Test higher-order derivatives\n    expr = Derivative(sin(x), x, x)\n    assert tensorflow_code(expr) == \"tensorflow.gradients(tensorflow.gradients(tensorflow.math.sin(x), x)[0], x)[0]\"\n    _compare_tensorflow_scalar((x,), expr)\n", "def test_tensorflow_piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session = tf.compat.v1.Session(graph=graph)\n        \n        expr = Piecewise((x + 1, x > 0), (x - 1, True))\n        assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n            'tensorflow.where(tensorflow.math.greater(x, 0), tensorflow.math.add(x, 1), tensorflow.math.subtract(x, 1))'\n        assert tensorflow_code(expr, tensorflow_version='0.12') == \\\n            'tensorflow.select(tensorflow.math.greater(x, 0), tensorflow.math.add(x, 1), tensorflow.math.subtract(x, 1))'\n\n        f = lambdify((x,), expr, 'tensorflow')\n        y = session.run(f(2))\n        assert y == 3\n        y = session.run(f(-2))\n        assert y == -3\n\n        expr = Piecewise((x + y, x > 0), (x * y, True))\n        assert tensorflow_code(expr) == \\\n            'tensorflow.where(tensorflow.math.greater(x, 0), tensorflow.math.add(x, y), tensorflow.math.multiply(x, y))'\n\n        f = lambdify((x, y), expr, 'tensorflow')\n        y = session.run(f(2, 3))\n        assert y == 5\n        y = session.run(f(-2, 3))\n        assert y == -6\n", "def test_tensorflow_piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n    \n    from sympy import Piecewise\n\n    expr = Piecewise((x**2, x < 1), (x, True))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        \"tensorflow.where(tensorflow.math.less(x, 1), tensorflow.math.pow(x, 2), x)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.uniform(-1, 2))\n\n    expr = Piecewise((x**2, x < 1), (x**3, x < 2), (x, True))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        \"tensorflow.where(tensorflow.math.less(x, 1), tensorflow.math.pow(x, 2), tensorflow.where(tensorflow.math.less(x, 2), tensorflow.math.pow(x, 3), x))\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.uniform(-1, 3))\n"], "sample_1065": ["def test_subfactorial():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    r = Symbol('r', integer=False)\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).is_integer is True\n    assert subfactorial(r).is_integer is None\n    assert subfactorial(k).is_integer\n\n    assert subfactorial(n).is_positive is None\n    assert subfactorial(k).is_positive is True\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n    assert subfactorial(k).rewrite(uppergamma) == uppergamma(k + 1, -1) / S.Exp1\n    assert subfactorial(r).rewrite(uppergamma) == uppergamma(r + 1, -1) / S.Exp1\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(k).is_even is None\n    assert subfactorial(2*k).is_even is True\n    assert subfactorial(2*k + 1).is_odd is True\n\n    # Test for nonnegative subfactorials\n    for i in range(10):\n        assert subfactorial(i).is_nonnegative is True\n\n    # Test for nonnegative subfactorials as odd or even\n    for i in range(10):\n        assert subfactorial(i).is_even == (i % 2 == 0)\n        assert subfactorial(i).is_odd == (i % 2 == 1)\n", "def test_subfactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    m = Symbol('m', integer=True, nonnegative=True)\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n    assert subfactorial(10) == 1334961\n    assert subfactorial(15) == 481066515734\n    assert subfactorial(-5) == 0\n    assert subfactorial(n).func == subfactorial\n\n    assert subfactorial(x).is_integer is None\n    assert subfactorial(n).is_integer is True\n    assert subfactorial(m).is_integer is True\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(7).is_even is True\n    assert subfactorial(8).is_even is False\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n    assert subfactorial(n).rewrite(factorial) == subfactorial(n)\n    assert subfactorial(n).rewrite(Product) == subfactorial(n)\n\n    assert subfactorial(oo) == oo\n    assert subfactorial(-oo) == 0\n    assert subfactorial(nan) == nan\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).func == subfactorial\n    assert subfactorial(n + 1).func == subfactorial\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n\n    assert subfactorial(oo) == oo\n    assert subfactorial(n).is_integer is True\n    assert subfactorial(n).is_even is None\n    assert subfactorial(n).is_odd is None\n\n    assert subfactorial(0).is_even is False\n    assert subfactorial(1).is_even is True\n    assert subfactorial(2).is_even is False\n    assert subfactorial(3).is_even is True\n\n    assert subfactorial(n).is_positive is None\n    assert subfactorial(n).is_nonnegative is True\n    assert subfactorial(n).is_real is True\n\n    raises(ValueError, lambda: subfactorial(-1))\n    raises(ValueError, lambda: subfactorial(S(1)/2))\n    raises(ValueError, lambda: subfactorial(I))\n\n    assert subfactorial(n).rewrite(factorial) == factorial(n) * sum((-1)**k / factorial(k) for k in range(n + 1))\n", "def test_subfactorial():\n    n = Symbol('n', integer=True, nonnegative=True)\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n    assert subfactorial(n + 1) == subfactorial(n + 1)\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(3).is_even is False\n    assert subfactorial(4).is_even is True\n\n    assert subfactorial(n).is_integer is True\n    assert subfactorial(n).is_nonnegative is True\n\n    assert subfactorial(n).diff(n) == subfactorial(n + 1).diff(n)\n    raises(ArgumentIndexError, lambda: subfactorial(n).fdiff(2))\n\n    assert subfactorial(n).evalf(subs={n: 10}) == 1334961\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n    assert subfactorial(10) == 1334961\n    assert subfactorial(n).func == subfactorial\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(3).is_integer\n    assert subfactorial(-3).is_integer is None\n\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(3).is_nonnegative\n    assert subfactorial(-3).is_nonnegative is None\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(3).is_even is None\n    assert subfactorial(4).is_even\n\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(3).is_odd\n    assert subfactorial(4).is_odd is None\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n"], "sample_1066": ["def test_mathml_complex_expression():\n    expr = Integral(x**2 + y**2, (x, 0, 1), (y, 0, 1)) + Sum(z**2, (z, 1, 5))\n    assert mp.doprint(expr) == '<apply><plus/><apply><int/><bvar><ci>x</ci></bvar><lowlimit><cn>0</cn></lowlimit><uplimit><cn>1</cn></uplimit><bvar><ci>y</ci></bvar><lowlimit><cn>0</cn></lowlimit><uplimit><cn>1</cn></uplimit><apply><plus/><apply><power/><ci>x</ci><cn>2</cn></apply><apply><power/><ci>y</ci><cn>2</cn></apply></apply></apply><apply><sum/><bvar><ci>z</ci></bvar><lowlimit><cn>1</cn></lowlimit><uplimit><cn>5</cn></uplimit><apply><power/><ci>z</ci><cn>2</cn></apply></apply></apply>'\n    assert mpp.doprint(expr) == '<mrow><mrow><mo>&#x222C;</mo><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><msup><mi>y</mi><mn>2</mn></msup></mrow><mo>&dd;</mo><mi>y</mi><mo>&dd;</mo><mi>x</mi></mrow><mo>+</mo><mrow><munderover><mo>&#x2211;</mo><mn>1</mn><mn>5</mn></munderover><msup><mi>z</mi><mn>2</mn></msup></mrow></mrow>'\n", "def test_print_Numbersymbols():\n    from sympy.matrices.expressions import Determinant\n    assert mathml(GoldenRatio, printer='content') == '<cn>&#966;</cn>'\n    assert mathml(EulerGamma, printer='content') == '<eulergamma/>'\n    assert mathml(pi, printer='content') == '<pi/>'\n    assert mathml(oo, printer='content') == '<infinity/>'\n    assert mathml(S.NaN, printer='content') == '<notanumber/>'\n    assert mathml(1 + pi, printer='content') == '<apply><plus/><cn>1</cn><pi/></apply>'\n    assert mathml(oo - pi, printer='content') == '<apply><minus/><infinity/><pi/></apply>'\n    assert mathml(EulerGamma + GoldenRatio, printer='content') == '<apply><plus/><eulergamma/><cn>&#966;</cn></apply>'\n    assert mathml(Determinant(Matrix([[1, 2], [3, 4]])), printer='content') == '<apply><determinant/><matrix><matrixrow><cn>1</cn><cn>2</cn></matrixrow><matrixrow><cn>3</cn><cn>4</cn></matrixrow></matrix></apply>'\n", "def test_content_mathml_special_functions():\n    mml = mp._print(Exp1)\n    assert mml.nodeName == 'exponentiale'\n    assert mml.childNodes == []\n\n    mml = mp._print(EulerGamma)\n    assert mml.nodeName == 'eulergamma'\n    assert mml.childNodes == []\n\n    mml = mp._print(GoldenRatio)\n    assert mml.nodeName == 'cn'\n    assert mml.childNodes[0].nodeValue == u\"\\N{GREEK SMALL LETTER PHI}\"\n\n    mml = mp._print(Pi)\n    assert mml.nodeName == 'pi'\n    assert mml.childNodes == []\n\n    mml = mp._print(Infinity)\n    assert mml.nodeName == 'infinity'\n    assert mml.childNodes == []\n\n    mml = mp._print(NegativeInfinity)\n    assert mml.nodeName == 'apply'\n    assert mml.childNodes[0].nodeName == 'minus'\n    assert mml.childNodes[1].nodeName == 'infinity'\n\n    mml = mp._print(NaN)\n    assert mml.nodeName == 'notanumber'\n    assert mml.childNodes == []\n\n    mml = mp._print(EmptySet)\n    assert mml.nodeName == 'emptyset'\n    assert mml.childNodes == []\n\n    mml = mp._print(BooleanTrue)\n    assert mml.nodeName == 'true'\n    assert mml.childNodes == []\n\n    mml = mp._print(BooleanFalse)\n    assert mml.nodeName == 'false'\n    assert mml.childNodes == []\n", "def test_print_basic_operations():\n    expr = Basic(Abs(x), Conjugate(y))\n    assert mpp.doprint(expr) == \\\n        '<mrow><mi>basic</mi><mfenced><mrow><mfenced close=\"|\" open=\"|\">' \\\n        '<mi>x</mi></mfenced></mrow><mrow><menclose notation=\"top\">' \\\n        '<mi>y</mi></menclose></mrow></mfenced></mrow>'\n    assert mp.doprint(expr) == \\\n        '<basic><apply><abs/><ci>x</ci></apply><apply><conjugate/>' \\\n        '<ci>y</ci></apply></basic>'\n", "def test_print_Heaviside():\n    assert mpp.doprint(Heaviside(x)) == '<mrow><mi>&#x398;</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(Heaviside(x)) == '<apply><Heaviside/><ci>x</ci></apply>'\n"], "sample_1067": ["def test_unevaluated_mul():\n    x, y, z = symbols('x y z')\n    a, b, c = S(3), S(2), S(4)\n\n    # Check that numbers are combined correctly\n    e = _unevaluated_Mul(a, b, c)\n    assert e == S(24)\n\n    # Check that non-commutative arguments are handled correctly\n    A, B = symbols('A B', commutative=False)\n    e = _unevaluated_Mul(a, b, A, B)\n    assert e == Mul(6, A, B, evaluate=False)\n\n    # Check that sorting of arguments is done correctly\n    e = _unevaluated_Mul(y, x, z, a)\n    assert e == Mul(3, x, y, z, evaluate=False)\n\n    # Check that powers are handled correctly\n    e = _unevaluated_Mul(x, x)\n    assert e == Mul(x, x, evaluate=False)\n", "def test_unevaluated_Mul():\n    from sympy import S, sqrt, Mul\n    from sympy.abc import x\n\n    # Test multiplication of Numbers and symbols\n    a = _unevaluated_Mul(S(3.0), x, S(2))\n    assert a.args[0] == S(6.0)\n    assert a.args[1] == x\n\n    # Test commutativity in unevaluated Muls\n    m = _unevaluated_Mul(sqrt(2), sqrt(3))\n    assert m == _unevaluated_Mul(sqrt(3), sqrt(2))\n\n    # Test equality with unevaluated Muls created through Mul with evaluate=False\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m == _unevaluated_Mul(u)\n    assert m != Mul(*m.args)\n\n    # Test multiplication of nested unevaluated Muls\n    b = _unevaluated_Mul(S(2), S(3), S(4), evaluate=False)\n    assert b.args[0] == S(24)\n", "def test_unevaluated_Mul():\n    from sympy.abc import x, y\n    from sympy import sqrt, Rational, Mul, S\n\n    # Test number collection and sorting\n    m = _unevaluated_Mul(S(3.0), x, S(2))\n    assert m.args[0] == 6.0\n    assert m.args[1] == x\n\n    # Test flattening of nested Muls\n    m = _unevaluated_Mul(S(2), x, _unevaluated_Mul(S(3), y))\n    assert m.args == (6, x, y)\n\n    # Test with non-commutative arguments\n    m = _unevaluated_Mul(S(2), x, S(3), sqrt(2), sqrt(3), evaluate=False)\n    assert m == Mul(6, sqrt(6), x, evaluate=False)\n\n    # Test when all arguments are same\n    assert _unevaluated_Mul(S(2), S(2)) == 4\n    assert _unevaluated_Mul(S(2), S(2), S(2)) == 8\n\n    # Test with Rational numbers\n    m = _unevaluated_Mul(Rational(1, 3), Rational(3, 4), x)\n    assert m.args == (Rational(1, 4), x)\n", "def test_issue_7000():\n    x, y, z = symbols('x y z')\n    a, b, c = symbols('a b c', cls=Wild, exclude=[x, y])\n\n    e = x*y + z\n    pattern = a*b + c\n    assert e.match(pattern) == {a: x, b: y, c: z}\n\n    e = x*y*z\n    assert e.match(a*b*c) == {a: x, b: y, c: z}\n    \n    e = (x + 1)*(y + 1)\n    pattern = (a + 1)*(b + 1)\n    assert e.match(pattern) == {a: x, b: y}\n", "def test_mul_subs():\n    x, y, z = symbols('x y z')\n    a, b, c = symbols('a b c', cls=Wild)\n\n    expr = 2*x*y*z\n    pattern = a*x*y\n    new_expr = a*y\n\n    assert expr.subs(pattern, new_expr) == 2*y**2*z\n\n    expr = 3*x**2*y*z\n    pattern = a*x*y\n    new_expr = a**2\n\n    assert expr.subs(pattern, new_expr) == 9*x*z\n\n    expr = x*y**2*z\n    pattern = a*y\n    new_expr = a**2\n\n    assert expr.subs(pattern, new_expr) == x*y**4*z\n\n    expr = 4*x*y*z\n    pattern = b*x*z\n    new_expr = b**2\n\n    assert expr.subs(pattern, new_expr) == 16*y\n"], "sample_1068": ["def test_octave_code_with_assignment():\n    # Test the octave_code function with assignment to a variable\n    assert mcode(sin(x), assign_to='s') == 's = sin(x);'\n    assert mcode(cos(x) + sin(x), assign_to='c') == 'c = cos(x) + sin(x);'\n    assert mcode(x**2 + y**2, assign_to='result') == 'result = x.^2 + y.^2;'\n    assert mcode(sqrt(x) + exp(x), assign_to='expr') == 'expr = sqrt(x) + exp(x);'\n    assert mcode(log(x) + factorial(x), assign_to='f') == 'f = log(x) + factorial(x);'\n", "def test_user_functions():\n    custom_functions = {\n        \"f\": \"custom_f\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_g_matrix\"),\n              (lambda x: not x.is_Matrix, \"custom_g\")]\n    }\n    f = Function('f')\n    g = Function('g')\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n        \"custom_f(x) + custom_g(x) + custom_g_matrix([1 x])\"\n", "def test_Mul():\n    assert mcode(3 * x * y) == \"3*x.*y\"\n    assert mcode(2 * pi * x / y) == \"2*pi*x./y\"\n    assert mcode(1 / (x * y)) == \"1./(x.*y)\"\n    assert mcode(3 * (x + y)) == \"3*(x + y)\"\n    assert mcode((x + y) * (x - y)) == \"(x + y).*(x - y)\"\n    assert mcode((x + y) / (x - y)) == \"(x + y)./(x - y)\"\n", "def test_lambdify_with_octave_code():\n    from sympy.utilities.lambdify import lambdify\n    f = lambdify(x, sin(x) ** cos(x), 'octave')\n    assert f.__doc__ == \"sin(x).^cos(x)\"\n    assert f(0) == 0\n    f = lambdify((x, y), Max(x, y), 'octave')\n    assert f.__doc__ == \"max(x, y)\"\n    assert f(3, 4) == 4\n    assert f(5, 2) == 5\n", "def test_user_defined_function():\n    custom_functions = {\n        \"custom_func\": \"custom_octave_func\"\n    }\n    f = Function('custom_func')\n    assert mcode(f(x), user_functions=custom_functions) == \"custom_octave_func(x)\"\n"], "sample_1069": ["def test_glsl_code():\n    from sympy import glsl_code, Matrix, Piecewise, symbols, Rational\n\n    # Test basic expressions\n    assert glsl_code(sin(x)) == \"sin(x)\"\n    assert glsl_code(cos(x)) == \"cos(x)\"\n    assert glsl_code(x + y) == \"x + y\"\n    assert glsl_code(x * y) == \"x*y\"\n    assert glsl_code(x**2) == \"pow(x, 2.0)\"\n    assert glsl_code(Rational(3, 7)) == \"3.0/7.0\"\n\n    # Test assignment\n    assert glsl_code(sin(x), assign_to=\"result\") == \"result = sin(x);\"\n\n    # Test matrix\n    mat = Matrix([[1, 2], [3, 4]])\n    assert glsl_code(mat) == \"mat2(1, 2, 3, 4)\"\n    assert glsl_code(mat, mat_transpose=True) == \"mat2(1, 3, 2, 4)\"\n\n    # Test Piecewise\n    expr = Piecewise((x, x < 1), (x**2, True))\n    assert glsl_code(expr) == \"((x < 1) ? (\\nx\\n)\\n: (\\npow(x, 2.0)\\n) )\"\n    assert glsl_code(expr, assign_to=\"result\") == (\n        \"if (x < 1) {\\n\"\n        \"result = x;\\n\"\n        \"}\\n\"\n        \"else {\\n\"\n        \"result = pow(x, 2.0);\\n\"\n        \"}\"\n    )\n\n    # Test nested Piecewise\n    expr = Piecewise((x, x < 1), (Piecewise((y, y < 1), (y**2, True)), True))\n    assert glsl_code(expr) == (\n        \"((x < 1) ? (\\nx\\n)\\n: (\\n\"\n        \"((y < 1) ? (\\ny\\n)\\n: (\\npow(y, 2.0)\\n) )\\n\"\n        \") )\"\n    )\n    assert glsl_code(expr, assign_to=\"result\") == (\n        \"if (x < 1) {\\n\"\n        \"result = x;\\n\"\n        \"}\\n\"\n        \"else {\\n\"\n        \"if (y <", "def test_glsl_basic_operations():\n    assert glsl_code(Integer(67)) == \"67.0\"\n    assert glsl_code(Integer(-1)) == \"-1.0\"\n    assert glsl_code(Rational(3, 7)) == \"3.0/7.0\"\n    assert glsl_code(x + Rational(3, 7)) == \"add(x, 3.0/7.0)\"\n    assert glsl_code(Rational(3, 7)*x) == \"mul(x, 3.0/7.0)\"\n    assert glsl_code(Eq(x, y)) == \"x == y\"\n    assert glsl_code(Ne(x, y)) == \"x != y\"\n    assert glsl_code(Le(x, y)) == \"x <= y\"\n    assert glsl_code(Lt(x, y)) == \"x < y\"\n    assert glsl_code(Gt(x, y)) == \"x > y\"\n    assert glsl_code(Ge(x, y)) == \"x >= y\"\n    assert glsl_code(sin(x)) == \"sin(x)\"\n    assert glsl_code(sign(x)) == \"sign(x)\"\n    assert glsl_code(exp(x)) == \"exp(x)\"\n    assert glsl_code(log(x)) == \"log(x)\"\n    assert glsl_code(floor(x)) == \"floor(x)\"\n    assert glsl_code(atan2(y, x)) == \"atan(y, x)\"\n    assert glsl_code(abs(x)) == \"abs(x)\"\n    assert glsl_code(ceiling(x)) == \"ceil(x)\"\n    assert glsl_code(sqrt(x)) == \"sqrt(x)\"\n    assert glsl_code(x**Rational(2, 3)) == \"pow(x, 2.0/3.0)\"\n    assert glsl_code(x**3) == \"pow(x, 3.0)\"\n    assert glsl_code(x**(y**3)) == \"pow(x, pow(y, 3.0))\"\n    assert glsl_code(x*(y+z)) == \"mul(x, add(y, z))\"\n    assert glsl_code(1/x) == \"1.0/x\"\n    assert glsl_code(Matrix([[1, 2], [3, 4]])) == \"mat2(1, 2, 3, 4)\"\n    assert glsl_code(Matrix([[1, 2, 3,", "def test_GLSL_Integer():\n    assert glsl_code(Integer(67)) == \"67.0\"\n    assert glsl_code(Integer(-1)) == \"-1.0\"\n\n", "def test_GLSLPrinter():\n    assert glsl_code(Integer(5)) == \"5.0\"\n    assert glsl_code(Rational(3, 7)) == \"3.0/7.0\"\n    assert glsl_code(x + Rational(3, 7)) == \"x + 3.0/7.0\"\n    assert glsl_code(x**2) == \"pow(x, 2.0)\"\n    assert glsl_code(x**0.5) == \"sqrt(x)\"\n    assert glsl_code(1/x) == \"1.0/x\"\n    assert glsl_code(x*y) == \"x*y\"\n    assert glsl_code(x + y) == \"x + y\"\n    assert glsl_code(sin(x)) == \"sin(x)\"\n    assert glsl_code(cos(x)) == \"cos(x)\"\n    assert glsl_code(exp(x)) == \"exp(x)\"\n    assert glsl_code(log(x)) == \"log(x)\"\n    assert glsl_code(abs(x)) == \"abs(x)\"\n    assert glsl_code(ceiling(x)) == \"ceil(x)\"\n    assert glsl_code(floor(x)) == \"floor(x)\"\n    assert glsl_code(Matrix([[1, 2], [3, 4]])) == 'mat2(1, 2, 3, 4)'\n    assert glsl_code(Matrix([1, 2, 3, 4])) == 'vec4(1, 2, 3, 4)'\n    expr = Piecewise((x + 1, x > 0), (x, True))\n    assert glsl_code(expr) == \"((x > 0) ? (\\nx + 1\\n)\\n: (\\nx\\n))\"\n    assert glsl_code(expr, assign_to=\"result\") == (\n        \"if (x > 0) {\\n\"\n        \"result = x + 1;\\n\"\n        \"}\\n\"\n        \"else {\\n\"\n        \"result = x;\\n\"\n        \"}\"\n    )\n    assert glsl_code(MatrixSymbol('A', 2, 2)) == \"A\"\n    assert glsl_code(MatrixSymbol('A', 2, 2)[0, 0]) == \"A[0][0]\"\n    assert glsl_code(MatrixSymbol('A', 2, 2)[1, 1]) == \"A[1][1]\"\n    assert gl", "def test_glsl_code():\n    # Test Integer and Rational\n    assert glsl_code(Integer(67)) == \"67.0\"\n    assert glsl_code(Rational(3, 7)) == \"3.0/7.0\"\n\n    # Test basic arithmetic operations\n    assert glsl_code(x + y) == \"x + y\"\n    assert glsl_code(x - y) == \"x - y\"\n    assert glsl_code(x * y) == \"x * y\"\n    assert glsl_code(x / y) == \"x / y\"\n\n    # Test power and sqrt\n    assert glsl_code(x ** 2) == \"pow(x, 2.0)\"\n    assert glsl_code(sqrt(x)) == \"sqrt(x)\"\n\n    # Test trigonometric functions\n    assert glsl_code(sin(x)) == \"sin(x)\"\n    assert glsl_code(cos(x)) == \"cos(x)\"\n    assert glsl_code(tan(x)) == \"tan(x)\"\n\n    # Test relational operators\n    assert glsl_code(Eq(x, y)) == \"x == y\"\n    assert glsl_code(Ne(x, y)) == \"x != y\"\n    assert glsl_code(Lt(x, y)) == \"x < y\"\n    assert glsl_code(Gt(x, y)) == \"x > y\"\n    assert glsl_code(Le(x, y)) == \"x <= y\"\n    assert glsl_code(Ge(x, y)) == \"x >= y\"\n\n    # Test Piecewise function\n    expr = Piecewise((x, x < 1), (x**2, True))\n    expected = (\n        \"if (x < 1) {\\n\"\n        \"   x\\n\"\n        \"}\\n\"\n        \"else {\\n\"\n        \"   pow(x, 2.0)\\n\"\n        \"}\"\n    )\n    assert glsl_code(expr) == expected\n\n    # Test Matrix\n    mat = Matrix([[1, 2], [3, 4]])\n    assert glsl_code(mat) == 'mat2(1, 2, 3, 4)'\n\n    # Test custom user function\n    custom_functions = {\n        \"Abs\": \"abs\",\n        \"ceiling\": \"ceil\"\n    }\n    assert glsl_code(abs(x) + ceiling(x), user_functions=custom_functions) == 'abs(x) + ceil(x)'\n"], "sample_1070": ["def test_exp_eval():\n    assert exp(S.Zero) == S.One\n    assert exp(S.One) == S.Exp1\n    assert exp(S.Infinity) == S.Infinity\n    assert exp(S.NegativeInfinity) == S.Zero\n    assert exp(S.ComplexInfinity) == S.NaN\n    assert exp(log(S.One)) == S.One\n    assert exp(log(S.Exp1)) == S.Exp1\n    assert exp(AccumBounds(1, 2)).evalf() == AccumBounds(exp(1).evalf(), exp(2).evalf())\n    assert exp(SetExpr(AccumBounds(1, 2))).eval() == SetExpr(AccumBounds(exp(1), exp(2)))\n", "def test_exp_evalf():\n    x = Symbol('x', real=True)\n    assert exp(3).evalf() == exp(3).n()\n    assert exp(pi).evalf() == exp(pi).n()\n    assert exp(2*I*pi).evalf() == exp(2*I*pi).n()\n    assert exp(I*pi/4).evalf() == exp(I*pi/4).n()\n    assert exp(x).evalf(subs={x: 2.5}) == exp(2.5)\n", "def test_exp_fdiff():\n    x = Symbol('x')\n    raises(ArgumentIndexError, lambda: exp(x).fdiff(2))\n    assert exp(x).fdiff() == exp(x)\n    ", "def test_exp_conjugate_extended():\n    from sympy import conjugate, Symbol\n    a, b = symbols('a b', real=True)\n    c = Symbol('c', complex=True)\n\n    assert conjugate(exp(a + I*b)) == exp(a - I*b)\n    assert conjugate(exp(c)) == exp(conjugate(c))\n    assert conjugate(exp(a + b*I + c)) == exp(a - b*I + conjugate(c))\n\n    x = Symbol('x', real=True)\n    assert exp(x*I).conjugate() == exp(-x*I)\n    assert exp(x + I*pi/4).conjugate() == exp(x - I*pi/4)\n    assert exp(I*(pi/4 + x)).conjugate() == exp(I*(-pi/4 - x))\n", "def test_exp_properties():\n    a = Symbol('a', extended_real=True)\n    b = Symbol('b', extended_real=True)\n    \n    # Test _eval_is_finite\n    assert exp(a).is_finite == (a != oo and a != -oo)\n    assert exp(-oo).is_finite is True\n    assert exp(oo).is_finite is False\n    \n    # Test _eval_is_extended_real\n    assert exp(a).is_extended_real is True\n    assert exp(I*a).is_extended_real == ((2*a/pi).is_integer and (2*a/pi).is_even)\n    \n    # Test _eval_is_extended_positive\n    assert exp(a).is_extended_positive == (a != -oo)\n    assert exp(I*a).is_extended_positive == ((a/pi).is_integer and (a/pi).is_even)\n    \n    # Test _eval_is_algebraic\n    assert exp(a).is_algebraic is None\n    assert exp(a + b).is_algebraic is None\n    assert exp(0).is_algebraic is True\n    assert exp(a*I).is_algebraic is None\n    assert exp(pi*Rational(1, 2)).is_algebraic is None\n    assert exp(pi*Rational(1, 3)).is_algebraic is False\n    assert exp(pi*Rational(1, 4)).is_algebraic is None\n"], "sample_1071": ["def test_check_dimensions():\n    from sympy import symbols\n    from sympy.physics.units import meter, second, joule\n\n    x = symbols('x')\n\n    assert check_dimensions(meter + meter) == meter + meter\n    assert check_dimensions(meter * second + meter * second) == meter * second + meter * second\n    assert check_dimensions(meter * joule) == meter * joule\n\n    with raises(ValueError):\n        check_dimensions(meter + 1)\n\n    with raises(ValueError):\n        check_dimensions(meter + second)\n\n    # Test with symbolic expressions\n    assert check_dimensions(x * meter + x * meter) == x * meter + x * meter\n\n    with raises(ValueError):\n        check_dimensions(x * meter + x)\n\n    with raises(ValueError):\n        check_dimensions(x * meter + x * second)\n", "def test_check_dimensions():\n    from sympy.physics.units import meter, second, kilogram\n    from sympy import symbols\n\n    x, y = symbols('x y')\n\n    # Compatible dimensions\n    assert check_dimensions(meter + 2*meter) == 3*meter\n    assert check_dimensions(meter * second + 2*meter * second) == 3*meter * second\n    assert check_dimensions(kilogram + 2*kilogram) == 3*kilogram\n\n    # Incompatible dimensions\n    with raises(ValueError):\n        check_dimensions(meter + second)\n    with raises(ValueError):\n        check_dimensions(meter + kilogram)\n    with raises(ValueError):\n        check_dimensions(meter * second + kilogram)\n\n    # Check for unitless addition with dimensional quantities\n    with raises(ValueError):\n        check_dimensions(meter + 1)\n    with raises(ValueError):\n        check_dimensions(2*meter + 3)\n", "def test_check_dimensions():\n    from sympy.physics.units.util import check_dimensions\n    from sympy.physics.units import meter, second, kilogram, newton\n\n    # Testing valid dimensional quantities\n    assert check_dimensions(meter + meter) == meter + meter\n    assert check_dimensions(3 * meter + 2 * meter) == 5 * meter\n    assert check_dimensions(meter * second) == meter * second\n    assert check_dimensions(newton + kilogram * meter / second**2) == newton + kilogram * meter / second**2\n\n    # Testing invalid dimensional additions\n    raises(ValueError, lambda: check_dimensions(meter + second))\n    raises(ValueError, lambda: check_dimensions(meter + 1))\n    raises(ValueError, lambda: check_dimensions(meter + kilogram))\n    raises(ValueError, lambda: check_dimensions(newton + meter))\n", "def test_check_dimensions():\n    from sympy import symbols\n    from sympy.physics.units import joule, meter, second, kilogram, watt, ampere, volt\n\n    x, y = symbols('x y')\n\n    # Valid expressions\n    assert check_dimensions(3 * meter) == 3 * meter\n    assert check_dimensions(joule / (meter * second)) == joule / (meter * second)\n    assert check_dimensions(kilogram + 2 * kilogram) == 3 * kilogram\n    assert check_dimensions(volt / ampere - watt) == 0\n\n    # Invalid expressions\n    raises(ValueError, lambda: check_dimensions(meter + 1))\n    raises(ValueError, lambda: check_dimensions(joule + meter))\n    raises(ValueError, lambda: check_dimensions(kilogram * second + joule))\n    raises(ValueError, lambda: check_dimensions(2 * kilogram + 3 * meter))\n", "def test_check_dimensions():\n    q1 = 3 * meter\n    q2 = 2 * meter\n    assert check_dimensions(q1 + q2) == 5 * meter\n\n    q3 = 4 * second\n    with raises(ValueError):\n        check_dimensions(q1 + q3)\n\n    q4 = 5 * meter * second\n    q5 = 2 * meter / second\n    assert check_dimensions(q4 * q5) == 10 * meter**2\n\n    q6 = 3 / second\n    q7 = 2 * second\n    assert check_dimensions(q6 * q7) == 6\n\n    q8 = 3 * meter + 2 * kilometer\n    assert check_dimensions(q8) == 2003 * meter\n\n    q9 = 2 * kilometer + 500 * meter\n    assert check_dimensions(q9) == 2.5 * kilometer\n\n    with raises(ValueError):\n        check_dimensions(meter + second)\n"], "sample_1072": ["def test_floor_complex_cases():\n    # Test floor function with complex numbers having both real and imaginary parts\n    assert floor(3 + 4*I) == 3 + 4*I\n    assert floor(-3 - 4*I) == -3 - 4*I\n    assert floor(3.5 + 4.7*I) == 3 + 4*I\n    assert floor(-3.5 - 4.7*I) == -4 - 5*I\n    \n    # Testing with complex expressions\n    assert floor(3 + 2*E + (4.7 + pi)*I) == 8 + 7*I\n    assert floor(-3 - 2*E - (4.7 + pi)*I) == -9 - 8*I\n", "def test_additional_frac_cases():\n    assert frac(0) == 0\n    assert frac(1) == 0\n    assert frac(-1) == 0\n    assert frac(1.5) == 0.5\n    assert frac(-1.5) == 0.5\n    assert frac(1 + I) == I\n    assert frac(-1 + I) == I\n    assert frac(1 - I) == -I\n    assert frac(-1 - I) == -I\n    assert frac(Rational(5, 3)) == Rational(2, 3)\n    assert frac(-Rational(5, 3)) == Rational(1, 3)\n    assert frac(exp(1)) == exp(1) - floor(exp(1))\n    assert frac(log(2)) == log(2) - floor(log(2))\n    assert frac(sqrt(3)) == sqrt(3) - floor(sqrt(3))\n    assert frac(factorial(5)) == 0\n    assert frac(floor(pi)) == 0\n    assert frac(ceiling(pi)) == 0\n    assert frac(frac(pi)) == frac(pi)\n    assert frac(frac(-pi)) == frac(-pi)\n    assert frac(frac(I)) == frac(I)\n    assert frac(frac(-I)) == frac(-I)\n    assert frac(frac(oo)) == AccumBounds(0, 1)\n    assert frac(frac(-oo)) == AccumBounds(0, 1)\n    assert frac(frac(zoo)) == nan\n", "def test_frac_complex_numbers():\n    assert frac(1 + I) == 0\n    assert frac(1 - I) == 0\n    assert frac(-1 + I) == 0\n    assert frac(-1 - I) == 0\n\n    r = Symbol('r', real=True)\n    i = Symbol('i', imaginary=True)\n    assert frac(r + i) == frac(r) + frac(i)\n    assert frac(r + I) == frac(r)\n    assert frac(1 + I*r) == frac(I*r)\n    assert frac(-1 + I*r) == frac(I*r)\n    assert frac(I*x) == I*frac(x)\n\n    assert (frac(I) <= 1) == True\n    assert (frac(I) < 1) == True\n    assert (frac(I) >= 0) == True\n    assert (frac(I) > 0) == True\n\n    assert (frac(I) <= -1) == False\n    assert (frac(I) < -1) == False\n    assert (frac(I) >= 1) == False\n    assert (frac(I) > 1) == False\n", "def test_issue_12345():\n    r = Symbol('r', real=True)\n    assert floor(r + 0.5) == floor(r)  # Testing edge case with fractional part 0.5\n    assert ceiling(r - 0.5) == ceiling(r)  # Testing edge case with fractional part -0.5\n\n    assert frac(2.5) == 0.5  # Testing frac with positive number\n    assert frac(-2.5) == 0.5  # Testing frac with negative number\n\n    assert floor(0.9999999) == 0  # Testing floor close to integer\n    assert ceiling(0.0000001) == 1  # Testing ceiling close to integer\n\n    assert frac(0.0000001) == 0.0000001  # Testing frac close to zero\n    assert frac(1 - 0.0000001) == 0.9999999  # Testing frac close to one\n\n    assert floor(exp(1)) == 2  # Testing floor with Euler's number\n    assert ceiling(exp(1)) == 3  # Testing ceiling with Euler's number\n\n    assert floor(sqrt(2)) == 1  # Testing floor with square root of 2\n    assert ceiling(sqrt(2)) == 2  # Testing ceiling with square root of 2\n\n    assert frac(sqrt(2)) == sqrt(2) - 1  # Testing frac with square root of 2\n", "def test_eval_number():\n    assert floor._eval_number(5.7) == 5\n    assert floor._eval_number(-5.7) == -6\n    assert floor._eval_number(pi) == 3\n    assert floor._eval_number(-pi) == -4\n    assert floor._eval_number(Float('0.5')) == 0\n    assert floor._eval_number(-Float('0.5')) == -1\n    assert ceiling._eval_number(5.7) == 6\n    assert ceiling._eval_number(-5.7) == -5\n    assert ceiling._eval_number(pi) == 4\n    assert ceiling._eval_number(-pi) == -3\n    assert ceiling._eval_number(Float('0.5')) == 1\n    assert ceiling._eval_number(-Float('0.5')) == 0\n"], "sample_1073": ["def test_is_sqrt():\n    assert is_sqrt(sqrt(2)) is True\n    assert is_sqrt(2**S.Half) is True\n    assert is_sqrt(2**(S(1)/2)) is True\n    assert is_sqrt(2**3) is False\n    assert is_sqrt(2) is False\n", "def test_sqrtdenest_edge_cases():\n    # Test with zero input\n    assert sqrtdenest(0) == 0\n    # Test with one input\n    assert sqrtdenest(1) == 1\n    # Test with a negative input that is not a perfect square\n    assert sqrtdenest(-5) == sqrt(-5)\n    # Test with an irrational input that cannot be simplified\n    assert sqrtdenest(sqrt(2) + sqrt(3)) == sqrt(2) + sqrt(3)\n    # Test with nested square roots that don't denest\n    assert sqrtdenest(sqrt(1 + sqrt(1 + sqrt(1 + sqrt(1 + sqrt(2)))))) == sqrt(1 + sqrt(1 + sqrt(1 + sqrt(1 + sqrt(2)))))\n", "def test_is_sqrt():\n    assert is_sqrt(sqrt(2))\n    assert not is_sqrt(2)\n    assert not is_sqrt(sqrt(2) + 1)\n    assert not is_sqrt(sqrt(2) * sqrt(3))\n    assert is_sqrt(sqrt(2) * sqrt(3) * sqrt(2))\n", "def test_is_sqrt():\n    assert is_sqrt(sqrt(2)) is True\n    assert is_sqrt(2**(S(1)/2)) is True\n    assert is_sqrt(2**(S(3)/2)) is False\n    assert is_sqrt(2) is False\n    assert is_sqrt(2**(S(1)/3)) is False\n", "def test_is_sqrt():\n    from sympy import Symbol\n    x = Symbol('x')\n    assert is_sqrt(sqrt(2)) == False\n    assert is_sqrt(2**S.Half) == True\n    assert is_sqrt(x**2) == False\n    assert is_sqrt(x**S.Half) == True\n    assert is_sqrt(x**(1/3)) == False\n    assert is_sqrt(1 + sqrt(2)) == False\n"], "sample_1074": ["def test_coset_representative():\n    G = SymmetricGroup(4)\n    H = PermutationGroup(Permutation(0, 1, 2))\n    g = Permutation(2, 3)\n    rep = G._coset_representative(g, H)\n    assert rep == Permutation(2, 3)\n    assert G._coset_representative(Permutation(0, 1, 2, 3), H) is None\n\n    # Testing coset representative for trivial group\n    I = PermutationGroup(Permutation(3))\n    assert G._coset_representative(Permutation(1, 2), I) == Permutation(1, 2)\n\n    # Testing when g is not in the group\n    assert G._coset_representative(Permutation(4), H) is None\n", "def test_is_perfect():\n    G = AlternatingGroup(3)\n    assert not G.is_perfect\n    G = AlternatingGroup(5)\n    assert G.is_perfect\n    G = PermutationGroup([Permutation(0, 1)(2, 3), Permutation(0, 2)(1, 3)])\n    assert not G.is_perfect\n    S = SymmetricGroup(3)\n    assert not S.is_perfect\n    S = SymmetricGroup(5)\n    assert not S.is_perfect\n", "def test_commutator_subgroups():\n    G = SymmetricGroup(5)\n    A = AlternatingGroup(5)\n    D = DihedralGroup(4)\n    \n    # Verify commutator subgroups\n    commutator_A_G = G.commutator(A, G)\n    assert commutator_A_G.is_subgroup(A)\n    \n    commutator_D_G = G.commutator(D, G)\n    assert commutator_D_G.order() == 20  # Dihedral group of order 4 and Symmetric group of order 120\n    \n    commutator_D_A = A.commutator(D, A)\n    assert commutator_D_A.order() == 5\n\n    # Test commutators with trivial groups\n    triv = PermutationGroup([Permutation([0, 1, 2, 3, 4])])\n    commutator_triv_G = G.commutator(triv, G)\n    assert commutator_triv_G.is_trivial\n\n    commutator_triv_A = A.commutator(triv, A)\n    assert commutator_triv_A.is_trivial\n", "def test_commutator_subgroup():\n    # Commutator of SymmetricGroup(3) and its AlternatingGroup(3)\n    S = SymmetricGroup(3)\n    A = AlternatingGroup(3)\n    comm = S.commutator(S, A)\n    assert comm.order() == 3  # Expected order of A(3)\n    assert comm.is_subgroup(A)\n    assert not A.is_subgroup(comm)\n\n    # Commutator of CyclicGroup(5) with itself should be trivial\n    C = CyclicGroup(5)\n    comm = C.commutator(C, C)\n    assert comm.is_trivial\n\n    # Commutator of DihedralGroup(4) and SymmetricGroup(4)\n    D = DihedralGroup(4)\n    S4 = SymmetricGroup(4)\n    comm = S4.commutator(S4, D)\n    assert comm.order() == 4  # Since it should reduce to A(4)\n    assert comm.is_subgroup(SymmetricGroup(3))\n\n    # Commutator of trivial group with any group should be trivial\n    triv = PermutationGroup([Permutation([0, 1, 2, 3])])\n    assert S.commutator(triv, S).is_trivial\n    assert A.commutator(triv, A).is_trivial\n", "def test_composition_series():\n    # Test composition series of cyclic group\n    G = CyclicGroup(12)\n    series = G.composition_series()\n    orders = [H.order() for H in series]\n    assert orders == [12, 6, 3, 1]\n\n    # Test composition series of symmetric group\n    S = SymmetricGroup(3)\n    series = S.composition_series()\n    orders = [H.order() for H in series]\n    assert orders == [6, 3, 1]\n\n    # Test composition series of dihedral group\n    D = DihedralGroup(8)\n    series = D.composition_series()\n    orders = [H.order() for H in series]\n    assert orders == [8, 4, 2, 1]\n\n    # Test composition series of alternating group\n    A = AlternatingGroup(4)\n    series = A.composition_series()\n    orders = [H.order() for H in series]\n    assert orders == [12, 6, 3, 1]\n"], "sample_1075": ["def test_beta_fdiff():\n    assert beta(x, y).fdiff(1) == beta(x, y) * (digamma(x) - digamma(x + y))\n    assert beta(x, y).fdiff(2) == beta(x, y) * (digamma(y) - digamma(x + y))\n    raises(ArgumentIndexError, lambda: beta(x, y).fdiff(3))\n", "def test_beta_fdiff():\n    x, y = Symbol('x'), Symbol('y')\n    \n    # Test differentiation with respect to x\n    assert beta(x, y).fdiff(1) == beta(x, y) * (digamma(x) - digamma(x + y))\n    \n    # Test differentiation with respect to y\n    assert beta(x, y).fdiff(2) == beta(x, y) * (digamma(y) - digamma(x + y))\n    \n    # Test for invalid argument index\n    raises(ArgumentIndexError, lambda: beta(x, y).fdiff(3))\n", "def test_beta_fdiff():\n    x = Symbol('x')\n    y = Symbol('y')\n    \n    # Test differentiation with respect to x\n    assert beta(x, y).fdiff(1) == beta(x, y)*(digamma(x) - digamma(x + y))\n    \n    # Test differentiation with respect to y\n    assert beta(x, y).fdiff(2) == beta(x, y)*(digamma(y) - digamma(x + y))\n    \n    # Test invalid argument index\n    raises(ArgumentIndexError, lambda: beta(x, y).fdiff(3))\n", "def test_beta_basic_properties():\n    x = Symbol('x')\n    y = Symbol('y')\n    \n    # Test the basic properties of the beta function\n    assert beta(1, x) == 1 / x\n    assert beta(x, 1) == 1 / x\n    assert beta(x, y) == beta(y, x)\n    assert beta(2, 3) == 1/6\n    assert beta(3, 2) == 1/6\n\n    # Test differentiation wrt x and y\n    assert diff(beta(x, y), x) == beta(x, y) * (digamma(x) - digamma(x + y))\n    assert diff(beta(x, y), y) == beta(x, y) * (digamma(y) - digamma(x + y))\n\n    # Test conjugate\n    assert conjugate(beta(x, y)) == beta(conjugate(x), conjugate(y))\n\n    # Test expansion\n    assert expand_func(beta(x, y)) == gamma(x) * gamma(y) / gamma(x + y)\n\n    # Test ArgumentIndexError\n    raises(ArgumentIndexError, lambda: beta(x, y).fdiff(3))\n", "def test_beta_diff():\n    x = Symbol('x')\n    y = Symbol('y')\n    \n    # Test differentiation with respect to x\n    assert diff(beta(x, y), x) == (digamma(x) - digamma(x + y)) * beta(x, y)\n    \n    # Test differentiation with respect to y\n    assert diff(beta(x, y), y) == (digamma(y) - digamma(x + y)) * beta(x, y)\n    \n    # Test invalid argument index\n    raises(ArgumentIndexError, lambda: beta(x, y).fdiff(3))\n"], "sample_1076": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n\n    assert p.doprint(Identity(3)) == 'sympy.eye(3)'\n\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n\n    smat = SparseMatrix(2, 5, {(0, 1): 3})\n    assert p.doprint(smat) == 'sympy.Matrix([[0, 3, 0, 0, 0], [0, 0, 0, 0, 0]])'\n    assert 'sympy' in p.module_imports\n\n    expr = MatrixSolve(Identity(3), MatrixSymbol('x', 3, 1))\n    assert p.doprint(expr) == 'sympy.solve_linear_system_LU(sympy.eye(3), x)'\n", "def test_print_while():\n    from sympy.codegen.ast import While\n\n    cond = x < 5\n    body = [Assignment(x, x + 1), Assignment(y, y * 2)]\n    while_node = While(cond, body)\n\n    prntr = PythonCodePrinter()\n    expected = \"while x < 5:\\n    x = x + 1\\n    y = y * 2\"\n    assert prntr.doprint(while_node) == expected\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    \n    expr = sqrt(x)\n    assert p.doprint(expr) == 'sympy.sqrt(x)'\n    assert 'sympy' in p.module_imports\n    \n    expr = x**2 + y**2\n    assert p.doprint(expr) == 'x**2 + y**2'\n    \n    expr = Mod(x, 2)\n    assert p.doprint(expr) == 'x % 2'\n    \n    expr = Piecewise((1, Eq(x, 0)), (2, x > 6))\n    assert p.doprint(expr) == 'sympy.Piecewise((1, x == 0), (2, x > 6))'\n    \n    expr = MatrixSymbol(\"A\", 2, 2) * MatrixSymbol(\"B\", 2, 2)\n    assert p.doprint(expr) == 'sympy.MatMul(A, B)'\n", "def test_custom_user_functions():\n    # Create a custom printer with user-defined functions\n    user_funcs = {'customfunc': 'custom_module.customfunc'}\n    prntr = PythonCodePrinter({'user_functions': user_funcs})\n\n    class CustomFunction(Expr):\n        pass\n\n    # Register the custom function in the printer\n    setattr(prntr, '_print_CustomFunction', lambda self, expr: self._print_known_func(expr))\n\n    # Test printing a custom function\n    custom_expr = CustomFunction()\n    assert prntr.doprint(custom_expr) == 'custom_module.customfunc()'\n\n    # Check module imports for the custom function\n    assert prntr.module_imports == {'custom_module': {'customfunc'}}\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(Mod(x, 2)) == 'x % 2'\n    assert p.doprint(Identity(3)) == 'sympy.eye(3)'\n    assert p.doprint(x**y) == 'x**y'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x > 6))) == 'Piecewise((1, Eq(x, 0)), (2, Gt(x, 6)))'\n    assert p.doprint(MatrixSymbol(\"A\", 2, 2)) == 'MatrixSymbol(\"A\", 2, 2)'\n    assert p.doprint(zoo) == 'zoo'\n    assert p.doprint(oo) == 'oo'\n    assert p.doprint(-oo) == '-oo'\n"], "sample_1077": ["def test_rationals_boundary():\n    R = S.Rationals\n    assert R.boundary == R\n    # Check that all boundary elements are in the set\n    assert all(x in R for x in R.boundary)\n    # Verify the boundary properties with some rational and non-rational numbers\n    assert S(1)/3 in R.boundary\n    assert S(-1)/3 in R.boundary\n    assert S(5) in R.boundary\n    assert S.Half in R.boundary\n    assert S(2)/7 in R.boundary\n    assert pi not in R.boundary\n    assert sqrt(2) not in R.boundary\n    assert 1.5 not in R.boundary\n    assert 0.1 not in R.boundary\n", "def test_rationals():\n    R = S.Rationals\n    assert 1/2 in R\n    assert -1/3 in R\n    assert 2 not in S.Rationals  # 2 should be included\n    assert 0.5 not in R\n    assert Rational(3, 4) in R\n    assert Rational(-5, 7) in R\n    assert Basic() not in R\n    assert 1.0 not in R\n    assert Rational(0, 1) in R\n    assert Rational(0, 1) == S.Zero\n    assert Rational(1, 1) == S.One\n    assert Rational(-1, 1) == S.NegativeOne\n\n    # Boundary tests\n    assert R.boundary == R\n    assert S.Zero in R.boundary\n    assert S.One in R.boundary\n    assert S.NegativeOne in R.boundary\n", "def test_complex_region_properties():\n    # Testing properties of ComplexRegion\n    unit_interval = Interval(0, 1)\n    theta_interval = Interval(0, 2 * pi)\n    polar_region = ComplexRegion(unit_interval * theta_interval, polar=True)\n\n    # Testing _measure property\n    assert polar_region._measure == pi * 2\n\n    # Testing sets property\n    assert polar_region.sets == unit_interval * theta_interval\n\n    # Testing variables and expr properties\n    assert polar_region.variables == symbols('r theta')\n    assert polar_region.expr == symbols('r') * (cos(symbols('theta')) + I * sin(symbols('theta')))\n    \n    # Testing psets property\n    assert polar_region.psets == (unit_interval * theta_interval, )\n\n    # Testing a_interval and b_interval properties\n    assert polar_region.a_interval == unit_interval\n    assert polar_region.b_interval == theta_interval\n\n    # Testing polar property\n    assert polar_region.polar == True\n\n    # Testing with rectangular form\n    rectangular_region = ComplexRegion(Interval(-1, 1) * Interval(-1, 1), polar=False)\n    assert rectangular_region._measure == 4\n    assert rectangular_region.sets == Interval(-1, 1) * Interval(-1, 1)\n    assert rectangular_region.variables == symbols('x y')\n    assert rectangular_region.expr == symbols('x') + I * symbols('y')\n    assert rectangular_region.psets == (Interval(-1, 1) * Interval(-1, 1), )\n    assert rectangular_region.a_interval == Interval(-1, 1)\n    assert rectangular_region.b_interval == Interval(-1, 1)\n    assert rectangular_region.polar == False\n", "def test_rationals_as_relational():\n    assert S.Rationals.as_relational(x) == And(Eq(floor(x), x), Eq(x, x))\n", "def test_rationals_boundary():\n    R = S.Rationals\n    assert R.boundary == R\n    assert R._boundary == R\n"], "sample_1078": ["def test_IndexedBase_iteration():\n    from sympy import Matrix\n    i = symbols('i', integer=True)\n    A = IndexedBase('A', shape=(3,))\n    B = IndexedBase('B', shape=(2, 2))\n    C = IndexedBase('C', shape=(2, 2))\n\n    # Test iteration over a simple 1D IndexedBase\n    iterable_1d = [A[i] for i in range(3)]\n    expected_1d = [A[0], A[1], A[2]]\n    assert iterable_1d == expected_1d\n\n    # Test iteration over a 2D IndexedBase\n    iterable_2d = [[B[i, j] for j in range(2)] for i in range(2)]\n    expected_2d = [[B[0, 0], B[0, 1]], [B[1, 0], B[1, 1]]]\n    assert iterable_2d == expected_2d\n\n    # Test iteration with symbolic indices\n    i, j = symbols('i j', integer=True)\n    iterable_symbolic = [[C[i, j] for j in range(2)] for i in range(2)]\n    expected_symbolic = [[C[0, 0], C[0, 1]], [C[1, 0], C[1, 1]]]\n    assert iterable_symbolic == expected_symbolic\n\n    # Test that iteration works with numpy arrays (if numpy is available)\n    try:\n        import numpy as np\n        A_np = IndexedBase('A_np', shape=(3,))\n        iterable_np = np.array([A_np[i] for i in range(3)])\n        expected_np = np.array([A_np[0], A_np[1], A_np[2]])\n        assert np.array_equal(iterable_np, expected_np)\n    except ImportError:\n        pass  # If numpy is not available, just skip this part of the test\n\n    # Ensure that multi-dimensional iteration returns a list of lists\n    assert isinstance(iterable_2d, list)\n    assert isinstance(iterable_2d[0], list)\n", "def test_IndexedBase_strides():\n    i, j, k = symbols('i j k', integer=True)\n    a = IndexedBase('a', shape=(3, 4, 5), strides=(12, 3, 1))\n    assert a.strides == (12, 3, 1)\n    b = Indexed(a, i, j, k)\n    assert b.base.strides == (12, 3, 1)\n    assert b.strides == (12, 3, 1)\n    c = IndexedBase('c', strides='C')\n    assert c.strides == 'C'\n    d = IndexedBase('d', strides='F')\n    assert d.strides == 'F'\n    e = IndexedBase('e')\n    assert e.strides is None\n    f = Indexed(e, i, j)\n    assert f.strides is None\n", "def test_IndexedBase_offset_and_strides():\n    i, j, k = symbols('i j k', integer=True)\n    m, n, o, p = symbols('m n o p', integer=True)\n    a = IndexedBase('a', shape=(m, n), offset=o, strides=(p, k))\n    assert a.shape == Tuple(m, n)\n    assert a.offset == o\n    assert a.strides == (p, k)\n    assert a[i, j].shape == Tuple(m, n)\n    assert a[i, j].offset == o\n    assert a[i, j].strides == (p, k)\n    assert a[i, j]._hashable_content() == ('a', Tuple(m, n), o, (p, k))\n", "def test_IndexedBase_strides_and_offset():\n    i, j, k = symbols('i j k', integer=True)\n    a = IndexedBase('a', shape=(10, 20), strides=(1, 10), offset=5)\n    assert a.strides == (1, 10)\n    assert a.offset == 5\n    A = a[i, j]\n    assert A.base.strides == (1, 10)\n    assert A.base.offset == 5\n\n    # Test if strides and offset are correctly applied in code generation\n    from sympy.printing import ccode\n    assert ccode(A) == \"a[1*i + 10*j + 5]\"\n    assert ccode(a[i]) == \"a[i + 5]\"\n    assert ccode(a[i, j, k]) == \"a[1*i + 10*j + k + 5]\"\n", "def test_IndexedBase_offset_strides():\n    i, j = symbols('i j', integer=True)\n    l, m, n, o = symbols('l m n o', integer=True)\n    A = IndexedBase('A', strides=(l, m, n), offset=o)\n    assert A.strides == (l, m, n)\n    assert A.offset == o\n    assert A[i, j].strides == (l, m, n)\n    assert A[i, j].offset == o\n\n    A_no_offset = IndexedBase('A', strides=(l, m, n))\n    assert A_no_offset.strides == (l, m, n)\n    assert A_no_offset.offset == S.Zero\n    assert A_no_offset[i, j].strides == (l, m, n)\n    assert A_no_offset[i, j].offset == S.Zero\n\n    A_no_strides = IndexedBase('A', offset=o)\n    assert A_no_strides.strides is None\n    assert A_no_strides.offset == o\n    assert A_no_strides[i, j].strides is None\n    assert A_no_strides[i, j].offset == o\n"], "sample_1079": ["def test_point_edge_cases():\n    # Test Point with negative coordinates\n    p_neg = Point(-1, -2)\n    assert p_neg.x == -1\n    assert p_neg.y == -2\n    assert p_neg + Point(1, 2) == Point(0, 0)\n    assert p_neg - Point(1, 2) == Point(-2, -4)\n\n    # Test Point with zero coordinates\n    p_zero = Point(0, 0)\n    assert p_zero.x == 0\n    assert p_zero.y == 0\n    assert p_zero + Point(1, 1) == Point(1, 1)\n    assert p_zero - Point(1, 1) == Point(-1, -1)\n\n    # Test distance with zero and negative coordinates\n    assert p_zero.distance(p_neg) == sqrt(5)\n    assert p_neg.distance(p_zero) == sqrt(5)\n    assert p_zero.taxicab_distance(p_neg) == 3\n    assert p_neg.taxicab_distance(p_zero) == 3\n\n    # Test midpoint with zero and negative coordinates\n    assert Point.midpoint(p_zero, p_neg) == Point(-0.5, -1)\n    assert Point.midpoint(p_neg, p_zero) == Point(-0.5, -1)\n\n    # Test Point with large coordinates\n    p_large = Point(1e10, 1e10)\n    assert p_large.x == 1e10\n    assert p_large.y == 1e10\n    assert p_large + Point(1, 1) == Point(1e10 + 1, 1e10 + 1)\n    assert p_large - Point(1, 1) == Point(1e10 - 1, 1e10 - 1)\n    assert p_large.distance(Point(0, 0)) == sqrt(2) * 1e10\n    assert p_large.taxicab_distance(Point(0, 0)) == 2 * 1e10\n\n    # Test Point with symbolic coordinates\n    x = Symbol('x')\n    y = Symbol('y')\n    p_sym = Point(x, y)\n    assert p_sym.x == x\n    assert p_sym.y == y\n    assert p_sym + Point(1, 1) == Point(x + 1, y + 1)\n    assert p_sym - Point(1", "def test_point3d_transformations():\n    p = Point3D(1, 2, 3)\n    assert p.translate(1, 1, 1) == Point3D(2, 3, 4)\n    assert p.scale(2, 2, 2) == Point3D(2, 4, 6)\n    assert p.scale(0.5, 0.5, 0.5) == Point3D(0.5, 1, 1.5)\n    assert p.rotate(pi/2, axis=\"x\") == Point3D(1, -3, 2)\n    assert p.rotate(pi, axis=\"y\") == Point3D(-1, 2, -3)\n    assert p.rotate(3*pi/2, axis=\"z\") == Point3D(-2, 1, 3)\n    \n    # Check invalid axis for rotation\n    raises(ValueError, lambda: p.rotate(pi/2, axis=\"invalid\"))\n", "def test_project():\n    # Testing projection of points onto lines\n\n    # Projecting a point onto a line between the origin and another point\n    origin = Point2D(0, 0)\n    p1 = Point2D(3, 4)\n    assert Point.project(p1, origin) == origin\n\n    # Projecting a point onto a line that is not the origin\n    p2 = Point2D(1, 1)\n    p3 = Point2D(2, 2)\n    assert Point.project(p2, p3) == p2\n\n    # Projecting 3D points\n    p4 = Point3D(1, 2, 3)\n    p5 = Point3D(4, 5, 6)\n    assert Point.project(p4, p5) == Point3D(Rational(14, 11), Rational(28, 11), Rational(42, 11))\n\n    # Projecting onto a zero vector should raise an error\n    raises(ValueError, lambda: Point.project(p1, origin))\n", "def test_project():\n    p1 = Point(3, 4)\n    p2 = Point(0, 2)\n    assert Point.project(p1, p2) == Point(0, 4)\n    assert Point.project(p2, p1) == Point(1.32, 1.76, evaluate=False)\n\n    p3 = Point3D(1, 2, 2)\n    p4 = Point3D(2, 1, 0)\n    assert Point3D.project(p3, p4) == Point3D(1.2, 0.6, 0, evaluate=False)\n\n    raises(ValueError, lambda: Point.project(Point(1, 1), Point(0, 0)))\n    raises(ValueError, lambda: Point3D.project(Point3D(1, 1, 1), Point3D(0, 0, 0)))\n", "def test_point_zero_vector_projection():\n    p1 = Point(1, 2)\n    p2 = Point(0, 0)\n    raises(ValueError, lambda: Point.project(p1, p2))\n"], "sample_1080": ["def test_complex_abs():\n    assert refine(Abs(x*y), Q.real(x) & Q.real(y)) == Abs(x*y)\n    assert refine(Abs(x*y*z), Q.real(x) & Q.real(y) & Q.real(z)) == Abs(x*y*z)\n    assert refine(Abs((w + I*x)*(y + I*z)), Q.real(w) & Q.real(x) & Q.real(y) & Q.real(z)) == \\\n        Abs(w + I*x) * Abs(y + I*z)\n", "def test_eval_refine_with_assumptions():\n    from sympy.core.expr import Expr\n    class MockExpr(Expr):\n            return assumptions\n    \n    expr = MockExpr()\n    assumptions = Q.positive(x)\n    assert refine(expr, assumptions) == assumptions\n", "def test_refine_abs_with_mul():\n    from sympy import Mul\n    assert refine(Abs(x * y), Q.positive(x) & Q.positive(y)) == x * y\n    assert refine(Abs(x * y), Q.negative(x) & Q.positive(y)) == -x * y\n    assert refine(Abs(x * y), Q.positive(x) & Q.negative(y)) == x * -y\n    assert refine(Abs(x * y), Q.negative(x) & Q.negative(y)) == -x * -y\n    assert refine(Abs(x * y * z), Q.positive(x) & Q.positive(y) & Q.positive(z)) == x * y * z\n    assert refine(Abs(x * y * z), Q.positive(x) & Q.positive(y) & Q.negative(z)) == x * y * -z\n", "def test_refine_edge_cases():\n    # Test refinement with non-Basic types\n    assert refine(\"non-basic\") == \"non-basic\"\n    assert refine(12345) == 12345\n\n    # Test refinement of non-atomic Basic instances\n    assert refine(x + y, Q.real(x) & Q.real(y)) == x + y\n    assert refine(x * y, Q.real(x) & Q.real(y)) == x * y\n\n    # Test refinement with non-existing handlers\n    class UnhandledExpr(Expr):\n        pass\n\n    expr = UnhandledExpr(x, y)\n    assert refine(expr, Q.real(x) & Q.real(y)) == expr\n\n    # Test refinement with None assumptions\n    assert refine(x + y, None) == x + y\n    assert refine(Abs(x), None) == Abs(x)\n\n    # Test refinement with combinations of handlers\n    assert refine(x**2 + Abs(x), Q.positive(x)) == x**2 + x\n    assert refine(x**2 + Abs(x), Q.negative(x)) == x**2 - x\n\n    # Test refinement with a mix of real and imaginary assumptions\n    assert refine(re(x) + im(y), Q.real(x) & Q.imaginary(y)) == x - I*y\n\n    # Test a more complex expression with multiple refinements\n    assert refine((Abs(x) + sqrt(x**2))**2, Q.real(x)) == (x + Abs(x))**2\n", "def test_refine_abs_non_real():\n    # Test that refine_abs correctly handles non-real inputs\n    assert refine(Abs(x*y), Q.positive(x) & Q.real(y)) == Abs(y) * x\n    assert refine(Abs(x*y), Q.negative(x) & Q.real(y)) == Abs(y) * -x\n    assert refine(Abs(x*y), Q.real(x) & Q.positive(y)) == Abs(x) * y\n    assert refine(Abs(x*y), Q.real(x) & Q.negative(y)) == Abs(x) * -y\n    assert refine(Abs(x*y), Q.imaginary(x) & Q.real(y)) == Abs(x) * Abs(y)\n    assert refine(Abs(x*y), Q.real(x) & Q.imaginary(y)) == Abs(x) * Abs(y)\n"], "sample_1081": ["def test_pollard_rho():\n    # Test on a composite number with small factors\n    assert pollard_rho(10403) in [101, 103]  # 10403 = 101 * 103\n\n    # Test on a large composite number\n    assert pollard_rho(1234567891011) in [123457, 10000841]  # 1234567891011 = 123457 * 10000841\n\n    # Test on a prime number (should return None)\n    assert pollard_rho(101) is None\n\n    # Test with retries and custom seed\n    assert pollard_rho(10403, retries=2, seed=42) in [101, 103]\n\n    # Test with a custom function F\n    F = lambda x: (x**2 + 1) % 10403\n    assert pollard_rho(10403, F=F) in [101, 103]\n\n    # Test with a number having a known factor\n    assert pollard_rho(2**64 + 1) == 274177  # 2^64 + 1 = 274177 * 67280421310721\n\n    # Test with a different start value s\n    assert pollard_rho(10403, s=3) in [101, 103]\n\n    # Test with a number that will not find a factor within max_steps\n    assert pollard_rho(10403, max_steps=1) is None\n", "def test_perfect_power_additional():\n    # Edge cases\n    assert perfect_power(0) is False\n    assert perfect_power(1) is False\n    \n    # Larger numbers\n    assert perfect_power(5**10) == (5, 10)\n    assert perfect_power(13**7) == (13, 7)\n    assert perfect_power(10**8) == (10, 8)\n    assert perfect_power(2**20 * 3**10) is False  # Not a perfect power\n    \n    # Complex base and exponent\n    assert perfect_power(6**9) == (6, 9)\n    assert perfect_power(8**5) == (2, 15)  # 2**15 == 8**5\n    assert perfect_power(7**12) == (7, 12)\n    \n    # Testing with specific candidates\n    assert perfect_power(81, [2, 4]) == (9, 2)  # 9^2 == 81\n    assert perfect_power(1024, [10]) is False  # 1024 is 2^10, not 10^x\n    assert perfect_power(625, [2, 4, 5]) == (5, 4)  # 5^4 == 625\n    \n    # Specific factor=False\n    assert perfect_power(16*81, factor=False) is False  # 16*81 == 2^4 * 3^4, not a single base\n", "def test_pollard_pm1():\n    assert pollard_pm1(21477639576571, B=2000) == 4410317\n    assert pollard_pm1(21477639576571, B=5000) == 4410317\n    assert pollard_pm1(257*1009, B=16, a=3) == 1009\n    assert pollard_pm1(257*1009, B=256, a=257) == 1009\n    assert pollard_pm1(257*1009, B=256) is None\n\n    raises(ValueError, lambda: pollard_pm1(3))\n    raises(ValueError, lambda: pollard_pm1(10, B=2))\n    raises(ValueError, lambda: pollard_pm1(21477639576571, B=2000, seed=-1234))\n\n    # verbose coverage for pollard_pm1\n    capture(lambda: pollard_pm1(21477639576571, B=2000, verbose=1))\n", "def test_pollard_rho():\n    # Test Pollard's rho on some known composite numbers\n    assert pollard_rho(91, seed=42) == 7\n    assert pollard_rho(8051, seed=42) in [83, 97]\n    assert pollard_rho(10403, seed=42) in [101, 103]\n    \n    # Test Pollard's rho with different seeds\n    assert pollard_rho(10403, seed=1) in [101, 103]\n    assert pollard_rho(10403, seed=5) in [101, 103]\n    \n    # Test Pollard's rho on a prime number should return None\n    assert pollard_rho(101) is None\n    \n    # Test Pollard's rho on a small composite number\n    assert pollard_rho(77, seed=1) == 7\n    assert pollard_rho(77, seed=2) == 11\n    \n    # Test Pollard's rho on a large composite number\n    large_composite = 2**61 - 1\n    assert pollard_rho(large_composite * 3, seed=1) == 3\n    assert pollard_rho(large_composite * 5, seed=2) == 5\n    \n    # Test edge cases with retries and max_steps\n    assert pollard_rho(8051, seed=42, retries=10) in [83, 97]\n    assert pollard_rho(8051, seed=42, max_steps=10000) in [83, 97]\n    \n    # Test ValueError for n <= 4\n    raises(ValueError, lambda: pollard_rho(4))\n    raises(ValueError, lambda: pollard_rho(3))\n    raises(ValueError, lambda: pollard_rho(2))\n    raises(ValueError, lambda: pollard_rho(1))\n    raises(ValueError, lambda: pollard_rho(0))\n", "def test_pollard_pm1():\n    assert pollard_pm1(10403, B=1000) == 101  # 10403 = 101 * 103\n    assert pollard_pm1(10403, B=10) is None  # 10403 is not B-smooth for B=10\n    assert pollard_pm1(21477639576571, B=1787) == 4410317  # 21477639576571 = 4410317 * 4869863\n    assert pollard_pm1(21477639576571, B=1786) is None  # B is too small\n    assert pollard_pm1(2347**2 * 7039 * 7043, B=1000) == 2347  # factor out one of the primes\n    raises(ValueError, lambda: pollard_pm1(3))\n    raises(ValueError, lambda: pollard_pm1(10, B=2))\n"], "sample_1082": ["def test_csch_rewrite():\n    x = Symbol('x')\n    assert csch(x).rewrite(exp) == 1 / (exp(x)/2 - exp(-x)/2) \\\n        == csch(x).rewrite('tractable')\n    assert csch(x).rewrite(cosh) == I/cosh(x + I*pi/2)\n    tanh_half = tanh(S.Half*x)\n    assert csch(x).rewrite(tanh) == (1 - tanh_half**2)/(2*tanh_half)\n    assert csch(x).rewrite(coth) == (coth_half**2 - 1)/(2*coth_half)\n", "def test_sinh_rewrite_as_cosh():\n    x = Symbol('x')\n    assert sinh(x).rewrite(cosh) == -S.ImaginaryUnit * cosh(x + S.Pi * S.ImaginaryUnit / 2)\n    assert sinh(2*x).rewrite(cosh) == -S.ImaginaryUnit * cosh(2*x + S.Pi * S.ImaginaryUnit / 2)\n    assert sinh(-x).rewrite(cosh) == -S.ImaginaryUnit * cosh(-x + S.Pi * S.ImaginaryUnit / 2)\n", "def test_sinh_eval():\n    x = Symbol('x')\n    \n    # Test for sinh with Add argument, which triggers _peeloff_ipi\n    assert sinh(pi*I/2 + x).expand() == sinh(x) + I*cosh(x)\n    assert sinh(pi*I*Rational(5, 2) + x).expand() == sinh(x) + I*cosh(x)\n    \n    # Test for sinh with asinh argument\n    assert sinh(asinh(x)) == x\n    \n    # Test for sinh with acosh argument\n    assert sinh(acosh(x)) == sqrt(x - 1) * sqrt(x + 1)\n    \n    # Test for sinh with atanh argument\n    assert sinh(atanh(x)) == x/sqrt(1 - x**2)\n    \n    # Test for sinh with acoth argument\n    assert sinh(acoth(x)) == 1/(sqrt(x - 1) * sqrt(x + 1))\n", "def test_csch_rewrite():\n    x = Symbol('x')\n    assert csch(x).rewrite(exp) == 2 / (exp(x) - exp(-x)) == csch(x).rewrite('tractable')\n    assert csch(x).rewrite(cosh) == I / cosh(x + I*pi/2)\n    tanh_half = tanh(S.Half*x)\n    assert csch(x).rewrite(tanh) == (1 - tanh_half**2) / (2 * tanh_half)\n    coth_half = coth(S.Half*x)\n    assert csch(x).rewrite(coth) == (coth_half**2 - 1) / (2 * coth_half)\n", "def test_sinh_eval_conditions():\n    x = Symbol('x', extended_real=True)\n    assert sinh(I*pi/3).eval(x) == S.Half*sqrt(3)*I\n    assert sinh(I*2*pi/3 + I*pi*x).eval(x) == Rational(-1, 2)*sqrt(3)*I*cosh(x) + cosh(I*2*pi/3)*sinh(x)\n    assert sinh(-x).eval(x) == -sinh(x)\n"], "sample_1083": ["def test_inverse_hyperbolic_rewrite():\n    x = Symbol('x')\n    assert asinh(x).rewrite(log) == log(x + sqrt(x**2 + 1))\n    assert acosh(x).rewrite(log) == log(x + sqrt(x + 1) * sqrt(x - 1))\n    assert atanh(x).rewrite(log) == (log(1 + x) - log(1 - x)) / 2\n    assert acoth(x).rewrite(log) == (log(1 + 1/x) - log(1 - 1/x)) / 2\n    assert asech(x).rewrite(log) == log(1/x + sqrt(1/x - 1) * sqrt(1/x + 1))\n    assert acsch(x).rewrite(log) == log(1/x + sqrt(1/x**2 + 1))\n", "def test_acoth_values():\n    x = Symbol('x')\n\n    # Values at specific points\n    assert acoth(2) == log((2 + 1)/(2 - 1)) / 2\n    assert acoth(-2) == -log((2 + 1)/(2 - 1)) / 2\n    assert acoth(Rational(1, 2)) == I*pi/2 + log(3)/2\n    assert acoth(-Rational(1, 2)) == -I*pi/2 - log(3)/2\n\n    # Edge cases\n    assert acoth(1 + I*0) == oo\n    assert acoth(-1 + I*0) == -oo\n    assert acoth(oo + I*0) == 0\n    assert acoth(-oo + I*0) == 0\n\n    # Testing with imaginary numbers\n    assert acoth(I) == -I*pi/4\n    assert acoth(-I) == I*pi/4\n    assert acoth(2*I) == -I*pi/8\n    assert acoth(-2*I) == I*pi/8\n\n    # Properties\n    assert acoth(-x) == -acoth(x)\n    assert acoth(1/x) == atanh(x)\n\n    # Testing for simplifications\n    assert sinh(acoth(x)) == 1/(sqrt(x - 1) * sqrt(x + 1))\n    assert cosh(acoth(x)) == x/(sqrt(x - 1) * sqrt(x + 1))\n    assert tanh(acoth(x)) == 1/x\n\n    # inverse composition\n    assert acoth(coth(2)) == 2\n    assert acoth(coth(-2)) == -2\n    assert acoth(coth(x)).simplify() == x\n", "def test_inverses_rewrite():\n    x = Symbol('x')\n\n    assert sinh(asinh(x)).rewrite(sinh) == x\n    assert cosh(acosh(x)).rewrite(cosh) == x\n    assert tanh(atanh(x)).rewrite(tanh) == x\n    assert coth(acoth(x)).rewrite(coth) == x\n    assert sech(asech(x)).rewrite(sech) == x\n    assert csch(acsch(x)).rewrite(csch) == x\n\n    assert asinh(sinh(x)).rewrite(asinh) == x\n    assert acosh(cosh(x)).rewrite(acosh) == x\n    assert atanh(tanh(x)).rewrite(atanh) == x\n    assert acoth(coth(x)).rewrite(acoth) == x\n    assert asech(sech(x)).rewrite(asech) == x\n    assert acsch(csch(x)).rewrite(acsch) == x\n", "def test_sinh_eval():\n    x = Symbol('x')\n\n    # Test the evaluation of sinh at various arguments\n    assert sinh(asinh(x)) == x\n    assert sinh(acosh(x)) == sqrt(x - 1) * sqrt(x + 1)\n    assert sinh(atanh(x)) == x / sqrt(1 - x**2)\n    assert sinh(acoth(x)) == 1 / (sqrt(x - 1) * sqrt(x + 1))\n    assert sinh(0) == 0\n    assert sinh(pi*I) == 0\n    assert sinh(2*I) == I*sin(2)\n    assert sinh(1 + I) == sinh(1) * cos(1) + I * cosh(1) * sin(1)\n", "def test_acsch_series():\n    x = Symbol('x')\n    assert acsch(x).series(x, 0, 10) == \\\n        log(1/x) - x**2/2 - 3*x**4/8 - 5*x**6/16 - 35*x**8/128 + O(x**10)\n"], "sample_1084": ["def test_intersection_sets():\n    A = FiniteSet(1, 2, 3)\n    B = FiniteSet(3, 4, 5)\n    assert intersection_sets(A, B) == FiniteSet(3)\n    \n    A = Interval(1, 5)\n    B = Interval(3, 7)\n    assert intersection_sets(A, B) == Interval(3, 5)\n\n    A = Interval(1, 5, True, True)\n    B = Interval(3, 7, True, True)\n    assert intersection_sets(A, B) == Interval(3, 5, True, True)\n\n    A = Interval(1, 5)\n    B = S.Naturals\n    assert intersection_sets(A, B) == FiniteSet(1, 2, 3, 4, 5)\n\n    A = ComplexRegion(Interval(1, 2) * Interval(0, pi), polar=True)\n    B = S.Reals\n    assert intersection_sets(A, B) == Interval(-2, 2)\n\n    A = Range(1, 10, 2)\n    B = Range(2, 11, 2)\n    assert intersection_sets(A, B) == S.EmptySet\n\n    A = ImageSet(Lambda(x, x**2), S.Naturals)\n    B = S.Integers\n    assert intersection_sets(A, B) == ImageSet(Lambda(x, x**2), S.Naturals)\n\n    A = S.Integers\n    B = S.Rationals\n    assert intersection_sets(A, B) == S.Integers\n\n    A = S.UniversalSet\n    B = S.Integers\n    assert intersection_sets(A, B) == S.Integers\n\n    A = EmptySet()\n    B = S.Naturals\n    assert intersection_sets(A, B) == S.EmptySet\n\n    A = ProductSet(S.Naturals, Interval(0, 1))\n    B = ProductSet(S.Naturals0, Interval(0, 1))\n    assert intersection_sets(A, B) == ProductSet(S.Naturals, Interval(0, 1))\n", "def test_ComplexRegion_intersection_with_other_sets():\n    # Test intersection of ComplexRegion with other sets\n\n    # Rectangular form\n    rect_region = ComplexRegion(Interval(0, 2)*Interval(0, 2))\n    interval_set = Interval(1, 3)\n    finite_set = FiniteSet(1 + I, 2 + 2*I, 3 + I)\n    product_set = ProductSet(Interval(0, 2), Interval(0, 2))\n    union_set = Union(Interval(0, 1), Interval(1, 2))\n\n    assert rect_region.intersect(interval_set) == FiniteSet(1, 2)\n    assert rect_region.intersect(finite_set) == FiniteSet(1 + I, 2 + 2*I)\n    assert rect_region.intersect(product_set) == rect_region\n    assert rect_region.intersect(union_set) == rect_region\n\n    # Polar form\n    polar_region = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    interval_set = Interval(0, 1)\n    finite_set = FiniteSet(1, I, 1 + I)\n    product_set = ProductSet(Interval(0, 1), Interval(0, 2*S.Pi))\n    union_set = Union(Interval(0, 0.5), Interval(0.5, 1))\n\n    assert polar_region.intersect(interval_set) == Interval(0, 1)\n    assert polar_region.intersect(finite_set) == FiniteSet(1)\n    assert polar_region.intersect(product_set) == polar_region\n    assert polar_region.intersect(union_set) == polar_region\n", "def test_intersection_sets():\n    # Test intersection of ConditionSet with Set\n    from sympy.sets.conditionset import ConditionSet\n    from sympy import Eq\n    C1 = ConditionSet(x, Eq(x % 2, 0), S.Integers)\n    C2 = ConditionSet(x, Eq(x % 3, 0), S.Integers)\n    I = Intersection(C1, C2)\n    assert intersection_sets(C1, S.Reals) == ConditionSet(x, Eq(x % 2, 0), S.Integers)\n    assert intersection_sets(C1, S.Integers) == ConditionSet(x, Eq(x % 2, 0), S.Integers)\n    assert intersection_sets(C1, C2) is None  # Currently returns None as per the implementation\n\n    # Test intersection of ComplexRegion with Interval\n    CR1 = ComplexRegion(Interval(0, 1) * Interval(0, 2 * S.Pi), polar=True)\n    assert intersection_sets(CR1, Interval(0, 1)) == Interval(0, 1)\n    assert intersection_sets(CR1, Interval(1, 2)) == S.EmptySet\n\n    # Test intersection of FiniteSet with Set\n    F1 = FiniteSet(1, 2, 3)\n    assert intersection_sets(F1, Interval(1, 2)) == FiniteSet(1, 2)\n    assert intersection_sets(F1, Interval(4, 5)) == S.EmptySet\n\n    # Test intersection of Rationals with Reals\n    assert intersection_sets(S.Rationals, S.Reals) == S.Rationals\n\n    # Test intersection of Integers with Interval\n    assert intersection_sets(S.Integers, Interval(0, 10)) == Range(0, 11)\n    assert intersection_sets(S.Integers, Interval(-5, 5)) == Range(-5, 6)\n", "def test_ComplexRegion_intersections():\n    polar_set1 = ComplexRegion(Interval(0, 2)*Interval(0, S.Pi), polar=True)\n    polar_set2 = ComplexRegion(Interval(1, 3)*Interval(S.Pi/2, 3*S.Pi/2), polar=True)\n    assert polar_set1.intersect(polar_set2) == ComplexRegion(Interval(1, 2)*Interval(S.Pi/2, S.Pi), polar=True)\n\n    rectangular_set1 = ComplexRegion(Interval(1, 4)*Interval(1, 4))\n    rectangular_set2 = ComplexRegion(Interval(3, 5)*Interval(3, 5))\n    assert rectangular_set1.intersect(rectangular_set2) == ComplexRegion(Interval(3, 4)*Interval(3, 4))\n\n    mixed_set1 = ComplexRegion(Interval(0, 2)*Interval(0, S.Pi), polar=True)\n    mixed_set2 = ComplexRegion(Interval(-1, 1)*Interval(-1, 1))\n    assert mixed_set1.intersect(mixed_set2) == Intersection(mixed_set1, mixed_set2, evaluate=False)\n\n    # Test more complex intersections involving FiniteSets\n    finite_set1 = ComplexRegion(FiniteSet(2, 3)*FiniteSet(5, 6))\n    finite_set2 = ComplexRegion(FiniteSet(3, 4)*FiniteSet(6, 7))\n    assert finite_set1.intersect(finite_set2) == ComplexRegion(FiniteSet(3)*FiniteSet(6))\n", "def test_complexregion_intersection_with_subset_of_reals():\n    # Test intersection of ComplexRegion with subsets of Reals\n    real_interval = Interval(-2, 2)\n    complex_region_rect = ComplexRegion(Interval(-3, 3) * Interval(-3, 3))\n    complex_region_polar = ComplexRegion(Interval(0, 2) * Interval(0, 2*pi), polar=True)\n\n    # Rectangular form\n    intersection_rect = complex_region_rect.intersection(real_interval)\n    assert intersection_rect == Interval(-2, 2)\n\n    # Polar form\n    intersection_polar = complex_region_polar.intersection(real_interval)\n    assert intersection_polar == Intersection(Interval(-2, 2), Interval(-2, 2))\n"], "sample_1085": ["def test_as_integer_ratio():\n    assert Float(0.5).as_integer_ratio() == (1, 2)\n    assert Float(1.5).as_integer_ratio() == (3, 2)\n    assert Rational(3, 4).as_integer_ratio() == (3, 4)\n    assert Integer(5).as_integer_ratio() == (5, 1)\n    assert (2**53).as_integer_ratio() == (2**53, 1)\n    assert raises(AttributeError, lambda: Float('nan').as_integer_ratio())\n    assert raises(AttributeError, lambda: Float('inf').as_integer_ratio())\n    assert raises(AttributeError, lambda: Float('-inf').as_integer_ratio())\n", "def test_comp_special_cases():\n    # Testing special cases with complex numbers\n    assert comp(2 + 3*I, 2 + 3*I)\n    assert not comp(2 + 3*I, 2 + 4*I)\n    assert not comp(2 + 3*I, 3 + 3*I)\n    assert comp(0, 0 + 0*I)\n    assert comp(3*I, 0 + 3*I)\n\n    # Testing comparison with real and imaginary parts\n    assert comp(sqrt(2) + sqrt(3)*I, sqrt(2) + sqrt(3)*I, tol=1e-10)\n    assert not comp(sqrt(2) + sqrt(3)*I, sqrt(2) + sqrt(4)*I, tol=1e-10)\n    assert not comp(sqrt(2) + sqrt(3)*I, sqrt(2) + sqrt(3.1)*I, tol=1e-10)\n    assert comp(sqrt(2) + sqrt(3)*I, sqrt(2) + sqrt(3.0000000001)*I, tol=1e-10)\n\n    # Testing with different types\n    assert comp(0.5, Rational(1, 2))\n    assert not comp(0.5, Rational(2, 3))\n    assert comp(3.14, Float('3.14'))\n    assert not comp(3.14, Float('3.141'))\n    assert comp(1 + 2*I, 1.0 + 2.0*I)\n    assert not comp(1 + 2*I, 1.0 + 2.1*I)\n\n    # Testing with None type\n    raises(TypeError, lambda: comp(None, 1))\n    raises(TypeError, lambda: comp(1, None))\n    raises(TypeError, lambda: comp(None, None))\n", "def test_comp_additional_cases():\n    assert comp(1, 1.0)\n    assert not comp(1, 1.1)\n    assert comp(2.0, 2)\n    assert not comp(2.0, 2.1)\n    assert not comp(pi, 3.14, 0.0001)\n    assert comp(pi, 3.14, 0.01)\n    assert not comp(0.1 + 0.2, 0.3)  # Testing IEEE floating point precision issue\n    assert comp(0.1 + 0.2, 0.3, tol=1e-15)\n", "def test_comp_errors():\n    raises(ValueError, lambda: comp(1, 'string'))\n    raises(ValueError, lambda: comp(pi, 'string'))\n    raises(ValueError, lambda: comp('string', pi))\n    raises(ValueError, lambda: comp('string', 1.5))\n    raises(ValueError, lambda: comp('string', Rational(1, 3)))\n    raises(ValueError, lambda: comp(1.5, 'string'))\n    raises(ValueError, lambda: comp(Rational(1, 3), 'string'))\n", "def test_mpf_norm_zero_cases():\n    # Test cases for mpf_norm function to check proper handling of zero cases\n    # Case when mantissa is 0 and bc (bit count) is 0, should return fzero\n    assert mpf_norm((0, 0, 0, 0), 53) == fzero\n    \n    # Case when mantissa is 0 but bc is non-zero, should not change the mpf tuple\n    mpf_tuple = (0, 0, 0, 1)\n    assert mpf_norm(mpf_tuple, 53) == mpf_tuple\n    \n    # Normalizing a typical mpf tuple\n    mpf_tuple = (0, 1, 1, 53)\n    expected_result = (0, MPZ(1), 1, 53)\n    assert mpf_norm(mpf_tuple, 53) == expected_result\n"], "sample_1086": ["def test_Complexes():\n    assert str(S.Complexes) == 'Complexes'\n    assert str(S.Complexes - S.Reals) == 'Complexes \\ Reals'\n    assert str(Complement(S.Complexes, S.Reals)) == 'Complement(Complexes, Reals)'\n", "def test_Integral_with_multiple_variables():\n    assert str(Integral(x*y, (x, 0, 1), (y, 0, 2))) == \"Integral(x*y, (x, 0, 1), (y, 0, 2))\"\n    assert str(Integral(x**2 + y**2, (x, -oo, oo), (y, -oo, oo))) == \"Integral(x**2 + y**2, (x, -oo, oo), (y, -oo, oo))\"\n    assert str(Integral(sin(x)*cos(y), (x, 0, pi), (y, 0, pi))) == \"Integral(sin(x)*cos(y), (x, 0, pi), (y, 0, pi))\"\n    assert str(Integral(log(x)*exp(y), (x, 1, E), (y, 0, 1))) == \"Integral(log(x)*exp(y), (x, 1, E), (y, 0, 1))\"\n", "def test_Exp1():\n    assert str(E) == \"E\"\n    assert str(E**2) == \"E**2\"\n    assert str(1/E) == \"1/E\"\n", "def test_issue_12345():\n    class CustomClass(Expr):\n            self.value = value\n        \n            return f\"CustomClass({self.value})\"\n        \n    cc = CustomClass(42)\n    assert str(cc) == \"CustomClass(42)\"\n    assert sstr(cc) == \"CustomClass(42)\"\n\n    # Test with nested CustomClass\n    nested_cc = CustomClass(CustomClass(10))\n    assert str(nested_cc) == \"CustomClass(CustomClass(10))\"\n    assert sstr(nested_cc) == \"CustomClass(CustomClass(10))\"\n", "def test_Exp1():\n    assert str(E) == \"E\"\n"], "sample_1087": ["def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [t.rep.rep for t in [f, g, h]] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n"], "sample_1088": ["def test_viete():\n    r1, r2 = symbols('r1 r2')\n    p = a * x**2 + b * x + c\n\n    assert viete(p, [r1, r2], x) == [(r1 + r2, -b/a), (r1*r2, c/a)]\n    \n    p = x**2 + 5*x + 6\n    assert viete(p, [r1, r2], x) == [(r1 + r2, -5), (r1*r2, 6)]\n    \n    raises(MultivariatePolynomialError, lambda: viete(x*y + y**2 + x**2, [r1, r2], x, y))\n\n    p = 3 * x**3 - 7 * x**2 + 2 * x - 5\n    r1, r2, r3 = symbols('r1 r2 r3')\n    assert viete(p, [r1, r2, r3], x) == [(r1 + r2 + r3, 7/3), (r1*r2 + r2*r3 + r3*r1, 2/3), (r1*r2*r3, -5/3)]\n    \n    raises(ValueError, lambda: viete(a, [r1], x))\n    raises(ValueError, lambda: viete(a*x, [r1], x))\n", "def test_viete():\n    r1, r2, r3 = symbols('r1 r2 r3')\n    \n    # Test for quadratic polynomial\n    assert viete(a*x**2 + b*x + c, [r1, r2], x) == [(r1 + r2, -b/a), (r1*r2, c/a)]\n\n    # Test for cubic polynomial\n    assert viete(a*x**3 + b*x**2 + c*x + d, [r1, r2, r3], x) == [\n        (r1 + r2 + r3, -b/a), \n        (r1*r2 + r2*r3 + r3*r1, c/a), \n        (r1*r2*r3, -d/a)\n    ]\n\n    # Test exception for constant polynomial\n    raises(ValueError, lambda: viete(a, [r1], x))\n    \n    # Test exception for multivariate polynomial\n    raises(MultivariatePolynomialError, lambda: viete(a*x**2 + b*y + c, [r1, r2], x, y))\n\n    # Test automatically generating roots\n    assert viete(a*x**2 + b*x + c, x) == [(r1 + r2, -b/a), (r1*r2, c/a)]\n", "def test_viete():\n    r1, r2, r3 = symbols('r1 r2 r3')\n\n    assert viete(a*x**2 + b*x + c, [r1, r2], x) == [(r1 + r2, -b/a), (r1*r2, c/a)]\n    assert viete(x**2 - 1, [r1, r2], x) == [(r1 + r2, 0), (r1*r2, -1)]\n    assert viete(x**2 + 3*x + 2, [r1, r2], x) == [(r1 + r2, -3), (r1*r2, 2)]\n    \n    raises(ValueError, lambda: viete(1, [r1, r2], x))\n    raises(MultivariatePolynomialError, lambda: viete(a*x*y + b*x + c, [r1, r2], x, y))\n", "def test_viete():\n    r1, r2 = symbols('r1 r2')\n    \n    assert viete(a*x**2 + b*x + c, [r1, r2], x) == [(r1 + r2, -b/a), (r1*r2, c/a)]\n    \n    raises(MultivariatePolynomialError, lambda: viete(a*x*y + b*x + c, [r1, r2], x, y))\n    raises(ValueError, lambda: viete(a, [r1], x))\n\n    assert viete(x**3 + 6*x**2 + 11*x + 6, [r1, r2, r3], x) == [\n        (r1 + r2 + r3, -6), (r1*r2 + r2*r3 + r1*r3, 11), (r1*r2*r3, -6)\n    ]\n", "def test_viete():\n    r1, r2, r3 = symbols('r1 r2 r3')\n\n    assert viete(a*x**2 + b*x + c, [r1, r2], x) == [(r1 + r2, -b/a), (r1*r2, c/a)]\n    assert viete(x**2 + 5*x + 6, [r1, r2], x) == [(r1 + r2, -5), (r1*r2, 6)]\n    assert viete(x**3 - 6*x**2 + 11*x - 6, [r1, r2, r3], x) == [\n        (r1 + r2 + r3, 6), (r1*r2 + r2*r3 + r1*r3, -11), (r1*r2*r3, 6)\n    ]\n\n    raises(ValueError, lambda: viete(a, [r1, r2], x))\n    raises(MultivariatePolynomialError, lambda: viete(a*x**2 + b*y + c, [r1, r2], x, y))\n"], "sample_1089": ["def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(y*Rational(2, 3))) == (x**(y/3), 2)\n    assert decompose_power_rat(x**sqrt(2)) == (x, sqrt(2))\n    assert decompose_power_rat(x**(2*sqrt(2))) == (x**sqrt(2), 2)\n", "def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(y*Rational(2, 3))) == (x**(y/3), 2)\n    assert decompose_power_rat(x**sqrt(2)) == (x, sqrt(2))\n    assert decompose_power_rat(x**(2*I)) == (x**I, 2)\n    assert decompose_power_rat(2**(2/3)) == (2**(1/3), 2)\n", "def test_decompose_power_edge_cases():\n    # Test decompose_power with edge cases and additional scenarios\n    assert decompose_power(S(2)**x) == (S(2), x)\n    assert decompose_power(S(2)**Rational(3, 2)) == (S(2), Rational(3, 2))\n    assert decompose_power((x*y)**2) == (x*y, 2)\n    assert decompose_power((x**2*y**2)**3) == (x**2*y**2, 3)\n    assert decompose_power((x**(2*y))**3) == (x**(2*y), 3)\n    assert decompose_power(S(2)**(x*y)) == (S(2), x*y)\n    assert decompose_power(1) == (S.One, 1)\n    assert decompose_power(x**0) == (S.One, 1)\n    assert decompose_power((x**2)**(1/2)) == (x, 1)\n", "def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(y*Rational(2, 3))) == (x**(y/3), 2)\n    assert decompose_power_rat(x**2.5) == (x, 2.5)\n", "def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(y*Rational(2, 3))) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(2*y/3 + 1/3)) == (x**(y/3 + Rational(1, 6)), 2)\n"], "sample_1090": ["def test_comp():\n    from sympy.core.numbers import comp\n\n    assert comp(3.142, 3.142) == True\n    assert comp(3.142, 3.141) == False\n    assert comp(3.142, 3.143) == False\n\n    assert comp(3.142, '3.142') == True\n    assert comp(3.142, '3.141') == False\n\n    assert comp(3.142, 3.1415) == True\n    assert comp(3.142, 3.1415, '') == False\n\n    assert comp(3.142, 3.14, .001) == True\n    assert comp(3.142, 3.14, .0005) == False\n\n    assert comp(1/3.142, 0.3183, 1e-5) == True\n\n    assert comp(3.142 - 3.14, 0, .002) == True\n    assert comp(3.142 - 3.14, 0, .001) == False\n\n    assert comp(0, 0) == True\n    assert comp(0, 0.0) == True\n    assert comp(3.142, 0) == False\n", "def test_comp():\n    from sympy.core.numbers import comp, Float\n    pi4 = Float('3.142')\n\n    assert comp(pi4, 3.142) is True\n    assert comp(pi4, 3.141) is False\n    assert comp(pi4, 3.143) is False\n    assert comp(pi4, '3.142') is True\n    assert comp(pi4, '3.1415') is True\n    assert comp(pi4, '3.1415', '') is False\n\n    assert comp(pi4, 3.14, 0.001) is True\n    assert comp(pi4, 3.14, 0.0005) is False\n\n    inv_pi4 = Float('0.3183')\n    assert comp(inv_pi4, 0.3183, 1e-5) is True\n\n    assert comp(pi4 - 3.14, 0, 0.002) is True\n    assert comp(pi4 - 3.14, 0, 0.001) is False\n", "def test_comp():\n    from sympy import pi\n\n    # Test when tol is None\n    pi4 = pi.n(4)  # 3.142\n    assert comp(pi4, 3.142) == True\n    assert comp(pi4, 3.141) == False\n    assert comp(pi4, 3.143) == False\n\n    # Test comparison with strings\n    assert comp(pi4, '3.142') == True\n    assert comp(pi4, '3.141') == False\n    assert comp(pi4, '3.143') == False\n    assert comp(pi4, '3.1415') == True\n    assert comp(pi4, '3.1415', '') == False\n\n    # Test with tolerance provided\n    assert comp(pi4, 3.14, 0.001) == True\n    assert comp(pi4, 3.14, 0.0005) == False\n\n    # Test when abs(z1) <= 1\n    assert comp(1 / pi4, 0.3183, 1e-5) == True\n    assert comp(1 / pi4, 0.31831, 1e-5) == False\n\n    # Test absolute error comparison\n    assert comp(pi4 - 3.14, 0, 0.002) == True\n    assert comp(pi4 - 3.14, 0, 0.001) == False\n\n    # Test edge cases\n    assert comp(0, 0) == True\n    assert comp(0, 1) == False\n", "def test_comp():\n    from sympy import pi, Float\n    pi4 = pi.n(4)\n    \n    assert comp(pi4, 3.142) is True\n    assert comp(pi4, 3.141) is False\n    assert comp(pi4, 3.143) is False\n    assert comp(pi4, 3.1415) is True\n    assert comp(pi4, 3.1415, '') is False\n    assert comp(pi4, 3.14, 0.001) is True\n    assert comp(pi4, 3.14, 0.0005) is False\n    assert comp(1/pi4, 0.3183, 1e-5) is True\n    assert comp(pi4 - 3.14, 0, 0.002) is True\n    assert comp(pi4 - 3.14, 0, 0.001) is False\n    assert comp(Float('0.3'), 0.3) is True\n    assert comp(Float('0.3'), 0.30000000000000004) is False\n\n    # Test raising ValueError when comparing with string\n    try:\n        comp(pi, 'not a number')\n    except ValueError as e:\n        assert str(e) == 'when z2 is a str z1 must be a Number'\n", "def test_comp():\n    # Test for None tolerance\n    assert comp(3.142, 3.142) is True\n    assert comp(3.142, 3.141) is False\n    assert comp(3.142, 3.143) is False\n\n    # Test for string comparison\n    assert comp(3.142, '3.142') is True\n    assert comp(3.142, '3.141') is False\n\n    # Test for tolerance as string\n    assert comp(3.142, 3.142, '') is True\n    assert comp(3.142, 3.141, '') is False\n\n    # Test for provided tolerance\n    assert comp(3.142, 3.14, 0.001) is True\n    assert comp(3.142, 3.14, 0.0005) is False\n\n    # Test for absolute error tolerance\n    assert comp(0.3183, 0.3183, 1e-5) is True\n    assert comp(0.3183, 0.3184, 1e-5) is False\n\n    # Test for absolute error between z1 and z2\n    assert comp(3.142 - 3.14, 0, 0.002) is True\n    assert comp(3.142 - 3.14, 0, 0.001) is False\n\n    # Test for invalid comparison\n    try:\n        comp(3.142, 'invalid')\n    except ValueError as e:\n        assert str(e) == 'when z2 is a str z1 must be a Number'\n\n    # Test for non-number comparison\n    try:\n        comp('invalid', 3.142)\n    except ValueError as e:\n        assert str(e) == 'expecting 2 numbers'\n"], "sample_1091": ["def test_issue_new_test():\n    # Test for Eq with invalid relational operator symbol\n    raises(ValueError, lambda: Rel(x, y, 'invalid'))\n\n    # Test for canonical with BooleanAtoms\n    a = Symbol('a', real=True)\n    b = Symbol('b', real=True)\n    assert Eq(a > b, True).canonical == (a > b)\n    assert Eq(a > b, False).canonical == (a <= b)\n\n    # Test for reversed and reversedsign with BooleanAtoms\n    assert (a > b).reversed.reversedsign == (a > b).reversedsign.reversed\n    assert (a < b).reversed.reversedsign == (a < b).reversedsign.reversed\n\n    # Test for equals method with BooleanAtoms\n    assert (a > b).equals((a > b), True) == True\n    assert (a > b).equals((a <= b), True) == False\n\n    # Test for negated property with BooleanAtoms\n    assert (a > b).negated == (a <= b)\n    assert (a <= b).negated == (a > b)\n\n    # Test for reversedsign property with BooleanAtoms\n    assert (a > b).reversedsign == (b < a)\n    assert (a <= b).reversedsign == (b >= a)\n", "def test_issue_20000():\n    # Additional tests for negated property\n    eq = Gt(x, y)\n    assert eq.negated == Le(x, y)\n\n    eq = Lt(x, y)\n    assert eq.negated == Ge(x, y)\n\n    eq = Ge(x, y)\n    assert eq.negated == Lt(x, y)\n\n    eq = Le(x, y)\n    assert eq.negated == Gt(x, y)\n\n    for f in (Gt, Lt, Ge, Le):\n        assert f(x, y).negated.negated == f(x, y)\n", "def test_issue_20000():\n    # Test for issue where relational comparisons involving symbolic expressions\n    # with irrational numbers were failing.\n    from sympy import sqrt, Rational\n    a = sqrt(2)\n    b = sqrt(3)\n    assert (a < b) is S.true\n    assert (b > a) is S.true\n    assert (a <= b) is S.true\n    assert (b >= a) is S.true\n    assert (a > b) is S.false\n    assert (b < a) is S.false\n    assert (a >= b) is S.false\n    assert (b <= a) is S.false\n    # Check with rational approximations\n    ra = Rational(str(a.evalf()))\n    rb = Rational(str(b.evalf()))\n    assert (ra < rb) is S.true\n    assert (rb > ra) is S.true\n    assert (ra <= rb) is S.true\n    assert (rb >= ra) is S.true\n    assert (ra > rb) is S.false\n    assert (rb < ra) is S.false\n    assert (ra >= rb) is S.false\n    assert (rb <= ra) is S.false\n", "def test_relational_negation():\n    assert Eq(x, y).negated == Ne(x, y)\n    assert Ne(x, y).negated == Eq(x, y)\n    assert Gt(x, y).negated == Le(x, y)\n    assert Lt(x, y).negated == Ge(x, y)\n    assert Ge(x, y).negated == Lt(x, y)\n    assert Le(x, y).negated == Gt(x, y)\n", "def test_negated_property_with_expressions():\n    expr1 = x + y\n    expr2 = 2*z + 3\n\n    assert Eq(expr1, expr2).negated == Ne(expr1, expr2)\n    assert Ne(expr1, expr2).negated == Eq(expr1, expr2)\n    assert Ge(expr1, expr2).negated == Lt(expr1, expr2)\n    assert Le(expr1, expr2).negated == Gt(expr1, expr2)\n    assert Gt(expr1, expr2).negated == Le(expr1, expr2)\n    assert Lt(expr1, expr2).negated == Ge(expr1, expr2)\n\n    for f in (Eq, Ne, Ge, Gt, Le, Lt):\n        assert f(expr1, expr2).negated.negated == f(expr1, expr2)\n"], "sample_1092": ["def test_func_arg_tracker():\n    # Test FuncArgTracker class and its methods\n    a, b, c, d, e, f = symbols('a b c d e f')\n    \n    funcs = [Add(a, b), Add(b, c), Mul(a, b), Mul(a, c), Pow(a, b), Pow(c, d)]\n    tracker = cse_main.FuncArgTracker(funcs)\n    \n    # Test get_or_add_value_number\n    assert tracker.get_or_add_value_number(a) == 0\n    assert tracker.get_or_add_value_number(b) == 1\n    assert tracker.get_or_add_value_number(c) == 2\n    \n    # Test get_args_in_value_order\n    assert tracker.get_args_in_value_order([2, 0, 1]) == [a, b, c]\n    \n    # Test get_common_arg_candidates\n    common_candidates = tracker.get_common_arg_candidates(tracker.func_to_argset[0])\n    assert common_candidates == {1: 1, 2: 1, 3: 1}\n    \n    # Test get_subset_candidates\n    subset_candidates = tracker.get_subset_candidates(tracker.func_to_argset[0])\n    assert subset_candidates == {0, 1}\n    \n    # Test update_func_argset\n    tracker.update_func_argset(0, OrderedSet([d, e]))\n    assert tracker.func_to_argset[0] == OrderedSet([tracker.get_or_add_value_number(d), tracker.get_or_add_value_number(e)])\n    \n    # Test stop_arg_tracking\n    tracker.stop_arg_tracking(0)\n    assert all(0 not in s for s in tracker.arg_to_funcset)\n", "def test_funcargtracker():\n    # Initializing function arguments\n    a, b, c = symbols('a b c')\n    funcs = [Add(a, b), Mul(a, c), Pow(b, c)]\n    \n    tracker = cse_main.FuncArgTracker(funcs)\n    \n    # Testing value number assignment\n    assert tracker.get_or_add_value_number(a) == 0\n    assert tracker.get_or_add_value_number(b) == 1\n    assert tracker.get_or_add_value_number(c) == 2\n    \n    # Testing argument order\n    assert tracker.get_args_in_value_order({0, 1}) == [a, b]\n    assert tracker.get_args_in_value_order({1, 2}) == [b, c]\n    \n    # Common argument candidates\n    common_args = tracker.get_common_arg_candidates({0, 1})\n    assert common_args == {0: 1}\n    \n    # Testing subset candidates\n    subset_candidates = tracker.get_subset_candidates({0, 1})\n    assert subset_candidates == {0}\n    \n    # Updating function argument set\n    tracker.update_func_argset(0, {1, 2})\n    assert tracker.func_to_argset[0] == {1, 2}\n    assert tracker.arg_to_funcset[1] == {0, 2}\n    \n    # Stopping argument tracking\n    tracker.stop_arg_tracking(0)\n    assert tracker.arg_to_funcset[1] == {2}\n", "def test_cse_with_complex_expressions():\n    # Test CSE with complex nested expressions\n    e = (x + y)*(z + w) + (x + y)*(z - w) + (x - y)*(z + w) + (x - y)*(z - w)\n    substs, reduced = cse([e])\n    assert substs == [(x0, x + y), (x1, x - y), (x2, z + w), (x3, z - w)]\n    assert reduced == [x0*x3 + x0*x2 + x1*x2 + x1*x3]\n\n    # Test CSE with functions and trigonometric expressions\n    e = sin(x + y) + cos(x + y) + exp(x + y)\n    substs, reduced = cse([e])\n    assert substs == [(x0, x + y)]\n    assert reduced == [sin(x0) + cos(x0) + exp(x0)]\n\n    # Test CSE with logarithmic and exponential expressions\n    e = log(x + y) + exp(x + y) + sqrt(x + y)\n    substs, reduced = cse([e])\n    assert substs == [(x0, x + y)]\n    assert reduced == [log(x0) + exp(x0) + sqrt(x0)]\n", "def test_cse_symbols_exhaustion():\n    sym = symbols('a:z')  # 26 symbols\n    exprs = [i**2 + 2*i + 1 for i in range(30)]  # More expressions than symbols\n    with raises(ValueError):\n        cse(exprs, symbols=sym)\n", "def test_func_arg_tracker():\n    # Test for FuncArgTracker class\n    funcs = [Add(x, y), Mul(x, z), Pow(x, y)]\n    tracker = cse_main.FuncArgTracker(funcs)\n\n    # Test value number mappings\n    assert tracker.get_or_add_value_number(x) == 0\n    assert tracker.get_or_add_value_number(y) == 1\n    assert tracker.get_or_add_value_number(z) == 2\n\n    # Test argument mappings\n    assert tracker.func_to_argset[0] == OrderedSet([0, 1])  # Add(x, y)\n    assert tracker.func_to_argset[1] == OrderedSet([0, 2])  # Mul(x, z)\n    assert tracker.func_to_argset[2] == OrderedSet([0, 1])  # Pow(x, y)\n\n    # Test stopping argument tracking\n    tracker.stop_arg_tracking(0)\n    assert 0 not in tracker.arg_to_funcset[0]  # x should no longer map to func 0\n    assert 0 not in tracker.arg_to_funcset[1]  # y should no longer map to func 0\n\n    # Test common argument candidates\n    assert tracker.get_common_arg_candidates(OrderedSet([0, 1])) == {2: 2}\n    assert tracker.get_common_arg_candidates(OrderedSet([0, 2])) == {1: 1}\n\n    # Test subset candidates\n    assert tracker.get_subset_candidates(OrderedSet([0])) == OrderedSet([1, 2])\n    assert tracker.get_subset_candidates(OrderedSet([0, 1])) == OrderedSet([2])\n    assert tracker.get_subset_candidates(OrderedSet([0, 2])) == OrderedSet([1])\n\n    # Test updating function argument sets\n    tracker.update_func_argset(1, OrderedSet([1, 2]))\n    assert tracker.func_to_argset[1] == OrderedSet([1, 2])\n    assert 1 in tracker.arg_to_funcset[1]  # y should now map to func 1\n    assert 1 in tracker.arg_to_funcset[2]  # z should still map to func 1\n"], "sample_1093": ["def test_abstract_python_code_printer_methods():\n    prntr = AbstractPythonCodePrinter()\n\n    # Test _declare_number_const\n    assert prntr._declare_number_const('PI', '3.14') == 'PI = 3.14'\n\n    # Test _module_format\n    assert prntr._module_format('math.sqrt') == 'math.sqrt'\n    prntr._settings['fully_qualified_modules'] = False\n    assert prntr._module_format('math.sqrt') == 'sqrt'\n\n    # Test _format_code\n    assert prntr._format_code(['line1', 'line2']) == ['line1', 'line2']\n\n    # Test _get_statement\n    assert prntr._get_statement('x = 5') == 'x = 5'\n\n    # Test _get_comment\n    assert prntr._get_comment('This is a comment') == '  # This is a comment'\n\n    # Test _expand_fold_binary_op\n    assert prntr._expand_fold_binary_op('add', [x, y, z]) == 'add(add(x, y), z)'\n\n    # Test _expand_reduce_binary_op\n    assert prntr._expand_reduce_binary_op('add', [x, y, z, S(1)]) == 'add(add(x, y), add(z, 1))'\n\n    # Test _get_einsum_string\n    einsum_str, free, dum = prntr._get_einsum_string([2, 2], [[0, 1], [2, 3]])\n    assert einsum_str == 'aa,bb'\n    assert free == ['a', 'b']\n    assert dum == ['a', 'b']\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert p.doprint(x**2) == 'x**2'\n    assert p.doprint(x**Rational(1, 2)) == 'sympy.sqrt(x)'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.doprint(S.Exp1) == 'sympy.E'\n    assert p.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n    assert p.doprint(S.EulerGamma) == 'sympy.EulerGamma'\n    assert p.doprint(S.NaN) == 'sympy.nan'\n    assert p.doprint(S.Infinity) == 'sympy.oo'\n    assert p.doprint(S.NegativeInfinity) == 'sympy.oo*-1'\n    assert p.doprint(Mod(x, 2)) == 'x % 2'\n    assert p.doprint(And(x, y)) == 'x and y'\n    assert p.doprint(Or(x, y)) == 'x or y'\n    assert p.doprint(Assignment(x, 2)) == 'x = 2'\n    assert p.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6))) == 'sympy.Piecewise((1, sympy.Eq(x, 0)), (2, x > 6))'\n    assert p.doprint(Piecewise((2, Le(x, 0)),\n                        (3, Gt(x, 0)), evaluate=False)) == 'sympy.Piecewise((2, x <= 0), (3, x > 0))'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(p[0, 1]) == 'p[0, 1]'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Mod(x, 2)) == 'x % 2'\n    assert p.doprint(And(x, y)) == 'x and y'\n    assert p.doprint(Or(x, y)) == 'x or y'\n    assert not p.module_imports\n\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.module_imports == {}\n\n    assert p.doprint(x**Rational(1, 2)) == 'sympy.sqrt(x)'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(Assignment(x, 2)) == 'x = 2'\n    assert p.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert p.doprint(Piecewise((2, Le(x, 0)),\n                        (3, Gt(x, 0)), evaluate=False)) == '((2) if (x <= 0) else'\\\n                                                        ' (3) if (x > 0) else None)'\n    assert p.doprint(p[0, 1]) == 'p[0, 1]'\n\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n\n    # Test symbolic matrix dimensions\n    A = MatrixSymbol(\"A\", 3, 3)\n    I = Identity(3)\n    assert prntr.doprint(I) == 'sympy.eye(3)'\n    assert prntr.doprint(A.T) == 'A.T'\n    assert prntr.doprint(A + I) == 'A + sympy.eye(3)'\n\n    # Test user-defined function\n    from sympy import Function\n    f = Function('f')\n    assert prntr.doprint(f(x)) == 'f(x)'\n\n    # Test", "def test_AbstractPythonCodePrinter_methods():\n    prntr = AbstractPythonCodePrinter()\n    \n    assert prntr._declare_number_const('x', 10) == 'x = 10'\n    \n    prntr.module_imports = defaultdict(set)\n    assert prntr._module_format('math.sqrt') == 'math.sqrt'\n    assert prntr.module_imports == {'math': {'sqrt'}}\n    \n    assert prntr._get_statement('print(x)') == 'print(x)'\n    assert prntr._get_comment('This is a comment') == '  # This is a comment'\n    \n    assert prntr._expand_fold_binary_op('add', [x, y, z]) == 'add(add(x, y), z)'\n    assert prntr._expand_reduce_binary_op('add', [x, y, z, a, b]) == 'add(add(x, y), add(z, add(a, b)))'\n    \n    assert prntr._get_einsum_string([2, 2], [(0, 2), (1, 3)]) == ('abcd,efgh', ['a', 'b', 'e', 'f'], ['c', 'd', 'g', 'h'])\n    \n    assert prntr._print_NaN(S.NaN) == \"float('nan')\"\n    assert prntr._print_Infinity(S.Infinity) == \"float('inf')\"\n    assert prntr._print_NegativeInfinity(-S.Infinity) == \"float('-inf')\"\n    assert prntr._print_ComplexInfinity(S.ComplexInfinity) == \"float('nan')\"\n    \n    assert prntr._print_Mod(Mod(x, y)) == 'x % y'\n    assert prntr._print_Relational(Eq(x, y)) == '(x == y)'\n    assert prntr._print_ITE(Piecewise((1, Eq(x, 0)), (0, True))) == '((1) if (x == 0) else (0))'\n    assert prntr._print_Sum(Sum(x, (x, 1, 10))) == '(builtins.sum(x for x in range(1, 11)))'\n    \n    assert prntr._print_ImaginaryUnit(S.ImaginaryUnit) == '1j'\n    \n    mat = SparseMatrix([[x, y], [z, a", "def test_Indexed():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(p[0]) == 'p[0]'\n    assert prntr.doprint(p[0, 1]) == 'p[0, 1]'\n    assert prntr.doprint(p[0, 1, 2]) == 'p[0, 1, 2]'\n    \n    prntr = NumPyPrinter()\n    assert prntr.doprint(p[0]) == 'p[0]'\n    assert prntr.doprint(p[0, 1]) == 'p[0, 1]'\n    assert prntr.doprint(p[0, 1, 2]) == 'p[0, 1, 2]'\n    \n    prntr = MpmathPrinter()\n    assert prntr.doprint(p[0]) == 'p[0]'\n    assert prntr.doprint(p[0, 1]) == 'p[0, 1]'\n    assert prntr.doprint(p[0, 1, 2]) == 'p[0, 1, 2]'\n    \n    prntr = SciPyPrinter()\n    assert prntr.doprint(p[0]) == 'p[0]'\n    assert prntr.doprint(p[0, 1]) == 'p[0, 1]'\n    assert prntr.doprint(p[0, 1, 2]) == 'p[0, 1, 2]'\n    \n    prntr = SymPyPrinter()\n    assert prntr.doprint(p[0]) == 'p[0]'\n    assert prntr.doprint(p[0, 1]) == 'p[0, 1]'\n    assert prntr.doprint(p[0, 1, 2]) == 'p[0, 1, 2]'\n"], "sample_1094": ["def test_compare():\n    x, y, z = symbols('x y z')\n    assert (x + y).compare(x + y) == 0\n    assert (x + y).compare(x + z) == (y > z) - (y < z)\n    assert (x + y).compare(y + x) == 0\n    assert (x + y).compare(x + y + z) == -1\n    assert (x + y + z).compare(x + y) == 1\n    assert (x + y).compare(x*y) == -1\n    assert (x*y).compare(x + y) == 1\n", "def test_compare():\n    x, y, z = symbols('x y z')\n    a = Basic(x)\n    b = Basic(y)\n    c = Basic(x)\n    d = Basic(x, y)\n    e = Basic(y, x)\n    \n    # Compare same objects\n    assert a.compare(a) == 0\n    assert b.compare(b) == 0\n    \n    # Compare different objects\n    assert a.compare(b) == -1\n    assert b.compare(a) == 1\n    \n    # Compare objects with same content\n    assert a.compare(c) == 0\n    \n    # Compare nested objects\n    assert d.compare(e) == -1\n    assert e.compare(d) == 1\n", "def test_compare():\n    # Test compare method\n    b1 = Basic()\n    b2 = Basic()\n    b3 = Basic(b1, b2)\n    b4 = Basic(b2, b1)\n\n    assert b1.compare(b2) == 0\n    assert b3.compare(b4) == -1\n    assert b4.compare(b3) == 1\n\n    assert b3.compare(b1) == 1\n    assert b1.compare(b3) == -1\n\n    assert b3.compare(b2) == 1\n    assert b2.compare(b3) == -1\n\n    assert b4.compare(b1) == 1\n    assert b1.compare(b4) == -1\n\n    assert b4.compare(b2) == 1\n    assert b2.compare(b4) == -1\n", "def test_compare():\n    x, y = symbols('x y')\n    assert Basic(x).compare(Basic(y)) == -1\n    assert Basic(y).compare(Basic(x)) == 1\n    assert Basic(x).compare(Basic(x)) == 0\n    assert Basic((x, y)).compare(Basic((y, x))) == -1\n    assert Basic((y, x)).compare(Basic((x, y))) == 1\n", "def test_compare():\n    assert b1.compare(b2) == -1\n    assert b2.compare(b1) == 1\n    assert b21.compare(b2) == 1\n    assert b2.compare(b21) == -1\n    assert b1.compare(b1) == 0\n    assert b2.compare(b2) == 0\n    assert b21.compare(b21) == 0\n"], "sample_1095": ["def test_af_functions():\n    # Test _af_rmul with different cases\n    assert _af_rmul([1, 0, 2], [0, 2, 1]) == [1, 2, 0]\n    assert _af_rmul([2, 0, 1], [1, 2, 0]) == [0, 1, 2]\n    assert _af_rmul([3, 1, 2, 0], [0, 2, 1, 3]) == [3, 2, 1, 0]\n\n    # Test _af_rmuln with different number of permutations\n    assert _af_rmuln([1, 0, 2], [0, 2, 1]) == [1, 2, 0]\n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [1, 2, 0]) == [2, 0, 1]\n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [1, 2, 0], [2, 1, 0]) == [0, 2, 1]\n\n    # Test _af_parity with even and odd permutations\n    assert _af_parity([0, 1, 2, 3]) == 0  # Even permutation\n    assert _af_parity([3, 2, 0, 1]) == 1  # Odd permutation\n    assert _af_parity([1, 0, 3, 2]) == 1  # Odd permutation\n    assert _af_parity([2, 1, 3, 0]) == 0  # Even permutation\n\n    # Test _af_invert with basic permutation inversions\n    assert _af_invert([1, 2, 0, 3]) == [2, 0, 1, 3]\n    assert _af_invert([0, 1, 2, 3]) == [0, 1, 2, 3]\n    assert _af_invert([3, 0, 1, 2]) == [1, 2, 3, 0]\n\n    # Test _af_pow with various powers\n    assert _", "def test_af_functions():\n    # Test _af_rmul\n    a = [1, 0, 2]\n    b = [0, 2, 1]\n    assert _af_rmul(a, b) == [1, 2, 0]\n\n    # Test _af_rmuln\n    c = [2, 1, 0]\n    assert _af_rmuln(a, b, c) == [2, 0, 1]\n\n    d = [1, 2, 0]\n    assert _af_rmuln(a, b, c, d) == [0, 1, 2]\n\n    e = [2, 0, 1]\n    assert _af_rmuln(a, b, c, d, e) == [2, 0, 1]\n\n    f = [0, 1, 2]\n    assert _af_rmuln(a, b, c, d, e, f) == [1, 2, 0]\n\n    g = [1, 0, 2]\n    assert _af_rmuln(a, b, c, d, e, f, g) == [1, 0, 2]\n\n    h = [0, 2, 1]\n    assert _af_rmuln(a, b, c, d, e, f, g, h) == [2, 0, 1]\n\n    # Test _af_parity\n    assert _af_parity([0, 1, 2, 3]) == 0\n    assert _af_parity([3, 2, 0, 1]) == 1\n\n    # Test _af_invert\n    assert _af_invert([1, 2, 0, 3]) == [2, 0, 1, 3]\n\n    # Test _af_pow\n    assert _af_pow([2, 0, 3, 1], 4) == [0, 1, 2, 3]\n    assert _af_pow([1, 0, 2], -1) == [1, 0, 2]\n    assert _af_pow([0, 2, 1], 2) == [0, 1, 2]\n\n    # Test _af_commutes_with\n    assert _af_commutes_with([1", "def test_af_invert():\n    p = [1, 2, 0, 3]\n    inv_p = _af_invert(p)\n    assert inv_p == [2, 0, 1, 3]\n    assert _af_rmul(inv_p, p) == list(range(len(p)))\n    assert _af_rmul(p, inv_p) == list(range(len(p)))\n\n    p = [3, 2, 1, 0]\n    inv_p = _af_invert(p)\n    assert inv_p == [3, 2, 1, 0]\n    assert _af_rmul(inv_p, p) == list(range(len(p)))\n    assert _af_rmul(p, inv_p) == list(range(len(p)))\n", "def test_af_functions():\n    assert _af_rmul([1, 0, 2], [0, 2, 1]) == [1, 2, 0]\n    assert _af_rmul([2, 0, 1], [1, 2, 0]) == [0, 1, 2]\n    \n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [2, 1, 0]) == [2, 1, 0]\n    assert _af_rmuln([2, 0, 1], [1, 2, 0], [0, 2, 1]) == [1, 0, 2]\n    assert _af_rmuln([1, 0, 2], [2, 1, 0], [0, 1, 2]) == [2, 1, 0]\n\n    assert _af_invert([1, 2, 0, 3]) == [2, 0, 1, 3]\n    assert _af_invert([2, 0, 1, 3]) == [1, 2, 0, 3]\n\n    assert _af_pow([2, 0, 3, 1], 4) == [0, 1, 2, 3]\n    assert _af_pow([1, 2, 0, 3], 3) == [0, 1, 2, 3]\n    assert _af_pow([2, 0, 3, 1], -1) == [1, 3, 0, 2]\n\n    assert _af_commutes_with([1, 2, 0], [0, 2, 1]) == False\n    assert _af_commutes_with([0, 1, 2], [0, 1, 2]) == True\n", "def test_cycle_structure():\n    p = Permutation(1, 2, 3)\n    q = Permutation(1, 2)(3, 4)\n    r = Permutation([4, 0, 2, 1, 3])\n    s = Permutation([2, 3, 1, 0, 4])\n\n    assert p.cycle_structure == {4: 1}\n    assert q.cycle_structure == {2: 2, 1: 1}\n    assert r.cycle_structure == {3: 1, 1: 2}\n    assert s.cycle_structure == {2: 2, 1: 1}\n"], "sample_1096": ["def test_IndexedBase_with_strides():\n    i, j = symbols('i j', integer=True)\n    m, n = symbols('m n', integer=True)\n    a = IndexedBase('a', shape=(m, n), strides=(1, m))\n    assert a.strides == (1, m)\n    expr = a[i, j]\n    assert expr.shape == Tuple(m, n)\n    assert expr.base.strides == (1, m)\n    assert expr.base.offset == 0\n    assert expr.ranges == [Tuple(0, m - 1), Tuple(0, n - 1)]\n\n    # Testing with non-default offset\n    b = IndexedBase('b', shape=(m, n), strides=(1, m), offset=2)\n    assert b.offset == 2\n    expr_b = b[i, j]\n    assert expr_b.base.offset == 2\n    assert expr_b.shape == Tuple(m, n)\n    assert expr_b.ranges == [Tuple(0, m - 1), Tuple(0, n - 1)]\n\n    # Testing invalid strides\n    raises(TypeError, lambda: IndexedBase('c', shape=(m, n), strides='invalid'))\n", "def test_Idx_with_symbols_as_bounds():\n    i, j, m, n = symbols('i j m n', integer=True)\n    idx1 = Idx(i, (m, n))\n    idx2 = Idx(j, (m + 1, n - 1))\n    \n    assert idx1.lower == m\n    assert idx1.upper == n\n    assert idx2.lower == m + 1\n    assert idx2.upper == n - 1\n\n    raises(TypeError, lambda: Idx(i, (m, n + 0.5)))  # n + 0.5 is not an integer\n    raises(TypeError, lambda: Idx(i, (m + 0.5, n)))  # m + 0.5 is not an integer\n", "def test_IndexedBase_with_strides_and_offset():\n    i, j, k = symbols('i j k', integer=True)\n    A = IndexedBase('A', shape=(10, 10), strides=(2, 3), offset=5)\n    assert A.strides == (2, 3)\n    assert A.offset == 5\n\n    # Checking that the strides and offset are preserved in Indexed objects\n    idx = A[i, j]\n    assert idx.base.strides == (2, 3)\n    assert idx.base.offset == 5\n\n    # Ensure that the correct index calculation happens with strides and offset\n    assert idx._sympystr(lambda x: x) == \"A[2*i + 3*j + 5]\"\n\n    # Test the use of strides='C' and strides='F'\n    B = IndexedBase('B', shape=(10, 10), strides='C')\n    assert B.strides == 'C'\n    B = IndexedBase('B', shape=(10, 10), strides='F')\n    assert B.strides == 'F'\n", "def test_IndexedBase_with_strides_and_offset():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a', shape=(4, 4), strides=(1, 4), offset=2)\n    assert a.shape == Tuple(4, 4)\n    assert a.strides == (1, 4)\n    assert a.offset == 2\n    assert a[i, j].func(*a[i, j].args) == a[i, j]\n    assert a[i, j].shape == Tuple(4, 4)\n    assert a[i, j].ranges == [None, None]\n    assert a[i, j]._sympystr(lambda x: str(x)) == 'a[i, j]'\n    raises(IndexException, lambda: a[i])\n    raises(IndexException, lambda: a[i, i, j])\n\n    # Test with numpy compatible base\n    from sympy import NDimArray\n    import numpy as np\n    A_np = NDimArray(np.arange(16).reshape(4, 4))\n    B = IndexedBase(A_np, shape=(4, 4), strides='C')\n    assert B.shape == Tuple(4, 4)\n    assert B.strides == 'C'\n    assert B.label == A_np\n    assert B[i, j].func(*B[i, j].args) == B[i, j]\n    assert B[i, j].shape == Tuple(4, 4)\n    assert B[i, j].ranges == [None, None]\n    assert B[i, j]._sympystr(lambda x: str(x)) == 'NDimArray[i, j]'\n    raises(IndexException, lambda: B[i])\n    raises(IndexException, lambda: B[i, i, j])\n", "def test_IndexedBase_strides():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a', shape=(10, 20), strides=(2, 3))\n    assert a.shape == Tuple(10, 20)\n    assert a.strides == (2, 3)\n    assert a.offset == S.Zero\n    A = a[i, j]\n    assert A.shape == Tuple(10, 20)\n    assert A.ranges == [Tuple(0, 9), Tuple(0, 19)]\n    raises(IndexException, lambda: a[i])\n    raises(IndexException, lambda: a[i, j, i])\n"], "sample_1097": ["def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    real_A = MatrixSymbol('real_A', n, n)\n    imag_A = MatrixSymbol('imag_A', n, n)\n    real_B = MatrixSymbol('real_B', n, n)\n    imag_B = MatrixSymbol('imag_B', n, n)\n    real_C = MatrixSymbol('real_C', n, n)\n    imag_C = MatrixSymbol('imag_C', n, n)\n    real_D = MatrixSymbol('real_D', n, n)\n    imag_D = MatrixSymbol('imag_D', n, n)\n    \n    X = BlockMatrix([[A, B], [C, D]])\n    real_X, imag_X = X.as_real_imag()\n    \n    assert real_X == Matrix([[re(A), re(B)], [re(C), re(D)]])\n    assert imag_X == Matrix([[im(A), im(B)], [im(C), im(D)]])\n", "def test_BlockMatrix_methods():\n    # Test structurally_equal method\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    BM1 = BlockMatrix([[A, B], [C, D]])\n    BM2 = BlockMatrix([[A, B], [C, D]])\n    BM3 = BlockMatrix([[A, B], [D, C]])\n\n    assert BM1.structurally_equal(BM2) is True\n    assert BM1.structurally_equal(BM3) is False\n\n    # Test as_real_imag method\n    BM4 = BlockMatrix([[A + B*1j, C - D*1j], [B - A*1j, D + C*1j]])\n    real_part, imag_part = BM4.as_real_imag()\n    assert real_part == BlockMatrix([[A, C], [B, D]])\n    assert imag_part == BlockMatrix([[B, -D], [-A, C]])\n\n    # Test is_Identity property\n    Id = Identity(n)\n    Z = ZeroMatrix(n, n)\n    BM5 = BlockMatrix([[Id, Z], [Z, Id]])\n    assert BM5.is_Identity is True\n    BM6 = BlockMatrix([[Id, Z], [Z, A]])\n    assert BM6.is_Identity is False\n\n    # Test is_structurally_symmetric property\n    BM7 = BlockMatrix([[A, B], [B, A]])\n    assert BM7.is_structurally_symmetric is True\n    BM8 = BlockMatrix([[A, B], [C, D]])\n    assert BM8.is_structurally_symmetric is False\n", "def test_BlockMatrix_inverse():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', m, n)\n    D = MatrixSymbol('D', m, m)\n    X = BlockMatrix([[A, B], [C, D]])\n\n    from sympy.assumptions import assuming, Q\n    with assuming(Q.invertible(A), Q.invertible(D)):\n        inv_X = block_collapse(X.inverse())\n        expected_inv = BlockMatrix([\n            [(A - B * D.I * C).I, -(A - B * D.I * C).I * B * D.I],\n            [-D.I * C * (A - B * D.I * C).I, D.I + D.I * C * (A - B * D.I * C).I * B * D.I]\n        ])\n        assert inv_X == expected_inv\n\n    # Test non-square block matrix inversion, which should raise an error\n    E = MatrixSymbol('E', n, m)\n    Y = BlockMatrix([[A, E]])\n    try:\n        Y.inverse()\n    except ShapeError:\n        assert True\n    else:\n        assert False\n", "def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    X = BlockMatrix([[A, B], [C, D]])\n    real_X, imag_X = X.as_real_imag()\n\n    assert real_X.blocks[0, 0] == re(A)\n    assert real_X.blocks[0, 1] == re(B)\n    assert real_X.blocks[1, 0] == re(C)\n    assert real_X.blocks[1, 1] == re(D)\n\n    assert imag_X.blocks[0, 0] == im(A)\n    assert imag_X.blocks[0, 1] == im(B)\n    assert imag_X.blocks[1, 0] == im(C)\n    assert imag_X.blocks[1, 1] == im(D)\n", "def test_BlockMatrix_structurally_equal():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', m, m)\n    C = MatrixSymbol('C', n, m)\n    D = MatrixSymbol('D', m, n)\n    X = BlockMatrix([[A, C], [D, B]])\n    Y = BlockMatrix([[A, C], [D, B]])\n    Z = BlockMatrix([[A, B], [C, D]])\n\n    assert X.structurally_equal(Y)\n    assert not X.structurally_equal(Z)\n"], "sample_1098": ["def test_TupleArg_limit():\n    from sympy import sin\n    f = TupleArg(sin(x))\n    assert f.limit(x, 0) == TupleArg(S.Zero)\n    assert f.limit(x, 0, dir='-') == TupleArg(S.Zero)\n    assert f.limit(x, oo) == TupleArg(AccumBounds(-1, 1))\n    assert f.limit(x, -oo) == TupleArg(AccumBounds(-1, 1))\n", "def test_prep_tuple():\n    from sympy.functions.special.hyper import _prep_tuple\n    from sympy import Polar\n    # Test that _prep_tuple correctly converts and unpolarifies inputs\n    assert _prep_tuple([1, 2, 3]) == Tuple(1, 2, 3)\n    assert _prep_tuple((4, 5)) == Tuple(4, 5)\n    assert _prep_tuple([Polar(1, 0), Polar(2, 0)]) == Tuple(1, 2)\n\n    # Test with nested tuples\n    assert _prep_tuple([(1, 2), (3, 4)]) == Tuple(Tuple(1, 2), Tuple(3, 4))\n\n    # Test with mixed data types\n    assert _prep_tuple([1, 2.5, 3 + 4*I]) == Tuple(1, 2.5, 3 + 4*I)\n", "def test_appellf1_eval():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    assert appellf1(a, b1, b2, c, 1, 1) == appellf1(a, b1, b2, c, 1, 1)\n    assert appellf1(a, b1, b2, c, -1, -1) == appellf1(a, b1, b2, c, -1, -1)\n    assert appellf1(1, 1, 1, 2, x, y).series(x) == 1 + x + y + x*y + O(x**2)\n    assert appellf1(1, 1, 1, 2, x, y).series(y) == 1 + x + y + x*y + O(y**2)\n    assert appellf1(1, 2, 3, 4, x, y).subs(x, 1).subs(y, 0) == hyper([1, 2], [4], 1)\n    assert appellf1(1, 2, 3, 4, x, y).subs(x, 0).subs(y, 1) == hyper([1, 3], [4], 1)\n", "def test_appellf1_eval():\n    a, b1, b2, c = randcplx(), randcplx(), randcplx(), randcplx()\n    x_val, y_val = randcplx(), randcplx()\n    assert appellf1(a, b1, b2, c, x_val, y_val).evalf() == tn(appellf1(a, b1, b2, c, x_val, y_val), \n                                                             appellf1(a, b1, b2, c, x_val, y_val).evalf())\n    \n    # Specific values for known results\n    assert appellf1(1, 1, 1, 2, 0.5, 0.5).evalf() == 2.0\n    assert appellf1(2, 3, 4, 5, 0.1, 0.2).evalf() == 1.2552\n", "def test_appellf1_special_cases():\n    from sympy import simplify\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    \n    # Test special values that reduce to simpler functions\n    assert simplify(appellf1(1, 1, 1, 2, x, x)) == hyper([1, 1], [2], 2*x)\n    assert simplify(appellf1(2, 3, 4, 5, 1, 1)) == gamma(5)*gamma(1)/gamma(3)*gamma(4)/gamma(6)\n    \n    # Test symmetry properties\n    assert simplify(appellf1(a, b1, b2, c, x, y)) == simplify(appellf1(a, b2, b1, c, y, x))\n\n    # Test cases with zero parameters\n    assert appellf1(a, b1, b2, c, 0, y) == 1\n    assert appellf1(a, b1, b2, c, x, 0) == 1\n\n    # Test evaluation at specific numerical values\n    assert appellf1(1, 1, 1, 2, 0.5, 0.5).n() == 1.73205080756888\n"], "sample_1099": ["def test_doit_partial_derivative():\n    # Testing the `doit` method for `PartialDerivative`\n    \n    # Scalar differentiation\n    tau, alpha = symbols(\"tau alpha\")\n    expr1 = PartialDerivative(tau**alpha, tau).doit()\n    assert expr1 == alpha * tau ** (alpha - 1)\n\n    expr2 = PartialDerivative(2*tau + 3*tau**4, tau).doit()\n    assert expr2 == 2 + 12 * tau ** 3\n\n    expr3 = PartialDerivative(2*tau + 3*tau**4, alpha).doit()\n    assert expr3 == 0\n\n    # Tensor differentiation with respect to another tensor\n    expr4 = PartialDerivative(A(i), A(j)).doit()\n    assert expr4 == L.delta(i, -j)\n\n    expr5 = PartialDerivative(A(i), A(-j)).doit()\n    assert expr5 == L.metric(i, L_0) * L.delta(-L_0, j)\n\n    expr6 = PartialDerivative(H(i, j), H(m, m1)).doit()\n    assert expr6 == L.delta(i, -m) * L.delta(j, -m1)\n\n    expr7 = PartialDerivative(A(i)*(H(-i, j) + H(j, -i)), A(j)).doit()\n    assert expr7 == 2 * L.delta(L_0, -L_0)\n\n    # Mixed differentiation with scalar and tensor\n    expr8 = PartialDerivative(tau**alpha * A(i), tau).doit()\n    assert expr8 == alpha * tau ** (alpha - 1) * A(i)\n", "def test_partial_derivative_replace_indices():\n    # Test replacing indices in a partial derivative expression\n    expr1 = PartialDerivative(A(i), A(j))\n    repl = {i: k, j: m}\n    replaced_expr1 = expr1._replace_indices(repl)\n    assert replaced_expr1.expr == A(k)\n    assert replaced_expr1.variables == (A(m),)\n    assert replaced_expr1.get_indices() == [k, -m]\n\n    expr2 = PartialDerivative(A(i)*H(-i, j), A(j))\n    repl = {i: k, j: m}\n    replaced_expr2 = expr2._replace_indices(repl)\n    assert replaced_expr2.expr == A(k) * H(-k, m)\n    assert replaced_expr2.variables == (A(m),)\n    assert replaced_expr2.get_indices() == [k, -k, m, -m]\n", "def test_partial_derivative_invalid_cases():\n    tau, alpha, beta = symbols(\"tau alpha beta\")\n\n    # Test invalid cases: partial derivative with non-existent tensor or symbol\n    raises(TypeError, lambda: PartialDerivative(A(i), tau, beta))\n    raises(TypeError, lambda: PartialDerivative(A(i), A(j), tau, beta))\n    raises(TypeError, lambda: PartialDerivative(H(i, j), tau, beta))\n    raises(TypeError, lambda: PartialDerivative(A(i)*H(-i, j), A(k), beta))\n    raises(TypeError, lambda: PartialDerivative(A(i)*H(-i, j), tau, A(k), beta))\n\n    # Test invalid cases: partial derivative with incompatible types\n    raises(TypeError, lambda: PartialDerivative(PartialDerivative(A(i), A(j)), tau))\n    raises(TypeError, lambda: PartialDerivative(PartialDerivative(H(i, j), H(k, m)), tau))\n\n    # Ensure that scalar derivatives don't break tensorial structure\n    expr = PartialDerivative(A(i)*tau**alpha, tau)\n    assert expr._expand_partial_derivative() == A(i) * alpha * tau**(alpha - 1)\n\n    # Test partial derivative with an unsupported type\n    raises(TypeError, lambda: PartialDerivative(A(i), 'unsupported_type'))\n", "def test_partial_derivative_doit():\n    # Testing the doit method for various expressions\n    expr1 = PartialDerivative(A(i), A(j))\n    assert expr1.doit() == expr1._perform_derivative()\n\n    expr2 = PartialDerivative(A(i) + B(i), A(j))\n    assert expr2.doit() == PartialDerivative(A(i), A(j)).doit() + PartialDerivative(B(i), A(j)).doit()\n\n    expr3 = PartialDerivative(A(i)*B(j), A(k))\n    assert expr3.doit() == PartialDerivative(A(i), A(k)).doit() * B(j) + A(i) * PartialDerivative(B(j), A(k)).doit()\n\n    expr4 = PartialDerivative(A(i)*B(j) + C(k), A(m))\n    assert expr4.doit() == (PartialDerivative(A(i)*B(j), A(m)).doit() + PartialDerivative(C(k), A(m)).doit())\n\n    expr5 = PartialDerivative(A(i) * (B(j) + C(k)), A(m))\n    assert expr5.doit() == (PartialDerivative(A(i) * B(j), A(m)).doit() + PartialDerivative(A(i) * C(k), A(m)).doit())\n", "def test_eval_partial_derivative_mixed_expressions():\n    tau, alpha = symbols(\"tau alpha\")\n    \n    expr1 = PartialDerivative(tau**alpha * A(i) * H(j, -i), tau)\n    assert expr1._perform_derivative() == alpha * tau**(alpha-1) * A(i) * H(j, -i)\n\n    expr2 = PartialDerivative((tau + alpha)*A(i)*B(j) + C(i)*D(j), tau)\n    assert expr2._perform_derivative() == A(i)*B(j)\n\n    expr3 = PartialDerivative(A(i)*B(j) + tau**2*C(i)*D(j), tau)\n    assert expr3._perform_derivative() == 2*tau*C(i)*D(j)\n\n    expr4 = PartialDerivative(A(i)*B(j) + C(i)*D(j) + tau*H(i, j), tau)\n    assert expr4._perform_derivative() == H(i, j)\n\n    expr5 = PartialDerivative(tau*A(i)*B(j)*C(k), tau, tau)\n    assert expr5._perform_derivative() == 0\n\n    expr6 = PartialDerivative(A(i)*(B(j) + tau*C(j)), tau)\n    assert expr6._perform_derivative() == A(i)*C(j)\n"], "sample_1100": ["def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(24) == 4\n    assert isqrt(25) == 5\n\n    raises(ValueError, lambda: isqrt(-1))\n\n    # Test the fallback to the slow path for large n\n    assert isqrt(4503599761588223) == 67108863\n    assert isqrt(4503599761588224) == 67108864\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(27) == 5\n    assert isqrt(4503599761588223) == int(_sqrt(4503599761588223))\n    with raises(ValueError):\n        isqrt(-1)\n    with raises(ValueError):\n        isqrt(-10)\n", "def test_isqrt():\n    # Test for positive integers\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(16) == 4\n    assert isqrt(26) == 5\n    assert isqrt(4503599761588223) == int(sqrt(4503599761588223))\n\n    # Test for large integers\n    assert isqrt(4503599761588224) == int(sqrt(4503599761588224))\n    assert isqrt(10**50) == 10**25\n\n    # Test for non-integer input\n    raises(ValueError, lambda: isqrt(-1))\n    raises(ValueError, lambda: isqrt(-100))\n\n    # Test for float input\n    assert isqrt(25.0) == 5\n    assert isqrt(4503599761588223.0) == int(sqrt(4503599761588223.0))\n\n    # Test for non-number input\n    raises(TypeError, lambda: isqrt(\"string\"))\n    raises(TypeError, lambda: isqrt(None))\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(10) == 3\n    assert isqrt(4503599761588223) == 67108863\n    assert isqrt(4503599761588224) == 67108864\n    raises(ValueError, lambda: isqrt(-1))\n\n    # Testing with large numbers\n    assert isqrt(1234567890123456789012345678901234567890) == int(sqrt(1234567890123456789012345678901234567890))\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(10) == 3\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(4503599761588223) == 67108863\n    assert isqrt(4503599761588224) == 67108864\n\n    with raises(ValueError):\n        isqrt(-1)\n"], "sample_1101": ["def test_schur_number():\n    # Test known values for k <= 4\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n\n    # Test k = 0\n    assert SchurNumber(0) == 0\n\n    # Test k is Infinity\n    assert SchurNumber(S.Infinity) == S.Infinity\n\n    # Test k > 4\n    assert isinstance(SchurNumber(5), SchurNumber)\n    assert isinstance(SchurNumber(6), SchurNumber)\n\n    # Test lower_bound method\n    assert SchurNumber(5).lower_bound() == (3**5 - 1)/2\n    assert SchurNumber(6).lower_bound() == (3**6 - 1)/2\n\n    # Test invalid k values\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(2.5))\n    raises(ValueError, lambda: SchurNumber(\"a\"))\n", "def test_schur_number():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(5) == SchurNumber(5)\n    assert SchurNumber(6).lower_bound() == (3**6 - 1) / 2\n\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(2.5))\n    raises(ValueError, lambda: SchurNumber(S.Infinity))\n", "def test_schur_number():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(6) == SchurNumber(6)\n    assert SchurNumber(S.Infinity) == S.Infinity\n    raises(ValueError, lambda: SchurNumber(0))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(2.5))\n    raises(ValueError, lambda: SchurNumber(\"a\"))\n\n    assert SchurNumber(6).lower_bound() == (3**6 - 1)/2\n    assert SchurNumber(7).lower_bound() == (3**7 - 1)/2\n", "def test_schur_number():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(5) == SchurNumber(5)  # Should return the object itself\n    assert SchurNumber(S.Infinity) == S.Infinity\n    raises(ValueError, lambda: SchurNumber(0))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)))\n    assert SchurNumber(6).lower_bound() == 364\n", "def test_schur_number():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(S.Infinity) == S.Infinity\n    raises(ValueError, lambda: SchurNumber(0))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(2.5))\n    raises(ValueError, lambda: SchurNumber(Rational(3, 2)))\n    assert SchurNumber(6).lower_bound() == 364\n    assert SchurNumber(5).lower_bound() == 121\n"], "sample_1102": ["def test_Poly_eval_subs():\n    assert Poly(x**2 + 1).subs(x, 2) == 5\n    assert Poly(x**2 + y).subs(x, 2) == Poly(4 + y, y)\n    assert Poly(x**2 + y, x).subs(x, 2) == 4 + y\n    assert Poly(x**2 + y, x).subs(x, y + 2) == y**2 + 4*y + 4 + y\n    assert Poly(x**2 + y, x, y).subs(y, 3) == Poly(x**2 + 3, x)\n    assert Poly(x**2 + y, x, y).subs(x, y) == Poly(y**2 + y, y)\n    assert Poly(x**2 + y, x, y).subs(x, z) == Poly(z**2 + y, z, y)\n    assert Poly(x**2 + y, x, y).subs({x: 2, y: 3}) == 7\n\n    # Multivariate polynomial substitutions\n    assert Poly(x**2 + x*y, x, y).subs(x, y) == Poly(y**2 + y**2, y)\n    assert Poly(x**2 + x*y + y*z, x, y, z).subs({x: y, y: z, z: x}) == Poly(y**2 + y*x + z*y, y, z)\n    assert Poly(x**2 + x*y + y*z, x, y, z).subs({x: y + 1}) == Poly((y + 1)**2 + (y + 1)*y + y*z, y, z)\n    assert Poly(x**2 + x*y + y*z, x, y, z).subs({x: z + 1}) == Poly((z + 1)**2 + (z + 1)*y + y*z, y, z)\n    assert Poly(x**2 + x*y + y*z, x, y, z).subs({x: 2, y: 3, z: 4}) == 2**2 + 2*3 + 3*4\n\n    # Invalid substitutions\n    raises(PolynomialError, lambda: Poly(x + y).subs(x, y))\n    raises(PolynomialError, lambda: Poly(x + y, x, y", "def test_Poly_per():\n    f = Poly(x**2 + y*x + 1, x, y)\n    g = f.per(f.rep, gens=(x, y))\n\n    assert g == f\n    assert g.gens == (x, y)\n    assert g.domain == ZZ\n\n    h = f.per(f.rep, gens=(y, x))\n\n    assert h != f\n    assert h.gens == (y, x)\n    assert h.domain == ZZ\n\n    raises(PolynomialError, lambda: f.per(f.rep, gens=(x, y, z)))\n\n    # Testing per with remove parameter\n    f = Poly(x**3 + 2*x**2 + 3*x + 4)\n    g = f.per(f.rep, gens=(x,))\n    h = f.per(f.rep, gens=(x,), remove=0)\n\n    assert g == f\n    assert h == f.rep.dom.to_sympy(f.rep)\n\n    # Testing per with remove parameter and no gens\n    g = f.per(f.rep)\n    h = f.per(f.rep, remove=0)\n\n    assert g == f\n    assert h == f.rep.dom.to_sympy(f.rep)\n\n    # Testing per with different domain\n    f = Poly(x**3 + 2*x**2 + 3*x + 4, domain='QQ')\n    g = f.per(f.rep, gens=(x,))\n    h = f.per(f.rep, gens=(x,), remove=0)\n\n    assert g == f\n    assert h == f.rep.dom.to_sympy(f.rep)\n\n    # Testing per with EX domain\n    f = Poly(x**3 + 2*x**2 + 3*x + sqrt(2), domain='EX')\n    g = f.per(f.rep, gens=(x,))\n    h = f.per(f.rep, gens=(x,), remove=0)\n\n    assert g == f\n    assert h == f.rep.dom.to_sympy(f.rep)\n", "def test_Poly_pdiv():\n    # Test Polynomial pseudo-division\n\n    f = Poly(x**4 + 3*x**3 + 2*x**2 + x + 1, x)\n    g = Poly(x**2 + 1, x)\n    q = Poly(x**2 + 3*x + 1, x)\n    r = Poly(x**2 + 2*x, x)\n    \n    assert f.pdiv(g) == (q, r)\n    assert pdiv(f, g) == (q, r)\n    assert pdiv(f.as_expr(), g.as_expr()) == (q.as_expr(), r.as_expr())\n\n    f = Poly(3*x**3 + 5*x**2 + 4*x + 2, x)\n    g = Poly(2*x + 1, x)\n    q = Poly(3*x**2 + x, x)\n    r = Poly(3*x + 1, x)\n\n    assert f.pdiv(g) == (q, r)\n    assert pdiv(f, g) == (q, r)\n    assert pdiv(f.as_expr(), g.as_expr()) == (q.as_expr(), r.as_expr())\n\n    # Checking for edge cases where remainder should be zero\n    f = Poly(x**2 + 3*x + 2, x)\n    g = Poly(x + 1, x)\n    q = Poly(x + 2, x)\n    r = Poly(0, x)\n\n    assert f.pdiv(g) == (q, r)\n    assert pdiv(f, g) == (q, r)\n    assert pdiv(f.as_expr(), g.as_expr()) == (q.as_expr(), r.as_expr())\n\n    # Edge case where f is a multiple of g\n    f = Poly(4*x**2 + 8*x + 4, x)\n    g = Poly(2*x + 2, x)\n    q = Poly(2*x + 2, x)\n    r = Poly(0, x)\n\n    assert f.pdiv(g) == (q, r)\n    assert pdiv(f, g) == (q, r)\n    assert pdiv(f.as_expr(), g.as_expr()) == (q.as_expr(), r.as_expr())\n", "def test_Poly_unify_with_symbolic_domain():\n    # Test unification with different symbolic domains\n    f = Poly(x**2 + 1, x, domain=ZZ)\n    g = Poly(x**2 + 1, x, domain=ZZ[y])\n    \n    F, G = f._unify(g)[2:]\n    assert F == f.as_expr()\n    assert G == g.as_expr()\n\n    g = Poly(x**2 + y, x, domain=ZZ[y])\n    F, G = f._unify(g)[2:]\n    assert F == f.as_expr()\n    assert G == g.as_expr()\n\n    g = Poly(x**2 + y, x, y, domain=ZZ)\n    F, G = f._unify(g)[2:]\n    assert F == f.as_expr()\n    assert G == g.as_expr()\n\n    f = Poly(x**2 + y*z, x, y, z, domain=ZZ)\n    g = Poly(x**2 + y*z + t, x, y, z, t, domain=ZZ)\n    F, G = f._unify(g)[2:]\n    assert F == f.as_expr()\n    assert G == g.as_expr()\n\n    f = Poly(x**2 + 1, x, domain=ZZ)\n    g = Poly(x**2 + sqrt(2), x, domain=QQ.algebraic_field(sqrt(2)))\n    F, G = f._unify(g)[2:]\n    assert F == f.as_expr()\n    assert G == g.as_expr()\n", "def test_Poly_eval_subs():\n    f = Poly(x**2 + 2*x + 1, x)\n\n    assert f.eval(1) == 4\n    assert f.eval({x: 1}) == 4\n    assert f.eval(x, 1) == 4\n    assert f.eval({x: 1, y: 2}) == 4\n    assert f.eval(1, 2) == 4\n\n    assert f.subs(x, 1) == 4\n    assert f.subs({x: 1}) == 4\n    assert f.subs(x, y) == Poly(y**2 + 2*y + 1, y)\n    assert f.subs({x: y}) == Poly(y**2 + 2*y + 1, y)\n    assert f.subs({x: y, y: z}) == Poly(z**2 + 2*z + 1, z)\n\n    f = Poly(x**2*y + y**2*z + z**2*x, x, y, z)\n\n    assert f.eval({x: 1, y: 1, z: 1}) == 3\n    assert f.eval({x: 1, y: 2, z: 3}) == 17\n    assert f.eval(1, 1, 1) == 3\n    assert f.eval(1, 2, 3) == 17\n\n    assert f.subs(x, 1) == Poly(y + y**2*z + z**2, y, z)\n    assert f.subs({x: 1}) == Poly(y + y**2*z + z**2, y, z)\n    assert f.subs(x, y) == Poly(y**3 + y**2*z + z**2*y, y, z)\n    assert f.subs({x: y}) == Poly(y**3 + y**2*z + z**2*y, y, z)\n    assert f.subs({x: y, y: z}) == Poly(z**3 + z**2*x + x**2*z, z, x)\n"], "sample_1103": ["def test_isqrt():\n    # Test valid input\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(17) == 4\n    assert isqrt(4503599761588223) == 67108863\n\n    # Test negative input\n    raises(ValueError, lambda: isqrt(-1))\n\n    # Test large input\n    assert isqrt(2**100) == 2**50\n\n    # Test input for slow path\n    assert isqrt(4503599761588224) == 67108864\n\n    # Test input as float\n    assert isqrt(15.6) == 3\n    assert isqrt(16.0) == 4\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(2) == 1\n    assert isqrt(4) == 2\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(17) == 4\n    assert isqrt(4503599761588224 - 1) == 67108863  # 2**52 + 2**27 - 1\n    assert isqrt(4503599761588224) == 67108864\n    assert isqrt(4503599761588224 + 1) == 67108864\n    assert isqrt(2**63 - 1) == 3037000499\n    assert isqrt(2**63) == 3037000500\n    assert isqrt(2**63 + 1) == 3037000500\n    raises(ValueError, lambda: isqrt(-1))\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(8) == 2\n    assert isqrt(9) == 3\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(4503599761588223) == int(4503599761588223**0.5)\n    assert isqrt(4503599761588224) == int(4503599761588224**0.5)\n    assert isqrt(4503599761588225) == int(4503599761588225**0.5)\n\n    with raises(ValueError):\n        isqrt(-1)\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(24) == 4\n    assert isqrt(25) == 5\n    raises(ValueError, lambda: isqrt(-1))\n", "def test_Pow_isqrt_integer_nthroot():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(2) == 1\n    assert isqrt(4) == 2\n    assert isqrt(4503599761588223) == 67108863  # 2**52 + 2**27 - 1\n    assert isqrt(4503599761588224) == 67108864  # 2**52 + 2**27\n\n    # Testing for negative input\n    raises(ValueError, lambda: isqrt(-1))\n\n    # Testing integer_nthroot\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(0, 5) == (0, True)\n    assert integer_nthroot(1, 1) == (1, True)\n    assert integer_nthroot(1, 2) == (1, True)\n    assert integer_nthroot(8, 3) == (2, True)\n    raises(ValueError, lambda: integer_nthroot(-1, 2))\n    raises(ValueError, lambda: integer_nthroot(8, 0))\n    raises(ValueError, lambda: integer_nthroot(8, -1))\n"], "sample_1104": ["def test_Function_with_different_arguments():\n    f = Function('f')\n    assert str(f(x, y)) == \"f(x, y)\"\n    assert str(f(x, y, z)) == \"f(x, y, z)\"\n    assert str(f(x + y, z**2)) == \"f(x + y, z**2)\"\n    assert str(f(sin(x), cos(y))) == \"f(sin(x), cos(y))\"\n    assert str(f(1, 2, 3)) == \"f(1, 2, 3)\"\n    assert str(f(x, 1 + y, z - x)) == \"f(x, y + 1, z - x)\"\n", "def test_Transpose():\n    assert str(x.T) == \"x.T\"\n    M = MatrixSymbol(\"M\", 3, 3)\n    assert str(M.T) == \"M.T\"\n    assert str((M + M.T).T) == \"(M + M.T).T\"\n", "def test_ImaginaryUnit_operations():\n    assert str(I + 1) == \"I + 1\"\n    assert str(1 + I) == \"1 + I\"\n    assert str(I - 1) == \"I - 1\"\n    assert str(1 - I) == \"1 - I\"\n    assert str(I * 2) == \"2*I\"\n    assert str(2 * I) == \"2*I\"\n    assert str(I * I) == \"-1\"\n    assert str(I**2) == \"-1\"\n    assert str(I**3) == \"-I\"\n    assert str(I**4) == \"1\"\n    assert str(1 / I) == \"-I\"\n    assert str((1 + I) * (1 - I)) == \"2\"\n    assert str((1 + I)**2) == \"2*I\"\n    assert str((1 + I)**3) == \"-2 + 2*I\"\n", "def test_Rational_edge_cases():\n    assert str(Rational(1, 1)) == \"1\"\n    assert str(Rational(0, 1)) == \"0\"\n    assert str(Rational(1, 0)) == \"zoo\"  # infinity representation\n    assert str(Rational(-1, 0)) == \"-zoo\"\n    assert str(Rational(0, 0)) == \"nan\"  # not a number representation\n    assert str(Rational(123456789, 987654321)) == \"13717421/109739369\"  # reduced form\n", "def test_Complexes():\n    assert str(S.Complexes) == 'Complexes'\n"], "sample_1105": ["def test_inverse():\n    assert MatMul(A, Inverse(A), evaluate=True) == Identity(A.shape[0])\n    assert MatMul(Inverse(C), C, evaluate=True) == Identity(C.shape[0])\n    assert MatMul(Inverse(A), A, B, evaluate=True).doit() == MatMul(Identity(A.shape[1]), B).doit()\n", "def test_validate_shape_error():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 4, 2)\n    try:\n        MatMul(A, B)\n    except ShapeError as e:\n        assert str(e) == \"Matrices A and B are not aligned\"\n", "def test_eval_inverse():\n    assert MatMul(A, B).inv().doit() == MatMul(B.inv(), A.inv())\n    assert MatMul(C, D).inv().doit() == MatMul(D.inv(), C.inv())\n    assert MatMul(A, B, C).inv().doit() == MatMul(C.inv(), B.inv(), A.inv())\n    assert MatMul(2, C).inv().doit() == MatMul(C.inv(), 1/2)\n", "def test_validate_shape():\n    F = MatrixSymbol('F', 3, 2)\n    G = MatrixSymbol('G', 2, 4)\n    H = MatrixSymbol('H', 4, 3)\n    with pytest.raises(ShapeError):\n        MatMul(F, G, H, evaluate=True)\n    try:\n        MatMul(F, G, evaluate=True)\n    except ShapeError:\n        pytest.fail(\"ShapeError raised unexpectedly!\")\n", "def test_validate():\n    from sympy.matrices.expressions.matexpr import ShapeError\n    M1 = MatrixSymbol('M1', 2, 3)\n    M2 = MatrixSymbol('M2', 3, 4)\n    M3 = MatrixSymbol('M3', 4, 2)\n    # Valid cases\n    validate(M1, M2)\n    validate(M1, M2, M3)\n    # Invalid cases\n    with pytest.raises(ShapeError):\n        validate(M1, M3)\n    with pytest.raises(ShapeError):\n        validate(M2, M1)\n"], "sample_1106": ["def test_matadd_construction():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    \n    # Test MatAdd construction\n    expr = MatAdd(A, B, C)\n    assert expr.args == (A, B, C)\n    assert expr.shape == (2, 2)\n    \n    # Test with evaluate=True\n    expr_eval = MatAdd(A, B, C, evaluate=True)\n    assert expr_eval == MatAdd(A, B, C).doit()\n    assert expr_eval.shape == (2, 2)\n\n    # Test construction with zero elements\n    Z = ZeroMatrix(2, 2)\n    expr_zero = MatAdd(A, B, Z, evaluate=True)\n    assert expr_zero == MatAdd(A, B).doit()\n    assert expr_zero.shape == (2, 2)\n    \n    # Test construction with different shaped matrices (should raise ShapeError)\n    D = MatrixSymbol('D', 3, 3)\n    try:\n        MatAdd(A, D)\n    except ShapeError:\n        pass  # Expected behavior\n    else:\n        assert False, \"ShapeError not raised with different shaped matrices\"\n", "def test_matadd():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    Z = ZeroMatrix(2, 2)\n\n    expr = MatAdd(A, B, C)\n    assert expr.shape == (2, 2)\n    assert str(expr) == \"A + B + C\"\n    \n    # Test identity property\n    assert MatAdd(A, Z, B) == MatAdd(A, B)\n    \n    # Test transpose\n    assert transpose(MatAdd(A, B)) == MatAdd(transpose(A), transpose(B)).doit()\n    \n    # Test adjoint\n    assert adjoint(MatAdd(A, B)) == MatAdd(adjoint(A), adjoint(B)).doit()\n\n    # Test trace\n    from sympy.matrices.expressions.trace import trace\n    assert trace(MatAdd(A, B)).doit() == Add(trace(A), trace(B)).doit()\n    \n    # Test doit method\n    assert MatAdd(A, B, C).doit() == MatAdd(A, B, C)\n    \n    # Test construction with different shapes\n    E = MatrixSymbol('E', 3, 3)\n    with pytest.raises(ShapeError):\n        MatAdd(A, E)\n\n    # Test validation for mixed types\n    scalar = Symbol('s')\n    with pytest.raises(TypeError):\n        MatAdd(A, scalar)\n", "def test_matadd_basic():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n    D = MatrixSymbol('D', 3, 3)\n\n    expr = MatAdd(A, B, C)\n    assert expr.shape == (3, 3)\n    assert expr.args == (A, B, C)\n    assert isinstance(expr, MatAdd)\n\n    expr = MatAdd(A, B, C, D)\n    assert expr.shape == (3, 3)\n    assert expr.args == (A, B, C, D)\n    assert isinstance(expr, MatAdd)\n", "def test_matadd_basic_operations():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n\n    # Test basic addition\n    expr = MatAdd(A, B, C)\n    assert expr.shape == (2, 2)\n    assert expr == A + B + C\n\n    # Test addition with zero matrix\n    expr = MatAdd(A, ZeroMatrix(2, 2))\n    assert expr == A\n\n    # Test addition with evaluate=True\n    expr = MatAdd(A, B, C, evaluate=True)\n    assert expr == A + B + C\n\n    # Test addition with check=True\n    expr = MatAdd(A, B, C, check=True)\n    assert expr == A + B + C\n\n    # Test addition with evaluate=False\n    expr = MatAdd(A, B, C, evaluate=False)\n    assert expr == A + B + C\n\n    # Test addition with different shapes should raise ShapeError\n    E = MatrixSymbol('E', 3, 3)\n    try:\n        MatAdd(A, E)\n    except ShapeError:\n        pass\n    else:\n        assert False, \"ShapeError not raised\"\n\n    # Test addition with non-matrix types should raise TypeError\n    try:\n        MatAdd(A, 1)\n    except TypeError:\n        pass\n    else:\n        assert False, \"TypeError not raised\"\n", "def test_matadd_construction():\n    assert MatAdd(C, D) == MatAdd(C, D)\n    assert MatAdd(D, C) == MatAdd(D, C)\n    assert MatAdd(C) == C\n    assert MatAdd() == GenericZeroMatrix()\n"], "sample_1107": ["def test_ibin():\n    assert ibin(2) == [1, 0]\n    assert ibin(2, 4) == [0, 0, 1, 0]\n    assert ibin(2, 4)[::-1] == [0, 1, 0, 0]\n    assert list(ibin(3, 'all')) == [(0, 0), (0, 1), (1, 0), (1, 1)]\n    assert ibin(123, 10, str=True) == '0001111011'\n    assert ibin(123, 10, str=True)[::-1] == '1101111000'\n    assert list(ibin(3, 'all', str=True)) == ['000', '001', '010', '011', '100', '101', '110', '111']\n", "def test_permute_signs():\n    assert list(permute_signs((1, 2, 0))) == [(1, 2, 0), (-1, 2, 0), (1, -2, 0), (-1, -2, 0)]\n    assert list(permute_signs([3, 0, -4])) == [(3, 0, -4), (-3, 0, -4), (3, 0, 4), (-3, 0, 4)]\n    assert list(permute_signs([0, 0])) == [(0, 0)]\n    assert list(permute_signs([1, -1])) == [(1, -1), (-1, -1)]\n", "def test_generate_bell_edge_cases():\n    # Test with n = 0 which should raise an error\n    raises(ValueError, lambda: list(generate_bell(0)))\n\n    # Test with n = 1 which should yield only one permutation\n    assert list(generate_bell(1)) == [(0,)]\n\n    # Test with n = 2 which should yield two permutations\n    assert list(generate_bell(2)) == [(0, 1), (1, 0)]\n\n    # Test with n = 5 to verify the length and some sample permutations\n    permutations = list(generate_bell(5))\n    assert len(permutations) == factorial(5)\n    assert (0, 1, 2, 3, 4) in permutations\n    assert (4, 3, 2, 1, 0) in permutations\n\n    # Verify that each permutation is unique\n    assert len(permutations) == len(set(permutations))\n", "def test_ibin():\n    assert ibin(2) == [1, 0]\n    assert ibin(2, 4) == [0, 0, 1, 0]\n    assert ibin(2, 4)[::-1] == [0, 1, 0, 0]\n    assert list(ibin(3, 'all', str=True)) == ['000', '001', '010', '011', '100', '101', '110', '111']\n    assert ibin(7, str=True) == '111'\n    assert ibin(7, 5, str=True) == '00111'\n    assert ibin(3, str=True)[::-1] == '110'\n    assert ibin(5, 3, str=True) == '101'\n    assert list(ibin(4, 'all')) == [(0, 0), (0, 1), (1, 0), (1, 1)]\n", "def test_ibin():\n    assert ibin(2) == [1, 0]\n    assert ibin(2, 4) == [0, 0, 1, 0]\n    assert ibin(2, 4)[::-1] == [0, 1, 0, 0]\n    assert list(ibin(2, 'all')) == [(0, 0), (0, 1), (1, 0), (1, 1)]\n    assert ibin(123, 10, str=True) == '0001111011'\n    assert ibin(123, 10, str=True)[::-1] == '1101111000'\n    assert list(ibin(3, 'all', str=True)) == ['000', '001', '010', '011', '100', '101', '110', '111']\n    raises(ValueError, lambda: ibin(3, -1))\n"], "sample_1108": ["def test_common_prefix_suffix_edge_cases():\n    # Edge cases for common_prefix and common_suffix\n    assert common_prefix([1, 2, 3], [4, 5, 6]) == []\n    assert common_prefix(['a', 'b', 'c'], ['a', 'b', 'c']) == ['a', 'b', 'c']\n    assert common_prefix(['a', 'b', 'c'], ['a', 'b', 'd']) == ['a', 'b']\n    assert common_suffix([1, 2, 3], [4, 5, 3]) == [3]\n    assert common_suffix(['x', 'y', 'z'], ['a', 'b', 'z']) == ['z']\n    assert common_suffix(['x', 'y', 'z'], ['x', 'y', 'z']) == ['x', 'y', 'z']\n", "def test_multiset():\n    assert multiset('aabbc') == {'a': 2, 'b': 2, 'c': 1}\n    assert multiset([1, 2, 2, 3, 3, 3]) == {1: 1, 2: 2, 3: 3}\n    assert multiset('') == {}\n    assert multiset([1]) == {1: 1}\n    assert multiset([1, 1, 1, 1]) == {1: 4}\n    assert multiset('abcabc') == {'a': 2, 'b': 2, 'c': 2}\n    assert multiset((1, 2, 2, 3, 3, 3)) == {1: 1, 2: 2, 3: 3}\n    assert multiset('mississippi') == {'i': 4, 'm': 1, 'p': 2, 's': 4}\n", "def test_interactive_traversal():\n    from sympy.abc import a, b, c, d\n    expr = a + b * (c + d)\n        responses = iter([\"0\", \"1\", \"f\", \"d\"])\n        return next(responses)\n    import builtins\n    orig_input = builtins.input\n    builtins.input = mock_input\n    try:\n        result = interactive_traversal(expr)\n        assert result == b * (c + d)\n    finally:\n        builtins.input = orig_input\n", "def test_unflatten():\n    r = list(range(10))\n    assert unflatten(r) == list(zip(r[::2], r[1::2]))\n    assert unflatten(r, 5) == [tuple(r[:5]), tuple(r[5:])]\n    raises(ValueError, lambda: unflatten(list(range(10)), 3))\n    raises(ValueError, lambda: unflatten(list(range(10)), -2))\n", "def test_ibin():\n    assert ibin(2) == [1, 0]\n    assert ibin(2, 4) == [0, 0, 1, 0]\n    assert ibin(2, 4)[::-1] == [0, 1, 0, 0]\n    assert list(ibin(3, 'all')) == [(0, 0), (0, 1), (1, 0), (1, 1)]\n    assert ibin(123, 10, str=True) == '0001111011'\n    assert ibin(123, 10, str=True)[::-1] == '1101111000'\n    assert list(ibin(3, 'all', str=True)) == ['000', '001', '010', '011', '100', '101', '110', '111']\n"], "sample_1109": ["def test_floor_ceiling_frac_with_symbols():\n    # Testing floor, ceiling, and frac with symbolic arguments\n    alpha = Symbol('alpha', real=True)\n    beta = Symbol('beta', positive=True)\n    gamma = Symbol('gamma', negative=True)\n    delta = Symbol('delta', nonnegative=True)\n    epsilon = Symbol('epsilon', nonpositive=True)\n    \n    assert floor(alpha + beta) == floor(alpha) + floor(beta)\n    assert ceiling(alpha + beta) == ceiling(alpha) + ceiling(beta)\n    assert frac(alpha + beta) == frac(alpha) + frac(beta)\n\n    assert floor(alpha - beta) == floor(alpha) - ceiling(beta)\n    assert ceiling(alpha - beta) == ceiling(alpha) - floor(beta)\n    assert frac(alpha - beta) == frac(alpha) - frac(beta)\n    \n    assert floor(gamma + epsilon) == floor(gamma) + floor(epsilon)\n    assert ceiling(gamma + epsilon) == ceiling(gamma) + ceiling(epsilon)\n    assert frac(gamma + epsilon) == frac(gamma) + frac(epsilon)\n\n    assert floor(delta - epsilon) == floor(delta) - ceiling(epsilon)\n    assert ceiling(delta - epsilon) == ceiling(delta) - floor(epsilon)\n    assert frac(delta - epsilon) == frac(delta) - frac(epsilon)\n    \n    assert floor(floor(alpha)) == floor(alpha)\n    assert ceiling(ceiling(alpha)) == ceiling(alpha)\n    assert frac(frac(alpha)) == frac(alpha)\n\n    # Testing with combination of integers and symbols\n    assert floor(3 + alpha) == floor(alpha) + 3\n    assert ceiling(2 + beta) == ceiling(beta) + 2\n    assert frac(5 - gamma) == frac(-gamma) + 5\n    \n    # Testing relations with symbols\n    assert (floor(alpha) < alpha) == False\n    assert (ceiling(alpha) >= alpha) == True\n    assert (frac(alpha) < 1) == True\n    assert (frac(alpha) >= 0) == True\n", "def test_floor_ceiling_with_mixed_expressions():\n    expr1 = floor(x + ceiling(y) + pi*I)\n    expr2 = ceiling(y - floor(x) - E*I)\n    expr3 = floor(exp(1) + ceiling(log(2)) - I)\n    expr4 = ceiling(sqrt(2) + floor(3.5) + I)\n    \n    assert expr1 == floor(x) + ceiling(y) + 3*I\n    assert expr2 == ceiling(y) - floor(x) - 2*I\n    assert expr3 == 3 - I\n    assert expr4 == 4 + I\n", "def test_frac_complex_numbers():\n    from sympy import re, im\n\n    # Complex numbers\n    z1 = 2.5 + 3.5*I\n    z2 = -2.5 - 3.5*I\n    z3 = 2.5 - 3.5*I\n    z4 = -2.5 + 3.5*I\n\n    assert frac(z1) == frac(re(z1)) + I*frac(im(z1))\n    assert frac(z2) == frac(re(z2)) + I*frac(im(z2))\n    assert frac(z3) == frac(re(z3)) + I*frac(im(z3))\n    assert frac(z4) == frac(re(z4)) + I*frac(im(z4))\n\n    assert frac(z1).rewrite(floor) == (re(z1) - floor(re(z1))) + I*(im(z1) - floor(im(z1)))\n    assert frac(z2).rewrite(floor) == (re(z2) - floor(re(z2))) + I*(im(z2) - floor(im(z2)))\n    assert frac(z3).rewrite(floor) == (re(z3) - floor(re(z3))) + I*(im(z3) - floor(im(z3)))\n    assert frac(z4).rewrite(floor) == (re(z4) - floor(re(z4))) + I*(im(z4) - floor(im(z4)))\n\n    assert frac(z1).rewrite(ceiling) == (re(z1) + ceiling(-re(z1))) + I*(im(z1) + ceiling(-im(z1)))\n    assert frac(z2).rewrite(ceiling) == (re(z2) + ceiling(-re(z2))) + I*(im(z2) + ceiling(-im(z2)))\n    assert frac(z3).rewrite(ceiling) == (re(z3) + ceiling(-re(z3))) + I*(im(z3) + ceiling(-im(z3)))\n    assert frac(z4).rewrite(ceiling) == (re(z4) + ceiling(-re(z4))) + I*(im(z4) + ceiling(-im(z4)))\n\n    # Check properties\n    assert frac(z1).is_finite == True\n    assert frac(z1).is_real == False\n    assert frac(z1).is_imaginary == None\n    assert frac(z1).is_integer == None\n   ", "def test_fractional_argument_behavior():\n    from sympy import cos\n\n    # Testing floor with fractional arguments\n    assert floor(1.5) == 1\n    assert floor(-1.5) == -2\n    assert floor(2.5*I) == 2*I\n    assert floor(-2.5*I) == -3*I\n\n    # Testing ceiling with fractional arguments\n    assert ceiling(1.5) == 2\n    assert ceiling(-1.5) == -1\n    assert ceiling(2.5*I) == 3*I\n    assert ceiling(-2.5*I) == -2*I\n\n    # Testing frac with fractional arguments\n    assert frac(1.5) == 0.5\n    assert frac(-1.5) == 0.5\n    assert frac(2.5*I) == 0.5*I\n    assert frac(-2.5*I) == 0.5*I\n\n    # Testing floor, ceiling, and frac with trigonometric functions\n    assert floor(cos(1)) == 0\n    assert ceiling(cos(1)) == 1\n    assert frac(cos(1)).rewrite(floor) == cos(1) - floor(cos(1))\n    assert frac(cos(1)).rewrite(ceiling) == cos(1) + ceiling(-cos(1))\n\n    # Testing floor, ceiling, and frac with exponential functions\n    assert floor(exp(1)) == 2\n    assert ceiling(exp(1)) == 3\n    assert frac(exp(1)).rewrite(floor) == exp(1) - floor(exp(1))\n    assert frac(exp(1)).rewrite(ceiling) == exp(1) + ceiling(-exp(1))\n", "def test_floor_ceiling_frac_combinations():\n    assert floor(ceiling(frac(x))) == 0\n    assert ceiling(floor(frac(x))) == 0\n    assert frac(floor(x)) == 0\n    assert frac(ceiling(x)) == 0\n\n    assert floor(frac(x) + frac(y)) == 0\n    assert ceiling(frac(x) + frac(y)) == 2\n    assert floor(frac(x) - frac(y)) == -1\n    assert ceiling(frac(x) - frac(y)) == 1\n\n    assert floor(frac(x)*frac(y)) == 0\n    assert ceiling(frac(x)*frac(y)) == 1\n\n    assert floor(frac(x)**2) == 0\n    assert ceiling(frac(x)**2) == 1\n\n    assert floor(2*frac(x)) == 0\n    assert ceiling(2*frac(x)) == 2\n\n    assert floor(frac(x)/2) == 0\n    assert ceiling(frac(x)/2) == 1\n\n    assert floor(frac(x) + 1) == 1\n    assert ceiling(frac(x) + 1) == 2\n\n    assert floor(frac(x) - 1) == -1\n    assert ceiling(frac(x) - 1) == 0\n"], "sample_1110": ["def test_ITE():\n    from sympy.functions import Piecewise, ITE\n\n    expr = ITE(x > 0, 1, -1)\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '((1) if (x > 0) else (-1) if True else None)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == '((1) if (x > 0) else (-1) if True else None)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.select([x > 0, True], [1, -1], default=nan)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.select([x > 0, True], [1, -1], default=nan)'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == 'sympy.Piecewise((1, x > 0), (-1, True))'\n", "def test_AbstractPythonCodePrinter():\n    from sympy import ITE, Matrix, Function, Indexed\n    from sympy.codegen.ast import FunctionDefinition, While, Return, Declaration, Print, Stream\n\n    x, y = symbols('x y')\n    prntr = AbstractPythonCodePrinter()\n\n    assert prntr.doprint(ITE(x > 0, x, -x)) == '((x) if (x > 0) else (-x))'\n    assert prntr.doprint(Matrix([[1, 2], [3, 4]])) == 'Matrix([[1, 2], [3, 4]])'\n    assert prntr.doprint(Function('func')(x, y)) == 'func(x, y)'\n    assert prntr.doprint(Indexed('A', x, y)) == 'A[x, y]'\n\n    fd = FunctionDefinition(Function('func'), [x, y], [Return(x + y)])\n    assert prntr.doprint(fd) == 'def func(x, y):\\n    return x + y'\n\n    whl = While(x < 10, [x + 1])\n    assert prntr.doprint(whl) == 'while x < 10:\\n    x + 1'\n\n    decl = Declaration(x, y)\n    assert prntr.doprint(decl) == 'x = y'\n\n    prnt = Print(None, [x, y])\n    assert prntr.doprint(prnt) == 'print(x, y)'\n\n    stream = Stream('stdout')\n    assert prntr.doprint(stream) == 'sys.stdout'\n", "def test_print_known_func():\n    from sympy import Abs\n    prntr = PythonCodePrinter()\n    expr = Abs(x)\n    assert prntr.doprint(expr) == 'abs(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'abs(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'abs(x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'abs(x)'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == 'sympy.Abs(x)'\n", "def test_Relational():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(Eq(x, y)) == '(x == y)'\n    assert prntr.doprint(x != y) == '(x != y)'\n    assert prntr.doprint(x < y) == '(x < y)'\n    assert prntr.doprint(x <= y) == '(x <= y)'\n    assert prntr.doprint(x > y) == '(x > y)'\n    assert prntr.doprint(x >= y) == '(x >= y)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(Eq(x, y)) == 'numpy.equal(x, y)'\n    assert prntr.doprint(x != y) == 'numpy.not_equal(x, y)'\n    assert prntr.doprint(x < y) == 'numpy.less(x, y)'\n    assert prntr.doprint(x <= y) == 'numpy.less_equal(x, y)'\n    assert prntr.doprint(x > y) == 'numpy.greater(x, y)'\n    assert prntr.doprint(x >= y) == 'numpy.greater_equal(x, y)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(Eq(x, y)) == 'numpy.equal(x, y)'\n    assert prntr.doprint(x != y) == 'numpy.not_equal(x, y)'\n    assert prntr.doprint(x < y) == 'numpy.less(x, y)'\n    assert prntr.doprint(x <= y) == 'numpy.less_equal(x, y)'\n    assert prntr.doprint(x > y) == 'numpy.greater(x, y)'\n    assert prntr.doprint(x >= y) == 'numpy.greater_equal(x, y)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr1 = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr1) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n\n    assert p.doprint(S.Exp1) == 'sympy.E'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n\n    expr2 = S.Half\n    assert p.doprint(expr2) == 'x**(1/2)'\n\n    expr3 = sqrt(x)\n    assert p.doprint(expr3) == 'sympy.sqrt(x)'\n\n    assert p.doprint(Sign(x)) == 'sympy.sign(x)'\n\n    expr4 = Piecewise((1, Eq(x, 0)), (2, x > 6))\n    assert p.doprint(expr4) == 'sympy.Piecewise((1, sympy.Eq(x, 0)), (2, x > 6))'\n\n    expr5 = KroneckerDelta(x, y)\n    assert p.doprint(expr5) == 'sympy.KroneckerDelta(x, y)'\n"], "sample_1111": ["def test_quadratic():\n    x = Symbol('x')\n    lines = [\n        '     100 |                                                   .   ',\n        '        |                                                      .',\n        '        |                                                      .',\n        '        |                                                     . ',\n        '        |                                                     . ',\n        '        |                                                    .  ',\n        '        |                                                   .   ',\n        '        |                                                  .    ',\n        '        |                                                 .     ',\n        '        |                                                .      ',\n        '      50 |---------------------------------------------..------',\n        '        |                                             .        ',\n        '        |                                            .         ',\n        '        |                                           .          ',\n        '        |                                         ..           ',\n        '        |                                        .             ',\n        '        |                                      ..              ',\n        '        |                                    ..                ',\n        '        |                                   .                  ',\n        '        |                                 ..                   ',\n        '      0 |_______________________________________________________',\n        '         -10                        0                          10'\n    ]\n    assert lines == list(textplot_str(x**2, -10, 10))\n", "def test_quadratic():\n    x = Symbol('x')\n    lines = [\n        '     100 | .                                                     ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      50 |------------------------------------------------------',\n        '        |                .                                      ',\n        '        |                   .                                   ',\n        '        |                      .                                ',\n        '        |                         .                             ',\n        '        |                            .                          ',\n        '        |                               .                       ',\n        '        |                                  .                    ',\n        '        |                                     .                 ',\n        '        |                                        .              ',\n        '       0 |______________________________________________________',\n        '         -5                          0                          5'\n    ]\n    assert lines == list(textplot_str(x**2, -5, 5))\n", "def test_quadratic():\n    x = Symbol('x')\n    lines = [\n        '      4 |                                                  .    ',\n        '        |                                                 .     ',\n        '        |                                                .      ',\n        '        |                                               .       ',\n        '        |                                              .        ',\n        '        |                                             .         ',\n        '        |                                            .          ',\n        '        |                                           .           ',\n        '        |                                          .            ',\n        '        |                                         .             ',\n        '      0 |----------------------------------------.--------------',\n        '        |                                       .               ',\n        '        |                                      .                ',\n        '        |                                     .                 ',\n        '        |                                    .                  ',\n        '        |                                   .                   ',\n        '        |                                  .                    ',\n        '        |                                 .                     ',\n        '        |                                .                      ',\n        '        |                               .                       ',\n        '     -4 |_______________________________________________________',\n        '         -2                         0                          2'\n    ]\n    assert lines == list(textplot_str(x**2 - 4, -2, 2))\n", "def test_sqrt():\n    x = Symbol('x')\n    lines = [\n        '      2 |.                                                      ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      1 |--------------------------..---------------------------',\n        '        |                         /                             ',\n        '        |                       ..                              ',\n        '        |                     ..                                ',\n        '        |                   ..                                  ',\n        '        |                 ..                                    ',\n        '        |               ..                                      ',\n        '        |             ..                                        ',\n        '        |           ..                                          ',\n        '        |        ...                                            ',\n        '      0 |.....__________________________________________________',\n        '         0                          1                          4'\n    ]\n    assert lines == list(textplot_str(sqrt(x), 0, 4))\n", "def test_quadratic():\n    x = Symbol('x')\n    lines = [\n        '    100 |..                                                    ..',\n        '        |  ..                                                ..  ',\n        '        |    ..                                            ..    ',\n        '        |      ..                                        ..      ',\n        '        |        ..                                    ..        ',\n        '        |          ..                                ..          ',\n        '        |            ..                            ..            ',\n        '        |              ..                        ..              ',\n        '        |                ..                    ..                ',\n        '        |                  ..                ..                  ',\n        '      0 |--------------------..------------..--------------------',\n        '        |                  ..  ..        ..  ..                  ',\n        '        |                ..      ..    ..      ..                ',\n        '        |              ..          ....          ..              ',\n        '        |            ..                            ..            ',\n        '        |          ..                                ..          ',\n        '        |        ..                                    ..        ',\n        '        |      ..                                        ..      ',\n        '        |    ..                                            ..    ',\n        '        |  ..                                                ..  ',\n        '   -100 |..                                                    ..',\n        '        -10                         0                          10'\n    ]\n    assert lines == list(textplot_str(x**2 - 100, -10, 10))\n"], "sample_1112": ["def test_is_palindromic():\n    assert is_palindromic(121)\n    assert not is_palindromic(123)\n    assert is_palindromic(12321)\n    assert is_palindromic(0)\n    assert is_palindromic(1)\n    assert is_palindromic(1221)\n    assert not is_palindromic(1234)\n    assert is_palindromic(88)\n    assert not is_palindromic(88, 8)\n    assert is_palindromic(0o121, 8)\n    assert not is_palindromic(81, 10)\n    assert is_palindromic(121, 8)\n    assert is_palindromic(121, 10)\n", "def test_is_palindromic():\n    assert is_palindromic(121) == True\n    assert is_palindromic(12321) == True\n    assert is_palindromic(123) == False\n    assert is_palindromic(88, 8) == False\n    assert is_palindromic(0o121, 8) == True\n    assert is_palindromic(0b1001, 2) == True\n    assert is_palindromic(0b1010, 2) == False\n    assert is_palindromic(121, 5) == False\n    raises(ValueError, lambda: is_palindromic(121, 1))\n", "def test_is_palindromic():\n    assert is_palindromic(1)\n    assert is_palindromic(22)\n    assert is_palindromic(121)\n    assert not is_palindromic(123)\n    assert is_palindromic(88, 10)\n    assert not is_palindromic(88, 8)\n    assert is_palindromic(0o121, 8)\n    assert not is_palindromic(81, 10)\n    assert is_palindromic(121, 8)\n    assert not is_palindromic(1234, 5)\n    assert is_palindromic(0b101, 2)\n", "def test_is_palindromic():\n    assert is_palindromic(121)\n    assert is_palindromic(12321)\n    assert not is_palindromic(123)\n    assert is_palindromic(-121)  # negative numbers can be palindromic\n    assert is_palindromic(88, 10)\n    assert not is_palindromic(88, 8)\n    assert is_palindromic(0o121, 8)\n    assert not is_palindromic(0o121, 10)\n    assert is_palindromic(0x121, 16)\n    raises(ValueError, lambda: is_palindromic(123, 1))  # base must be greater than 1\n", "def test_is_palindromic():\n    assert is_palindromic(121)\n    assert is_palindromic(11)\n    assert is_palindromic(12321)\n    assert not is_palindromic(123)\n    assert is_palindromic(0)\n    assert is_palindromic(9)\n    assert is_palindromic(88, 10)\n    assert not is_palindromic(88, 8)\n    assert not is_palindromic(0o121, 10)\n    assert is_palindromic(0o121, 8)\n    assert is_palindromic(0x1A1, 16)\n    assert not is_palindromic(0x1A2, 16)\n"], "sample_1113": ["def test_BlockDiagMatrix():\n    I = Identity(2)\n    Z = ZeroMatrix(2, 2)\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n\n    BDM = BlockDiagMatrix(I, Z, X, Y)\n    assert BDM.shape == (8, 8)\n    assert BDM.blockshape == (4, 4)\n    assert BDM.rowblocksizes == [2, 2, 2, 2]\n    assert BDM.colblocksizes == [2, 2, 2, 2]\n\n    # Test inverse\n    inv_BDM = BDM._eval_inverse()\n    assert isinstance(inv_BDM, BlockDiagMatrix)\n    assert inv_BDM.shape == BDM.shape\n\n    # Test transpose\n    trans_BDM = BDM._eval_transpose()\n    assert isinstance(trans_BDM, BlockDiagMatrix)\n    assert trans_BDM.shape == BDM.shape\n\n    # Test block multiplication\n    result = BDM._blockmul(BDM)\n    assert isinstance(result, BlockDiagMatrix)\n    assert result.shape == BDM.shape\n\n    # Test block addition\n    result = BDM._blockadd(BDM)\n    assert isinstance(result, BlockDiagMatrix)\n    assert result.shape == BDM.shape\n\n    # Test is_Identity property\n    assert not BDM.is_Identity\n    BDM_identity = BlockDiagMatrix(Identity(2), Identity(2))\n    assert BDM_identity.is_Identity\n", "def test_blockmatrix_trace():\n    I = Identity(3)\n    Z = ZeroMatrix(3, 3)\n    B = BlockMatrix([[I, Z], [Z, I]])\n    assert trace(B) == 6\n\n    A1 = MatrixSymbol('A1', n, n)\n    A2 = MatrixSymbol('A2', m, m)\n    B = BlockMatrix([[A1, ZeroMatrix(n, m)], [ZeroMatrix(m, n), A2]])\n    assert trace(B) == trace(A1) + trace(A2)\n", "def test_blockmatrix_creation():\n    # Test creation of a valid BlockMatrix\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    block_matrix = BlockMatrix([[X, Y], [Y, Z]])\n    assert block_matrix.shape == (4, 4)\n    assert block_matrix.blockshape == (2, 2)\n    assert block_matrix.rowblocksizes == [2, 2]\n    assert block_matrix.colblocksizes == [2, 2]\n\n    # Test creation of an invalid BlockMatrix\n    W = MatrixSymbol('W', 2, 3)\n    with raises(ValueError):\n        BlockMatrix([[X, W], [Y, Z]])\n\n    # Test creation of a BlockMatrix with irregular blocks\n    with raises(ValueError):\n        BlockMatrix([[X, Y], [Z]])\n", "def test_blockmatrix_shape():\n    I = Identity(3)\n    Z = ZeroMatrix(3, 3)\n    B = BlockMatrix([[I, Z], [Z, I]])\n    assert B.shape == (6, 6)\n    assert B.blockshape == (2, 2)\n    assert B.rowblocksizes == [3, 3]\n    assert B.colblocksizes == [3, 3]\n\n    A1 = MatrixSymbol('A1', n, k)\n    A2 = MatrixSymbol('A2', n, l)\n    A3 = MatrixSymbol('A3', m, k)\n    A4 = MatrixSymbol('A4', m, l)\n    A = BlockMatrix([[A1, A2], [A3, A4]])\n    assert A.shape == (n + m, k + l)\n    assert A.blockshape == (2, 2)\n    assert A.rowblocksizes == [n, m]\n    assert A.colblocksizes == [k, l]\n", "def test_BlockDiagMatrix():\n    I = Identity(2)\n    Z = ZeroMatrix(2, 2)\n    BDM = BlockDiagMatrix(I, Z, I)\n    assert BDM.shape == (6, 6)\n    assert BDM.blockshape == (3, 3)\n    assert BDM.blocks[0, 0] == I\n    assert BDM.blocks[0, 1] == ZeroMatrix(2, 2)\n    assert BDM.blocks[1, 1] == Z\n    assert BDM.blocks[2, 2] == I\n\n    # Test transpose\n    assert BDM.transpose().blocks[0, 0] == I.T\n    assert BDM.transpose().blocks[1, 1] == Z.T\n    assert BDM.transpose().blocks[2, 2] == I.T\n\n    # Test inverse\n    inv_BDM = BlockDiagMatrix(I.inverse(), Z.inverse(), I.inverse())\n    assert BDM._eval_inverse() == inv_BDM\n\n    # Test block multiplication\n    BDM2 = BlockDiagMatrix(I, I, I)\n    assert BDM._blockmul(BDM2).blocks[0, 0] == I*I\n    assert BDM._blockmul(BDM2).blocks[1, 1] == Z*I\n    assert BDM._blockmul(BDM2).blocks[2, 2] == I*I\n\n    # Test block addition\n    BDM3 = BlockDiagMatrix(2*I, Z, I)\n    assert BDM._blockadd(BDM3).blocks[0, 0] == 3*I\n    assert BDM._blockadd(BDM3).blocks[1, 1] == Z+Z\n    assert BDM._blockadd(BDM3).blocks[2, 2] == 2*I\n"], "sample_1114": ["def test_rationals_boundary():\n    assert S.Rationals.boundary == S.Reals\n    assert S.Rationals.closure == S.Reals\n    assert S.Rationals.is_open == False\n    assert S.Rationals.is_closed == False\n    assert S.Rationals.as_relational(x) == And(Eq(floor(x), x), Eq(x, floor(x)), x >= S.NegativeInfinity, x < S.Infinity)\n", "def test_Rationals_contains():\n    assert S.Zero in S.Rationals\n    assert S.One in S.Rationals\n    assert S.NegativeOne in S.Rationals\n    assert Rational(1, 3) in S.Rationals\n    assert Rational(-1, 3) in S.Rationals\n    assert sqrt(2) not in S.Rationals\n    assert pi not in S.Rationals\n    assert 1.1 not in S.Rationals\n    assert S.Complexes not in S.Rationals\n    assert Range(1, 10) not in S.Rationals\n", "def test_Rationals_contains():\n    # Verify basic properties of Rationals\n    assert Rational(1, 3) in S.Rationals\n    assert Rational(-1, 3) in S.Rationals\n    assert Rational(1, 2) in S.Rationals\n    assert Rational(-1, 2) in S.Rationals\n    assert 0 in S.Rationals\n    assert 1 in S.Rationals\n    assert -1 in S.Rationals\n    assert pi not in S.Rationals\n    assert sqrt(2) not in S.Rationals\n    assert I not in S.Rationals\n    assert 1.5 not in S.Rationals\n    assert S('1/3') in S.Rationals\n    assert S('1/3') in S.Rationals\n    assert Rational(2, 3) in S.Rationals\n    assert Rational(-2, 3) in S.Rationals\n    assert 2 not in S.Rationals\n    assert -2 not in S.Rationals\n    assert S(2) in S.Rationals\n    assert Basic() not in S.Rationals\n", "def test_Rationals_contains():\n    assert S.Rationals._contains(S.Half) == True\n    assert S.Rationals._contains(Rational(-1, 2)) == True\n    assert S.Rationals._contains(2) == True\n    assert S.Rationals._contains(sqrt(2)) == False\n    assert S.Rationals._contains(\"string\") == False\n    assert S.Rationals._contains(1.5) == False\n    # symbolic checks\n    x = Symbol('x', real=True, rational=True)\n    assert S.Rationals._contains(x) == True\n    y = Symbol('y', real=True, irrational=True)\n    assert S.Rationals._contains(y) == False\n", "def test_rationals_contains():\n    R = S.Rationals\n    assert S.Half in R\n    assert -S.Half in R\n    assert 5 in R\n    assert -5 in R\n    assert 0 in R\n    assert 5.5 not in R\n    assert 1.0 not in R\n    assert S.Pi not in R\n    assert S(0.5) not in R\n    assert sqrt(2) not in R\n    assert Rational(22, 7) in R\n    assert Rational(355, 113) in R\n"], "sample_1115": ["def test_contract_tensor():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    a, b, c, d = tensor_indices('a,b,c,d', Lorentz)\n    p, q, r = tensor_heads('p,q,r', [Lorentz])\n    g = Lorentz.metric\n\n    # Contraction with metric\n    expr = g(a, b) * p(-b)\n    contracted_expr = expr.contract_metric(g)\n    assert contracted_expr == p(a)\n\n    # Double contraction with metric\n    expr = g(a, b) * g(b, c) * p(-c)\n    contracted_expr = expr.contract_metric(g)\n    assert contracted_expr == p(a)\n\n    # Nested contraction with metric\n    expr = g(a, b) * (g(c, d) * p(-d) + q(c))\n    contracted_expr = expr.contract_metric(g)\n    assert contracted_expr == g(a, b) * p(-b) + q(a)\n    contracted_expr = contracted_expr.contract_metric(g)\n    assert contracted_expr == p(a) + q(a)\n\n    # Contract a tensor with itself\n    expr = p(a) * p(-a)\n    assert expr.contract_metric(g) == p(a) * p(-a)\n\n    # Raise and lower indices\n    expr = g(a, b) * p(-b)\n    assert expr.contract_metric(g) == p(a)\n\n    # Contraction in a more complex expression\n    expr = p(a) * q(b) * g(-a, -b)\n    contracted_expr = expr.contract_metric(g)\n    assert contracted_expr == p(a) * q(-a)\n", "def test_tensor_element():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    i, j, k, l = tensor_indices('i j k l', Lorentz)\n    A = TensorHead('A', [Lorentz]*2)\n    B = TensorHead('B', [Lorentz]*2)\n    C = TensorHead('C', [Lorentz])\n\n    expr = A(i, j)\n    index_map = {i: 0}\n    te = TensorElement(expr, index_map)\n    assert te.get_free_indices() == [j]\n    assert str(te) == \"A(0, j)\"\n    assert te.indices == (i, j)\n\n    expr = A(i, j) * B(-j, k)\n    index_map = {i: 0, k: 1}\n    te = TensorElement(expr, index_map)\n    assert te.get_free_indices() == [-j]\n    assert str(te) == \"A(0, j)*B(-j, 1)\"\n    assert te.indices == (i, j, -j, k)\n    assert te.index_map == {i: 0, k: 1}\n\n    expr = C(i) + C(j)\n    index_map = {i: 0}\n    te = TensorElement(expr, index_map)\n    assert te.get_free_indices() == [j]\n    assert str(te) == \"C(0) + C(j)\"\n    assert te.indices == (i, j)\n    assert te.index_map == {i: 0}\n\n    # Test with additional indices\n    expr = A(i, j) + B(i, -j)\n    index_map = {i: 0, j: 1}\n    te = TensorElement(expr, index_map)\n    assert te.get_free_indices() == []\n    assert str(te) == \"A(0, 1) + B(0, -1)\"\n    assert te.indices == (i, j, i, -j)\n    assert te.index_map == {i: 0, j: 1}\n\n    # Test substitution of a single index\n    expr = A(i, j) * B(-j, k)\n    index_map = {i: 0}\n    te = TensorElement(expr, index_map)\n    assert te.get_free_indices() == [k, -j]\n    assert str(te) == \"A(0, j)*B(-j,", "def test_TensorElement():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    a, b, c, d = tensor_indices('a, b, c, d', Lorentz)\n    A = TensorHead('A', [Lorentz])\n    B = TensorHead('B', [Lorentz])\n    \n    # Define TensorElement\n    t_elem = TensorElement(A(a), {a: 2})\n    \n    # Test correct substitution of index value\n    assert t_elem.get_free_indices() == [b]\n    assert t_elem.expr == A(a)\n    assert t_elem.index_map == {a: 2}\n    \n    # Test that the TensorElement behaves as expected in expressions\n    expr = t_elem * B(b)\n    assert expr.get_free_indices() == [b]\n    \n    repl = {A(a): [1, 2, 3, 4], B(b): [5, 6, 7, 8], Lorentz: diag(1, -1, -1, -1)}\n    assert expr.replace_with_arrays(repl, [b]) == Array([30])\n    \n    # Test multiple substitutions\n    t_elem_multi = TensorElement(A(a)*B(b), {a: 1, b: 2})\n    assert t_elem_multi.get_free_indices() == [c]\n    assert t_elem_multi.expr == A(a) * B(b)\n    assert t_elem_multi.index_map == {a: 1, b: 2}\n    \n    expr_multi = t_elem_multi * B(c)\n    assert expr_multi.get_free_indices() == [c]\n    \n    repl_multi = {A(a): [1, 2, 3, 4], B(b): [5, 6, 7, 8], Lorentz: diag(1, -1, -1, -1)}\n    assert expr_multi.replace_with_arrays(repl_multi, [c]) == Array([48])\n    \n    # Test with multiple TensorElement instances\n    t_elem1 = TensorElement(A(a), {a: 1})\n    t_elem2 = TensorElement(B(b), {b: 2})\n    expr_combined = t_elem1 * t_elem2\n    assert expr_combined.get_free_indices() == [c]\n    \n    repl_combined = {A(a): [1, 2, 3, 4], B(b): [5, 6, ", "def test_tensor_data_contraction():\n    L = TensorIndexType(\"L\", dim=4, dummy_name='L')\n    i0, i1, i2, i3 = tensor_indices('i0:4', L)\n    A = TensorHead(\"A\", [L, L], TensorSymmetry.no_symmetry(2))\n    B = TensorHead(\"B\", [L, L], TensorSymmetry.no_symmetry(2))\n\n    A.data = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12],\n        [13, 14, 15, 16]\n    ]\n\n    B.data = [\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]\n    ]\n\n    t = A(i0, i1) * B(-i1, i2)\n    result = t.data\n    expected_result = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12],\n        [13, 14, 15, 16]\n    ]\n    assert result == Array(expected_result)\n\n    t = A(i0, -i1) * B(i1, i2)\n    result = t.data\n    expected_result = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12],\n        [13, 14, 15, 16]\n    ]\n    assert result == Array(expected_result)\n", "def test_tensor_element():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    A, B = tensor_heads(\"A B\", [L])\n    D = TensorHead(\"D\", [L, L])\n\n    # Test TensorElement creation\n    te = TensorElement(A(i) + B(i), {i: 1})\n    assert te.expr == A(i) + B(i)\n    assert te.index_map == {i: 1}\n\n    # Test nested TensorElement creation\n    te_nested = TensorElement(A(i) + TensorElement(B(i), {i: 2}), {i: 1})\n    assert te_nested.expr == A(i) + TensorElement(B(i), {i: 2})\n    assert te_nested.index_map == {i: 1}\n\n    # Test tensor element with contraction\n    expr = D(i, -i)\n    repl = {D(i, -i): 2}\n    assert expr.replace_with_arrays(repl, []) == 2\n    expr = D(i, -j) + D(j, -i)\n    assert expr.replace_with_arrays(repl, [i, j]) == Array([[2, 2], [2, 2]])\n\n    # Test TensorElement with replacement\n    expr = TensorElement(D(i, -j) + D(j, -i), {i: 0})\n    repl = {D(i, -j): [[1, 2], [3, 4]], L: diag(1, -1)}\n    assert expr._extract_data(repl) == ([j], Array([1, 2]))\n\n    # Test TensorElement equality\n    te1 = TensorElement(A(i), {i: 1})\n    te2 = TensorElement(A(i), {i: 1})\n    te3 = TensorElement(A(i), {i: 2})\n    assert te1 == te2\n    assert te1 != te3\n\n    # Test TensorElement's get_free_indices\n    te = TensorElement(A(i) + B(i), {i: 1})\n    assert te.get_free_indices() == []\n\n    # Test nested TensorElement replacement\n    te_nested = TensorElement(A(i) + TensorElement(B(i), {i: 2}), {i: 1})\n    assert te_nested.replace_with_arrays(repl, [j]) == Array([1 + 2])\n"], "sample_1116": ["def test_refine_unitary():\n    assert refine(C.I, Q.unitary(C)) == C.conjugate()\n", "def test_refine_unitary_singular():\n    # Test for unitary matrix\n    U = MatrixSymbol('U', n, n)\n    assert refine(U.I, Q.unitary(U)) == U.conjugate()\n\n    # Test for singular matrix\n    S = MatrixSymbol('S', n, n)\n    with raises(ValueError, match=\"Inverse of singular matrix\"):\n        refine(S.I, Q.singular(S))\n", "def test_refine_unitary_singular():\n    U = MatrixSymbol('U', n, n)\n    S = MatrixSymbol('S', n, n)\n\n    # Test for unitary matrix\n    assert refine(U.I, Q.unitary(U)) == U.conjugate()\n\n    # Test for singular matrix\n    raises(ValueError, lambda: refine(S.I, Q.singular(S)))\n", "def test_refine_unitary_and_singular():\n    from sympy import conjugate\n    \n    X = MatrixSymbol('X', n, n)\n    \n    with assuming(Q.unitary(X)):\n        assert refine(Inverse(X), Q.unitary(X)) == conjugate(X)\n    \n    raises(ValueError, lambda: refine(Inverse(X), Q.singular(X)))\n", "def test_refine_unitary_singular():\n    U = MatrixSymbol('U', n, n)\n    S = MatrixSymbol('S', n, n)\n    \n    # Testing for unitary matrix\n    assert refine(U.I, Q.unitary(U)) == U.conjugate()\n    \n    # Testing for singular matrix\n    raises(ValueError, lambda: refine(S.I, Q.singular(S)))\n"], "sample_1117": ["def test_invertible_orthogonal():\n    assert ask(Q.invertible(X), Q.orthogonal(X)) is True\n    assert ask(Q.invertible(Z), Q.orthogonal(Z)) is None\n    assert ask(Q.invertible(Identity(3)), Q.orthogonal(Identity(3))) is True\n    assert ask(Q.invertible(ZeroMatrix(3, 3)), Q.orthogonal(ZeroMatrix(3, 3))) is False\n    assert ask(Q.invertible(OneMatrix(1, 1)), Q.orthogonal(OneMatrix(1, 1))) is True\n    assert ask(Q.invertible(OneMatrix(3, 3)), Q.orthogonal(OneMatrix(3, 3))) is False\n", "def test_MatMul_elements():\n    assert ask(Q.integer_elements(X*Z), Q.integer_elements(X) & Q.integer_elements(Z)) is None\n    assert ask(Q.integer_elements(X*Z), Q.integer_elements(X) & Q.integer_elements(Z) & Q.invertible(X)) is None\n    assert ask(Q.real_elements(X*Z), Q.real_elements(X) & Q.real_elements(Z))\n    assert ask(Q.complex_elements(X*Z), Q.complex_elements(X) & Q.complex_elements(Z))\n    assert ask(Q.integer_elements(2*X), Q.integer_elements(X))\n    assert ask(Q.real_elements(2*X), Q.real_elements(X))\n    assert ask(Q.complex_elements(2*X), Q.complex_elements(X))\n", "def test_factorization():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.unitary(X), Q.unitary(Y)) is None\n    assert ask(Q.orthogonal(X), Q.orthogonal(Y)) is None\n    assert ask(Q.upper_triangular(X), Q.upper_triangular(Y)) is None\n    assert ask(Q.lower_triangular(X), Q.lower_triangular(Y)) is None\n    assert ask(Q.diagonal(X), Q.diagonal(Y)) is None\n    assert ask(Q.unitary(X), Q.unitary(X) & Q.unitary(Y))\n    assert ask(Q.orthogonal(X), Q.orthogonal(X) & Q.orthogonal(Y))\n    assert ask(Q.upper_triangular(X), Q.upper_triangular(X) & Q.upper_triangular(Y))\n    assert ask(Q.lower_triangular(X), Q.lower_triangular(X) & Q.lower_triangular(Y))\n    assert ask(Q.diagonal(X), Q.diagonal(X) & Q.diagonal(Y))\n", "def test_ask_positive_definite_invertible():\n    assert ask(Q.positive_definite(X), Q.invertible(X)) is None\n    assert ask(Q.positive_definite(X), Q.positive_definite(X) & Q.invertible(X)) is True\n    assert ask(Q.positive_definite(X), Q.positive_definite(X) & ~Q.invertible(X)) is False\n    assert ask(Q.positive_definite(X*Z*X), Q.positive_definite(X) & Q.positive_definite(Z) & Q.invertible(X)) is True\n    assert ask(Q.positive_definite(X*Z), Q.positive_definite(X) & ~Q.invertible(Z)) is False\n", "def test_determinant():\n    X = MatrixSymbol('X', 3, 3)\n    Y = MatrixSymbol('Y', 3, 3)\n    assert ask(Q.invertible(Determinant(X)), Q.invertible(X))\n    assert ask(Q.positive(Determinant(X)), Q.positive_definite(X))\n    assert ask(Q.real_elements(Determinant(X)), Q.real_elements(X))\n    assert ask(Q.integer_elements(Determinant(X)), Q.integer_elements(X))\n    assert ask(Q.complex_elements(Determinant(X)), Q.complex_elements(X))\n    assert ask(Q.invertible(Determinant(X * Y)), Q.invertible(X) & Q.invertible(Y))\n"], "sample_1118": ["def test_matpow():\n    assert MatPow(C, 2).doit() == C**2\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, -1).doit() == Inverse(C).doit()\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    assert MatPow(Identity(n), 3).doit() == Identity(n)\n    \n    raises(TypeError, lambda: MatPow(2, 2))\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n\n    P = PermutationMatrix(n, (0, 1, 2))\n    assert MatPow(P, 2).doit() == P**2\n    assert MatPow(P, 0).doit() == Identity(n)\n    assert MatPow(P, -1).doit() == P**-1\n\n    assert MatPow(C, S.One).doit() == C\n    assert MatPow(C, S.NegativeOne).doit() == Inverse(C).doit()\n    assert MatPow(C, S.Zero).doit() == Identity(n)\n", "def test_matpow():\n    assert MatPow(C, 2).doit() == C * C\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(ZeroMatrix(n, n), -1).doit() == ZeroMatrix(n, n).I\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n    raises(NonSquareMatrixError, lambda: MatPow(A*E, 2).doit())\n    assert MatPow(Identity(n), 2).doit() == Identity(n)\n    assert MatPow(Identity(n), -1).doit() == Identity(n)\n    assert MatPow(OneMatrix(1, 1), 2).doit() == OneMatrix(1, 1)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n", "def test_matpow():\n    # Testing MatPow with evaluate=True\n    assert MatPow(C, 2, evaluate=True).doit() == C**2\n    assert MatPow(C, -1, evaluate=True).doit() == Inverse(C)\n    assert MatPow(C, 0, evaluate=True).doit() == Identity(C.rows)\n    assert MatPow(ZeroMatrix(n, n), -1, evaluate=True) == NonInvertibleMatrixError\n\n    # Testing MatPow with evaluate=False\n    assert isinstance(MatPow(C, 2, evaluate=False), MatPow)\n    assert MatPow(C, 2, evaluate=False).base == C\n    assert MatPow(C, 2, evaluate=False).exp == 2\n\n    # Testing base and exp properties\n    assert MatPow(C, 3).base == C\n    assert MatPow(C, 3).exp == 3\n\n    # Testing shape property\n    assert MatPow(C, 3).shape == C.shape\n\n    # Testing errors\n    raises(TypeError, lambda: MatPow(2, 3))\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n    \n    # Testing _eval_transpose\n    assert MatPow(C, 2).T == MatPow(C.T, 2)\n    \n    # Testing other cases\n    assert MatPow(C, S.One).doit() == C\n    assert MatPow(C, S.Zero).doit() == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 3).doit() == ZeroMatrix(n, n)\n    assert MatPow(Identity(n), 3).doit() == Identity(n)\n", "def test_matpow():\n    M = MatrixSymbol('M', n, n)\n    \n    # Test power of Identity matrix\n    assert MatPow(Identity(n), 2).doit() == Identity(n)\n    assert MatPow(Identity(n), 0).doit() == Identity(n)\n    \n    # Test power of Zero matrix\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n    \n    # Test power of general matrix\n    assert MatPow(M, 1).doit() == M\n    assert MatPow(M, 0).doit() == Identity(n)\n    \n    # Test exponent properties\n    assert MatPow(MatPow(M, 2), 3).doit() == MatPow(M, 6).doit()\n    \n    # Test non-square matrix\n    N = MatrixSymbol('N', n, m)\n    raises(NonSquareMatrixError, lambda: MatPow(N, 2).doit())\n    \n    # Test transpose property\n    assert MatPow(M, 2).T == MatPow(M.T, 2)\n    \n    # Test exponent as symbolic value\n    k = symbols('k')\n    assert MatPow(M, k).exp == k\n    assert MatPow(M, k).base == M\n", "def test_matpow():\n    assert MatPow(C, 2).doit() == C * C\n    assert MatPow(C, 0).doit() == Identity(C.rows)\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    assert MatPow(ZeroMatrix(n, n), 0).doit() == Identity(n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n    raises(TypeError, lambda: MatPow(S.One, 2))\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n    assert MatPow(Identity(n), 2).doit() == Identity(n)\n"], "sample_1119": ["def test_deferred_vector():\n    from sympy import lambdify\n    X = DeferredVector('X')\n    assert str(X) == 'X'\n    assert repr(X) == \"DeferredVector('X')\"\n    assert X[0] == Symbol('X[0]')\n    assert X[1] == Symbol('X[1]')\n    assert X[10] == Symbol('X[10]')\n    raises(IndexError, lambda: X[-1])\n    expr = (X[0] + 2, X[2] + 3)\n    func = lambdify(X, expr)\n    assert func([1, 2, 3]) == (3, 6)\n", "def test_deferred_vector():\n    from sympy import lambdify\n    X = DeferredVector('X')\n    assert str(X) == 'X'\n    assert repr(X) == \"DeferredVector('X')\"\n    assert X[0] == Symbol('X[0]')\n    assert X[1] == Symbol('X[1]')\n    assert X[10] == Symbol('X[10]')\n    raises(IndexError, lambda: X[-1])\n    expr = (X[0] + 2, X[2] + 3)\n    func = lambdify(X, expr)\n    assert func([1, 2, 3]) == (3, 6)\n", "def test_matrix_determinant():\n    from sympy.matrices.dense import Matrix\n    M1 = Matrix([[1, 2], [3, 4]])\n    M2 = Matrix([[1, 2, 3], [0, 1, 4], [5, 6, 0]])\n    assert M1.det() == -2\n    assert M2.det() == 1\n    assert M1._eval_det_bareiss() == -2\n    assert M2._eval_det_bareiss() == 1\n    assert M1._eval_det_berkowitz() == -2\n    assert M2._eval_det_berkowitz() == 1\n    assert M1._eval_det_lu() == -2\n    assert M2._eval_det_lu() == 1\n    assert M1._eval_determinant() == -2\n    assert M2._eval_determinant() == 1\n", "def test_matrix_operations():\n    from sympy import Matrix\n    M = Matrix([[1, 2], [3, 4]])\n\n    # Test echelon_form\n    assert M.echelon_form() == Matrix([[1, 2], [0, -2]])\n\n    # Test rank\n    assert M.rank() == 2\n\n    # Test rref\n    rref_matrix, pivots = M.rref()\n    assert rref_matrix == Matrix([[1, 0], [0, 1]])\n    assert pivots == (0, 1)\n\n    # Test adjugate\n    assert M.adjugate() == Matrix([[4, -2], [-3, 1]])\n\n    # Test charpoly\n    assert M.charpoly().as_expr() == Matrix([['x', -2], [-3, 'x-5']]).det()\n\n    # Test cofactor\n    assert M.cofactor(0, 0) == 4\n    assert M.cofactor(0, 1) == -3\n\n    # Test cofactor_matrix\n    assert M.cofactor_matrix() == Matrix([[4, -3], [-2, 1]])\n\n    # Test det\n    assert M.det() == -2\n\n    # Test minor\n    assert M.minor(0, 0) == 4\n    assert M.minor(0, 1) == 3\n\n    # Test minor_submatrix\n    assert M.minor_submatrix(0, 0) == Matrix([[4]])\n", "def test_matrix_determinant():\n    from sympy import Matrix\n    M = Matrix([\n        [1, 2],\n        [3, 4]\n    ])\n    assert M.det() == -2\n    assert M.adjugate() == Matrix([\n        [4, -2],\n        [-3, 1]\n    ])\n    assert M.charpoly().as_expr() == M.det() * Matrix([\n        [1, -4],\n        [-3, 1]\n    ]).det()\n    assert M.cofactor(0, 1) == -3\n    assert M.cofactor_matrix() == Matrix([\n        [4, -3],\n        [-2, 1]\n    ])\n    assert M.minor(1, 0) == 2\n    assert M.minor_submatrix(0, 1) == Matrix([\n        [3]\n    ])\n\n    # Check various determinant methods\n    assert M._eval_det_bareiss() == -2\n    assert M._eval_det_berkowitz() == -2\n    assert M._eval_det_lu() == -2\n"], "sample_1120": ["def test_matrix_expr_creation():\n    assert isinstance(MatrixExpr(), MatrixExpr)\n    assert MatrixExpr().is_MatrixExpr\n    assert not MatrixExpr().is_Matrix\n    assert MatrixExpr()._op_priority == 11.0\n    assert MatrixExpr().is_commutative == False\n    assert MatrixExpr().is_number == False\n    assert MatrixExpr().is_symbol == False\n    assert MatrixExpr().is_scalar == False\n", "def test_matrix_expr_as_real_imag():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = A + I*B\n    \n    real, imag = expr.as_real_imag()\n    \n    assert real == A\n    assert imag == B\n\n    C = MatrixSymbol('C', 3, 3)\n    D = MatrixSymbol('D', 3, 3)\n    expr2 = C - I*D\n    \n    real2, imag2 = expr2.as_real_imag()\n    \n    assert real2 == C\n    assert imag2 == -D\n", "def test_matrix_expr_operations():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n\n    # Test negation\n    assert -A == MatMul(S.NegativeOne, A).doit()\n\n    # Test addition\n    assert A + B == MatAdd(A, B, check=True).doit()\n    assert B + A == MatAdd(B, A, check=True).doit()\n\n    # Test subtraction\n    assert A - B == MatAdd(A, -B, check=True).doit()\n    assert B - A == MatAdd(B, -A, check=True).doit()\n\n    # Test multiplication\n    assert A * B == MatMul(A, B).doit()\n    assert B * A == MatMul(B, A).doit()\n\n    # Test matrix power\n    assert A**2 == MatPow(A, 2).doit()\n    raises(NonSquareMatrixError, lambda: MatrixSymbol('E', 2, 3)**2)\n\n    # Test division\n    assert A / 2 == MatMul(A, S.Half).doit()\n    raises(NotImplementedError, lambda: 2 / A)\n\n    # Test matrix element indexing\n    assert A[0, 0] == MatrixElement(A, 0, 0)\n    assert A[1, 1] == MatrixElement(A, 1, 1)\n    raises(IndexError, lambda: A[2, 2])\n\n    # Test complex expressions\n    expr = A + B*C - D\n    assert expr == MatAdd(A, MatMul(B, C).doit(), -D, check=True).doit()\n\n    expr = A * (B + C) - D\n    assert expr == MatAdd(MatMul(A, MatAdd(B, C, check=True).doit()).doit(), -D, check=True).doit()\n\n    expr = A**2 - 2*A + Identity(2)\n    assert expr == MatAdd(MatPow(A, 2).doit(), MatMul(S.NegativeOne, S(2), A).doit(), Identity(2), check=True).doit()\n\n    # Test equals method for different types\n    assert A.equals(A)", "def test_matrix_element_invalid_index():\n    A = MatrixSymbol('A', 3, 3)\n    raises(IndexError, lambda: A[3, 1])\n    raises(IndexError, lambda: A[1, 3])\n    raises(IndexError, lambda: A[3, 3])\n    raises(IndexError, lambda: A[-1, 1])\n    raises(IndexError, lambda: A[1, -1])\n    raises(IndexError, lambda: A[-1, -1])\n", "def test_matrixexpr_canonicalize():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n    expr = (A + B).canonicalize()\n    assert expr == A + B\n    expr = (A * B).canonicalize()\n    assert expr == A * B\n    expr = (A * B + C).canonicalize()\n    assert expr == A * B + C\n    expr = (A - B).canonicalize()\n    assert expr == A - B\n    expr = (A - B + C).canonicalize()\n    assert expr == A - B + C\n"], "sample_1121": ["def test_unevaluated_Mul():\n    from sympy import S, Mul, sqrt\n    from sympy.abc import x\n\n    # Check if the multiplication is not evaluated\n    a = _unevaluated_Mul(*[S(3.0), x, S(2)])\n    assert a.args[0] == 6.0\n    assert a.args[1] == x\n\n    # Check commutative property with unevaluated Mul\n    m = _unevaluated_Mul(sqrt(2), sqrt(3))\n    assert m == _unevaluated_Mul(sqrt(3), sqrt(2))\n    \n    # Check if the multiplication remains unevaluated\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m == _unevaluated_Mul(u)\n    assert m != Mul(*m.args)\n", "def test_mul_is_zero_with_symbols():\n    n = Symbol('n', integer=True)\n    z = Symbol('z', zero=True)\n    assert (n * z).is_zero\n    assert (z * n).is_zero\n    assert (z * 3).is_zero\n    assert (3 * z).is_zero\n    assert (z * z).is_zero\n    assert (z * x).is_zero\n    assert (x * z).is_zero\n    assert (z * (x + y)).is_zero\n    assert ((x + y) * z).is_zero\n\n    zf = Symbol('zf', zero=True, finite=True)\n    assert (n * zf).is_zero\n    assert (zf * n).is_zero\n    assert (zf * 3).is_zero\n    assert (3 * zf).is_zero\n    assert (zf * zf).is_zero\n    assert (zf * x).is_zero\n    assert (x * zf).is_zero\n    assert (zf * (x + y)).is_zero\n    assert ((x + y) * zf).is_zero\n\n    znz = Symbol('znz', zero=True, nonzero=False)\n    assert (n * znz).is_zero\n    assert (znz * n).is_zero\n    assert (znz * 3).is_zero\n    assert (3 * znz).is_zero\n    assert (znz * znz).is_zero\n    assert (znz * x).is_zero\n    assert (x * znz).is_zero\n    assert (znz * (x + y)).is_zero\n    assert ((x + y) * znz).is_zero\n", "def test_unevaluated_Mul():\n    from sympy.abc import x, y\n    from sympy import S, Mul, sqrt\n\n    # Test with simple numbers\n    a = _unevaluated_Mul(*[S(3.0), x, S(2)])\n    assert a.args[0] == 6.0\n    assert a.args[1] == x\n\n    # Test with symbolic expressions\n    b = _unevaluated_Mul(*[sqrt(2), sqrt(3)])\n    assert b == _unevaluated_Mul(sqrt(3), sqrt(2))\n    assert b == _unevaluated_Mul(Mul(sqrt(3), sqrt(2), evaluate=False))\n\n    # Test with complex expression\n    c = _unevaluated_Mul(*[S(2), x + 1, y])\n    assert c == Mul(2, x + 1, y, evaluate=False)\n\n    # Ensure proper sorting and combining\n    d = _unevaluated_Mul(*[S(4), x, S(3.0), sqrt(2), sqrt(2)])\n    assert d.args[0] == 12.0\n    assert d.args[1] == 2\n    assert d.args[2] == x\n\n    # Ensure it handles non-commutative parts correctly\n    e = _unevaluated_Mul(*[S(2), x, y, sqrt(2), sqrt(3)])\n    assert e.args[0] == 2\n    assert e.args[-1] == Mul(x, y, evaluate=False)\n", "def test_unevaluated_Mul():\n    from sympy.abc import a, b, c, d\n    from sympy import sqrt, Rational\n    # Test that _unevaluated_Mul behaves correctly\n    assert _unevaluated_Mul(a, b, c) == Mul(a, b, c, evaluate=False)\n    assert _unevaluated_Mul(2, a, b) == Mul(2, a, b, evaluate=False)\n    assert _unevaluated_Mul(sqrt(2), sqrt(3)) == Mul(sqrt(2), sqrt(3), evaluate=False)\n    assert _unevaluated_Mul(Rational(1, 2), 2) == S.One\n    assert _unevaluated_Mul(2, a, _unevaluated_Mul(b, c)) == Mul(2, a, b, c, evaluate=False)\n    assert _unevaluated_Mul(a, b, _unevaluated_Mul(c, d)) == Mul(a, b, c, d, evaluate=False)\n\n    m = _unevaluated_Mul(sqrt(2), sqrt(3))\n    assert m == _unevaluated_Mul(sqrt(3), sqrt(2))\n    assert m != Mul(sqrt(2), sqrt(3))\n    assert m == _unevaluated_Mul(Mul(sqrt(3), sqrt(2), evaluate=False))\n", "def test__unevaluated_Mul():\n    from sympy import Mul, S\n    # Test basic unevaluated multiplication\n    a = S(2)\n    b = S(3)\n    c = S(4)\n    result = Mul._from_args([a, b, c], evaluate=False)\n    assert result.args == (a, b, c)\n    assert result == 24\n\n    # Test with symbolic expressions\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    result = Mul._from_args([x, y, z], evaluate=False)\n    assert result.args == (x, y, z)\n    assert result == x*y*z\n\n    # Test with mix of numbers and symbols\n    result = Mul._from_args([a, x, b, y], evaluate=False)\n    assert result.args == (a, x, b, y)\n    assert result == 6*x*y\n\n    # Test with nested unevaluated Mul\n    nested_mul = Mul._from_args([a, x], evaluate=False)\n    result = Mul._from_args([nested_mul, b, y], evaluate=False)\n    assert result.args == (nested_mul, b, y)\n    assert result == 6*x*y\n\n    # Test commutativity\n    result = Mul._from_args([x, y, b, a], evaluate=False)\n    assert result.args == (x, y, b, a)\n    assert result == 6*x*y\n"], "sample_1122": ["def test_conjugate_properties():\n    a = Symbol('a', real=True)\n    b = Symbol('b', imaginary=True)\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test conjugate on various types\n    assert conjugate(a).is_real is True\n    assert conjugate(a).is_imaginary is False\n    assert conjugate(b).is_real is False\n    assert conjugate(b).is_imaginary is True\n    assert conjugate(x).is_real is None\n    assert conjugate(x).is_imaginary is None\n\n    # Test conjugate on expressions\n    assert conjugate(a + b) == a - b\n    assert conjugate(a * b) == a * (-b)\n    assert conjugate(a / b) == a / (-b)\n    assert conjugate(x + I*y) == conjugate(x) - I*y\n\n    # Test conjugate on complex expressions\n    expr = (1 + I)*(2 - I)\n    assert conjugate(expr) == (1 - I)*(2 + I)\n\n    # Test conjugate on more complex expressions\n    expr = (1 + I + x)*(2 - I - y)\n    assert conjugate(expr) == (1 - I + conjugate(x))*(2 + I - conjugate(y))\n\n    # Test conjugate on trigonometric functions\n    assert conjugate(sin(x + I*y)) == sin(conjugate(x) - I*conjugate(y))\n    assert conjugate(cos(x + I*y)) == cos(conjugate(x) - I*conjugate(y))\n\n    # Test conjugate on exponential and logarithmic functions\n    assert conjugate(exp(x + I*y)) == exp(conjugate(x) - I*conjugate(y))\n    assert conjugate(log(x + I*y)) == log(conjugate(x) - I*conjugate(y))\n\n    # Ensure conjugate on complex conjugate reverts to original\n    assert conjugate(conjugate(a + b)) == a + b\n    assert conjugate(conjugate(x + I*y)) == x + I*y\n\n    # Test symbolic complex numbers\n    assert conjugate(re(x) + I*im(x)) == re(x) - I*im(x)\n", "def test_sqrt():\n    x, y = symbols('x y')\n    r = Symbol('r', real=True)\n    i = Symbol('i', imaginary=True)\n    a, b = symbols('a b', real=True)\n\n    # Real and imaginary parts of sqrt\n    assert re(sqrt(a + b*I)) == (a**2 + b**2)**Rational(1, 4)*cos(atan2(b, a)/2)\n    assert im(sqrt(a + b*I)) == (a**2 + b**2)**Rational(1, 4)*sin(atan2(b, a)/2)\n\n    # Derivatives involving sqrt\n    assert re(sqrt(r)).diff(r) == re(Derivative(sqrt(r), r, evaluate=True))\n    assert im(sqrt(r)).diff(r) == im(Derivative(sqrt(r), r, evaluate=True))\n    assert re(sqrt(i)).diff(i) == re(Derivative(sqrt(i), i, evaluate=True))\n    assert im(sqrt(i)).diff(i) == im(Derivative(sqrt(i), i, evaluate=True))\n\n    # Sqrt of expressions involving real and imaginary parts\n    assert re(sqrt(x + y*I)) == (re(x + y*I)**2 + im(x + y*I)**2)**Rational(1, 4)*cos(atan2(im(x + y*I), re(x + y*I))/2)\n    assert im(sqrt(x + y*I)) == (re(x + y*I)**2 + im(x + y*I)**2)**Rational(1, 4)*sin(atan2(im(x + y*I), re(x + y*I))/2)\n    \n    # Rewriting in terms of re and im\n    assert sqrt(x).rewrite(re, im) == sqrt(x)\n    assert sqrt(x + y*I).rewrite(re, im) == sqrt(re(x + y*I) + I*im(x + y*I))\n", "def test_unpolarify_with_expressions():\n    from sympy import exp_polar, polar_lift, tan, cosh\n    x = Symbol('x')\n    p = exp_polar(I*pi/3) + polar_lift(1 + I)\n\n    # Ensure that expressions with nested polar functions unpolarify correctly\n    assert unpolarify(tan(p)) == tan(exp(I*pi/3) + (1 + I))\n    assert unpolarify(cosh(p)) == cosh(exp(I*pi/3) + (1 + I))\n\n    # Test unpolarify on more complex nested expressions\n    expr = 2*cosh(polar_lift(x)) - polar_lift(I*pi)\n    assert unpolarify(expr) == 2*cosh(x) - I*pi\n\n    expr = tan(exp_polar(2*I*pi) + polar_lift(3*x))\n    assert unpolarify(expr) == tan(exp(2*I*pi) + 3*x)\n\n    # Ensure that periodic_argument is correctly handled within unpolarify\n    assert unpolarify(periodic_argument(p, 2*pi)) == periodic_argument(exp(I*pi/3) + (1 + I), 2*pi)\n", "def test_issue_12345():\n    # Test specific cases for `sign` function with complex inputs\n    from sympy import sign, I, sqrt, Rational\n\n    assert sign(2 + 2*I).doit() == sqrt(2)*(2 + 2*I)/4\n    assert sign(2 + 3*I).simplify() == sign(2 + 3*I)\n    assert sign(-3 + 4*I).doit() == sqrt(25)*(-3 + 4*I)/25\n\n    # Test edge cases of `Abs` function with complex powers\n    assert Abs((4 + 5*I)**(6 + 7*I)).rewrite(exp) == (4 + 5*I)**(6 + 7*I)\n    assert Abs((4 + 5*I)**(Rational(1, 2) + 3*I)).rewrite(exp) == (4 + 5*I)**(Rational(1, 2) + 3*I)\n\n    # Test specific cases for `arg` function with polar numbers and mixed inputs\n    assert arg(exp_polar(4*pi*I) * (1 + I)) == 4*pi + arg(1 + I)\n    assert arg(exp_polar(5 - 3*pi*I/4) * (1 - I)) == pi*Rational(-3, 4) + arg(1 - I)\n", "def test_sign_special_cases():\n    x, y = symbols('x y')\n    \n    # Test sign function with various complex numbers\n    assert sign(1 + I) == sqrt(2)*(1 + I)/2\n    assert sign(-1 + I) == sqrt(2)*(-1 + I)/2\n    assert sign(1 - I) == sqrt(2)*(1 - I)/2\n    assert sign(-1 - I) == sqrt(2)*(-1 - I)/2\n    \n    # Test sign function with symbolic complex numbers\n    expr = x + I*y\n    result = expr / Abs(expr)\n    assert sign(expr) == result\n\n    # Test sign function with zero\n    assert sign(0) == 0\n\n    # Test sign function with nan\n    assert sign(nan) is nan\n\n    # Test sign function with oo and -oo\n    assert sign(oo) == 1\n    assert sign(-oo) == -1\n\n    # Test sign function with complex infinity\n    assert sign(zoo) is nan\n\n    # Test sign function with expressions involving re and im\n    expr = re(x) + im(x)*I\n    assert sign(expr) == expr / Abs(expr)\n\n    # Test sign function with purely imaginary numbers\n    i = Symbol('i', imaginary=True)\n    assert sign(i*I) == I*sign(i)\n    assert sign(-i*I) == -I*sign(i)\n\n    # Test sign function with algebraic and transcendental numbers\n    a = Symbol('a', algebraic=True)\n    t = Symbol('t', transcendental=True)\n    assert sign(a).is_algebraic\n    assert sign(t).is_transcendental\n    assert sign(x).is_algebraic is None\n    assert sign(x).is_transcendental is None\n\n    # Test derivatives involving sign function\n    assert sign(x).diff(x) == 2*DiracDelta(x)\n    assert sign(i).diff(i) == 2*DiracDelta(i)\n"], "sample_1123": ["def test_as_relational():\n    cs = ConditionSet(x, x**2 > 4, Interval(-10, 10))\n    expr = cs.as_relational(x)\n    assert expr == And((x**2 > 4), Interval(-10, 10).contains(x))\n\n    cs2 = ConditionSet(x, Eq(y, 0), Interval(-10, 10))\n    expr2 = cs2.as_relational(x)\n    assert expr2 == And(Eq(y, 0), Interval(-10, 10).contains(x))\n\n    cs3 = ConditionSet(x, x < 1, S.Integers)\n    expr3 = cs3.as_relational(x)\n    assert expr3 == And(x < 1, Contains(x, S.Integers))\n", "def test_condition_evaluation():\n    # Test with different conditions and base sets\n    assert 2 in ConditionSet(x, x**2 < 5, S.Reals)\n    assert -2 in ConditionSet(x, x**2 < 5, S.Reals)\n    assert 3 not in ConditionSet(x, x**2 < 5, S.Reals)\n    \n    # Test with more complex conditions\n    assert 3 in ConditionSet(x, And(x > 1, x < 5), S.Reals)\n    assert 0 not in ConditionSet(x, And(x > 1, x < 5), S.Reals)\n\n    # Test with universal set\n    universal_set = ConditionSet(x, x < 10)\n    assert 5 in universal_set\n    assert 15 not in universal_set\n\n    # Test with EmptySet as base set\n    assert 1 not in ConditionSet(x, x > 0, S.EmptySet)\n    assert S.EmptySet == ConditionSet(x, x > 0, S.EmptySet)\n\n    # Test when condition is always true\n    assert 5 in ConditionSet(x, S.true, Interval(1, 10))\n    assert 11 not in ConditionSet(x, S.true, Interval(1, 10))\n\n    # Test when condition is always false\n    assert 5 not in ConditionSet(x, S.false, Interval(1, 10))\n    assert ConditionSet(x, S.false, Interval(1, 10)) == S.EmptySet\n", "def test_as_relational():\n    cset = ConditionSet(x, x**2 > 4, Interval(-10, 10))\n    rel = And(Contains(x, Interval(-10, 10)), x**2 > 4)\n    assert cset.as_relational(x) == rel\n\n    cset = ConditionSet(x, Eq(x, 0), Interval(-10, 10))\n    rel = And(Contains(x, Interval(-10, 10)), Eq(x, 0))\n    assert cset.as_relational(x) == rel\n\n    cset = ConditionSet(y, y < x, Interval(0, 5))\n    rel = And(Contains(y, Interval(0, 5)), y < x)\n    assert cset.as_relational(y) == rel\n", "def test_as_relational():\n    condset = ConditionSet(x, x**2 < 4, Interval(-3, 3))\n    assert condset.as_relational(x) == And(x**2 < 4, Contains(x, Interval(-3, 3)))\n\n    condset = ConditionSet(x, x > 2, S.Reals)\n    assert condset.as_relational(x) == And(x > 2, S.Reals.contains(x))\n\n    condset = ConditionSet(x, x > y, Interval(0, 10))\n    assert condset.as_relational(x) == And(x > y, Contains(x, Interval(0, 10)))\n\n    condset = ConditionSet(y, y < x, Interval(1, 5))\n    assert condset.as_relational(y) == And(y < x, Contains(y, Interval(1, 5)))\n\n    # Test with complex conditions\n    condset = ConditionSet(x, And(x > 2, x < 5), S.Reals)\n    assert condset.as_relational(x) == And(x > 2, x < 5, S.Reals.contains(x))\n", "def test_as_relational():\n    # Test for as_relational method\n    cond_set = ConditionSet(x, x > 0, Interval(-2, 2))\n    rel = cond_set.as_relational(x)\n    assert rel == And(x > 0, Contains(x, Interval(-2, 2)))\n\n    cond_set = ConditionSet(y, y**2 > 4, S.Reals)\n    rel = cond_set.as_relational(y)\n    assert rel == And(y**2 > 4, Contains(y, S.Reals))\n\n    cond_set = ConditionSet(x, x < 1, FiniteSet(1, 2, 3))\n    rel = cond_set.as_relational(x)\n    assert rel == And(x < 1, Contains(x, FiniteSet(1, 2, 3)))\n"], "sample_1124": ["def test_FracElement_set_field():\n    F1, x, y = field(\"x,y\", ZZ)\n    F2, X, Y = field(\"x,y\", QQ)\n    \n    f = (x**2 + 3*y)/(2*y)\n    assert f.set_field(F2) == (X**2 + 3*Y)/(2*Y)\n    \n    g = (x**2 + 3*y)/(2*y)\n    assert g.set_field(F1) == (x**2 + 3*y)/(2*y)\n    \n    f = (x + 1)/(y + 2)\n    assert f.set_field(F2) == (X + 1)/(Y + 2)\n    \n    raises(NotImplementedError, lambda: f.set_field(field(\"x,z\", QQ)[0]))\n", "def test_FracField_new():\n    F, x, y = field(\"x,y\", ZZ)\n    \n    # Test with both numerator and denominator\n    f = F.new(F.ring.gens[0]*2, F.ring.gens[1]*3)\n    assert f == (2*x)/(3*y)\n\n    # Test with only numerator (denominator should be 1)\n    f = F.new(F.ring.gens[0]*2)\n    assert f == 2*x\n\n    # Test with another FracElement\n    g = F.new(f)\n    assert g == f\n\n    # Test with a ground element\n    h = F.new(5)\n    assert h == 5\n\n    # Test with a tuple (numer, denom)\n    i = F.new((F.ring.gens[0]*3, F.ring.gens[1]*4))\n    assert i == (3*x)/(4*y)\n\n    # Test with an invalid type (should raise CoercionFailed)\n    raises(CoercionFailed, lambda: F.new(sqrt(2)))\n", "def test_FracField_raw_new():\n    F, x, y, z = field(\"x,y,z\", ZZ)\n    numer = F.ring.gens[0] * F.ring.gens[1]\n    denom = F.ring.gens[2]\n    element = F.raw_new(numer, denom)\n    \n    assert isinstance(element, FracElement)\n    assert element.numer == numer\n    assert element.denom == denom\n", "def test_FracElement_set_field():\n    F1, x, y = field(\"x,y\", ZZ)\n    F2, u, v = field(\"u,v\", QQ)\n    \n    f = (x**2 + 3*y)/x\n    g = f.set_field(F2)\n    \n    assert g.field == F2\n    assert g.numer == F2.ring.ground_new(f.numer)\n    assert g.denom == F2.ring.ground_new(f.denom)\n", "def test_FracElement_set_field():\n    F1, x1, y1 = field(\"x1,y1\", ZZ)\n    F2, x2, y2 = field(\"x2,y2\", ZZ)\n\n    f1 = (x1 + y1)/x1\n    f2 = f1.set_field(F2)\n\n    assert f2.numer == F2.ring.gens[0] + F2.ring.gens[1]\n    assert f2.denom == F2.ring.gens[0]\n    assert f2.field == F2\n"], "sample_1125": ["def test_operator_properties():\n    A = Operator('A')\n    B = Operator('B')\n    \n    assert A.label == ('A',)\n    assert B.label == ('B',)\n    \n    assert A.is_commutative is False\n    assert B.is_commutative is False\n    \n    assert A * B != B * A\n    assert A * A.inv() == 1\n    assert A.inv() * A == 1\n", "def test_operator_inverse():\n    A = Operator('A')\n    assert A.inv() == A**(-1)\n    assert (A * A.inv()) == IdentityOperator()\n    assert (A.inv() * A) == IdentityOperator()\n", "def test_operator_mul():\n    A = Operator('A')\n    B = Operator('B')\n    I = IdentityOperator()\n\n    # Test multiplication with IdentityOperator\n    assert A * I == A\n    assert I * A == A\n\n    # Test commutativity property of Operators\n    assert (A * B).is_commutative is False\n    assert (B * A).is_commutative is False\n    assert A * B != B * A\n\n    # Test operator inverses\n    assert A * A.inv() == IdentityOperator()\n    assert A.inv() * A == IdentityOperator()\n    assert A * A.inv() * B == B\n    assert B * A.inv() * A == B\n\n    # Polynomial of operators\n    expr = (A + B)**2\n    expanded_expr = expr.expand()\n    expected_expr = A*B + B*A + A**2 + B**2\n    assert expanded_expr == expected_expr\n", "def test_operator_mul():\n    A = Operator('A')\n    B = Operator('B')\n    I = IdentityOperator()\n    \n    assert A * B != B * A\n    assert A * I == A\n    assert I * B == B\n    assert (A * B).is_commutative is False\n", "def test_operator_mul():\n    A = Operator('A')\n    B = Operator('B')\n    C = 2*A*A + I*B\n    assert C == 2*A**2 + I*B\n\n    # Test commutative property\n    assert A.is_commutative is False\n    assert B.is_commutative is False\n    assert A*B != B*A\n\n    # Test operator inverse\n    assert A.inv() == A**(-1)\n    assert A*A.inv() == 1\n\n    # Test operator addition and multiplication\n    D = A + B\n    assert D.expand() == A + B\n    E = A*B + B*A\n    assert E.expand() == A*B + B*A\n"], "sample_1126": ["def test_dagger_operator_addition():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n", "def test_dagger_expressions():\n    x, y = symbols('x y', commutative=False)\n    expr1 = x + y\n    expr2 = x * y\n\n    assert Dagger(expr1) == Dagger(x) + Dagger(y)\n    assert Dagger(expr2) == Dagger(y) * Dagger(x)\n    \n    expr3 = x**2 + y\n    assert Dagger(expr3) == Dagger(x)**2 + Dagger(y)\n    \n    expr4 = (x + y)**2\n    assert Dagger(expr4) == (Dagger(x) + Dagger(y))**2\n\n    expr5 = x * y + y * x\n    assert Dagger(expr5) == Dagger(y) * Dagger(x) + Dagger(x) * Dagger(y)\n", "def test_dagger_commutativity():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n    assert Dagger(A * B) == Dagger(B) * Dagger(A)\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_dagger_power():\n    O = Operator('O')\n    assert Dagger(O**2) == Dagger(O)**2\n    assert Dagger(O**3) == Dagger(O)**3\n", "def test_dagger_add():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n"], "sample_1127": ["def test_conjugacy_class_of_identity():\n    G = SymmetricGroup(5)\n    identity = Permutation(4)\n    conjugacy_class = G.conjugacy_class(identity)\n    expected_class = {identity}\n    assert conjugacy_class == expected_class\n", "def test_is_alternating():\n    a = Permutation(0, 1, 2)\n    b = Permutation(0, 1, size=3)\n    assert PermutationGroup(a, b).is_alternating == False\n\n    a = Permutation(0, 2, 1)\n    b = Permutation(1, 2, size=3)\n    assert PermutationGroup(a, b).is_alternating == True\n\n    a = Permutation(0, 1, 2, 3)\n    b = Permutation(0, 3)(1, 2)\n    assert PermutationGroup(a, b).is_alternating == False\n\n    a = Permutation(0, 1, 2, 3, 4)\n    b = Permutation(0, 4, 1, 3)(2)\n    assert PermutationGroup(a, b).is_alternating == True\n", "def test_perm_group():\n    # Test the creation of an empty PermutationGroup\n    empty_group = PermutationGroup()\n    assert empty_group.order() == 1\n    assert empty_group.degree == 1\n    assert empty_group.is_trivial\n\n    # Test the creation of a SymmetricGroup using PermutationGroup\n    sym_group = SymmetricGroup(4)\n    assert sym_group.order() == 24\n    assert sym_group.degree == 4\n    assert sym_group.is_symmetric\n\n    # Test the creation of a CyclicGroup using PermutationGroup\n    cyc_group = CyclicGroup(5)\n    assert cyc_group.order() == 5\n    assert cyc_group.degree == 5\n    assert cyc_group.is_cyclic\n\n    # Test the creation of an AlternatingGroup using PermutationGroup\n    alt_group = AlternatingGroup(4)\n    assert alt_group.order() == 12\n    assert alt_group.degree == 4\n    assert alt_group.is_alternating\n\n    # Test the creation of a DihedralGroup using PermutationGroup\n    dihedral_group = DihedralGroup(6)\n    assert dihedral_group.order() == 12\n    assert dihedral_group.degree == 6\n    assert not dihedral_group.is_symmetric\n\n    # Test if the trivial group is a subgroup of any group\n    assert empty_group.is_subgroup(sym_group)\n    assert empty_group.is_subgroup(cyc_group)\n    assert empty_group.is_subgroup(alt_group)\n    assert empty_group.is_subgroup(dihedral_group)\n\n    # Test if any group is a subgroup of itself\n    assert sym_group.is_subgroup(sym_group)\n    assert cyc_group.is_subgroup(cyc_group)\n    assert alt_group.is_subgroup(alt_group)\n    assert dihedral_group.is_subgroup(dihedral_group)\n\n    # Test the identity element of the permutation groups\n    assert sym_group.identity == Permutation(3)\n    assert cyc_group.identity == Permutation(4)\n    assert alt_group.identity == Permutation(3)\n    assert dihedral_group.identity == Permutation(5)\n\n    # Test the elements method\n    assert len(sym_group.elements) == 24\n    assert len(cyc_group.elements) == 5\n    assert len(alt_group.elements) == 12\n    assert len(dihedral_group.elements) == 12\n", "def test_permutation_group_properties():\n    # Create a permutation group\n    a = Permutation([1, 2, 0])\n    b = Permutation([0, 2, 1])\n    G = PermutationGroup([a, b])\n\n    # Test some properties of the group\n    assert G.is_group\n    assert G.degree == 3\n    assert G.order() == 3\n    assert G.is_transitive()\n    assert G.is_abelian is False\n    assert G.is_solvable\n    assert G.is_polycyclic\n\n    # Test the identity permutation\n    assert G.identity == Permutation([0, 1, 2])\n\n    # Test the base and strong generators\n    G.schreier_sims()\n    assert G.base == [0]\n    assert _verify_bsgs(G, G.base, G.strong_gens)\n\n    # Test conjugacy classes\n    assert len(G.conjugacy_classes()) == 2\n\n    # Test generating elements of the group\n    elements = list(G.generate())\n    assert len(elements) == G.order()\n    assert all(isinstance(elem, Permutation) for elem in elements)\n\n    # Test random element generation\n    random_elem = G.random()\n    assert isinstance(random_elem, Permutation)\n    assert random_elem in G\n\n    # Test if the group is perfect (should be False)\n    assert G.is_perfect is False\n\n    # Test abelian invariants\n    assert G.abelian_invariants() == [3]\n", "def test_subgroup_contains():\n    a = Permutation([0, 1, 2])\n    G = PermutationGroup([a])\n    assert G.contains(a)\n    b = Permutation([1, 0, 2])\n    assert not G.contains(b)\n    c = Permutation([2, 1, 0])\n    assert not G.contains(c)\n    assert G.contains(Permutation([0, 1, 2, 3, 4, 5]), strict=False)\n\n    S = SymmetricGroup(3)\n    assert S.contains(Permutation([1, 0, 2]))\n    assert S.contains(Permutation([0, 1, 2]))\n    assert not S.contains(Permutation([0, 2, 1, 3]))\n\n    T = SymmetricGroup(4)\n    assert T.contains(Permutation([0, 1, 2, 3]))\n    assert T.contains(Permutation([3, 2, 1, 0]))\n    assert not T.contains(Permutation([0, 1, 2, 4]), strict=True)\n    assert T.contains(Permutation([0, 1, 2, 4]), strict=False)\n"], "sample_1128": ["def test_point_set_pos():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = O.locatenew('P', 10 * N.x)\n    \n    # Check setting position with a valid vector\n    assert P.pos_from(O) == 10 * N.x\n    \n    # Check reverse position is also set\n    assert O.pos_from(P) == -10 * N.x\n    \n    Q = Point('Q')\n    raises(TypeError, lambda: P.set_pos(Q, \"invalid_vector\"))  # Invalid vector\n    \n    Q.set_pos(P, 5 * N.y)\n    assert Q.pos_from(P) == 5 * N.y\n    \n    # Check reverse position is also set\n    assert P.pos_from(Q) == -5 * N.y\n    \n    # Check setting position to zero\n    R = Point('R')\n    R.set_pos(Q, 0)\n    assert R.pos_from(Q) == Vector(0)\n    assert Q.pos_from(R) == Vector(0)\n", "def test_point_set_pos():\n    q, q2 = dynamicsymbols('q q2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    \n    # Basic set_pos and pos_from tests\n    P.set_pos(O, q * N.x)\n    assert P.pos_from(O) == q * N.x\n    assert O.pos_from(P) == -q * N.x\n    \n    # Updating position and verifying\n    P.set_pos(O, q2 * N.y)\n    assert P.pos_from(O) == q2 * N.y\n    assert O.pos_from(P) == -q2 * N.y\n\n    # Ensuring positions are updated correctly with multiple points\n    Q = Point('Q')\n    Q.set_pos(P, q * N.z)\n    assert Q.pos_from(O) == q2 * N.y + q * N.z\n    assert O.pos_from(Q) == -q2 * N.y - q * N.z\n\n    # Checking symmetry in positions\n    R = Point('R')\n    R.set_pos(Q, q2 * B.y)\n    assert R.pos_from(O) == q2 * N.y + q * N.z + q2 * B.y\n    assert O.pos_from(R) == -q2 * N.y - q * N.z - q2 * B.y\n\n    # Setting position to zero vector\n    S = Point('S')\n    S.set_pos(R, 0)\n    assert S.pos_from(R) == Vector(0)\n    assert R.pos_from(S) == Vector(0)\n", "def test_point_locatenew():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    P1 = Point('P1')\n    P2 = P1.locatenew('P2', q * N.x)\n    assert P2.pos_from(P1) == q * N.x\n    assert P1.pos_from(P2) == -q * N.x\n\n    P3 = P2.locatenew('P3', 2 * q * N.y)\n    assert P3.pos_from(P2) == 2 * q * N.y\n    assert P3.pos_from(P1) == q * N.x + 2 * q * N.y\n    assert P1.pos_from(P3) == -q * N.x - 2 * q * N.y\n\n    raises(TypeError, lambda: P1.locatenew('P4', \"invalid_vector\"))\n    raises(TypeError, lambda: P1.locatenew(123, q * N.x))\n", "def test_point_acc():\n    q1, q2 = dynamicsymbols('q1 q2')\n    q1d, q2d = dynamicsymbols('q1 q2', 1)\n    q1dd, q2dd = dynamicsymbols('q1 q2', 2)\n    N = ReferenceFrame('N')\n    P = Point('P')\n    P.set_vel(N, q1d * N.x)\n    assert P.acc(N) == q1dd * N.x\n    P.set_acc(N, q2dd * N.y)\n    assert P.acc(N) == q2dd * N.y\n    O = Point('O')\n    O.set_pos(P, q1 * N.y)\n    P.set_vel(N, q1d * N.y)\n    O.set_vel(N, q2d * N.x)\n    assert O.acc(N) == q2dd * N.x\n    assert P.acc(N) == q1dd * N.y\n", "def test_set_pos_typeerror():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    raises(TypeError, lambda: p1.set_pos(p2, \"10 * N.x\"))  # Invalid vector type\n"], "sample_1129": ["def test_MatrixFunctions():\n    from sympy import Matrix, MatrixSymbol\n    A = MatrixSymbol('A', 3, 3)\n    B = Matrix([[1, 2], [3, 4]])\n    prntr = PythonCodePrinter()\n    \n    assert prntr.doprint(A.det()) == 'A.det()'\n    assert prntr.doprint(B.det()) == '1*4 - 2*3'\n    \n    prntr = NumPyPrinter()\n    assert prntr.doprint(B.det()) == 'numpy.linalg.det([[1, 2], [3, 4]])'\n    \n    prntr = SciPyPrinter()\n    assert prntr.doprint(B.det()) == 'numpy.linalg.det([[1, 2], [3, 4]])'\n    \n    prntr = SymPyPrinter()\n    assert prntr.doprint(B.det()) == 'sympy.Matrix([[1, 2], [3, 4]]).det()'\n    \n    prntr = MpmathPrinter()\n    assert prntr.doprint(B.det()) == 'mpmath.det(mpmath.matrix([[1, 2], [3, 4]]))'\n", "def test_precedence():\n    prntr = PythonCodePrinter()\n    \n    from sympy.core import Pow, Mul, Add\n    a, b, c = symbols('a b c')\n    \n    # Testing precedence in expressions\n    assert prntr.doprint(a + b * c) == 'a + b*c'\n    assert prntr.doprint(a * b + c) == 'a*b + c'\n    assert prntr.doprint(a / (b + c)) == 'a/(b + c)'\n    assert prntr.doprint((a + b) / c) == '(a + b)/c'\n    assert prntr.doprint(a ** (b + c)) == 'a**(b + c)'\n    assert prntr.doprint((a + b) ** c) == '(a + b)**c'\n    assert prntr.doprint(a ** b ** c) == 'a**(b**c)'\n    assert prntr.doprint((a ** b) ** c) == '(a**b)**c'\n\n    # Testing Pow with rational exponents\n    assert prntr.doprint(Pow(a, Rational(1, 2))) == 'math.sqrt(a)'\n    assert prntr.doprint(Pow(a, Rational(1, 3))) == 'a**(1/3)'\n\n    # Testing precedence with mixed operations\n    expr1 = Mul(Add(a, b), c, evaluate=False)\n    assert prntr.doprint(expr1) == '(a + b)*c'\n\n    expr2 = Mul(a, Pow(b, c), evaluate=False)\n    assert prntr.doprint(expr2) == 'a*b**c'\n", "def test_print_known_functions_constants():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(S.Exp1) == 'math.e'\n    assert prntr.doprint(S.Pi) == 'math.pi'\n    assert prntr.doprint(S.E) == 'math.e'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(S.GoldenRatio) == 'scipy.constants.golden_ratio'\n    assert prntr.doprint(S.Pi) == 'scipy.constants.pi'\n    assert prntr.doprint(S.Exp1) == 'numpy.e'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(S.Exp1) == 'mpmath.e'\n    assert prntr.doprint(S.Pi) == 'mpmath.pi'\n    assert prntr.doprint(S.GoldenRatio) == 'mpmath.phi'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(S.Exp1) == 'numpy.e'\n    assert prntr.doprint(S.Pi) == 'numpy.pi'\n    assert prntr.doprint(S.EulerGamma) == 'numpy.euler_gamma'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(S.Exp1) == 'sympy.E'\n    assert prntr.doprint(S.Pi) == 'sympy.pi'\n    assert prntr.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n", "def test_print_ComplexInfinity():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"float('nan')\"\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"mpmath.nan\"\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"numpy.nan\"\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"numpy.nan\"\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"sympy.nan\"\n", "def test_logical_operators():\n    prntr = PythonCodePrinter()\n    assert prntr._print_And(And(x, y)) == 'x and y'\n    assert prntr._print_Or(Or(x, y)) == 'x or y'\n    assert prntr._print_Not(Not(x)) == 'not x'\n\n    prntr = NumPyPrinter()\n    assert prntr._print_And(And(x, y)) == 'numpy.logical_and.reduce((x, y))'\n    assert prntr._print_Or(Or(x, y)) == 'numpy.logical_or.reduce((x, y))'\n    assert prntr._print_Not(Not(x)) == 'numpy.logical_not(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr._print_And(And(x, y)) == 'mpmath.logical_and(x, y)'  # Assuming mpmath has logical_and\n    assert prntr._print_Or(Or(x, y)) == 'mpmath.logical_or(x, y)'  # Assuming mpmath has logical_or\n    assert prntr._print_Not(Not(x)) == 'mpmath.logical_not(x)'  # Assuming mpmath has logical_not\n\n    prntr = SciPyPrinter()\n    assert prntr._print_And(And(x, y)) == 'numpy.logical_and.reduce((x, y))'\n    assert prntr._print_Or(Or(x, y)) == 'numpy.logical_or.reduce((x, y))'\n    assert prntr._print_Not(Not(x)) == 'numpy.logical_not(x)'\n\n    prntr = SymPyPrinter()\n    assert prntr._print_And(And(x, y)) == 'sympy.And(x, y)'\n    assert prntr._print_Or(Or(x, y)) == 'sympy.Or(x, y)'\n    assert prntr._print_Not(Not(x)) == 'sympy.Not(x)'\n"], "sample_1130": ["def test_point_set_pos():\n    N = ReferenceFrame('N')\n    P1 = Point('P1')\n    P2 = Point('P2')\n    \n    # Ensure TypeError is raised with an invalid name\n    raises(TypeError, lambda: P1.set_pos(123, N.x))\n    \n    # Ensure TypeError is raised with an invalid vector\n    raises(TypeError, lambda: P1.set_pos(P2, 123))\n    \n    P1.set_pos(P2, 10 * N.x)\n    assert P1.pos_from(P2) == 10 * N.x\n    assert P2.pos_from(P1) == -10 * N.x\n    \n    P3 = Point('P3')\n    P3.set_pos(P1, 5 * N.y)\n    assert P3.pos_from(P1) == 5 * N.y\n    assert P3.pos_from(P2) == 5 * N.y + 10 * N.x\n", "def test_point_acc():\n    q, q2 = dynamicsymbols('q q2')\n    qd, q2d = dynamicsymbols('q q2', 1)\n    qdd, q2dd = dynamicsymbols('q q2', 2)\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    \n    P.set_acc(N, qdd * N.x + q2dd * N.y)\n    assert P.acc(N) == qdd * N.x + q2dd * N.y\n    \n    P.set_acc(N, 0)\n    assert P.acc(N) == Vector(0)\n    \n    O.set_vel(N, qd * N.x)\n    P.set_pos(O, q * N.x)\n    assert P.acc(N) == qdd * N.x + 2 * qd.diff() * N.x + qd * qd * N.x  # Testing auto calculation of acceleration\n", "def test_point_acc():\n    q1, q2 = dynamicsymbols('q1 q2')\n    q1d, q2d = dynamicsymbols('q1 q2', 1)\n    q1dd, q2dd = dynamicsymbols('q1 q2', 2)\n    N = ReferenceFrame('N')\n    P = Point('P')\n    O = Point('O')\n    Q = Point('Q')\n    \n    P.set_acc(N, q1dd * N.x)\n    assert P.acc(N) == q1dd * N.x\n    \n    Q.set_pos(P, q2 * N.y)\n    Q.set_vel(N, q2d * N.y)\n    Q.set_acc(N, q2dd * N.y)\n    assert Q.acc(N) == q2dd * N.y\n    \n    O.set_pos(P, q1 * N.x)\n    O.set_vel(N, q1d * N.x)\n    assert O.acc(N) == q1dd * N.x  # Automatically calculated based on O's position and velocity\n    \n    raises(ValueError, lambda: O.acc(B))  # Acceleration of O in B is not defined\n", "def test_point_set_pos_type_error():\n    N = ReferenceFrame('N')\n    P1 = Point('P1')\n    P2 = Point('P2')\n    raises(TypeError, lambda: P1.set_pos(P2, 10))  # should raise TypeError as the value is not a Vector\n    raises(TypeError, lambda: P1.set_pos(\"P2\", 10 * N.x))  # should raise TypeError as the other point is not a Point\n", "def test_point_acc(): #Basic functionality\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    O.set_vel(N, q1 * N.x)\n    raises(ValueError, lambda: O.acc(N)) # Acceleration of O in N is not defined\n    O.set_acc(N, q2 * N.y)\n    assert O.acc(N) == q2 * N.y\n    raises(ValueError, lambda: O.acc(B)) # Acceleration of O is not defined in B\n"], "sample_1131": ["def test_NaN_and_Infinity():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(S.NaN) == \"float('nan')\"\n    assert prntr.doprint(S.Infinity) == \"float('inf')\"\n    assert prntr.doprint(S.NegativeInfinity) == \"float('-inf')\"\n    assert prntr.doprint(S.ComplexInfinity) == \"float('nan')\"\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(S.NaN) == 'numpy.nan'\n    assert prntr.doprint(S.Infinity) == 'numpy.PINF'\n    assert prntr.doprint(S.NegativeInfinity) == 'numpy.NINF'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(S.NaN) == 'mpmath.nan'\n    assert prntr.doprint(S.Infinity) == 'mpmath.inf'\n    assert prntr.doprint(S.NegativeInfinity) == 'mpmath.ninf'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(S.NaN) == 'numpy.nan'\n    assert prntr.doprint(S.Infinity) == 'numpy.PINF'\n    assert prntr.doprint(S.NegativeInfinity) == 'numpy.NINF'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(S.NaN) == 'sympy.nan'\n    assert prntr.doprint(S.Infinity) == 'sympy.oo'\n    assert prntr.doprint(S.NegativeInfinity) == 'sympy.oo'\n", "def test_custom_user_functions():\n    custom_functions = {'customfunc': 'custom_module.custom_function'}\n    prntr = PythonCodePrinter({'user_functions': custom_functions})\n\n    class CustomFunc(Expr):\n        pass\n\n    custom_expr = CustomFunc(x)\n    setattr(PythonCodePrinter, '_print_CustomFunc', lambda self, expr: self._print_known_func(expr))\n\n    assert prntr.doprint(custom_expr) == 'custom_module.custom_function(x)'\n    assert 'custom_module' in prntr.module_imports\n    assert prntr.module_imports['custom_module'] == {'custom_function'}\n\n    prntr = NumPyPrinter({'user_functions': custom_functions})\n    assert prntr.doprint(custom_expr) == 'custom_module.custom_function(x)'\n\n    prntr = SciPyPrinter({'user_functions': custom_functions})\n    assert prntr.doprint(custom_expr) == 'custom_module.custom_function(x)'\n\n    prntr = MpmathPrinter({'user_functions': custom_functions})\n    assert prntr.doprint(custom_expr) == 'custom_module.custom_function(x)'\n\n    prntr = SymPyPrinter({'user_functions': custom_functions})\n    assert prntr.doprint(custom_expr) == 'custom_module.custom_function(x)'\n", "def test_custom_functions():\n    prntr = PythonCodePrinter({'user_functions': {'custom_func': 'my_module.custom_func'}})\n    \n    class CustomFunc(Expr):\n            self.arg = arg\n            return \"custom_func(%s)\" % self.arg\n\n    custom_expr = CustomFunc(x)\n    \n    assert prntr.doprint(custom_expr) == \"my_module.custom_func(x)\"\n    assert 'my_module' in prntr.module_imports\n\n    prntr = MpmathPrinter({'user_functions': {'custom_func': 'mpmath.custom_func'}})\n    assert prntr.doprint(custom_expr) == \"mpmath.custom_func(x)\"\n    assert 'mpmath' in prntr.module_imports\n\n    prntr = NumPyPrinter({'user_functions': {'custom_func': 'numpy.custom_func'}})\n    assert prntr.doprint(custom_expr) == \"numpy.custom_func(x)\"\n    assert 'numpy' in prntr.module_imports\n\n    prntr = SciPyPrinter({'user_functions': {'custom_func': 'scipy.special.custom_func'}})\n    assert prntr.doprint(custom_expr) == \"scipy.special.custom_func(x)\"\n    assert 'scipy.special' in prntr.module_imports\n\n    prntr = SymPyPrinter({'user_functions': {'custom_func': 'sympy.custom_func'}})\n    assert prntr.doprint(custom_expr) == \"sympy.custom_func(x)\"\n    assert 'sympy' in prntr.module_imports\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n\n    # Test known functions for SymPy printer\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(sign(x)) == '(0.0 if x == 0 else sympy.copysign(1, x))'\n    assert p.doprint(KroneckerDelta(x, y)) == '(1 if x == y else 0)'\n\n    # Test known constants for SymPy printer\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    assert p.doprint(S.Exp1) == 'sympy.E'\n\n    # Test function definition\n    from sympy.codegen.ast import FunctionDefinition, Variable\n    f = FunctionDefinition(\"f\", [Variable(\"x\")], [Assignment(x, 2)])\n    assert p.doprint(f) == 'def f(x):\\n    x = 2'\n\n    # Test relational expressions\n    assert p.doprint(Eq(x, y)) == '(x == y)'\n    assert p.doprint(Gt(x, y)) == '(x > y)'\n    assert p.doprint(Le(x, y)) == '(x <= y)'\n\n    # Test Piecewise\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x > 6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n\n    # Test MatrixBase\n    from sympy import Matrix\n    M = Matrix([[1, 2], [3, 4]])\n    assert p.doprint(M) == 'sympy.Matrix([[1, 2], [3, 4]])'\n\n    # Test Indexed\n    assert p.doprint(p[0, 1]) == 'p[0, 1]'\n\n    # Test logical operators\n    assert p.doprint(And(x, y)) == 'x and y'\n    assert p.doprint(Or(x, y)) == 'x or y'\n    assert p.doprint(~x) == 'not x'\n", "def test_print_Rational():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(Rational(2, 3)) == '2/3'\n    prntr = PythonCodePrinter({'standard': 'python2'})\n    assert prntr.doprint(Rational(2, 3)) == '2./3.'\n    prntr = MpmathPrinter()\n    assert prntr.doprint(Rational(2, 3)) == 'mpmath.mpf(2)/mpmath.mpf(3)'\n    prntr = NumPyPrinter()\n    assert prntr.doprint(Rational(2, 3)) == '2/3'\n    prntr = SciPyPrinter()\n    assert prntr.doprint(Rational(2, 3)) == '2/3'\n    prntr = SymPyPrinter()\n    assert prntr.doprint(Rational(2, 3)) == '2/3'\n"], "sample_1132": ["def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((3, 0, -4))) == [(3, 0, -4), (-3, 0, -4), (3, 0, 4), (-3, 0, 4)]\n    assert list(permute_signs((0, 0))) == [(0, 0)]\n    assert list(permute_signs((1, 0, -1))) == [(1, 0, -1), (-1, 0, -1), (1, 0, 1), (-1, 0, 1)]\n    ", "def test_ibin():\n    assert ibin(2) == [1, 0]\n    assert ibin(2, 4) == [0, 0, 1, 0]\n    assert ibin(0, 3) == [0, 0, 0]\n    assert ibin(1, 1) == [1]\n    assert ibin(1, 2) == [0, 1]\n    assert ibin(0, 0) == []\n    assert ibin(2, str=True) == '10'\n    assert ibin(2, 4, str=True) == '0010'\n    assert ibin(0, 3, str=True) == '000'\n    assert ibin(1, 1, str=True) == '1'\n    assert ibin(1, 2, str=True) == '01'\n    assert ibin(0, 0, str=True) == ''\n    assert list(ibin(2, 'all')) == [(0, 0), (0, 1), (1, 0), (1, 1)]\n    assert list(ibin(3, 'all', str=True)) == ['000', '001', '010', '011', '100', '101', '110', '111']\n    raises(ValueError, lambda: ibin(-1))\n", "def test_ibin():\n    assert ibin(2) == [1, 0]\n    assert ibin(2, 4) == [0, 0, 1, 0]\n    assert list(ibin(2, 'all')) == [(0, 0), (0, 1), (1, 0), (1, 1)]\n    assert ibin(2, 4, str=True) == '0010'\n    assert ibin(2, 4, str=True)[::-1] == '0100'\n    assert list(ibin(3, 'all', str=True)) == ['000', '001', '010', '011', '100', '101', '110', '111']\n    raises(ValueError, lambda: ibin(-1))\n    assert ibin(5, 3) == [1, 0, 1]\n    assert ibin(0) == []\n    assert ibin(1, 5) == [0, 0, 0, 0, 1]\n", "def test_ibin():\n    assert ibin(2) == [1, 0]\n    assert ibin(2, 4) == [0, 0, 1, 0]\n    assert list(ibin(2, 'all')) == [(0, 0), (0, 1), (1, 0), (1, 1)]\n    assert ibin(123, 10, str=True) == '0001111011'\n    assert ibin(123, 10, str=True)[::-1] == '1101111000'\n    assert list(ibin(3, 'all', str=True)) == ['000', '001', '010', '011', '100', '101', '110', '111']\n    raises(ValueError, lambda: ibin(-1))\n", "def test_common_prefix_suffix_edge_cases():\n    # Test cases with empty sequences and one-element sequences\n    assert common_prefix([], []) == []\n    assert common_suffix([], []) == []\n    assert common_prefix([1], [1]) == [1]\n    assert common_suffix([1], [1]) == [1]\n    assert common_prefix([1], [2]) == []\n    assert common_suffix([1], [2]) == []\n    assert common_prefix([1, 2, 3], []) == []\n    assert common_suffix([1, 2, 3], []) == []\n\n    # Test cases with different lengths\n    assert common_prefix([1, 2, 3], [1, 2, 3, 4]) == [1, 2, 3]\n    assert common_suffix([1, 2, 3], [0, 1, 2, 3]) == [1, 2, 3]\n    assert common_prefix([1, 2, 3, 4], [1, 2, 3]) == [1, 2, 3]\n    assert common_suffix([0, 1, 2, 3], [1, 2, 3]) == [1, 2, 3]\n\n    # Test cases with no common prefix/suffix\n    assert common_prefix([1, 2, 3], [4, 5, 6]) == []\n    assert common_suffix([1, 2, 3], [4, 5, 6]) == []\n"], "sample_1133": ["def test_critical_angle_exceptions():\n    m1 = Medium('m1', n=1)\n    m2 = Medium('m2', n=1.5)\n    raises(ValueError, lambda: critical_angle(m1, m2))\n    raises(ValueError, lambda: critical_angle(1, 1.5))\n    raises(ValueError, lambda: critical_angle(1, 2))\n", "def test_critical_angle_total_internal_reflection_impossible():\n    m1 = Medium('m1', n=1)\n    m2 = Medium('m2', n=1.5)\n    raises(ValueError, lambda: critical_angle(m1, m2))\n    raises(ValueError, lambda: critical_angle(1, 1.5))\n", "def test_refraction_angle_TIR():\n    n1, n2 = symbols('n1, n2')\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    normal = Matrix([0, 0, 1])\n    assert refraction_angle(r1, 1.5, 1, normal) == 0  # TIR\n    raises(ValueError, lambda: refraction_angle(1.0, 1.33, 1))  # TIR for angle of incidence as input\n    raises(ValueError, lambda: refraction_angle(0.9, 1.5, 1))  # TIR for angle of incidence as input\n", "def test_critical_angle_invalid():\n    m1 = Medium('m1', n=1)\n    m2 = Medium('m2', n=1.33)\n    raises(ValueError, lambda: critical_angle(m1, m2))\n", "def test_additional_refraction_angle():\n    # Testing refraction angle when normal and incident rays are not orthogonal\n    i = Matrix([1, 1, 0])\n    n = Matrix([1, 0, 0])\n    assert refraction_angle(i, 1, 1.33, n) == Matrix([\n                                            [ 1],\n                                            [ 1/sqrt(1.33)],\n                                            [ 0]])\n    # Testing refraction angle when incident is a float and normal/plane not provided\n    assert ae(refraction_angle(0.3, 1.0, 1.5), 0.19513, 5)\n    assert ae(refraction_angle(0.3, 1.5, 1.0), 0.49441, 5)\n    # Testing invalid angle of incidence\n    raises(ValueError, lambda: refraction_angle(1.7, 1, 1.33))\n    # Testing invalid type for normal\n    raises(TypeError, lambda: refraction_angle(i, 1, 1.33, normal=\"invalid\"))\n    # Testing invalid type for plane\n    raises(TypeError, lambda: refraction_angle(i, 1, 1.33, plane=\"invalid\"))\n"], "sample_1134": ["def test_latex_PartialDerivative():\n    from sympy import Function, diff\n    x, y = symbols('x y')\n    f = Function('f')\n    \n    assert latex(diff(f(x, y), x, y)) == r'\\frac{\\partial^{2}}{\\partial x \\partial y} f{\\left(x, y \\right)}'\n    assert latex(diff(f(x, y), x, y, y)) == r'\\frac{\\partial^{3}}{\\partial y^{2} \\partial x} f{\\left(x, y \\right)}'\n    assert latex(diff(f(x, y), y, x, y)) == r'\\frac{\\partial^{3}}{\\partial y \\partial x \\partial y} f{\\left(x, y \\right)}'\n    assert latex(diff(f(x, y), y, x, y)**2) == r'\\left(\\frac{\\partial^{3}}{\\partial y \\partial x \\partial y} f{\\left(x, y \\right)}\\right)^{2}'\n    assert latex(diff(f(x, y, x), x, y)) == r'\\frac{\\partial^{2}}{\\partial x \\partial y} f{\\left(x, y, x \\right)}'\n", "def test_latex_escape_special_characters():\n    assert latex_escape(r\"%&$#_{}~^\") == r\"\\%\\&\\$\\#\\_\\{\\}\\textasciitilde\\textasciicircum\"\n    assert latex_escape(r\"hello_world\") == r\"hello\\_world\"\n    assert latex_escape(r\"\\frac{1}{2}\") == r\"\\textbackslash frac\\{1\\}\\{2\\}\"\n    assert latex_escape(r\"normal_text\") == r\"normal\\_text\"\n    assert latex_escape(r\"123_456\") == r\"123\\_456\"\n", "def test_latex_IndexedBase_and_Indexed():\n    # Test for IndexedBase and Indexed objects\n    A = IndexedBase('A')\n    B = IndexedBase('B', shape=(2, 3))\n\n    assert latex(A[0]) == r'A_{0}'\n    assert latex(A[i]) == r'A_{i}'\n    assert latex(A[i, j]) == r'A_{i, j}'\n\n    assert latex(B[0, 0]) == r'B_{0, 0}'\n    assert latex(B[i, j]) == r'B_{i, j}'\n    assert latex(B[i, j] + B[i + 1, j - 1]) == r'B_{i, j} + B_{i + 1, j - 1}'\n\n    # Test for Indexed object with dimension argument\n    C = IndexedBase('C', shape=(2, 2))\n    i, j, k = symbols('i j k', integer=True)\n    assert latex(C[0]) == r'C_{0}'\n    assert latex(C[i]) == r'C_{i}'\n    assert latex(C[i, j]) == r'C_{i, j}'\n    assert latex(C[i, j, k]) == r'C_{i, j, k}'\n", "def test_latex_escape_special_characters():\n    # Test escaping of various special characters\n    special_chars = r\"&%$#_{}\"\n    escaped_chars = r\"\\&\\%\\$\\#\\_\\{\\}\"\n    assert latex_escape(special_chars) == escaped_chars\n\n    # Test escaping of backslashes\n    assert latex_escape(r\"\\\\\") == r\"\\textbackslash\\textbackslash\"\n\n    # Test escaping of tildes\n    assert latex_escape(r\"~\") == r\"\\textasciitilde\"\n\n    # Test escaping of carets\n    assert latex_escape(r\"^\") == r\"\\textasciicircum\"\n\n    # Test escaping of mixed characters\n    mixed_chars = r\"\\&%~_^$#{}\"\n    mixed_escaped = r\"\\textbackslash\\&\\%\\textasciitilde\\_\\textasciicircum\\$\\#\\{\\}\"\n    assert latex_escape(mixed_chars) == mixed_escaped\n", "def test_multiline_latex_custom():\n    # Test multiline_latex with custom settings\n    a, b, c = symbols('a b c')\n    expr = a + b + c\n    expected = r\"\\begin{equation}\" + \"\\n\"\\\n    r\"f = a + b + c\" + \"\\n\"\\\n    r\"\\end{equation}\"\n    assert multiline_latex(f, expr, 3, environment=\"equation\") == expected\n\n    # Test multiline_latex with 4 terms per line\n    expr = a + b + c + x + y + z\n    expected = r\"\\begin{eqnarray}\" + \"\\n\"\\\n    r\"f & = &a + b + c + x \\nonumber\\\\\" + \"\\n\"\\\n    r\"& & + y + z \" + \"\\n\"\\\n    r\"\\end{eqnarray}\"\n    assert multiline_latex(f, expr, 4, environment=\"eqnarray\") == expected\n\n    # Test multiline_latex with long expression and limited terms per line\n    expr = a + b + c + x + y + z + t + w\n    expected = r\"\\begin{align*}\" + \"\\n\"\\\n    r\"f = &a + b + c + x + y + z \\\\& + t + w \" + \"\\n\"\\\n    r\"\\end{align*}\"\n    assert multiline_latex(f, expr, 6, environment=\"align*\") == expected\n"], "sample_1135": ["def test__unevaluated_Mul():\n    from sympy.abc import a, b, c, d\n\n    # Case 1: Combining Numbers\n    result = _unevaluated_Mul(S(2), S(3))\n    assert result.args[0] == 6\n\n    # Case 2: Combining Numbers and Symbols\n    result = _unevaluated_Mul(S(2), a)\n    assert result.args[0] == 2\n    assert result.args[1] == a\n\n    # Case 3: Nested Muls\n    result = _unevaluated_Mul(S(2), Mul(a, b, evaluate=False))\n    assert result.args[0] == 2\n    assert result.args[1] == a\n    assert result.args[2] == b\n\n    # Case 4: Combining Commutative and Non-Commutative Parts\n    result = _unevaluated_Mul(Mul(S(2), a, evaluate=False), Mul(b, c, evaluate=False))\n    assert result.args[0] == 2\n    assert result.args[1] == a\n    assert result.args[2] == b\n    assert result.args[3] == c\n\n    # Case 5: Check Sorting\n    result = _unevaluated_Mul(c, a, b)\n    assert result.args[0] == a\n    assert result.args[1] == b\n    assert result.args[2] == c\n\n    # Case 6: Check with Number in Between\n    result = _unevaluated_Mul(S(3), c, S(2), a, b)\n    assert result.args[0] == 6\n    assert result.args[1] == a\n    assert result.args[2] == b\n    assert result.args[3] == c\n\n    # Case 7: Non-Commutative Objects\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    result = _unevaluated_Mul(S(2), A, B, S(3))\n    assert result.args[0] == 6\n    assert result.args[1] == Mul(A, B, evaluate=False)\n\n    # Case 8: Check with Unevaluated Mul\n    result = _unevaluated_Mul(Mul(a, b, evaluate=False))\n    assert result == Mul(a, b", "def test_unevaluated_Mul():\n    from sympy.abc import a, b, c\n    from sympy import Mul, S, sqrt\n\n    # Test that _unevaluated_Mul returns a Mul without evaluating\n    result = Mul._unevaluated_Mul(a, b, c)\n    assert result == a * b * c\n    assert result.is_Mul\n\n    # Test that numbers are collected and put in slot 0\n    result = Mul._unevaluated_Mul(2, 3.0, a)\n    assert result.args[0] == 6.0\n    assert result.args[1] == a\n\n    # Test that arguments that are Muls are flattened\n    nested_mul = Mul(a, b)\n    result = Mul._unevaluated_Mul(nested_mul, c)\n    assert result == a * b * c\n\n    # Test that args are sorted\n    result = Mul._unevaluated_Mul(c, a, b)\n    assert result == a * b * c\n\n    # Test comparison of unevaluated Muls\n    m = Mul._unevaluated_Mul(sqrt(2), sqrt(3))\n    assert m == Mul._unevaluated_Mul(sqrt(3), sqrt(2))\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m == Mul._unevaluated_Mul(u)\n    assert m != Mul(*m.args)\n\n    # Test that coefficients are merged correctly\n    result = Mul._unevaluated_Mul(S(2), S(3.0), a)\n    assert result.args[0] == 6.0\n    assert result.args[1] == a\n\n    # Test with non-commutative arguments\n    nc1 = Basic()\n    nc2 = Basic()\n    result = Mul._unevaluated_Mul(nc1, nc2)\n    assert result.args[-1] == Mul._from_args([nc1, nc2])\n", "def test_mul_is_hermitian():\n    a = Symbol('a', hermitian=True)\n    b = Symbol('b', hermitian=True)\n    c = Symbol('c', hermitian=True)\n    d = Symbol('d', hermitian=False)\n    e = Symbol('e', antihermitian=True)\n\n    assert (a*b).is_hermitian is True\n    assert (a*c).is_hermitian is True\n    assert (a*d).is_hermitian is None\n    assert (a*e).is_hermitian is False\n    assert (a*b*c).is_hermitian is True\n    assert (a*b*d).is_hermitian is None\n    assert (a*b*e).is_hermitian is False\n    assert (a*b*c*d).is_hermitian is None\n    assert (a*b*c*e).is_hermitian is False\n", "def test__unevaluated_Mul():\n    from sympy.abc import a, b, c, x\n    from sympy import sqrt, Mul, S\n    \n    # Test with numbers\n    m = _unevaluated_Mul(S(3), S(4))\n    assert m == S(12)\n    \n    # Test with a mix of numbers and symbols\n    m = _unevaluated_Mul(S(2), x, S(3), a)\n    assert m == Mul(6, a, x)\n    \n    # Test with nested Muls\n    m = _unevaluated_Mul(Mul(2, x), Mul(3, a))\n    assert m == Mul(6, a, x)\n    \n    # Test with an unevaluated Mul\n    m = _unevaluated_Mul(Mul(a, b, evaluate=False), c)\n    assert m == Mul(a, b, c, evaluate=False)\n    \n    # Test with nested unevaluated Muls\n    m = _unevaluated_Mul(Mul(a, b, evaluate=False), Mul(c, x, evaluate=False))\n    assert m == Mul(a, b, c, x, evaluate=False)\n    \n    # Test commutative properties\n    m1 = _unevaluated_Mul(S(3.0), x, S(2))\n    m2 = _unevaluated_Mul(S(2), x, S(3.0))\n    assert m1 == m2\n    \n    # Test with sqrt\n    m = _unevaluated_Mul(sqrt(2), sqrt(3))\n    assert m == sqrt(6)\n    \n    # Ensure it returns the same unevaluated Mul\n    m = _unevaluated_Mul(Mul(a, b, evaluate=False))\n    assert m == Mul(a, b, evaluate=False)\n", "def test__unevaluated_Mul():\n    from sympy import sqrt, S, Mul\n    a, b, c = symbols('a b c')\n    \n    # Test that unevaluated Mul works as expected\n    expr = _unevaluated_Mul(a, b, c)\n    assert expr == a * b * c\n\n    # Test that Numbers are collected and put in slot 0\n    expr = _unevaluated_Mul(a, S(2), b)\n    assert expr.args[0] == 2\n    assert expr.args[1] == a\n    assert expr.args[2] == b\n\n    # Test that unevaluated Mul with same arguments compare as equal\n    m1 = _unevaluated_Mul(sqrt(2), sqrt(3))\n    m2 = _unevaluated_Mul(sqrt(3), sqrt(2))\n    assert m1 == m2\n\n    # Test unevaluated Mul with nested unevaluated Muls\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    m3 = _unevaluated_Mul(u)\n    assert m1 == m3\n\n    # Test unevaluated Mul with nested evaluated Muls\n    m4 = Mul(*m1.args)\n    assert m1 != m4\n"], "sample_1136": ["def test_ExpressionDomain_operations():\n    from sympy import EX, sympify\n\n    # Testing basic arithmetic operations and properties\n    a = EX(2)\n    b = EX(3)\n    c = EX(2 * sympify(3))\n\n    assert a + b == EX(5)\n    assert a - b == EX(-1)\n    assert a * b == c\n    assert a / b == EX(2 / 3)\n    assert a**2 == EX(4)\n    assert abs(a) == a\n    assert -a == EX(-2)\n    assert a == EX(2)\n    assert a != b\n    assert bool(a)\n    assert not bool(EX(0))\n\n    # Test gcd and lcm\n    assert a.gcd(b) == EX(1)\n    assert a.lcm(b) == EX(6)\n\n    # Test __hash__\n    assert hash(a) == hash(EX(2))\n\n    # Test __repr__ and __str__\n    assert repr(a) == \"EX(2)\"\n    assert str(a) == \"EX(2)\"\n\n    # Test conversion between sympy expressions and EX\n    expr = sympify(\"x + 1\")\n    ex = EX(expr)\n    assert ex.as_expr() == expr\n\n    # Test simplify\n    f = EX(\"x**2 + 2*x + 1\")\n    assert f.simplify(f.ex) == EX(\"(x + 1)**2\")\n", "def test_ExpressionDomain_basic_operations():\n    EX = ExpressionDomain()\n    a = EX.from_sympy(x + 1)\n    b = EX.from_sympy(y + 2)\n\n    # Test addition\n    assert a + b == EX.from_sympy(x + y + 3)\n    assert b + a == EX.from_sympy(x + y + 3)\n    \n    # Test subtraction\n    assert a - b == EX.from_sympy(x - y - 1)\n    assert b - a == EX.from_sympy(y - x + 1)\n\n    # Test multiplication\n    assert a * b == EX.from_sympy((x + 1)*(y + 2))\n    assert b * a == EX.from_sympy((x + 1)*(y + 2))\n\n    # Test division\n    assert a / b == EX.from_sympy((x + 1)/(y + 2))\n    assert b / a == EX.from_sympy((y + 2)/(x + 1))\n\n    # Test power\n    assert a ** 2 == EX.from_sympy((x + 1)**2)\n    assert b ** 3 == EX.from_sympy((y + 2)**3)\n\n    # Test negation\n    assert -a == EX.from_sympy(-(x + 1))\n    assert -b == EX.from_sympy(-(y + 2))\n\n    # Test absolute value\n    assert abs(a) == EX.from_sympy(abs(x + 1))\n    assert abs(b) == EX.from_sympy(abs(y + 2))\n\n    # Test equality\n    assert a == EX.from_sympy(x + 1)\n    assert b == EX.from_sympy(y + 2)\n    assert a != b\n", "def test_ExpressionDomain_init():\n    domain = ExpressionDomain()\n    assert domain.rep == 'EX'\n    assert domain.zero == ExpressionDomain.zero\n    assert domain.one == ExpressionDomain.one\n    assert domain.dtype == ExpressionDomain.dtype\n    assert domain.has_assoc_Ring == False\n    assert domain.has_assoc_Field == True\n", "def test_ExpressionDomain_operations():\n    EX = ExpressionDomain()\n\n    # Test ExpressionDomain to_sympy and from_sympy\n    expr = EX.to_sympy(EX.one)\n    assert expr == 1\n    expr = EX.to_sympy(EX.zero)\n    assert expr == 0\n    expr = EX.from_sympy(1)\n    assert expr.ex == 1\n    expr = EX.from_sympy(0)\n    assert expr.ex == 0\n\n    # Test basic operations\n    one = EX.one\n    zero = EX.zero\n    assert one + one == one.simplify(2)\n    assert one * one == one\n    assert one - one == zero\n    assert one / one == one\n    assert -one == EX(-1)\n    assert abs(-one) == one\n\n    # Test gcd and lcm\n    gcd = one.gcd(one)\n    assert gcd.ex == 1\n    lcm = one.lcm(one)\n    assert lcm.ex == 1\n\n    # Test _to_ex and simplify\n    ex_obj = one._to_ex(1)\n    assert ex_obj.ex == 1\n    simplified = one.simplify(2)\n    assert simplified.ex == 2\n\n    # Test conversions from other domains\n    from_zz = EX.from_ZZ_python(3, EX)\n    assert from_zz.ex == 3\n    from_qq = EX.from_QQ_python(Rational(1, 2), EX)\n    assert from_qq.ex == Rational(1, 2)\n    from_zz_gmpy = EX.from_ZZ_gmpy(2, EX)\n    assert from_zz_gmpy.ex == 2\n    from_qq_gmpy = EX.from_QQ_gmpy(Rational(3, 4), EX)\n    assert from_qq_gmpy.ex == Rational(3, 4)\n    from_gaussian_int = EX.from_GaussianIntegerRing(4, EX)\n    assert from_gaussian_int.ex == 4\n    from_gaussian_rat = EX.from_GaussianRationalField(Rational(5, 6), EX)\n    assert from_gaussian_rat.ex == Rational(5, 6)\n    from_real_field = EX.from_RealField(3.14, EX)\n    assert from_real_field.ex == 3.14\n    from_poly_ring = EX.from_PolynomialRing(", "def test_ExpressionDomain_operations():\n    EXD = EX()\n    a = EXD.dtype('2*x')\n    b = EXD.dtype('3*y')\n    c = EXD.dtype('4')\n    \n    assert a + b == EXD.dtype('2*x + 3*y')\n    assert a - b == EXD.dtype('2*x - 3*y')\n    assert a * b == EXD.dtype('6*x*y')\n    assert a / c == EXD.dtype('x/2')\n    \n    # test gcd and lcm\n    assert a.gcd(b) == EXD.dtype('1')\n    assert a.lcm(b) == EXD.dtype('6*x*y')\n    \n    # test numer and denom\n    assert (a*b).numer() == a*b\n    assert (a*b).denom() == EXD.one\n    \n    # test hashing\n    assert hash(a) == hash(EXD.dtype('2*x'))\n    assert hash(b) == hash(EXD.dtype('3*y'))\n    assert hash(c) == hash(EXD.dtype('4'))\n\n    # test __bool__ method\n    assert bool(a) is True\n    assert bool(EXD.zero) is False\n"], "sample_1137": ["def test_quantity_simplify():\n    from sympy.physics.units.prefixes import milli, centi, deci\n\n    expr = kilo*foot*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 250*foot**2/3\n\n    expr = foot - 6*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == foot/2\n\n    expr = milli*meter + centi*meter + deci*meter\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 111*milli*meter\n", "def test_quantity_simplify():\n    from sympy.physics.units.prefixes import milli\n    from sympy.physics.units import inch, foot\n\n    expr = milli*inch*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == Rational(250, 127) * inch**2\n\n    expr = kilo*foot*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 250 * foot**2 / 3\n\n    expr = foot - 6 * inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == foot / 2\n", "def test_quantity_simplify():\n    from sympy.physics.units import joule, watt, hour, newton, meter, second, foot, inch, kilo, gram\n    from sympy.physics.units.util import quantity_simplify\n    \n    assert quantity_simplify(kilo*foot*inch) == 250*foot**2/3\n    assert quantity_simplify(foot - 6*inch) == foot/2\n    assert quantity_simplify(kilo*gram) == 1000*gram\n    assert quantity_simplify(2*joule/watt) == 2*second\n    assert quantity_simplify(3*watt*hour) == 10800*joule\n    assert quantity_simplify(newton*meter) == newton*meter\n", "def test_quantity_simplify():\n    from sympy.physics.units.prefixes import milli, centi\n    from sympy.physics.units import foot, inch\n\n    # Test simplification with prefixes\n    expr = milli*foot*centi*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == Rational(250, 127)*foot**2\n\n    # Test simplification with subtraction\n    expr = foot - 6*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == foot / 2\n\n    # Test simplification with mixed units\n    expr = kilo*foot / milli*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == Rational(250000, 127) * foot / inch\n", "def test_quantity_simplify():\n    from sympy.physics.units import joule, kilo, foot, inch\n\n    expr = kilo*foot*inch\n    simplified = quantity_simplify(expr)\n    assert simplified == 250*foot**2/3\n\n    expr = foot - 6*inch\n    simplified = quantity_simplify(expr)\n    assert simplified == foot/2\n\n    expr = 3*kilo*joule\n    simplified = quantity_simplify(expr)\n    assert simplified == 3000*joule\n\n    expr = 2*inch + 5*foot\n    simplified = quantity_simplify(expr)\n    assert simplified == 62*inch\n"], "sample_1138": ["def test_TR111():\n    assert TR111(1 - 1/tan(x)**2) == 1 - cot(x)**2\n    assert TR111(1/sin(x)**2) == csc(x)**2\n    assert TR111(1/cos(x)**2) == sec(x)**2\n    assert TR111(tan(x)**-2) == cot(x)**2\n    assert TR111(sin(x)**-2) == csc(x)**2\n    assert TR111(cos(x)**-2) == sec(x)**2\n    assert TR111(tan(x)) == tan(x)\n    assert TR111(sin(x)) == sin(x)\n    assert TR111(cos(x)) == cos(x)\n", "def test_TR4():\n    assert TR4(sin(0)) == 0\n    assert TR4(cos(0)) == 1\n    assert TR4(sin(pi/6)) == S.Half\n    assert TR4(cos(pi/6)) == sqrt(3)/2\n    assert TR4(sin(pi/4)) == sqrt(2)/2\n    assert TR4(cos(pi/4)) == sqrt(2)/2\n    assert TR4(sin(pi/3)) == sqrt(3)/2\n    assert TR4(cos(pi/3)) == S.Half\n    assert TR4(sin(pi/2)) == 1\n    assert TR4(cos(pi/2)) == 0\n    assert TR4(tan(pi/4)) == 1\n    assert TR4(cot(pi/4)) == 1\n    assert TR4(sec(0)) == 1\n    assert TR4(csc(pi/2)) == 1\n", "def test_TR4():\n    # Test special angle values\n    angles = [0, pi/6, pi/4, pi/3, pi/2]\n    cos_values = [1, sqrt(3)/2, sqrt(2)/2, S.Half, 0]\n    sin_values = [0, S.Half, sqrt(2)/2, sqrt(3)/2, 1]\n    tan_values = [0, sqrt(3)/3, 1, sqrt(3), S.ComplexInfinity]\n    \n    for angle, cos_val, sin_val, tan_val in zip(angles, cos_values, sin_values, tan_values):\n        assert cos(angle) == cos_val\n        assert sin(angle) == sin_val\n        assert (tan(angle) == tan_val) or (tan(angle).equals(tan_val))\n    \n    # Ensure no changes for TR4 (it should be identity)\n    assert TR4(cos(x)) == cos(x)\n    assert TR4(sin(x)) == sin(x)\n    assert TR4(tan(x)) == tan(x)\n", "def test_TR4():\n    assert TR4(cos(0)) == cos(0)\n    assert TR4(cos(pi/6)) == sqrt(3)/2\n    assert TR4(cos(pi/4)) == sqrt(2)/2\n    assert TR4(cos(pi/3)) == S.Half\n    assert TR4(cos(pi/2)) == 0\n    assert TR4(sin(0)) == 0\n    assert TR4(sin(pi/6)) == S.Half\n    assert TR4(sin(pi/4)) == sqrt(2)/2\n    assert TR4(sin(pi/3)) == sqrt(3)/2\n    assert TR4(sin(pi/2)) == 1\n    assert TR4(tan(0)) == 0\n    assert TR4(tan(pi/6)) == sqrt(3)/3\n    assert TR4(tan(pi/4)) == 1\n    assert TR4(tan(pi/3)) == sqrt(3)\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n    assert TR22(cot(x)**4) == (csc(x)**2 - 1)**2\n    assert TR22(tan(x)**2 + cot(x)**2) == sec(x)**2 + csc(x)**2 - 2\n    assert TR22(tan(x)**2 - cot(x)**2) == sec(x)**2 - csc(x)**2\n"], "sample_1139": ["def test_rationals_properties():\n    R = S.Rationals\n    assert R.is_iterable\n    assert not R.is_empty\n    assert not R.is_finite_set\n    assert R.boundary == S.Reals\n    assert R.inf == -oo\n    assert R.sup == oo\n    assert R.closure == S.Reals\n    assert not R.is_open\n    assert not R.is_closed\n    assert R.contains(Rational(3, 7))\n    assert R.contains(1.5) == Contains(1.5, S.Rationals, evaluate=False)\n", "def test_Range_symbolic_boundary():\n    from sympy import ceiling\n    i = Symbol('i', integer=True)\n    j = Symbol('j', integer=True, positive=True)\n    assert Range(i, i + 20, 2).boundary == Range(i, i + 20, 2)\n    assert Range(i, i + 20, 2).closure == Range(i, i + 20, 2)\n    assert Range(i, i + 20, 2).is_open == False\n    assert Range(i, i + 20, 2).is_closed == True\n\n    r = Range(i, i + 20, 2)\n    assert r._inf == i\n    assert r._sup == i + 18\n\n    r = Range(j, j + 15, 3)\n    assert r._inf == j\n    assert r._sup == j + 12\n\n    r = Range(i, j, 1)\n    assert r._inf == i\n    assert r._sup == j - 1\n\n    r = Range(j, i, -1)\n    assert r._inf == i + 1\n    assert r._sup == j\n\n    r = Range(i, i + 100, 1)\n    assert r._inf == i\n    assert r._sup == i + 99\n\n    r = Range(ceiling(i), i + 20, 2)\n    assert r.boundary == r\n    assert r.closure == r\n    assert r.is_open == False\n    assert r.is_closed == True\n", "def test_Rationals_subset():\n    assert S.Rationals.is_subset(S.Rationals)\n    assert S.Rationals.is_subset(S.Reals)\n    assert not S.Rationals.is_subset(S.Naturals)\n    assert not S.Rationals.is_subset(S.Integers)\n    assert S.Naturals.is_subset(S.Rationals)\n    assert S.Naturals0.is_subset(S.Rationals)\n    assert S.Integers.is_subset(S.Rationals)\n    \n    # Check if certain rationals are in S.Rationals\n    assert Rational(1, 3) in S.Rationals\n    assert Rational(-1, 3) in S.Rationals\n    assert Rational(3, 7) in S.Rationals\n    \n    # Check if non-rationals are not in S.Rationals\n    assert sqrt(2) not in S.Rationals\n    assert pi not in S.Rationals\n    assert E not in S.Rationals\n", "def test_Rationals_intersection():\n    Q = S.Rationals\n    N = S.Naturals\n    Z = S.Integers\n\n    # Intersection with Integers\n    assert Q.intersect(Z) == Z\n    assert Z.intersect(Q) == Z\n\n    # Intersection with Naturals\n    assert Q.intersect(N) == N\n    assert N.intersect(Q) == N\n\n    # Intersection with empty set\n    assert Q.intersect(S.EmptySet) == S.EmptySet\n\n    # Intersection with Reals\n    assert Q.intersect(S.Reals) == Q\n    assert S.Reals.intersect(Q) == Q\n\n    # Intersection with complex numbers\n    assert Q.intersect(S.Complexes) == Q\n    assert S.Complexes.intersect(Q) == Q\n\n    # Intersection with a finite set of rationals\n    assert Q.intersect(FiniteSet(1, Rational(1, 2), Rational(2, 3))) == FiniteSet(1, Rational(1, 2), Rational(2, 3))\n    assert FiniteSet(1, Rational(1, 2), Rational(2, 3)).intersect(Q) == FiniteSet(1, Rational(1, 2), Rational(2, 3))\n\n    # Intersection with an interval\n    assert Q.intersect(Interval(1, 2)) == Intersection(Q, Interval(1, 2))\n    assert Interval(1, 2).intersect(Q) == Intersection(Interval(1, 2), Q)\n", "def test_rationals_contains():\n    R = S.Rationals\n    assert R._contains(S.Half) == True\n    assert R._contains(S.Pi) == False\n    assert R._contains(3) == True\n    assert R._contains(-4) == True\n    assert R._contains(sqrt(2)) == False\n    assert R._contains(I) == False\n    assert R._contains(Symbol('x')) is None\n    assert R._contains(1.5) == False\n"], "sample_1140": ["def test_pretty_PrinterSettings():\n    # Tests for PrettyPrinter settings and their effects\n\n    # Create an instance of PrettyPrinter with custom settings\n    pp = PrettyPrinter({'use_unicode': False, 'wrap_line': False})\n\n    # Test an expression with the default settings\n    expr = x**2 + x + 1\n    assert pp.doprint(expr) == \"x**2 + x + 1\"\n\n    # Test changing the use_unicode setting\n    pp._settings['use_unicode'] = True\n    assert pp.doprint(expr) == \"x\u00b2 + x + 1\"\n\n    # Test changing the wrap_line setting\n    pp._settings['wrap_line'] = True\n    long_expr = x**2 + x + 1 + x**3 + x**4 + x**5 + x**6\n    assert pp.doprint(long_expr) == \"x\u00b2 + x + 1 + x\u00b3 + x\u2074 + x\u2075 + x\u2076\"\n\n    # Test use_unicode_sqrt_char setting\n    pp._settings['use_unicode_sqrt_char'] = False\n    sqrt_expr = sqrt(x)\n    assert pp.doprint(sqrt_expr) == \"sqrt(x)\"\n    pp._settings['use_unicode_sqrt_char'] = True\n    assert pp.doprint(sqrt_expr) == \"\u221ax\"\n\n    # Test imaginary_unit setting\n    pp._settings['imaginary_unit'] = 'j'\n    imag_expr = x + I\n    assert pp.doprint(imag_expr) == \"x + j\"\n\n    # Test mat_symbol_style setting\n    pp._settings['mat_symbol_style'] = \"bold\"\n    matrix_expr = MatrixSymbol('A', 2, 2)\n    assert pp.doprint(matrix_expr) == \"\ud835\udc00\"\n    pp._settings['mat_symbol_style'] = \"plain\"\n    assert pp.doprint(matrix_expr) == \"A\"\n", "def test_pretty_function_with_invalid_imaginary_unit():\n    # Test case for invalid imaginary unit type\n    expr = 1 + I\n    raises(TypeError, lambda: pretty(expr, imaginary_unit=1))\n    raises(ValueError, lambda: pretty(expr, imaginary_unit=\"invalid\"))\n", "def test_prettyprinter_initialization():\n    # Test if PrettyPrinter initializes with correct settings\n    pp_default = PrettyPrinter()\n    assert pp_default._settings[\"order\"] is None\n    assert pp_default._settings[\"full_prec\"] == \"auto\"\n    assert pp_default._settings[\"use_unicode\"] is None\n    assert pp_default._settings[\"wrap_line\"] is True\n    assert pp_default._settings[\"num_columns\"] is None\n    assert pp_default._settings[\"use_unicode_sqrt_char\"] is True\n    assert pp_default._settings[\"root_notation\"] is True\n    assert pp_default._settings[\"mat_symbol_style\"] == \"plain\"\n    assert pp_default._settings[\"imaginary_unit\"] == \"i\"\n    assert pp_default._settings[\"perm_cyclic\"] is True\n\n    # Test if PrettyPrinter raises exceptions with incorrect settings\n    with raises(TypeError):\n        PrettyPrinter({\"imaginary_unit\": 1})\n    with raises(ValueError):\n        PrettyPrinter({\"imaginary_unit\": \"k\"})\n", "def test_pretty_not_and_or():\n    # Testing pretty printing of Not, And, Or with different inputs\n    assert pretty(Not(x)) == 'Not(x)'\n    assert upretty(Not(x)) == '\u00acx'\n\n    assert pretty(And(x, y, z)) == 'And(x, y, z)'\n    assert upretty(And(x, y, z)) == 'x \u2227 y \u2227 z'\n\n    assert pretty(Or(x, y, z)) == 'Or(x, y, z)'\n    assert upretty(Or(x, y, z)) == 'x \u2228 y \u2228 z'\n\n    assert pretty(Not(And(x, y, z))) == 'Not(And(x, y, z))'\n    assert upretty(Not(And(x, y, z))) == '\u00ac(x \u2227 y \u2227 z)'\n\n    assert pretty(Not(Or(x, y, z))) == 'Not(Or(x, y, z))'\n    assert upretty(Not(Or(x, y, z))) == '\u00ac(x \u2228 y \u2228 z)'\n\n    assert pretty(And(Not(x), y, z)) == 'And(Not(x), y, z)'\n    assert upretty(And(Not(x), y, z)) == '\u00acx \u2227 y \u2227 z'\n\n    assert pretty(Or(Not(x), y, z)) == 'Or(Not(x), y, z)'\n    assert upretty(Or(Not(x), y, z)) == '\u00acx \u2228 y \u2228 z'\n", "def test_prettyprinter_imaginary_unit_settings():\n    # Verify that PrettyPrinter raises correct errors when invalid settings for imaginary_unit are provided\n    with raises(TypeError):\n        PrettyPrinter({\"imaginary_unit\": 1})\n\n    with raises(ValueError):\n        PrettyPrinter({\"imaginary_unit\": \"k\"})\n\n    # Valid settings should not raise errors\n    try:\n        pp = PrettyPrinter({\"imaginary_unit\": \"i\"})\n        assert pp._settings['imaginary_unit'] == \"i\"\n        pp = PrettyPrinter({\"imaginary_unit\": \"j\"})\n        assert pp._settings['imaginary_unit'] == \"j\"\n    except Exception:\n        assert False, \"PrettyPrinter raised an unexpected error\"\n"], "sample_1141": ["def test_Expr_methods():\n    e = x + x**2 + y\n    assert e.as_coeff_add(x) == (y, (x + x**2,))\n    assert e.as_coeff_mul(x) == (1, (e,))\n    assert e.as_base_exp() == (e, S.One)\n    assert e.normal() == e\n\n    assert e.extract_multiplicatively(x) == x*(1 + x + y/x)\n    assert e.extract_additively(x) == x + x**2 + y - x\n    assert e.equals(x + x**2 + y)\n    assert not e.equals(x + x**2 + 2*y)\n    assert e.as_ordered_factors() == [e]\n    assert e.as_ordered_terms() == [x**2, x, y]\n    assert e.as_coefficients_dict() == {1: y, x: 1, x**2: 1}\n    assert e.as_numer_denom() == (e, S.One)\n    assert e.is_constant(x) == False\n    assert e.is_constant() == False\n    assert e.is_number == False\n    assert e._eval_is_positive() == None\n    assert e._eval_is_negative() == None\n    assert e.series(x, 0, 2) == x + x**2 + O(x**3)\n    assert e.series(x, 1, 2) == x + x**2 + y\n    assert e.lseries(x, 0) == iter([x, x**2, y, O(x**3)])\n    assert e.nseries(x, 0, 2) == x + x**2 + O(x**3)\n    assert e.as_leading_term(x) == x\n    assert e.leadterm(x) == (x, 1)\n    assert e.taylor_term(2, x) == x**2 / 2\n    assert e.simplify() == x + x**2 + y\n    assert e.factor() == x + x**2 + y\n    assert e.expand() == x + x**2 + y\n    assert e.collect(x) == x + x**2 + y\n    assert e.together() == x + x**2 + y\n    assert e.ratsimp() == x + x**2 + y\n    assert e.trigsimp() == x + x**2 + y", "def test_Expr_diff_wrt():\n    class MyScalar(Expr):\n        _diff_wrt = True\n\n    class MySymbol(Expr):\n        _diff_wrt = True\n        is_scalar = False\n\n    e = Expr()\n    assert e._diff_wrt is False\n\n    scalar = MyScalar()\n    assert scalar._diff_wrt is True\n    assert scalar.diff(scalar) == 1\n\n    symbol = MySymbol()\n    assert symbol._diff_wrt is True\n    assert symbol.diff(symbol) == Derivative(symbol, symbol)\n", "def test_matrixsymbol_properties():\n    # Testing various properties and methods of MatrixSymbol\n    from sympy import conjugate, trace\n\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', n, n)\n\n    # Test conjugate\n    assert conjugate(A) == A.applyfunc(conjugate)\n\n    # Test trace\n    assert trace(C).is_commutative\n\n    # Test add with ZeroMatrix\n    assert A + ZeroMatrix(n, m) == A\n\n    # Test mul with Identity\n    assert C * Identity(n) == C\n\n    # Test subs with expressions involving MatrixSymbols\n    expr = A + B\n    assert expr.subs(A, C) == C + B\n\n    # Test differentiation with respect to MatrixSymbol\n    assert diff(A[0, 0], A[0, 0]) == 1\n    assert diff(A[0, 0], A[1, 1]) == 0\n\n    # Test inverse of a symbolically defined matrix\n    D = MatrixSymbol('D', 2, 2)\n    assert D.inv() == D**-1\n\n    # Test should handle subs correctly involving inverse\n    assert (D.inv()*D).subs(D, Matrix([[1, 2], [3, 4]])) == Identity(2)\n\n    # Test pow of MatrixSymbol\n    assert (C**2).shape == (n, n)\n    assert (C**3).shape == (n, n)\n", "def test_matrix_symbol_arithmetics():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n    E = MatrixSymbol('E', 2, 2)\n\n    # Test addition of MatrixSymbol instances\n    expr = A + B + C\n    assert expr == MatAdd(A, B, C)\n\n    # Test multiplication of MatrixSymbol instances\n    expr = A * B * C\n    assert expr == MatMul(A, B, C)\n\n    # Test subtraction of MatrixSymbol instances\n    expr = A - B - C\n    assert expr == MatAdd(A, MatMul(S.NegativeOne, B), MatMul(S.NegativeOne, C))\n\n    # Test scalar multiplication and division\n    expr = 2 * A\n    assert expr == MatMul(2, A)\n    expr = A / 2\n    assert expr == MatMul(Rational(1, 2), A)\n\n    # Test combinations of operations\n    expr = 2 * A + B - C * D\n    assert expr == MatAdd(MatMul(2, A), B, MatMul(S.NegativeOne, MatMul(C, D)))\n\n    expr = (A + B) * (C - D) + E\n    assert expr == MatAdd(MatMul(MatAdd(A, B), MatAdd(C, MatMul(S.NegativeOne, D))), E)\n\n    # Ensure simplification occurs when appropriate\n    assert (A + A) == MatMul(2, A)\n    assert (A - A) == ZeroMatrix(2, 2)\n", "def test_expr_methods():\n    # Testing basic Expr methods like __add__, __sub__, __mul__, etc.\n    a, b = symbols('a b')\n    expr1 = a + b\n    expr2 = a * b\n\n    # Testing __add__ and __radd__\n    assert expr1 == Add(a, b)\n    assert a + b == b + a\n    assert a + 1 == Add(a, 1)\n    assert 1 + a == Add(1, a)\n\n    # Testing __sub__ and __rsub__\n    assert a - b == Add(a, -b)\n    assert 1 - a == Add(1, -a)\n    assert a - 1 == Add(a, -1)\n    assert (a - b) - (b - a) == a - b - b + a\n\n    # Testing __mul__ and __rmul__\n    assert expr2 == Mul(a, b)\n    assert a * b == b * a\n    assert a * 2 == Mul(a, 2)\n    assert 2 * a == Mul(2, a)\n\n    # Testing __div__ and __rdiv__\n    assert a / b == Mul(a, Pow(b, -1))\n    assert 1 / a == Pow(a, -1)\n    assert a / 1 == a\n    assert (a / b) / (b / a) == a / b * a / b\n\n    # Testing __pow__ and __rpow__\n    assert a ** b == Pow(a, b)\n    assert 2 ** a == Pow(2, a)\n    assert a ** 2 == Pow(a, 2)\n\n    # Testing __mod__ and __rmod__\n    assert a % b == Mod(a, b)\n    assert a % 2 == Mod(a, 2)\n    assert 2 % a == Mod(2, a)\n\n    # Testing equality\n    assert expr1 == expr1\n    assert expr1 != expr2\n    assert a == a\n    assert a != b\n"], "sample_1142": ["def test_matrix_expr_operations():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, p)\n    D = MatrixSymbol('D', n, l)\n    \n    # Test matrix addition\n    assert (A + A).shape == (n, m)\n    assert (A + A).doit() == MatAdd(A, A).doit()\n    assert (A + ZeroMatrix(n, m)).doit() == A\n\n    # Test matrix subtraction\n    assert (A - A).shape == (n, m)\n    assert (A - A).doit() == ZeroMatrix(n, m)\n    \n    # Test matrix multiplication\n    assert (A * B).shape == (n, l)\n    assert (A * B).doit() == MatMul(A, B).doit()\n    assert (A * Identity(m)).doit() == A\n\n    # Test matrix exponentiation\n    E = MatrixSymbol('E', n, n)\n    assert (E ** 2).shape == (n, n)\n    assert (E ** 2).doit() == MatPow(E, 2).doit()\n    assert (Identity(n) ** 2).doit() == Identity(n)\n\n    # Test matrix negation\n    assert (-A).shape == (n, m)\n    assert (-A).doit() == MatMul(S.NegativeOne, A).doit()\n\n    # Test absolute value raises NotImplementedError\n    raises(NotImplementedError, lambda: abs(A))\n\n    # Test division by scalar\n    assert (A / 2).shape == (n, m)\n    assert (A / 2).doit() == MatMul(A, S.Half).doit()\n    \n    # Test matrix element-wise operations\n    assert A[0, 0] != A[1, 0]\n    assert A[0, 0] * B[0, 0] == B[0, 0] * A[0, 0]\n    \n    # Test matrix inversion for square matrix\n    F = MatrixSymbol('F', n, n)\n    assert F.inverse().shape == (n, n)\n    assert F.inverse().doit() == Inverse(F).doit()\n    raises(NonSquareMatrixError, lambda: A.inverse())\n\n    # Test", "def test_applyfunc():\n    from sympy import Function\n    f = Function('f')\n    A = MatrixSymbol('A', 2, 2)\n    B = A.applyfunc(f)\n    \n    assert B.shape == (2, 2)\n    assert B[0, 0] == f(A[0, 0])\n    assert B[0, 1] == f(A[0, 1])\n    assert B[1, 0] == f(A[1, 0])\n    assert B[1, 1] == f(A[1, 1])\n\n    raises(NotImplementedError, lambda: A.applyfunc(lambda x: x**2))\n", "def test_matrixexpr_inheritance():\n    assert isinstance(A, MatrixExpr)\n    assert isinstance(A, Expr)\n    assert issubclass(MatrixSymbol, MatrixExpr)\n    assert issubclass(MatrixSymbol, Expr)\n\n    class CustomMatrix(MatrixExpr):\n            self.name = name\n            self.shape = (n, m)\n            \n        @property\n            return (self.name, *self.shape)\n\n            return Symbol(f'{self.name}_{i}_{j}')\n\n    CM = CustomMatrix('CM', 2, 2)\n    assert CM.shape == (2, 2)\n    assert CM[0, 0] == Symbol('CM_0_0')\n    assert isinstance(CM, MatrixExpr)\n    assert isinstance(CM, Expr)\n    \n    expr = CM + Identity(2)\n    assert isinstance(expr, MatAdd)\n    assert expr.shape == (2, 2)\n", "def test_applyfunc():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n\n        return x**2\n\n    # Test applying a function element-wise\n    applied_matrix = A.applyfunc(square)\n    assert applied_matrix.shape == A.shape\n    assert isinstance(applied_matrix, MatrixExpr)\n\n    # Test as_explicit with applyfunc\n    explicit_applied = applied_matrix.as_explicit()\n    expected_explicit = ImmutableMatrix([\n        [A[0, 0]**2, A[0, 1]**2],\n        [A[1, 0]**2, A[1, 1]**2]\n    ])\n    assert explicit_applied == expected_explicit\n\n    # Test function application on specific elements\n    assert applied_matrix[0, 0] == A[0, 0]**2\n    assert applied_matrix[1, 1] == A[1, 1]**2\n\n    # Ensure other methods are still functional with applied matrix\n    assert (applied_matrix + B).shape == (2, 2)\n    assert (applied_matrix * C).shape == (2, 2)\n", "def test_as_mutable():\n    Z = MatrixSymbol('Z', 2, 3)\n    assert Z.as_mutable() == Matrix([\n        [Z[0, 0], Z[0, 1], Z[0, 2]],\n        [Z[1, 0], Z[1, 1], Z[1, 2]],\n    ])\n    raises(ValueError, lambda: A.as_mutable())\n"], "sample_1143": ["def test_comp():\n    # Test cases for comp() function\n    assert comp(1, 1) == True\n    assert comp(1, 2) == False\n    assert comp(1.0000000001, 1, 1e-9) == True\n    assert comp(1.0000000001, 1, 1e-10) == False\n    assert comp(1 + I, 1 + I) == True\n    assert comp(1 + I, 1 + 2*I) == False\n    assert comp(1 + 0.0000000001*I, 1, 1e-9) == True\n    assert comp(1 + 0.0000000001*I, 1, 1e-10) == False\n    assert comp(1, \"1\") == False\n    assert comp(S(1), \"1\") == True\n    assert comp(S(1) + S(\"1e-10\"), \"1\") == True\n    assert comp(S(1) + S(\"1e-9\"), \"1\") == False\n    raises(ValueError, lambda: comp(S(1), \"a\"))\n    raises(ValueError, lambda: comp(S(1), \"1a\"))\n    raises(ValueError, lambda: comp(\"1\", \"1\"))\n", "def test_comp_extended():\n    # More tests for comp function with various combinations\n    assert comp(sqrt(3).n(5), 1.73205, 1e-5)\n    assert not comp(sqrt(3).n(5), 1.73204, 1e-5)\n    assert comp(sqrt(3).n(5), 1.732051, 1e-5)\n    assert not comp(sqrt(3).n(5), 1.73206, 1e-5)\n    assert comp((sqrt(2) + sqrt(3)*I).n(3), 1.41 + 1.73*I, .1)\n    assert not comp((sqrt(2) + sqrt(3)*I).n(3), 1.41 + 1.75*I, .1)\n    assert comp((sqrt(5) + sqrt(7)*I).n(4), 2.2361 + 2.6458*I, .0001)\n    assert not comp((sqrt(5) + sqrt(7)*I).n(4), 2.2360 + 2.6458*I, .0001)\n    assert not comp((sqrt(5) + sqrt(7)*I).n(4), 2.2361 + 2.6457*I, .0001)\n    assert comp((sqrt(5) + sqrt(7)*I).n(4), 2.2361 + 2.6458*I, .0001)\n    assert not comp((sqrt(5) + sqrt(7)*I).n(4), 2.2362 + 2.6458*I, .0001)\n    assert not comp((sqrt(5) + sqrt(7)*I).n(4), 2.2361 + 2.6459*I, .0001)\n    assert comp((sqrt(7) + sqrt(11)*I).n(4), 2.6458 + 3.3166*I, .0001)\n    assert not comp((sqrt(7) + sqrt(11)*I).n(4), 2.6457 + 3.3166*I, .0001)\n    assert not comp((sqrt(7) + sqrt(11)*I).n(4), 2.6458", "def test_issue_2785():\n    assert Pow(0, 0, evaluate=False) == 1\n    assert Pow(oo, 0, evaluate=False) == 1\n    assert Pow(-oo, 0, evaluate=False) == 1\n    assert Pow(S.ComplexInfinity, 0, evaluate=False) == 1\n    assert Pow(S.NaN, 0, evaluate=False) == 1\n    assert Pow(0, S.ComplexInfinity, evaluate=False) == 0\n    assert Pow(0, -S.ComplexInfinity, evaluate=False) == oo\n    assert Pow(oo, S.ComplexInfinity, evaluate=False) == oo\n    assert Pow(oo, -S.ComplexInfinity, evaluate=False) == 0\n    assert Pow(-oo, S.ComplexInfinity, evaluate=False) == oo + oo*I\n    assert Pow(-oo, -S.ComplexInfinity, evaluate=False) == 0\n    assert Pow(S.ComplexInfinity, S.ComplexInfinity, evaluate=False) == S.ComplexInfinity\n    assert Pow(S.ComplexInfinity, -S.ComplexInfinity, evaluate=False) == S.Zero\n", "def test_comp_types():\n    # Test comp function with various types\n    assert comp(1, 1.0)\n    assert comp(1, 1)\n    assert comp(1.0, 1.0)\n    assert comp(1.0, 1)\n    assert not comp(1, '1')\n    assert not comp(1.0, '1.0')\n    raises(ValueError, lambda: comp(1, 'a'))\n    raises(ValueError, lambda: comp('a', 1))\n    assert comp(S.Half, 0.5)\n    assert not comp(S.Half, 0.50001)\n    assert comp(S.Half, 1/2)\n    assert not comp(S.Half, 1/3)\n    assert comp(2 + 2*I, 2 + 2j)\n    assert not comp(2 + 2*I, 2 + 3j)\n    assert not comp(2 + 3*I, 2 + 2j)\n    assert comp(1 + sqrt(2), 2.41421356237, tol=1e-11)\n    assert not comp(1 + sqrt(2), 2.41421356236, tol=1e-11)\n    assert comp(0, 0)\n    assert not comp(0, 1)\n", "def test_issue_2888():\n    assert sqrt(Rational(4, 9)) == Rational(2, 3)\n    assert sqrt(Rational(1, 4)) == Rational(1, 2)\n    assert sqrt(Rational(1, 9)) == Rational(1, 3)\n    assert sqrt(Rational(4, 25)) == Rational(2, 5)\n    assert sqrt(Rational(9, 16)) == Rational(3, 4)\n"], "sample_1144": ["def test_split_super_sub_edge_cases():\n    # Test with only superscripts\n    assert split_super_sub(\"x^a\") == (\"x\", [\"a\"], [])\n    assert split_super_sub(\"x^a^b\") == (\"x\", [\"a\", \"b\"], [])\n    assert split_super_sub(\"x__a\") == (\"x\", [\"a\"], [])\n    assert split_super_sub(\"x__a__b\") == (\"x\", [\"a\", \"b\"], [])\n\n    # Test with mixed superscripts and subscripts\n    assert split_super_sub(\"x^a_b\") == (\"x\", [\"a\"], [\"b\"])\n    assert split_super_sub(\"x__a_b\") == (\"x\", [\"a\"], [\"b\"])\n    assert split_super_sub(\"x^a__b\") == (\"x\", [\"a\", \"b\"], [])\n    assert split_super_sub(\"x__a^b\") == (\"x\", [\"a\", \"b\"], [])\n    assert split_super_sub(\"x^a^b_c_d\") == (\"x\", [\"a\", \"b\"], [\"c\", \"d\"])\n    assert split_super_sub(\"x__a__b_c_d\") == (\"x\", [\"a\", \"b\"], [\"c\", \"d\"])\n\n    # Test with symbols containing digits in superscripts and subscripts\n    assert split_super_sub(\"x_1^2\") == (\"x\", [\"2\"], [\"1\"])\n    assert split_super_sub(\"x_12^34\") == (\"x\", [\"34\"], [\"12\"])\n    assert split_super_sub(\"x_1__2\") == (\"x\", [\"2\"], [\"1\"])\n    assert split_super_sub(\"x_12__34\") == (\"x\", [\"34\"], [\"12\"])\n\n    # Test edge cases with invalid combinations (should not raise exceptions)\n    assert split_super_sub(\"x_^^a\") == (\"x\", [\"^a\"], [\"\"])\n    assert split_super_sub(\"x__^^a\") == (\"x\", [\"^a\"], [\"\"])\n    assert split_super_sub(\"x_^_a\") == (\"x\", [\"\"], [\"_a\"])\n", "def test_split_super_sub_edge_cases():\n    # Edge case with only superscripts\n    assert split_super_sub(\"a^b^c^d\") == (\"a\", [\"b\", \"c\", \"d\"], [])\n    assert split_super_sub(\"__a__b__c__d\") == (\"\", [\"a\", \"b\", \"c\", \"d\"], [])\n\n    # Edge case with non-alphanumeric characters\n    assert split_super_sub(\"a_1_!_@_#\") == (\"a\", [], [\"1\", \"!\", \"@\", \"#\"])\n    assert split_super_sub(\"a^1^!^@^#\") == (\"a\", [\"1\", \"!\", \"@\", \"#\"], [])\n\n    # Edge case with mixed alphanumeric and special characters\n    assert split_super_sub(\"a_1^b_!^@_#\") == (\"a\", [\"b\", \"@\"], [\"1\", \"!\", \"#\"])\n    assert split_super_sub(\"a__1__^b__!__@__#\") == (\"a\", [\"1\", \"b\", \"!\", \"@\", \"#\"], [])\n\n    # Empty string edge case\n    assert split_super_sub(\"\") == (\"\", [], [])\n\n    # Edge case with number at the end of name\n    assert split_super_sub(\"var123\") == (\"var\", [], [\"123\"])\n\n    # Single character name with sub and superscripts\n    assert split_super_sub(\"a_b^c\") == (\"a\", [\"c\"], [\"b\"])\n", "def test_split_super_sub_edge_cases():\n    # Test with only superscripts\n    assert split_super_sub(\"x^1^2^3\") == (\"x\", [\"1\", \"2\", \"3\"], [])\n    assert split_super_sub(\"beta__sup__sub\") == (\"beta\", [\"sup\", \"sub\"], [])\n\n    # Test with mixed positions of superscripts and subscripts\n    assert split_super_sub(\"beta_1^2_3^4\") == (\"beta\", [\"2\", \"4\"], [\"1\", \"3\"])\n    assert split_super_sub(\"alpha__beta_gamma__delta\") == (\"alpha\", [\"beta\", \"delta\"], [\"gamma\"])\n\n    # Test with numbers and special characters\n    assert split_super_sub(\"a1_b2^3_4^5\") == (\"a\", [\"3\", \"5\"], [\"1\", \"b2\", \"4\"])\n    assert split_super_sub(\"x_!_@^#_^$\") == (\"x\", [\"#\", \"$\"], [\"!\", \"@\"])\n    assert split_super_sub(\"x__!_@__#__$\") == (\"x\", [\"!\", \"#\"], [\"@\", \"$\"])\n\n    # Test when name is empty but has subs and superscripts\n    assert split_super_sub(\"_1^2\") == (\"\", [\"2\"], [\"1\"])\n    assert split_super_sub(\"__sup_sub\") == (\"\", [\"sup\"], [\"sub\"])\n", "def test_split_super_sub_edge_cases():\n    # Empty string\n    assert split_super_sub(\"\") == (\"\", [], [])\n    \n    # Only superscripts\n    assert split_super_sub(\"x^a^b^c\") == (\"x\", [\"a\", \"b\", \"c\"], [])\n    assert split_super_sub(\"x__a__b__c\") == (\"x\", [\"a\", \"b\", \"c\"], [])\n    \n    # Symbols with no underscores or hats\n    assert split_super_sub(\"alpha\") == (\"alpha\", [], [])\n    \n    # Consecutive underscores and hats\n    assert split_super_sub(\"x^^\") == (\"x\", [\"\"], [])\n    assert split_super_sub(\"x__\") == (\"x\", [\"\"], [])\n    assert split_super_sub(\"x_^_^_\") == (\"x\", [\"\", \"\"], [\"\"])\n    \n    # Mixed types in subscripts and superscripts\n    assert split_super_sub(\"a_1^b_2^c\") == (\"a\", [\"b\", \"c\"], [\"1\", \"2\"])\n    \n    # Complex Unicode characters\n    assert split_super_sub(\"\u03c9\ud835\udfd9\ud835\udfda^\u03b1\u03b2\") == (\"\u03c9\", [\"\u03b1\u03b2\"], [\"\ud835\udfd9\ud835\udfda\"])\n    assert split_super_sub(\"\ud835\udefc_\ud835\udefd^\ud835\udefe\") == (\"\ud835\udefc\", [\"\ud835\udefe\"], [\"\ud835\udefd\"])\n    \n    # Name ending with digits followed by subscripts and superscripts\n    assert split_super_sub(\"var123_x^y_z\") == (\"var\", [\"y\"], [\"123\", \"x\", \"z\"])\n", "def test_requires_partial_with_mixed_symbols():\n    x, y, z, t, nu, mu = symbols('x y z t nu mu')\n    n = symbols('n', integer=True)\n    \n    # Mixed integer and non-integer symbols\n    f = x * y * z\n    assert requires_partial(Derivative(f, x)) is True\n    assert requires_partial(Derivative(f, y)) is True\n    assert requires_partial(Derivative(f, z)) is True\n    assert requires_partial(Derivative(f, x, y)) is True\n    assert requires_partial(Derivative(f, x, z)) is True\n    assert requires_partial(Derivative(f, y, z)) is True\n\n    # Mixed function with integer and non-integer symbols\n    f = besselj(nu, x) * legendre(n, y) * bell(n, z)\n    assert requires_partial(Derivative(f, x)) is True\n    assert requires_partial(Derivative(f, nu)) is True\n    assert requires_partial(Derivative(f, y)) is False\n    assert requires_partial(Derivative(f, n)) is False\n    assert requires_partial(Derivative(f, z)) is False\n\n    # Mixed free symbols and explicit variables\n    expr = Derivative(x * y, z)\n    assert requires_partial(expr) is False\n    assert requires_partial(Derivative(expr, x)) is False\n    assert requires_partial(Derivative(expr, y)) is False\n\n    f = symbols('f', cls=Function)\n    expr = Derivative(f(x, y, t), x, y, z)\n    assert requires_partial(expr) is True\n    assert requires_partial(Derivative(expr, t)) is True\n    assert requires_partial(Derivative(expr, nu)) is True\n    assert requires_partial(Derivative(expr, n)) is False\n"], "sample_1145": ["def test_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[2, 2], Q.symmetric(X)) == X[2, 2]\n    assert refine(X[1, 2], Q.symmetric(X)) == X[1, 2]\n    assert refine(X[2, 1], Q.symmetric(X)) == X[1, 2]\n", "def test_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[2, 2], Q.symmetric(X)) == X[2, 2]\n    assert refine(X[0, 2], Q.symmetric(X)) == X[2, 0]\n    assert refine(X[0, 1], ~Q.symmetric(X)) == X[0, 1]  # no change when not symmetric\n", "def test_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[2, 1], Q.symmetric(X)) == X[1, 2]\n    assert refine(X[1, 2], Q.symmetric(X)) == X[1, 2]\n", "def test_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[2, 2], Q.symmetric(X)) == X[2, 2]\n    Y = MatrixSymbol('Y', 2, 2)\n    assert refine(Y[0, 1], Q.symmetric(Y)) == Y[0, 1]\n    assert refine(Y[1, 0], Q.symmetric(Y)) == Y[0, 1]\n    assert refine(Y[1, 1], Q.symmetric(Y)) == Y[1, 1]\n", "def test_refine_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    Y = MatrixSymbol('Y', 3, 3)\n    \n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[0, 2], Q.symmetric(X)) == X[0, 2]\n    assert refine(X[2, 1], Q.symmetric(X)) == X[1, 2]\n    \n    # Test with non-symmetric matrices\n    assert refine(X[0, 1], ~Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], ~Q.symmetric(X)) == X[1, 0]\n    \n    # Test with assumptions involving multiple matrices\n    assert refine(Y[0, 1] + X[1, 0], Q.symmetric(X) & Q.symmetric(Y)) == Y[0, 1] + X[0, 1]\n    assert refine(Y[2, 0] * X[0, 2], Q.symmetric(X) & Q.symmetric(Y)) == Y[0, 2] * X[0, 2]\n"], "sample_1146": ["def test_latex_ComplexRegion_conjugate():\n    # Test for ComplexRegion with conjugate\n    from sympy.sets import ComplexRegion\n    from sympy import Interval, conjugate\n\n    x, y, z = symbols('x y z')\n    \n    region = ComplexRegion(Interval(1, 2) * Interval(3, 4))\n    assert latex(conjugate(region)) == r'\\left\\{z\\; \\middle|\\; z \\in \\left[1, 2\\right] \\times \\left[3, 4\\right] \\right\\}'\n\n    region_polar = ComplexRegion(Interval(0, 1) * Interval(0, 2*pi), polar=True)\n    assert latex(conjugate(region_polar)) == r'\\left\\{\\overline{r \\left(i \\sin{\\left(\\theta \\right)} + \\cos{\\left(\\theta \\right)}\\right)}\\; \\middle|\\; r, \\theta \\in \\left[0, 1\\right] \\times \\left[0, 2 \\pi\\right) \\right\\}'\n", "def test_latex_escape_strings():\n    assert latex(latex_escape(\"a&b\")) == r\"\\text{a\\&b}\"\n    assert latex(latex_escape(\"x^y\")) == r\"\\text{x\\textasciicircum y}\"\n    assert latex(latex_escape(\"%value\")) == r\"\\text{\\%value}\"\n    assert latex(latex_escape(\"cost\\\\100\")) == r\"\\text{cost\\textbackslash 100}\"\n    assert latex(latex_escape(\"~home~\")) == r\"\\text{\\textasciitilde home\\textasciitilde}\"\n    assert latex(latex_escape(\"price_1\")) == r\"\\text{price\\_1}\"\n    assert latex(latex_escape(\"{key:value}\")) == r\"\\text{\\{key:value\\}}\"\n", "def test_latex_subs_alternate_notations():\n    # Test different notations for Subs\n    expr = Subs(f(x + y), (x, y), (1, 2))\n    assert latex(expr) == r'\\left. f{\\left(x + y \\right)} \\right|_{\\substack{ x=1\\\\ y=2 }}'\n    \n    expr = Subs(g(x + y + z), (x, y, z), (1, 2, 3))\n    assert latex(expr) == r'\\left. g{\\left(x + y + z \\right)} \\right|_{\\substack{ x=1\\\\ y=2\\\\ z=3 }}'\n    \n    expr = Subs(h(x**2 + y**2), (x, y), (a, b))\n    assert latex(expr) == r'\\left. h{\\left(x^{2} + y^{2} \\right)} \\right|_{\\substack{ x=a\\\\ y=b }}'\n", "def test_latex_LatexPrinter():\n    # Test initializing LatexPrinter with various settings\n    printer = LatexPrinter({'mode': 'inline'})\n    assert printer._settings['mode'] == 'inline'\n    assert printer.doprint(x + y) == r'$x + y$'\n\n    printer = LatexPrinter({'itex': True})\n    assert printer._settings['itex']\n    assert printer.doprint(x + y) == r'$$x + y$$'\n\n    # Test valid modes\n    valid_modes = ['inline', 'plain', 'equation', 'equation*']\n    for mode in valid_modes:\n        printer = LatexPrinter({'mode': mode})\n        assert printer._settings['mode'] == mode\n\n    # Test invalid mode\n    raises(ValueError, lambda: LatexPrinter({'mode': 'invalid'}))\n\n    # Test valid imaginary units\n    valid_units = [None, \"i\", \"ri\", \"ti\", \"j\", \"rj\", \"tj\"]\n    for unit in valid_units:\n        printer = LatexPrinter({'imaginary_unit': unit})\n        assert printer._settings['imaginary_unit'] == unit\n\n    # Test invalid imaginary unit\n    raises(KeyError, lambda: LatexPrinter({'imaginary_unit': 'invalid'}))\n\n    # Test custom settings\n    printer = LatexPrinter({'symbol_names': {x: 'x_i'}, 'decimal_separator': 'comma'})\n    assert printer._settings['symbol_names'] == {x: 'x_i'}\n    assert printer._settings['decimal_separator'] == 'comma'\n    assert printer.doprint(x + y) == r'x_i + y'\n", "def test_latex_partial_derivatives():\n    f = Function('f')\n    g = Function('g')\n    x, y, z = symbols('x y z')\n    \n    # Test partial derivatives with non-standard variables\n    expr1 = Derivative(f(x, y), x)\n    expr2 = Derivative(f(x, y), y, x)\n    expr3 = Derivative(f(x, y, z), x, y, z)\n    expr4 = Derivative(g(x), x)\n    expr5 = Derivative(f(x, y)**2, x, y)\n\n    assert latex(expr1) == r'\\frac{\\partial}{\\partial x} f{\\left(x,y \\right)}'\n    assert latex(expr2) == r'\\frac{\\partial^{2}}{\\\\partial y \\partial x} f{\\left(x,y \\right)}'\n    assert latex(expr3) == r'\\frac{\\partial^{3}}{\\partial z \\partial y \\partial x} f{\\left(x,y,z \\right)}'\n    assert latex(expr4) == r'\\frac{\\partial}{\\partial x} g{\\left(x \\right)}'\n    assert latex(expr5) == r'\\frac{\\partial^{2}}{\\partial y \\partial x} \\left(f{\\left(x,y \\right)}\\right)^{2}'\n"], "sample_1147": ["def test_mathring():\n    expr = Symbol('mathring')\n    expr_mathring = Symbol('mathringMathring')\n    assert latex(expr) == r'\\mathring{mathring}'\n    assert latex(expr_mathring) == r'\\mathring{\\mathring{mathring}}'\n    # Testing combinations with another symbol\n    expr_xmathring = Symbol('xmathring')\n    expr_xmathringmathring = Symbol('xmathringMathring')\n    assert latex(expr_xmathring) == r'\\mathring{xmathring}'\n    assert latex(expr_xmathringmathring) == r'\\mathring{\\mathring{xmathring}}'\n", "def test_LatexPrinter_customization():\n    custom_settings = {\n        \"full_prec\": True,\n        \"fold_frac_powers\": True,\n        \"fold_func_brackets\": True,\n        \"fold_short_frac\": True,\n        \"inv_trig_style\": \"full\",\n        \"itex\": True,\n        \"ln_notation\": True,\n        \"long_frac_ratio\": 1.5,\n        \"mat_delim\": \"(\",\n        \"mat_str\": \"bmatrix\",\n        \"mode\": \"inline\",\n        \"mul_symbol\": \"times\",\n        \"order\": \"old\",\n        \"symbol_names\": {x: \"x_i\", y: \"y_j\"},\n        \"root_notation\": False,\n        \"mat_symbol_style\": \"bold\",\n        \"imaginary_unit\": \"j\",\n        \"gothic_re_im\": True,\n        \"decimal_separator\": \"comma\",\n        \"parenthesize_super\": False,\n        \"min\": -5,\n        \"max\": 5,\n    }\n\n    expr = (x + y)**Rational(1, 3) * exp(I*x) / sin(x)\n    assert latex(expr, **custom_settings) == r\"$\\left(x_i + y_j\\right)^{1/3} \\cdot e^{j x_i} / \\sin{x_i}$\"\n\n    matrix_expr = Matrix([[x + 1, y], [y, x - 1]])\n    assert latex(matrix_expr, **custom_settings) == r\"$\\left(\\begin{bmatrix}x_i + 1 & y_j\\\\y_j & x_i - 1\\end{bmatrix}\\right)$\"\n\n    trig_expr = asin(x) + acos(y)\n    assert latex(trig_expr, **custom_settings) == r\"$\\arcsin{x_i} + \\arccos{y_j}$\"\n", "def test_latex_AccumBounds_addition():\n    a = Symbol('a', real=True)\n    b = Symbol('b', real=True)\n    expr = AccumBounds(0, a) + AccumBounds(1, b)\n    assert latex(expr) == r\"\\left\\langle 0, a\\right\\rangle + \\left\\langle 1, b\\right\\rangle\"\n    expr2 = AccumBounds(0, a) + AccumBounds(1, b) + AccumBounds(-1, 2)\n    assert latex(expr2) == r\"\\left\\langle 0, a\\right\\rangle + \\left\\langle 1, b\\right\\rangle + \\left\\langle -1, 2\\right\\rangle\"\n", "def test_latexprinter_initialization():\n    # Test default settings\n    lp = LatexPrinter()\n    assert lp._settings['full_prec'] == False\n    assert lp._settings['fold_frac_powers'] == False\n    assert lp._settings['fold_func_brackets'] == False\n    assert lp._settings['fold_short_frac'] == None\n    assert lp._settings['inv_trig_style'] == \"abbreviated\"\n    assert lp._settings['itex'] == False\n    assert lp._settings['ln_notation'] == False\n    assert lp._settings['long_frac_ratio'] == None\n    assert lp._settings['mat_delim'] == \"[\"\n    assert lp._settings['mat_str'] == None\n    assert lp._settings['mode'] == \"plain\"\n    assert lp._settings['mul_symbol'] == None\n    assert lp._settings['order'] == None\n    assert lp._settings['symbol_names'] == {}\n    assert lp._settings['root_notation'] == True\n    assert lp._settings['mat_symbol_style'] == \"plain\"\n    assert lp._settings['imaginary_unit'] == \"i\"\n    assert lp._settings['gothic_re_im'] == False\n    assert lp._settings['decimal_separator'] == \"period\"\n    assert lp._settings['perm_cyclic'] == True\n    assert lp._settings['parenthesize_super'] == True\n    assert lp._settings['min'] == None\n    assert lp._settings['max'] == None\n\n    # Test custom settings\n    custom_settings = {\n        \"full_prec\": True,\n        \"fold_frac_powers\": True,\n        \"fold_func_brackets\": True,\n        \"fold_short_frac\": True,\n        \"inv_trig_style\": \"full\",\n        \"itex\": True,\n        \"ln_notation\": True,\n        \"long_frac_ratio\": 2,\n        \"mat_delim\": \"(\",\n        \"mat_str\": \"bmatrix\",\n        \"mode\": \"equation\",\n        \"mul_symbol\": \"dot\",\n        \"order\": \"lex\",\n        \"symbol_names\": {Symbol('x'): 'x_i'},\n        \"root_notation\": False,\n        \"mat_symbol_style\": \"bold\",\n        \"imaginary_unit\": \"j\",\n        \"gothic_re_im\": True,\n        \"decimal_separator\": \"comma\",\n        \"perm_cyclic\": False,\n        \"parenthesize_super\": False", "def test_latex_symbols_with_modifiers():\n    # Test symbols with multiple combined modifiers\n    assert latex(Symbol('xvecdotbold')) == r'\\boldsymbol{\\dot{\\vec{x}}}'\n    assert latex(Symbol('xbreveacutevec')) == r'\\vec{\\acute{\\breve{x}}}'\n    assert latex(Symbol('ymathringcheckprime')) == r'{\\check{\\mathring{y}}}\\''\n    assert latex(Symbol('zbargravebm')) == r'\\boldsymbol{\\grave{\\bar{z}}}'\n    assert latex(Symbol('thetatildehatnorm')) == r'\\left\\|{\\hat{\\tilde{\\theta}}}\\right\\|'\n    assert latex(Symbol('phidddotdddotabs')) == r'\\left|{\\dddot{\\dddot{\\phi}}}\\right|'\n    assert latex(Symbol('alphaacutevecvec')) == r'\\vec{\\vec{\\acute{\\alpha}}}'\n    assert latex(Symbol('betaboldprmbm')) == r'\\boldsymbol{\\boldsymbol{\\beta}}\\''\n    assert latex(Symbol('gammadotmathringmathring')) == r'\\mathring{\\mathring{\\dot{\\gamma}}}'\n    assert latex(Symbol('deltaddotdddotddot')) == r'\\ddot{\\ddot{\\dddot{\\delta}}}'\n    # Test symbols with Greek letter names and modifiers\n    assert latex(Symbol('alphadothat')) == r'\\hat{\\dot{\\alpha}}'\n    assert latex(Symbol('betacheckprime')) == r'{\\check{\\beta}}\\''\n    assert latex(Symbol('gammadotvec')) == r'\\vec{\\dot{\\gamma}}'\n    assert latex(Symbol('deltabrevebar')) == r'\\bar{\\breve{\\delta}}'\n"], "sample_1148": ["def test_from_index_summation():\n    i, j, k, l, N = symbols('i j k l N', integer=True)\n    A = MatrixSymbol(\"A\", N, N)\n    B = MatrixSymbol(\"B\", N, N)\n    C = MatrixSymbol(\"C\", N, N)\n    \n    expr1 = Sum(A[i, j] * B[j, k], (j, 0, N-1))\n    expr2 = Sum(A[j, i] * B[j, k], (j, 0, N-1))\n    expr3 = Sum(A[i, i], (i, 0, N-1))\n    expr4 = Sum(A[i, j] * B[k, j] * C[l, k], (j, 0, N-1), (k, 0, N-1))\n    \n    assert MatrixExpr.from_index_summation(expr1) == A * B\n    assert MatrixExpr.from_index_summation(expr2) == A.T * B\n    assert MatrixExpr.from_index_summation(expr3) == A.trace()\n    assert MatrixExpr.from_index_summation(expr4) == A * B.T * C.T\n\n    raises(ValueError, lambda: MatrixExpr.from_index_summation(Sum(A[i, j] * B[j, k], (j, 1, N-1))))\n", "def test_matrix_expr_properties():\n    A = MatrixSymbol('A', 3, 3)\n    assert A.is_Matrix\n    assert A.is_MatrixExpr\n    assert A.is_Identity is None\n    assert not A.is_Inverse\n    assert not A.is_Transpose\n    assert not A.is_ZeroMatrix\n    assert not A.is_MatAdd\n    assert not A.is_MatMul\n    assert not A.is_commutative\n    assert not A.is_number\n    assert not A.is_symbol\n    assert not A.is_scalar\n    assert isinstance(A.kind, MatrixKind)\n", "def test_matrixexpr_subtraction():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n\n    assert isinstance(A - B, MatAdd)\n    assert (A - B).shape == A.shape\n    assert isinstance(A - A - B, MatMul)\n\n    raises(ShapeError, lambda: A - B.T)\n    raises(TypeError, lambda: A - 1)\n    raises(TypeError, lambda: 5 - A)\n\n    assert A - ZeroMatrix(n, m) == A\n    assert ZeroMatrix(n, m) - A == -A\n    with raises(TypeError):\n        ZeroMatrix(n,m) - S.Zero\n", "def test_matrix_expr_operations():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n    I = Identity(3)\n\n    # Testing addition of matrices\n    assert (A + B).doit() == MatAdd(A, B).doit()\n    assert (A + B + C).doit() == MatAdd(A, B, C).doit()\n\n    # Testing multiplication of matrices\n    assert (A * B).doit() == MatMul(A, B).doit()\n    assert (A * B * C).doit() == MatMul(A, B, C).doit()\n\n    # Testing matrix power\n    assert (A**2).doit() == MatPow(A, 2).doit()\n    assert (A**3).doit() == MatPow(A, 3).doit()\n\n    # Testing commutativity properties\n    assert A * I == A\n    assert I * A == A\n\n    # Testing combinations of operations\n    expr = (A + B) * C\n    assert expr.doit() == MatMul(MatAdd(A, B), C).doit()\n\n    expr = A * (B + C)\n    assert expr.doit() == MatMul(A, MatAdd(B, C)).doit()\n\n    # Testing nested operations\n    expr = (A * B) + (B * C)\n    assert expr.doit() == MatAdd(MatMul(A, B), MatMul(B, C)).doit()\n\n    expr = A * (B * C)\n    assert expr.doit() == MatMul(A, MatMul(B, C)).doit()\n\n    # Testing scalar multiplication\n    assert (2 * A).doit() == MatMul(2, A).doit()\n    assert (A * 2).doit() == MatMul(A, 2).doit()\n\n    # Testing subtraction\n    assert (A - B).doit() == MatAdd(A, MatMul(S.NegativeOne, B)).doit()\n    assert (A - B - C).doit() == MatAdd(A, MatMul(S.NegativeOne, B), MatMul(S.NegativeOne, C)).doit()\n", "def test_matrixexpr_shape_property():\n    A = MatrixSymbol('A', 3, 3)\n    assert A.shape == (3, 3)\n    assert A.T.shape == (3, 3)\n    assert (A * A).shape == (3, 3)\n\n    B = MatrixSymbol('B', 2, 3)\n    assert B.shape == (2, 3)\n    assert B.T.shape == (3, 2)\n    assert (B.T * B).shape == (3, 3)\n    raises(ShapeError, lambda: A + B)\n"], "sample_1149": ["def test_sympify_integration():\n    assert S(1) == sympify(1)\n    assert S(\"x**2 + 2*x + 1\") == sympify(\"x**2 + 2*x + 1\")\n    assert S(1/2) == sympify(1/2)\n    assert S([1, 2, 3]) == sympify([1, 2, 3])\n", "compilation error", "def test_singleton_registry_call():\n    # Test that calling S with a value returns the sympified value\n    assert S(5) == sympify(5)\n    assert S('2/3') == Rational(2, 3)\n    assert S('x**2 + 2*x + 1') == sympify('x**2 + 2*x + 1')\n", "def test_sympify_shortcut():\n    # Test that S(1) calls sympify(1)\n    from sympy.core.sympify import sympify\n    assert S(1) == sympify(1)\n    assert S(\"x**2 + 1\") == sympify(\"x**2 + 1\")\n\n    # Ensure that S can handle different types\n    assert S(0.5) == sympify(0.5)\n    assert S(\"Rational(1, 2)\") == sympify(\"Rational(1, 2)\")\n", "def test_SingletonRegistry_sympify():\n    assert S(1) == sympify(1)\n    assert S(\"x**2\") == sympify(\"x**2\")\n    assert S(Rational(1, 2)) == sympify(Rational(1, 2))\n"], "sample_1150": ["def test_rationals():\n    R = S.Rationals\n    assert 1/2 in R\n    assert -1/2 in R\n    assert 2/3 in R\n    assert -2/3 in R\n    assert 0 in R\n    assert 1 not in R  # Rational numbers can be integers, but not strictly\n    assert 2 not in R\n    assert 1.5 not in R\n    assert pi not in R\n    assert I not in R\n    \n    ri = iter(R)\n    a, b, c, d, e, f, g, h, i, j, k, l = [next(ri) for _ in range(12)]\n    assert (a, b, c, d, e, f, g, h, i, j, k, l) == (0, 1, -1, S.Half, 2, Rational(-1, 2), -2, Rational(1, 3), 3, Rational(-1, 3), -3, Rational(2, 3))\n\n    assert R.intersect(Interval(0, 1)) == Intersection(S.Rationals, Interval(0, 1))\n    assert R.intersect(Interval(0, 1, True, True)) == Intersection(S.Rationals, Interval(0, 1, True, True))\n\n    assert R.boundary == S.Reals\n    assert R.is_open == False\n    assert R.is_closed == False\n\n    assert R.as_relational(x) == Contains(x, S.Rationals, evaluate=False)\n", "def test_Rationals_iterable():\n    it = iter(S.Rationals)\n    assert next(it) == 0\n    assert next(it) == 1\n    assert next(it) == -1\n    assert next(it) == S.Half\n    assert next(it) == 2\n    assert next(it) == Rational(-1, 2)\n    assert next(it) == -2\n    assert next(it) == Rational(1, 3)\n    assert next(it) == 3\n    assert next(it) == Rational(-1, 3)\n    assert next(it) == -3\n    assert next(it) == Rational(2, 3)\n    # Verify that the iterable continues correctly\n    assert next(it) == Rational(1, 4)\n    assert next(it) == Rational(4, 1)\n    assert next(it) == Rational(-1, 4)\n    assert next(it) == Rational(-4, 1)\n", "def test_Rationals_contains():\n    R = S.Rationals\n    assert 3/2 in R\n    assert Rational(5, 2) in R\n    assert -Rational(7, 3) in R\n    assert sqrt(2) not in R\n    assert pi not in R\n    assert (1, 2) not in R\n\n    # Test containment with symbolic expressions\n    n = Symbol('n', integer=True)\n    assert (n/2).is_rational is None  # Can't decide if n/2 is rational without more info\n    assert R.contains(n/2) is None\n\n    assert R.contains(n + S.Half) is None\n    assert R.contains(n/2 + S.Half) is None\n\n    # Test containment with non-Expr types\n    assert R.contains(\"test\") == False\n    assert R.contains(None) == False\n\n    # Test boundary conditions\n    assert R._contains(S.PositiveInfinity) == False\n    assert R._contains(S.NegativeInfinity) == False\n", "def test_rationals_contains():\n    R = S.Rationals\n    assert 0 in R\n    assert 1 in R\n    assert -1 in R\n    assert Rational(1, 2) in R\n    assert Rational(-1, 2) in R\n    assert 1.5 not in R  # float\n    assert sqrt(2) not in R  # irrational\n    assert I not in R  # imaginary\n    assert S.Pi not in R  # transcendental\n    r = symbols('r', rational=True)\n    assert R.contains(r) == Contains(r, R, evaluate=False)\n\n    # Check using sympy numbers\n    assert S(1) in R\n    assert S(-1) in R\n    assert S.Half in R\n    assert S.Zero in R\n\n    # Check symbolic expressions\n    assert R.contains(x + y).subs({x: Rational(1, 2), y: Rational(1, 3)}) == True\n    assert R.contains(x + y).subs({x: sqrt(2), y: Rational(1, 3)}) == False\n", "def test_Rationals_infinity_handling():\n    R = S.Rationals\n    assert oo not in R\n    assert -oo not in R\n    # Check that positive infinity and negative infinity are not in the iterables\n    rit = iter(R)\n    assert oo not in take(100, rit)\n    assert -oo not in take(100, rit)\n    assert 1 in R\n    assert -1 in R\n    assert 1/2 in R\n    assert -1/2 in R\n"], "sample_1151": ["def test_Mod_edge_cases():\n    # Test Mod with extremely large numbers\n    assert Mod(10**100, 3) == 1\n    assert Mod(3, 10**100) == 3\n\n    # Test Mod with negative numbers\n    assert Mod(-10**100, 3) == 2\n    assert Mod(-3, 10**100) == 10**100 - 3\n\n    # Test Mod with zero as the dividend\n    assert Mod(0, 10**100) == 0\n    assert Mod(0, -10**100) == 0\n\n    # Test Mod with one as the divisor\n    assert Mod(10**100, 1) == 0\n    assert Mod(-10**100, 1) == 0\n\n    # Test Mod with symbolic expressions\n    assert Mod(x**2 + 3*x + 2, x + 1) == x + 2\n    assert Mod(x**3 + 2*x**2 + x, x**2 + x) == x\n\n    # Test Mod with mixed types\n    assert Mod(5, 2.5) == 0.0\n    assert Mod(5.5, 2) == 1.5\n    assert Mod(5.5, 2.5) == 0.5\n    assert Mod(5.5, Rational(1, 2)) == 0.0\n", "def test_Mod_inequality():\n    # Test that Mod(x, y) correctly handles inequalities\n    a = Symbol('a', integer=True)\n    b = Symbol('b', integer=True, positive=True)\n    \n    assert Mod(a, b) >= 0\n    assert Mod(a, b) < b\n    \n    # Test with specific values\n    assert Mod(5, 3) < 3\n    assert Mod(-5, 3) >= 0\n    assert Mod(7, 4) >= 0\n    assert Mod(7, 4) < 4\n", "def test_Mod_simplifications():\n    # Test various simplifications and properties of Mod\n    assert Mod(x, 1) == 0\n    assert Mod(0, y) == 0\n    assert Mod(5, 2) == 1\n    assert Mod(-5, 2) == 1\n    assert Mod(5, -2) == -1\n    assert Mod(-5, -2) == -1\n    assert Mod(x + y, y) == x % y\n    assert Mod(x - y, y) == -y % y\n    assert Mod(3*x, x) == 0\n    assert Mod(x**3, x**2) == 0\n    assert Mod(x*y, y) == 0\n    assert Mod(x*y, x) == 0\n    assert Mod(2*x, x) == 0\n    assert Mod(2*x + y, x) == y % x\n    assert Mod(x + y, 1) == Mod(x, 1) + Mod(y, 1)\n    assert Mod(x + 1, x) == 1 % x\n    assert Mod(x**2 + x*y, x) == 0\n    assert Mod(3*x + 2*y, x) == 2*y % x\n    assert Mod(x + y + z, y) == (x + z) % y\n", "def test_Mod_special_cases():\n    # Test Mod with various special cases\n\n    # Mod with zero divisor should raise an exception\n    with raises(ZeroDivisionError):\n        Mod(1, 0)\n\n    # Mod with zero dividend should be zero\n    assert Mod(0, 2) == 0\n\n    # Mod with positive and negative numbers\n    assert Mod(5, -3) == -1\n    assert Mod(-5, 3) == 1\n\n    # Mod with negative dividend and divisor\n    assert Mod(-5, -3) == -2\n\n    # Mod with nan should return nan\n    assert Mod(nan, 1) is nan\n    assert Mod(1, nan) is nan\n    assert Mod(nan, nan) is nan\n\n    # Mod with infinity should return nan\n    from sympy import oo\n    assert Mod(oo, 1) is nan\n    assert Mod(-oo, 1) is nan\n    assert Mod(1, oo) == 1\n    assert Mod(1, -oo) == 1\n    assert Mod(oo, oo) is nan\n    assert Mod(-oo, -oo) is nan\n", "def test_Mod_with_negative_operands():\n    assert Mod(-10, 3) == 2\n    assert Mod(-10, -3) == -1\n    assert Mod(10, -3) == -2\n    assert Mod(x, -2) == Mod(x, 2)\n    assert Mod(-x, 3) == 3 - Mod(x, 3)\n    assert Mod(-x, -3) == -Mod(x, 3)\n    assert Mod(x - y, -3).subs({x: 10, y: 1}) == -2\n    assert Mod(x - y, -3).subs({x: -10, y: 1}) == -1\n    assert Mod(x - y, -3).subs({x: 10, y: -1}) == -1\n    assert Mod(x - y, -3).subs({x: -10, y: -1}) == -2\n"], "sample_1152": ["def test_powsimp_deep_base_exp():\n    x, y, z = symbols('x y z')\n    a, b = symbols('a b', positive=True)\n\n    expr = (x**a * y**b * z**(a+b))\n    assert powsimp(expr, combine='base', deep=True) == (x * z * y)**a * z**b\n    assert powsimp(expr, combine='exp', deep=True) == x**a * y**b * z**(a + b)\n    assert powsimp(expr, combine='all', deep=True) == (x * y * z)**(a + b)\n\n    expr = (x**(2*y) * y**(2*x) * z**(2*x + 2*y))\n    assert powsimp(expr, combine='base', deep=True) == (x**y * z**x * y**x)**2 * z**(2*y)\n    assert powsimp(expr, combine='exp', deep=True) == x**(2*y) * y**(2*x) * z**(2*x + 2*y)\n    assert powsimp(expr, combine='all', deep=True) == (x * y * z)**(2*x + 2*y)\n", "def test_powsimp_deep():\n    x, y, z = symbols('x y z')\n    expr = (x**y * y**z)**x * (x**y * y**z)**y\n    # Testing deep=True for powsimp should simplify the inner powers as well\n    assert powsimp(expr, deep=True) == (x**y * y**z)**(x + y)\n\n    f = Function('f')\n    expr = f(x**y * y**z)\n    assert powsimp(expr, deep=True) == f(x**y * y**z)\n\n    expr = f(x**y * y**z)\n    assert powsimp(expr, deep=False) == f(x**y * y**z)\n", "def test_powsimp_with_noncommutative_symbols():\n    A, B = symbols('A B', commutative=False)\n    assert powsimp(A**x * A**y * B**x * B**y, combine='all') == A**(x + y) * B**(x + y)\n    assert powsimp(A**x * B**x * A**y * B**y, combine='all') == A**x * B**x * A**y * B**y\n    assert powsimp(A**x * A**y * B**x * B**y, combine='exp') == A**(x + y) * B**(x + y)\n    assert powsimp(A**x * B**x * A**y * B**y, combine='exp') == A**x * B**x * A**y * B**y\n", "def test_powsimp_nested_exponents():\n    x, y, z = symbols('x,y,z')\n    assert powsimp((x**y)**z) == x**(y*z)\n    assert powsimp(((x**y)**z)**a) == x**(y*z*a)\n    assert powsimp((x**(y**z))**a) == x**(y**(z*a))\n    assert powsimp((x**y)**(z*a)) == x**(y*z*a)\n    assert powsimp(((x**y)**z)*(x**y)**(a*z)) == x**(y*z + y*a*z)\n    assert powsimp((x**(y**z))**(a*b)) == x**(y**(z*a*b))\n    assert powsimp((x**(y*z))**a) == x**(y*z*a)\n    assert powsimp((x**(y*z))**(a*b)) == x**(y*z*a*b)\n", "def test_powsimp_with_multiplicity():\n    from sympy.ntheory.factor_ import multiplicity\n    x, y, z, n = symbols('x,y,z,n')\n    assert powsimp(2**x * 4**y * 8**z) == 2**(x + 2*y + 3*z)\n    assert powsimp(2**x * 4**y * 8**z, force=True) == 2**(x + 2*y + 3*z)\n    assert powsimp(3**x * 9**y * 27**z) == 3**(x + 2*y + 3*z)\n    assert powsimp(3**x * 9**y * 27**z, force=True) == 3**(x + 2*y + 3*z)\n    assert powsimp((-2)**x * (-4)**y * (-8)**z) == (-2)**(x + 2*y + 3*z)\n    assert powsimp((-2)**x * (-4)**y * (-8)**z, force=True) == (-2)**(x + 2*y + 3*z)\n"], "sample_1153": ["def test_conjugate_transpose_properties():\n    x = Symbol('x', complex=True)\n    y = Symbol('y', real=True)\n    A = Matrix([[1, I], [I, 1]])\n    B = Matrix([[x, y], [conjugate(y), conjugate(x)]])\n    assert adjoint(A) == A.H\n    assert adjoint(B) == B.H\n    assert adjoint(A).H == A\n    assert adjoint(B).H == B\n    assert adjoint(transpose(A)) == conjugate(A)\n    assert adjoint(transpose(B)) == conjugate(B)\n    assert adjoint(conjugate(A)) == transpose(A)\n    assert adjoint(conjugate(B)) == transpose(B)\n    assert adjoint(adjoint(A)) == A\n    assert adjoint(adjoint(B)) == B\n    assert transpose(conjugate(adjoint(A))) == A\n    assert transpose(conjugate(adjoint(B))) == B\n", "def test_conjugate_derivative():\n    # Test conjugate derivative for real and imaginary variables\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    f = Function('f')\n\n    assert conjugate(f(x).diff(x)) == (conjugate(f(x)).diff(x))\n    assert conjugate(f(y).diff(y)) == -(conjugate(f(y)).diff(y))\n\n    # Check for higher order derivatives\n    assert conjugate(f(x).diff(x, 2)) == (conjugate(f(x)).diff(x, 2))\n    assert conjugate(f(y).diff(y, 2)) == (conjugate(f(y)).diff(y, 2))\n\n    # Mixed real and imaginary derivatives\n    assert conjugate(f(x + y).diff(x)) == (conjugate(f(x + y)).diff(x))\n    assert conjugate(f(x + y).diff(y)) == -(conjugate(f(x + y)).diff(y))\n", "def test_conjugate_with_matrices():\n    n, m = symbols('n m')\n    A = MatrixSymbol('A', n, m)\n    \n    B = Matrix([[1 + I, 2 - 2*I], [3*I, 4 - I]])\n    C = ImmutableMatrix([[1 + I, 2 - 2*I], [3*I, 4 - I]])\n    D = SparseMatrix([[1 + I, 0], [0, 4 - I]])\n\n    assert conjugate(A) == conjugate(A)\n    assert conjugate(B) == Matrix([[1 - I, 2 + 2*I], [-3*I, 4 + I]])\n    assert conjugate(C) == ImmutableMatrix([[1 - I, 2 + 2*I], [-3*I, 4 + I]])\n    assert conjugate(D) == SparseMatrix([[1 - I, 0], [0, 4 + I]])\n\n    X = FunctionMatrix(2, 2, Lambda((n, m), n + m*I))\n    assert conjugate(X) == Matrix([[0, 1 - I], [1 + I, 2 - 2*I]])\n\n    Y = ImmutableSparseMatrix([[1 + I, 0], [0, 4 - I]])\n    assert conjugate(Y) == ImmutableSparseMatrix([[1 - I, 0], [0, 4 + I]])\n\n    Z = MatrixSymbol('Z', 2, 2)\n    assert conjugate(Z) == conjugate(Z)\n", "def test_conjugate_derivative():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    z = Symbol('z')\n    f = Function('f')\n    \n    # Real variable\n    assert conjugate(Derivative(f(x), x)) == Derivative(conjugate(f(x)), x)\n    \n    # Imaginary variable\n    assert conjugate(Derivative(f(y), y)) == -Derivative(conjugate(f(y)), y)\n    \n    # Complex variable\n    assert conjugate(Derivative(f(z), z)) == Derivative(conjugate(f(z)), z)\n    \n    # Mixed expressions\n    expr = x + I*y\n    assert conjugate(Derivative(expr, x)) == Derivative(conjugate(expr), x)\n    assert conjugate(Derivative(expr, y)) == -I\n    assert conjugate(Derivative(expr, z)) == Derivative(conjugate(expr), z)\n", "def test_polar_lift():\n    from sympy import polar_lift, exp_polar, pi, I, sqrt\n\n    assert polar_lift(2) == 2 * exp_polar(0)\n    assert polar_lift(-2) == 2 * exp_polar(I * pi)\n    assert polar_lift(I) == exp_polar(I * pi / 2)\n    assert polar_lift(1 + I) == polar_lift(1 + I)\n    assert polar_lift(sqrt(2) + I * sqrt(2)) == polar_lift(2 * exp_polar(I * pi / 4))\n\n    x = Symbol('x')\n    assert polar_lift(x) == polar_lift(x)\n    p = Symbol('p', polar=True)\n    assert polar_lift(4 * p) == 4 * p\n    assert polar_lift(3 * x * I) == 3 * polar_lift(x * I)\n\n    # Test for matrices\n    A = Matrix([[2, -2], [I, -I]])\n    assert polar_lift(A) == Matrix([[2 * exp_polar(0), 2 * exp_polar(I * pi)], [exp_polar(I * pi / 2), exp_polar(-I * pi / 2)]])\n    IA = ImmutableMatrix([[2, -2], [I, -I]])\n    assert polar_lift(IA) == ImmutableMatrix([[2 * exp_polar(0), 2 * exp_polar(I * pi)], [exp_polar(I * pi / 2), exp_polar(-I * pi / 2)]])\n    SA = SparseMatrix([[2, -2], [I, -I]])\n    assert polar_lift(SA) == SparseMatrix([[2 * exp_polar(0), 2 * exp_polar(I * pi)], [exp_polar(I * pi / 2), exp_polar(-I * pi / 2)]])\n"], "sample_1154": ["def test__linsolve_nullspace():\n    # Test nullspace solution for underdetermined system\n    eqs = [Eq(x + y + z, 0)]\n    sol = _linsolve(eqs, (x, y, z))\n    assert sol == {x: -y - z, y: y, z: z}\n\n    eqs = [Eq(x + 2*y + 3*z, 0), Eq(2*x + 4*y + 6*z, 0)]\n    sol = _linsolve(eqs, (x, y, z))\n    assert sol == {x: -2*y - 3*z, y: y, z: z}\n\n    eqs = [Eq(x, 1), Eq(y, 2)]\n    sol = _linsolve(eqs, (x, y, z))\n    assert sol == {x: 1, y: 2, z: z}\n", "def test__linsolve_underdetermined():\n    eqs = [Eq(x + y, 0)]\n    sol = {x: -y, y: y}\n    assert _linsolve(eqs, [x, y]) == sol\n\n    eqs = [Eq(x + y + z, 0)]\n    sol = {x: -y - z, y: y, z: z}\n    assert _linsolve(eqs, [x, y, z]) == sol\n\n    eqs = [Eq(x + y, 0), Eq(y + z, 0)]\n    sol = {x: -z, y: z, z: z}\n    assert _linsolve(eqs, [x, y, z]) == sol\n\n    eqs = [Eq(x, 0), Eq(y, 0)]\n    sol = {x: 0, y: 0}\n    assert _linsolve(eqs, [x, y]) == sol\n", "def test__linsolve_under_determined():\n    eqs = [Eq(x + y, 0)]\n    sol = {x: -y, y: y}\n    assert _linsolve(eqs, (x, y)) == sol\n\n    eqs = [Eq(x + y, 0), Eq(z, 1)]\n    sol = {x: -y, y: y, z: 1}\n    assert _linsolve(eqs, (x, y, z)) == sol\n\n    eqs = [Eq(x + y + z, 0)]\n    sol = {x: -y - z, y: y, z: z}\n    assert _linsolve(eqs, (x, y, z)) == sol\n", "def test__linsolve_underdetermined():\n    # Test underdetermined system\n    eqs = [Eq(x + y, 0)]\n    sol = {x: -y, y: y}\n    assert _linsolve(eqs, (x, y)) == sol\n\n    # Test larger underdetermined system\n    eqs = [Eq(x + y + z, 1), Eq(x - z, 2)]\n    sol = {x: 1 + z, y: 1 - 2*z, z: z}\n    assert _linsolve(eqs, (x, y, z)) == sol\n\n    # Test underdetermined system with no solution\n    eqs = [Eq(x + y, 1), Eq(x + y, 2)]\n    assert _linsolve(eqs, (x, y)) is None\n", "def test__linsolve_underdetermined():\n    # Underdetermined system with free variables\n    eqs = [Eq(x + y, 0)]\n    sol = {x: -y, y: y}\n    assert _linsolve(eqs, [x, y]) == sol\n\n    # Another underdetermined system with more free variables\n    eqs = [Eq(x + y, 0), Eq(y + z, 0)]\n    sol = {x: -y - z, y: y, z: z}\n    assert _linsolve(eqs, [x, y, z]) == sol\n\n    # Underdetermined system with no equations\n    eqs = []\n    sol = {x: x, y: y, z: z}\n    assert _linsolve(eqs, [x, y, z]) == sol\n"], "sample_1155": ["def test_multiple_roots_extension():\n    ext1 = QQ.algebraic_field(sqrt(2))\n    ext2 = QQ.algebraic_field(sqrt(3))\n\n    assert construct_domain([sqrt(2), sqrt(3)], extension=True) == \\\n        (ext1, [ext1.convert(sqrt(2)), ext1.convert(sqrt(3))])\n\n    result = construct_domain([sqrt(2) + sqrt(3)], extension=True)\n    assert result[0].is_algebraic\n    assert result[1] == [result[0].convert(sqrt(2) + sqrt(3))]\n", "def test_mixed_symbols_rationals():\n    dom = QQ[x, y]\n    assert construct_domain([2*x + 3*y, S(3)/4]) == \\\n        (dom, [dom.convert(2*x + 3*y), dom.convert(S(3)/4)])\n\n    dom = QQ[x, y]\n    assert construct_domain([S(3)/4, 2*x + 3*y]) == \\\n        (dom, [dom.convert(S(3)/4), dom.convert(2*x + 3*y)])\n\n    dom = QQ[x, y]\n    assert construct_domain([S(3)/4, 2*x + 3*y, S(1)/2]) == \\\n        (dom, [dom.convert(S(3)/4), dom.convert(2*x + 3*y), dom.convert(S(1)/2)])\n\n    dom = QQ.frac_field(x, y)\n    assert construct_domain([2/x, 3*y, S(3)/4]) == \\\n        (dom, [dom.convert(2/x), dom.convert(3*y), dom.convert(S(3)/4)])\n", "def test_nested_expressions():\n    # Testing nested expressions with different types\n    nested_expr = [S(1)/2, sqrt(2), x/2 + y*sqrt(3)]\n    domain, elements = construct_domain(nested_expr)\n    assert domain == EX\n    assert elements == [EX(S(1)/2), EX(sqrt(2)), EX(x/2 + y*sqrt(3))]\n\n    nested_expr = [x/2 + y*sqrt(3), 1 + I]\n    domain, elements = construct_domain(nested_expr)\n    assert domain == EX\n    assert elements == [EX(x/2 + y*sqrt(3)), EX(1 + I)]\n\n    nested_expr = [sin(x), exp(I*y)]\n    domain, elements = construct_domain(nested_expr)\n    assert domain == EX\n    assert elements == [EX(sin(x)), EX(exp(I*y))]\n\n    nested_expr = [Rational(3, 4), x, sqrt(2)]\n    domain, elements = construct_domain(nested_expr)\n    assert domain == EX\n    assert elements == [EX(Rational(3, 4)), EX(x), EX(sqrt(2))]\n\n    nested_expr = [GoldenRatio, Catalan]\n    domain, elements = construct_domain(nested_expr)\n    assert domain == EX\n    assert elements == [EX(GoldenRatio), EX(Catalan)]\n", "def test_mixed_symbols_and_numbers():\n    dom = ZZ[x, y]\n    assert construct_domain([x + y, 2]) == \\\n        (dom, [dom.convert(x + y), dom.convert(2)])\n\n    dom = QQ[x, y]\n    assert construct_domain([x/2 + y/3, 2/5]) == \\\n        (dom, [dom.convert(x/2 + y/3), dom.convert(2/5)])\n\n    dom = CC[x]\n    assert construct_domain([x + I, 2.5]) == \\\n        (dom, [dom.convert(x + I), dom.convert(2.5)])\n\n    dom = CC[x, y]\n    assert construct_domain([x/2 + I*y/3, 2.5 + I*3.5]) == \\\n        (dom, [dom.convert(x/2 + I*y/3), dom.convert(2.5 + I*3.5)])\n\n    dom = RR[x, y]\n    assert construct_domain([x + y, 2.5]) == \\\n        (dom, [dom.convert(x + y), dom.convert(2.5)])\n\n    assert construct_domain([x + sin(y), 2.5], composite=False) == \\\n        (EX, [EX(x + sin(y)), EX(2.5)])\n", "def test_construct_domain_with_multiple_types():\n\n    # Test with mixed types: integers, rationals, floats, complex, and algebraic numbers\n    result = construct_domain([1, Rational(1, 2), 1.5, 2 + I, sqrt(3)])\n    assert result[0] == EX\n    assert all(isinstance(x, EX.dtype) for x in result[1])\n    assert result[1] == [EX(1), EX(Rational(1, 2)), EX(1.5), EX(2 + I), EX(sqrt(3))]\n\n    # Test with mixed types including symbolic expressions\n    result = construct_domain([x, Rational(1, 2), pi, GoldenRatio, Catalan])\n    assert result[0] == EX\n    assert all(isinstance(x, EX.dtype) for x in result[1])\n    assert result[1] == [EX(x), EX(Rational(1, 2)), EX(pi), EX(GoldenRatio), EX(Catalan)]\n\n    # Test with dictionary input containing different types\n    result = construct_domain({(0,): 1, (1,): Rational(1, 2), (2,): 1.5, (3,): 2 + I, (4,): sqrt(3)})\n    assert result[0] == EX\n    assert all(isinstance(v, EX.dtype) for v in result[1].values())\n    assert result[1] == {(0,): EX(1), (1,): EX(Rational(1, 2)), (2,): EX(1.5), (3,): EX(2 + I), (4,): EX(sqrt(3))}\n\n    # Test with mixed types including trigonometric and exponential functions\n    result = construct_domain([sin(x), exp(x), Rational(1, 3), Float('1.23'), I * pi])\n    assert result[0] == EX\n    assert all(isinstance(x, EX.dtype) for x in result[1])\n    assert result[1] == [EX(sin(x)), EX(exp(x)), EX(Rational(1, 3)), EX(Float('1.23')), EX(I * pi)]\n"], "sample_1156": ["def test_reciprocal_hyperbolic_functions():\n    x, y = symbols('x, y')\n    assert csch(x).rewrite(sinh) == 1 / sinh(x)\n    assert sech(x).rewrite(cosh) == 1 / cosh(x)\n    assert acsch(x).rewrite(log) == log(1/x + sqrt(1/x**2 + 1))\n    assert asech(x).rewrite(log) == log(1/x + sqrt(1/x - 1) * sqrt(1/x + 1))\n    assert acsch(-1) == -log(1 + sqrt(2))\n    assert asech(2) == I*pi/3\n\n    assert csch(x).diff(x) == -coth(x) * csch(x)\n    assert sech(x).diff(x) == -tanh(x) * sech(x)\n    assert acsch(x).diff(x) == -1/(x**2 * sqrt(1 + x**(-2)))\n    assert asech(x).diff(x) == -1/(x * sqrt(1 - x**2))\n\n    assert csch(x + y).expand(trig=True) == 1/(sinh(x)*cosh(y) + cosh(x)*sinh(y))\n    assert sech(x + y).expand(trig=True) == 1/(cosh(x)*cosh(y) + sinh(x)*sinh(y))\n\n    assert sech(x).is_real is None\n    assert sech(x).is_finite is None\n    assert csch(x).is_real is None\n    assert csch(x).is_finite is None\n\n    assert sech(I*pi/3).is_real is True\n    assert sech(I*pi/3).is_finite is True\n    assert csch(I*pi/3).is_real is False\n    assert csch(I*pi/3).is_finite is True\n", "def test_csch_expand_trig():\n    x, y = symbols('x, y')\n    assert csch(x + y).expand(trig=True) == 1 / (sinh(x) * cosh(y) + cosh(x) * sinh(y))\n    assert csch(2 * x).expand(trig=True) == 1 / (2 * sinh(x) * cosh(x))\n    assert csch(3 * x).expand(trig=True).expand() == 1 / (sinh(x) * (cosh(2 * x) + sinh(2 * x)))\n", "def test_sinh_eval():\n    x = Symbol('x')\n\n    # Test evaluation for specific symbolic cases\n    assert sinh(asinh(x)) == x\n    assert sinh(acosh(x)) == sqrt(x - 1) * sqrt(x + 1)\n    assert sinh(atanh(x)) == x/sqrt(1 - x**2)\n    assert sinh(acoth(x)) == 1/(sqrt(x - 1) * sqrt(x + 1))\n\n    # Test evaluation for specific numeric cases\n    assert sinh(0) == 0\n    assert sinh(1) == sinh(1)\n    assert sinh(-1) == -sinh(1)\n    assert sinh(I) == I * sin(1)\n    assert sinh(-I) == -I * sin(1)\n\n    # Test evaluation for extended real cases\n    y = Symbol('y', extended_real=True)\n    assert sinh(y).is_real is True\n    assert sinh(I*y).is_real is False\n", "def test_acsch_series():\n    x = Symbol('x')\n    assert acsch(x).series(x, 0, 10) == \\\n        log(2/x) - x**2/4 + 3*x**4/32 - 5*x**6/96 + 35*x**8/1024 + O(x**10)\n", "def test_csch_rewrite_as_cosh():\n    x = Symbol('x')\n    assert csch(x).rewrite(cosh) == I/cosh(x + I*pi/2)\n"], "sample_1157": ["def test_lambda_notation():\n    transformations = standard_transformations\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr('lambda x: x + 1', transformations=transformations) == Function('Lambda')((x,), x + 1)\n    assert parse_expr('lambda x, y: x + y', transformations=transformations) == Function('Lambda')((x, y), x + y)\n    assert parse_expr('lambda x: x**2', transformations=transformations) == Function('Lambda')((x,), x**2)\n\n    # Ensure that lone 'lambda' does not get replaced\n    raises(SyntaxError, lambda: parse_expr('lambda', transformations=transformations))\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    f = Function('f')\n    x = Symbol('x')\n    assert parse_expr('lambda x: x + 1', transformations=transformations) == Function('Lambda')((x,), x + 1)\n    assert parse_expr('lambda x, y: x + y', transformations=transformations) == Function('Lambda')((x, Symbol('y')), x + Symbol('y'))\n    raises(TokenError, lambda: parse_expr('lambda *args: sum(args)', transformations=transformations))\n", "def test_auto_number():\n    inputs = {\n        '123': Integer(123),\n        '1.23': Float('1.23'),\n        '4e5': Float('4e5'),\n        '1e-5': Float('1e-5'),\n        '0.2j': I * Float('0.2'),\n        '1.23e4j': I * Float('1.23e4')\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text) == result\n", "def test_lambda_notation():\n    inputs = {\n        'lambda x: x + 1': Lambda(Symbol('x'), Symbol('x') + 1),\n        'lambda x, y: x + y': Lambda((Symbol('x'), Symbol('y')), Symbol('x') + Symbol('y')),\n        'lambda x=1: x + 1': Lambda(Symbol('x'), Symbol('x') + 1, evaluate=False),\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text) == result\n\n    # Test that 'lambda' by itself does not get transformed\n    assert parse_expr('lambda') == Symbol('lambda')\n    # Test that lambda with starred arguments raises TokenError\n    raises(TokenError, lambda: parse_expr('lambda *x: x'))\n    raises(TokenError, lambda: parse_expr('lambda **x: x'))\n", "def test_implicit_multiplication_application():\n    transformations = standard_transformations + (implicit_multiplication_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    assert parse_expr(\"10sin**2 x**2 + 3xyz + tan theta\", transformations=transformations) == 10*sin(x**2)**2 + 3*x*y*z + Function('tan')(Symbol('theta'))\n    assert parse_expr(\"5log10 + 3cos pi\", transformations=transformations) == 5*Function('log')(10) + 3*Function('cos')(Symbol('pi'))\n"], "sample_1158": ["def test_sympify_numpy_scalar_conversion():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    # Test scalar conversion for numpy float, int, and complex types\n    np_float = numpy.float64(2.5)\n    np_int = numpy.int32(10)\n    np_complex = numpy.complex128(3 + 4j)\n\n    # Ensure sympify converts numpy scalars correctly\n    assert sympify(np_float) == Float(2.5)\n    assert sympify(np_int) == Integer(10)\n    assert sympify(np_complex) == 3 + 4*I\n\n    # Test strict mode for numpy scalars\n    assert sympify(np_float, strict=True) == Float(2.5)\n    assert sympify(np_int, strict=True) == Integer(10)\n    assert sympify(np_complex, strict=True) == 3 + 4*I\n\n    # Ensure sympify raises error for unknown types in strict mode\n    class UnknownType:\n        pass\n\n    raises(SympifyError, lambda: sympify(UnknownType(), strict=True))\n", "def test_sympify_cantsympify():\n    class MyClass(CantSympify):\n        pass\n\n    instance = MyClass()\n    raises(SympifyError, lambda: sympify(instance))\n\n    class AnotherClass(CantSympify):\n            return \"AnotherClass instance\"\n\n    another_instance = AnotherClass()\n    raises(SympifyError, lambda: sympify(another_instance))\n", "def test_issue_99999_CantSympify():\n    class CustomClass(CantSympify):\n            self.value = value\n\n    instance = CustomClass(42)\n    raises(SympifyError, lambda: sympify(instance))\n\n    # Ensure that instance of CantSympify cannot be sympified\n    try:\n        sympify(instance)\n    except SympifyError as e:\n        assert str(e) == \"SympifyError: SympifyError: %r\" % instance\n", "def test_sympify_nonnumeric():\n    class MyClass:\n        pass\n\n    obj = MyClass()\n\n    # Attempting to sympify an instance of a custom class without _sympy_ method should raise SympifyError\n    raises(SympifyError, lambda: sympify(obj))\n\n    # Custom class with a _sympy_ method\n    class MyClassWithSympy:\n            return Symbol('custom_sympy')\n\n    obj_with_sympy = MyClassWithSympy()\n    assert sympify(obj_with_sympy) == Symbol('custom_sympy')\n\n    # Custom class with a converter\n    converter[MyClass] = lambda x: Symbol('converted_class')\n    assert sympify(obj) == Symbol('converted_class')\n", "def test_sympify_circular_import():\n    class Circular:\n            return 'Circular Instance'\n    circular_instance = Circular()\n\n    with raises(SympifyError):\n        sympify(circular_instance)\n"], "sample_1159": ["def test_common_assumptions():\n    exprs = [S(2), S(4), S(6)]\n    assert common_assumptions(exprs) == {\n        'commutative': True,\n        'integer': True,\n        'real': True,\n        'complex': True,\n        'rational': True,\n        'algebraic': True,\n        'finite': True,\n        'nonnegative': True,\n        'positive': True,\n        'even': True,\n        'odd': False,\n        'prime': False,\n        'composite': True,\n        'extended_real': True,\n        'extended_nonnegative': True,\n    }\n    exprs = [S(2), S(4), S(6), S(8), S(10)]\n    assert common_assumptions(exprs, ['even', 'odd', 'composite']) == {\n        'even': True,\n        'odd': False,\n        'composite': True\n    }\n    exprs = [S(2), S(4), S(6), S(9), S(10)]\n    assert common_assumptions(exprs, ['positive', 'integer', 'composite']) == {\n        'positive': True,\n        'integer': True,\n    }\n", "def test_assumptions_edge_cases():\n    assert assumptions(S.Zero) == {'commutative': True, 'complex': True, 'extended_nonnegative': True, 'extended_nonpositive': True, 'even': True, 'finite': True, 'integer': True, 'negative': False, 'noninteger': False, 'nonnegative': True, 'nonpositive': True, 'nonzero': False, 'odd': False, 'positive': False, 'prime': False, 'rational': True, 'real': True, 'zero': True, 'algebraic': True, 'transcendental': False, 'irrational': False, 'extended_real': True, 'extended_negative': False, 'extended_positive': False, 'extended_nonzero': False, 'imaginary': False, 'hermitian': True, 'infinite': False}\n    assert assumptions(S.NaN) == {'commutative': True, 'number': True, 'comparable': False}\n    assert assumptions(S.Infinity) == {'commutative': True, 'number': True, 'finite': False, 'extended_real': True, 'infinite': True, 'extended_positive': True, 'extended_nonnegative': True, 'extended_nonzero': True, 'extended_negative': False, 'extended_nonpositive': False, 'imaginary': False}\n    assert assumptions(S.NegativeInfinity) == {'commutative': True, 'number': True, 'finite': False, 'extended_real': True, 'infinite': True, 'extended_negative': True, 'extended_nonpositive': True, 'extended_nonzero': True, 'extended_positive': False, 'extended_nonnegative': False, 'imaginary': False}\n    assert assumptions(S.ComplexInfinity) == {'commutative': True, 'complex': False, 'finite': False, 'extended_real': False, 'infinite': True, 'imaginary': False}\n", "def test_symbol_hermitian():\n    a = Symbol('a', hermitian=True)\n    assert a.is_hermitian is True\n    assert a.is_antihermitian is False\n\n    b = Symbol('b', hermitian=False)\n    assert b.is_hermitian is False\n    assert b.is_antihermitian is None\n\n    c = Symbol('c', hermitian=True, antihermitian=True)\n    assert c.is_hermitian is True\n    assert c.is_antihermitian is True\n\n    x = Symbol('x', real=True)\n    assert x.is_hermitian is True\n    assert x.is_antihermitian is False\n\n    y = Symbol('y', imaginary=True)\n    assert y.is_hermitian is False\n    assert y.is_antihermitian is True\n", "def test_common_assumptions():\n    exprs = [S(2), S(4), S(6)]\n    assert common_assumptions(exprs) == {\n        'commutative': True,\n        'integer': True,\n        'rational': True,\n        'algebraic': True,\n        'complex': True,\n        'real': True,\n        'finite': True,\n        'even': True,\n        'extended_real': True,\n        'positive': True,\n        'nonnegative': True,\n        'nonzero': True,\n        'zero': False,\n        'prime': False,\n        'irrational': False,\n        'odd': False,\n        'negative': False,\n        'nonpositive': False,\n        'noninteger': False\n    }\n\n    exprs = [S(-2), S(-4), S(-6)]\n    assert common_assumptions(exprs) == {\n        'commutative': True,\n        'integer': True,\n        'rational': True,\n        'algebraic': True,\n        'complex': True,\n        'real': True,\n        'finite': True,\n        'even': True,\n        'extended_real': True,\n        'negative': True,\n        'nonpositive': True,\n        'nonzero': True,\n        'zero': False,\n        'prime': False,\n        'irrational': False,\n        'odd': False,\n        'nonnegative': False,\n        'positive': False,\n        'noninteger': False\n    }\n", "def test_assumptions_prime_composite():\n    p = Symbol('p', prime=True)\n    assert p.is_prime is True\n    assert p.is_composite is False\n    assert p.is_integer is True\n    assert p.is_positive is True\n    assert p.is_nonpositive is False\n    assert p.is_nonnegative is True\n    assert p.is_negative is False\n\n    c = Symbol('c', composite=True)\n    assert c.is_prime is False\n    assert c.is_composite is True\n    assert c.is_integer is True\n    assert c.is_positive is True\n    assert c.is_nonpositive is False\n    assert c.is_nonnegative is True\n    assert c.is_negative is False\n\n    np = Symbol('np', prime=False)\n    assert np.is_prime is False\n    assert np.is_composite is None\n    assert np.is_integer is None\n    assert np.is_positive is None\n    assert np.is_nonpositive is None\n    assert np.is_nonnegative is None\n    assert np.is_negative is None\n\n    nc = Symbol('nc', composite=False)\n    assert nc.is_prime is None\n    assert nc.is_composite is False\n    assert nc.is_integer is None\n    assert nc.is_positive is None\n    assert nc.is_nonpositive is None\n    assert nc.is_nonnegative is None\n    assert nc.is_negative is None\n"], "sample_1160": ["def test_intersection_ComplexRegion_and_Set():\n    from sympy.abc import a, b\n\n    # Test intersection of ComplexRegion in rectangular form with Set\n    rect_region = ComplexRegion(Interval(0, 5) * Interval(0, 5))\n    set_intervals = Interval(2, 4)\n    set_finiteset = FiniteSet(1 + 1*I, 2 + 2*I, 6 + 6*I)\n    intersection_result = Intersection(rect_region, set_intervals)\n    assert intersection_result == FiniteSet(2, 3, 4)\n    intersection_result = Intersection(rect_region, set_finiteset)\n    assert intersection_result == FiniteSet(2 + 2*I)\n\n    # Test intersection of ComplexRegion in polar form with Set\n    polar_region = ComplexRegion(Interval(0, 2) * Interval(0, pi), polar=True)\n    set_finiteset = FiniteSet(1 + I, 2*I, 1, -1, 2 - I)\n    intersection_result = Intersection(polar_region, set_finiteset)\n    assert intersection_result == FiniteSet(1)\n    set_intervals = Interval(-2, 2)\n    intersection_result = Intersection(polar_region, set_intervals)\n    assert intersection_result == Union(Interval.Ropen(0, 1), FiniteSet(1))\n\n    # Test intersection of ComplexRegion in rectangular form with ImageSet\n    img_set = ImageSet(Lambda(a, a + I*b), S.Integers)\n    intersection_result = Intersection(rect_region, img_set)\n    assert intersection_result == ImageSet(Lambda(a, a + I*b), Intersection(S.Integers, Interval(0, 5)))\n\n    # Test intersection of ComplexRegion in polar form with ImageSet\n    img_set = ImageSet(Lambda(a, a*exp(I*b)), S.Integers)\n    intersection_result = Intersection(polar_region, img_set)\n    assert intersection_result == ImageSet(Lambda(a, a*exp(I*b)), Intersection(S.Integers, Interval(0, 2)))\n", "def test_ComplexRegion_intersections():\n    r1 = Interval(0, 2)\n    theta1 = Interval(0, S.Pi)\n    c1 = ComplexRegion(r1*theta1, polar=True)\n    \n    r2 = Interval(1, 3)\n    theta2 = Interval(S.Pi/2, S.Pi)\n    c2 = ComplexRegion(r2*theta2, polar=True)\n    \n    intersection = c1.intersect(c2)\n    \n    assert intersection == ComplexRegion(Interval(1, 2) * Interval(S.Pi/2, S.Pi), polar=True)\n    \n    r3 = Interval(0, 1)\n    theta3 = Interval(0, S.Pi/2)\n    c3 = ComplexRegion(r3*theta3, polar=True)\n    \n    intersection2 = c1.intersect(c3)\n    \n    assert intersection2 == c3\n    \n    r4 = Interval(2, 4)\n    theta4 = Interval(3*S.Pi/4, S.Pi)\n    c4 = ComplexRegion(r4*theta4, polar=True)\n    \n    intersection3 = c1.intersect(c4)\n    \n    assert intersection3 == S.EmptySet\n\n    r5 = Interval(1, 3)\n    theta5 = Interval(0, 2*S.Pi)\n    c5 = ComplexRegion(r5*theta5, polar=True)\n\n    intersection4 = c1.intersect(c5)\n    \n    assert intersection4 == ComplexRegion(Interval(1, 2) * Interval(0, S.Pi), polar=True)\n\n    # Test intersection of rectangular form ComplexRegions\n    cr1 = ComplexRegion(Interval(0, 2)*Interval(0, 2))\n    cr2 = ComplexRegion(Interval(1, 3)*Interval(1, 3))\n    intersection5 = cr1.intersect(cr2)\n    \n    assert intersection5 == ComplexRegion(Interval(1, 2)*Interval(1, 2))\n", "def test_intersection_sets():\n    x = Symbol('x')\n    y = Symbol('y')\n    \n    # Testing ConditionSet intersection with ConditionSet\n    C1 = ConditionSet(x, x > 0, S.Reals)\n    C2 = ConditionSet(y, y < 5, S.Reals)\n    assert intersection_sets(C1, C2) is None\n\n    # Testing ConditionSet intersection with a Set\n    C = ConditionSet(x, x > 2, Interval(1, 4))\n    S1 = Interval(2, 3)\n    result = intersection_sets(C, S1)\n    assert isinstance(result, ConditionSet)\n    assert result.base_set == Interval(2, 3)\n    assert result.condition == (x > 2)\n    \n    # Testing Naturals intersection with Integers\n    assert intersection_sets(S.Naturals, S.Integers) == S.Naturals\n    \n    # Testing Naturals intersection with Naturals\n    assert intersection_sets(S.Naturals, S.Naturals) == S.Naturals\n\n    # Testing Interval intersection with Naturals\n    assert intersection_sets(Interval(1, 5), S.Naturals) == Range(1, 6)\n    \n    # Testing ComplexRegion intersection with another ComplexRegion\n    CR1 = ComplexRegion(Interval(0, 1) * Interval(0, 2 * pi), polar=True)\n    CR2 = ComplexRegion(Interval(0.5, 1.5) * Interval(pi/2, 3*pi/2), polar=True)\n    result = intersection_sets(CR1, CR2)\n    assert isinstance(result, ComplexRegion)\n    assert result.sets == Intersection(Interval(0, 1), Interval(0.5, 1.5)) * Intersection(Interval(0, 2 * pi), Interval(pi/2, 3*pi/2))\n\n    # Testing ComplexRegion intersection with Reals\n    assert intersection_sets(CR1, S.Reals) == FiniteSet(0, 1)\n    \n    # Testing FiniteSet intersection with FiniteSet\n    FS1 = FiniteSet(1, 2, 3)\n    FS2 = FiniteSet(3, 4, 5)\n    assert intersection_sets(FS1, FS2) == FiniteSet(3)\n    \n    # Testing FiniteSet intersection with Set\n    assert intersection_sets(FS1", "def test_intersection_sets():\n    # Test intersection with ConditionSet and Set\n    x = symbols('x')\n    cs = ConditionSet(x, x > 0, S.Integers)\n    assert intersection_sets(cs, S.Naturals) == cs\n    assert intersection_sets(cs, Interval(1, 10)) == ConditionSet(x, x > 0, Intersection(S.Integers, Interval(1, 10)))\n\n    # Test intersection of two ConditionSets\n    cs1 = ConditionSet(x, x > 0, S.Integers)\n    cs2 = ConditionSet(x, x < 10, S.Integers)\n    assert intersection_sets(cs1, cs2) is None\n\n    # Test intersection with Naturals and Integers\n    assert intersection_sets(S.Naturals, S.Integers) == S.Naturals\n    assert intersection_sets(S.Naturals, S.Naturals) == S.Naturals\n\n    # Test intersection with Interval and Naturals\n    assert intersection_sets(Interval(1, 10), S.Naturals) == Range(1, 11)\n    assert intersection_sets(Interval(0, 0.5), S.Naturals) == S.EmptySet\n\n    # Test intersection with ComplexRegion and Set\n    cr = ComplexRegion(Interval(0, 1) * Interval(0, 2*pi), polar=True)\n    assert intersection_sets(cr, S.Reals) == Interval(0, 1)\n    assert intersection_sets(cr, Interval(-1, 1)) == Interval(0, 1)\n\n    # Test intersection with Integers and Reals\n    assert intersection_sets(S.Integers, S.Reals) == S.Integers\n\n    # Test intersection with Range and Interval\n    assert intersection_sets(Range(1, 10), Interval(3, 7)) == Range(3, 8)\n    assert intersection_sets(Range(1, 10), Interval(10, 20)) == FiniteSet(10)\n\n    # Test intersection with ProductSet\n    ps1 = ProductSet(Interval(0, 1), Interval(0, 1))\n    ps2 = ProductSet(Interval(0.5, 1.5), Interval(0.5, 1.5))\n    assert intersection_sets(ps1, ps2) == ProductSet(Interval(0.5, 1), Interval(0.", "def test_intersection_sets():\n    # Test intersection_sets for FiniteSet and Interval\n    assert intersection_sets(FiniteSet(1, 2, 3), Interval(2, 4)) == FiniteSet(2, 3)\n    assert intersection_sets(FiniteSet(1, 2, 3), Interval(4, 6)) == S.EmptySet\n    assert intersection_sets(FiniteSet(1, 2, 3), Interval(0, 1, left_open=True)) == FiniteSet(1)\n    \n    # Test intersection_sets for Integers and Interval\n    assert intersection_sets(S.Integers, Interval(1, 5)) == Range(1, 6)\n    assert intersection_sets(S.Integers, Interval(1, 5, left_open=True, right_open=True)) == Range(2, 5)\n    \n    # Test intersection_sets for Naturals and Interval\n    assert intersection_sets(S.Naturals, Interval(1, 5)) == Range(1, 6)\n    assert intersection_sets(S.Naturals, Interval(1, 5, left_open=True, right_open=True)) == Range(2, 5)\n    \n    # Test intersection_sets for ConditionSet and FiniteSet\n    x = symbols('x')\n    cond_set = ConditionSet(x, x > 1, S.Reals)\n    assert intersection_sets(cond_set, FiniteSet(1, 2, 3)) == ConditionSet(x, x > 1, FiniteSet(2, 3))\n    \n    # Test intersection_sets for UniversalSet and FiniteSet\n    assert intersection_sets(S.UniversalSet, FiniteSet(1, 2, 3)) == FiniteSet(1, 2, 3)\n    \n    # Test intersection_sets for ComplexRegion and Interval\n    a = Interval(0, 1)\n    b = Interval(0, 2*S.Pi)\n    c_region = ComplexRegion(a*b, polar=True)\n    assert intersection_sets(c_region, Interval(0, 1)) == Interval(0, 1)\n    \n    # Test intersection_sets for FiniteSet and ComplexRegion\n    assert intersection_sets(FiniteSet(1, 2, 3), ComplexRegion(a*b, polar=True)) == S.EmptySet\n"], "sample_1161": ["def test_Integral_with_mul_limits():\n    expr = Integral(x**2, (x, 0, 1), (y, 0, 2))\n    assert str(expr) == \"Integral(x**2, (x, 0, 1), (y, 0, 2))\"\n", "def test_Exp1():\n    assert str(exp(1)) == \"E\"\n    assert str(exp(x)) == \"exp(x)\"\n    assert str(exp(x + y)) == \"exp(x + y)\"\n", "def test_Fraction():\n    from fractions import Fraction\n    assert str(Fraction(1, 2)) == \"1/2\"\n    assert str(Fraction(2, 3)) == \"2/3\"\n    assert str(Fraction(3, 4)) == \"3/4\"\n    assert str(Fraction(5, 6)) == \"5/6\"\n    assert str(Fraction(7, 8)) == \"7/8\"\n    assert str(Fraction(-1, 2)) == \"-1/2\"\n    assert str(Fraction(-2, 3)) == \"-2/3\"\n    assert str(Fraction(-3, 4)) == \"-3/4\"\n    assert str(Fraction(-5, 6)) == \"-5/6\"\n    assert str(Fraction(-7, 8)) == \"-7/8\"\n    assert str(Fraction(1, -2)) == \"-1/2\"\n    assert str(Fraction(2, -3)) == \"-2/3\"\n    assert str(Fraction(3, -4)) == \"-3/4\"\n    assert str(Fraction(5, -6)) == \"-5/6\"\n    assert str(Fraction(7, -8)) == \"-7/8\"\n    assert str(Fraction(-1, -2)) == \"1/2\"\n    assert str(Fraction(-2, -3)) == \"2/3\"\n    assert str(Fraction(-3, -4)) == \"3/4\"\n    assert str(Fraction(-5, -6)) == \"5/6\"\n    assert str(Fraction(-7, -8)) == \"7/8\"\n", "def test_BlockMatrix():\n    from sympy import BlockMatrix, Matrix\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    C = BlockMatrix([[A, B], [B, A]])\n    assert str(C) == \"Matrix([Matrix([\\n[1, 2],\\n[3, 4]]), Matrix([\\n[5, 6],\\n[7, 8]])]), Matrix([Matrix([\\n[5, 6],\\n[7, 8]]), Matrix([\\n[1, 2],\\n[3, 4]])])\"\n", "def test_ExprCondPair():\n    e1 = ExprCondPair(x > 1, x**2)\n    e2 = ExprCondPair(y < 0, y - 1)\n    assert str(e1) == \"(x**2, x > 1)\"\n    assert str(e2) == \"(y - 1, y < 0)\"\n"], "sample_1162": ["def test_FunctionClass_kind():\n    from sympy import Function\n\n    f = Function('f')\n    assert f.kind is UndefinedKind\n    assert f(comm_x).kind is UndefinedKind\n    assert f(2).kind is NumberKind\n", "def test_Lambda_kind():\n    f = Lambda(comm_x, comm_x + 1)\n    assert f.kind is NumberKind\n    assert f(comm_x).kind is NumberKind\n    g = Lambda((comm_x, noncomm_x), comm_x + noncomm_x)\n    assert g.kind is UndefinedKind\n    assert g(comm_x, noncomm_x).kind is UndefinedKind\n", "def test_Function_kind():\n    f = Function('f')\n    g = Function('g', nargs=2)\n    assert f.kind is UndefinedKind\n    assert f(comm_x).kind is NumberKind\n    assert f(noncomm_x).kind is UndefinedKind\n    assert g.kind is UndefinedKind\n    assert g(comm_x, comm_x).kind is NumberKind\n    assert g(comm_x, noncomm_x).kind is UndefinedKind\n", "def test_FunctionClass_kind():\n    from sympy import Function, symbols\n    x, y = symbols('x y')\n    \n    class MyFunc(Function):\n        nargs = 1\n\n    f = MyFunc(x)\n    assert f.kind is UndefinedKind\n    assert MyFunc.kind is UndefinedKind\n    assert f(x).kind is UndefinedKind\n    assert MyFunc(x).kind is UndefinedKind\n    assert MyFunc(y).kind is UndefinedKind\n", "def test_Lambda_kind():\n    A = MatrixSymbol('A', 2, 2)\n    f = Lambda(comm_x, comm_x + 1)\n    g = Lambda((comm_x, noncomm_x), comm_x + noncomm_x)\n    h = Lambda(A, A + Matrix.eye(2))\n\n    assert f.kind is UndefinedKind\n    assert g.kind is UndefinedKind\n    assert h.kind is UndefinedKind\n"], "sample_1163": ["def test_conjugate_derivatives():\n    x, y = symbols('x y', real=True)\n    z = symbols('z', complex=True)\n\n    # Test conjugate of derivatives for real variables\n    f = Function('f')(x)\n    assert conjugate(Derivative(f, x)) == Derivative(conjugate(f), x)\n    assert conjugate(Derivative(f, (x, 2))) == Derivative(conjugate(f), (x, 2))\n\n    # Test conjugate of mixed real and imaginary variables\n    g = Function('g')(x + I*y)\n    assert conjugate(Derivative(g, x)) == Derivative(conjugate(g), x)\n    assert conjugate(Derivative(g, y)) == Derivative(conjugate(g), y)\n    assert conjugate(Derivative(g, x, y)) == Derivative(conjugate(g), x, y)\n\n    # Test conjugate of derivatives for a complex variable\n    h = Function('h')(z)\n    assert conjugate(Derivative(h, z)) == Derivative(conjugate(h), conjugate(z))\n    assert conjugate(Derivative(h, (z, 2))) == Derivative(conjugate(h), (conjugate(z), 2))\n\n    # Derivative of conjugate function\n    assert Derivative(conjugate(f), x) == conjugate(Derivative(f, x))\n    assert Derivative(conjugate(h), z) == conjugate(Derivative(h, z))\n\n    # Higher order derivatives\n    assert Derivative(conjugate(f), (x, 2)) == conjugate(Derivative(f, (x, 2)))\n    assert Derivative(conjugate(h), (z, 3)) == conjugate(Derivative(h, (z, 3)))\n", "def test_conjugate_transpose_matrices():\n    A = Matrix([[1, 2 + 3*I], [-I, 4]])\n    B = ImmutableMatrix([[1, 2 + 3*I], [-I, 4]])\n    S = SparseMatrix(2, 2, {(0, 1): 2 + 3*I, (1, 1): 4})\n    IS = ImmutableSparseMatrix(2, 2, {(0, 1): 2 + 3*I, (1, 1): 4})\n\n    # Test conjugate\n    assert conjugate(A) == Matrix([[1, 2 - 3*I], [I, 4]])\n    assert conjugate(B) == ImmutableMatrix([[1, 2 - 3*I], [I, 4]])\n    assert conjugate(S) == SparseMatrix(2, 2, {(0, 1): 2 - 3*I, (1, 1): 4})\n    assert conjugate(IS) == ImmutableSparseMatrix(2, 2, {(0, 1): 2 - 3*I, (1, 1): 4})\n\n    # Test transpose\n    assert transpose(A) == Matrix([[1, -I], [2 + 3*I, 4]])\n    assert transpose(B) == ImmutableMatrix([[1, -I], [2 + 3*I, 4]])\n    assert transpose(S) == SparseMatrix(2, 2, {(1, 0): 2 + 3*I, (1, 1): 4})\n    assert transpose(IS) == ImmutableSparseMatrix(2, 2, {(1, 0): 2 + 3*I, (1, 1): 4})\n\n    # Test adjoint\n    assert adjoint(A) == Matrix([[1, I], [2 - 3*I, 4]])\n    assert adjoint(B) == ImmutableMatrix([[1, I], [2 - 3*I, 4]])\n    assert adjoint(S) == SparseMatrix(2, 2, {(1, 0): 2 - 3*I, (1, 1): 4})\n    assert adjoint(IS) == ImmutableSparseMatrix(2, 2, {(1, 0): 2 - 3*I, (", "def test_conjugate_transpose_properties():\n    # Test conjugate transpose properties for matrices and expressions\n    A = Matrix([[1 + I, 2], [3, 4 + I]])\n    B = Matrix([[I, -2], [2, -I]])\n    C = Matrix([[1, 2], [3, 4]])\n    \n    assert adjoint(A) == conjugate(transpose(A))\n    assert adjoint(B) == conjugate(transpose(B))\n    assert adjoint(C) == conjugate(transpose(C))\n\n    assert adjoint(A + B) == adjoint(A) + adjoint(B)\n    assert adjoint(A - B) == adjoint(A) - adjoint(B)\n    assert adjoint(A * B) == adjoint(B) * adjoint(A)\n    assert adjoint(2 * A) == 2 * adjoint(A)\n\n    # Check for expressions\n    x, y = symbols('x y', commutative=False)\n    assert adjoint(x * y) == adjoint(y) * adjoint(x)\n    assert adjoint(x + y) == adjoint(x) + adjoint(y)\n    assert adjoint(2 * x) == 2 * adjoint(x)\n    assert adjoint(x - y) == adjoint(x) - adjoint(y)\n    \n    # Check for nested adjoints\n    assert adjoint(adjoint(x)) == x\n\n    # Check for mixed properties with transpose and conjugate\n    assert adjoint(transpose(A + B)) == adjoint(transpose(A)) + adjoint(transpose(B))\n    assert adjoint(conjugate(A + B)) == adjoint(conjugate(A)) + adjoint(conjugate(B))\n    assert adjoint(transpose(conjugate(A + B))) == adjoint(transpose(conjugate(A))) + adjoint(transpose(conjugate(B)))\n    assert adjoint(conjugate(transpose(A + B))) == adjoint(conjugate(transpose(A))) + adjoint(conjugate(transpose(B)))\n", "def test_polar_lift():\n    x, y = symbols('x y')\n    assert polar_lift(2).is_polar\n    assert polar_lift(-2) == 2 * exp_polar(I * pi)\n    assert polar_lift(2 + 3*I).args == (2 + 3*I,)\n    assert polar_lift(2*x).args[0] == 2 * polar_lift(x)\n    assert polar_lift(x * y).args[0] == polar_lift(x * y)\n    assert polar_lift(exp(x)) == exp(x)\n    assert polar_lift(exp_polar(x)).is_polar\n\n    # Check if the _eval_Abs method works properly\n    assert Abs(polar_lift(2 + 3*I)) == Abs(2 + 3*I)\n    assert Abs(polar_lift(2)) == 2\n    assert Abs(polar_lift(-2)) == 2\n", "def test_polar_lift():\n    from sympy import polar_lift, pi\n\n    x = Symbol('x', positive=True)\n    assert polar_lift(0) == 0\n    assert polar_lift(1) == exp_polar(0)\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(2 + I) == 2*polar_lift(1 + I/2)\n    assert polar_lift(x).is_positive is True\n    assert polar_lift(-x).is_positive is False\n    assert polar_lift(polar_lift(x)) == polar_lift(x)\n\n    # Test that polar_lift does not lift the integration variable\n    mu = Symbol(\"mu\")\n    sigma = Symbol(\"sigma\", positive=True)\n    expr = Integral(sqrt(2)*x*exp(-(-mu + x)**2/(2*sigma**2))/(2*sqrt(pi)*sigma), (x, -oo, oo))\n    assert polar_lift(expr) == expr\n"], "sample_1164": ["def test_wigner_6j_9j_evaluation():\n    wigner6j = Wigner6j(1, 1, 1, 1, 1, 1)\n    wigner9j = Wigner9j(1, 1, 1, 1, 1, 1, 1, 1, 1)\n    \n    assert wigner6j.doit() == Rational(1, 60)\n    assert wigner9j.doit() == Rational(1, 660)\n    \n    wigner6j = Wigner6j(2, 2, 2, 2, 2, 2)\n    wigner9j = Wigner9j(2, 2, 2, 2, 2, 2, 2, 2, 2)\n    \n    assert wigner6j.doit() == Rational(-1, 280)\n    assert wigner9j.doit() == Rational(1, 1400)\n", "def test_cg_simp():\n    from sympy import S, Add\n    from sympy.physics.quantum.cg import cg_simp\n    a = CG(1, 1, 0, 0, 1, 1)\n    b = CG(1, 0, 0, 0, 1, 0)\n    c = CG(1, -1, 0, 0, 1, -1)\n    d = CG(S(3)/2, S(1)/2, S(1)/2, S(-1)/2, 1, 0)\n    e = CG(S(3)/2, S(1)/2, S(1)/2, S(1)/2, 1, 1)\n    f = CG(S(3)/2, S(-1)/2, S(1)/2, S(1)/2, 1, 0)\n    sum_1 = Add(a, b, c)\n    sum_2 = Add(d, e, f)\n    assert cg_simp(sum_1) == 3\n    assert cg_simp(sum_2) == S(1)\n", "def test_wigner_symbols_evaluation():\n    w3j = Wigner3j(1, 0, 1, 0, 1, 0)\n    w6j = Wigner6j(1, 1, 1, 1, 1, 1)\n    w9j = Wigner9j(1, 1, 1, 1, 1, 1, 1, 1, 1)\n    assert w3j.doit() == sqrt(3) / 3\n    assert w6j.doit() == S(1) / 3\n    assert w9j.doit() == S(1) / 6\n    assert str(w3j) == 'Wigner3j(1, 0, 1, 0, 1, 0)'\n    assert str(w6j) == 'Wigner6j(1, 1, 1, 1, 1, 1)'\n    assert str(w9j) == 'Wigner9j(1, 1, 1, 1, 1, 1, 1, 1, 1)'\n    ascii_str_w3j = \\", "def test_cg_simp():\n    from sympy import Sum, symbols, Rational\n    alpha = symbols('alpha')\n    cg1 = CG(1, alpha, 0, 0, 1, alpha)\n    cg2 = CG(1, -alpha, 0, 0, 1, -alpha)\n    cg3 = CG(1, 0, 0, 0, 1, 0)\n    expr = cg1 + cg2 + cg3\n    simplified_expr = cg_simp(expr)\n    assert simplified_expr == 3\n\n    cg4 = CG(1, 1, 0, 0, 1, 1)\n    cg5 = CG(1, 0, 0, 0, 1, 0)\n    cg6 = CG(1, -1, 0, 0, 1, -1)\n    expr = Sum(cg4 + cg5 + cg6, (alpha, -1, 1))\n    simplified_expr = cg_simp(expr.doit())\n    assert simplified_expr == 3\n\n    cg7 = CG(Rational(1, 2), Rational(1, 2), Rational(1, 2), Rational(-1, 2), 0, 0)\n    cg8 = CG(Rational(1, 2), Rational(-1, 2), Rational(1, 2), Rational(1, 2), 0, 0)\n    expr = cg7 * cg8\n    simplified_expr = cg_simp(expr)\n    assert simplified_expr == 1 / 2\n", "def test_cg_simp():\n    from sympy import Sum, symbols\n    a, alpha, b, beta, c, gamma = symbols('a alpha b beta c gamma')\n    cg_expr = CG(a, alpha, b, beta, c, gamma) + CG(a, alpha, b, beta, c, gamma)\n    assert cg_simp(cg_expr) == 2 * CG(a, alpha, b, beta, c, gamma)\n\n    cg_expr_sum = Sum(CG(a, alpha, 0, 0, a, alpha), (alpha, -a, a))\n    simplified_sum = cg_simp(cg_expr_sum)\n    assert simplified_sum == (2*a + 1)\n\n    cg_expr_sum_2 = Sum((-1)**(a - alpha) * CG(a, alpha, a, -alpha, c, 0), (alpha, -a, a))\n    simplified_sum_2 = cg_simp(cg_expr_sum_2)\n    assert simplified_sum_2 == sqrt(2*a + 1) * KroneckerDelta(c, 0)\n\n    cg_expr_sum_3 = Sum(CG(a, alpha, b, beta, c, gamma) * CG(a, alpha, b, beta, c, gamma), (alpha, -a, a), (beta, -b, b))\n    simplified_sum_3 = cg_simp(cg_expr_sum_3)\n    assert simplified_sum_3 == 1\n"], "sample_1165": ["def test_quaternion_exponential_and_logarithm():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(0, 0, 0, 0)\n\n    # Test the exponential function\n    assert q1.exp() == Quaternion(E * cos(sqrt(29)),\n                                  2 * sqrt(29) * E * sin(sqrt(29)) / 29,\n                                  3 * sqrt(29) * E * sin(sqrt(29)) / 29,\n                                  4 * sqrt(29) * E * sin(sqrt(29)) / 29)\n\n    # Test the natural logarithm function\n    assert q1._ln() == Quaternion(log(sqrt(30)),\n                                  2 * sqrt(29) * acos(sqrt(30)/30) / 29,\n                                  3 * sqrt(29) * acos(sqrt(30)/30) / 29,\n                                  4 * sqrt(29) * acos(sqrt(30)/30) / 29)\n\n    # Check that exp and ln are inverses\n    assert (q1.exp()._ln() - q1).norm().evalf() < 1e-10\n\n    # Test edge cases\n    raises(ValueError, lambda: q2._ln())  # Logarithm of zero quaternion should raise an error\n", "def test_quaternion_special_cases():\n    # Testing zero quaternion\n    q0 = Quaternion(0, 0, 0, 0)\n    assert q0.norm() == 0\n    assert q0.normalize() == Quaternion(0, 0, 0, 0)\n    raises(ValueError, lambda: q0.inverse())\n\n    # Testing quaternion with only real part\n    q_real = Quaternion(3, 0, 0, 0)\n    assert q_real.norm() == 3\n    assert q_real.normalize() == Quaternion(1, 0, 0, 0)\n    assert q_real.inverse() == Quaternion(1/3, 0, 0, 0)\n\n    # Testing quaternion with only imaginary parts\n    q_imaginary = Quaternion(0, 1, 1, 1)\n    assert q_imaginary.norm() == sqrt(3)\n    assert q_imaginary.normalize() == Quaternion(0, 1/sqrt(3), 1/sqrt(3), 1/sqrt(3))\n    assert q_imaginary.inverse() == Quaternion(0, -1/3, -1/3, -1/3)\n\n    # Testing quaternion multiplication with zero quaternion\n    q1 = Quaternion(1, 2, 3, 4)\n    assert q1 * q0 == q0\n    assert q0 * q1 == q0\n\n    # Testing quaternion addition and multiplication with identity quaternion (1, 0, 0, 0)\n    q_identity = Quaternion(1, 0, 0, 0)\n    assert q1 + q_identity == Quaternion(2, 2, 3, 4)\n    assert q1 * q_identity == q1\n    assert q_identity * q1 == q1\n", "def test_quaternion_from_axis_angle():\n    v = (sqrt(3)/3, sqrt(3)/3, sqrt(3)/3)\n    angle = 2 * pi / 3\n    q = Quaternion.from_axis_angle(v, angle)\n    assert q == Quaternion(S.Half, S.Half, S.Half, S.Half)\n\n    v = (1, 0, 0)\n    angle = pi / 2\n    q = Quaternion.from_axis_angle(v, angle)\n    assert q == Quaternion(sqrt(2)/2, sqrt(2)/2, 0, 0)\n\n    v = (0, 1, 0)\n    angle = pi\n    q = Quaternion.from_axis_angle(v, angle)\n    assert q == Quaternion(0, 0, 1, 0)\n\n    v = (0, 0, 1)\n    angle = 0\n    q = Quaternion.from_axis_angle(v, angle)\n    assert q == Quaternion(1, 0, 0, 0)\n", "def test_quaternion_division():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    assert q1 / 2 == Quaternion(1/2, 1, 3/2, 2)\n    assert q1 / q2 == q1 * q2.inverse()\n\n    q3 = Quaternion(1, 0, 0, 0)\n    assert 2 / q3 == Quaternion(2, 0, 0, 0)\n\n    q4 = Quaternion(0, 1, 0, 0)\n    raises(ZeroDivisionError, lambda: 2 / q4)\n", "def test_quaternion_division():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    q3 = Quaternion(0, 1, 0, 0)\n    \n    # Division by quaternion\n    assert q1 / q2 == q1 * q2.inverse()\n    assert q2 / q1 == q2 * q1.inverse()\n\n    # Division by scalar\n    assert q1 / 2 == Quaternion(1/2, 1, 3/2, 2)\n    assert 2 / q1 == 2 * q1.inverse()\n\n    # Division by a quaternion with norm zero should raise an error\n    raises(ValueError, lambda: q1 / q3)\n    raises(ValueError, lambda: q3.inverse())\n"], "sample_1166": ["def test_monomial_class():\n    m1 = Monomial((3, 4, 1))\n    m2 = Monomial((1, 2, 0))\n\n    # Test initialization and string representation\n    assert str(m1) == \"Monomial((3, 4, 1))\"\n    assert str(m2) == \"Monomial((1, 2, 0))\"\n\n    # Test equality and inequality\n    assert m1 == Monomial((3, 4, 1))\n    assert m1 != m2\n\n    # Test multiplication\n    assert m1 * m2 == Monomial((4, 6, 1))\n\n    # Test division\n    assert m1 / m2 == Monomial((2, 2, 1))\n    raises(ExactQuotientFailed, lambda: m1 / Monomial((1, 2, 2)))\n\n    # Test power\n    assert m1 ** 2 == Monomial((6, 8, 2))\n    raises(ValueError, lambda: m1 ** -1)\n\n    # Test gcd and lcm\n    assert m1.gcd(m2) == Monomial((1, 2, 0))\n    assert m1.lcm(m2) == Monomial((3, 4, 1))\n\n    # Test as_expr\n    assert m1.as_expr(x, y, z) == x**3 * y**4 * z\n    raises(ValueError, lambda: m1.as_expr())\n\n    # Test rebuilding\n    assert m1.rebuild((1, 2, 3)) == Monomial((1, 2, 3))\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_div_exceptions():\n    assert monomial_div((3, 4, 1), (1, 2, 2)) is None\n    assert monomial_div((3, 4, 1), (0, 0, 0)) == (3, 4, 1)  # Edge case where B is zero tuple\n    raises(ValueError, lambda: monomial_div((3, 4), (1, 2, 1)))  # Different lengths\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_div_no_remainder():\n    assert monomial_div((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_div((3, 4, 1), (3, 4, 1)) == (0, 0, 0)\n    assert monomial_div((3, 4, 1), (2, 2, 0)) == (1, 2, 1)\n    assert monomial_div((3, 4, 1), (0, 0, 0)) == (3, 4, 1)\n"], "sample_1167": ["def test_unary_function_latex():\n    from sympy.functions.special.tensor_functions import KroneckerDelta\n    from sympy.functions.special.gamma_functions import gamma\n    from sympy.functions.special.delta_functions import DiracDelta\n    from sympy.functions.special.error_functions import Chi\n    from sympy import beta, lowergamma, zeta\n\n    unary_functions = [\n        KroneckerDelta,\n        gamma,\n        DiracDelta,\n        Chi,\n        beta,\n        lowergamma,\n        zeta\n    ]\n\n    for func in unary_functions:\n        # Test LaTeX representation for the function without arguments\n        assert latex(func) == rf'\\operatorname{{{func.__name__.lower()}}}'\n\n        # Test LaTeX representation for the function with one argument\n        x = Symbol('x')\n        assert latex(func(x)) == rf'\\operatorname{{{func.__name__.lower()}}}\\left({{x}}\\right)'\n\n        # Test LaTeX representation for the function squared\n        assert latex(func(x)**2) == rf'\\operatorname{{{func.__name__.lower()}}}^{{2}}\\left({{x}}\\right)'\n", "def test_latex_ComplexRegion_polar():\n    from sympy.sets import Interval\n    r = Symbol('r', positive=True)\n    theta = Symbol('theta', real=True)\n    region = ComplexRegion(Interval(0, r)*Interval(0, 2*pi), polar=True)\n    assert latex(region) == r\"\\left\\{r \\left(i \\sin{\\left(\\theta \\right)} + \\cos{\\left(\\theta \\right)}\\right)\\; \\middle|\\; r, \\theta \\in \\left[0, r\\right] \\times \\left[0, 2 \\pi\\right) \\right\\}\"\n", "def test_latex_Mul_symbol_settings():\n    # test different mul_symbol settings\n    assert latex(4*4**x, mul_symbol='times') == r\"4 \\times 4^{x}\"\n    assert latex(4*4**x, mul_symbol='dot') == r\"4 \\cdot 4^{x}\"\n    assert latex(4*4**x, mul_symbol='ldot') == r\"4 \\,.\\, 4^{x}\"\n    assert latex(4*4**x, mul_symbol=None) == r\"4 4^{x}\"\n\n    assert latex(4*x, mul_symbol='times') == r\"4 \\times x\"\n    assert latex(4*x, mul_symbol='dot') == r\"4 \\cdot x\"\n    assert latex(4*x, mul_symbol='ldot') == r\"4 \\,.\\, x\"\n    assert latex(4*x, mul_symbol=None) == r\"4 x\"\n", "def test_latex_imaginary_unit():\n    assert latex(1 + I, imaginary_unit='i') == r'1 + i'\n    assert latex(1 + I, imaginary_unit='j') == r'1 + j'\n    assert latex(1 + I, imaginary_unit='ri') == r'1 + \\mathrm{i}'\n    assert latex(1 + I, imaginary_unit='rj') == r'1 + \\mathrm{j}'\n    assert latex(1 + I, imaginary_unit='ti') == r'1 + \\text{i}'\n    assert latex(1 + I, imaginary_unit='tj') == r'1 + \\text{j}'\n", "def test_latex_BooleanTrueFalse():\n    assert latex(true) == r'\\text{True}'\n    assert latex(false) == r'\\text{False}'\n    assert latex(true & false) == r'\\text{True} \\wedge \\text{False}'\n    assert latex(true | false) == r'\\text{True} \\vee \\text{False}'\n    assert latex(~true) == r'\\neg \\text{True}'\n    assert latex(~false) == r'\\neg \\text{False}'\n    assert latex(Implies(true, false)) == r'\\text{True} \\Rightarrow \\text{False}'\n    assert latex(Equivalent(true, false)) == r'\\text{True} \\Leftrightarrow \\text{False}'\n"], "sample_1168": ["def test_interactive_traversal():\n    expr = z + w*(x + y)\n    traversal_result = []\n\n        result = interactive_traversal(expr)\n        traversal_result.append(result)\n\n    # Mocking input to simulate user interactions\n    inputs = iter(['0', 'f', '0', 'd'])\n    original_input = __builtins__.input\n    __builtins__.input = lambda _: next(inputs)\n\n    try:\n        capture(traverse_and_collect)\n    finally:\n        __builtins__.input = original_input\n\n    assert traversal_result == [z]\n\n    # Testing with another expression\n    expr = (x + y) * (x - y)\n    traversal_result = []\n\n    inputs = iter(['1', 'l', '0', 'd'])\n    __builtins__.input = lambda _: next(inputs)\n\n    try:\n        capture(traverse_and_collect)\n    finally:\n        __builtins__.input = original_input\n\n    assert traversal_result == [x]\n", "def test_is_palindromic_edge_cases():\n    assert is_palindromic([1, 2, 1])\n    assert not is_palindromic([1, 2, 3])\n    assert is_palindromic([1])\n    assert is_palindromic([1, 1])\n    assert is_palindromic([1, 2, 2, 1])\n    assert not is_palindromic([1, 2, 3, 4])\n\n    assert is_palindromic(\"radar\")\n    assert not is_palindromic(\"hello\")\n    assert is_palindromic(\"a\")\n    assert is_palindromic(\"aa\")\n    assert is_palindromic(\"abba\")\n    assert not is_palindromic(\"abcd\")\n", "def test_ibin():\n    assert ibin(2) == [1, 0]\n    assert ibin(2, 4) == [0, 0, 1, 0]\n    assert list(ibin(2, 'all')) == [(0, 0), (0, 1), (1, 0), (1, 1)]\n    assert ibin(123, 10, str=True) == '0001111011'\n    assert ibin(123, 10, str=True)[::-1] == '1101111000'\n    assert list(ibin(3, 'all', str=True)) == ['000', '001', '010', '011', '100', '101', '110', '111']\n    raises(ValueError, lambda: ibin(-1))  # negative number should raise an error\n", "def test_interactive_traversal():\n    expr = z + w*(x + y)\n    results = []\n\n        response = results.pop(0)\n        if isinstance(response, Exception):\n            raise response\n        return response\n\n    import builtins\n    original_input = builtins.input\n    builtins.input = mock_input\n    try:\n        results = ['?', 'f', 'd']\n        result = interactive_traversal(expr)\n        assert result == w, \"Interactive traversal did not return the expected result.\"\n\n        results = ['1', '1', 'd']\n        result = interactive_traversal(expr)\n        assert result == y, \"Interactive traversal did not return the expected result.\"\n\n        results = ['0', 'd']\n        result = interactive_traversal(expr)\n        assert result == z, \"Interactive traversal did not return the expected result.\"\n\n    finally:\n        builtins.input = original_input\n", "def test_is_palindromic_edge_cases():\n    assert is_palindromic([1, 2, 1])\n    assert not is_palindromic([1, 2, 3])\n    assert is_palindromic([1, 2, 2, 1])\n    assert not is_palindromic([1, 2, 3, 2])\n    assert is_palindromic([1, 2, 3, 2, 1])\n    assert is_palindromic([1, 2, 3, 3, 2, 1])\n    assert not is_palindromic([1, 2, 3, 4, 2, 1])\n    assert is_palindromic([1] * 1000)\n    assert not is_palindromic([1, 2] * 500)\n"], "sample_1169": ["def test_FockState():\n    # Testing FockState class directly\n\n    # Case 1: Creating a FockState with integer occupation numbers\n    fs1 = FockState([0, 1, 2, 3])\n    assert len(fs1) == 4\n    assert fs1[0] == 0\n    assert fs1[1] == 1\n    assert fs1[2] == 2\n    assert fs1[3] == 3\n    assert fs1.__repr__() == \"FockState((0, 1, 2, 3))\"\n\n    # Case 2: Creating a FockState with symbolic occupation numbers\n    n, m = symbols('n m')\n    fs2 = FockState([n, m, n+m])\n    assert len(fs2) == 3\n    assert fs2[0] == n\n    assert fs2[1] == m\n    assert fs2[2] == n + m\n    assert fs2.__repr__() == \"FockState((n, m, n + m))\"\n\n    # Case 3: Testing __getitem__ with out-of-bounds index\n    raises(IndexError, lambda: fs1[5])\n\n    # Case 4: Testing __str__\n    assert str(fs1) == \"|(0, 1, 2, 3)>\"\n    assert str(fs2) == \"|(n, m, n + m)>\"\n\n    # Case 5: Testing __len__ method\n    assert len(fs1) == 4\n    assert len(fs2) == 3\n", "def test_tensor_symbol():\n    i, j = symbols('i,j', below_fermi=True)\n    a, b = symbols('a,b', above_fermi=True)\n    t = AntiSymmetricTensor('t', (a, b), (i, j))\n    \n    assert isinstance(t, AntiSymmetricTensor)\n    assert t.symbol == Symbol('t')\n    assert t.upper == (a, b)\n    assert t.lower == (i, j)\n    assert t._latex(None) == '{t^{ab}_{ij}}'\n    assert str(t) == 't((_a, _b),(_i, _j))'\n    assert repr(t) == \"t((_a, _b), (_i, _j))\"\n    \n    # Check antisymmetry\n    assert AntiSymmetricTensor('t', (b, a), (i, j)) == -t\n    assert AntiSymmetricTensor('t', (a, b), (j, i)) == -t\n    assert AntiSymmetricTensor('t', (b, a), (j, i)) == t\n    \n    # Check substitutions\n    assert t.subs(a, Symbol('c')) == AntiSymmetricTensor('t', (Symbol('c'), b), (i, j))\n    assert t.subs(i, Symbol('k')) == AntiSymmetricTensor('t', (a, b), (Symbol('k'), j))\n    \n    # Ensure zero for repeated indices\n    assert AntiSymmetricTensor('t', (a, a), (i, j)) == 0\n    assert AntiSymmetricTensor('t', (a, b), (i, i)) == 0\n", "def test_bosonic_operator():\n    # Test for BosonicOperator class\n\n    # Check if BosonicOperator can be instantiated\n    i = symbols('i')\n    b_op = BosonicOperator(i)\n    assert isinstance(b_op, BosonicOperator)\n\n    # Check state property\n    assert b_op.state == i\n\n    # Check if state is symbolic\n    assert b_op.is_symbolic\n\n    # Check if state is not symbolic\n    j = 1\n    b_op_num = BosonicOperator(j)\n    assert not b_op_num.is_symbolic\n\n    # Check representation\n    assert str(b_op) == 'sq(i)'\n    assert repr(b_op) == 'sq(i)'\n\n    # Check doit method\n    assert b_op.doit() == b_op\n\n    # Check __new__ method with sympified argument\n    assert BosonicOperator('i').state == symbols('i')\n", "def test_anti_symmetric_tensor():\n    # Testing AntiSymmetricTensor properties and methods\n    i, j, a, b = symbols('i j a b', cls=Dummy)\n    tensor = AntiSymmetricTensor('T', (a, b), (i, j))\n\n    # Check symbol property\n    assert tensor.symbol == Symbol('T')\n\n    # Check upper indices\n    assert tensor.upper == (a, b)\n\n    # Check lower indices\n    assert tensor.lower == (i, j)\n\n    # Test str and repr methods\n    assert str(tensor) == \"T((_a, _b),(_i, _j))\"\n    assert repr(tensor) == \"T((_a, _b),(_i, _j))\"\n\n    # Test latex method\n    assert latex(tensor) == \"{T^{ab}_{ij}}\"\n\n    # Test doit method\n    assert tensor.doit() == tensor\n\n    # Test antisymmetry\n    assert tensor == -AntiSymmetricTensor('T', (b, a), (i, j))\n    assert tensor == AntiSymmetricTensor('T', (a, b), (j, i))\n\n    # Substitution tests\n    assert tensor.subs(a, Symbol('c')) == AntiSymmetricTensor('T', (Symbol('c'), b), (i, j))\n    assert tensor.subs(i, Symbol('k')) == AntiSymmetricTensor('T', (a, b), (Symbol('k'), j))\n\n    # Zero tensor tests\n    assert AntiSymmetricTensor('T', (a, a), (i, j)) == 0\n    assert AntiSymmetricTensor('T', (a, b), (i, i)) == 0\n\n    # Test sort key method\n    assert tensor._sortkey(a) < tensor._sortkey(b)\n    assert tensor._sortkey(i) > tensor._sortkey(j)\n", "def test_apply_operators_with_multiple_terms():\n    n, m, p = symbols(\"n, m, p\")\n    e1 = B(0)*BKet([n])\n    e2 = Bd(0)*BKet([m])\n    e3 = B(1)*Bd(1)*BKet([p])\n    e = e1 + e2 + e3\n    assert apply_operators(e) == sqrt(n)*BKet([n - 1]) + sqrt(m + 1)*BKet([m + 1]) + (p + 1)*BKet([p])\n"], "sample_1170": ["def test_BooleanTrueFalse():\n    assert str(true) == \"True\"\n    assert str(false) == \"False\"\n", "def test_TribonacciConstant():\n    assert str(TribonacciConstant) == \"TribonacciConstant\"\n", "def test_Function_with_args():\n    f = Function('f')\n    g = Function('g')\n    assert str(f(x)) == \"f(x)\"\n    assert str(f(x, y)) == \"f(x, y)\"\n    assert str(f(x, y, z)) == \"f(x, y, z)\"\n    assert str(g(x)) == \"g(x)\"\n    assert str(g(x + y, z)) == \"g(x + y, z)\"\n    assert str(g(f(x), z)) == \"g(f(x), z)\"\n    assert str(f(x, g(y, z))) == \"f(x, g(y, z))\"\n", "def test_Or():\n    assert str(Or(x, y)) == \"x | y\"\n    assert str(Or(x, y, z)) == \"x | y | z\"\n    assert str(Or(And(x, y), z)) == \"(x & y) | z\"\n    assert str(Or(x, And(y, z))) == \"x | (y & z)\"\n    assert str(Or(x, Or(y, z))) == \"x | y | z\"\n    assert str(Or(Or(x, y), z)) == \"x | y | z\"\n", "def test_ImaginaryUnit_in_expr():\n    expr1 = I * x\n    expr2 = I + x\n    expr3 = I - x\n    expr4 = I / x\n    expr5 = I**2\n    assert str(expr1) == \"I*x\"\n    assert str(expr2) == \"I + x\"\n    assert str(expr3) == \"I - x\"\n    assert str(expr4) == \"I/x\"\n    assert str(expr5) == \"-1\"\n"], "sample_1171": ["def test_integers_contains_symbolic():\n    i = Symbol('i', integer=True)\n    f = Function('f', integer=True)\n    z = Symbol('z')\n    assert S.Integers.contains(i) == Eq(i, i)  # Contains should work with integer symbols\n    assert S.Integers.contains(f(z)) == Eq(f(z), f(z))  # Contains should work with integer functions\n    assert S.Integers.contains(2*i + 3) == Eq(Mod(2*i + 3, 1), 0)  # Contains should handle expressions\n    assert S.Integers.contains(3*f(z) - 4) == Eq(Mod(3*f(z) - 4, 1), 0)  # Contains should handle functions in expressions\n", "def test_rationals_contains():\n    R = S.Rationals\n    assert 0 in R\n    assert 1 in R\n    assert -1 in R\n    assert Rational(3, 4) in R\n    assert Rational(-3, 4) in R\n    assert 0.5 not in R\n    assert sqrt(2) not in R\n    assert pi not in R\n    assert I not in R\n", "def test_rationals_boundary():\n    assert S.Rationals.boundary == S.Reals\n    assert S.Rationals.closure == S.Reals\n    assert S.Rationals.is_open == False\n    assert S.Rationals.is_closed == False\n", "def test_Rationals_contains():\n    r = S.Rationals\n    assert r.contains(1) == True\n    assert r.contains(Rational(2, 3)) == True\n    assert r.contains(sqrt(2)) == False\n    assert r.contains(pi) == False\n    assert r.contains(I) == False\n    assert r.contains(oo) == False\n    assert r.contains(-oo) == False\n\n    # Test with symbolic expressions\n    expr = Rational(1, 2) + Rational(1, 3)\n    assert r.contains(expr) == True\n\n    symb = Symbol('a', real=True)\n    assert r.contains(symb) == Contains(symb, r, evaluate=False)\n\n    symb = Symbol('b', integer=True)\n    assert r.contains(symb) == True\n\n    symb = Symbol('c', rational=True)\n    assert r.contains(symb) == True\n\n    # Test with non-Expr types\n    assert r.contains(\"1/2\") == False\n    assert r.contains([1, 2, 3]) == False\n", "def test_rationals_contains():\n    R = S.Rationals\n    assert R._contains(S.Half) == True\n    assert R._contains(1/3) == True\n    assert R._contains(3) == True\n    assert R._contains(sqrt(2)) == False\n    assert R._contains(I) == False\n    assert R._contains(pi) == False\n"], "sample_1172": ["def test_solve_generic():\n    # Test a simple system of linear equations\n    assert solve_generic([Poly(x + y - 1), Poly(x - y - 1)], Options((x, y), {})) == [(1, 0)]\n\n    # Test a system with no solution\n    assert solve_generic([Poly(x + y - 1), Poly(x + y - 2)], Options((x, y), {})) is None\n\n    # Test a system with multiple solutions\n    assert solve_generic([Poly(x**2 - y**2), Poly(x - 1)], Options((x, y), {})) == [(1, -1), (1, 1)]\n\n    # Test a more complex system\n    assert solve_generic([Poly(x**2 + y, x, y, domain='ZZ'), Poly(x + y*4, x, y, domain='ZZ')],\n                         Options((x, y), {'domain': 'ZZ'})) == [(0, 0), (1/4, -1/16)]\n\n    # Test a system with irrational solutions\n    dom = QQ.algebraic_field(sqrt(2))\n    assert solve_generic([Poly(x**2 - 2, x, domain=dom)], Options((x,), {'domain': dom})) == [(-sqrt(2),), (sqrt(2),)]\n\n    # Test raising CoercionFailed\n    raises(NotImplementedError, lambda: solve_generic([Poly(x**2 + y**2)], Options((x, y, z), {})))\n", "def test_solve_generic():\n    from sympy.polys import Options\n\n    f_1 = Poly(x**2 + y, x, y, domain='ZZ')\n    f_2 = Poly(x + 4*y, x, y, domain='ZZ')\n    opt = Options((x, y), {'domain': 'ZZ'})\n\n    assert solve_generic([f_1, f_2], opt) == [(0, 0), (1/4, -1/16)]\n\n    f_3 = Poly(x**2 - y**2, x, y, domain='ZZ')\n    f_4 = Poly(x - 1, x, y, domain='ZZ')\n\n    assert solve_generic([f_3, f_4], opt) == [(1, 1), (1, -1)]\n\n    f_5 = Poly(x**3 - y**3, x, y, domain='ZZ')\n    f_6 = Poly(x - 1, x, y, domain='ZZ')\n\n    raises(NotImplementedError, lambda: solve_generic([f_5, f_6], opt))\n\n    f_7 = Poly(x**2 + y**2 - 2, x, y, domain='ZZ')\n    f_8 = Poly(y**2 - 1, x, y, domain='ZZ')\n\n    assert solve_generic([f_7, f_8], opt) == [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n\n    f_9 = Poly(x*y - y, x, y, domain='ZZ')\n    f_10 = Poly(x**2 - x, x, y, domain='ZZ')\n\n    raises(SolveFailed, lambda: solve_generic([f_9, f_10], opt))\n", "def test_solve_generic():\n    from sympy.polys.polytools import Options\n\n    f_1 = Poly(x**2 + y, x, y, domain='ZZ')\n    f_2 = Poly(x + y*4, x, y, domain='ZZ')\n    opt = Options((x, y), {'domain': 'ZZ'})\n\n    assert solve_generic([f_1, f_2], opt) == [(0, 0), (Rational(1, 4), Rational(-1, 16))]\n\n    f_3 = Poly(x**2 + y**2 - 1, x, y, domain='ZZ')\n    f_4 = Poly(x + y - 1, x, y, domain='ZZ')\n    assert solve_generic([f_3, f_4], opt) == [(Rational(1, 2), Rational(1, 2)), (-Rational(1, 2), Rational(3, 2))]\n\n    f_5 = Poly(x**2 - y, x, y, domain='ZZ')\n    f_6 = Poly(y - x**2, x, y, domain='ZZ')\n    assert solve_generic([f_5, f_6], opt) == [(0, 0)]\n\n    f_7 = Poly(x**3 - y**3, x, y, domain='ZZ')\n    f_8 = Poly(x - y, x, y, domain='ZZ')\n    with raises(NotImplementedError):\n        solve_generic([f_7, f_8], opt)\n\n    f_9 = Poly(x**2 + 1, x, domain='ZZ')\n    opt = Options((x,), {'domain': 'ZZ'})\n    assert solve_generic([f_9], opt) == [(-I, ), (I, )]\n", "def test_solve_generic():\n    from sympy.polys import Options\n\n    NewOption = Options((x, y), {'domain': 'ZZ'})\n\n    # Test basic solving with generic solver\n    a = Poly(x - y + 5, x, y, domain='ZZ')\n    b = Poly(x + y - 3, x, y, domain='ZZ')\n    assert solve_generic([a, b], NewOption) == [(-1, 4)]\n\n    # Test solving with fractions\n    a = Poly(x - 2*y + 5, x, y, domain='ZZ')\n    b = Poly(2*x - y - 3, x, y, domain='ZZ')\n    assert solve_generic([a, b], NewOption) == [(11/3, 13/3)]\n\n    # Test solving with more complex system\n    a = Poly(x**2 + y, x, y, domain='ZZ')\n    b = Poly(x + y*4, x, y, domain='ZZ')\n    assert solve_generic([a, b], NewOption) == [(0, 0), (1/4, -1/16)]\n\n    # Test case that should raise NotImplementedError for non-zero-dimensional system\n    a = Poly(x**2 + y**2 - 1, x, y, domain='ZZ')\n    b = Poly(x + y - 1, x, y, domain='ZZ')\n    raises(NotImplementedError, lambda: solve_generic([a, b], NewOption))\n\n    # Test case that should raise NotImplementedError for coercion failure\n    raises(NotImplementedError, lambda: solve_generic([Poly(x + y + I, x, y)], NewOption))\n", "def test_solve_generic():\n    NewOption = Poly(x + y).get_options()\n\n    # Test case 1: simple linear system\n    a = Poly(x + y - 2, x, y, domain='ZZ')\n    b = Poly(x - y - 2, x, y, domain='ZZ')\n    assert solve_generic([a, b], NewOption) == [(2, 0)]\n\n    # Test case 2: quadratic system\n    a = Poly(x**2 - y**2 - 1, x, y, domain='ZZ')\n    b = Poly(x - y - 1, x, y, domain='ZZ')\n    assert solve_generic([a, b], NewOption) == [(2, 1), (-1, -2)]\n\n    # Test case 3: cubic system (should raise NotImplementedError)\n    a = Poly(x**3 - y, x, y, domain='ZZ')\n    b = Poly(x - y - 1, x, y, domain='ZZ')\n    raises(NotImplementedError, lambda: solve_generic([a, b], NewOption))\n\n    # Test case 4: no solution system\n    a = Poly(x + y - 1, x, y, domain='ZZ')\n    b = Poly(x + y - 3, x, y, domain='ZZ')\n    assert solve_generic([a, b], NewOption) == []\n\n    # Test case 5: redundant system\n    a = Poly(x + y - 1, x, y, domain='ZZ')\n    b = Poly(2*x + 2*y - 2, x, y, domain='ZZ')\n    assert solve_generic([a, b], NewOption) == [(1, 0)]\n"], "sample_1173": ["def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    f = Symbol('f')\n    x = Symbol('x')\n    assert parse_expr('lambda x: x + 1', transformations=transformations) == Function('Lambda')(x, x + 1)\n    assert parse_expr('lambda x, y: x + y', transformations=transformations) == Function('Lambda')((x, Symbol('y')), x + Symbol('y'))\n    \n    raises(TokenError, lambda: parse_expr('lambda *args: sum(args)', transformations=transformations))\n    raises(TokenError, lambda: parse_expr('lambda **kwargs: sum(kwargs.values())', transformations=transformations))\n", "def test_auto_symbol():\n    local_dict = {}\n    global_dict = {}\n    inputs = {\n        'x': Symbol('x'),\n        'y': Symbol('y'),\n        'sin(x)': Function('sin')(Symbol('x')),\n        'x + y': Symbol('x') + Symbol('y'),\n        '2*x + 3*y': 2*Symbol('x') + 3*Symbol('y'),\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, local_dict=local_dict, global_dict=global_dict) == result\n\n    local_dict = {'x': Symbol('x')}\n    inputs = {\n        'x': Symbol('x'),\n        'y': Symbol('y'),\n        'x + y': Symbol('x') + Symbol('y'),\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, local_dict=local_dict, global_dict=global_dict) == result\n", "def test_auto_symbol_transformation():\n    transformations = standard_transformations + (implicit_multiplication,)\n    local_dict = {}\n    global_dict = {}\n    assert parse_expr('x', local_dict=local_dict, transformations=transformations, global_dict=global_dict) == Symbol('x')\n    assert parse_expr('my_var', local_dict=local_dict, transformations=transformations, global_dict=global_dict) == Symbol('my_var')\n    assert parse_expr('x', local_dict={'x': 10}, transformations=transformations, global_dict=global_dict) == 10\n    assert parse_expr('foo', local_dict={'foo': Function('foo')}, transformations=transformations, global_dict=global_dict) == Function('foo')\n    assert parse_expr('bar', local_dict={'bar': Symbol('bar')}, transformations=transformations, global_dict=global_dict) == Symbol('bar')\n    # Test for automatically converting undefined variables to Symbols\n    assert parse_expr('a + b', local_dict=local_dict, transformations=transformations, global_dict=global_dict) == Symbol('a') + Symbol('b')\n    assert parse_expr('alpha + beta', local_dict=local_dict, transformations=transformations, global_dict=global_dict) == Symbol('alpha') + Symbol('beta')\n", "def test_implicit_multiplication_application():\n    transformations = standard_transformations + (implicit_multiplication_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"2sin(x)cos(x)\", transformations=transformations) == 2*sin(x)*cos(x)\n    assert parse_expr(\"3x(y+1)\", transformations=transformations) == 3*x*(y+1)\n    assert parse_expr(\"(x+2)(x+3)\", transformations=transformations) == (x+2)*(x+3)\n    assert parse_expr(\"2sin x\", transformations=transformations) == 2*sin(x)\n    assert parse_expr(\"cos x sin(x)\", transformations=transformations) == cos(x)*sin(x)\n    assert parse_expr(\"(x+1)sin(x)\", transformations=transformations) == (x+1)*sin(x)\n", "def test_auto_symbol_with_custom_local_dict():\n    local_dict = {'custom_var': 5, 'custom_func': lambda x: x * 2}\n    inputs = {\n        'custom_var + 3': Integer(8),\n        'custom_func(4)': Integer(8),\n        'undefined_var': Symbol('undefined_var'),\n        'undefined_func(3)': Function('undefined_func')(3)\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, local_dict=local_dict) == result\n"], "sample_1174": ["def test_rewrite_as_re_im():\n    x, y = symbols('x y')\n    assert re(x + y).rewrite(re) == re(x) + re(y)\n    assert re(x + y).rewrite(im) == x + y - I * (im(x) + im(y))\n    assert im(x + y).rewrite(re) == im(x) + im(y)\n    assert im(x + y).rewrite(im) == im(x + y)\n    assert re(x * y).rewrite(re) == re(x) * re(y) - im(x) * im(y)\n    assert re(x * y).rewrite(im) == x * y - I * (im(x * y))\n    assert im(x * y).rewrite(re) == re(x) * im(y) + im(x) * re(y)\n    assert im(x * y).rewrite(im) == im(x * y)\n    assert re(x/y).rewrite(re) == (re(x) * re(y) + im(x) * im(y)) / (re(y)**2 + im(y)**2)\n    assert re(x/y).rewrite(im) == (x * y.conjugate()).expand() / (re(y)**2 + im(y)**2)\n    assert im(x/y).rewrite(re) == (im(x) * re(y) - re(x) * im(y)) / (re(y)**2 + im(y)**2)\n    assert im(x/y).rewrite(im) == ((x * y.conjugate()).expand(im=True) / (re(y)**2 + im(y)**2)).expand(im=True)\n", "def test_abs_matrix():\n    A = Matrix([[1 + 2*I, 3 - 4*I], [0, -5*I]])\n    expected = Matrix([[sqrt(5), 5], [0, 5]])\n    assert abs(A) == expected\n\n    B = SparseMatrix([[1 - I, 2], [I, -3 - 2*I]])\n    expected_sparse = SparseMatrix([[sqrt(2), 2], [1, sqrt(13)]])\n    assert abs(B) == expected_sparse\n\n    C = ImmutableMatrix([[1 + I, 3 - I], [0, -I]])\n    expected_immutable = ImmutableMatrix([[sqrt(2), sqrt(10)], [0, 1]])\n    assert abs(C) == expected_immutable\n\n    D = ImmutableSparseMatrix([[1 + I, 0], [I, 1 - I]])\n    expected_immutable_sparse = ImmutableSparseMatrix([[sqrt(2), 0], [1, sqrt(2)]])\n    assert abs(D) == expected_immutable_sparse\n", "def test_polar_lift():\n    from sympy.functions.elementary.complexes import polar_lift, exp_polar\n\n    x = Symbol('x')\n    y = Symbol('y')\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n\n    assert polar_lift(1) == 1 * exp_polar(0)\n    assert polar_lift(-1) == 1 * exp_polar(I * pi)\n    assert polar_lift(I) == exp_polar(-I * pi / 2)\n    assert polar_lift(-I) == exp_polar(I * pi / 2)\n    assert polar_lift(2 * I) == 2 * exp_polar(-I * pi / 2)\n    assert polar_lift(2 * exp_polar(I * pi)) == 2 * exp_polar(I * pi)\n    assert polar_lift(exp_polar(I * pi)) == exp_polar(I * pi)\n    \n    assert polar_lift(x).is_polar is True\n    assert polar_lift(2 * x) == 2 * polar_lift(x)\n    assert polar_lift(x * y) == polar_lift(x) * polar_lift(y)\n    assert polar_lift(p) == p\n    assert polar_lift(n) == -n\n    \n    raises(TypeError, lambda: polar_lift(Interval(2, 3)))  # issue 8717\n    raises(TypeError, lambda: polar_lift({1, 2, 3}))      # invalid argument type\n    raises(TypeError, lambda: polar_lift([1, 2, 3]))      # invalid argument type\n", "def test_rewrite_as_re_im():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Testing rewrite as re and im for re() function\n    assert re(x + I*y).rewrite(re, im) == re(x) - im(y)\n    assert re(2*x + I*y).rewrite(re, im) == 2*re(x) - im(y)\n    assert re(2*x + 3*I*y).rewrite(re, im) == 2*re(x) - 3*im(y)\n\n    # Testing rewrite as re and im for im() function\n    assert im(x + I*y).rewrite(re, im) == im(x) + re(y)\n    assert im(2*x + I*y).rewrite(re, im) == im(x) + re(y)\n    assert im(2*x + 3*I*y).rewrite(re, im) == im(x) + 3*re(y)\n\n    # Ensure rewrites are correct when nested\n    assert re(im(x + I*y)).rewrite(re, im) == re(im(x) + re(y))\n    assert im(re(x + I*y)).rewrite(re, im) == im(re(x) - im(y))\n", "def test_abs_matrix():\n    # Test Abs with different types of matrices\n    M = Matrix([[1 + I, -3], [2*I, -4 - I]])\n    assert Abs(M) == Matrix([[Abs(1 + I), Abs(-3)], [Abs(2*I), Abs(-4 - I)]])\n    \n    IM = ImmutableMatrix([[1 + I, -3], [2*I, -4 - I]])\n    assert Abs(IM) == ImmutableMatrix([[Abs(1 + I), Abs(-3)], [Abs(2*I), Abs(-4 - I)]])\n    \n    SM = SparseMatrix([[1 + I, -3], [2*I, -4 - I]])\n    assert Abs(SM) == SparseMatrix([[Abs(1 + I), Abs(-3)], [Abs(2*I), Abs(-4 - I)]])\n    \n    ISM = ImmutableSparseMatrix([[1 + I, -3], [2*I, -4 - I]])\n    assert Abs(ISM) == ImmutableSparseMatrix([[Abs(1 + I), Abs(-3)], [Abs(2*I), Abs(-4 - I)]])\n    \n    n, m = symbols('n m')\n    MS = MatrixSymbol('A', n, m)\n    assert Abs(MS) == MatrixSymbol('Abs(A)', n, m)\n    \n    FM = FunctionMatrix(2, 2, Lambda((i, j), i + j*I))\n    assert Abs(FM) == Matrix([[0, 1], [1, sqrt(2)]])\n"], "sample_1175": ["def test_prettyprinter_settings():\n    pp = PrettyPrinter()\n    assert pp._settings['order'] is None\n    assert pp._settings['full_prec'] == 'auto'\n    assert pp._settings['use_unicode'] is None\n    assert pp._settings['wrap_line'] is True\n    assert pp._settings['num_columns'] is None\n    assert pp._settings['use_unicode_sqrt_char'] is True\n    assert pp._settings['root_notation'] is True\n    assert pp._settings['mat_symbol_style'] == 'plain'\n    assert pp._settings['imaginary_unit'] == 'i'\n    assert pp._settings['perm_cyclic'] is True\n\n    pp2 = PrettyPrinter({'use_unicode': False, 'imaginary_unit': 'j'})\n    assert pp2._settings['use_unicode'] is False\n    assert pp2._settings['imaginary_unit'] == 'j'\n", "def test_issue_18456():\n    expr = Matrix([[2*x, 1/y], [z, x+y]])\n    ascii_str = \\", "def test_pretty_lambdified_functions():\n    from sympy import lambdify\n    f = lambdify(x, x + 1, 'numpy')\n    g = lambdify((x, y), x*y + 1, 'numpy')\n\n    expr = f(x)\n    assert pretty(expr) == 'x + 1'\n    assert upretty(expr) == 'x + 1'\n\n    expr = g(x, y)\n    assert pretty(expr) == 'x*y + 1'\n    assert upretty(expr) == 'x\u22c5y + 1'\n\n    expr = lambdify(x, x**2, 'numpy')(x)\n    assert pretty(expr) == 'x**2'\n    assert upretty(expr) == 'x\u00b2'\n", "def test_pretty_print_Piecewise_with_abs():\n    expr = Piecewise((Abs(x), x < 1), (x**2, True))\n    ascii_str = \\", "def test_prettyprinter_imaginary_unit():\n    # Test for setting the imaginary unit to 'j' in PrettyPrinter\n    settings = {\"imaginary_unit\": \"j\"}\n    pp = PrettyPrinter(settings)\n\n    expr = 1 + I\n    expected_str = \"1 + j\"\n    assert pp.doprint(expr) == expected_str\n\n    expr = 1 - I\n    expected_str = \"1 - j\"\n    assert pp.doprint(expr) == expected_str\n\n    expr = I**2\n    expected_str = \"-1\"\n    assert pp.doprint(expr) == expected_str\n\n    expr = I*2\n    expected_str = \"2*j\"\n    assert pp.doprint(expr) == expected_str\n"], "sample_1176": ["def test_mpf_norm_special_cases():\n    # Test special cases for mpf_norm function\n    assert mpf_norm((1, 0, 0, 0), 10) == mpf('0')._mpf_  # Should be zero\n    assert mpf_norm((0, 123456789, 10, 30), 20) == (0, 123456789, 10, 30)  # Should be normalized\n    assert mpf_norm((0, 1, 2**70, 70), 30) == (0, 1, 2**70, 30)  # Large exponent should normalize\n    ", "def test_comp_edge_cases():\n    # Edge cases for `comp` function\n    assert comp(0, 0)\n    assert comp(1, 1)\n    assert comp(-1, -1)\n    assert comp(0.0, 0)\n    assert comp(-0.0, 0)\n    assert comp(1e-10, 1e-10)\n    assert not comp(1e-10, 2e-10)\n    assert comp(S(1)/3, 1/3)\n    assert not comp(S(1)/3, 1/2)\n    assert comp(pi.evalf(10), pi.n(10))\n    assert not comp(pi.evalf(10), 3.14159)\n    assert comp(I, I)\n    assert not comp(I, -I)\n    assert comp(1 + I, 1 + I)\n    assert not comp(1 + I, 1 - I)\n    assert comp(oo, oo)\n    assert not comp(oo, -oo)\n    assert comp(zoo, zoo)\n    assert not comp(zoo, oo)\n    assert comp(nan, nan)\n    assert not comp(nan, 1)\n    assert comp(1 + sqrt(2), 1 + sqrt(2), tol=1e-10)\n    assert not comp(1 + sqrt(2), 1 + sqrt(3), tol=1e-10)\n    assert comp(1 + sqrt(2)*I, 1 + sqrt(2)*I, tol=1e-10)\n    assert not comp(1 + sqrt(2)*I, 1 + sqrt(3)*I, tol=1e-10)\n", "def test_issue_12345():\n    \"\"\"Test _as_integer_ratio for different types of numbers\"\"\"\n    assert _as_integer_ratio(1.5) == (3, 2)\n    assert _as_integer_ratio(Float('1.25')) == (5, 4)\n    assert _as_integer_ratio(2) == (2, 1)\n    assert _as_integer_ratio(-3.5) == (-7, 2)\n    assert _as_integer_ratio(S.Half) == (1, 2)\n    raises(ValueError, lambda: _as_integer_ratio(I))  # complex numbers not supported\n    raises(TypeError, lambda: _as_integer_ratio('string'))  # non-numeric type not supported\n", "def test_mpf_norm_special_cases():\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((1, 0, 0, 0), 10) == (1, 0, 0, 0)\n    assert mpf_norm((-1, 0, 0, 0), 10) == (-1, 0, 0, 0)\n    assert mpf_norm((0, 1, 0, 0), 10) == (0, 1, 0, 0)\n    assert mpf_norm((1, 1, 0, 0), 10) == (1, 1, 0, 0)\n    assert mpf_norm((-1, 1, 0, 0), 10) == (-1, 1, 0, 0)\n    assert mpf_norm((0, 0, 1, 0), 10) == (0, 0, 1, 0)\n    assert mpf_norm((1, 0, 1, 0), 10) == (1, 0, 1, 0)\n    assert mpf_norm((-1, 0, 1, 0), 10) == (-1, 0, 1, 0)\n    assert mpf_norm((0, 1, 1, 0), 10) == (0, 1, 1, 0)\n    assert mpf_norm((1, 1, 1, 0), 10) == (1, 1, 1, 0)\n    assert mpf_norm((-1, 1, 1, 0), 10) == (-1, 1, 1, 0)\n", "def test_comp_edge_cases():\n    # Test edge cases for the `comp` function.\n    assert comp(0, \"0\")\n    assert not comp(1, \"0\")\n    assert comp(0, 0)\n    assert comp(1, 1)\n    assert comp(1.0, \"1.0\")\n    assert not comp(1.0, \"1.1\")\n    assert comp(1.0000000001, \"1.0000000001\")\n    assert not comp(1.0000000001, \"1.0000000002\")\n    assert comp(1.0000000001, 1.0000000001)\n    assert not comp(1.0000000001, 1.0000000002)\n    assert comp(Float(\"1.0000000001\", 15), \"1.0000000001\")\n    assert not comp(Float(\"1.0000000001\", 15), \"1.0000000002\")\n    assert comp(S.Half, \"0.5\")\n    assert not comp(S.Half, \"0.6\")\n    assert comp(S.Half, 0.5)\n    assert not comp(S.Half, 0.6)\n    assert comp(pi, str(pi.evalf(15)))\n    assert not comp(pi, str((pi + 1).evalf(15)))\n    assert comp(pi, pi)\n    assert not comp(pi, E)\n    assert comp(1/pi, str((1/pi).evalf(15)))\n    assert not comp(1/pi, str((1/pi + 1).evalf(15)))\n    assert comp(1/pi, 1/pi)\n    assert not comp(1/pi, 1/E)\n    assert not comp(pi + I, pi)\n    assert not comp(pi + I, E)\n"], "sample_1177": ["def test_conjugate_expressions():\n    x, y = symbols('x y')\n    a, b = symbols('a b', real=True)\n\n    assert conjugate(x + y*I) == conjugate(x) - y*I\n    assert conjugate(a + b*I) == a - b*I\n    assert conjugate(x*y) == conjugate(x) * conjugate(y)\n    assert conjugate(a*b) == a*b\n    assert conjugate(a*y) == a*conjugate(y)\n    assert conjugate(x**2) == conjugate(x)**2\n    assert conjugate((a + b*I)**2) == (a - b*I)**2\n\n    assert conjugate(exp(x + y*I)) == exp(conjugate(x + y*I))\n    assert conjugate(log(x + y*I)) == log(conjugate(x + y*I))\n\n    expr = (a + b*I) / (x + y*I)\n    assert conjugate(expr) == conjugate(a + b*I) / conjugate(x + y*I)\n\n    expr = (x + y*I)**3\n    assert conjugate(expr) == conjugate(x + y*I)**3\n\n    expr = sqrt(a + b*I)\n    assert conjugate(expr) == sqrt(conjugate(a + b*I))\n\n    expr = sin(x + y*I)\n    assert conjugate(expr) == sin(conjugate(x + y*I))\n\n    expr = cos(x + y*I)\n    assert conjugate(expr) == cos(conjugate(x + y*I))\n\n    expr = tan(x + y*I)\n    assert conjugate(expr) == tan(conjugate(x + y*I))\n\n    expr = sinh(x + y*I)\n    assert conjugate(expr) == sinh(conjugate(x + y*I))\n\n    expr = cosh(x + y*I)\n    assert conjugate(expr) == cosh(conjugate(x + y*I))\n\n    expr = tanh(x + y*I)\n    assert conjugate(expr) == tanh(conjugate(x + y*I))\n", "def test_conjugate_matrix():\n    # Test conjugate for various matrix types\n\n    # Basic dense matrix\n    A = Matrix([[1 + 2*I, 3 - 4*I], [5 + 6*I, 7 - 8*I]])\n    assert conjugate(A) == Matrix([[1 - 2*I, 3 + 4*I], [5 - 6*I, 7 + 8*I]])\n\n    # Immutable dense matrix\n    B = ImmutableMatrix([[1 + I, 2 - I], [3 + I, 4 - I]])\n    assert conjugate(B) == ImmutableMatrix([[1 - I, 2 + I], [3 - I, 4 + I]])\n\n    # Sparse matrix\n    C = SparseMatrix([[0, 1 + I], [2 - I, 0]])\n    assert conjugate(C) == SparseMatrix([[0, 1 - I], [2 + I, 0]])\n\n    # Immutable sparse matrix\n    D = ImmutableSparseMatrix([[0, 1 + I], [2 - I, 0]])\n    assert conjugate(D) == ImmutableSparseMatrix([[0, 1 - I], [2 + I, 0]])\n\n    # Function matrix\n    E = FunctionMatrix(2, 2, Lambda((i, j), i + j*I))\n    assert conjugate(E) == FunctionMatrix(2, 2, Lambda((i, j), i - j*I))\n\n    # Matrix symbol\n    n, m = symbols('n m')\n    X = MatrixSymbol('X', n, m)\n    assert conjugate(X) == adjoint(X)\n", "def test_conjugate_derivative():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    z = Symbol('z', complex=True)\n    f = Function('f')\n\n    assert conjugate(Derivative(f(x), x)).doit() == conjugate(Derivative(f(x), x).doit())\n    assert conjugate(Derivative(f(y), y)).doit() == conjugate(Derivative(f(y), y).doit())\n    assert conjugate(Derivative(f(z), z)).doit() == conjugate(Derivative(f(z), z).doit())\n\n    g = Function('g', real=True)\n    h = Function('h', imaginary=True)\n    assert conjugate(Derivative(g(x), x)).doit() == Derivative(g(x), x)\n    assert conjugate(Derivative(h(y), y)).doit() == -Derivative(h(y), y)\n\n    u = Symbol('u', real=True)\n    v = Symbol('v', imaginary=True)\n    w = Symbol('w', complex=True)\n    assert conjugate(Derivative(conjugate(u), u)).doit() == Derivative(u, u)\n    assert conjugate(Derivative(conjugate(v), v)).doit() == -Derivative(v, v)\n    assert conjugate(Derivative(conjugate(w), w)).doit() == Derivative(conjugate(w), w)\n\n    assert Derivative(conjugate(f(x)), x).doit() == conjugate(Derivative(f(x), x).doit())\n    assert Derivative(conjugate(f(y)), y).doit() == conjugate(Derivative(f(y), y).doit())\n    assert Derivative(conjugate(f(z)), z).doit() == conjugate(Derivative(f(z), z).doit())\n", "def test_conjugate_diff():\n    x = Symbol('x')\n    y = Symbol('y', real=True)\n    z = Symbol('z', imaginary=True)\n\n    assert conjugate(y + z).diff(x) == 0\n    assert conjugate(y + z).diff(y) == 1\n    assert conjugate(y + z).diff(z) == -I\n    assert conjugate(2*y + 3*z).diff(y) == 2\n    assert conjugate(2*y + 3*z).diff(z) == -3*I\n\n    # Higher order derivatives\n    assert conjugate(y**2).diff(y, y) == 2\n    assert conjugate(z**2).diff(z, z) == -2\n\n    f = Function('f')\n    assert conjugate(f(y)).diff(y) == conjugate(Derivative(f(y), y))\n    assert conjugate(f(z)).diff(z) == -I * conjugate(Derivative(f(z), z))\n", "def test_conjugate_transpose_matrix():\n    A = Matrix([[1 + I, 2 - 3*I], [4 + 2*I, 5 - I]])\n    B = Matrix([[1, 2], [3, 4]])\n    \n    assert conjugate(A) == Matrix([[1 - I, 2 + 3*I], [4 - 2*I, 5 + I]])\n    assert transpose(A) == Matrix([[1 + I, 4 + 2*I], [2 - 3*I, 5 - I]])\n    assert adjoint(A) == Matrix([[1 - I, 4 - 2*I], [2 + 3*I, 5 + I]])\n\n    assert conjugate(B) == B\n    assert transpose(B) == Matrix([[1, 3], [2, 4]])\n    assert adjoint(B) == B\n\n    A_adj = adjoint(A)\n    A_conj = conjugate(A)\n    assert A_adj == A_conj.T\n    assert A.T == transpose(A)\n    assert A.H == adjoint(A)\n\n    A = ImmutableMatrix([[1 + I, 2 - 3*I], [4 + 2*I, 5 - I]])\n    assert A.conjugate() == ImmutableMatrix([[1 - I, 2 + 3*I], [4 - 2*I, 5 + I]])\n    assert A.transpose() == ImmutableMatrix([[1 + I, 4 + 2*I], [2 - 3*I, 5 - I]])\n    assert A.adjoint() == ImmutableMatrix([[1 - I, 4 - 2*I], [2 + 3*I, 5 + I]])\n"], "sample_1178": ["def test_Token():\n    # Test Token instantiation\n    token = Token()\n    assert token.is_Atom == True\n    assert token == Token()\n    assert token.func(*token.args) == token\n\n    # Test __eq__ method\n    assert token != None\n    assert token != Assignment(x, y)\n\n    # Test __hash__ method\n    assert hash(token) == hash(Token())\n\n    # Test _hashable_content method\n    assert token._hashable_content() == ()\n\n    # Test _joiner method\n    assert token._joiner('body', 4) == ',\\n    '\n    assert token._joiner('not_body', 4) == ', '\n\n    # Test _indented method\n    printer_mock = lambda x, *args, **kwargs: str(x)\n    assert token._indented(printer_mock, 'body', Tuple(x, y)) == '(\\n    x,\\n    y,\\n)'\n    assert token._indented(printer_mock, 'not_body', Tuple(x, y)) == '(x, y)'\n\n    # Test _sympyrepr method\n    assert token._sympyrepr(printer_mock) == 'Token()'\n    assert token._sympystr(printer_mock) == 'Token()'\n\n    # Test __repr__ method\n    assert repr(token) == 'Token()'\n\n    # Test kwargs method\n    assert token.kwargs() == {}\n    assert token.kwargs(apply=str) == {}\n", "def test_BreakToken():\n    assert break_ == BreakToken()\n    assert isinstance(break_, BreakToken)\n    assert str(break_) == \"break_\"\n    assert repr(break_) == \"BreakToken()\"\n", "def test_Token():\n    class TestToken(Token):\n        __slots__ = ('attr1', 'attr2')\n        defaults = {'attr1': Integer(0), 'attr2': Integer(1)}\n\n    t = TestToken(attr1=2)\n    assert t.attr1 == Integer(2)\n    assert t.attr2 == Integer(1)\n    assert t.func(*t.args) == t\n\n    t2 = TestToken(attr2=3)\n    assert t2.attr1 == Integer(0)\n    assert t2.attr2 == Integer(3)\n    assert t2.func(*t2.args) == t2\n\n    t3 = TestToken(3, 4)\n    assert t3.attr1 == Integer(3)\n    assert t3.attr2 == Integer(4)\n    assert t3.func(*t3.args) == t3\n\n    t4 = TestToken(5)\n    assert t4.attr1 == Integer(5)\n    assert t4.attr2 == Integer(1)\n    assert t4.func(*t4.args) == t4\n\n    # Check equality\n    t5 = TestToken(5, 6)\n    t6 = TestToken(5, 6)\n    assert t5 == t6\n    assert t5 is not t6\n\n    # Check inequality\n    assert t5 != t4\n    assert t4 != t6\n\n    # Testing error cases\n    raises(ValueError, lambda: TestToken(1, 2, 3))\n    raises(ValueError, lambda: TestToken(attr3=4))\n\n    # Testing hashing\n    assert hash(t5) == hash(t6)\n\n    # Testing properties and methods\n    assert t5.is_Atom is False\n    assert t5._hashable_content() == (Integer(5), Integer(6))\n    assert repr(t5) == \"TestToken(5, 6)\"\n", "def test_aug_assign_edge_cases():\n    # Test with different types and edge cases\n    raises(ValueError, lambda: aug_assign(x, '@', y))  # Invalid operator\n    raises(ValueError, lambda: aug_assign(x, '+', None))  # None as rhs\n    raises(ValueError, lambda: aug_assign(None, '+', y))  # None as lhs\n\n    # Test with complex expressions\n    expr1 = x + y*z\n    expr2 = x**2 + sin(y)\n    aug1 = aug_assign(expr1, '+', expr2)\n    assert aug1.lhs == expr1\n    assert aug1.rhs == expr2\n    assert aug1.op == '+='\n\n    # Test with matrix expressions\n    from sympy.matrices.expressions.matexpr import MatrixSymbol, MatrixElement\n    M = MatrixSymbol('M', 3, 3)\n    N = MatrixSymbol('N', 3, 3)\n    aug2 = aug_assign(M[1, 1], '+', N[1, 1])\n    assert isinstance(aug2.lhs, MatrixElement)\n    assert isinstance(aug2.rhs, MatrixElement)\n    assert aug2.op == '+='\n\n    # Ensure that invalid operations raise errors\n    raises(ValueError, lambda: aug_assign(M, '+', x))  # Matrix to scalar\n    raises(ValueError, lambda: aug_assign(x, '+', M))  # Scalar to matrix\n", "def test_Token():\n    class CustomToken(Token):\n        __slots__ = ('field1', 'field2')\n        defaults = {'field1': None, 'field2': None}\n    \n    t1 = CustomToken(field1=x, field2=y)\n    assert t1.field1 == x\n    assert t1.field2 == y\n    assert t1.func(*t1.args) == t1\n    \n    t2 = CustomToken(field2=z)\n    assert t2.field1 == None\n    assert t2.field2 == z\n    assert t2.func(*t2.args) == t2\n    \n    assert t1 != t2\n    assert hash(t1) != hash(t2)\n    \n    # Test attribute construction\n    class CustomTokenWithConstructor(Token):\n        __slots__ = ('field1', 'field2')\n        defaults = {'field1': None, 'field2': None}\n        \n        @staticmethod\n            return val * 2\n    \n    t3 = CustomTokenWithConstructor(field1=2, field2=3)\n    assert t3.field1 == 4\n    assert t3.field2 == 3\n    assert t3.func(*t3.args) == t3\n"], "sample_1179": ["def test_MatrixElement_Substitution():\n    A = MatrixSymbol(\"A\", 3, 3)\n    expr = A[1, 1]\n    assert str(expr.subs(A, Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))) == \"5\"\n", "def test_LatticeOp():\n    from sympy.core.operations import LatticeOp\n    class LatticeOpExample(LatticeOp):\n        identity = 0\n    expr = LatticeOpExample(x, y, x*y)\n    assert str(expr) == \"LatticeOpExample(x, x*y, y)\"\n    assert str(LatticeOpExample(x)) == \"LatticeOpExample(x)\"\n", "def test_blockmatrix():\n    from sympy.matrices import BlockMatrix, Matrix\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    C = BlockMatrix([[A, B]])\n    assert str(C) == 'Matrix([\\n[1, 2, 5, 6],\\n[3, 4, 7, 8]])'\n    assert sstr(C) == 'Matrix([\\n[1, 2, 5, 6],\\n[3, 4, 7, 8]])'\n    D = BlockMatrix([[A], [B]])\n    assert str(D) == 'Matrix([\\n[1, 2],\\n[3, 4],\\n[5, 6],\\n[7, 8]])'\n    assert sstr(D) == 'Matrix([\\n[1, 2],\\n[3, 4],\\n[5, 6],\\n[7, 8]])'\n", "def test_BlockMatrix():\n    from sympy.matrices import Matrix, BlockMatrix\n    M = Matrix([[1, 2], [3, 4]])\n    N = Matrix([[5, 6], [7, 8]])\n    B = BlockMatrix([[M, N], [N, M]])\n    assert str(B) == \"Matrix([[(Matrix([\\n[1, 2],\\n[3, 4]])), (Matrix([\\n[5, 6],\\n[7, 8]]))], [(Matrix([\\n[5, 6],\\n[7, 8]])), (Matrix([\\n[1, 2],\\n[3, 4]]))]])\"\n", "def test_ExprCondPair():\n    expr_cond_pair = ExprCondPair(x, x > 0)\n    assert str(expr_cond_pair) == \"(x, x > 0)\"\n    expr_cond_pair = ExprCondPair(x**2, x < 0)\n    assert str(expr_cond_pair) == \"(x**2, x < 0)\"\n"], "sample_1180": ["def test_point_neg():\n    # Test negation of Point\n    p1 = Point(2, -3)\n    p2 = Point(-2, 3)\n    p3 = Point(0, 0)\n    p4 = Point(-1, -1, 1)\n\n    assert -p1 == Point(-2, 3)\n    assert -p2 == Point(2, -3)\n    assert -p3 == Point(0, 0)\n    assert -p4 == Point(1, 1, -1)\n\n    # Ensure negation doesn't alter the original point\n    assert p1 == Point(2, -3)\n    assert p2 == Point(-2, 3)\n    assert p3 == Point(0, 0)\n    assert p4 == Point(-1, -1, 1)\n", "def test_Point_project():\n    p1 = Point(1, 2)\n    p2 = Point(3, 4)\n    origin = Point(0, 0)\n    assert Point.project(p1, p2) == Point(11/25, 14/25)\n    assert Point.project(p1, origin) == origin\n    raises(ValueError, lambda: Point.project(p1, Point(0, 0)))\n\n    # test 3D projection\n    p3_1 = Point3D(1, 2, 3)\n    p3_2 = Point3D(4, 5, 6)\n    assert Point.project(p3_1, p3_2) == Point3D(32/91, 40/91, 48/91)\n    raises(ValueError, lambda: Point.project(p3_1, Point3D(0, 0, 0)))\n", "def test_point_transformations():\n    # Ensure transformations such as rotation, scaling, and translation work as expected\n    p = Point2D(1, 1)\n    assert p.rotate(pi/2) == Point2D(-1, 1)\n    assert p.rotate(pi, p) == p\n    assert p.scale(2, 2) == Point2D(2, 2)\n    assert p.scale(2, 3) == Point2D(2, 3)\n    assert p.translate(1, 1) == Point2D(2, 2)\n    assert p.translate(-1, -1) == Point2D(0, 0)\n\n    p3d = Point3D(1, 1, 1)\n    assert p3d.scale(2, 2, 2) == Point3D(2, 2, 2)\n    assert p3d.scale(2, 3, 4) == Point3D(2, 3, 4)\n    assert p3d.translate(1, 1, 1) == Point3D(2, 2, 2)\n    assert p3d.translate(-1, -1, -1) == Point3D(0, 0, 0)\n\n    identity_matrix_3d = Matrix([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n    assert p3d.transform(identity_matrix_3d) == p3d\n\n    trans_matrix = Matrix([[1, 0, 0, 1], [0, 1, 0, 1], [0, 0, 1, 1], [0, 0, 0, 1]])\n    assert p3d.transform(trans_matrix) == Point3D(2, 2, 2)\n", "def test_orthogonal_direction():\n    # Test orthogonal_direction property for 2D points\n    p1 = Point2D(1, 2)\n    p2 = Point2D(3, 4)\n    p3 = Point2D(0, 1)\n    p4 = Point2D(1, 0)\n\n    assert p1.orthogonal_direction in [Point2D(-2, 1), Point2D(2, -1)]\n    assert p2.orthogonal_direction in [Point2D(-4, 3), Point2D(4, -3)]\n    assert p3.orthogonal_direction == Point2D(1, 0)\n    assert p4.orthogonal_direction == Point2D(0, 1)\n\n    # Test orthogonal_direction property for 3D points\n    p5 = Point3D(1, 2, 3)\n    p6 = Point3D(4, 5, 6)\n    p7 = Point3D(0, 1, 0)\n    p8 = Point3D(1, 0, 0)\n\n    assert p5.orthogonal_direction in [Point3D(-2, 1, 0), Point3D(2, -1, 0)]\n    assert p6.orthogonal_direction in [Point3D(-5, 4, 0), Point3D(5, -4, 0)]\n    assert p7.orthogonal_direction == Point3D(1, 0, 0)\n    assert p8.orthogonal_direction == Point3D(0, 1, 0)\n", "def test_point3D_intersection():\n    # Test intersection of Point3D with other geometric entities\n    point = Point3D(2, 3, 4)\n    other_point = Point3D(2, 3, 4)\n    diff_point = Point3D(1, 2, 3)\n    line = Line3D(Point3D(0, 0, 0), Point3D(2, 3, 4))\n    plane = Plane(Point3D(0, 0, 0), normal_vector=(1, 0, 0))\n\n    # Intersection with same point\n    assert point.intersection(other_point) == [point]\n\n    # Intersection with different point\n    assert point.intersection(diff_point) == []\n\n    # Intersection with line (point is on the line)\n    assert point.intersection(line) == [point]\n\n    # Intersection with plane (plane through the point)\n    assert point.intersection(plane) == [point]\n\n    # Intersection with plane (point not on the plane)\n    plane_not_through_point = Plane(Point3D(0, 0, 1), normal_vector=(1, 0, 0))\n    assert point.intersection(plane_not_through_point) == []\n"], "sample_1181": ["def test_numpy_matrix_operations():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    M = MatrixSymbol(\"M\", 2, 2)\n    N = MatrixSymbol(\"N\", 2, 2)\n\n    # Matrix Multiplication\n    expr = M * N\n    f = lambdify((M, N), expr, \"numpy\")\n    ma = np.array([[1, 2], [3, 4]])\n    mb = np.array([[2, 0], [1, 3]])\n    expected_result = np.dot(ma, mb)\n    assert np.array_equal(f(ma, mb), expected_result)\n\n    # Matrix Power\n    expr = M**2\n    f = lambdify((M,), expr, \"numpy\")\n    expected_result = np.linalg.matrix_power(ma, 2)\n    assert np.array_equal(f(ma), expected_result)\n\n    # Matrix Inverse\n    expr = M**-1\n    f = lambdify((M,), expr, \"numpy\")\n    expected_result = np.linalg.inv(ma)\n    assert np.allclose(f(ma), expected_result)\n\n    # Hadamard Product\n    expr = M.multiply_elementwise(N)\n    f = lambdify((M, N), expr, \"numpy\")\n    expected_result = np.multiply(ma, mb)\n    assert np.array_equal(f(ma, mb), expected_result)\n\n    # Kronecker Product\n    from sympy import KroneckerProduct\n    expr = KroneckerProduct(M, N)\n    f = lambdify((M, N), expr, \"numpy\")\n    expected_result = np.kron(ma, mb)\n    assert np.array_equal(f(ma, mb), expected_result)\n\n    # Diagonal of a Matrix\n    from sympy import DiagonalOf\n    expr = DiagonalOf(M)\n    f = lambdify((M,), expr, \"numpy\")\n    expected_result = np.diag(ma)\n    assert np.array_equal(f(ma), expected_result)\n", "def test_numpy_print_methods_new():\n    prntr = NumPyPrinter()\n    assert prntr._print_Re(x) == 'numpy.real(x)'\n    assert prntr._print_Im(x) == 'numpy.imag(x)'\n    assert prntr._print_Sign(x) == 'numpy.sign(x)'\n    assert prntr._print_Abs(x) == 'numpy.abs(x)'\n    assert prntr._print_Piecewise(Piecewise((1, x < 0), (0, True))) == \\\n        'numpy.select([numpy.less(x, 0),True], [1,0], default=numpy.nan)'\n", "def test_numpy_printer_seq():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    printer = NumPyPrinter()\n    seq = [x, y, z]\n    assert printer._print_seq(seq) == '(x, y, z)'\n", "def test_numpy_matmul():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    M = MatrixSymbol(\"M\", 2, 2)\n    N = MatrixSymbol(\"N\", 2, 2)\n    printer = NumPyPrinter()\n\n    expr = M * N\n    code = printer.doprint(expr)\n    assert code == '(M).dot(N)'\n\n    f = lambdify((M, N), expr, 'numpy')\n\n    ma = np.array([[1, 2], [3, 4]])\n    mb = np.array([[2, 0], [1, 3]])\n    result = f(ma, mb)\n    expected_result = np.dot(ma, mb)\n    assert np.array_equal(result, expected_result)\n", "def test_scipy_lowergamma_uppergamma():\n    if not np:\n        skip(\"NumPy not installed\")\n    sp = import_module('scipy')\n    if not sp:\n        skip(\"SciPy not installed\")\n\n    from sympy import lowergamma, uppergamma\n\n    a_ = 5\n    x_ = 2\n\n    printer = SciPyPrinter()\n    lowergamma_expr = lowergamma(a, x)\n    uppergamma_expr = uppergamma(a, x)\n\n    assert printer.doprint(lowergamma_expr) == 'scipy.special.gamma(5)*scipy.special.gammainc(5, 2)'\n    assert printer.doprint(uppergamma_expr) == 'scipy.special.gamma(5)*scipy.special.gammaincc(5, 2)'\n\n    f_lowergamma = lambdify((a, x), lowergamma_expr, 'scipy')\n    f_uppergamma = lambdify((a, x), uppergamma_expr, 'scipy')\n\n    assert np.isclose(f_lowergamma(a_, x_), sp.special.gamma(a_) * sp.special.gammainc(a_, x_))\n    assert np.isclose(f_uppergamma(a_, x_), sp.special.gamma(a_) * sp.special.gammaincc(a_, x_))\n"], "sample_1182": ["def test_Indexed():\n    A = IndexedBase('A')\n    i, j = symbols('i j', integer=True)\n    prntr = PythonCodePrinter()\n    \n    assert prntr.doprint(A[i]) == 'A[i]'\n    assert prntr.doprint(A[i, j]) == 'A[i, j]'\n    \n    prntr = MpmathPrinter()\n    assert prntr.doprint(A[i]) == 'A[i]'\n    assert prntr.doprint(A[i, j]) == 'A[i, j]'\n    \n    prntr = NumPyPrinter()\n    assert prntr.doprint(A[i]) == 'A[i]'\n    assert prntr.doprint(A[i, j]) == 'A[i, j]'\n    \n    prntr = SciPyPrinter()\n    assert prntr.doprint(A[i]) == 'A[i]'\n    assert prntr.doprint(A[i, j]) == 'A[i, j]'\n    \n    prntr = SymPyPrinter()\n    assert prntr.doprint(A[i]) == 'A[i]'\n    assert prntr.doprint(A[i, j]) == 'A[i, j]'\n", "def test_unary_operations():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(-x) == '-x'\n    assert prntr.doprint(-(-x)) == 'x'\n    assert prntr.doprint(+x) == 'x'\n    assert prntr.doprint(+(-x)) == '-x'\n    assert prntr.doprint(~x) == '~x'\n    assert prntr.doprint(not x) == 'not x'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(-x) == '-x'\n    assert prntr.doprint(+x) == 'x'\n    assert prntr.doprint(~x) == '~x'\n    assert prntr.doprint(not x) == 'not x'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(-x) == '-x'\n    assert prntr.doprint(+x) == 'x'\n    assert prntr.doprint(~x) == '~x'\n    assert prntr.doprint(not x) == 'not x'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(-x) == '-x'\n    assert prntr.doprint(+x) == 'x'\n    assert prntr.doprint(~x) == '~x'\n    assert prntr.doprint(not x) == 'not x'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(-x) == '-x'\n    assert prntr.doprint(+x) == 'x'\n    assert prntr.doprint(~x) == '~x'\n    assert prntr.doprint(not x) == 'not x'\n", "def test_SymPyPrinter():\n    prntr = SymPyPrinter()\n    assert prntr.doprint(x**y) == 'x**y'\n    assert prntr.doprint(Mod(x, 2)) == 'x % 2'\n    assert prntr.doprint(-Mod(x, y)) == '-(x % y)'\n    assert prntr.doprint(Mod(-x, y)) == '(-x) % y'\n    assert prntr.doprint(And(x, y)) == 'x and y'\n    assert prntr.doprint(Or(x, y)) == 'x or y'\n    assert prntr.doprint(pi) == 'sympy.pi'\n    assert prntr.doprint(x**Rational(1, 2)) == 'sympy.sqrt(x)'\n    assert prntr.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert prntr.doprint(acos(x)) == 'sympy.acos(x)'\n    assert prntr.doprint(Assignment(x, 2)) == 'x = 2'\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert prntr.doprint(Piecewise((2, Le(x, 0)),\n                        (3, Gt(x, 0)), evaluate=False)) == '((2) if (x <= 0) else'\\\n                                                        ' (3) if (x > 0) else None)'\n    assert prntr.doprint(sign(x)) == '(0.0 if x == 0 else sympy.sign(1, x))'\n    assert prntr.doprint(p[0, 1]) == 'p[0, 1]'\n    assert prntr.doprint(KroneckerDelta(x,y)) == '(1 if x == y else 0)'\n    assert prntr.doprint((2,3)) == \"(2, 3)\"\n    assert prntr.doprint([2,3]) == \"[2, 3]\"\n    assert prntr.doprint(Min(x, y)) == \"sympy.Min(x, y)\"\n    assert prntr.dopr", "def test_Integral_special_cases():\n    from sympy.integrals.integrals import Integral\n    from sympy.functions.special.gamma_functions import gamma\n\n    # Test integrals with special functions\n    expr = Integral(gamma(x), (x, 0, oo))\n    \n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.quad(lambda x: mpmath.gamma(x), (0, mpmath.inf))'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.integrate.quad(lambda x: scipy.special.gamma(x), 0, numpy.PINF)[0]'\n", "def test_Integral_undeclared_limits():\n    from sympy.functions.elementary.exponential import exp\n    from sympy.integrals.integrals import Integral\n\n    indefinite = Integral(x**2, x)\n    evaluateat = Integral(x**2, (x, 1))\n\n    prntr = SciPyPrinter()\n    raises(NotImplementedError, lambda: prntr.doprint(indefinite))\n    raises(NotImplementedError, lambda: prntr.doprint(evaluateat))\n\n    prntr = MpmathPrinter()\n    raises(NotImplementedError, lambda: prntr.doprint(indefinite))\n    raises(NotImplementedError, lambda: prntr.doprint(evaluateat))\n"], "sample_1183": ["def test_FracField_construction():\n    from sympy.polys.domains import ZZ, QQ\n    from sympy.abc import x, y\n\n    F1, f1 = field(\"x\", ZZ)\n    assert str(F1) == \"Rational function field in x over ZZ with lex order\"\n    assert str(f1) == \"x\"\n\n    F2, f2 = field(\"x, y\", QQ)\n    assert str(F2) == \"Rational function field in x, y over QQ with lex order\"\n    assert str(f2) == \"(x, y)\"\n\n    F3, (f3x, f3y) = xfield(\"x, y\", ZZ)\n    assert str(F3) == \"Rational function field in x, y over ZZ with lex order\"\n    assert str(f3x) == \"x\"\n    assert str(f3y) == \"y\"\n\n    F4 = vfield(\"x, y\", QQ)\n    assert str(F4) == \"Rational function field in x, y over QQ with lex order\"\n    assert str(F4.gens) == \"(x, y)\"\n", "def test_FractionField_operations():\n    F, x, y, z = field(\"x,y,z\", ZZ)\n\n    f1 = (x**2 + y**2) / (x + 1)\n    f2 = (y**2 + z**2) / (y + 1)\n    f3 = (x*y*z) / (x*y + y*z + z*x)\n    f4 = (x + y + z) / (x*y*z)\n    f5 = (x**2 - y**2) / (x - y)\n    \n    assert f1 + f2 == F((x**2 + y**2)*(y + 1) + (y**2 + z**2)*(x + 1), (x + 1)*(y + 1))\n    assert f1 - f2 == F((x**2 + y**2)*(y + 1) - (y**2 + z**2)*(x + 1), (x + 1)*(y + 1))\n    assert f1 * f2 == F((x**2 + y**2)*(y**2 + z**2), (x + 1)*(y + 1))\n    assert f1 / f2 == F((x**2 + y**2)*(y + 1), (y**2 + z**2)*(x + 1))\n    assert f1 ** 2 == F((x**2 + y**2)**2, (x + 1)**2)\n    assert f1.diff(x) == F(2*x*(x + 1) - (x**2 + y**2), (x + 1)**2)\n    \n    assert f3.subs(x, 1) == F(y*z, y + z + y*z)\n    assert f4.evaluate(y, 1) == F(x + z + 1, x*z)\n    assert f5 == F(x + y)\n    \n    # Test conversion from expression\n    expr = (x + y) / (x*y)\n    frac = F.from_expr(expr)\n    assert frac == F(x + y, x*y)\n", "def test_FracField_creation_and_basic_operations():\n    # Test creation of FracField with ZZ domain\n    K, x, y = field(\"x,y\", ZZ)\n    assert str(K) == \"Rational function field in x, y over ZZ with lex order\"\n    assert str(x) == \"x\"\n    assert str(y) == \"y\"\n\n    # Test creation of FracField with QQ domain\n    K, x, y = field(\"x,y\", QQ)\n    assert str(K) == \"Rational function field in x, y over QQ with lex order\"\n    assert str(x) == \"x\"\n    assert str(y) == \"y\"\n\n    # Test basic arithmetic operations\n    f = x / y\n    g = y / x\n    h = x / y + y / x\n\n    assert str(f) == \"x/y\"\n    assert str(g) == \"y/x\"\n    assert str(h) == \"x/y + y/x\"\n\n    # Test multiplication\n    assert str(f * g) == \"1\"\n\n    # Test addition\n    assert str(f + g) == \"x/y + y/x\"\n\n    # Test subtraction\n    assert str(f - g) == \"x/y - y/x\"\n\n    # Test division\n    assert str(f / g) == \"x**2/y**2\"\n\n    # Test exponentiation\n    assert str(f**2) == \"x**2/y**2\"\n    assert str(g**-1) == \"x/y\"\n\n    # Test creation with integers\n    assert str(K(2)) == \"2\"\n    assert str(K(QQ(2, 3))) == \"2/3\"\n\n    # Test domain conversion\n    assert str(K.convert_from(ZZ(1), ZZ)) == \"1\"\n    assert str(K.convert_from(QQ(1, 2), QQ)) == \"1/2\"\n\n    # Test from_expr method\n    expr = x + y\n    frac = K.from_expr(expr)\n    assert str(frac) == \"x + y\"\n", "def test_FractionField_operations():\n    R, x, y = ring(\"x,y\", ZZ)\n    F, X, Y = field(\"x,y\", ZZ)\n\n    # Check addition\n    f1 = F((x**2 + y**2, x + 1))\n    f2 = F((x + y, x**2 - 1))\n    f3 = F((2*x**2 + 2*y**2 + x + y, x**3 - x**2 + x - 1))\n    assert f1 + f2 == f3\n\n    # Check subtraction\n    f4 = F((x**2 + y**2 - (x + y), x**2 - 1))\n    assert f1 - f2 == f4\n\n    # Check multiplication\n    f5 = F((x**4 + 2*x**2*y**2 + y**4, x**2 + x))\n    assert f1 * f2 == f5\n\n    # Check division\n    f6 = F((x**3 + x**2 - x - 1, x**3 + x**2 - x - 1))\n    assert f1 / f1 == f6\n\n    # Check power\n    f7 = F((x**4 + 2*x**2*y**2 + y**4, x**2 + 2*x + 1))\n    assert f1**2 == f7\n\n    # Check conversion to polynomial ring and back\n    p = X**2 + Y**2\n    assert F.from_PolynomialRing(p, R.to_domain()) == f1\n    assert R.to_domain().from_FractionField(f1, F.to_domain()) == p\n", "def test_FracField_initialization():\n    # Test FracField initialization with basic parameters\n    field1 = FracField([\"x\", \"y\"], ZZ)\n    assert field1.symbols == (Symbol(\"x\"), Symbol(\"y\"))\n    assert field1.domain == ZZ\n    assert field1.ngens == 2\n\n    field2 = FracField(\"z\", QQ)\n    assert field2.symbols == (Symbol(\"z\"),)\n    assert field2.domain == QQ\n    assert field2.ngens == 1\n\n    # Test the hash value for different fields\n    assert hash(field1) != hash(field2)\n\n    # Test zero and one elements\n    assert field1.zero == field1.dtype(field1.ring.zero)\n    assert field1.one == field1.dtype(field1.ring.one)\n\n    # Test generator elements\n    x, y = field1.gens\n    assert x == field1.dtype(field1.ring.gens[0])\n    assert y == field1.dtype(field1.ring.gens[1])\n\n    # Check if the field objects are stored in the cache\n    assert field1._hash_tuple in _field_cache\n    assert field2._hash_tuple in _field_cache\n"], "sample_1184": ["def test_RayTransferMatrix():\n    A, B, C, D = symbols('A B C D')\n    mat = RayTransferMatrix(A, B, C, D)\n    assert mat.A == A\n    assert mat.B == B\n    assert mat.C == C\n    assert mat.D == D\n    assert mat.shape == (2, 2)\n\n    mat2 = RayTransferMatrix(Matrix([[A, B], [C, D]]))\n    assert mat2 == mat\n\n    try:\n        RayTransferMatrix(A, B, C)  # Should raise ValueError\n    except ValueError:\n        pass\n    else:\n        assert False, \"Expected ValueError\"\n\n    try:\n        RayTransferMatrix(Matrix([[A, B, C], [D, A, B]]))  # Should raise ValueError\n    except ValueError:\n        pass\n    else:\n        assert False, \"Expected ValueError\"\n\n    mat3 = RayTransferMatrix(1, 2, 3, 4) * RayTransferMatrix(5, 6, 7, 8)\n    assert mat3 == Matrix([[19, 22], [43, 50]])\n\n    geom_ray = GeometricRay(1, 2)\n    result_geom_ray = RayTransferMatrix(1, 0, 0, 1) * geom_ray\n    assert result_geom_ray == geom_ray\n\n    beam_param = BeamParameter(530e-9, 1, w=1e-3)\n    result_beam_param = RayTransferMatrix(1, 0, 0, 1) * beam_param\n    assert result_beam_param == beam_param\n", "def test_RayTransferMatrix_properties():\n    mat = RayTransferMatrix(1, 2, 3, 4)\n    assert mat.A == 1\n    assert mat.B == 2\n    assert mat.C == 3\n    assert mat.D == 4\n\n    mat2 = RayTransferMatrix(Matrix([[5, 6], [7, 8]]))\n    assert mat2.A == 5\n    assert mat2.B == 6\n    assert mat2.C == 7\n    assert mat2.D == 8\n", "def test_RayTransferMatrix_properties():\n    mat = RayTransferMatrix(1, 2, 3, 4)\n    assert mat.A == 1\n    assert mat.B == 2\n    assert mat.C == 3\n    assert mat.D == 4\n\n    mat2 = RayTransferMatrix(Matrix([[5, 6], [7, 8]]))\n    assert mat2.A == 5\n    assert mat2.B == 6\n    assert mat2.C == 7\n    assert mat2.D == 8\n", "def test_RayTransferMatrix_properties():\n    # Testing the properties of RayTransferMatrix\n    mat = RayTransferMatrix(1, 2, 3, 4)\n    assert mat.A == 1\n    assert mat.B == 2\n    assert mat.C == 3\n    assert mat.D == 4\n\n    mat2 = RayTransferMatrix(Matrix([[5, 6], [7, 8]]))\n    assert mat2.A == 5\n    assert mat2.B == 6\n    assert mat2.C == 7\n    assert mat2.D == 8\n\n    # Test multiplication\n    mat3 = mat * mat2\n    assert mat3 == RayTransferMatrix(Matrix([[19, 22], [43, 50]]))\n\n    # Ensure ValueError is raised for invalid input\n    try:\n        RayTransferMatrix(1, 2, 3)\n    except ValueError as e:\n        assert str(e) == 'Expecting 2x2 Matrix or the 4 elements of the Matrix but got (1, 2, 3)'\n\n    try:\n        RayTransferMatrix(Matrix([[1, 2], [3, 4], [5, 6]]))\n    except ValueError as e:\n        assert str(e) == 'Expecting 2x2 Matrix or the 4 elements of the Matrix but got Matrix([[1, 2], [3, 4], [5, 6]])'\n", "def test_ray_transfer_matrix():\n    A, B, C, D = symbols('A B C D')\n    \n    # Test creation with 4 elements\n    mat = RayTransferMatrix(A, B, C, D)\n    assert mat.shape == (2, 2)\n    assert mat[0, 0] == A\n    assert mat[0, 1] == B\n    assert mat[1, 0] == C\n    assert mat[1, 1] == D\n\n    # Test creation with 2x2 matrix\n    mat2 = RayTransferMatrix(Matrix([[A, B], [C, D]]))\n    assert mat2.shape == (2, 2)\n    assert mat2[0, 0] == A\n    assert mat2[0, 1] == B\n    assert mat2[1, 0] == C\n    assert mat2[1, 1] == D\n\n    # Test invalid creation\n    try:\n        RayTransferMatrix(A, B, C)\n    except ValueError as e:\n        assert str(e) == \"Expecting 2x2 Matrix or the 4 elements of the Matrix but got (A, B, C)\"\n    \n    try:\n        RayTransferMatrix(Matrix([[A, B], [C]]))\n    except ValueError as e:\n        assert str(e) == \"Expecting 2x2 Matrix or the 4 elements of the Matrix but got Matrix([[A, B], [C]])\"\n\n    # Test properties\n    assert mat.A == A\n    assert mat.B == B\n    assert mat.C == C\n    assert mat.D == D\n\n    # Test multiplication with another RayTransferMatrix\n    E, F, G, H = symbols('E F G H')\n    mat3 = RayTransferMatrix(E, F, G, H)\n    result = mat * mat3\n    assert isinstance(result, RayTransferMatrix)\n    assert result[0, 0] == A*E + B*G\n    assert result[0, 1] == A*F + B*H\n    assert result[1, 0] == C*E + D*G\n    assert result[1, 1] == C*F + D*H\n\n    # Test multiplication with GeometricRay\n    h, angle = symbols('h angle')\n    ray = GeometricRay(h, angle)\n    result"], "sample_1185": ["def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n    assert compogen([exp(x), x + 1, x**2], x) == exp(x**2 + 1)\n    assert compogen([x**2 + 1, x + 1, x**2], x) == (x**2 + 1 + 1)**2 + 1\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n    assert compogen([Abs(x), x**2 + 3*x - 4, cos(x)], x) == Abs(cos(x)**2 + 3*cos(x) - 4)\n    assert compogen([x], x) == x\n    assert compogen([1], x) == 1\n    assert compogen([Max(3, x)], x) == Max(3, x)\n    assert compogen([Max(sqrt(2*x + 3), (2*x + 3)**2), 2*x + 3], x) == Max(sqrt(2*x + 3), (2*x + 3)**2)\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n    assert compogen([Abs(x), x**2 + 3*x - 4, cos(x)], x) == Abs(cos(x)**2 + 3*cos(x) - 4)\n    assert compogen([Max(sqrt(x), x**2), 2*x + 3], x) == Max(sqrt(2*x + 3), (2*x + 3)**2)\n    assert compogen([Max(x, x**2, y), 2*x + 3], x) == Max(2*x + 3, (2*x + 3)**2, y)\n    assert compogen([Max(2*x + 3, sin(x)), x], x) == Max(2*x + 3, sin(x))\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n    assert compogen([exp(x), x**2 + 1], x) == exp(x**2 + 1)\n    assert compogen([cos(x), x + 2], x) == cos(x + 2)\n    assert compogen([x**2, x + 3], x) == (x + 3)**2\n"], "sample_1186": ["def test_array_creation_from_matrix():\n    for ArrayType in array_types:\n        matrix = Matrix([[1, 2], [3, 4]])\n        test_array = ArrayType(matrix)\n        assert test_array.shape == (2, 2)\n        assert test_array.tolist() == [[1, 2], [3, 4]]\n        assert test_array[0, 0] == 1\n        assert test_array[0, 1] == 2\n        assert test_array[1, 0] == 3\n        assert test_array[1, 1] == 4\n", "def test_ndarray_creation():\n    for ArrayType in array_types:\n        a = ArrayType.zeros(2, 3, 4)\n        assert a.shape == (2, 3, 4)\n        assert all(e == 0 for e in a)\n\n        b = ArrayType([[2, 3], [4, 5]])\n        assert b.shape == (2, 2)\n        assert b[0, 0] == 2\n        assert b[1, 1] == 5\n\n        c = ArrayType([[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11, 12]]])\n        assert c.shape == (2, 3, 2)\n        assert c[0, 0, 0] == 1\n        assert c[1, 2, 1] == 12\n\n        d = ArrayType([1, 2, 3, 4, 5, 6], (2, 3))\n        assert d.shape == (2, 3)\n        assert d[0, 0] == 1\n        assert d[1, 2] == 6\n\n        e = ArrayType(Matrix([[1, 2], [3, 4]]))\n        assert e.shape == (2, 2)\n        assert e[0, 0] == 1\n        assert e[1, 1] == 4\n", "def test_array_arithmetic_operations():\n    for ArrayType in array_types:\n        a = ArrayType([1, 2, 3, 4], (2, 2))\n        b = ArrayType([5, 6, 7, 8], (2, 2))\n        \n        # Test addition\n        c = a + b\n        assert c == ArrayType([6, 8, 10, 12], (2, 2))\n        \n        # Test subtraction\n        d = a - b\n        assert d == ArrayType([-4, -4, -4, -4], (2, 2))\n        \n        # Test scalar multiplication\n        e = a * 2\n        assert e == ArrayType([2, 4, 6, 8], (2, 2))\n        \n        # Test scalar right multiplication\n        f = 3 * a\n        assert f == ArrayType([3, 6, 9, 12], (2, 2))\n        \n        # Test scalar division\n        g = b / 2\n        assert g == ArrayType([2.5, 3, 3.5, 4], (2, 2))\n        \n        # Test negation\n        h = -a\n        assert h == ArrayType([-1, -2, -3, -4], (2, 2))\n        \n        # Test invalid operations\n        raises(ValueError, lambda: a * b)\n        raises(ValueError, lambda: a / b)\n", "def test_array_addition_subtraction():\n    for ArrayType in array_types:\n        a = ArrayType([1, 1, 1, 1], (2, 2))\n        b = ArrayType([4, 4, 4, 4], (2, 2))\n        c = a + b\n        assert c.tolist() == [[5, 5], [5, 5]]\n        d = a - b\n        assert d.tolist() == [[-3, -3], [-3, -3]]\n        raises(ValueError, lambda: a + ArrayType([1, 2, 3], (3,)))\n        raises(ValueError, lambda: a - ArrayType([1, 2, 3], (3,)))\n", "def test_ndimarray_arithmetic_operations():\n    a = MutableDenseNDimArray([1, 1, 1, 1], (2, 2))\n    b = MutableDenseNDimArray([2, 2, 2, 2], (2, 2))\n\n    # Test addition\n    c = a + b\n    assert c.tolist() == [[3, 3], [3, 3]]\n\n    # Test subtraction\n    d = b - a\n    assert d.tolist() == [[1, 1], [1, 1]]\n\n    # Test scalar multiplication\n    e = a * 3\n    assert e.tolist() == [[3, 3], [3, 3]]\n\n    # Test scalar right multiplication\n    f = 2 * a\n    assert f.tolist() == [[2, 2], [2, 2]]\n\n    # Test scalar division\n    g = b / 2\n    assert g.tolist() == [[1, 1], [1, 1]]\n\n    # Test negation\n    h = -a\n    assert h.tolist() == [[-1, -1], [-1, -1]]\n"], "sample_1187": ["def test_hyperplane_parameters():\n    # Testing for a 2D Polygon\n    poly2d = Polygon(Point(0, 0), Point(1, 0), Point(1, 1), Point(0, 1))\n    assert hyperplane_parameters(poly2d) == [((0, 1), 0), ((-1, 0), 1), ((0, -1), -1), ((1, 0), 0)]\n    \n    # Testing for a 3D Polytope\n    poly3d = [[(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0), (0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)],\n              [0, 1, 2, 3], [4, 5, 6, 7], [0, 1, 5, 4], [2, 3, 7, 6], [1, 2, 6, 5], [0, 3, 7, 4]]\n    vertices = poly3d[0]\n    faces = poly3d[1:]\n    assert hyperplane_parameters(faces, vertices) == [\n        ([-1, 0, 0], 0), ([1, 0, 0], 1), ([0, -1, 0], 0), ([0, 1, 0], 1),\n        ([0, 0, -1], 0), ([0, 0, 1], 1)\n    ]\n", "def test_gradient_terms():\n    # Test for 2D case\n    assert gradient_terms(2) == [\n        [1, 0, 0, 0], \n        [y, 0, 1, 0], \n        [y**2, 0, 2, 0], \n        [x, 1, 0, 0], \n        [x*y, 1, 1, 0], \n        [x**2, 2, 0, 0]\n    ]\n    # Test for 3D case\n    assert gradient_terms(2, 3) == [\n        [[[1, 0, 0, 0, 0, 0, 0, 0]]], \n        [[[y, 0, 1, 0, 1, 0, 0, 0], z, 0, 0, 1, 1, 0, 1, 0]], \n        [[x, 1, 0, 0, 1, 1, 0, 0]]], \n        [[[y**2, 0, 2, 0, 2, 0, 0, 0], [y*z, 0, 1, 1, 2, 0, 1, 0], [z**2, 0, 0, 2, 2, 0, 2, 0]], \n        [[x*y, 1, 1, 0, 2, 1, 0, 0], [x*z, 1, 0, 1, 2, 1, 1, 0]], \n        [[x**2, 2, 0, 0, 2, 2, 0, 0]]]\n    ]\n", "def test_strip():\n    assert strip(x**2) == (1, x**2)\n    assert strip(3 * x**2) == (3, x**2)\n    assert strip(S.Zero) == (S.Zero, S.Zero)\n    assert strip(5) == (5, S.One)\n    assert strip(x**2 * y**3) == (1, x**2 * y**3)\n    assert strip(4 * x * y**2) == (4, x * y**2)\n", "def test_gradient_terms():\n    # Test for 2D case\n    terms_2d = gradient_terms(2)\n    assert terms_2d == [\n        [1, 0, 0, 0], [y, 0, 1, 0], [y**2, 0, 2, 0], [x, 1, 0, 0], \n        [x*y, 1, 1, 0], [x**2, 2, 0, 0]\n    ]\n\n    # Test for 3D case\n    terms_3d = gradient_terms(2, 3)\n    assert terms_3d == [\n        [[[1, 0, 0, 0, 0, 0, 0, 0]]], \n        [[[y, 0, 1, 0, 1, 0, 0, 0], [z, 0, 0, 1, 1, 0, 1, 0]], [[x, 1, 0, 0, 1, 1, 0, 0]]], \n        [[[y**2, 0, 2, 0, 2, 0, 0, 0], [y*z, 0, 1, 1, 2, 0, 1, 0], [z**2, 0, 0, 2, 2, 0, 2, 0]], \n         [[x*y, 1, 1, 0, 2, 1, 0, 0], [x*z, 1, 0, 1, 2, 1, 1, 0]], [[x**2, 2, 0, 0, 2, 2, 0, 0]]]\n    ]\n", "def test_hyperplane_parameters():\n    # 2D Polygon\n    polygon = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    hp_params = hyperplane_parameters(polygon)\n    assert hp_params == [((0, 1), 3), ((1, -2), -1), ((-2, -1), -3)]\n    \n    # 3D Polytope\n    polyhedron = [[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)],\n                  [0, 1, 3, 2], [4, 5, 7, 6], [0, 1, 5, 4], [2, 3, 7, 6], [0, 2, 6, 4], [1, 3, 7, 5]]\n    vertices = polyhedron[0]\n    faces = polyhedron[1:]\n    hp_params_3d = hyperplane_parameters(faces, vertices)\n    expected_params_3d = [([-1, 0, 0], 0), ([1, 0, 0], 1), \n                          ([0, -1, 0], 0), ([0, 1, 0], 1),\n                          ([0, 0, -1], 0), ([0, 0, 1], 1)]\n    assert hp_params_3d == expected_params_3d\n"], "sample_1188": ["def test_pretty_print_special_functions():\n    from sympy import gamma, lowergamma, beta, KroneckerDelta, DiracDelta, Chi, lerchphi, LambertW, EulerGamma, GoldenRatio, Catalan, Probability, Expectation, MatrixSymbol, Matrix\n    x, y = symbols('x y')\n    \n    assert pretty(gamma(x)) == 'Gamma(x)'\n    assert upretty(gamma(x)) == '\u0393(x)'\n    \n    assert pretty(lowergamma(x, y)) == 'lowergamma(x, y)'\n    assert upretty(lowergamma(x, y)) == '\u03b3(x, y)'\n    \n    assert pretty(beta(x, y)) == 'B(x, y)'\n    assert upretty(beta(x, y)) == '\u0392(x, y)'\n    \n    assert pretty(KroneckerDelta(x, y)) == 'd(x, y)'\n    assert upretty(KroneckerDelta(x, y)) == '\u03b4(x, y)'\n    \n    assert pretty(DiracDelta(x)) == 'DiracDelta(x)'\n    assert upretty(DiracDelta(x)) == '\u03b4(x)'\n    \n    assert pretty(Chi(x)) == 'Chi(x)'\n    assert upretty(Chi(x)) == 'Chi(x)'\n    \n    assert pretty(lerchphi(x, y, z)) == 'lerchphi(x, y, z)'\n    assert upretty(lerchphi(x, y, z)) == '\u03a6(x, y, z)'\n    \n    assert pretty(LambertW(x)) == 'W(x)'\n    assert upretty(LambertW(x)) == 'W(x)'\n    \n    assert pretty(EulerGamma) == 'EulerGamma'\n    assert upretty(EulerGamma) == '\u03b3'\n    \n    assert pretty(GoldenRatio) == 'GoldenRatio'\n    assert upretty(GoldenRatio) == '\u03c6'\n    \n    assert pretty(Catalan) == 'Catalan'\n    assert upretty(Catalan) == 'G'\n    \n    assert pretty(Probability(x > 0)) == 'P(x > 0)'\n    assert upretty(Probability(x > 0)) == 'P(x > 0)'\n    \n    assert pretty(Expectation(x)) == 'E[x]'\n    assert upretty(Expectation(x)) == 'E[x]'\n    \n    A = MatrixSymbol('A', 2, 2)\n    assert pretty(A) == 'A", "def test_pretty_print_derivatives():\n    from sympy import symbols, Function, Derivative, Rational\n\n    x, y = symbols('x y')\n    f = Function('f')(x, y)\n    deriv = Derivative(f, x, y, y)\n\n    deriv_str = \"\"\"\\\n  \u2202                 ", "def test_pretty_printing_settings():\n    expr = (a**2 + b)*N.i + (Integral(f(b)))*N.k\n    unicode_output = \"\"\"\\", "def test_pretty_print_complex_expr():\n    from sympy import symbols, sqrt, I, pi, exp, sin\n    x, y = symbols('x y')\n    \n    expr1 = sqrt(x**2 + y**2)\n    expr2 = exp(I * pi)\n    expr3 = sin(x + y) + I*cos(x - y)\n\n    expr1_str = \"sqrt(x**2 + y**2)\"\n    expr2_str = \"exp(I*pi)\"\n    expr3_str = \"sin(x + y) + I*cos(x - y)\"\n\n    assert pretty(expr1) == expr1_str\n    assert pretty(expr2) == expr2_str\n    assert pretty(expr3) == expr3_str\n\n    expr1_upretty = \"\"\"\\\n  ______________\\n\\", "def test_pretty_printer_settings():\n    p = PrettyPrinter({\"use_unicode\": False})\n    assert p._settings[\"use_unicode\"] == False\n    p = PrettyPrinter({\"use_unicode\": True})\n    assert p._settings[\"use_unicode\"] == True\n\n    p = PrettyPrinter({\"imaginary_unit\": \"j\"})\n    assert p._settings[\"imaginary_unit\"] == \"j\"\n\n    try:\n        PrettyPrinter({\"imaginary_unit\": 1})\n    except TypeError as e:\n        assert str(e) == \"'imaginary_unit' must a string, not 1\"\n\n    try:\n        PrettyPrinter({\"imaginary_unit\": \"k\"})\n    except ValueError as e:\n        assert str(e) == \"'imaginary_unit' must be either 'i' or 'j', not 'k'\"\n"], "sample_1189": ["def test_import_module():\n    from sympy.external import import_module\n\n    # Try importing an existing module\n    np = import_module('numpy')\n    assert np is not None\n    assert np.__name__ == 'numpy'\n\n    # Try importing a non-existing module\n    non_exist_mod = import_module('nonexistentmodule')\n    assert non_exist_mod is None\n\n    # Ensure it works with a submodule\n    sp = import_module('scipy', import_kwargs={'fromlist': ['special']})\n    assert sp is not None\n    assert sp.__name__ == 'scipy.special'\n\n    # Ensure it gracefully handles import errors\n    try:\n        broken_mod = import_module('brokenmodule', min_module_version='1000.0.0')\n    except ImportError:\n        broken_mod = None\n    assert broken_mod is None\n", "def test_user_defined_function_with_numpy():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n        return numpy.exp(x) + numpy.sin(x)\n\n    f = implemented_function(Function('custom_func'), custom_func)\n    lam = lambdify(x, f(x), modules='numpy')\n    assert numpy.allclose(lam(1), custom_func(1))\n    assert numpy.allclose(lam(2), custom_func(2))\n", "def test_numpy_dtype():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    f = lambdify((A, B), A + B, modules='numpy')\n    A_val = numpy.ones((3, 3))\n    B_val = numpy.ones((3, 3))\n    result = f(A_val, B_val)\n    assert result.dtype == numpy.float64\n    assert numpy.array_equal(result, 2 * numpy.ones((3, 3)))\n", "def test_lambdify_special_cases():\n    # Test for constants\n    f = lambdify([], pi)\n    assert f() == pi\n\n    f = lambdify([], oo)\n    assert f() == oo\n\n    # Test for infinity with addition and subtraction\n    f = lambdify(x, x + oo)\n    assert f(2) == oo\n    assert f(-oo) == 0\n\n    f = lambdify(x, x - oo)\n    assert f(2) == -oo\n    assert f(oo) == 0\n\n    # Test with dictionary input and nested lambdify\n    nested_f = lambdify(x, lambdify(y, x + y))\n    assert nested_f(2)(3) == 5\n\n    # Test with Piecewise function\n    f = lambdify(x, Piecewise((0, x < 0), (1, x >= 0)))\n    assert f(-1) == 0\n    assert f(0) == 1\n    assert f(1) == 1\n\n    # Test with matrix input\n    M = Matrix([[x, y], [z, w]])\n    f = lambdify((x, y, z, w), M)\n    result = f(1, 2, 3, 4)\n    assert result == Matrix([[1, 2], [3, 4]])\n", "def test_lambdify_with_custom_printer():\n    from sympy.printing.lambdarepr import LambdaPrinter\n\n    class CustomPrinter(LambdaPrinter):\n            return f\"custom_{expr}\"\n\n    x = symbols('x')\n    expr = x + 1\n    f = lambdify(x, expr, printer=CustomPrinter)\n    assert f(2) == 3\n    src = inspect.getsource(f)\n    assert \"custom_x\" in src\n"], "sample_1190": ["def test_get_units_non_prefixed():\n    from sympy.physics.units import meter, second, kilogram, prefix\n    from sympy.physics.units.systems.si import dimsys_SI\n\n    # Create custom units\n    non_prefixed_unit = Quantity(\"unit1\")\n    non_prefixed_unit.set_global_relative_scale_factor(1, meter)\n    \n    prefixed_unit = Quantity(\"unit2\")\n    prefixed_unit.set_global_relative_scale_factor(1 * prefix.kilo, meter)\n\n    physical_constant_unit = Quantity(\"unit3\")\n    physical_constant_unit.set_global_relative_scale_factor(1, speed_of_light)\n\n    # Define a unit system\n    base_units = (meter, second, kilogram)\n    units = (non_prefixed_unit, prefixed_unit, physical_constant_unit)\n    unit_system = UnitSystem(base_units, units, \"custom_system\", \"\", dimsys_SI)\n\n    non_prefixed_units = unit_system.get_units_non_prefixed()\n    \n    assert non_prefixed_unit in non_prefixed_units\n    assert prefixed_unit not in non_prefixed_units\n    assert physical_constant_unit not in non_prefixed_units\n", "def test_unit_system_initialization():\n    base_units = (meter, second)\n    units = (kilometer, hour, gram)\n    name = \"CustomSystem\"\n    descr = \"A custom unit system.\"\n    dimension_system = SI.get_dimension_system()\n    derived_units = {length: kilometer, time: hour}\n\n    custom_system = UnitSystem(base_units, units, name, descr, dimension_system, derived_units)\n\n    assert custom_system.name == name\n    assert custom_system.descr == descr\n    assert custom_system.dim == 2\n    assert custom_system.is_consistent\n    assert custom_system.derived_units == derived_units\n    assert set(custom_system.get_units_non_prefixed()) == {meter, second, kilometer, hour, gram}\n\n    extended_system = custom_system.extend((coulomb,), (volt,), \"ExtendedSystem\", \"An extended unit system\")\n    assert extended_system.name == \"ExtendedSystem\"\n    assert extended_system.descr == \"An extended unit system\"\n    assert extended_system.dim == 3\n    assert set(extended_system.get_units_non_prefixed()) == {meter, second, kilometer, hour, gram, coulomb, volt}\n", "def test_unit_system_str_repr():\n    base_units = (meter, second)\n    us = UnitSystem(base_units, name=\"TestUnitSystem\")\n    assert str(us) == \"TestUnitSystem\"\n    assert repr(us) == \"<UnitSystem: ('meter', 'second')>\"\n\n    us2 = UnitSystem(base_units)\n    assert str(us2) == \"UnitSystem((meter, second))\"\n    assert repr(us2) == \"<UnitSystem: ('meter', 'second')>\"\n", "def test_get_units_non_prefixed():\n    # Define some quantities\n    meter = Quantity(\"meter\")\n    second = Quantity(\"second\")\n    joule = Quantity(\"joule\")\n    kilo_meter = Quantity(\"kilometer\")\n    milli_second = Quantity(\"millisecond\")\n    prefixed_joule = Quantity(\"kJ\", abbrev=\"kiloJoule\")\n    \n    # Set dimensions and scale factors\n    meter.set_global_relative_scale_factor(1, length)\n    second.set_global_relative_scale_factor(1, time)\n    joule.set_global_relative_scale_factor(1, energy)\n    kilo_meter.set_global_relative_scale_factor(kilo, meter)\n    milli_second.set_global_relative_scale_factor(S.One/1000, second)\n    prefixed_joule.set_global_relative_scale_factor(1000, joule)\n    \n    # Create a unit system\n    base_units = (meter, second)\n    units = (meter, second, joule, kilo_meter, milli_second, prefixed_joule)\n    us = UnitSystem(base_units, units)\n    \n    # Get non-prefixed units\n    non_prefixed_units = us.get_units_non_prefixed()\n    \n    # Check if the correct units are returned\n    assert meter in non_prefixed_units\n    assert second in non_prefixed_units\n    assert joule in non_prefixed_units\n    assert kilo_meter not in non_prefixed_units\n    assert milli_second not in non_prefixed_units\n    assert prefixed_joule not in non_prefixed_units\n", "def test_get_units_non_prefixed():\n    us = UnitSystem((meter, second), (kilometer, gram, joule, second))\n    non_prefixed_units = us.get_units_non_prefixed()\n    assert meter in non_prefixed_units\n    assert second in non_prefixed_units\n    assert gram in non_prefixed_units\n    assert kilometer not in non_prefixed_units\n    assert joule not in non_prefixed_units\n"], "sample_1191": ["def test_hermite_normal():\n\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    # Test with D provided\n    D = ZZ(30)\n    hnf_mod_D = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m, D=D).to_dense() == hnf_mod_D\n\n    # Check with non-ZZ domain\n    m_non_zz = DM([[1, 2], [3, 4]], QQ)\n    raises(DMDomainError, lambda: hermite_normal_form(m_non_zz))\n\n    # Check with more rows than columns\n    m_more_rows = DM([[1, 2], [3, 4], [5, 6]], ZZ)\n    raises(DMShapeError, lambda: hermite_normal_form(m_more_rows, D=ZZ(10)))\n", "def test_hermite_normal_form():\n\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    m = DM([[1, 2, 3], [4, 5, 6], [7, 8, 9]], ZZ)\n    hnf = DM([[1, 2, 3], [0, 1, 2], [0, 0, 0]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    D = ZZ(30)\n    assert hermite_normal_form(m, D=D).to_dense() == hnf\n\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1]], QQ)))\n    raises(DMShapeError, lambda: hermite_normal_form(DM([[1, 2], [3, 4]], ZZ), D=ZZ(1)))\n\n    m = DM([[4, 7], [2, 6]], ZZ)\n    hnf = DM([[2, 1], [0, 4]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    # Check handling of a matrix with a single row\n    m = DM([[6, 10, 15]], ZZ)\n    hnf = DM([[1, 0, 0]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    # Check handling of a matrix with a single column\n    m = DM([[6], [10], [15]], ZZ)\n    hnf = DM([[1], [0], [0]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    # Check handling of a matrix with all zeros\n    m = DM([[0, 0], [0, 0]], ZZ)\n    hnf = DM([[0, 0], [0, 0]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n", "def test_hermite_normal_form():\n    # Test for basic HNF calculation\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    # Test HNF with a known modulus\n    D = ZZ(60)\n    assert hermite_normal_form(m, D=D).to_dense() == hnf\n\n    # Test HNF with check_rank=True\n    assert hermite_normal_form(m, D=D, check_rank=True).to_dense() == hnf\n\n    # Test HNF with more rows than columns should raise DMShapeError\n    m_invalid = DM([[2, 3], [5, 7], [11, 13]], ZZ)\n    raises(DMShapeError, lambda: hermite_normal_form(m_invalid, D=D))\n\n    # Test HNF with a matrix over QQ should raise DMDomainError\n    m_qq = DM([[2, 3], [5, 7]], QQ)\n    raises(DMDomainError, lambda: hermite_normal_form(m_qq))\n\n    # Test _hermite_normal_form with matrix having negative values\n    m_neg = DM([[12, -6, 4], [3, 9, -6], [2, -16, 14]], ZZ)\n    hnf_neg = DM([[1, 0, -1], [0, 3, 0], [0, 0, 2]], ZZ)\n    assert _hermite_normal_form(m_neg).to_dense() == hnf_neg\n\n    # Test _hermite_normal_form_modulo_D with invalid modulus\n    invalid_D = ZZ(-10)\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(m, invalid_D))\n\n    # Test with zero matrix\n    zero_matrix = DM([[0, 0], [0, 0]], ZZ)\n    assert hermite_normal_form(zero_matrix).to_dense() == zero_matrix\n", "def test_hermite_normal_form():\n\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    m = DM([[12, 6, 4], [3, 9, 6]], ZZ)\n    hnf = DM([[3, 3, 2], [0, 3, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    m = DM([[12, 6], [3, 9], [2, 16]], ZZ)\n    hnf = DM([[1, 0], [0, 3], [0, 1]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    m = DM([[1, 2, 3], [4, 5, 6], [7, 8, 9]], ZZ)\n    hnf = DM([[1, 0, -1], [0, 3, 6], [0, 0, 0]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    # Test hermite normal form with D parameter\n    m = DM([[2, 3], [5, 4]], ZZ)\n    D = 17\n    hnf = DM([[1, 3], [0, 7]], ZZ)\n    assert hermite_normal_form(m, D=D).to_dense() == hnf\n\n    # Test hermite normal form raises DMDomainError for non-ZZ domain\n    m = DM([[1, 2], [3, 4]], QQ)\n    raises(DMDomainError, lambda: hermite_normal_form(m))\n\n    # Test hermite normal form raises DMDomainError for invalid D\n    m = DM([[1, 2], [3, 4]], ZZ)\n    raises(DMDomainError, lambda: hermite_normal_form(m, D=-1))\n\n    # Test hermite normal form with check_rank parameter\n    m = DM([[2, 3], [5, 4]], ZZ)\n    D =", "def test_hermite_normal_form():\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    D = ZZ(30)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m, D=D).to_dense() == hnf\n\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    D = ZZ(30)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m, D=D, check_rank=True).to_dense() == hnf\n\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1]], QQ)))\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1]], ZZ), D=QQ(2)))\n    raises(DMShapeError, lambda: hermite_normal_form(DM([[1, 2]], ZZ), D=ZZ(2)))\n"], "sample_1192": ["def test_Wild_exclude():\n    from sympy.abc import a, b, c, d\n    w = Wild('w', exclude=[a, b])\n    \n    assert c.match(w) == {w: c}\n    assert d.match(w) == {w: d}\n    assert a.match(w) is None\n    assert b.match(w) is None\n\n    expr = 2*c + 3*d\n    assert expr.match(2*w + 3*w) == {w: c + d}\n    assert expr.match(2*w + 3*w) == {w: c + d}\n    assert expr.match(2*a + 3*w) is None\n    assert expr.match(2*w + 3*a) is None\n", "def test_Wild_exclude():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    w = Symbol('w')\n    \n    W = Wild('W', exclude=[x, y])\n    \n    assert W.matches(z) == {W: z}\n    assert W.matches(w) == {W: w}\n    assert W.matches(x) is None\n    assert W.matches(y) is None\n", "def test_Wild_exclude():\n    x = Symbol(\"x\")\n    y = Symbol(\"y\")\n    z = Symbol(\"z\")\n    p = Symbol(\"p\", positive=True)\n    k = Symbol(\"k\", integer=True)\n    n = Symbol(\"n\", integer=True, positive=True)\n\n    S = Wild(\"S\", exclude=[x, y])\n    assert S.matches(x) is None\n    assert S.matches(y) is None\n    assert S.matches(z) == {S: z}\n    assert S.matches(p) == {S: p}\n    assert S.matches(k) == {S: k}\n    assert S.matches(n) == {S: n}\n\n    R = Wild(\"R\", exclude=[p, k])\n    assert R.matches(x) == {R: x}\n    assert R.matches(y) == {R: y}\n    assert R.matches(z) == {R: z}\n    assert R.matches(p) is None\n    assert R.matches(k) is None\n    assert R.matches(n) == {R: n}\n", "def test_disambiguate():\n    from sympy.abc import a, b, c, x, y\n    # Basic test with symbols\n    symbols_list = (Symbol('x'), Symbol('x'), Symbol('y'))\n    disambiguated_symbols = disambiguate(*symbols_list)\n    assert disambiguated_symbols == (Symbol('x'), Symbol('x_1'), Symbol('y'))\n\n    # Test with Dummy symbols\n    d1 = Dummy('x')\n    d2 = Dummy('x')\n    disambiguated_dummies = disambiguate(d1, d2)\n    assert disambiguated_dummies == (Symbol('x'), Symbol('x_1'))\n\n    # Test with symbols having different assumptions\n    ix = Symbol('x', integer=True)\n    vx = Symbol('x')\n    disambiguated = disambiguate(vx + ix)\n    assert disambiguated == (Symbol('x') + Symbol('x_1', integer=True),)\n\n    # Test with Symbols in expressions\n    expr = (Symbol('a') + Symbol('x'), Symbol('a') + Symbol('y'))\n    disambiguated_expr = disambiguate(*expr)\n    assert disambiguated_expr == (Symbol('a') + Symbol('x'), Symbol('a_1') + Symbol('y'))\n\n    # Test with repeated symbols in different forms\n    repeated_symbols = (Symbol('x'), Symbol('x'), Dummy('x'), Dummy('x'))\n    disambiguated_repeated = disambiguate(*repeated_symbols)\n    assert disambiguated_repeated == (Symbol('x'), Symbol('x_1'), Symbol('x_2'), Symbol('x_3'))\n\n    # Test with other named symbols\n    sym1 = Symbol('_x')\n    sym2 = Dummy('x')\n    sym3 = Dummy('x')\n    disambiguated_other = disambiguate(sym1, sym2, sym3)\n    assert disambiguated_other == (Symbol('x_2'), Symbol('x'), Symbol('x_1'))\n", "def test_Wild_exclude_and_properties():\n    x = Symbol(\"x\")\n    y = Symbol(\"y\")\n    p = Symbol(\"p\", positive=True)\n    k = Symbol(\"k\", integer=True)\n    n = Symbol(\"n\", integer=True, positive=True)\n    r = Symbol(\"r\", real=True)\n    \n    exclude_symbols = [x, y, k]\n    properties = [lambda expr: expr.is_real, lambda expr: not expr.is_integer]\n    \n    W = Wild(\"W\", exclude=exclude_symbols, properties=properties)\n    \n    assert W.matches(r) == {W: r}\n    assert W.matches(p) == {W: p}\n    assert W.matches(n) is None\n    assert W.matches(k) is None\n    assert W.matches(x) is None\n    assert W.matches(y) is None\n    \n    # Test with an expression\n    expr = Symbol(\"a\", real=True) + Symbol(\"b\", real=True, integer=False)\n    assert W.matches(expr) == {W: expr}\n"], "sample_1193": ["def test_are_coplanar():\n    # Testing are_coplanar with various cases\n    from sympy.geometry import Plane, Line3D\n    # Case: Points in the same plane\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n    p3 = Point3D(2, 2, 2)\n    assert are_coplanar(p1, p2, p3) == True\n\n    # Case: Points not in the same plane\n    p4 = Point3D(1, 2, 3)\n    assert are_coplanar(p1, p2, p4) == False\n\n    # Case: Points and line in the same plane\n    line = Line3D(p1, p2)\n    assert are_coplanar(p1, p2, p3, line) == True\n\n    # Case: Plane and points in the same plane\n    plane = Plane(p1, p2, p3)\n    assert are_coplanar(p1, p2, plane) == True\n\n    # Case: Plane and points not in the same plane\n    assert are_coplanar(p1, p4, plane) == False\n\n    # Case: 2D points converted to 3D\n    p5 = Point(0, 0)\n    p6 = Point(1, 1)\n    p7 = Point(2, 2)\n    assert are_coplanar(p5, p6, p7) == True\n    assert are_coplanar(p5, p6, p4) == False\n", "def test_are_coplanar():\n    # Test coplanarity of 3D points\n    a = Point3D(1, 1, 1)\n    b = Point3D(2, 2, 2)\n    c = Point3D(3, 3, 3)\n    d = Point3D(1, 2, 3)\n    assert are_coplanar(a, b, c) == False  # Collinear points are not coplanar\n    assert are_coplanar(a, b, d) == True  # Points are coplanar\n\n    # Test coplanarity with lines\n    from sympy.geometry import Line3D\n    l1 = Line3D(a, b)\n    l2 = Line3D(c, d)\n    l3 = Line3D(Point3D(0, 0, 0), Point3D(1, 1, 1))\n    assert are_coplanar(l1, l2) == False  # Lines are not coplanar\n    assert are_coplanar(l1, l3) == True  # Lines are coplanar\n\n    # Test with a mix of points and lines\n    assert are_coplanar(a, l1, d) == True  # Points and line are coplanar\n    assert are_coplanar(a, l1, c) == False  # Point c makes them non-coplanar\n\n    # Test with Plane\n    from sympy.geometry import Plane\n    plane = Plane(a, b, d)\n    assert are_coplanar(plane, c) == True  # Point c is on the plane\n    assert are_coplanar(plane, Point3D(4, 5, 6)) == False  # Point is not on the plane\n\n    # 2D points should be converted to 3D and handled\n    p1 = Point(1, 2)\n    p2 = Point(3, 4)\n    p3 = Point(5, 6)\n    assert are_coplanar(p1, p2, p3) == True  # 2D points in xy plane are coplanar\n", "def test_are_similar():\n    from sympy import Circle, Triangle\n    c1 = Circle(Point(0, 0), 4)\n    c2 = Circle(Point(1, 4), 4)\n    c3 = Circle(Point(1, 4), 5)\n    t1 = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n    t2 = Triangle(Point(0, 0), Point(2, 0), Point(0, 2))\n    t3 = Triangle(Point(0, 0), Point(3, 0), Point(0, 1))\n    \n    assert are_similar(c1, c2) == True\n    assert are_similar(c1, c3) == False\n    assert are_similar(t1, t2) == True\n    assert are_similar(t1, t3) == False\n    \n    raises(GeometryError, lambda: are_similar(c1, t1))\n", "def test_are_coplanar():\n    from sympy.geometry.plane import Plane\n    from sympy.geometry.line import Line3D\n\n    a = Line3D(Point3D(1, 0, 0), Point3D(0, 1, 1))\n    b = Line3D(Point3D(0, 1, 1), Point3D(1, 2, 0))\n    c = Line3D(Point3D(2, 2, 2), Point3D(3, 3, 3))\n    d = Line3D(Point3D(2, 1, 0), Point3D(3, 2, 1))\n    plane = Plane(Point3D(0, 0, 0), normal_vector=(1, 1, 1))\n\n    assert not are_coplanar(a, b, c)\n    assert are_coplanar(a, b, plane)\n    assert not are_coplanar(a, d)\n    assert are_coplanar(a, b, c, d)  # All lines intersect at (1, 1, 1)\n\n    # Test with 2D points converted to 3D points\n    p1 = Point(0, 0)\n    p2 = Point(1, 0)\n    p3 = Point(1, 1)\n    assert are_coplanar(p1, p2, p3)\n    assert not are_coplanar(p1, p2, Point3D(1, 1, 1))\n\n    # Test with not enough points\n    assert not are_coplanar(Point3D(1, 0, 0), Point3D(0, 1, 1))\n    assert not are_coplanar()\n", "def test_are_coplanar():\n    # Test when all points are coplanar\n    p1 = Point3D(1, 1, 1)\n    p2 = Point3D(2, 2, 2)\n    p3 = Point3D(3, 3, 3)\n    assert are_coplanar(p1, p2, p3) == False\n    \n    # Test when points and line entities are coplanar\n    l1 = Line3D(Point3D(1, 1, 1), Point3D(2, 2, 2))\n    l2 = Line3D(Point3D(2, 2, 2), Point3D(3, 3, 3))\n    assert are_coplanar(p1, p2, p3, l1, l2) == False\n    \n    # Test when points and plane entities are coplanar\n    from sympy.geometry import Plane\n    pl = Plane(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(1, -1, 0))\n    assert are_coplanar(p1, p2, p3, pl) == False\n    \n    # Test when points are not coplanar\n    p4 = Point3D(1, 0, 0)\n    assert are_coplanar(p1, p2, p3, p4) == False\n    \n    # Test mixed 2D and 3D entities\n    p2d = Point(1, 1)\n    assert are_coplanar(p1, p2d) == False\n    \n    # Test less than 3 points\n    assert are_coplanar(p1, p2) == False\n"], "sample_1194": ["def test_user_defined_function():\n    g = implemented_function('g', Lambda(x, 2*x + 1))\n    assert julia_code(g(x)) == \"g(x)\"\n    assert julia_code(g(x) + g(y)) == \"g(x) + g(y)\"\n    assert julia_code(g(x) * g(y)) == \"g(x) .* g(y)\"\n    assert julia_code(g(x)**2) == \"g(x) .^ 2\"\n    assert julia_code(g(x).diff(x)) == \"2\"\n    custom_funcs = {\"h\": \"custom_func\"}\n    h = Function(\"h\")\n    assert julia_code(h(x), user_functions=custom_funcs) == \"custom_func(x)\"\n    assert julia_code(h(x) + h(y), user_functions=custom_funcs) == \"custom_func(x) + custom_func(y)\"\n    assert julia_code(h(x) * h(y), user_functions=custom_funcs) == \"custom_func(x) .* custom_func(y)\"\n", "def test_UnknownFunction():\n    f = Function('f')\n    assert julia_code(f(x)) == \"f(x)\"\n    assert julia_code(f(x, y)) == \"f(x, y)\"\n    assert julia_code(f(x) + f(y)) == \"f(x) + f(y)\"\n    assert julia_code(f(x) * f(y)) == \"f(x) .* f(y)\"\n", "def test_user_defined_functions():\n    from sympy import Function\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_f\",\n        \"g\": [(lambda x: x.is_Matrix, \"matrix_g\"),\n              (lambda x: not x.is_Matrix, \"scalar_g\")]\n    }\n    \n    assert julia_code(f(x), user_functions=custom_functions) == \"custom_f(x)\"\n    assert julia_code(g(x), user_functions=custom_functions) == \"scalar_g(x)\"\n    A = Matrix([[1, 2], [3, 4]])\n    assert julia_code(g(A), user_functions=custom_functions) == \"matrix_g([1 2;\\n3 4])\"\n", "def test_lambda_function():\n    f = Lambda(x, x**2 + y)\n    assert julia_code(f) == \"x -> x .^ 2 + y\"\n    assert julia_code(f(z)) == \"(z -> z .^ 2 + y)(z)\"\n    g = Lambda((x, y), x**2 + y**2)\n    assert julia_code(g) == \"(x, y) -> x .^ 2 + y .^ 2\"\n    assert julia_code(g(z, 1)) == \"((z, one) -> z .^ 2 + one .^ 2)(z, 1)\"\n", "def test_user_functions():\n    from sympy import Function\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_julia_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert julia_code(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n           'existing_julia_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])'\n"], "sample_1195": ["def test_gamma_trace():\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n\n    # Test trace of a single line of gamma matrices\n    t = G(i0)*G(i1)\n    r = gamma_trace(t)\n    assert r.equals(4 * LorentzIndex.metric(i0, i1))\n\n    # Test trace involving momentum terms\n    p = TensorHead('p', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    t = ps*ps\n    r = gamma_trace(t)\n    assert r.equals(4 * p(i0)*p(-i0))\n\n    # Test trace with multiple gamma matrices\n    t = G(i0) * G(i1) * G(i2) * G(i3)\n    r = gamma_trace(t)\n    assert r.equals(4 * (LorentzIndex.metric(i0, i1)*LorentzIndex.metric(i2, i3) - LorentzIndex.metric(i0, i2)*LorentzIndex.metric(i1, i3) + LorentzIndex.metric(i0, i3)*LorentzIndex.metric(i1, i2)))\n\n    # Test trace with complex expressions\n    t = G(i0) * G(i1) * G(i2) * p(-i2) * G(i3) * p(-i3)\n    r = gamma_trace(t)\n    assert r.equals(4 * (LorentzIndex.metric(i0, i1) * p(i2) * p(-i2) - LorentzIndex.metric(i0, i2) * p(i1) * p(-i2) + LorentzIndex.metric(i0, i3) * p(i1) * p(-i3)))\n\n    # Test trace with tensor addition\n    t1 = G(i0) * G(i1)\n    t2 = G(i1) * G(i0)\n    t = t1 + t2\n    r = gamma_trace(t)\n    assert r.equals(4 * (LorentzIndex.metric(i0, i1) + LorentzIndex.metric(i1, i0)))\n", "def test_simplify_gpgp():\n    i0, i1, i2, i3, i4, i5, i6 = tensor_indices('i0:7', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n\n    # Test simplify_gpgp function with gamma matrices and momenta\n    expr = p(i0) * G(-i0) * p(i1) * G(-i1)\n    simplified_expr = simplify_gpgp(expr)\n    expected_expr = 4 * p(i0) * p(-i0)\n    assert _is_tensor_eq(simplified_expr, expected_expr)\n\n    expr = p(i0) * G(-i0) * p(i1) * G(-i1) * p(i2) * G(-i2)\n    simplified_expr = simplify_gpgp(expr)\n    expected_expr = 4 * p(i0) * p(i1) * p(-i1) * G(-i0)\n    assert _is_tensor_eq(simplified_expr, expected_expr)\n\n    expr = G(i0) * p(-i0) * G(i1) * p(-i1) * G(i2) * p(-i2)\n    simplified_expr = simplify_gpgp(expr)\n    expected_expr = 4 * p(-i0) * p(-i1) * G(i0) * G(i1) * G(i2)\n    assert _is_tensor_eq(simplified_expr, expected_expr)\n\n    expr = G(i0) * G(i1) * G(i2) * G(i3) * p(-i0) * p(-i1) * p(-i2) * p(-i3)\n    simplified_expr = simplify_gpgp(expr)\n    expected_expr = 16 * p(-i0) * p(-i1) * p(-i2) * p(-i3) * G(i0) * G(i1) * G(i2) * G(i3)\n    assert _is_tensor_eq(simplified_expr, expected_expr)\n", "def test_gamma_trace():\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n    mu, nu = tensor_indices(\"mu, nu\", LorentzIndex)\n    \n    # Test gamma trace of simple products\n    t = G(i0) * G(i1)\n    r = gamma_trace(t)\n    assert r.equals(4 * LorentzIndex.metric(i0, i1))\n\n    t = G(i0) * G(i1) * G(i2)\n    r = gamma_trace(t)\n    assert r.equals(0)\n\n    t = G(i0) * G(-i0)\n    r = gamma_trace(t)\n    assert r.equals(4 * 4)  # 4 * identity matrix trace\n\n    t = G(i0) * G(i1) * G(-i0) * G(-i1)\n    r = gamma_trace(t)\n    assert r.equals(4 * (2 * 4 - 4**2))\n\n    t = G(mu) * G(nu) * G(-nu) * G(-mu)\n    r = gamma_trace(t)\n    assert r.equals(4 * 4 * 4)\n    \n    t = G(mu) * G(nu) * G(-mu) * G(-nu)\n    r = gamma_trace(t)\n    assert r.equals(4 * (2 * 4 - 4**2))\n", "def test_simplify_gpgp():\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    \n    ps = p(i0) * G(-i0)\n    qs = q(i0) * G(-i0)\n    \n    # Test the simplification of the expression\n    t = ps * qs * qs\n    ts = simplify_gpgp(t)\n    assert _is_tensor_eq(ts, G(-i0) * p(i0) * q(i1) * q(-i1))\n    \n    # Another test with more complex expression\n    t = ps * qs * ps * qs\n    ts = simplify_gpgp(t)\n    assert _is_tensor_eq(ts, G(-i0) * p(i0) * q(-i0) * G(-i1) * p(i1) * q(-i1))\n", "def test_gamma_trace():\n    i0, i1, i2, i3, i4, i5 = tensor_indices('i0:6', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n\n    # Test simple trace with two gamma matrices\n    t = G(i0) * G(i1)\n    expected = 4 * LorentzIndex.metric(i0, i1)\n    assert _is_tensor_eq(gamma_trace(t), expected)\n\n    # Test trace with gamma matrices and momenta\n    ps = p(i0) * G(-i0)\n    qs = q(i0) * G(-i0)\n    expected_trace_psps = 4 * p(i0) * p(-i0)\n    expected_trace_psqs_psps = 4 * p(i0) * p(-i0) + 4 * p(i0) * q(-i0)\n    \n    assert _is_tensor_eq(gamma_trace(ps * ps), expected_trace_psps)\n    assert _is_tensor_eq(gamma_trace(ps * qs + ps * ps), expected_trace_psqs_psps)\n\n    # Test more complex traces\n    t = G(i0) * G(i1) * G(-i0)\n    expected = -2 * G(i1)\n    assert _is_tensor_eq(gamma_trace(t), expected)\n\n    t = G(i0) * G(i1) * G(-i0) * G(-i1)\n    expected = 4\n    assert _is_tensor_eq(gamma_trace(t), expected)\n\n    t = G(i0) * G(i1) * G(i2) * G(-i0) * G(-i1) * G(-i2)\n    expected = 4 * (LorentzIndex.metric(i1, i2) * LorentzIndex.metric(i1, i2) - LorentzIndex.metric(i1, i1) * LorentzIndex.metric(i2, i2) + LorentzIndex.metric(i1, i2) * LorentzIndex.metric(i2, i1))\n    assert _is_tensor_eq(gamma_trace(t), expected)\n"], "sample_1196": ["def test_invalid_set_type():\n    x = Symbol('x')\n    not_a_set = [1, 2, 3]\n    raises(TypeError, lambda: Contains(x, not_a_set))\n", "def test_contains_with_non_set():\n    x = Symbol('x')\n    raises(TypeError, lambda: Contains(x, 5))\n    raises(TypeError, lambda: Contains(x, \"string\"))\n", "def test_contains_invalid_set_type():\n    x = Symbol('x')\n    with raises(TypeError, match='expecting Set, not'):\n        Contains(x, 123)\n    with raises(TypeError, match='expecting Set, not'):\n        Contains(x, 'string')\n", "def test_contains_non_boolean_return():\n    class CustomSet(Set):\n            return S.true if element == 42 else S.false\n\n    assert Contains(42, CustomSet()) is S.true\n    assert Contains(43, CustomSet()) is S.false\n\n    class AnotherCustomSet(Set):\n            return Contains(element, self)\n\n    x = Symbol('x')\n    assert Contains(x, AnotherCustomSet()).eval() == Contains(x, AnotherCustomSet(), evaluate=False)\n", "def test_contains_non_set_type():\n    x = Symbol('x')\n    non_set = 123  # Using an integer, which is not a Set\n    raises(TypeError, lambda: Contains(x, non_set))\n"], "sample_1197": ["def test_get_units_non_prefixed():\n    base_units = (meter, second, kilogram)\n    units = (centimeter, kilometer, gram, joule, volt)\n    us = UnitSystem(base_units, units)\n\n    non_prefixed_units = us.get_units_non_prefixed()\n\n    assert meter in non_prefixed_units\n    assert second in non_prefixed_units\n    assert kilogram in non_prefixed_units\n    assert centimeter not in non_prefixed_units\n    assert kilometer not in non_prefixed_units\n    assert joule in non_prefixed_units\n    assert volt in non_prefixed_units\n", "def test_extend_unit_system():\n    base_units = (meter, kilogram, second)\n    units = (joule, watt, newton)\n    name = \"ExtendedSystem\"\n    description = \"Extended unit system with additional units\"\n    extended_system = SI.extend(base_units, units, name, description)\n\n    assert extended_system.name == name\n    assert extended_system.descr == description\n    assert set(extended_system._base_units) == set(SI._base_units + base_units)\n    assert set(extended_system._units) == set(SI._units + units)\n    assert extended_system._dimension_system == SI._dimension_system\n    assert extended_system.is_consistent == SI.is_consistent\n", "def test_get_unit_system():\n    us = UnitSystem([meter, second], [joule], \"TestSystem\")\n    \n    # Test getting an existing unit system by name\n    assert UnitSystem.get_unit_system(\"TestSystem\") is us\n    \n    # Test getting an existing unit system by instance\n    assert UnitSystem.get_unit_system(us) is us\n    \n    # Test ValueError for unsupported unit system\n    with raises(ValueError):\n        UnitSystem.get_unit_system(\"UnsupportedSystem\")\n\n    # Test getting the default unit system (assumes \"SI\" exists in `_unit_systems`)\n    assert UnitSystem.get_default_unit_system() is UnitSystem._unit_systems[\"SI\"]\n", "def test_get_units_non_prefixed():\n    meter.set_global_relative_scale_factor(S.One, length)\n    second.set_global_relative_scale_factor(S.One, time)\n    gram.set_global_relative_scale_factor(S.One, mass)\n\n    # Create a UnitSystem with both prefixed and non-prefixed units\n    us = UnitSystem(base_units=(meter, second), units=(meter, second, gram, kilogram, centimeter), name=\"test\")\n\n    non_prefixed_units = us.get_units_non_prefixed()\n\n    # Test that only non-prefixed units are returned\n    assert meter in non_prefixed_units\n    assert second in non_prefixed_units\n    assert gram in non_prefixed_units\n    assert kilogram not in non_prefixed_units\n    assert centimeter not in non_prefixed_units\n", "def test_unit_system_str_repr():\n    base_units = (meter, second)\n    units = (centimeter, kilometer, day)\n    name = \"CustomSystem\"\n    descr = \"A custom unit system for testing.\"\n\n    us = UnitSystem(base_units, units, name, descr)\n\n    assert str(us) == \"CustomSystem\"\n    assert repr(us) == \"<UnitSystem: ('meter', 'second')>\"\n\n    # Test unnamed unit system\n    unnamed_us = UnitSystem(base_units)\n    assert str(unnamed_us) == \"UnitSystem((meter, second))\"\n    assert repr(unnamed_us) == \"<UnitSystem: ('meter', 'second')>\"\n"], "sample_1198": ["def test_mathematica_with_additional_translations():\n    additional_translations = {\n        'CustomSin[x]': 'sin(x)',\n        'CustomCos[x]': 'cos(x)',\n        'CustomTan[x]': 'tan(x)',\n        'CustomExp[x]': 'exp(x)',\n        'CustomLog[x]': 'log(x)',\n        'CustomSqrt[x]': 'sqrt(x)',\n        'CustomAdd[x,y]': 'x + y',\n        'CustomMul[x,y]': 'x * y'\n    }\n\n    d = {\n        'CustomSin[30]': 'sin(30)',\n        'CustomCos[45]': 'cos(45)',\n        'CustomTan[60]': 'tan(60)',\n        'CustomExp[1]': 'exp(1)',\n        'CustomLog[10]': 'log(10)',\n        'CustomSqrt[4]': 'sqrt(4)',\n        'CustomAdd[2,3]': '2 + 3',\n        'CustomMul[2,3]': '2 * 3',\n    }\n\n    parser = MathematicaParser(additional_translations)\n\n    for e in d:\n        assert sympify(parser.parse(e)) == sympify(d[e])\n\n    assert parser.parse('CustomSin[x] + CustomCos[y]') == sympify('sin(x) + cos(y)')\n    assert parser.parse('CustomAdd[CustomMul[x,y], CustomSqrt[z]]') == sympify('x*y + sqrt(z)')\n\n    # Check with built-in translations along with additional translations\n    assert parser.parse('Sin[x] + CustomSin[y]') == sympify('sin(x) + sin(y)')\n    assert parser.parse('Exp[CustomLog[10]]') == sympify('exp(log(10))')\n", "def test_mathematica_invalid_syntax():\n    # Test invalid inputs that should raise SyntaxError\n    invalid_inputs = [\n        \"Sin[\",  # Unmatched opening bracket\n        \"Cos]\",  # Unmatched closing bracket\n        \"Exp[Log[\",  # Nested unmatched opening brackets\n        \"{a, b, c\",  # Unmatched opening curly brace\n        \"[a, b, c\",  # Unmatched opening square bracket\n        \"a + b *\",  # Trailing operator\n        \"a //\",  # Trailing postfix operator\n        \"a // + b\",  # Postfix followed by operator\n        \"#1 + #2 &[x, y, ]\",  # Trailing comma in function application\n        \"a b[\",  # Function application with unmatched bracket\n    ]\n    \n    for expr in invalid_inputs:\n        raises(SyntaxError, lambda: parse_mathematica(expr))\n", "def test_additional_mathematica_cases():\n    d = {\n        'ArcCosh[2]': 'acosh(2)',\n        'ArcSinh[2]': 'asinh(2)',\n        'ArcTanh[2]': 'atanh(2)',\n        'ArcCoth[2]': 'acoth(2)',\n        'ArcSech[2]': 'asech(2)',\n        'ArcCsch[2]': 'acsch(2)',\n        '1/2': '1/2',\n        '3^2': '3**2',\n        '2^3^4': '2**(3**4)',  # Verify right-associativity of exponentiation\n        'Sin[Pi/4]': 'sin(pi/4)',\n        'Exp[2 + 3]': 'exp(2 + 3)',\n        'Log[10, 1000]': 'log(1000, 10)',\n        'Gamma[x]': 'gamma(x)',  # Check Gamma function translation\n        'Abs[-x]': 'abs(-x)',  # Check absolute value\n        'Ceiling[Pi]': 'ceiling(pi)',  # Check ceiling function\n        'Floor[Pi]': 'floor(pi)',  # Check floor function\n        'CosIntegral[x]': 'Ci(x)',\n        'SinIntegral[x]': 'Si(x)',\n        'AiryAi[x]': 'airyai(x)',\n        'AiryBi[x]': 'airybi(x)',\n        'PolyLog[2, x]': 'polylog(2, x)',\n        'Max[1,2,3]': 'Max(1, 2, 3)',\n        'Min[1,2,3]': 'Min(1, 2, 3)',\n        'Max[1,2,3,4,5]': 'Max(1, 2, 3, 4, 5)',\n        'Min[1,2,3,4,5]': 'Min(1, 2, 3, 4, 5)',\n    }\n\n    for e in d:\n        assert parse_mathematica(e) == sympify(d[e])\n", "def test_mathematica_special_cases():\n    # Special case with underscore in function names, should not be treated as pattern\n    assert parse_mathematica(\"MyFunc_underscore[x_]\") == Function(\"MyFunc_underscore\")(symbols('x'))\n\n    # Special case with double underscores, should not be treated as pattern\n    assert parse_mathematica(\"Another__Func[x_]\") == Function(\"Another__Func\")(symbols('x'))\n\n    # Special case with single digit numbers\n    assert parse_mathematica(\"Sin[3]\") == sin(3)\n    assert parse_mathematica(\"3^2\") == 9\n    assert parse_mathematica(\"5!\") == sympy.factorial(5)\n\n    # Special case with variable-like function names\n    assert parse_mathematica(\"func1[x]\") == Function(\"func1\")(x)\n    assert parse_mathematica(\"func2[3]\") == Function(\"func2\")(3)\n\n    # Special case with deeply nested functions\n    nested_expr = \"f[g[h[i[j[k[l[m[n[o[p[q[r[s[t[u[v[w[x[y[z]]]]]]]]]]]]]]]]]]]]]\"\n    parsed_nested_expr = Function(\"f\")(Function(\"g\")(Function(\"h\")(Function(\"i\")(Function(\"j\")(\n        Function(\"k\")(Function(\"l\")(Function(\"m\")(Function(\"n\")(Function(\"o\")(Function(\"p\")(Function(\"q\")(\n        Function(\"r\")(Function(\"s\")(Function(\"t\")(Function(\"u\")(Function(\"v\")(Function(\"w\")(Function(\"x\")(\n        Function(\"y\")(Function(\"z\")))))))))))))))))))))\n    assert parse_mathematica(nested_expr) == parsed_nested_expr\n", "def test_mathematica_edge_cases():\n    # Edge cases with nested functions and edge symbols\n    assert parse_mathematica(\"Sin[Exp[Log[Cos[x]]]]\") == sin(exp(log(cos(x))))\n    assert parse_mathematica(\"ArcTan[Sin[x], Cos[y]]\") == atan2(cos(y), sin(x))\n    assert parse_mathematica(\"Exp[Sin[Log[Cos[x]]]]\") == exp(sin(log(cos(x))))\n    assert parse_mathematica(\"ArcSin[Exp[Cos[Log[x]]]]\") == asin(exp(cos(log(x))))\n    \n    # Edge cases with complex numbers and symbols\n    assert parse_mathematica(\"Exp[I Pi]\") == exp(I * pi)\n    assert parse_mathematica(\"Sin[2 I x]\") == sin(2 * I * x)\n    assert parse_mathematica(\"Cos[3 + 4 I]\") == cos(3 + 4 * I)\n    assert parse_mathematica(\"ArcTan[1, I]\") == atan2(I, 1)\n    assert parse_mathematica(\"Exp[2 Pi I]\") == exp(2 * pi * I)\n    \n    # Edge cases with invalid input\n    raises(SyntaxError, lambda: parse_mathematica(\"Sin[\"))\n    raises(SyntaxError, lambda: parse_mathematica(\"Exp[Log[Cos[]]]\"))\n    raises(SyntaxError, lambda: parse_mathematica(\"ArcTan[Sin[Cos[x]]]\"))\n    raises(SyntaxError, lambda: parse_mathematica(\"ArcSin[Exp[Cos[Log[x]]]]\"))\n"], "sample_1199": ["def test_tensor_product_latex():\n    from sympy.printing.latex import latex\n    ket = Qubit('0')\n    bra = QubitBra('1')\n    tp = TP(ket, bra)\n    assert latex(tp) == r'{\\left|0\\right\\rangle}\\otimes {\\left\\langle1\\right|}'\n    \n    combined_tensor_printing(True)\n    assert latex(tp) == r'{\\left|{0, 1}\\right\\rangle}'\n    \n    combined_tensor_printing(False)\n    assert latex(tp) == r'{\\left|0\\right\\rangle}\\otimes {\\left\\langle1\\right|}'\n", "def test_tensor_product_latex():\n    from sympy.printing.latex import latex\n    tp = TP(A, B)\n    assert latex(tp) == r'{A}\\otimes {B}'\n    tp = TP(A + B, C)\n    assert latex(tp) == r'{\\left(A + B\\right)}\\otimes {C}'\n    tp = TP(A, B + C)\n    assert latex(tp) == r'{A}\\otimes {\\left(B + C\\right)}'\n    tp = TP(A * B, C)\n    assert latex(tp) == r'{A B}\\otimes {C}'\n", "def test_tensor_product_trace():\n    matrix1 = Matrix([[1, 2], [3, 4]])\n    matrix2 = Matrix([[0, 5], [6, 7]])\n    tp = TP(matrix1, matrix2)\n    trace_indices = [0, 1]\n    \n    # No indices provided\n    assert tp._eval_trace() == Tr(matrix1).doit() * Tr(matrix2).doit()\n    \n    # Specific indices provided\n    assert tp._eval_trace(indices=trace_indices) == Tr(matrix1).doit() * matrix2\n", "def test_tensor_product_latex():\n    from sympy.printing.latex import latex\n    assert latex(TensorProduct(A, B)) == r'{A}\\otimes {B}'\n    assert latex(TensorProduct(A + B, C)) == r'{\\left({A} + {B}\\right)}\\otimes {C}'\n    assert latex(TensorProduct(A, B + C)) == r'{A}\\otimes {\\left({B} + {C}\\right)}'\n    assert latex(TensorProduct(A, B**2)) == r'{A}\\otimes {\\left({B}\\right)^{2}}'\n    assert latex(TensorProduct(A*B, C)) == r'{\\left({A}{B}\\right)}\\otimes {C}'\n", "def test_tensor_product_trace():\n    mat = Matrix([[1, 2], [3, 4]])\n    tp = TensorProduct(mat, mat)\n    assert Tr(tp).doit() == Tr(mat).doit() * Tr(mat).doit()\n\n    tp_with_indices = TensorProduct(A, B, C)\n    assert Tr(tp_with_indices, indices=[0, 2]).doit() == Tr(A).doit() * C * Tr(B).doit()\n"], "sample_1200": ["def test_extend_unit_system():\n    m = Quantity(\"meter\")\n    s = Quantity(\"second\")\n    kg = Quantity(\"kilogram\")\n    newton = Quantity(\"newton\")\n    base_units = [m, s, kg]\n    derived_units = {force: newton}\n    \n    us = UnitSystem(base_units, name=\"TestUnitSystem\", derived_units=derived_units)\n    extended_us = us.extend([Quantity(\"ampere\")], [Quantity(\"volt\")], \"ExtendedUnitSystem\")\n    \n    assert extended_us.name == \"ExtendedUnitSystem\"\n    assert len(extended_us._base_units) == 4\n    assert len(extended_us._units) == 6  # base_units + units (including derived ones)\n    assert extended_us._derived_units == derived_units\n    assert m in extended_us._base_units\n    assert Quantity(\"volt\") in extended_us._units\n", "def test_get_units_non_prefixed():\n    # Create some units\n    base_units = [meter, second]\n    units = [centimeter, kilometer, joule, volt, ohm, meter, second, kilogram, day, pebibyte]\n\n    # Create a UnitSystem instance\n    unit_system = UnitSystem(base_units, units)\n\n    # Get non-prefixed units\n    non_prefixed_units = unit_system.get_units_non_prefixed()\n\n    # Assert that the non-prefixed units are as expected\n    expected_non_prefixed_units = {meter, second, joule, volt, ohm, kilogram, day}\n    assert non_prefixed_units == expected_non_prefixed_units\n", "def test_unit_system_extend():\n    from sympy.physics.units import meter, second, joule, dimensionless\n    from sympy.physics.units.dimensions import length, time, energy\n\n    base_units = (meter, second)\n    derived_units = {energy: joule}\n    us = UnitSystem(base_units, derived_units=derived_units, name=\"test_system\")\n\n    assert us.name == \"test_system\"\n    assert us.dim == 2\n    assert us.derived_units == derived_units\n\n    extended_us = us.extend(base=[second], units=[joule], name=\"extended_system\")\n\n    assert extended_us.name == \"extended_system\"\n    assert extended_us.dim == 3\n    assert len(extended_us._units) == 4  # meter, second (base units), second, joule (units)\n    assert extended_us.derived_units == derived_units\n    assert extended_us.get_dimension_system() is None\n", "def test_unit_system_extend():\n    base_units = [meter, kilogram, second]\n    derived_units = {length: meter, mass: kilogram, time: second}\n    us = UnitSystem(base_units, name=\"BaseSystem\", derived_units=derived_units)\n    \n    extended_us = us.extend([ampere], [coulomb], name=\"ExtendedSystem\", description=\"Extended unit system\")\n    \n    assert extended_us.name == \"ExtendedSystem\"\n    assert extended_us.descr == \"Extended unit system\"\n    assert set(extended_us._base_units) == {meter, kilogram, second, ampere}\n    assert set(extended_us._units) == {meter, kilogram, second, ampere, coulomb}\n    assert extended_us._derived_units == {length: meter, mass: kilogram, time: second}\n    \n    # Check that the original unit system remains unchanged\n    assert us.name == \"BaseSystem\"\n    assert us._base_units == (meter, kilogram, second)\n    assert us._units == (meter, kilogram, second)\n    assert us._derived_units == derived_units\n", "def test_get_units_non_prefixed():\n    m = Quantity(\"m\")\n    cm = Quantity(\"cm\")\n    km = Quantity(\"km\")\n    second = Quantity(\"second\")\n    hour = Quantity(\"hour\")\n    minute = Quantity(\"minute\")\n    kilobyte = Quantity(\"kilobyte\")\n    mebibyte = Quantity(\"mebibyte\")\n\n    m.set_global_relative_scale_factor(1, meter)\n    cm.set_global_relative_scale_factor(1, centimeter)\n    km.set_global_relative_scale_factor(1, kilometer)\n    second.set_global_relative_scale_factor(1, s)\n    hour.set_global_relative_scale_factor(1, hour)\n    minute.set_global_relative_scale_factor(1, minute)\n    kilobyte.set_global_relative_scale_factor(1, kilo * byte)\n    mebibyte.set_global_relative_scale_factor(1, mebi * byte)\n\n    unit_system = UnitSystem(base_units=[m, second], units=[cm, km, hour, minute, kilobyte, mebibyte])\n\n    non_prefixed_units = unit_system.get_units_non_prefixed()\n\n    assert m in non_prefixed_units\n    assert second in non_prefixed_units\n    assert hour in non_prefixed_units\n    assert minute in non_prefixed_units\n    assert cm not in non_prefixed_units\n    assert km not in non_prefixed_units\n    assert kilobyte not in non_prefixed_units\n    assert mebibyte not in non_prefixed_units\n"], "sample_1201": ["def test_additional_conversions():\n    # Test conversion for magnetic_density\n    assert convert_to(gauss, tesla, cgs_gauss) == tesla/10**4\n    assert convert_to(tesla, gauss, cgs_gauss) == 10**4*gauss\n\n    # Test conversion for magnetic_flux\n    assert convert_to(maxwell, weber, cgs_gauss) == weber/10**8\n    assert convert_to(weber, maxwell, cgs_gauss) == 10**8*maxwell\n\n    # Test conversion for capacitance\n    assert convert_to(farad, sqrt(gram*centimeter**3)/second, cgs_gauss) == centimeter/10**5*speed_of_light**2\n    assert convert_to(sqrt(gram*centimeter**3)/second, farad, cgs_gauss) == farad*10**5/speed_of_light**2/centimeter\n\n    # Test conversion for inductance\n    assert convert_to(henry, sqrt(gram*centimeter**3)/second, cgs_gauss) == second**2/10**5*speed_of_light**2/centimeter\n    assert convert_to(sqrt(gram*centimeter**3)/second, henry, cgs_gauss) == henry*10**5/speed_of_light**2*centimeter/second**2\n\n    # Test conversion for resistance\n    assert convert_to(ohm, sqrt(gram*centimeter**3)/second, cgs_gauss) == second/10**5*speed_of_light**2/centimeter\n    assert convert_to(sqrt(gram*centimeter**3)/second, ohm, cgs_gauss) == ohm*10**5/speed_of_light**2*centimeter/second\n", "def test_conversion_magnetic_units():\n    assert convert_to(gauss, tesla, cgs_gauss) == tesla/10**4\n    assert convert_to(tesla, gauss, cgs_gauss) == 10**4*gauss\n    assert convert_to(maxwell, weber, cgs_gauss) == weber/10**8\n    assert convert_to(weber, maxwell, cgs_gauss) == 10**8*maxwell\n    assert convert_to(oersted, sqrt(gram/centimeter)/second, cgs_gauss) == sqrt(gram/centimeter)/second\n\n    # Magnetic units conversion in SI should remain the same, no conversion due to different systems\n    assert convert_to(gauss, tesla, SI) == gauss\n    assert convert_to(tesla, gauss, SI) == tesla\n    assert convert_to(maxwell, weber, SI) == maxwell\n    assert convert_to(weber, maxwell, SI) == weber\n    assert convert_to(oersted, sqrt(gram/centimeter)/second, SI) == oersted\n", "def test_voltage_conversion():\n    assert convert_to(statvolt, volt, cgs_gauss) == 2997924580*statvolt/10**6\n    assert convert_to(volt, statvolt, cgs_gauss) == 10**6*statvolt/2997924580\n    assert convert_to(statvolt, erg/statcoulomb, cgs_gauss) == erg/statcoulomb\n    assert convert_to(volt, erg/statcoulomb, cgs_gauss) == 10**6*erg/(2997924580*statcoulomb)\n    assert convert_to(statvolt, volt, SI) == statvolt\n    assert convert_to(volt, statvolt, SI) == volt\n", "def test_magnetic_units_conversion():\n    assert convert_to(gauss, tesla, cgs_gauss) == tesla/10**4\n    assert convert_to(tesla, gauss, cgs_gauss) == 10**4*gauss\n    assert convert_to(maxwell, weber, cgs_gauss) == weber/10**8\n    assert convert_to(weber, maxwell, cgs_gauss) == 10**8*maxwell\n    assert convert_to(oersted, sqrt(gram/centimeter)/second, cgs_gauss) == sqrt(gram/centimeter)/second\n    assert convert_to(sqrt(gram/centimeter)/second, oersted, cgs_gauss) == oersted\n", "def test_conversion_of_electric_units():\n    # Testing conversion of electric units\n    assert convert_to(statvolt, volt, cgs_gauss) == 299.792458*volt\n    assert convert_to(volt, statvolt, cgs_gauss) == volt / 299.792458\n    assert convert_to(statvolt, erg/statcoulomb, cgs_gauss) == erg/statcoulomb\n    assert convert_to(volt, erg/statcoulomb, cgs_gauss) == 299.792458*erg/statcoulomb\n\n    assert convert_to(ohm, ohm, cgs_gauss) == ohm\n    assert convert_to(ohm, second/centimeter, cgs_gauss) == 10**5/speed_of_light**2 * second/centimeter\n    assert convert_to(second/centimeter, ohm, cgs_gauss) == speed_of_light**2 / 10**5 * ohm\n\n    assert convert_to(farad, farad, cgs_gauss) == farad\n    assert convert_to(farad, centimeter, cgs_gauss) == 10**5/speed_of_light**2 * farad\n    assert convert_to(centimeter, farad, cgs_gauss) == speed_of_light**2 / 10**5 * centimeter\n\n    assert convert_to(henry, henry, cgs_gauss) == henry\n    assert convert_to(henry, second**2/centimeter, cgs_gauss) == 10**5/speed_of_light**2 / centimeter * second**2\n    assert convert_to(second**2/centimeter, henry, cgs_gauss) == speed_of_light**2 / 10**5 * second**2/centimeter\n"], "sample_1202": ["def test_mpf_norm_special_cases():\n    \"\"\"Test special cases for mpf_norm with infinities and NaN.\"\"\"\n    from mpmath.libmp.libmpf import (fnan, finf, fninf)\n    \n    assert mpf_norm(fnan, 53) == fnan\n    assert mpf_norm(finf, 53) == finf\n    assert mpf_norm(fninf, 53) == fninf\n    \n    inf_test = (0, 0, 0, 0)\n    assert Float._new(inf_test, 10)._mpf_ == (0, 0, 0, 0)\n    inf_test = (0, 0, -123, -1)\n    assert Float._new(inf_test, 10)._mpf_ == (0, 0, 0, 0)\n", "def test_issue_9999():\n    # Test for coverage of specific methods in the code file\n    # Test mpf_norm with special mpf values\n    from mpmath.libmp.libmpf import fnan, finf, fninf, fzero\n\n    assert mpf_norm(fnan, 53) == fnan\n    assert mpf_norm(finf, 53) == finf\n    assert mpf_norm(fninf, 53) == fninf\n    assert mpf_norm(fzero, 53) == fzero\n\n    # Test Rational constructor with a combination of numerator, denominator and gcd\n    r1 = Rational(8, -20, gcd=2)\n    assert r1.p == -4 and r1.q == 10  # gcd is applied to numerator only\n\n    # Test _eval_evalf method in Number class for AlgebraicNumber\n    a = AlgebraicNumber(sqrt(2), [1, 0])\n    assert a._eval_evalf(10) == sqrt(2).evalf(10)\n", "def test_issue_9999():\n    # Ensure `_as_integer_ratio` properly converts mpf\n    a = Float(2.5, 15)\n    assert _as_integer_ratio(a) == (5, 2)\n\n    # Test `seterr` functionality\n    seterr(divide=True)\n    raises(ValueError, lambda: 1 / S.Zero)\n    seterr(divide=False)\n    assert 1 / S.Zero is S.NaN\n\n    # Test `_decimal_to_Rational_prec` conversion\n    from decimal import Decimal\n    d = Decimal('1.5')\n    r, p = _decimal_to_Rational_prec(d)\n    assert r == Rational(3, 2)\n    assert p == 2\n", "def test_mpf_norm_special_cases():\n    assert mpf_norm((0, int(0), -123, -1), 10) == (0, int(0), 0, 0)  # NaN case\n    assert mpf_norm((0, int(0), -456, -2), 10) == (0, int(0), 0, 0)  # +inf case\n    assert mpf_norm((1, int(0), -789, -3), 10) == (0, int(0), 0, 0)  # -inf case\n", "def test_mpf_norm_special_cases():\n    from mpmath.libmp.libmpf import _normalize\n    import mpmath.libmp as mlib\n    rnd = mlib.round_nearest\n\n    # Special cases with zero mantissa and different exp types\n    # Make sure that zero, inf, and nan are correctly normalized\n    inf_mpf = (0, int(0), -456, -2, 53, rnd)\n    nan_mpf = (0, int(0), -123, -1, 53, rnd)\n    zero_mpf = (0, int(0), 0, 0, 53, rnd)\n\n    assert _normalize(inf_mpf, 53) == mlib.finf\n    assert _normalize((-1, int(0), -456, -2, 53, rnd), 53) == mlib.fninf\n    assert _normalize(nan_mpf, 53) == mlib.fnan\n    assert _normalize(zero_mpf, 53) == mlib.fzero\n"], "sample_1203": ["def test_additional_homomorphism_cases():\n    # Test cases for homomorphism method with different group types\n\n    # PermutationGroup -> FpGroup\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**2, b**3, (a*b)**4])\n    p1 = Permutation(0, 1)\n    p2 = Permutation(1, 2, 3)\n    P = PermutationGroup([p1, p2])\n    T = homomorphism(P, G, P.generators, [a, b])\n    assert T(p1 * p2 * p1**-1) == a * b * a**-1\n    assert T.is_surjective()\n\n    # Testing trivial homomorphism case\n    T = homomorphism(P, G, P.generators, [])\n    assert T.is_trivial()\n    assert T.image().order() == 1\n\n    # Testing with isomorphism check = False\n    p = Permutation(0, 1, 2)\n    P = PermutationGroup([p])\n    raises(ValueError, lambda: homomorphism(P, G, [p], [a], check=False))\n\n    # FpGroup -> FpGroup with different generators\n    H, x, y = free_group(\"x, y\")\n    K = FpGroup(H, [x**2, y**3])\n    T = homomorphism(G, K, G.generators, [x, y])\n    assert T.is_injective() is not None\n    assert T.is_surjective() is not None\n\n    # Testing is_injective and is_surjective for specific homomorphisms\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**4, b**6])\n    H, x, y = free_group(\"x, y\")\n    K = FpGroup(H, [x**2, y**3])\n    T = homomorphism(G, K, G.generators, [x**2, y**2])\n    assert not T.is_injective()\n    assert T.is_surjective()\n\n    # Testing invalid homomorphism creation\n    raises(ValueError, lambda: homomorphism(G, K, [a, b], [x]))\n    raises(ValueError, lambda: homomorphism(G, K, [", "def test_additional_homomorphisms():\n    # PermutationGroup -> FpGroup\n    P = PermutationGroup([Permutation(0, 1)(2, 3), Permutation(1, 2, 3)])\n    F, x, y = free_group(\"x, y\")\n    G = FpGroup(F, [x**2, y**3, (x*y)**2])\n    T = homomorphism(P, G, P.generators, G.generators)\n    assert T(P.generators[0] * P.generators[1]) == x * y\n\n    # Test injectivity and surjectivity for non-isomorphic groups\n    T = homomorphism(P, G, P.generators, [G.identity, G.identity])\n    assert not T.is_injective()\n    assert not T.is_surjective()\n\n    # Test trivial homomorphism for non-trivial groups\n    T = homomorphism(P, G, P.generators, [G.identity, G.identity])\n    assert T.is_trivial()\n\n    # GroupHomomorphism kernel computation for trivial homomorphism\n    assert T.kernel().order() == P.order()\n\n    # Test kernel for non-trivial homomorphism\n    P = PermutationGroup([Permutation(0, 1)(2, 3), Permutation(1, 2)])\n    T = homomorphism(P, G, P.generators, [x, y])\n    assert T.kernel().order() == 1  # Since T is injective\n\n    # Ensure correct error is raised for invalid homomorphism definition\n    with raises(ValueError):\n        homomorphism(P, G, P.generators, [x])\n", "def test_more_homomorphism_cases():\n    # Test homomorphism with trivial images\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    H = AlternatingGroup(4)\n    trivial_images = [H.identity] * len(G.generators)\n    T = homomorphism(G, H, G.generators, trivial_images)\n    assert T.is_trivial()\n    assert T.kernel().order() == G.order()\n\n    # Test homomorphism with mismatched generators and images\n    raises(ValueError, lambda: homomorphism(G, H, [a, b], [Permutation(0, 1, 2)]))\n\n    # Test homomorphism with invalid domain type\n    raises(TypeError, lambda: homomorphism(\"invalid_domain\", H, [a], [Permutation(0, 1, 2)]))\n\n    # Test homomorphism with invalid codomain type\n    raises(TypeError, lambda: homomorphism(G, \"invalid_codomain\", [a], [Permutation(0, 1, 2)]))\n\n    # Test kernel computation for a known kernel\n    G = DihedralGroup(4)\n    H = CyclicGroup(2)\n    gen = G.generators\n    T = homomorphism(G, H, [gen[0], gen[1]], [H.generators[0], H.identity])\n    kernel = T.kernel()\n    assert kernel.order() == 4  # Known kernel size for this mapping\n\n    # Test injectivity and surjectivity checks\n    T = homomorphism(G, H, [gen[0], gen[1]], [H.generators[0], H.identity])\n    assert not T.is_injective()\n    assert T.is_surjective()\n\n    # Test isomorphism for a direct isomorphism\n    C3 = CyclicGroup(3)\n    C3_prime = CyclicGroup(3)\n    T = homomorphism(C3, C3_prime, C3.generators, C3_prime.generators)\n    assert T.is_isomorphism()\n\n    # Test inversion and application in various cases\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    c = Permutation(3)(0,", "def test_additional_homomorphism_methods():\n    # Testing GroupHomomorphism methods: restrict_to, invert_subgroup, is_trivial, and compose\n\n    # Setup groups and homomorphisms\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    c = Permutation(3)(0, 1, 2)\n    d = Permutation(3)(1, 2, 3)\n    A = AlternatingGroup(4)\n    T = homomorphism(G, A, [a, b], [c, d])\n\n    # Test restrict_to\n    H = G.subgroup([a])\n    restricted_hom = T.restrict_to(H)\n    assert restricted_hom.domain == H\n    assert restricted_hom.codomain == A\n    assert restricted_hom(a) == c\n\n    # Test invert_subgroup\n    A_subgroup = A.subgroup([c])\n    inverted_subgroup = T.invert_subgroup(A_subgroup)\n    assert inverted_subgroup.order() == 3\n    assert all(T(g) in A_subgroup for g in inverted_subgroup.generators)\n\n    # Test is_trivial\n    trivial_hom = homomorphism(G, A, G.generators, [A.identity]*len(G.generators))\n    assert trivial_hom.is_trivial()\n\n    # Test compose\n    E, e = free_group(\"e\")\n    G_prime = FpGroup(E, [e**4])\n    P = PermutationGroup([Permutation(0, 1, 2, 3), Permutation(0, 2)])\n    T1 = homomorphism(G_prime, P, [e], [Permutation(0, 1, 2, 3)])\n    T2 = homomorphism(P, A, P.generators, [c, d])\n    composed_hom = T2.compose(T1)\n    assert composed_hom.domain == G_prime\n    assert composed_hom.codomain == A\n    assert composed_hom(e) == c\n", "def test_orbit_homomorphism():\n    # PermutationGroup induced homomorphism by action on a set\n    G = DihedralGroup(4)\n    omega = {0, 1, 2, 3}\n    T = orbit_homomorphism(G, omega)\n    assert T.is_surjective()\n    assert T(T.invert(Permutation(0, 2))) == Permutation(0, 2)\n    assert T.kernel().order() == 2\n    assert T.image().order() == 6\n"], "sample_1204": ["def test_base():\n    S = SymmetricGroup(5)\n    assert S.base == [0, 1, 2, 3]\n\n    A = AlternatingGroup(4)\n    assert A.base == [0, 1]\n\n    C = CyclicGroup(7)\n    assert C.base == [0]\n\n    D = DihedralGroup(6)\n    assert D.base == [0, 1]\n    \n    G = PermutationGroup(Permutation(1, 2, 3), Permutation(0, 3))\n    assert G.base == [0, 1]\n", "def test_derived_series_length():\n    G = SymmetricGroup(5)\n    derived_series = G.derived_series()\n    assert len(derived_series) == 4\n    H = AlternatingGroup(6)\n    derived_series = H.derived_series()\n    assert len(derived_series) == 1\n", "def test_abelian():\n    # Test if the group is abelian\n    a = Permutation([1, 2, 0])\n    b = Permutation([0, 2, 1])\n    G = PermutationGroup([a, b])\n    assert not G.is_abelian\n    \n    # Test for abelian groups\n    C = CyclicGroup(5)\n    assert C.is_abelian\n    A = AbelianGroup(3, 5)\n    assert A.is_abelian\n    \n    # Test for non-abelian groups\n    S = SymmetricGroup(3)\n    assert not S.is_abelian\n    D = DihedralGroup(4)\n    assert not D.is_abelian\n    \n    # Large group\n    S5 = SymmetricGroup(5)\n    assert not S5.is_abelian\n", "def test_permutation_group_contains():\n    a = Permutation([1, 0, 2])\n    b = Permutation([2, 0, 1])\n    G = PermutationGroup([a, b])\n    assert G.contains(a)\n    assert G.contains(b)\n    assert not G.contains(Permutation([0, 2, 1]))\n\n    S = SymmetricGroup(4)\n    assert S.contains(Permutation([0, 1, 2, 3]))\n    assert S.contains(Permutation([0, 3, 2, 1]))\n    assert not S.contains(Permutation([0, 4, 2, 1]))\n\n    C = CyclicGroup(5)\n    assert C.contains(Permutation([1, 2, 3, 4, 0]))\n    assert C.contains(Permutation([4, 0, 1, 2, 3]))\n    assert not C.contains(Permutation([0, 2, 1, 3, 4]))\n", "def test_trivial_group():\n    # Test basic properties of the trivial group\n    G = PermutationGroup(Permutation([]))\n    assert G.is_trivial\n    assert G.order() == 1\n    assert G.degree == 0\n    assert G.generators == [Permutation([])]\n    assert G.identity == Permutation([])\n    assert G.base == []\n    assert G.strong_gens == [Permutation([])]\n    assert G.is_abelian\n    assert G.is_nilpotent\n    assert G.is_solvable\n    assert G.is_cyclic\n    assert G.is_perfect\n    assert G.is_transitive\n    assert G.is_symmetric\n    assert G.is_alternating\n\n    H = PermutationGroup()\n    assert G == H\n    assert G.is_subgroup(H)\n    assert H.is_subgroup(G)\n    assert G.equals(H)\n\n    assert G.center().is_subgroup(G)\n    assert G.centralizer(G).is_subgroup(G)\n    assert G.normal_closure(G).is_subgroup(G)\n    assert G.derived_series() == [G]\n    assert G.lower_central_series() == [G]\n    assert G.commutator(G, G).is_subgroup(G)\n    assert G.pointwise_stabilizer([]).is_subgroup(G)\n    assert G.coset_factor(Permutation([])) == []\n    assert G.coset_rank(Permutation([])) == 0\n    assert G.coset_unrank(0) == Permutation([])\n    assert G.orbit(Permutation([])) == {Permutation([])}\n    assert G.orbit_rep(Permutation([]), Permutation([])) == Permutation([])\n    assert G.schreier_vector(Permutation([])) == [-1]\n    assert G.stabilizer(Permutation([])).is_subgroup(G)\n    assert G.random().is_Identity\n    assert G.sylow_subgroup(2).is_trivial\n    assert G.transitivity_degree == 0\n    assert G.minimal_block([]) == [0]\n    assert G.is_elementary(2)\n"], "sample_1205": ["def test_PolyRing_clone():\n    R1, x, y = ring(\"x,y\", ZZ)\n    R2 = R1.clone()\n    R3 = R1.clone(symbols=\"u,v\")\n    R4 = R1.clone(domain=QQ)\n    R5 = R1.clone(order=grlex)\n\n    assert R1 == R2\n    assert R1.symbols == R2.symbols\n    assert R1.domain == R2.domain\n    assert R1.order == R2.order\n\n    assert R3.symbols == _symbols(\"u,v\", seq=True)\n    assert R3.domain == R1.domain\n    assert R3.order == R1.order\n\n    assert R4.symbols == R1.symbols\n    assert R4.domain == QQ\n    assert R4.order == R1.order\n\n    assert R5.symbols == R1.symbols\n    assert R5.domain == R1.domain\n    assert R5.order == grlex\n", "def test_PolyElement_add_gens():\n    R, x = ring(\"x\", ZZ)\n    S = R.add_gens(\"y\")\n    assert S.gens == (x, S.y)\n    assert S.dtype.__name__ == 'PolyElement'\n    assert S.symbols == (R.symbols[0], S.symbols[1])\n    assert R != S\n    assert S.drop(\"y\") == R\n\n    R, x, y = ring(\"x,y\", ZZ)\n    S = R.add_gens(\"z\")\n    assert S.gens == (x, y, S.z)\n    assert S.symbols == (R.symbols[0], R.symbols[1], S.symbols[2])\n    assert R != S\n    assert S.drop(\"z\") == R\n", "def test_PolyRing_clone():\n    R, x, y, z = ring(\"x,y,z\", QQ, lex)\n    S = R.clone()\n\n    assert S == R\n    assert S.gens == R.gens\n    assert S.domain == R.domain\n    assert S.order == R.order\n\n    # Change symbols\n    S2 = R.clone(symbols=\"a,b,c\")\n    assert S2.gens != R.gens\n    assert S2.domain == R.domain\n    assert S2.order == R.order\n\n    # Change domain\n    S3 = R.clone(domain=ZZ)\n    assert S3.gens == R.gens\n    assert S3.domain != R.domain\n    assert S3.order == R.order\n\n    # Change order\n    S4 = R.clone(order=grlex)\n    assert S4.gens == R.gens\n    assert S4.domain == R.domain\n    assert S4.order != R.order\n", "def test_PolyRing_clone():\n    R, x, y, z = ring(\"x,y,z\", ZZ)\n    clone_R = R.clone()\n\n    assert R == clone_R\n    assert R is not clone_R\n    assert R.domain == clone_R.domain\n    assert R.symbols == clone_R.symbols\n    assert R.order == clone_R.order\n\n    clone_R_with_diff_order = R.clone(order=grlex)\n    assert clone_R_with_diff_order.order == grlex\n    assert R.order != clone_R_with_diff_order.order\n\n    clone_R_with_diff_symbols = R.clone(symbols=\"a,b,c\")\n    assert clone_R_with_diff_symbols.symbols == _symbols(\"a,b,c\", seq=True)\n    assert R.symbols != clone_R_with_diff_symbols.symbols\n\n    clone_R_with_diff_domain = R.clone(domain=QQ)\n    assert clone_R_with_diff_domain.domain == QQ\n    assert R.domain != clone_R_with_diff_domain.domain\n", "def test_PolyElement_add_gens():\n    R, x, y = ring(\"x,y\", ZZ)\n    z = Symbol(\"z\")\n\n    # Adding a new generator 'z' to the existing ring\n    R_new = R.add_gens([z])\n    assert R_new.symbols == (x, y, z)\n    assert R_new.domain == R.domain\n\n    # Adding an existing generator should not change the ring\n    R_same = R.add_gens([x])\n    assert R_same.symbols == (x, y)\n\n    # Check if the new ring can handle operations with the new generator\n    f = R_new.from_expr(x + y + z)\n    assert f == x + y + R_new.gens[2]\n\n    # Check if the new ring handles polynomials correctly\n    g = R_new.from_expr(x**2 + y*z + z**2)\n    assert g == x**2 + y*R_new.gens[2] + R_new.gens[2]**2\n\n    # Ensure the new generator is not in the original ring\n    raises(ValueError, lambda: R.from_expr(z))\n"], "sample_1206": ["def test_mpf_special_values():\n    # Special values in mpmath\n    from mpmath.libmp.libmpf import fnan, finf, fninf, fzero\n    assert Float(fnan) is S.NaN\n    assert Float(finf) is S.Infinity\n    assert Float(fninf) is S.NegativeInfinity\n    assert Float(fzero) == 0\n    # check that special values remain normalized\n    assert mpf_norm(fnan, 53) == fnan\n    assert mpf_norm(finf, 53) == finf\n    assert mpf_norm(fninf, 53) == fninf\n    assert mpf_norm(fzero, 53) == fzero\n", "def test_mod_inverse_edge_cases():\n    assert mod_inverse(1, 1) == 1\n    assert mod_inverse(-1, 1) == 0\n    assert mod_inverse(1, -1) == 1\n    assert mod_inverse(-1, -1) == 0\n    assert mod_inverse(0, 1) == 0\n    assert mod_inverse(0, -1) == 0\n    raises(ValueError, lambda: mod_inverse(0, 0))\n    raises(ValueError, lambda: mod_inverse(1, 0))\n    raises(ValueError, lambda: mod_inverse(-1, 0))\n\n    # Check edge cases with both positive and negative values\n    assert mod_inverse(1, 2) == 1\n    assert mod_inverse(1, -2) == -1\n    assert mod_inverse(-1, 2) == 1\n    assert mod_inverse(-1, -2) == -1\n\n    assert mod_inverse(2, 3) == 2\n    assert mod_inverse(-2, 3) == 1\n    assert mod_inverse(2, -3) == -1\n    assert mod_inverse(-2, -3) == -2\n\n    # Test with larger numbers\n    assert mod_inverse(123456789, 1000000007) == 18633540\n    assert mod_inverse(-123456789, 1000000007) == 813664467\n    assert mod_inverse(123456789, -1000000007) == -813664467\n    assert mod_inverse(-123456789, -1000000007) == -18633540\n", "def test_issue_12345():\n    from sympy.core import sqrt\n    assert Integer(4).floor() == 4\n    assert Integer(4).ceiling() == 4\n    assert Rational(7, 3).floor() == 2\n    assert Rational(7, 3).ceiling() == 3\n    assert Float(4.3).floor() == 4\n    assert Float(4.3).ceiling() == 5\n    assert sqrt(4).floor() == 2\n    assert sqrt(4).ceiling() == 2\n    assert sqrt(2).floor() == 1\n    assert sqrt(2).ceiling() == 2\n", "def test_mpf_norm():\n    assert mpf_norm((0, 0, 0, 0), 10) == fzero\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((0, 1, 0, 0), 10) == (0, 1, 0, 1)\n    assert mpf_norm((0, 1, 1, 0), 10) == (0, 5, 0, 3)\n    assert mpf_norm((1, 0, 1, 0), 10) == (1, 0, 1, 0)\n    assert mpf_norm((0, 0, 0, 3), 10) == (0, 0, 0, 0)\n", "def test_issue_11438():\n    assert divmod(5, S.Infinity) == (0, 5)\n    assert divmod(-5, S.Infinity) == (0, -5)\n    assert divmod(5, S.NegativeInfinity) == (-1, S.Infinity)\n    assert divmod(-5, S.NegativeInfinity) == (1, S.NegativeInfinity)\n    assert divmod(S.Infinity, 5) == (S.Infinity, 0)\n    assert divmod(S.NegativeInfinity, 5) == (S.NegativeInfinity, 0)\n"], "sample_1207": ["def test_explicit_multiplication():\n    transformations = standard_transformations + (implicit_multiplication_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    inputs = {\n        '2*x*y': 2 * x * y,\n        '3*x*y*z': 3 * x * y * z,\n        '(x + y) * (x - y)': (x + y) * (x - y),\n        'x*(y + z)': x * (y + z),\n        '(x + y)*z': (x + y) * z,\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n", "def test_invalid_syntax():\n    inputs = [\n        '2x +',\n        '3 + (4 * 5',\n        'sin(x',\n        'cos(x))',\n        '(2 * x))',\n        '((3 + 4) * 5',\n        'x 2',\n        '5 3 + 4',\n        '2**',\n        '((2 + 3)'\n    ]\n\n    for text in inputs:\n        raises(SyntaxError, lambda: parse_expr(text))\n", "def test_exponential_exponent():\n    transformations = standard_transformations + (function_exponentiation,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr('e^x', transformations=transformations) == exp(x)\n    assert parse_expr('10^2', transformations=transformations) == 10**2\n    assert parse_expr('2^3^2', transformations=transformations) == 2**(3**2)\n    assert parse_expr('(2^3)^2', transformations=transformations) == (2**3)**2\n    assert parse_expr('e^x^2', transformations=transformations) == exp(x**2)\n", "def test_convert_xor():\n    x = Symbol('x')\n    y = Symbol('y')\n    inputs = {\n        'x ^ y': Pow(x, y, evaluate=False),\n        'x ^ 2': Pow(x, 2, evaluate=False),\n        '2 ^ 3': Pow(2, 3, evaluate=False)\n    }\n    transformations = standard_transformations + (convert_xor,)\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n", "def test_transform_function_with_nested_parentheses():\n    transformations = standard_transformations + (convert_equals_signs, )\n    x = Symbol('x')\n    y = Symbol('y')\n    inputs = {\n        \"(1 + (2 + 3)) = x\": Eq(1 + (2 + 3), x),\n        \"((1 + 2) + 3) = x\": Eq((1 + 2) + 3, x),\n        \"(y + (x + 1)) = 0\": Eq(y + (x + 1), 0),\n        \"((y + x) + 1) = 0\": Eq((y + x) + 1, 0),\n        \"(1 + (2 + (3 + x))) = y\": Eq(1 + (2 + (3 + x)), y),\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n"], "sample_1208": ["def test_sample_numpy():\n    distribs_numpy = [\n        MatrixNormal('M', [[5, 6], [3, 4]], [[1, 0], [0, 1]], [[2, 1], [1, 2]]),\n        Wishart('W', 7, [[2, 1], [1, 2]])\n    ]\n    size = 3\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy is not installed. Abort tests for _sample_numpy.')\n    else:\n        for X in distribs_numpy:\n            raises(NotImplementedError, lambda: sample(X, size=size, library='numpy'))\n", "def test_sample_numpy():\n    distribs_numpy = [\n        MatrixNormal('M', [[5, 6]], [4], [[2, 1], [1, 2]])\n    ]\n    size = 3\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy not installed. Abort tests for _sample_numpy.')\n    else:\n        for X in distribs_numpy:\n            raises(NotImplementedError, lambda: sample(X, size=size, library='numpy'))\n        M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n        raises(NotImplementedError, lambda: sample(M, size=3, library='numpy'))\n", "def test_MatrixGamma_distribution():\n    alpha, beta = symbols('alpha beta', positive=True)\n    scale_matrix = Matrix([[2, 1], [1, 2]])\n    M = MatrixGamma('M', alpha, beta, scale_matrix)\n    X = MatrixSymbol('X', 2, 2)\n    num = exp(Trace(Matrix([[-1/3, 1/6], [1/6, -1/3]])*X)/beta)\n    term1 = (3**alpha * beta**(2*alpha) * pi**(3/2) * gamma(alpha) * gamma(alpha - 1/2))\n    assert density(M)(X).doit() == num * Determinant(X)**(alpha - 3/2) / term1\n    assert density(M)([[1, 0], [0, 1]]).doit() == exp(-4/(3*beta)) / term1.subs({X: ImmutableMatrix([[1, 0], [0, 1]])})\n", "def test_invalid_samples():\n    # Test invalid sampling libraries\n    M = MatrixNormal('M', [[1, 2]], [[1, 0], [0, 1]], [[1, 0], [0, 1]])\n    raises(NotImplementedError, lambda: sample(M, size=3, library='nonexistent_library'))\n    raises(ValueError, lambda: sample(M, size=3, library='nonexistent_library', seed=42))\n    \n    # Test invalid seed types\n    raises(ValueError, lambda: sample(M, size=3, library='scipy', seed='invalid_seed'))\n    raises(ValueError, lambda: sample(M, size=3, library='pymc', seed='invalid_seed'))\n\n    # Test sample shapes that are not tuples\n    raises(TypeError, lambda: sample(M, size=[3], library='scipy'))\n    raises(TypeError, lambda: sample(M, size=\"3\", library='pymc'))\n", "def test_MatrixPSpace_sample():\n    from sympy.stats.matrix_distributions import MatrixNormal, Wishart\n    scipy = import_module('scipy')\n    if not scipy:\n        skip('Scipy not installed. Abort tests for MatrixPSpace.sample.')\n    else:\n        # Test sampling for MatrixNormal distribution\n        M = MatrixNormal('M', [[1, 2]], [1], [[1, 0], [0, 1]])\n        samps = M.pspace.sample(size=5)\n        assert len(samps[M]) == 5\n        for sam in samps[M]:\n            assert Matrix(sam) in M.pspace.distribution.set\n\n        # Test sampling for Wishart distribution\n        W = Wishart('W', 3, [[2, 1], [1, 2]])\n        samps = W.pspace.sample(size=3)\n        assert len(samps[W]) == 3\n        for sam in samps[W]:\n            assert Matrix(sam) in W.pspace.distribution.set\n"], "sample_1209": ["def test_prefix_properties():\n    micro = PREFIXES['mu']\n\n    assert micro.name == 'micro'\n    assert micro.abbrev == 'mu'\n    assert micro.scale_factor == 10**-6\n    assert micro._latex(None) == r\"\\mu\"\n    assert str(micro) == 'mu'\n    assert repr(micro) == \"Prefix('micro', 'mu', -6)\"\n", "def test_prefix_properties():\n    p = Prefix('test', 't', 5)\n    assert p.name == 'test'\n    assert p.abbrev == 't'\n    assert p.scale_factor == 10**5\n\n    p_bin = Prefix('test_bin', 'tb', 3, base=2)\n    assert p_bin.name == 'test_bin'\n    assert p_bin.abbrev == 'tb'\n    assert p_bin.scale_factor == 2**3\n    assert p_bin.base == 2\n", "def test_prefix_representation():\n    y = PREFIXES['Y']\n    z = PREFIXES['Z']\n    e = PREFIXES['E']\n\n    assert str(y) == 'Y'\n    assert str(z) == 'Z'\n    assert str(e) == 'E'\n\n    assert repr(y) == \"Prefix('yotta', 'Y', 24)\"\n    assert repr(z) == \"Prefix('zetta', 'Z', 21)\"\n    assert repr(e) == \"Prefix('exa', 'E', 18)\"\n", "def test_prefix_initialization():\n    y = Prefix('yotta', 'Y', 24)\n    assert y.name == 'yotta'\n    assert y.abbrev == 'Y'\n    assert y.scale_factor == 10**24\n    assert y.base == 10\n    assert y.__str__() == 'Y'\n    assert y.__repr__() == \"Prefix('yotta', 'Y', 24)\"\n\n    mi = Prefix('mibi', 'Mi', 20, base=2, latex_repr=r\"\\text{Mi}\")\n    assert mi.name == 'mibi'\n    assert mi.abbrev == 'Mi'\n    assert mi.scale_factor == 2**20\n    assert mi.base == 2\n    assert mi._latex(None) == r\"\\text{Mi}\"\n    assert mi.__str__() == 'Mi'\n    assert mi.__repr__() == \"Prefix('mibi', 'Mi', 20, 2)\"\n", "def test_prefix_initialization():\n    p1 = Prefix('test_prefix', 'tp', 3)\n    assert p1.name == 'test_prefix'\n    assert p1.abbrev == 'tp'\n    assert p1.scale_factor == 10**3\n    assert p1.base == 10\n    assert str(p1) == 'tp'\n    assert repr(p1) == \"Prefix('test_prefix', 'tp', 3)\"\n\n    p2 = Prefix('binary_prefix', 'bp', 4, base=2)\n    assert p2.name == 'binary_prefix'\n    assert p2.abbrev == 'bp'\n    assert p2.scale_factor == 2**4\n    assert p2.base == 2\n    assert str(p2) == 'bp'\n    assert repr(p2) == \"Prefix('binary_prefix', 'bp', 4, 2)\"\n"], "sample_4": ["    def test_read_html_table_invalid_format(self, read, write, tmp_path):\n        \"\"\"Test read_html_table with invalid format.\"\"\"\n        fp = tmp_path / \"test_invalid_format.html\"\n\n        with open(fp, \"w\") as f:\n            f.write(\"<html><body><table><tr><td>Invalid Format</td></tr></table></body></html>\")\n\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html'\"):\n            read(fp, format=\"ascii.csv\")\n", "    def test_read_html_table_invalid_format(self):\n        \"\"\"Test read_html_table raises ValueError for invalid format.\"\"\"\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html'\"):\n            read_html_table(\"dummy_file.html\", format=\"csv\")\n", "    def test_html_identify_invalid_format(self, invalid_filepath):\n        \"\"\"Test html_identify function with invalid file formats.\"\"\"\n        assert not html_identify(None, invalid_filepath, None)\n", "    def test_read_html_table_invalid_format(self, read, tmp_path):\n        \"\"\"Test read_html_table raises ValueError on invalid format.\"\"\"\n        fp = tmp_path / \"test_invalid_format.html\"\n        fp.write_text(\"<table><tr><td>Invalid format test</td></tr></table>\")\n\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html'\"):\n            read(fp, format=\"ascii.csv\")\n", "    def test_html_identify(self, tmp_path):\n        \"\"\"Test identification of HTML format files.\"\"\"\n        fp = tmp_path / \"test_html_identify.html\"\n        assert html_identify(None, str(fp), None)\n        assert not html_identify(None, str(fp.with_suffix(\".txt\")), None)\n"], "sample_11": ["def test_sanitize_slices_combined_slices():\n    # Test combining sanitize_slices and combine_slices\n    slices1 = sanitize_slices([slice(None), 1, slice(2, 5)], 4)\n    slices2 = sanitize_slices([slice(3, 6), Ellipsis], 4)\n    \n    combined_slices = [combine_slices(s1, s2) for s1, s2 in zip(slices1, slices2)]\n    \n    expected_combined_slices = [\n        slice(3, 6),  # slice(None) combined with slice(3, 6)\n        1,            # 1 combined with slice(None)\n        slice(5, 8),  # slice(2, 5) combined with Ellipsis (fills in the rest with slice(None))\n        slice(None)   # Ellipsis fills in with slice(None)\n    ]\n    \n    assert combined_slices == expected_combined_slices\n", "def test_invalid_ndim_slices():\n    with pytest.raises(ValueError):\n        sanitize_slices((1, 2, 3, 4), 3)\n\n    with pytest.raises(ValueError):\n        sanitize_slices((1, 2, 3, 4, 5), 2)\n", "def test_sanitize_slices_integer():\n    slices = 2\n    ndim = 4\n    expected = [2, slice(None), slice(None), slice(None)]\n    sanitized = sanitize_slices(slices, ndim)\n    assert sanitized == expected, f\"Expected {expected} but got {sanitized}\"\n", "def test_combined_slices():\n    wcs = WCS_SPECTRAL_CUBE\n    sub1 = SlicedLowLevelWCS(wcs, [slice(None), slice(1, 10), slice(None)])\n    sub2 = SlicedLowLevelWCS(sub1, [3, slice(2, None)])\n    sub3 = SlicedLowLevelWCS(sub2, [slice(None), slice(2, 8)])\n\n    # Assert that sub3 should have the same effect as a single combined slice\n    combined_sub = SlicedLowLevelWCS(wcs, [3, slice(3, 10), slice(2, 8)])\n\n    assert_allclose(sub3.pixel_to_world_values(3, 5), combined_sub.pixel_to_world_values(3, 5))\n    assert_allclose(sub3.world_to_pixel_values(10, 25), combined_sub.world_to_pixel_values(10, 25))\n    assert sub3.pixel_n_dim == combined_sub.pixel_n_dim\n    assert sub3.world_n_dim == combined_sub.world_n_dim\n    assert_equal(sub3.axis_correlation_matrix, combined_sub.axis_correlation_matrix)\n", "def test_sanitize_slices_int():\n    slices = 2\n    ndim = 3\n    result = sanitize_slices(slices, ndim)\n    expected = [2, slice(None), slice(None)]\n    assert result == expected\n"], "sample_15": ["    def test_value_property(self, quantity, expected):\n        assert np.all(quantity.value == expected)\n", "def test_testcases_logaddexp_logaddexp2(self, tc):\n    return test_testcase(tc)\n", "    def test_to(self):\n        q = 100.0 * u.m\n        q_converted = q.to(u.cm)\n        assert q_converted.value == 10000.0\n        assert q_converted.unit == u.cm\n", "def test_quantity_iterator():\n    q = np.array([1, 2, 3, 4]) * u.m\n    q_iter = q.flat\n    assert isinstance(q_iter, QuantityIterator)\n    \n    # Test iteration\n    for idx, value in enumerate(q_iter):\n        assert value == q[idx]\n    \n    # Test indexing\n    assert q_iter[1] == q[1]\n    assert np.all(q_iter[1:3] == q[1:3])\n    \n    # Test length\n    assert len(q_iter) == len(q)\n    \n    # Test base\n    assert q_iter.base is q\n    \n    # Test setting items\n    q_iter[1] = 5 * u.m\n    assert q[1] == 5 * u.m\n    \n    # Test coords and index\n    assert q_iter.coords == (0,)\n    assert q_iter.index == 0\n    \n    # Test copying the iterator\n    q_copy = q_iter.copy()\n    assert np.all(q_copy == q.flatten())\n    assert q_copy.unit == q.unit\n", "    def test_new_like(self):\n        q1 = [1, 2, 3] * u.m\n        q2 = [4, 5, 6] * u.m\n        q3 = [7, 8, 9] * u.m\n        cols = [q1, q2, q3]\n        result = q1.info.new_like(cols, length=4)\n        assert result.unit == u.m\n        assert result.shape == (4,)\n        assert result.info.dtype == q1.info.dtype\n        assert result.info.meta == q1.info.meta\n"], "sample_27": ["def test_diff_table_with_different_units():\n    \"\"\"Test tables that are otherwise identical but have different units.\"\"\"\n\n    c1 = Column(\"A\", format=\"E\", unit=\"m\", array=[1.0, 2.0, 3.0])\n    c2 = Column(\"A\", format=\"E\", unit=\"cm\", array=[1.0, 2.0, 3.0])\n\n    ta = BinTableHDU.from_columns([c1])\n    tb = BinTableHDU.from_columns([c2])\n\n    diff = TableDataDiff(ta.data, tb.data)\n\n    assert not diff.identical\n    assert diff.diff_column_attributes == [(('A', 'unit'), ('m', 'cm'))]\n\n    report = diff.report()\n    assert \"Column A has different units:\" in report\n    assert \"a> 'm'\" in report\n    assert \"b> 'cm'\" in report\n", "    def test_ignore_table_fields_with_diff(self):\n        \"\"\"\n        Test ignoring specific fields in tables with differences and ensuring the remaining fields are compared.\n        \"\"\"\n        c1 = Column(\"A\", format=\"L\", array=[True, False])\n        c2 = Column(\"B\", format=\"X\", array=[[0], [1]])\n        c3 = Column(\"C\", format=\"4I\", dim=\"(2, 2)\", array=[[0, 1, 2, 3], [4, 5, 6, 7]])\n\n        c4 = Column(\"B\", format=\"X\", array=[[1], [0]])  # Different data\n        c5 = Column(\"C\", format=\"4I\", dim=\"(2, 2)\", array=[[1, 2, 3, 4], [5, 6, 7, 8]])  # Different data\n        c6 = Column(\"D\", format=\"L\", array=[True, True])  # Additional column\n\n        ta = BinTableHDU.from_columns([c1, c2, c3])\n        tb = BinTableHDU.from_columns([c1, c4, c5, c6])\n\n        # Ignore fields B and C, but not the new field D\n        diff = TableDataDiff(ta.data, tb.data, ignore_fields=[\"B\", \"C\"])\n        assert not diff.identical\n\n        # The only common column should be c1, and field D should be detected as different\n        assert len(diff.common_columns) == 1\n        assert diff.common_column_names == {\"a\"}\n        assert diff.diff_column_names == ([], [\"D\"])\n        assert diff.diff_ratio == 0\n        assert diff.diff_total == 0\n\n        report = diff.report()\n        assert \"Extra column D of format L in b\" in report\n", "def test_fitsdiff_ignore_keywords_and_comments(tmp_path):\n    \"\"\"Test FITSDiff with ignore_keywords and ignore_comments parameters.\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdr1 = Header([(\"A\", 1), (\"B\", 2), (\"C\", 3, \"Comment1\"), (\"D\", 4, \"Comment2\")])\n    hdr2 = Header([(\"A\", 1), (\"B\", 2), (\"C\", 5, \"Comment3\"), (\"D\", 4, \"Comment4\")])\n\n    hdulist1 = HDUList([PrimaryHDU(header=hdr1)])\n    hdulist2 = HDUList([PrimaryHDU(header=hdr2)])\n\n    hdulist1.writeto(path1)\n    hdulist2.writeto(path2)\n\n    diff = FITSDiff(\n        path1, path2, ignore_keywords=[\"C\"], ignore_comments=[\"D\"]\n    )\n    assert diff.identical, diff.report()\n\n    # Ensure ignoring keywords and comments works as expected\n    diff = FITSDiff(\n        path1, path2, ignore_keywords=[\"C\"]\n    )\n    assert not diff.identical\n    assert \"Keyword C        has different values\" in diff.report()\n\n    diff = FITSDiff(\n        path1, path2, ignore_comments=[\"D\"]\n    )\n    assert not diff.identical\n    assert \"Keyword C        has different values\" in diff.report()\n    assert \"Keyword D        has different comments\" not in diff.report()\n", "def test_identical_headers_with_blank_cards():\n    ha = Header([(\"A\", 1), (\"B\", 2), (\"C\", 3), (\"\", \"\")])\n    hb = ha.copy()\n    hc = ha.copy()\n    hc.append()\n\n    diff_ab = HeaderDiff(ha, hb)\n    diff_ac = HeaderDiff(ha, hc)\n    diff_bc = HeaderDiff(hb, hc)\n\n    assert diff_ab.identical\n    assert not diff_ac.identical\n    assert not diff_bc.identical\n\n    assert diff_ac.diff_keyword_count == (4, 5)\n    assert diff_bc.diff_keyword_count == (4, 5)\n\n    report_ac = diff_ac.report()\n    report_bc = diff_bc.report()\n    assert \"Headers have different number of cards\" in report_ac\n    assert \"Headers have different number of cards\" in report_bc\n", "def test_fitsdiff_with_ignore_fields(tmp_path):\n    \"\"\"Test FITSDiff with ignored fields in table data.\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    c1 = Column(\"A\", format=\"L\", array=[True, False])\n    c2 = Column(\"B\", format=\"J\", array=[1, 2])\n    c3 = Column(\"C\", format=\"E\", array=[0.1, 0.2])\n    \n    # Create two tables with slight differences\n    t1 = BinTableHDU.from_columns([c1, c2, c3])\n    t2 = BinTableHDU.from_columns([c1, Column(\"B\", format=\"J\", array=[2, 3]), c3])\n    \n    hdulist1 = HDUList([PrimaryHDU(), t1])\n    hdulist2 = HDUList([PrimaryHDU(), t2])\n    \n    hdulist1.writeto(path1)\n    hdulist2.writeto(path2)\n    \n    # Ignore column 'B'\n    diff = FITSDiff(path1, path2, ignore_fields=[\"B\"])\n    assert diff.identical, diff.report()\n    \n    # Do not ignore column 'B'\n    diff = FITSDiff(path1, path2)\n    assert not diff.identical, diff.report()\n    assert \"Column B data differs\" in diff.report()\n"], "sample_68": ["    def test_cleanse_setting_dict(self):\n        \"\"\" Test cleansing a dictionary setting \"\"\"\n        sensitive_dict = {\n            'API_KEY': 'super_secret_key',\n            'NESTED': {\n                'TOKEN': 'nested_secret_token'\n            },\n            'NON_SENSITIVE': 'visible_value'\n        }\n        cleansed = cleanse_setting('TEST_DICT', sensitive_dict)\n        self.assertEqual(cleansed['API_KEY'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed['NESTED']['TOKEN'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed['NON_SENSITIVE'], 'visible_value')\n", "    def test_get_safe_settings(self):\n        safe_settings = get_safe_settings()\n\n        # Ensure sensitive settings are cleansed\n        self.assertEqual(safe_settings[\"SECRET_KEY\"], CLEANSED_SUBSTITUTE)\n        self.assertEqual(safe_settings[\"API_KEY\"], CLEANSED_SUBSTITUTE)\n        \n        # Ensure regular settings are intact\n        self.assertEqual(safe_settings[\"REGULAR_SETTING\"], \"regular_value\")\n        \n        # Ensure nested sensitive settings are cleansed\n        self.assertEqual(safe_settings[\"NESTED_SETTING\"][\"INNER_SECRET\"], CLEANSED_SUBSTITUTE)\n        self.assertEqual(safe_settings[\"NESTED_SETTING\"][\"INNER_REGULAR\"], \"inner_regular_value\")\n        \n        # Ensure callable settings are wrapped\n        self.assertIsInstance(safe_settings[\"CALLABLE_SETTING\"], CallableSettingWrapper)\n", "    def test_dict_with_sensitive_keys(self):\n        sensitive_dict = {\n            'API_KEY': 'should_not_display',\n            'nested': {\n                'SECRET_KEY': 'nested_should_not_display',\n                'normal_key': 'display'\n            }\n        }\n        expected_result = {\n            'API_KEY': CLEANSED_SUBSTITUTE,\n            'nested': {\n                'SECRET_KEY': CLEANSED_SUBSTITUTE,\n                'normal_key': 'display'\n            }\n        }\n        self.assertEqual(cleanse_setting('sensitive_dict', sensitive_dict), expected_result)\n    ", "    def test_cleanse_setting_with_dict(self):\n        settings_dict = {\n            'API_KEY': 'my_api_key',\n            'NESTED': {\n                'PASSWORD': 'my_password',\n                'NON_SENSITIVE': 'safe_value'\n            }\n        }\n        expected_dict = {\n            'API_KEY': CLEANSED_SUBSTITUTE,\n            'NESTED': {\n                'PASSWORD': CLEANSED_SUBSTITUTE,\n                'NON_SENSITIVE': 'safe_value'\n            }\n        }\n        self.assertEqual(cleanse_setting('SETTINGS', settings_dict), expected_dict)\n", "    def test_cleanse_setting_with_nested_dict(self):\n        nested_dict = {\n            'API_KEY': 'should_not_be_displayed',\n            'nested': {\n                'SECRET': 'hidden_value',\n                'non_sensitive': 'visible_value'\n            }\n        }\n        expected_dict = {\n            'API_KEY': CLEANSED_SUBSTITUTE,\n            'nested': {\n                'SECRET': CLEANSED_SUBSTITUTE,\n                'non_sensitive': 'visible_value'\n            }\n        }\n        self.assertEqual(cleanse_setting('SOME_SETTING', nested_dict), expected_dict)\n"], "sample_80": ["    def test_chained_raw_query(self):\n        raw_query = RawQuery(\"SELECT * FROM author WHERE num > %s\", 'default', params=(2,))\n        chained_query = raw_query.chain(using='default')\n        self.assertEqual(chained_query.sql, raw_query.sql)\n        self.assertEqual(chained_query.params, raw_query.params)\n        self.assertEqual(chained_query.using, 'default')\n", "    def test_add_annotation(self):\n        query = Query(Author)\n        query.add_annotation(Count('id'), alias='id_count')\n        self.assertIn('id_count', query.annotations)\n        self.assertIsInstance(query.annotations['id_count'], Count)\n        self.assertEqual(query.annotations['id_count'].source_expressions[0].name, 'id')\n", "    def test_rawquery_get_columns(self):\n        query = RawQuery(\"SELECT id, name FROM author\", using=\"default\")\n        class MockCursor:\n            description = [(\"id\",), (\"name\",)]\n\n        class MockConnection:\n            class introspection:\n                @staticmethod\n                    return column.lower()\n\n            class ops:\n                @staticmethod\n                    return val\n\n            @staticmethod\n                return MockCursor()\n\n        connections[\"default\"] = MockConnection()\n        columns = query.get_columns()\n        self.assertEqual(columns, [\"id\", \"name\"])\n", "    def test_field_error_invalid_lookup(self):\n        query = Query(Author)\n        msg = 'Unsupported lookup'\n        with self.assertRaisesMessage(FieldError, msg):\n            query.build_where(Q(name__unsupported='test'))\n", "    def test_raw_query_get_columns(self):\n        raw_query = RawQuery(\"SELECT * FROM Author\", using='default')\n        with self.assertRaises(RuntimeError):\n            raw_query.get_columns()\n"], "sample_86": ["    def test_lazy_object_deepcopy(self):\n        \"\"\"\n        deepcopy works correctly for Promises.\n        \"\"\"\n        original_object = [1, 2, 3]\n        lazy_obj = lazy(lambda: original_object, list)()\n        copied_lazy_obj = copy.deepcopy(lazy_obj)\n        \n        self.assertEqual(lazy_obj, copied_lazy_obj)\n        self.assertIsNot(lazy_obj, copied_lazy_obj)\n", "    def test_lazy_cast_methods(self):\n        \"\"\"\n        Ensure that lazy properly casts to str and bytes.\n        \"\"\"\n        lazy_text = lazy(lambda: \"This is a lazy text\", str)\n        lazy_bytes = lazy(lambda: b\"This is a lazy byte string\", bytes)\n\n        self.assertEqual(str(lazy_text()), \"This is a lazy text\")\n        self.assertEqual(bytes(lazy_bytes()), b\"This is a lazy byte string\")\n", "    def test_lazy_object_methods(self):\n        \"\"\"\n        Lazy object should correctly delegate method calls to the wrapped object.\n        \"\"\"\n        class Klazz:\n                return 'method one'\n\n                return 'method two'\n\n        t = lazy(lambda: Klazz(), Klazz)()\n        self.assertEqual(t.method_one(), 'method one')\n        self.assertEqual(t.method_two(), 'method two')\n", "    def test_partition(self):\n        \"\"\"Test the partition function.\"\"\"\n        result = partition(lambda x: x > 3, range(5))\n        self.assertEqual(result, ([0, 1, 2, 3], [4]))\n        result = partition(lambda x: x % 2 == 0, range(10))\n        self.assertEqual(result, ([1, 3, 5, 7, 9], [0, 2, 4, 6, 8]))\n        result = partition(lambda x: True, [])\n        self.assertEqual(result, ([], []))\n", "    def test_lazy_object_evaluation(self):\n        \"\"\"\n        Ensure that lazy evaluation happens correctly and only when needed.\n        \"\"\"\n        eval_tracker = mock.MagicMock(return_value='Evaluated')\n\n        lazy_func = lazy(eval_tracker, str)\n        lazy_obj = lazy_func()\n\n        # Ensure the function hasn't been called yet\n        eval_tracker.assert_not_called()\n\n        # Accessing the lazy object should call the function\n        self.assertEqual(str(lazy_obj), 'Evaluated')\n        eval_tracker.assert_called_once()\n"], "sample_95": ["    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600, public=True, must_revalidate=True)\n        self.assertEqual(\n            set(response['Cache-Control'].split(', ')),\n            {'max-age=3600', 'public', 'must-revalidate'}\n        )\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, public=True, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'public, max-age=3600')\n\n        patch_cache_control(response, private=True, no_cache=True)\n        self.assertEqual(set(response['Cache-Control'].split(', ')), {'private', 'no-cache', 'max-age=3600'})\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, public=True, max_age=3600, no_cache=True)\n        self.assertIn('Cache-Control', response)\n        cache_control_values = set(response['Cache-Control'].split(', '))\n        expected_values = {'public', 'max-age=3600', 'no-cache'}\n        self.assertEqual(cache_control_values, expected_values)\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600, public=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600, public')\n\n        patch_cache_control(response, private=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600, private')\n\n        patch_cache_control(response, max_age=1800)\n        self.assertEqual(response['Cache-Control'], 'max-age=1800, private')\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, no_cache=True, max_age=3600)\n        self.assertEqual(\n            set(response['Cache-Control'].split(', ')),\n            {'no-cache', 'max-age=3600'}\n        )\n\n        patch_cache_control(response, private=True, max_age=1800)\n        self.assertEqual(\n            set(response['Cache-Control'].split(', ')),\n            {'private', 'max-age=1800'}\n        )\n"], "sample_97": ["    def test_echo_on_enabled(self, mocked_termios, mocked_isatty):\n        attr_list = [None, None, None, 0]  # Initial state without ECHO\n        mocked_termios.tcgetattr.return_value = attr_list\n\n        autoreload.ensure_echo_on()\n\n        attr_list[3] |= mocked_termios.ECHO  # ECHO should be added\n        mocked_termios.tcsetattr.assert_called_once_with(\n            sys.stdin, mocked_termios.TCSANOW, attr_list\n        )\n", "    def test_ensure_echo_on_enabled(self, mock_tcsetattr, mock_tcgetattr, mock_isatty, mock_termios):\n        attrs = [0, 0, 0, 0]  # Mock termios attribute list\n        mock_tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        mock_tcsetattr.assert_not_called()  # No changes if ECHO is already enabled\n", "    def test_ensure_echo_on(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        # Simulate ECHO being off\n        attrs = [0, 0, 0, 0, 0, 0]\n        attrs[3] = 0\n        mock_tcgetattr.return_value = attrs\n\n        autoreload.ensure_echo_on()\n\n        mock_isatty.assert_called_once()\n        mock_tcgetattr.assert_called_once()\n        mock_tcsetattr.assert_called_once()\n        self.assertTrue(mock_tcgetattr.return_value[3] & termios.ECHO)\n", "    def test_ensure_echo_on_enabled(self, mocked_stdin, mocked_termios):\n        mocked_stdin.isatty.return_value = True\n        attr_list = [0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attr_list\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 0)\n", "    def test_ensure_echo_on(self, mocked_stdin, mocked_termios):\n        # Mock the stdin to be a TTY.\n        mocked_stdin.isatty.return_value = True\n        attrs = [0, 0, 0, 0]\n        attrs[3] = 0  # Initial flag without ECHO.\n        mocked_termios.tcgetattr.return_value = attrs\n\n        autoreload.ensure_echo_on()\n\n        # Ensure ECHO flag is set.\n        self.assertTrue(attrs[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called_once_with(mocked_stdin, mocked_termios.TCSANOW, attrs)\n"], "sample_108": ["def test_regex_pattern_with_trailing_dollar(self):\n        pattern = RegexPattern(r'^foo/$')\n        self.assertEqual(pattern.check(), [])\n        pattern_with_dollar = RegexPattern(r'^foo$')\n        warnings = pattern_with_dollar.check()\n        self.assertEqual(len(warnings), 1)\n        self.assertEqual(warnings[0].id, 'urls.W001')\n        self.assertIn('uses include with a route ending with a \\'$\\'.', warnings[0].msg)\n", "    def setUp(self):\n        # Setting up a simple URL resolver for testing\n        self.resolver = URLResolver(\n            RoutePattern('simple/<int:id>/'),\n            'test_urls',\n            default_kwargs={'default_arg': 'default_value'},\n            app_name='test_app',\n            namespace='test_ns'\n        )\n", "    def setUp(self):\n        self.pattern = RegexPattern(r'^test/$')\n        self.urlconf_name = 'urlpatterns.path_urls'\n        self.resolver = URLResolver(self.pattern, self.urlconf_name)\n", "def test_route_to_regex_whitespace_error(self):\n        msg = \"URL route 'foo /bar/' cannot contain whitespace.\"\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            _route_to_regex('foo /bar/')\n", "    def test_reverse_with_mixed_args_and_kwargs(self):\n        # This test ensures ValueError is raised when both args and kwargs are used in reverse().\n        with self.assertRaisesMessage(ValueError, \"Don't mix *args and **kwargs in call to reverse()!\"):\n            reverse('articles-year-month-day', args=(2015,), kwargs={'month': 4, 'day': 12})\n"], "sample_117": ["    def test_bound_data(self):\n        field = ReadOnlyPasswordHashField()\n        initial = 'initial_password'\n        data = 'new_password'\n        self.assertEqual(field.bound_data(data, initial), initial)\n", "    def test_invalid_password_format(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'invalid_password_format'\n        context = widget.get_context('password', value, {'id': 'id_password'})\n        self.assertIn('Invalid password format or unknown hashing algorithm.', context['summary'][0]['label'])\n", "    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context('password', None, {})\n        self.assertIn('summary', context)\n        self.assertEqual(context['summary'], [{'label': gettext(\"No password set.\")}])\n", "    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(name='password', value=None, attrs={})\n        self.assertIn('summary', context)\n        self.assertEqual(context['summary'], [{'label': gettext(\"No password set.\")}])\n", "    def test_widget_context_no_password_set(self):\n        widget = ReadOnlyPasswordHashWidget()\n        name = 'password'\n        value = ''\n        attrs = {}\n        context = widget.get_context(name, value, attrs)\n        self.assertIn('summary', context)\n        self.assertEqual(context['summary'][0]['label'], \"No password set.\")\n"], "sample_133": ["    def test_jsi18n_with_multiple_packages(self):\n        \"\"\"\n        Test that JavaScriptCatalog can handle multiple packages.\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            response = self.client.get('/jsi18n/?packages=view_tests.app1+view_tests.app2')\n            self.assertContains(response, 'il faut traduire cette cha\\\\u00eene de caract\\\\u00e8res de app1')\n            self.assertContains(response, 'il faut traduire cette cha\\\\u00eene de caract\\\\u00e8res de app2')\n", "    def test_js_catalog_context_data(self):\n        \"\"\"Test the context data used in the JavaScript catalog.\"\"\"\n        view = JavaScriptCatalog()\n        request = RequestFactory().get('/')\n        view.request = request\n        context_data = view.get_context_data()\n        \n        # Ensure the catalog is not empty and contains expected keys\n        self.assertIn('catalog', context_data)\n        self.assertIsInstance(context_data['catalog'], dict)\n        \n        # Check formats\n        self.assertIn('formats', context_data)\n        self.assertIsInstance(context_data['formats'], dict)\n        self.assertIn('DATE_FORMAT', context_data['formats'])\n        \n        # Check plural string\n        self.assertIn('plural', context_data)\n        self.assertIsInstance(context_data['plural'], str)\n", "    def test_js_catalog_template_structure(self):\n        \"\"\"\n        Test that the JavaScript catalog template is rendered with the correct structure.\n        \"\"\"\n        with override('fr'):\n            response = self.client.get('/jsi18n/')\n            self.assertContains(response, 'var django = globals.django || (globals.django = {});')\n            self.assertContains(response, 'django.pluralidx = function(n)')\n            self.assertContains(response, 'django.catalog = django.catalog || {};')\n            self.assertContains(response, 'django.gettext = function(msgid)')\n            self.assertContains(response, 'django.ngettext = function(singular, plural, count)')\n            self.assertContains(response, 'django.gettext_noop = function(msgid)')\n            self.assertContains(response, 'django.pgettext = function(context, msgid)')\n            self.assertContains(response, 'django.npgettext = function(context, singular, plural, count)')\n            self.assertContains(response, 'django.interpolate = function(fmt, obj, named)')\n            self.assertContains(response, 'django.formats =')\n            self.assertContains(response, 'django.get_format = function(format_type)')\n            self.assertContains(response, 'globals.pluralidx = django.pluralidx;')\n            self.assertContains(response, 'globals.gettext = django.gettext;')\n            self.assertContains(response, 'globals.ngettext = django.ngettext;')\n            self.assertContains(response, 'globals.gettext_noop = django.gettext_noop;')\n            self.assertContains(response, 'globals.pgettext = django.pgettext;')\n            self.assertContains(response, 'globals.npgettext = django.npgettext;')\n            self.assertContains(response, 'globals.interpolate = django.interpolate;')\n            self.assertContains(response, 'globals.get_format = django.get_format;')\n", "    def test_js_catalog_with_specific_packages(self):\n        \"\"\"\n        Test JavaScriptCatalog with specific packages provided.\n        \"\"\"\n        with override('fr'):\n            response = self.client.get('/jsi18n/app6/')\n            self.assertContains(response, 'traduire cette cha\u00eene de caract\u00e8res de app6')\n            self.assertNotContains(response, 'Untranslated string')\n", "    def test_jsi18n_with_different_domain(self):\n        \"\"\"\n        The javascript_catalog view returns the correct translation catalog\n        when a custom domain is provided.\n        \"\"\"\n        with override('fr'):\n            response = self.client.get('/jsi18n_custom_domain/')\n            self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n            self.assertContains(response, 'var django = globals.django || (globals.django = {});')\n            self.assertContains(response, '\"this is a custom domain translation\": \"Ceci est une traduction de domaine personnalis\u00e9\"')\n"], "sample_138": ["    def test_hashed_name_with_no_extension(self):\n        \"\"\"\n        Test the hashed_name method with a file that has no extension.\n        \"\"\"\n        content = ContentFile(b\"content of the file\")\n        filename = 'file_without_extension'\n        configured_storage = storage.staticfiles_storage\n        hashed_name = configured_storage.hashed_name(filename, content)\n        self.assertRegex(hashed_name, r'file_without_extension\\.[0-9a-f]{12}')\n        \n        # Save the file and verify that the hashed file is created.\n        saved_name = configured_storage.save(hashed_name, content)\n        self.assertTrue(configured_storage.exists(saved_name))\n        \n        # Ensure that the saved file has the correct hashed name.\n        self.assertEqual(hashed_name, saved_name)\n", "    def setUp(self):\n        self.storage = storage.StaticFilesStorage()\n        self.test_file_content = b'This is a test file content for hashing.'\n        self.test_file = tempfile.NamedTemporaryFile(delete=False)\n        self.test_file.write(self.test_file_content)\n        self.test_file.close()\n", "    def test_manifest_file_creation(self):\n        \"\"\"\n        Ensure that the manifest file is created and written correctly during post-processing.\n        \"\"\"\n        # Create temporary manifest storage location\n        temp_manifest_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, temp_manifest_dir)\n        custom_storage = CustomManifestStorage(manifest_location=temp_manifest_dir)\n        \n        # Run collectstatic to trigger post-processing and manifest creation\n        with override_settings(STATICFILES_STORAGE='path.to.CustomManifestStorage'):\n            call_command('collectstatic', verbosity=0, interactive=False)\n        \n        # Check that the manifest file is created\n        manifest_file = Path(temp_manifest_dir) / custom_storage.manifest_name\n        self.assertTrue(manifest_file.exists())\n        \n        # Load and verify manifest content\n        with manifest_file.open('r') as f:\n            manifest_content = json.load(f)\n        self.assertIn('paths', manifest_content)\n        self.assertEqual(manifest_content['version'], custom_storage.manifest_version)\n        self.assertGreater(len(manifest_content['paths']), 0)\n\n        # Verify that hashed files in manifest match expected hashed names\n        for original_path, hashed_path in manifest_content['paths'].items():\n            self.assertTrue(storage.staticfiles_storage.exists(hashed_path))\n", "    def setUp(self):\n        self.mixin = HashedFilesMixin()\n", "    def test_file_hash(self):\n        # Test the file_hash method of HashedFilesMixin\n        test_content = ContentFile(b\"test content\")\n        hashed_files_mixin = storage.staticfiles_storage\n        file_hash = hashed_files_mixin.file_hash(\"test.txt\", test_content)\n        expected_hash = hashlib.md5(b\"test content\").hexdigest()[:12]\n        self.assertEqual(file_hash, expected_hash)\n"], "sample_651": ["def test_warns_re_emit_non_matching_warning() -> None:\n    \"\"\"Test that a non-matching warning is re-emitted after context exits.\"\"\"\n    with pytest.warns(UserWarning, match=\"specific warning\"):\n        with pytest.warns(UserWarning):\n            warnings.warn(\"specific warning\", UserWarning)\n            warnings.warn(\"another warning\", UserWarning)\n", "def test_recwarn_no_warnings(recwarn: WarningsRecorder) -> None:\n    \"\"\"Ensure that recwarn captures no warnings when none are emitted.\"\"\"\n    assert len(recwarn) == 0\n    assert recwarn.list == []\n", "def test_warns_pop_exact_match() -> None:\n    \"\"\"Test that pytest.warns pop method correctly identifies and pops the exact warning category.\"\"\"\n    with pytest.warns(UserWarning) as record:\n        warnings.warn(\"user warning\", UserWarning)\n        warnings.warn(\"another user warning\", UserWarning)\n        warnings.warn(\"runtime warning\", RuntimeWarning)\n    \n    assert len(record) == 3\n    popped_warn = record.pop(UserWarning)\n    assert popped_warn.category is UserWarning\n    assert str(popped_warn.message) == \"user warning\"\n    assert len(record) == 2\n", "def test_recwarn_clear(recwarn: WarningsRecorder) -> None:\n    \"\"\"Test that recwarn.clear() properly clears the recorded warnings list.\"\"\"\n    warnings.warn(\"warning 1\", UserWarning)\n    warnings.warn(\"warning 2\", DeprecationWarning)\n    assert len(recwarn) == 2\n    recwarn.clear()\n    assert len(recwarn) == 0\n", "def test_warns_catches_multiple_warnings_of_different_types() -> None:\n    with pytest.warns((UserWarning, DeprecationWarning)) as record:\n        warnings.warn(\"user warning\", UserWarning)\n        warnings.warn(\"deprecation warning\", DeprecationWarning)\n    assert len(record) == 2\n    assert str(record[0].message) == \"user warning\"\n    assert str(record[1].message) == \"deprecation warning\"\n"], "sample_653": ["def test_log_file_custom_formatter(testdir):\n    log_file = testdir.tmpdir.join(\"pytest_custom.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level=INFO\n        log_file_format=%(asctime)s %(levelname)-8s [%(name)s] %(message)s\n        log_file_date_format=%Y-%m-%d %H:%M:%S\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.getLogger('catchlog').info(\"This is an info message\")\n            logging.getLogger('catchlog').error(\"This is an error message\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines([\"test_log_file_custom_formatter.py PASSED\"])\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"This is an info message\" in contents\n        assert \"This is an error message\" in contents\n        assert re.search(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\", contents)\n", "def test_log_level_error(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.error('error message from logger')\n            assert False\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=ERROR\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured *log call -*\",\n            \"*error message from logger*\"\n        ]\n    )\n    assert \"INFO\" not in result.stdout.str()\n    assert \"WARNING\" not in result.stdout.str()\n", "def test_set_log_path_creates_directory(testdir):\n    import logging\n    import os\n    from _pytest.logging import LoggingPlugin\n\n    log_dir = testdir.tmpdir.join(\"logs\")\n    log_file = log_dir.join(\"test_log.log\")\n\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n        from _pytest.logging import LoggingPlugin\n\n        @pytest.hookimpl(tryfirst=True)\n            plugin = LoggingPlugin(config)\n            plugin.set_log_path('logs/test_log.log')\n            config.pluginmanager.register(plugin, \"logging-plugin\")\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logger = logging.getLogger(__name__)\n            logger.warning(\"This is a warning message\")\n            assert True\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n    assert os.path.isfile(log_file.strpath)\n\n    with open(log_file.strpath) as rfh:\n        contents = rfh.read()\n        assert \"This is a warning message\" in contents\n", "def test_log_capture_fixture_levels(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n        @pytest.fixture\n            caplog.set_level(logging.DEBUG)\n            yield\n            caplog.set_level(logging.WARNING)\n\n            logger.debug('This is a debug message')\n            logger.info('This is an info message')\n            logger.warning('This is a warning message')\n            logger.error('This is an error message')\n            assert True\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=DEBUG\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\n        \"*- Captured *log call -*\",\n        \"*DEBUG*This is a debug message*\",\n        \"*INFO*This is an info message*\",\n        \"*WARNING*This is a warning message*\",\n        \"*ERROR*This is an error message*\",\n    ])\n", "def test_log_formatting_colors(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging.DEBUG)\n\n            logger.debug('debug message')\n            logger.info('info message')\n            logger.warning('warning message')\n            logger.error('error message')\n            logger.critical('critical message')\n            assert True\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=DEBUG\", \"--log-format=%(levelname)s:%(message)s\", \"--color=yes\")\n    assert result.ret == 0\n\n    result.stdout.fnmatch_lines([\n        \"*-- live log call --*\",\n        \"*DEBUG:debug message*\",\n        \"*INFO:info message*\",\n        \"*WARNING:warning message*\",\n        \"*ERROR:error message*\",\n        \"*CRITICAL:critical message*\"\n    ])\n"], "sample_654": ["def test_fixture_request_properties(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return request.scope, request.fixturename, request.function.__name__\n\n            assert setup1 == (\"function\", \"setup1\", \"test_func\")\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_fixture_with_teardown_exception(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        values = []\n\n        @pytest.fixture\n            values.append(\"setup\")\n            yield\n            values.append(\"teardown\")\n            raise RuntimeError(\"Error in teardown\")\n\n            assert values == [\"setup\"]\n        \n            assert \"teardown\" in values\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-s\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*setup*\",\n            \"*teardown*\",\n            \"*RuntimeError: Error in teardown*\",\n            \"*1 passed*1 error*\"\n        ]\n    )\n", "def test_fixture_def_initialization():\n    fixture_manager = fixtures.FixtureManager(None)\n    fixture_def = fixtures.FixtureDef(\n        fixture_manager,\n        baseid=\"some_baseid\",\n        argname=\"some_arg\",\n        func=lambda: None,\n        scope=\"function\",\n        params=[1, 2, 3],\n        unittest=False,\n        ids=[\"one\", \"two\", \"three\"]\n    )\n    assert fixture_def._fixturemanager == fixture_manager\n    assert fixture_def.baseid == \"some_baseid\"\n    assert fixture_def.argname == \"some_arg\"\n    assert fixture_def.scope == \"function\"\n    assert fixture_def.params == [1, 2, 3]\n    assert fixture_def.ids == (\"one\", \"two\", \"three\")\n", "def test_scope_mismatch_with_complex_dependency(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope=\"function\")\n            return \"function_fixture\"\n\n        @pytest.fixture(scope=\"module\")\n            return f\"module_fixture_depends_on_{func_fix}\"\n\n        @pytest.fixture(scope=\"class\")\n            return f\"class_fixture_depends_on_{module_fix}\"\n\n            assert class_fix == \"class_fixture_depends_on_module_fixture_depends_on_function_fixture\"\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\n        \"*ScopeMismatch*\",\n        \"*ScopeMismatch: You tried to access the 'function' scoped fixture 'func_fix' with a 'class' scoped request object*\"\n    ])\n", "def test_request_parametrized_fixture_raising_error(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(params=[1, 2])\n            if request.param == 2:\n                raise ValueError(\"Invalid param value\")\n            return request.param\n\n            assert fix == 1\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_request_parametrized_fixture_raising_error.py::test_fix[1] PASSED*\",\n            \"*test_request_parametrized_fixture_raising_error.py::test_fix[2] ERROR*\",\n            \"*ValueError: Invalid param value*\",\n            \"*1 passed, 1 error in*\",\n        ]\n    )\n"]}, "raw_scored_generations": {"sample_1": [false, false, false, false, true], "sample_4": [false, true, false, false, false], "sample_3": [true, true, true, false, true], "sample_0": [false, false, false, false, false], "sample_2": [false, true, true, true, true], "sample_5": [false, false, false, false, false], "sample_6": [false, true, false, false, false], "sample_7": [false, false, false, false, false], "sample_8": [false, false, false, true, false], "sample_9": [false, true, true, true, true], "sample_10": [false, false, true, true, true], "sample_11": [false, true, true, false, true], "sample_12": [false, false, false, false, false], "sample_13": [true, false, false, false, false], "sample_14": [true, true, false, false, false], "sample_15": [true, false, true, false, true], "sample_16": [false, false, false, true, true], "sample_17": [false, true, false, true, false], "sample_18": [false, true, true, false, false], "sample_19": [true, false, true, true, false], "sample_20": [false, true, true, false, false], "sample_21": [true, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_23": [false, false, true, true, false], "sample_24": [false, false, false, false, false], "sample_25": [false, false, false, true, false], "sample_26": [true, true, false, true, true], "sample_27": [false, true, false, false, true], "sample_28": [false, false, false, true, true], "sample_29": [false, false, false, false, false], "sample_30": [true, false, false, false, false], "sample_31": [false, false, false, false, false], "sample_32": [false, false, false, false, false], "sample_33": [true, true, true, true, true], "sample_34": [false, false, false, true, false], "sample_35": [true, true, true, false, true], "sample_36": [false, false, true, true, false], "sample_37": [false, false, true, false, false], "sample_38": [false, false, false, true, true], "sample_39": [false, false, false, false, false], "sample_40": [true, false, true, false, false], "sample_41": [false, true, false, false, false], "sample_42": [false, false, false, false, false], "sample_43": [true, true, true, false, true], "sample_44": [false, false, false, false, false], "sample_45": [false, true, false, false, false], "sample_46": [false, false, false, false, false], "sample_47": [false, false, false, false, false], "sample_48": [false, true, false, false, false], "sample_49": [true, false, false, false, false], "sample_50": [true, true, true, true, true], "sample_51": [false, true, true, true, false], "sample_52": [false, true, true, false, true], "sample_54": [true, true, true, true, true], "sample_53": [false, false, true, false, true], "sample_55": [false, false, false, false, false], "sample_58": [true, true, true, true, true], "sample_56": [true, false, true, true, true], "sample_57": [true, true, true, true, true], "sample_59": [true, true, true, false, false], "sample_60": [false, false, false, false, false], "sample_61": [true, true, true, true, true], "sample_62": [true, true, false, false, false], "sample_63": [false, false, false, false, false], "sample_64": [false, false, false, false, false], "sample_65": [false, true, true, true, true], "sample_67": [true, false, false, false, false], "sample_66": [false, false, false, false, false], "sample_68": [true, false, false, true, true], "sample_69": [false, false, false, false, false], "sample_70": [false, false, false, true, false], "sample_71": [false, true, false, false, false], "sample_72": [false, false, false, false, false], "sample_73": [false, true, false, false, false], "sample_75": [false, false, false, false, false], "sample_74": [true, true, true, true, true], "sample_76": [true, true, true, true, true], "sample_77": [false, false, false, false, false], "sample_78": [true, false, false, false, false], "sample_79": [false, false, false, false, false], "sample_80": [false, false, false, true, false], "sample_82": [false, false, false, true, true], "sample_81": [false, false, false, false, false], "sample_83": [false, false, false, true, false], "sample_85": [false, true, true, true, true], "sample_84": [false, false, false, false, false], "sample_86": [false, true, false, false, true], "sample_88": [false, false, false, false, false], "sample_87": [false, false, false, false, false], "sample_89": [false, false, false, false, false], "sample_90": [false, false, true, false, false], "sample_91": [false, false, true, false, false], "sample_92": [false, false, false, false, false], "sample_93": [false, false, true, false, false], "sample_94": [true, false, false, false, false], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, false], "sample_96": [true, true, true, true, true], "sample_99": [false, true, false, false, false], "sample_97": [false, false, false, false, false], "sample_100": [false, false, false, false, false], "sample_102": [true, true, true, false, true], "sample_101": [false, false, false, false, false], "sample_103": [false, false, false, false, false], "sample_104": [false, false, false, false, false], "sample_107": [false, true, true, false, false], "sample_106": [true, false, true, false, true], "sample_105": [false, false, false, false, false], "sample_108": [true, false, false, true, true], "sample_109": [false, true, false, true, true], "sample_111": [true, true, true, true, true], "sample_110": [false, false, false, false, false], "sample_112": [false, false, false, false, false], "sample_113": [false, true, true, true, true], "sample_114": [true, false, true, true, false], "sample_115": [false, false, false, false, false], "sample_116": [true, false, false, false, false], "sample_117": [true, true, false, false, true], "sample_118": [false, true, true, true, false], "sample_119": [false, false, false, true, true], "sample_120": [false, false, false, false, false], "sample_121": [true, true, false, false, true], "sample_122": [true, true, true, true, false], "sample_123": [true, false, false, false, true], "sample_124": [true, true, true, true, false], "sample_125": [true, true, false, true, true], "sample_126": [true, false, true, true, false], "sample_127": [false, false, false, false, false], "sample_128": [true, true, true, true, true], "sample_129": [false, true, true, true, false], "sample_130": [true, true, true, false, true], "sample_131": [true, false, false, false, true], "sample_132": [false, false, false, false, false], "sample_133": [true, true, true, true, true], "sample_135": [true, true, false, true, false], "sample_134": [false, false, false, false, false], "sample_136": [false, false, false, false, false], "sample_139": [true, true, false, false, false], "sample_137": [false, false, false, false, false], "sample_138": [false, false, false, false, false], "sample_140": [false, false, false, false, false], "sample_141": [true, false, false, false, false], "sample_142": [false, false, false, false, false], "sample_143": [true, true, true, true, true], "sample_144": [false, true, true, true, false], "sample_145": [true, true, true, true, true], "sample_146": [true, true, true, true, true], "sample_147": [true, true, true, false, false], "sample_148": [false, false, false, false, false], "sample_151": [true, false, true, true, true], "sample_149": [true, true, true, true, true], "sample_152": [false, false, false, false, false], "sample_150": [false, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [false, false, false, false, false], "sample_155": [true, true, true, true, true], "sample_156": [false, false, false, false, false], "sample_157": [false, false, false, false, false], "sample_158": [false, false, true, false, true], "sample_159": [false, true, false, true, false], "sample_160": [false, false, false, false, false], "sample_161": [false, false, true, false, false], "sample_162": [false, true, true, true, true], "sample_163": [false, false, false, false, true], "sample_164": [false, false, false, false, false], "sample_165": [false, false, false, false, false], "sample_166": [false, true, true, true, true], "sample_168": [false, false, false, false, false], "sample_167": [false, false, false, false, false], "sample_169": [false, false, false, false, false], "sample_171": [false, false, false, false, false], "sample_170": [false, false, false, false, false], "sample_172": [true, false, false, false, true], "sample_173": [false, false, false, false, false], "sample_174": [false, false, false, false, false], "sample_175": [false, true, false, true, true], "sample_176": [false, true, false, false, false], "sample_177": [false, true, false, true, false], "sample_178": [true, false, false, false, true], "sample_180": [false, false, false, false, false], "sample_179": [false, true, true, true, true], "sample_182": [false, false, false, true, false], "sample_181": [true, true, true, true, false], "sample_183": [false, false, false, false, false], "sample_184": [false, true, true, false, false], "sample_185": [false, false, false, true, false], "sample_186": [true, true, false, true, true], "sample_187": [true, false, true, false, true], "sample_188": [false, false, false, false, true], "sample_189": [false, false, false, false, false], "sample_190": [false, false, false, false, true], "sample_191": [false, false, false, false, true], "sample_192": [false, false, false, false, true], "sample_193": [false, false, false, false, false], "sample_194": [true, true, true, false, true], "sample_195": [false, false, false, false, false], "sample_196": [false, false, false, false, false], "sample_198": [false, false, false, false, false], "sample_197": [false, true, false, true, false], "sample_199": [false, false, true, true, true], "sample_200": [false, true, false, false, true], "sample_201": [false, false, false, false, false], "sample_202": [true, true, true, false, false], "sample_203": [true, true, true, false, false], "sample_204": [false, false, false, false, false], "sample_205": [true, true, false, true, false], "sample_206": [false, true, true, false, false], "sample_207": [false, false, false, false, false], "sample_208": [false, false, false, false, false], "sample_209": [true, false, false, true, true], "sample_210": [false, false, false, false, false], "sample_211": [false, true, true, false, true], "sample_213": [false, false, false, false, false], "sample_214": [false, false, false, true, false], "sample_212": [false, false, false, false, false], "sample_215": [false, false, false, true, true], "sample_216": [false, false, true, false, true], "sample_217": [false, false, true, false, true], "sample_218": [false, true, false, true, true], "sample_219": [false, false, true, false, false], "sample_220": [false, true, true, true, true], "sample_221": [false, false, false, true, true], "sample_222": [true, true, true, true, true], "sample_223": [true, true, false, true, false], "sample_224": [true, true, true, true, true], "sample_225": [false, false, false, false, false], "sample_226": [false, false, false, false, false], "sample_227": [false, true, false, false, true], "sample_228": [false, false, true, false, true], "sample_229": [true, true, true, true, true], "sample_230": [false, true, true, false, false], "sample_231": [false, false, false, false, false], "sample_232": [true, false, false, false, false], "sample_233": [true, true, true, true, true], "sample_234": [false, true, false, true, true], "sample_235": [true, true, false, true, true], "sample_236": [true, true, false, false, false], "sample_237": [true, true, false, true, false], "sample_238": [false, false, false, false, false], "sample_239": [false, true, true, false, true], "sample_240": [true, true, true, false, true], "sample_241": [false, false, false, false, false], "sample_242": [true, true, false, false, false], "sample_243": [true, false, false, false, false], "sample_244": [true, false, true, false, true], "sample_245": [true, true, false, true, true], "sample_246": [true, true, true, true, true], "sample_247": [true, false, true, false, false], "sample_248": [false, false, false, false, false], "sample_249": [false, false, true, false, true], "sample_250": [true, true, true, true, false], "sample_251": [true, true, false, true, true], "sample_252": [true, false, false, true, false], "sample_253": [false, false, false, false, false], "sample_254": [true, false, true, false, true], "sample_256": [true, false, true, false, false], "sample_255": [false, false, false, false, false], "sample_257": [false, false, false, false, true], "sample_258": [false, false, false, false, false], "sample_259": [true, true, true, true, false], "sample_260": [false, false, true, false, false], "sample_261": [true, true, true, false, false], "sample_262": [false, false, false, false, false], "sample_263": [false, true, true, false, true], "sample_264": [false, true, true, false, false], "sample_265": [false, false, false, false, false], "sample_266": [false, false, false, false, false], "sample_267": [false, false, true, false, true], "sample_268": [false, true, false, false, false], "sample_269": [false, true, true, true, true], "sample_270": [false, false, false, false, false], "sample_271": [false, false, false, false, false], "sample_272": [false, false, false, false, false], "sample_273": [false, false, false, false, false], "sample_274": [false, true, false, false, false], "sample_275": [false, false, false, true, false], "sample_276": [false, true, true, true, true], "sample_277": [false, false, false, false, false], "sample_278": [false, false, false, false, false], "sample_279": [true, true, true, true, false], "sample_280": [false, false, true, false, false], "sample_281": [true, true, true, true, true], "sample_282": [true, true, true, true, true], "sample_283": [true, true, true, true, true], "sample_284": [false, false, false, false, false], "sample_285": [false, true, true, true, false], "sample_286": [false, true, false, true, false], "sample_287": [true, true, true, true, false], "sample_288": [false, false, false, true, false], "sample_289": [false, true, true, true, true], "sample_290": [true, false, false, true, true], "sample_291": [true, false, true, true, false], "sample_292": [false, true, false, false, true], "sample_293": [true, false, true, false, false], "sample_294": [true, false, false, true, true], "sample_295": [true, false, false, false, false], "sample_296": [true, true, false, true, false], "sample_297": [false, false, false, false, false], "sample_298": [true, true, true, true, true], "sample_299": [true, true, true, true, true], "sample_300": [true, true, true, true, true], "sample_301": [true, false, false, false, false], "sample_302": [false, false, false, false, false], "sample_303": [false, false, false, false, false], "sample_304": [true, true, false, true, true], "sample_305": [false, false, false, false, false], "sample_306": [true, true, true, false, false], "sample_307": [true, true, true, true, false], "sample_308": [true, true, true, true, false], "sample_309": [false, false, false, false, false], "sample_310": [true, true, true, true, false], "sample_312": [true, true, true, true, true], "sample_311": [false, false, false, false, false], "sample_313": [false, false, false, false, false], "sample_314": [true, false, false, false, false], "sample_315": [true, true, true, false, true], "sample_316": [false, false, true, false, true], "sample_317": [true, true, false, true, true], "sample_318": [true, false, false, true, false], "sample_319": [false, false, false, true, false], "sample_320": [false, false, true, true, false], "sample_321": [false, false, false, true, false], "sample_322": [false, false, false, false, false], "sample_323": [false, false, false, false, false], "sample_324": [true, false, false, true, true], "sample_325": [false, true, true, false, true], "sample_326": [false, false, false, false, false], "sample_327": [true, true, false, false, true], "sample_328": [false, true, false, false, true], "sample_329": [false, false, false, false, false], "sample_330": [false, false, false, false, false], "sample_331": [false, false, false, false, true], "sample_332": [false, false, false, false, false], "sample_333": [true, true, false, true, true], "sample_334": [true, true, false, true, false], "sample_335": [false, false, false, false, true], "sample_336": [false, false, false, true, false], "sample_337": [false, false, true, true, true], "sample_338": [false, true, false, false, false], "sample_339": [false, false, true, false, false], "sample_340": [false, false, false, false, false], "sample_341": [true, true, false, false, false], "sample_342": [true, true, true, true, true], "sample_343": [false, true, false, true, false], "sample_344": [true, false, true, false, true], "sample_345": [false, false, false, false, false], "sample_346": [false, false, false, false, false], "sample_347": [true, true, true, true, true], "sample_348": [false, false, false, false, false], "sample_349": [false, false, false, true, true], "sample_350": [true, true, true, false, false], "sample_351": [true, false, true, true, true], "sample_352": [false, true, false, false, false], "sample_353": [true, false, false, true, false], "sample_354": [false, false, false, false, false], "sample_355": [true, false, true, false, true], "sample_356": [false, false, false, false, true], "sample_357": [false, true, false, false, false], "sample_358": [false, false, false, false, false], "sample_359": [false, false, true, false, false], "sample_360": [false, false, false, false, false], "sample_361": [false, false, false, false, false], "sample_362": [false, true, false, false, false], "sample_363": [true, true, true, true, true], "sample_364": [false, false, false, true, false], "sample_365": [false, false, false, false, false], "sample_366": [true, false, false, true, true], "sample_367": [false, false, false, false, false], "sample_368": [false, false, false, false, false], "sample_369": [false, false, false, false, false], "sample_370": [false, false, true, false, false], "sample_371": [false, false, false, false, false], "sample_372": [false, false, false, true, true], "sample_373": [false, false, false, false, false], "sample_374": [false, false, true, false, false], "sample_375": [false, false, true, true, true], "sample_376": [true, true, false, false, true], "sample_377": [false, false, false, false, false], "sample_378": [true, false, true, true, false], "sample_379": [true, true, true, false, true], "sample_380": [false, false, true, true, false], "sample_381": [false, false, true, false, false], "sample_382": [false, false, false, false, false], "sample_383": [true, false, false, false, false], "sample_384": [true, false, false, false, false], "sample_385": [false, false, false, true, false], "sample_386": [true, false, true, true, true], "sample_387": [true, false, true, true, true], "sample_388": [true, false, false, true, false], "sample_389": [true, true, true, true, true], "sample_390": [false, false, false, false, false], "sample_391": [false, false, false, false, true], "sample_392": [false, false, false, false, false], "sample_393": [false, true, true, false, true], "sample_394": [false, false, false, false, false], "sample_395": [false, false, false, false, false], "sample_396": [true, false, false, false, false], "sample_397": [false, false, false, false, false], "sample_398": [false, true, false, false, false], "sample_399": [true, false, false, false, true], "sample_400": [false, false, true, false, true], "sample_401": [true, false, true, false, false], "sample_402": [false, true, true, false, false], "sample_403": [false, false, false, false, true], "sample_404": [true, true, true, true, true], "sample_405": [false, false, true, false, false], "sample_406": [false, false, false, true, false], "sample_407": [false, false, true, false, true], "sample_408": [false, false, false, false, false], "sample_409": [false, true, false, true, true], "sample_410": [false, false, false, false, false], "sample_411": [true, true, true, true, true], "sample_412": [false, false, false, false, false], "sample_413": [true, true, true, true, true], "sample_414": [false, true, false, true, true], "sample_415": [false, false, true, false, true], "sample_416": [true, true, true, true, true], "sample_417": [true, false, true, false, true], "sample_418": [true, false, true, true, false], "sample_419": [false, false, true, true, false], "sample_420": [true, false, true, false, false], "sample_421": [false, false, false, false, false], "sample_422": [false, true, false, true, false], "sample_423": [false, false, false, false, false], "sample_424": [false, false, true, false, true], "sample_425": [false, false, true, false, false], "sample_426": [true, true, true, true, false], "sample_427": [false, true, true, false, false], "sample_428": [false, false, false, false, false], "sample_429": [true, false, false, true, true], "sample_430": [false, false, false, false, true], "sample_431": [true, true, false, false, false], "sample_432": [true, true, true, true, true], "sample_433": [true, true, true, true, true], "sample_434": [false, false, false, false, false], "sample_435": [false, false, false, false, false], "sample_436": [false, false, true, false, false], "sample_437": [false, false, false, false, false], "sample_438": [true, true, false, false, false], "sample_439": [true, true, true, true, true], "sample_440": [false, false, false, false, true], "sample_441": [false, true, false, false, false], "sample_442": [true, false, true, true, true], "sample_443": [true, false, false, false, false], "sample_444": [false, false, false, false, false], "sample_445": [true, false, true, false, true], "sample_446": [false, true, true, false, true], "sample_447": [false, false, true, true, true], "sample_448": [true, false, true, true, false], "sample_449": [false, false, false, false, false], "sample_450": [false, false, false, false, false], "sample_451": [false, false, true, false, false], "sample_453": [false, false, false, false, false], "sample_452": [false, false, false, false, false], "sample_454": [false, false, false, false, false], "sample_455": [false, false, true, true, false], "sample_456": [true, false, false, true, true], "sample_457": [false, true, true, true, true], "sample_458": [false, false, false, true, false], "sample_459": [false, false, false, false, false], "sample_460": [false, false, false, false, false], "sample_461": [true, false, false, false, true], "sample_462": [true, true, false, true, true], "sample_463": [false, false, false, true, false], "sample_464": [true, false, true, false, false], "sample_465": [true, true, false, false, false], "sample_466": [false, false, true, false, false], "sample_467": [true, false, false, false, false], "sample_469": [true, false, false, false, false], "sample_468": [true, true, true, true, false], "sample_470": [false, false, false, false, false], "sample_471": [false, false, true, false, false], "sample_472": [false, false, true, false, false], "sample_473": [false, false, false, false, false], "sample_474": [false, false, false, false, false], "sample_475": [true, true, true, true, true], "sample_476": [false, false, false, true, false], "sample_477": [false, false, false, false, false], "sample_478": [true, true, true, true, true], "sample_479": [false, false, false, false, false], "sample_480": [false, false, false, false, false], "sample_481": [false, true, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [true, true, true, true, false], "sample_484": [false, false, false, false, false], "sample_485": [false, false, false, false, false], "sample_486": [false, false, false, false, false], "sample_487": [true, true, true, true, true], "sample_488": [false, false, false, true, false], "sample_489": [true, false, true, false, false], "sample_490": [true, false, false, true, true], "sample_491": [false, false, true, true, false], "sample_492": [false, false, false, true, false], "sample_493": [false, false, false, false, false], "sample_494": [false, false, false, true, false], "sample_495": [false, true, true, true, false], "sample_496": [false, false, true, false, false], "sample_497": [false, true, false, false, false], "sample_498": [false, false, false, false, false], "sample_499": [true, false, false, false, false], "sample_500": [false, true, false, true, true], "sample_501": [false, false, true, false, false], "sample_502": [true, false, false, true, true], "sample_503": [false, false, true, false, true], "sample_504": [true, true, true, true, false], "sample_505": [true, false, true, false, false], "sample_506": [false, false, false, true, false], "sample_507": [true, true, true, true, false], "sample_508": [true, true, false, false, false], "sample_509": [false, true, true, false, false], "sample_510": [true, true, false, false, true], "sample_511": [false, false, false, false, false], "sample_512": [true, true, true, false, true], "sample_513": [true, false, false, false, false], "sample_514": [true, false, true, false, true], "sample_515": [true, false, true, false, false], "sample_516": [false, false, true, false, false], "sample_517": [false, true, true, true, true], "sample_518": [true, false, false, false, false], "sample_519": [true, true, false, false, true], "sample_520": [false, true, true, false, true], "sample_521": [true, true, true, true, false], "sample_522": [false, true, false, false, true], "sample_523": [false, false, true, false, true], "sample_524": [false, false, false, false, false], "sample_525": [false, true, true, false, false], "sample_526": [true, true, true, false, true], "sample_527": [false, false, false, true, false], "sample_528": [false, false, false, false, false], "sample_529": [false, false, false, false, false], "sample_530": [true, false, false, false, true], "sample_531": [false, false, true, false, false], "sample_532": [false, false, false, false, true], "sample_533": [false, false, false, false, true], "sample_534": [false, false, false, false, false], "sample_535": [false, true, true, true, false], "sample_536": [false, false, true, false, true], "sample_537": [true, true, false, true, true], "sample_538": [true, true, false, false, false], "sample_539": [false, true, true, false, false], "sample_540": [false, false, false, false, false], "sample_541": [false, false, true, true, true], "sample_542": [false, true, true, true, true], "sample_543": [true, false, false, true, false], "sample_544": [true, true, true, false, true], "sample_545": [true, false, false, true, true], "sample_546": [true, true, true, true, false], "sample_547": [false, false, false, false, false], "sample_548": [false, false, false, false, false], "sample_549": [false, false, true, false, true], "sample_550": [true, false, false, false, false], "sample_551": [true, true, true, true, true], "sample_552": [false, false, false, true, false], "sample_553": [false, false, false, false, false], "sample_554": [true, false, true, false, false], "sample_555": [false, false, false, false, false], "sample_556": [false, false, true, false, true], "sample_557": [false, true, false, true, false], "sample_558": [false, true, true, true, true], "sample_559": [false, true, false, false, false], "sample_560": [false, false, false, false, true], "sample_561": [false, true, true, false, true], "sample_562": [false, false, true, true, false], "sample_563": [false, true, true, false, false], "sample_564": [false, false, false, false, false], "sample_565": [false, false, false, false, false], "sample_566": [false, false, true, true, true], "sample_567": [false, false, false, false, true], "sample_568": [true, true, false, true, true], "sample_569": [false, false, false, false, false], "sample_570": [true, false, false, false, false], "sample_571": [true, true, false, false, true], "sample_572": [false, false, false, false, false], "sample_573": [false, true, false, true, false], "sample_574": [false, true, false, false, false], "sample_575": [false, false, false, false, false], "sample_576": [false, false, false, false, false], "sample_577": [false, false, true, false, false], "sample_578": [false, false, false, true, false], "sample_579": [false, false, false, false, false], "sample_580": [false, true, true, true, true], "sample_581": [true, false, false, false, false], "sample_582": [false, false, false, false, false], "sample_583": [true, false, true, true, false], "sample_584": [false, false, false, false, true], "sample_585": [false, true, false, false, false], "sample_586": [false, false, false, true, true], "sample_587": [true, true, false, false, true], "sample_588": [true, false, true, true, true], "sample_589": [false, true, false, false, false], "sample_590": [false, false, false, false, false], "sample_591": [false, true, false, false, false], "sample_592": [true, false, true, true, true], "sample_593": [true, true, true, false, true], "sample_594": [false, true, false, false, true], "sample_595": [true, true, false, false, true], "sample_596": [false, false, false, true, true], "sample_597": [false, true, false, false, false], "sample_598": [true, false, false, true, true], "sample_599": [false, false, true, false, false], "sample_600": [true, false, true, true, true], "sample_601": [false, false, false, false, false], "sample_602": [false, false, false, true, false], "sample_603": [true, false, true, true, true], "sample_604": [false, false, true, false, true], "sample_605": [false, true, false, true, false], "sample_606": [false, false, false, false, false], "sample_607": [false, false, true, true, true], "sample_608": [true, false, true, true, true], "sample_609": [false, false, false, false, false], "sample_610": [false, false, false, false, false], "sample_611": [false, false, false, false, false], "sample_612": [false, true, false, false, false], "sample_613": [true, false, false, true, false], "sample_614": [true, true, true, true, true], "sample_615": [false, false, false, false, false], "sample_616": [false, false, false, false, false], "sample_617": [false, false, false, false, false], "sample_618": [false, false, false, true, false], "sample_619": [false, false, true, true, true], "sample_620": [false, false, false, false, true], "sample_621": [false, true, false, false, false], "sample_622": [true, true, true, true, true], "sample_623": [true, false, false, false, false], "sample_624": [true, true, false, true, false], "sample_625": [false, false, false, false, false], "sample_626": [false, false, false, false, false], "sample_627": [false, true, true, true, false], "sample_628": [true, true, true, true, true], "sample_629": [false, false, false, false, false], "sample_630": [false, false, false, true, true], "sample_631": [false, false, false, false, false], "sample_632": [false, false, false, false, false], "sample_633": [false, false, false, false, false], "sample_634": [false, true, true, false, true], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [true, false, false, false, true], "sample_638": [false, false, false, false, false], "sample_639": [false, false, false, false, true], "sample_640": [false, false, false, false, false], "sample_641": [false, false, false, false, false], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [false, false, false, false, false], "sample_646": [false, false, false, false, false], "sample_647": [false, false, false, true, true], "sample_648": [false, true, false, true, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, true, false, true, true], "sample_652": [false, false, false, false, false], "sample_653": [false, false, false, false, false], "sample_654": [false, false, false, false, false], "sample_655": [false, true, false, true, false], "sample_656": [false, false, true, false, true], "sample_657": [false, false, false, false, false], "sample_658": [true, false, true, true, false], "sample_659": [false, false, false, false, false], "sample_660": [false, false, true, false, false], "sample_661": [false, false, true, false, false], "sample_662": [false, true, false, false, false], "sample_663": [true, false, false, false, false], "sample_664": [false, false, false, false, false], "sample_665": [false, false, true, true, false], "sample_666": [false, false, false, false, false], "sample_667": [true, false, true, false, false], "sample_668": [false, false, false, false, false], "sample_669": [true, false, true, true, false], "sample_670": [false, false, true, true, true], "sample_671": [false, false, false, false, false], "sample_672": [false, false, false, false, false], "sample_673": [false, false, false, false, false], "sample_674": [false, true, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, false, false, false], "sample_677": [false, true, false, true, true], "sample_678": [true, false, false, false, false], "sample_679": [false, false, false, false, false], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, false], "sample_682": [false, false, false, false, false], "sample_683": [false, false, false, true, true], "sample_684": [false, false, false, false, false], "sample_685": [false, false, false, true, false], "sample_686": [false, false, false, false, false], "sample_687": [true, false, true, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [true, false, false, false, true], "sample_693": [false, false, false, false, false], "sample_694": [false, false, false, false, false], "sample_695": [false, false, false, true, false], "sample_696": [false, false, false, false, false], "sample_697": [false, true, true, false, true], "sample_698": [false, false, false, false, false], "sample_699": [false, true, false, true, true], "sample_700": [false, false, false, false, false], "sample_701": [false, false, false, false, false], "sample_702": [false, false, false, false, true], "sample_703": [true, true, true, true, true], "sample_704": [true, false, false, false, false], "sample_705": [false, false, false, false, false], "sample_706": [true, true, true, true, true], "sample_707": [false, false, false, false, false], "sample_708": [false, false, false, false, false], "sample_709": [false, false, false, false, false], "sample_710": [false, false, false, false, false], "sample_711": [false, false, false, false, false], "sample_712": [false, true, false, false, false], "sample_713": [true, false, true, false, false], "sample_714": [false, false, false, false, false], "sample_715": [false, true, false, false, true], "sample_716": [false, true, true, false, false], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [false, false, false, true, false], "sample_720": [false, true, true, true, true], "sample_721": [false, false, true, false, false], "sample_722": [false, false, true, true, false], "sample_723": [true, false, true, true, true], "sample_724": [true, true, true, true, false], "sample_725": [false, false, false, false, false], "sample_726": [true, true, false, false, true], "sample_727": [true, true, true, false, true], "sample_728": [true, false, false, true, false], "sample_729": [false, false, false, false, false], "sample_730": [false, false, false, true, false], "sample_731": [true, false, true, true, true], "sample_732": [true, true, true, true, true], "sample_733": [false, true, false, false, false], "sample_734": [false, true, true, true, false], "sample_735": [false, true, false, false, true], "sample_736": [false, false, true, true, false], "sample_737": [false, false, false, false, false], "sample_738": [false, false, true, false, false], "sample_739": [true, true, true, false, true], "sample_740": [false, false, false, false, false], "sample_741": [false, true, true, false, true], "sample_742": [false, false, false, false, true], "sample_743": [false, false, false, false, false], "sample_744": [false, false, false, false, true], "sample_745": [true, false, false, true, true], "sample_746": [false, true, false, false, false], "sample_747": [false, false, false, false, true], "sample_748": [true, false, true, true, false], "sample_749": [false, true, false, false, false], "sample_750": [true, true, true, false, false], "sample_751": [false, false, false, false, false], "sample_752": [true, false, false, true, false], "sample_753": [false, false, false, false, true], "sample_754": [false, false, false, false, false], "sample_755": [false, true, false, false, true], "sample_756": [false, false, false, false, false], "sample_757": [false, true, false, false, true], "sample_758": [true, true, true, false, false], "sample_759": [true, false, true, false, false], "sample_760": [true, true, false, false, false], "sample_761": [false, true, false, false, true], "sample_762": [false, false, false, false, false], "sample_763": [false, true, true, false, false], "sample_764": [false, false, false, false, false], "sample_765": [false, false, false, true, false], "sample_766": [true, false, false, false, true], "sample_767": [true, false, false, true, false], "sample_768": [true, true, true, true, true], "sample_769": [false, false, false, false, false], "sample_770": [true, false, false, false, true], "sample_771": [true, false, true, false, false], "sample_772": [true, false, true, false, false], "sample_773": [false, false, false, true, false], "sample_774": [false, false, false, false, false], "sample_775": [false, false, false, false, false], "sample_776": [true, true, true, false, true], "sample_777": [false, false, false, false, false], "sample_778": [false, false, true, false, true], "sample_779": [false, false, false, false, false], "sample_780": [true, true, true, true, true], "sample_781": [true, false, false, false, false], "sample_782": [true, false, false, false, true], "sample_783": [true, true, true, true, false], "sample_784": [false, true, false, false, false], "sample_785": [true, true, true, true, true], "sample_786": [true, false, true, true, false], "sample_787": [false, false, false, true, false], "sample_788": [false, false, true, true, false], "sample_789": [true, false, false, true, true], "sample_790": [true, false, true, false, false], "sample_791": [false, true, false, false, true], "sample_792": [true, true, true, false, true], "sample_793": [true, true, false, true, true], "sample_794": [false, false, true, true, false], "sample_795": [false, true, false, false, false], "sample_796": [true, false, false, true, false], "sample_797": [true, true, true, false, false], "sample_798": [false, false, false, false, true], "sample_799": [true, false, true, false, true], "sample_800": [false, false, false, false, false], "sample_801": [false, false, false, false, false], "sample_802": [false, false, true, false, false], "sample_803": [false, false, false, true, false], "sample_804": [false, false, true, false, false], "sample_805": [false, true, true, false, false], "sample_806": [false, true, false, false, false], "sample_807": [false, true, true, false, false], "sample_808": [false, false, false, true, true], "sample_809": [true, false, false, false, true], "sample_810": [false, true, false, false, true], "sample_811": [false, true, false, false, false], "sample_812": [false, false, true, false, false], "sample_813": [true, true, true, false, true], "sample_814": [true, false, false, true, false], "sample_815": [false, false, true, false, false], "sample_816": [true, false, true, false, false], "sample_817": [true, true, true, false, false], "sample_818": [true, true, false, false, true], "sample_819": [false, true, true, true, true], "sample_820": [true, false, true, false, false], "sample_821": [true, false, true, true, true], "sample_822": [true, false, true, false, false], "sample_823": [false, false, true, false, false], "sample_824": [false, false, true, true, false], "sample_825": [true, true, false, false, false], "sample_826": [false, false, false, true, false], "sample_827": [false, false, false, false, false], "sample_828": [false, true, true, false, false], "sample_829": [true, true, false, false, true], "sample_830": [true, true, true, true, true], "sample_831": [false, true, true, false, true], "sample_832": [true, true, true, false, false], "sample_833": [true, false, true, false, true], "sample_834": [true, false, true, true, false], "sample_835": [false, false, false, true, false], "sample_836": [false, false, false, false, false], "sample_837": [true, false, false, true, true], "sample_838": [true, false, false, false, true], "sample_839": [true, false, false, false, false], "sample_840": [true, true, true, true, false], "sample_841": [false, true, false, false, false], "sample_842": [true, true, false, false, false], "sample_843": [false, false, true, true, false], "sample_844": [true, true, true, false, false], "sample_845": [false, false, false, false, false], "sample_846": [false, false, false, false, false], "sample_847": [true, false, false, false, false], "sample_848": [true, false, true, true, true], "sample_849": [false, true, true, true, false], "sample_850": [true, true, true, true, false], "sample_851": [false, false, true, true, true], "sample_852": [false, true, false, false, false], "sample_853": [true, false, true, true, true], "sample_854": [true, false, false, false, false], "sample_855": [false, true, true, true, true], "sample_856": [false, true, false, true, false], "sample_857": [true, true, true, true, true], "sample_858": [false, true, false, true, false], "sample_859": [false, false, false, false, false], "sample_860": [false, false, true, true, false], "sample_861": [true, true, true, true, true], "sample_862": [false, false, false, false, true], "sample_863": [false, true, false, false, false], "sample_864": [true, true, true, true, true], "sample_865": [true, true, false, true, true], "sample_866": [true, true, false, true, false], "sample_867": [false, false, true, true, false], "sample_868": [false, false, false, false, false], "sample_869": [false, false, true, false, false], "sample_870": [true, false, true, false, false], "sample_871": [false, false, true, false, true], "sample_872": [false, true, true, false, true], "sample_873": [false, false, false, true, true], "sample_874": [true, false, false, false, false], "sample_875": [false, false, false, false, false], "sample_876": [true, true, true, false, true], "sample_877": [true, false, false, true, true], "sample_878": [false, false, false, false, false], "sample_879": [false, false, false, false, false], "sample_880": [true, false, false, false, false], "sample_881": [true, false, true, true, false], "sample_882": [true, true, true, false, true], "sample_883": [false, true, false, false, true], "sample_884": [true, true, true, true, true], "sample_885": [false, false, false, false, false], "sample_886": [false, false, false, false, false], "sample_887": [true, false, false, false, true], "sample_888": [true, false, true, true, false], "sample_889": [false, false, false, true, false], "sample_890": [false, false, false, false, false], "sample_891": [true, false, true, true, false], "sample_892": [true, false, true, false, false], "sample_893": [false, true, false, true, false], "sample_894": [false, true, true, false, false], "sample_895": [false, true, false, false, false], "sample_896": [true, true, false, false, false], "sample_897": [false, false, false, true, false], "sample_898": [false, false, false, false, false], "sample_899": [true, false, false, false, false], "sample_900": [true, true, true, true, true], "sample_901": [false, true, true, false, true], "sample_902": [false, false, false, true, false], "sample_903": [false, true, true, false, false], "sample_904": [false, false, false, false, false], "sample_905": [true, false, true, false, true], "sample_906": [false, false, false, false, false], "sample_907": [false, false, false, false, false], "sample_908": [false, true, true, true, true], "sample_909": [false, false, false, false, false], "sample_910": [false, false, false, false, false], "sample_911": [true, false, false, false, false], "sample_912": [false, false, false, false, false], "sample_913": [false, true, false, true, true], "sample_914": [false, true, false, true, false], "sample_915": [false, false, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, true], "sample_918": [false, false, false, true, true], "sample_919": [false, true, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [false, false, false, false, false], "sample_922": [true, false, true, false, true], "sample_923": [false, false, false, true, false], "sample_924": [false, false, false, false, false], "sample_925": [true, false, true, false, false], "sample_926": [false, false, false, false, false], "sample_927": [false, false, true, false, false], "sample_928": [false, true, false, true, false], "sample_929": [false, false, false, false, false], "sample_930": [false, false, false, false, false], "sample_931": [false, false, true, false, false], "sample_932": [false, false, false, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, false, false, false, false], "sample_935": [false, false, false, false, false], "sample_936": [true, false, true, true, false], "sample_937": [false, false, false, false, false], "sample_938": [false, false, false, false, false], "sample_939": [false, false, true, true, false], "sample_940": [false, false, false, false, true], "sample_941": [true, true, true, true, false], "sample_942": [false, false, false, false, false], "sample_943": [false, false, false, false, false], "sample_944": [false, false, false, true, true], "sample_945": [true, false, false, false, false], "sample_946": [false, false, false, false, true], "sample_947": [false, false, false, true, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [true, false, true, false, true], "sample_951": [false, false, false, true, true], "sample_952": [false, true, false, false, false], "sample_953": [false, false, false, false, false], "sample_954": [false, false, false, false, false], "sample_955": [true, false, false, false, false], "sample_956": [true, true, false, true, false], "sample_957": [false, false, false, true, false], "sample_958": [false, false, false, false, false], "sample_959": [false, false, false, false, false], "sample_960": [false, false, false, false, false], "sample_961": [true, false, true, false, false], "sample_962": [false, false, false, false, false], "sample_963": [true, false, false, true, true], "sample_964": [true, false, true, false, false], "sample_965": [true, false, false, false, true], "sample_966": [true, false, true, true, true], "sample_967": [false, false, false, false, false], "sample_968": [false, false, false, false, false], "sample_969": [false, false, false, false, false], "sample_970": [true, false, true, true, false], "sample_971": [false, false, false, false, false], "sample_972": [false, false, false, false, false], "sample_973": [false, false, false, false, false], "sample_974": [false, true, true, true, false], "sample_975": [false, false, false, false, false], "sample_976": [false, false, false, false, false], "sample_977": [true, true, true, true, true], "sample_978": [false, false, false, false, true], "sample_979": [true, false, true, true, true], "sample_980": [false, false, true, false, false], "sample_981": [false, false, false, false, false], "sample_982": [false, true, false, false, false], "sample_983": [false, false, false, true, true], "sample_984": [false, false, false, true, true], "sample_985": [false, false, false, false, false], "sample_986": [true, false, false, false, false], "sample_987": [false, false, false, false, false], "sample_988": [false, false, false, true, false], "sample_989": [false, false, false, false, true], "sample_990": [false, false, false, false, false], "sample_991": [false, false, false, false, true], "sample_992": [false, false, false, false, false], "sample_993": [true, false, true, true, false], "sample_994": [true, false, false, false, false], "sample_995": [false, true, false, false, false], "sample_996": [false, false, false, false, false], "sample_997": [false, true, false, false, false], "sample_998": [false, false, false, true, false], "sample_999": [false, false, false, false, true], "sample_1000": [true, false, true, false, false], "sample_1001": [false, true, false, false, false], "sample_1002": [false, false, false, false, false], "sample_1003": [true, true, true, true, true], "sample_1004": [false, false, false, true, true], "sample_1005": [false, false, false, false, true], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, false, true, false], "sample_1008": [false, true, false, true, true], "sample_1009": [false, true, false, false, false], "sample_1010": [false, false, true, false, false], "sample_1011": [true, false, true, true, false], "sample_1012": [false, false, false, false, false], "sample_1013": [true, false, false, false, false], "sample_1014": [false, false, false, false, false], "sample_1015": [true, false, false, false, false], "sample_1016": [true, false, true, true, false], "sample_1017": [false, false, false, false, false], "sample_1018": [false, false, false, true, false], "sample_1019": [false, false, false, false, false], "sample_1020": [true, true, true, true, true], "sample_1021": [false, true, false, true, false], "sample_1022": [false, false, false, false, false], "sample_1023": [false, true, false, false, true], "sample_1024": [false, false, true, false, false], "sample_1025": [false, false, false, true, false], "sample_1026": [false, false, true, false, false], "sample_1027": [false, false, false, false, false], "sample_1028": [false, true, false, true, false], "sample_1029": [false, false, false, false, false], "sample_1030": [false, false, false, false, false], "sample_1031": [false, false, false, false, false], "sample_1032": [true, false, false, true, true], "sample_1033": [false, false, true, false, false], "sample_1034": [false, false, false, false, false], "sample_1035": [false, false, false, false, true], "sample_1036": [true, false, false, false, false], "sample_1037": [false, true, false, true, false], "sample_1038": [true, true, false, true, true], "sample_1039": [true, true, false, true, false], "sample_1040": [true, false, false, false, true], "sample_1041": [false, false, true, false, true], "sample_1042": [true, false, true, false, true], "sample_1043": [true, false, false, false, false], "sample_1044": [true, false, false, false, false], "sample_1045": [false, true, false, true, false], "sample_1046": [false, false, false, false, false], "sample_1047": [false, true, false, false, false], "sample_1048": [false, false, false, false, false], "sample_1049": [false, false, true, true, true], "sample_1050": [false, false, false, false, false], "sample_1051": [true, true, false, false, true], "sample_1052": [false, false, false, false, false], "sample_1053": [false, false, false, false, false], "sample_1054": [false, false, false, false, false], "sample_1055": [false, false, false, true, true], "sample_1056": [true, true, false, true, false], "sample_1057": [false, false, false, false, false], "sample_1058": [false, false, false, false, false], "sample_1059": [false, false, false, false, false], "sample_1060": [false, false, false, false, false], "sample_1061": [true, false, true, false, false], "sample_1062": [false, false, false, false, false], "sample_1063": [false, false, false, false, true], "sample_1064": [true, true, true, true, true], "sample_1065": [false, false, false, false, false], "sample_1066": [false, false, false, false, false], "sample_1067": [false, false, false, false, false], "sample_1068": [false, true, false, false, true], "sample_1069": [false, false, false, false, false], "sample_1070": [false, true, true, false, false], "sample_1071": [false, true, true, false, false], "sample_1072": [true, false, false, false, false], "sample_1073": [false, false, false, false, false], "sample_1074": [false, true, false, false, false], "sample_1075": [false, true, true, false, true], "sample_1076": [false, false, false, false, false], "sample_1077": [false, false, false, false, true], "sample_1078": [true, false, false, false, false], "sample_1079": [false, false, false, false, true], "sample_1080": [false, false, true, false, false], "sample_1081": [false, false, false, false, false], "sample_1082": [false, true, false, false, false], "sample_1083": [true, false, false, false, false], "sample_1084": [false, false, false, true, true], "sample_1085": [false, false, false, false, false], "sample_1086": [false, false, false, false, true], "sample_1087": [true, true, true, true, true], "sample_1088": [false, true, true, false, false], "sample_1089": [false, false, false, false, false], "sample_1090": [false, false, false, false, false], "sample_1091": [false, true, true, true, true], "sample_1092": [false, false, false, false, false], "sample_1093": [false, false, false, false, true], "sample_1094": [false, true, false, false, true], "sample_1095": [false, false, false, false, false], "sample_1096": [false, true, false, false, false], "sample_1097": [false, false, false, false, false], "sample_1098": [false, false, false, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [false, false, false, false, false], "sample_1101": [false, false, false, false, false], "sample_1102": [false, false, false, false, false], "sample_1103": [false, false, false, false, false], "sample_1104": [false, false, false, false, false], "sample_1105": [false, false, false, false, false], "sample_1106": [false, false, false, false, false], "sample_1107": [false, false, true, false, false], "sample_1108": [true, true, false, true, false], "sample_1109": [false, false, false, true, false], "sample_1110": [false, false, false, false, false], "sample_1111": [false, false, false, false, false], "sample_1112": [true, true, true, true, true], "sample_1113": [false, false, true, true, false], "sample_1114": [false, true, false, false, true], "sample_1115": [false, false, false, false, false], "sample_1116": [true, false, false, false, false], "sample_1117": [false, false, true, false, false], "sample_1118": [false, false, false, false, false], "sample_1119": [false, false, true, false, false], "sample_1120": [false, false, false, true, true], "sample_1121": [false, false, false, false, false], "sample_1122": [false, false, false, false, false], "sample_1123": [true, true, false, false, false], "sample_1124": [false, false, true, false, false], "sample_1125": [false, false, false, true, true], "sample_1126": [true, true, true, true, true], "sample_1127": [true, false, false, false, false], "sample_1128": [false, false, true, false, true], "sample_1129": [false, false, false, false, false], "sample_1130": [true, false, false, true, false], "sample_1131": [false, false, false, false, true], "sample_1132": [false, false, false, true, true], "sample_1133": [true, true, true, true, false], "sample_1134": [false, false, false, true, false], "sample_1135": [false, false, false, false, false], "sample_1136": [false, false, false, false, false], "sample_1137": [false, false, false, false, false], "sample_1138": [true, false, false, false, false], "sample_1139": [true, false, false, true, false], "sample_1140": [false, false, false, false, false], "sample_1141": [false, false, false, false, false], "sample_1142": [true, false, false, false, true], "sample_1143": [false, false, false, false, true], "sample_1144": [false, false, false, false, false], "sample_1145": [true, false, true, true, true], "sample_1146": [false, false, false, false, false], "sample_1147": [false, false, false, false, false], "sample_1148": [false, false, true, true, true], "sample_1149": [false, false, false, true, false], "sample_1150": [false, false, false, false, false], "sample_1151": [false, false, false, false, false], "sample_1152": [false, true, true, false, false], "sample_1153": [true, true, false, false, false], "sample_1154": [true, false, true, false, false], "sample_1155": [false, false, false, false, true], "sample_1156": [false, false, false, false, true], "sample_1157": [false, false, true, false, false], "sample_1158": [true, false, false, false, false], "sample_1159": [false, false, false, false, true], "sample_1160": [false, false, false, false, false], "sample_1161": [true, true, true, false, false], "sample_1162": [false, false, false, false, false], "sample_1163": [false, false, true, false, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, false, true, false, false], "sample_1166": [true, false, false, false, true], "sample_1167": [false, true, false, true, false], "sample_1168": [false, true, true, false, true], "sample_1169": [false, false, false, false, false], "sample_1170": [true, true, true, false, false], "sample_1171": [false, false, true, false, false], "sample_1172": [false, false, false, false, false], "sample_1173": [false, false, false, false, true], "sample_1174": [false, true, false, false, false], "sample_1175": [false, false, false, false, false], "sample_1176": [false, false, false, false, false], "sample_1177": [false, false, false, false, false], "sample_1178": [false, false, false, false, false], "sample_1179": [true, false, false, false, false], "sample_1180": [true, false, true, true, false], "sample_1181": [true, false, true, true, true], "sample_1182": [false, false, false, true, true], "sample_1183": [false, false, false, false, false], "sample_1184": [true, true, true, false, false], "sample_1185": [false, false, true, true, true], "sample_1186": [true, false, true, true, true], "sample_1187": [false, false, false, false, false], "sample_1188": [false, false, false, false, false], "sample_1189": [false, true, true, false, false], "sample_1190": [false, false, false, false, false], "sample_1191": [false, false, false, false, false], "sample_1192": [false, true, true, false, true], "sample_1193": [false, false, false, false, false], "sample_1194": [false, false, true, false, true], "sample_1195": [false, false, true, false, false], "sample_1196": [true, true, false, false, true], "sample_1197": [false, false, false, false, false], "sample_1198": [false, false, false, false, false], "sample_1199": [false, false, false, false, false], "sample_1200": [false, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [false, false, false, false, false], "sample_1203": [false, false, false, false, false], "sample_1204": [false, false, true, false, false], "sample_1205": [false, false, false, false, false], "sample_1206": [true, false, false, false, false], "sample_1207": [true, false, false, false, true], "sample_1208": [true, true, false, false, true], "sample_1209": [false, false, true, false, false]}}