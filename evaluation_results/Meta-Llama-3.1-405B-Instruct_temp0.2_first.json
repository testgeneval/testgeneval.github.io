{"raw_generations": {"sample_0": ["def test_uncertainty_unit_conversion(uncertainty_types_with_conversion_support):\n    for UncertClass in uncertainty_types_with_conversion_support:\n        uncert = UncertClass([1, 2, 3], unit='m')\n        ndd = NDData([1, 2, 3], unit='cm', uncertainty=uncert)\n        assert ndd.uncertainty.unit == uncert._data_unit_to_uncertainty_unit(ndd.unit)\n"], "sample_3": ["def test_ecsv_roundtrip_mixin_columns():\n    # Test round-trip of mixin columns (e.g. Quantity, Time) in ECSV.\n    # This is a regression test for issue #10428.\n    t = mixin_cols()\n    with nullcontext() if NUMPY_LT_1_19 else pytest.warns(AstropyUserWarning):\n        ascii.write(t, 'test.ecsv', format='ecsv')\n    t2 = ascii.read('test.ecsv', format='ecsv')\n    compare_attrs(t, t2)\n    os.remove('test.ecsv')\n"], "sample_1": ["def test_separable_arithmetic_operator():\n    # Test that arithmetic operators always return a non-separable output\n    model1 = models.Scale(1)\n    model2 = models.Scale(2)\n    compound_model = model1 + model2\n    assert not is_separable(compound_model).any()\n    assert separability_matrix(compound_model).all()\n\n    compound_model = model1 - model2\n    assert not is_separable(compound_model).any()\n    assert separability_matrix(compound_model).all()\n\n    compound_model = model1 * model2\n    assert not is_separable(compound_model).any()\n    assert separability_matrix(compound_model).all()\n\n    compound_model = model1 / model2\n    assert not is_separable(compound_model).any()\n    assert separability_matrix(compound_model).all()\n\n    compound_model = model1 ** model2\n    assert not is_separable(compound_model).any()\n    assert separability_matrix(compound_model).all()\n"], "sample_6": ["def test_angle_to_string_precision():\n    # Test that the precision argument in Angle.to_string works as expected\n    angle = Angle(1.23456789, unit=u.deg)\n    assert angle.to_string(decimal=True, precision=3) == '1.235'\n    assert angle.to_string(decimal=True, precision=5) == '1.23457'\n    assert angle.to_string(decimal=False, precision=2) == '1d14m04.44s'\n    assert angle.to_string(decimal=False, precision=4) == '1d14m04.4448s'\n"], "sample_9": ["def test_html_table_id():\n    # Test that the table_id parameter works as expected\n    table = Table({'a': [1, 2], 'b': [3, 4]})\n    html_string = ascii.write(table, format='html')\n    soup = BeautifulSoup(html_string, 'html.parser')\n    soup.find('table')['id'] = 'my_table'\n\n    # Test reading with table_id as string\n    read_table = ascii.read(str(soup), format='html', htmldict={'table_id': 'my_table'})\n    assert np.all(read_table == table)\n\n    # Test reading with table_id as integer\n    read_table = ascii.read(str(soup), format='html', htmldict={'table_id': 1})\n    assert np.all(read_table == table)\n"], "sample_12": ["def test_angle_to_string_precision():\n    a = Angle(1.23456789 * u.deg)\n    assert a.to_string(precision=3) == '1.235d'\n    assert a.to_string(decimal=True, precision=3) == '1.235'\n    assert a.to_string(precision=8) == '1.23456789d'\n    assert a.to_string(decimal=True, precision=8) == '1.23456789'\n\n    # Test with numpy array\n    a = Angle(np.array([1.23456789, 2.3456789]) * u.deg)\n    assert np.all(a.to_string(precision=3) == np.array(['1.235d', '2.346d']))\n    assert np.all(a.to_string(decimal=True, precision=3) == np.array(['1.235', '2.346']))\n"], "sample_13": ["def test_angle_to_string_precision():\n    a = Angle(1.23456789, unit=u.deg)\n    assert a.to_string(decimal=True, precision=3) == '1.235'\n    assert a.to_string(decimal=True, precision=8) == '1.23456789'\n\n    # Also test with non-decimal to_string\n    assert a.to_string(precision=2) == '1:14:04.44'\n    assert a.to_string(precision=4) == '1:14:04.4436'\n"], "sample_14": ["def test_angle_to_string_precision():\n    # Test that the precision argument in to_string works as expected\n    a = Angle(1.23456789, unit=u.deg)\n    assert a.to_string(precision=0) == '1d'\n    assert a.to_string(precision=2) == '1d14m'\n    assert a.to_string(precision=4) == '1d14m04s'\n    assert a.to_string(decimal=True, precision=2) == '1.23'\n    assert a.to_string(decimal=True, precision=5) == '1.23457'\n"], "sample_20": ["def test_read_table_fits_with_memmap(self):\n    # Create a FITS file with a table\n    filename = \"test.fits\"\n    hdu = BinTableHDU.from_columns(\n        [fits.Column(name=\"a\", array=[1, 2, 3], format=\"I\")]\n    )\n    hdu.writeto(filename)\n\n    # Read the table with memmap=True\n    with fits.open(filename, memmap=True) as hdul:\n        table = Table(hdul[1].data)\n\n    # Check that the table data is correct\n    assert_array_equal(table[\"a\"], [1, 2, 3])\n\n    # Remove the FITS file\n    import os\n\n    os.remove(filename)\n"], "sample_21": ["def test_get_lines_from_file(tmpdir):\n    # Create a temporary file with some content\n    filename = tmpdir.join('test.qdp')\n    with open(filename, 'w') as f:\n        f.write('Line 1\\nLine 2\\nLine 3')\n\n    # Test reading from a file\n    lines = _get_lines_from_file(str(filename))\n    assert lines == ['Line 1', 'Line 2', 'Line 3']\n\n    # Test reading from a string\n    lines = _get_lines_from_file('Line 1\\nLine 2\\nLine 3')\n    assert lines == ['Line 1', 'Line 2', 'Line 3']\n\n    # Test reading from an iterable\n    lines = _get_lines_from_file(['Line 1', 'Line 2', 'Line 3'])\n    assert lines == ['Line 1', 'Line 2', 'Line 3']\n\n    # Test invalid input\n    with pytest.raises(ValueError):\n        _get_lines_from_file(123)\n"], "sample_22": ["def test_rotation_matrix():\n    # Test that rotation_matrix gives the correct results for simple cases\n    angle = 90 * u.deg\n    axis = \"z\"\n\n    # Rotate by 90 degrees around z-axis\n    rmat = rotation_matrix(angle, axis)\n    assert_allclose(rmat, np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]]))\n\n    # Rotate by -90 degrees around z-axis\n    rmat = rotation_matrix(-angle, axis)\n    assert_allclose(rmat, np.array([[0, 1, 0], [-1, 0, 0], [0, 0, 1]]))\n\n    # Rotate by 90 degrees around x-axis\n    axis = \"x\"\n    rmat = rotation_matrix(angle, axis)\n    assert_allclose(rmat, np.array([[1, 0, 0], [0, 0, -1], [0, 1, 0]]))\n\n    # Rotate by 90 degrees around y-axis\n    axis = \"y\"\n    rmat = rotation_matrix(angle, axis)\n    assert_allclose(rmat, np.array([[0, 0, 1], [0, 1, 0], [-1, 0, 0]]))\n"], "sample_23": ["def test_angle_to_string_precision():\n    # Test that the precision argument works as expected for to_string\n    a = Angle(1.23456789, unit=u.deg)\n    assert a.to_string(precision=0) == \"1d\"\n    assert a.to_string(precision=2) == \"1d14m\"\n    assert a.to_string(precision=4) == \"1d14m04s\"\n    assert a.to_string(decimal=True, precision=3) == \"1.235\"\n"], "sample_25": ["def test_card_invalid_value():\n    # Test that creating a Card with an invalid value raises the correct error\n    with pytest.raises(ValueError):\n        fits.Card(\"KEYWORD\", np.nan)\n\n    with pytest.raises(ValueError):\n        fits.Card(\"KEYWORD\", np.inf)\n"], "sample_26": ["def test_image_hdu_scale_back(tmpdir):\n    # Test that when scale_back=True, the BSCALE/BZERO values are written back\n    # to the file correctly\n\n    # Create an HDU with some sample data and BSCALE/BZERO values\n    data = np.array([1, 2, 3], dtype=np.int16)\n    hdu = fits.ImageHDU(data=data, do_not_scale_image_data=True)\n    hdu.header['BSCALE'] = 2.0\n    hdu.header['BZERO'] = 10.0\n\n    # Write the HDU to a file with scale_back=True\n    filename = tmpdir.join('test.fits')\n    hdu.writeto(filename, overwrite=True, scale_back=True)\n\n    # Read the file back in and check that the BSCALE/BZERO values were written\n    # correctly\n    with fits.open(filename) as hdul:\n        assert hdul[0].header['BSCALE'] == 2.0\n        assert hdul[0].header['BZERO'] == 10.0\n"], "sample_27": ["def test_diff_header_comments():\n    header_a = Header(cards=[(\"COMMENT\", \"This is a comment\")])\n    header_b = Header(cards=[(\"COMMENT\", \"This is another comment\")])\n\n    diff = HeaderDiff(header_a, header_b)\n    assert not diff.identical\n\n    report = diff.report()\n    assert \"Headers contain differences\" in report\n    assert \"COMMENT\" in report\n    assert \"This is a comment\" in report\n    assert \"This is another comment\" in report\n"], "sample_28": ["def test_card_image_long_string():\n    # Test that a Card with a long string value gets broken up into CONTINUE\n    # cards correctly\n    long_value = \"a\" * 1000\n    c = fits.Card(\"LONGVAL\", long_value)\n    assert len(c.image) > 80\n    # Just a sanity check to make sure it's not too long\n    assert len(c.image) < 10000\n\n    # Make sure the value can be read back out again correctly\n    with BytesIO() as f:\n        f.write(encode_ascii(c.image))\n        f.seek(0)\n        with fits.open(f, memmap=False) as hdul:\n            assert hdul[0].header[\"LONGVAL\"] == long_value\n"], "sample_29": ["def test_write_latex_format_table(self, cosmo):\n    \"\"\"Test `_FORMAT_TABLE` keys match `cosmo.__parameters__`.\"\"\"\n    # get cosmology parameters\n    params = set(cosmo.__parameters__)\n\n    # get _FORMAT_TABLE keys\n    keys = set(_FORMAT_TABLE.keys())\n\n    # check that all parameters are in _FORMAT_TABLE\n    assert params.issubset(keys)\n"], "sample_30": ["def test_votable_file_from_table(tmpdir):\n    from astropy.table import Table\n\n    table = Table({\"a\": [1, 2, 3], \"b\": [4.0, 5.0, 6.0]})\n    votable_file = tree.VOTableFile.from_table(table)\n\n    filename = str(tmpdir.join(\"test.xml\"))\n    votable_file.to_xml(filename)\n\n    # Check that the file can be read back in without errors\n    parse_single_table(filename)\n"], "sample_31": ["def test_write_latex_format_table(cosmo):\n    \"\"\"Test that the _FORMAT_TABLE is correctly applied.\"\"\"\n    # Get table with default (latex) names\n    table_default = write_latex(cosmo, None, format=\"latex\", latex_names=True)\n\n    # Get table without latex names\n    table_no_latex = write_latex(cosmo, None, format=\"latex\", latex_names=False)\n\n    # Check column names are as expected\n    assert list(table_default.columns.keys()) == [_FORMAT_TABLE.get(k, k) for k in cosmo.__parameters__]\n    assert list(table_no_latex.columns.keys()) == cosmo.__parameters__\n"], "sample_32": ["    def setup_class(self):\n        \"\"\"Setup for testing.\"\"\"\n        self._cls = w0wzCDM\n        self._cls_args = (\"H0\", \"Om0\", \"Ode0\")\n        self._cls_kwargs = {\"w0\": -0.8, \"wz\": 0.1}\n        self._func_kwargs = {\"Tcmb0\": 2.725 * u.K}\n"], "sample_33": ["def test_dtype_bytes_or_chars():\n    # Test numeric dtype\n    assert misc.dtype_bytes_or_chars(np.dtype('int32')) == 32\n    \n    # Test string dtype\n    assert misc.dtype_bytes_or_chars(np.dtype('S10')) == 10\n    \n    # Test unicode dtype\n    assert misc.dtype_bytes_or_chars(np.dtype('U10')) == 10\n    \n    # Test invalid dtype\n    assert misc.dtype_bytes_or_chars(np.dtype('object')) is None\n"], "sample_34": ["def test_unit_scale_error():\n    with raises(u.UnitScaleError):\n        u.Unit(\"m\", namespace={})\n    with raises(u.UnitScaleError):\n        u.def_unit(\"m\", namespace={})\n"], "sample_35": ["def test_find_mod_objs():\n    localnames, fqnames, objs = find_mod_objs('astropy.utils.introspection')\n    assert 'find_mod_objs' in localnames\n    assert any(fqn.endswith('.find_mod_objs') for fqn in fqnames)\n    assert any(obj is find_mod_objs for obj in objs)\n\n    # Test with onlylocals=True\n    localnames, fqnames, objs = find_mod_objs('astropy.utils.introspection', onlylocals=True)\n    assert 'find_mod_objs' in localnames\n    assert all(fqn.startswith('astropy.utils.introspection') for fqn in fqnames)\n    assert any(obj is find_mod_objs for obj in objs)\n\n    # Test with a specific list of local modules\n    localnames, fqnames, objs = find_mod_objs('astropy.utils.introspection', onlylocals=['astropy.utils'])\n    assert 'find_mod_objs' in localnames\n    assert all(fqn.startswith('astropy.utils') for fqn in fqnames)\n    assert any(obj is find_mod_objs for obj in objs)\n"], "sample_36": ["def test_biweight_midcorrelation():\n    # Test biweight_midcorrelation with a known result\n    x = np.array([1, 2, 3, 4, 5])\n    y = np.array([2, 3, 5, 7, 11])\n    assert_allclose(biweight_midcorrelation(x, y), 0.9607692307692308)\n\n    # Test biweight_midcorrelation with an outlier\n    x = np.array([1, 2, 3, 4, 100])\n    y = np.array([2, 3, 5, 7, 11])\n    assert_allclose(biweight_midcorrelation(x, y), 0.7385174825174826)\n\n    # Test biweight_midcorrelation with two outliers\n    x = np.array([1, 2, 3, 4, 100])\n    y = np.array([2, 3, 5, 7, 1000])\n    assert_allclose(biweight_midcorrelation(x, y), 0.7058823529411765)\n"], "sample_41": ["def test_unit_scale_error():\n    with raises(u.UnitScaleError):\n        u.Unit(\"m\", scale=2, format=None)\n"], "sample_43": ["def test_bayesian_blocks_events():\n    # Test bayesian blocks with event data\n    t = np.random.normal(size=100)\n    edges = bayesian_blocks(t, fitness='events', p0=0.01)\n    assert len(edges) > 2  # there should be at least two edges for the start and end of the data\n    assert np.all(np.diff(edges) > 0)  # edges should be in ascending order\n"], "sample_44": ["    def test_function_quantity_creation(self):\n        for lu, lq in zip(lu_units, lq_subclasses):\n            for pu in pu_sample:\n                fq = lq(1, unit=lu(pu))\n                assert fq.unit.physical_unit == pu\n                assert fq.unit.function_unit == lu\n"], "sample_47": ["    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('SECRET_KEY', 'my_secret_key'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('OTHER_SETTING', 'other_value'), 'other_value')\n"], "sample_48": ["def test_aggregate_filter(self):\n    # Test that filter is applied to aggregates\n    books = Book.objects.annotate(avg_rating=Avg('rating', filter=Q(pages__gt=300)))\n    self.assertEqual(books.get(isbn='159059725').avg_rating, None)\n    self.assertAlmostEqual(books.get(isbn='067232959').avg_rating, 3.0)\n\n    # Test that filter can be a complex expression\n    books = Book.objects.annotate(\n        avg_rating=Avg('rating', filter=(Q(pages__gt=300) & Q(price__lt=50)))\n    )\n    self.assertEqual(books.get(isbn='159059725').avg_rating, None)\n    self.assertAlmostEqual(books.get(isbn='067232959').avg_rating, 3.0)\n\n    # Test that filter works with multiple aggregates\n    books = Book.objects.annotate(\n        avg_rating=Avg('rating', filter=Q(pages__gt=300)),\n        max_pages=Max('pages', filter=Q(rating__gt=4))\n    )\n    self.assertEqual(books.get(isbn='159059725').avg_rating, None)\n    self.assertEqual(books.get(isbn='159059725').max_pages, None)\n    self.assertAlmostEqual(books.get(isbn='067232959').avg_rating, 3.0)\n    self.assertEqual(books.get(isbn='067232959').max_pages, None)\n"], "sample_49": ["def test_media_add(self):\n    class TestForm(Form):\n        field1 = CharField(max_length=10, widget=TextInput(attrs={'class': 'field1'}))\n        field2 = CharField(max_length=10, widget=TextInput(attrs={'class': 'field2'}))\n\n        class Media:\n            css = {\n                'all': ('path/to/css1.css',)\n            }\n            js = ('path/to/js1.js',)\n\n    form = TestForm()\n    media = form.media\n\n    widget_media = Media(css={'all': ('path/to/css2.css',)}, js=('path/to/js2.js',))\n    new_media = media + widget_media\n\n    self.assertEqual(new_media._css, {'all': ['path/to/css1.css', 'path/to/css2.css']})\n    self.assertEqual(new_media._js, ['path/to/js1.js', 'path/to/js2.js'])\n"], "sample_50": ["def test_runshell_db_with_password(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'mydb',\n        'user': 'myuser',\n        'password': 'mypassword'\n    }\n    args, pgpassword = self._run_it(dbinfo)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(pgpassword, 'mypassword')\n"], "sample_51": ["def test_parse_date(self):\n    self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n    self.assertIsNone(parse_date('2022-01'))  # incomplete date\n    self.assertIsNone(parse_date('20220101'))  # invalid format\n    self.assertIsNone(parse_date('abcdefg'))  # invalid format\n    with self.assertRaises(ValueError):\n        parse_date('2022-02-30')  # invalid day for month\n"], "sample_54": ["def test_file_response_with_non_ascii_filename(self):\n    filename = 'test_\u00fcnicode.txt'\n    response = FileResponse(io.BytesIO(b'Hello, world!'), filename=filename)\n    self.assertEqual(response['Content-Disposition'], f'inline; filename*=utf-8\\'\\'{filename}')\n"], "sample_58": ["def test_form_add_prefix(self):\n    # Test prefix construction\n    form = Person(prefix='person')\n    self.assertEqual(form.add_prefix('first_name'), 'person-first_name')\n\n    # If there is no prefix, the name should not be altered\n    form = Person()\n    self.assertEqual(form.add_prefix('first_name'), 'first_name')\n"], "sample_57": ["    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n"], "sample_59": ["    def test_fields_cache(self):\n        obj = Model1()\n        self.assertEqual(obj._state.fields_cache, {})\n        obj._state.fields_cache['test'] = 'value'\n        self.assertEqual(obj._state.fields_cache, {'test': 'value'})\n"], "sample_61": ["    def test_ascii_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertIsNone(validator('testuser'))\n        self.assertIsNone(validator('test.user'))\n        self.assertIsNone(validator('test_user'))\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid username. This value may contain only English letters, numbers, and @/./+/-/_ characters.'):\n            validator('test user')\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid username. This value may contain only English letters, numbers, and @/./+/-/_ characters.'):\n            validator('test\u00fcser')\n"], "sample_62": ["def test_register(self):\n    self.site.register(Person, NameAdmin)\n    self.assertEqual(len(self.site._registry), 1)\n    self.assertIn(Person, self.site._registry)\n    self.assertIsInstance(self.site._registry[Person], NameAdmin)\n\n    # Test registering a model with the default ModelAdmin\n    self.site.register(Location)\n    self.assertEqual(len(self.site._registry), 2)\n    self.assertIn(Location, self.site._registry)\n    self.assertIsInstance(self.site._registry[Location], admin.ModelAdmin)\n"], "sample_63": ["def test_render_to_string_with_list_template_name(self):\n    template = self.engine.render_to_string(['template.txt', 'nonexistent.txt'], {'name': 'John'})\n    self.assertEqual(template.strip(), 'Hello, John!')\n"], "sample_65": ["    def test_catalog(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = gettext.NullTranslations()\n        catalog.locale = 'en'\n        context = catalog.get_context_data()\n        self.assertIn('catalog', context)\n        self.assertIn('formats', context)\n        self.assertIn('plural', context)\n"], "sample_68": ["    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('foo', 'bar'), 'bar')\n        self.assertEqual(cleanse_setting('API_KEY', 'secret'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('DATABASES', {'default': {'PASSWORD': 'secret'}}), {'default': {'PASSWORD': CLEANSED_SUBSTITUTE}})\n        self.assertEqual(cleanse_setting('ALLOWED_HOSTS', ['example.com']), ['example.com'])\n"], "sample_69": ["def test_iter_modules_and_files_with_zip_file(self):\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        zip_filename = Path(tmp_dir) / 'test.zip'\n        with zipfile.ZipFile(zip_filename, 'w') as zip_file:\n            zip_file.writestr('test.py', b'')\n        sys.path.insert(0, str(zip_filename))\n        self.addCleanup(sys.path.remove, str(zip_filename))\n        self.import_and_cleanup('test')\n        self.assertFileFound(zip_filename)\n"], "sample_70": ["def test_protect(self):\n    collector = Collector(using='default')\n    a = create_a()\n    with self.assertRaises(ProtectedError) as cm:\n        PROTECT(collector, A.r.field, [a], 'default')\n    self.assertEqual(cm.exception.protected_objects, [a])\n"], "sample_71": ["def test_decimal_pos_rounding(self):\n    self.assertEqual(nformat(Decimal('123.456'), '.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(Decimal('123.456'), '.', decimal_pos=1), '123.5')\n    self.assertEqual(nformat(Decimal('123.456'), '.', decimal_pos=0), '123')\n"], "sample_74": ["def test_runshell_db_with_ssl_params(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'mydb',\n        'user': 'myuser',\n        'password': 'mypassword',\n        'sslmode': 'require',\n        'sslrootcert': '/path/to/root/cert',\n        'sslcert': '/path/to/cert',\n        'sslkey': '/path/to/key',\n    }\n    args, env = self._run_it(dbinfo)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/root/cert',\n        'PGSSLCERT': '/path/to/cert',\n        'PGSSLKEY': '/path/to/key',\n    })\n"], "sample_76": ["def test_check_setting_language_code(self):\n    with self.settings(LANGUAGE_CODE='en'):\n        errors = check_setting_language_code(None)\n        self.assertEqual(errors, [])\n\n    with self.settings(LANGUAGE_CODE=None):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n\n    with self.settings(LANGUAGE_CODE=' invalid'):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n"], "sample_77": ["def test_escapejs(self):\n    # Make sure escapejs works correctly with quotes and other special characters\n    self.check_output(escapejs, 'Hello, world!', None)\n    self.check_output(escapejs, '\"Hello, world!\"', None)\n    self.check_output(escapejs, \"'Hello, world!'\", None)\n    self.check_output(escapejs, '<script>alert(\"Hello, world!\");</script>', None)\n    self.check_output(escapejs, 'Hello,\\nworld!', None)\n"], "sample_78": ["def test_base_command_style(self):\n    class TestCommand(BaseCommand):\n            self.stdout.write(self.style.SUCCESS('Success message'))\n            self.stderr.write(self.style.ERROR('Error message'))\n\n    command = TestCommand()\n    with captured_stderr() as stderr:\n        with mock.patch.object(command.stdout, 'write') as write_mock:\n            command.handle()\n            write_mock.assert_called_once_with(command.style.SUCCESS('Success message') + '\\n')\n        self.assertIn(command.style.ERROR('Error message') + '\\n', stderr.getvalue())\n"], "sample_82": ["def test_use_required_attribute(self):\n    # 'required' is okay for <select> without choices.\n    widget = SelectDateWidget(required=True)\n    self.assertTrue(widget.use_required_attribute(None))\n    # 'required' is not okay for <select> with choices.\n    widget = SelectDateWidget(required=True, years=('2007', '2008'))\n    self.assertFalse(widget.use_required_attribute(None))\n"], "sample_81": ["def test_regex_pattern_match(self):\n    pattern = RegexPattern(r'^users/(?P<username>\\w+)/$')\n    match = pattern.match('/users/john/')\n    self.assertIsNotNone(match)\n    self.assertEqual(match[0], '')\n    self.assertEqual(match[1], ())\n    self.assertEqual(match[2], {'username': 'john'})\n"], "sample_83": ["def test_filter_registration(self):\n        return value.upper()\n\n    self.library.filter('test_filter', filter_func)\n    self.assertIn('test_filter', self.library.filters)\n    self.assertEqual(self.library.filters['test_filter'], filter_func)\n"], "sample_84": ["def test_urlencode_with_non_string_values(self):\n    data = {'a': 1, 'b': True, 'c': None}\n    with self.assertRaises(TypeError) as e:\n        urlencode(data)\n    self.assertEqual(str(e.exception), self.cannot_encode_none_msg)\n\n    data = {'a': 1, 'b': True, 'c': 'hello'}\n    self.assertEqual(urlencode(data), 'a=1&b=True&c=hello')\n\n    data = MultiValueDict({'a': [1, 2], 'b': [True, False]})\n    self.assertEqual(urlencode(data), 'a=1&a=2&b=True&b=False')\n"], "sample_86": ["def test_lazy(self):\n    # Test that lazy works with a function\n        return x + y\n\n    lazy_func = lazy(my_function, int)\n    self.assertIsInstance(lazy_func(1, 2), int)\n    self.assertEqual(lazy_func(1, 2), 3)\n\n    # Test that lazy works with a method\n    class MyClass:\n            return x + y\n\n    obj = MyClass()\n    lazy_method = lazy(obj.my_method, int)\n    self.assertIsInstance(lazy_method(1, 2), int)\n    self.assertEqual(lazy_method(1, 2), 3)\n"], "sample_87": ["def test_iter_modules_and_files_with_zip_importer(self):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with zipfile.ZipFile(Path(tmpdir) / 'test.zip', 'w') as zip_file:\n            zip_file.writestr('test.py', b'')\n        sys.path.append(str(Path(tmpdir) / 'test.zip'))\n        self.import_and_cleanup('test')\n        self.assertFileFound(Path(tmpdir) / 'test.zip')\n        sys.path.pop()\n"], "sample_89": ["def test_iter_modules_and_files_zip_import(self):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with zipfile.ZipFile(Path(tmpdir) / 'test.zip', 'w') as zip_file:\n            zip_file.writestr('test.py', b'')\n        sys.path.insert(0, str(Path(tmpdir) / 'test.zip'))\n        self.import_and_cleanup('test')\n        self.assertFileFound(Path(tmpdir) / 'test.zip')\n        sys.path.pop(0)\n"], "sample_91": ["def test_page_not_found_renders_template(self):\n    \"\"\"Test that page_not_found view renders 404 template.\"\"\"\n    request = self.request_factory.get('/nonexistent_url/')\n    response = page_not_found(request, Http404('Non-existent URL'))\n    self.assertTemplateUsed(response, '404.html')\n    self.assertEqual(response.status_code, 404)\n"], "sample_92": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.superuser = User.objects.create_superuser('super', 'super@example.com', 'super')\n"], "sample_94": ["def test_create_superuser_password_validation_bypass(self):\n    with mock.patch('sys.stdin', MockTTY()):\n        call_command('createsuperuser', verbosity=0)\n    self.assertEqual(User.objects.count(), 1)\n    user = User.objects.get(username='alice')\n    self.assertTrue(user.is_superuser)\n    self.assertEqual(user.password, 'test1234')\n"], "sample_98": ["    def setUpClass(cls):\n        super().setUpClass()\n        cls.server = WSGIServer(('localhost', 0), QuietWSGIRequestHandler)\n"], "sample_100": ["def test_iter_modules_and_files_with_zip_file(self):\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        zip_filename = Path(tmp_dir) / 'test.zip'\n        with zipfile.ZipFile(zip_filename, 'w') as zip_file:\n            zip_file.writestr('test.py', b'')\n        sys.path.insert(0, str(zip_filename))\n        self.addCleanup(sys.path.remove, str(zip_filename))\n        self.import_and_cleanup('test')\n        self.assertFileFound(zip_filename)\n"], "sample_101": ["def test_wsgi_request_path_info(self):\n    environ = {\n        'PATH_INFO': '/test/',\n        'SCRIPT_NAME': '/script/',\n        'REQUEST_METHOD': 'GET',\n        'wsgi.input': BytesIO(),\n    }\n    request = WSGIRequest(environ)\n    self.assertEqual(request.path_info, '/test/')\n    self.assertEqual(request.path, '/script/test/')\n"], "sample_103": ["def test_aggregate_filter_clause(self):\n    # Test that the filter clause is correctly applied to the aggregate function\n    books = Book.objects.annotate(\n        num_authors=Count('authors', filter=Q(authors__age__gt=30))\n    )\n    self.assertEqual(books.get(isbn='159059725').num_authors, 1)\n    self.assertEqual(books.get(isbn='067232959').num_authors, 1)\n    self.assertEqual(books.get(isbn='159059996').num_authors, 0)\n"], "sample_107": ["    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('SETTING', 'value'), 'value')\n        self.assertEqual(cleanse_setting('API_SETTING', 'value'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('SETTING', {'key': 'value'}), {'key': 'value'})\n        self.assertEqual(cleanse_setting('API_SETTING', {'key': 'value'}), CLEANSED_SUBSTITUTE)\n"], "sample_106": ["    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600')\n"], "sample_105": ["def test_view_as_view(self):\n    view = SimpleView.as_view()\n    self.assertIsInstance(view, type(lambda: None))\n    response = view(self.rf.get('/'))\n    self._assert_simple(response)\n    self.assertEqual(view.view_class, SimpleView)\n    self.assertEqual(view.view_initkwargs, {})\n"], "sample_108": ["def test_resolver_match(self):\n    resolver_match = resolve('/base64/aGVsbG8=/')\n    self.assertEqual(resolver_match.func, empty_view)\n    self.assertEqual(resolver_match.args, ())\n    self.assertEqual(resolver_match.kwargs, {'value': b'hello'})\n    self.assertEqual(resolver_match.url_name, 'base64')\n    self.assertEqual(resolver_match.route, '/base64/<base64:value>/')\n"], "sample_111": ["def test_get_ordering_field_columns(self):\n    request = self._mocked_authenticated_request('/admin/auth/user/', self.superuser)\n    changelist = ChangeList(request, User, list_display=['username', 'email'], list_display_links=None,\n                            list_filter=None, date_hierarchy=None, search_fields=None, list_select_related=None,\n                            list_per_page=None, list_max_show_all=None, list_editable=None, model_admin=UserAdmin,\n                            sortable_by=None)\n\n    # Test with default ordering\n    ordering_fields = changelist.get_ordering_field_columns()\n    self.assertEqual(ordering_fields, {0: 'asc'})\n\n    # Test with custom ordering\n    request.GET['o'] = '1.-2'\n    changelist = ChangeList(request, User, list_display=['username', 'email'], list_display_links=None,\n                            list_filter=None, date_hierarchy=None, search_fields=None, list_select_related=None,\n                            list_per_page=None, list_max_show_all=None, list_editable=None, model_admin=UserAdmin,\n                            sortable_by=None)\n    ordering_fields = changelist.get_ordering_field_columns()\n    self.assertEqual(ordering_fields, {1: 'desc'})\n"], "sample_112": ["def test_prepopulated_fields_js(self):\n    adminform = ArticleAdmin(Article, site).get_form(request=self.request_factory.get('/'))(instance=Article())\n    context = {'adminform': adminform}\n    prepopulated_fields_js(context)\n    self.assertIn('prepopulated_fields', context)\n    self.assertIn('prepopulated_fields_json', context)\n    self.assertIsInstance(context['prepopulated_fields'], list)\n    self.assertIsInstance(context['prepopulated_fields_json'], str)\n"], "sample_113": ["def test_parse_docstring(self):\n    docstring = \"\"\"This is a title\n"], "sample_115": ["    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings = filter.get_safe_settings()\n        self.assertIn('SECRET_KEY', settings)\n        self.assertEqual(settings['SECRET_KEY'], '********************')\n"], "sample_117": ["    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n"], "sample_122": ["    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600')\n"], "sample_123": ["    def test_urlsafe_base64_encode(self):\n        s = b'Hello, World!'\n        encoded = urlsafe_base64_encode(s)\n        self.assertEqual(urlsafe_base64_decode(encoded), s)\n"], "sample_125": ["def test_set_cookie_secure(self):\n    response = HttpResponse()\n    response.set_cookie('key', 'value', secure=True)\n    self.assertEqual(response.cookies['key']['secure'], True)\n    response.set_cookie('key', 'value', secure=False)\n    self.assertEqual(response.cookies['key']['secure'], False)\n"], "sample_128": ["def test_index_name_truncation(self):\n    index = Index(fields=['very_long_field_name_with_excessive_length'])\n    index.set_name_with_model(Article)\n    self.assertLessEqual(len(index.name), Index.max_name_length)\n"], "sample_131": ["    def test_create_test_db(self):\n        test_connection = self.get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        with mock.patch.object(creation, '_create_test_db') as _create_test_db:\n            _create_test_db.return_value = 'test_database'\n            db_name = creation.create_test_db(verbosity=0)\n            self.assertEqual(db_name, 'test_database')\n            self.assertEqual(test_connection.settings_dict['NAME'], 'test_database')\n"], "sample_132": ["    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings = filter.get_safe_settings()\n        self.assertIsInstance(settings, dict)\n        self.assertNotIn('SECRET_KEY', settings)\n"], "sample_133": ["    def test_javascript_catalog(self):\n        response = self.client.get(reverse('jsi18n'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'jsi18n.html')\n        catalog = get_formats()\n        catalog.update({\n            'catalog': {},\n            'formats': get_formats(),\n            'plural': None,\n        })\n        self.assertJSONEqual(response.content, catalog)\n"], "sample_139": ["def test_changelist_view_search(self):\n    model_admin = BandAdmin(Band, admin.site)\n    request = self._mocked_authenticated_request('/admin/band/', self.superuser)\n    request.GET = {'q': 'The'}\n    changelist = model_admin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(queryset.count(), 1)\n    self.assertEqual(queryset[0].name, 'The Doors')\n"], "sample_137": ["def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n    self.assertIn(\"**Context**\", body)\n    self.assertIn(\":template:`myapp/my_template.html` (DESCRIPTION)\", body)\n    self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n"], "sample_138": ["    def test_post_process(self):\n        \"\"\"\n        Test post processing of files.\n        \"\"\"\n        storage.staticfiles_storage = storage.HashedFilesMixin()\n        self.addCleanup(shutil.rmtree, settings.STATIC_ROOT)\n        call_command('collectstatic', verbosity=0, interactive=False)\n\n        # Check the manifest file was created.\n        manifest_path = os.path.join(settings.STATIC_ROOT, 'staticfiles.json')\n        self.assertTrue(os.path.exists(manifest_path))\n\n        # Check a file was hashed and saved to the manifest.\n        original_file_path = os.path.join(TEST_ROOT, 'project', 'app', 'static', 'app', 'test.txt')\n        hashed_file_path = self.hashed_file_path('app/test.txt')\n        with open(original_file_path, 'r') as f:\n            original_content = f.read()\n        with open(os.path.join(settings.STATIC_ROOT, hashed_file_path), 'r') as f:\n            hashed_content = f.read()\n        self.assertEqual(original_content, hashed_content)\n\n        # Check the manifest contains the correct data.\n        with open(manifest_path, 'r') as f:\n            manifest_data = json.load(f)\n        self.assertIn(hashed_file_path, manifest_data['paths'].values())\n"], "sample_140": ["    def test_explicit_variable_names(self):\n        @sensitive_variables('user', 'password')\n            self.assertEqual(my_function.sensitive_variables, ('user', 'password'))\n        my_function('jacob', 'secret')\n"], "sample_141": ["def test_build_instance_with_natural_key(self):\n    # Create a model instance without a primary key.\n    data = {'name': 'Test Category'}\n    instance = serializers.build_instance(Category, data, db='default')\n\n    # Check that the instance was created with the correct natural key.\n    self.assertEqual(instance.name, 'Test Category')\n    self.assertIsNone(instance.pk)\n\n    # Save the instance to the database and check that it has a primary key.\n    instance.save()\n    self.assertIsNotNone(instance.pk)\n\n    # Try to retrieve the instance using its natural key.\n    retrieved_instance = Category.objects.get_by_natural_key('Test Category')\n    self.assertEqual(retrieved_instance.pk, instance.pk)\n"], "sample_143": ["def test_unescape_string_literal(self):\n    self.assertEqual(text.unescape_string_literal('\"abc\"'), 'abc')\n    self.assertEqual(text.unescape_string_literal(\"'abc'\"), 'abc')\n    self.assertEqual(text.unescape_string_literal('\"a \\\\\"bc\\\\\"\"'), 'a \"bc\"')\n    self.assertEqual(text.unescape_string_literal(\"'\\\\'ab\\\\' c'\"), \"'ab' c\")\n    with self.assertRaises(ValueError):\n        text.unescape_string_literal('abc')\n    with self.assertRaises(ValueError):\n        text.unescape_string_literal('\"abc')\n"], "sample_144": ["def test_deferred_fields(self):\n    # Create an instance of Model with deferred fields\n    obj = Person.objects.create(name='John')\n    obj.refresh_from_db(fields=['name'])\n\n    # Check that deferred fields are properly set\n    self.assertEqual(obj.name, 'John')\n    self.assertIn('age', obj.get_deferred_fields())\n\n    # Accessing a deferred field should load its value from the database\n    self.assertEqual(obj.age, None)\n    self.assertNotIn('age', obj.get_deferred_fields())\n"], "sample_146": ["def test_check_setting_language_code(self):\n    with override_settings(LANGUAGE_CODE='en'):\n        self.assertEqual(check_setting_language_code(None), [])\n\n    with override_settings(LANGUAGE_CODE=None):\n        self.assertEqual(len(check_setting_language_code(None)), 1)\n        self.assertIsInstance(check_setting_language_code(None)[0], Error)\n\n    with override_settings(LANGUAGE_CODE=123):\n        self.assertEqual(len(check_setting_language_code(None)), 1)\n        self.assertIsInstance(check_setting_language_code(None)[0], Error)\n\n    with override_settings(LANGUAGE_CODE='invalid-language-code'):\n        self.assertEqual(len(check_setting_language_code(None)), 1)\n        self.assertIsInstance(check_setting_language_code(None)[0], Error)\n"], "sample_148": ["def test_nested_objects_edge_cases(self):\n    # Test empty NestedObjects\n    n = NestedObjects(using=DEFAULT_DB_ALIAS)\n    self.assertEqual(n.nested(), [])\n\n    # Test single object with no relationships\n    obj = Count.objects.create(num=1)\n    n.collect([obj])\n    self.assertEqual(n.nested(lambda obj: obj.num), [1])\n\n    # Test multiple objects with no relationships\n    objs = [Count.objects.create(num=i) for i in range(3)]\n    n.collect(objs)\n    self.assertEqual(n.nested(lambda obj: obj.num), [0, 1, 2])\n"], "sample_149": ["def test_check_user_model_required_fields_is_not_list(self):\n    @self.apps.get_model('auth_tests', 'CustomUserNonListRequiredFields')\n    class CustomUserNonListRequiredFields(AbstractBaseUser):\n        REQUIRED_FIELDS = 'non_list'\n\n    errors = check_user_model(app_configs=self.apps.app_configs)\n    expected_errors = [\n        checks.Error(\n            \"'REQUIRED_FIELDS' must be a list or tuple.\",\n            obj=CustomUserNonListRequiredFields,\n            id='auth.E001',\n        ),\n    ]\n    self.assertEqual(errors, expected_errors)\n"], "sample_152": ["def test_sort(self):\n    collector = Collector(using='default')\n    a1 = A.objects.create()\n    b1 = B.objects.create(a=a1)\n    c1 = Child.objects.create(b=b1)\n    collector.collect([a1])\n    self.assertEqual(list(collector.data.keys()), [A, B, Child])\n    collector.sort()\n    self.assertEqual(list(collector.data.keys()), [Child, B, A])\n"], "sample_150": ["    def test_base_command(self):\n        command = BaseCommand()\n        self.assertEqual(command.help, '')\n        self.assertFalse(command.requires_system_checks)\n"], "sample_154": ["def test_check_database_backends_no_databases(self):\n    with mock.patch('django.db.connections') as connections_mock:\n        issues = check_database_backends(databases=None)\n        self.assertEqual(issues, [])\n        connections_mock.assert_not_called()\n"], "sample_155": ["def test_file_response_with_as_attachment(self):\n    file = ContentFile(b'Hello, world!', name='example.txt')\n    response = FileResponse(file, as_attachment=True)\n    self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"example.txt\"')\n"], "sample_156": ["def test_form_add_prefix(self):\n    # Test that add_prefix is called for each field.\n    class Person(Form):\n        first_name = CharField()\n        last_name = CharField()\n\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('first_name'), 'person-first_name')\n    self.assertEqual(p.add_prefix('last_name'), 'person-last_name')\n\n    # If prefix is not specified, the fields should not have any prefix.\n    p = Person()\n    self.assertEqual(p.add_prefix('first_name'), 'first_name')\n    self.assertEqual(p.add_prefix('last_name'), 'last_name')\n"], "sample_157": ["    def test_create_test_db(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        with mock.patch.object(creation, '_create_test_db') as _create_test_db:\n            _create_test_db.return_value = 'test_database'\n            db_name = creation.create_test_db(verbosity=0)\n            self.assertEqual(db_name, 'test_database')\n            self.assertEqual(test_connection.settings_dict['NAME'], 'test_database')\n"], "sample_159": ["def test_check_user_model_required_fields_is_not_list(self):\n    @self.apps.get_model('CustomUserNonListRequiredFields')\n    class CustomUserNonListRequiredFields(AbstractBaseUser):\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = 'non_list'\n\n        username = models.CharField(max_length=10, unique=True)\n        non_list = models.CharField(max_length=10)\n\n    errors = checks.run_checks()\n    expected_errors = [\n        checks.Error(\n            \"'REQUIRED_FIELDS' must be a list or tuple.\",\n            obj=CustomUserNonListRequiredFields,\n            id='auth.E001',\n        ),\n    ]\n    self.assertEqual(errors, expected_errors)\n"], "sample_160": ["def test_decimal_pos_rounding(self):\n    self.assertEqual(nformat(123.456, decimal_sep='.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(123.456, decimal_sep='.', decimal_pos=1), '123.5')\n    self.assertEqual(nformat(123.456, decimal_sep='.', decimal_pos=0), '123')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep='.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep='.', decimal_pos=1), '123.5')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep='.', decimal_pos=0), '123')\n"], "sample_162": ["def test_makemessages_with_domain(self):\n    with captured_stdout() as stdout, captured_stderr() as stderr:\n        management.call_command('makemessages', locale=[LOCALE], domain='djangojs')\n    self.assertTrue(os.path.exists('locale/%s/LC_MESSAGES/djangojs.po' % LOCALE))\n    self.assertIn(\"processing locale\", stdout.getvalue())\n    self.assertNotIn(\"errors happened while running xgettext\", stderr.getvalue())\n"], "sample_163": ["def test_password_reset_confirm_view(self):\n    # Create a password reset token for the user\n    token = default_token_generator.make_token(self.u1)\n    uidb64 = urlsafe_base64_encode(str(self.u1.pk).encode())\n\n    # Try to access the password reset confirm view with an invalid token\n    response = self.client.get(reverse(\"password_reset_confirm\", args=[uidb64, \"invalid-token\"]))\n    self.assertEqual(response.status_code, 200)\n    self.assertFormError(response, \"The password reset link was invalid, possibly because it has already been used. Please request a new password reset.\")\n\n    # Try to access the password reset confirm view with a valid token\n    response = self.client.get(reverse(\"password_reset_confirm\", args=[uidb64, token]))\n    self.assertEqual(response.status_code, 200)\n\n    # Try to submit the password reset form with mismatched passwords\n    response = self.client.post(\n        reverse(\"password_reset_confirm\", args=[uidb64, token]),\n        {\n            \"new_password1\": \"new_password\",\n            \"new_password2\": \"different_password\",\n        },\n    )\n    self.assertEqual(response.status_code, 200)\n    self.assertFormError(response, \"The two password fields didn\u2019t match.\")\n\n    # Try to submit the password reset form with a valid password\n    response = self.client.post(\n        reverse(\"password_reset_confirm\", args=[uidb64, token]),\n        {\n            \"new_password1\": \"new_password\",\n            \"new_password2\": \"new_password\",\n        },\n    )\n    self.assertRedirects(response, reverse(\"password_reset_complete\"))\n"], "sample_164": ["    def test_emit(self):\n        handler = AdminEmailHandler()\n        record = logging.makeLogRecord('name', logging.ERROR, 'path', 1, 'message', None, None)\n        handler.emit(record)\n        self.assertEqual(len(mail.outbox), 1)\n"], "sample_166": ["def test_salted_hmac_invalid_algorithm(self):\n    with self.assertRaises(InvalidAlgorithm):\n        salted_hmac('test_key_salt', 'test_value', algorithm='invalid_algorithm')\n"], "sample_167": ["def test_ordinal(self):\n    self.assertEqual(humanize.ordinal(1), '1st')\n    self.assertEqual(humanize.ordinal(2), '2nd')\n    self.assertEqual(humanize.ordinal(3), '3rd')\n    self.assertEqual(humanize.ordinal(4), '4th')\n    self.assertEqual(humanize.ordinal(10), '10th')\n    self.assertEqual(humanize.ordinal(11), '11th')\n    self.assertEqual(humanize.ordinal(12), '12th')\n    self.assertEqual(humanize.ordinal(13), '13th')\n    self.assertEqual(humanize.ordinal(20), '20th')\n    self.assertEqual(humanize.ordinal(21), '21st')\n    self.assertEqual(humanize.ordinal(22), '22nd')\n    self.assertEqual(humanize.ordinal(23), '23rd')\n    self.assertEqual(humanize.ordinal(24), '24th')\n"], "sample_168": ["def test_remove_stale_content_types_interactive(self):\n    with captured_stdout() as stdout:\n        call_command('remove_stale_content_types', interactive=True, verbosity=0)\n    self.assertIn(\"Some content types in your database are stale and can be deleted.\", stdout.getvalue())\n    self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n\n    with mock.patch('builtins.input', return_value='yes'):\n        with captured_stdout() as stdout:\n            call_command('remove_stale_content_types', interactive=True, verbosity=2)\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n    self.assertIn(\"Deleting stale content type 'contenttypes_tests | Fake'\", stdout.getvalue())\n"], "sample_171": ["def test_migrate_fake_initial(self):\n    \"\"\"\n    Tests that --fake-initial works correctly.\n    \"\"\"\n    # Make the initial migration of the 'migrations' app fake-apply.\n    call_command('migrate', '--fake-initial', verbosity=0)\n    # Test that all tables have been created.\n    self.assertTableExists('migrations_testmodel')\n    self.assertTableExists('migrations_testmodel2')\n    # Test that the migrations have been marked as applied.\n    recorder = MigrationRecorder(connection)\n    self.assertTrue(recorder.applied_migrations().filter(app='migrations', name='0001_initial').exists())\n    self.assertTrue(recorder.applied_migrations().filter(app='migrations', name='0002_second').exists())\n"], "sample_170": ["    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings_dict = filter.get_safe_settings()\n        self.assertIsInstance(settings_dict, dict)\n        for key, value in settings_dict.items():\n            self.assertNotIn('SECRET', key)\n            self.assertNotIn('PASSWORD', key)\n"], "sample_177": ["def test_state_clone(self):\n    \"\"\"\n    Cloning a ProjectState results in an exact copy.\n    \"\"\"\n    project_state = ProjectState()\n    model_state = ModelState('app', 'Model', [\n        ('id', models.AutoField(primary_key=True)),\n    ])\n    project_state.add_model(model_state)\n\n    cloned_state = project_state.clone()\n\n    self.assertEqual(cloned_state.models, project_state.models)\n    self.assertEqual(cloned_state.real_apps, project_state.real_apps)\n    self.assertEqual(cloned_state.apps.models, project_state.apps.models)\n"], "sample_178": ["def test_formset_management_form_validation(self):\n    formset = self.make_choiceformset(formset_data=[('Choice 1', '10'), ('Choice 2', '20')])\n    # Make sure the management form is validated.\n    formset.management_form.data['choices-TOTAL_FORMS'] = 'abc'\n    with self.assertRaises(ValidationError):\n        formset.is_valid()\n"], "sample_185": ["def test_get_format_lazy(self):\n    with translation.override('de'):\n        lazy_format = get_format_lazy('DATE_FORMAT')\n        self.assertEqual(lazy_format, get_format('DATE_FORMAT'))\n        with translation.override('en'):\n            self.assertEqual(lazy_format, get_format('DATE_FORMAT'))\n"], "sample_187": ["def test_unescape_string_literal(self):\n    self.assertEqual(text.unescape_string_literal('\"abc\"'), 'abc')\n    self.assertEqual(text.unescape_string_literal(\"'abc'\"), 'abc')\n    self.assertEqual(text.unescape_string_literal('\"a \\\\\"bc\\\\\"\"'), 'a \"bc\"')\n    self.assertEqual(text.unescape_string_literal(\"'\\\\'ab\\\\' c'\"), \"'ab' c\")\n    with self.assertRaises(ValueError):\n        text.unescape_string_literal('abc')\n    with self.assertRaises(ValueError):\n        text.unescape_string_literal('\"abc')\n"], "sample_189": ["    def test_default_key_func(self):\n        cache = caches['default']\n        key = cache.make_key('test', version=1)\n        self.assertEqual(key, 'test:1:test')\n"], "sample_191": ["def test_iter_modules_and_files_zip_import(self):\n    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:\n        zip_filename = Path(tmp_file.name)\n        self.addCleanup(zip_filename.unlink)\n\n        with zipfile.ZipFile(str(zip_filename), 'w') as zip_file:\n            zip_file.writestr('test.py', b'print(\"Hello, World!\")')\n\n        with extend_sys_path(str(zip_filename)):\n            self.import_and_cleanup('test')\n            self.assertFileFound(zip_filename)\n"], "sample_192": ["def test_formset_absolute_max(self):\n    # Test that the absolute_max parameter is enforced.\n    ChoiceFormSetAbsoluteMax = formset_factory(Choice, max_num=10, absolute_max=5)\n    formset = self.make_choiceformset([('1', '1'), ('2', '2'), ('3', '3'), ('4', '4'), ('5', '5'), ('6', '6')], formset_class=ChoiceFormSetAbsoluteMax)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.errors), 0)\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertIn('Please submit 5 or fewer forms.', str(formset.non_form_errors()[0]))\n"], "sample_193": ["    def test_related_field_deconstruction(self):\n        \"\"\"\n        Test that a related field (ForeignKey, ManyToManyField) can be deconstructed.\n        \"\"\"\n        class Model(models.Model):\n            other = models.ForeignKey('Other', on_delete=models.CASCADE)\n\n        field = Model._meta.get_field('other')\n        name, path, args, kwargs = field.deconstruct()\n        new_field = field.__class__(*args, **kwargs)\n        self.assertEqual(field.remote_field.model, new_field.remote_field.model)\n"], "sample_194": ["    def test_check_constraint(self):\n        # Create a model with a check constraint.\n        class Product(models.Model):\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(price__gte=0), name='price_gte_0'),\n                ]\n\n        # Create the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Product)\n\n        # Try to insert an invalid value.\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(price=-1)\n\n        # Insert a valid value.\n        Product.objects.create(price=1)\n\n        # Drop the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.delete_model(Product)\n"], "sample_197": ["def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t - self.oneweek, self.t, depth=1), '1 week')\n        self.assertEqual(timesince(self.t - self.oneweek - self.oneday, self.t, depth=2), '1 week, 1 day')\n        self.assertEqual(timesince(self.t - self.oneweek - self.oneday - self.onehour, self.t, depth=3), '1 week, 1 day, 1 hour')\n"], "sample_200": ["def test_sanitize_address(self):\n    # Test that sanitize_address handles different types of input\n    self.assertEqual(sanitize_address('test@example.com', 'utf-8'), 'test@example.com')\n    self.assertEqual(sanitize_address(('Name', 'test@example.com'), 'utf-8'), 'Name <test@example.com>')\n    self.assertEqual(sanitize_address('Name <test@example.com>', 'utf-8'), 'Name <test@example.com>')\n\n    # Test that sanitize_address raises an error for invalid addresses\n    with self.assertRaises(ValueError):\n        sanitize_address('invalid', 'utf-8')\n\n    # Test that sanitize_address handles non-ASCII characters\n    self.assertEqual(sanitize_address(('N\u00e4m\u00e9', 'test@example.com'), 'utf-8'), '=?utf-8?q?N%C3%A4m%C3%A9?= <test@example.com>')\n\n    # Test that sanitize_address handles newline characters in the address\n    with self.assertRaises(ValueError):\n        sanitize_address(('Name\\nwith newline', 'test@example.com'), 'utf-8')\n"], "sample_201": ["def test_legacy_hash(self):\n    storage = self.storage_class(FakeRequest())\n    value = 'legacy_value'\n    legacy_hash = storage._legacy_hash(value)\n    self.assertEqual(len(legacy_hash), 40)  # SHA1 hash length\n\n    # Test that the _legacy_decode method correctly decodes a value with a valid legacy hash\n    data = f'{legacy_hash}${value}'\n    decoded_value = storage._legacy_decode(data)\n    self.assertEqual(decoded_value, value)\n\n    # Test that the _legacy_decode method returns None for a value with an invalid legacy hash\n    invalid_data = f'invalid_hash${value}'\n    decoded_value = storage._legacy_decode(invalid_data)\n    self.assertIsNone(decoded_value)\n"], "sample_202": ["def test_process_messages_with_nested_data(self):\n    decoder = MessageDecoder()\n    data = [\n        ['__json_message', 1, constants.INFO, 'Test message'],\n        {'nested': ['__json_message', 0, constants.ERROR, 'Nested message']},\n    ]\n    processed_data = decoder.process_messages(data)\n    self.assertIsInstance(processed_data[0], Message)\n    self.assertIsInstance(processed_data[1]['nested'], Message)\n"], "sample_203": ["def test_regex_validator(self):\n    # Test regex validator with string\n    validator = validators.RegexValidator(r'^[a-zA-Z]+$')\n    with self.assertRaises(ValidationError):\n        validator('123')\n\n    # Test regex validator with compiled regex\n    validator = validators.RegexValidator(re.compile(r'^[a-zA-Z]+$'))\n    with self.assertRaises(ValidationError):\n        validator('123')\n\n    # Test regex validator with flags\n    validator = validators.RegexValidator(r'^[a-z]+$', re.IGNORECASE)\n    try:\n        validator('ABC')\n    except ValidationError:\n        self.fail(\"RegexValidator failed with flags\")\n\n    # Test regex validator with message\n    validator = validators.RegexValidator(r'^[a-zA-Z]+$', message='Invalid input')\n    with self.assertRaisesMessage(ValidationError, 'Invalid input'):\n        validator('123')\n"], "sample_204": ["    def test_load_migrations(self):\n        \"\"\"\n        Tests that the loader can load migrations.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.assertEqual(loader.disk_migrations, {})\n        self.assertEqual(loader.applied_migrations, {})\n"], "sample_205": ["def test_validation_error_hash(self):\n    error1 = ValidationError('Error message', code='error_code')\n    error2 = ValidationError('Error message', code='error_code')\n    self.assertEqual(hash(error1), hash(error2))\n\n    error3 = ValidationError('Error message', code='different_code')\n    self.assertNotEqual(hash(error1), hash(error3))\n\n    error4 = ValidationError('Different message', code='error_code')\n    self.assertNotEqual(hash(error1), hash(error4))\n"], "sample_206": ["def test_file_field_with_upload_to_callable(self):\n        return f'path/to/{filename}'\n\n    class Document(models.Model):\n        file = models.FileField(upload_to=upload_to)\n\n    with isolate_apps('file_field'):\n        doc = Document(file='example.txt')\n        self.assertEqual(doc.file.upload_to, upload_to)\n        self.assertEqual(doc.file.generate_filename(doc, 'example.txt'), 'path/to/example.txt')\n"], "sample_208": ["def test_alter_check_constraint(self):\n    \"\"\"Tests adding and removing CheckConstraints.\"\"\"\n    before = self.make_project_state([self.author_name])\n    after = self.make_project_state([self.author_name_check_constraint])\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name_contains_bob')\n    # Test removing CheckConstraints.\n    before, after = after, before\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RemoveConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name_contains_bob')\n"], "sample_210": ["def test_as_view_passes_kwargs_to_init(self):\n    class TestView(View):\n            super().__init__(**kwargs)\n            self.init_kwargs = kwargs\n\n            return HttpResponse('Test view')\n\n    view = TestView.as_view(test_kwarg='test_value')\n    request = self.rf.get('/')\n    response = view(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(view.view_initkwargs, {'test_kwarg': 'test_value'})\n    self.assertEqual(view.view_class().init_kwargs, {'test_kwarg': 'test_value'})\n"], "sample_211": ["def test_as_view(self):\n    view = View.as_view()\n    self.assertTrue(callable(view))\n    self.assertEqual(view.view_class, View)\n    self.assertEqual(view.view_initkwargs, {})\n\n    # Test that as_view() raises an error for invalid keyword arguments.\n    with self.assertRaises(TypeError):\n        View.as_view(get='invalid')\n    with self.assertRaises(TypeError):\n        View.as_view(invalid='keyword')\n"], "sample_213": ["    def test_field_file(self):\n        # Create a model with a FileField\n        from .models import MyModel\n        obj = MyModel.objects.create(file='example.txt')\n\n        # Get the FieldFile instance\n        field_file = obj.file\n\n        # Check that the FieldFile has the correct properties\n        self.assertEqual(field_file.name, 'example.txt')\n        self.assertEqual(field_file.path, os.path.join(temp_storage_location, 'example.txt'))\n        self.assertEqual(field_file.url, '/media/example.txt')\n        self.assertEqual(field_file.size, 0)\n\n        # Check that the FieldFile can be opened and read\n        with field_file.open() as f:\n            self.assertEqual(f.read(), b'')\n"], "sample_212": ["    def test_session_middleware_process_request(self):\n        request = HttpRequest()\n        middleware = SessionMiddleware()\n        middleware.process_request(request)\n        self.assertIsNotNone(request.session)\n"], "sample_216": ["def test_field_references(self):\n    model_tuple = (\"testapp\", \"Author\")\n    field = models.ForeignKey(\"testapp.Publisher\", models.CASCADE)\n    reference_model_tuple = (\"testapp\", \"Publisher\")\n\n    # Test with no reference field name\n    reference = field_references(model_tuple, field, reference_model_tuple)\n    self.assertIsInstance(reference, FieldReference)\n    self.assertEqual(reference.to, (field.remote_field, None))\n    self.assertIsNone(reference.through)\n\n    # Test with reference field name\n    reference_field_name = \"name\"\n    reference = field_references(model_tuple, field, reference_model_tuple, reference_field_name)\n    self.assertIsInstance(reference, FieldReference)\n    self.assertEqual(reference.to, (field.remote_field, [reference_field_name]))\n    self.assertIsNone(reference.through)\n\n    # Test with through model\n    field = models.ManyToManyField(\"testapp.Publisher\", through=\"testapp.Contract\")\n    reference = field_references(model_tuple, field, reference_model_tuple)\n    self.assertIsInstance(reference, FieldReference)\n    self.assertIsNone(reference.to)\n    self.assertEqual(reference.through, (field.remote_field, [\"author\", \"publisher\"]))\n\n    # Test with no match\n    field = models.CharField(max_length=100)\n    reference = field_references(model_tuple, field, reference_model_tuple)\n    self.assertFalse(reference)\n"], "sample_220": ["def test_set_cookie_max_age(self):\n    response = HttpResponse()\n    max_age = 3600  # 1 hour\n    expires = datetime.utcnow() + timedelta(seconds=max_age)\n    response.set_cookie('test', 'value', max_age=max_age)\n    self.assertEqual(response.cookies['test']['max-age'], str(max_age))\n    self.assertEqual(response.cookies['test']['expires'], http_date(expires.timestamp()))\n"], "sample_222": ["def test_locks(self):\n    with tempfile.TemporaryFile() as f:\n        # Test exclusive lock\n        self.assertTrue(locks.lock(f, locks.LOCK_EX))\n        self.assertFalse(locks.lock(f, locks.LOCK_EX))  # Lock is already held\n        locks.unlock(f)\n\n        # Test shared lock\n        self.assertTrue(locks.lock(f, locks.LOCK_SH))\n        self.assertTrue(locks.lock(f, locks.LOCK_SH))  # Shared lock can be acquired multiple times\n        locks.unlock(f)\n\n        # Test non-blocking lock\n        self.assertTrue(locks.lock(f, locks.LOCK_EX | locks.LOCK_NB))\n        self.assertFalse(locks.lock(f, locks.LOCK_EX | locks.LOCK_NB))  # Lock is already held\n        locks.unlock(f)\n"], "sample_226": ["    def test_create_test_db(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        with mock.patch.object(creation, '_create_test_db') as mock_create:\n            mock_create.return_value = 'test_database'\n            db_name = creation.create_test_db(verbosity=0)\n            self.assertEqual(db_name, 'test_database')\n            self.assertEqual(test_connection.settings_dict['NAME'], 'test_database')\n"], "sample_227": ["    def test_generic_relation(self):\n        bookmark = Bookmark.objects.create(url='https://www.example.com')\n        tag = TaggedItem.objects.create(tag='example', content_object=bookmark)\n        self.assertEqual(bookmark.tags.all()[0], tag)\n"], "sample_228": ["def test_formset_management_form_validation(self):\n    formset = self.make_choiceformset(formset_data=[('Choice 1', '10'), ('Choice 2', '20')])\n    # Make sure the management form is valid.\n    self.assertTrue(formset.management_form.is_valid())\n\n    # Now, make the management form invalid by changing the TOTAL_FORMS value.\n    data = formset.data.copy()\n    data['choices-TOTAL_FORMS'] = '3'\n    formset = self.make_choiceformset(data=data)\n    self.assertFalse(formset.management_form.is_valid())\n    self.assertEqual(formset.management_form.errors, {'TOTAL_FORMS': ['ManagementForm data is missing or has been tampered with']})\n"], "sample_231": ["    def test_cleansed_multivaluedict(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        request = RequestFactory().post('/test', {'foo': 'bar'})\n        request.sensitive_post_parameters = ['foo']\n        multivaluedict = request.POST.copy()\n        cleansed_dict = reporter_filter.get_cleansed_multivaluedict(request, multivaluedict)\n        self.assertEqual(cleansed_dict['foo'], reporter_filter.cleansed_substitute)\n"], "sample_233": ["def test_token_generator_timeout(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'password')\n    token_generator = MockedPasswordResetTokenGenerator(datetime.now())\n    token = token_generator.make_token(user)\n    self.assertTrue(token_generator.check_token(user, token))\n\n    # Move the clock forward by the token timeout plus one second\n    now = datetime.now() + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n    token_generator = MockedPasswordResetTokenGenerator(now)\n    self.assertFalse(token_generator.check_token(user, token))\n"], "sample_235": ["def test_on_commit_nested(self):\n    \"\"\"Callbacks registered in nested transactions are called after the outermost transaction commits.\"\"\"\n    with transaction.atomic():\n        self.do(1)\n        with transaction.atomic():\n            self.do(2)\n    self.assertDone([1, 2])\n"], "sample_237": ["def test_check_user_model_required_fields_is_not_list(self):\n    @self.apps.get_model('auth_tests', 'CustomUserNonListRequiredFields')\n    class CustomUserNonListRequiredFields(AbstractBaseUser):\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = 'non_list'\n\n        username = models.CharField(max_length=10, unique=True)\n        non_list = models.CharField(max_length=10)\n\n    errors = check_user_model(app_configs=self.apps.app_configs)\n    expected_errors = [\n        checks.Error(\n            \"'REQUIRED_FIELDS' must be a list or tuple.\",\n            obj=CustomUserNonListRequiredFields,\n            id='auth.E001',\n        ),\n    ]\n    self.assertEqual(errors, expected_errors)\n"], "sample_239": ["def test_formset_management_form_validation(self):\n    # Test that the management form validates the number of forms.\n    formset = self.make_choiceformset(formset_data=[('Choice 1', '10'), ('Choice 2', '20')])\n    formset.management_form.cleaned_data['TOTAL_FORMS'] = '3'\n    with self.assertRaises(ValidationError):\n        formset.management_form.clean()\n"], "sample_240": ["def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'testpassword')\n    generator = PasswordResetTokenGenerator()\n    timestamp = generator._num_seconds(datetime(2022, 1, 1))\n    token = generator._make_token_with_timestamp(user, timestamp)\n    self.assertRegex(token, r'^[0-9a-z]{6}-[0-9a-f]{13}$')\n"], "sample_241": ["def test_resolve_ref(self):\n    qs = Company.objects.annotate(\n        employee_count=Count('employees'),\n        employee_count_squared=F('employee_count') ** 2,\n    )\n    # Test that we can resolve a reference to an annotation.\n    self.assertEqual(\n        qs.resolve_ref('employee_count', allow_joins=False).target,\n        qs.annotations['employee_count'].output_field,\n    )\n    # Test that we can resolve a reference to an annotation that depends on another annotation.\n    self.assertEqual(\n        qs.resolve_ref('employee_count_squared', allow_joins=False).target,\n        qs.annotations['employee_count_squared'].output_field,\n    )\n    # Test that we can't resolve a reference to a non-existent field or annotation.\n    with self.assertRaises(FieldError):\n        qs.resolve_ref('non_existent_field', allow_joins=False)\n"], "sample_244": ["def test_formset_management_form_validation(self):\n    formset = self.make_choiceformset(formset_data=[('Choice 1', '10'), ('Choice 2', '20')])\n    # Make sure the management form is valid.\n    self.assertTrue(formset.management_form.is_valid())\n\n    # Now, make the management form invalid by changing the TOTAL_FORMS value.\n    data = formset.data.copy()\n    data['choices-TOTAL_FORMS'] = 'abc'\n    formset = self.make_choiceformset(formset_data=[('Choice 1', '10'), ('Choice 2', '20')], data=data)\n    self.assertFalse(formset.management_form.is_valid())\n    self.assertEqual(formset.management_form.errors, {'TOTAL_FORMS': ['Enter a whole number.']})\n"], "sample_245": ["def test_makemessages_with_no_wrap(self):\n    with override_settings(LOCALE_PATHS=[self.test_dir]):\n        output, po_contents = self._run_makemessages(no_wrap=True)\n    self.assertMsgId(\"This is a long string that should not be wrapped.\", po_contents)\n    self.assertNotRegex(po_contents, r'\"This is a long string that should not be wrapped.\"\\n\"  \"')\n"], "sample_248": ["def test_handle_command_option(self):\n    with captured_stdout() as stdout:\n        call_command('shell', '--command', self.script_globals)\n    self.assertEqual(stdout.getvalue().strip(), 'True')\n\n    with captured_stdout() as stdout:\n        call_command('shell', '--command', self.script_with_inline_function)\n    self.assertEqual(stdout.getvalue().strip(), __version__)\n"], "sample_249": ["    def test_create_test_db(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        with mock.patch.object(creation, '_create_test_db') as _create_test_db:\n            _create_test_db.return_value = 'test_database'\n            db_name = creation.create_test_db(verbosity=0)\n            self.assertEqual(db_name, 'test_database')\n            self.assertEqual(test_connection.settings_dict['NAME'], 'test_database')\n"], "sample_253": ["    def test_common_roots(self):\n        paths = [\n            Path('/home/user/project/app1/models.py'),\n            Path('/home/user/project/app1/views.py'),\n            Path('/home/user/project/app2/models.py'),\n            Path('/home/user/project/app2/views.py'),\n        ]\n        expected_roots = (\n            Path('/home/user/project'),\n        )\n        self.assertEqual(autoreload.common_roots(paths), expected_roots)\n"], "sample_256": ["    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n"], "sample_255": ["def test_handle_one_request(self):\n    request = self.request_factory.get('/test/')\n    handler = WSGIRequestHandler(Stub(), BytesIO(), BytesIO())\n    handler.server = Stub(get_app=lambda: None)\n    handler.raw_requestline = b'GET /test/ HTTP/1.1\\r\\n'\n    handler.rfile = BytesIO(b'Host: example.com\\r\\n\\r\\n')\n    handler.handle_one_request()\n    self.assertEqual(handler.close_connection, True)\n"], "sample_258": ["def test_connect_with_dispatch_uid(self):\n        pass\n\n    a_signal.connect(receiver, dispatch_uid='test_uid')\n    self.assertEqual(len(a_signal.receivers), 1)\n\n    # Connecting the same receiver again with the same dispatch_uid should not add it again.\n    a_signal.connect(receiver, dispatch_uid='test_uid')\n    self.assertEqual(len(a_signal.receivers), 1)\n\n    # But connecting the same receiver with a different dispatch_uid should add it again.\n    a_signal.connect(receiver, dispatch_uid='test_uid2')\n    self.assertEqual(len(a_signal.receivers), 2)\n"], "sample_261": ["def test_parse_date(self):\n    self.assertEqual(parse_date('2022-02-28'), date(2022, 2, 28))\n    self.assertIsNone(parse_date('2022-02'))  # incomplete date\n    self.assertIsNone(parse_date('2022-02-30'))  # invalid day for month\n    self.assertIsNone(parse_date('2022-13-01'))  # invalid month\n    self.assertIsNone(parse_date('2022-02-28T12:00'))  # not a date string\n"], "sample_262": ["def test_lazy(self):\n    # Test that lazy works on functions with multiple arguments\n        return a + b + c\n\n    lazy_func = lazy(my_func, int)\n    self.assertEqual(lazy_func(1, 2, 3), 6)\n\n    # Test that lazy works on functions with no arguments\n        return 42\n\n    lazy_func_no_args = lazy(my_func_no_args, int)\n    self.assertEqual(lazy_func_no_args(), 42)\n"], "sample_264": ["def test_legacy_hash(self):\n    storage = self.storage_class(FakeRequest())\n    value = get_random_string()\n    legacy_hash = storage._legacy_hash(value)\n    self.assertEqual(len(legacy_hash), 40)  # SHA1 hash length\n\n    # Test that the legacy hash is correctly validated\n    data = f\"{legacy_hash}${value}\"\n    decoded_data = storage._legacy_decode(data)\n    self.assertEqual(decoded_data, value)\n\n    # Test that an invalid legacy hash is not validated\n    invalid_data = f\"invalid${value}\"\n    decoded_data = storage._legacy_decode(invalid_data)\n    self.assertIsNone(decoded_data)\n"], "sample_265": ["def test_get_templatetag_libraries(self):\n    # Test that get_templatetag_libraries returns a dictionary of libraries\n    # from installed applications and the supplied custom_libraries argument.\n    engine = self.engine_class({\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'libraries': {\n                'custom_tags': 'path.to.custom.tags',\n            },\n        },\n    })\n\n    libraries = engine.get_templatetag_libraries({\n        'custom_tags2': 'path.to.custom.tags2',\n    })\n\n    self.assertIn('custom_tags', libraries)\n    self.assertIn('custom_tags2', libraries)\n\n    # Test that an InvalidTemplateLibrary is raised when a library cannot be loaded\n    with self.assertRaises(InvalidTemplateLibrary):\n        engine.get_templatetag_libraries({\n            'invalid_tags': 'path.to.invalid.tags',\n        })\n"], "sample_266": ["    def test_load(self):\n        \"\"\"\n        Makes sure the loader can load the migrations for an app from the disk.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.assertEqual(loader.get_migration('migrations', '0001_initial').name, '0001_initial')\n        self.assertEqual(loader.get_migration('migrations', '0002_second').name, '0002_second')\n"], "sample_269": ["    def test_catalog(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = gettext.NullTranslations()\n        catalog.locale = 'en'\n        context = catalog.get_context_data()\n        self.assertIn('catalog', context)\n        self.assertIn('formats', context)\n        self.assertIn('plural', context)\n"], "sample_271": ["def test_iter_modules_and_files_with_zip_file(self):\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        zip_filename = Path(tmp_dir) / 'test.zip'\n        with zipfile.ZipFile(zip_filename, 'w') as zip_file:\n            zip_file.writestr('test.py', b'')\n        sys.path.insert(0, str(zip_filename))\n        self.addCleanup(sys.path.remove, str(zip_filename))\n        self.import_and_cleanup('test')\n        self.assertFileFound(zip_filename)\n"], "sample_272": ["def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Tests the migration plan with replacements.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n    # Create a fake migration history\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    # Load the graph\n    graph = MigrationGraph()\n    graph.add_node((\"migrations\", \"0001_initial\"), None)\n    graph.add_node((\"migrations\", \"0002_second\"), None)\n    graph.add_node((\"migrations\", \"0003_third\"), [(\"migrations\", \"0002_second\")])\n    graph.add_node((\"migrations\", \"0004_merge\"), [(\"migrations\", \"0003_third\")])\n    # Add a replacement migration\n    graph.add_node((\"migrations\", \"0005_replace\"), None)\n    graph.add_replacement((\"migrations\", \"0005_replace\"), [(\"migrations\", \"0004_merge\")])\n    # Make a plan to go to 0005_replace\n    plan = executor.migration_plan([(\"migrations\", \"0005_replace\")])\n    self.assertEqual(len(plan), 2)\n    self.assertEqual(plan[0][0].name, \"0004_merge\")\n    self.assertEqual(plan[1][0].name, \"0005_replace\")\n"], "sample_276": ["def test_model_detail_view(self):\n    response = self.client.get(reverse('django-admindocs-model-detail', args=['sites', 'site']))\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, 'admin_doc/model_detail.html')\n    self.assertContains(response, 'Site')\n    self.assertContains(response, 'id')\n    self.assertContains(response, 'domain')\n    self.assertContains(response, 'name')\n"], "sample_277": ["def test_combine_Q_objects(self):\n    q1 = Q(a=1)\n    q2 = Q(b=2)\n    combined = q1 & q2\n    self.assertEqual(combined.children, [q1, q2])\n    self.assertEqual(combined.connector, Q.AND)\n\n    combined = q1 | q2\n    self.assertEqual(combined.children, [q1, q2])\n    self.assertEqual(combined.connector, Q.OR)\n\n    combined = ~q1\n    self.assertEqual(combined.children, [q1])\n    self.assertTrue(combined.negated)\n"], "sample_278": ["    def test_combine(self):\n        q1 = Q(x=1)\n        q2 = Q(y=2)\n        combined = q1 & q2\n        self.assertEqual(combined.children, [q1, q2])\n        self.assertEqual(combined.connector, Q.AND)\n\n        combined = q1 | q2\n        self.assertEqual(combined.children, [q1, q2])\n        self.assertEqual(combined.connector, Q.OR)\n"], "sample_280": ["def test_aggregate_with_filter(self):\n    # Test that an aggregate with a filter clause works correctly.\n    books = Book.objects.annotate(\n        num_authors=Count('authors', filter=Q(authors__name='Adrian Holovaty'))\n    )\n    self.assertEqual(books.get(isbn='159059725').num_authors, 1)\n    self.assertEqual(books.get(isbn='067232959').num_authors, 0)\n\n    # Test that an aggregate with a filter clause and a distinct argument works correctly.\n    books = Book.objects.annotate(\n        num_authors=Count('authors', filter=Q(authors__name='Adrian Holovaty'), distinct=True)\n    )\n    self.assertEqual(books.get(isbn='159059725').num_authors, 1)\n    self.assertEqual(books.get(isbn='067232959').num_authors, 0)\n"], "sample_281": ["def test_get_paginator(self):\n    request = self.factory.get(self.url, self.opts)\n    request.user = self.user\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    paginator = response.context_data['paginator']\n    self.assertIsInstance(paginator, type(self.site._registry[Answer].get_paginator(request)))\n"], "sample_282": ["def test_boundfield_widget_type(self):\n    form = ComplexFieldForm()\n    bound_field = form['field1']\n    self.assertEqual(bound_field.widget_type, 'complexmultiwidget')\n"], "sample_283": ["def test_settings_to_cmd_args_env_with_ssl(self):\n    settings_dict = {\n        'NAME': 'mydb',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'OPTIONS': {\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/root/cert',\n            'sslcert': '/path/to/cert',\n            'sslkey': '/path/to/key',\n        }\n    }\n    args, env = self.settings_to_cmd_args_env(settings_dict)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/root/cert',\n        'PGSSLCERT': '/path/to/cert',\n        'PGSSLKEY': '/path/to/key',\n    })\n"], "sample_284": ["def test_hashed_files_mixin_max_post_process_passes(self):\n    # Test that max_post_process_passes is respected.\n    storage.staticfiles_storage.max_post_process_passes = 1\n\n    with override_settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.HashedFilesMixin'):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            settings.STATIC_ROOT = tmpdir\n            call_command('collectstatic', verbosity=0, interactive=False)\n\n            # Check that an exception was raised and logged.\n            self.assertEqual(len(storage.staticfiles_storage.hashed_files), 0)\n            self.assertPostCondition()\n"], "sample_285": ["    def test_find_all(self):\n        class TestFinder(BaseFinder):\n                return ['path1', 'path2'] if all else 'path1'\n\n        finder = TestFinder()\n        self.assertEqual(finder.find('test', all=True), ['path1', 'path2'])\n        self.assertEqual(finder.find('test', all=False), 'path1')\n"], "sample_290": ["def test_suggest_name(self):\n    migration = Migration(\"testapp\", \"0001_initial\")\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Author\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        ),\n    ]\n    self.assertEqual(migration.suggest_name(), \"initial\")\n\n    migration.operations = [\n        migrations.AddField(\n            model_name=\"author\",\n            name=\"new_field\",\n            field=models.CharField(max_length=200),\n        ),\n    ]\n    self.assertRegex(migration.suggest_name(), r\"^auto_\\d{14}$\")\n\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Author\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.CreateModel(\n            name=\"Book\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n        ),\n    ]\n    self.assertEqual(migration.suggest_name(), \"author_and_book\")\n\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Author\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.CreateModel(\n            name=\"Book\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.CreateModel(\n            name=\"Publisher\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        ),\n    ]\n    self.assertEqual(migration.suggest_name(), \"author_and_more\")\n"], "sample_291": ["def test_view_as_view(self):\n    view = View.as_view()\n    self.assertIsInstance(view, type(lambda: None))\n    self.assertEqual(view.__name__, 'View')\n    self.assertEqual(view.__doc__, View.__doc__)\n    self.assertEqual(view.__module__, View.__module__)\n\n    request = self.rf.get('/')\n    response = view(request)\n    self.assertIsInstance(response, HttpResponseNotAllowed)\n\n    # Test that the view instance is created only once.\n    view2 = View.as_view()\n    self.assertIs(view.view_class, view2.view_class)\n    self.assertEqual(view.view_initkwargs, view2.view_initkwargs)\n"], "sample_292": ["def test_csrf_token_rotation(self):\n    req = self._get_GET_csrf_cookie_request()\n    get_token(req)\n    original_token = req.META['CSRF_COOKIE']\n    rotate_token(req)\n    self.assertNotEqual(original_token, req.META['CSRF_COOKIE'])\n    self.assertTrue(equivalent_tokens(original_token, req.META['CSRF_COOKIE'][:CSRF_TOKEN_LENGTH]))\n"], "sample_293": ["def test_get_resolver(self):\n    resolver = get_resolver()\n    self.assertIsInstance(resolver, URLResolver)\n    self.assertEqual(resolver.urlconf_name, settings.ROOT_URLCONF)\n\n    # Test that get_resolver returns the same instance when called multiple times.\n    resolver2 = get_resolver()\n    self.assertIs(resolver, resolver2)\n\n    # Test that get_resolver returns a new instance when the urlconf is different.\n    resolver3 = get_resolver('urlpatterns_reverse.urls')\n    self.assertIsNot(resolver, resolver3)\n"], "sample_296": ["def test_cookie_storage_max_cookie_size(self):\n    storage = self.storage_class(FakeRequest())\n    response = FakeResponse()\n\n    # Set max_cookie_size to a small value to make it easier to test.\n    storage.max_cookie_size = 100\n\n    messages = [\n        Message(constants.INFO, 'Test message 1'),\n        Message(constants.INFO, 'Test message 2'),\n        Message(constants.INFO, 'Test message 3'),\n    ]\n\n    unstored_messages = storage._store(messages, response)\n\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(stored_cookie_messages_count(storage, response), 2)\n"], "sample_298": ["def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'password')\n    token_generator = PasswordResetTokenGenerator()\n    timestamp = token_generator._num_seconds(datetime(2022, 1, 1))\n    token = token_generator._make_token_with_timestamp(user, timestamp)\n    self.assertRegex(token, r'^[0-9a-z]{6}-[0-9a-f]{13}$')\n"], "sample_301": ["    def test_common_roots(self):\n        paths = [\n            Path('/home/user/project/app1/models.py'),\n            Path('/home/user/project/app1/views.py'),\n            Path('/home/user/project/app2/models.py'),\n            Path('/home/user/project/app2/views.py'),\n        ]\n        roots = autoreload.common_roots(paths)\n        self.assertEqual(roots, (Path('/home/user/project'),))\n"], "sample_302": ["def test_settings_to_cmd_args_env(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydb',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'passfile': '/path/to/passfile',\n            'service': 'myservice',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/sslrootcert',\n            'sslcert': '/path/to/sslcert',\n            'sslkey': '/path/to/sslkey',\n        }\n    }\n    parameters = ['--extra-param']\n    args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb', '--extra-param'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSERVICE': 'myservice',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/sslrootcert',\n        'PGSSLCERT': '/path/to/sslcert',\n        'PGSSLKEY': '/path/to/sslkey',\n        'PGPASSFILE': '/path/to/passfile',\n    })\n"], "sample_303": ["def test_runshell_raises_not_implemented_error(self):\n    with self.assertRaises(NotImplementedError):\n        self.client.runshell(parameters={})\n"], "sample_304": ["def test_deconstructible(self):\n    # Test deconstructible validators\n    for validator in [\n        RegexValidator(),\n        URLValidator(),\n        EmailValidator(),\n        validate_integer,\n        validate_email,\n        validate_image_file_extension,\n        validate_ipv4_address,\n        validate_ipv6_address,\n        validate_ipv46_address,\n        validate_slug,\n        validate_unicode_slug,\n        int_list_validator(),\n        MaxValueValidator(10),\n        MinValueValidator(-10),\n        MaxLengthValidator(10),\n        MinLengthValidator(10),\n        DecimalValidator(max_digits=2, decimal_places=2),\n        FileExtensionValidator(['txt']),\n        ProhibitNullCharactersValidator(),\n    ]:\n        self.assertEqual(validator.__class__, type(reconstruct(validator)))\n"], "sample_306": ["def test_parse_date(self):\n    self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n    self.assertIsNone(parse_date('2022-01'))  # incomplete date\n    self.assertIsNone(parse_date('20220101'))  # invalid format\n    self.assertIsNone(parse_date('2022-02-30'))  # invalid day for month\n    self.assertIsNone(parse_date('2022-13-01'))  # invalid month\n"], "sample_309": ["    def test_urlsafe_base64_encode(self):\n        s = b'Hello, World!'\n        encoded = urlsafe_base64_encode(s)\n        self.assertEqual(encoded, 'SGVsbG8sIFdvcmxkIQ')\n"], "sample_310": ["def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_user_list'), 'List')\n    self.assertEqual(get_return_data_type('get_user_count'), 'Integer')\n    self.assertEqual(get_return_data_type('get_user'), '')\n"], "sample_312": ["def test_node_add_squash(self):\n    node3_children = [('c', 3), ('d', 4)]\n    node3 = Node(node3_children)\n    self.node1.add(node3, 'DEFAULT')\n    self.assertEqual(len(self.node1.children), 4)\n    self.assertIn(('a', 1), self.node1.children)\n    self.assertIn(('b', 2), self.node1.children)\n    self.assertIn(('c', 3), self.node1.children)\n    self.assertIn(('d', 4), self.node1.children)\n"], "sample_313": ["    def test_reset_loaders_called_on_template_change(self):\n        # Mock the reset_loaders function to track if it's called\n        with mock.patch('django.template.autoreload.reset_loaders') as mock_reset:\n            # Get the template directories\n            template_dirs = autoreload.get_template_directories()\n            \n            # Simulate a file change in one of the template directories\n            file_path = template_dirs.pop() / 'test_template.html'\n            autoreload.template_changed(None, file_path)\n            \n            # Assert that reset_loaders was called\n            self.assertTrue(mock_reset.called)\n"], "sample_314": ["    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n"], "sample_315": ["    def test_process_request(self):\n        # Create a request with a language code in the URL\n        rf = RequestFactory()\n        request = rf.get('/nl/test/')\n\n        # Set up the middleware\n        middleware = LocaleMiddleware()\n\n        # Call process_request\n        middleware.process_request(request)\n\n        # Check that the language code has been activated\n        self.assertEqual(translation.get_language(), 'nl')\n        self.assertEqual(request.LANGUAGE_CODE, 'nl')\n"], "sample_316": ["def test_get_image_dimensions_with_corrupted_image(self):\n    # Create a corrupted image file\n    corrupted_image = BytesIO(b'Invalid image data')\n\n    # Test that get_image_dimensions returns (None, None) for corrupted images\n    self.assertEqual(images.get_image_dimensions(corrupted_image), (None, None))\n\n    # Test that the file is closed after attempting to read its dimensions\n    self.assertTrue(corrupted_image.closed)\n"], "sample_318": ["def test_resolver_match_repr(self):\n    resolver_match = ResolverMatch(\n        func=views.empty_view,\n        args=(),\n        kwargs={'arg1': '42', 'arg2': '37'},\n        url_name='normal-view',\n        app_names=['testapp'],\n        namespaces=['test-ns1'],\n        route='/normal/42/37/',\n    )\n    self.assertEqual(\n        repr(resolver_match),\n        \"ResolverMatch(func='urlpatterns_reverse.views.empty_view', args=(), \"\n        \"kwargs={'arg1': '42', 'arg2': '37'}, url_name='normal-view', \"\n        \"app_names=['testapp'], namespaces=['test-ns1'], route='/normal/42/37/')\"\n    )\n"], "sample_321": ["def test_csrf_token_length(self):\n    req = self._get_GET_no_csrf_cookie_request()\n    token = get_token(req)\n    self.assertEqual(len(token), CSRF_TOKEN_LENGTH)\n"], "sample_322": ["def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Test that replaced migrations are properly handled in the migration plan.\n    \"\"\"\n    # Create a mock migration graph with a replaced migration\n    graph = MigrationGraph()\n    graph.add_node((\"app\", \"0001_initial\"), None)\n    graph.add_node((\"app\", \"0002_second\"), [(\"app\", \"0001_initial\")])\n    graph.add_node((\"app\", \"0003_third\"), [(\"app\", \"0002_second\")])\n    graph.add_replacement((\"app\", \"0002_second\"), (\"app\", \"0002_2_second\"))\n    graph.add_replacement((\"app\", \"0003_third\"), (\"app\", \"0003_2_third\"))\n\n    # Create a mock migration recorder\n    recorder = MigrationRecorder(connection)\n\n    # Create a mock migration loader\n    loader = mock.Mock()\n    loader.graph = graph\n    loader.applied_migrations = {(\"app\", \"0001_initial\"): None}\n\n    # Create a migration executor\n    executor = MigrationExecutor(connection, progress_callback=None)\n    executor.loader = loader\n    executor.recorder = recorder\n\n    # Get the migration plan\n    targets = [(\"app\", \"0003_2_third\")]\n    plan = executor.migration_plan(targets)\n\n    # Check that the replaced migrations are in the plan\n    self.assertEqual(len(plan), 3)\n    self.assertEqual(plan[0][0].name, \"0002_2_second\")\n    self.assertEqual(plan[1][0].name, \"0003_2_third\")\n"], "sample_323": ["def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Test that the migration plan includes replacements.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n    # Create a fake migration history\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    # Load the graph with a replacement migration\n    graph = MigrationGraph()\n    graph.add_node((\"migrations\", \"0001_initial\"), None)\n    graph.add_node((\"migrations\", \"0002_second\"), None)\n    graph.add_node((\"migrations\", \"0003_third\"), None)\n    graph.add_dependency((\"migrations\", \"0003_third\"), (\"migrations\", \"0002_second\"))\n    graph.add_dependency((\"migrations\", \"0003_third\"), (\"migrations\", \"0001_initial\"))\n    graph.add_replacement((\"migrations\", \"0003_third\"), ((\"migrations\", \"0001_initial\"), (\"migrations\", \"0002_second\")))\n    executor.loader.graph = graph\n    targets = [(\"migrations\", \"0003_third\")]\n    plan = executor.migration_plan(targets)\n    self.assertEqual(len(plan), 1)\n    self.assertEqual(plan[0][0].name, \"0003_third\")\n    self.assertFalse(plan[0][1])\n"], "sample_324": ["def test_csrf_token_length(self):\n    req = self._get_GET_no_csrf_cookie_request()\n    token = get_token(req)\n    self.assertEqual(len(token), CSRF_TOKEN_LENGTH)\n"], "sample_326": ["def test_escapejs(self):\n    # Make sure escapejs does not mangle SafeStrings\n    safe_string = mark_safe('Hello, world!')\n    self.assertEqual(escapejs(safe_string), safe_string)\n\n    # Test escapejs on a normal string\n    s = 'Hello, world!'\n    self.assertEqual(escapejs(s), s)\n\n    # Test escapejs on a string that needs escaping\n    s = 'Hello, \"world\"!'\n    self.assertEqual(escapejs(s), 'Hello, \\\\u0022world\\\\u0022!')\n\n    # Test escapejs on a SafeString that needs escaping\n    s = mark_safe('Hello, \"world\"!')\n    self.assertEqual(escapejs(s), mark_safe('Hello, \\\\u0022world\\\\u0022!'))\n"], "sample_330": ["    def test_typecast_date(self):\n        self.assertIsNone(typecast_date(''))\n        self.assertEqual(typecast_date('2022-01-01'), datetime.date(2022, 1, 1))\n"], "sample_331": ["def test_parse_date_valid(self):\n    self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n    self.assertEqual(parse_date('2022-12-31'), date(2022, 12, 31))\n    self.assertIsNone(parse_date('2022-02-30'))  # invalid date\n    self.assertIsNone(parse_date('not a date'))  # not a valid date string\n"], "sample_332": ["def test_formset_management_form_validation(self):\n    # Test that the management form is validated correctly.\n    formset = self.make_choiceformset(formset_data=[('Choice 1', '10'), ('Choice 2', '20')])\n    formset.management_form.data['choices-TOTAL_FORMS'] = 'abc'\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.management_form.errors), 1)\n    self.assertIn('TOTAL_FORMS', formset.management_form.errors)\n"], "sample_333": ["def test_form_add_prefix(self):\n    # Test that add_prefix is called for each field.\n    class Person(Form):\n        first_name = CharField()\n        last_name = CharField()\n\n    p = Person(prefix='person')\n    self.assertEqual(p['first_name'].html_name, 'person-first_name')\n    self.assertEqual(p['last_name'].html_name, 'person-last_name')\n\n    # If prefix is not given, the fields should not have a prefix.\n    p = Person()\n    self.assertEqual(p['first_name'].html_name, 'first_name')\n    self.assertEqual(p['last_name'].html_name, 'last_name')\n"], "sample_336": ["def test_resolver_match_repr(self):\n    resolver_match = ResolverMatch(\n        func=views.empty_view,\n        args=(),\n        kwargs={'arg1': '42', 'arg2': '37'},\n        url_name='normal-view',\n        app_names=['testapp'],\n        namespaces=['test-ns1'],\n        route='/normal/42/37/',\n    )\n    expected_repr = (\n        \"ResolverMatch(func=urlpatterns_reverse.views.empty_view, args=(), \"\n        \"kwargs={'arg1': '42', 'arg2': '37'}, url_name='normal-view', \"\n        \"app_names=['testapp'], namespaces=['test-ns1'], route='/normal/42/37/')\"\n    )\n    self.assertEqual(repr(resolver_match), expected_repr)\n"], "sample_337": ["def test_csrf_trusted_origins_hosts(self):\n    @override_settings(CSRF_TRUSTED_ORIGINS=['http://example.com', 'https://sub.example.com'])\n        middleware = CsrfViewMiddleware()\n        self.assertEqual(middleware.csrf_trusted_origins_hosts, ['example.com', 'sub.example.com'])\n\n    test_get_trusted_origins_hosts()\n\n    @override_settings(CSRF_TRUSTED_ORIGINS=['http://*.example.com'])\n        middleware = CsrfViewMiddleware()\n        self.assertEqual(middleware.csrf_trusted_origins_hosts, ['example.com'])\n\n    test_get_trusted_origins_hosts_with_wildcard()\n"], "sample_339": ["    def test_model_formset_factory(self):\n        FormSet = modelformset_factory(Book, fields=('name',))\n        self.assertIsInstance(FormSet.form, forms.ModelForm)\n        self.assertEqual(FormSet.model, Book)\n"], "sample_340": ["    def test_load(self):\n        \"\"\"\n        Makes sure the loader can load the migrations for an app from disk.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.assertEqual(loader.disk_migrations, {\n            ('migrations', '0001_initial'): loader.get_migration('migrations', '0001_initial'),\n            ('migrations', '0002_second'): loader.get_migration('migrations', '0002_second'),\n        })\n"], "sample_341": ["def test_formset_absolute_max(self):\n    # Test that the absolute_max parameter is enforced.\n    ChoiceFormSetAbsoluteMax = formset_factory(Choice, max_num=5, absolute_max=10)\n    formset = self.make_choiceformset([('1', '1')] * 11, ChoiceFormSetAbsoluteMax)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertIn('Please submit at most 10 forms.', str(formset.non_form_errors()[0]))\n"], "sample_342": ["def test_get_paginator(self):\n    request = self.factory.get(self.url, self.opts)\n    request.user = self.user\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    paginator = response.context_data['paginator']\n    self.assertIsInstance(paginator, type(self.admin_site._registry[Answer].get_paginator(request)))\n"], "sample_345": ["def test_iter_modules_and_files_with_zip_importer(self):\n    # Create a temporary zip file containing a Python module.\n    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:\n        zip_filename = Path(tmp_file.name)\n        self.addCleanup(zip_filename.unlink)\n\n        with zipfile.ZipFile(tmp_file, 'w') as zip_file:\n            zip_file.writestr('test_module.py', b'pass')\n\n        # Add the zip file to sys.path and import the module.\n        with extend_sys_path(str(zip_filename)):\n            self.import_and_cleanup('test_module')\n\n        # The module file inside the zip archive should be found.\n        self.assertFileFound(zip_filename)\n"], "sample_346": ["def test_method_decorator(self):\n    class TestClass:\n            return \"Expected result\"\n\n    decorator = lambda func: lambda *args, **kwargs: \"Decorated result\"\n    method_decorator(decorator, name='test_method')(TestClass)\n\n    instance = TestClass()\n    self.assertEqual(instance.test_method(), \"Decorated result\")\n"], "sample_347": ["def test_make_aware(self):\n    naive = datetime.datetime(2011, 9, 1, 13, 20, 30)\n    for tz in PARIS_IMPLS:\n        aware = timezone.make_aware(naive, tz)\n        self.assertEqual(aware.tzinfo, tz)\n        self.assertEqual(aware, naive.replace(tzinfo=tz))\n"], "sample_351": ["def test_model_choice_field_iterator(self):\n    field = forms.ModelChoiceField(Category.objects.all())\n    iterator = ModelChoiceIterator(field)\n    self.assertEqual(len(iterator), 3)\n    choices = list(iterator)\n    self.assertEqual(choices[0][0], '')\n    self.assertEqual(choices[1][0], self.c1.pk)\n    self.assertEqual(choices[2][0], self.c2.pk)\n    self.assertEqual(choices[3][0], self.c3.pk)\n"], "sample_353": ["def test_create_superuser_with_bypass_password_validation(self):\n    with mock.patch('sys.stdin', MockTTY()):\n        call_command('createsuperuser', verbosity=0)\n    self.assertEqual(User.objects.count(), 1)\n    user = User.objects.get(username='alice')\n    self.assertTrue(user.is_superuser)\n    self.assertEqual(user.email, 'alice@example.com')\n"], "sample_354": ["def test_create_superuser_password_bypass(self):\n    with mock.patch('sys.stdin', MockTTY()):\n        call_command('createsuperuser', verbosity=0)\n    self.assertEqual(User.objects.count(), 1)\n    user = User.objects.get(username='alice')\n    self.assertTrue(user.check_password('test1234'))\n"], "sample_355": ["    def setUpTestData(cls):\n        cls.content_type = ContentType.objects.get_for_model(User)\n        cls.permission = Permission.objects.create(\n            name='test_permission',\n            content_type=cls.content_type,\n            codename='test_codename',\n        )\n"], "sample_358": ["    def setUp(self):\n        self.reference = TableColumns('table', ['column1', 'column2'])\n"], "sample_360": ["    def setUp(self):\n        self.factory = RequestFactory()\n        self.middleware = UpdateCacheMiddleware(get_response=lambda req: HttpResponse())\n"], "sample_361": ["def test_escapejs(self):\n    # Make sure escapejs works correctly with non-ASCII characters and quotes.\n    self.check_output(escapejs, 'h\u00ebll\u00f6', 'h\\\\u00ebl\\\\u00f6')\n    self.check_output(escapejs, '\"hello\"', '\\\\u0022hello\\\\u0022')\n    self.check_output(escapejs, \"'hello'\", '\\\\u0027hello\\\\u0027')\n"], "sample_364": ["def test_include_with_namespace(self):\n    with self.assertRaises(ImproperlyConfigured):\n        include(('urlpatterns.path_urls', 'app_name'), namespace='overridden_namespace')\n"], "sample_365": ["def test_cached_property(self):\n    class TestClass:\n        @cached_property\n            return 'Cached attribute value'\n\n    obj = TestClass()\n    self.assertEqual(obj.cached_attr, 'Cached attribute value')\n    # Subsequent access should return the cached value.\n    self.assertEqual(obj.cached_attr, 'Cached attribute value')\n\n    # The cached value should be stored in the object's __dict__.\n    self.assertIn('cached_attr', obj.__dict__)\n    self.assertEqual(obj.__dict__['cached_attr'], 'Cached attribute value')\n"], "sample_366": ["def test_parse_date(self):\n    self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n    self.assertIsNone(parse_date('2022-01'))  # incomplete date\n    self.assertIsNone(parse_date('20220101'))  # invalid format\n    self.assertIsNone(parse_date('2022-02-30'))  # invalid date\n    self.assertEqual(parse_date('2022-12-31'), date(2022, 12, 31))\n"], "sample_367": ["def test_cache_page(self):\n    # Test that cache_page decorator sets the correct Cache-Control header\n    @cache_page(60)\n        return HttpResponse('Test')\n\n    request = HttpRequest()\n    response = view(request)\n\n    self.assertIn('Cache-Control', response.headers)\n    self.assertEqual(response.headers['Cache-Control'], 'max-age=60')\n"], "sample_368": ["def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Test that replaced migrations are properly handled in the migration plan.\n    \"\"\"\n    # Create a mock migration graph with a replaced migration\n    graph = MigrationGraph()\n    graph.add_node((\"app\", \"0001_initial\"), None)\n    graph.add_node((\"app\", \"0002_second\"), [(\"app\", \"0001_initial\")])\n    graph.add_node((\"app\", \"0003_third\"), [(\"app\", \"0002_second\")])\n    graph.add_replacement((\"app\", \"0002_second\"), (\"app\", \"0002_2_second\"))\n    graph.add_replacement((\"app\", \"0003_third\"), (\"app\", \"0003_3_third\"))\n\n    # Create a mock migration loader and recorder\n    loader = mock.Mock(spec=migrations.loader.MigrationLoader)\n    loader.graph = graph\n    recorder = mock.Mock(spec=MigrationRecorder)\n\n    # Create a migration executor\n    executor = MigrationExecutor(connection, progress_callback=None)\n    executor.loader = loader\n    executor.recorder = recorder\n\n    # Get the migration plan\n    targets = [(\"app\", \"0003_3_third\")]\n    plan = executor.migration_plan(targets)\n\n    # Check that the replaced migrations are in the plan\n    self.assertEqual(len(plan), 3)\n    self.assertEqual(plan[0][0].name, \"0001_initial\")\n    self.assertEqual(plan[1][0].name, \"0002_2_second\")\n    self.assertEqual(plan[2][0].name, \"0003_3_third\")\n"], "sample_371": ["    def test_cleansed_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('SETTING', 'value'), 'value')\n        self.assertEqual(filter.cleanse_setting('API_KEY', 'secret'), '********************')\n"], "sample_372": ["def test_resolver_match_repr(self):\n    resolver_match = ResolverMatch(\n        func=views.empty_view,\n        args=(),\n        kwargs={},\n        url_name='test',\n        app_names=['app1', 'app2'],\n        namespaces=['ns1', 'ns2'],\n        route='/test/',\n        tried=[]\n    )\n    self.assertEqual(\n        repr(resolver_match),\n        \"ResolverMatch(func=urlpatterns_reverse.views.empty_view, args=(), kwargs={}, url_name='test', \"\n        \"app_names=['app1', 'app2'], namespaces=['ns1', 'ns2'], route='/test/')\"\n    )\n"], "sample_373": ["def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_test_list'), 'List')\n    self.assertEqual(get_return_data_type('get_test_count'), 'Integer')\n    self.assertEqual(get_return_data_type('test_method'), '')\n"], "sample_376": ["def test_message_encoder_decoder(self):\n    storage = self.get_storage()\n    message = Message(constants.DEBUG, 'Test message', extra_tags='tag1 tag2')\n    encoded = json.dumps([message], cls=MessageEncoder)\n    decoded = json.loads(encoded, cls=MessageDecoder)\n    self.assertEqual(len(decoded), 1)\n    self.assertIsInstance(decoded[0], Message)\n    self.assertEqual(decoded[0].level, constants.DEBUG)\n    self.assertEqual(decoded[0].message, 'Test message')\n    self.assertEqual(decoded[0].extra_tags, 'tag1 tag2')\n\n    # Test SafeData encoding/decoding\n    safe_message = Message(constants.DEBUG, mark_safe('Safe message'))\n    encoded = json.dumps([safe_message], cls=MessageEncoder)\n    decoded = json.loads(encoded, cls=MessageDecoder)\n    self.assertIsInstance(decoded[0].message, SafeData)\n    self.assertEqual(decoded[0].message, 'Safe message')\n"], "sample_377": ["    def test_cleansed_multivaluedict(self):\n        request = RequestFactory().post(\"/test/\", {\"password\": \"secret\", \"public\": \"value\"})\n        request.sensitive_post_parameters = [\"password\"]\n        filter = SafeExceptionReporterFilter()\n        cleaned_dict = filter.get_cleansed_multivaluedict(request, request.POST)\n        self.assertEqual(cleaned_dict[\"password\"], \"********************\")\n        self.assertEqual(cleaned_dict[\"public\"], \"value\")\n"], "sample_379": ["def test_mark_safe_decorator(self):\n    @mark_safe\n        return '<p>Hello, World!</p>'\n\n    self.assertIsInstance(marked_safe_string(), SafeString)\n    self.assertEqual(marked_safe_string().__html__(), '<p>Hello, World!</p>')\n"], "sample_380": ["def test_aggregate_with_filter(self):\n    # Test that an aggregate with a filter clause is correctly generated.\n    agg = Count('id', filter=Q(age__gt=30))\n    self.assertEqual(agg.as_sql(connection), ('COUNT(CASE WHEN (\"tests_author\".\"age\" > 30) THEN \"tests_author\".\"id\" ELSE NULL END)', []))\n\n    # Test that an aggregate with a filter clause and a distinct argument is correctly generated.\n    agg = Count('id', filter=Q(age__gt=30), distinct=True)\n    self.assertEqual(agg.as_sql(connection), ('COUNT(DISTINCT CASE WHEN (\"tests_author\".\"age\" > 30) THEN \"tests_author\".\"id\" ELSE NULL END)', []))\n"], "sample_382": ["    def test_reset_loaders_called_on_template_change(self):\n        with mock.patch('django.template.autoreload.reset_loaders') as mock_reset:\n            file_path = EXTRA_TEMPLATES_DIR / 'test_template.html'\n            autoreload.template_changed(None, file_path)\n            mock_reset.assert_called_once()\n"], "sample_383": ["    def setUpTestData(cls):\n        cls.t1 = Tag.objects.create(name=\"t1\")\n        cls.t2 = Tag.objects.create(name=\"t2\", parent=cls.t1)\n        cls.i1 = Item.objects.create(name=\"one\")\n        cls.i1.tags.set([cls.t1, cls.t2])\n"], "sample_385": ["def test_autocomplete_select_required(self):\n    form = RequiredBandForm()\n    self.assertHTMLEqual(\n        form[\"band\"].as_html(),\n        '<select data-ajax--cache=\"true\" data-ajax--delay=\"250\" '\n        'data-ajax--type=\"GET\" data-ajax--url=\"/autocomplete/\" '\n        'data-app-label=\"admin_widgets\" data-model-name=\"album\" '\n        'data-field-name=\"band\" data-theme=\"admin-autocomplete\" '\n        'data-allow-clear=\"false\" data-placeholder=\"\" lang=\"en\" '\n        'class=\"admin-autocomplete\" name=\"band\" required>\\n'\n        + self.empty_option\n        + \"\\n</select>\",\n    )\n"], "sample_386": ["def test_safe_string_addition(self):\n    safe_str = SafeString(\"<p>Hello</p>\")\n    other_safe_str = SafeString(\"<p>World</p>\")\n    unsafe_str = \"<script>alert('XSS')</script>\"\n\n    self.assertIsInstance(safe_str + other_safe_str, SafeString)\n    self.assertNotIsInstance(safe_str + unsafe_str, SafeString)\n\n    self.assertEqual((safe_str + other_safe_str).__html__(), \"<p>Hello</p><p>World</p>\")\n    self.assertEqual((safe_str + unsafe_str).__html__(), None)\n"], "sample_388": ["def test_remote_user_created(self):\n    # Test that a user is created when the RemoteUserBackend is used with\n    # create_unknown_user=True.\n    backend = RemoteUserBackend()\n    user = backend.authenticate(request=None, remote_user=\"newuser\")\n    self.assertIsNotNone(user)\n    self.assertEqual(user.username, \"newuser\")\n\n    # Test that a user is not created when the RemoteUserBackend is used with\n    # create_unknown_user=False.\n    backend.create_unknown_user = False\n    user = backend.authenticate(request=None, remote_user=\"newuser2\")\n    self.assertIsNone(user)\n"], "sample_390": ["def test_serve_directory_index(self):\n    \"\"\"Test serving a directory index\"\"\"\n    with self.settings(MEDIA_ROOT=media_dir):\n        response = self.client.get(\"/site_media/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"static/directory_index.html\")\n        self.assertContains(response, \"<h1>Index of /</h1>\")\n        self.assertContains(response, \"<li><a href=\\\"testfile.txt\\\">testfile.txt</a></li>\")\n"], "sample_393": ["def test_makemessages_with_locale_dir_outside_of_basedir(self):\n    # Create a locale directory outside of the project directory.\n    external_locale_dir = tempfile.mkdtemp()\n    os.mkdir(os.path.join(external_locale_dir, LOCALE))\n    os.mkdir(os.path.join(external_locale_dir, LOCALE, \"LC_MESSAGES\"))\n\n    with override_settings(LOCALE_PATHS=[external_locale_dir]):\n        management.call_command(\"makemessages\", locale=[LOCALE])\n\n    po_file = os.path.join(external_locale_dir, LOCALE, \"LC_MESSAGES\", \"django.po\")\n    self.assertTrue(os.path.exists(po_file))\n\n    shutil.rmtree(external_locale_dir)\n"], "sample_395": ["def test_reset_loaders_called_on_template_change(self):\n    with mock.patch(\"django.template.autoreload.reset_loaders\") as mock_reset:\n        file_path = EXTRA_TEMPLATES_DIR / \"test_template.html\"\n        autoreload.file_changed.send(sender=None, file_path=file_path)\n        mock_reset.assert_called_once()\n"], "sample_397": ["def test_engine_get_default(self):\n    engine = Engine.get_default()\n    self.assertIsInstance(engine, Engine)\n    # Test that the default engine is properly configured\n    self.assertTrue(engine.debug)\n    self.assertEqual(engine.string_if_invalid, \"\")\n    self.assertEqual(engine.file_charset, \"utf-8\")\n    self.assertTrue(engine.autoescape)\n"], "sample_398": ["def test_password_reset_complete_view(self):\n    \"\"\"\n    Test that the password reset complete view renders the correct template.\n    \"\"\"\n    url = reverse(\"password_reset_complete\")\n    response = self.client.get(url)\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, \"registration/password_reset_complete.html\")\n\n    # Test that the login URL is in the context.\n    self.assertIn(\"login_url\", response.context)\n    self.assertEqual(response.context[\"login_url\"], reverse(settings.LOGIN_URL))\n"], "sample_402": ["    def test_append_slash_with_query_string(self):\n        request = self.rf.get(\"/path?query=string\")\n        middleware = CommonMiddleware(get_response_404)\n        response = middleware.process_request(request)\n        self.assertIsInstance(response, HttpResponsePermanentRedirect)\n        self.assertEqual(response.url, \"/path/?query=string\")\n"], "sample_404": ["    def test_template_tokens(self):\n        source = 'This is a {{ template }} string with {% a %} tag.'\n        expected = [\n            ('This is a ', (0, 10), None),\n            ('{{ template }}', (10, 23), 'template'),\n            (' string with ', (23, 36), None),\n            ('{% a %}', (36, 41), 'a'),\n            (' tag.', (41, 46), None),\n        ]\n        lexer = self._engine().lexer\n        for token, expected_token in zip(lexer.tokenize(source), expected):\n            self.assertEqual(token.token_type, TokenType.TEXT if expected_token[2] is None else TokenType.VAR if expected_token[0].startswith('{{') else TokenType.BLOCK)\n            self.assertEqual(token.contents, expected_token[0])\n            self.assertEqual(token.position, expected_token[1])\n            self.assertEqual(token.lineno, 1)\n"], "sample_406": ["    def test_base_manager_deconstruct(self):\n        manager = BaseManager()\n        with self.assertRaises(ValueError):\n            manager.deconstruct()\n"], "sample_407": ["def test_foreign_key_on_delete(self):\n    # Create a new Article with a different reporter\n    a3 = Article(headline=\"Third article\", pub_date=datetime.date(2005, 7, 27), reporter=self.r2)\n    a3.save()\n\n    # If we delete the reporter, his articles should be deleted when on_delete is CASCADE\n    self.r2.delete()\n    with self.assertRaises(Article.DoesNotExist):\n        Article.objects.get(pk=a3.pk)\n\n    # Try setting on_delete to PROTECT and make sure that works\n    p = Parent.objects.create(name='Parent')\n    c = Child.objects.create(name='Child', parent=p)\n\n    with transaction.atomic():\n        with self.assertRaises(IntegrityError):\n            p.delete()\n\n    # Try setting on_delete to SET_NULL\n    p = Parent.objects.create(name='Parent')\n    c = ChildNullableParent.objects.create(name='Child', parent=p)\n    p.delete()\n    c.refresh_from_db()\n    self.assertIsNone(c.parent)\n\n    # Try setting on_delete to SET_DEFAULT\n    p = Parent.objects.create(name='Parent')\n    c = Child.objects.create(name='Child', parent=p)\n    p.delete()\n    c.refresh_from_db()\n    self.assertEqual(c.parent.name, 'Default')\n\n    # Try setting on_delete to DO_NOTHING\n    p = Parent.objects.create(name='Parent')\n    c = Child.objects.create(name='Child', parent=p)\n    p.delete()\n    c.refresh_from_db()\n    with self.assertRaises(Parent.DoesNotExist):\n        c.parent\n\n    # Try setting on_delete to SET\n    p = Parent.objects.create(name='Parent')\n    c = Child.objects.create(name='Child', parent=p)\n    p.delete()\n    c.refresh_from_db()\n    self.assertEqual(c.parent.name, 'Set')\n"], "sample_409": ["def test_blocktrans_context(self):\n    output = self.engine.render_to_string(\"i18n04\")\n    self.assertEqual(output, \"foo\")\n    with translation.override(\"de\"):\n        output = self.engine.render_to_string(\"i18n04\")\n        self.assertEqual(output, \"foo\")\n"], "sample_410": ["    def test_normalize_username(self):\n        class TestUser(AbstractBaseUser):\n            USERNAME_FIELD = 'username'\n\n        user = TestUser(username='\u00e1\u00e9\u00ed\u00f3\u00fa')\n        self.assertEqual(user.normalize_username('\u00e1\u00e9\u00ed\u00f3\u00fa'), 'aeiou')\n"], "sample_411": ["def test_base_command_style(self):\n    class TestCommand(BaseCommand):\n            self.stdout.write(self.style.SUCCESS(\"Success message\"))\n            self.stderr.write(self.style.ERROR(\"Error message\"))\n\n    command = TestCommand()\n    with captured_stderr() as stderr:\n        with mock.patch.object(command.stdout, \"write\") as write_mock:\n            command.handle()\n            write_mock.assert_called_once_with(command.style.SUCCESS(\"Success message\") + \"\\n\")\n        self.assertIn(command.style.ERROR(\"Error message\") + \"\\n\", stderr.getvalue())\n"], "sample_412": ["def test_escapejs(self):\n    # Make sure escapejs works correctly with non-ASCII characters.\n    self.check_output(escapejs, 'h\u00ebll\u00f6', 'h\\\\u00ebll\\\\u00f6')\n    # Make sure escapejs works correctly with special characters.\n    self.check_output(escapejs, 'h\"e\\'l\\\\l/o', 'h\\\\u0022e\\\\u0027l\\\\\\\\l\\\\u002Fo')\n"], "sample_413": ["    def test_string_if_invalid_is_not_string(self):\n        errors = check_string_if_invalid_is_string(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, E002.id)\n"], "sample_416": ["def test_settings_to_cmd_args_env_with_ssl_options(self):\n    settings_dict = {\n        \"NAME\": \"mydb\",\n        \"USER\": \"myuser\",\n        \"PASSWORD\": \"mypassword\",\n        \"HOST\": \"localhost\",\n        \"PORT\": 5432,\n        \"OPTIONS\": {\n            \"sslmode\": \"require\",\n            \"sslrootcert\": \"/path/to/root/cert\",\n            \"sslcert\": \"/path/to/cert\",\n            \"sslkey\": \"/path/to/key\",\n        },\n    }\n    args, env = self.settings_to_cmd_args_env(settings_dict)\n    self.assertEqual(args, [\"psql\", \"-U\", \"myuser\", \"-h\", \"localhost\", \"-p\", \"5432\", \"mydb\"])\n    self.assertEqual(env, {\n        \"PGSSLMODE\": \"require\",\n        \"PGSSLROOTCERT\": \"/path/to/root/cert\",\n        \"PGSSLCERT\": \"/path/to/cert\",\n        \"PGSSLKEY\": \"/path/to/key\",\n        \"PGPASSWORD\": \"mypassword\",\n    })\n"], "sample_420": ["def test_modelform_defines_fields(self):\n    self.assertTrue(modelform_defines_fields(ProductForm))\n    self.assertFalse(modelform_defines_fields(forms.ModelForm))\n"], "sample_426": ["def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t, depth=1), '11 years')\n        self.assertEqual(timesince(self.t, depth=2), '11 years, 4 months')\n        self.assertEqual(timesince(self.t + self.oneweek, depth=3), '11 years, 4 months, 1 week')\n"], "sample_428": ["def test_decimal_pos_rounding(self):\n    self.assertEqual(nformat(123.456, '.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(123.456, '.', decimal_pos=1), '123.5')\n    self.assertEqual(nformat(123.456, '.', decimal_pos=0), '123')\n    self.assertEqual(nformat(Decimal('123.456'), '.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(Decimal('123.456'), '.', decimal_pos=1), '123.5')\n    self.assertEqual(nformat(Decimal('123.456'), '.', decimal_pos=0), '123')\n"], "sample_429": ["def test_prohibit_null_characters_validator(self):\n    validator = ProhibitNullCharactersValidator()\n    with self.assertRaisesMessage(ValidationError, \"Null characters are not allowed.\"):\n        validator(\"\\x00something\")\n    validator(\"something\")  # Should not raise an error.\n    validator(None)  # Should not raise an error.\n    validator(\"\")  # Should not raise an error.\n\n    # Test that the validator works with non-string inputs\n    with self.assertRaisesMessage(ValidationError, \"Null characters are not allowed.\"):\n        validator(b\"\\x00something\")\n    validator(b\"something\")  # Should not raise an error.\n    validator(123)  # Should not raise an error.\n"], "sample_433": ["def test_suggest_name(self):\n    migration = Migration(\"auto_123\", \"testapp\")\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Author\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        )\n    ]\n    self.assertEqual(migration.suggest_name(), \"create_author\")\n\n    migration.operations = [\n        migrations.AddField(\n            model_name=\"author\",\n            name=\"bio\",\n            field=models.TextField(),\n        )\n    ]\n    self.assertEqual(migration.suggest_name(), \"add_bio_to_author\")\n\n    migration.operations = [\n        migrations.AlterField(\n            model_name=\"author\",\n            name=\"name\",\n            field=models.CharField(max_length=300),\n        )\n    ]\n    self.assertEqual(migration.suggest_name(), \"alter_author_name\")\n"], "sample_434": ["    def test_view_is_async(self):\n        class AsyncView(View):\n            async def get(self, request):\n                return HttpResponse(\"Hello\")\n\n        self.assertTrue(AsyncView.view_is_async)\n\n        class SyncView(View):\n                return HttpResponse(\"Hello\")\n\n        self.assertFalse(SyncView.view_is_async)\n\n        class MixedView(View):\n            async def get(self, request):\n                return HttpResponse(\"Hello\")\n\n                return HttpResponse(\"Hello\")\n\n        with self.assertRaises(ImproperlyConfigured):\n            MixedView.view_is_async\n"], "sample_435": ["    def test_readonlypasswordhashfield_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n"], "sample_436": ["    def test_runserver_command(self):\n        # Test that the runserver command runs without errors\n        with captured_stdout() as stdout:\n            call_command(\"runserver\", \"--nothreading\", \"--noreload\", stdout=StringIO())\n        self.assertIn(\"Starting development server\", stdout.getvalue())\n"], "sample_437": ["def test_validate_thread_sharing(self):\n    wrapper = BaseDatabaseWrapper({}, DEFAULT_DB_ALIAS)\n    wrapper._thread_ident = 123\n    with patch.object(wrapper, 'allow_thread_sharing', False):\n        with self.assertRaises(DatabaseError):\n            wrapper.validate_thread_sharing()\n"], "sample_438": ["def test_deferred_model_unpickle(self):\n    # Make sure model instances with deferred fields can be pickled and unpickled.\n    post = Post.objects.create(title=\"Test\", body=\"Test\")\n    post = Post.objects.defer(\"body\").get(pk=post.pk)\n    self.assertIs(post.body, models.DEFERRED)\n\n    # Pickle and unpickle the post.\n    post_data = json.dumps({\n        '__class__': 'model_unpickle',\n        'args': [('contenttypes_tests', 'Post')],\n        '__state__': post.__dict__,\n    })\n\n    unpickled_post = json.loads(post_data, object_hook=lambda d: model_unpickle(d['__class__'])(*d['args'], **d['__state__']))\n\n    # The unpickled post should still have a deferred body.\n    self.assertIs(unpickled_post.body, models.DEFERRED)\n"], "sample_441": ["    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n"], "sample_442": ["def test_signer_signature(self):\n    signer = signing.Signer()\n    value = \"hello\"\n    signed_value = signer.sign(value)\n    self.assertEqual(signer.unsign(signed_value), value)\n\n    # Test signature check\n    with self.assertRaises(signing.BadSignature):\n        signer.unsign(\"hello:wrong-signature\")\n"], "sample_443": ["    def test_filebased_cache(self):\n        cache = caches[\"default\"]\n        key = \"test_key\"\n        value = \"test_value\"\n\n        # Test set and get\n        cache.set(key, value)\n        self.assertEqual(cache.get(key), value)\n\n        # Test add\n        self.assertFalse(cache.add(key, \"new_value\"))\n        cache.delete(key)\n        self.assertTrue(cache.add(key, \"new_value\"))\n        self.assertEqual(cache.get(key), \"new_value\")\n\n        # Test touch\n        cache.touch(key, 10)\n        self.assertGreater(cache.ttl(key), 0)\n\n        # Test ttl\n        cache.set(key, value, 10)\n        self.assertGreater(cache.ttl(key), 0)\n\n        # Test exists\n        self.assertTrue(cache.has_key(key))\n        cache.delete(key)\n        self.assertFalse(cache.has_key(key))\n\n        # Test clear\n        cache.set(key, value)\n        cache.clear()\n        self.assertFalse(cache.has_key(key))\n\n        # Test cull\n        for i in range(100):\n            cache.set(f\"key_{i}\", f\"value_{i}\")\n        cache._cull()\n        self.assertLess(len(cache._list_cache_files()), 100)\n"], "sample_444": ["    def test_post_process(self):\n        \"\"\"\n        Test post processing of files.\n        \"\"\"\n        storage.staticfiles_storage = storage.HashedFilesMixin()\n        self.addCleanup(shutil.rmtree, settings.STATIC_ROOT)\n        call_command('collectstatic', verbosity=0, interactive=False)\n\n        # Check the manifest file was created\n        manifest_path = os.path.join(settings.STATIC_ROOT, 'staticfiles.json')\n        self.assertTrue(os.path.exists(manifest_path))\n\n        # Check a file was hashed and saved\n        original_file_path = os.path.join(TEST_ROOT, 'project', 'css', 'file.css')\n        hashed_file_path = self.hashed_file_path('css/file.css')\n        self.assertTrue(os.path.exists(os.path.join(settings.STATIC_ROOT, hashed_file_path)))\n\n        # Check the manifest contains the correct hash\n        with open(manifest_path, 'r') as f:\n            manifest = json.load(f)\n        self.assertIn(hashed_file_path, manifest['paths'])\n"], "sample_445": ["def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t, depth=1), '11 years')\n        self.assertEqual(timesince(self.t, depth=2), '11 years, 4 months')\n        self.assertEqual(timesince(self.t + self.oneweek, depth=3), '11 years, 4 months, 1 week')\n"], "sample_448": ["    def test_check_constraint(self):\n        class CheckModel(models.Model):\n            value = models.IntegerField()\n            check_constraint = models.CheckConstraint(\n                check=models.Q(value__gte=0), name=\"check_value_gte_0\"\n            )\n\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(CheckModel)\n\n        constraints = get_constraints(CheckModel._meta.db_table)\n        self.assertIn((\"check_value_gte_0\", \"CHECK\"), constraints.items())\n\n        # Try to create an instance that violates the constraint.\n        with self.assertRaises(IntegrityError):\n            CheckModel.objects.create(value=-1)\n\n        # Create an instance that doesn't violate the constraint.\n        instance = CheckModel.objects.create(value=1)\n        instance.refresh_from_db()\n\n        # Update the instance to violate the constraint.\n        instance.value = -1\n        with self.assertRaises(IntegrityError):\n            instance.save()\n\n        # Delete the instance.\n        instance.delete()\n\n        with connection.schema_editor() as schema_editor:\n            schema_editor.delete_model(CheckModel)\n"], "sample_449": ["def test_get_environ(self):\n    request = self.request_factory.get('/path')\n    handler = WSGIRequestHandler(request, Stub(), UnclosableBytesIO())\n    environ = handler.get_environ()\n    self.assertNotIn('HTTP_HEADER_WITH_UNDERSCORE', environ)\n    self.assertIn('HTTP_HOST', environ)\n    self.assertEqual(environ['REQUEST_METHOD'], 'GET')\n    self.assertEqual(environ['PATH_INFO'], '/path')\n"], "sample_450": ["def test_get_admin_log_template_tag(self):\n    template = \"\"\"\n        {% load admin_utils %}\n        {% get_admin_log 10 as admin_log %}\n        {{ admin_log|length }}\n    \"\"\"\n    rendered = self.render_template(template, {\"log_entries\": LogEntry.objects.all()})\n    self.assertEqual(rendered.strip(), \"1\")\n\n    template = \"\"\"\n        {% load admin_utils %}\n        {% get_admin_log 10 as admin_log for_user user %}\n        {{ admin_log|length }}\n    \"\"\"\n    rendered = self.render_template(template, {\"log_entries\": LogEntry.objects.all(), \"user\": self.user})\n    self.assertEqual(rendered.strip(), \"1\")\n\n    template = \"\"\"\n        {% load admin_utils %}\n        {% get_admin_log 10 as admin_log for_user 999 %}\n        {{ admin_log|length }}\n    \"\"\"\n    rendered = self.render_template(template, {\"log_entries\": LogEntry.objects.all()})\n    self.assertEqual(rendered.strip(), \"0\")\n"], "sample_453": ["def test_prepopulated_fields_js(self):\n    request = self.request_factory.get(reverse('admin:auth_user_add'))\n    adminform = UserAdmin(User, site).get_form(request)\n    context = {\n        'adminform': adminform,\n        'add': True,\n        'change': False,\n        'is_popup': False,\n        'save_as': False,\n        'has_add_permission': True,\n        'has_change_permission': True,\n        'has_view_permission': True,\n        'has_editable_inline_admin_formsets': False,\n    }\n    context = prepopulated_fields_js(context)\n    self.assertIn('prepopulated_fields', context)\n    self.assertIn('prepopulated_fields_json', context)\n    self.assertEqual(context['prepopulated_fields'], [])\n    self.assertEqual(context['prepopulated_fields_json'], '[]')\n"], "sample_456": ["def test_formset_management_form_validation(self):\n    formset = self.make_choiceformset(formset_data=[(\"Choice 1\", 10), (\"Choice 2\", 20)])\n    management_form = ManagementForm(\n        {\n            TOTAL_FORM_COUNT: \"2\",\n            INITIAL_FORM_COUNT: \"0\",\n            MIN_NUM_FORM_COUNT: \"0\",\n            MAX_NUM_FORM_COUNT: \"1000\",\n        },\n        prefix=\"choices\",\n    )\n    self.assertTrue(management_form.is_valid())\n\n    # Test that missing fields are correctly reported.\n    management_form = ManagementForm(\n        {\n            TOTAL_FORM_COUNT: \"2\",\n            INITIAL_FORM_COUNT: \"0\",\n            MIN_NUM_FORM_COUNT: \"0\",\n        },\n        prefix=\"choices\",\n    )\n    self.assertFalse(management_form.is_valid())\n    self.assertEqual(len(management_form.errors), 1)\n    self.assertIn(MAX_NUM_FORM_COUNT, management_form.errors)\n\n    # Test that non-integer values are correctly reported.\n    management_form = ManagementForm(\n        {\n            TOTAL_FORM_COUNT: \"abc\",\n            INITIAL_FORM_COUNT: \"0\",\n            MIN_NUM_FORM_COUNT: \"0\",\n            MAX_NUM_FORM_COUNT: \"1000\",\n        },\n        prefix=\"choices\",\n    )\n    self.assertFalse(management_form.is_valid())\n    self.assertEqual(len(management_form.errors), 1)\n    self.assertIn(TOTAL_FORM_COUNT, management_form.errors)\n"], "sample_464": ["def test_file_response_with_non_seekable_file(self):\n    file = UnseekableBytesIO(b'Hello, world!')\n    response = FileResponse(file)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(response['Content-Length'], '13')\n    self.assertEqual(response.getvalue(), b'Hello, world!')\n"], "sample_466": ["def test_operation_writer_serialize_with_manual_porting(self):\n    operation = migrations.RunPython(\n        custom_migration_operations.operations.test_operation\n    )\n    writer = OperationWriter(operation)\n    result, imports = writer.serialize()\n    self.assertIn(\"manual copying\", result)\n    self.assertIn(\"# Functions from the following migrations need manual copying.\", result)\n    self.assertIn(\n        \"# Move them and any dependencies into this file, then update the\",\n        result,\n    )\n    self.assertIn(\"# RunPython operations to refer to the local versions:\", result)\n    self.assertIn(\n        \"# custom_migration_operations.operations\",\n        result,\n    )\n"], "sample_470": ["def test_cached_property(self):\n    class TestClass:\n        @cached_property\n            return 'Cached value'\n\n    obj = TestClass()\n    self.assertEqual(obj.cached_attr, 'Cached value')\n    # Check that the attribute is cached and doesn't call the method again.\n    with mock.patch.object(TestClass, 'cached_attr', new_callable=mock.PropertyMock) as mock_cached_attr:\n        self.assertEqual(obj.cached_attr, 'Cached value')\n        mock_cached_attr.assert_not_called()\n"], "sample_472": ["def test_validate_number(self):\n    paginator = Paginator([1, 2, 3], 2)\n    with self.assertRaises(PageNotAnInteger):\n        paginator.validate_number(\"a\")\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(0)\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(paginator.num_pages + 1)\n    self.assertEqual(paginator.validate_number(1), 1)\n    self.assertEqual(paginator.validate_number(2), 2)\n"], "sample_473": ["def test_asgi_handler(self):\n    async def receive():\n        return {\n            \"type\": \"http.request\",\n            \"body\": b\"Hello, World!\",\n            \"more_body\": False,\n        }\n\n    async def send(message):\n        self.assertEqual(message[\"type\"], \"http.response.start\")\n        self.assertEqual(message[\"status\"], 200)\n\n    handler = ASGIHandler()\n    scope = {\n        \"type\": \"http\",\n        \"path\": \"/\",\n        \"method\": \"GET\",\n        \"headers\": [],\n    }\n    with override_settings(ROOT_URLCONF=\"django.urls\"):\n        handler(scope, receive, send)\n"], "sample_474": ["    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n"], "sample_476": ["def test_image_field_update_dimension_fields(self):\n    \"\"\"\n    Tests that the update_dimension_fields method updates the width and height\n    fields of the model instance when an image is assigned to the ImageField.\n    \"\"\"\n    instance = self.PersonModel()\n    instance.mugshot = self.file1\n    self.check_dimensions(instance, 4, 8)\n\n    # Check that dimensions are updated when a new image is assigned.\n    instance.mugshot = self.file2\n    self.check_dimensions(instance, 8, 4)\n\n    # Check that dimensions are cleared when the image is deleted.\n    instance.mugshot.delete()\n    self.check_dimensions(instance, None, None)\n"], "sample_477": ["    def test_stringformat_float(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 3.14})\n        self.assertEqual(output, \"3.140000E+00\")\n"], "sample_478": ["    def test_raw_id_fields_valid(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = (\"id\",)\n\n        self.assertIsValid(RawIdModelAdmin, Band)\n"], "sample_484": ["    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n"], "sample_485": ["def test_escapejs(self):\n    # Make sure escapejs works correctly with non-ASCII characters.\n    self.check_output(escapejs, 'h\u00ebll\u00f6', 'h\\\\u00ebll\\\\u00f6')\n    self.check_output(escapejs, 'hello\\nworld', 'hello\\\\nworld')\n    self.check_output(escapejs, 'hello\\rworld', 'hello\\\\rworld')\n    self.check_output(escapejs, 'hello\\tworld', 'hello\\\\tworld')\n    self.check_output(escapejs, 'hello\\bworld', 'hello\\\\bworld')\n    self.check_output(escapejs, 'hello\\fworld', 'hello\\\\fworld')\n    self.check_output(escapejs, '<hello>', '\\\\u003Chello\\\\u003E')\n"], "sample_488": ["def test_get_containing_app_config(self):\n    self.assertIsNone(apps.get_containing_app_config(\"nonexistent_module\"))\n    self.assertEqual(\n        apps.get_containing_app_config(\"django.contrib.auth.models\"),\n        apps.get_app_config(\"auth\"),\n    )\n    self.assertEqual(\n        apps.get_containing_app_config(\"django.contrib.auth\"),\n        apps.get_app_config(\"auth\"),\n    )\n    with self.settings(INSTALLED_APPS=[\"apps.apps.MyAdmin\"]):\n        new_apps.clear_cache()\n        self.assertEqual(\n            new_apps.get_containing_app_config(\"apps.apps\"),\n            new_apps.get_app_config(\"myadmin\"),\n        )\n        self.assertEqual(\n            new_apps.get_containing_app_config(\"apps.apps.models\"),\n            new_apps.get_app_config(\"myadmin\"),\n        )\n"], "sample_492": ["def test_operation_writer_serialize_with_manual_porting(self):\n    operation = migrations.RunPython(\n        custom_migration_operations.operations.test_operation\n    )\n    writer = OperationWriter(operation)\n    output, imports = writer.serialize()\n    self.assertIn(\"# Functions from the following migrations need manual copying.\", imports)\n    self.assertIn(\"custom_migration_operations.operations\", imports)\n"], "sample_495": ["def test_validate_number(self):\n    paginator = Paginator([1, 2, 3, 4, 5], 2)\n    with self.assertRaises(PageNotAnInteger):\n        paginator.validate_number('a')\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(-1)\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(paginator.num_pages + 1)\n    self.assertEqual(paginator.validate_number(1), 1)\n    self.assertEqual(paginator.validate_number(2), 2)\n    self.assertEqual(paginator.validate_number(3), 3)\n"], "sample_496": ["    def test_base_command_style(self):\n        command = BaseCommand()\n        self.assertEqual(command.style.ERROR('error'), '\\x1b[31;1merror\\x1b[0m')\n        self.assertEqual(command.style.SUCCESS('success'), '\\x1b[32;1msuccess\\x1b[0m')\n        self.assertEqual(command.style.NOTICE('notice'), '\\x1b[34;1mnotice\\x1b[0m')\n        self.assertEqual(command.style.SQL_FIELD('sql_field'), '\\x1b[35;1msql_field\\x1b[0m')\n        self.assertEqual(command.style.SQL_COLTYPE('sql_coltype'), '\\x1b[36;1msql_coltype\\x1b[0m')\n        self.assertEqual(command.style.SQL_KEYWORD('sql_keyword'), '\\x1b[37;1msql_keyword\\x1b[0m')\n        self.assertEqual(command.style.SQL_TABLE('sql_table'), '\\x1b[33;1msql_table\\x1b[0m')\n"], "sample_499": ["def test_legend_handler_map():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], label='Line 1')\n    line2, = ax.plot([4, 5, 6], label='Line 2')\n\n    # Create a custom legend handler map\n    custom_handler_map = {mlines.Line2D: HandlerTuple(ndivide=None)}\n\n    # Pass the custom handler map to the legend function\n    ax.legend(handles=[line1, line2], handler_map=custom_handler_map)\n\n    # Check that the custom handler map is used\n    legend = ax.get_legend()\n    assert legend.get_legend_handler_map() == custom_handler_map\n"], "sample_500": ["def test_colorbar_extension_length():\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return fig1, fig2\n"], "sample_504": ["def test_colorbar_with_lines():\n    fig, ax = plt.subplots()\n    data = np.linspace(0, 1, 100)\n    cont = ax.contour(data.reshape((10, 10)), levels=[0.2, 0.5, 0.8])\n    cbar = fig.colorbar(cont, ax=ax)\n    cbar.add_lines(cont)\n    return fig\n"], "sample_507": ["def test_unit_data_update(self):\n    unit_data = cat.UnitData()\n    unit_data.update([\"A\", \"B\", \"C\"])\n    assert len(unit_data._mapping) == 3\n    unit_data.update([\"D\", \"E\", \"F\"])\n    assert len(unit_data._mapping) == 6\n    unit_data.update([\"A\", \"B\", \"C\"])\n    assert len(unit_data._mapping) == 6\n"], "sample_508": ["def test_artist_setp():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3])\n    martist.setp(line, 'linestyle', '--')\n    assert line.get_linestyle() == '--'\n    martist.setp(line, linestyle=':', color='r')\n    assert line.get_linestyle() == ':'\n    assert line.get_color() == 'r'\n\n    # Test MATLAB style string/value pairs\n    martist.setp(line, 'linewidth', 2, 'color', 'b')\n    assert line.get_linewidth() == 2\n    assert line.get_color() == 'b'\n\n    # Test listing allowed values for a property\n    with io.StringIO() as file:\n        martist.setp(line, 'linestyle', file=file)\n        file.seek(0)\n        output = file.read()\n    assert 'linestyle: {' in output\n\n    # Test listing all properties and their allowed values\n    with io.StringIO() as file:\n        martist.setp(line, file=file)\n        file.seek(0)\n        output = file.read()\n    assert 'agg_filter: a filter function' in output\n"], "sample_509": ["def test_date_ticker_factory():\n    # Test that date_ticker_factory returns the correct locator and formatter\n    # for different spans of time.\n    spans = [0, 1 / 24, 1, 7, 30, 365]\n    for span in spans:\n        locator, formatter = mdates.date_ticker_factory(span)\n        assert isinstance(locator, mdates.DateLocator)\n        assert isinstance(formatter, mdates.DateFormatter)\n"], "sample_513": ["def test_legend_with_fancybox(fig_test, fig_ref):\n    fig_test.subplots()\n    ax = fig_ref.subplots()\n\n    ax.plot([1, 2, 3], label='Line')\n    ax.legend(fancybox=True)\n\n    # Create a mock legend with fancybox=True\n    with mock.patch('matplotlib.legend.Legend') as MockLegend:\n        MockLegend._default_handler_map = mlegend.Legend.get_default_handler_map()\n        MockLegend.get_legend_handler_map.return_value = MockLegend._default_handler_map\n\n        fig_test.legends = [MockLegend(fancybox=True)]\n\n    # Check that the two figures are equal\n    assert len(fig_test.legends) == 1\n    assert fig_test.legends[0].fancybox\n"], "sample_514": ["def test_colorbar_remove():\n    fig, ax = plt.subplots()\n    m = ax.pcolormesh(np.random.rand(10, 10))\n    cbar = fig.colorbar(m)\n    assert len(fig.axes) == 2\n    cbar.remove()\n    assert len(fig.axes) == 1\n"], "sample_515": ["def test_colorbar_orientation():\n    fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n    cmap = mpl.colormaps[\"RdBu\"]\n    norm = Normalize(vmin=-1, vmax=1)\n    for ax, orientation in zip(axs.flat, ['vertical', 'horizontal'] * 2):\n        Colorbar(ax, cmap=cmap, norm=norm, orientation=orientation)\n        ax.tick_params(left=False, labelleft=False,\n                       bottom=False, labelbottom=False)\n    return fig\n"], "sample_520": ["def test_art3d_line_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Create a 2D LineCollection\n    xs = np.array([0.1, 0.2, 0.3])\n    ys = np.array([0.4, 0.5, 0.6])\n    line_collection = LineCollection([np.column_stack((xs, ys))])\n\n    # Convert the LineCollection to 3D\n    art3d.line_collection_2d_to_3d(line_collection, zs=0.7, zdir='z')\n\n    # Add the LineCollection to the axes\n    ax.add_collection(line_collection)\n\n    # Check that the LineCollection has been correctly converted to 3D\n    assert isinstance(line_collection, art3d.Line3DCollection)\n    assert line_collection.get_zs() == 0.7\n    assert line_collection.get_zdir() == 'z'\n"], "sample_522": ["def test_colorbar_extension_length():\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return fig1, fig2\n"], "sample_523": ["def test_legend_with_fancybox(fig_test, fig_ref):\n    fig_test.subplots()\n    ax = fig_test.axes[0]\n    ax.plot([1, 2, 3], label='test')\n    ax.legend(fancybox=True)\n\n    fig_ref.subplots()\n    ax = fig_ref.axes[0]\n    ax.plot([1, 2, 3], label='test')\n    ax.legend(fancybox=True)\n    ax.get_legend().legendPatch.set_boxstyle(\"round,pad=0,rounding_size=0.2\")\n"], "sample_524": ["def test_figure_suptitle(fig_test, fig_ref):\n    fig_test.suptitle('Hello, World!')\n    fig_ref.text(0.5, 0.98, 'Hello, World!', ha='center', va='top',\n                 transform=fig_ref.transFigure)\n"], "sample_525": ["def test_figure_layout_engine():\n    fig = Figure()\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n\n    with pytest.raises(ValueError):\n        fig.set_layout_engine('invalid')\n\n    # Test setting layout engine with kwargs\n    fig.set_layout_engine('constrained', w_pad=2, h_pad=3)\n    engine = fig.get_layout_engine()\n    assert engine.w_pad == 2\n    assert engine.h_pad == 3\n\n    # Test setting layout engine with a custom engine instance\n    custom_engine = ConstrainedLayoutEngine(w_pad=4, h_pad=5)\n    fig.set_layout_engine(custom_engine)\n    assert fig.get_layout_engine() is custom_engine\n"], "sample_526": ["def test_date_converter(converter):\n    with rc_context({\"date.converter\": converter}):\n        fig, ax = plt.subplots()\n        dates = [datetime.date(2000, 1, 1), datetime.date(2000, 1, 2)]\n        ax.plot(dates, [1, 2])\n        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n        ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(ax.xaxis.get_major_locator()))\n        fig.canvas.draw()\n        ticks = ax.get_xticklabels()\n        assert len(ticks) > 0\n        for tick in ticks:\n            assert tick.get_text() != \"\"\n"], "sample_527": ["def test_figure_repr():\n    fig = Figure()\n    repr_fig = repr(fig)\n    assert \"Figure\" in repr_fig\n    assert re.search(r\"size \\d+\\.\\d+x\\d+\\.\\d+\", repr_fig)\n    assert re.search(r\"with \\d+ Axes\", repr_fig)\n\n    fig.add_subplot(111)\n    repr_fig = repr(fig)\n    assert \"Figure\" in repr_fig\n    assert re.search(r\"size \\d+\\.\\d+x\\d+\\.\\d+\", repr_fig)\n    assert re.search(r\"with 1 Axes\", repr_fig)\n"], "sample_528": ["def test_reload_library():\n    \"\"\"Test that reload_library() updates the library and available styles.\"\"\"\n    original_library = style.library.copy()\n    original_available = style.available.copy()\n\n    with temp_style('test_reload', DUMMY_SETTINGS):\n        assert 'test_reload' in style.library\n        assert 'test_reload' in style.available\n\n    style.reload_library()\n    assert style.library == original_library\n    assert style.available == original_available\n"], "sample_529": ["def test_legend_get_texts():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], label='Line 1')\n    line2, = ax.plot([4, 5, 6], label='Line 2')\n    legend = ax.legend(handles=[line1, line2])\n    assert len(legend.get_texts()) == 2\n    assert legend.get_texts()[0].get_text() == 'Line 1'\n    assert legend.get_texts()[1].get_text() == 'Line 2'\n"], "sample_530": ["def test_offsetbox_alignment():\n    fig, ax = plt.subplots()\n    box1 = TextArea(\"Box 1\")\n    box2 = TextArea(\"Box 2\")\n\n    # Test horizontal alignment\n    hpacker = HPacker(children=[box1, box2], align=\"center\")\n    assert_allclose(hpacker.get_extent_offsets(ax.figure.canvas.renderer)[4],\n                    [(0.5, 0), (1.5, 0)])\n\n    # Test vertical alignment\n    vpacker = VPacker(children=[box1, box2], align=\"center\")\n    assert_allclose(vpacker.get_extent_offsets(ax.figure.canvas.renderer)[4],\n                    [(0, 0.5), (0, 1.5)])\n"], "sample_533": ["def test_contourf_lognorm(algorithm):\n    # See https://github.com/matplotlib/matplotlib/issues/21047\n    z = np.array([[0.1, 0.3], [0.2, 0.4]])\n    fig, ax = plt.subplots()\n    with rc_context({\"contour.algorithm\": algorithm}):\n        ax.contourf(z, levels=[0.05, 0.15, 0.25, 0.35], norm=LogNorm())\n    assert_array_almost_equal(ax.collections[0].get_clim(), (0.05, 0.35))\n"], "sample_534": ["def test_contourf_lognorm(algorithm):\n    z = np.array([[0.1, 0.3], [0.2, 0.4]])\n    with rc_context({\"contour.algorithm\": algorithm}):\n        fig, ax = plt.subplots()\n        cs = ax.contourf(z, norm=LogNorm(), levels=[0.1, 0.2, 0.4])\n        assert_array_almost_equal(cs.levels, [0.1, 0.2, 0.4])\n        assert same_color(cs.tcolors[0], cs.cmap(0))\n        assert same_color(cs.tcolors[1], cs.cmap(0.5))\n        assert same_color(cs.tcolors[2], cs.cmap(1))\n"], "sample_535": ["def test_table_edges():\n    fig, ax = plt.subplots()\n    ax.axis('off')\n    table = Table(ax, loc='center')\n    table.add_cell(0, 0, width=1, height=1, text='Cell 1', edges='open')\n    table.add_cell(1, 0, width=1, height=1, text='Cell 2', edges='closed')\n    table.add_cell(0, 1, width=1, height=1, text='Cell 3', edges='horizontal')\n    table.add_cell(1, 1, width=1, height=1, text='Cell 4', edges='vertical')\n    ax.add_table(table)\n    return fig\n"], "sample_537": ["def test_detrend_mean(self):\n    x = np.array([1, 2, 3, 4, 5])\n    expected = np.array([-2, -1, 0, 1, 2])\n    assert_array_equal(mlab.detrend_mean(x), expected)\n\n    x = np.array([[1, 2], [3, 4], [5, 6]])\n    expected = np.array([[-2, -2], [-1, -1], [0, 0]])\n    assert_array_equal(mlab.detrend_mean(x, axis=0), expected)\n"], "sample_538": ["def test_transformed_bbox():\n    # Create a sample bbox.\n    bbox = mtransforms.Bbox([[1, 2], [3, 4]])\n\n    # Create an affine transform.\n    transform = mtransforms.Affine2D().scale(2).translate(1, 1)\n\n    # Create a TransformedBbox.\n    tbbox = mtransforms.TransformedBbox(bbox, transform)\n\n    # Check that the transformed bbox is correct.\n    assert_array_almost_equal(tbbox.get_points(),\n                              np.array([[3, 5], [7, 9]]))\n\n    # Check that the inverted method returns the correct transform.\n    inverted_transform = tbbox.get_transform().inverted()\n    assert_array_almost_equal(inverted_transform.transform(np.array([3, 5])),\n                              np.array([1, 2]))\n"], "sample_540": ["def test_animation_save_with_null_writer(anim):\n    writer = NullMovieWriter()\n    anim.save('foo.mp4', writer=writer)\n    assert writer.fig is anim._fig\n    assert writer.outfile == 'foo.mp4'\n    assert writer.dpi == mpl.rcParams['savefig.dpi']\n    assert writer.args == ()\n    assert writer.savefig_kwargs == {}\n    assert writer._count == 5\n"], "sample_544": ["def test_pil_png_to_float_array():\n    # Create a sample PNG image with different pixel values.\n    img = Image.new('RGB', (2, 2))\n    img.putpixel((0, 0), (255, 0, 0))  # Red\n    img.putpixel((1, 0), (0, 255, 0))  # Green\n    img.putpixel((0, 1), (0, 0, 255))  # Blue\n    img.putpixel((1, 1), (255, 255, 255))  # White\n\n    # Save the image to a bytes buffer.\n    buf = io.BytesIO()\n    img.save(buf, format='PNG')\n    buf.seek(0)\n\n    # Load the image using PIL and convert it to a float array.\n    pil_img = Image.open(buf)\n    float_array = mimage._pil_png_to_float_array(pil_img)\n\n    # Check that the resulting array has the correct shape and values.\n    assert float_array.shape == (2, 2, 3)\n    assert np.allclose(float_array[0, 0], [1.0, 0.0, 0.0])  # Red\n    assert np.allclose(float_array[1, 0], [0.0, 1.0, 0.0])  # Green\n    assert np.allclose(float_array[0, 1], [0.0, 0.0, 1.0])  # Blue\n    assert np.allclose(float_array[1, 1], [1.0, 1.0, 1.0])  # White\n"], "sample_547": ["def test_offsetbox_alignment():\n    fig, ax = plt.subplots()\n    box1 = TextArea(\"Box 1\")\n    box2 = TextArea(\"Box 2\")\n    box3 = TextArea(\"Box 3\")\n\n    hpack = HPacker(children=[box1, box2, box3], align=\"center\")\n    ax.add_artist(hpack)\n\n    vpack = VPacker(children=[box1, box2, box3], align=\"baseline\")\n    ax.add_artist(vpack)\n\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n\n    assert_allclose(hpack.get_bbox(fig.canvas.get_renderer()).x0, 5.0)\n    assert_allclose(vpack.get_bbox(fig.canvas.get_renderer()).y0, 5.0)\n"], "sample_548": ["def test_colorbar_remove():\n    fig, ax = plt.subplots()\n    m = ax.pcolormesh(np.random.rand(10, 10))\n    cbar = fig.colorbar(m)\n    assert len(fig.axes) == 2\n    cbar.remove()\n    assert len(fig.axes) == 1\n"], "sample_555": ["def test_fancyarrowpatch_connectionstyle():\n    fig, ax = plt.subplots()\n    arrow = FancyArrowPatch((0.1, 0.1), (0.8, 0.8),\n                            connectionstyle=\"arc3,rad=0.2\",\n                            mutation_scale=20,\n                            fc=\"r\", ec=\"b\")\n    ax.add_patch(arrow)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    assert isinstance(arrow.get_connectionstyle(), mpatches.ConnectionStyle)\n"], "sample_558": ["def test_imagegrid_cbar_mode():\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode='each')\n    assert len(grid.cbar_axes) == 4\n\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode='single')\n    assert len(grid.cbar_axes) == 1\n\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode='edge')\n    assert len(grid.cbar_axes) == 2\n\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode=None)\n    assert len(grid.cbar_axes) == 4\n    for ax in grid.cbar_axes:\n        assert not ax.get_visible()\n"], "sample_560": ["def test_legend_alignment(align):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label=\"test\")\n    legend = ax.legend(loc=\"upper right\", alignment=align)\n    assert legend.get_alignment() == align\n"], "sample_561": ["def test_marker_fillstyle(fig_test, fig_ref):\n    fig_test.subplots()\n    marker = markers.MarkerStyle(marker=\"o\", fillstyle=\"left\")\n    fig_ref.subplots().plot([1], marker=marker)\n    fig_test.subplots().plot([1], marker=marker)\n"], "sample_563": ["def test_AnnotationBbox():\n    fig, ax = plt.subplots()\n    offsetbox = TextArea(\"Test\", minimumdescent=False)\n    ab = AnnotationBbox(offsetbox, (0.5, 0.5), xycoords='data',\n                        boxcoords=\"offset points\", pad=0.25)\n\n    ax.add_artist(ab)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    assert ab.contains(MouseEvent('test', fig.canvas, 100, 100))[0]\n    assert not ab.contains(MouseEvent('test', fig.canvas, 0, 0))[0]\n"], "sample_565": ["def test_inset_axes():\n    fig, ax = plt.subplots()\n    inset_ax = inset_axes(ax, width='40%', height='30%', loc='lower left')\n    assert isinstance(inset_ax, mpl_toolkits.axes_grid1.mpl_axes.HostAxes)\n    assert inset_ax.get_position().x0 < 0.5\n    assert inset_ax.get_position().y0 < 0.5\n\n    # Test with bbox_to_anchor and bbox_transform\n    inset_ax2 = inset_axes(ax, width='40%', height='30%', loc='upper right',\n                           bbox_to_anchor=(0, 0, 1, 1),\n                           bbox_transform=ax.transAxes)\n    assert inset_ax2.get_position().x0 > 0.5\n    assert inset_ax2.get_position().y0 > 0.5\n\n    # Test with axes_class and axes_kwargs\n    inset_ax3 = inset_axes(ax, width='40%', height='30%', loc='center',\n                           axes_class=mpl_toolkits.axes_grid1.mpl_axes.Axes,\n                           axes_kwargs={'facecolor': 'red'})\n    assert isinstance(inset_ax3, mpl_toolkits.axes_grid1.mpl_axes.Axes)\n    assert inset_ax3.get_facecolor() == (1.0, 0.0, 0.0, 1.0)\n"], "sample_567": ["def test_text_set_bbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Test')\n    text.set_bbox(dict(facecolor='red', alpha=0.5))\n    assert text.get_bbox_patch() is not None\n    assert text.get_bbox_patch().get_facecolor() == (1, 0, 0, 0.5)\n"], "sample_570": ["def test_kde_univariate(self, x):\n    kde = KDE()\n    density, support = kde(x)\n    assert_array_almost_equal(self.integrate(density, support), 1)\n"], "sample_572": ["def test_kde_univariate(self, x):\n    kde = KDE()\n    density, support = kde(x)\n    assert_array_almost_equal(self.integrate(density, support), 1)\n"], "sample_573": ["def test_polyfit_single_group(self, df):\n\n    groupby = GroupBy([\"group\"])\n    stat = PolyFit()\n    result = stat(df, groupby, \"x\", None)\n\n    assert isinstance(result, pd.DataFrame)\n    assert result.shape == (stat.gridsize, 2)\n    assert_array_almost_equal(result[\"x\"].min(), df[\"x\"].min())\n    assert_array_almost_equal(result[\"x\"].max(), df[\"x\"].max())\n"], "sample_578": ["def test_bar_color_alpha_fill(self):\n\n    x = [1, 2, 3]\n    y = [1, 2, 3]\n    color = to_rgba_array([\"red\", \"green\", \"blue\"])\n    alpha = [.5, .6, .7]\n    fill = [True, False, True]\n\n    bars = self.plot_bars(\n        {\"x\": x, \"y\": y},\n        {\"color\": color, \"alpha\": alpha, \"fill\": fill},\n        {},\n    )\n\n    for bar, c, a, f in zip(bars, color, alpha, fill):\n        facecolor = bar.get_facecolor()\n        assert_array_equal(to_rgba_array(facecolor), [c])\n        assert bar.get_alpha() == pytest.approx(a)\n        assert (facecolor[:, 3] > 0) == f\n"], "sample_579": ["def test_heatmap_dataframe_index_name(self):\n    p = mat._HeatMapper(self.df_norm, **self.default_kws)\n    assert p.xlabel == \"letters\"\n    assert p.ylabel is None\n"], "sample_580": ["def test_variable_type_numeric():\n    a = pd.Series([1, 2, 3, np.nan])\n    assert variable_type(a) == VarType(\"numeric\")\n\n    b = pd.Series([1.0, 2.0, 3.0, np.nan])\n    assert variable_type(b) == VarType(\"numeric\")\n\n    c = pd.Series([\"1\", \"2\", \"3\"], dtype=\"category\")\n    assert variable_type(c) != VarType(\"numeric\")\n\n    d = pd.Series([1, 2, \"3\"])\n    assert variable_type(d) != VarType(\"numeric\")\n"], "sample_582": ["def test_cli_with_appcontext():\n    app = Flask(__name__)\n\n    @app.cli.command()\n    @with_appcontext\n        click.echo(current_app.name)\n\n    runner = CliRunner()\n    result = runner.invoke(app.cli, [\"hello\"])\n    assert result.exit_code == 0\n    assert result.output.strip() == app.name\n"], "sample_584": ["def test_infer_concat_order_from_coords_edge_cases():\n    # Ensure that _infer_concat_order_from_coords raises an error when it\n    # encounters conflicting or inconsistent coordinates.\n\n    # Create sample datasets with conflicting coordinates.\n    ds1 = Dataset()\n    ds1['x'] = DataArray([1, 2, 3], dims='x')\n    ds2 = Dataset()\n    ds2['x'] = DataArray([4, 5, 6], dims='x')\n    ds3 = Dataset()\n    ds3['x'] = DataArray([1, 2, 3], dims='x')  # conflicts with ds1\n\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds1, ds2, ds3])\n\n    # Create sample datasets with inconsistent coordinate lengths.\n    ds4 = Dataset()\n    ds4['x'] = DataArray([1, 2], dims='x')\n    ds5 = Dataset()\n    ds5['x'] = DataArray([3, 4, 5], dims='x')\n\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds4, ds5])\n"], "sample_585": ["def test_consolidate_slices():\n    slices = [slice(0, 1), slice(1, 3), slice(3, 5)]\n    expected = [slice(0, 5)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 1), slice(2, 3), slice(4, 5)]\n    expected = [slice(0, 1), slice(2, 3), slice(4, 5)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 1), slice(1, 2), slice(2, 3), slice(3, 4)]\n    expected = [slice(0, 4)]\n    assert _consolidate_slices(slices) == expected\n"], "sample_586": ["def test_concat_dataset_dim_order():\n    # Test that the order of dimensions is preserved when concatenating datasets.\n    # This is a regression test for GH issue #3475.\n\n    # Create two datasets with different dimension orders.\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"x\", [1, 2])\n    ds1[\"y\"] = (\"y\", [3, 4])\n    ds1[\"z\"] = ((\"x\", \"y\"), [[5, 6], [7, 8]])\n\n    ds2 = Dataset()\n    ds2[\"y\"] = (\"y\", [3, 4])\n    ds2[\"x\"] = (\"x\", [1, 2])\n    ds2[\"z\"] = ((\"x\", \"y\"), [[9, 10], [11, 12]])\n\n    # Concatenate the datasets along a new dimension.\n    concatenated = concat([ds1, ds2], dim=\"new_dim\")\n\n    # Check that the order of dimensions is preserved.\n    assert concatenated.dims == (\"new_dim\", \"x\", \"y\")\n"], "sample_588": ["def test_infer_concat_order_from_coords_edge_cases():\n    # no varying dimension coordinates\n    ds1 = Dataset()\n    ds2 = Dataset()\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds1, ds2])\n\n    # coordinate present in some datasets but not others\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"x\", [1])\n    ds2 = Dataset()\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds1, ds2])\n\n    # dimension present in some datasets but not others\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"x\", [1])\n    ds2 = Dataset()\n    ds2[\"y\"] = (\"y\", [1])\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds1, ds2])\n\n    # non-monotonic coordinate values\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"x\", [1, 2, 3])\n    ds2 = Dataset()\n    ds2[\"x\"] = (\"x\", [4, 5, 1])\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds1, ds2])\n"], "sample_589": ["def test_interp_limit(da):\n    actual = da.interp(time=da.time, limit=2)\n    expected = xr.DataArray(\n        [0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims=\"time\"\n    )\n    assert_array_equal(actual, expected)\n\n    actual = da.interp(time=da.time, limit=1)\n    expected = xr.DataArray(\n        [0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims=\"time\"\n    )\n    assert_array_equal(actual, expected)\n\n    actual = da.interp(time=da.time, limit=0)\n    expected = xr.DataArray(\n        [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan],\n        dims=\"time\",\n    )\n    assert_array_equal(actual, expected)\n"], "sample_590": ["def test_concat_dataset_dim_order():\n    # Test that the order of dimensions is preserved when concatenating datasets.\n    # This requires that we create datasets with different dimension orders.\n\n    # Create two datasets with different dimension orders.\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"x\", [1, 2, 3])\n    ds1[\"y\"] = (\"y\", [4, 5, 6])\n    ds1[\"z\"] = ((\"x\", \"y\"), np.random.randn(3, 3))\n\n    ds2 = Dataset()\n    ds2[\"y\"] = (\"y\", [4, 5, 6])\n    ds2[\"x\"] = (\"x\", [1, 2, 3])\n    ds2[\"z\"] = ((\"x\", \"y\"), np.random.randn(3, 3))\n\n    # Concatenate the datasets along a new dimension.\n    concatenated = concat([ds1, ds2], dim=\"new_dim\")\n\n    # Check that the order of dimensions is preserved.\n    assert concatenated.dims == {\"new_dim\": 2, \"x\": 3, \"y\": 3}\n"], "sample_592": ["def test_pretty_print():\n    max_name_length = 10\n    col_width = formatting._calculate_col_width([\"short\", \"very_long_variable_name\"])\n    assert col_width == max_name_length + 6\n\n    short_name = \"short\"\n    long_name = \"very_long_variable_name\"\n\n    # short name should be padded to col_width\n    assert len(formatting.pretty_print(short_name, col_width)) == col_width\n\n    # long name should be truncated and padded to col_width\n    assert len(formatting.pretty_print(long_name, col_width)) == col_width\n"], "sample_593": ["def test_array_repr_multiindex(multiindex):\n    html = fh.array_repr(multiindex[\"x\"])\n    assert \"level_1\" in html\n    assert \"level_2\" in html\n    assert \"(a, 1)\" in html\n    assert \"(b, 2)\" in html\n"], "sample_594": ["def test_pretty_print():\n    max_name_length = 10\n    col_width = formatting._calculate_col_width([\"short\", \"very_long_variable_name\"])\n    assert col_width == max_name_length + 6\n\n    short_name = \"short\"\n    long_name = \"very_long_variable_name\"\n\n    # short name should be padded to col_width\n    assert len(formatting.pretty_print(short_name, col_width)) == col_width\n\n    # long name should be truncated and ellipsis added\n    assert (\n        formatting.pretty_print(long_name, col_width)\n        == long_name[: max_name_length - 3] + \"...\"\n    )\n"], "sample_596": ["def test_concat_dataset_dim_order():\n    # Test that the order of dimensions is preserved when concatenating datasets.\n    # This is a regression test for GH issue #3475.\n\n    # Create two datasets with different dimension orders.\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"x\", [1, 2])\n    ds1[\"y\"] = (\"y\", [3, 4])\n    ds1[\"z\"] = ((\"x\", \"y\"), [[5, 6], [7, 8]])\n\n    ds2 = Dataset()\n    ds2[\"y\"] = (\"y\", [3, 4])\n    ds2[\"x\"] = (\"x\", [1, 2])\n    ds2[\"z\"] = ((\"x\", \"y\"), [[5, 6], [7, 8]])\n\n    # Concatenate the datasets along the x dimension.\n    concatenated = concat([ds1, ds2], dim=\"x\")\n\n    # Check that the order of dimensions in the concatenated dataset is correct.\n    assert concatenated.dims == {\"x\": 4, \"y\": 2}\n    assert concatenated[\"z\"].dims == (\"x\", \"y\")\n"], "sample_597": ["def test_merge_collected():\n    # Test merge_collected function with different compat options\n    collected = {\n        \"var1\": [(xr.Variable(), None), (xr.Variable(), None)],\n        \"var2\": [(xr.Variable(), pd.Index([1, 2, 3])), (xr.Variable(), pd.Index([1, 2, 3]))],\n    }\n\n    # Test with compat=\"identical\"\n    with pytest.raises(MergeError):\n        merge.merge_collected(collected, compat=\"identical\")\n\n    # Test with compat=\"equals\"\n    result = merge.merge_collected(collected, compat=\"equals\")\n    assert len(result) == 2\n\n    # Test with compat=\"broadcast_equals\"\n    result = merge.merge_collected(collected, compat=\"broadcast_equals\")\n    assert len(result) == 2\n\n    # Test with compat=\"no_conflicts\"\n    result = merge.merge_collected(collected, compat=\"no_conflicts\")\n    assert len(result) == 2\n\n    # Test with compat=\"override\"\n    result = merge.merge_collected(collected, compat=\"override\")\n    assert len(result) == 2\n"], "sample_598": ["def test_pretty_print():\n    max_name_length = 10\n    col_width = formatting._calculate_col_width([\"short\", \"very_long_variable_name\"])\n    assert col_width == max_name_length + 6\n\n    short_name = \"short\"\n    pretty_short_name = formatting.pretty_print(short_name, col_width)\n    assert len(pretty_short_name) == col_width\n    assert pretty_short_name.strip() == short_name\n\n    long_name = \"very_long_variable_name\"\n    pretty_long_name = formatting.pretty_print(long_name, col_width)\n    assert len(pretty_long_name) == col_width\n    assert pretty_long_name.strip() == long_name[:col_width]\n"], "sample_599": ["def test_CFScaleOffsetCoder_encode_decode():\n    coder = variables.CFScaleOffsetCoder()\n    data = np.array([1, 2, 3], dtype=np.float32)\n    scale_factor = 2.0\n    add_offset = 1.0\n    encoding = {\"scale_factor\": scale_factor, \"add_offset\": add_offset}\n    attrs = {}\n    variable = xr.Variable((\"x\",), data, attrs, encoding)\n\n    encoded_var = coder.encode(variable)\n    assert_allclose(encoded_var.data, (data - add_offset) / scale_factor)\n\n    decoded_var = coder.decode(encoded_var)\n    assert_identical(decoded_var, variable)\n"], "sample_600": ["def test_CFMaskCoder_encode():\n    coder = variables.CFMaskCoder()\n    data = np.array([1, 2, np.nan, 4])\n    variable = xr.Variable((\"x\",), data, {\"_FillValue\": 3})\n    encoded_variable = coder.encode(variable)\n    assert_identical(encoded_variable, xr.Variable((\"x\",), [1, 2, 3, 4]))\n"], "sample_601": ["def test_season(self):\n    seasons = self.data.time.dt.season\n    assert_array_equal(\n        seasons.values,\n        np.array(\n            [\n                \"DJF\"\n                if (month == 12 or month < 3)\n                else \"MAM\"\n                if month < 6\n                else \"JJA\"\n                if month < 9\n                else \"SON\"\n                for month in self.times.month\n            ]\n        ),\n    )\n"], "sample_602": ["def test_get_default_engine_remote_uri():\n    engine = _get_default_engine(\"http://example.com/test.nc\", allow_remote=True)\n    assert engine == \"netcdf4\"\n"], "sample_603": ["def test_array_section_with_empty_dataarray():\n    arr = xr.DataArray(np.empty((0, 0)))\n    html = fh.array_section(arr)\n    assert \"xr-array-wrap\" in html\n    assert \"xr-array-preview\" in html\n    assert \"xr-array-data\" in html\n"], "sample_604": ["def test_inline_variable_array_repr():\n    # Test that inline_variable_array_repr works with different types of data\n    array = xr.DataArray(np.arange(10), dims=\"x\")\n    assert formatting.inline_variable_array_repr(array, 80) == \"[0 1 2 ... 7 8 9]\"\n\n    array = xr.DataArray(np.random.rand(3, 4), dims=(\"x\", \"y\"))\n    assert formatting.inline_variable_array_repr(array, 80).startswith(\"[[\")\n"], "sample_605": ["def test_groupby_consolidate_slices():\n    slices = [slice(0, 1), slice(1, 2), slice(2, 3)]\n    expected = [slice(0, 3)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 1), slice(2, 3)]\n    expected = [slice(0, 1), slice(2, 3)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 1, 2), slice(1, 3, 2)]\n    with pytest.raises(ValueError):\n        _consolidate_slices(slices)\n"], "sample_607": ["def test_remove_duplicates(dummy_duplicated_entrypoints):\n    unique_eps = plugins.remove_duplicates(dummy_duplicated_entrypoints)\n    assert len(unique_eps) == 2\n    names = [ep.name for ep in unique_eps]\n    assert names == [\"engine1\", \"engine2\"]\n"], "sample_608": ["def test_inline_variable_array_repr():\n    # Test that inline_variable_array_repr works with different types of data\n    array = xr.DataArray(np.array([1, 2, 3]))\n    assert formatting.inline_variable_array_repr(array, 80) == \"[1 2 3]\"\n\n    array = xr.DataArray(np.array([1.0, 2.0, 3.0]))\n    assert formatting.inline_variable_array_repr(array, 80) == \"[1. 2. 3.]\"\n\n    array = xr.DataArray(np.array([\"a\", \"b\", \"c\"], dtype=\"object\"))\n    assert formatting.inline_variable_array_repr(array, 80) == \"['a' 'b' 'c']\"\n"], "sample_609": ["def test_apply_ufunc_signature():\n    sig = _UFuncSignature([(\"x\", \"y\")], [(\"z\",)])\n    assert sig.num_inputs == 1\n    assert sig.num_outputs == 1\n    assert sig.all_input_core_dims == {\"x\", \"y\"}\n    assert sig.all_output_core_dims == {\"z\"}\n    assert sig.all_core_dims == {\"x\", \"y\", \"z\"}\n    assert sig.dims_map == {\"x\": \"dim0\", \"y\": \"dim1\", \"z\": \"dim2\"}\n\n    sig = _UFuncSignature([(\"x\",), (\"y\",)], [(\"z\",)])\n    assert sig.num_inputs == 2\n    assert sig.num_outputs == 1\n    assert sig.all_input_core_dims == {\"x\", \"y\"}\n    assert sig.all_output_core_dims == {\"z\"}\n    assert sig.all_core_dims == {\"x\", \"y\", \"z\"}\n    assert sig.dims_map == {\"x\": \"dim0\", \"y\": \"dim1\", \"z\": \"dim2\"}\n"], "sample_610": ["def test_parse_iso8601_like_invalid_string():\n    with pytest.raises(ValueError):\n        parse_iso8601_like(\"1999-01-32\")\n"], "sample_611": ["def test_cftime_offsets_rule_code(offset, expected_freq):\n    assert offset.rule_code() == expected_freq\n"], "sample_612": ["def test_groupby_concat_dim_order(dataset):\n    # GH 4220\n    expected = dataset.groupby(\"x\").mean(dim=\"y\")\n    actual = dataset.groupby(\"x\").mean(dim=[\"y\"])\n    assert_identical(expected, actual)\n"], "sample_613": ["def test_groupby_consolidate_slices():\n    slices = [slice(0, 1), slice(1, 2), slice(2, 3)]\n    expected = [slice(0, 3)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 1), slice(2, 3), slice(1, 2)]\n    expected = [slice(0, 1), slice(1, 3)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 1), slice(1, 2), slice(4, 5)]\n    expected = [slice(0, 2), slice(4, 5)]\n    assert _consolidate_slices(slices) == expected\n"], "sample_614": ["def test_format_timestamp():\n    # Test that NaT is formatted correctly\n    assert formatting.format_timestamp(pd.NaT) == \"NaT\"\n\n    # Test that a Timestamp object is formatted correctly\n    timestamp = pd.Timestamp(\"2022-01-01 12:00:00\")\n    assert formatting.format_timestamp(timestamp) == \"2022-01-01T12:00:00\"\n\n    # Test that a datetime64 object is formatted correctly\n    datetime64 = np.datetime64(\"2022-01-01T12:00:00\")\n    assert formatting.format_timestamp(datetime64) == \"2022-01-01T12:00:00\"\n"], "sample_619": ["def test_encode_cf_datetime_overflow_error():\n    # Test that encode_cf_datetime raises an OverflowError when the dates are too far away from the reference date.\n    dates = pd.date_range(\"2500-01-01\", periods=10)\n    with pytest.raises(OverflowError):\n        encode_cf_datetime(dates, units=\"days since 2000-01-01\")\n"], "sample_620": ["def test_concat_dataset_dim_order():\n    # Test that the order of dimensions is preserved with concat.\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"x\", [1])\n    ds1[\"y\"] = (\"y\", [1])\n    ds1[\"z\"] = (\"z\", [1])\n\n    ds2 = Dataset()\n    ds2[\"x\"] = (\"x\", [2])\n    ds2[\"y\"] = (\"y\", [2])\n    ds2[\"z\"] = (\"z\", [2])\n\n    expected = Dataset()\n    expected[\"x\"] = (\"x\", [1, 2])\n    expected[\"y\"] = (\"y\", [1])\n    expected[\"z\"] = (\"z\", [1])\n\n    result = concat([ds1, ds2], dim=\"x\")\n    assert_identical(result, expected)\n"], "sample_622": ["def test_decode_cf_variables_bounds_attribute():\n    original = Dataset()\n    original[\"time\"] = (\"time\", pd.date_range(\"2000-01-01\", periods=3))\n    original[\"time_bnds\"] = (\n        (\"time\", \"bnds\"),\n        [\n            [pd.Timestamp(\"1999-12-31\"), pd.Timestamp(\"2000-01-01\")],\n            [pd.Timestamp(\"2000-01-01\"), pd.Timestamp(\"2000-01-02\")],\n            [pd.Timestamp(\"2000-01-02\"), pd.Timestamp(\"2000-01-03\")],\n        ],\n    )\n    original[\"time\"].attrs[\"bounds\"] = \"time_bnds\"\n\n    expected = original.copy(deep=True)\n\n    # simulate encoding/decoding to CF by removing the time variable's bounds attribute\n    encoded = original.copy(deep=True)\n    del encoded[\"time\"].attrs[\"bounds\"]\n\n    decoded, _, _ = conventions.decode_cf_variables(\n        encoded._variables,\n        encoded.attrs,\n        decode_times=True,\n        decode_coords=True,\n    )\n\n    result = Dataset(decoded)\n    assert_identical(result, expected)\n"], "sample_623": ["def test_open_dataset_invalid_netcdf():\n    with pytest.raises(ValueError) as excinfo:\n        xr.open_dataset(\"example.nc\", engine=\"invalid\")\n    assert \"unrecognized engine\" in str(excinfo.value)\n"], "sample_627": ["def test_concat_dataset_with_empty_data_vars():\n    # Test that concatenating datasets with empty data variables works correctly\n    ds1 = Dataset(coords={\"x\": [1, 2, 3]})\n    ds2 = Dataset(coords={\"x\": [4, 5, 6]})\n\n    result = concat([ds1, ds2], dim=\"x\")\n\n    expected = Dataset(coords={\"x\": [1, 2, 3, 4, 5, 6]})\n    assert_identical(result, expected)\n"], "sample_628": ["def test_spelling_suggestions(self):\n    \"\"\"Test that suggestions are provided for misspelled words.\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        # tezt is a misspelling of 'test'\n        tezt = 1\n        \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\n            msg_id=\"wrong-spelling-in-comment\",\n            line=3,\n            args=(\"tezt\", \"# tezt is a misspelling of 'test'\\n\", \"     ^^^^\", self._get_msg_suggestions(\"tezt\")),\n        )\n    ):\n        self.checker.process_tokens(_tokenize_str(node.as_string()))\n"], "sample_629": ["def test_get_python_path(tmpdir):\n    \"\"\"Test that get_python_path returns the correct path.\"\"\"\n    some_file = tmpdir.join(\"some_file.py\")\n    some_file.write(\"\")\n    assert get_python_path(str(some_file)) == str(tmpdir)\n\n    some_package = tmpdir.mkdir(\"some_package\")\n    init_file = some_package.join(\"__init__.py\")\n    init_file.write(\"\")\n    assert get_python_path(str(some_package)) == str(tmpdir)\n"], "sample_630": ["def test_vcg_writer(setup, tmpdir):\n    project = get_project(os.path.join(os.path.dirname(__file__), \"data\"))\n    linker = Linker(project)\n    CONFIG = Config()\n    CONFIG.output_format = \"vcg\"\n    handler = DiadefsHandler(CONFIG)\n    dd = DefaultDiadefGenerator(linker, handler).visit(project)\n    for diagram in dd:\n        diagram.extract_relationships()\n    writer = VCGWriter(CONFIG)\n    writer.write(dd)\n    vcg_files = [\"packages_No_Name.vcg\", \"classes_No_Name.vcg\"]\n    for fname in vcg_files:\n        assert os.path.exists(fname)\n        with open(fname) as f:\n            contents = f.read()\n            assert contents.startswith(\"graph:\")\n        os.remove(fname)\n"], "sample_633": ["def test_filter_noncode_lines():\n    lineset1 = similar.LineSet(\n        \"test_file1\",\n        [\n            \"# Comment line\\n\",\n            \"def function():\\n\",\n            '    \"\"\"Docstring\"\"\"',\n            \"    pass\\n\",\n            \"    # Another comment\\n\",\n        ],\n    )\n    lineset2 = similar.LineSet(\n        \"test_file2\",\n        [\n            \"# Comment line\\n\",\n            \"def function():\\n\",\n            '    \"\"\"Docstring\"\"\"',\n            \"    pass\\n\",\n            \"    # Another comment\\n\",\n        ],\n    )\n\n    stindex_1 = 0\n    stindex_2 = 0\n    common_lines_nb = 5\n\n    filtered_lines = similar.filter_noncode_lines(\n        lineset1, stindex_1, lineset2, stindex_2, common_lines_nb\n    )\n\n    assert filtered_lines == 2\n"], "sample_634": ["def test_get_python_path():\n    \"\"\"Test that get_python_path returns the correct path.\"\"\"\n    filepath = \"/path/to/module.py\"\n    assert get_python_path(filepath) == \"/path/to\"\n\n    filepath = \"/path/to/package/__init__.py\"\n    assert get_python_path(filepath) == \"/path/to\"\n\n    filepath = \"/path/to/package/submodule.py\"\n    assert get_python_path(filepath) == \"/path/to/package\"\n"], "sample_635": ["    def test_property_type(self):\n        \"\"\"Test that property type is correctly parsed from docstring\"\"\"\n        node = astroid.extract_node(\"\"\"\n            class MyClass:\n                @property\n                    \\\"\\\"\\\"\n                    :type: int\n                    \\\"\\\"\\\"\n                    return 42\n        \"\"\")\n        with set_config_directly(self.checker, {'default_docstring_type': 'sphinx'}):\n            self.checker.visit_functiondef(node.body[0])\n            self.assert_no_messages()\n"], "sample_637": ["    def test_by_id_managed_messages(self):\n        linter = self.linter\n        linter._by_id_managed_msgs = [\n            (\"module_name\", \"msgid\", \"symbol\", 1, False),\n            (\"other_module\", \"msgid\", \"symbol\", 2, True),\n        ]\n        node = self.extract_node(\n            \"\"\"\n            # pylint: disable=msgid\n            \"\"\"\n        )\n        node.name = \"module_name\"\n        with self.assertAddsMessages(\n            MessageTest(\"use-symbolic-message-instead\", line=1)\n        ):\n            self.checker.process_module(node)\n"], "sample_638": ["def test_run_with_invalid_format(mock_graphviz, capsys):\n    with mock.patch(\"sys.argv\", [\"pyreverse\", \"--output-format=invalid\"]):\n        with pytest.raises(SystemExit) as exc_info:\n            main.Run([\"dummy_package\"])\n        assert exc_info.value.code == 0\n        captured = capsys.readouterr()\n        assert \"Format invalid is not supported natively.\" in captured.out\n"], "sample_639": ["def test_get_full_documentation():\n    checker = OtherBasicChecker()\n    doc = checker.get_full_documentation(\n        msgs=checker.msgs, options=checker.options_and_values(), reports=checker.reports\n    )\n    assert \"basic checker\" in doc\n    assert \"basic-checker-example\" in doc\n    assert \"Used nowhere and serves no purpose.\" in doc\n\n    checker_with_options = LessBasicChecker()\n    doc_with_options = checker_with_options.get_full_documentation(\n        msgs=checker_with_options.msgs,\n        options=checker_with_options.options_and_values(),\n        reports=checker_with_options.reports,\n    )\n    assert \"example-args\" in doc_with_options\n    assert \"<int>\" in doc_with_options\n    assert \"Example of integer argument for the checker.\" in doc_with_options\n"], "sample_641": ["def test_load_results(tmp_path):\n    stats = LinterStats()\n    save_results(stats, tmp_path / \"test\", pylint_home=tmp_path)\n    loaded_stats = load_results(tmp_path / \"test\", pylint_home=tmp_path)\n    assert loaded_stats == stats\n\n    # Test loading non-existent file\n    assert load_results(tmp_path / \"non_existent\", pylint_home=tmp_path) is None\n\n    # Test loading invalid data\n    with open(_get_pdata_path(tmp_path / \"invalid\", 1, tmp_path), \"wb\") as stream:\n        stream.write(b\"Invalid data\")\n    assert load_results(tmp_path / \"invalid\", pylint_home=tmp_path) is None\n"], "sample_642": ["def test_preprocess_options() -> None:\n    \"\"\"Test _preprocess_options function.\"\"\"\n    run = Run([\"test.py\"])\n    args = [\"--init-hook=import os\", \"--rcfile=pylintrc\", \"--output=output.txt\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == []\n    assert run._rcfile == \"pylintrc\"\n    assert run._output == \"output.txt\"\n\n    # Test with abbreviations\n    args = [\"--init-h=import os\", \"--rcf=pylintrc\", \"--out=output.txt\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == []\n    assert run._rcfile == \"pylintrc\"\n    assert run._output == \"output.txt\"\n\n    # Test with invalid options\n    args = [\"--invalid-option\"]\n    with pytest.raises(config.ArgumentPreprocessingError):\n        config._preprocess_options(run, args)\n\n    # Test with options that don't take arguments\n    args = [\"--verbose\", \"-v\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == []\n    assert run.verbose\n\n    # Test with enable-all-extensions option\n    args = [\"--enable-all-extensions\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == []\n    assert len(run._plugins) > 0\n"], "sample_643": ["def test_colorized_text_reporter_color_mapping_deprecation(\n    capsys: pytest.CaptureFixture[str],"], "sample_645": ["def test_caplog_set_level(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            caplog.set_level(logging.INFO, logger=\"root\")\n            logging.getLogger(\"root\").info(\"1\")\n            logging.getLogger(\"foo\").info(\"2\")\n\n            caplog.set_level(logging.INFO)\n            logging.getLogger(\"root\").info(\"3\")\n            logging.getLogger(\"foo\").info(\"4\")\n\n            assert [r.message for r in caplog.records] == [\"1\", \"3\", \"4\"]\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_646": ["def test_unittest_skip_reason(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestFoo(unittest.TestCase):\n            @unittest.skip(\"reason\")\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"SKIP [1] *\"])\n"], "sample_647": ["def test_warn_explicit_for(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n        from _pytest.warning_types import warn_explicit_for, PytestWarning\n\n            pass\n\n        warn_explicit_for(foo, PytestWarning(\"some warning\"))\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n    assert \"some warning\" in result.stdout.str()\n    assert \"at test_warn_explicit_for.py:5\" in result.stdout.str()\n"], "sample_648": ["def test_mark_decorator_with_args(self) -> None:\n    mark = MarkDecorator(Mark(\"test_mark\", (), {}))\n    new_mark = mark.with_args(\"arg1\", \"arg2\", kwarg1=\"value1\")\n    assert new_mark.mark.name == \"test_mark\"\n    assert new_mark.mark.args == (\"arg1\", \"arg2\")\n    assert new_mark.mark.kwargs == {\"kwarg1\": \"value1\"}\n"], "sample_649": ["def test_catching_logs_context_manager(request: FixtureRequest) -> None:\n    \"\"\"Test that the catching_logs context manager properly sets and restores\n    the log level of the handler.\"\"\"\n    handler = LogCaptureHandler()\n    original_level = handler.level\n    with catching_logs(handler, level=logging.DEBUG):\n        assert handler.level == logging.DEBUG\n    assert handler.level == original_level\n"], "sample_650": ["def test_catching_logs_context_manager(\n    pytester: Pytester, caplog: pytest.LogCaptureFixture"], "sample_651": ["def test_warnings_checker_matches(pytester: Pytester) -> None:\n    \"\"\"Test that WarningsChecker correctly matches warnings.\"\"\"\n    with pytest.warns(UserWarning, match=\"exact message\") as record:\n        warnings.warn(\"exact message\", UserWarning)\n    assert len(record) == 1\n\n    with pytest.warns(UserWarning, match=r\"regex.*pattern\") as record:\n        warnings.warn(\"regex matching pattern\", UserWarning)\n    assert len(record) == 1\n\n    with pytest.raises(AssertionError):\n        with pytest.warns(UserWarning, match=\"non-matching message\"):\n            warnings.warn(\"other message\", UserWarning)\n"], "sample_652": ["def test_pytest_generate_tests_collect_report(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert metafunc.function.__name__ == \"test_foo\"\n            metafunc.parametrize(\"arg\", [1, 2])\n\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--collect-only\")\n    result.stdout.fnmatch_lines([\"*test_foo[arg=1]*\", \"*test_foo[arg=2]*\"])\n"], "sample_653": ["def test_get_option_ini(config):\n    config.addini(\"log_print\", \"True\")\n    assert get_option_ini(config, \"log_print\") == True\n\n    config.addini(\"log_level\", \"DEBUG\")\n    assert get_option_ini(config, \"log_level\") == \"DEBUG\"\n\n    config.addoption(\"--log-cli-level\", dest=\"log_cli_level\", default=None)\n    assert get_option_ini(config, \"log_cli_level\") is None\n\n    config.option.log_cli_level = \"INFO\"\n    assert get_option_ini(config, \"log_cli_level\") == \"INFO\"\n"], "sample_655": ["def test_capture_manager_suspend_resume_global_capture(capsys):\n    capman = CaptureManager(\"fd\")\n    capman.start_global_capturing()\n    print(\"Hello\")\n    capman.suspend_global_capture()\n    print(\"World\")\n    capman.resume_global_capture()\n    print(\"!\")\n    out, err = capman.read_global_capture()\n    assert out == \"Hello\\n!\\n\"\n    assert err == \"\"\n"], "sample_656": ["def test_capture_manager_suspend_resume(capsys):\n    capmanager = CaptureManager(\"fd\")\n    capmanager.start_global_capturing()\n    print(\"Hello\")\n    capmanager.suspend_global_capture()\n    print(\"World\")\n    capmanager.resume_global_capture()\n    out, err = capmanager.read_global_capture()\n    assert out == \"Hello\\n\"\n    assert err == \"\"\n    capsys.readouterr()  # Clear captured output\n"], "sample_657": ["def test_mark_decorator_with_args_and_kwargs():\n    mark = MarkDecorator(Mark(\"test\", (\"arg1\",), {\"kwarg1\": \"value\"}))\n    assert mark.mark.name == \"test\"\n    assert mark.mark.args == (\"arg1\",)\n    assert mark.mark.kwargs == {\"kwarg1\": \"value\"}\n\n    new_mark = mark.with_args(\"arg2\", kwarg2=\"value\")\n    assert new_mark.mark.name == \"test\"\n    assert new_mark.mark.args == (\"arg1\", \"arg2\")\n    assert new_mark.mark.kwargs == {\"kwarg1\": \"value\", \"kwarg2\": \"value\"}\n"], "sample_658": ["def test_doctest_item_repr_failure(monkeypatch):\n    class DummyDoctestFailure(Exception):\n        pass\n\n    excinfo = pytest.raises(DummyDoctestFailure)\n    monkeypatch.setattr(excinfo, \"errisinstance\", lambda x: True)\n\n    doctest_item = DoctestItem(\"test\", None)\n    failure_repr = doctest_item.repr_failure(excinfo)\n    assert isinstance(failure_repr, doctest_item.ReprFailDoctest)\n"], "sample_660": ["def test_log_passing_tests(testdir):\n    \"\"\"Test that log_passing_tests works as expected.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    resultpath = testdir.tmpdir.join(\"junit.xml\")\n    testdir.runpytest(\"--junitxml=%s\" % resultpath, \"--log-cli-level=INFO\")\n    xmldoc = minidom.parse(str(resultpath))\n    node = DomNode(xmldoc).find_first_by_tag(\"testcase\")\n    assert node.find_first_by_tag(\"system-out\") is None\n\n    testdir.runpytest(\n        \"--junitxml=%s\" % resultpath, \"--log-cli-level=INFO\", \"--junit-logging=no\"\n    )\n    xmldoc = minidom.parse(str(resultpath))\n    node = DomNode(xmldoc).find_first_by_tag(\"testcase\")\n    assert node.find_first_by_tag(\"system-out\") is None\n\n    testdir.runpytest(\n        \"--junitxml=%s\" % resultpath,\n        \"--log-cli-level=INFO\",\n        \"--junit-log-passing-tests\",\n    )\n    xmldoc = minidom.parse(str(resultpath))\n    node = DomNode(xmldoc).find_first_by_tag(\"testcase\")\n    assert node.find_first_by_tag(\"system-out\") is not None\n"], "sample_661": ["def test_junitxml_property(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_property(\"example_key\", 1)\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    props = node.find_by_tag(\"property\")\n    assert len(props) == 1\n    prop = props[0]\n    assert prop[\"name\"] == \"example_key\"\n    assert prop[\"value\"] == \"1\"\n"], "sample_663": ["def test_collect_ignore_collect_path(tmpdir, monkeypatch):\n    \"\"\"Test that pytest_ignore_collect is called for paths.\"\"\"\n    dir1 = tmpdir.mkdir(\"dir1\")\n    dir2 = tmpdir.mkdir(\"dir2\")\n\n        if path == dir1:\n            return True\n        return False\n\n    monkeypatch.setattr(Session, \"gethookproxy\", lambda self, fspath: type(\"\", (), {\"pytest_ignore_collect\": pytest_ignore_collect})())\n\n    session = Session(pytest.config)\n    assert session._collect(dir1) == []\n    assert session._collect(dir2) != []\n"], "sample_664": ["def test_deprecated_external_plugins():\n    assert \"pytest_catchlog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_capturelog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_faulthandler\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n"], "sample_665": ["def test_collect_report_info():\n    \"\"\"Test that the reportinfo() method of a collector returns the correct information.\"\"\"\n    session = Session()\n    config = pytest.config\n    collector = pytest.Collector(\"collector\", parent=session)\n    collector.fspath = py.path.local(\"path/to/collector\")\n    collector.nodeid = \"nodeid\"\n\n    reportinfo = collector.reportinfo()\n\n    assert reportinfo == (\"path/to/collector\", None, \"nodeid\")\n"], "sample_666": ["def test_catch_log_handler():\n    handler = LogCaptureHandler()\n    record = logging.LogRecord(\"test\", logging.INFO, \"test.py\", 10, \"Test message\", None, None)\n    handler.emit(record)\n    assert len(handler.records) == 1\n    assert handler.records[0] == record\n    assert handler.stream.getvalue() == _remove_ansi_escape_sequences(handler.formatter.format(record))\n"], "sample_667": ["def test_tmp_path_factory_basetemp(tmp_path_factory, monkeypatch):\n    basetemp = tmp_path_factory.getbasetemp()\n    assert basetemp.exists()\n\n    # Test that setting the --basetemp option changes the base directory.\n    new_basetemp = Path(\"new-basetemp\")\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", new_basetemp)\n    assert tmp_path_factory.getbasetemp() == new_basetemp\n\n    # Test that the original basetemp is restored after the monkeypatch is undone.\n    monkeypatch.undo()\n    assert tmp_path_factory.getbasetemp() == basetemp\n"], "sample_668": ["def test_deprecated_external_plugins():\n    assert isinstance(deprecated.DEPRECATED_EXTERNAL_PLUGINS, set)\n    assert len(deprecated.DEPRECATED_EXTERNAL_PLUGINS) > 0\n    for plugin in deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n        assert isinstance(plugin, str)\n"], "sample_669": ["def test_capturing_methods(capsys, capfd, capsysbinary, capfdbinary):\n        print(\"Hello\")\n\n        import sys\n        print(\"World\", file=sys.stderr)\n\n        write_to_stdout()\n        write_to_stderr()\n        captured = fixture.readouterr()\n        assert captured.out == \"Hello\\n\"\n        assert captured.err == \"World\\n\"\n\n    test_capture_fixture(capsys)\n    test_capture_fixture(capfd)\n    test_capture_fixture(capsysbinary)\n    test_capture_fixture(capfdbinary)\n"], "sample_670": ["def test_empty_expression():\n    assert not evaluate(\"\", lambda x: True)\n"], "sample_671": ["def test_evaluator_istrue():\n    item = pytest.Item(\"test_func\", None)\n    item.keywords = [\"skipif\"]\n    item.funcargs = {}\n    me = MarkEvaluator(item, \"skipif\")\n    assert not me.istrue()\n\n    item.keywords = [\"skipif('True')\"]\n    me = MarkEvaluator(item, \"skipif\")\n    assert me.istrue()\n\n    item.keywords = [\"skipif('False')\"]\n    me = MarkEvaluator(item, \"skipif\")\n    assert not me.istrue()\n"], "sample_672": ["def test_saferepr_dispatches_to_pretty_printer():\n    class CustomPrettyPrinted:\n            return \"normal repr\"\n\n            p.text(\"pretty repr\")\n\n    assert saferepr(CustomPrettyPrinted()) == \"pretty repr\"\n    assert _pformat_dispatch(CustomPrettyPrinted()) == \"pretty repr\\n\"\n"], "sample_673": ["def test_doctest_namespace():\n    \"\"\"Test that the doctest_namespace fixture is injected into the namespace of doctests.\"\"\"\n    doctest_namespace = {\"my_variable\": \"hello\"}\n    item = DoctestItem.from_parent(\n        parent=DoctestModule.from_parent(parent=None, fspath=\"path/to/module.py\"),\n        name=\"test\",\n        runner=None,\n        dtest=None,\n    )\n    item.fixture_request = pytest.FixtureRequest(item)\n    item.fixture_request._fillfixtures()\n    assert item.fixture_request.getfixturevalue(\"doctest_namespace\") == doctest_namespace\n"], "sample_674": ["def test_node_repr():\n    node = nodes.Node(name=\"test_node\", nodeid=\"node_id\")\n    assert repr(node) == \"<Node test_node>\"\n"], "sample_676": ["def test__plugin_nameversions():\n    plugininfo = [\n        (\"plugin1\", DistInfo(project_name=\"test\", version=1)),\n        (\"plugin2\", DistInfo(project_name=\"pytest-test\", version=1)),\n        (\"plugin3\", DistInfo(project_name=\"test\", version=1)),\n    ]\n    expected = [\"test-1\"]\n    assert _plugin_nameversions(plugininfo) == expected\n"], "sample_677": ["def test_empty_expression():\n    assert not evaluate(\"\", lambda x: True)\n"], "sample_678": ["def test_get_lock_path(tmp_path):\n    path = tmp_path / \"example\"\n    lock_path = get_lock_path(path)\n    assert lock_path == path / \".lock\"\n    assert lock_path.parent == path\n    assert lock_path.name == \".lock\"\n"], "sample_679": ["def test_MarkEvaluator_istrue_condition_string(self, monkeypatch):\n    item = mock.Mock(obj=mock.Mock(__globals__={\"foo\": \"bar\"}))\n    item.istrue.return_value = True\n    mark_evaluator = MarkEvaluator(item, \"my_mark\")\n    mark_evaluator._marks = [Mark(name=\"my_mark\", args=(\"foo == 'bar'\",))]\n    assert mark_evaluator.istrue()\n    assert mark_evaluator.getexplanation() == \"condition: foo == 'bar'\"\n\n        raise SyntaxError(\"invalid syntax\")\n\n    monkeypatch.setattr(sys.modules[__name__], \"compiled_eval\", raise_syntax_error)\n    assert not mark_evaluator.istrue()\n    assert \"SyntaxError: invalid syntax\" in mark_evaluator.getexplanation()\n"], "sample_680": ["def test_evaluate_xfail_mark_strict(self, item):\n    mark = pytest.mark.xfail(reason=\"some reason\", strict=True)\n    item.add_marker(mark)\n    result = evaluate_xfail_marks(item)\n    assert result is not None\n    assert result.reason == \"some reason\"\n    assert result.strict is True\n"], "sample_681": ["def test_catchlog_captured_records(testdir):\n    \"\"\"Test that the captured records are stored in the item's store.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.getLogger().info(\"info message\")\n            logging.getLogger().warning(\"warning message\")\n            assert len(caplog.records) == 2\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.OK\n"], "sample_682": ["def test_evaluate_xfail_mark_strict(self, testdir):\n    item = testdir.getitem(\"def test_func(): pass\")\n    mark = pytest.mark.xfail(reason=\"some reason\", strict=True)\n    item.add_marker(mark)\n    result = evaluate_xfail_marks(item)\n    assert result.reason == \"some reason\"\n    assert result.strict is True\n"], "sample_685": ["def test_caplog_captures_log_records_with_nested_loggers(caplog):\n    logger.info(\"info message\")\n    sublogger.warning(\"warning message\")\n    assert len(caplog.records) == 2\n    assert caplog.records[0].name == __name__\n    assert caplog.records[1].name == sublogger.name\n"], "sample_686": ["def test_deprecated_external_plugins():\n    assert \"pytest_catchlog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_capturelog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_faulthandler\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n"], "sample_689": ["def test_deprecated_external_plugins():\n    assert \"pytest_catchlog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_capturelog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_faulthandler\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n"], "sample_690": ["def test_xfail_strict(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(strict=True)\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*\"])\n    assert result.ret == 1\n"], "sample_691": ["def test_faulthandler_hooks_unconfigure(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import faulthandler\n\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--faulthandler-timeout=1\")\n    assert result.ret == 0\n    # Check that faulthandler is disabled after pytest unconfigures.\n    assert not faulthandler.is_enabled()\n"], "sample_692": ["def test_get_user(monkeypatch: MonkeyPatch) -> None:\n    import getpass\n\n        raise ImportError()\n\n    monkeypatch.setattr(getpass, \"getuser\", raise_error)\n    assert get_user() is None\n\n        raise KeyError()\n\n    monkeypatch.setattr(getpass, \"getuser\", raise_key_error)\n    assert get_user() is None\n\n        return \"test_user\"\n\n    monkeypatch.setattr(getpass, \"getuser\", return_user)\n    assert get_user() == \"test_user\"\n"], "sample_693": ["def test_unittest_skip(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestFoo(unittest.TestCase):\n            @unittest.skip(\"reason\")\n                assert 0\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(skipped=1)\n    assert \"reason\" in result.stdout.fnmatch_lines(\"*reason*\")[0]\n"], "sample_695": ["def test_node_repr(tmp_path: Path) -> None:\n    class DummyNode(nodes.Node):\n            super().__init__(name, parent=parent)\n\n    root = DummyNode(\"root\", parent=None)\n    child = DummyNode(\"child\", parent=root)\n    assert repr(root) == \"<DummyNode root>\"\n    assert repr(child) == \"<DummyNode child>\"\n"], "sample_696": ["def test_hook_legacy_path_arg_warning(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n            pass\n        \"\"\"\n    )\n    with pytest.warns(UnformattedWarning) as record:\n        pytester.runpytest(\"--help\")\n    assert len(record) == 1\n    warning = record[0]\n    assert warning.category == PytestDeprecationWarning\n    assert \"The (pylib_path_arg: py.path.local)\" in str(warning.message)\n"], "sample_697": ["def test_get_user(monkeypatch: MonkeyPatch) -> None:\n    import getpass\n\n        raise ImportError(\"Mocked ImportError\")\n\n    monkeypatch.setattr(getpass, \"getuser\", raise_error)\n    assert get_user() is None\n\n        raise KeyError(\"Mocked KeyError\")\n\n    monkeypatch.setattr(getpass, \"getuser\", raise_key_error)\n    assert get_user() is None\n\n        return \"test_user\"\n\n    monkeypatch.setattr(getpass, \"getuser\", return_user)\n    assert get_user() == \"test_user\"\n"], "sample_698": ["def test_colored_level_formatter_create_terminal_writer(\n    capsys: Any, request: Any"], "sample_699": ["def test_doctest_namespace_fixture(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope=\"session\")\n            return {\"myvar\": \"test\"}\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n            '''\n            >>> myvar\n            'test'\n            '''\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--doctest-modules\")\n    assert result.ret == 0\n"], "sample_700": ["def test_parametrize_with_fixture_ref_in_param_id(\n    pytester: Pytester,"], "sample_701": ["def test_check_ispytest():\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        deprecated.check_ispytest(False)\n        assert len(w) == 1\n        assert issubclass(w[0].category, PytestDeprecationWarning)\n        assert str(w[0].message) == \"A private pytest class or function was used.\"\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        deprecated.check_ispytest(True)\n        assert len(w) == 0\n"], "sample_703": ["def test_compile_empty() -> None:\n    with pytest.raises(ParseError):\n        Expression.compile(\"\")\n"], "sample_706": ["def test_empty_expression() -> None:\n    assert not evaluate(\"\", lambda x: True)\n"], "sample_707": ["def test_node_repr_failure(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert 0\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--tb=short\")\n    result.stdout.fnmatch_lines([\"*test_foo*\"])\n    assert result.ret == 1\n"], "sample_708": ["def test_source_getstatementrange_ast():\n    src = Source(\n        \"\"\"\n            pass\n\n            pass\n    \"\"\"\n    )\n    ast_tree, start, end = getstatementrange_ast(1, src)\n    assert start == 0\n    assert end == 2\n\n    ast_tree, start, end = getstatementrange_ast(3, src)\n    assert start == 3\n    assert end == 5\n"], "sample_710": ["def test_unittest_skip(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestFoo(unittest.TestCase):\n            @unittest.skip(\"skipped\")\n                assert 0\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(skipped=1)\n    assert result.ret == ExitCode.OK\n"], "sample_712": ["def test_one_hot_encoder_inverse_transform():\n    # Test inverse transform with known categories\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X_tr = enc.fit_transform(X)\n    assert_array_equal(enc.inverse_transform(toarray(X_tr)), X)\n\n    # Test inverse transform with unknown categories\n    X_inv = toarray(enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]]))\n    assert_array_equal(X_inv, [['Male', 1], [None, 2]])\n"], "sample_713": ["def test_ridgecv_store_cv_values():\n    # Test if cv_values are stored correctly\n    X, y = make_regression(n_samples=10, n_features=5)\n    ridge_cv = RidgeCV(store_cv_values=True)\n    ridge_cv.fit(X, y)\n    assert_array_almost_equal(ridge_cv.cv_values_.shape,\n                              (X.shape[0], len(ridge_cv.alphas)))\n"], "sample_714": ["def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Bad beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n\n    # Bad average option\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  [1, 2, 3], [1, 2, 3], average=\"invalid\")\n\n    # Bad pos_label\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  [1, 2, 3], [1, 2, 3], pos_label=\"invalid\")\n"], "sample_715": ["def test_cross_validate_return_train_score_warn():\n    # Test that cross_validate raises a warning when return_train_score is set\n    # to 'warn' and train scores are requested.\n    X, y = make_classification(n_samples=20, n_features=5, random_state=0)\n    estimator = MockClassifier()\n    with assert_warns_message(FutureWarning,\n                              \"You are accessing a training score\"):\n        scores = cross_validate(estimator, X, y, return_train_score='warn')\n        _ = scores['train_score']\n"], "sample_716": ["def test_ridgecv_scorer():\n    # Test that RidgeCV can be used with a scorer and that the scorer is used.\n    X, y = make_regression(n_samples=10, n_features=5, random_state=0)\n    ridge_cv = RidgeCV(scoring='neg_mean_squared_error')\n    ridge_cv.fit(X, y)\n\n    # Check that the scorer was used to determine the best alpha\n    assert_equal(ridge_cv.scorer_, get_scorer('neg_mean_squared_error'))\n\n    # Check that the best alpha is not the first or last in the list of alphas\n    assert_greater(ridge_cv.alpha_, ridge_cv.alphas[0])\n    assert_greater(ridge_cv.alphas[-1], ridge_cv.alpha_)\n"], "sample_719": ["def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter of TfidfVectorizer works as expected\n    vectorizer = TfidfVectorizer(max_features=5)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n\n    assert_equal(X.shape[1], 5)\n    assert_equal(len(vectorizer.vocabulary_), 5)\n    assert_equal(len(vectorizer.idf_), 5)\n"], "sample_721": ["def test_check_is_fitted():\n    class Estimator:\n            self.coef_ = None\n\n            self.coef_ = np.array([1., 2.])\n\n    estimator = Estimator()\n    assert_raises_regex(NotFittedError,\n                        \"This Estimator instance is not fitted yet\",\n                        check_is_fitted, estimator, 'coef_')\n\n    estimator.fit(None, None)\n    check_is_fitted(estimator, 'coef_')\n\n    # Test with multiple attributes\n    class MultiEstimator:\n            self.coef_ = None\n            self.intercept_ = None\n\n            self.coef_ = np.array([1., 2.])\n            self.intercept_ = 1.\n\n    multi_estimator = MultiEstimator()\n    assert_raises_regex(NotFittedError,\n                        \"This MultiEstimator instance is not fitted yet\",\n                        check_is_fitted, multi_estimator, ['coef_', 'intercept_'])\n\n    multi_estimator.fit(None, None)\n    check_is_fitted(multi_estimator, ['coef_', 'intercept_'])\n"], "sample_722": ["def test_kmeans_init():\n    # Check that the KMeans initialization is correct\n    kmeans = KMeans(n_clusters=3, init='random', random_state=0)\n    assert_array_almost_equal(kmeans._init_centroids(X, 3, 'random', np.random.RandomState(0)),\n                              [[1.9233856 , 4.50911155, 2.29319414, 2.48939335, 1.42413446],\n                               [4.35745729, 2.93942052, 2.62385428, 2.45671662, 2.86683115],\n                               [2.02793298, 1.30866469, 4.28498699, 2.01866858, 1.55274642]])\n\n    # Check that the KMeans initialization with an array is correct\n    init_centers = np.array([[1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.]])\n    kmeans = KMeans(n_clusters=3, init=init_centers)\n    assert_array_almost_equal(kmeans._init_centroids(X, 3, init_centers), init_centers)\n\n    # Check that the KMeans initialization with a callable is correct\n        return np.array([[1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.]])\n    kmeans = KMeans(n_clusters=3, init=init_callable)\n    assert_array_almost_equal(kmeans._init_centroids(X, 3, init_callable), init_callable(None, 3, None))\n"], "sample_723": ["def test_imputer_pipeline_grid_search():\n    \"\"\"Test imputer within a pipeline + GridSearch.\"\"\"\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n    y = np.array([0, 1, 0])\n    imputer = SimpleImputer()\n    clf = tree.DecisionTreeClassifier()\n\n    pipeline = Pipeline([('imputer', imputer), ('clf', clf)])\n\n    parameters = {\n        'imputer__strategy': ['mean', 'median', 'most_frequent'],\n        'clf__max_depth': [1, 2, 3]\n    }\n\n    grid_search = GridSearchCV(pipeline, parameters)\n    grid_search.fit(X, y)\n\n    assert_equal(grid_search.best_params_['imputer__strategy'], 'mean')\n    assert_equal(grid_search.best_params_['clf__max_depth'], 1)\n"], "sample_724": ["def test_imputer_copy():\n    # Test that copy was done correctly\n    X = np.array([[1, 2], [np.nan, 3]])\n\n    for sparse in [True, False]:\n        for axis in [0, 1]:\n            imputer = Imputer(strategy='mean', axis=axis, copy=True)\n            X_trans = imputer.fit_transform(X)\n\n            if sparse:\n                X_trans = sparse.csr_matrix(X_trans)\n\n            assert_false(np.may_share_memory(X, X_trans))\n\n            imputer = Imputer(strategy='mean', axis=axis, copy=False)\n            X_trans_nocopy = imputer.fit_transform(X)\n\n            if sparse:\n                X_trans_nocopy = sparse.csr_matrix(X_trans_nocopy)\n\n            assert_array_equal(X_trans, X_trans_nocopy)\n"], "sample_725": ["def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = False\n\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises_regex(NotFittedError,\n                        \"This Estimator instance is not fitted yet\",\n                        check_is_fitted, estimator, 'fitted_')\n\n    estimator.fit(None)\n    assert_no_warnings(check_is_fitted, estimator, 'fitted_')\n"], "sample_726": ["def test_label_binarizer_fit_transform_multiclass():\n    # Test fit_transform with multiclass target\n    lb = LabelBinarizer()\n    Y = np.array([1, 2, 3, 4])\n    Y_bin = lb.fit_transform(Y)\n    assert_array_equal(lb.classes_, [1, 2, 3, 4])\n    assert_array_equal(toarray(Y_bin), [[1, 0, 0, 0],\n                                        [0, 1, 0, 0],\n                                        [0, 0, 1, 0],\n                                        [0, 0, 0, 1]])\n"], "sample_727": ["def test_imputer_copy():\n    \"\"\"Test that imputer produces the same results with copy and without copy\"\"\"\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n\n    for strategy in ['mean', 'median', 'most_frequent']:\n        imputer_with_copy = Imputer(strategy=strategy, copy=True)\n        imputer_without_copy = Imputer(strategy=strategy, copy=False)\n\n        X_with_copy = imputer_with_copy.fit_transform(X.copy())\n        X_without_copy = imputer_without_copy.fit_transform(X.copy())\n\n        assert_array_equal(X_with_copy, X_without_copy)\n"], "sample_729": ["def test_lasso_path_positive():\n    # Test that the positive Lasso path returns a solution with non-negative\n    # coefficients.\n    X, y = load_boston(return_X_y=True)\n    alphas, coefs, _ = lasso_path(X, y, positive=True)\n    assert_array_equal(coefs >= 0, True)\n"], "sample_730": ["def test_lasso_path_return_n_iter():\n    # Test that lasso_path return n_iter\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 6])\n    max_iter = 100\n    alphas, coefs, dual_gaps, n_iters = lasso_path(X, y, return_n_iter=True,\n                                                  max_iter=max_iter)\n    assert_array_equal(n_iters.shape, (len(alphas),))\n    assert_greater(max_iter, n_iters.max())\n"], "sample_731": ["def test_california_housing_fetch():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n\n    assert (20, 640) == data.data.shape\n    assert (20, 640) == data.target.shape\n    assert 8 == len(data.feature_names)\n    assert data.DESCR.startswith(\"California housing dataset.\")\n"], "sample_732": ["def test_fetch_kddcup99_shuffle():\n    try:\n        dataset = fetch_kddcup99(shuffle=True, random_state=0)\n    except IOError as e:\n        raise SkipTest(\"KDDCup 99 dataset can not be loaded: %s\" % e)\n\n    assert_equal(dataset.data.shape, (494021, 41))\n    assert_equal(dataset.target.shape, (494021,))\n    assert_equal(dataset.data[0][0], 0)  # check that shuffling worked\n\n    # test subset='SA'\n    dataset_sa = fetch_kddcup99(subset='SA', shuffle=True, random_state=0)\n    assert_equal(dataset_sa.data.shape, (976158, 41))\n    assert_equal(dataset_sa.target.shape, (976158,))\n\n    # test subset='SF'\n    dataset_sf = fetch_kddcup99(subset='SF', shuffle=True, random_state=0)\n    assert_equal(dataset_sf.data.shape, (699691, 4))\n    assert_equal(dataset_sf.target.shape, (699691,))\n\n    # test subset='http'\n    dataset_http = fetch_kddcup99(subset='http', shuffle=True, random_state=0)\n    assert_equal(dataset_http.data.shape, (619052, 3))\n    assert_equal(dataset_http.target.shape, (619052,))\n\n    # test subset='smtp'\n    dataset_smtp = fetch_kddcup99(subset='smtp', shuffle=True, random_state=0)\n    assert_equal(dataset_smtp.data.shape, (95373, 3))\n    assert_equal(dataset_smtp.target.shape, (95373,))\n"], "sample_733": ["def test_tfidf_vectorizer_max_features():\n    \"\"\"Check that the max_features parameter is handled correctly\"\"\"\n    vectorizer = TfidfVectorizer(max_features=4)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert_equal(X.shape[1], 4)\n\n    vectorizer = TfidfVectorizer(max_features=None)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert_greater(X.shape[1], 4)\n\n    vectorizer = TfidfVectorizer(max_features=0.5)\n    with pytest.raises(ValueError):\n        X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n"], "sample_734": ["def test_entropy():\n    # Test the entropy function\n    labels = np.array([1, 1, 1, 2, 2, 3])\n    assert_almost_equal(entropy(labels), 1.2958368665453396)\n\n    labels = np.array([1, 1, 1, 1, 1, 1])\n    assert_almost_equal(entropy(labels), 0.0)\n\n    labels = np.array([])\n    assert_almost_equal(entropy(labels), 1.0)\n"], "sample_736": ["def test_logistic_regression_saga_solver():\n    # Test the saga solver on a simple dataset\n    X, y = make_classification(n_samples=100, n_features=5, random_state=0)\n    clf = LogisticRegression(solver='saga', max_iter=1000, random_state=0)\n    check_predictions(clf, X, y)\n\n    # Test the saga solver with L1 penalty\n    clf = LogisticRegression(solver='saga', penalty='l1', max_iter=1000,\n                             random_state=0)\n    check_predictions(clf, X, y)\n\n    # Test the saga solver with warm starting\n    clf = LogisticRegression(solver='saga', max_iter=1000, warm_start=True,\n                             random_state=0)\n    clf.fit(X, y)\n    clf.max_iter = 2000\n    clf.fit(X, y)\n"], "sample_737": ["def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter of TfidfVectorizer works as expected\n    vectorizer = TfidfVectorizer(max_features=5)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n\n    assert_equal(X.shape[1], 5)\n    assert_equal(len(vectorizer.vocabulary_), 5)\n    assert_equal(len(vectorizer.idf_), 5)\n"], "sample_738": ["def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter of TfidfVectorizer works as expected\n    vectorizer = TfidfVectorizer(max_features=10)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape[1], 10)\n\n    # Test that max_features=None does not limit the number of features\n    vectorizer = TfidfVectorizer(max_features=None)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_greater(X.shape[1], 10)\n\n    # Test that max_features < 1 raises a ValueError\n    vectorizer = TfidfVectorizer(max_features=0)\n    assert_raises(ValueError, vectorizer.fit_transform, ALL_FOOD_DOCS)\n"], "sample_739": ["def test_label_binarizer_set_output():\n    # Test that LabelBinarizer behaves as expected with one class\n    # in multi-label indicator matrix format\n\n    lb = LabelBinarizer()\n    y = [[0, 1], [1, 0]]\n    lb.fit(y)\n    assert_equal(lb.classes_.shape[0], 2)\n\n    y_test = [[0, 1], [1, 0], [1, 1], [0, 0]]\n    y_trans = lb.transform(y_test)\n    assert_array_equal(toarray(y_trans), np.array([[1, 0], [0, 1], [1, 1], [0, 0]]))\n\n    y_inv = lb.inverse_transform(y_trans)\n    assert_array_equal(toarray(y_inv), np.array([[0, 1], [1, 0], [1, 1], [0, 0]]))\n"], "sample_740": ["def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = False\n\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises_regex(NotFittedError, \"Estimator\", check_is_fitted, estimator, 'fitted_')\n\n    estimator.fit(None)\n    check_is_fitted(estimator, 'fitted_')\n\n    # checking multiple attributes\n    class MultiAttrEstimator:\n            self.attr1_ = None\n            self.attr2_ = None\n\n            self.attr1_ = True\n            self.attr2_ = True\n\n    multi_attr_estimator = MultiAttrEstimator()\n    assert_raises_regex(NotFittedError, \"Estimator\", check_is_fitted,\n                        multi_attr_estimator, ['attr1_', 'attr2_'])\n\n    multi_attr_estimator.fit(None)\n    check_is_fitted(multi_attr_estimator, ['attr1_', 'attr2_'])\n\n    # check that it works with any\n    class OneAttrEstimator:\n            self.attr1_ = None\n            self.attr2_ = None\n\n            self.attr1_ = True\n\n    one_attr_estimator = OneAttrEstimator()\n    assert_raises_regex(NotFittedError, \"Estimator\", check_is_fitted,\n                        one_attr_estimator, ['attr1_', 'attr2_'], all_or_any=any)\n\n    one_attr_estimator.fit(None)\n    check_is_fitted(one_attr_estimator, ['attr1_', 'attr2_'], all_or_any=any)\n"], "sample_742": ["def test_logistic_regression_solvers():\n    \"\"\"Test that all solvers converge to the same solution for a given dataset\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n\n    # L2 regularization\n    params = {'penalty': 'l2', 'max_iter': 1000, 'tol': 1e-6}\n    results = []\n    for solver in solvers:\n        clf = LogisticRegression(solver=solver, **params)\n        clf.fit(X, y)\n        results.append(clf.coef_)\n\n    assert_allclose(results[0], results[1:], atol=1e-3)\n\n    # L1 regularization\n    params = {'penalty': 'l1', 'max_iter': 1000, 'tol': 1e-6}\n    results = []\n    for solver in ['liblinear', 'saga']:\n        clf = LogisticRegression(solver=solver, **params)\n        clf.fit(X, y)\n        results.append(clf.coef_)\n\n    assert_allclose(results[0], results[1:], atol=1e-3)\n"], "sample_743": ["def test_neighbors_base_algorithm_metric():\n    # Test that the algorithm and metric are properly set\n    X = iris.data\n\n    # Test that algorithm is set to 'auto' by default\n    nn = neighbors.NeighborsBase()\n    assert_equal(nn.algorithm, 'auto')\n\n    # Test that setting algorithm to an invalid value raises an error\n    nn = neighbors.NeighborsBase(algorithm='invalid')\n    assert_raises(ValueError, nn._check_algorithm_metric)\n\n    # Test that setting metric to an invalid value raises an error\n    nn = neighbors.NeighborsBase(metric='invalid')\n    assert_raises(ValueError, nn._check_algorithm_metric)\n\n    # Test that setting p to an invalid value raises an error\n    nn = neighbors.NeighborsBase(p=-1)\n    assert_raises(ValueError, nn._check_algorithm_metric)\n\n    # Test that setting metric_params to an invalid value raises an error\n    nn = neighbors.NeighborsBase(metric_params={'p': -1})\n    assert_raises(ValueError, nn._check_algorithm_metric)\n\n    # Test that setting algorithm to 'ball_tree' or 'kd_tree' sets the\n    # correct valid metrics\n    for algorithm in ['ball_tree', 'kd_tree']:\n        nn = neighbors.NeighborsBase(algorithm=algorithm)\n        assert_in(nn.metric, VALID_METRICS[algorithm])\n\n    # Test that setting algorithm to 'brute' sets the correct valid metrics\n    nn = neighbors.NeighborsBase(algorithm='brute')\n    assert_in(nn.metric, VALID_METRICS['brute'])\n"], "sample_745": ["def test_function_transformer_inverse():\n    X = np.array([[1, 2], [3, 4]])\n\n    # Define a simple function and its inverse\n        return X * 2\n\n        return X / 2\n\n    transformer = FunctionTransformer(func=func, inverse_func=inverse_func)\n\n    # Fit and transform\n    transformer.fit(X)\n    X_transformed = transformer.transform(X)\n\n    # Check that the inverse transformation leads to the original data\n    assert_allclose_dense_sparse(X, transformer.inverse_transform(X_transformed))\n"], "sample_750": ["def test_omp_path():\n    # Test that the path of orthogonal_mp and orthogonal_mp_gram are equal\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 6])\n    n_nonzero_coefs = 2\n\n    coefs_omp = orthogonal_mp(X, y, n_nonzero_coefs=n_nonzero_coefs,\n                              return_path=True)\n    G, Xy = np.dot(X.T, X), np.dot(X.T, y)\n    coefs_gram = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=n_nonzero_coefs,\n                                    return_path=True)\n\n    assert_array_almost_equal(coefs_omp, coefs_gram)\n"], "sample_751": ["def test_random_trees_embedding_sparse_output():\n    # Test that sparse output is a csr_matrix and has less than 50% non-zero\n    # values (i.e. it is actually sparse)\n    X, y = iris.data, iris.target\n    hasher = RandomTreesEmbedding(n_estimators=10, random_state=0,\n                                  sparse_output=True)\n    X_transformed = hasher.fit_transform(X)\n\n    assert isinstance(X_transformed, csr_matrix)\n    assert_less(X_transformed.nnz / X_transformed.shape[0] / X_transformed.shape[1], 0.5)\n"], "sample_752": ["def test_iforest_sparse():\n    # Test Isolation Forest on sparse matrix\n    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n                                                        boston.target,\n                                                        test_size=0.5,\n                                                        random_state=rng)\n    X_train_sparse = csc_matrix(X_train)\n    X_test_sparse = csr_matrix(X_test)\n\n    iforest_dense = IsolationForest(random_state=rng)\n    iforest_sparse = IsolationForest(random_state=rng)\n\n    iforest_dense.fit(X_train)\n    iforest_sparse.fit(X_train_sparse)\n\n    assert_array_almost_equal(iforest_dense.decision_function(X_test),\n                              iforest_sparse.decision_function(X_test_sparse))\n"], "sample_754": ["def test_sparse_pca_init():\n    # Test that SparsePCA and MiniBatchSparsePCA initialize correctly\n    spca = SparsePCA(n_components=5, alpha=0.1, ridge_alpha=0.01,\n                     max_iter=1000, tol=1e-8, method='lars', n_jobs=1,\n                     U_init=None, V_init=None, verbose=False,\n                     random_state=42, normalize_components=True)\n\n    assert_equal(spca.n_components, 5)\n    assert_equal(spca.alpha, 0.1)\n    assert_equal(spca.ridge_alpha, 0.01)\n    assert_equal(spca.max_iter, 1000)\n    assert_equal(spca.tol, 1e-8)\n    assert_equal(spca.method, 'lars')\n    assert_equal(spca.n_jobs, 1)\n    assert_equal(spca.verbose, False)\n    assert_equal(spca.random_state, 42)\n    assert_equal(spca.normalize_components, True)\n\n    mb_spca = MiniBatchSparsePCA(n_components=5, alpha=0.1, ridge_alpha=0.01,\n                                  n_iter=100, callback=None, batch_size=3,\n                                  verbose=False, shuffle=True, n_jobs=1,\n                                  method='lars', random_state=42,\n                                  normalize_components=True)\n\n    assert_equal(mb_spca.n_components, 5)\n    assert_equal(mb_spca.alpha, 0.1)\n    assert_equal(mb_spca.ridge_alpha, 0.01)\n    assert_equal(mb_spca.n_iter, 100)\n    assert_equal(mb_spca.callback, None)\n    assert_equal(mb_spca.batch_size, 3)\n    assert_equal(mb_spca.verbose, False)\n    assert_equal(mb_spca.shuffle, True)\n    assert_equal(mb_spca.n_jobs, 1)\n    assert_equal(mb_spca.method, 'lars')\n    assert_equal(mb_spca.random_state, 42)\n    assert_equal(mb_spca.normalize_components, True)\n"], "sample_755": ["def test_silhouette_score_precomputed():\n    # Test silhouette score using precomputed distances\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    labels = np.array([0, 0, 1])\n    distances = pairwise_distances(X)\n    score = silhouette_score(distances, labels, metric='precomputed')\n    assert isinstance(score, float)\n    assert score >= -1 and score <= 1\n"], "sample_756": ["def test_optics_extract_dbscan():\n    # Test OPTICS extract_dbscan method for various epsilon values\n    X, _ = make_blobs(n_samples=50, random_state=0)\n    optics = OPTICS(min_samples=10, max_eps=5.0).fit(X)\n\n    # Test with eps < max_eps\n    eps = 2.0\n    core_sample_indices, labels = optics.extract_dbscan(eps)\n    assert len(core_sample_indices) > 0\n    assert len(labels) == len(X)\n\n    # Test with eps > max_eps\n    eps = 6.0\n    with pytest.raises(ValueError):\n        optics.extract_dbscan(eps)\n\n    # Test with eps close to max_eps\n    eps = 4.9\n    with pytest.warns(RuntimeWarning):\n        optics.extract_dbscan(eps)\n"], "sample_757": ["def test_one_hot_encoder_inverse_transform():\n    # Test inverse transform with known categories\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X_tr = enc.fit_transform(X)\n    assert_array_equal(enc.inverse_transform(toarray(X_tr)), X)\n\n    # Test inverse transform with unknown categories\n    X_inv = toarray(X_tr.copy())\n    X_inv[0, :] = 0\n    assert_array_equal(enc.inverse_transform(X_inv)[0, :], [None, None])\n"], "sample_758": ["def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises(NotFittedError, check_is_fitted, estimator, 'fitted_')\n\n    estimator.fit(None)\n    check_is_fitted(estimator, 'fitted_')\n\n    # checking multiple attributes\n    class MultiEstimator:\n            self.fitted_ = True\n            self.fitted2_ = True\n\n    multi_estimator = MultiEstimator()\n    assert_raises(NotFittedError, check_is_fitted, multi_estimator,\n                  ['fitted_', 'fitted2_'])\n\n    multi_estimator.fit(None)\n    check_is_fitted(multi_estimator, ['fitted_', 'fitted2_'])\n"], "sample_759": ["def test_one_hot_encoder_inverse_transform():\n    # Test inverse transform with known categories\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X_tr = enc.fit_transform(X)\n    X_inv = enc.inverse_transform(toarray(X_tr))\n    assert_array_equal(X, X_inv)\n\n    # Test inverse transform with unknown categories\n    X = [['Male', 1], ['Female', 3], ['Female', 4]]\n    X_tr = enc.transform(X)\n    X_inv = enc.inverse_transform(toarray(X_tr))\n    assert_array_equal([['Male', 1], ['Female', 3], [None, None]], X_inv)\n"], "sample_760": ["def test_make_scorer_with_kwargs():\n    # Test that make_scorer captures kwargs\n        return kwargs['foo']\n\n    scorer = make_scorer(my_score, greater_is_better=True, needs_proba=False,\n                         foo='bar')\n    assert_almost_equal(scorer(None, None, None), 'bar')\n"], "sample_761": ["def test_iterative_imputer_check_statistics():\n    # Test that the statistics (mean, median, mode) are correct\n    # for a given strategy with IterativeImputer\n\n    X = np.array([[1, 2],\n                  [np.nan, 3],\n                  [7, 6]])\n\n    imputer = IterativeImputer(initial_strategy='mean')\n    imputer.fit(X)\n\n    assert_array_almost_equal(imputer.initial_imputer_.statistics_,\n                              np.array([4., 3.66666667]))\n\n    X = np.array([[1, 2],\n                  [np.nan, 3],\n                  [7, 6]])\n\n    imputer = IterativeImputer(initial_strategy='median')\n    imputer.fit(X)\n\n    assert_array_almost_equal(imputer.initial_imputer_.statistics_,\n                              np.array([4., 3.]))\n\n    X = np.array([[1, 2],\n                  [np.nan, 3],\n                  [7, 6]])\n\n    imputer = IterativeImputer(initial_strategy='most_frequent')\n    imputer.fit(X)\n\n    assert_array_almost_equal(imputer.initial_imputer_.statistics_,\n                              np.array([1., 2.]))\n"], "sample_762": ["def test_clone_modify_init_params():\n    # Test that clone doesn't propagate changes to the default\n    # parameters of an estimator\n    estimator = ModifyInitParams()\n    estimator.a[0] = 1\n\n    cloned_estimator = clone(estimator)\n    assert_array_equal(cloned_estimator.a, np.array([0]))\n"], "sample_763": ["def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = False\n\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises_regex(NotFittedError,\n                        \"This Estimator instance is not fitted yet\",\n                        check_is_fitted, estimator, 'fitted_')\n\n    estimator.fit(None)\n    assert_no_warnings(check_is_fitted, estimator, 'fitted_')\n"], "sample_766": ["def test_sparse_encode_shapes():\n    n_samples, n_features = 10, 8\n    X = rng_global.randn(n_samples, n_features)\n    n_components = 5\n    dictionary = rng_global.randn(n_components, n_features)\n\n    for algorithm in ['lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold']:\n        code = sparse_encode(X, dictionary, algorithm=algorithm)\n        assert_equal(code.shape, (n_samples, n_components))\n"], "sample_770": ["def test_silhouette_score_sparse_input():\n    # Test silhouette score with sparse input\n    X, y = datasets.make_blobs(n_samples=50, n_features=2, centers=3,\n                               cluster_std=0.8, random_state=0)\n    X_sparse = csr_matrix(X)\n\n    score_dense = silhouette_score(X, y, metric='euclidean')\n    score_sparse = silhouette_score(X_sparse, y, metric='euclidean')\n\n    assert_almost_equal(score_dense, score_sparse)\n"], "sample_772": ["def check_parallel_classification(name):\n    \"\"\"Check classification on iris dataset using parallelism.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=1, n_jobs=2)\n    clf.fit(iris.data, iris.target)\n    y_pred = clf.predict(iris.data)\n\n    clf = ForestClassifier(n_estimators=10, random_state=1, n_jobs=1)\n    clf.fit(iris.data, iris.target)\n    y_pred_single_core = clf.predict(iris.data)\n\n    assert_array_equal(y_pred, y_pred_single_core)\n"], "sample_773": ["def test_logistic_regression_solvers():\n    \"\"\"Test that all solvers converge to a similar solution\"\"\"\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    X = scale(X)\n\n    # LBFGS is one of the most stable solver and will be used as reference.\n    clf_ref = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)\n    clf_ref.fit(X, y)\n\n    for solver in ['newton-cg', 'liblinear', 'sag', 'saga']:\n        clf = LogisticRegression(solver=solver, max_iter=1000, random_state=42)\n        clf.fit(X, y)\n        assert_allclose(clf.coef_, clf_ref.coef_, atol=1e-2)\n        assert_allclose(clf.intercept_, clf_ref.intercept_, atol=1e-2)\n"], "sample_774": ["def test_one_hot_encoder_inverse_transform():\n    # Test inverse transform with different types of input data\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder()\n    X_trans = enc.fit_transform(X)\n    \n    # Inverse transform should return original data\n    assert_array_equal(enc.inverse_transform(toarray(X_trans)), X)\n\n    # Inverse transform should handle unknown categories\n    X_unknown = [[0, 0, 0, 0, 1]]  # Unknown category in second feature\n    assert_warns(UserWarning, enc.inverse_transform, X_unknown)\n    result = enc.inverse_transform(X_unknown)\n    assert_array_equal(result, [['Male', None]])\n\n    # Inverse transform should handle dropped categories\n    enc = OneHotEncoder(drop='first')\n    X_trans = enc.fit_transform(X)\n    assert_array_equal(enc.inverse_transform(toarray(X_trans)), X)\n"], "sample_776": ["def test_lars_path_positive():\n    # Test that lars_path can handle positive parameter correctly\n    X, y = diabetes.data, diabetes.target\n    _, _, coefs_lasso_positive = lars_path(X, y, method='lasso', positive=True)\n    assert_greater(np.min(coefs_lasso_positive), 0)\n"], "sample_777": ["def test_gradient_boosting_loss_functions():\n    # Check if the loss functions are implemented correctly.\n    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n\n    # Least squares\n    ls = _gb_losses.LeastSquaresError(1)\n    assert_almost_equal(ls(y, np.array([np.mean(y)] * 10)), 0.0)\n\n    # Least absolute deviation\n    lad = _gb_losses.LeastAbsoluteError(1)\n    assert_almost_equal(lad(y, np.array([np.median(y)] * 10)), 0.0)\n\n    # Huber loss (least squares when error < alpha, least absolute deviation otherwise)\n    huber = _gb_losses.HuberLossFunction(1, alpha=0.5)\n    assert_almost_equal(huber(y, np.array([np.mean(y)] * 10)), 0.0)\n\n    # Quantile loss\n    quantile = _gb_losses.QuantileLossFunction(1, alpha=0.5)\n    assert_almost_equal(quantile(y, np.array([np.percentile(y, 50)] * 10)), 0.0)\n\n    # Binomial deviance\n    binomial_deviance = _gb_losses.BinomialDeviance(1)\n    assert_almost_equal(binomial_deviance(np.array([0, 1]), np.array([0.5, 0.5])), 0.0)\n\n    # Multinomial deviance\n    multinomial_deviance = _gb_losses.MultinomialDeviance(3)\n    assert_almost_equal(multinomial_deviance(np.array([0, 1, 2]), np.array([[1/3, 1/3, 1/3], [1/3, 1/3, 1/3], [1/3, 1/3, 1/3]])), 0.0)\n\n    # Exponential loss\n    exponential_loss = _gb_losses.ExponentialLoss(1)\n    assert_almost_equal(exponential_loss(np.array([0, 1]), np.array([0.5, 0.5])), 0.0)\n"], "sample_778": ["def test_nmf_multiplicative_update_sparse():\n    # Test that NMF with multiplicative update gives the same result for dense\n    # and sparse input\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(10, 5))\n    X_sparse = csc_matrix(X)\n\n    W_dense, H_dense, _ = non_negative_factorization(\n        X, n_components=3, solver='mu', beta_loss='frobenius',\n        random_state=0)\n\n    W_sparse, H_sparse, _ = non_negative_factorization(\n        X_sparse, n_components=3, solver='mu', beta_loss='frobenius',\n        random_state=0)\n\n    assert_array_almost_equal(W_dense, W_sparse)\n    assert_array_almost_equal(H_dense, H_sparse)\n"], "sample_780": ["def test_lda_partial_fit():\n    # Test if the partial fit method works as expected\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    learning_method='online',\n                                    random_state=0)\n    n_samples, n_features = X.shape\n\n    # Fit on a part of the data\n    lda.partial_fit(X[:n_samples // 2])\n\n    # Check that the model has been fitted\n    assert hasattr(lda, 'components_')\n\n    # Continue fitting on the rest of the data\n    lda.partial_fit(X[n_samples // 2:])\n\n    # Check that the model has been updated\n    assert lda.n_batch_iter_ > 1\n"], "sample_781": ["def check_regression_toy(name):\n    \"\"\"Check regression on a toy dataset.\"\"\"\n    ForestRegressor = FOREST_REGRESSORS[name]\n\n    reg = ForestRegressor(n_estimators=10, random_state=1)\n    reg.fit(X, np.array(y).astype(np.float32))\n    assert_array_almost_equal(reg.predict(T), [-1., 1., 1.], decimal=3)\n    assert_equal(10, len(reg))\n\n    reg = ForestRegressor(n_estimators=10, max_features=1, random_state=1)\n    reg.fit(X, np.array(y).astype(np.float32))\n    assert_array_almost_equal(reg.predict(T), [-1., 1., 1.], decimal=3)\n    assert_equal(10, len(reg))\n\n    # also test apply\n    leaf_indices = reg.apply(X)\n    assert_equal(leaf_indices.shape, (len(X), reg.n_estimators))\n"], "sample_783": ["def test_imputer_copy():\n    # Test that copy was done correctly\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n\n    for sparse in [True, False]:\n        X_test = sparse.csr_matrix(X) if sparse else X\n\n        imputer = SimpleImputer(missing_values=np.nan, strategy='mean',\n                                copy=True)\n        imputer.fit(X_test)\n\n        X_trans = imputer.transform(X_test)\n\n        # Set some values to nan\n        X_test[0, 0] = np.nan\n        X_test[1, 1] = np.nan\n\n        # Check that the imputed values are not changed\n        assert_allclose(X_trans, imputer.transform(X_test))\n"], "sample_784": ["def test_calibration_curve():\n    # Test calibration_curve function\n    y_true, y_pred = make_classification(n_samples=100, n_features=20,\n                                         n_informative=10, n_redundant=5,\n                                         random_state=42)\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=5)\n\n    assert_equal(len(prob_true), 5)\n    assert_equal(len(prob_pred), 5)\n    assert_greater_equal(prob_true.min(), 0)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_greater_equal(1, prob_true.max())\n    assert_greater_equal(1, prob_pred.max())\n\n    # Test with strategy='quantile'\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=5,\n                                             strategy='quantile')\n\n    assert_equal(len(prob_true), 5)\n    assert_equal(len(prob_pred), 5)\n    assert_greater_equal(prob_true.min(), 0)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_greater_equal(1, prob_true.max())\n    assert_greater_equal(1, prob_pred.max())\n\n    # Test with normalize=True\n    y_pred_unnormalized = y_pred * 2 + 1\n    prob_true, prob_pred = calibration_curve(y_true, y_pred_unnormalized,\n                                             n_bins=5, normalize=True)\n\n    assert_equal(len(prob_true), 5)\n    assert_equal(len(prob_pred), 5)\n    assert_greater_equal(prob_true.min(), 0)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_greater_equal(1, prob_true.max())\n    assert_greater_equal(1, prob_pred.max())\n"], "sample_786": ["def test_kbins_discretizer_encode():\n    # Test that the encode parameter works as expected\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n\n    # Test ordinal encoding\n    discretizer = KBinsDiscretizer(n_bins=2, encode='ordinal')\n    Xt = discretizer.fit_transform(X)\n    assert_array_equal(Xt, [[0, 0], [1, 1], [1, 1]])\n\n    # Test onehot encoding\n    discretizer = KBinsDiscretizer(n_bins=2, encode='onehot')\n    Xt = discretizer.fit_transform(X)\n    assert isinstance(Xt, sp.csr_matrix)\n    assert_array_equal(Xt.toarray(), [[1, 0, 1, 0], [0, 1, 0, 1], [0, 1, 0, 1]])\n\n    # Test onehot-dense encoding\n    discretizer = KBinsDiscretizer(n_bins=2, encode='onehot-dense')\n    Xt = discretizer.fit_transform(X)\n    assert_array_equal(Xt, [[1, 0, 1, 0], [0, 1, 0, 1], [0, 1, 0, 1]])\n"], "sample_788": ["def test_kbins_discretizer_encode():\n    # Test 'ordinal', 'onehot' and 'onehot-dense' encoding strategies\n    discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    discretizer.fit(X)\n    Xt = discretizer.transform(X)\n    assert_array_equal(Xt.shape, (4, 4))\n\n    discretizer = KBinsDiscretizer(n_bins=3, encode='onehot')\n    discretizer.fit(X)\n    Xt = discretizer.transform(X)\n    assert isinstance(Xt, sp.csr_matrix)\n    assert_array_equal(Xt.shape, (4, 10))\n\n    discretizer = KBinsDiscretizer(n_bins=3, encode='onehot-dense')\n    discretizer.fit(X)\n    Xt = discretizer.transform(X)\n    assert_array_equal(Xt.shape, (4, 10))\n"], "sample_789": ["def test_adaboost_regressor_with_svr():\n    # Test AdaBoostRegressor with support vector regressor as base estimator.\n    boston = datasets.load_boston()\n    X, y = boston.data, boston.target\n\n    svr = SVR(kernel='linear')\n    adaboost_regressor = AdaBoostRegressor(base_estimator=svr, n_estimators=5)\n    adaboost_regressor.fit(X, y)\n\n    assert_array_less(adaboost_regressor.estimator_errors_, 1.)\n    assert_greater(adaboost_regressor.score(X, y), 0.5)\n"], "sample_790": ["def test_kernel_pca_precomputed():\n    # Test that KernelPCA works with precomputed kernels\n    X, _ = make_circles(n_samples=400, factor=.2, noise=.05)\n    K = rbf_kernel(X)\n\n    kpca = KernelPCA(kernel='precomputed', n_components=2)\n    X_kpca = kpca.fit_transform(K)\n\n    assert_equal(X_kpca.shape[1], 2)\n    assert_not_equal(X_kpca[0, 0], X_kpca[1, 0])\n    assert_not_equal(X_kpca[0, 1], X_kpca[1, 1])\n"], "sample_791": ["def test_one_hot_encoder_inverse_transform():\n    # Test that inverse transform works with numpy arrays and sparse matrices\n    X = np.array([[1, 2], [3, 4]])\n    ohe = OneHotEncoder()\n    ohe.fit(X)\n    X_ohe = ohe.transform(X)\n    X_inv = ohe.inverse_transform(toarray(X_ohe))\n    assert_array_equal(X, X_inv)\n\n    X_inv_sparse = ohe.inverse_transform(X_ohe)\n    assert_array_equal(X, X_inv_sparse)\n"], "sample_793": ["def test_iforest_average_path_length():\n    # Test the average path length function\n    n_samples_leaf = np.array([1, 2, 3, 4, 5])\n    average_path_length = _average_path_length(n_samples_leaf)\n\n    # Check if the output has the correct shape\n    assert_array_equal(average_path_length.shape, n_samples_leaf.shape)\n\n    # Check if the function returns the correct values for n_samples_leaf <= 2\n    assert_array_equal(average_path_length[n_samples_leaf <= 2], [0., 1.])\n\n    # Check if the function returns the correct values for n_samples_leaf > 2\n    expected_values = (2.0 * (np.log(n_samples_leaf[n_samples_leaf > 2] - 1.0) + np.euler_gamma)\n                       - 2.0 * (n_samples_leaf[n_samples_leaf > 2] - 1.0) / n_samples_leaf[n_samples_leaf > 2])\n    assert_allclose(average_path_length[n_samples_leaf > 2], expected_values)\n"], "sample_794": ["def test_ridgecv_store_cv_values():\n    # Test RidgeCV with store_cv_values=True and cv=None\n    X, y = make_regression(n_samples=10, n_features=5, random_state=0)\n    ridgecv = RidgeCV(store_cv_values=True, cv=None)\n    ridgecv.fit(X, y)\n    assert hasattr(ridgecv, 'cv_values_')\n    assert ridgecv.cv_values_.shape == (10, len(ridgecv.alphas))\n"], "sample_795": ["def test_check_estimator_idempotent():\n    # Test that check_estimator_idempotent raises an error if estimator is not\n    # idempotent.\n    class NotIdempotentEstimator(BaseEstimator):\n            self.fitted_ = True\n            return self\n\n            if not hasattr(self, 'fitted_'):\n                raise ValueError(\"Estimator is not fitted\")\n            return np.ones(X.shape[0])\n\n    msg = \"Estimator NotIdempotentEstimator is not idempotent\"\n    assert_raises_regex(ValueError, msg, check_fit_idempotent,\n                        \"NotIdempotentEstimator\", NotIdempotentEstimator())\n"], "sample_796": ["def test_huber_regression_with_sample_weights():\n    # Test that the HuberRegressor works with sample weights.\n    X, y = make_regression_with_outliers(n_samples=50, n_features=20)\n    sample_weight = np.ones_like(y)\n    sample_weight[:10] *= 2.0\n\n    huber = HuberRegressor(max_iter=1000)\n    huber.fit(X, y, sample_weight=sample_weight)\n\n    # Check that the coef_ and intercept_ are not NaN.\n    assert_array_equal(np.isnan(huber.coef_), np.zeros_like(huber.coef_))\n    assert not np.isnan(huber.intercept_)\n"], "sample_798": ["def test_ridgecv_store_cv_values():\n    # Test RidgeCV with store_cv_values=True and cv=None\n    X, y = make_regression(n_samples=10, n_features=5, random_state=0)\n    ridgecv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1], store_cv_values=True, cv=None)\n    ridgecv.fit(X, y)\n    assert hasattr(ridgecv, 'cv_values_')\n    assert ridgecv.cv_values_.shape == (10, 3)\n\n    # Test RidgeCV with store_cv_values=True and cv=int\n    ridgecv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1], store_cv_values=True, cv=5)\n    with pytest.raises(ValueError):\n        ridgecv.fit(X, y)\n"], "sample_802": ["def test_pipeline_score_samples():\n    # Test that score_samples works on classifier pipeline\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    clf = LogisticRegression()\n    pipe = make_pipeline(StandardScaler(), clf)\n    pipe.fit(X, y)\n\n    assert_array_equal(pipe.score_samples(X), clf.score_samples(pipe[:-1].transform(X)))\n"], "sample_803": ["def test_roc_auc_score_multiclass():\n    # Test Area under Receiver Operating Characteristic Curve (ROC AUC)\n    # with multiclass classification by recursively calling the binary auc\n    rng = np.random.RandomState(0)\n    y_pred = rng.rand(20, 3)\n    y_true = rng.randint(0, 3, size=20)\n\n    # one-vs-one multi-class auc\n    ovo_auc = roc_auc_score(y_true, y_pred, multi_class='ovo')\n\n    # one-vs-rest multi-class auc\n    ovr_auc = roc_auc_score(y_true, y_pred, multi_class='ovr')\n\n    assert ovo_auc != ovr_auc\n\n    # check that weighted version is equivalent for equal weights\n    ovo_auc_weighted = roc_auc_score(y_true, y_pred, multi_class='ovo',\n                                     sample_weight=np.ones_like(y_true))\n    assert_almost_equal(ovo_auc, ovo_auc_weighted)\n\n    ovr_auc_weighted = roc_auc_score(y_true, y_pred, multi_class='ovr',\n                                     sample_weight=np.ones_like(y_true))\n    assert_almost_equal(ovr_auc, ovr_auc_weighted)\n"], "sample_804": ["def test_one_hot_encoder_inverse_transform():\n    # Test that inverse transform works with numpy arrays and pandas DataFrames\n    X = np.array([[1, 2], [3, 4]])\n    enc = OneHotEncoder(sparse=False)\n    X_trans = enc.fit_transform(X)\n    X_inv = enc.inverse_transform(X_trans)\n\n    # Check that the inverse transformed array is equal to the original array\n    assert_array_equal(toarray(X_inv), X)\n\n    # Check that the inverse transformed array has the correct dtype\n    assert X_inv.dtype == X.dtype\n\n    # Repeat the test with a pandas DataFrame\n    import pandas as pd\n    X_df = pd.DataFrame(X)\n    X_trans_df = enc.fit_transform(X_df)\n    X_inv_df = enc.inverse_transform(X_trans_df)\n\n    # Check that the inverse transformed DataFrame is equal to the original DataFrame\n    assert_array_equal(toarray(X_inv_df), X_df.values)\n\n    # Check that the inverse transformed DataFrame has the correct dtype\n    assert X_inv_df.dtype == X_df.values.dtype\n"], "sample_805": ["def test_mean_tweedie_deviance():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    # check power parameter on the edges\n    assert_raises_regex(ValueError, \"Tweedie deviance is only defined for\",\n                        mean_tweedie_deviance, y_true, y_pred, p=0.5)\n\n    # check when y_true and y_pred have different lengths\n    assert_raises_regex(ValueError, \"y_true and y_pred have different number\",\n                        mean_tweedie_deviance, y_true, y_pred[:-1], p=1)\n\n    # check with a constant y_pred\n    assert_almost_equal(mean_tweedie_deviance(y_true, [2, 2, 2, 2], p=1),\n                        0.8249999999999999)\n\n    # check with a constant y_true\n    assert_almost_equal(mean_tweedie_deviance([2, 2, 2, 2], y_pred, p=1),\n                        0.8249999999999999)\n\n    # check when y_true contains negative values\n    assert_raises_regex(ValueError, \"Mean Tweedie deviance error with p=1 can\",\n                        mean_tweedie_deviance, [-1, 0, 1, 4], y_pred, p=1)\n\n    # check when y_pred contains non-positive values\n    assert_raises_regex(ValueError, \"Mean Tweedie deviance error with p=1 can\",\n                        mean_tweedie_deviance, y_true, [0, 0.5, 2., 2.], p=1)\n"], "sample_806": ["def test_gradient_boosting_oob_improvement():\n    # Test the oob improvement calculation.\n    X, y = make_classification(n_samples=1000, n_features=10,\n                               n_informative=5, n_redundant=3, random_state=1)\n\n    clf = GradientBoostingClassifier(n_estimators=20, random_state=1,\n                                     subsample=0.5)\n    clf.fit(X, y)\n\n    # check if oob improvement is in the correct range and is equal to\n    # the train score at the last iteration\n    assert np.all(clf.oob_improvement_ >= -1)\n    assert_almost_equal(clf.oob_improvement_[-1], clf.train_score_[-1])\n"], "sample_807": ["def test_calibration_curve():\n    # Test calibration_curve function\n    y_true, y_pred = make_classification(n_samples=100, n_features=20,\n                                         n_informative=10, n_redundant=0,\n                                         random_state=42)\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=5)\n\n    assert_equal(len(prob_true), 5)\n    assert_equal(len(prob_pred), 5)\n    assert_greater_equal(prob_true.min(), 0)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_greater_equal(1, prob_true.max())\n    assert_greater_equal(1, prob_pred.max())\n"], "sample_808": ["def test_iforest_sparse_input():\n    # Test that IsolationForest works with sparse input\n    X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.5, random_state=rng)\n    X_train_sparse = csc_matrix(X_train)\n    X_test_sparse = csr_matrix(X_test)\n\n    iforest = IsolationForest(contamination=0.1, random_state=rng)\n    iforest.fit(X_train_sparse)\n    y_pred_sparse = iforest.predict(X_test_sparse)\n\n    iforest = IsolationForest(contamination=0.1, random_state=rng)\n    iforest.fit(X_train)\n    y_pred_dense = iforest.predict(X_test)\n\n    assert_array_equal(y_pred_sparse, y_pred_dense)\n"], "sample_809": ["def test_mutual_info_classif_discrete_features():\n    # Test mutual information between discrete features and a target variable.\n    X = np.array([[0, 1, 0],\n                  [1, 0, 1],\n                  [0, 1, 1],\n                  [1, 0, 0]])\n    y = np.array([0, 1, 1, 0])\n    mi = mutual_info_classif(X, y, discrete_features=True)\n    assert_array_equal(mi, np.array([0.5, 0.5, 0.5]))\n"], "sample_810": ["def test_pipeline_memory():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Create a pipeline with caching\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(location=cachedir, verbose=10)\n        pipe = Pipeline([('transf', DummyTransf()), ('clf', SVC())],\n                        memory=memory)\n\n        # Check if the timestamp of the transformer is different each time\n        # fit is called.\n        timestamps = []\n        for _ in range(3):\n            pipe.fit(X, y)\n            timestamps.append(pipe.named_steps['transf'].timestamp_)\n        assert len(set(timestamps)) == 1\n\n        # Check if the cache is used\n        pipe.named_steps['transf'].timestamp_ = None\n        pipe.fit(X, y)\n        assert pipe.named_steps['transf'].timestamp_ is None\n\n    finally:\n        shutil.rmtree(cachedir)\n"], "sample_813": ["def test_bayesian_ridge_log_marginal_likelihood_value():\n    # Test log marginal likelihood value for BayesianRidge\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Fit BayesianRidge with a specific alpha and lambda value\n    br = BayesianRidge(alpha_1=0.1, alpha_2=0.1, lambda_1=0.1, lambda_2=0.1)\n    br.fit(X, y)\n\n    # Compute log marginal likelihood using _log_marginal_likelihood\n    s = br._log_marginal_likelihood(n_samples, n_features,\n                                    np.linalg.eigvalsh(np.dot(X.T, X)),\n                                    br.alpha_, br.lambda_,\n                                    br.coef_, np.sum((y - np.dot(X, br.coef_)) ** 2))\n\n    # Check if the computed log marginal likelihood is close to the actual value\n    assert_almost_equal(s, br.scores_[-1], decimal=10)\n"], "sample_815": ["def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Bad beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n\n    # Bad average option\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  [1, 2, 3], [1, 2, 3], average=\"invalid\")\n\n    # Bad pos_label\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, pos_label=\"invalid\", average='binary')\n"], "sample_816": ["def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter of TfidfVectorizer works as expected\n    vectorizer = TfidfVectorizer(max_features=5)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n\n    assert_equal(X.shape[1], 5)\n    assert_equal(len(vectorizer.vocabulary_), 5)\n    assert_equal(len(vectorizer.idf_), 5)\n    assert_equal(len(vectorizer.stop_words_), 0)  # no stop words with max_features\n\n    # Check that the most frequent terms are selected\n    freqs = {term: sum(1 for doc in ALL_FOOD_DOCS if term in doc)\n             for term in vectorizer.vocabulary_}\n    most_frequent_terms = sorted(freqs, key=freqs.get, reverse=True)[:5]\n    assert_equal(set(most_frequent_terms), set(vectorizer.vocabulary_.keys()))\n"], "sample_817": ["def test_variance_threshold_sparse():\n    X = csr_matrix(data)\n    selector = VarianceThreshold()\n    X_trans = selector.fit_transform(X)\n    assert_array_equal(X_trans.toarray(), [[1, 2, 3],\n                                            [2, 2, 3],\n                                            [1, 2, 4]])\n"], "sample_819": ["def test_voting_regressor_weights():\n    # Test weights with VotingRegressor\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=10, random_state=0)\n    vr = VotingRegressor([('lr', r1), ('rf', r2)], weights=[0.5, 0.5])\n    vr.fit(X_r, y_r)\n    assert_almost_equal(vr.predict(X_r), (r1.fit(X_r, y_r).predict(X_r) +\n                                          r2.fit(X_r, y_r).predict(X_r)) / 2)\n\n    vr = VotingRegressor([('lr', r1), ('rf', r2)], weights=[0.8, 0.2])\n    vr.fit(X_r, y_r)\n    assert_almost_equal(vr.predict(X_r), (0.8 * r1.fit(X_r, y_r).predict(X_r) +\n                                          0.2 * r2.fit(X_r, y_r).predict(X_r)))\n"], "sample_820": ["def test_voting_regressor_weights():\n    # Test weights with VotingRegressor\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    vr = VotingRegressor([('lr', r1), ('rf', r2)], weights=[0.5, 0.5])\n    vr.fit(X_r, y_r)\n    assert_almost_equal(vr.predict(X_r), (r1.fit(X_r, y_r).predict(X_r) +\n                                          r2.fit(X_r, y_r).predict(X_r)) / 2)\n\n    vr = VotingRegressor([('lr', r1), ('rf', r2)], weights=[0.8, 0.2])\n    vr.fit(X_r, y_r)\n    assert_almost_equal(vr.predict(X_r), (0.8 * r1.fit(X_r, y_r).predict(X_r) +\n                                          0.2 * r2.fit(X_r, y_r).predict(X_r)))\n"], "sample_821": ["def test_affinity_propagation_equal_similarities_and_preferences():\n    # Test the case where all similarities and preferences are equal.\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 1, 1])\n\n    assert _equal_similarities_and_preferences(S, preference)\n\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert_array_equal(cluster_centers_indices, np.arange(3))\n    assert_array_equal(labels, np.arange(3))\n\n    # Test the case where all similarities are equal but preferences are not.\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([0.5, 0.5, 0.5])\n\n    assert _equal_similarities_and_preferences(S, preference)\n\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert_array_equal(cluster_centers_indices, np.array([0]))\n    assert_array_equal(labels, np.array([0, 0, 0]))\n"], "sample_825": ["def test_pls_regression():\n    # Test PLS regression\n    X, Y = load_linnerud(return_X_y=True)\n    pls = pls_.PLSRegression(n_components=2)\n    pls.fit(X, Y)\n    assert_equal(pls.x_weights_.shape, (3, 2))\n    assert_equal(pls.y_weights_.shape, (3, 2))\n    assert_array_almost_equal(pls.transform(X).shape, (20, 2))\n    assert_array_almost_equal(pls.predict(X).shape, (20, 3))\n"], "sample_826": ["def test_one_hot_encoder_inverse_transform():\n    # Test that inverse_transform works correctly for OneHotEncoder\n    X = np.array([[1, 2], [3, 4]])\n    enc = OneHotEncoder()\n    X_trans = enc.fit_transform(X)\n    X_inv = enc.inverse_transform(toarray(X_trans))\n    assert_array_equal(X, X_inv)\n"], "sample_827": ["def test_inplace_swap_column():\n    # Test swapping two columns of a CSR matrix\n    X = sp.csr_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    inplace_swap_column(X, 0, 2)\n    expected = np.array([[3, 2, 1], [6, 5, 4]])\n    assert_array_equal(X.toarray(), expected)\n\n    # Test swapping two columns of a CSC matrix\n    X = sp.csc_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    inplace_swap_column(X, 0, 2)\n    expected = np.array([[3, 2, 1], [6, 5, 4]])\n    assert_array_equal(X.toarray(), expected)\n\n    # Test swapping with negative indices\n    X = sp.csr_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    inplace_swap_column(X, -1, -3)\n    expected = np.array([[3, 2, 1], [6, 5, 4]])\n    assert_array_equal(X.toarray(), expected)\n"], "sample_829": ["def test_incremental_pca_inverse_transform():\n    # Test that the inverse transform of IncrementalPCA is correct\n    X = iris.data\n    ipca = IncrementalPCA(n_components=2)\n    X_ipca = ipca.fit_transform(X)\n    X_ipca_inv = ipca.inverse_transform(X_ipca)\n    assert_array_almost_equal(X, X_ipca_inv)\n"], "sample_830": ["def test_get_sys_info():\n    sys_info = _get_sys_info()\n    assert 'python' in sys_info\n    assert 'executable' in sys_info\n    assert 'machine' in sys_info\n    assert isinstance(sys_info['python'], str)\n    assert isinstance(sys_info['executable'], str)\n    assert isinstance(sys_info['machine'], str)\n"], "sample_831": ["def test_export_text():\n    # Check export_text function\n    clf = DecisionTreeClassifier(random_state=0, max_depth=2)\n    clf.fit(X, y)\n\n    report = export_text(clf, feature_names=['feature_0', 'feature_1'])\n    assert_in('feature_0 <=', report)\n    assert_in('class: 0', report)\n    assert_in('class: 1', report)\n\n    report = export_text(clf, feature_names=['feature_0', 'feature_1'],\n                         decimals=5)\n    assert_in('feature_0 <=', report)\n    assert_in('class: 0', report)\n    assert_in('class: 1', report)\n\n    report = export_text(clf, feature_names=['feature_0', 'feature_1'],\n                         spacing=4)\n    assert_in('feature_0     <=', report)\n    assert_in('class: 0', report)\n    assert_in('class: 1', report)\n\n    report = export_text(clf, feature_names=['feature_0', 'feature_1'],\n                         max_depth=1)\n    assert_in('feature_0 <=', report)\n    assert_in('...', report)\n\n    report = export_text(clf, feature_names=['feature_0', 'feature_1'],\n                         show_weights=True)\n    assert_in('feature_0 <=', report)\n    assert_in('weights: ', report)\n"], "sample_832": ["def test_bayesian_ridge_log_marginal_likelihood():\n    # Test that the log marginal likelihood is computed correctly for BayesianRidge\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Initialize parameters\n    alpha_1 = 1.e-6\n    alpha_2 = 1.e-6\n    lambda_1 = 1.e-6\n    lambda_2 = 1.e-6\n    alpha_init = 1. / np.var(y)\n    lambda_init = 1.\n\n    # Create a BayesianRidge object and fit it to the data\n    br = BayesianRidge(alpha_1=alpha_1, alpha_2=alpha_2,\n                       lambda_1=lambda_1, lambda_2=lambda_2,\n                       alpha_init=alpha_init, lambda_init=lambda_init,\n                       compute_score=True)\n    br.fit(X, y)\n\n    # Compute the log marginal likelihood using the fitted parameters\n    s = (lambda_1 * log(br.lambda_) - lambda_2 * br.lambda_)\n    s += alpha_1 * log(br.alpha_) - alpha_2 * br.alpha_\n    s += 0.5 * (n_features * log(br.lambda_) +\n                n_samples * log(br.alpha_) -\n                br.alpha_ * np.sum((y - np.dot(X, br.coef_)) ** 2) -\n                br.lambda_ * np.sum(br.coef_ ** 2))\n\n    # Check that the log marginal likelihood is close to the score\n    assert_almost_equal(s, br.scores_[-1])\n"], "sample_833": ["def test_logistic_regression_solvers():\n    \"\"\"Test that all solvers give the same result on a simple problem.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=5,\n                               n_informative=3, n_redundant=0,\n                               n_classes=2, random_state=42)\n\n    # Test that all solvers give the same result\n    solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n    results = []\n    for solver in solvers:\n        clf = LogisticRegression(solver=solver, max_iter=1000)\n        clf.fit(X, y)\n        results.append(clf.score(X, y))\n\n    assert_allclose(results, results[0], atol=1e-4)\n"], "sample_834": ["def test_nca_init():\n    # Test that the initialization of the transformation is correct\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_repeated=2, random_state=0)\n\n    # Test 'auto' initialization\n    nca = NeighborhoodComponentsAnalysis(init='auto', n_components=5)\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (5, 10))\n\n    # Test 'pca' initialization\n    nca = NeighborhoodComponentsAnalysis(init='pca', n_components=5)\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (5, 10))\n\n    # Test 'lda' initialization\n    nca = NeighborhoodComponentsAnalysis(init='lda', n_components=5)\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (5, 10))\n\n    # Test 'identity' initialization\n    nca = NeighborhoodComponentsAnalysis(init='identity', n_components=5)\n    nca.fit(X, y)\n    assert_array_equal(nca.components_, np.eye(5, 10))\n\n    # Test 'random' initialization\n    nca = NeighborhoodComponentsAnalysis(init='random', n_components=5,\n                                         random_state=0)\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (5, 10))\n\n    # Test initialization with a numpy array\n    init = rng.rand(5, 10)\n    nca = NeighborhoodComponentsAnalysis(init=init)\n    nca.fit(X, y)\n    assert_array_equal(nca.components_, init)\n"], "sample_835": ["def test_adaboost_regressor_with_svr():\n    # Test that AdaBoostRegressor works with a Support Vector Regressor\n    boston = datasets.load_boston()\n    X, y = boston.data, boston.target\n\n    svr = SVR(kernel='linear')\n    ada = AdaBoostRegressor(base_estimator=svr, n_estimators=5)\n    ada.fit(X, y)\n\n    assert_array_less(0, ada.score(X, y))\n"], "sample_836": ["def test_type_of_target_invalid_input():\n    # Test that an error is raised when the input is not array-like\n    invalid_inputs = [\n        {1, 2, 3},  # set\n        {0: 'a', 1: 'b'},  # dict\n        'abc',  # string\n        frozenset([1, 2, 3]),  # frozenset\n        None,  # None\n    ]\n\n    for y in invalid_inputs:\n        with pytest.raises(ValueError):\n            type_of_target(y)\n"], "sample_837": ["def test_get_sys_info():\n    sys_info = _get_sys_info()\n    assert 'python' in sys_info\n    assert 'executable' in sys_info\n    assert 'machine' in sys_info\n    assert isinstance(sys_info['python'], str)\n    assert isinstance(sys_info['executable'], str)\n    assert isinstance(sys_info['machine'], str)\n"], "sample_839": ["def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter of TfidfVectorizer works as expected\n    vectorizer = TfidfVectorizer(max_features=4)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert X.shape[1] == 4\n\n    # Check that the vocabulary is correctly limited to the max_features most frequent terms\n    freqs = np.array([sum(w in doc for doc in JUNK_FOOD_DOCS) for w in vectorizer.vocabulary_])\n    assert np.all(freqs >= np.sort(freqs)[-4])\n\n    # Check that the idf values are correctly computed for the limited vocabulary\n    idf = vectorizer.idf_\n    assert len(idf) == 4\n    assert np.isclose(idf.sum(), 4.0)\n"], "sample_840": ["def test_pls_regression():\n    # Test PLS regression\n    X, Y = load_linnerud(return_X_y=True)\n    pls = pls_.PLSRegression(n_components=2)\n    pls.fit(X, Y)\n    assert hasattr(pls, 'x_weights_')\n    assert hasattr(pls, 'y_weights_')\n    assert hasattr(pls, 'x_loadings_')\n    assert hasattr(pls, 'y_loadings_')\n    assert hasattr(pls, 'x_scores_')\n    assert hasattr(pls, 'y_scores_')\n    assert hasattr(pls, 'x_rotations_')\n    assert hasattr(pls, 'y_rotations_')\n    assert hasattr(pls, 'coef_')\n    assert hasattr(pls, 'n_iter_')\n\n    Y_pred = pls.predict(X)\n    assert Y_pred.shape == Y.shape\n"], "sample_841": ["def test_ridge_cv_store_cv_values():\n    # Test that cv_values are stored when store_cv_values=True\n    X, y = make_regression(n_samples=10, n_features=5)\n    ridge_cv = RidgeCV(alphas=[0.1, 1.0], store_cv_values=True)\n    ridge_cv.fit(X, y)\n    assert hasattr(ridge_cv, 'cv_values_')\n    assert ridge_cv.cv_values_.shape == (10, 2)\n"], "sample_842": ["def test_clone_kernel():\n    # Test that kernel can be cloned\n    kernel = RBF(length_scale=2.0) + WhiteKernel(noise_level=3.0)\n    kernel_cloned = clone(kernel)\n    assert isinstance(kernel_cloned, KernelOperator)\n    assert len(kernel_cloned.kernels) == 2\n    assert isinstance(kernel_cloned.kernels[0], RBF)\n    assert isinstance(kernel_cloned.kernels[1], WhiteKernel)\n    assert kernel_cloned.kernels[0].length_scale == 2.0\n    assert kernel_cloned.kernels[1].noise_level == 3.0\n"], "sample_843": ["def test_kernel_gradient():\n    # Compare analytic and numeric gradient of kernels\n    kernel = RBF(length_scale=1.0)\n        kernel_clone = clone(kernel)\n        kernel_clone.theta = theta\n        return kernel_clone(X)\n    theta = kernel.theta\n    K, K_gradient = kernel(X, eval_gradient=True)\n    approx_K_gradient = _approx_fprime(theta, f, 1e-10)\n    assert_almost_equal(K_gradient, approx_K_gradient, 4)\n\n    kernel = Matern(length_scale=1.0, nu=1.5)\n        kernel_clone = clone(kernel)\n        kernel_clone.theta = theta\n        return kernel_clone(X)\n    theta = kernel.theta\n    K, K_gradient = kernel(X, eval_gradient=True)\n    approx_K_gradient = _approx_fprime(theta, f, 1e-10)\n    assert_almost_equal(K_gradient, approx_K_gradient, 4)\n\n    kernel = ConstantKernel(constant_value=2.0) * RBF(length_scale=1.0)\n        kernel_clone = clone(kernel)\n        kernel_clone.theta = theta\n        return kernel_clone(X)\n    theta = kernel.theta\n    K, K_gradient = kernel(X, eval_gradient=True)\n    approx_K_gradient = _approx_fprime(theta, f, 1e-10)\n    assert_almost_equal(K_gradient, approx_K_gradient, 4)\n"], "sample_845": ["def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter of TfidfVectorizer works as expected\n    vectorizer = TfidfVectorizer(max_features=5)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert X.shape[1] == 5\n\n    # Check that the vocabulary is correctly limited to the top max_features\n    # terms with highest tf-idf\n    vectorizer = TfidfVectorizer(max_features=5, use_idf=False)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    vocabulary = vectorizer.vocabulary_\n    assert len(vocabulary) == 5\n    # The vocabulary should be the top 5 most frequent terms in the corpus\n    freqs = defaultdict(int)\n    for doc in ALL_FOOD_DOCS:\n        for term in doc.split():\n            freqs[term] += 1\n    top_5_freqs = sorted(freqs.items(), key=lambda x: x[1], reverse=True)[:5]\n    assert set(vocabulary.keys()) == set(term for term, _ in top_5_freqs)\n"], "sample_847": ["def test_lasso_path_return_n_iter():\n    # Test that lasso_path return correct n_iter\n    X, y = load_boston(return_X_y=True)\n    alphas, coefs, dual_gaps, n_iters = lasso_path(\n        X, y, return_n_iter=True, max_iter=1000, tol=1e-3)\n    assert_array_equal(n_iters.shape, (len(alphas),))\n    assert np.all(n_iters > 0)\n    assert np.all(n_iters <= 1000)\n"], "sample_848": ["def test_multi_output_regressor_partial_fit():\n    # Test if multi-output regressor can be partially fit\n    X, y = datasets.make_regression(n_samples=100, n_features=10, n_targets=3)\n    X_train, y_train = X[:50], y[:50]\n    X_test, y_test = X[50:], y[50:]\n\n    estimator = MultiOutputRegressor(Ridge())\n    estimator.partial_fit(X_train, y_train)\n\n    y_pred = estimator.predict(X_test)\n    assert y_pred.shape == (50, 3)\n\n    # Continue training the model with the next batch of data\n    estimator.partial_fit(X_test, y_test)\n    y_pred = estimator.predict(X_test)\n    assert y_pred.shape == (50, 3)\n"], "sample_850": ["def test_nystroem_approximation():\n    # Test that Nystroem gives a similar approximation with different\n    # numbers of components.\n    n_components = 100\n    gamma = 0.2\n\n    X = np.random.RandomState(0).uniform(size=(300, 50))\n\n    nystrom_1 = Nystroem(kernel='rbf', gamma=gamma, n_components=n_components)\n    nystrom_2 = Nystroem(kernel='rbf', gamma=gamma, n_components=2 * n_components)\n\n    X_reduced_1 = nystrom_1.fit_transform(X)\n    X_reduced_2 = nystrom_2.fit_transform(X)[:, :n_components]\n\n    assert_array_almost_equal(X_reduced_1, X_reduced_2, decimal=5)\n"], "sample_851": ["def test_mean_tweedie_deviance():\n    # Check for power = 0, which is equivalent to mean squared error\n    y_true = np.array([1.0, 2.0, 3.0, 4.0])\n    y_pred = np.array([1.0, 2.0, 3.0, 5.0])\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=0),\n                        mean_squared_error(y_true, y_pred))\n\n    # Check for power = 1, which is equivalent to mean Poisson deviance\n    y_true = np.array([1.0, 2.0, 3.0, 4.0])\n    y_pred = np.array([1.0, 2.0, 3.0, 5.0])\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=1),\n                        mean_poisson_deviance(y_true, y_pred))\n\n    # Check for power = 2, which is equivalent to mean Gamma deviance\n    y_true = np.array([1.0, 2.0, 3.0, 4.0])\n    y_pred = np.array([1.0, 2.0, 3.0, 5.0])\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=2),\n                        mean_gamma_deviance(y_true, y_pred))\n\n    # Check for invalid power value\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, power=-1)\n\n    # Check for non-numeric power value\n    with pytest.raises(TypeError):\n        mean_tweedie_deviance(y_true, y_pred, power='a')\n"], "sample_853": ["def test_function_transformer_check_inverse():\n    X, y = friedman\n\n    # Define a transformer with inverse that does not perfectly recover the input\n        return x + 1\n\n        return x - 0.5\n\n    tt = TransformedTargetRegressor(\n        regressor=LinearRegression(),\n        func=func,\n        inverse_func=inverse_func,\n        check_inverse=True\n    )\n\n    assert_warns_message(UserWarning, \"The provided functions or transformer are\"\n                         \" not strictly inverse of each other.\",\n                         tt.fit, X, y)\n"], "sample_854": ["def test_libsvm_sparse_predict_proba():\n    # Test that predict_proba works with sparse input\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    X_sparse = sparse.csr_matrix(X)\n\n    clf = svm.SVC(probability=True, random_state=0)\n    clf.fit(X, y)\n\n    y_proba = clf.predict_proba(X_sparse)\n    assert_array_almost_equal(y_proba.sum(axis=1), np.ones(len(X_sparse)))\n"], "sample_855": ["def test_dummy_classifier_predict_sparse():\n    # Test that predict works with sparse targets\n    X = np.array([[0], [0], [0], [0]])  # ignored\n    y = sp.csr_matrix(np.array([1, 2, 1, 1]))\n    clf = DummyClassifier(strategy='most_frequent')\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, np.array([1, 1, 1, 1]))\n"], "sample_857": ["def test_tree_pruning():\n    # Test that the tree pruning works as expected.\n    X, y = DATASETS[\"iris\"][\"X\"], DATASETS[\"iris\"][\"y\"]\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    # Prune the tree to have only 5 leaves\n    clf.prune_tree(ccp_alpha=0.1)\n\n    # Check that the pruned tree has the correct number of leaves\n    assert clf.tree_.n_leaves == 5\n\n    # Check that the pruned tree still gives the same predictions\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, clf.predict(X))\n"], "sample_858": ["def test_voting_regressor_weights():\n    # Test weights with VotingRegressor\n    X, y = datasets.make_regression(n_samples=100, n_features=5, random_state=0)\n    estimator1 = DecisionTreeRegressor(random_state=0)\n    estimator2 = RandomForestRegressor(n_estimators=10, random_state=0)\n    voting_regressor = VotingRegressor(estimators=[('dt', estimator1), ('rf', estimator2)],\n                                       weights=[0.8, 0.2])\n    voting_regressor.fit(X, y)\n    assert_array_almost_equal(voting_regressor.predict(X),\n                              (0.8 * estimator1.predict(X) +\n                               0.2 * estimator2.predict(X)))\n"], "sample_861": ["def test_parameter_grid_iter_and_getitem_equivalence():\n    param_grid = {'a': [1, 2], 'b': [True, False]}\n    grid = ParameterGrid(param_grid)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n    grid = ParameterGrid(param_grid)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_distributions = {'a': [1, 2], 'b': [True, False]}\n    sampler = ParameterSampler(param_distributions, n_iter=10)\n    assert_grid_iter_equals_getitem(sampler)\n\n    param_distributions = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n    sampler = ParameterSampler(param_distributions, n_iter=10)\n    assert_grid_iter_equals_getitem(sampler)\n"], "sample_862": ["def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter of TfidfVectorizer is equivalent to\n    # applying CountVectorizer with max_features and then TfidfTransformer.\n    n_samples = 100\n    n_features = 20\n    X = np.random.randint(0, 2, size=(n_samples, n_features))\n    y = np.random.randint(0, 2, size=n_samples)\n\n    vectorizer1 = TfidfVectorizer(max_features=10)\n    vectorizer1.fit_transform(X.astype(str))\n\n    vectorizer2 = CountVectorizer(max_features=10)\n    X_count = vectorizer2.fit_transform(X.astype(str))\n    tfidf = TfidfTransformer()\n    tfidf.fit(X_count)\n    assert_array_equal(vectorizer1.vocabulary_, vectorizer2.vocabulary_)\n    assert_array_almost_equal(vectorizer1.idf_, tfidf.idf_)\n"], "sample_864": ["def test_estimate_bandwidth():\n    # Test estimate_bandwidth with different quantiles\n    bandwidth = estimate_bandwidth(X, quantile=0.5)\n    assert bandwidth > 0\n\n    # Test estimate_bandwidth with random_state\n    bandwidth_random_state = estimate_bandwidth(X, random_state=42)\n    assert bandwidth_random_state > 0\n\n    # Test estimate_bandwidth with n_samples\n    bandwidth_n_samples = estimate_bandwidth(X, n_samples=100)\n    assert bandwidth_n_samples > 0\n\n    # Test estimate_bandwidth with a small quantile\n    bandwidth_small_quantile = estimate_bandwidth(X, quantile=0.1)\n    assert bandwidth_small_quantile > 0\n\n    # Test estimate_bandwidth with a large quantile\n    bandwidth_large_quantile = estimate_bandwidth(X, quantile=0.9)\n    assert bandwidth_large_quantile > 0\n"], "sample_865": ["def test_tree_pruning():\n    # Test that tree pruning is correctly applied\n    X, y = DATASETS[\"clf_small\"][\"X\"], DATASETS[\"clf_small\"][\"y\"]\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=0.1)\n    clf.fit(X, y)\n\n    # Get the number of nodes before pruning\n    num_nodes_before_pruning = clf.tree_.node_count\n\n    # Prune the tree\n    clf._prune_tree()\n\n    # Get the number of nodes after pruning\n    num_nodes_after_pruning = clf.tree_.node_count\n\n    # Check that the number of nodes has decreased\n    assert num_nodes_after_pruning < num_nodes_before_pruning\n\n    # Check that the pruned tree still gives the same predictions\n    y_pred_before_pruning = DecisionTreeClassifier(random_state=0).fit(X, y).predict(X)\n    y_pred_after_pruning = clf.predict(X)\n    assert_array_equal(y_pred_before_pruning, y_pred_after_pruning)\n"], "sample_866": ["def test_affinity_propagation_equal_similarities_and_preferences():\n    # Test the case where all similarities and preferences are equal.\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 1, 1])\n\n    assert _equal_similarities_and_preferences(S, preference)\n\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert_array_equal(cluster_centers_indices, np.arange(3))\n    assert_array_equal(labels, np.arange(3))\n"], "sample_867": ["def test_parameter_grid_getitem():\n    param_grid = {'a': [1, 2], 'b': [3, 4]}\n    grid = ParameterGrid(param_grid)\n    assert len(grid) == 4\n\n    expected = [{'a': 1, 'b': 3}, {'a': 1, 'b': 4},\n                {'a': 2, 'b': 3}, {'a': 2, 'b': 4}]\n    for i in range(len(grid)):\n        assert grid[i] == expected[i]\n"], "sample_870": ["def test_gpr_log_marginal_likelihood_value():\n    # Check that log marginal likelihood value is computed correctly\n    kernel = C(1.0, (1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-3, 1e3))\n    gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-5)\n    gpr.fit(X, y)\n\n    # Compute log marginal likelihood manually\n    K = kernel(X)\n    K[np.diag_indices_from(K)] += 1e-5\n    L = np.linalg.cholesky(K)\n    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))\n    log_likelihood = -0.5 * np.dot(y.T, alpha) - np.log(np.diag(L)).sum() - 0.5 * len(X) * np.log(2 * np.pi)\n\n    assert_almost_equal(gpr.log_marginal_likelihood_value_, log_likelihood)\n"], "sample_871": ["def test_silhouette_score_sparse_input():\n    # Test silhouette score with sparse input\n    X, y = datasets.make_blobs(n_samples=50, n_features=2, centers=3,\n                               cluster_std=0.8, random_state=0)\n    X_sparse = csr_matrix(X)\n\n    score_dense = silhouette_score(X, y, metric=\"euclidean\")\n    score_sparse = silhouette_score(X_sparse, y, metric=\"euclidean\")\n\n    assert_allclose(score_dense, score_sparse)\n"], "sample_872": ["def test_roc_curve_drop_intermediate():\n    # Test that drop_intermediate drops the correct thresholds\n    y_true = np.array([0, 0, 0, 1, 1])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8, 0.9])\n    fpr, tpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n\n    assert len(thresholds) == 3\n    assert_array_almost_equal(fpr, [0, 0.5, 1])\n    assert_array_almost_equal(tpr, [0.5, 0.5, 1])\n\n    fpr, tpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=False)\n    assert len(thresholds) == 4\n    assert_array_almost_equal(fpr, [0, 0.5, 0.5, 1])\n    assert_array_almost_equal(tpr, [0.5, 0.5, 1, 1])\n"], "sample_873": ["def test_transform_1d_array():\n    # Test that transform works with 1D arrays\n    selector = StepSelector(step=2)\n    selector.fit(X)\n    X_1d = np.arange(10)\n    Xt_1d = selector.transform(X_1d)\n    assert_array_equal(Xt_1d, X_1d[::2])\n"], "sample_876": ["def test_mlp_classifier_partial_fit():\n    # Test if partial_fit method works as expected\n    X, y = classification_datasets[0]\n    clf = MLPClassifier(max_iter=1)\n    for _ in range(10):\n        clf.partial_fit(X, y, classes=np.unique(y))\n    y_pred = clf.predict(X)\n    assert np.mean(y_pred == y) > 0.5\n"], "sample_877": ["def test_isotonic_regression_ties():\n    # Test isotonic regression with ties in the input data.\n    X = np.array([1, 2, 3, 4, 5, 6])\n    y = np.array([1, 2, 2, 4, 5, 5])\n\n    ir = IsotonicRegression()\n    ir.fit(X, y)\n\n    assert_array_almost_equal(ir.predict(X), [1., 2., 2., 4., 5., 5.])\n"], "sample_879": ["def test_one_hot_encoder_handle_unknown_if_exist():\n    # Test the 'infrequent_if_exist' handle unknown strategy of OneHotEncoder\n    X = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3], dtype=object).T\n\n    ohe = OneHotEncoder(max_categories=3, handle_unknown=\"infrequent_if_exist\")\n    ohe.fit(X)\n\n    # 'd' is infrequent and should be mapped to the infrequent category\n    X_test = [[\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"]]\n    X_trans = ohe.transform(X_test)\n    assert X_trans.shape == (len(X_test), 4)  # 3 frequent categories + 1 infrequent\n    assert_array_equal(X_trans.toarray()[-2:], X_trans.toarray()[-1:])\n"], "sample_881": ["def test_roc_auc_score_binary_class():\n    # Test Area under Receiver Operating Characteristic Curve (ROC AUC)\n    # from prediction scores for binary class problem.\n    y_true, _, y_score = make_prediction(binary=True)\n\n    auc_score = roc_auc_score(y_true, y_score)\n    assert_array_almost_equal(auc_score, 0.90, decimal=2)\n\n    assert_almost_equal(auc_score, _auc(y_true, y_score), decimal=5)\n    assert_almost_equal(auc_score, _partial_roc_auc_score(y_true, y_score, 1.0), decimal=5)\n"], "sample_882": ["def test_mlp_classifier_partial_fit():\n    # Test if partial_fit method works as expected\n    X, y = make_multilabel_classification(n_samples=200, n_features=10)\n    clf = MLPClassifier(max_iter=1, warm_start=True)\n\n    with ignore_warnings(category=ConvergenceWarning):\n        for i in range(10):\n            clf.partial_fit(X, y, classes=np.unique(y))\n            assert clf.n_iter_ == i + 1\n\n    # Test if partial_fit method raises an error when classes are not provided\n    clf = MLPClassifier(max_iter=1)\n    with pytest.raises(ValueError):\n        clf.partial_fit(X, y)\n\n    # Test if partial_fit method raises an error when classes are different\n    clf = MLPClassifier(max_iter=1)\n    clf.partial_fit(X, y, classes=np.unique(y))\n    with pytest.raises(ValueError):\n        clf.partial_fit(X, y, classes=np.array([0, 2]))\n"], "sample_883": ["def test_bayesian_ridge_log_marginal_likelihood():\n    # Test that the log marginal likelihood is computed correctly\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Initialize BayesianRidge with a fixed alpha and lambda\n    br = BayesianRidge(alpha_1=0.1, alpha_2=0.1, lambda_1=0.1, lambda_2=0.1)\n    br.fit(X, y)\n\n    # Compute the log marginal likelihood using the _log_marginal_likelihood method\n    s = br._log_marginal_likelihood(n_samples, n_features, np.linalg.eigvalsh(np.dot(X.T, X)), br.alpha_, br.lambda_, br.coef_, np.sum((y - np.dot(X, br.coef_)) ** 2))\n\n    # Compute the log marginal likelihood manually\n    lambda_1 = br.lambda_1\n    lambda_2 = br.lambda_2\n    alpha_1 = br.alpha_1\n    alpha_2 = br.alpha_2\n    score = lambda_1 * log(br.lambda_) - lambda_2 * br.lambda_\n    score += alpha_1 * log(br.alpha_) - alpha_2 * br.alpha_\n    score += 0.5 * (\n        n_features * log(br.lambda_)\n        + n_samples * log(br.alpha_)\n        - br.alpha_ * np.sum((y - np.dot(X, br.coef_)) ** 2)\n        - br.lambda_ * np.sum(br.coef_**2)\n        + fast_logdet(np.linalg.inv(np.dot(X.T, X) + br.lambda_ / br.alpha_ * np.eye(n_features)))\n        - n_samples * log(2 * np.pi)\n    )\n\n    assert_almost_equal(s, score)\n"], "sample_884": ["def test_deprecated_class_override_new():\n    # Test that a deprecated class that overrides __new__ still raises a\n    # deprecation warning when instantiated.\n    with pytest.warns(FutureWarning, match=\"Class MockClass6 is deprecated\"):\n        MockClass6(1)\n"], "sample_886": ["def test_wrap_in_pandas_container_sparse():\n    # Test that wrapping sparse data raises an error\n    sparse_data = csr_matrix(np.array([[1, 2], [3, 4]]))\n    with pytest.raises(ValueError):\n        _wrap_in_pandas_container(sparse_data, columns=[\"col1\", \"col2\"])\n"], "sample_887": ["def test_calibration_display_attributes(data):\n    X, y = data\n    clf = LogisticRegression()\n    clf.fit(X, y)\n    y_prob = clf.predict_proba(X)[:, 1]\n\n    disp = CalibrationDisplay.from_estimator(clf, X, y)\n    assert hasattr(disp, \"line_\")\n    assert hasattr(disp, \"ax_\")\n    assert hasattr(disp, \"figure_\")\n\n    disp = CalibrationDisplay.from_predictions(y, y_prob)\n    assert hasattr(disp, \"line_\")\n    assert hasattr(disp, \"ax_\")\n    assert hasattr(disp, \"figure_\")\n"], "sample_888": ["def test_iforest_average_path_length():\n    # Test the average path length computation function\n    n_samples_leaf = np.array([1, 2, 3, 4, 5])\n    average_path_length_computed = _average_path_length(n_samples_leaf)\n    average_path_length_expected = np.array([\n        0., 1., 2 * (np.log(2) + np.euler_gamma) - 2. / 3,\n        2 * (np.log(3) + np.euler_gamma) - 6. / 4,\n        2 * (np.log(4) + np.euler_gamma) - 12. / 5\n    ])\n    assert_allclose(average_path_length_computed, average_path_length_expected)\n"], "sample_889": ["def test_calibration_display_from_estimator(data):\n    X, y = data\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X, y)\n\n    display = CalibrationDisplay.from_estimator(clf, X, y)\n    assert hasattr(display, \"line_\")\n    assert hasattr(display, \"ax_\")\n    assert hasattr(display, \"figure_\")\n\n    # check that the plot displays the correct number of lines\n    lines = display.ax_.lines\n    assert len(lines) == 2  # one for the calibration curve and one for the reference line\n\n    # check that the plot displays the correct title and labels\n    assert display.ax_.get_title() is None\n    assert display.ax_.get_xlabel() == \"Mean predicted probability\"\n    assert display.ax_.get_ylabel() == \"Fraction of positives\"\n"], "sample_890": ["def test_sequential_feature_selector_with_auto_and_tol():\n    # Test that SequentialFeatureSelector with 'auto' and tol works as expected\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    estimator = LinearRegression()\n    sfs = SequentialFeatureSelector(estimator, n_features_to_select='auto', tol=0.01)\n    sfs.fit(X, y)\n    assert sfs.n_features_to_select_ > 0\n    assert sfs.support_.sum() == sfs.n_features_to_select_\n"], "sample_892": ["def test_adaboost_regressor_with_sparse_inputs():\n    # Test AdaBoostRegressor with sparse inputs\n    X_sparse = csr_matrix(X)\n    y_sparse = np.array(y_regr)\n\n    # Trained on sparse inputs\n    sparse_regressor = AdaBoostRegressor()\n    sparse_regressor.fit(X_sparse, y_sparse)\n\n    # Predict on sparse inputs\n    y_pred_sparse = sparse_regressor.predict(X_sparse)\n\n    # Compare with dense inputs\n    dense_regressor = AdaBoostRegressor()\n    dense_regressor.fit(np.array(X), y_sparse)\n    y_pred_dense = dense_regressor.predict(np.array(X))\n\n    assert_array_almost_equal(y_pred_sparse, y_pred_dense)\n"], "sample_893": ["def test_export_text_feature_names():\n    # Check export_text with feature_names\n    clf = DecisionTreeClassifier(random_state=0, max_depth=2)\n    clf.fit(X, y)\n\n    report = export_text(clf, feature_names=[\"feature0\", \"feature1\"])\n    assert \"feature0\" in report\n    assert \"feature1\" in report\n\n    # Check export_text without feature_names\n    report = export_text(clf)\n    assert \"feature_0\" in report\n    assert \"feature_1\" in report\n"], "sample_894": ["def test_forest_regressor_sample_weight():\n    \"\"\"Check sample weighting for forest regressors.\"\"\"\n    # Create a weighted regression problem\n    rng = np.random.RandomState(0)\n    n_samples = 100\n    weights = rng.rand(n_samples)\n    X = rng.rand(n_samples, 5)\n    y = 3 * X[:, 0] + 2 * X[:, 1] + rng.randn(n_samples)\n\n    # Fit a weighted and an unweighted regressor\n    regressor = RandomForestRegressor(n_estimators=10, random_state=0)\n    weighted_regressor = RandomForestRegressor(n_estimators=10, random_state=0)\n\n    regressor.fit(X, y)\n    weighted_regressor.fit(X, y, sample_weight=weights)\n\n    # Check that the weighted regressor predicts closer to the weighted targets\n    y_pred = regressor.predict(X)\n    weighted_y_pred = weighted_regressor.predict(X)\n    assert mean_squared_error(y, weighted_y_pred, sample_weight=weights) < mean_squared_error(\n        y, y_pred, sample_weight=weights\n    )\n"], "sample_898": ["def test_auc_score_non_proba():\n    # Test AUC score for non-probability predictions\n    y_true = np.array([0, 0, 1, 1])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8])\n\n    # Check for normal case\n    auc = roc_auc_score(y_true, y_score)\n    assert_almost_equal(auc, 0.75)\n\n    # Check for the case when only positive or negative labels are present\n    y_true = np.array([0, 0, 0, 0])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    assert_raises(ValueError, roc_auc_score, y_true, y_score)\n\n    y_true = np.array([1, 1, 1, 1])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    assert_raises(ValueError, roc_auc_score, y_true, y_score)\n"], "sample_900": ["def test_mlp_classifier_partial_fit():\n    # Test partial fit for various activation functions\n    for act in ACTIVATION_TYPES:\n        clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1,\n                            activation=act, solver='sgd', learning_rate_init=0.01)\n        for X, y in classification_datasets:\n            with ignore_warnings(category=ConvergenceWarning):\n                clf.partial_fit(X, y, classes=np.unique(y))\n                assert clf.score(X, y) > 0.5\n"], "sample_901": ["def test_kmeans_init_centers():\n    # Check that the KMeans initialization does not crash when the number of\n    # clusters is larger than the number of samples.\n    X = np.array([[1, 2], [3, 4]])\n    kmeans = KMeans(n_clusters=5)\n    assert_warns(ConvergenceWarning, kmeans.fit, X)\n"], "sample_902": ["def test_pipeline_transformer_weights():\n    # Test that transformer weights are taken into account in pipeline\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n\n    transf1 = Transf()\n    transf2 = Transf()\n\n    pipe = Pipeline([('transf1', transf1), ('transf2', transf2)])\n    pipe.transformer_weights = {'transf1': 0.5, 'transf2': 2.}\n\n    pipe.fit(X, y)\n    X_trans = pipe.transform(X)\n\n    # Check that transformer weights are applied correctly\n    assert_array_almost_equal(X_trans, 2. * 2 * X)\n"], "sample_904": ["def test_std_domain_resolve_xref_option():\n    app = mock.Mock()\n    env = mock.Mock()\n    builder = mock.Mock()\n    fromdocname = 'test'\n    typ = 'option'\n    target = '-a'\n    node = pending_xref('', refdomain='std', reftype='option', reftarget=target)\n    contnode = nodes.inline('', '')\n\n    domain = StandardDomain(env)\n    domain.progoptions['program', target] = ('test', 'labelid')\n\n    result = domain.resolve_xref(app, fromdocname, builder, typ, target, node, contnode)\n    assert isinstance(result, nodes.reference)\n    assert result.get('refuri') == '#labelid'\n"], "sample_908": ["def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse_arguments(node) == expected\n"], "sample_909": ["    def test_attributes_with_type(self):\n        docstring = \"\"\"\\"], "sample_912": ["def test_parse_annotation():\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"List[Tuple[int, str]]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"Dict[str, List[int]]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n"], "sample_913": ["def test_parse_annotation():\n    annotation = \"Union[int, str]\"\n    expected = [\n        addnodes.desc_sig_name(text=\"Union\", fullname=\"Union\"),\n        addnodes.desc_sig_punctuation(text=\"[\", prefix=\"\", suffix=\"\"),\n        addnodes.desc_sig_name(text=\"int\", fullname=\"int\"),\n        addnodes.desc_sig_punctuation(text=\", \", prefix=\"\", suffix=\"\"),\n        addnodes.desc_sig_name(text=\"str\", fullname=\"str\"),\n        addnodes.desc_sig_punctuation(text=\"]\", prefix=\"\", suffix=\"\"),\n    ]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n"], "sample_914": ["def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse_arguments(node) == expected\n"], "sample_916": ["def test_template_parameter_pack():\n    check(\"type\", \"myType<Types...>\", {1: \"cpp-type-myType-Typess\", 2: \"cpp-type-myType-Typess\"},\n          \"myType<Types...>\")\n"], "sample_918": ["def test_parse_annotation():\n    annotation = \"Union[int, str]\"\n    expected = [\n        addnodes.desc_sig_name('', 'Union'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n"], "sample_920": ["    def test_attributes_with_trailing_colon(self):\n        docstring = \"\"\"\\"], "sample_925": ["def test_mock_object():\n    mock_obj = _MockObject()\n    assert isinstance(mock_obj, _MockObject)\n    assert mock_obj.__display_name__ == '_MockObject'\n    assert mock_obj.__sphinx_mock__ is True\n\n    # Test __new__ method with 3 arguments\n    subclass_mock_obj = _MockObject('subclass', (_MockObject,), {})\n    assert isinstance(subclass_mock_obj, _MockObject)\n    assert subclass_mock_obj.__display_name__ == '_MockObject'\n\n    # Test __len__, __contains__, and __iter__ methods\n    assert len(mock_obj) == 0\n    assert 'key' not in mock_obj\n    assert list(mock_obj) == []\n\n    # Test __mro_entries__ method\n    assert mock_obj.__mro_entries__(()) == (_MockObject,)\n\n    # Test __getitem__ and __getattr__ methods\n    item_mock_obj = mock_obj['item']\n    assert isinstance(item_mock_obj, _MockObject)\n    attr_mock_obj = mock_obj.attr\n    assert isinstance(attr_mock_obj, _MockObject)\n\n    # Test __call__ method\n    called_mock_obj = mock_obj()\n    assert called_mock_obj is mock_obj\n\n    # Test __repr__ method\n    assert repr(mock_obj) == '_MockObject'\n"], "sample_926": ["def test_anon_union():\n    check('type', 'union { int a; float b; }',\n          {1: 'union-anon-TestDoc', 2: 'union-anon-TestDoc'},\n          key='anon')\n"], "sample_928": ["def test_default_role():\n    docname = 'test'\n    name = 'test_role'\n\n    with default_role(docname, name) as _:\n        pass\n\n    # Test if the role is unregistered after the context manager exits\n    assert docutils.unregister_role('') is None\n"], "sample_929": ["def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"List[Tuple[int, str]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"Dict[str, int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n"], "sample_930": ["def test_create_index(app, status, warning):\n    app.add_object_type('index', 'pair', objname='index pair',\n                        indextemplate='pair: %s; %s')\n    restructuredtext.app = app\n    src = \"\"\"\n    .. index:: pair: hello; world\n\n    .. index:: single: foo\n    \"\"\"\n    app.add_source(src, name='test')\n    entries = IndexEntries(app.env)\n    genindex = entries.create_index(app.builder)\n    assert len(genindex) == 2\n    assert genindex[0][0] == 'F'\n    assert genindex[1][0] == 'H'\n"], "sample_931": ["def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[str, int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"Optional[str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"List[int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"Dict[str, int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n"], "sample_933": ["def test_gettext_additional_targets(app, status, warning):\n    app.builder.build_all()\n    pot_files = [f for f in os.listdir(app.outdir) if f.endswith('.pot')]\n    assert len(pot_files) == 2\n    assert 'index.pot' in pot_files\n    assert 'extra.pot' in pot_files\n\n    with open(os.path.join(app.outdir, 'index.pot'), 'r') as f:\n        content = f.read()\n        assert re.search(r'msgid \"Index entry\"', content)\n        assert re.search(r'msgstr \"\"', content)\n\n    with open(os.path.join(app.outdir, 'extra.pot'), 'r') as f:\n        content = f.read()\n        assert re.search(r'msgid \"Extra entry\"', content)\n        assert re.search(r'msgstr \"\"', content)\n"], "sample_936": ["def test_stringify_py37():\n    # Test stringify for Python 3.7+\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef\n\n        # Test ForwardRef\n        forward_ref = ForwardRef('MyClass1')\n        assert stringify(forward_ref) == 'MyClass1'\n\n        # Test Union with None\n        union_type = Union[MyClass1, None]\n        assert stringify(union_type) == 'Optional[MyClass1]'\n\n        # Test Callable\n        callable_type = Callable[[MyClass1], MyClass2]\n        assert stringify(callable_type) == 'Callable[[MyClass1], MyClass2]'\n\n        # Test Annotated (Python 3.9+)\n        if sys.version_info >= (3, 9):\n            from typing import Annotated\n\n            annotated_type = Annotated[MyClass1, 'annotation']\n            assert stringify(annotated_type) == 'MyClass1'\n"], "sample_937": ["def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse(node) == expected\n"], "sample_938": ["def test_default_man_pages():\n    config = Config({\n        'project': 'Test Project',\n        'release': '1.0',\n        'master_doc': 'index',\n        'author': 'John Doe'\n    })\n    pages = default_man_pages(config)\n    assert len(pages) == 1\n    assert pages[0][0] == 'index'\n    assert pages[0][1] == 'test-project'\n    assert pages[0][2] == 'Test Project 1.0'\n    assert pages[0][3] == ['John Doe']\n    assert pages[0][4] == 1\n"], "sample_941": ["def test_restify_py37():\n    # Test restify for Python 3.7+\n    if sys.version_info >= (3, 7):\n        assert restify(Union[int, str]) == ':obj:`Union`\\\\ [:class:`int`, :class:`str`]'\n        assert restify(Callable[[int], str]) == ':class:`Callable`\\\\ [[:class:`int`], :class:`str`]'\n        assert restify(Tuple[int, str]) == ':class:`Tuple`\\\\ [:class:`int`, :class:`str`]'\n        assert restify(Optional[int]) == ':obj:`Optional`\\\\ [:class:`int`]'\n        assert restify(MyList[int]) == ':class:`MyList`\\\\ [:class:`int`]'\n"], "sample_943": ["def test_apidoc_include_private(apidoc):\n    \"\"\"Test that the -P option includes private modules.\"\"\"\n    assert (apidoc.outdir / 'test-root._private.rst').exists()\n"], "sample_944": ["def test_restify_py37():\n    if sys.version_info < (3, 7):\n        pytest.skip('Python 3.7+ required')\n\n    # Test restify with a TypeVar\n    assert restify(T) == ':obj:`T`'\n\n    # Test restify with a NewType\n    assert restify(MyInt) == ':class:`MyInt`'\n\n    # Test restify with a GenericMeta\n    assert restify(MyList[int]) == ':class:`MyList`\\\\ [int]'\n\n    # Test restify with a Union\n    assert restify(Union[int, str]) == ':obj:`Union`\\\\ [int, str]'\n\n    # Test restify with an Optional\n    assert restify(Optional[int]) == ':obj:`Optional`\\\\ [int]'\n\n    # Test restify with a Callable\n    assert restify(Callable[[int], str]) == ':class:`Callable`\\\\ [[int], str]'\n"], "sample_949": ["def test_default_man_pages():\n    config = Config(project='Test Project', release='1.0', author='John Doe',\n                    root_doc='index')\n    pages = default_man_pages(config)\n    assert len(pages) == 1\n    assert pages[0] == ('index', 'test-project', 'Test Project 1.0', ['John Doe'], 1)\n"], "sample_953": ["def test_ask_user():\n    answers = {\n        'Root path for the documentation': '.',\n        'Separate source and build directories (y/n)': 'n',\n        'Name prefix for templates and static dir': '_',\n        'Project name': 'Test Project',\n        'Author name(s)': 'John Doe',\n        'Project version': '',\n        'Project release': '',\n        'Project language': 'en',\n        'Source file suffix': '.rst',\n        'Name of your master document (without suffix)': 'index',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    assert d['path'] == '.'\n    assert not d['sep']\n    assert d['dot'] == '_'\n    assert d['project'] == 'Test Project'\n    assert d['author'] == 'John Doe'\n    assert d['version'] == ''\n    assert d['release'] == ''\n    assert d['language'] is None\n    assert d['suffix'] == '.rst'\n    assert d['master'] == 'index'\n"], "sample_954": ["def test_manpage_translator():\n    translator = ManualPageTranslator(None, None)\n    assert translator._docinfo == {\n        'title': '',\n        'subtitle': '',\n        'author': '',\n        'manual_section': '',\n        'title_upper': '',\n        'date': '',\n        'copyright': '',\n        'version': '',\n        'manual_group': ''\n    }\n    assert translator.section_level == -1\n    assert translator.in_productionlist == 0\n"], "sample_955": ["def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    func_def = tree.body[0]  # type: ignore\n    assert ast.unparse(func_def.args) == expected\n"], "sample_956": ["def test_resolve_reference_detect_inventory(app):\n    # Create a fake inventory with a named inventory 'other'\n    app.env.intersphinx_named_inventory = {\n        'other': {'std:term': {'fake-target': ('', '', 'fake-uri', '-')}}\n    }\n\n    # Create a node that should be resolved in the named inventory\n    node, contnode = fake_node('std', 'term', 'other:fake-target', 'Fake target')\n\n    # Call resolve_reference_detect_inventory\n    result = resolve_reference_detect_inventory(app.env, node, contnode)\n\n    # Check that the reference was resolved correctly\n    assert result is not None\n    assert result['refuri'] == 'fake-uri'\n\n    # Create a node that should not be resolved in the named inventory\n    node, contnode = fake_node('std', 'term', 'non-existent:fake-target', 'Fake target')\n\n    # Call resolve_reference_detect_inventory\n    result = resolve_reference_detect_inventory(app.env, node, contnode)\n\n    # Check that the reference was not resolved\n    assert result is None\n"], "sample_957": ["def test_restify_py37():\n    if sys.version_info >= (3, 7):\n        assert restify(Union[int, str]) == ':obj:`~typing.Union`\\\\ [:class:`int`, :class:`str`]'\n        assert restify(Callable[[int], str]) == ':class:`~typing.Callable`\\\\ [[:class:`int`], :class:`str`]'\n        assert restify(List[int]) == ':class:`~typing.List`\\\\ [:class:`int`]'\n        assert restify(MyList[int]) == ':class:`MyList`\\\\ [:class:`int`]'\n        assert restify(Dict[str, int]) == ':class:`~typing.Dict`\\\\ [:class:`str`, :class:`int`]'\n        assert restify(Optional[int]) == ':obj:`~typing.Optional`\\\\ [:class:`int`]'\n        assert restify(Union[int, None]) == ':obj:`~typing.Optional`\\\\ [:class:`int`]'\n        assert restify(Tuple[int, str]) == ':class:`~typing.Tuple`\\\\ [:class:`int`, :class:`str`]'\n        assert restify(Generator[int, None, None]) == ':class:`~typing.Generator`\\\\ [:class:`int`, :obj:`None`, :obj:`None`]'\n"], "sample_960": ["def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 3\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"Union\"\n    assert isinstance(result[1], pending_xref)\n    assert result[1].astext() == \"int\"\n    assert isinstance(result[2], pending_xref)\n    assert result[2].astext() == \"str\"\n\n    annotation = \"List[Tuple[str, int]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"List\"\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], nodes.Text)\n    assert result[2].astext() == \"Tuple\"\n    assert isinstance(result[3], pending_xref)\n    assert result[3].astext() == \"str\"\n    assert isinstance(result[4], pending_xref)\n    assert result[4].astext() == \"int\"\n"], "sample_962": ["def test_ismock():\n    with mock(['sphinx.ext.autodoc.mock']):\n        from sphinx.ext.autodoc.mock import _MockObject, _MockModule\n\n        assert ismock(_MockObject())\n        assert ismock(_MockModule('module'))\n        assert not ismock(object())\n"], "sample_963": ["def test_restify_py37():\n    # test restify for Python 3.7+\n    if sys.version_info >= (3, 7):\n        assert restify(Union[int, str]) == ':py:obj:`~typing.Union`\\\\ [:py:class:`int`, :py:class:`str`]'\n        assert restify(Optional[int]) == ':py:obj:`~typing.Optional`\\\\ [:py:class:`int`]'\n        assert restify(Callable[[int], str]) == ':py:obj:`~typing.Callable`\\\\ [[ :py:class:`int`], :py:class:`str`]'\n        assert restify(MyList[int]) == ':py:class:`MyList`\\\\ [:py:class:`int`]'\n        assert restify(Tuple[int, str]) == ':py:class:`tuple`\\\\ [:py:class:`int`, :py:class:`str`]'\n        assert restify(Generator[int, None, None]) == ':py:class:`generator`\\\\ [:py:class:`int`, :py:obj:`None`, :py:obj:`None`]'\n"], "sample_967": ["def test_mathjax_config(app, status, warning):\n    app.builder.build_all()\n    html = (app.outdir / 'index.html').read_text()\n    assert MATHJAX_URL in html\n    assert 'MathJax.Hub.Config' not in html  # mathjax2_config is not used by default\n    assert 'window.MathJax =' in html  # mathjax3_config is used by default\n"], "sample_968": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # simple annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n\n    # annotation with module name\n    annotation = \"typing.List[int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 3\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], desc_sig_punctuation)\n    assert isinstance(result[2], pending_xref)\n\n    # annotation with nested types\n    annotation = \"typing.Dict[str, typing.List[int]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 7\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], desc_sig_punctuation)\n    assert isinstance(result[2], pending_xref)\n    assert isinstance(result[3], desc_sig_punctuation)\n    assert isinstance(result[4], pending_xref)\n    assert isinstance(result[5], desc_sig_punctuation)\n    assert isinstance(result[6], pending_xref)\n\n    # annotation with None\n    annotation = \"None\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n"], "sample_969": ["def test_restify_py37():\n    # Test restify with Python 3.7+\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef\n\n        assert restify(ForwardRef('MyClass1')) == ':py:class:`MyClass1`'\n        assert restify(Optional[MyClass1]) == ':py:obj:`~typing.Optional`\\\\ [:py:class:`MyClass1`]'\n        assert restify(Union[MyClass1, MyClass2]) == ':py:obj:`~typing.Union`\\\\ [:py:class:`MyClass1`, :py:class:`<MyClass2>`]'\n        assert restify(Callable[[MyClass1], MyClass2]) == ':py:obj:`~typing.Callable`\\\\ [[ :py:class:`MyClass1`], :py:class:`<MyClass2>`]'\n        assert restify(MyList[int]) == ':py:class:`~typing.List`\\\\ [:py:class:`int`]'\n"], "sample_972": ["def test_restify_py37():\n    # test restify with Python 3.7+\n    if sys.version_info >= (3, 7):\n        assert restify(Optional[int]) == ':py:obj:`~typing.Optional`\\\\ [:py:class:`int`]'\n        assert restify(Union[int, str]) == ':py:obj:`~typing.Union`\\\\ [:py:class:`int`, :py:class:`str`]'\n        assert restify(Callable[[int], str]) == ':py:obj:`~typing.Callable`\\\\ [[:py:class:`int`], :py:class:`str`]'\n        assert restify(List[int]) == ':py:class:`~typing.List`\\\\ [:py:class:`int`]'\n        assert restify(MyList[int]) == ':py:class:`MyList`\\\\ [:py:class:`int`]'\n        assert restify(Tuple[int, str]) == ':py:class:`~typing.Tuple`\\\\ [:py:class:`int`, :py:class:`str`]'\n        assert restify(Dict[str, int]) == ':py:class:`~typing.Dict`\\\\ [:py:class:`str`, :py:class:`int`]'\n        assert restify(Generator[int, None, None]) == ':py:class:`~typing.Generator`\\\\ [:py:class:`int`, :py:obj:`None`, :py:obj:`None`]'\n"], "sample_974": ["def test_ccode_sinc():\n    expr = sinc(x)\n    assert ccode(expr) == '((x > 0) - (x < 0))*((x > 0) - (x < 0))*sin(x)/x'\n"], "sample_975": ["def test_nsolve_no_matrix():\n    x = Symbol('x')\n    raises(TypeError, lambda: nsolve(sin(x), 1, 2, 3))\n"], "sample_976": ["def test_symbols():\n    x, y, z = symbols('x,y,z')\n    assert [str(s) for s in symbols('x:2(1:3)')] == ['x01', 'x02', 'x11', 'x12']\n    assert [str(s) for s in symbols(':3:2')] == ['00', '01', '10', '11', '20', '21']\n    assert [str(s) for s in symbols('x:z')] == ['x', 'y', 'z']\n    assert [str(s) for s in symbols('a:d,x:z')] == ['a', 'b', 'c', 'd', 'x', 'y', 'z']\n    assert symbols('x', integer=True).is_integer\n    assert all(s.is_real for s in symbols('x,y,z', real=True))\n    a = symbols('a', cls=Dummy)\n    assert isinstance(a, Dummy)\n"], "sample_977": ["def test_Pow():\n    assert mcode(x**2) == 'x^2'\n    assert mcode(x**(y**2)) == 'x^(y^2)'\n    assert mcode((x**2)**3) == '(x^2)^3'\n"], "sample_978": ["def test_bspline_basis():\n    d = 0\n    knots = range(5)\n    n = 0\n    result = bspline_basis(d, knots, n, x)\n    expected = Piecewise((1, And(x >= 0, x <= 1)), (0, True))\n    assert result == expected\n\n    d = 3\n    knots = range(5)\n    n = 0\n    result = bspline_basis(d, knots, n, x)\n    expected = Piecewise(\n        (x**3/6, And(x >= 0, x <= 1)),\n        (-x**3/2 + 2*x**2 - 2*x + Rational(2, 3), And(x >= 1, x <= 2)),\n        (x**3/2 - 4*x**2 + 10*x - Rational(22, 3), And(x >= 2, x <= 3)),\n        (-x**3/6 + 2*x**2 - 8*x + Rational(32, 3), And(x >= 3, x <= 4)),\n        (0, True)\n    )\n    assert result == expected\n\n    d = 1\n    knots = [0, 0, 2, 3, 4]\n    n = 0\n    result = bspline_basis(d, knots, n, x)\n    expected = Piecewise((-x/2 + 1, And(x >= 0, x <= 2)), (0, True))\n    assert result == expected\n"], "sample_979": ["def test_matrix_element():\n    i, j = symbols('i j', integer=True)\n    A = MatrixSymbol('A', n, m)\n    x = MatrixElement(A, i, j)\n    assert x.parent == A\n    assert x.i == i\n    assert x.j == j\n    assert x.is_symbol\n    assert x.is_commutative\n    assert x.diff(x) == 1\n    assert x.diff(i) == 0\n    assert x.diff(j) == 0\n    assert x.diff(A) == KroneckerDelta(i, i)*KroneckerDelta(j, j)\n"], "sample_982": ["def test_core():\n    assert core(24, 2) == 6\n    assert core(9424, 3) == 1178\n    assert core(379238) == 379238\n    assert core(15**11, 10) == 15\n    raises(ValueError, lambda: core(-1))\n    raises(ValueError, lambda: core(0))\n    raises(ValueError, lambda: core(1, 1))\n    raises(ValueError, lambda: core(1, 0))\n"], "sample_984": ["def test_print_Sample():\n    from sympy.statistics.distributions import Sample\n    expr = Sample([1, 2, 3])\n    assert sstr(expr) == \"Sample([1, 2, 3])\"\n"], "sample_985": ["def test_MinMaxBase():\n    # Test that MinMaxBase classes are properly created\n    assert isinstance(Max(1, 2), Max)\n    assert isinstance(Min(1, 2), Min)\n\n    # Test that MinMaxBase classes can handle multiple arguments\n    assert Max(1, 2, 3) == Max(1, Max(2, 3))\n    assert Min(1, 2, 3) == Min(1, Min(2, 3))\n\n    # Test that MinMaxBase classes can handle nested arguments\n    assert Max(1, Max(2, 3)) == Max(1, 2, 3)\n    assert Min(1, Min(2, 3)) == Min(1, 2, 3)\n\n    # Test that MinMaxBase classes can handle comparable arguments\n    assert Max(1, 2) == 2\n    assert Min(1, 2) == 1\n\n    # Test that MinMaxBase classes can handle non-comparable arguments\n    x = symbols('x')\n    assert Max(x, 2) == Max(x, 2)\n    assert Min(x, 2) == Min(x, 2)\n"], "sample_991": ["def test_product_is_convergent():\n    assert Product(n/(n + 1), (n, 1, oo)).is_convergent() is False\n    assert Product(1/n**2, (n, 1, oo)).is_convergent() is False\n    assert Product(cos(pi/n), (n, 1, oo)).is_convergent() is True\n    assert Product(exp(-n**2), (n, 1, oo)).is_convergent() is False\n    raises(NotImplementedError, lambda: Product(f(n), (n, 1, oo)).is_convergent())\n"], "sample_992": ["def test_PythonCodePrinter_print_ITE():\n    expr = x if y > 0 else z\n    assert PythonCodePrinter().doprint(expr) == 'x if (y > 0) else z'\n"], "sample_993": ["def test_free_group_element_power():\n    assert (x**3)**2 == x**6\n    assert (x**-3)**-2 == x**6\n    assert (x**3)**-2 == x**-6\n    assert (x*y)**3 == x*y*x*y*x*y\n    assert (x*y)**-3 == y**-1*x**-1*y**-1*x**-1*y**-1*x**-1\n"], "sample_996": ["def test_product_reverse_order():\n    # Test reverse order with variable names\n    P = Product(x, (x, a, b))\n    Pr = P.reverse_order(x)\n    assert Pr == Product(1/x, (x, b + 1, a - 1))\n\n    # Test reverse order with numerical indices\n    S = Sum(x*y, (x, a, b), (y, c, d))\n    S0 = S.reverse_order(0)\n    assert S0 == Sum(-x*y, (x, b + 1, a - 1), (y, c, d))\n    S1 = S0.reverse_order(1)\n    assert S1 == Sum(x*y, (x, b + 1, a - 1), (y, d + 1, c - 1))\n\n    # Test reverse order with mixed notation\n    assert Sum(x*y, (x, a, b), (y, 2, 5)).reverse_order(x, 1) == \\\n        Sum(x*y, (x, b + 1, a - 1), (y, 6, 1))\n    assert Sum(x*y, (x, a, b), (y, 2, 5)).reverse_order(y, x) == \\\n        Sum(x*y, (x, b + 1, a - 1), (y, 6, 1))\n"], "sample_998": ["def test_latex_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == \"{1} + {2} i + {3} j + {4} k\"\n"], "sample_1000": ["def test_octave_code_sinc():\n    assert octave_code(sinc(x)) == \"sinc(x/pi)\"\n"], "sample_1004": ["def test_conditionset_subs():\n    c = ConditionSet(x, x < 1, Interval(0, 2))\n    assert c.subs(x, y) == ConditionSet(x, x < 1, Interval(0, 2))\n    assert c.subs(x, y).subs(x, y) == ConditionSet(y, y < 1, Interval(0, 2))\n    raises(ValueError, lambda: c.subs(x, 1))\n"], "sample_1006": ["def test_factorial():\n    x = Symbol('x')\n    l = [factorial(i) for i in range(10)]\n    assert l == [1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880]\n    assert factorial(-1) == zoo\n    assert factorial(x).fdiff(x) == gamma(x + 1)*polygamma(0, x + 1)\n    assert factorial(nan) == nan\n    assert factorial(oo) == oo\n"], "sample_1007": ["def test_factorial():\n    x = Symbol('x')\n    assert factorial(-1) == zoo\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(2) == 2\n    assert factorial(3) == 6\n    assert factorial(4) == 24\n    assert factorial(x) == factorial(x)\n    assert factorial(2*x) == factorial(2*x)\n    assert factorial(S(1)/2) == factorial(S(1)/2)\n    assert factorial(nan) == nan\n    assert factorial(oo) == oo\n"], "sample_1011": ["def test_octave_code_sinc():\n    assert octave_code(sinc(x)) == \"sinc(x/pi)\"\n"], "sample_1013": ["def test_lambdify_with_integer_function():\n    f = lambdify(x, x**2, 'math')\n    assert f(2) == 4\n    f = lambdify(x, x**2, 'mpmath')\n    assert f(2) == 4\n    if numpy:\n        f = lambdify(x, x**2, 'numpy')\n        assert f(2) == 4\n    if numexpr:\n        f = lambdify(x, x**2, 'numexpr')\n        assert f(2) == 4\n    if tensorflow:\n        f = lambdify(x, x**2, 'tensorflow')\n        assert f(2).eval(session=tf.Session()) == 4\n"], "sample_1014": ["def test_DenseNDimArray_getitem():\n    array = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert array[0, 0] == 1\n    assert array[1, 1] == 4\n\n    array = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    assert array[0, 1] == 2\n    assert array[1, 2] == 6\n\n    i, j = symbols('i j', integer=True)\n    assert array[i, j].subs({i: 0, j: 1}) == 2\n    assert array[i, j].subs({i: 1, j: 2}) == 6\n\n    raises(IndexError, lambda: array[2, 3])\n"], "sample_1015": ["def test_ccode_reserved_words():\n    x, y = symbols('x, y')\n    expr = x + y\n    assign_to = 'import'\n    with raises(ValueError):\n        ccode(expr, assign_to=assign_to)\n    printer = C89CodePrinter({'error_on_reserved': False})\n    assert printer.doprint(expr, assign_to) == 'import = x + y;'\n"], "sample_1016": ["def test_octave_code_sinc():\n    assert octave_code(sinc(x)) == \"sinc(x/pi)\"\n"], "sample_1018": ["def test_printing_of_While():\n    x = symbols('x')\n    expr = While(x < 5, AugAssign(x, '+', 1))\n    result = fcode(expr)\n    assert result == 'do while (x < 5)\\n  x = x + 1\\nend do'\n"], "sample_1020": ["def test_Pow():\n    assert mcode(x**2) == 'x^2'\n    assert mcode(x**(y**2)) == 'x^(y^2)'\n    assert mcode((x**2)**3) == '(x^2)^3'\n"], "sample_1023": ["def test_cycle_length():\n    f = lambda i: (i**2 + 1) % 51\n    assert next(cycle_length(f, 4)) == (6, 2)\n    assert list(cycle_length(f, 4, values=True)) == [17, 35, 2, 5, 26, 14, 44, 50, 2, 5, 26, 14]\n    assert next(cycle_length(f, 4, nmax=4)) == (4, None)\n    assert list(cycle_length(f, 4, nmax=4, values=True)) == [17, 35, 2, 5]\n"], "sample_1025": ["def test_PythonCodePrinter_print_ImaginaryUnit():\n    assert PythonCodePrinter().doprint(1j) == '1j'\n"], "sample_1026": ["def test_lambdify_matrix_arg():\n    x, y = symbols('x y')\n    M = MatrixSymbol('M', 2, 2)\n    f = lambdify((M,), M*x + y, 'numpy')\n    A = numpy.array([[1, 2], [3, 4]])\n    assert numpy.array_equal(f(A), A*1 + 0)\n"], "sample_1028": ["def test_Mod_eval():\n    # test for issue 12822\n    assert Mod(5, 3).eval() == 2\n    assert Mod(-5, 3).eval() == 1\n    assert Mod(5, -3).eval() == -1\n    assert Mod(-5, -3).eval() == -2\n\n    # test for issue 13146\n    assert Mod(0, 3).eval() == 0\n    assert Mod(0, -3).eval() == 0\n\n    # test for issue 13847\n    assert Mod(Mod(x, y), y).eval() == Mod(x, y)\n    assert Mod(Mod(x, y), z).eval() != Mod(x, z)\n\n    # test for floating point numbers\n    assert same_and_same_prec(Mod(3.4, 1.2).eval(), Float(1.0))\n    assert same_and_same_prec(Mod(-3.4, 1.2).eval(), Float(0.8))\n\n    # test for complex numbers\n    assert Mod(3 + 4*I, 2).eval() == 1 + 4*I\n    assert Mod(-3 - 4*I, 2).eval() == 1 - 4*I\n"], "sample_1029": ["def test_srepr_function():\n    f = Function('f')\n    sT(f(x, y), \"Function('f')(Symbol('x'), Symbol('y'))\")\n    sT(sin(x), \"sin(Symbol('x'))\")\n    sT(Abs(x), \"Abs(Symbol('x'))\")\n"], "sample_1032": ["def test_real_root():\n    x = Symbol('x', real=True)\n    assert real_root(x**3, 3) == x\n    assert real_root(-x**3, 3) == -x\n    assert real_root(x**4, 4) == Abs(x)\n    assert real_root(-x**4, 4) == -Abs(x)\n    assert real_root(x**5, 5) == x\n    assert real_root(-x**5, 5) == -x\n\n    assert real_root(8, 3) == 2\n    assert real_root(-8, 3) == -2\n    assert real_root(16, 4) == 2\n    assert real_root(-16, 4) == -2\n    assert real_root(32, 5) == 2\n    assert real_root(-32, 5) == -2\n\n    assert real_root(x**3 + 1, 3) == real_root(x**3 + 1, 3)\n    assert real_root(-x**3 - 1, 3) == real_root(-x**3 - 1, 3)\n\n    assert real_root((x + 1)**3, 3) == x + 1\n    assert real_root(-(x + 1)**3, 3) == -(x + 1)\n"], "sample_1033": ["def test_Add_as_coefficients_dict():\n    assert Add(3*x + a*x + 4).as_coefficients_dict() == {1: 4, x: 3, a*x: 1}\n    assert Add(3*a*x).as_coefficients_dict() == {a*x: 3}\n    assert Add(3*x + 2*y + 4).as_coefficients_dict()[a] == 0\n"], "sample_1034": ["def test_WGate_apply():\n    wgate = WGate(2)\n    basis_states = superposition_basis(2)\n    result = qapply(wgate * basis_states)\n    assert result == basis_states\n"], "sample_1035": ["def test_grover_algorithm():\n    # Apply the Grover algorithm to two qubits\n    nqubits = 2\n    target = IntQubit(2, nqubits=nqubits)\n    oracle = OracleGate(nqubits, return_one_on_two)\n    initial_state = superposition_basis(nqubits)\n    result = apply_grover(initial_state, oracle, iterations=1)\n    # Check if the result is correct\n    assert represent(result).norm() == 1\n    # Check if the target state has a higher probability\n    target_prob = (represent(target).H*represent(result))[0, 0]\n    non_target_prob = (represent(IntQubit(0, nqubits=nqubits)).H*represent(result))[0, 0]\n    assert abs(target_prob) > abs(non_target_prob)\n"], "sample_1037": ["def test_MatMul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = Matrix([[1, 2], [3, 4]])\n    C = Matrix([[5, 6], [7, 8]])\n\n    X = MatMul(A, B, C)\n    assert X.doit() == MatMul(A, B*C)\n\n    X = MatMul(B, A, C)\n    assert X.doit() == MatMul(B*A, C)\n"], "sample_1038": ["def test_matrix_element():\n    i, j = symbols('i j')\n    assert MatrixElement(A, i, j).parent == A\n    assert MatrixElement(A, 0, 0).doit() == A[0, 0]\n    assert MatrixElement(Identity(3), 1, 1).doit() == 1\n    assert MatrixElement(ZeroMatrix(3, 3), 1, 1).doit() == 0\n    raises(IndexError, lambda: MatrixElement(A, 1, n + 1))\n"], "sample_1041": ["def test_matrix_element():\n    i, j = symbols('i j')\n    assert MatrixElement(A, i, j).parent == A\n    assert MatrixElement(A, 0, 0).doit() == A[0, 0]\n    assert MatrixElement(Identity(n), i, j).doit() == KroneckerDelta(i, j)\n    assert MatrixElement(ZeroMatrix(n, n), i, j).doit() == 0\n"], "sample_1042": ["def test_Idx_properties():\n    i, j = symbols('i j', integer=True)\n    idx1 = Idx(i)\n    idx2 = Idx(j, 5)\n    idx3 = Idx(j, (1, 5))\n    \n    assert idx1.label == i\n    assert idx1.lower is None\n    assert idx1.upper is None\n    \n    assert idx2.label == j\n    assert idx2.lower == 0\n    assert idx2.upper == 4\n    \n    assert idx3.label == j\n    assert idx3.lower == 1\n    assert idx3.upper == 5\n    \n    assert idx1.name == 'i'\n    assert idx2.name == 'j'\n    assert idx3.name == 'j'\n    \n    assert idx1.free_symbols == {i}\n    assert idx2.free_symbols == {j}\n    assert idx3.free_symbols == {j}\n"], "sample_1043": ["def test_mcode_Pow():\n    assert mcode(x**2) == 'x^2'\n    assert mcode(x**(1/2)) == 'x^(1/2)'\n    assert mcode(x**-1) == 'x^(-1)'\n    assert mcode(1/x) == '1/x'\n    assert mcode(1/(x*y)) == '1/(x*y)'\n"], "sample_1044": ["def test_Pow_is_integer():\n    x = Symbol('x')\n    assert Pow(2, 3, evaluate=False).is_integer\n    assert Pow(x, 0, evaluate=False).is_integer\n    assert not Pow(2, x, evaluate=False).is_integer\n    assert not Pow(x, 3, evaluate=False).is_integer\n    assert Pow(x, 3, evaluate=False).is_integer is None\n    assert Pow(Rational(1, 2), -2, evaluate=False).is_integer\n    assert Pow(Rational(1, 2), -1, evaluate=False).is_integer is False\n    assert Pow(Rational(3, 4), 2, evaluate=False).is_integer is False\n    assert Pow(Rational(3, 4), -2, evaluate=False).is_integer is False\n    assert Pow(Rational(3, 4), -1, evaluate=False).is_integer is False\n    assert Pow(Rational(5, 6), 3, evaluate=False).is_integer is False\n    assert Pow(Rational(5, 6), 4, evaluate=False).is_integer is False\n    assert Pow(Rational(5, 6), -3, evaluate=False).is_integer is False\n    assert Pow(Rational(5, 6), -4, evaluate=False).is_integer is False\n"], "sample_1047": ["def test_assumptions_is_nonzero():\n    x = Symbol('x', nonzero=True)\n    assert x.is_nonzero is True\n    assert x.is_zero is False\n\n    y = Symbol('y', zero=True)\n    assert y.is_nonzero is False\n    assert y.is_zero is True\n\n    z = Symbol('z')\n    assert z.is_nonzero is None\n    assert z.is_zero is None\n"], "sample_1048": ["def test_parabola_equation():\n    x, y = symbols('x y', real=True)\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    assert p1.equation() == -x**2 - 16*y + 64\n    assert p1.equation('f') == -symbols('f', real=True)**2 - 16*y + 64\n    assert p1.equation(y='z') == -x**2 - 16*symbols('z', real=True) + 64\n"], "sample_1051": ["def test_dotprint_options():\n    # Test the options of dotprint function\n    expr = x + 2\n    styles = [(Basic, {'color': 'red', 'shape': 'box'})]\n    graphstyle = {'rankdir': 'LR'}\n    assert 'color=\"red\"' in dotprint(expr, styles=styles)\n    assert 'rankdir=\"LR\"' in dotprint(expr, **graphstyle)\n    assert 'Symbol(\\'x\\')' in dotprint(expr, labelfunc=srepr)\n    assert 'Integer(2)_(0,)' not in dotprint(expr, repeat=False)\n    assert 'Integer(2)_(0,)' in dotprint(expr, repeat=True)\n    assert len(dotprint(expr, maxdepth=1).split('\\n')) < len(dotprint(expr).split('\\n'))\n"], "sample_1052": ["def test_make_routine_multiple_results():\n    x, y = symbols('x y')\n    expr1 = x + y\n    expr2 = x - y\n    routine = make_routine('test', [expr1, expr2])\n    assert len(routine.results) == 2\n    assert routine.results[0].result_var != routine.results[1].result_var\n"], "sample_1054": ["def test_ComplexRegion_contains():\n    a = Interval(2, 3)\n    b = Interval(4, 5)\n    c = Interval(1, 7)\n    C1 = ComplexRegion(a*b)\n    assert (2.5 + 4.5*I) in C1\n    assert (2.5 + 6.5*I) not in C1\n\n    C2 = ComplexRegion(Union(a*b, b*c))\n    assert (2.5 + 4.5*I) in C2\n    assert (2.5 + 6.5*I) in C2\n\n    r = Interval(0, 1)\n    theta = Interval(0, 2*pi)\n    C3 = ComplexRegion(r*theta, polar=True)\n    assert 0.5 + 0.5*I in C3\n    assert 1 + 2*I not in C3\n"], "sample_1056": ["def test_LambdaPrinter_boolean():\n    assert lambdarepr(x & y) == \"(x and y)\"\n    assert lambdarepr(x | y) == \"(x or y)\"\n    assert lambdarepr(~x) == \"(not (x))\"\n    assert lambdarepr(True) == \"True\"\n    assert lambdarepr(False) == \"False\"\n"], "sample_1057": ["def test_render_as_module_with_fully_qualified_modules():\n    content = Print(\"Hello, World!\")\n    result = render_as_module(content, standard='python3')\n    assert \"import sympy\" in result\n    assert \"sympy.printing.pycode\" not in result\n\n    printer = PythonCodePrinter({'standard':'python3', 'fully_qualified_modules': True})\n    pystr = printer.doprint(content)\n    module_imports_str = '\\n'.join('import %s' % k for k in printer.module_imports)\n    fully_qualified_result = module_imports_str + '\\n\\n' + pystr\n    assert fully_qualified_result == result\n"], "sample_1060": ["def test_PythonCodePrinter_print_Identity():\n    M = Identity(3)\n    assert PythonCodePrinter().doprint(M) == \"eye(3)\"\n"], "sample_1063": ["def test_lambdify_with_tensorflow():\n    if not tensorflow:\n        skip(\"TensorFlow not installed\")\n\n    f = lambdify(x, x + 1, 'tensorflow')\n    assert f(1) == 2\n\n    f = lambdify((x, y), x + y, 'tensorflow')\n    assert f(1, 2) == 3\n\n    f = lambdify(x, sin(x), 'tensorflow')\n    assert f(0) == 0\n\n    f = lambdify(x, exp(x), 'tensorflow')\n    assert f(0) == 1\n"], "sample_1065": ["def test_factorial2():\n    assert factorial2(0) == 1\n    assert factorial2(5) == 15\n    assert factorial2(-1) == 1\n    assert factorial2(-5) == 1/3\n    x = Symbol('x')\n    assert factorial2(x).rewrite(gamma) == 2**(x/2)*gamma(x/2 + 1) * Piecewise((1, Eq(Mod(x, 2), 0)), (sqrt(2/pi), Eq(Mod(x, 2), 1)))\n    assert factorial2(5).is_integer\n    assert factorial2(-3).is_integer\n    assert factorial2(4).is_even\n    assert factorial2(3).is_odd\n    assert factorial2(0).is_nonnegative\n    assert factorial2(1).is_positive\n"], "sample_1067": ["def test_Mul_coeff():\n    x, y = symbols('x y')\n    assert (2*x).as_coeff_Mul()[0] == 2\n    assert (2*x).as_coeff_Mul()[1] == x\n    assert (2*x*y).as_coeff_Mul()[0] == 2\n    assert (2*x*y).as_coeff_Mul()[1] == x*y\n    assert (x*y).as_coeff_Mul()[0] == 1\n    assert (x*y).as_coeff_Mul()[1] == x*y\n"], "sample_1068": ["def test_octave_code_sinc():\n    assert octave_code(sinc(x)) == \"sinc(x/pi)\"\n    assert octave_code(sinc(pi*x)) == \"sinc(x)\"\n"], "sample_1069": ["def test_glsl_code_Pow():\n    assert glsl_code(Pow(x, 2)) == \"pow(x, 2.0)\"\n    assert glsl_code(Pow(x, -1)) == \"1.0/x\"\n    assert glsl_code(Pow(x, 0.5)) == \"sqrt(x)\"\n    assert glsl_code(Pow(x, -0.5)) == \"1.0/sqrt(x)\"\n    assert glsl_code(Pow(x, Rational(1, 3))) == \"pow(x, 1.0/3.0)\"\n"], "sample_1070": ["def test_exp_polar():\n    x = Symbol('x', polar=True)\n    assert exp_polar(x).is_polar\n    assert exp_polar(0).is_polar\n    assert exp_polar(I*pi).is_polar\n    assert exp_polar(2*I*pi).is_polar\n    assert exp_polar(-I*pi).is_polar\n    assert exp_polar(-2*I*pi).is_polar\n\n    assert exp_polar(x).as_real_imag() == (exp(re(x))*cos(im(x)), exp(re(x))*sin(im(x)))\n    assert exp_polar(0).as_real_imag() == (1, 0)\n    assert exp_polar(I*pi).as_real_imag() == (-1, 0)\n    assert exp_polar(2*I*pi).as_real_imag() == (1, 0)\n    assert exp_polar(-I*pi).as_real_imag() == (-1, 0)\n    assert exp_polar(-2*I*pi).as_real_imag() == (1, 0)\n\n    assert exp_polar(x).conjugate() == exp_polar(conjugate(x))\n    assert exp_polar(0).conjugate() == 1\n    assert exp_polar(I*pi).conjugate() == -1\n    assert exp_polar(2*I*pi).conjugate() == 1\n    assert exp_polar(-I*pi).conjugate() == -1\n    assert exp_polar(-2*I*pi).conjugate() == 1\n"], "sample_1071": ["def test_convert_to_quantity_with_prefix():\n    q1 = 3 * kilogram * meter / second**2\n    q2 = convert_to(q1, [gram, centimeter, second])\n    assert q2 == 300000 * gram * centimeter / second**2\n\n    q1 = 2 * mile\n    q2 = convert_to(q1, kilometer)\n    assert NS(q2) == NS(3218.69 * kilometer)\n\n    q1 = newton\n    q2 = convert_to(q1, [kilogram, meter, second])\n    assert q2 == kilogram * meter / second**2\n"], "sample_1072": ["def test_floor_ceiling_frac():\n    assert floor(3.7) == 3\n    assert ceiling(3.7) == 4\n    assert frac(3.7) == 0.7\n    assert floor(-3.7) == -4\n    assert ceiling(-3.7) == -3\n    assert frac(-3.7) == 0.3\n    assert floor(I/2) == 0\n    assert ceiling(I/2) == I\n    assert frac(I/2) == I/2\n    assert floor(-I/2) == -I\n    assert ceiling(-I/2) == 0\n    assert frac(-I/2) == -I/2\n    assert floor(2*I) == 2*I\n    assert ceiling(2*I) == 2*I\n    assert frac(2*I) == 0\n    assert floor(-2*I) == -2*I\n    assert ceiling(-2*I) == -2*I\n    assert frac(-2*I) == 0\n"], "sample_1073": ["def test_sqrt_match():\n    assert _sqrt_match(1 + r2 + 2*r3) == [1 + r2, 2, 3]\n    assert _sqrt_match(r2*(1 + r3)) == [0, r2, 3]\n    assert _sqrt_match(sqrt(2 + r3)) == []\n    assert _sqrt_match(1 + sqrt(2 + r3)) == [1, 1, 2 + r3]\n    assert _sqrt_match(sqrt(2 + r3) + sqrt(3 + r5)) == [0, 1, 2 + r3]\n"], "sample_1075": ["def test_beta_eval_expand_func():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert beta(x, y)._eval_expand_func() == gamma(x)*gamma(y) / gamma(x + y)\n"], "sample_1076": ["def test_PythonCodePrinter_print_Identity():\n    A = Identity(3)\n    assert PythonCodePrinter().doprint(A) == 'eye(3)'\n"], "sample_1078": ["def test_Idx_properties():\n    i = Idx('i', 3)\n    assert i.lower == 0\n    assert i.upper == 2\n    assert i.label == Symbol('i')\n    assert i.name == 'i'\n    assert i.free_symbols == {i}\n    assert i.is_integer\n    assert i.is_finite\n    assert i.is_real\n\n    j = Idx('j')\n    assert j.lower is None\n    assert j.upper is None\n    assert j.label == Symbol('j')\n    assert j.name == 'j'\n    assert j.free_symbols == {j}\n    assert j.is_integer\n    assert j.is_finite\n    assert j.is_real\n"], "sample_1080": ["def test_refine_Pow():\n    assert refine((-1)**x, Q.real(x)) == (-1)**x\n    assert refine((-1)**x, Q.even(x)) == 1\n    assert refine((-1)**x, Q.odd(x)) == -1\n    assert refine((-1)**(x+y), Q.even(x)) == (-1)**y\n    assert refine((-1)**(x+y+z), Q.odd(x) & Q.odd(z)) == (-1)**y\n    assert refine((-1)**(x+y+2), Q.odd(x)) == (-1)**(y + 1)\n    assert refine((-1)**(x+3), True) == (-1)**(x + 1)\n"], "sample_1084": ["def test_intersection_sets():\n    # Test intersection of finite sets\n    assert intersection_sets(FiniteSet(1, 2, 3), FiniteSet(2, 3, 4)) == FiniteSet(2, 3)\n    assert intersection_sets(FiniteSet(1, 2, 3), FiniteSet(4, 5, 6)) == EmptySet()\n\n    # Test intersection of intervals\n    assert intersection_sets(Interval(1, 3), Interval(2, 4)) == Interval(2, 3)\n    assert intersection_sets(Interval(1, 3), Interval(4, 5)) == EmptySet()\n\n    # Test intersection of ranges\n    assert intersection_sets(Range(1, 5), Range(3, 7)) == Range(3, 5)\n    assert intersection_sets(Range(1, 5), Range(6, 8)) == EmptySet()\n\n    # Test intersection of image sets\n    x = Symbol('x')\n    f = Lambda(x, x**2)\n    g = Lambda(x, x + 1)\n    assert intersection_sets(imageset(f, Range(-2, 3)), imageset(g, Range(-1, 4))) == FiniteSet(0, 1, 4)\n\n    # Test intersection of complex regions\n    a = Interval(1, 3)\n    b = Interval(2, 4)\n    theta = Interval(0, pi)\n    assert intersection_sets(ComplexRegion(a*b, theta), ComplexRegion(b*a, theta)) == ComplexRegion((a&b)*b, theta)\n"], "sample_1087": ["def test_swinnerton_dyer_poly():\n    raises(ValueError, lambda: swinnerton_dyer_poly(0))\n    assert swinnerton_dyer_poly(1) == x**2 - 2\n    assert swinnerton_dyer_poly(2) == x**4 - 10*x**2 + 1\n    assert swinnerton_dyer_poly(3) == x**8 - 40*x**6 + 352*x**4 - 960*x**2 + 576\n    assert swinnerton_dyer_poly(4, polys=True).is_Poly\n"], "sample_1088": ["def test_symmetrize():\n    raises(MultivariatePolynomialError, lambda: symmetrize(x**2 + y**2, x, y, formal=True))\n    assert symmetrize(x**2 + y**2) == (-2*x*y + (x + y)**2, 0)\n    assert symmetrize(x**2 + y**2, formal=True) == (S(1)**2*(x + y)**2 - 2*S(1)*x*y, 0, [(S(1), x + y), (S(1)*x*y, x*y)])\n"], "sample_1089": ["def test_factor_nc():\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    C = Symbol('C', commutative=False)\n\n    assert factor_nc((x**2 + 2*A*x + A**2).expand()) == (x + A)**2\n    assert factor_nc(((x + A)*(x + B)).expand()) == (x + A)*(x + B)\n    assert factor_nc(((x + A)*(x + B)*(x + C)).expand()) == (x + A)*(x + B)*(x + C)\n    assert factor_nc(((x + A)*y + (x + B)*y).expand()) == y*((x + A) + (x + B))\n"], "sample_1092": ["compilation error"], "sample_1094": ["def test_doit():\n    f = Basic()\n    assert f.doit() == f\n    assert f.doit(deep=False) == f\n    assert f.doit(deep=True) == f\n\n    class MyBasic(Basic):\n            return Basic()\n\n    f = MyBasic()\n    assert isinstance(f.doit(), Basic)\n    assert isinstance(f.doit(deep=False), Basic)\n    assert isinstance(f.doit(deep=True), Basic)\n"], "sample_1096": ["def test_indexedbase_shape():\n    A = IndexedBase('A', shape=(3, 4))\n    assert A.shape == (3, 4)\n    i, j = symbols('i j', integer=True)\n    assert A[i, j].shape == (3, 4)\n\n    B = IndexedBase('B')\n    assert B.shape is None\n    assert B[i, j].shape == (None, None)\n\n    C = IndexedBase('C', shape=(oo, 2))\n    assert C.shape == (oo, 2)\n    assert C[i, j].shape == (oo, 2)\n"], "sample_1099": ["def test_partial_derivative_doit():\n    expr = PartialDerivative(A(i), A(j))\n    assert expr.doit() == PartialDerivative(A(i), A(j))\n\n    expr = PartialDerivative(A(i) + B(i), A(j))\n    assert expr.doit() == PartialDerivative(A(i), A(j)) + PartialDerivative(B(i), A(j))\n\n    expr = PartialDerivative(A(i)*B(i), A(j))\n    assert expr.doit() == PartialDerivative(A(i), A(j))*B(i) + A(i)*PartialDerivative(B(i), A(j))\n"], "sample_1101": ["def test_schur_number_eval():\n    # Test eval method of SchurNumber class\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(S.Infinity) == S.Infinity\n    assert SchurNumber(0) == 0\n\n    # Test ValueError for non-integer and negative values\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(symbols('x')))\n"], "sample_1106": ["def test_MatAdd_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = eye(2)\n    C = Matrix([[1, 2], [3, 4]])\n    X = MatAdd(A, B, C)\n    assert X.doit() == MatAdd(A, Matrix([[2, 2], [3, 5]]))\n    assert X.doit(deep=False) == MatAdd(A, B, C)\n"], "sample_1109": ["def test_floor_ceiling_frac_relational():\n    x = Symbol('x', real=True)\n    assert floor(x) <= x\n    assert ceiling(x) >= x\n    assert 0 <= frac(x) < 1\n\n    assert floor(3.5) == 3\n    assert ceiling(3.5) == 4\n    assert frac(3.5) == 0.5\n\n    assert floor(-3.5) == -4\n    assert ceiling(-3.5) == -3\n    assert frac(-3.5) == 0.5\n\n    assert Eq(floor(x), x) == Eq(x, floor(x))\n    assert Eq(ceiling(x), x) == Eq(x, ceiling(x))\n    assert Eq(frac(x), x) == Eq(x, frac(x))\n\n    assert Le(floor(x), x)\n    assert Ge(ceiling(x), x)\n    assert Lt(frac(x), 1)\n    assert Gt(frac(x), 0)\n\n    assert Ne(floor(x), x) == Ne(x, floor(x))\n    assert Ne(ceiling(x), x) == Ne(x, ceiling(x))\n    assert Ne(frac(x), x) == Ne(x, frac(x))\n"], "sample_1111": ["def test_textplot_str_linear():\n    x = Symbol('x')\n    expr = 2*x + 3\n    a, b = 0, 5\n    W, H = 55, 21\n    lines = list(textplot_str(expr, a, b, W, H))\n    assert len(lines) == H + 1\n    for line in lines[:-1]:\n        assert len(line) == W + 9  # 7 spaces for margin + 2 for ' |'\n    assert len(lines[-1]) > W  # last line has x values and is longer\n"], "sample_1112": ["def test_digits():\n    # Test digits function with different bases and number of digits\n    assert digits(35) == [10, 3, 5]\n    assert digits(-35) == [-10, 3, 5]\n    assert digits(27, b=2) == [2, 1, 1, 0, 1, 1]\n    assert digits(35, digits=4) == [10, 0, 0, 3, 5]\n\n    # Test edge cases\n    raises(ValueError, lambda: digits(35, b=1))\n    raises(ValueError, lambda: digits(35, digits=2))\n"], "sample_1113": ["def test_block_collapse():\n    B = BlockMatrix([[X, Z], [ZeroMatrix(m, l), Y]])\n    C = BlockMatrix([[Identity(l), Z]])\n    assert block_collapse(C*B) == BlockMatrix([[X, Z + Z*Y]])\n\n    B = BlockMatrix([[X1, X2], [X3, X4]])\n    assert block_collapse(B*B) == BlockMatrix([\n        [X1*X1 + X2*X3, X1*X2 + X2*X4],\n        [X3*X1 + X4*X3, X3*X2 + X4*X4]\n    ])\n\n    B = BlockMatrix([[X1, ZeroMatrix(m, m)], [ZeroMatrix(m, m), X2]])\n    assert block_collapse(B*B) == BlockMatrix([\n        [X1*X1, ZeroMatrix(m, m)],\n        [ZeroMatrix(m, m), X2*X2]\n    ])\n"], "sample_1114": ["def test_ComplexRegion_contains():\n    a, b = Interval(1, 3), Interval(4, 6)\n    c = Interval(6, 7)\n    d = Interval(8, 9)\n    r1 = ComplexRegion(a*b)\n    assert 2 + 5*I in r1\n    assert 3 + 5*I not in r1\n    assert 2 + 6*I not in r1\n    r2 = ComplexRegion(Union(a*c, b*d))\n    assert 2 + 6.5*I in r2\n    assert 3 + 6.5*I not in r2\n    assert 2 + 8.5*I not in r2\n    assert 3 + 8.5*I in r2\n"], "sample_1116": ["def test_Inverse_properties():\n    assert Inverse(C).arg == C\n    assert Inverse(C).shape == (n, n)\n    raises(NonSquareMatrixError, lambda: Inverse(A))\n    raises(TypeError, lambda: Inverse(n))\n"], "sample_1118": ["def test_MatPow_doit():\n    # Test that doit() method works correctly for different cases\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(Identity(n), 2).doit() == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n\n    # Test that doit() raises an error for non-invertible matrix\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n\n    # Test that doit() raises an error for non-square matrix\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n"], "sample_1122": ["def test_polar_lift():\n    x = Symbol('x')\n    p = Symbol('p', polar=True)\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n"], "sample_1123": ["def test_ConditionSet_subs():\n    c = ConditionSet(x, x < 1, Interval(0, 2))\n    assert c.subs(x, y) == ConditionSet(y, y < 1, Interval(0, 2))\n    assert c.subs(x, 1) == ConditionSet(x, x < 1, Interval(0, 2))\n    raises(ValueError, lambda: c.subs(x, x + 1))\n    c = ConditionSet(x, x < 1, FiniteSet(x, z))\n    assert c.subs(x, y) == ConditionSet(x, x < 1, FiniteSet(y, z))\n    assert c.subs(x, y).subs(x, y) == ConditionSet(y, y < 1, FiniteSet(y, z))\n"], "sample_1124": ["def test_FracField_to_domain():\n    F, x, y = field(\"x,y\", ZZ)\n    assert F.to_domain() == F\n"], "sample_1125": ["def test_operator_inv():\n    A = Operator('A')\n    assert A.inv() == A**(-1)\n    assert A.inv().inv() == A\n    assert (A**2).inv() == A**(-2)\n    assert (A**-2).inv() == A**2\n"], "sample_1126": ["def test_dagger_operation():\n    # Test Dagger operation on complex numbers and matrices\n    assert Dagger(I) == -I\n    assert Dagger(Matrix([[1, I], [2, I]])) == Matrix([[1, 2], [-I, -I]])\n\n    # Test Dagger operation on operators\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n\n    # Test Dagger operation with IdentityOperator\n    assert Dagger(A)*IdentityOperator() == Dagger(A)\n\n    # Test Dagger operation on symbols\n    x = symbols('x', real=True)\n    y = symbols('y', real=False)\n    assert Dagger(x) == x\n    assert Dagger(y) == conjugate(y)\n"], "sample_1128": ["def test_point_vel():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    p3 = Point('p3')\n    q = dynamicsymbols('q')\n    p1.set_vel(N, 10 * N.x)\n    p2.set_pos(p1, q*N.x)\n    p3.set_pos(p2, 2*q*N.x)\n    assert p2.vel(N) == (q.diff() + 10)*N.x\n    assert p3.vel(N) == (3*q.diff() + 10)*N.x\n"], "sample_1137": ["def test_convert_to_multiple_units():\n    # Test conversion to multiple units\n    assert convert_to(speed_of_light, [meter, second]) == 299792458*meter/second\n    assert convert_to(3*newton, [centimeter, gram, second]) == 300000*centimeter*gram/second**2\n\n    # Test conversion to Planck units\n    planck_units = [gravitational_constant, speed_of_light, hbar]\n    assert convert_to(atomic_mass_constant, planck_units).n() == 7.62963085040767e-20*gravitational_constant**(-0.5)*hbar**0.5*speed_of_light**0.5\n"], "sample_1139": ["def test_ComplexRegion_contains():\n    a, b = Interval(1, 3), Interval(2, 4)\n    c = ComplexRegion(a*b)\n    assert 2 + 3*I in c\n    assert 1 + 2*I in c\n    assert 10 + 10*I not in c\n\n    d = Interval(0, 2*S.Pi)\n    e = ComplexRegion(Interval(1, 2)*d, polar=True)\n    assert 1.5 + I in e\n    assert -1.5 + I in e\n    assert 1.5 - I in e\n    assert -1.5 - I in e\n    assert 1.5 + 2*I not in e\n"], "sample_1144": ["def test_split_super_sub():\n    assert split_super_sub('') == ('', [], [])\n    assert split_super_sub('a') == ('a', [], [])\n    assert split_super_sub('a1') == ('a', [], ['1'])\n    assert split_super_sub('a_x') == ('a', [], ['x'])\n    assert split_super_sub('a^1') == ('a', ['1'], [])\n    assert split_super_sub('a__1') == ('a', ['1'], [])\n    assert split_super_sub('a_x^1') == ('a', ['1'], ['x'])\n    assert split_super_sub('var_sub1__sup_sub2') == ('var', ['sup'], ['sub1', 'sub2'])\n"], "sample_1145": ["def test_refine_abs():\n    assert refine(Abs(x), Q.real(x)) == Abs(x)\n    assert refine(Abs(x), Q.positive(x)) == x\n    assert refine(Abs(x), Q.negative(x)) == -x\n    assert refine(Abs(x*y), Q.positive(x) & Q.real(y)) == x*Abs(y)\n    assert refine(Abs(x*y), Q.negative(x) & Q.real(y)) == -x*Abs(y)\n"], "sample_1149": ["def test_singleton_registry():\n    # Test that S is a singleton registry\n    assert isinstance(S, SingletonRegistry)\n    assert S.__call__ == sympify\n\n    # Test that S can be used to access singleton instances\n    assert S.Zero is Integer(0)\n    assert S.Half is Rational(1, 2)\n\n    # Test that S can be used to create new instances\n    assert S(1) is Integer(1)\n    assert S(1)/2 is Rational(1, 2)\n\n    # Test that S raises an AttributeError for unknown attributes\n    with raises(AttributeError):\n        S.UnknownAttribute\n"], "sample_1150": ["def test_ComplexRegion_contains():\n    a, b = Interval(1, 3), Interval(4, 6)\n    c1 = ComplexRegion(a*b)\n    assert 2 + 5*I in c1\n    assert 5*I not in c1\n\n    rset = Interval(0, oo)\n    thetaset = Interval(0, pi)\n    upper_half_plane = ComplexRegion(rset * thetaset, polar=True)\n    assert 1 + I in upper_half_plane\n    assert 1 - I not in upper_half_plane\n"], "sample_1151": ["def test_Mod_eval():\n    # Test Mod.eval with various inputs\n    assert Mod(5, 3).eval() == 2\n    assert Mod(-5, 3).eval() == 1\n    assert Mod(5, -3).eval() == -1\n    assert Mod(-5, -3).eval() == -2\n    assert Mod(x, x).eval() == 0\n    assert Mod(x, -x).eval() == 0\n    assert Mod(x + y, x).eval() == Mod(y, x)\n    assert Mod(x*y, x).eval() == 0\n    assert Mod(x**2, x).eval() == 0\n    assert Mod(x, 1).eval() == 0\n    assert Mod(x, -1).eval() == 0\n    assert Mod(x, 0).eval() is None  # cannot divide by zero\n\n    # Test Mod.eval with nested Mod\n    assert Mod(Mod(x, y), y).eval() == Mod(x, y)\n    assert Mod(Mod(x, y), z).eval() == Mod(Mod(x, y), z)\n\n    # Test Mod.eval with Add and Mul\n    assert Mod(Add(x, y), z).eval() == Mod(x + y, z)\n    assert Mod(Mul(x, y), z).eval() == Mod(x*y, z)\n"], "sample_1152": ["def test_powsimp_do_not_combine():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert powsimp(x**2*x**y, combine='all') == x**(2 + y)\n    assert powsimp(x**2*x**y, combine='exp') == x**(2 + y)\n    assert powsimp(x**2*x**y, combine='base') == x**2*x**y\n"], "sample_1154": ["def test_linsolve_nonlinear_error():\n    # Test that PolyNonlinearError is raised for nonlinear equations\n    raises(PolyNonlinearError, lambda: _linsolve([Eq(x**2 + y, 1)], [x, y]))\n"], "sample_1155": ["def test_construct_domain_extension():\n    # Test with extension=True\n    K, elements = construct_domain([sqrt(2)], extension=True)\n    assert isinstance(K, QQ.algebraic_field((sqrt(2), sqrt(2))))\n    assert len(elements) == 1\n\n    # Test with multiple extensions\n    K, elements = construct_domain([sqrt(2), sqrt(3)], extension=True)\n    assert isinstance(K, QQ.algebraic_field((sqrt(2) + sqrt(3), (sqrt(2) + sqrt(3))**2 - 5)))\n    assert len(elements) == 2\n\n    # Test with algebraic numbers that are not roots of integers\n    K, elements = construct_domain([GoldenRatio], extension=True)\n    assert isinstance(K, QQ.algebraic_field((GoldenRatio, GoldenRatio**2 - GoldenRatio - 1)))\n    assert len(elements) == 1\n"], "sample_1157": ["def test_factorial_notation():\n    assert parse_expr('3!', transformations=standard_transformations) == factorial(3)\n    assert parse_expr('3!!', transformations=standard_transformations) == factorial2(3)\n    raises(TokenError, lambda: parse_expr('3!!!', transformations=standard_transformations))\n"], "sample_1158": ["def test_sympify_numpy_array():\n    if not numpy:\n        skip(\"NumPy is not installed.\")\n\n    # Test sympification of numpy arrays\n    array_int = numpy.array([1, 2, 3])\n    assert sympify(array_int) == Tuple(1, 2, 3)\n\n    array_float = numpy.array([1.0, 2.0, 3.0])\n    assert sympify(array_float) == Tuple(1.0, 2.0, 3.0)\n\n    array_complex = numpy.array([1 + 1j, 2 + 2j, 3 + 3j])\n    assert sympify(array_complex) == Tuple(1 + I, 2 + 2*I, 3 + 3*I)\n\n    array_2d = numpy.array([[1, 2], [3, 4]])\n    assert sympify(array_2d) == Matrix([[1, 2], [3, 4]])\n\n    array_scalar = numpy.array(1)\n    assert sympify(array_scalar) == 1\n\n    array_scalar_float = numpy.array(1.0)\n    assert sympify(array_scalar_float) == 1.0\n"], "sample_1159": ["def test_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y')\n    assert assumptions(x) == {\n        'algebraic': True,\n        'commutative': True,\n        'complex': True,\n        'extended_negative': False,\n        'extended_nonnegative': True,\n        'extended_nonpositive': False,\n        'extended_nonzero': True,\n        'extended_positive': True,\n        'extended_real': True,\n        'finite': True,\n        'hermitian': True,\n        'imaginary': False,\n        'infinite': False,\n        'integer': False,\n        'irrational': False,\n        'negative': False,\n        'noninteger': True,\n        'nonnegative': True,\n        'nonpositive': False,\n        'nonzero': True,\n        'odd': False,\n        'positive': True,\n        'prime': False,\n        'rational': False,\n        'real': True,\n        'transcendental': False,\n        'zero': False\n    }\n    assert assumptions(y) == {\n        'algebraic': None,\n        'commutative': True,\n        'complex': None,\n        'composite': None,\n        'even': None,\n        'extended_negative': None,\n        'extended_nonnegative': None,\n        'extended_nonpositive': None,\n        'extended_nonzero': None,\n        'extended_positive': None,\n        'extended_real': None,\n        'finite': None,\n        'hermitian': None,\n        'imaginary': None,\n        'infinite': None,\n        'integer': None,\n        'irrational': None,\n        'negative': None,\n        'noninteger': None,\n        'nonnegative': None,\n        'nonpositive': None,\n        'nonzero': None,\n        'odd': None,\n        'positive': None,\n        'prime': None,\n        'rational': None,\n        'real': None,\n        'transcendental': None,\n        'zero': None\n    }\n"], "sample_1162": ["def test_derivative_subs():\n    x, y = Symbol('x'), Symbol('y')\n    f = Function('f')\n    assert Derivative(f(x), x).subs(f(x), y) == Derivative(y, x)\n    assert Derivative(f(x), x).subs(x, y) == Derivative(f(y), y)\n    assert Derivative(f(x), x).subs(f(x), y).subs(x, y) == Derivative(y, y)\n"], "sample_1166": ["def test_Monomial_as_expr():\n    x, y = symbols('x y')\n    m = Monomial((2, 3), (x, y))\n    assert m.as_expr() == x**2*y**3\n    raises(ValueError, lambda: Monomial((2, 3)).as_expr())\n"], "sample_1170": ["def test_print_Subs():\n    expr = Subs(x + y, x, 2)\n    assert sstr(expr) == \"Subs(x + y, x, 2)\"\n"], "sample_1171": ["def test_ComplexRegion_contains():\n    a, b = Interval(1, 3), Interval(2, 4)\n    c1 = ComplexRegion(a*b)\n    assert 2 + 3*I in c1\n    assert 5 + 3*I not in c1\n    assert 2 + 5*I not in c1\n\n    rset = Interval(0, 2)\n    thetaset = Interval(0, pi/2)\n    c2 = ComplexRegion(rset*thetaset, polar=True)\n    assert 1 + I in c2\n    assert -1 + I not in c2\n    assert 1 - I not in c2\n"], "sample_1172": ["def test_solve_biquadratic():\n    # Test for two bivariate quadratic equations\n    f = Poly(x**2 + y**2 - 4, x, y)\n    g = Poly(x*y - 1, x, y)\n    assert solve_biquadratic(f, g, dict=True) == \\\n        [{x: -sqrt(2), y: -sqrt(2)}, {x: sqrt(2), y: sqrt(2)}]\n\n    # Test for exception when not zero-dimensional\n    f = Poly(x**2 + y, x, y)\n    g = Poly(x*y, x, y)\n    raises(SolveFailed, lambda: solve_biquadratic(f, g))\n\n    # Test for exception when not bivariate\n    f = Poly(x**2 + y**2 + z, x, y, z)\n    g = Poly(x*y - 1, x, y)\n    raises(PolificationFailed, lambda: solve_biquadratic(f, g))\n"], "sample_1173": ["def test_factorial_notation():\n    assert parse_expr('2*6!') == 2 * factorial(6)\n    assert parse_expr('2*6!!') == 2 * factorial2(6)\n    raises(TokenError, lambda: parse_expr('2*6!!!'))\n"], "sample_1182": ["def test_printing_Piecewise():\n    expr = Piecewise((x, x < 0), (y, y > 1), (z, True))\n    assert pycode(expr) == \"\"\"\\"], "sample_1183": ["def test_FracField():\n    F, x, y = field(\"x,y\", ZZ)\n    f = F(x**2/y)\n    assert f.numer == x**2\n    assert f.denom == y\n    assert f == F(x**2, y)\n\n    F1, x1, y1 = field(\"x,y\", QQ)\n    assert F != F1\n\n    F2, x2, y2 = field(\"x,y\", ZZ)\n    assert F == F2\n\n    assert f.diff(x) == F(2*x/y)\n    assert f.diff(y) == F(-x**2/y**2)\n\n    assert f.subs(x, 2) == F(4/y)\n    assert f.subs(y, 2) == F(x**2/2)\n\n    assert f.evaluate(x, 2) == F(4/y)\n    assert f.evaluate(y, 2) == F(x**2/2)\n\n    raises(ZeroDivisionError, lambda: F(1, 0))\n"], "sample_1185": ["def test_decompogen_min_max():\n    assert decompogen(Max(x, y), x) == [Max(x, y)]\n    assert decompogen(Max(x**2, y), x) == [Max(x, y), x**2]\n    assert decompogen(Min(x, y), x) == [Min(x, y)]\n    assert decompogen(Min(x**2, y), x) == [Min(x, y), x**2]\n"], "sample_1186": ["def test_array_equality():\n    # Test equality of arrays with same shape and data\n    array1 = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    array2 = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert array1 == array2\n\n    # Test inequality of arrays with same shape but different data\n    array3 = ImmutableDenseNDimArray([1, 2, 3, 5], (2, 2))\n    assert array1 != array3\n\n    # Test inequality of arrays with different shapes\n    array4 = ImmutableDenseNDimArray([1, 2, 3, 4], (1, 4))\n    assert array1 != array4\n\n    # Test equality of sparse arrays\n    sparse_array1 = ImmutableSparseNDimArray({(0, 0): 1, (1, 1): 2}, (2, 2))\n    sparse_array2 = ImmutableSparseNDimArray({(0, 0): 1, (1, 1): 2}, (2, 2))\n    assert sparse_array1 == sparse_array2\n\n    # Test inequality of sparse arrays\n    sparse_array3 = ImmutableSparseNDimArray({(0, 0): 1, (1, 1): 3}, (2, 2))\n    assert sparse_array1 != sparse_array3\n"], "sample_1189": ["def test_lambdify_with_cse():\n    f = lambdify(x, sin(x) + cos(x), cse=True)\n    assert abs(f(1) - (math.sin(1) + math.cos(1))) < 1e-10\n\n    f = lambdify((x, y), sin(x) + cos(y), cse=True)\n    assert abs(f(1, 2) - (math.sin(1) + math.cos(2))) < 1e-10\n\n    f = lambdify(x, sin(x) + sin(x), cse=True)\n    assert abs(f(1) - (2 * math.sin(1))) < 1e-10\n"], "sample_1190": ["def test_unit_system_get_dimensional_expr():\n    # Create a unit system\n    unit_system = UnitSystem((meter, second), name=\"SI\")\n\n    # Test get_dimensional_expr with different types of expressions\n    assert unit_system.get_dimensional_expr(meter) == length.name\n    assert unit_system.get_dimensional_expr(second) == time.name\n    assert unit_system.get_dimensional_expr(meter/second) == length.name / time.name\n    assert unit_system.get_dimensional_expr(meter**2) == length.name**2\n    assert unit_system.get_dimensional_expr(sin(meter)) == sin(length.name)\n    assert unit_system.get_dimensional_expr(diff(meter, second)) == length.name / time.name\n\n    # Test get_dimensional_expr with an Add expression\n    expr = meter + second\n    assert unit_system.get_dimensional_expr(expr) == length.name\n\n    # Test get_dimensional_expr with a Mul expression\n    expr = meter * second\n    assert unit_system.get_dimensional_expr(expr) == length.name * time.name\n\n    # Test get_dimensional_expr with a Pow expression\n    expr = meter ** 2\n    assert unit_system.get_dimensional_expr(expr) == length.name ** 2\n\n    # Test get_dimensional_expr with a Function expression\n    expr = sin(meter)\n    assert unit_system.get_dimensional_expr(expr) == sin(length.name)\n\n    # Test get_dimensional_expr with a Derivative expression\n    expr = diff(meter, second)\n    assert unit_system.get_dimensional_expr(expr) == length.name / time.name\n"], "sample_1191": ["def test_hermite_normal_form_modulo_D():\n    # Test case for _hermite_normal_form_modulo_D function\n    A = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\n                      [ZZ(3), ZZ(9), ZZ(6)],\n                      [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\n    D = 10  # a multiple of the determinant of HNF(A)\n    raises(DMShapeError, lambda: _hermite_normal_form_modulo_D(A[:, :2], D))\n    assert _hermite_normal_form_modulo_D(A, D) == hermite_normal_form(A)\n"], "sample_1192": ["def test_disambiguate():\n    x = Symbol('x')\n    y = Symbol('y')\n    d1 = Dummy('x')\n    d2 = Dummy('x')\n    ix = Symbol('x', integer=True)\n    vx = Symbol('x')\n\n    assert disambiguate(x, d1, d2) == (x, Symbol('x_1'), Symbol('x_2'))\n    assert disambiguate(ix + vx) == (vx + Symbol('x_1', integer=True),)\n    assert disambiguate(ix + vx, x) == (vx + Symbol('x_1', integer=True), x)\n\n    eqs = (x/y, d1/y)\n    free = eqs[0].free_symbols | eqs[1].free_symbols\n    mapping = dict(zip(free, disambiguate(*free)))\n    assert eqs[0].xreplace(mapping) == x/y\n    assert eqs[1].xreplace(mapping) == Symbol('x_1')/y\n\n    # make sure no other symbols get subscripted\n    z = Symbol('z')\n    assert disambiguate(x, d1, d2, z) == (x, Symbol('x_1'), Symbol('x_2'), z)\n"], "sample_1193": ["def test_convex_hull():\n    p1 = Point2D(0, 0)\n    p2 = Point2D(1, 1)\n    p3 = Point2D(2, 0)\n    p4 = Point2D(1, -1)\n\n    assert convex_hull(p1, p2, p3) == Polygon(p1, p2, p3)\n    assert convex_hull(p1, p2, p3, p4) == Polygon(p1, p2, p3, p4)\n\n    # Test with collinear points\n    p5 = Point2D(3, 0)\n    assert convex_hull(p1, p2, p3, p5) == Segment(p1, p5)\n\n    # Test with duplicate points\n    assert convex_hull(p1, p1, p2, p3) == Polygon(p1, p2, p3)\n"], "sample_1194": ["def test_julia_code_Rational():\n    assert julia_code(Rational(3, 7)) == \"3 // 7\"\n    assert julia_code(Rational(18, 9)) == \"2\"\n    assert julia_code(Rational(0, 10)) == \"0\"\n    assert julia_code(Rational(1, 2)) == \"1 // 2\"\n"], "sample_1195": ["def test_kahane_simplify():\n    i0, i1, i2 = tensor_indices('i0:3', LorentzIndex)\n    ta = G(i0)*G(-i0)\n    assert _is_tensor_eq(kahane_simplify(ta), 4*eye(4))\n    tb = G(i0)*G(i1)*G(-i0)\n    assert _is_tensor_eq(kahane_simplify(tb), -2*G(i1))\n    tc = G(i0)*G(i1)\n    assert _is_tensor_eq(kahane_simplify(tc), G(i0)*G(i1))\n"], "sample_1196": ["def test_contains_binary_symbols():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    # Test with finite set\n    s1 = FiniteSet(x, y)\n    assert Contains(z, s1).binary_symbols == {x, y}\n\n    # Test with interval\n    s2 = Interval(0, 1)\n    assert Contains(z, s2).binary_symbols == set()\n\n    # Test with boolean in set\n    s3 = FiniteSet(True, False)\n    assert Contains(z, s3).binary_symbols == set()\n\n    # Test with Eq and Ne in set\n    s4 = FiniteSet(Eq(x, y), Ne(x, y))\n    assert Contains(z, s4).binary_symbols == {x, y}\n"], "sample_1197": ["def test_unit_system_get_dimensional_expr():\n    # Create a unit system\n    unit_system = UnitSystem((meter, second), name=\"SI\")\n\n    # Test get_dimensional_expr with different types of expressions\n    assert unit_system.get_dimensional_expr(meter) == length.name\n    assert unit_system.get_dimensional_expr(second) == time.name\n    assert unit_system.get_dimensional_expr(meter/second) == length.name / time.name\n    assert unit_system.get_dimensional_expr(meter**2) == length.name ** 2\n\n    # Test with a more complex expression\n    expr = (meter/second) * (kilogram * meter / second**2)\n    assert unit_system.get_dimensional_expr(expr) == (length.name / time.name) * (mass.name * length.name / time.name**2)\n\n    # Test with an Add expression\n    expr = meter + second\n    assert unit_system.get_dimensional_expr(expr) == length.name\n\n    # Test with a Function expression\n    expr = sin(meter)\n    assert unit_system.get_dimensional_expr(expr) == sin(length.name)\n\n    # Test with a Derivative expression\n    x = symbols('x')\n    expr = diff(meter*x, x)\n    assert unit_system.get_dimensional_expr(expr) == length.name\n"], "sample_1198": ["def test_mathematica_parser_trigonometric():\n    assert parse_mathematica(\"Sin[x]\") == sin(x)\n    assert parse_mathematica(\"Cos[x]\") == cos(x)\n    assert parse_mathematica(\"Tan[x]\") == sin(x)/cos(x)\n    assert parse_mathematica(\"Cot[x]\") == cos(x)/sin(x)\n    assert parse_mathematica(\"Sec[x]\") == 1/cos(x)\n    assert parse_mathematica(\"Csc[x]\") == 1/sin(x)\n"], "sample_1199": ["def test_tensor_product_simp_Mul():\n    assert tensor_product_simp(TP(A, B)*TP(C, D)) == TP(A*C, B*D)\n    assert tensor_product_simp(TP(A, B)*x*TP(C, D)) == x*TP(A*C, B*D)\n    assert tensor_product_simp(x*TP(A, B)*TP(C, D)) == x*TP(A*C, B*D)\n    assert tensor_product_simp(TP(A, B)*TP(C, D)*x) == x*TP(A*C, B*D)\n"], "sample_1200": ["def test_unit_system_extend():\n    base_units = (meter, kilogram, second)\n    units = (joule, newton)\n    name = \"SI_extended\"\n    description = \"Extended SI system\"\n    dimension_system = SI.get_dimension_system()\n    derived_units = {Dimension(\"length\"): meter}\n\n    extended_si = SI.extend(base_units, units, name, description, dimension_system, derived_units)\n\n    assert extended_si.name == name\n    assert extended_si.descr == description\n    assert extended_si._base_units == base_units + SI._base_units\n    assert extended_si._units == units + SI._units\n    assert extended_si._dimension_system == dimension_system\n    assert extended_si.derived_units == {**SI.derived_units, **derived_units}\n"], "sample_1201": ["def test_cgs_gauss_units():\n    # Test conversions of units specific to cgs_gauss system\n    assert convert_to(statcoulomb, cgs_gauss, coulomb) == 10*speed_of_light\n    assert convert_to(coulomb, cgs_gauss, statcoulomb) == 1/(10*speed_of_light)\n    \n    assert convert_to(statvolt, cgs_gauss, volt) == speed_of_light/10**6\n    assert convert_to(volt, cgs_gauss, statvolt) == 10**6/speed_of_light\n    \n    assert convert_to(erg, cgs_gauss, joule) == 10**-7\n    assert convert_to(joule, cgs_gauss, erg) == 10**7\n    \n    assert convert_to(dyne, cgs_gauss, newton) == 10**-5\n    assert convert_to(newton, cgs_gauss, dyne) == 10**5\n    \n    assert convert_to(centimeter, cgs_gauss, meter) == 100\n    assert convert_to(meter, cgs_gauss, centimeter) == 1/100\n    \n    assert convert_to(gram, cgs_gauss, SI.base_unit('mass')) == 1000\n    assert convert_to(SI.base_unit('mass'), cgs_gauss, gram) == 1/1000\n    \n    assert convert_to(second, cgs_gauss, second) == 1\n    \n    assert convert_to(farad, cgs_gauss, farad) == 1\n    assert convert_to(henry, cgs_gauss, henry) == 1\n    \n    assert convert_to(statvolt, cgs_gauss, statvolt) == 1\n    assert convert_to(volt, cgs_gauss, volt) == 1\n    \n    assert convert_to(ohm, cgs_gauss, ohm) == 1\n"], "sample_1207": ["def test_factorial_notation():\n    x = Symbol('x')\n    assert parse_expr('x!', transformations=standard_transformations) == factorial(x)\n    assert parse_expr('x!!', transformations=standard_transformations) == factorial2(x)\n    raises(TokenError, lambda: parse_expr('x!!!', transformations=standard_transformations))\n"], "sample_1209": ["def test_prefix_operations():\n    assert kilo * 3 == 3000\n    assert kilo / 3 == Rational(1000, 3)\n    assert 3 * kilo == 3000\n    assert 3 / kilo == Rational(3, 1000)\n    assert kilo * kilo == 1000000\n    assert kilo / kilo == 1\n    assert kilo * x == kilo.scale_factor * x\n    assert x * kilo == x * kilo.scale_factor\n    assert kilo / x == kilo.scale_factor / x\n    assert x / kilo == x / kilo.scale_factor\n\n    # Test with binary prefix\n    assert kibi * 3 == 3072\n    assert kibi / 3 == Rational(1024, 3)\n    assert 3 * kibi == 3072\n    assert 3 / kibi == Rational(3, 1024)\n    assert kibi * kibi == 1048576\n    assert kibi / kibi == 1\n    assert kibi * x == kibi.scale_factor * x\n    assert x * kibi == x * kibi.scale_factor\n    assert kibi / x == kibi.scale_factor / x\n    assert x / kibi == x / kibi.scale_factor\n"], "sample_2": ["def test_ccddata_init_with_wcs():\n    # Create a CCDData object with WCS information\n    wcs = WCS(naxis=2)\n    wcs.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    wcs.wcs.crval = [12.5, 34.2]\n    wcs.wcs.crpix = [100, 200]\n    wcs.wcs.cdelt = [1.0, 1.0]\n\n    ccd = CCDData(_random_array.copy(), unit=u.adu, wcs=wcs)\n\n    # Check that the WCS information is stored correctly\n    assert isinstance(ccd.wcs, WCS)\n    assert ccd.wcs.naxis == 2\n    assert ccd.wcs.wcs.ctype == [\"RA---TAN\", \"DEC--TAN\"]\n    assert np.allclose(ccd.wcs.wcs.crval, [12.5, 34.2])\n    assert np.allclose(ccd.wcs.wcs.crpix, [100, 200])\n    assert np.allclose(ccd.wcs.wcs.cdelt, [1.0, 1.0])\n"], "sample_4": ["def test_read_html_table_format_mapping(self, cosmo):\n    \"\"\"Test ``read_html_table`` with format mapping.\"\"\"\n    # Create a table with latex names\n    table = QTable({k: [v] for k, v in _FORMAT_TABLE.items()})\n    for k, v in cosmo.to_format(\"dict\").items():\n        table[_FORMAT_TABLE[k]] = v\n\n    # Write to HTML and read back\n    write_html_table(cosmo, \"test.html\", latex_names=True)\n    got = read_html_table(\"test.html\")\n\n    assert got == cosmo\n"], "sample_11": ["def test_sliced_low_level_wcs_dropped_world_dimensions():\n    # Test that the dropped_world_dimensions property returns the correct information\n    wcs = WCS_SPECTRAL_CUBE\n    slices = (slice(None), 10, slice(None))\n    sliced_wcs = SlicedLowLevelWCS(wcs, slices)\n\n    dropped_info = sliced_wcs.dropped_world_dimensions\n\n    assert_equal(dropped_info[\"value\"], [wcs.wcs.crval[1]])\n    assert_equal(dropped_info[\"world_axis_names\"], [\"Frequency\"])\n    assert_equal(dropped_info[\"world_axis_physical_types\"], [\"em.freq\"])\n    assert_equal(dropped_info[\"world_axis_units\"], [\"Hz\"])\n    assert_equal(dropped_info[\"world_axis_object_components\"], [(\"pos\", 0, \"freq\")])\n    assert_equal(dropped_info[\"world_axis_object_classes\"][\"pos\"], \"spectral\")\n"], "sample_15": ["def test_quantity_isclose():\n    # Test that isclose works with quantities\n    q1 = np.array([1, 2, 3]) * u.m\n    q2 = np.array([1.00001, 2.00001, 3.00001]) * u.m\n    assert np.all(isclose(q1, q2))\n\n    # Test that isclose raises an error for incompatible units\n    q3 = np.array([1, 2, 3]) * u.s\n    with pytest.raises(u.UnitsError):\n        isclose(q1, q3)\n\n    # Test that isclose works with scalar quantities\n    q4 = 1 * u.m\n    q5 = 1.00001 * u.m\n    assert isclose(q4, q5)\n"], "sample_17": ["    def test_atleast_1d(self):\n        self.check(np.atleast_1d)\n"], "sample_18": ["def test_info_preserved_in_arithmetic(self):\n    q1 = self.q + 1 * u.m / u.s\n    assert_info_equal(q1, self.q)\n    q2 = self.q - 1 * u.m / u.s\n    assert_info_equal(q2, self.q)\n    q3 = self.q * 2\n    assert_info_equal(q3, self.q)\n    q4 = self.q / 2\n    assert_info_equal(q4, self.q)\n"], "sample_24": ["    def test_array_equal(self):\n        assert np.array_equal(self.ma, self.ma)\n        assert not np.array_equal(self.ma, self.mb)\n"], "sample_40": ["def test_brightness_temperature():\n    freq = 30 * u.GHz\n    equiv = u.brightness_temperature(freq)\n    assert_quantity_allclose(1*u.Jy/u.sr, (1*u.Jy/u.sr).to(u.K, equivalencies=equiv).to(u.Jy/u.sr, equivalencies=equiv))\n    assert_quantity_allclose(1*u.K, (1*u.K).to(u.Jy/u.sr, equivalencies=equiv).to(u.K, equivalencies=equiv))\n"], "sample_45": ["def test_trunc_functions_with_tzinfo(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, tzinfo=pytz.UTC)\n    end_datetime = datetime(2018, 3, 1, 12, 45, tzinfo=pytz.UTC)\n\n    model = self.create_model(start_datetime, end_datetime)\n\n    tests = [\n        (TruncYear('start_datetime'), start_datetime.replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0)),\n        (TruncQuarter('start_datetime'), start_datetime.replace(month=4, day=1, hour=0, minute=0, second=0, microsecond=0)),\n        (TruncMonth('start_datetime'), start_datetime.replace(day=1, hour=0, minute=0, second=0, microsecond=0)),\n        (TruncWeek('start_datetime'), start_datetime - timedelta(days=start_datetime.weekday())),\n        (TruncDay('start_datetime'), start_datetime.replace(hour=0, minute=0, second=0, microsecond=0)),\n        (TruncDate('start_datetime'), start_datetime.date()),\n        (TruncTime('start_datetime'), start_datetime.time()),\n        (TruncHour('start_datetime'), start_datetime.replace(minute=0, second=0, microsecond=0)),\n        (TruncMinute('start_datetime'), start_datetime.replace(second=0, microsecond=0)),\n        (TruncSecond('start_datetime'), start_datetime.replace(microsecond=0)),\n    ]\n\n    for func, expected in tests:\n        with self.subTest(func=func):\n            self.assertEqual(DTModel.objects.filter(pk=model.pk).annotate(truncated=func).first().truncated, expected)\n"], "sample_46": ["    def setUp(self):\n        self.obj1 = UUIDModel.objects.create(uuid_field='01234567-89ab-cdef-0123-456789abcdef')\n        self.obj2 = UUIDModel.objects.create(uuid_field='fedcba98-7654-3210-fedc-ba9876543210')\n"], "sample_52": ["def test_model_choice_field_iterator(self):\n    field = forms.ModelChoiceField(Category.objects.all())\n    iterator = ModelChoiceIterator(field)\n    self.assertEqual(len(iterator), 3)\n    self.assertEqual(list(iterator), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n"], "sample_64": ["    def test_content_type(self):\n        response = HttpResponse()\n        self.assertEqual(response['Content-Type'], 'text/html; charset=utf-8')\n"], "sample_67": ["def test_modelform_factory_with_widgets(self):\n    # Create a form class using modelform_factory, passing a dictionary of widgets.\n    AuthorForm = modelform_factory(Author, fields=('name', 'title'), widgets={'name': forms.Textarea})\n    form = AuthorForm()\n    self.assertIsInstance(form.fields['name'].widget, forms.Textarea)\n    self.assertIsInstance(form.fields['title'].widget, forms.TextInput)\n"], "sample_73": ["    def test_post_process(self):\n        \"\"\"\n        Test post processing of files.\n        \"\"\"\n        storage.staticfiles_storage.max_post_process_passes = 1\n        self.addCleanup(shutil.rmtree, settings.STATIC_ROOT)\n        call_command('collectstatic', verbosity=0, interactive=False)\n\n        # Check the manifest file was created.\n        manifest_path = os.path.join(settings.STATIC_ROOT, 'staticfiles.json')\n        self.assertTrue(os.path.exists(manifest_path))\n\n        # Check the hashed file was created.\n        source_path = os.path.join(TEST_ROOT, 'project', 'css', 'styles.css')\n        hashed_path = hashed_file_path(self, 'css/styles.css')\n        self.assertTrue(os.path.exists(os.path.join(settings.STATIC_ROOT, hashed_path)))\n\n        # Check the contents of the manifest file.\n        with open(manifest_path, 'r') as f:\n            manifest = json.load(f)\n        self.assertIn(hashed_path, manifest['paths'].values())\n\n        # Check the contents of the hashed file.\n        with open(os.path.join(settings.STATIC_ROOT, hashed_path), 'r') as f:\n            hashed_contents = f.read()\n        with open(source_path, 'r') as f:\n            source_contents = f.read()\n        self.assertEqual(hashed_contents, source_contents)\n"], "sample_75": ["def test_prefetch_related_with_to_attr(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        authors = Author.objects.prefetch_related('books').all()\n        for author in authors:\n            list(author.books.all())\n\n    self.assertEqual(len(captured_queries), 2)\n\n    # Check that the lookups are correct.\n    query_sql = captured_queries[1]['sql']\n    self.assertWhereContains(query_sql, 'author_books.author_id IN')\n"], "sample_88": ["def test_sanitize_address(self):\n    # Test that sanitize_address handles email addresses with non-ASCII characters.\n    addr = ('Test User', 'test@\u00f6\u00e4\u00fc.com')\n    sanitized_addr = sanitize_address(addr, 'utf-8')\n    self.assertEqual(sanitized_addr, '=?utf-8?q?Test_User?= <test@xn--4ca9at.com>')\n\n    # Test that sanitize_address handles email addresses with ASCII characters only.\n    addr = ('Test User', 'test@example.com')\n    sanitized_addr = sanitize_address(addr, 'utf-8')\n    self.assertEqual(sanitized_addr, 'Test User <test@example.com>')\n\n    # Test that sanitize_address raises a ValueError for invalid email addresses.\n    addr = ('Test User', 'invalid_email')\n    with self.assertRaises(ValueError):\n        sanitize_address(addr, 'utf-8')\n"], "sample_90": ["def test_modelform_defines_fields(self):\n    self.assertTrue(modelform_defines_fields(ProductForm))\n    self.assertFalse(modelform_defines_fields(forms.ModelForm))\n"], "sample_93": ["def test_exists_subquery(self):\n    subquery = Book.objects.filter(author=self.a1)\n    qs = Author.objects.filter(Exists(subquery))\n    self.assertQuerysetEqual(qs, [self.a1], ordered=False)\n\n    # Check that the subquery is properly nested.\n    subquery_sql = str(subquery.query).replace('SELECT ', 'SELECT EXISTS (SELECT ')\n    self.assertSQLRegex(qs, rf'SELECT .* WHERE {re.escape(subquery_sql)}')\n"], "sample_95": ["def test_patch_cache_control(self):\n    response = HttpResponse()\n    patch_cache_control(response, max_age=3600, public=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=3600, public')\n\n    response = HttpResponse()\n    response['Cache-Control'] = 'max-age=300'\n    patch_cache_control(response, max_age=3600)\n    self.assertEqual(response['Cache-Control'], 'max-age=300')\n\n    response = HttpResponse()\n    patch_cache_control(response, private=True)\n    self.assertEqual(response['Cache-Control'], 'private')\n\n    response = HttpResponse()\n    patch_cache_control(response, no_cache=True, no_store=True, must_revalidate=True)\n    self.assertEqual(response['Cache-Control'], 'no-cache, no-store, must-revalidate')\n"], "sample_96": ["    def test_raw_id_fields_valid(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ('id',)\n\n        self.assertIsValid(RawIdModelAdmin, Band)\n"], "sample_99": ["def test_trunc_timezone_aware(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, tzinfo=pytz.UTC)\n    model = self.create_model(start_datetime, None)\n\n    # Test truncation with a timezone-aware datetime.\n    tzinfo = pytz.timezone('US/Eastern')\n    for lookup in ('year', 'quarter', 'month', 'week', 'day', 'hour', 'minute', 'second'):\n        with self.subTest(lookup):\n            truncated_value = DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', lookup, tzinfo=tzinfo),\n            ).values_list('truncated', flat=True).first()\n            expected_value = truncate_to(start_datetime, lookup, tzinfo=tzinfo)\n            self.assertEqual(truncated_value, expected_value)\n"], "sample_97": ["def test_iter_modules_and_files_with_zip_importer(self):\n    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:\n        zip_filename = Path(tmp_file.name)\n        self.addCleanup(zip_filename.unlink)\n\n        with zipfile.ZipFile(str(zip_filename), 'w') as zip_file:\n            zip_file.writestr('test_module.py', b'pass')\n\n        sys.path.insert(0, str(zip_filename))\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(zip_filename)\n        sys.path.remove(str(zip_filename))\n\n        # Test that the zip file is not cached.\n        self.clear_autoreload_caches()\n        self.assertFileNotFound(zip_filename)\n"], "sample_104": ["    def test_post_process_passes(self):\n        \"\"\"\n        Test that post_process() runs the specified number of passes.\n        \"\"\"\n        storage.staticfiles_storage.max_post_process_passes = 2\n        with mock.patch.object(storage.staticfiles_storage, '_post_process') as post_process:\n            post_process.return_value = []\n            storage.staticfiles_storage.post_process({}, dry_run=False)\n            self.assertEqual(post_process.call_count, 3)  # initial + 2 passes\n"], "sample_109": ["def test_autocomplete_select_required(self):\n    form = RequiredBandForm()\n    self.assertHTMLEqual(\n        form['band'].as_html(),\n        '<select data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" '\n        'data-ajax--url=\"/autocomplete/album/band/\" data-allow-clear=\"false\" '\n        'data-placeholder=\"\" data-theme=\"admin-autocomplete\" class=\"my-class admin-autocomplete\" '\n        'name=\"band\" required>\\n'\n        + self.empty_option +\n        '\\n</select>'\n    )\n"], "sample_110": ["    def test_resolve_expression(self):\n        # Create a simple expression\n        expr = models.F('id')\n\n        # Resolve the expression\n        resolved_expr = expr.resolve_expression(None)\n\n        # Check that the resolved expression is the same as the original\n        self.assertEqual(resolved_expr, expr)\n"], "sample_116": ["def test_make_template_fragment_key(self):\n    # Test with no vary_on\n    key = make_template_fragment_key('test_fragment')\n    self.assertRegex(key, r'template\\.cache\\.test_fragment\\.[0-9a-f]{32}')\n\n    # Test with single vary_on\n    key = make_template_fragment_key('test_fragment', vary_on=['test_value'])\n    self.assertRegex(key, r'template\\.cache\\.test_fragment\\.[0-9a-f]{32}')\n\n    # Test with multiple vary_on\n    key = make_template_fragment_key('test_fragment', vary_on=['test_value1', 'test_value2'])\n    self.assertRegex(key, r'template\\.cache\\.test_fragment\\.[0-9a-f]{32}')\n\n    # Test that different vary_on values produce different keys\n    key1 = make_template_fragment_key('test_fragment', vary_on=['test_value1'])\n    key2 = make_template_fragment_key('test_fragment', vary_on=['test_value2'])\n    self.assertNotEqual(key1, key2)\n"], "sample_119": ["def test_query_setup_joins_with_m2m(self):\n    query = Query(Author)\n    query.add_filter(('item__name', 'test'))\n    self.assertEqual(len(query.alias_map), 3)\n    self.assertIn('author', query.alias_map)\n    self.assertIn('author__item', query.alias_map)\n    self.assertIn('author__item__name', query.alias_map)\n"], "sample_130": ["def test_query_add_filter(self):\n    q = Query(Author)\n    q.add_filter(('name__isnull', True))\n    self.assertEqual(len(q.where.children), 1)\n    self.assertIsInstance(q.where.children[0], IsNull)\n    self.assertEqual(q.where.connector, OR)\n"], "sample_136": ["def test_http_headers(self):\n    environ = {\n        'HTTP_ACCEPT': 'text/html',\n        'HTTP_ACCEPT_LANGUAGE': 'en-us,en;q=0.5',\n        'HTTP_HOST': 'example.com',\n        'CONTENT_TYPE': 'text/html; charset=utf-8',\n        'CONTENT_LENGTH': '123',\n    }\n    headers = HttpHeaders(environ)\n    self.assertEqual(headers['Accept'], 'text/html')\n    self.assertEqual(headers['Accept-Language'], 'en-us,en;q=0.5')\n    self.assertEqual(headers['Host'], 'example.com')\n    self.assertEqual(headers['Content-Type'], 'text/html; charset=utf-8')\n    self.assertEqual(headers['Content-Length'], '123')\n    self.assertEqual(headers.get('Nonexistent-Header'), None)\n"], "sample_145": ["    def test_raw_id_fields_valid(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ('id',)\n\n        self.assertIsValid(RawIdModelAdmin, Band)\n"], "sample_151": ["def test_generate_added_indexes(self):\n    \"\"\"Tests that add indexes are correctly generated.\"\"\"\n    # Make state\n    before_state = self.make_project_state([self.author_name])\n    after_state = self.make_project_state([self.book_indexes])\n    autodetector = MigrationAutodetector(before_state, after_state)\n    changes = autodetector._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertNumberMigrations(changes, 'testapp', 0)\n    # Right operations?\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"book\")\n    self.assertOperationTypes(changes, 'otherapp', 1, [\"AddIndex\"])\n    self.assertOperationAttributes(changes, 'otherapp', 1, 0, model_name='book', index=models.Index(fields=[\"author\", \"title\"], name=\"book_title_author_idx\"))\n"], "sample_153": ["def test_model_validation(self):\n    class TestModel(models.Model):\n        name = models.CharField(max_length=10)\n\n    # Create a model instance with an invalid field value\n    test_model = TestModel(name='a' * 11)\n\n    # Check that full_clean raises a ValidationError\n    with self.assertRaises(ValidationError):\n        test_model.full_clean()\n\n    # Check that validate_unique doesn't raise an error for a new instance\n    try:\n        test_model.validate_unique()\n    except ValidationError:\n        self.fail(\"validate_unique() raised ValidationError unexpectedly\")\n\n    # Save the model and then check that validate_unique does raise an error\n    test_model.save()\n    with self.assertRaises(ValidationError):\n        test_model.validate_unique()\n"], "sample_169": ["def test_xml_serializer(self):\n    # Create a test model instance\n    obj = JSONModel(json_field={'key': 'value'})\n\n    # Serialize the object to XML\n    xml = serializers.serialize('xml', [obj], indent=4)\n\n    # Check that the XML contains the expected data\n    self.assertContains(xml, '<field name=\"json_field\" type=\"JSONField\">')\n    self.assertContains(xml, '{\"key\": \"value\"}')\n\n    # Deserialize the XML back into an object\n    deserialized_obj = next(serializers.deserialize('xml', xml))\n\n    # Check that the deserialized object matches the original object\n    self.assertEqual(deserialized_obj.object.json_field, obj.json_field)\n"], "sample_172": ["    def test_charfield_with_textareawidget(self):\n        model = Album\n        fieldname = 'name'\n        widgetclass = widgets.AdminTextareaWidget\n        admin_overrides = {\n            'formfield_overrides': {CharField: {'widget': widgetclass}}\n        }\n        self.assertFormfield(model, fieldname, widgetclass, **admin_overrides)\n"], "sample_184": ["    def test_check_swappable(self):\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'myapp.MyModel'\n\n        errors = SwappableModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'models.E001')\n"], "sample_209": ["def test_model_full_clean(self):\n    # Create an instance of Model1 with a CharField longer than the max_length\n    model = Model1(char_field='a' * 11)\n    with self.assertRaises(ValidationError):\n        model.full_clean()\n"], "sample_215": ["    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings_dict = filter.get_safe_settings()\n        self.assertIn('DATABASES', settings_dict)\n        self.assertIsInstance(settings_dict['DATABASES'], dict)\n"], "sample_217": ["def test_media_add(self):\n    \"\"\"Tests that media objects can be added together\"\"\"\n    m1 = Media(css={'all': ['path/to/css1']}, js=['path/to/js1'])\n    m2 = Media(css={'all': ['path/to/css2']}, js=['path/to/js2'])\n\n    combined = m1 + m2\n\n    self.assertEqual(combined._css, {'all': ['path/to/css1', 'path/to/css2']})\n    self.assertEqual(combined._js, ['path/to/js1', 'path/to/js2'])\n"], "sample_221": ["def test_queryset_pickle_with_filter(self):\n    qs = Happening.objects.filter(id=self.happening.id)\n    self.assert_pickles(qs)\n"], "sample_225": ["def test_each_context_site_title(self):\n    self.assertEqual(self.ctx['site_title'], 'Django site admin')\n    self.assertEqual(self.ctx['site_header'], 'Django administration')\n    self.assertEqual(self.ctx['site_url'], '/')\n"], "sample_236": ["def test_sort(self):\n    collector = Collector(using='default')\n    a1 = A.objects.create()\n    b1 = B.objects.create(a=a1)\n    c1 = Child.objects.create(b=b1)\n\n    collector.add([c1])\n    collector.collect([b1], source=A, nullable=False)\n    collector.collect([a1], source=B, nullable=False)\n\n    # Before sorting, the order is: Child, B, A\n    self.assertEqual(list(collector.data.keys()), [Child, B, A])\n\n    collector.sort()\n\n    # After sorting, the order should be: A, B, Child\n    self.assertEqual(list(collector.data.keys()), [A, B, Child])\n"], "sample_238": ["    def test_abs(self):\n        self.assertEqual(Book.objects.annotate(abs_rating=Abs('rating')).get(isbn='159059725').abs_rating, 4.5)\n        self.assertEqual(Book.objects.annotate(abs_rating=Abs('rating')).get(isbn='067232959').abs_rating, 3.0)\n"], "sample_243": ["def test_query_add_filter(self):\n    q = Query(Author)\n    q.add_filter(('name', 'John'))\n    self.assertEqual(len(q.where.children), 1)\n    lookup = q.where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Col)\n    self.assertEqual(lookup.rhs, 'John')\n"], "sample_246": ["    def test_is_templatized(self):\n        command = MakeMessagesCommand()\n        translatable_file = MakeMessagesCommand.translatable_file_class('path', 'file.py', 'locale_dir')\n        build_file = MakeMessagesCommand.build_file_class(command, 'django', translatable_file)\n        self.assertFalse(build_file.is_templatized)\n\n        translatable_file = MakeMessagesCommand.translatable_file_class('path', 'file.html', 'locale_dir')\n        build_file = MakeMessagesCommand.build_file_class(command, 'django', translatable_file)\n        self.assertTrue(build_file.is_templatized)\n\n        translatable_file = MakeMessagesCommand.translatable_file_class('path', 'file.js', 'locale_dir')\n        build_file = MakeMessagesCommand.build_file_class(command, 'djangojs', translatable_file)\n        if gettext_version < (0, 18, 3):\n            self.assertTrue(build_file.is_templatized)\n        else:\n            self.assertFalse(build_file.is_templatized)\n"], "sample_263": ["    def test_dumpdata_uses_default_manager(self):\n        # Create an instance of a model that has a custom manager as the default\n        article = Article.objects.create(title='Test Article')\n        # Use dumpdata to serialize the data\n        with NamedTemporaryFile(mode='w+', encoding='utf-8') as tmp_file:\n            management.call_command('dumpdata', 'fixtures.Article', stdout=tmp_file)\n            tmp_file.seek(0)\n            output = tmp_file.read()\n            self.assertIn('\"title\": \"Test Article\"', output)\n"], "sample_267": ["def test_sqlite_version_check(self):\n    with mock.patch('django.db.backends.sqlite3.base.Database.sqlite_version_info') as version_info:\n        version_info.return_value = (3, 8, 0)\n        with self.assertRaises(ImproperlyConfigured):\n            check_sqlite_version()\n        version_info.return_value = (3, 9, 0)\n        check_sqlite_version()  # Should not raise an exception\n"], "sample_268": ["def test_iter_modules_and_files_zip_import(self):\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        filename = 'test_zip.zip'\n        zip_path = Path(tmp_dir) / filename\n        with zipfile.ZipFile(str(zip_path), 'w') as zip_file:\n            zip_file.writestr('test.py', b'')\n        sys.path.insert(0, str(zip_path))\n        self.import_and_cleanup('test')\n        self.assertFileFound(zip_path)\n        sys.path.remove(str(zip_path))\n"], "sample_270": ["    def test_model_name_db_lookup_clashes(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=10)\n\n            class Meta:\n                app_label = 'myapp'\n\n        model = Model()\n        errors = model.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'models.E023')\n        self.assertEqual(\n            errors[0].msg,\n            \"The model name 'Model' cannot start or end with an underscore \"\n            \"as it collides with the query lookup syntax.\",\n        )\n"], "sample_274": ["    def test_model_choice_field(self):\n        class TestForm(Form):\n            choice = ModelChoiceField(ChoiceModel.objects.all())\n\n        form = TestForm({'choice': '1'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['choice'].pk, 1)\n"], "sample_279": ["    def test_check_constraint(self):\n        # Create a model with a check constraint.\n        class Product(models.Model):\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(price__gt=0), name='price_gt_0'),\n                ]\n\n        # Create the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Product)\n\n        # Try to insert an invalid value.\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(price=-1)\n\n        # Insert a valid value.\n        Product.objects.create(price=1)\n\n        # Drop the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.delete_model(Product)\n"], "sample_289": ["def test_ordered_set_init(self):\n    # Test that OrderedSet initializes correctly with an iterable\n    ordered_set = OrderedSet([3, 1, 2, 3])\n    self.assertEqual(list(ordered_set), [3, 1, 2])\n\n    # Test that OrderedSet initializes correctly without an iterable\n    ordered_set = OrderedSet()\n    self.assertEqual(list(ordered_set), [])\n\n    # Test that OrderedSet raises a TypeError if initialized with a non-iterable\n    with self.assertRaises(TypeError):\n        OrderedSet(123)\n"], "sample_294": ["def test_csrf_token_rotation(self):\n    req = self._get_POST_request_with_token()\n    get_token(req)\n    token1 = req.META['CSRF_COOKIE']\n    rotate_token(req)\n    token2 = req.META['CSRF_COOKIE']\n    self.assertNotEqual(token1, token2)\n    self.assertTrue(equivalent_tokens(token1, token1))\n    self.assertFalse(equivalent_tokens(token1, token2))\n"], "sample_295": ["def test_expression_pickle(self):\n    expr = F('num_employees') + Value(5)\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n    self.assertEqual(expr.as_sql(connection), unpickled_expr.as_sql(connection))\n"], "sample_299": ["def test_check_default_cache_is_configured(self):\n    with override_settings(CACHES=self.INVALID_CACHES_CONFIGURATION):\n        errors = check_default_cache_is_configured(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], E001)\n\n    with override_settings(CACHES=self.VALID_CACHES_CONFIGURATION):\n        errors = check_default_cache_is_configured(None)\n        self.assertEqual(errors, [])\n"], "sample_300": ["def test_query_build_where(self):\n    q = Query(Author)\n    lookup = q.build_where(('name', 'test'))\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Col)\n    self.assertEqual(lookup.rhs, 'test')\n"], "sample_317": ["def test_rfc2822_date(self):\n    dt = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=TZ)\n    self.assertEqual(rfc2822_date(dt), 'Sat, 01 Jan 2022 12:00:00 +0000')\n    dt = datetime.date(2022, 1, 1)\n    self.assertEqual(rfc2822_date(dt), 'Sat, 01 Jan 2022 00:00:00 +0000')\n"], "sample_325": ["def test_boundfield_label_tag(self):\n    # Test label_tag() on a BoundField with an ID attribute.\n    bf = PersonNew().fields['first_name']\n    self.assertHTMLEqual(bf.label_tag(), '<label for=\"first_name_id\">First name:</label>')\n\n    # Test label_tag() on a BoundField without an ID attribute.\n    bf = Person().fields['first_name']\n    self.assertEqual(bf.label_tag(), 'First name:')\n\n    # Test label_tag() with custom attrs.\n    bf = PersonNew().fields['first_name']\n    self.assertHTMLEqual(bf.label_tag(attrs={'class': 'my_class'}), '<label for=\"first_name_id\" class=\"my_class\">First name:</label>')\n\n    # Test label_tag() with contents.\n    bf = PersonNew().fields['first_name']\n    self.assertHTMLEqual(bf.label_tag(contents='Custom label'), '<label for=\"first_name_id\">Custom label</label>')\n\n    # Test label_tag() with label_suffix.\n    bf = PersonNew().fields['first_name']\n    self.assertHTMLEqual(bf.label_tag(label_suffix='?'), '<label for=\"first_name_id\">First name?</label>')\n"], "sample_334": ["def test_form_add_prefix(self):\n    p = Person()\n    self.assertEqual(p.add_prefix('first_name'), 'first_name')\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('first_name'), 'person-first_name')\n"], "sample_338": ["def test_alter_field_with_choices(self):\n    \"\"\"\n    Altering a field with choices should not create an AlterField operation if\n    the choices are the same.\n    \"\"\"\n    before = self.make_project_state([self.author_name])\n    after = self.make_project_state([\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, choices=[(\"a\", \"A\"), (\"b\", \"B\")])),\n        ])\n    ])\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name')\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, choices=[(\"a\", \"A\"), (\"b\", \"B\")])\n\n    # Now check that altering the choices creates an AlterField operation.\n    before = after\n    after = self.make_project_state([\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, choices=[(\"a\", \"A\"), (\"c\", \"C\")])),\n        ])\n    ])\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name')\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, choices=[(\"a\", \"A\"), (\"c\", \"C\")])\n"], "sample_343": ["def test_generic_foreign_key_check_field_name(self):\n    class Model(models.Model):\n        gfk = GenericForeignKey('content_type', 'object_id', '_')\n\n    errors = Model.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'fields.E001')\n"], "sample_344": ["def test_state_clone(self):\n    \"\"\"Test that cloning a state results in an equivalent but independent copy\"\"\"\n    project_state = ProjectState()\n    model_state = ModelState(\n        app_label='myapp',\n        name='MyModel',\n        fields=[\n            ('id', models.AutoField(primary_key=True)),\n        ],\n    )\n    project_state.add_model(model_state)\n\n    cloned_state = project_state.clone()\n\n    self.assertEqual(project_state.models, cloned_state.models)\n    self.assertIsNot(project_state.models, cloned_state.models)\n\n    # Modify the original model state\n    model_state.fields['new_field'] = models.IntegerField()\n\n    # The cloned state should not be affected\n    self.assertNotIn('new_field', cloned_state.models[('myapp', 'mymodel')].fields)\n"], "sample_348": ["    def test_modelformset_factory(self):\n        class MyModelForm(forms.ModelForm):\n            class Meta:\n                model = Band\n                fields = ('name',)\n\n        MyModelFormSet = forms.modelformset_factory(Band, form=MyModelForm)\n        self.assertIsInstance(MyModelFormSet, type)\n        self.assertTrue(issubclass(MyModelFormSet, BaseModelFormSet))\n"], "sample_349": ["def test_autocomplete_select_required(self):\n    form = RequiredBandForm()\n    self.assertHTMLEqual(\n        form['band'].as_html(),\n        '<select class=\"admin-autocomplete\" data-ajax--cache=\"true\" '\n        'data-ajax--delay=\"250\" data-ajax--type=\"GET\" '\n        'data-ajax--url=\"/autocomplete/\" data-allow-clear=\"false\" '\n        'data-app-label=\"myapp\" data-field-name=\"band\" '\n        'data-model-name=\"album\" data-placeholder=\"\" data-theme=\"admin-autocomplete\" '\n        'id=\"id_band\" name=\"band\" required>\\n'\n        + self.empty_option +\n        '\\n</select>'\n    )\n"], "sample_352": ["    def test_empty_where_node(self):\n        node = WhereNode(children=[], connector='AND', negated=False)\n        sql, params = node.as_sql(None, None)\n        self.assertEqual(sql, '')\n        self.assertEqual(params, [])\n"], "sample_357": ["def test_alter_index_together_add(self):\n    \"\"\"Tests adding an index_together constraint.\"\"\"\n    changes = self.get_changes([self.book], [self.book_foo_together])\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    # Right operations in migration?\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AlterIndexTogether'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', index_together={('author', 'title')})\n"], "sample_359": ["    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=255)),\n            ],\n        )\n        self.apply_operations(operation)\n        self.assertTableExists('testapp_testmodel')\n        self.assertColumnExists('testapp_testmodel', 'id')\n        self.assertColumnExists('testapp_testmodel', 'name')\n"], "sample_362": ["def test_alter_index_together(self):\n    \"\"\"Tests that AlterIndexTogether is correctly generated.\"\"\"\n    changes = self.get_changes(\n        [self.book_foo_together],\n        [self.book_foo_together_2],\n    )\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AlterIndexTogether'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', index_together={('title', 'author')})\n"], "sample_363": ["    def test_filtered_select_multiple(self):\n        widget = widgets.FilteredSelectMultiple('test', False)\n        self.assertEqual(widget.media._js, [\n            'admin/js/core.js',\n            'admin/js/SelectBox.js',\n            'admin/js/SelectFilter2.js',\n        ])\n"], "sample_369": ["def test_alter_index_together(self):\n    \"\"\"Tests that AlterIndexTogether is correctly generated.\"\"\"\n    changes = self.get_changes(\n        [self.book_foo_together],\n        [self.book_foo_together_2],\n    )\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AlterIndexTogether'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, option_name='index_together')\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, option_value={('title', 'author')})\n"], "sample_381": ["def test_alter_db_table(self):\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_with_new_db_table_options],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', table='author_two')\n\n    changes = self.get_changes(\n        [self.author_renamed_with_db_table_options],\n        [self.author_renamed_with_new_db_table_options],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='newauthor', table='author_three')\n"], "sample_389": ["    def test_http_request_repr(self):\n        request = HttpRequest()\n        request.method = \"GET\"\n        request.path = \"/path/\"\n        self.assertEqual(repr(request), \"<HttpRequest: GET '/path/'>\")\n"], "sample_399": ["def test_exists_subquery(self):\n    subquery = Book.objects.filter(pages__gt=300)\n    qs = Author.objects.filter(Exists(subquery.filter(authors=OuterRef(\"pk\"))))\n    self.assertEqual(qs.count(), 6)\n\n    # Check that the subquery is properly nested.\n    subquery_sql = str(subquery.query).replace(\"%s\", \"'%s'\")\n    self.assertNotIn(subquery_sql, str(qs.query))\n"], "sample_401": ["def test_formset_management_form_validation(self):\n    formset = self.make_choiceformset(formset_data=[(\"Choice 1\", 10), (\"Choice 2\", 20)])\n    management_form = ManagementForm(\n        {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"\",\n            \"choices-MIN_NUM_FORMS\": \"\",\n        },\n        prefix=\"choices\",\n    )\n    self.assertTrue(management_form.is_valid())\n\n    # Test that missing fields are correctly reported.\n    management_form = ManagementForm(\n        {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"\",\n        },\n        prefix=\"choices\",\n    )\n    self.assertFalse(management_form.is_valid())\n    self.assertEqual(len(management_form.errors), 1)\n    self.assertIn(\"MIN_NUM_FORMS\", management_form.errors)\n\n    # Test that non-integer values are correctly reported.\n    management_form = ManagementForm(\n        {\n            \"choices-TOTAL_FORMS\": \"two\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"\",\n            \"choices-MIN_NUM_FORMS\": \"\",\n        },\n        prefix=\"choices\",\n    )\n    self.assertFalse(management_form.is_valid())\n    self.assertEqual(len(management_form.errors), 1)\n    self.assertIn(\"TOTAL_FORMS\", management_form.errors)\n"], "sample_5": ["def test_parameter_value_setter():\n    p = Parameter(name='test', default=1.0)\n    assert p.value == 1.0\n\n    p.value = 2.0\n    assert p.value == 2.0\n\n    with pytest.raises(InputParameterError):\n        p.value = 'invalid'\n\n    with pytest.raises(InputParameterError):\n        p.value = [1, 2, 3]\n\n    p = Parameter(name='test', default=1.0, unit=u.m)\n    assert p.value == 1.0\n\n    p.value = 2.0\n    assert p.value == 2.0\n\n    with pytest.raises(TypeError):\n        p.value = Quantity(1.0, u.s)\n\n    p.quantity = Quantity(1.0, u.m)\n    assert p.value == 1.0\n"], "sample_7": ["def test_column_insert():\n    # Test inserting into a Column with different dtypes\n    col = table.Column([1, 2, 3], name='a', dtype='i4')\n    col2 = col.insert(1, [4])\n    assert_array_equal(col2.data, np.array([1, 4, 2, 3], dtype='i4'))\n\n    col = table.Column([1.0, 2.0, 3.0], name='a', dtype='f8')\n    col2 = col.insert(1, [4.0])\n    assert_array_equal(col2.data, np.array([1.0, 4.0, 2.0, 3.0], dtype='f8'))\n\n    col = table.Column(['a', 'b', 'c'], name='a')\n    col2 = col.insert(1, ['d'])\n    assert_array_equal(col2.data, np.array(['a', 'd', 'b', 'c'], dtype='U1'))\n\n    # Test inserting into a MaskedColumn\n    col = table.MaskedColumn([1, 2, 3], name='a', dtype='i4', mask=[True, False, True])\n    col2 = col.insert(1, [4], mask=[False])\n    assert_array_equal(col2.data.data, np.array([1, 4, 2, 3], dtype='i4'))\n    assert_array_equal(col2.mask, np.array([True, False, False, True], dtype='bool'))\n"], "sample_8": ["def test_masked_array_str():\n    a = Masked(self.a, self.mask_a)\n    assert str(a) == \"\"\"[[-- 1. 2.]\n [3. -- 5.]]\"\"\"\n    assert repr(a) == \"\"\"MaskedNDArray([[--, 1., 2.],\n           [3., --, 5.]])\"\"\"\n"], "sample_16": ["    def test_atleast_1d(self):\n        self.check(np.atleast_1d)\n"], "sample_37": ["def test_wcs_to_header(self):\n    # Test that WCS.to_header() produces a header that can be used to\n    # reconstruct the original WCS object.\n    for filename in self._file_list:\n        w = wcs.WCS(get_pkg_data_filename(filename))\n        header = w.to_header()\n        w2 = wcs.WCS(header)\n        assert_array_almost_equal(w.wcs.crval, w2.wcs.crval)\n        assert_array_almost_equal(w.wcs.crpix, w2.wcs.crpix)\n        assert_array_almost_equal(w.wcs.cdelt, w2.wcs.cdelt)\n        assert_array_almost_equal(w.wcs.pc, w2.wcs.pc)\n"], "sample_38": ["def test_wcs_to_header(self):\n    # Test that to_header produces the expected output for a simple WCS\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN']\n    w.wcs.crval = [12.5, 34.2]\n    w.wcs.crpix = [100, 200]\n    w.wcs.cdelt = [-0.01, 0.01]\n\n    header = w.to_header()\n    assert header['CTYPE1'] == 'RA---TAN'\n    assert header['CTYPE2'] == 'DEC--TAN'\n    assert header['CRVAL1'] == 12.5\n    assert header['CRVAL2'] == 34.2\n    assert header['CRPIX1'] == 100\n    assert header['CRPIX2'] == 200\n    assert header['CDELT1'] == -0.01\n    assert header['CDELT2'] == 0.01\n"], "sample_39": ["def test_wcs_invalid_axis_type(self):\n    # Test WCS with invalid axis type\n    header = fits.Header.fromtextfile(get_pkg_data_filename(\"data/invalid_axis_type.hdr\"))\n    with pytest.raises(_wcs.InconsistentAxisTypesError):\n        wcs.WCS(header)\n"], "sample_42": ["def test_brightness_temperature():\n    freq = 30 * u.GHz\n    equiv = u.brightness_temperature(freq)\n    val = 1 * u.Jy / u.sr\n    assert_quantity_allclose(val.to(u.K, equivalencies=equiv),\n                             55.06350777515121 * u.K)\n"], "sample_53": ["def test_autocomplete_widget_render(self):\n    form = AlbumForm()\n    with translation.override('en'):\n        band_field = form.fields['band']\n        widget = band_field.widget\n        html = widget.render('band', '')\n        self.assertHTMLEqual(\n            html,\n            '<select class=\"my-class\" data-ajax--cache=\"true\" '\n            'data-ajax--delay=\"250\" data-ajax--type=\"GET\" '\n            'data-autocomplete-light-function=\"select2\" '\n            'data-autocomplete-light-url=\"/autocomplete/band/\" '\n            'data-theme=\"admin-autocomplete\" id=\"id_band\" name=\"band\">'\n            + self.empty_option +\n            '</select>'\n        )\n"], "sample_56": ["def test_check_dependencies(self):\n    with override_settings(INSTALLED_APPS=['django.contrib.admin']):\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 4)\n        self.assertEqual(errors[0].id, 'admin.E401')\n        self.assertEqual(errors[1].id, 'admin.E402')\n        self.assertEqual(errors[2].id, 'admin.E403')\n        self.assertEqual(errors[3].id, 'admin.E404')\n\n    with override_settings(\n        INSTALLED_APPS=[\n            'django.contrib.admin',\n            'django.contrib.auth',\n            'django.contrib.contenttypes',\n            'django.contrib.sessions',\n        ],\n        MIDDLEWARE=[\n            'django.contrib.auth.middleware.AuthenticationMiddleware',\n            'django.contrib.messages.middleware.MessageMiddleware',\n        ],\n        TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [],\n            'APP_DIRS': True,\n            'OPTIONS': {\n                'context_processors': [\n                    'django.template.context_processors.debug',\n                    'django.template.context_processors.request',\n                    'django.contrib.auth.context_processors.auth',\n                ],\n            },\n        }],\n    ):\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E404')\n\n    with override_settings(\n        INSTALLED_APPS=[\n            'django.contrib.admin',\n            'django.contrib.auth',\n            'django.contrib.contenttypes',\n            'django.contrib.sessions',\n            'django.contrib.messages',\n        ],\n        MIDDLEWARE=[\n            'django.contrib.auth.middleware.AuthenticationMiddleware',\n        ],\n        TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [],\n            'APP_DIRS': True,\n            'OPTIONS': {\n                'context_processors': [\n                    'django.template.context_processors.debug',\n                    'django.template.context_processors.request',\n                    'django.contrib.auth.context_processors.auth',\n                    'django.contrib.messages.context_processors.messages',\n                ],\n            },\n        }],\n    ):\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E409')\n"], "sample_60": ["def test_inline_model_admin_get_queryset(self):\n    class TestInline(admin.TabularInline):\n        model = Media\n\n    inline = TestInline(Episode, admin_site)\n    request = RequestFactory().get(reverse('admin:index'))\n    request.user = self.superuser\n\n    queryset = inline.get_queryset(request)\n    self.assertEqual(list(queryset), list(Media.objects.all()))\n\n    inline.has_view_or_change_permission = lambda request, obj=None: False\n    queryset = inline.get_queryset(request)\n    self.assertEqual(list(queryset), [])\n"], "sample_66": ["def test_request_get_host_with_x_forwarded_host(self):\n    request = HttpRequest()\n    request.META = {\n        'HTTP_X_FORWARDED_HOST': 'example.com',\n        'HTTP_HOST': 'other.example.com',\n        'SERVER_NAME': 'server.example.com',\n        'SERVER_PORT': 80,\n    }\n    with override_settings(USE_X_FORWARDED_HOST=True):\n        self.assertEqual(request.get_host(), 'example.com')\n"], "sample_72": ["    def test_serializer_factory(self):\n        value = decimal.Decimal('1.2')\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, DecimalSerializer)\n"], "sample_79": ["    def test_add(self):\n        output = self.engine.render_to_string('t', {'value': 5})\n        self.assertEqual(output, '10')\n"], "sample_80": ["def test_query_add_filter(self):\n    q = Query(Author)\n    q.add_filter(('name', 'John'))\n    self.assertEqual(len(q.where.children), 1)\n    lookup = q.where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, SimpleCol)\n    self.assertEqual(lookup.rhs, 'John')\n"], "sample_85": ["def test_on_delete_do_nothing(self):\n    r = R.objects.create()\n    m = M.objects.create(r=r)\n    r.delete()\n    self.assertEqual(M.objects.filter(r=r).count(), 1)\n"], "sample_102": ["def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, range(10))\n"], "sample_114": ["def test_generate_added_indexes(self):\n    \"\"\"Tests that add indexes are correctly generated.\"\"\"\n    # Make state\n    before_state = self.make_project_state([self.author_name])\n    after_state = self.make_project_state([self.author_name_with_index])\n\n    # Create the tables\n    with connection.schema_editor() as schema_editor:\n        schema_editor.create_model(self.author_name)\n\n    # Generate the plans\n    autodetector = MigrationAutodetector(before_state, after_state)\n    changes = autodetector._detect_changes()\n\n    # Check the result\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddIndex'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author_name_idx')\n"], "sample_118": ["def test_lookup_with_bilateral_transforms(self):\n    # Test that bilateral transformations are applied correctly.\n    articles = Article.objects.filter(headline__startswith='Article')\n    transformed_articles = articles.filter(slug__in=['a1', 'a2'])\n    self.assertEqual(transformed_articles.count(), 2)\n"], "sample_120": ["    def test_serializer_factory(self):\n        # Test that serializer_factory can handle a variety of types.\n        tests = [\n            (1, BaseSimpleSerializer),\n            ('a', BaseSimpleSerializer),\n            (b'a', BaseSimpleSerializer),\n            (True, BaseSimpleSerializer),\n            (None, BaseSimpleSerializer),\n            ([1, 2, 3], SequenceSerializer),\n            ((1, 2, 3), TupleSerializer),\n            ({1, 2, 3}, SetSerializer),\n            (frozenset({1, 2, 3}), FrozensetSerializer),\n            ({'a': 1, 'b': 2}, DictionarySerializer),\n            (datetime.datetime(2022, 1, 1), DatetimeDatetimeSerializer),\n            (datetime.date(2022, 1, 1), DateTimeSerializer),\n            (datetime.time(12, 0), DateTimeSerializer),\n            (datetime.timedelta(days=1), DateTimeSerializer),\n            (SettingsReference('TEST'), SettingsReferenceSerializer),\n            (decimal.Decimal('1.2'), DecimalSerializer),\n            (functools.partial(int, base=2), FunctoolsPartialSerializer),\n            (re.compile('test'), RegexSerializer),\n            (uuid.UUID('6ca6245d-7c4e-4f56-8e36-35a683a64a45'), UUIDSerializer),\n        ]\n        for value, expected_serializer in tests:\n            with self.subTest(value=value):\n                serializer = serializer_factory(value)\n                self.assertIsInstance(serializer, expected_serializer)\n"], "sample_121": ["    def test_check_swappable(self):\n        class SwappedModel(models.Model):\n            class Meta:\n                swappable = 'myapp.MyModel'\n\n        errors = SwappedModel.check()\n        self.assertEqual(errors, [\n            Error(\n                \"'myapp.MyModel' is not of the form 'app_label.app_name'.\",\n                id='models.E001',\n            ),\n        ])\n\n        class SwappedModel(models.Model):\n            class Meta:\n                swappable = 'invalid_app_label.myapp.MyModel'\n\n        errors = SwappedModel.check()\n        self.assertEqual(errors, [\n            Error(\n                \"'invalid_app_label.myapp.MyModel' references 'invalid_app_label.myapp.MyModel', \"\n                \"which has not been installed, or is abstract.\",\n                id='models.E002',\n            ),\n        ])\n"], "sample_126": ["def test_generate_altered_order_with_respect_to(self):\n    \"\"\"Tests the order_with_respect_to detection.\"\"\"\n    # Make a model with order_with_respect_to\n    before = self.make_project_state([self.author_with_book])\n    after = self.make_project_state([self.author_with_book_order_wrt])\n    changes = self.get_changes(before, after)\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    # Right number of operations?\n    migration = changes['testapp'][0]\n    self.assertEqual(len(migration.operations), 1)\n    self.assertIsInstance(migration.operations[0], operations.AlterOrderWithRespectTo)\n    self.assertEqual(migration.operations[0].name, \"author\")\n    self.assertEqual(migration.operations[0].order_with_respect_to, \"book\")\n\n    # And going back the other way?\n    before = self.make_project_state([self.author_with_book_order_wrt])\n    after = self.make_project_state([self.author_with_book])\n    changes = self.get_changes(before, after)\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    # Right number of operations?\n    migration = changes['testapp'][0]\n    self.assertEqual(len(migration.operations), 1)\n    self.assertIsInstance(migration.operations[0], operations.AlterOrderWithRespectTo)\n    self.assertEqual(migration.operations[0].name, \"author\")\n    self.assertIsNone(migration.operations[0].order_with_respect_to)\n"], "sample_127": ["def test_bulk_create_empty(self):\n    self.assertEqual(Country.objects.bulk_create([]), [])\n"], "sample_129": ["    def test_floatformat_with_positive_arg(self):\n        with localcontext() as ctx:\n            ctx.prec = 10\n            val = Decimal('123.456789')\n            self.assertEqual(floatformat(val, 3), '123.457')\n"], "sample_135": ["def test_date_format(self):\n    d = date(2009, 2, 12)\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'F'), 'February')\n    self.assertEqual(dateformat.format(d, 'E'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'M'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'b'), 'feb')\n    self.assertEqual(dateformat.format(d, 'N'), 'Feb.')\n    self.assertEqual(dateformat.format(d, 'm'), '02')\n    self.assertEqual(dateformat.format(d, 'n'), '2')\n    self.assertEqual(dateformat.format(d, 'd'), '12')\n    self.assertEqual(dateformat.format(d, 'j'), '12')\n    self.assertEqual(dateformat.format(d, 'D'), 'Thu')\n    self.assertEqual(dateformat.format(d, 'l'), 'Thursday')\n    self.assertEqual(dateformat.format(d, 'w'), '4')\n    self.assertEqual(dateformat.format(d, 'z'), '43')\n    self.assertEqual(dateformat.format(d, 't'), '28')\n    self.assertEqual(dateformat.format(d, 'L'), False)\n    self.assertEqual(dateformat.format(d, 'o'), '2009')\n    self.assertEqual(dateformat.format(d, 'W'), '7')\n    self.assertEqual(dateformat.format(d, 'c'), '2009-02-12')\n    self.assertEqual(dateformat.format(d, 'r'), 'Thu, 12 Feb 2009 00:00:00 +0100')\n    self.assertEqual(dateformat.format(d, 'U'), '1234406400')\n"], "sample_134": ["    def test_serialize_float(self):\n        serializer = FloatSerializer(1.5)\n        self.assertEqual(serializer.serialize(), ('1.5', set()))\n"], "sample_142": ["def test_modelform_defines_fields(self):\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Album\n            fields = ('name',)\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Album\n            exclude = ('name',)\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Album\n\n    self.assertFalse(modelform_defines_fields(MyModelForm))\n"], "sample_147": ["def test_union(self):\n    qs1 = Number.objects.filter(num__lte=3)\n    qs2 = Number.objects.filter(num__gte=7)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 7, 8, 9], ordered=False)\n"], "sample_158": ["def test_resolve_relation(self):\n    model = models.Model()\n    self.assertEqual(resolve_relation(model, 'self'), model)\n    self.assertEqual(resolve_relation(model, 'app.Model'), 'app.Model')\n    self.assertEqual(resolve_relation(model, 'Model'), 'invalid_models_tests.Model')\n"], "sample_161": ["def test_resolve_relation(self):\n    model = models.Model()\n    self.assertEqual(resolve_relation(model, 'self'), model)\n    self.assertEqual(resolve_relation(model, 'app.Model'), 'app.Model')\n    self.assertEqual(resolve_relation(model, 'Model'), 'invalid_models_tests.Model')\n"], "sample_165": ["    def test_model_choice_field(self):\n        class ModelChoiceForm(Form):\n            category = ModelChoiceField(ChoiceModel.objects.all())\n\n        # Create a ChoiceModel instance to be the selected value.\n        choice1 = ChoiceModel.objects.create(choice='Choice 1')\n        choice2 = ChoiceModel.objects.create(choice='Choice 2')\n\n        # Test with a valid choice.\n        form = ModelChoiceForm({'category': choice1.pk})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['category'], choice1)\n\n        # Test with an invalid choice.\n        form = ModelChoiceForm({'category': 'invalid'})\n        self.assertFormErrors(['Select a valid choice. That choice is not one of the available choices.'], form.full_clean)\n"], "sample_175": ["def test_can_fast_delete(self):\n    a = create_a()\n    collector = Collector(using='default')\n    self.assertTrue(collector.can_fast_delete(a))\n    self.assertFalse(collector.can_fast_delete(A.objects.all()))\n"], "sample_176": ["def test_generate_added_indexes(self):\n    \"\"\"Tests that add indexes are correctly generated.\"\"\"\n    # Make state\n    before_state = self.make_project_state([self.author_name])\n    after_state = self.make_project_state([\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ], {\n            \"indexes\": [models.Index(fields=[\"name\"], name=\"author_name_idx\")],\n        }),\n    ])\n    autodetector = MigrationAutodetector(before_state, after_state)\n    changes = autodetector._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    # Right operations?\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddIndex\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author')\n    self.assertEqual(changes['testapp'][0].operations[0].index.name, \"author_name_idx\")\n"], "sample_180": ["    def test_check_swappable(self):\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'myapp.MyModel'\n\n        errors = SwappableModel.check()\n        self.assertEqual(errors, [\n            Error(\n                \"'myapp.MyModel' is not of the form 'app_label.app_name'.\",\n                id='models.E001',\n            ),\n        ])\n\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'myapp.mymodel'\n\n        errors = SwappableModel.check()\n        self.assertEqual(errors, [\n            Error(\n                \"'myapp.mymodel' references 'myapp.mymodel', which has not been \"\n                \"installed, or is abstract.\",\n                id='models.E002',\n            ),\n        ])\n"], "sample_179": ["    def test_model_name_db_lookup_clashes(self):\n        class Model(models.Model):\n            pass\n\n        Model.__name__ = '_Model'\n        errors = Model.check()\n        self.assertEqual(errors, [\n            Error(\n                \"The model name '_Model' cannot start or end with an underscore \"\n                \"as it collides with the query lookup syntax.\",\n                obj=Model,\n                id='models.E023',\n            ),\n        ])\n\n        Model.__name__ = 'Model_'\n        errors = Model.check()\n        self.assertEqual(errors, [\n            Error(\n                \"The model name 'Model_' cannot start or end with an underscore \"\n                \"as it collides with the query lookup syntax.\",\n                obj=Model,\n                id='models.E023',\n            ),\n        ])\n\n        Model.__name__ = 'Model__'\n        errors = Model.check()\n        self.assertEqual(errors, [\n            Error(\n                \"The model name 'Model__' cannot contain double underscores as \"\n                \"it collides with the query lookup syntax.\",\n                obj=Model,\n                id='models.E024',\n            ),\n        ])\n"], "sample_182": ["def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, range(10))\n"], "sample_181": ["def test_annotate_with_filtered_relation(self):\n    qs = Author.objects.annotate(\n        num_books=Count('book', filter=Q(book__rating__gt=3.0)),\n        num_friends=Count('friends', filter=Q(friends__age__gt=60)),\n    )\n    self.assertEqual(qs.get(name='test').num_books, 1)\n    self.assertEqual(qs.get(name='test').num_friends, 1)\n    self.assertEqual(qs.get(name='test2').num_books, 0)\n    self.assertEqual(qs.get(name='test2').num_friends, 0)\n    self.assertEqual(qs.get(name='test3').num_books, 2)\n    self.assertEqual(qs.get(name='test3').num_friends, 0)\n"], "sample_183": ["def test_case_expression_with_aggregate(self):\n    # Ensure that a Case expression with an aggregate expression works.\n    agg = Sum(Case(When(integer=1, then=1), default=Value(0)))\n    self.assertEqual(\n        CaseTestModel.objects.aggregate(result=agg),\n        {'result': 2}\n    )\n"], "sample_186": ["def test_check_dependencies(self):\n    with override_settings(\n        INSTALLED_APPS=[\n            'django.contrib.admin',\n            'django.contrib.auth',\n            'django.contrib.contenttypes',\n        ],\n        MIDDLEWARE=[\n            'django.contrib.sessions.middleware.SessionMiddleware',\n            'django.contrib.auth.middleware.AuthenticationMiddleware',\n            'django.contrib.messages.middleware.MessageMiddleware',\n        ],\n        TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [],\n            'APP_DIRS': True,\n            'OPTIONS': {\n                'context_processors': [\n                    'django.template.context_processors.request',\n                    'django.contrib.auth.context_processors.auth',\n                    'django.contrib.messages.context_processors.messages',\n                ],\n            },\n        }],\n    ):\n        self.assertEqual(check_dependencies(), [])\n\n    with override_settings(\n        INSTALLED_APPS=[\n            'django.contrib.admin',\n            'django.contrib.auth',\n        ],\n    ):\n        errors = check_dependencies()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E401')\n\n    with override_settings(\n        MIDDLEWARE=[\n            'django.contrib.auth.middleware.AuthenticationMiddleware',\n            'django.contrib.messages.middleware.MessageMiddleware',\n        ],\n    ):\n        errors = check_dependencies()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E410')\n\n    with override_settings(\n        TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [],\n            'APP_DIRS': True,\n            'OPTIONS': {\n                'context_processors': [\n                    'django.template.context_processors.request',\n                    'django.contrib.messages.context_processors.messages',\n                ],\n            },\n        }],\n    ):\n        errors = check_dependencies()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E402')\n"], "sample_188": ["def test_expression_pickle(self):\n    expr = F('num_employees') + Value(5)\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n\n    # Check that the expression still works after being unpickled.\n    company = Company.objects.create(name='Test', num_employees=10)\n    self.assertEqual(unpickled_expr.resolve_expression(None).as_sql(connection), ('num_employees + 5', []))\n"], "sample_190": ["def test_lookup_with_bilateral_transforms(self):\n    # Test that bilateral transformations are applied correctly.\n    season = Season.objects.create(year=2022)\n    game1 = Game.objects.create(season=season, home_score=3, away_score=2)\n    game2 = Game.objects.create(season=season, home_score=2, away_score=3)\n\n    # Use a bilateral transformation to create a lookup that filters on the\n    # absolute difference between home and away scores.\n    lookup = models.Q(home_score__abs_diff=models.F('away_score'))\n\n    # Apply the lookup to the Game model.\n    filtered_games = Game.objects.filter(lookup)\n\n    # Check that the correct games are returned.\n    self.assertEqual(filtered_games.count(), 2)\n    self.assertIn(game1, filtered_games)\n    self.assertIn(game2, filtered_games)\n"], "sample_195": ["    def test_adapt_datetimefield_value(self):\n        ops = connection.ops\n        dt = timezone.make_aware(timezone.datetime(2018, 1, 1, 12, 0, 0))\n        self.assertEqual(ops.adapt_datetimefield_value(dt), '2018-01-01 12:00:00')\n        self.assertIsNone(ops.adapt_datetimefield_value(None))\n"], "sample_198": ["def test_expression_pickle(self):\n    expr = F('num_employees') + Value(5)\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n\n    # Test that the expression still works after being unpickled.\n    company = Company.objects.create(name='Test', num_employees=10)\n    self.assertEqual(unpickled_expr.resolve_expression(None), 15)\n"], "sample_199": ["def test_expression_wrapper(self):\n    wrapper = ExpressionWrapper(Value(1), output_field=IntegerField())\n    self.assertEqual(wrapper.output_field.get_internal_type(), 'IntegerField')\n    self.assertEqual(wrapper.as_sql(None, connection), ('%s', [1]))\n"], "sample_207": ["def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform('test', 'test')\n    with self.assertRaises(TypeError):\n        mixin.__init__(key_transform, 'test', 'test')\n\n    class TestKeyTransform(KeyTransform):\n            return 'TEST', []\n\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = TestKeyTransform('test', 'test')\n    mixin.__init__(key_transform, 'test', 'test')\n    self.assertIsInstance(mixin.lhs, KeyTextTransform)\n"], "sample_214": ["def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform('test', 'test')\n    with self.assertRaises(TypeError):\n        mixin.__init__(key_transform, 'test', 'test')\n\n    class TestKeyTransform(KeyTransform):\n            super().__init__(*args, **kwargs)\n            self.key_name = 'test'\n\n    key_transform = TestKeyTransform('test', 'test')\n    mixin.__init__(key_transform, 'test', 'test')\n    self.assertEqual(mixin.lhs, key_transform)\n"], "sample_218": ["    def test_trunc_with_timezone(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, tzinfo=pytz.timezone('US/Eastern'))\n        model = self.create_model(start_datetime, None)\n\n        # Test truncation with a specific timezone.\n        tzinfo = pytz.timezone('US/Pacific')\n        truncated = DTModel.objects.annotate(\n            trunc=Trunc('start_datetime', 'day', tzinfo=tzinfo),\n        ).get(pk=model.pk).trunc\n        expected = truncate_to(start_datetime, 'day', tzinfo=tzinfo)\n        self.assertEqual(truncated, expected)\n"], "sample_224": ["def test_exists_subquery(self):\n    subquery = Book.objects.filter(publisher=OuterRef('pk'))\n    publishers = Publisher.objects.annotate(has_books=Exists(subquery)).filter(has_books=True)\n    self.assertEqual(publishers.count(), 4)\n    self.assertEqual(Publisher.objects.filter(name='Jonno\\'s House of Books').count(), 1)\n"], "sample_229": ["def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, range(10))\n"], "sample_230": ["def test_jsonfield_clean_invalid(self):\n    field = JSONField()\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n        field.clean('{\"key\": \"value\"')\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n        field.clean('{\"key\": \"value\",}')\n"], "sample_232": ["def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform('test', 'test')\n    with self.assertRaises(TypeError):\n        mixin.__init__(key_transform, 'test', 'test')\n\n    class TestKeyTransform(KeyTransform):\n            return 'TEST', []\n\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = TestKeyTransform('test', 'test')\n    mixin.__init__(key_transform, 'test', 'test')\n    self.assertIsInstance(mixin.lhs, KeyTextTransform)\n"], "sample_234": ["def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    self.assertNumbersEqual(qs1.union(qs2), range(10))\n"], "sample_242": ["def test_get_source_expressions(self):\n    lookup = CustomLookup(lhs='field', rhs='value')\n    self.assertEqual(lookup.get_source_expressions(), ['field'])\n\n    lookup = CustomLookup(lhs='field', rhs=Value('value'))\n    self.assertEqual(lookup.get_source_expressions(), ['field', Value('value')])\n"], "sample_247": ["def test_annotate_with_expression(self):\n    # Test that annotations can be made with expressions.\n    books = Book.objects.annotate(\n        is_expensive=Case(\n            When(price__gt=50, then=Value(True)),\n            default=Value(False),\n            output_field='BooleanField',\n        )\n    ).filter(is_expensive=True)\n    self.assertEqual(books.count(), 2)\n"], "sample_250": ["def test_date_format(self):\n    d = date(2009, 2, 12)\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'F'), 'February')\n    self.assertEqual(dateformat.format(d, 'E'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'M'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'b'), 'feb')\n    self.assertEqual(dateformat.format(d, 'N'), 'Feb.')\n    self.assertEqual(dateformat.format(d, 'm'), '02')\n    self.assertEqual(dateformat.format(d, 'n'), '2')\n    self.assertEqual(dateformat.format(d, 'd'), '12')\n    self.assertEqual(dateformat.format(d, 'j'), '12')\n    self.assertEqual(dateformat.format(d, 'D'), 'Thu')\n    self.assertEqual(dateformat.format(d, 'l'), 'Thursday')\n    self.assertEqual(dateformat.format(d, 'w'), '4')\n    self.assertEqual(dateformat.format(d, 'z'), '43')\n    self.assertEqual(dateformat.format(d, 't'), '28')\n    self.assertEqual(dateformat.format(d, 'L'), False)\n    self.assertEqual(dateformat.format(d, 'o'), '2009')\n    self.assertEqual(dateformat.format(d, 'W'), '7')\n    self.assertEqual(dateformat.format(d, 'S'), 'th')\n"], "sample_251": ["def test_expression_wrapper(self):\n    wrapper = ExpressionWrapper(Value(1), output_field=IntegerField())\n    self.assertEqual(wrapper.output_field.get_internal_type(), 'IntegerField')\n    self.assertEqual(wrapper.as_sql(connection), ('%s', [1]))\n"], "sample_252": ["def test_key_transform_factory(self):\n    factory = KeyTransformFactory('test_key')\n    key_transform = factory(JSONModel.data)\n    self.assertIsInstance(key_transform, KeyTransform)\n    self.assertEqual(key_transform.key_name, 'test_key')\n    self.assertEqual(key_transform.lhs, JSONModel.data)\n"], "sample_254": ["def test_inline_model_admin_get_queryset(self):\n    request = self.factory.get('/admin/holder/add/')\n    request.user = self.superuser\n\n    class MyInline(TabularInline):\n        model = Inner\n\n    inline = MyInline(Holder, admin_site)\n    queryset = inline.get_queryset(request)\n\n    self.assertEqual(queryset.model, Inner)\n    self.assertFalse(queryset.query.select_related)\n"], "sample_257": ["def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform('test', 'test')\n    with self.assertRaises(TypeError):\n        mixin.__init__(object())\n    mixin.__init__(key_transform)\n    self.assertEqual(mixin.lhs, key_transform)\n"], "sample_259": ["def test_prefetch_related_objects_with_to_attr(self):\n    authors = list(Author.objects.all())\n    prefetch_related_objects(authors, Prefetch('books', to_attr='all_books'))\n    with self.assertNumQueries(0):\n        for author in authors:\n            self.assertEqual(hasattr(author, 'all_books'), True)\n            self.assertEqual(isinstance(author.all_books, list), True)\n"], "sample_260": ["def test_optimizer_remove_index(self):\n    operations = [\n        migrations.AddIndex(\n            model_name='unicode',\n            index=models.Index(fields=['charfield'], name='my_index'),\n        ),\n        migrations.RemoveIndex(\n            model_name='unicode',\n            name='my_index',\n        ),\n    ]\n    self.assertOptimizesTo(operations, [])\n"], "sample_273": ["    def test_model_str_repr(self):\n        class MyModel(models.Model):\n            pass\n\n        model = MyModel()\n        self.assertEqual(str(model), 'MyModel object (None)')\n        self.assertEqual(repr(model), '<MyModel: MyModel object (None)>')\n"], "sample_275": ["    def test_queryset_values_list_slice(self):\n        # Test that slicing a queryset after calling values_list() returns the correct results.\n        person = Person.objects.create(name='John Doe')\n        Email.objects.create(person=person, email='john@example.com')\n        Email.objects.create(person=person, email='johndoe@example.com')\n\n        emails = Email.objects.values_list('email', flat=True)[1:]\n        self.assertEqual(emails[0], 'johndoe@example.com')\n"], "sample_286": ["    def test_fields_cache(self):\n        instance = Article()\n        self.assertEqual(instance._state.fields_cache, {})\n        instance._state.fields_cache['test'] = 'value'\n        self.assertEqual(instance._state.fields_cache, {'test': 'value'})\n"], "sample_287": ["def test_check_dependencies(self):\n    @override_settings(INSTALLED_APPS=['django.contrib.admin'])\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 3)\n        self.assertEqual(errors[0].id, 'admin.E401')\n        self.assertEqual(errors[1].id, 'admin.E405')\n        self.assertEqual(errors[2].id, 'admin.E406')\n\n    check_without_required_apps()\n\n    @override_settings(\n        INSTALLED_APPS=[\n            'django.contrib.admin',\n            'django.contrib.auth',\n            'django.contrib.contenttypes',\n            'django.contrib.messages',\n        ],\n        MIDDLEWARE=[\n            'django.contrib.sessions.middleware.SessionMiddleware',\n            'django.contrib.auth.middleware.AuthenticationMiddleware',\n            'django.contrib.messages.middleware.MessageMiddleware',\n        ],\n        TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [],\n            'APP_DIRS': True,\n            'OPTIONS': {\n                'context_processors': [\n                    'django.template.context_processors.request',\n                    'django.contrib.auth.context_processors.auth',\n                    'django.contrib.messages.context_processors.messages',\n                ],\n            },\n        }],\n    )\n        errors = checks.run_checks()\n        self.assertEqual(errors, [])\n\n    check_with_required_apps()\n"], "sample_288": ["def test_key_transform_factory(self):\n    factory = KeyTransformFactory('test_key')\n    transform = factory(JSONModel.data)\n    self.assertIsInstance(transform, KeyTransform)\n    self.assertEqual(transform.key_name, 'test_key')\n    self.assertEqual(transform.lhs, JSONModel.data)\n"], "sample_305": ["def test_lookup_with_bilateral_transforms(self):\n    # Create a bilateral transform that converts the lhs and rhs to lowercase\n    class LowerCase(Transform):\n        lookup_name = 'lower'\n        bilateral = True\n\n            lhs, params = compiler.compile(self.lhs)\n            return \"LOWER(%s)\" % lhs, params\n\n    with register_lookup(Field, LowerCase):\n        # Use the bilateral transform in a lookup\n        books = Book.objects.filter(name__lower='the definitive guide to django')\n        self.assertEqual(len(books), 1)\n        self.assertEqual(books[0].name, 'The Definitive Guide to Django: Web Development Done Right')\n"], "sample_307": ["def test_date_format(self):\n    d = date(2009, 2, 12)\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'm'), '02')\n    self.assertEqual(dateformat.format(d, 'n'), '2')\n    self.assertEqual(dateformat.format(d, 'M'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'b'), 'feb')\n    self.assertEqual(dateformat.format(d, 'd'), '12')\n    self.assertEqual(dateformat.format(d, 'j'), '12')\n    self.assertEqual(dateformat.format(d, 'D'), 'Thu')\n    self.assertEqual(dateformat.format(d, 'l'), 'Thursday')\n    self.assertEqual(dateformat.format(d, 'S'), 'th')\n    self.assertEqual(dateformat.format(d, 'w'), '4')\n    self.assertEqual(dateformat.format(d, 'N'), 'February')\n    self.assertEqual(dateformat.format(d, 'o'), '2009')\n    self.assertEqual(dateformat.format(d, 't'), '28')\n"], "sample_308": ["def test_date_format(self):\n    d = date(2009, 2, 12)\n    self.assertEqual(dateformat.format(d, 'b'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'c'), '2009-02-12')\n    self.assertEqual(dateformat.format(d, 'd'), '12')\n    self.assertEqual(dateformat.format(d, 'D'), 'Thu')\n    self.assertEqual(dateformat.format(d, 'E'), 'February')\n    self.assertEqual(dateformat.format(d, 'F'), 'February')\n    self.assertEqual(dateformat.format(d, 'I'), '')\n    self.assertEqual(dateformat.format(d, 'j'), '12')\n    self.assertEqual(dateformat.format(d, 'l'), 'Thursday')\n    self.assertEqual(dateformat.format(d, 'L'), False)\n    self.assertEqual(dateformat.format(d, 'm'), '02')\n    self.assertEqual(dateformat.format(d, 'M'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'n'), '2')\n    self.assertEqual(dateformat.format(d, 'N'), 'February')\n    self.assertEqual(dateformat.format(d, 'o'), '2009')\n    self.assertEqual(dateformat.format(d, 'r'), '')\n    self.assertEqual(dateformat.format(d, 'S'), 'th')\n    self.assertEqual(dateformat.format(d, 't'), '28')\n    self.assertEqual(dateformat.format(d, 'U'), '1234406400')\n    self.assertEqual(dateformat.format(d, 'w'), '4')\n    self.assertEqual(dateformat.format(d, 'W'), '7')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'z'), '43')\n"], "sample_327": ["def test_jsonfield_clean(self):\n    field = JSONField(encoder=DjangoJSONEncoder)\n    self.assertIsNone(field.clean(None))\n    self.assertEqual(field.clean('{\"a\": 1}'), {\"a\": 1})\n    with self.assertRaises(ValidationError):\n        field.clean('{ invalid }')\n"], "sample_328": ["def test_bulk_update(self):\n    notes = self.notes\n    Note.objects.bulk_update(notes, ['note'])\n    for note in notes:\n        note.refresh_from_db()\n        self.assertEqual(note.note, str(note.pk))\n    # Test update of one field on instances with different pk values.\n    notes[0].note = 'changed'\n    notes[1].note = 'updated'\n    Note.objects.bulk_update(notes[:2], ['note'])\n    for note in notes[:2]:\n        note.refresh_from_db()\n        self.assertEqual(note.note, note.note)\n    # Test updating model with only primary key.\n    CustomPk.objects.create(id=1)\n    CustomPk.objects.create(id=2)\n    CustomPk.objects.bulk_update(CustomPk.objects.all(), [])\n"], "sample_329": ["    def test_serializer_factory(self):\n        # Test that serializer_factory can handle various types of values\n        values = [\n            1,\n            'a',\n            b'bytes',\n            True,\n            None,\n            [1, 2, 3],\n            (1, 2, 3),\n            {'a': 1, 'b': 2},\n            set([1, 2, 3]),\n            frozenset([1, 2, 3]),\n            datetime.datetime(2022, 1, 1),\n            datetime.date(2022, 1, 1),\n            datetime.time(12, 0),\n            decimal.Decimal('3.14'),\n            uuid.UUID('01234567-89ab-cdef-0123-456789abcdef'),\n            re.compile(r'^test$'),\n            pathlib.Path('/path/to/file'),\n            os.PathLike('/path/to/file'),\n            TextEnum.A,\n            IntEnum.A,\n            BinaryEnum.A,\n            functools.partial(int, base=2),\n            DeconstructibleInstances(),\n            Money('3.14'),\n        ]\n\n        for value in values:\n            try:\n                serializer_factory(value)\n            except ValueError as e:\n                self.fail(f\"serializer_factory failed for {value}: {e}\")\n"], "sample_335": ["def test_decimalfield_uses_correct_step(self):\n    f = DecimalField(decimal_places=2)\n    self.assertEqual(f.widget_attrs(Widget())['step'], '0.01')\n    f = DecimalField(decimal_places=4)\n    self.assertEqual(f.widget_attrs(Widget())['step'], '0.0001')\n    f = DecimalField()\n    self.assertEqual(f.widget_attrs(Widget())['step'], 'any')\n"], "sample_350": ["def test_union(self):\n    qs1 = Number.objects.filter(num__lte=3)\n    qs2 = Number.objects.filter(num__gte=7)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 7, 8, 9])\n"], "sample_356": ["def test_alter_index_together(self):\n    \"\"\"Tests index_together changes are correctly detected.\"\"\"\n    # Make state\n    before = self.make_project_state([self.book_foo_together])\n    after = self.make_project_state([self.book_foo_together_2])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    # Right operations?\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterIndexTogether\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', index_together={('title', 'author')})\n"], "sample_370": ["def test_prefetch_related_with_m2m_through_model(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        authors = Author.objects.prefetch_related('books').all()\n        for author in authors:\n            list(author.books.all())\n\n    self.assertEqual(len(captured_queries), 3)\n    self.assertWhereContains(captured_queries[1]['sql'], 'author_id')\n    self.assertWhereContains(captured_queries[2]['sql'], 'book_id IN')\n"], "sample_374": ["def test_prefetch_related_objects_with_to_attr(self):\n    qs = Book.objects.prefetch_related(Prefetch('authors', to_attr='book_authors'))\n    with self.assertNumQueries(2):\n        books = list(qs)\n        for book in books:\n            self.assertTrue(hasattr(book, 'book_authors'))\n            self.assertEqual(list(book.book_authors), list(book.authors.all()))\n"], "sample_375": ["def test_state_clone(self):\n    \"\"\"Test that cloning a ProjectState results in an equivalent state.\"\"\"\n    project_state = ProjectState()\n    new_state = project_state.clone()\n    self.assertEqual(project_state.models, new_state.models)\n    self.assertEqual(project_state.real_apps, new_state.real_apps)\n    self.assertEqual(project_state.apps, new_state.apps)\n    self.assertEqual(project_state.is_delayed, new_state.is_delayed)\n"], "sample_378": ["def test_bulk_update(self):\n    notes = Note.objects.all()\n    self.assertEqual(notes.count(), 10)\n    Note.objects.bulk_update(notes, ['note'])\n    self.assertEqual(notes.count(), 10)\n    for note in notes:\n        note.note = 'x'\n    Note.objects.bulk_update(notes, ['note'])\n    self.assertEqual(Note.objects.filter(note='x').count(), 10)\n"], "sample_384": ["def test_bulk_update(self):\n    notes = Note.objects.all()\n    notes.bulk_update([{\"id\": note.id, \"note\": \"x\"} for note in notes])\n    self.assertEqual(Note.objects.filter(note=\"x\").count(), 10)\n"], "sample_387": ["    def test_charfield_override(self):\n        # Override the default CharField widget to be a TextInput with a specific size\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {\n                CharField: {\"widget\": forms.TextInput(attrs={\"size\": \"20\"})}\n            }\n\n        ma = MyModelAdmin(Band, admin.site)\n        ff = ma.formfield_for_dbfield(Band._meta.get_field(\"name\"), request=None)\n\n        self.assertIsInstance(ff.widget, forms.TextInput)\n        self.assertEqual(ff.widget.attrs[\"size\"], \"20\")\n"], "sample_391": ["def test_create_model_options(self):\n    operations = [\n        migrations.CreateModel(\n            name=\"TestModel\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n            options={\"verbose_name\": \"Test Model\"},\n        ),\n        migrations.AlterModelOptions(name=\"TestModel\", options={\"verbose_name\": \"New Test Model\"}),\n    ]\n    expected = [\n        migrations.CreateModel(\n            name=\"TestModel\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n            options={\"verbose_name\": \"New Test Model\"},\n        )\n    ]\n    self.assertOptimizesTo(operations, expected)\n"], "sample_392": ["def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform(\"test\", \"test\")\n    with self.assertRaises(TypeError):\n        mixin.__init__(key_transform, \"test\", \"test\")\n\n    class TestKeyTransform(KeyTransform):\n            return \"TEST\", []\n\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = TestKeyTransform(\"test\", \"test\")\n    mixin.__init__(key_transform, \"test\", \"test\")\n    self.assertIsInstance(mixin.lhs, KeyTextTransform)\n"], "sample_414": ["    def test_charfield_with_textareawidget_override(self):\n        model = Album\n        fieldname = \"name\"\n        widgetclass = widgets.AdminTextareaWidget\n\n        self.assertFormfield(\n            model, fieldname, widgetclass, formfield_overrides={CharField: {\"widget\": widgetclass}}\n        )\n"], "sample_415": ["    def test_check_constraint(self):\n        class Model(models.Model):\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gte=0), name='value_gte_0')\n                ]\n\n        with atomic():\n            Model.objects.create(value=1)\n            with self.assertRaises(IntegrityError):\n                Model.objects.create(value=-1)\n"], "sample_417": ["    def test_floatformat_with_arg(self):\n        out = self.engine.render_to_string(\n            \"{% autoescape off %}{{ a|floatformat:3 }} {{ b|floatformat:2 }}{% endautoescape %}\",\n            {\"a\": 34.23234, \"b\": 34.00000},\n        )\n        self.assertEqual(out, \"34.232 34.00\")\n"], "sample_418": ["    def test_addslashes(self):\n        output = self.engine.render_to_string(\"string_filter01\")\n        self.assertEqual(output, \"Hello\\\\, World\\\\!\")\n"], "sample_419": ["def test_formset_management_form_required_fields(self):\n    formset = self.make_choiceformset()\n    management_form = ManagementForm(\n        {\n            TOTAL_FORM_COUNT: \"1\",\n        },\n        prefix=\"choices\",\n    )\n    self.assertFalse(management_form.is_valid())\n    self.assertIn(INITIAL_FORM_COUNT, management_form.errors)\n    self.assertNotIn(TOTAL_FORM_COUNT, management_form.errors)\n\n    management_form = ManagementForm(\n        {\n            INITIAL_FORM_COUNT: \"0\",\n        },\n        prefix=\"choices\",\n    )\n    self.assertFalse(management_form.is_valid())\n    self.assertIn(TOTAL_FORM_COUNT, management_form.errors)\n    self.assertNotIn(INITIAL_FORM_COUNT, management_form.errors)\n"], "sample_421": ["def test_case_expression_with_generic_ip_address_field(self):\n    case_test_model = CaseTestModel.objects.create(\n        integer=1, integer2=1, string=\"1\", ip_address=\"192.168.1.1\"\n    )\n    qs = CaseTestModel.objects.annotate(\n        ip_address_type=Case(\n            When(ip_address__startswith=\"192.168.\", then=Value(\"private\")),\n            default=Value(\"public\"),\n            output_field=GenericIPAddressField(),\n        )\n    )\n    self.assertEqual(qs.get(pk=case_test_model.pk).ip_address_type, \"private\")\n"], "sample_422": ["def test_forward_many_to_one_descriptor_get_prefetch_queryset(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        authors = Author.objects.prefetch_related(\"first_book\").all()\n        list(authors)\n\n    self.assertEqual(len(captured_queries), 2)\n    query_sql = captured_queries[1][\"sql\"]\n    self.assertWhereContains(query_sql, \"book_id IN\")\n"], "sample_425": ["    def test_serialize_float(self):\n        value = 1.5\n        serializer = FloatSerializer(value)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, repr(value))\n        self.assertEqual(imports, set())\n"], "sample_427": ["def test_formset_management_form_validation(self):\n    # Test that the ManagementForm validates correctly\n    formset = self.make_choiceformset(\n        formset_data=[(\"Choice 1\", 10), (\"Choice 2\", 20)],\n        total_forms=2,\n        initial_forms=0,\n        max_num_forms=5,\n        min_num_forms=0,\n    )\n    management_form = formset.management_form\n    self.assertTrue(management_form.is_valid())\n\n    # Test that an invalid TOTAL_FORMS value causes a ValidationError\n    data = management_form.data.copy()\n    data[management_form.add_prefix(TOTAL_FORM_COUNT)] = \"abc\"\n    management_form.data = data\n    self.assertFalse(management_form.is_valid())\n    self.assertEqual(len(management_form.errors), 1)\n    self.assertIn(TOTAL_FORM_COUNT, management_form.errors)\n\n    # Test that an invalid INITIAL_FORMS value causes a ValidationError\n    data = management_form.data.copy()\n    data[management_form.add_prefix(INITIAL_FORM_COUNT)] = \"abc\"\n    management_form.data = data\n    self.assertFalse(management_form.is_valid())\n    self.assertEqual(len(management_form.errors), 1)\n    self.assertIn(INITIAL_FORM_COUNT, management_form.errors)\n\n    # Test that an invalid MAX_NUM_FORMS value causes a ValidationError\n    data = management_form.data.copy()\n    data[management_form.add_prefix(MAX_NUM_FORM_COUNT)] = \"abc\"\n    management_form.data = data\n    self.assertFalse(management_form.is_valid())\n    self.assertEqual(len(management_form.errors), 1)\n    self.assertIn(MAX_NUM_FORM_COUNT, management_form.errors)\n\n    # Test that an invalid MIN_NUM_FORMS value causes a ValidationError\n    data = management_form.data.copy()\n    data[management_form.add_prefix(MIN_NUM_FORM_COUNT)] = \"abc\"\n    management_form.data = data\n    self.assertFalse(management_form.is_valid())\n    self.assertEqual(len(management_form.errors), 1)\n    self.assertIn(MIN_NUM_FORM_COUNT, management_form.errors)\n"], "sample_431": ["    def test_deferred_fields(self):\n        article = Article.objects.defer(\"headline\").get(id=1)\n        self.assertEqual(article.get_deferred_fields(), {\"headline\"})\n"], "sample_432": ["def test_changelist_view_search(self):\n    modeladmin = BandAdmin(Band, admin.site)\n    request = self._mocked_authenticated_request(\"/admin/\", self.superuser)\n    changelist = modeladmin.get_changelist_instance(request)\n    # Make sure there are no messages before the request\n    request._messages = CookieStorage(FakeRequest())\n    response = modeladmin.changelist_view(request, {\"q\": \"s\"})\n    # One message for empty search result\n    self.assertEqual(len(request._messages), 1)\n    self.assertEqual(response.status_code, 200)\n"], "sample_440": ["def test_bulk_create_conflict_update_fields(self):\n    Country.objects.create(name=\"United States of America\", iso_two_letter=\"US\")\n    countries = [\n        Country(name=\"The Netherlands\", iso_two_letter=\"NL\"),\n        Country(name=\"Germany\", iso_two_letter=\"DE\"),\n        Country(name=\"United States of America\", iso_two_letter=\"US\"),\n    ]\n    with self.assertRaises(IntegrityError):\n        Country.objects.bulk_create(countries)\n    Country.objects.bulk_create(\n        countries, update_conflicts=True, update_fields=[\"name\"], unique_fields=[\"iso_two_letter\"]\n    )\n    self.assertEqual(Country.objects.get(iso_two_letter=\"US\").name, \"United States of America\")\n"], "sample_446": ["    def test_floatformat_with_arg(self):\n        with localcontext() as ctx:\n            ctx.prec = 10\n            out = self.engine.render_to_string(\n                \"{% autoescape off %}{{ a|floatformat:3 }} {{ b|floatformat:4 }}\"\n                \"{% endautoescape %}\",\n                {\"a\": Decimal(\"34.23234\"), \"b\": Decimal(\"34.26000\")},\n            )\n            self.assertEqual(out, \"34.232 34.2600\")\n"], "sample_447": ["def test_expression_wrapper(self):\n    wrapper = ExpressionWrapper(Value(1), output_field=IntegerField())\n    self.assertEqual(wrapper.output_field.get_internal_type(), \"IntegerField\")\n    self.assertEqual(wrapper.as_sql(None, None), (\"1\", []))\n"], "sample_451": ["def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n    self.assertIn(\"**Context**\", body)\n    self.assertIn(\":template:`myapp/my_template.html` (DESCRIPTION)\", body)\n    self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n"], "sample_454": ["    def test_create_exclusion_constraint(self):\n        constraint = ExclusionConstraint(\n            name=\"test_exclusion_constraint\",\n            expressions=[(\"field1\", \"=\"), (\"field2\", \"<>\")],\n        )\n        sql = constraint.create_sql(Product, connection.schema_editor())\n        self.assertRegex(sql, r\"ALTER TABLE .* ADD CONSTRAINT test_exclusion_constraint EXCLUDE USING GIST\")\n"], "sample_455": ["    def test_check_constraint(self):\n        class CheckModel(models.Model):\n            value = models.IntegerField()\n            check_constraint = models.CheckConstraint(\n                check=models.Q(value__gte=0), name=\"check_value_gte_0\"\n            )\n\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(CheckModel)\n\n        constraints = get_constraints(CheckModel._meta.db_table)\n        self.assertIn((\"check_value_gte_0\", \"CHECK\"), constraints.items())\n\n        # Try to create a model instance that violates the constraint.\n        with self.assertRaises(IntegrityError):\n            CheckModel.objects.create(value=-1)\n\n        # Create a model instance that does not violate the constraint.\n        instance = CheckModel.objects.create(value=1)\n        self.assertEqual(instance.value, 1)\n\n        # Update the model instance to violate the constraint.\n        instance.value = -1\n        with self.assertRaises(IntegrityError):\n            instance.save()\n\n        # Delete the model instance.\n        instance.delete()\n\n        with connection.schema_editor() as schema_editor:\n            schema_editor.delete_model(CheckModel)\n"], "sample_457": ["    def test_check_constraint(self):\n        class Product(models.Model):\n            name = models.CharField(max_length=255)\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(price__gt=0), name=\"price_positive\"\n                    )\n                ]\n\n        with atomic():\n            Product.objects.create(name=\"Product 1\", price=10.99)\n            try:\n                Product.objects.create(name=\"Product 2\", price=-5.99)\n            except IntegrityError as e:\n                self.assertIn(\"price_positive\", str(e))\n            else:\n                self.fail(\"IntegrityError not raised\")\n"], "sample_458": ["    def test_floatformat_with_arg(self):\n        out = self.engine.render_to_string(\n            \"{% autoescape off %}{{ a|floatformat:3 }} {{ b|floatformat:2 }}{% endautoescape %}\",\n            {\"a\": 34.23234, \"b\": 34.00000},\n        )\n        self.assertEqual(out, \"34.232 34.00\")\n"], "sample_459": ["    def testBigIntegerFieldOverflow(self):\n        value = self.documented_range[1] + 1\n        with self.assertRaises(IntegrityError):\n            self.model.objects.create(value=value)\n"], "sample_461": ["def test_urlfield_uses_default_scheme(self):\n    f = URLField()\n    self.assertEqual(f.clean('www.example.com'), 'http://www.example.com')\n"], "sample_462": ["def test_choicefield_1(self):\n    f = ChoiceField(choices=[('1', '1'), ('2', '2'), ('3', '3')])\n    self.assertWidgetRendersTo(f, '<select name=\"f\"><option value=\"1\">1</option><option value=\"2\">2</option><option value=\"3\">3</option></select>')\n    with self.assertRaisesMessage(ValidationError, \"'4' is not one of the available choices.\"):\n        f.clean('4')\n"], "sample_465": ["def test_get_content_type_for_model(self):\n    content_type = get_content_type_for_model(Band)\n    self.assertEqual(content_type.model_class(), Band)\n    self.assertEqual(content_type.app_label, \"admin_views\")\n    self.assertEqual(content_type.model, \"band\")\n"], "sample_467": ["def test_use_required_attribute(self):\n    widget = SelectDateWidget()\n    self.assertTrue(widget.use_required_attribute(None))\n    self.assertFalse(widget.use_required_attribute(date.today()))\n"], "sample_469": ["def test_get_field_names_from_opts(self):\n    field_names = get_field_names_from_opts(Author._meta)\n    self.assertIn(\"name\", field_names)\n    self.assertIn(\"age\", field_names)\n    self.assertIn(\"friends\", field_names)\n\n    # Check that the method returns a set of unique field names.\n    self.assertEqual(len(field_names), len(set(field_names)))\n"], "sample_468": ["def test_context_get_method(self):\n    context = Context({'a': 1, 'b': 2})\n    self.assertEqual(context.get('a'), 1)\n    self.assertEqual(context.get('b'), 2)\n    self.assertIsNone(context.get('c'))\n    self.assertEqual(context.get('c', 3), 3)\n"], "sample_471": ["    def test_charfield_1(self):\n        f = CharField()\n        self.assertEqual(f.clean('hello'), 'hello')\n        self.assertEqual(f.clean('  hello  '), '  hello  ')\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean('')\n        f = CharField(required=False)\n        self.assertEqual(f.clean(''), '')\n        f = CharField(empty_value='hello')\n        self.assertEqual(f.clean(''), 'hello')\n        f = CharField(min_length=5)\n        self.assertEqual(f.clean('hello'), 'hello')\n        with self.assertRaisesMessage(ValidationError, \"Ensure this value has at least 5 characters (it has 4).\"):\n            f.clean('hell')\n        f = CharField(max_length=5)\n        self.assertEqual(f.clean('hello'), 'hello')\n        with self.assertRaisesMessage(ValidationError, \"Ensure this value has at most 5 characters (it has 6).\"):\n            f.clean('helloo')\n"], "sample_475": ["    def test_list_display_must_be_a_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            list_display = \"foo\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'list_display' must be a list or tuple.\",\n            id=\"admin.E107\",\n        )\n"], "sample_479": ["def test_rename_index(self):\n    operations = [\n        migrations.RenameIndex(\n            model_name=\"mymodel\",\n            old_name=\"old_index\",\n            new_name=\"new_index\",\n        ),\n        migrations.RenameIndex(\n            model_name=\"mymodel\",\n            old_name=\"new_index\",\n            new_name=\"final_index\",\n        ),\n    ]\n    expected = [\n        migrations.RenameIndex(\n            model_name=\"mymodel\",\n            old_name=\"old_index\",\n            new_name=\"final_index\",\n        )\n    ]\n    self.assertOptimizesTo(operations, expected)\n"], "sample_480": ["def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform(\"test\", \"test\")\n    with self.assertRaises(TypeError):\n        mixin.__init__(key_transform, \"test\", \"test\")\n\n    class TestKeyTransform(KeyTransform):\n            return \"TEST\", []\n\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = TestKeyTransform(\"test\", \"test\")\n    lookup = mixin(key_transform, \"test\")\n    self.assertIsInstance(lookup.lhs, KeyTextTransform)\n"], "sample_481": ["    def test_stringformat_float(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 3.14})\n        self.assertEqual(output, \"3.140000E+00\")\n"], "sample_482": ["    def test_stringformat(self):\n        output = self.engine.render_to_string(\n            \"stringformat_valid\", {\"a\": 12345.6789}, debug=False\n        )\n        self.assertEqual(output, \"1.235e+04\")\n\n        output = self.engine.render_to_string(\n            \"stringformat_invalid\", {\"a\": 12345.6789}, debug=False\n        )\n        self.assertEqual(output, \"\")\n\n        output = self.engine.render_to_string(\n            \"stringformat_empty\", {\"a\": 12345.6789}, debug=False\n        )\n        self.assertEqual(output, \"\")\n"], "sample_486": ["def test_inline_formset_factory_with_uuid_pk(self):\n    ChildFormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n    parent = UUIDPKParent.objects.create()\n    formset = ChildFormSet(instance=parent)\n    self.assertEqual(formset.model, UUIDPKChild)\n    self.assertEqual(formset.fk, UUIDPKChild._meta.get_field(\"parent\"))\n"], "sample_487": ["    def test_list_display_must_be_a_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            list_display = \"foo\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'list_display' must be a list or tuple.\",\n            id=\"admin.E107\",\n        )\n"], "sample_489": ["def test_bulk_create_conflict_update_fields(self):\n    Country.objects.create(name=\"United States of America\", iso_two_letter=\"US\")\n    data = [\n        Country(name=\"The Netherlands\", iso_two_letter=\"NL\"),\n        Country(name=\"Germany\", iso_two_letter=\"DE\"),\n        Country(name=\"United States of America\", iso_two_letter=\"US\"),\n    ]\n    with self.assertRaises(IntegrityError):\n        Country.objects.bulk_create(data)\n\n    # Test update_fields.\n    Country.objects.bulk_create(\n        data, update_conflicts=True, update_fields=[\"name\"], unique_fields=[\"iso_two_letter\"]\n    )\n    self.assertEqual(Country.objects.get(iso_two_letter=\"US\").name, \"United States of America\")\n\n    # Test update_fields with non-concrete fields.\n    with self.assertRaises(ValueError):\n        Country.objects.bulk_create(\n            data,\n            update_conflicts=True,\n            update_fields=[\"non_concrete_field\"],\n            unique_fields=[\"iso_two_letter\"],\n        )\n\n    # Test update_fields with primary key fields.\n    with self.assertRaises(ValueError):\n        Country.objects.bulk_create(\n            data,\n            update_conflicts=True,\n            update_fields=[\"id\"],\n            unique_fields=[\"iso_two_letter\"],\n        )\n"], "sample_490": ["    def test_check_constraint(self):\n        class Product(models.Model):\n            name = models.CharField(max_length=255)\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(price__gt=0), name=\"price_gt_zero\"\n                    )\n                ]\n\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(name=\"Test\", price=-1)\n\n        product = Product.objects.create(name=\"Test\", price=1)\n        self.assertEqual(product.price, 1)\n"], "sample_493": ["def test_filtering_on_annotate_with_subquery(self):\n    subquery = Book.objects.filter(rating__gt=3.0).values(\"publisher\")\n    books = Publisher.objects.annotate(has_good_books=Exists(subquery)).filter(\n        has_good_books=True\n    )\n    self.assertEqual(books.count(), 4)\n"], "sample_494": ["    def test_serializer_factory(self):\n        # Test that serializer_factory can handle various types of values.\n        values = [\n            1,\n            \"hello\",\n            3.14,\n            True,\n            None,\n            [1, 2, 3],\n            (1, 2, 3),\n            {\"a\": 1, \"b\": 2},\n            set([1, 2, 3]),\n            frozenset([1, 2, 3]),\n            datetime.datetime(2022, 1, 1),\n            datetime.date(2022, 1, 1),\n            datetime.time(12, 0),\n            decimal.Decimal(\"3.14\"),\n            uuid.UUID(\"01234567-89ab-cdef-0123-456789abcdef\"),\n            re.compile(r\"hello\"),\n            pathlib.Path(\"/path/to/somewhere\"),\n            os.PathLike(\"/path/to/somewhere\"),\n            TextEnum.A,\n            IntEnum.A,\n            IntFlagEnum.A,\n            BinaryEnum.A,\n            functools.partial(max, 1, 2),\n            DeconstructibleInstances(),\n            Money(\"3.14\"),\n            SettingsReference(\"SETTING_NAME\"),\n            models.Field(),\n            models.manager.BaseManager(),\n            migrations.RunPython(lambda apps, schema_editor: None),\n        ]\n\n        for value in values:\n            try:\n                serializer_factory(value)\n            except ValueError as e:\n                self.fail(f\"serializer_factory failed for {value}: {e}\")\n"], "sample_497": ["def test_axis_get_tick_space():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    assert ax.xaxis.get_tick_space() > 0\n    assert ax.yaxis.get_tick_space() > 0\n    ax.xaxis.set_major_locator(mticker.NullLocator())\n    ax.yaxis.set_major_locator(mticker.NullLocator())\n    assert ax.xaxis.get_tick_space() == 2**31 - 1\n    assert ax.yaxis.get_tick_space() == 2**31 - 1\n"], "sample_498": ["def test_legend_handler_map():\n    fig, ax = plt.subplots()\n    p1, = ax.plot([1, 2, 3])\n    p2, = ax.plot([4, 5, 6])\n\n    # Test that legend handler map is used correctly\n    with rc_context({'legend.handlelength': 10}):\n        ax.legend([p1, p2], ['Line 1', 'Line 2'],\n                  handler_map={mlines.Line2D: HandlerTuple(ndivide=None)})\n\n    # Test that custom handler map updates the default handler map\n    custom_handler_map = {mlines.Line2D: HandlerTuple(ndivide=None)}\n    ax.legend([p1, p2], ['Line 1', 'Line 2'],\n              handler_map=custom_handler_map)\n    assert mlegend.Legend.get_default_handler_map()[mlines.Line2D] == custom_handler_map[mlines.Line2D]\n\n    # Test that setting handler map to None resets it to default\n    ax.legend([p1, p2], ['Line 1', 'Line 2'], handler_map=None)\n    assert mlegend.Legend.get_default_handler_map() == mlegend.Legend._default_handler_map\n\n    plt.close(fig)\n"], "sample_501": ["def test_legend_handler_map():\n    fig, ax = plt.subplots()\n    p1, = ax.plot([1, 2, 3], label='Line 1')\n    p2, = ax.plot([4, 5, 6], label='Line 2')\n\n    # Create a custom legend handler map\n    custom_handler_map = {mlines.Line2D: HandlerTuple(ndivide=None)}\n\n    # Pass the custom handler map to the legend function\n    ax.legend(handles=[p1, p2], handler_map=custom_handler_map)\n\n    # Check that the custom handler map is used\n    legend = ax.get_legend()\n    assert legend.get_legend_handler_map()[mlines.Line2D] == custom_handler_map[mlines.Line2D]\n"], "sample_503": ["def test_line2d_update_from():\n    line1 = mlines.Line2D([0, 1], [0, 1])\n    line1.set_color('red')\n    line1.set_linestyle('--')\n    line1.set_linewidth(2)\n    line1.set_marker('o')\n    line1.set_markersize(10)\n\n    line2 = mlines.Line2D([0, 1], [0, 1])\n    line2.update_from(line1)\n\n    assert line2.get_color() == 'red'\n    assert line2.get_linestyle() == '--'\n    assert line2.get_linewidth() == 2\n    assert line2.get_marker() == 'o'\n    assert line2.get_markersize() == 10\n"], "sample_505": ["def test_date_ticker_factory():\n    # Test that date_ticker_factory returns the correct locator and formatter\n    # for different spans of time.\n    spans = [0, 1 / 24, 1, 7, 30, 365]\n    for span in spans:\n        locator, formatter = mdates.date_ticker_factory(span)\n        assert isinstance(locator, mticker.Locator)\n        assert isinstance(formatter, mticker.Formatter)\n"], "sample_506": ["def test_spine_position(fig_test, fig_ref):\n    fig_test.subplots()\n    fig_test.axes[0].spines['left'].set_position(('outward', 10))\n    fig_test.axes[0].spines['right'].set_position(('axes', 0.8))\n    fig_test.axes[0].spines['top'].set_position(('data', 0.5))\n\n    fig_ref.subplots()\n    fig_ref.axes[0].spines['left'].set_position(('outward', 10))\n    fig_ref.axes[0].spines['right'].set_position(('axes', 0.8))\n    fig_ref.axes[0].spines['top'].set_position(('data', 0.5))\n    fig_ref.axes[0].set_xlim([0, 1])\n    fig_ref.axes[0].set_ylim([0, 1])\n"], "sample_511": ["def test_subplot2grid():\n    fig = plt.figure()\n    ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3)\n    ax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)\n    ax3 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n    ax4 = plt.subplot2grid((3, 3), (2, 0))\n    ax5 = plt.subplot2grid((3, 3), (2, 1))\n\n    assert ax1.get_position().width > ax2.get_position().width\n    assert ax3.get_position().height > ax4.get_position().height\n    assert ax5.get_position().x0 > ax4.get_position().x0\n\n    plt.close(fig)\n"], "sample_512": ["def test_pyplot_switch_backend():\n    # Test switching backends with clf()\n    plt.switch_backend('Agg')\n    fig = plt.figure()\n    assert len(plt.get_fignums()) == 1\n    plt.clf()\n    assert len(plt.get_fignums()) == 0\n    plt.switch_backend('pdf')\n    fig = plt.figure()\n    assert len(plt.get_fignums()) == 1\n    plt.close('all')\n    assert len(plt.get_fignums()) == 0\n"], "sample_516": ["def test_pdf_pages_metadata():\n    metadata = {\n        'Title': 'My title',\n        'Author': 'Me',\n        'Subject': 'Testing PDF metadata',\n        'Keywords': 'matplotlib,testing,pdf',\n        'Creator': 'Matplotlib',\n        'Producer': 'Matplotlib',\n        'CreationDate': datetime.datetime(2022, 1, 1),\n        'ModDate': datetime.datetime(2022, 1, 2),\n        'Trapped': 'True'\n    }\n\n    with NamedTemporaryFile(suffix='.pdf') as tmp:\n        with PdfPages(tmp.name, metadata=metadata) as pdf:\n            fig = plt.figure()\n            pdf.savefig(fig)\n\n        with open(tmp.name, 'rb') as f:\n            pdf_content = f.read()\n\n    for key, value in metadata.items():\n        if isinstance(value, datetime.datetime):\n            value = value.strftime('/D:%Y%m%d%H%M%S')\n        elif key == 'Trapped':\n            value = '/True'\n        assert f'/{key} {value}' in pdf_content.decode('utf-8')\n"], "sample_517": ["def test_text_get_window_extent():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    renderer = fig.canvas.get_renderer()\n    extent = text.get_window_extent(renderer)\n    assert isinstance(extent, mtransforms.Bbox)\n    assert extent.width > 0\n    assert extent.height > 0\n"], "sample_518": ["def test_fancyarrowpatch_get_path():\n    # Test that FancyArrowPatch.get_path returns the correct path.\n    fig, ax = plt.subplots()\n    arrow = FancyArrowPatch((0.1, 0.1), (0.8, 0.8),\n                            mutation_scale=20, fc=\"none\", color='black')\n    ax.add_patch(arrow)\n    path = arrow.get_path()\n    assert isinstance(path, mpath.Path)\n    assert len(path.vertices) > 2\n"], "sample_519": ["def test_figure_layout_engine():\n    fig = Figure()\n    assert fig.get_layout_engine() is None\n\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n\n    with pytest.raises(ValueError):\n        fig.set_layout_engine('invalid')\n\n    fig.set_layout_engine(None)\n    assert fig.get_layout_engine() is None\n"], "sample_521": ["def test_art3d_line_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Create a 2D line collection\n    xs = np.array([0, 1])\n    ys = np.array([0, 1])\n    line = LineCollection([np.column_stack([xs, ys])])\n\n    # Convert the line collection to 3D\n    art3d.line_collection_2d_to_3d(line, zs=1)\n\n    # Add the line collection to the axes\n    ax.add_collection3d(line)\n\n    # Check that the line collection has been correctly converted\n    assert isinstance(line, art3d.Line3DCollection)\n    assert line.get_zs() == [1]\n"], "sample_531": ["def test_figure_layout_engine():\n    fig = Figure()\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n\n    fig.set_layout_engine(None)\n    assert fig.get_layout_engine() is None\n\n    with pytest.raises(ValueError):\n        fig.set_layout_engine('invalid')\n\n    # Test that setting layout engine to None removes the current engine\n    fig.set_layout_engine('constrained')\n    fig.set_layout_engine(None)\n    assert fig.get_layout_engine() is None\n"], "sample_532": ["def test_contour_corner_mask():\n    # Test that corner masking works correctly.\n    x = np.arange(10)\n    y = np.arange(10)\n    X, Y = np.meshgrid(x, y)\n    Z = np.ma.array(X + Y, mask=((X + Y) < 5))\n    fig, ax = plt.subplots()\n    ax.contourf(X, Y, Z, levels=[0, 10], corner_mask=True)\n    ax.set_xlim(-1, 10)\n    ax.set_ylim(-1, 10)\n"], "sample_542": ["def test_text_get_window_extent():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    renderer = fig.canvas.get_renderer()\n    extent = text.get_window_extent(renderer)\n    assert isinstance(extent, mtransforms.Bbox)\n    assert extent.width > 0\n    assert extent.height > 0\n"], "sample_545": ["def test_figure_repr():\n    fig = Figure()\n    repr_fig = repr(fig)\n    assert \"Figure\" in repr_fig\n    assert \"size\" in repr_fig\n    assert \"x\" in repr_fig\n    assert str(len(fig.axes)) in repr_fig\n\n    ax = fig.add_subplot(111)\n    repr_fig = repr(fig)\n    assert str(len(fig.axes)) in repr_fig\n"], "sample_546": ["def test_figure_repr():\n    fig = Figure()\n    repr_fig = repr(fig)\n    assert \"Figure\" in repr_fig\n    assert re.search(r\"size \\d+\\.\\d+x\\d+\\.\\d+\", repr_fig)\n    assert re.search(r\"with \\d+ Axes\", repr_fig)\n"], "sample_549": ["def test_safe_masked_invalid():\n    # Test that safe_masked_invalid returns masked arrays for non-native byte order.\n    data = np.array([1, 2, np.nan, 4], dtype=np.float32)\n    if data.dtype.byteorder == '=':\n        data = data.byteswap().newbyteorder()\n    masked = cbook.safe_masked_invalid(data)\n    assert isinstance(masked, np.ma.MaskedArray)\n    assert_array_equal(masked.mask, [False, False, True, False])\n"], "sample_550": ["def test_axes_set_position():\n    fig, ax = plt.subplots()\n    orig_pos = ax.get_position()\n    new_pos = [orig_pos.x0 + 0.1, orig_pos.y0 + 0.1,\n               orig_pos.width - 0.2, orig_pos.height - 0.2]\n    ax.set_position(new_pos)\n    assert np.allclose(ax.get_position().extents, new_pos)\n    ax.reset_position()\n    assert np.allclose(ax.get_position().extents, orig_pos.extents)\n"], "sample_551": ["def test_text3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    text = ax.text(1, 2, 3, 'test', zdir='x')\n    assert isinstance(text, art3d.Text3D)\n    assert text._dir_vec == np.array([1, 0, 0])\n\n    text.set_position_3d((4, 5, 6), 'y')\n    assert text._dir_vec == np.array([0, 1, 0])\n    assert text.get_position_3d() == (4, 5, 6)\n\n    text.set_z(7)\n    assert text.get_position_3d() == (4, 5, 7)\n\n    text.set_3d_properties(z=8, zdir='z')\n    assert text._dir_vec == np.array([0, 0, 1])\n    assert text.get_position_3d() == (4, 5, 8)\n"], "sample_552": ["def test_figure_repr():\n    fig = Figure()\n    repr_fig = repr(fig)\n    assert \"Figure\" in repr_fig\n    assert \"size\" in repr_fig\n    assert \"x\" in repr_fig\n    assert \"with 0 Axes\" in repr_fig\n\n    ax = fig.add_subplot(111)\n    repr_fig = repr(fig)\n    assert \"with 1 Axes\" in repr_fig\n"], "sample_553": ["def test_animation_save_without_rendering(anim, tmpdir):\n    # Test saving an animation without rendering it first.\n    # This should not raise any warnings about the animation being garbage collected.\n    anim.save(str(tmpdir / 'test.mp4'))\n"], "sample_554": ["def test_text_get_window_extent():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    renderer = fig.canvas.get_renderer()\n    extent = text.get_window_extent(renderer)\n    assert isinstance(extent, mtransforms.Bbox)\n    assert extent.width > 0\n    assert extent.height > 0\n"], "sample_556": ["def test_figure_repr():\n    fig = Figure()\n    repr_fig = repr(fig)\n    assert 'Figure' in repr_fig\n    assert 'size' in repr_fig\n    assert 'Axes' in repr_fig\n\n    fig.add_subplot(111)\n    repr_fig = repr(fig)\n    assert 'Figure' in repr_fig\n    assert 'size' in repr_fig\n    assert '1 Axes' in repr_fig\n"], "sample_557": ["def test_figure_repr():\n    fig = Figure()\n    repr_fig = repr(fig)\n    assert 'Figure' in repr_fig\n    assert 'size' in repr_fig\n    assert 'Axes' in repr_fig\n\n    fig2 = Figure(figsize=(8, 6), dpi=100)\n    repr_fig2 = repr(fig2)\n    assert 'Figure' in repr_fig2\n    assert '800x600' in repr_fig2\n    assert 'Axes' in repr_fig2\n"], "sample_562": ["def test_line2d_update_from():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([0, 1], [0, 1])\n    line2, = ax.plot([0, 1], [1, 0])\n\n    line1.set_color('red')\n    line1.set_linestyle('--')\n    line1.set_marker('o')\n\n    line2.update_from(line1)\n\n    assert line2.get_color() == 'red'\n    assert line2.get_linestyle() == '--'\n    assert line2.get_marker() == 'o'\n"], "sample_566": ["def test_figure_repr():\n    fig = Figure()\n    repr_fig = repr(fig)\n    assert \"Figure\" in repr_fig\n    assert \"size\" in repr_fig\n    assert \"Axes\" in repr_fig\n\n    fig2 = Figure()\n    ax = fig2.add_subplot(111)\n    repr_fig2 = repr(fig2)\n    assert \"Figure\" in repr_fig2\n    assert \"size\" in repr_fig2\n    assert \"1 Axes\" in repr_fig2\n"], "sample_568": ["def test_art3d_line_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # create a line collection\n    xs = np.array([0.1, 0.2, 0.3])\n    ys = np.array([0.4, 0.5, 0.6])\n    zs = np.array([0.7, 0.8, 0.9])\n    line_collection = LineCollection([np.column_stack([xs, ys])])\n\n    # convert to 3D\n    art3d.line_collection_2d_to_3d(line_collection, zs=zs, zdir='z')\n\n    # check that the line collection has been converted correctly\n    assert isinstance(line_collection, art3d.Line3DCollection)\n    assert np.allclose(line_collection._segments3d, [np.column_stack([xs, ys, zs])])\n"], "sample_569": ["def test_regression_plotter_dropna(self):\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=True)\n    assert not pd.isnull(p.y).any()\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=False)\n    npt.assert_array_equal(pd.isnull(p.y), pd.isnull(self.df[\"y_na\"]))\n"], "sample_571": ["def test_regression_plotter_dropna(self):\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=True)\n    npt.assert_equal(p.x.shape[0], 57)\n    npt.assert_equal(p.y.shape[0], 57)\n\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=False)\n    npt.assert_equal(p.x.shape[0], 60)\n    npt.assert_equal(p.y.shape[0], 60)\n"], "sample_574": ["def test_continuous_label_formatter(self, x):\n\n    a, locs = self.setup_labels(x)\n    labels = a.major.formatter.format_ticks(locs)\n\n    assert len(labels) == len(locs)\n    for label in labels:\n        assert isinstance(label, str)\n\n    # Test with custom formatter\n    fmt = mpl.ticker.FuncFormatter(lambda x, pos: f\"{x:.2f}\")\n    a, locs = self.setup_labels(x, formatter=fmt)\n    labels = a.major.formatter.format_ticks(locs)\n    for label in labels:\n        assert re.match(r\"^\\d+\\.\\d\\d$\", label)\n\n    # Test with \"like\" string\n    a, locs = self.setup_labels(x, like=\".1f\")\n    labels = a.major.formatter.format_ticks(locs)\n    for label in labels:\n        assert re.match(r\"^\\d+\\.\\d$\", label)\n\n    # Test with \"like\" function\n        return f\"{x:.2f}\"\n    a, locs = self.setup_labels(x, like=fmt_func)\n    labels = a.major.formatter.format_ticks(locs)\n    for label in labels:\n        assert re.match(r\"^\\d+\\.\\d\\d$\", label)\n"], "sample_575": ["def test_continuous_label_formatter(self, x):\n\n    a, locs = self.setup_labels(x)\n    labels = a.major.formatter.format_ticks(locs)\n\n    assert len(labels) == len(locs)\n    for label in labels:\n        assert isinstance(label, str)\n\n    a, locs = self.setup_labels(x, like=\".2f\")\n    labels = a.major.formatter.format_ticks(locs)\n    assert all(re.match(r\"^\\d+\\.\\d\\d$\", label) for label in labels)\n\n    a, locs = self.setup_labels(x, like=\"${x:.2f}\")\n    labels = a.major.formatter.format_ticks(locs)\n    assert all(re.match(r\"^\\$\\d+\\.\\d\\d$\", label) for label in labels)\n\n        return f\"[{x}]\"\n\n    a, locs = self.setup_labels(x, like=fmt)\n    labels = a.major.formatter.format_ticks(locs)\n    assert all(re.match(r\"^\\[\\d+\\]$\", label) for label in labels)\n"], "sample_576": ["def test_plot_init_positional_args():\n\n    # Test that passing data as first positional argument works\n    p = Plot(pd.DataFrame({\"x\": [1, 2, 3]}))\n    assert isinstance(p._data.frame, pd.DataFrame)\n\n    # Test that passing x and y as positional arguments works\n    p = Plot([1, 2, 3], [4, 5, 6])\n    assert \"x\" in p._data.frame\n    assert \"y\" in p._data.frame\n\n    # Test that passing data, x, and y as positional arguments raises an error\n    with pytest.raises(TypeError):\n        Plot(pd.DataFrame({\"x\": [1, 2, 3]}), [1, 2, 3], [4, 5, 6])\n\n    # Test that passing too many positional arguments raises an error\n    with pytest.raises(TypeError):\n        Plot([1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12])\n"], "sample_577": ["def test_plot_init_positional_args():\n\n    # Test that passing a DataFrame as the first positional argument works\n    df = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n    p = Plot(df)\n    assert_frame_equal(p._data.frame, df)\n\n    # Test that passing vectors as the first two positional arguments works\n    x = [1, 2, 3]\n    y = [4, 5, 6]\n    p = Plot(x, y)\n    assert_vector_equal(p._data.frame[\"x\"], x)\n    assert_vector_equal(p._data.frame[\"y\"], y)\n\n    # Test that passing a single vector as the first positional argument raises\n    with pytest.raises(TypeError):\n        Plot([1, 2, 3])\n\n    # Test that passing too many positional arguments raises\n    with pytest.raises(TypeError):\n        Plot(df, [1, 2, 3], [4, 5, 6], [7, 8, 9])\n"], "sample_581": ["def test_blueprint_before_app_request():\n    app = flask.Flask(__name__)\n    bp = flask.Blueprint(\"test\", __name__)\n\n    @bp.before_app_request\n        return \"before app request\"\n\n    @app.route(\"/\")\n        return \"index\"\n\n    bp.register(app)\n\n    with app.test_client() as c:\n        rv = c.get(\"/\")\n        assert rv.data == b\"before app request\"\n"], "sample_583": ["def test_orthogonal_indexer(self):\n    x = np.random.rand(10, 20, 30)\n    ortho_indexer = indexing.OuterIndexer((slice(2), slice(5), slice(7)))\n    expected = x[ortho_indexer.tuple]\n    result = indexing.NumpyIndexingAdapter(x)[ortho_indexer]\n    assert_array_equal(result, expected)\n\n    # Test with a list of integers\n    ortho_indexer = indexing.OuterIndexer(([0, 1], [2, 3], slice(7)))\n    expected = x[ortho_indexer.tuple]\n    result = indexing.NumpyIndexingAdapter(x)[ortho_indexer]\n    assert_array_equal(result, expected)\n"], "sample_587": ["def test_merge_variables():\n    # Create variables with different dimensions and values\n    var1 = xr.Variable((\"x\",), np.array([1, 2, 3]))\n    var2 = xr.Variable((\"x\",), np.array([4, 5, 6]))\n    var3 = xr.Variable((\"y\",), np.array([7, 8]))\n\n    # Test merge_variables with default compat\n    merged_vars = merge.merge_variables([{\"var1\": var1}, {\"var1\": var2, \"var2\": var3}])\n    assert len(merged_vars) == 2\n    assert \"var1\" in merged_vars\n    assert \"var2\" in merged_vars\n\n    # Test merge_variables with compat=\"override\"\n    merged_vars = merge.merge_variables(\n        [{\"var1\": var1}, {\"var1\": var2, \"var2\": var3}], compat=\"override\"\n    )\n    assert len(merged_vars) == 2\n    assert \"var1\" in merged_vars\n    assert \"var2\" in merged_vars\n\n    # Test merge_variables with compat=\"minimal\"\n    merged_vars = merge.merge_variables(\n        [{\"var1\": var1}, {\"var1\": var2, \"var2\": var3}], compat=\"minimal\"\n    )\n    assert len(merged_vars) == 2\n    assert \"var1\" in merged_vars\n    assert \"var2\" in merged_vars\n"], "sample_595": ["def test_str_accessor_methods(dtype):\n    values = np.array([\"hello\", \"world\", \"xarray\"], dtype=dtype)\n    da = xr.DataArray(values)\n\n    # length\n    result = da.str.len()\n    expected = xr.DataArray([5, 5, 6], dtype=\"int64\")\n    assert_equal(result, expected)\n\n    # indexing\n    result = da.str[0]\n    expected = xr.DataArray([\"h\", \"w\", \"x\"], dtype=dtype)\n    assert_equal(result, expected)\n\n    # slicing\n    result = da.str[:3]\n    expected = xr.DataArray([\"hel\", \"wor\", \"xar\"], dtype=dtype)\n    assert_equal(result, expected)\n\n    # lower, upper, title\n    result = da.str.lower()\n    expected = xr.DataArray([\"hello\", \"world\", \"xarray\"], dtype=dtype)\n    assert_equal(result, expected)\n\n    result = da.str.upper()\n    expected = xr.DataArray([\"HELLO\", \"WORLD\", \"XARRAY\"], dtype=dtype)\n    assert_equal(result, expected)\n\n    result = da.str.title()\n    expected = xr.DataArray([\"Hello\", \"World\", \"Xarray\"], dtype=dtype)\n    assert_equal(result, expected)\n"], "sample_606": ["def test_apply_ufunc_signature():\n    sig = _UFuncSignature([(\"x\",), (\"y\",)], [(\"z\",)])\n    assert sig.num_inputs == 2\n    assert sig.num_outputs == 1\n    assert sig.input_core_dims == ((\"x\",), (\"y\",))\n    assert sig.output_core_dims == ((\"z\",),)\n    assert sig.all_input_core_dims == {\"x\", \"y\"}\n    assert sig.all_output_core_dims == {\"z\"}\n    assert sig.all_core_dims == {\"x\", \"y\", \"z\"}\n    assert sig.dims_map == {\"x\": \"dim0\", \"y\": \"dim1\", \"z\": \"dim2\"}\n    assert str(sig) == \"(x),(y)->(z)\"\n    assert sig.to_gufunc_string() == \"(dim0),(dim1)->(dim2)\"\n"], "sample_615": ["def test_apply_ufunc_signature():\n    signature = _UFuncSignature([(\"x\",), (\"y\",)], [(\"z\",)])\n    assert str(signature) == \"(x)->(z)\"\n    assert repr(signature) == \"_UFuncSignature([('x',), ('y',)], [('z',)])\"\n\n    signature = _UFuncSignature([(\"x\", \"y\"), (\"y\", \"z\")], [(\"a\", \"b\")])\n    assert str(signature) == \"(x,y),(y,z)->(a,b)\"\n    assert (\n        repr(signature)\n        == \"_UFuncSignature([('x', 'y'), ('y', 'z')], [('a', 'b')])\"\n    )\n"], "sample_616": ["def test_apply_ufunc_signature():\n        return x + y\n\n    signature = _UFuncSignature([(\"x\",), (\"y\",)], [(\"out\",)])\n    result = apply_ufunc(\n        func, xr.DataArray([1], dims=\"x\"), xr.DataArray([2], dims=\"y\"), signature=signature\n    )\n    assert_identical(result, xr.DataArray([3], dims=\"out\"))\n"], "sample_617": ["def test_apply_ufunc_dtype():\n    # regression test for GH1686\n    da = xr.DataArray(np.array([1, 2, 3], dtype=\"int64\"))\n    result = apply_ufunc(lambda x: x, da)\n    assert result.dtype == \"int64\"\n"], "sample_618": ["def test_apply_ufunc_signature_validation():\n    with pytest.raises(ValueError):\n        apply_ufunc(\n            np.mean,\n            xr.DataArray([1, 2, 3]),\n            input_core_dims=[[\"x\"], [\"y\"]],\n            output_core_dims=[[\"z\"]],\n        )\n\n    with pytest.raises(ValueError):\n        apply_ufunc(\n            np.mean,\n            xr.DataArray([1, 2, 3]),\n            input_core_dims=[[\"x\"]],\n            output_core_dims=[[\"y\"], [\"z\"]],\n        )\n"], "sample_621": ["def test_create_variables_multi_index():\n    # Create a multi-index with two levels\n    index = pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]], names=[\"level1\", \"level2\"])\n    dim = \"dim\"\n    level_coords_dtype = {\"level1\": \"object\", \"level2\": \"int64\"}\n    multi_index = PandasMultiIndex(index, dim, level_coords_dtype=level_coords_dtype)\n\n    # Create variables for the multi-index\n    variables = multi_index.create_variables()\n\n    # Check that the variables are created correctly\n    assert len(variables) == 3\n    assert \"dim\" in variables\n    assert \"level1\" in variables\n    assert \"level2\" in variables\n\n    # Check the dtype of the variables\n    assert variables[\"level1\"].dtype == \"object\"\n    assert variables[\"level2\"].dtype == \"int64\"\n"], "sample_624": ["def test_format_timestamp():\n    # Test that NaT is formatted correctly\n    assert formatting.format_timestamp(pd.NaT) == \"NaT\"\n\n    # Test that a Timestamp object is formatted correctly\n    timestamp = pd.Timestamp(\"2022-01-01 12:00:00\")\n    assert formatting.format_timestamp(timestamp) == \"2022-01-01T12:00:00\"\n\n    # Test that a datetime64 object is formatted correctly\n    datetime64 = np.datetime64(\"2022-01-01 12:00:00\")\n    assert formatting.format_timestamp(datetime64) == \"2022-01-01T12:00:00\"\n"], "sample_625": ["def test_apply_ufunc_signature():\n    sig = _UFuncSignature([(\"a\", \"b\"), (\"b\", \"c\")], [(\"d\",), (\"e\",)])\n    assert sig.num_inputs == 2\n    assert sig.num_outputs == 2\n    assert sig.all_input_core_dims == {\"a\", \"b\", \"c\"}\n    assert sig.all_output_core_dims == {\"d\", \"e\"}\n    assert sig.all_core_dims == {\"a\", \"b\", \"c\", \"d\", \"e\"}\n\n    with pytest.raises(ValueError):\n        _UFuncSignature([(\"a\",)], [(\"b\", \"c\")])\n\n    with pytest.raises(ValueError):\n        _UFuncSignature([(\"a\", \"b\")], [(\"c\",)])\n\n    assert sig.to_gufunc_string() == \"(a,b),(b,c)->(d),(e)\"\n    assert sig.to_gufunc_string(exclude_dims={\"b\"}) == \"(a,b_0),(b_1,c)->(d),(e)\"\n"], "sample_626": ["def test_merge_sel_results() -> None:\n    # Test merging of multiple index selection results\n    result1 = IndexSelResult(\n        dim_indexers={\"x\": 1},\n        indexes={\"y\": pd.Index([1, 2, 3])},\n        variables={\"z\": Variable()},\n        drop_coords=[\"a\"],\n        drop_indexes=[\"b\"],\n        rename_dims={\"c\": \"d\"},\n    )\n    result2 = IndexSelResult(\n        dim_indexers={\"e\": 2},\n        indexes={\"f\": pd.Index([4, 5, 6])},\n        variables={\"g\": Variable()},\n        drop_coords=[\"h\"],\n        drop_indexes=[\"i\"],\n        rename_dims={\"j\": \"k\"},\n    )\n\n    merged = merge_sel_results([result1, result2])\n\n    expected = IndexSelResult(\n        dim_indexers={\"x\": 1, \"e\": 2},\n        indexes={\"y\": pd.Index([1, 2, 3]), \"f\": pd.Index([4, 5, 6])},\n        variables={\"z\": Variable(), \"g\": Variable()},\n        drop_coords=[\"a\", \"h\"],\n        drop_indexes=[\"b\", \"i\"],\n        rename_dims={\"c\": \"d\", \"j\": \"k\"},\n    )\n\n    assert_identical(merged, expected)\n"], "sample_631": ["    def test_undefined_variable_in_lambda(self):\n        code = \"\"\"\n            f = lambda x: y + x\n            return f(1)\n        \"\"\"\n        with self.assertAddsMessages(\n            Message(msg_id=\"undefined-variable\", node=astroid.Name(name=\"y\"))\n        ):\n            self.checker.visit_functiondef(astroid.parse(code).body[0])\n"], "sample_632": ["def test_ignore_imports():\n    with open(HIDE_CODE_WITH_IMPORTS) as f:\n        lines = f.readlines()\n    sim = similar.Similar(min_lines=4, ignore_imports=True)\n    sim.append_stream(\"test\", lines)\n    output = StringIO()\n    with redirect_stdout(output):\n        sim.run()\n    assert \"TOTAL lines=\" in output.getvalue()\n    assert \"duplicates=\" in output.getvalue()\n    assert \"percent=\" in output.getvalue()\n"], "sample_636": ["def test_similar_code_checker_ignore_comments():\n    with open(join(DATA, \"ignore_comments.py\"), encoding=\"utf-8\") as f:\n        code = f.read()\n    with _patch_streams(StringIO()) as out:\n        Run([\"--disable=all\", \"--enable=duplicate-code\", \"--ignore-comments=yes\"], exit_zero=False)\n        Run([code], exit_zero=False)\n    output = out.getvalue()\n    assert \"Similar lines in 1 files\" not in output\n"], "sample_644": ["def test_imports_order(self):\n    \"\"\"Test that imports are ordered correctly.\"\"\"\n    node = astroid.parse(\n        \"\"\"\n        import os\n        from pylint.checkers import imports\n        import sys\n        from pylint.interfaces import UNDEFINED\n        \"\"\"\n    )\n    with self.assertAddsMessages(\n        MessageTest(\"wrong-import-order\", line=3, args=(\"standard import 'sys'\", \"'pylint.checkers'\"))\n    ):\n        self.walk(node)\n"], "sample_654": ["def test_fixture_scope_mismatch_forbidden(request):\n    \"\"\"Check that a lower-scoped fixture cannot use a higher-scoped fixture (#2706)\"\"\"\n    with pytest.raises(FixtureLookupError) as excinfo:\n        request.getfixturevalue(\"scope_mismatch_fixture\")\n    assert \"ScopeMismatch\" in str(excinfo.value)\n"], "sample_659": ["def test_raises_match():\n    with pytest.raises(ValueError) as excinfo:\n        raise ValueError(\"test failure\")\n    excinfo.match(r\"^test failure$\")\n    pytest.raises(AssertionError, excinfo.match, r\"^wrong_regexp$\")\n"], "sample_662": ["def test_report_to_json_with_exception_chain_repr():\n    report = TestReport(\n        nodeid=\"test_node\",\n        location=(\"test_file.py\", 10, \"test_function\"),\n        keywords=[\"test_keyword\"],\n        outcome=\"failed\",\n        longrepr=ExceptionChainRepr([]),\n        when=\"call\",\n        sections=[],\n        duration=1.0,\n    )\n    data = report._to_json()\n    assert \"$report_type\" in data\n    assert data[\"$report_type\"] == \"TestReport\"\n    assert \"longrepr\" in data\n    assert \"chain\" in data[\"longrepr\"]\n    assert data[\"longrepr\"][\"chain\"] == []\n"], "sample_675": ["def test_catch_log_handler_emit(testdir):\n    \"\"\"Test that the LogCaptureHandler emit method correctly handles log records.\"\"\"\n    handler = LogCaptureHandler()\n    record = logging.LogRecord(\"name\", logging.INFO, \"pathname\", 1, \"message\", None, None)\n    handler.emit(record)\n    assert len(handler.records) == 1\n    assert handler.records[0] == record\n    assert handler.stream.getvalue() == f\"{record.getMessage()}\\n\"\n"], "sample_683": ["def test_capture_manager_fixture_control(capman: CaptureManager) -> None:\n    \"\"\"Test that the capture manager properly controls fixtures.\"\"\"\n    class DummyFixture(capture.CaptureFixture):\n            super().__init__(capture.SysCapture, request)\n\n    dummy_fixture = DummyFixture(None)\n    capman.set_fixture(dummy_fixture)\n    assert capman._capture_fixture is dummy_fixture\n\n    capman.unset_fixture()\n    assert capman._capture_fixture is None\n\n    capman.set_fixture(dummy_fixture)\n    capman.suspend_fixture()\n    assert dummy_fixture._capture is not None\n    assert dummy_fixture._capture._state == \"suspended\"\n\n    capman.resume_fixture()\n    assert dummy_fixture._capture._state == \"started\"\n"], "sample_684": ["def test_traceback_entry_repr():\n    excinfo = ExceptionInfo.for_later()\n    frame = mock.Mock(spec=FrameType)\n    frame.f_code.co_filename = \"example.py\"\n    frame.f_lineno = 10\n    entry = TracebackEntry(frame, excinfo=excinfo)\n    assert re.match(r\"<TracebackEntry example\\.py:10>\", repr(entry))\n"], "sample_687": ["def test_caplog_records(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.getLogger().info(\"info\")\n            logging.getLogger().warning(\"warning\")\n            assert len(caplog.records) == 2\n            assert caplog.records[0].levelname == \"INFO\"\n            assert caplog.records[1].levelname == \"WARNING\"\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.assert_outcomes(passed=1)\n"], "sample_688": ["def test_make_numbered_dir(tmp_path: Path) -> None:\n    \"\"\"Test that make_numbered_dir creates a directory with an increased number as suffix.\"\"\"\n    prefix = \"test-\"\n    root = tmp_path\n    p = make_numbered_dir(root, prefix)\n    assert p.parent == root\n    assert p.name.startswith(prefix)\n    assert p.is_dir()\n    # Check that the next call creates a new directory with an incremented number.\n    p2 = make_numbered_dir(root, prefix)\n    assert p2.parent == root\n    assert p2.name.startswith(prefix)\n    assert p2.is_dir()\n    assert p2.name != p.name\n    # Check that the current symlink points to the latest directory.\n    current_symlink = root.joinpath(prefix + \"current\")\n    assert current_symlink.resolve() == p2\n"], "sample_694": ["def test_nose_support_method_warning(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        class TestClass:\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        \"*Support for nose tests is deprecated and will be removed in a future release.*\"\n    )\n    assert result.ret == 0\n"], "sample_702": ["def test_runpytest_subprocess_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(10)\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess(\"--runpytest\", \"subprocess\", timeout=1)\n    assert isinstance(result, pytester_mod.RunResult)\n    assert result.ret == ExitCode.INTERRUPTED\n"], "sample_704": ["def test_node_repr(tmp_path: Path) -> None:\n    class MyNode(nodes.Node):\n            super().__init__(name, **kwargs)\n\n    node = MyNode(\"test_node\", nodeid=\"node_id\")\n    assert repr(node) == \"<MyNode test_node>\"\n"], "sample_705": ["def test_runpytest_subprocess_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(10)\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess(\"--runpytest\", \"subprocess\", timeout=1)\n    assert result.ret == ExitCode.INTERRUPTED\n"], "sample_709": ["def test_runpytest_subprocess_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(10)\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess(\"--runpytest\", \"subprocess\", timeout=1)\n    assert result.ret == ExitCode.INTERRUPTED\n    assert \"timeout expired\" in result.stderr[0]\n"], "sample_711": ["def test_iterparentnodeids_edge_cases():\n    assert list(nodes.iterparentnodeids(\"::\")) == [\"\", \"::\"]\n    assert list(nodes.iterparentnodeids(\"a::\")) == [\"\", \"a\", \"a::\"]\n    assert list(nodes.iterparentnodeids(\"::a\")) == [\"\", \"::a\"]\n    assert list(nodes.iterparentnodeids(\"a::b::c\")) == [\n        \"\",\n        \"a\",\n        \"a::b\",\n        \"a::b::c\",\n    ]\n    assert list(nodes.iterparentnodeids(\"\")) == [\"\"]\n"], "sample_717": ["def test_fetch_lfw_pairs_subset():\n    # Check that the train, test and 10_folds subsets are loaded correctly\n    for subset in ['train', 'test', '10_folds']:\n        lfw_pairs = fetch_lfw_pairs(subset=subset, data_home=SCIKIT_LEARN_DATA)\n        assert_equal(lfw_pairs.target.shape[0], lfw_pairs.pairs.shape[0])\n        assert_equal(lfw_pairs.target_names.shape[0], 2)\n        assert_array_equal(np.unique(lfw_pairs.target), [0, 1])\n"], "sample_718": ["def test_check_estimator_unfitted():\n    # Test that check_estimators_unfitted raises an exception when calling\n    # predict, predict_proba or decision_function on an unfitted estimator.\n\n    class UnfittedEstimator(BaseBadClassifier):\n            pass\n\n    msg = \"fit\"\n    assert_raise_message((AttributeError, ValueError), msg,\n                         check_estimators_unfitted, \"UnfittedEstimator\",\n                         UnfittedEstimator())\n"], "sample_728": ["def test_make_sparse_coded_signal():\n    X, D, code = make_sparse_coded_signal(n_samples=10, n_components=5,\n                                          n_features=20, n_nonzero_coefs=3)\n    assert_equal(X.shape, (10, 20))\n    assert_equal(D.shape, (20, 5))\n    assert_equal(code.shape, (5, 10))\n    assert_array_almost_equal(np.dot(D, code), X)\n    for i in range(10):\n        assert_less(np.sum(code[:, i] != 0), 4)\n"], "sample_735": ["def test_gaussian_mixture_covariance_type():\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_components = 100, 2, 2\n    X = rng.rand(n_samples, n_features)\n\n    # Check that the covariance type is correctly set\n    gmm = GaussianMixture(n_components=n_components, covariance_type='full')\n    gmm.fit(X)\n    assert_equal(gmm.covariance_type, 'full')\n\n    gmm = GaussianMixture(n_components=n_components, covariance_type='tied')\n    gmm.fit(X)\n    assert_equal(gmm.covariance_type, 'tied')\n\n    gmm = GaussianMixture(n_components=n_components, covariance_type='diag')\n    gmm.fit(X)\n    assert_equal(gmm.covariance_type, 'diag')\n\n    gmm = GaussianMixture(n_components=n_components, covariance_type='spherical')\n    gmm.fit(X)\n    assert_equal(gmm.covariance_type, 'spherical')\n\n    # Check that an invalid covariance type raises an error\n    gmm = GaussianMixture(n_components=n_components, covariance_type='invalid')\n    assert_raise_message(ValueError, \"Invalid value for 'covariance_type': \"\n                         \"'invalid' 'covariance_type' should be in \"\n                         \"['spherical', 'tied', 'diag', 'full']\",\n                         gmm.fit, X)\n"], "sample_741": ["def test_parameter_grid():\n    param_grid = {'a': [1, 2], 'b': [True, False]}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 4)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'a': [1, 2]}, {'b': [True, False]}]\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 3)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = {'a': [1, 2], 'b': [True, False], 'c': ['foo', 'bar']}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 8)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'a': [1, 2]}, {'b': [True, False]}, {'c': ['foo', 'bar']}]\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 7)\n    assert_grid_iter_equals_getitem(grid)\n"], "sample_746": ["def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Bad beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n\n    # Bad average option\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  [1, 2, 3], [1, 2, 3], average='invalid')\n\n    # Bad pos_label\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  [1, 2, 3], [1, 2, 3], pos_label='invalid')\n"], "sample_748": ["def test_parameter_grid():\n    # Test basic properties of ParameterGrid.\n    param_grid = {'a': [1, 2], 'b': [3, 4]}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 4)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'a': [1, 2]}, {'b': [3, 4]}]\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 2)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = {'a': [[1, 2], [3, 4]], 'b': [5, 6]}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 4)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = {'a': [], 'b': [1, 2]}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 0)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = {'a': [1, 2], 'b': []}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 0)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = {'a': [], 'b': []}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 0)\n    assert_grid_iter_equals_getitem(grid)\n"], "sample_749": ["def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # single transformer case\n    ct = ColumnTransformer([(\"trans\", Trans(), [0, 1])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), [\"trans__x0\", \"trans__x1\"])\n\n    # multiple transformers case\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1, 2])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), [\"trans1__x0\", \"trans2__x0\", \"trans2__x1\"])\n\n    # 'passthrough' transformer case\n    ct = ColumnTransformer([(\"trans\", Trans(), [0]), (\"passthrough\", \"passthrough\", [1, 2])])\n    with pytest.raises(NotImplementedError):\n        ct.fit(X).get_feature_names()\n\n    # 'drop' transformer case\n    ct = ColumnTransformer([(\"trans\", Trans(), [0]), (\"drop\", \"drop\", [1, 2])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), [\"trans__x0\"])\n"], "sample_753": ["def test_logistic_regression_solvers():\n    \"\"\"Test that all solvers give the same result on a simple problem.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=5,\n                               n_informative=3, n_redundant=0,\n                               n_classes=2, random_state=42)\n\n    # Test that all solvers give the same result\n    solvers = ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']\n    results = []\n    for solver in solvers:\n        clf = LogisticRegression(solver=solver, max_iter=1000)\n        clf.fit(X, y)\n        results.append(clf.coef_)\n\n    # Compare solutions\n    for i in range(len(solvers)):\n        for j in range(i+1, len(solvers)):\n            assert_allclose(results[i], results[j], atol=1e-4)\n"], "sample_764": ["def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    columns = ['a', 'b', 'c']\n    transformers = [\n        ('trans1', Trans(), ['a', 'b']),\n        ('trans2', Trans(), ['c'])\n    ]\n    ct = ColumnTransformer(transformers)\n    ct.fit(X)\n\n    feature_names = ct.get_feature_names()\n    assert_equal(len(feature_names), 3)\n    for name in feature_names:\n        assert name.startswith(('trans1__', 'trans2__'))\n"], "sample_765": ["def test_multilabel_confusion_matrix():\n    # Test multilabel confusion matrix function\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1]])\n\n    conf_mat = multilabel_confusion_matrix(y_true, y_pred)\n\n    expected_conf_mat = np.array([[[1, 0],\n                                   [0, 1]],\n                                  [[1, 0],\n                                   [0, 1]],\n                                  [[0, 1],\n                                   [1, 0]]])\n\n    assert_array_equal(conf_mat, expected_conf_mat)\n"], "sample_767": ["def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # single transformer case\n    ct = ColumnTransformer([('trans', Trans(), [0])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['trans__0'])\n\n    # multiple transformers case\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['trans1__0', 'trans2__0'])\n\n    # 'drop' transformer case\n    ct = ColumnTransformer([('trans', 'drop', [0])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), [])\n\n    # 'passthrough' transformer case\n    ct = ColumnTransformer([('trans', 'passthrough', [0])])\n    with pytest.raises(NotImplementedError):\n        ct.fit(X).get_feature_names()\n\n    # transformer with get_feature_names method\n    class TransWithFeatureNames(BaseEstimator):\n            return self\n\n            return X\n\n            return ['feature']\n\n    ct = ColumnTransformer([('trans', TransWithFeatureNames(), [0])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['trans__feature'])\n"], "sample_768": ["def test_validate_shuffle_split():\n    # Test if _validate_shuffle_split raises ValueError when test_size and/or \n    # train_size is invalid\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.6, 0.5)\n    assert_raises(ValueError, _validate_shuffle_split_init, 1.1, None)\n    assert_raises(ValueError, _validate_shuffle_split_init, -0.1, None)\n    assert_raises(ValueError, _validate_shuffle_split_init, None, 1.1)\n    assert_raises(ValueError, _validate_shuffle_split_init, None, -0.1)\n\n    # Test if _validate_shuffle_split raises ValueError when test_size and/or \n    # train_size is invalid with respect to the number of samples\n    assert_raises(ValueError, _validate_shuffle_split, 10, 11, None)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, 11)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 5, 6)\n"], "sample_769": ["def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Bad beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n\n    # Bad average option\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  [1, 2, 3], [1, 2, 3], average='invalid')\n\n    # Bad pos_label\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  [1, 2, 3], [1, 2, 3], pos_label='invalid')\n"], "sample_771": ["def test_power_transformer_check_lambda():\n    # Check that lambda is in the correct range for both methods\n    X = np.abs(np.random.RandomState(0).randn(100, 1))\n\n    pt = PowerTransformer(method='box-cox')\n    pt.fit(X)\n    assert_greater_equal(pt.lambdas_, 0)\n\n    pt = PowerTransformer(method='yeo-johnson')\n    pt.fit(X)\n    assert_greater_equal(pt.lambdas_, 1)\n"], "sample_775": ["def test_pprint_estimator():\n    # Test that the _EstimatorPrettyPrinter class can pretty-print an estimator\n    estimator = LogisticRegression(max_iter=1000)\n    printer = _EstimatorPrettyPrinter()\n    output = printer.pformat(estimator)\n    assert re.match(r\"LogisticRegression\\([^)]+\\)\", output)\n\n    # Test that the output is compact if there are many parameters\n    estimator = GridSearchCV(LogisticRegression(max_iter=1000), {\"C\": [1, 2, 3]})\n    output = printer.pformat(estimator)\n    assert re.match(r\"GridSearchCV\\([^)]+\\)\", output)\n    assert \"estimator=\" not in output\n\n    # Test that the output is compact if there are many estimators in a pipeline\n    estimator = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n    output = printer.pformat(estimator)\n    assert re.match(r\"Pipeline\\([^)]+\\)\", output)\n    assert \"steps=[\" not in output\n"], "sample_779": ["def test_check_estimator_idempotent():\n    # Test that check_estimator_idempotent raises an error when estimator\n    # is not idempotent.\n    msg = \"Estimator.* is not idempotent\"\n    assert_raises_regex(AssertionError, msg,\n                        check_fit_idempotent, \"NotInvariantPredict\",\n                        NotInvariantPredict())\n"], "sample_782": ["def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Test with transformers that have get_feature_names\n    ct = ColumnTransformer([\n        ('trans1', StandardScaler(), [0]),\n        ('trans2', OneHotEncoder(), [1])\n    ])\n\n    ct.fit(X)\n    feature_names = ct.get_feature_names()\n    assert_array_equal(feature_names, ['trans1__x0', 'trans2__x0_1', 'trans2__x0_2'])\n\n    # Test with transformers that don't have get_feature_names\n    ct = ColumnTransformer([\n        ('trans1', Trans(), [0]),\n        ('trans2', OneHotEncoder(), [1])\n    ])\n\n    ct.fit(X)\n    with pytest.raises(AttributeError):\n        ct.get_feature_names()\n\n    # Test with 'passthrough' transformer\n    ct = ColumnTransformer([\n        ('trans1', 'passthrough', [0]),\n        ('trans2', OneHotEncoder(), [1])\n    ])\n\n    ct.fit(X)\n    with pytest.raises(NotImplementedError):\n        ct.get_feature_names()\n"], "sample_785": ["def test_validate_shuffle_split():\n    with assert_raises_regexp(ValueError, 'test_size should be a number'):\n        _validate_shuffle_split_init(test_size='a', train_size=None)\n    with assert_raises_regexp(ValueError, 'train_size should be a number'):\n        _validate_shuffle_split_init(test_size=None, train_size='a')\n    with assert_raises_regexp(ValueError, 'test_size should be positive'):\n        _validate_shuffle_split_init(test_size=-1, train_size=None)\n    with assert_raises_regexp(ValueError, 'train_size should be positive'):\n        _validate_shuffle_split_init(test_size=None, train_size=-1)\n    with assert_raises_regexp(ValueError, 'test_size should be smaller'):\n        _validate_shuffle_split_init(test_size=1.5, train_size=None)\n    with assert_raises_regexp(ValueError, 'train_size should be smaller'):\n        _validate_shuffle_split_init(test_size=None, train_size=1.5)\n    with assert_raises_regexp(ValueError, 'The sum of test_size and train_size '\n                                         '= 1.2, should be in the (0, 1) range'):\n        _validate_shuffle_split_init(test_size=0.7, train_size=0.5)\n\n    # check that it doesn't raise any error when passing valid arguments.\n    _validate_shuffle_split_init(test_size=0.5, train_size=None)\n    _validate_shuffle_split_init(test_size=None, train_size=0.5)\n    _validate_shuffle_split_init(test_size=10, train_size=None)\n    _validate_shuffle_split_init(test_size=None, train_size=10)\n    _validate_shuffle_split_init(test_size=0.5, train_size=0.3)\n"], "sample_787": ["def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # test that we don't allow for mix type input\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true.astype(int), y_pred.astype(str))\n\n    # test that we don't allow for mix string/integer input\n    y_true = np.array(['a', 'b', 'c'])\n    y_pred = np.array([1, 2, 3])\n    assert_raises(ValueError, precision_recall_fscore_support, y_true, y_pred)\n\n    # test that multiclass classification metrics throw an error when there are\n    # no positive predictions\n    assert_raises(UndefinedMetricWarning, precision_recall_fscore_support,\n                  np.array([0, 1, 2]), np.array([0, 0, 0]), average='macro')\n"], "sample_792": ["def test_complementnb_alpha():\n    # Test if alpha is set correctly for ComplementNB\n    X, y = X2, y2\n    clf = ComplementNB(alpha=0.5)\n    clf.fit(X, y)\n    assert_almost_equal(clf._check_alpha(), 0.5)\n\n    # Test if alpha is set correctly when initialized with an array\n    alpha_array = np.array([0.1, 0.2, 0.3])\n    clf = ComplementNB(alpha=alpha_array)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf._check_alpha(), alpha_array)\n\n    # Test if alpha is set to the minimum allowed value when it's too small\n    clf = ComplementNB(alpha=1e-12)\n    clf.fit(X, y)\n    assert_almost_equal(clf._check_alpha(), _ALPHA_MIN)\n"], "sample_797": ["def test_power_transformer_zero_variance():\n    # Check that PowerTransformer handles zero-variance features correctly\n\n    X = np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3]])\n\n    pt = PowerTransformer()\n    with pytest.raises(ValueError):\n        pt.fit(X)\n"], "sample_799": ["def test_cross_validate_return_estimator():\n    # Check if the returned estimator is the best one\n    X, y = make_classification(n_samples=20, n_features=5, random_state=0)\n    estimator = LogisticRegression(random_state=0)\n    cv = KFold(n_splits=2, shuffle=True, random_state=0)\n    scores = cross_validate(estimator, X, y, cv=cv, return_estimator=True)\n    assert len(scores['estimator']) == cv.n_splits\n    for est in scores['estimator']:\n        assert isinstance(est, LogisticRegression)\n"], "sample_800": ["def test_check_estimator_idempotent():\n    # Test that check_estimator is idempotent, i.e. running check_estimator\n    # on an estimator does not change its internal state.\n    estimator = LogisticRegression()\n    set_random_state(estimator)\n    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],\n                      random_state=0, n_features=2, cluster_std=0.1)\n\n    # Run check_estimator once to ensure the estimator is fitted\n    check_estimator(estimator)\n\n    # Get the original predictions and parameters\n    y_pred_original = estimator.predict(X)\n    params_original = estimator.get_params()\n\n    # Run check_estimator again\n    check_estimator(estimator)\n\n    # Get the new predictions and parameters\n    y_pred_new = estimator.predict(X)\n    params_new = estimator.get_params()\n\n    # Check that the predictions and parameters are the same\n    assert_array_equal(y_pred_original, y_pred_new)\n    assert params_original == params_new\n"], "sample_801": ["def test_repr_multilines():\n    # Test estimator repr with newlines and tabs in the parameters\n    estimator = LogisticRegression(multi_class=\"multinomial\",\n                                   class_weight=\"balanced\")\n    estimator.class_weight = {\"class1\": 0.5, \"class2\": 0.3, \"class3\": 0.2}\n    estimator.intercept_scaling = 10\n\n    representation = repr(estimator)\n    assert len(representation.splitlines()) > 1\n    assert \"\\n\" not in representation.splitlines()[0]\n    assert \"LogisticRegression(\" in representation\n    assert \"multi_class='multinomial'\" in representation\n    assert \"intercept_scaling=10\" in representation\n    assert \"class_weight={'class1': 0.5, 'class2': 0.3, 'class3': 0.2}\" in representation\n"], "sample_811": ["def test_pairwise_distances_argmin_min():\n    # Test that pairwise_distances_argmin_min returns the correct minimum\n    # distances and indices for a given distance metric.\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[7, 8], [9, 10]])\n\n    distances_argmin, distances_min = pairwise_distances_argmin_min(X, Y)\n\n    expected_argmin = np.array([0, 0, 0])\n    expected_min = np.array([13., 25., 37.])\n\n    assert_array_equal(distances_argmin, expected_argmin)\n    assert_array_almost_equal(distances_min, expected_min)\n"], "sample_814": ["def test_gradient_boosting_regressor_boston():\n    # Check regression on boston dataset.\n    clf = GradientBoostingRegressor(n_estimators=100, random_state=1)\n    clf.fit(boston.data, boston.target)\n\n    y_pred = clf.predict(boston.data)\n    assert_greater(mean_squared_error(boston.target, y_pred), 0.0)\n\n    deviance_decrease = (clf.train_score_[:-1] - clf.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n    leaves = clf.apply(boston.data)\n    assert_equal(leaves.shape, (506, 100))\n"], "sample_818": ["def test_spectral_clustering_sparse_matrix():\n    # Test that spectral clustering works with sparse matrices\n    X, y = make_blobs(n_samples=200, random_state=1)\n    affinity_matrix = rbf_kernel(X)\n    affinity_matrix_sparse = sparse.csr_matrix(affinity_matrix)\n\n    clustering_dense = SpectralClustering(n_clusters=3, random_state=0,\n                                          affinity='precomputed')\n    clustering_sparse = SpectralClustering(n_clusters=3, random_state=0,\n                                           affinity='precomputed')\n\n    clustering_dense.fit(affinity_matrix)\n    clustering_sparse.fit(affinity_matrix_sparse)\n\n    assert_array_equal(clustering_dense.labels_, clustering_sparse.labels_)\n"], "sample_822": ["def test_pairwise_distances_chunked_reduce_func():\n    # Test pairwise_distances_chunked with a reduce function\n    X = np.random.RandomState(0).rand(5, 3)\n    D_chunk = next(pairwise_distances_chunked(X, reduce_func=lambda x, y: x))\n    assert_array_almost_equal(D_chunk, euclidean_distances(X))\n\n        return D_chunk.sum(axis=1)\n\n    sums = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n    assert_array_almost_equal(sums, euclidean_distances(X).sum(axis=1))\n"], "sample_823": ["def test_pairwise_distances_chunked_reduce_func():\n    # Test pairwise_distances_chunked with a reduce function\n    X = np.random.RandomState(0).rand(5, 3)\n    D = pairwise_distances(X)\n\n        return (D_chunk ** 2).sum(axis=1)\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    result = next(gen)\n    assert_array_almost_equal(result, (D ** 2).sum(axis=1))\n"], "sample_824": ["def test_pairwise_distances_chunked_reduce_func():\n    # Test pairwise_distances_chunked with a reduce function\n    X = np.random.RandomState(0).rand(5, 3)\n    D_chunk = next(pairwise_distances_chunked(X, reduce_func=lambda x, y: x.sum(axis=1)))\n    assert_array_almost_equal(D_chunk, euclidean_distances(X).sum(axis=1))\n"], "sample_828": ["def test_pairwise_distances_chunked_reduce_func():\n    # Test pairwise_distances_chunked with a reduce function\n    X = np.random.RandomState(0).rand(5, 3)\n    Y = np.random.RandomState(1).rand(4, 3)\n\n        return (D_chunk.sum(axis=1), D_chunk.mean(axis=1))\n\n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func)\n    sums, means = next(gen)\n\n    assert_array_almost_equal(sums, pairwise_distances(X, Y).sum(axis=1))\n    assert_array_almost_equal(means, pairwise_distances(X, Y).mean(axis=1))\n"], "sample_838": ["def test_column_transformer_get_feature_names():\n    # Test that get_feature_names works for different types of transformers\n\n    # Create a ColumnTransformer with different types of transformers\n    transformers = [\n        ('trans1', Trans(), [0, 1]),\n        ('trans2', OneHotEncoder(), [2]),\n        ('trans3', StandardScaler(), [3, 4]),\n        ('trans4', 'drop', [5])\n    ]\n    column_transformer = ColumnTransformer(transformers)\n\n    # Fit the ColumnTransformer to some data\n    X = np.array([[1, 2, 3, 4, 5, 6],\n                  [7, 8, 9, 10, 11, 12]])\n    column_transformer.fit(X)\n\n    # Get the feature names\n    feature_names = column_transformer.get_feature_names()\n\n    # Check that the feature names are correct\n    assert_array_equal(feature_names,\n                       ['trans1__x0', 'trans1__x1', 'trans2__x2_3',\n                        'trans2__x2_9', 'trans3__x3', 'trans3__x4'])\n\n    # Check that get_feature_names raises an error if there are remaining columns\n    column_transformer = ColumnTransformer(transformers, remainder='passthrough')\n    column_transformer.fit(X)\n    with pytest.raises(NotImplementedError):\n        column_transformer.get_feature_names()\n"], "sample_844": ["def test_optics_xi_predecessor_correction():\n    # Test predecessor correction in _xi_cluster function\n    reachability_plot = np.array([10, 8.9, 8.8, 8.7, 7, 6, np.inf])\n    predecessor_plot = np.array([0, 1, 2, 3, 4, 5, 6])\n    ordering = np.array([0, 1, 2, 3, 4, 5, 6])\n    xi = 0.05\n    min_samples = 2\n    min_cluster_size = 2\n\n    clusters = _xi_cluster(reachability_plot, predecessor_plot, ordering, xi,\n                           min_samples, min_cluster_size,\n                           predecessor_correction=True)\n\n    assert len(clusters) == 1\n    assert_array_equal(clusters[0], [0, 5])\n\n    clusters = _xi_cluster(reachability_plot, predecessor_plot, ordering, xi,\n                           min_samples, min_cluster_size,\n                           predecessor_correction=False)\n\n    assert len(clusters) == 1\n    assert_array_equal(clusters[0], [0, 6])\n"], "sample_846": ["def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Test get_feature_names with transformers that have get_feature_names\n    ct = ColumnTransformer([\n        ('trans1', StandardScaler(), [0]),\n        ('trans2', OneHotEncoder(), [1])\n    ])\n    ct.fit(X)\n    feature_names = ct.get_feature_names()\n    assert_array_equal(feature_names, ['trans1__x0', 'trans2__x0_1', 'trans2__x0_2'])\n\n    # Test get_feature_names with transformers that do not have get_feature_names\n    ct = ColumnTransformer([\n        ('trans1', Trans(), [0]),\n        ('trans2', OneHotEncoder(), [1])\n    ])\n    ct.fit(X)\n    with pytest.raises(AttributeError):\n        ct.get_feature_names()\n\n    # Test get_feature_names with 'passthrough' transformer\n    ct = ColumnTransformer([\n        ('trans1', 'passthrough', [0]),\n        ('trans2', OneHotEncoder(), [1])\n    ])\n    ct.fit(X)\n    with pytest.raises(NotImplementedError):\n        ct.get_feature_names()\n"], "sample_849": ["def test_build_repr():\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    repr = _build_repr(cv)\n    assert repr == \"KFold(n_splits=5, random_state=42, shuffle=True)\"\n"], "sample_852": ["def test_make_sparse_coded_signal():\n    # Test that the generated signal is indeed sparse\n    n_samples, n_components, n_features = 10, 5, 20\n    n_nonzero_coefs = 3\n\n    y, D, X = make_sparse_coded_signal(n_samples=n_samples,\n                                       n_components=n_components,\n                                       n_features=n_features,\n                                       n_nonzero_coefs=n_nonzero_coefs)\n\n    assert_array_equal(X.shape, (n_components, n_samples))\n    assert_array_equal(D.shape, (n_features, n_components))\n    assert_array_equal(y.shape, (n_samples, n_features))\n\n    for i in range(n_samples):\n        assert np.count_nonzero(X[:, i]) == n_nonzero_coefs\n"], "sample_856": ["def test_build_repr():\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    repr = _build_repr(cv)\n    assert repr == \"KFold(n_splits=5, random_state=42, shuffle=True)\"\n"], "sample_859": ["def test_lasso_path_return_n_iter():\n    # Test that lasso_path return correct n_iter\n    X, y = load_boston(return_X_y=True)\n    alphas, coefs, dual_gaps, n_iters = lasso_path(\n        X, y, return_n_iter=True, max_iter=1000, tol=1e-3)\n    assert_array_equal(n_iters.shape, alphas.shape)\n    assert np.all(n_iters > 0)\n    assert np.all(n_iters <= 1000)\n"], "sample_860": ["def test_check_is_fitted():\n    class Estimator:\n            self.n_features_in_ = None\n\n            self.n_features_in_ = X.shape[1]\n\n    estimator = Estimator()\n    assert_raises_regex(NotFittedError,\n                        \"This Estimator instance is not fitted yet\",\n                        check_is_fitted, estimator)\n\n    estimator.fit(np.array([[1, 2], [3, 4]]))\n    assert_no_warnings(check_is_fitted, estimator)\n"], "sample_863": ["def test_pipeline_transformer_weights():\n    # Test that transformer weights are correctly passed to the transformers\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    transf1 = Transf()\n    transf2 = Transf()\n\n    pipe = FeatureUnion([\n        ('transf1', transf1),\n        ('transf2', transf2)\n    ], transformer_weights={'transf1': 2, 'transf2': 3})\n\n    pipe.fit(X, y)\n    X_trans = pipe.transform(X)\n\n    assert_array_equal(X_trans[:, :2], 2 * X)\n    assert_array_equal(X_trans[:, 2:], 3 * X)\n"], "sample_868": ["def test_check_clusterings():\n    # Check that check_clusterings returns the correct values\n    labels_true = np.array([1, 1, 2, 2])\n    labels_pred = np.array([1, 2, 1, 2])\n\n    checked_labels_true, checked_labels_pred = check_clusterings(labels_true, labels_pred)\n\n    assert_array_equal(checked_labels_true, labels_true)\n    assert_array_equal(checked_labels_pred, labels_pred)\n\n    # Check that check_clusterings raises an error for non-1D arrays\n    labels_true_2d = np.array([[1, 1], [2, 2]])\n    with pytest.raises(ValueError):\n        check_clusterings(labels_true_2d, labels_pred)\n\n    # Check that check_clusterings raises an error for arrays of different lengths\n    labels_pred_diff_len = np.array([1, 2, 1])\n    with pytest.raises(ValueError):\n        check_clusterings(labels_true, labels_pred_diff_len)\n"], "sample_869": ["def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Bad beta\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, beta=0.0)\n\n    # Bad average option\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, average='invalid')\n\n    # Bad pos_label\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, pos_label='invalid',\n                                        average='binary')\n"], "sample_874": ["def test_clone():\n    # Tests that clone creates a new estimator with the same parameters.\n    selector = StepSelector(step=3)\n    new_selector = clone(selector)\n\n    assert new_selector.get_params() == selector.get_params()\n    assert new_selector is not selector\n"], "sample_878": ["def test_column_transformer_feature_names_out_pandas():\n    X_array = np.array([[1, 2], [3, 4]])\n    X_df = pd.DataFrame(X_array, columns=[\"col1\", \"col2\"])\n\n    # Use pandas DataFrame with ColumnTransformer\n    ct = ColumnTransformer(\n        [\n            (\"trans1\", Trans(), [\"col1\"]),\n            (\"trans2\", Trans(), [\"col2\"]),\n        ],\n        verbose_feature_names_out=False,\n    )\n    ct.fit(X_df)\n    assert_array_equal(ct.get_feature_names_out(), [\"col1\", \"col2\"])\n\n    # Use numpy array with ColumnTransformer\n    ct = ColumnTransformer(\n        [\n            (\"trans1\", Trans(), [0]),\n            (\"trans2\", Trans(), [1]),\n        ],\n        verbose_feature_names_out=False,\n    )\n    ct.fit(X_array)\n    assert_array_equal(ct.get_feature_names_out(), [\"x0\", \"x1\"])\n"], "sample_880": ["def test_type_of_target_invalid_input():\n    # Test that an error is raised when the input is not array-like\n    invalid_inputs = [\n        \"string\",\n        123,\n        None,\n        {\"a\": 1, \"b\": 2},\n        {1, 2, 3},\n        (1, 2, 3),  # tuple with no arrays or sequences\n    ]\n    for y in invalid_inputs:\n        with pytest.raises(ValueError, match=\"Expected array-like\"):\n            type_of_target(y)\n\n    # Test that an error is raised when the input is a sequence of non-sequences\n    # and non-arrays\n    invalid_sequence_inputs = [\n        [\"a\", 1, None],\n        [1, 2, \"three\"],\n        [(1, 2), 3, \"four\"],\n    ]\n    for y in invalid_sequence_inputs:\n        with pytest.raises(ValueError, match=\"Unknown label type\"):\n            type_of_target(y)\n"], "sample_885": ["def test_generate_invalid_param_val(constraint, invalid_val):\n    assert not constraint.is_satisfied_by(invalid_val)\n    generated_invalid_val = generate_invalid_param_val(constraint)\n    assert not constraint.is_satisfied_by(generated_invalid_val)\n"], "sample_891": ["def test_top_k_accuracy_score_binary():\n    # Test top-k accuracy score for binary classification\n    y_true = np.array([0, 1, 1, 0, 1])\n    y_score = np.array([[0.2, 0.8], [0.7, 0.3], [0.9, 0.1], [0.1, 0.9], [0.6, 0.4]])\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=1), 0.6)\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=2), 1.0)\n"], "sample_895": ["def test_column_transformer_feature_names_out_pandas():\n    # Test that the output of get_feature_names_out is correct when using pandas\n    X = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n    transformer = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"A\"]), (\"trans2\", Trans(), [\"B\"])]\n    )\n    transformer.fit(X)\n    assert_array_equal(\n        transformer.get_feature_names_out(),\n        np.array([\"trans1__A\", \"trans2__B\"]),\n    )\n\n    # Check with verbose_feature_names_out=False\n    transformer.verbose_feature_names_out = False\n    assert_array_equal(transformer.get_feature_names_out(), np.array([\"A\", \"B\"]))\n"], "sample_896": ["def test_nmf_multiplicative_update_sparse():\n    # Test that NMF with multiplicative update gives the same result for dense and sparse input\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(10, 5))\n    X_sparse = csc_matrix(X)\n\n    nmf_dense = NMF(solver=\"mu\", random_state=0)\n    nmf_sparse = NMF(solver=\"mu\", random_state=0)\n\n    W_dense = nmf_dense.fit_transform(X)\n    W_sparse = nmf_sparse.fit_transform(X_sparse)\n\n    assert_array_almost_equal(W_dense, W_sparse)\n"], "sample_897": ["def test_PartialDependenceDisplay_categorical_features(clf_diabetes, diabetes):\n    # Check that categorical features are properly handled\n    categorical_features = [1, 3]\n    feature_names = [\"feature_{}\".format(i) for i in range(diabetes.data.shape[1])]\n    PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        features=[(0,), (1, 2)],\n        categorical_features=categorical_features,\n        feature_names=feature_names,\n    )\n"], "sample_899": ["def test_check_estimator_sparse_data():\n    # Test that check_estimator works with sparse data\n    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],\n                      random_state=0, n_features=2, cluster_std=0.1)\n    X_sparse = sp.csr_matrix(X)\n\n    class DummyEstimator(BaseEstimator):\n            return self\n\n            return np.ones(X.shape[0])\n\n    estimator = DummyEstimator()\n    check_estimator(estimator, generate_only=True)\n\n    # Check that the estimator can handle sparse data\n    estimator.fit(X_sparse, y)\n    estimator.predict(X_sparse)\n"], "sample_903": ["def test_tsne_perplexity():\n    # Test perplexity parameter of TSNE\n    methods = ['exact', 'barnes_hut']\n    for method in methods:\n        tsne = TSNE(method=method, random_state=0)\n        X_embedded = tsne.fit_transform(X_2d_grid)\n        assert_almost_equal(tsne.kl_divergence_, 0.0, decimal=3)\n\n        tsne = TSNE(perplexity=1, method=method, random_state=0)\n        X_embedded = tsne.fit_transform(X_2d_grid)\n        assert_greater(tsne.kl_divergence_, 0.0)\n"], "sample_905": ["def test_signature_from_str():\n    sig = inspect.signature_from_str(\"(a: int, b: str) -> None\")\n    assert len(sig.parameters) == 2\n    assert sig.parameters[\"a\"].annotation == int\n    assert sig.parameters[\"b\"].annotation == str\n    assert sig.return_annotation == type(None)\n\n    sig = inspect.signature_from_str(\"(a: int, b: str = 'default') -> None\")\n    assert len(sig.parameters) == 2\n    assert sig.parameters[\"a\"].annotation == int\n    assert sig.parameters[\"b\"].annotation == str\n    assert sig.parameters[\"b\"].default == \"default\"\n    assert sig.return_annotation == type(None)\n"], "sample_910": ["def test_SphinxLogRecordTranslator():\n    class MockApp:\n        env = None\n\n    class MockEnv:\n            return \"path/to/\" + docname\n\n    app = MockApp()\n    app.env = MockEnv()\n\n    translator = logging.SphinxLogRecordTranslator(app)\n    record = logging.SphinxWarningLogRecord(\"name\", 10, \"path/to/file.py\", 10, \"message\", None, None)\n    record.location = (\"docname\", 10)\n\n    assert translator.filter(record) is True\n    assert record.location == \"path/to/docname:10\"\n\n    record = logging.SphinxWarningLogRecord(\"name\", 10, \"path/to/file.py\", 10, \"message\", None, None)\n    record.location = nodes.Node()\n\n    assert translator.filter(record) is True\n    assert record.location is None\n\n    record = logging.SphinxWarningLogRecord(\"name\", 10, \"path/to/file.py\", 10, \"message\", None, None)\n    record.location = \"docname\"\n\n    assert translator.filter(record) is True\n    assert record.location == \"path/to/docname\"\n"], "sample_915": ["def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__')\n    assert not inspect.is_builtin_class_method(datetime.datetime, '__init__')\n"], "sample_921": ["def test_is_builtin_class_method():\n    assert is_builtin_class_method(int, '__init__') is True\n    assert is_builtin_class_method(str, '__new__') is True\n    assert is_builtin_class_method(list, '__repr__') is True\n    assert is_builtin_class_method(datetime.datetime, '__init__') is False\n\n    class CustomClass:\n        pass\n\n    assert is_builtin_class_method(CustomClass, '__init__') is False\n"], "sample_922": ["def test_parse_annotation():\n    annotation = \"Union[int, str]\"\n    expected = [\n        addnodes.desc_sig_name('', 'Union'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n"], "sample_923": ["def test_anon_union():\n    check(\"type\", \"union { int a; float b; }\", {1: \"union-anon-TestDoc\", 2: \"union-anon-TestDoc\"},\n          key=\"anon\")\n"], "sample_939": ["def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    func_def = tree.body[0]\n    assert ast.unparse(func_def.args) == expected\n"], "sample_940": ["def test_is_builtin_class_method():\n    assert is_builtin_class_method(int, '__init__') is True\n    assert is_builtin_class_method(str, '__init__') is True\n    assert is_builtin_class_method(datetime.datetime, '__init__') is False\n"], "sample_942": ["def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"List[Tuple[int, str]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"Dict[str, int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n"], "sample_945": ["def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[str, int]\"\n    expected_nodes = [\n        addnodes.pending_xref(\"\", \"Union\", refdomain=\"py\", reftype=\"class\"),\n        nodes.Text(\"[\"),\n        addnodes.pending_xref(\"\", \"str\", refdomain=\"py\", reftype=\"class\"),\n        nodes.Text(\", \"),\n        addnodes.pending_xref(\"\", \"int\", refdomain=\"py\", reftype=\"class\"),\n        nodes.Text(\"]\"),\n    ]\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected_nodes)\n"], "sample_946": ["def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[int, str]\"\n    expected_nodes = [\n        addnodes.desc_sig_name('', 'Union'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected_nodes)\n"], "sample_947": ["def test_type_with_id_attributes():\n    check('type', 'int LIGHTGBM_C_EXPORT foo', {1: 'c-type-foo', 2: 'c-type-foo'},\n          key='typedef')\n"], "sample_950": ["def test_parse_annotation():\n    env = Mock()\n    annotations = _parse_annotation('int', env)\n    assert len(annotations) == 1\n    assert isinstance(annotations[0], pending_xref)\n\n    annotations = _parse_annotation('List[int]', env)\n    assert len(annotations) == 3\n    assert isinstance(annotations[0], desc_sig_name)\n    assert isinstance(annotations[1], desc_sig_punctuation)\n    assert isinstance(annotations[2], pending_xref)\n\n    annotations = _parse_annotation('Dict[str, List[int]]', env)\n    assert len(annotations) == 7\n    assert isinstance(annotations[0], desc_sig_name)\n    assert isinstance(annotations[1], desc_sig_punctuation)\n    assert isinstance(annotations[2], pending_xref)\n    assert isinstance(annotations[3], desc_sig_punctuation)\n    assert isinstance(annotations[4], desc_sig_name)\n    assert isinstance(annotations[5], desc_sig_punctuation)\n    assert isinstance(annotations[6], pending_xref)\n"], "sample_951": ["def test_signature_from_ast():\n    code = \"def func(a: int, b: str = 'hello', *args, **kwargs) -> None: pass\"\n    tree = ast.parse(code)\n    func_def = tree.body[0]\n    sig = inspect.signature_from_ast(func_def, code)\n    assert len(sig.parameters) == 4\n    assert sig.parameters['a'].annotation == int\n    assert sig.parameters['b'].annotation == str\n    assert sig.parameters['b'].default == 'hello'\n    assert sig.parameters['args'].kind == Parameter.VAR_POSITIONAL\n    assert sig.parameters['kwargs'].kind == Parameter.VAR_KEYWORD\n    assert sig.return_annotation is None\n"], "sample_952": ["def test_signature_from_ast():\n    code = \"def func(a: int, b: str = 'hello', *args, **kwargs) -> None: pass\"\n    tree = ast.parse(code)\n    func_def = tree.body[0]\n    sig = inspect.signature_from_ast(func_def, code)\n    assert stringify_signature(sig) == \"(a: int, b: str = 'hello', *args, **kwargs) -> None\"\n\n    # Test with positional-only parameters (Python 3.8+)\n    if sys.version_info >= (3, 8):\n        code = \"def func(a: int, /, b: str = 'hello', *args, **kwargs) -> None: pass\"\n        tree = ast.parse(code)\n        func_def = tree.body[0]\n        sig = inspect.signature_from_ast(func_def, code)\n        assert stringify_signature(sig) == \"(a: int, /, b: str = 'hello', *args, **kwargs) -> None\"\n"], "sample_961": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # simple annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n\n    # complex annotation\n    annotation = \"List[int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 3\n    assert isinstance(result[0], nodes.Text)\n    assert isinstance(result[1], pending_xref)\n    assert isinstance(result[2], nodes.Text)\n\n    # annotation with module name\n    annotation = \"typing.List[int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 3\n    assert isinstance(result[0], nodes.Text)\n    assert isinstance(result[1], pending_xref)\n    assert isinstance(result[2], nodes.Text)\n"], "sample_964": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # simple annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n\n    # annotation with module name\n    annotation = \"typing.List[int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"typing.\"\n    assert isinstance(result[1], pending_xref)\n    assert isinstance(result[2], desc_sig_punctuation)\n    assert result[2].astext() == \"[\"\n    assert isinstance(result[3], pending_xref)\n    assert isinstance(result[4], desc_sig_punctuation)\n    assert result[4].astext() == \"]\"\n\n    # annotation with nested types\n    annotation = \"typing.Dict[str, typing.List[int]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 13\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"typing.\"\n    assert isinstance(result[1], pending_xref)\n    assert isinstance(result[2], desc_sig_punctuation)\n    assert result[2].astext() == \"[\"\n    assert isinstance(result[3], pending_xref)\n    assert isinstance(result[4], desc_sig_punctuation)\n    assert result[4].astext() == \",\"\n    assert isinstance(result[5], desc_sig_space)\n    assert isinstance(result[6], nodes.Text)\n    assert result[6].astext() == \"typing.\"\n    assert isinstance(result[7], pending_xref)\n    assert isinstance(result[8], desc_sig_punctuation)\n    assert result[8].astext() == \"[\"\n    assert isinstance(result[9], pending_xref)\n    assert isinstance(result[10], desc_sig_punctuation)\n    assert result[10].astext() == \"]\"\n    assert isinstance(result[11], desc_sig_punctuation)\n    assert result[11].astext() == \"]\"\n    assert isinstance(result[12], desc_sig_punctuation)\n    assert result[12].astext() == \"]\"\n"], "sample_965": ["def test_signature_from_ast():\n    code = 'def func(a: int, b: str = \"hello\", *, c: float = 1.0) -> None: pass'\n    module = ast.parse(code)\n    function = module.body[0]\n\n    sig = inspect.signature_from_ast(function, code)\n    assert len(sig.parameters) == 3\n    assert sig.parameters['a'].annotation == int\n    assert sig.parameters['b'].annotation == str\n    assert sig.parameters['c'].annotation == float\n    assert sig.return_annotation is None\n"], "sample_966": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # Test parsing of simple annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n\n    # Test parsing of complex annotation\n    annotation = \"List[str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 3\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], desc_sig_punctuation)\n    assert isinstance(result[2], pending_xref)\n\n    # Test parsing of annotation with spaces\n    annotation = \" List [ str ] \"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert isinstance(result[0], desc_sig_space)\n    assert isinstance(result[1], pending_xref)\n    assert isinstance(result[2], desc_sig_punctuation)\n    assert isinstance(result[3], pending_xref)\n    assert isinstance(result[4], desc_sig_space)\n\n    # Test parsing of invalid annotation\n    annotation = \"InvalidAnnotation\"\n    with pytest.raises(SyntaxError):\n        _parse_annotation(annotation, env)\n"], "sample_970": ["def test_signature_from_ast():\n    code = 'def func(a: int, b: str = \"hello\", *args, **kwargs) -> None: pass'\n    module = ast.parse(code)\n    function = module.body[0]\n\n    sig = inspect.signature_from_ast(function, code)\n    assert len(sig.parameters) == 4\n    assert sig.parameters['a'].annotation == int\n    assert sig.parameters['b'].annotation == str\n    assert sig.parameters['b'].default == \"hello\"\n    assert sig.parameters['args'].kind == Parameter.VAR_POSITIONAL\n    assert sig.parameters['kwargs'].kind == Parameter.VAR_KEYWORD\n    assert sig.return_annotation is None\n"], "sample_971": ["def test_SphinxLogRecordTranslator():\n    class MockApp:\n            self.env = MockEnvironment()\n\n    class MockEnvironment:\n            return f\"mock_path/{docname}\"\n\n    app = MockApp()\n    translator = logging.SphinxLogRecordTranslator(app)\n\n    # Test with tuple location\n    record = logging.SphinxWarningLogRecord(\"name\", 10, \"path\", 1, \"message\", None, None)\n    record.location = (\"docname\", 10)\n    translator.filter(record)\n    assert record.location == \"mock_path/docname:10\"\n\n    # Test with node location\n    node = nodes.Node()\n    node.source = \"source\"\n    node.line = 10\n    record = logging.SphinxWarningLogRecord(\"name\", 10, \"path\", 1, \"message\", None, None)\n    record.location = node\n    translator.filter(record)\n    assert record.location == \"source:10\"\n\n    # Test with string location\n    record = logging.SphinxWarningLogRecord(\"name\", 10, \"path\", 1, \"message\", None, None)\n    record.location = \"docname\"\n    translator.filter(record)\n    assert record.location == \"mock_path/docname\"\n"], "sample_973": ["def test_signature_from_str():\n    sig = inspect.signature_from_str(\"(a, b: int, *, c: str = 'hello') -> None\")\n    assert len(sig.parameters) == 3\n    assert sig.parameters[\"a\"].annotation is Parameter.empty\n    assert sig.parameters[\"b\"].annotation == int\n    assert sig.parameters[\"c\"].annotation == str\n    assert sig.parameters[\"c\"].default == \"hello\"\n    assert sig.return_annotation is None\n"], "sample_981": ["def test_cycle():\n    # Test the Cycle class\n    assert Cycle(1, 2).list() == [0, 2, 1]\n    assert Cycle(1, 2)(2, 3).list() == [0, 3, 1, 2]\n    assert Cycle(1, 2)(2, 3)(4, 5).list() == [0, 3, 1, 2, 5, 4]\n    assert Cycle().list() == []\n    assert Cycle(1).list() == [0, 1]\n    assert Cycle(1, 2, 3).list() == [0, 3, 1, 2]\n    assert Cycle(1, 2, 3)(4).list() == [0, 3, 1, 2, 4]\n    assert Cycle(1, 2, 3)(4, 5).list() == [0, 3, 1, 2, 5, 4]\n    assert Cycle(1, 2, 3)(4, 5)(6).list() == [0, 3, 1, 2, 5, 4, 6]\n    assert Cycle(1, 2, 3)(4, 5)(6, 7).list() == [0, 3, 1, 2, 5, 7, 4, 6]\n"], "sample_983": ["def test_sparse_matrix_col_join():\n    A = SparseMatrix(ones(3))\n    B = SparseMatrix.eye(3)\n    C = A.col_join(B)\n    assert C == A.row_insert(A.rows, Matrix(B))\n"], "sample_986": ["def test_evalf_get_integer_part():\n    assert get_integer_part(123, 1, {}, return_ints=True) == (123, 0)\n    assert get_integer_part(-123, 1, {}, return_ints=True) == (-123, 0)\n    assert get_integer_part(123.456, 1, {}, return_ints=True) == (123, 0)\n    assert get_integer_part(-123.456, 1, {}, return_ints=True) == (-124, 0)\n    assert get_integer_part(E, 1, {}, return_ints=True) == (2, 0)\n    assert get_integer_part(pi, 1, {}, return_ints=True) == (3, 0)\n    assert get_integer_part(I, 1, {}, return_ints=True) == (0, 1)\n    assert get_integer_part(-I, 1, {}, return_ints=True) == (0, -1)\n"], "sample_987": ["def test_evalf_get_integer_part():\n    assert get_integer_part(123, 1, {}, return_ints=True) == (123, 0)\n    assert get_integer_part(-123, 1, {}, return_ints=True) == (-123, 0)\n    assert get_integer_part(123.456, 1, {}, return_ints=True) == (123, 0)\n    assert get_integer_part(-123.456, 1, {}, return_ints=True) == (-123, 0)\n    assert get_integer_part(E, 1, {}, return_ints=True) == (2, 0)\n    assert get_integer_part(pi, 1, {}, return_ints=True) == (3, 0)\n    assert get_integer_part(I, 1, {}, return_ints=True) == (0, 1)\n    assert get_integer_part(-I, 1, {}, return_ints=True) == (0, -1)\n"], "sample_988": ["def test_Rational_new():\n    assert Rational(1, 2) == Rational(1, 2)\n    assert Rational(1, 3) != Rational(1, 2)\n    assert Rational(0, 2) == S.Zero\n    assert Rational(1, 1) == S.One\n    assert Rational(-1, 1) == S.NegativeOne\n    assert Rational(1, -1) == S.NegativeOne\n    assert Rational(-1, -1) == S.One\n    assert Rational(0, 0) == S.NaN\n    assert Rational(1, 0) == S.Infinity\n    assert Rational(-1, 0) == S.NegativeInfinity\n"], "sample_989": ["def test_Float_hash():\n    assert hash(Float(1.0)) == hash(Float(1.0, 2))\n    assert hash(Float(1.0)) != hash(Float(1.0, 3))\n    assert hash(Float(1.0)) != hash(Float(1.1))\n"], "sample_990": ["def test_hyperbolic_rewrite_as_exp():\n    x = Symbol('x')\n    assert sinh(x)._eval_rewrite_as_exp(x) == (exp(x) - exp(-x)) / 2\n    assert cosh(x)._eval_rewrite_as_exp(x) == (exp(x) + exp(-x)) / 2\n    assert tanh(x)._eval_rewrite_as_exp(x) == (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n    assert coth(x)._eval_rewrite_as_exp(x) == (exp(x) + exp(-x)) / (exp(x) - exp(-x))\n    assert sech(x)._eval_rewrite_as_exp(x) == 2 / (exp(x) + exp(-x))\n    assert csch(x)._eval_rewrite_as_exp(x) == 2 / (exp(x) - exp(-x))\n"], "sample_997": ["def test_convert_equals_signs():\n    # Test that equals signs are correctly converted to Eq instances\n    assert parse_expr(\"1=2\", transformations=(standard_transformations +\n                                              (convert_equals_signs,))) == Eq(1, 2)\n    assert parse_expr(\"x=y\", transformations=(standard_transformations +\n                                              (convert_equals_signs,))) == Eq(Symbol('x'), Symbol('y'))\n    assert parse_expr(\"(1=2)=False\", transformations=(standard_transformations +\n                                                      (convert_equals_signs,))) == Eq(Eq(1, 2), False)\n"], "sample_999": ["def test_latex_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == \"1 + 2 i + 3 j + 4 k\"\n"], "sample_1001": ["def test_latex_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == r\"1 + 2 i + 3 j + 4 k\"\n"], "sample_1003": ["def test_Options_clone():\n    options = Options((x, y), {'domain': 'ZZ'})\n    cloned_options = options.clone({'domain': 'QQ'})\n\n    assert cloned_options['domain'] == QQ\n    assert options['domain'] == ZZ\n"], "sample_1008": ["def test_coordinate_sym():\n    A = ReferenceFrame('A')\n    q1, q2, q3 = dynamicsymbols('q1 q2 q3')\n    B = A.orientnew('B', 'Body', [q1, q2, q3], '123')\n    assert isinstance(A[0], CoordinateSym)\n    assert isinstance(B[1], CoordinateSym)\n    assert A.variable_map(B) == {A[0]: B[0]*cos(q2)*cos(q3) - B[1]*sin(q3) + B[2]*sin(q2)*cos(q3),\n                                 A[1]: B[0]*(sin(q1)*sin(q2)*cos(q3) + sin(q3)*cos(q1)) + B[1]*(sin(q1)*sin(q3) - sin(q2)*cos(q1)*cos(q3)) - B[2]*(sin(q1)*cos(q2)*cos(q3) - sin(q2)*sin(q3)*cos(q1)),\n                                 A[2]: B[0]*(sin(q1)*sin(q3) - sin(q2)*cos(q1)*cos(q3)) - B[1]*(sin(q1)*sin(q2)*cos(q3) + sin(q3)*cos(q1)) - B[2]*(sin(q1)*sin(q2)*sin(q3) - cos(q1)*cos(q2)*cos(q3))}\n"], "sample_1009": ["def test_vector_diff():\n    q1, q2 = dynamicsymbols('q1 q2')\n    u1, u2 = dynamicsymbols('u1 u2')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.y])\n    v = u1 * A.x + u2 * N.y\n    assert v.diff(u1, N) == A.x\n    assert v.diff(q1, N) == -u1 * A.z\n    assert v.diff(u2, N) == N.y\n    assert v.diff(q2, N) == Vector(0)\n"], "sample_1012": ["def test_PythonCodePrinter_print_While():\n    whl = While(x < 5, AugAssign(x, '+', 1))\n    assert PythonCodePrinter().doprint(whl) == \"while x < 5:\\n    x += 1\"\n"], "sample_1019": ["def test_factor_terms_fraction():\n    assert factor_terms(x/2 + 1/x, fraction=True) == (x**2 + 2)/(2*x)\n    assert factor_terms(x/2 + 1/x, fraction=False) == x/2 + 1/x\n    assert factor_terms(x/2/y + 1/x/y, fraction=True) == (x**2 + 2)/(2*x*y)\n    assert factor_terms(x/2/y + 1/x/y, fraction=False) == (x/2 + 1/x)/y\n"], "sample_1021": ["def test_quaternion_inverse():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(x, y, z, w)\n    assert q1.inverse() * q1 == Quaternion(1, 0, 0, 0)\n    assert q2.inverse() * q2 == Quaternion(1, 0, 0, 0)\n    raises(ValueError, lambda: Quaternion(0, 0, 0, 0).inverse())\n"], "sample_1022": ["def test_factorial_notation():\n    # Test factorial notation with numbers and symbols\n    assert parse_expr('5!', transformations=standard_transformations) == sympy.factorial(5)\n    assert parse_expr('n!', transformations=standard_transformations) == sympy.factorial(sympy.symbols('n'))\n"], "sample_1030": ["def test_convex_hull():\n    p1 = Point(0, 0)\n    p2 = Point(1, 1)\n    p3 = Point(1, -1)\n    p4 = Point(-1, -1)\n    p5 = Point(-1, 1)\n    assert convex_hull(p1, p2, p3, p4, p5) == Polygon(p1, p2, p3, p4, p5)\n\n    # Test with collinear points\n    p6 = Point(2, 2)\n    p7 = Point(3, 3)\n    assert convex_hull(p1, p2, p6, p7) == Segment(p1, p7)\n\n    # Test with single point\n    assert convex_hull(p1) == p1\n\n    # Test with two points\n    assert convex_hull(p1, p2) == Segment(p1, p2)\n"], "sample_1031": ["def test_quantity_scale_factors():\n    # Test that scale factors are correctly set for quantities\n    assert Quantity.SI_quantity_scale_factors[\"c\"] == 299792458\n    assert Quantity.SI_quantity_scale_factors[\"kg\"] == 1\n    assert Quantity.SI_quantity_scale_factors[\"m\"] == 1\n    assert Quantity.SI_quantity_scale_factors[\"s\"] == 1\n\n    # Test that scale factors are correctly set for derived quantities\n    assert Quantity.SI_quantity_scale_factors[\"newton\"] == kg * m / s**2\n    assert Quantity.SI_quantity_scale_factors[\"joule\"] == newton * m\n    assert Quantity.SI_quantity_scale_factors[\"watt\"] == joule / s\n"], "sample_1036": ["def test_Mul_doit():\n    assert Mul(2, 3).doit() == 6\n    assert Mul(2, 3, evaluate=False).doit() == 6\n    assert Mul(x, x, evaluate=False).doit() == x**2\n    assert Mul(x, 2*x, evaluate=False).doit() == 2*x**2\n"], "sample_1039": ["def test_mathml_functions():\n    # Test some basic math functions\n    assert mp.doprint(sin(x)) == '<apply><sin/><ci>x</ci></apply>'\n    assert mpp.doprint(sin(x)) == '<mrow><mi>sin</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(cos(x)) == '<apply><cos/><ci>x</ci></apply>'\n    assert mpp.doprint(cos(x)) == '<mrow><mi>cos</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(tan(x)) == '<apply><tan/><ci>x</ci></apply>'\n    assert mpp.doprint(tan(x)) == '<mrow><mi>tan</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(asin(x)) == '<apply><arcsin/><ci>x</ci></apply>'\n    assert mpp.doprint(asin(x)) == '<mrow><mi>arcsin</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(acos(x)) == '<apply><arccos/><ci>x</ci></apply>'\n    assert mpp.doprint(acos(x)) == '<mrow><mi>arccos</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(atan(x)) == '<apply><arctan/><ci>x</ci></apply>'\n    assert mpp.doprint(atan(x)) == '<mrow><mi>arctan</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n"], "sample_1040": ["def test_MathMLPrinter_print_AssocOp():\n    expr = (x + y) + x\n    assert mp.doprint(expr) == '<apply><plus/><ci>x</ci><ci>y</ci><ci>x</ci></apply>'\n    assert mpp.doprint(expr) == '<mrow><mi>x</mi><mo>+</mo><mi>y</mi><mo>+</mo><mi>x</mi></mrow>'\n"], "sample_1049": ["def test_plane_projection():\n    a = Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))\n    b = Point3D(1, 1)\n    c = a.projection(b)\n    assert c in a\n    assert a.projection(c) == c\n\n    d = Line3D(Point3D(1, 1, 1), Point3D(2, 2, 2))\n    e = a.projection(d)\n    assert e == Point3D(1, 1, 1)\n\n    f = Line3D(Point3D(1, 1), Point3D(2, 2))\n    g = a.projection(f)\n    assert g.p1 in a\n    assert g.p2 in a\n"], "sample_1050": ["def test_printing_Piecewise_with_NumPyPrinter():\n    expr = Piecewise((x, x < 1), (x**2, x >= 1))\n    assert NumPyPrinter().doprint(expr) == 'numpy.select([x < 1, x >= 1], [x, x**2], default=numpy.nan)'\n"], "sample_1055": ["def test_gm_public_key():\n    p = 257\n    q = 353\n    a, N = gm_public_key(p, q)\n    assert isprime(p)\n    assert isprime(q)\n    assert _legendre(a, p) == -1\n    assert _legendre(a, q) == -1\n    assert N == p * q\n"], "sample_1058": ["def test_PythonCodePrinter_print_While():\n    whl = While(x < 5, AugAssign(x, '+', 1))\n    assert PythonCodePrinter().doprint(whl) == \"while x < 5:\\n    x += 1\"\n"], "sample_1061": ["def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(-27, 3) == (-3, True)\n    assert integer_nthroot(1, 1) == (1, True)\n    assert integer_nthroot(0, 1) == (0, True)\n    assert integer_nthroot(0, 2) == (0, True)\n    raises(ValueError, lambda: integer_nthroot(-1, 2))\n    raises(ValueError, lambda: integer_nthroot(2, -1))\n    raises(ValueError, lambda: integer_nthroot(2, 0))\n"], "sample_1062": ["def test_TR14():\n    assert TR14((cos(x) - 1)*(cos(x) + 1)) == -sin(x)**2\n    assert TR14((sin(x) - 1)*(sin(x) + 1)) == -cos(x)**2\n    p1 = (cos(x) + 1)*(cos(x) - 1)\n    p2 = (cos(y) - 1)*2*(cos(y) + 1)\n    p3 = (3*(cos(y) - 1))*(3*(cos(y) + 1))\n    assert TR14(p1*p2*p3*(x - 1)) == -18*(x - 1)*sin(x)**2*sin(y)**4\n"], "sample_1064": ["def test_tensorflow_code_Pow():\n    x = Symbol('x')\n    expr = x**2\n    assert tensorflow_code(expr) == \"tensorflow.math.pow(x, 2)\"\n    expr = x**0.5\n    assert tensorflow_code(expr) == \"tensorflow.math.sqrt(x)\"\n"], "sample_1066": ["def test_mathml_core():\n    m = Matrix(2, 2, [1, 2, 3, 4])\n    assert mp.doprint(m) == (\n        '<matrix><matrixrow><cn>1</cn><cn>2</cn></matrixrow>'\n        '<matrixrow><cn>3</cn><cn>4</cn></matrixrow></matrix>'\n    )\n    assert mpp.doprint(m) == (\n        '<mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd></mtr>'\n        '<mtr><mtd><mn>3</mn></mtd><mtd><mn>4</mn></mtd></mtr></mtable>'\n    )\n\n    i = Interval(0, 1)\n    assert mp.doprint(i) == (\n        '<apply><interval><cn>0</cn><cn>1</cn></interval></apply>'\n    )\n    assert mpp.doprint(i) == (\n        '<mfenced close=\"]\" open=\"[\"><mn>0</mn><mn>1</mn></mfenced>'\n    )\n\n    p = Rational(3, 7)\n    assert mp.doprint(p) == (\n        '<apply><divide><cn>3</cn><cn>7</cn></divide></apply>'\n    )\n    assert mpp.doprint(p) == (\n        '<mfrac><mn>3</mn><mn>7</mn></mfrac>'\n    )\n\n    f = FiniteSet(1, 2, 3)\n    assert mp.doprint(f) == (\n        '<set><cn>1</cn><cn>2</cn><cn>3</cn></set>'\n    )\n    assert mpp.doprint(f) == (\n        '<mfenced close=\"}\" open=\"{\"><mn>1</mn><mn>2</mn><mn>3</mn></mfenced>'\n    )\n"], "sample_1077": ["def test_ComplexRegion_contains():\n    a, b = Interval(2, 5), Interval(4, 8)\n    c = Interval(0, 2*pi)\n    c1 = ComplexRegion(a*b)\n    assert 3 + 5*I in c1\n    assert 10 + 5*I not in c1\n    c2 = ComplexRegion(a*c, polar=True)\n    assert 3 + 5*I in c2\n    assert 10 + 5*I not in c2\n    assert 0 + 0*I in c2\n    assert 2 + 0*I in c2\n    assert 2 + 2*pi*I in c2\n    assert 3 + 2*pi*I not in c2\n"], "sample_1079": ["def test_point_transform():\n    p = Point(1, 2)\n    assert p.transform(Matrix([[1, 0, 1], [0, 1, 1], [0, 0, 1]])) == Point(2, 3)\n    p3d = Point3D(1, 2, 3)\n    assert p3d.transform(Matrix([[1, 0, 0, 1], [0, 1, 0, 1], [0, 0, 1, 1], [0, 0, 0, 1]])) == Point3D(2, 3, 4)\n"], "sample_1081": ["def test_smoothness_p():\n    assert smoothness_p(10431) == (-1, [(3, (2, 2, 2)), (19, (1, 3, 9)), (61, (1, 5, 5))])\n    assert smoothness_p(10431, m=1) == (1, [(3, (2, 2, 4)), (19, (1, 5, 5)), (61, (1, 31, 31))])\n    assert smoothness_p(10431, power=1) == (-1, [(3, (2, 2, 2)), (61, (1, 5, 5)), (19, (1, 3, 9))])\n    assert smoothness_p({3: 2, 19: 1, 61: 1}) == 'p**i=3**2 has p-1 B=2, B-pow=2\\np**i=19**1 has p-1 B=3, B-pow=9\\np**i=61**1 has p-1 B=5, B-pow=5'\n    assert smoothness_p('p**i=3**2 has p-1 B=2, B-pow=2\\np**i=19**1 has p-1 B=3, B-pow=9\\np**i=61**1 has p-1 B=5, B-pow=5') == {3: 2, 19: 1, 61: 1}\n"], "sample_1082": ["def test_hyperbolic_rewrites():\n    x = Symbol('x')\n    assert sinh(x).rewrite(exp) == (exp(x) - exp(-x))/2\n    assert cosh(x).rewrite(exp) == (exp(x) + exp(-x))/2\n    assert tanh(x).rewrite(exp) == (exp(x) - exp(-x))/(exp(x) + exp(-x))\n    assert coth(x).rewrite(exp) == (exp(x) + exp(-x))/(exp(x) - exp(-x))\n    assert sech(x).rewrite(exp) == 2/(exp(x) + exp(-x))\n    assert csch(x).rewrite(exp) == 2/(exp(x) - exp(-x))\n    assert asinh(x).rewrite(log) == log(x + sqrt(x**2 + 1))\n    assert acosh(x).rewrite(log) == log(x + sqrt(x - 1)*sqrt(x + 1))\n    assert atanh(x).rewrite(log) == log((1 + x)/(1 - x))/2\n    assert acoth(x).rewrite(log) == log((x + 1)/(x - 1))/2\n    assert asech(x).rewrite(log) == log(1/x + sqrt(1/x - 1)*sqrt(1/x + 1))\n    assert acsch(x).rewrite(log) == log(1/x + sqrt(1/x**2 + 1))\n"], "sample_1083": ["def test_hyperbolic_rewrite_as_exp():\n    x = symbols('x')\n    assert sinh(x)._eval_rewrite_as_exp(x) == (exp(x) - exp(-x)) / 2\n    assert cosh(x)._eval_rewrite_as_exp(x) == (exp(x) + exp(-x)) / 2\n    assert tanh(x)._eval_rewrite_as_exp(x) == (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n    assert coth(x)._eval_rewrite_as_exp(x) == (exp(x) + exp(-x)) / (exp(x) - exp(-x))\n    assert sech(x)._eval_rewrite_as_exp(x) == 2 / (exp(x) + exp(-x))\n    assert csch(x)._eval_rewrite_as_exp(x) == 2 / (exp(x) - exp(-x))\n"], "sample_1086": ["def test_print_MatMul():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert sstr(A*B) == 'A*B'\n    assert sstr(-A*B) == '-A*B'\n    assert sstr(2*A*B) == '2*A*B'\n"], "sample_1091": ["def test_relational_simplify():\n    # issue 16587\n    assert Eq(x, x + 1).simplify() is S.false\n    assert Eq(x, x - 1).simplify() is S.false\n    assert Eq(x, x*2).simplify() is S.false\n    assert Eq(x, x/2).simplify() is S.false\n\n    assert Ne(x, x + 1).simplify() is S.true\n    assert Ne(x, x - 1).simplify() is S.true\n    assert Ne(x, x*2).simplify() is S.true\n    assert Ne(x, x/2).simplify() is S.true\n\n    assert Lt(x, x + 1).simplify() is S.true\n    assert Lt(x, x - 1).simplify() is S.false\n    assert Lt(x, x*2).simplify() is S.true\n    assert Lt(x, x/2).simplify() is S.false\n\n    assert Le(x, x + 1).simplify() is S.true\n    assert Le(x, x - 1).simplify() is S.false\n    assert Le(x, x*2).simplify() is S.true\n    assert Le(x, x/2).simplify() is S.false\n\n    assert Gt(x, x + 1).simplify() is S.false\n    assert Gt(x, x - 1).simplify() is S.true\n    assert Gt(x, x*2).simplify() is S.false\n    assert Gt(x, x/2).simplify() is S.true\n\n    assert Ge(x, x + 1).simplify() is S.false\n    assert Ge(x, x - 1).simplify() is S.true\n    assert Ge(x, x*2).simplify() is S.false\n    assert Ge(x, x/2).simplify() is S.true\n"], "sample_1093": ["def test_PythonCodePrinter_print_Rational():\n    assert PythonCodePrinter().doprint(Rational(1, 2)) == '1/2'\n    assert PythonCodePrinter({'standard': 'python2'}).doprint(Rational(1, 2)) == '1./2.'\n"], "sample_1095": ["def test_AppliedPermutation():\n    x = Symbol('x', integer=True)\n    p = Permutation(0, 1, 2)\n    ap = AppliedPermutation(p, x)\n\n    assert unchanged(AppliedPermutation, p, x) == ap\n    assert ap.subs(x, 1) == 2\n    assert ap.subs(x, 3) == 3\n\n    raises(ValueError, lambda: AppliedPermutation(1, x))\n    raises(ValueError, lambda: AppliedPermutation(p, 1.5))\n\n    assert sstr(ap) == 'AppliedPermutation((0 1 2), x)'\n    assert srepr(ap) == \"AppliedPermutation(Permutation([1, 2, 0]), Symbol('x'))\"\n    assert pretty(ap) == pretty('AppliedPermutation((0 1 2), x)')\n    assert latex(ap) == latex('AppliedPermutation((0 1 2), x)')\n"], "sample_1097": ["def test_blockcut():\n    M = ImmutableMatrix(4, 4, range(16))\n    B = blockcut(M, (1, 3), (1, 3))\n    assert isinstance(B, BlockMatrix)\n    assert B.blocks[0, 1] == Matrix([[1, 2, 3]])\n    assert B.blocks[1, 0] == Matrix([[4], [8], [12]])\n"], "sample_1098": ["def test_hyper_diff():\n    a, b, c = symbols('a b c')\n    assert hyper((a, b), (c,), x).diff(x) == a*b*hyper((a+1, b+1), (c+1,), x)/c\n    assert hyper((a, b), (c, d), x).diff(x) == a*b*hyper((a+1, b+1), (c+1, d+1), x)/(c*d)\n    assert hyper([], [], x).diff(x) == 0\n    assert hyper([a], [], x).diff(x) == a*hyper([a+1], [], x)\n    assert hyper([], [b], x).diff(x) == -b*hyper([], [b+1], x)\n"], "sample_1100": ["def test_Pow_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True, positive=True)\n    assert Pow(x, 2, evaluate=False).is_integer\n    assert Pow(y, -1, evaluate=False).is_integer\n    assert not Pow(x, -1, evaluate=False).is_integer\n    assert not Pow(x, Rational(1, 3), evaluate=False).is_integer\n    assert Pow(Pow(x, Rational(1, 3)), 3, evaluate=False).is_integer\n    assert Pow(Pow(x, Rational(1, 3))**3, 0, evaluate=False).is_integer\n    assert Pow(Pow(x, Rational(1, 3))**3, -1, evaluate=False).is_integer\n    assert not Pow(Pow(x, Rational(1, 3))**2, -1, evaluate=False).is_integer\n"], "sample_1103": ["def test_Pow_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True, nonnegative=True)\n    assert Pow(x, 2, evaluate=False).is_integer\n    assert Pow(y, 2, evaluate=False).is_integer\n    assert Pow(x, -2, evaluate=False).is_noninteger is None\n    assert Pow(y, -2, evaluate=False).is_integer\n    assert Pow(3.5, 2).is_integer is False\n    assert Pow(Rational(7, 3), 3).is_integer is True\n    assert Pow(Rational(7, 3), 4).is_integer is False\n    assert Pow(Rational(7, 3), -3).is_integer is True\n    assert Pow(Rational(7, 3), -4).is_integer is False\n    assert Pow(pi, 0).is_integer is True\n    assert Pow(S.Half, 2).is_integer is True\n    assert Pow(S.Half, -2).is_integer is True\n    assert Pow(S.Half, 3).is_integer is False\n    assert Pow(S.Half, -3).is_integer is False\n"], "sample_1104": ["def test_print_Dimension():\n    assert sstr(Dimension(\"length\")) == \"length\"\n"], "sample_1105": ["def test_refine_MatMul():\n    assert refine(MatMul(A, A.T), Q.orthogonal(A)) == Identity(n)\n    assert refine(MatMul(A, A.conjugate()), Q.unitary(A)) == Identity(n)\n    assert refine(MatMul(A, B), Q.orthogonal(A)) == MatMul(A, B)\n"], "sample_1107": ["def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2], [3, 4, 5])) == [1, 3, 2, 4, 5]\n    assert list(roundrobin('AB', 'CD', 'EF')) == ['A', 'C', 'E', 'B', 'D', 'F']\n"], "sample_1108": ["def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2], [3, 4, 5])) == [1, 3, 2, 4, 5]\n    assert list(roundrobin('AB', 'CD', 'EF')) == ['A', 'C', 'E', 'B', 'D', 'F']\n"], "sample_1110": ["def test_pycode_Piecewise():\n    expr = Piecewise((x, x < 1), (x**2, True))\n    assert pycode(expr) == \"((x) if (x < 1) else (x**2))\"\n    assert NumPyPrinter().doprint(expr) == \"numpy.select([x < 1], [x], default=x**2)\"\n"], "sample_1117": ["def test_AskIntegerElementsHandler():\n    assert ask(Q.integer_elements(X), Q.integer_elements(X)) is True\n    assert ask(Q.integer_elements(Y), Q.integer_elements(Y)) is True\n    assert ask(Q.integer_elements(Z), Q.integer_elements(Z)) is True\n    assert ask(Q.integer_elements(A1x1), Q.integer_elements(A1x1)) is True\n    assert ask(Q.integer_elements(B1x1), Q.integer_elements(B1x1)) is True\n    assert ask(Q.integer_elements(C0x0), Q.integer_elements(C0x0)) is True\n    assert ask(Q.integer_elements(V1), Q.integer_elements(V1)) is True\n    assert ask(Q.integer_elements(V2), Q.integer_elements(V2)) is True\n\n    assert ask(Q.integer_elements(Identity(3)), Q.integer_elements(X)) is True\n    assert ask(Q.integer_elements(ZeroMatrix(3, 3)), Q.integer_elements(X)) is True\n    assert ask(Q.integer_elements(OneMatrix(3, 3)), Q.integer_elements(X)) is True\n\n    assert ask(Q.integer_elements(DiagMatrix([1, 2])), Q.integer_elements(X)) is True\n    assert ask(Q.integer_elements(DiagonalMatrix([1, 2])), Q.integer_elements(X)) is True\n\n    assert ask(Q.integer_elements(X + Y), Q.integer_elements(X) & Q.integer_elements(Y)) is True\n    assert ask(Q.integer_elements(X * Y), Q.integer_elements(X) & Q.integer_elements(Y)) is True\n    assert ask(Q.integer_elements(X**2), Q.integer_elements(X)) is True\n    assert ask(Q.integer_elements(Trace(X)), Q.integer_elements(X)) is True\n    assert ask(Q.integer_elements(Determinant(X)), Q.integer_elements(X)) is True\n    assert ask(Q.integer_elements(MatrixSlice(X, (0, 1), (0, 1))), Q.integer_elements(X)) is True\n"], "sample_1119": ["def test_inverse_block():\n    raises(NonSquareMatrixError, lambda: Inverse(A))\n    assert Inverse(C).shape == (n, n)\n    assert Inverse(D).shape == (n, n)\n    assert Inverse(E).shape == (m, n)  # This should raise an error\n    raises(NonSquareMatrixError, lambda: Inverse(E))\n\n    assert Inverse(Identity(n)).doit() == Identity(n)\n\n    raises(NonInvertibleMatrixError, lambda: Inverse(ZeroMatrix(n, n)).doit())\n    raises(NonInvertibleMatrixError, lambda: Inverse(OneMatrix(n, n)).doit())\n\n    assert Inverse(Inverse(C)) == C\n    assert Inverse(C * D) == Inverse(D) * Inverse(C)\n    assert Inverse(MatPow(C, 2)) == Inverse(C)**2\n"], "sample_1120": ["def test_matrix_element():\n    i, j = symbols('i j')\n    assert MatrixElement(A, 0, 0).doit() == A[0, 0]\n    assert MatrixElement(A, i, j).doit() == A[i, j]\n    assert MatrixElement(Identity(n), i, j).doit() == KroneckerDelta(i, j)\n    assert MatrixElement(ZeroMatrix(n, n), i, j).doit() == 0\n    raises(SympifyError, lambda: MatrixElement(A, 'a', j))\n"], "sample_1121": ["def test_Mul_coeff():\n    # these coefficients have been pulled off of an Add\n    assert Mul(2.0, x + 1).as_coeff_Mul()[0] == 2.0\n    assert Mul(2*x, x + 1).as_coeff_Mul()[0] == 2*x\n    assert Mul(x/2, x + 1).as_coeff_Mul()[0] == x/2\n    assert Mul(x*y/2, x + 1).as_coeff_Mul()[0] == x*y/2\n"], "sample_1129": ["def test_pycode_Pow():\n    assert pycode(Pow(x, 2)) == \"x**2\"\n    assert pycode(Pow(x, -1)) == \"1/x\"\n    assert pycode(Pow(x, -2)) == \"1/x**2\"\n    assert pycode(Pow(x, Rational(1, 2))) == \"math.sqrt(x)\"\n    assert pycode(Pow(x, Rational(-1, 2))) == \"1/math.sqrt(x)\"\n    assert pycode(Pow(x, Rational(3, 2))) == \"x*math.sqrt(x)\"\n    assert pycode(Pow(x, Rational(-3, 2))) == \"1/(x*math.sqrt(x))\"\n"], "sample_1130": ["def test_point_vel():\n    q, qd = dynamicsymbols('q qd')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    O = Point('O')\n    P = O.locatenew('P', 10 * B.x)\n    O.set_vel(N, 5 * N.x)\n    P.v2pt_theory(O, N, B)\n    assert P.vel(N) == 5*N.x + 10*qd*B.y\n"], "sample_1131": ["def test_pycode_Pow():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert pycode(Pow(A, -1), standard='python3') == 'numpy.linalg.inv(A)'\n    assert pycode(Pow(A, -2)) == 'numpy.linalg.inv(numpy.dot(A, A))'\n    assert pycode(Pow(A, 0)) == 'numpy.eye(3)'\n    assert pycode(Pow(A, 1)) == 'A'\n    assert pycode(Pow(A, 2)) == 'numpy.dot(A, A)'\n    assert pycode(Pow(A, S.Half)) == 'numpy.sqrt(A)'\n    assert pycode(Pow(A, -S.Half)) == 'numpy.linalg.inv(numpy.sqrt(A))'\n    assert pycode(Pow(x, 2)) == 'x**2'\n    assert pycode(Pow(x, S.Half)) == 'math.sqrt(x)'\n    assert pycode(Pow(x, -S.Half)) == '1/math.sqrt(x)'\n"], "sample_1132": ["def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2], [3, 4, 5])) == [1, 3, 2, 4, 5]\n    assert list(roundrobin([1, 2, 3], [])) == [1, 2, 3]\n    assert list(roundrobin([], [1, 2, 3])) == [1, 2, 3]\n"], "sample_1133": ["def test_refraction_angle_with_ray3D_and_plane():\n    n1, n2 = symbols('n1 n2')\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    assert refraction_angle(r1, n1, n2, plane=P) == \\\n        Ray3D(Point3D(0, 0, 0), Point3D(n1/n2, n1/n2, -sqrt(3)*sqrt(-2*n1**2/(3*n2**2) + 1)))\n"], "sample_1135": ["def test_Mul_coeff():\n    assert (3*x).as_coeff_Mul()[0] == 3\n    assert (3*x).as_coeff_Mul()[1] == x\n    assert (x*3).as_coeff_Mul()[0] == 3\n    assert (x*3).as_coeff_Mul()[1] == x\n    assert (2*x*y).as_coeff_Mul()[0] == 2\n    assert (2*x*y).as_coeff_Mul()[1] == x*y\n    assert (x*2*y).as_coeff_Mul()[0] == 2\n    assert (x*2*y).as_coeff_Mul()[1] == x*y\n    assert (x*y*2).as_coeff_Mul()[0] == 2\n    assert (x*y*2).as_coeff_Mul()[1] == x*y\n    assert (2*x*y*z).as_coeff_Mul()[0] == 2\n    assert (2*x*y*z).as_coeff_Mul()[1] == x*y*z\n    assert (x*2*y*z).as_coeff_Mul()[0] == 2\n    assert (x*2*y*z).as_coeff_Mul()[1] == x*y*z\n    assert (x*y*2*z).as_coeff_Mul()[0] == 2\n    assert (x*y*2*z).as_coeff_Mul()[1] == x*y*z\n    assert (x*y*z*2).as_coeff_Mul()[0] == 2\n    assert (x*y*z*2).as_coeff_Mul()[1] == x*y*z\n"], "sample_1138": ["def test_TR14():\n    assert TR14((cos(x) - 1)*(cos(x) + 1)) == -sin(x)**2\n    assert TR14((sin(x) - 1)*(sin(x) + 1)) == -cos(x)**2\n    p1 = (cos(x) + 1)*(cos(x) - 1)\n    p2 = (cos(y) - 1)*2*(cos(y) + 1)\n    p3 = (3*(cos(y) - 1))*(3*(cos(y) + 1))\n    assert TR14(p1*p2*p3*(x - 1)) == -18*(x - 1)*sin(x)**2*sin(y)**4\n"], "sample_1141": ["def test_ExprBuilder():\n    from sympy.core.expr import ExprBuilder\n\n    x, y = symbols('x y')\n\n    # Test ExprBuilder with simple expression\n    builder = ExprBuilder(Add, [x, y])\n    assert builder.build() == x + y\n\n    # Test ExprBuilder with nested expressions\n    builder = ExprBuilder(Mul, [x, ExprBuilder(Add, [y, 1])])\n    assert builder.build() == x * (y + 1)\n\n    # Test ExprBuilder with validator\n        if len(args) != 2:\n            raise ValueError(\"Expected two arguments\")\n\n    builder = ExprBuilder(Add, [x, y], validator=validator)\n    assert builder.build() == x + y\n\n    # Test ExprBuilder with invalid input\n    raises(TypeError, lambda: ExprBuilder(\"not a callable\", [x, y]))\n\n    # Test ExprBuilder's append_argument method\n    builder = ExprBuilder(Add, [x])\n    builder.append_argument(y)\n    assert builder.build() == x + y\n\n    # Test ExprBuilder's search_element method\n    builder = ExprBuilder(Mul, [x, ExprBuilder(Add, [y, 1])])\n    assert builder.search_element(y) == (1, 0)\n"], "sample_1142": ["def test_matrix_element():\n    i, j = symbols('i j')\n    assert MatrixElement(A, i, j).parent == A\n    assert MatrixElement(A, 0, 0).doit() == A[0, 0]\n    assert MatrixElement(Identity(3), i, j).doit() == KroneckerDelta(i, j, (0, 2))\n    raises(TypeError, lambda: MatrixElement(x, i, j))\n"], "sample_1148": ["def test_matrix_element():\n    i, j = symbols('i j')\n    assert MatrixElement(A, i, j).parent == A\n    assert MatrixElement(A, 0, 0).doit() == A[0, 0]\n    assert MatrixElement(Identity(3), i, j).doit() == KroneckerDelta(i, j, (0, 2))\n    raises(ValueError, lambda: MatrixElement(A, n, j))\n    raises(ValueError, lambda: MatrixElement(A, i, m))\n"], "sample_1153": ["def test_polar_lift():\n    x = Symbol('x')\n    p = Symbol('p', polar=True)\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n"], "sample_1156": ["def test_hyperbolic_rewrites():\n    x = symbols('x')\n    assert sinh(x).rewrite(exp) == (exp(x) - exp(-x))/2\n    assert cosh(x).rewrite(exp) == (exp(x) + exp(-x))/2\n    assert tanh(x).rewrite(exp) == (exp(x) - exp(-x))/(exp(x) + exp(-x))\n    assert coth(x).rewrite(exp) == (exp(x) + exp(-x))/(exp(x) - exp(-x))\n    assert sech(x).rewrite(exp) == 2/(exp(x) + exp(-x))\n    assert csch(x).rewrite(exp) == 2/(exp(x) - exp(-x))\n    assert asinh(x).rewrite(log) == log(x + sqrt(x**2 + 1))\n    assert acosh(x).rewrite(log) == log(x + sqrt(x - 1)*sqrt(x + 1))\n    assert atanh(x).rewrite(log) == log((1 + x)/(1 - x))/2\n    assert acoth(x).rewrite(log) == log((x + 1)/(x - 1))/2\n    assert asech(x).rewrite(log) == log(1/x + sqrt(1/x - 1)*sqrt(1/x + 1))\n    assert acsch(x).rewrite(log) == log(1/x + sqrt(1/x**2 + 1))\n"], "sample_1160": ["def test_intersection_sets():\n    # Test intersection of Integers and Interval\n    assert intersection_sets(S.Integers, Interval(1, 5)) == Range(2, 5)\n    assert intersection_sets(S.Integers, Interval(1, 5, left_open=False)) == Range(1, 5)\n    assert intersection_sets(S.Integers, Interval(-oo, 5)) == S.Integers.intersect(Interval(-oo, 5))\n\n    # Test intersection of Naturals and Interval\n    assert intersection_sets(S.Naturals, Interval(1, 5)) == Range(2, 5)\n    assert intersection_sets(S.Naturals, Interval(1, 5, left_open=False)) == Range(1, 5)\n    assert intersection_sets(S.Naturals, Interval(-oo, 5)) == S.Naturals.intersect(Interval(-oo, 5))\n"], "sample_1161": ["def test_print_Subs():\n    expr = Subs(x + y, x, 2)\n    assert sstr(expr) == \"Subs(x + y, x, 2)\"\n"], "sample_1163": ["def test_polar_lift():\n    p = Symbol('p', polar=True)\n    x = Symbol('x')\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n"], "sample_1164": ["def test_cg_simp_add():\n    j1, m1, j2, m2, j3, m3 = symbols('j1 m1 j2 m2 j3 m3')\n    cg1 = CG(j1, m1, j2, m2, j3, m3)\n    cg2 = CG(j1, m1, j2, m2, j3, m3)\n    assert cg_simp(cg1 + cg2) == 2*CG(j1, m1, j2, m2, j3, m3)\n"], "sample_1165": ["def test_quaternion_inverse():\n    q = Quaternion(w, x, y, z)\n    assert q.inverse() == conjugate(q) / q.norm()**2\n    raises(ValueError, lambda: Quaternion(0, 0, 0, 0).inverse())\n"], "sample_483": ["def test_check_dependencies(self):\n    @override_settings(\n        INSTALLED_APPS=[\n            \"django.contrib.admin\",\n            \"django.contrib.auth\",\n            \"django.contrib.contenttypes\",\n        ]\n    )\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, \"admin.E406\")\n\n    check_without_messages()\n\n    @override_settings(\n        INSTALLED_APPS=[\n            \"django.contrib.admin\",\n            \"django.contrib.auth\",\n            \"django.contrib.messages\",\n        ]\n    )\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, \"admin.E401\")\n\n    check_without_contenttypes()\n\n    @override_settings(\n        INSTALLED_APPS=[\n            \"django.contrib.admin\",\n            \"django.contrib.contenttypes\",\n            \"django.contrib.messages\",\n        ]\n    )\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, \"admin.E405\")\n\n    check_without_auth()\n\n    @override_settings(\n        MIDDLEWARE=[\n            \"django.contrib.sessions.middleware.SessionMiddleware\",\n            \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n        ]\n    )\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, \"admin.E409\")\n\n    check_without_messages_middleware()\n\n    @override_settings(\n        MIDDLEWARE=[\n            \"django.contrib.messages.middleware.MessageMiddleware\",\n            \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n        ]\n    )\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, \"admin.E410\")\n\n    check_without_sessions_middleware()\n\n    @override_settings(\n        MIDDLEWARE=[\n            \"django.contrib.sessions.middleware.SessionMiddleware\",\n            \"django.contrib.messages.middleware.MessageMiddleware\",\n        ]\n    )\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, \"admin.E408\")\n\n    check_without_auth_middleware()\n"], "sample_510": ["def test_switch_backend():\n    # Test switching to a different backend\n    original_backend = mpl.get_backend()\n    plt.switch_backend('agg')\n    assert mpl.get_backend() == 'agg'\n    plt.switch_backend(original_backend)\n    assert mpl.get_backend() == original_backend\n"], "sample_640": ["def test_is_classdef_type(node_type, expected):\n    node = node_type()\n    assert utils.is_classdef_type(node) == expected\n"], "sample_812": ["def test_pprint_estimator_with_n_max_elements_to_show():\n    # Test that the `n_max_elements_to_show` parameter is taken into account\n    # when printing an estimator with many parameters.\n    class EstimatorWithManyParams(BaseEstimator):\n            self.__dict__.update(kwargs)\n\n    estimator = EstimatorWithManyParams(**{f\"param_{i}\": i for i in range(100)})\n\n    printer = _EstimatorPrettyPrinter(n_max_elements_to_show=5)\n    output = printer.pformat(estimator)\n\n    assert \"...\" in output\n    assert len(re.findall(r\"param_\\d+\", output)) == 5\n"], "sample_980": ["def test_cycle():\n    # Test the Cycle class\n    assert Cycle(1, 2).list() == [0, 2, 1]\n    assert Cycle(1, 2)(3, 4).list() == [0, 2, 1, 4, 3]\n    assert Cycle(1, 2)(2, 3).list() == [0, 3, 1, 2]\n    assert Cycle().list(5) == list(range(5))\n    raises(ValueError, lambda: Cycle(-1, 2))\n    raises(ValueError, lambda: Cycle(1, 1))\n"], "sample_1017": ["def test_as_Boolean():\n    assert as_Boolean(True) is S.true\n    assert as_Boolean(False) is S.false\n    assert as_Boolean(1) is S.true\n    raises(TypeError, lambda: as_Boolean(2))\n    raises(TypeError, lambda: as_Boolean(S.Half))\n    assert as_Boolean(x) == x\n    assert as_Boolean(x > 0) == (x > 0)\n    assert as_Boolean(Eq(x, 0)) == Eq(x, 0)\n"], "sample_1168": ["def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2], [3, 4, 5])) == [1, 3, 2, 4, 5]\n    assert list(roundrobin([1, 2, 3], [])) == [1, 2, 3]\n    assert list(roundrobin([], [1, 2, 3])) == [1, 2, 3]\n    assert list(roundrobin([], [], [])) == []\n"], "sample_1169": ["def test_NO():\n    i, j = symbols('i,j', below_fermi=True)\n    a, b = symbols('a,b', above_fermi=True)\n    p, q = symbols('p,q')\n    f = Function('f')\n\n    assert NO(Fd(p)*F(q)).doit() == KroneckerDelta(Dummy('a'), q)*KroneckerDelta(p, q)*CreateFermion(Dummy('a'))*AnnihilateFermion(Dummy('a')) + \\\n        KroneckerDelta(Dummy('a'), p)*KroneckerDelta(Dummy('i'), q)*CreateFermion(Dummy('a'))*AnnihilateFermion(Dummy('i')) - \\\n        KroneckerDelta(Dummy('a'), q)*KroneckerDelta(Dummy('i'), p)*AnnihilateFermion(Dummy('a'))*CreateFermion(Dummy('i')) - \\\n        KroneckerDelta(Dummy('i'), p)*KroneckerDelta(Dummy('i'), q)*AnnihilateFermion(Dummy('i'))*CreateFermion(Dummy('i'))\n"], "sample_1174": ["def test_principal_branch():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z')\n    p = Symbol('p', positive=True)\n\n    assert principal_branch(exp_polar(2*pi*I)*3, 2*pi) == 3*exp_polar(0)\n    assert principal_branch(exp_polar(2*pi*I)*3*z, 2*pi) == 3*principal_branch(z, 2*pi)\n    assert principal_branch(exp_polar(pi*I/2)*x, pi) == x*exp_polar(I*pi/2)\n    assert principal_branch(exp_polar(3*pi*I/2)*y, pi) == y*exp_polar(-I*pi/2)\n    assert principal_branch(z, oo) == z\n    assert principal_branch(p, p) == p\n"], "sample_1177": ["def test_polar_lift():\n    p = Symbol('p', polar=True)\n    x = Symbol('x')\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n"], "sample_1178": ["def test_Variable():\n    v = Variable(x, type=real)\n    assert v.symbol == x\n    assert v.type == real\n\n    # Test that Variable can be instantiated with a string\n    v2 = Variable('x', type=real)\n    assert v2.symbol.name == 'x'\n    assert v2.type == real\n\n    # Test that Variable can be instantiated with a Symbol\n    v3 = Variable(Symbol('x'), type=real)\n    assert v3.symbol.name == 'x'\n    assert v3.type == real\n\n    # Test that Variable can be instantiated with attrs\n    v4 = Variable(x, type=real, attrs=[value_const])\n    assert v4.symbol == x\n    assert v4.type == real\n    assert value_const in v4.attrs\n\n    # Test that Variable can be instantiated with value\n    v5 = Variable(x, type=real, value=42)\n    assert v5.symbol == x\n    assert v5.type == real\n    assert v5.value == 42\n\n    # Test that Variable.deduced works\n    v6 = Variable.deduced(x)\n    assert v6.symbol == x\n    assert v6.type == real\n\n    # Test that Variable.as_Declaration works\n    decl = v.as_Declaration()\n    assert decl.variable == v\n"], "sample_1179": ["def test_print_Predicate():\n    assert sstr(Q.positive(x)) == \"Q.positive(x)\"\n"], "sample_1180": ["def test_point_transform():\n    p = Point2D(1, 1)\n    assert p.transform(Matrix([[1, 0, 1], [0, 1, 1], [0, 0, 1]])) == Point2D(2, 2)\n    p3d = Point3D(1, 1, 1)\n    assert p3d.transform(Matrix([[1, 0, 0, 1], [0, 1, 0, 1], [0, 0, 1, 1], [0, 0, 0, 1]])) == Point3D(2, 2, 2)\n"], "sample_1181": ["def test_print_Pow():\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert NumPyPrinter().doprint(Pow(A, -1)) == 'numpy.linalg.inv(A)'\n    assert NumPyPrinter().doprint(Pow(A, -2)) == '(numpy.linalg.inv(A)).dot(numpy.linalg.inv(A))'\n    assert NumPyPrinter().doprint(Pow(A, 0)) == 'numpy.eye(2)'\n    assert NumPyPrinter().doprint(Pow(A, 1)) == 'A'\n    assert NumPyPrinter().doprint(Pow(A, 2)) == 'A.dot(A)'\n    assert NumPyPrinter().doprint(Pow(A, 3)) == 'A.dot(A).dot(A)'\n"], "sample_1184": ["def test_BeamParameter():\n    wavelen, z, z_r, n = symbols('wavelen z z_r n')\n    p = BeamParameter(wavelen, z, z_r=z_r, n=n)\n    assert p.wavelen == wavelen\n    assert p.z == z\n    assert p.z_r == z_r\n    assert p.n == n\n\n    assert p.q == z + I*z_r\n    assert p.radius == z*(1 + (z_r/z)**2)\n    assert p.w == p.w_0*sqrt(1 + (z/z_r)**2)\n    assert p.w_0 == sqrt(z_r/(pi*n)*wavelen)\n    assert p.divergence == wavelen/pi/p.w_0\n    assert p.gouy == atan2(z, z_r)\n    assert p.waist_approximation_limit == 2*wavelen/pi\n"], "sample_1187": ["def test_integration_reduction_dynamic():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    facets = triangle.sides\n    a, b = hyperplane_parameters(triangle)[0]\n    x0 = facets[0].points[0]\n    monomial_values = [[0, 0, 0, 0], [1, 0, 0, 5],\n                       [y, 0, 1, 15], [x, 1, 0, None]]\n    assert integration_reduction_dynamic(facets, 0, a, b, x, 1, (x, y), 1, 0, 1,\n                                         x0, monomial_values, 3) == S(25)/2\n"], "sample_1188": ["def test_pretty_printing_of_del():\n    # Test the pretty printing of Del\n    assert upretty(Del()) == \"\\u2207\"\n    assert pretty(Del()) == \"Del()\"\n"], "sample_1203": ["def test_kernel():\n    # Test kernel computation for a homomorphism from A4 to C3\n    A4 = AlternatingGroup(4)\n    C3 = CyclicGroup(3)\n    T = homomorphism(A4, C3, A4.generators[1:], [C3.generators[0], C3.identity])\n    assert T.kernel().order() == 4\n"], "sample_1205": ["def test_PolyElement_as_expr():\n    R, x, y = ring('x, y', ZZ)\n    p = x**2 + 2*y\n    assert p.as_expr() == x**2 + 2*y\n    assert p.as_expr(x) == x**2\n    raises(ValueError, lambda: p.as_expr(y))\n    raises(ValueError, lambda: p.as_expr(x, y, Symbol('z')))\n"], "sample_1208": ["def test_matrix_gamma_distribution():\n    M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n    assert M.pspace.distribution.set == MatrixSet(2, 2, S.Reals)\n    X = MatrixSymbol('X', 2, 2)\n    assert density(M)(X).doit() == exp(-Trace(X)/2)/(4*pi*sqrt(pi))\n    assert density(M)([[1, 0], [0, 1]]).doit() == exp(-1)/(4*pi*sqrt(pi))\n    raises(ValueError, lambda: MatrixGamma('M', -1, 2, [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: MatrixGamma('M', 1, -2, [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: MatrixGamma('M', 1, 2, [[1, 0], [1, 1]]))\n"], "sample_219": ["def test_expression_pickle(self):\n    expr = F('num_employees') + Value(5)\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n\n    # Test that the expression can be used after being unpickled.\n    company = Company.objects.create(name='Test', num_employees=10)\n    self.assertEqual(unpickled_expr.resolve_expression().as_sql(connection), ('num_employees + 5', []))\n    self.assertEqual(Company.objects.filter(num_employees__gt=unpickled_expr).count(), 0)\n"], "sample_10": ["def test_setitem_replace_column_warning(table_types):\n    t = table_types.Table([[1, 2, 3], [4, 5, 6]], names=('a', 'b'))\n    with pytest.warns(TableReplaceWarning, match='replaced column'):\n        t['a'] = [10, 20, 30]\n    assert np.all(t['a'] == np.array([10, 20, 30]))\n"], "sample_19": ["def test_wcs_naxis(self):\n    # Test that WCS.naxis is correctly determined from the header\n    header = fits.Header()\n    header[\"NAXIS\"] = 2\n    header[\"CTYPE1\"] = \"RA---TAN\"\n    header[\"CTYPE2\"] = \"DEC--TAN\"\n    header[\"CRVAL1\"] = 0.0\n    header[\"CRVAL2\"] = 0.0\n    header[\"CRPIX1\"] = 1.0\n    header[\"CRPIX2\"] = 1.0\n    header[\"CDELT1\"] = 1.0\n    header[\"CDELT2\"] = 1.0\n\n    w = wcs.WCS(header)\n    assert w.naxis == 2\n\n    # Test that WCS.naxis is correctly determined from the WCSAXES keyword\n    header[\"WCSAXES\"] = 3\n    w = wcs.WCS(header)\n    assert w.naxis == 3\n\n    # Test that WCS.naxis is correctly determined from the highest axis number\n    # in any parameterized WCS keyword\n    header[\"CTYPE3\"] = \"LAT--TAN\"\n    w = wcs.WCS(header)\n    assert w.naxis == 3\n"], "sample_55": ["def test_admin_readonly_field(self):\n    response = self.client.get(reverse('admin:admin_views_article_change', args=(self.a1.pk,)))\n    readonly_field = self.get_admin_readonly_field(response, 'date')\n    self.assertIsNotNone(readonly_field)\n    self.assertEqual(readonly_field.field['name'], 'date')\n    self.assertEqual(readonly_field.contents(), self.a1.date.strftime('%Y-%m-%d %H:%M:%S'))\n"], "sample_124": ["def test_field_required_messages(self):\n    f = CharField(required=True)\n    self.assertEqual(f.clean('something'), 'something')\n    with self.assertRaisesMessage(ValidationError, 'This field is required.'):\n        f.clean('')\n    f = CharField(required=False)\n    self.assertEqual(f.clean(''), '')\n    f = CharField(required=True, error_messages={'required': 'Required'})\n    with self.assertRaisesMessage(ValidationError, 'Required'):\n        f.clean('')\n"], "sample_173": ["def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('3.14')\n    self.assertEqual(self.ops.adapt_decimalfield_value(value), '3.14')\n    self.assertEqual(self.ops.adapt_decimalfield_value(value, 5, 2), '3.14')\n"], "sample_174": ["def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('3.14')\n    self.assertEqual(self.ops.adapt_decimalfield_value(value), '3.14')\n    self.assertEqual(self.ops.adapt_decimalfield_value(value, max_digits=5, decimal_places=2), '3.14')\n"], "sample_196": ["def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('1.23')\n    max_digits = 5\n    decimal_places = 2\n    self.assertEqual(\n        self.ops.adapt_decimalfield_value(value, max_digits, decimal_places),\n        '1.23'\n    )\n"], "sample_223": ["def test_queryset_pickle(self):\n    qs = Tag.objects.all()\n    self.assertEqual(len(qs), 5)\n    qs_pickle = pickle.dumps(qs)\n    unpickled_qs = pickle.loads(qs_pickle)\n    self.assertEqual(len(unpickled_qs), 5)\n    self.assertEqual(list(qs), list(unpickled_qs))\n"], "sample_297": ["def test_filtering_on_annotation_with_outer_ref(self):\n    subquery = Item.objects.filter(tags=OuterRef('pk')).values('creator')\n    items = Tag.objects.annotate(has_creator=Exists(subquery)).filter(has_creator=True)\n    self.assertEqual(items.count(), 3)\n"], "sample_311": ["    def test_register_before_model_is_defined(self):\n        site = AdminSite()\n        try:\n            site.register('NotDefinedYet')\n        except TypeError as e:\n            self.assertEqual(str(e), \"The model NotDefinedYet is abstract, so it cannot be registered with admin.\")\n        else:\n            self.fail(\"Should have raised a TypeError\")\n"], "sample_319": ["def test_generate_altered_db_table_comment(self):\n    # Make state\n    before = self.make_project_state([self.author_with_db_table_comment])\n    after = self.make_project_state(\n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n                {\"db_table_comment\": \"New table comment\"},\n            )\n        ]\n    )\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    # Right operations?\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table_comment=\"New table comment\"\n    )\n"], "sample_320": ["    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        self.apply_operations(operation)\n        self.assertTableExists(\"pony\")\n        self.assertColumnExists(\"pony\", \"pink\")\n"], "sample_394": ["def test_admin_view_preserve_filters(self):\n    \"\"\"\n    The admin view preserves filters in the changelist.\n    \"\"\"\n    # Create some objects to filter\n    Book.objects.create(name=\"Book 1\", author=\"Author 1\")\n    Book.objects.create(name=\"Book 2\", author=\"Author 2\")\n\n    # Go to the changelist with a filter\n    response = self.client.get(reverse(\"admin:admin_views_book_changelist\") + \"?author__exact=Author+1\")\n    self.assertEqual(response.status_code, 200)\n\n    # Check that the filter query string is in the pagination links\n    self.assertContains(response, \"author__exact=Author+1\")\n\n    # Make sure the filter query string is in the 'add' link\n    add_link = reverse(\"admin:admin_views_book_add\")\n    self.assertContains(response, add_link + \"?author__exact=Author+1\")\n\n    # Make sure the filter query string is in the 'change' link\n    change_link = reverse(\"admin:admin_views_book_change\", args=(Book.objects.get(author=\"Author 1\").pk,))\n    self.assertContains(response, change_link + \"?author__exact=Author+1\")\n"], "sample_396": ["def test_filtering_on_annotation_with_filter(self):\n    Item.objects.filter(\n        num__gt=1000, rank__gt=0, name__in=[\"one\", \"two\", \"three\"]\n    ).annotate(is_top=RawSQL(\"num > 1000\", ())).filter(is_top=True)\n    self.assertQuerysetEqual(\n        Item.objects.filter(\n            num__gt=1000, rank__gt=0, name__in=[\"one\", \"two\", \"three\"]\n        ).annotate(is_top=RawSQL(\"num > 1000\", ())).filter(is_top=True),\n        [\"<Item: one>\", \"<Item: two>\"],\n        ordered=False,\n    )\n"], "sample_400": ["def test_alter_field_with_deconstructible(self):\n    \"\"\"Test altering a field with a deconstructible default.\"\"\"\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_2],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_3],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_list_1],\n        [self.author_name_deconstructible_list_2],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_list_1],\n        [self.author_name_deconstructible_list_3],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_tuple_1],\n        [self.author_name_deconstructible_tuple_2],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_tuple_1],\n        [self.author_name_deconstructible_tuple_3],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_dict_1],\n        [self.author_name_deconstructible_dict_2],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_dict_1],\n        [self.author_name_deconstructible_dict_3],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n\n    changes = self.get_changes(\n        [self.author_name_nested_deconstructible_1],\n        [self.author_name_nested_deconstructible_2],\n    )\n"], "sample_403": ["    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name=\"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        self.apply_operations(operation)\n        self.assertTableExists(\"pony\")\n        self.assertColumnExists(\"pony\", \"pink\")\n        self.assertColumnNotNull(\"pony\", \"pink\")\n"], "sample_405": ["    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name=\"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        self.apply_operations(operation)\n        self.assertTableExists(\"pony\")\n        self.assertColumnExists(\"pony\", \"pink\")\n"], "sample_408": ["def test_alter_index_together_with_field_renamed(self):\n    \"\"\"Tests that AlterIndexTogether works when a field is renamed.\"\"\"\n    before = self.make_project_state(\n        [\n            self.author_name,\n            ModelState(\n                \"testapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n                {\n                    \"index_together\": {(\"author\", \"title\")},\n                },\n            ),\n        ]\n    )\n    after = self.make_project_state(\n        [\n            self.author_name,\n            ModelState(\n                \"testapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"writer\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n                {\n                    \"index_together\": {(\"writer\", \"title\")},\n                },\n            ),\n        ]\n    )\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\", \"AlterIndexTogether\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"author\", new_name=\"writer\")\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        name=\"book\",\n        index_together={(\"writer\", \"title\")},\n    )\n"], "sample_423": ["def test_generate_altered_foo_together(self):\n    # Make a model with unique_together/index_together, then change it\n    model_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n        ],\n        {\n            \"unique_together\": {(\"name\", \"book\")},\n            \"index_together\": {(\"name\", \"book\")},\n        },\n    )\n    new_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n        ],\n        {\n            \"unique_together\": {(\"name\", \"book\"), (\"book\", \"name\")},\n            \"index_together\": {(\"name\", \"book\"), (\"book\", \"name\")},\n        },\n    )\n    changes = self.get_changes([model_state], [new_state])\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    # Right number of AddField operations?\n    migration = changes[\"testapp\"][0]\n    self.assertEqual(len(migration.operations), 2)\n    self.assertIsInstance(migration.operations[0], operations.AlterUniqueTogether)\n    self.assertIsInstance(migration.operations[1], operations.AlterIndexTogether)\n    self.assertEqual(\n        migration.operations[0].unique_together, {(\"name\", \"book\"), (\"book\", \"name\")}\n    )\n    self.assertEqual(\n        migration.operations[1].index_together, {(\"name\", \"book\"), (\"book\", \"name\")}\n    )\n"], "sample_424": ["    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        self.apply_operations(operation)\n        self.assertTableExists(\"pony\")\n        self.assertColumnExists(\"pony\", \"pink\")\n"], "sample_430": ["def test_generate_altered_db_table_comment(self):\n    # Make state\n    before = self.make_project_state([self.author_with_db_table_comment])\n    after = self.make_project_state(\n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n                {\"db_table_comment\": \"New table comment\"},\n            )\n        ]\n    )\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    # Right operations?\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        table_comment=\"New table comment\",\n    )\n"], "sample_439": ["def test_form_add_prefix(self):\n    # Test prefix construction\n    f = Person(prefix='person')\n    self.assertEqual(f.add_prefix('name'), 'person-name')\n    self.assertEqual(f.add_prefix('first_name'), 'person-first_name')\n\n    # If there is no prefix, the name should remain the same\n    f = Person()\n    self.assertEqual(f.add_prefix('name'), 'name')\n    self.assertEqual(f.add_prefix('first_name'), 'first_name')\n\n    # If the prefix is empty, the name should remain the same\n    f = Person(prefix='')\n    self.assertEqual(f.add_prefix('name'), 'name')\n    self.assertEqual(f.add_prefix('first_name'), 'first_name')\n"], "sample_452": ["    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        self.apply_operations([operation])\n        self.assertTableExists(\"pony\")\n        self.assertColumnExists(\"pony\", \"pink\")\n        self.assertColumnNotNull(\"pony\", \"pink\")\n"], "sample_460": ["    def test_get_log_entries(self):\n        # Create a log entry\n        log_entry = LogEntry.objects.log_action(\n            user_id=self.superuser.pk,\n            content_type_id=ContentType.objects.get_for_model(User).pk,\n            object_id=self.superuser.pk,\n            object_repr=repr(self.superuser),\n            action_flag=ADDITION,\n        )\n\n        # Get the log entries\n        log_entries = site.get_log_entries(self.client.request())\n\n        # Check that the log entry is in the list\n        self.assertIn(log_entry, log_entries)\n"], "sample_463": ["def test_generate_altered_db_table_comment(self):\n    # Test adding a db_table_comment\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_with_db_table_comment],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table_comment=\"Table comment\"\n    )\n\n    # Test removing a db_table_comment\n    changes = self.get_changes(\n        [self.author_with_db_table_comment],\n        [self.author_empty],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table_comment=None\n    )\n\n    # Test altering a db_table_comment\n    changes = self.get_changes(\n        [self.author_with_db_table_comment],\n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n                {\"db_table_comment\": \"New table comment\"},\n            ),\n        ],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table_comment=\"New table comment\"\n    )\n"], "sample_491": ["def test_boundfield_auto_id(self):\n    # Test that auto_id works correctly for BoundField\n    class MyForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n\n    form = MyForm(auto_id=True)\n    self.assertEqual(form[\"field1\"].auto_id, \"id_field1\")\n    self.assertEqual(form[\"field2\"].auto_id, \"id_field2\")\n\n    form = MyForm(auto_id=\"my_id_%s\")\n    self.assertEqual(form[\"field1\"].auto_id, \"my_id_field1\")\n    self.assertEqual(form[\"field2\"].auto_id, \"my_id_field2\")\n\n    form = MyForm(auto_id=False)\n    self.assertEqual(form[\"field1\"].auto_id, \"\")\n    self.assertEqual(form[\"field2\"].auto_id, \"\")\n"], "sample_502": ["def test_subplots():\n    # Simple test of pyplot.subplots()\n    fig, axs = plt.subplots(2, 2)\n    assert len(axs) == 2\n    assert len(axs[0]) == 2\n    for ax in axs.flat:\n        assert isinstance(ax, plt.Axes)\n    plt.close(fig)\n"], "sample_536": ["def test_LockDraw():\n    lock = widgets.LockDraw()\n    assert lock.available(lock) is True\n    lock(lock)\n    assert lock.available(lock) is False\n    lock.release(lock)\n    assert lock.available(lock) is True\n    with pytest.raises(ValueError):\n        lock(lock)\n        lock(lock)\n    with pytest.raises(ValueError):\n        lock.release(lock)\n        lock.release(lock)\n"], "sample_539": ["def test_rectangle_selector_interactive(ax):\n    onselect = mock.Mock()\n    tool = widgets.RectangleSelector(ax, onselect, interactive=True)\n    assert tool._interactive is True\n    assert tool._edge_handles is not None\n    assert tool._corner_handles is not None\n    assert tool._center_handle is not None\n"], "sample_541": ["def test_LockDraw():\n    lock = widgets.LockDraw()\n    assert lock.available(lock) is True\n    lock(lock)\n    assert lock.available(lock) is False\n    lock.release(lock)\n    assert lock.available(lock) is True\n    with pytest.raises(ValueError):\n        lock(lock)\n        lock(lock)\n    with pytest.raises(ValueError):\n        lock.release(lock)\n        lock.release(lock)\n"], "sample_543": ["def test_LockDraw():\n    lock = widgets.LockDraw()\n    assert lock.available(lock) == True\n    lock(lock)\n    assert lock.available(lock) == False\n    lock.release(lock)\n    assert lock.available(lock) == True\n    with pytest.raises(ValueError):\n        lock(lock)\n        lock(lock)\n    with pytest.raises(ValueError):\n        lock.release(lock)\n        lock.release(lock)\n"], "sample_564": ["def test_3d_stem(setup_test_data):\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    x, y, z = setup_test_data\n    ax.stem(x, y, z, orientation='z')\n    ax.set_xlim3d(-10, 10)\n    ax.set_ylim3d(-10, 10)\n    ax.set_zlim3d(-500, 500)\n"], "sample_720": ["def test_quantile_transformer_sparse():\n    # check that quantile transformer works with sparse data\n    rng = np.random.RandomState(0)\n    X = rng.randint(0, 10, size=(100, 3))\n    X_sparse = sparse.csr_matrix(X)\n\n    qt_dense = QuantileTransformer(n_quantiles=10, random_state=0)\n    qt_sparse = QuantileTransformer(n_quantiles=10, random_state=0)\n\n    X_trans_dense = qt_dense.fit_transform(X)\n    X_trans_sparse = qt_sparse.fit_transform(X_sparse)\n\n    assert_array_almost_equal(X_trans_dense, toarray(X_trans_sparse))\n"], "sample_591": ["def test_merge_data_and_coords():\n    # Test that merge_data_and_coords returns the expected Dataset\n    data_vars = {\"a\": (\"x\", [1, 2, 3])}\n    coords = {\"x\": [1, 2, 3]}\n    expected = xr.Dataset(data_vars, coords)\n    result = merge.merge_data_and_coords(data_vars, coords)\n    assert_identical(result, expected)\n"], "sample_559": ["def test_axes_divider_append_axes(fig_test, fig_ref):\n    fig_test.subplots()\n    fig_ref.subplots()\n\n    divider_test = make_axes_locatable(fig_test.axes[0])\n    divider_ref = make_axes_locatable(fig_ref.axes[0])\n\n    ax_test = divider_test.append_axes(\"right\", size=0.5, pad=0.1)\n    ax_ref = divider_ref.append_axes(\"right\", size=0.5, pad=0.1)\n\n    np.testing.assert_allclose(ax_test.get_position().bounds,\n                               ax_ref.get_position().bounds)\n"], "sample_744": ["def test_power_transformer_zero_variance():\n    # Check that PowerTransformer works fine on dataset with zero-variance\n    # feature.\n    X = np.array([[1., 0., 1.], [0., 0., 1.], [1., 0., 1.]])\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n    assert_array_equal(X_trans[:, 1], np.zeros(X.shape[0]))\n"], "sample_747": ["def test_power_transformer_inverse():\n    # Test that inverse_transform is correctly implemented\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_inv)\n"], "sample_906": ["def test_type_with_init():\n    check(\n        \"type\",\n        \"int i = 42\",\n        {1: \"c.type.i\", 2: \"c.type.i\"},\n        \"int i = 42\",\n        key=\"type\",\n        asTextOutput=\"int i = 42\"\n    )\n    check(\n        \"type\",\n        \"int i = 42U\",\n        {1: \"c.type.i\", 2: \"c.type.i\"},\n        \"int i = 42U\",\n        key=\"type\",\n        asTextOutput=\"int i = 42U\"\n    )\n    check(\n        \"type\",\n        \"int i = 42L\",\n        {1: \"c.type.i\", 2: \"c.type.i\"},\n        \"int i = 42L\",\n        key=\"type\",\n        asTextOutput=\"int i = 42L\"\n    )\n    check(\n        \"type\",\n        \"int i = 42UL\",\n        {1: \"c.type.i\", 2: \"c.type.i\"},\n        \"int i = 42UL\",\n        key=\"type\",\n        asTextOutput=\"int i = 42UL\"\n    )\n    check(\n        \"type\",\n        \"int i = 42LL\",\n        {1: \"c.type.i\", 2: \"c.type.i\"},\n        \"int i = 42LL\",\n        key=\"type\",\n        asTextOutput=\"int i = 42LL\"\n    )\n    check(\n        \"type\",\n        \"int i = 42ULL\",\n        {1: \"c.type.i\", 2: \"c.type.i\"},\n        \"int i = 42ULL\",\n        key=\"type\",\n        asTextOutput=\"int i = 42ULL\"\n    )\n"], "sample_875": ["def test_multilabel_confusion_matrix():\n    # Test multilabel_confusion_matrix function\n    y_true = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred = np.array([[1, 1, 0], [1, 0, 1]])\n\n    # Test without labels\n    mcm = multilabel_confusion_matrix(y_true, y_pred)\n    expected_mcm = np.array([[[1, 0], [0, 1]], [[1, 0], [0, 1]], [[0, 1], [1, 0]]])\n    assert_array_equal(mcm, expected_mcm)\n\n    # Test with labels\n    labels = [0, 2]\n    mcm = multilabel_confusion_matrix(y_true, y_pred, labels=labels)\n    expected_mcm = np.array([[[1, 0], [0, 1]], [[0, 1], [1, 0]]])\n    assert_array_equal(mcm, expected_mcm)\n\n    # Test with sample weights\n    sample_weight = np.array([1, 2])\n    mcm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    expected_mcm = np.array([[[1, 0], [0, 2]], [[1, 0], [0, 2]], [[0, 2], [2, 0]]])\n    assert_array_equal(mcm, expected_mcm)\n\n    # Test with samplewise\n    mcm = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    expected_mcm = np.array([[[1, 1], [1, 1]], [[1, 1], [1, 1]]])\n    assert_array_equal(mcm, expected_mcm)\n"], "sample_907": ["def test_template_parameter_pack():\n    check(\n        \"function\",\n        \"void f(Ts... ts);\",\n        {1: None, 2: \"fDp\", 3: \"fDp\", 4: \"fDp\"},\n        key=\"function\",\n        asTextOutput=\"void f(Ts... ts)\"\n    )\n    check(\n        \"function\",\n        \"void f(Ts...);\",\n        {1: None, 2: \"fDp\", 3: \"fDp\", 4: \"fDp\"},\n        key=\"function\",\n        asTextOutput=\"void f(Ts...)\"\n    )\n"], "sample_911": ["def test_template_parameter_pack():\n    check(\"function\", \"void f(int... Ts)\", {2: \"fDp\", 3: \"fDp\", 4: \"fDp\"},\n          \"void f(int ... Ts)\")\n    check(\"function\", \"void f(int Ts...)\", {2: \"fDp\", 3: \"fDp\", 4: \"fDp\"},\n          \"void f(int Ts ...)\")\n    check(\"function\", \"void f(int...)\", {2: \"fDp\", 3: \"fDp\", 4: \"fDp\"},\n          \"void f(int ...)\")\n"], "sample_917": ["def test_template_parameter_pack():\n    check(\"function\", \"void f(int... args)\", {1: None, 2: \"void(int...)\"},\n          \"void f(int ... args)\")\n    check(\"function\", \"void f(int...args)\", {1: None, 2: \"void(int...)\"},\n          \"void f(int ... args)\")\n    check(\"function\", \"void f(int ... args)\", {1: None, 2: \"void(int...)\"},\n          \"void f(int ... args)\")\n    check(\"function\", \"void f(int ...args)\", {1: None, 2: \"void(int...)\"},\n          \"void f(int ... args)\")\n"], "sample_919": ["def test_template_parameter_pack():\n    check(\"function\", \"void f(Ts... ts)\", {1: None, 2: \"f\", 3: \"f\", 4: \"f\"},\n          \"void f(Ts... ts)\")\n"], "sample_924": ["def test_template_parameter_pack():\n    check(\n        \"function\",\n        \"void f(Ts... ts);\",\n        {1: None, 2: \"fDpR1TsE\", 3: \"fDpR1TsE\", 4: \"fDpR1TsE\"},\n        \"void f(Ts... ts);\",\n        \"function \"\n    )\n"], "sample_927": ["def test_template_parameter_pack():\n    check(\"function\", \"void f(Ts... ts)\", {1: None, 2: \"f\", 3: \"f\", 4: \"f\"},\n          \"void f(Ts... ts)\")\n"], "sample_932": ["def test_template_parameter_pack():\n    check(\n        \"function\",\n        \"void f(Ts... ts);\",\n        {1: None, 2: \"fDpR1KcE\", 3: \"fDpR1KcE\", 4: \"fDpR1KcE\"},\n        key=\"function\",\n        asTextOutput=\"void f(Ts... ts)\"\n    )\n"], "sample_934": ["def test_type_with_template_params():\n    check(\n        \"type\",\n        \"std::map<int, std::string>\",\n        {1: \"std-map-std-string\", 2: \"std-map-std-string\"},\n        key=\"type\",\n    )\n"], "sample_948": ["def test_template_parameter_pack():\n    check(\n        \"function\",\n        \"void f(Ts... ts);\",\n        {1: None, 2: \"fDpR1KcE\", 3: \"fDpR1KcE\", 4: \"fDpR1KcE\"},\n        \"void f(Ts... ts);\",\n        \"function \"\n    )\n"], "sample_935": ["def test_template_parameter_pack():\n    check(\n        \"function\",\n        \"void f(Ts... ts);\",\n        {1: None, 2: \"fDp\", 3: \"fDp\", 4: \"fDp\"},\n        \"void f(Ts... ts);\",\n        key=\"function\",\n        asTextOutput=\"void f(Ts... ts);\"\n    )\n"], "sample_959": ["def test_template_parameter_pack():\n    check(\"function\", \"void f(Ts... ts)\", {1: None, 2: \"Dp\", 3: \"Dp\", 4: \"Dp\"},\n          \"void f(Ts... ts)\")\n"], "sample_994": ["def test_mpf_norm():\n    assert mpf_norm((1, 5, 0, 3), 53) == (1, 5, 0, 3)\n    assert mpf_norm((1, 0, 0, 0), 53) == (0, 0, 0, 0)\n    assert mpf_norm((1, 0, 1, 1), 53) == (1, 0, 1, 1)\n"], "sample_958": ["def test_alias_rendering():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n\n    # Create a parser and parse an alias declaration\n    parser = DefinitionParser(\"alias my_alias = my_type\", location=None, config=Config())\n    ast = parser.parse_declaration(\"alias\", \"alias\")\n    parser.assert_end()\n\n    # Create a symbol and add the alias declaration to it\n    rootSymbol = Symbol(None, None, None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\", line=42)\n\n    # Create a node and render the alias\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(\"alias my_alias = my_type\", '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n\n    # Check the rendered output\n    resAsText = parentNode.astext()\n    expectedOutput = \"alias my_alias = my_type\"\n    if resAsText != expectedOutput:\n        print(\"\")\n        print(\"Input:    \", \"alias my_alias = my_type\")\n        print(\"astext(): \", resAsText)\n        print(\"Expected: \", expectedOutput)\n        print(\"Node:\", parentNode)\n        raise DefinitionError(\"\")\n"], "sample_995": ["def test_mpf_norm():\n    assert mpf_norm((0, 1, 0, 1), 53) == (0, 1, 0, 1)\n    assert mpf_norm((1, 1, 0, 1), 53) == (1, 1, 0, 1)\n    assert mpf_norm((0, 0, 0, 1), 53) == (0, 0, 0, 0)\n    assert mpf_norm((0, 0, 1, 1), 53) == (0, 0, 1, 1)\n    assert mpf_norm((0, 0, -1, 1), 53) == (0, 0, -1, 1)\n"], "sample_1002": ["def test_mpf_norm():\n    assert mpf_norm((0, 1, 0, 1), 53) == (0, 1, 0, 1)\n    assert mpf_norm((1, 1, 0, 1), 53) == (1, 1, 0, 1)\n    assert mpf_norm((0, 0, 0, 1), 53) == (0, 0, 0, 0)\n    assert mpf_norm((1, 0, 0, 1), 53) == (0, 0, 0, 0)\n"], "sample_1010": ["def test_latex_matrix():\n    M = Matrix([[1, 2], [3, 4]])\n    assert latex(M) == r'\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]'\n    assert latex(M, mat_str='array') == r'\\left[\\begin{array}{cc}1 & 2\\\\3 & 4\\end{array}\\right]'\n    assert latex(M, mat_delim='(') == r'\\left(\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right)'\n"], "sample_1005": ["def test_latex_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == r\"1 + 2 i + 3 j + 4 k\"\n"], "sample_1045": ["def test_mpf_norm():\n    assert mpf_norm((0, 1, 0, 1), 53) == (0, 1, 0, 1)\n    assert mpf_norm((1, 1, 0, 1), 53) == (1, 1, 0, 1)\n    assert mpf_norm((1, 12345678901234567890, -50, 77), 53) == (1, 12345678901234567890, -50, 77)\n    assert mpf_norm((1, 12345678901234567890, -50, 77), 80) == (1, 12345678901234567890, -50, 80)\n"], "sample_1024": ["def test_Float_mpf_norm():\n    assert mpf_norm((0, 1, 0, 1), 53) == (0, 1, 0, 1)\n    assert mpf_norm((1, 1, 0, 1), 53) == (1, 1, 0, 1)\n    assert mpf_norm((1, 12345678901234567890, -50, 77), 53) == (1, 12345678901234567890, -50, 77)\n"], "sample_1027": ["def test_Poly_eq_ne():\n    assert _strict_eq(Poly(x + 1, x), Poly(x + 1, x))\n    assert not _strict_eq(Poly(x + 1, x), Poly(x + 1, y))\n    assert not _strict_eq(Poly(x + 1, x), Poly(x + 2, x))\n    assert not _strict_eq(Poly(x + 1, x, domain=ZZ), Poly(x + 1, x, domain=QQ))\n    assert not _strict_eq(Poly(x + 1, x, domain=ZZ), Poly(x + 1, x, domain=RR))\n\n    assert Poly(x + 1, x) == Poly(x + 1, x)\n    assert Poly(x + 1, x) != Poly(x + 1, y)\n    assert Poly(x + 1, x) != Poly(x + 2, x)\n    assert Poly(x + 1, x, domain=ZZ) == Poly(x + 1, x, domain=QQ)\n    assert Poly(x + 1, x, domain=ZZ) != Poly(x + 1, x, domain=RR)\n\n    assert Poly(x + 1, x) != x + 1\n    assert x + 1 != Poly(x + 1, x)\n"], "sample_1046": ["def test_TensorElement():\n    Lorentz = TensorIndexType(\"Lorentz\", dummy_fmt=\"L\")\n    i, j, k = symbols(\"i j k\")\n    A = tensorhead(\"A\", [Lorentz, Lorentz], [[1], [1]])\n    te = TensorElement(A(i, j), {i: 2})\n    assert te.get_free_indices() == [j]\n    assert te.get_indices() == [j]\n    assert te.expr == A(i, j)\n    assert te.index_map == {i: 2}\n"], "sample_1053": ["def test_tribonacci_constant():\n    assert TribonacciConstant.is_real\n    assert TribonacciConstant.is_positive\n    assert TribonacciConstant.is_irrational\n    assert TribonacciConstant.is_number\n    assert TribonacciConstant.is_algebraic\n    assert not TribonacciConstant.is_transcendental\n    assert TribonacciConstant > 1\n    assert TribonacciConstant.expand(func=True) == (1 + cbrt(19 - 3*sqrt(33)) + cbrt(19 + 3*sqrt(33))) / 3\n    assert TribonacciConstant.n(20) == Float('1.8392867552141611326', 20)\n    assert TribonacciConstant._eval_evalf(20) == Float('1.8392867552141611326', 20)\n    assert TribonacciConstant._as_mpf_val(20) == mpf('1.8392867552141611326', 20)\n"], "sample_1059": ["def test_jacobi_normalized():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n    x = Symbol(\"x\")\n    assert jacobi_normalized(n, a, b, x) == jacobi(n, a, b, x)/sqrt(2**(a + b + 1)*gamma(a + n + 1)*gamma(b + n + 1)/((a + b + 2*n + 1)*factorial(n)*gamma(a + b + n + 1)))\n"], "sample_1085": ["def test_Float_mpf_norm():\n    assert Float._new((0, 0, 0, 0), 1) == S.Zero\n    assert Float._new((1, 1, 0, 1), 1) == S.One\n    assert Float._new((1, 5, 0, 3), 1) == -5\n    assert Float._new((0, 5, 0, 3), 1) == 5\n    assert Float._new((0, 5, 1, 3), 1) == 10\n    assert Float._new((0, 5, -1, 3), 1) == 2.5\n    assert Float._new((0, 5, -2, 3), 1) == 1.25\n    assert Float._new((0, 5, -3, 3), 1) == 0.625\n"], "sample_1074": ["def test_sylow_subgroup():\n    G = SymmetricGroup(4)\n    S = G.sylow_subgroup(2)\n    assert S.order() == 8\n    assert G.order() % S.order() == 0\n    assert (G.order() // S.order()) % 2 != 0\n"], "sample_1090": ["def test_Rational_equality():\n    assert S(2)/3 == S(4)/6\n    assert S(2)/3 != S(3)/4\n    assert S(2)/3 != 1\n    assert S(2)/3 != 1.0\n    assert S(2)/3 != '1/2'\n    assert S(2)/3 != Float(2)/3.0\n"], "sample_1102": ["def test_Poly_free_symbols():\n    assert Poly(x**2 + 1).free_symbols == {x}\n    assert Poly(x**2 + y).free_symbols == {x, y}\n    assert Poly(x**2 + y, x).free_symbols == {x, y}\n    assert Poly(x**2 + y, x, z).free_symbols == {x, y}\n"], "sample_1115": ["def test_TensMul_data():\n    Lorentz = TensorIndexType(\"Lorentz\", dummy_name=\"L\")\n    i0, i1 = tensor_indices(\"i0:2\", Lorentz)\n    A, B = tensor_heads(\"A B\", [Lorentz]*2, TensorSymmetry.fully_symmetric(2))\n    expr = A(i0, i1)*B(-i0, -i1)\n    data = [[1, 2], [3, 4]]\n    A.data = [[1, 2], [3, 4]]\n    B.data = [[5, 6], [7, 8]]\n    assert _is_equal(expr.data, 87)\n"], "sample_1136": ["def test_ExpressionDomain():\n    K = EX\n\n    assert K(2).as_expr() == 2\n    assert K(2).numer().as_expr() == 2\n    assert K(2).denom().as_expr() == 1\n\n    assert K(2) + K(3) == K(5)\n    assert K(2) - K(3) == K(-1)\n    assert K(2) * K(3) == K(6)\n    assert K(2) / K(3) == K(Rational(2, 3))\n\n    assert K(2) + 3 == K(5)\n    assert K(2) - 3 == K(-1)\n    assert K(2) * 3 == K(6)\n    assert K(2) / 3 == K(Rational(2, 3))\n\n    assert 2 + K(3) == K(5)\n    assert 2 - K(3) == K(-1)\n    assert 2 * K(3) == K(6)\n    assert 2 / K(3) == K(Rational(2, 3))\n\n    assert K(2)**K(3) == K(8)\n    assert K(2)**3 == K(8)\n\n    assert K(2).gcd(K(3)) == K(1)\n    assert K(2).lcm(K(3)) == K(6)\n\n    assert K.is_positive(K(2))\n    assert K.is_negative(K(-2))\n    assert K.is_nonpositive(K(-2))\n    assert K.is_nonnegative(K(2))\n\n    assert K.numer(K(2)) == K(2)\n    assert K.denom(K(2)) == K(1)\n"], "sample_1127": ["def test_coset():\n    g = Permutation(1, 2)\n    G = PermutationGroup([Permutation(1, 2), Permutation(0, 1)])\n    cst = Coset(g, G, dir=\"+\")\n    assert cst.is_right_coset\n    assert not cst.is_left_coset\n    cst = Coset(g, G, dir=\"-\")\n    assert not cst.is_right_coset\n    assert cst.is_left_coset\n    assert len(cst.as_list()) == G.order()\n"], "sample_1134": ["def test_latex_transfer_function():\n    tf = TransferFunction(1, [1, 2], s)\n    assert latex(tf) == r\"\\frac{1}{s + 2}\"\n    tf = TransferFunction([1, 2], [1, 2, 3], s)\n    assert latex(tf) == r\"\\frac{s + 2}{s^2 + 2 s + 3}\"\n"], "sample_1146": ["def test_LatexPrinter_settings():\n    # issue 11732\n    x = Symbol('x')\n    lp = LatexPrinter({'order': 'lex'})\n    assert lp.doprint(x**2 + x) == \"x^{2} + x\"\n    lp = LatexPrinter({'order': 'old'})\n    assert lp.doprint(x**2 + x) == \"x^{2} + x\"\n    lp = LatexPrinter({'order': 'none'})\n    assert lp.doprint(x**2 + x) == \"x + x^{2}\"\n    lp = LatexPrinter({'order': None})\n    assert lp.doprint(x**2 + x) == \"x + x^{2}\"\n"], "sample_1140": ["def test_pretty_printing_of_conditionset():\n    p, q = symbols('p q', real=True)\n    assert upretty(ConditionSet(p, p > 0)) == \"{p | p \\u2208 \\u211d \\u2227 p > 0}\"\n    assert upretty(ConditionSet((p, q), (p > 0, q > 0))) == \"{(p, q) | p \\u2208 \\u211d \\u2227 p > 0 \\u2227 q \\u2208 \\u211d \\u2227 q > 0}\"\n    assert upretty(ConditionSet(p, p > 0, S.Reals)) == \"{p | p \\u2208 \\u211d \\u2227 p > 0}\"\n    assert upretty(ConditionSet((p, q), (p > 0, q > 0), S.Reals)) == \"{(p, q) | p \\u2208 \\u211d \\u2227 p > 0 \\u2227 q \\u2208 \\u211d \\u2227 q > 0}\"\n"], "sample_1143": ["def test_comp():\n    assert comp(1, 1) is True\n    assert comp(1, 1.0) is True\n    assert comp(1, 1 + 1e-15) is True\n    assert comp(1, 1 + 1e-5) is False\n    assert comp(1e100, 1e100 + 1e95) is True\n    assert comp(1e100, 1e100 + 1e94) is False\n    assert comp(1.0, 1) is True\n    assert comp(1.0, 1.0) is True\n    assert comp(1.0, 1.0 + 1e-15) is True\n    assert comp(1.0, 1.0 + 1e-5) is False\n    assert comp(1e100, 1e100 + 1e95, '') is False\n    assert comp(1e100, 1e100 + 1e94, '') is False\n    assert comp(1.0, 1, '') is False\n    assert comp(1.0, 1.0, '') is True\n    assert comp(1.0, 1.0 + 1e-15, '') is False\n    assert comp(1.0, 1.0 + 1e-5, '') is False\n    assert comp('1', 1) is True\n    assert comp('1', 1.0) is True\n    assert comp('1', 1 + 1e-15) is False\n    assert comp('1', 1 + 1e-5) is False\n    assert comp('1.0', 1) is True\n    assert comp('1.0', 1.0) is True\n    assert comp('1.0', 1.0 + 1e-15) is False\n    assert comp('1.0', 1.0 + 1e-5) is False\n    assert comp(1, '1') is True\n    assert comp(1.0, '1') is True\n    assert comp(1 + 1e-15, '1') is False\n    assert comp(1 + 1e-5, '1') is False\n    assert comp(1, '"], "sample_1147": ["def test_LatexPrinter_settings():\n    lp = LatexPrinter({'decimal_separator': 'comma'})\n    assert lp._print(Float(1.5)) == r\"1{,}5\"\n    assert lp._print(Rational(1, 2)) == r\"\\frac{1}{2}\"\n    assert lp._print([1, 2, 3]) == r\"\\left[ 1; 2; 3\\right]\"\n    assert lp._print((1, 2, 3)) == r\"\\left( 1; 2; 3\\right)\"\n    assert lp._print({1, 2, 3}) == r\"\\left\\{ 1; 2; 3\\right\\}\"\n    assert lp._print(Dict({1: 2, 3: 4})) == r\"\\left\\{ 1 : 2; 3 : 4\\right\\}\"\n"], "sample_1167": ["def test_latex_transfer_function_matrix():\n    s = symbols('s')\n    G = TransferFunctionMatrix([[1/(s+1), 1/(s+2)], [1/(s+3), 1/(s+4)]])\n    assert latex(G) == r'\\begin{bmatrix} \\frac{1}{s + 1} & \\frac{1}{s + 2} \\\\ \\frac{1}{s + 3} & \\frac{1}{s + 4} \\end{bmatrix}_\\tau'\n"], "sample_1176": ["def test_Float_mpf_norm():\n    assert mpf_norm((0, 471, 18, 9), 53) == (0, 471, 18, 9)\n    assert mpf_norm((1, 5, 0, 3), 53) == (1, 5, 0, 3)\n    assert mpf_norm((0, 0, 0, 0), 53) == (0, 0, 0, 0)\n    assert mpf_norm((0, 0, 0, 0), 100) == (0, 0, 0, 0)\n    assert mpf_norm((1, 0, 0, 0), 53) == (1, 0, 0, 0)\n    assert mpf_norm((1, 0, 0, 0), 100) == (1, 0, 0, 0)\n"], "sample_1175": ["def test_pretty_printing_of_MatPow():\n    M = MatrixSymbol('M', 2, 2)\n    n = Symbol('n')\n    assert upretty(M**n) == u\"M\\u207f\"\n"], "sample_1206": ["def test_Float_hash():\n    assert hash(Float(1.0)) == hash(Float(1.0, 1))\n    assert hash(Float(1.0)) != hash(Float(1.0, 2))\n    assert hash(Float(1.0)) != hash(Float(1.1))\n"], "sample_1202": ["def test_Float_mpf_norm():\n    # test that mpf_norm returns a normalized mpf tuple\n    assert mpf_norm((1, 5, 0, 3), 53) == (1, 5, 0, 3)\n    assert mpf_norm((0, 0, 0, 0), 53) == (0, 0, 0, 0)\n    assert mpf_norm((1, 0, 0, 0), 53) == (0, 0, 0, 0)\n    assert mpf_norm((1, 5, 0, 0), 53) == (1, 5, 0, 3)\n"], "sample_1204": ["def test_coset():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    cst = Coset(a, G, dir=\"-\")\n    assert cst.is_left_coset\n    assert not cst.is_right_coset\n    cst = Coset(a, G, dir=\"+\")\n    assert not cst.is_left_coset\n    assert cst.is_right_coset\n    assert len(cst.as_list()) == G.order()\n"]}, "raw_scored_generations": {"sample_1": [false], "sample_0": [false], "sample_3": [false], "sample_2": [false], "sample_4": [false], "sample_5": [false], "sample_6": [false], "sample_7": [false], "sample_8": [false], "sample_9": [true], "sample_10": [false], "sample_11": [false], "sample_12": [false], "sample_13": [false], "sample_14": [false], "sample_15": [true], "sample_16": [false], "sample_17": [false], "sample_18": [false], "sample_19": [false], "sample_20": [false], "sample_21": [false], "sample_22": [false], "sample_23": [false], "sample_24": [true], "sample_25": [true], "sample_26": [false], "sample_27": [false], "sample_28": [false], "sample_29": [false], "sample_30": [false], "sample_31": [false], "sample_32": [false], "sample_33": [false], "sample_34": [false], "sample_35": [false], "sample_36": [false], "sample_37": [false], "sample_38": [false], "sample_39": [false], "sample_40": [true], "sample_41": [false], "sample_42": [false], "sample_43": [true], "sample_44": [true], "sample_45": [true], "sample_46": [false], "sample_47": [true], "sample_48": [true], "sample_49": [true], "sample_50": [true], "sample_51": [false], "sample_52": [true], "sample_54": [false], "sample_53": [true], "sample_55": [false], "sample_58": [false], "sample_56": [false], "sample_57": [true], "sample_59": [true], "sample_60": [true], "sample_61": [false], "sample_62": [true], "sample_63": [true], "sample_64": [true], "sample_65": [false], "sample_67": [false], "sample_66": [false], "sample_68": [true], "sample_69": [true], "sample_70": [true], "sample_71": [false], "sample_72": [false], "sample_73": [true], "sample_75": [true], "sample_74": [true], "sample_76": [true], "sample_77": [false], "sample_78": [false], "sample_79": [false], "sample_80": [false], "sample_82": [true], "sample_81": [false], "sample_83": [false], "sample_85": [true], "sample_84": [true], "sample_86": [false], "sample_88": [true], "sample_87": [true], "sample_89": [true], "sample_90": [false], "sample_91": [true], "sample_92": [false], "sample_93": [true], "sample_94": [false], "sample_95": [false], "sample_98": [false], "sample_96": [false], "sample_99": [true], "sample_97": [true], "sample_100": [true], "sample_102": [true], "sample_101": [true], "sample_103": [true], "sample_104": [true], "sample_107": [false], "sample_106": [true], "sample_105": [true], "sample_108": [false], "sample_109": [true], "sample_111": [true], "sample_110": [false], "sample_112": [true], "sample_113": [false], "sample_114": [true], "sample_115": [true], "sample_116": [false], "sample_117": [true], "sample_118": [true], "sample_119": [false], "sample_120": [false], "sample_121": [false], "sample_122": [true], "sample_123": [true], "sample_124": [false], "sample_125": [false], "sample_126": [true], "sample_127": [true], "sample_128": [true], "sample_129": [true], "sample_130": [false], "sample_131": [false], "sample_132": [false], "sample_133": [false], "sample_135": [true], "sample_134": [false], "sample_136": [false], "sample_139": [true], "sample_137": [true], "sample_138": [true], "sample_140": [false], "sample_141": [true], "sample_142": [false], "sample_143": [false], "sample_144": [false], "sample_145": [false], "sample_146": [true], "sample_147": [true], "sample_148": [true], "sample_151": [true], "sample_149": [false], "sample_152": [true], "sample_150": [false], "sample_153": [false], "sample_154": [false], "sample_155": [false], "sample_156": [false], "sample_157": [false], "sample_158": [false], "sample_159": [false], "sample_160": [false], "sample_161": [false], "sample_162": [false], "sample_163": [false], "sample_164": [false], "sample_165": [false], "sample_166": [false], "sample_167": [false], "sample_168": [true], "sample_169": [false], "sample_171": [false], "sample_170": [false], "sample_172": [true], "sample_173": [true], "sample_174": [true], "sample_175": [true], "sample_176": [true], "sample_177": [true], "sample_178": [true], "sample_180": [false], "sample_179": [false], "sample_182": [true], "sample_181": [true], "sample_183": [true], "sample_184": [false], "sample_185": [false], "sample_186": [false], "sample_187": [false], "sample_188": [true], "sample_189": [false], "sample_190": [true], "sample_191": [true], "sample_192": [true], "sample_193": [false], "sample_194": [false], "sample_195": [false], "sample_196": [true], "sample_198": [true], "sample_197": [true], "sample_199": [true], "sample_200": [true], "sample_201": [false], "sample_202": [false], "sample_203": [false], "sample_204": [false], "sample_205": [false], "sample_206": [false], "sample_207": [false], "sample_208": [true], "sample_209": [false], "sample_210": [false], "sample_211": [true], "sample_213": [false], "sample_212": [false], "sample_214": [false], "sample_215": [true], "sample_216": [true], "sample_217": [true], "sample_218": [false], "sample_219": [true], "sample_220": [false], "sample_221": [true], "sample_222": [false], "sample_223": [true], "sample_224": [true], "sample_225": [true], "sample_226": [false], "sample_227": [true], "sample_228": [true], "sample_229": [true], "sample_230": [false], "sample_231": [true], "sample_232": [false], "sample_233": [false], "sample_234": [true], "sample_235": [true], "sample_236": [true], "sample_237": [false], "sample_238": [false], "sample_239": [true], "sample_240": [false], "sample_241": [true], "sample_242": [false], "sample_243": [false], "sample_244": [true], "sample_245": [false], "sample_246": [true], "sample_247": [true], "sample_248": [true], "sample_249": [false], "sample_250": [true], "sample_251": [true], "sample_252": [false], "sample_253": [false], "sample_254": [true], "sample_256": [true], "sample_255": [true], "sample_257": [false], "sample_258": [false], "sample_259": [true], "sample_260": [true], "sample_261": [false], "sample_262": [false], "sample_263": [false], "sample_264": [false], "sample_265": [true], "sample_266": [false], "sample_267": [true], "sample_268": [true], "sample_269": [false], "sample_270": [false], "sample_271": [true], "sample_272": [false], "sample_273": [true], "sample_274": [false], "sample_275": [true], "sample_276": [true], "sample_277": [false], "sample_278": [false], "sample_279": [false], "sample_280": [true], "sample_281": [true], "sample_282": [true], "sample_283": [true], "sample_284": [true], "sample_285": [false], "sample_286": [true], "sample_287": [false], "sample_288": [false], "sample_289": [false], "sample_290": [true], "sample_291": [true], "sample_292": [true], "sample_293": [false], "sample_294": [true], "sample_295": [true], "sample_296": [false], "sample_297": [true], "sample_298": [false], "sample_299": [false], "sample_300": [false], "sample_301": [false], "sample_302": [true], "sample_303": [true], "sample_304": [false], "sample_305": [false], "sample_306": [false], "sample_307": [true], "sample_308": [true], "sample_309": [true], "sample_310": [true], "sample_312": [true], "sample_311": [false], "sample_313": [false], "sample_314": [true], "sample_315": [false], "sample_316": [false], "sample_317": [true], "sample_318": [false], "sample_319": [true], "sample_320": [false], "sample_321": [true], "sample_322": [false], "sample_323": [false], "sample_324": [true], "sample_325": [false], "sample_326": [true], "sample_327": [false], "sample_328": [true], "sample_329": [false], "sample_330": [false], "sample_331": [false], "sample_332": [true], "sample_333": [false], "sample_334": [false], "sample_335": [false], "sample_336": [false], "sample_337": [false], "sample_338": [true], "sample_339": [false], "sample_340": [false], "sample_341": [true], "sample_342": [true], "sample_343": [false], "sample_344": [true], "sample_345": [true], "sample_346": [false], "sample_347": [false], "sample_348": [true], "sample_349": [true], "sample_350": [true], "sample_351": [true], "sample_352": [true], "sample_353": [false], "sample_354": [false], "sample_355": [false], "sample_356": [true], "sample_357": [true], "sample_358": [false], "sample_359": [false], "sample_360": [false], "sample_361": [false], "sample_362": [true], "sample_363": [true], "sample_364": [false], "sample_365": [false], "sample_366": [false], "sample_367": [false], "sample_368": [false], "sample_369": [true], "sample_370": [true], "sample_371": [true], "sample_372": [false], "sample_373": [true], "sample_374": [true], "sample_375": [true], "sample_376": [false], "sample_377": [true], "sample_378": [true], "sample_379": [false], "sample_380": [true], "sample_381": [true], "sample_382": [false], "sample_383": [false], "sample_384": [true], "sample_385": [true], "sample_386": [true], "sample_387": [true], "sample_388": [true], "sample_389": [true], "sample_390": [true], "sample_391": [true], "sample_392": [false], "sample_393": [false], "sample_394": [false], "sample_395": [false], "sample_396": [true], "sample_397": [true], "sample_398": [false], "sample_399": [true], "sample_400": [true], "sample_401": [true], "sample_402": [false], "sample_403": [false], "sample_404": [true], "sample_405": [false], "sample_406": [false], "sample_407": [true], "sample_408": [true], "sample_409": [false], "sample_410": [false], "sample_411": [false], "sample_412": [false], "sample_413": [false], "sample_414": [true], "sample_415": [false], "sample_416": [true], "sample_417": [false], "sample_418": [false], "sample_419": [true], "sample_420": [false], "sample_421": [true], "sample_422": [true], "sample_423": [true], "sample_424": [false], "sample_425": [false], "sample_426": [true], "sample_427": [true], "sample_428": [false], "sample_429": [false], "sample_430": [true], "sample_431": [false], "sample_432": [true], "sample_433": [true], "sample_434": [false], "sample_435": [true], "sample_436": [false], "sample_437": [false], "sample_438": [false], "sample_439": [false], "sample_440": [true], "sample_441": [true], "sample_442": [false], "sample_443": [false], "sample_444": [true], "sample_445": [true], "sample_446": [false], "sample_447": [true], "sample_448": [false], "sample_449": [true], "sample_450": [true], "sample_451": [true], "sample_453": [true], "sample_452": [false], "sample_454": [false], "sample_455": [false], "sample_456": [true], "sample_457": [false], "sample_458": [false], "sample_459": [false], "sample_460": [true], "sample_461": [false], "sample_462": [false], "sample_463": [true], "sample_464": [false], "sample_465": [true], "sample_466": [false], "sample_467": [true], "sample_469": [true], "sample_468": [false], "sample_470": [false], "sample_471": [false], "sample_472": [true], "sample_473": [true], "sample_474": [false], "sample_475": [true], "sample_476": [false], "sample_477": [false], "sample_478": [false], "sample_479": [true], "sample_480": [false], "sample_481": [false], "sample_482": [false], "sample_483": [false], "sample_484": [false], "sample_485": [false], "sample_486": [false], "sample_487": [true], "sample_488": [false], "sample_489": [true], "sample_490": [false], "sample_491": [false], "sample_492": [false], "sample_493": [true], "sample_494": [false], "sample_495": [true], "sample_496": [false], "sample_497": [false], "sample_498": [false], "sample_499": [false], "sample_500": [false], "sample_501": [false], "sample_502": [true], "sample_503": [true], "sample_504": [false], "sample_505": [true], "sample_506": [false], "sample_507": [false], "sample_508": [true], "sample_509": [true], "sample_510": [true], "sample_511": [true], "sample_512": [false], "sample_513": [false], "sample_514": [false], "sample_515": [false], "sample_516": [false], "sample_517": [false], "sample_518": [true], "sample_519": [false], "sample_520": [false], "sample_521": [false], "sample_522": [false], "sample_523": [false], "sample_524": [false], "sample_525": [false], "sample_526": [false], "sample_527": [false], "sample_528": [false], "sample_529": [true], "sample_530": [false], "sample_531": [false], "sample_532": [false], "sample_533": [false], "sample_534": [false], "sample_535": [false], "sample_536": [false], "sample_537": [false], "sample_538": [false], "sample_539": [false], "sample_540": [false], "sample_541": [false], "sample_542": [false], "sample_543": [false], "sample_544": [false], "sample_545": [false], "sample_546": [false], "sample_547": [false], "sample_548": [false], "sample_549": [false], "sample_550": [false], "sample_551": [false], "sample_552": [false], "sample_553": [false], "sample_554": [false], "sample_555": [false], "sample_556": [false], "sample_557": [false], "sample_558": [false], "sample_559": [false], "sample_560": [false], "sample_561": [false], "sample_562": [true], "sample_563": [false], "sample_564": [false], "sample_565": [false], "sample_566": [false], "sample_567": [false], "sample_568": [false], "sample_569": [false], "sample_570": [false], "sample_571": [false], "sample_572": [false], "sample_573": [false], "sample_574": [false], "sample_575": [false], "sample_576": [false], "sample_577": [false], "sample_578": [false], "sample_579": [false], "sample_580": [true], "sample_581": [false], "sample_582": [false], "sample_583": [false], "sample_584": [false], "sample_585": [true], "sample_586": [false], "sample_587": [false], "sample_588": [false], "sample_589": [false], "sample_590": [true], "sample_591": [false], "sample_592": [false], "sample_593": [false], "sample_594": [false], "sample_595": [false], "sample_596": [true], "sample_597": [false], "sample_598": [false], "sample_599": [false], "sample_600": [false], "sample_601": [false], "sample_602": [true], "sample_603": [true], "sample_604": [false], "sample_605": [false], "sample_606": [true], "sample_607": [true], "sample_608": [false], "sample_609": [true], "sample_610": [false], "sample_611": [false], "sample_612": [true], "sample_613": [false], "sample_614": [false], "sample_615": [false], "sample_616": [false], "sample_617": [true], "sample_618": [true], "sample_619": [false], "sample_620": [false], "sample_621": [true], "sample_622": [false], "sample_623": [true], "sample_624": [false], "sample_625": [false], "sample_626": [false], "sample_627": [true], "sample_628": [false], "sample_629": [false], "sample_630": [false], "sample_631": [false], "sample_632": [false], "sample_633": [false], "sample_634": [false], "sample_635": [false], "sample_636": [false], "sample_637": [false], "sample_638": [false], "sample_639": [false], "sample_640": [false], "sample_641": [false], "sample_642": [false], "sample_643": [false], "sample_644": [false], "sample_645": [false], "sample_646": [false], "sample_647": [false], "sample_648": [false], "sample_649": [false], "sample_650": [false], "sample_651": [false], "sample_652": [false], "sample_653": [false], "sample_654": [false], "sample_655": [false], "sample_656": [false], "sample_657": [false], "sample_658": [false], "sample_659": [false], "sample_660": [false], "sample_661": [false], "sample_662": [false], "sample_663": [false], "sample_664": [true], "sample_665": [false], "sample_666": [false], "sample_667": [false], "sample_668": [true], "sample_669": [false], "sample_670": [true], "sample_671": [false], "sample_672": [false], "sample_673": [false], "sample_674": [false], "sample_675": [false], "sample_676": [false], "sample_677": [true], "sample_678": [false], "sample_679": [false], "sample_680": [false], "sample_681": [false], "sample_682": [false], "sample_683": [false], "sample_684": [false], "sample_685": [false], "sample_686": [true], "sample_687": [false], "sample_688": [false], "sample_689": [false], "sample_690": [false], "sample_691": [false], "sample_692": [false], "sample_693": [false], "sample_694": [false], "sample_695": [false], "sample_696": [false], "sample_697": [false], "sample_698": [false], "sample_699": [false], "sample_700": [false], "sample_701": [false], "sample_702": [false], "sample_703": [false], "sample_704": [false], "sample_705": [false], "sample_706": [true], "sample_707": [false], "sample_708": [false], "sample_709": [false], "sample_710": [false], "sample_711": [false], "sample_712": [false], "sample_713": [true], "sample_714": [false], "sample_715": [false], "sample_716": [false], "sample_717": [true], "sample_718": [false], "sample_719": [true], "sample_720": [true], "sample_721": [false], "sample_722": [false], "sample_723": [false], "sample_724": [false], "sample_725": [false], "sample_726": [true], "sample_727": [true], "sample_728": [false], "sample_729": [true], "sample_730": [false], "sample_731": [true], "sample_732": [false], "sample_733": [false], "sample_734": [false], "sample_735": [false], "sample_736": [false], "sample_737": [true], "sample_738": [false], "sample_739": [false], "sample_740": [false], "sample_741": [false], "sample_742": [false], "sample_743": [false], "sample_744": [false], "sample_745": [false], "sample_746": [false], "sample_747": [true], "sample_748": [false], "sample_749": [false], "sample_750": [false], "sample_751": [false], "sample_752": [false], "sample_753": [false], "sample_754": [false], "sample_755": [true], "sample_756": [true], "sample_757": [false], "sample_758": [false], "sample_759": [false], "sample_760": [false], "sample_761": [true], "sample_762": [false], "sample_763": [false], "sample_764": [false], "sample_765": [true], "sample_766": [true], "sample_767": [false], "sample_768": [false], "sample_769": [false], "sample_770": [false], "sample_771": [false], "sample_772": [false], "sample_773": [false], "sample_774": [false], "sample_775": [false], "sample_776": [false], "sample_777": [false], "sample_778": [true], "sample_779": [false], "sample_780": [true], "sample_781": [true], "sample_782": [false], "sample_783": [false], "sample_784": [false], "sample_785": [false], "sample_786": [false], "sample_787": [false], "sample_788": [false], "sample_789": [true], "sample_790": [true], "sample_791": [true], "sample_792": [false], "sample_793": [true], "sample_794": [false], "sample_795": [false], "sample_796": [true], "sample_797": [false], "sample_798": [false], "sample_799": [true], "sample_800": [false], "sample_801": [true], "sample_802": [false], "sample_803": [false], "sample_804": [false], "sample_805": [false], "sample_806": [false], "sample_807": [false], "sample_808": [false], "sample_809": [false], "sample_810": [false], "sample_811": [false], "sample_812": [false], "sample_813": [false], "sample_814": [false], "sample_815": [true], "sample_816": [false], "sample_817": [false], "sample_818": [false], "sample_819": [true], "sample_820": [true], "sample_821": [true], "sample_822": [false], "sample_823": [false], "sample_824": [true], "sample_825": [true], "sample_826": [true], "sample_827": [true], "sample_828": [false], "sample_829": [false], "sample_830": [true], "sample_831": [false], "sample_832": [false], "sample_833": [true], "sample_834": [false], "sample_835": [true], "sample_836": [true], "sample_837": [true], "sample_838": [false], "sample_839": [false], "sample_840": [true], "sample_841": [false], "sample_842": [false], "sample_843": [false], "sample_844": [false], "sample_845": [true], "sample_846": [false], "sample_847": [false], "sample_848": [false], "sample_849": [true], "sample_850": [false], "sample_851": [false], "sample_852": [false], "sample_853": [false], "sample_854": [false], "sample_855": [false], "sample_856": [true], "sample_857": [false], "sample_858": [false], "sample_859": [false], "sample_860": [false], "sample_861": [false], "sample_862": [false], "sample_863": [true], "sample_864": [true], "sample_865": [false], "sample_866": [true], "sample_867": [false], "sample_868": [false], "sample_869": [false], "sample_870": [false], "sample_871": [true], "sample_872": [false], "sample_873": [false], "sample_874": [false], "sample_875": [false], "sample_876": [true], "sample_877": [true], "sample_878": [false], "sample_879": [false], "sample_880": [false], "sample_881": [false], "sample_882": [false], "sample_883": [false], "sample_884": [false], "sample_885": [false], "sample_886": [true], "sample_887": [false], "sample_888": [false], "sample_889": [false], "sample_890": [true], "sample_891": [false], "sample_892": [true], "sample_893": [false], "sample_894": [false], "sample_895": [false], "sample_896": [false], "sample_897": [false], "sample_898": [true], "sample_899": [false], "sample_900": [false], "sample_901": [false], "sample_902": [false], "sample_903": [false], "sample_904": [false], "sample_905": [false], "sample_906": [false], "sample_907": [false], "sample_908": [false], "sample_909": [false], "sample_910": [false], "sample_911": [false], "sample_912": [false], "sample_913": [false], "sample_914": [false], "sample_915": [false], "sample_916": [false], "sample_917": [false], "sample_918": [false], "sample_919": [false], "sample_920": [false], "sample_921": [false], "sample_922": [false], "sample_923": [false], "sample_924": [false], "sample_925": [true], "sample_926": [false], "sample_927": [false], "sample_928": [false], "sample_929": [false], "sample_930": [false], "sample_931": [false], "sample_932": [false], "sample_933": [false], "sample_934": [false], "sample_935": [false], "sample_936": [false], "sample_937": [false], "sample_938": [false], "sample_939": [false], "sample_940": [false], "sample_941": [false], "sample_942": [false], "sample_943": [false], "sample_944": [false], "sample_945": [false], "sample_946": [false], "sample_947": [false], "sample_948": [false], "sample_949": [false], "sample_950": [false], "sample_951": [false], "sample_952": [true], "sample_953": [true], "sample_954": [false], "sample_955": [false], "sample_956": [false], "sample_957": [false], "sample_958": [false], "sample_959": [false], "sample_960": [false], "sample_961": [false], "sample_962": [false], "sample_963": [false], "sample_964": [false], "sample_965": [false], "sample_966": [false], "sample_967": [true], "sample_968": [false], "sample_969": [false], "sample_970": [false], "sample_971": [false], "sample_972": [false], "sample_973": [false], "sample_974": [false], "sample_975": [true], "sample_976": [true], "sample_977": [false], "sample_978": [false], "sample_979": [false], "sample_980": [true], "sample_981": [false], "sample_982": [true], "sample_983": [false], "sample_984": [false], "sample_985": [false], "sample_986": [false], "sample_987": [false], "sample_988": [false], "sample_989": [false], "sample_990": [false], "sample_991": [false], "sample_992": [false], "sample_993": [true], "sample_994": [true], "sample_995": [false], "sample_996": [false], "sample_997": [true], "sample_998": [false], "sample_999": [true], "sample_1000": [true], "sample_1001": [true], "sample_1002": [false], "sample_1003": [false], "sample_1004": [false], "sample_1005": [true], "sample_1006": [false], "sample_1007": [false], "sample_1008": [false], "sample_1009": [true], "sample_1010": [true], "sample_1011": [true], "sample_1012": [false], "sample_1013": [true], "sample_1014": [false], "sample_1015": [false], "sample_1016": [true], "sample_1017": [true], "sample_1018": [false], "sample_1019": [false], "sample_1020": [false], "sample_1021": [false], "sample_1022": [true], "sample_1023": [true], "sample_1024": [false], "sample_1025": [false], "sample_1026": [false], "sample_1027": [false], "sample_1028": [false], "sample_1029": [false], "sample_1030": [false], "sample_1031": [false], "sample_1032": [false], "sample_1033": [true], "sample_1034": [true], "sample_1035": [false], "sample_1036": [true], "sample_1037": [false], "sample_1038": [false], "sample_1039": [false], "sample_1040": [false], "sample_1041": [true], "sample_1042": [false], "sample_1043": [false], "sample_1044": [false], "sample_1045": [false], "sample_1046": [false], "sample_1047": [true], "sample_1048": [true], "sample_1049": [false], "sample_1050": [false], "sample_1051": [false], "sample_1052": [true], "sample_1053": [false], "sample_1054": [false], "sample_1055": [false], "sample_1056": [false], "sample_1057": [false], "sample_1058": [false], "sample_1059": [true], "sample_1060": [false], "sample_1061": [false], "sample_1062": [false], "sample_1063": [true], "sample_1064": [true], "sample_1065": [false], "sample_1066": [false], "sample_1067": [true], "sample_1068": [true], "sample_1069": [false], "sample_1070": [false], "sample_1071": [false], "sample_1072": [false], "sample_1073": [false], "sample_1074": [true], "sample_1075": [true], "sample_1076": [false], "sample_1077": [false], "sample_1078": [false], "sample_1079": [false], "sample_1080": [true], "sample_1081": [true], "sample_1082": [false], "sample_1083": [false], "sample_1084": [false], "sample_1085": [false], "sample_1086": [true], "sample_1087": [false], "sample_1088": [false], "sample_1089": [true], "sample_1090": [false], "sample_1091": [false], "sample_1092": [false], "sample_1093": [true], "sample_1094": [false], "sample_1095": [false], "sample_1096": [false], "sample_1097": [false], "sample_1098": [false], "sample_1099": [false], "sample_1100": [false], "sample_1101": [false], "sample_1102": [true], "sample_1103": [false], "sample_1104": [false], "sample_1105": [false], "sample_1106": [false], "sample_1107": [true], "sample_1108": [true], "sample_1109": [false], "sample_1110": [false], "sample_1111": [true], "sample_1112": [false], "sample_1113": [false], "sample_1114": [false], "sample_1115": [false], "sample_1116": [true], "sample_1117": [false], "sample_1118": [false], "sample_1119": [false], "sample_1120": [false], "sample_1121": [false], "sample_1122": [false], "sample_1123": [false], "sample_1124": [false], "sample_1125": [false], "sample_1126": [false], "sample_1127": [true], "sample_1128": [true], "sample_1129": [false], "sample_1130": [false], "sample_1131": [false], "sample_1132": [true], "sample_1133": [true], "sample_1134": [false], "sample_1135": [true], "sample_1136": [true], "sample_1137": [false], "sample_1138": [false], "sample_1139": [true], "sample_1140": [false], "sample_1141": [false], "sample_1142": [true], "sample_1143": [false], "sample_1144": [true], "sample_1145": [true], "sample_1146": [false], "sample_1147": [false], "sample_1148": [false], "sample_1149": [false], "sample_1150": [true], "sample_1151": [false], "sample_1152": [true], "sample_1153": [false], "sample_1154": [true], "sample_1155": [false], "sample_1156": [false], "sample_1157": [true], "sample_1158": [true], "sample_1159": [false], "sample_1160": [false], "sample_1161": [true], "sample_1162": [false], "sample_1163": [false], "sample_1164": [false], "sample_1165": [true], "sample_1166": [true], "sample_1167": [false], "sample_1168": [true], "sample_1169": [false], "sample_1170": [true], "sample_1171": [true], "sample_1172": [false], "sample_1173": [true], "sample_1174": [false], "sample_1175": [false], "sample_1176": [false], "sample_1177": [false], "sample_1178": [false], "sample_1179": [true], "sample_1180": [false], "sample_1181": [false], "sample_1182": [false], "sample_1183": [false], "sample_1184": [true], "sample_1185": [false], "sample_1186": [true], "sample_1187": [true], "sample_1188": [false], "sample_1189": [true], "sample_1190": [false], "sample_1191": [false], "sample_1192": [false], "sample_1193": [false], "sample_1194": [true], "sample_1195": [true], "sample_1196": [false], "sample_1197": [false], "sample_1198": [false], "sample_1199": [true], "sample_1200": [false], "sample_1201": [false], "sample_1202": [false], "sample_1203": [false], "sample_1204": [true], "sample_1205": [false], "sample_1206": [false], "sample_1207": [true], "sample_1208": [false], "sample_1209": [false]}}