{"raw_generations": {"sample_0": ["def test_getitem(UncertClass):\n    uncert = np.arange(10)\n    fake_uncert = UncertClass(uncert)\n    sliced_uncert = fake_uncert[2:5]\n    assert_array_equal(sliced_uncert.array, np.array([2, 3, 4]))\n    assert sliced_uncert.unit == fake_uncert.unit\n    assert isinstance(sliced_uncert, UncertClass)\n", "def test_unknown_uncertainty_propagation():\n    # Test that UnknownUncertainty raises IncompatibleUncertaintiesException\n    # when attempting to propagate uncertainties\n    uncert1 = UnknownUncertainty([1, 2, 3])\n    uncert2 = UnknownUncertainty([1, 2, 3])\n    \n    with pytest.raises(IncompatibleUncertaintiesException):\n        uncert1._propagate_add(uncert2, result_data=np.array([2, 4, 6]), correlation=0)\n    \n    with pytest.raises(IncompatibleUncertaintiesException):\n        uncert1._propagate_subtract(uncert2, result_data=np.array([0, 0, 0]), correlation=0)\n    \n    with pytest.raises(IncompatibleUncertaintiesException):\n        uncert1._propagate_multiply(uncert2, result_data=np.array([1, 4, 9]), correlation=0)\n    \n    with pytest.raises(IncompatibleUncertaintiesException):\n        uncert1._propagate_divide(uncert2, result_data=np.array([1, 1, 1]), correlation=0)\n", "def test_uncertainty_repr(UncertClass):\n    uncert = UncertClass([1, 2, 3], unit=u.adu)\n    expected_repr = f\"{UncertClass.__name__}([1, 2, 3])\"\n    assert repr(uncert) == expected_repr\n", "def test_pickle_and_unpickle_with_parent_nddata(UncertClass):\n    data = np.ones(3)\n    parent_nddata = NDData(data)\n    uncertainty = UncertClass(data, unit=u.adu)\n    parent_nddata.uncertainty = uncertainty\n    pickled_uncertainty = pickle.dumps(uncertainty)\n    unpickled_uncertainty = pickle.loads(pickled_uncertainty)\n    assert_array_equal(unpickled_uncertainty.array, uncertainty.array)\n    assert unpickled_uncertainty.unit == uncertainty.unit\n    assert unpickled_uncertainty.parent_nddata.data is parent_nddata.data\n", "def test_propagate_with_correlation(UncertClass):\n    data1 = np.array([1.0, 2.0, 3.0])\n    data2 = np.array([4.0, 5.0, 6.0])\n    uncert1 = UncertClass(data1)\n    uncert2 = UncertClass(data2)\n    \n    # Mock NDData objects with the uncertainties\n    ndd1 = NDData(data1, uncertainty=uncert1)\n    ndd2 = NDData(data2, uncertainty=uncert2)\n    \n    # Test add operation with correlation\n    result_add = uncert1.propagate(np.add, ndd2, ndd1.data + ndd2.data, correlation=0.5)\n    expected_add = np.sqrt(data1**2 + data2**2 + 2 * 0.5 * data1 * data2)\n    assert_allclose(result_add.array, expected_add)\n\n    # Test subtract operation with correlation\n    result_sub = uncert1.propagate(np.subtract, ndd2, ndd1.data - ndd2.data, correlation=0.5)\n    expected_sub = np.sqrt(data1**2 + data2**2 - 2 * 0.5 * data1 * data2)\n    assert_allclose(result_sub.array, expected_sub)\n\n    # Test multiply operation with correlation\n    result_mul = uncert1.propagate(np.multiply, ndd2, ndd1.data * ndd2.data, correlation=0.5)\n    expected_mul = np.abs(ndd1.data * ndd2.data) * np.sqrt((data1/data1)**2 + (data2/data2)**2 + 2 * 0.5 * (data1/data1) * (data2/data2))\n    assert_allclose(result_mul.array, expected_mul)\n\n    # Test divide operation with correlation\n    result_div = uncert1.propagate(np.divide, ndd2, ndd1.data / ndd2.data, correlation=0.5)\n    expected_div = np.abs(ndd1.data / ndd2.data) * np.sqrt((data1/data1)**2 + (data2/data2)**2 - 2 * 0.5 * (data1/data1) * (data2/data2))\n    assert_all"], "sample_3": ["def test_ecsv_version_backward_compatibility():\n    \"\"\"\n    Test that ECSV format versions maintain backward compatibility for reading files.\n    Specifically, ensure that files written with older versions can still be read.\n    \"\"\"\n    ecsv_v09 = \"\"\"", "def test_ecsv_outputter_convert_vals():\n    \"\"\"\n    Test the `_convert_vals` method in EcsvOutputter.\n    This method is responsible for converting string values to their respective data types.\n    \"\"\"\n    from astropy.io.ascii.ecsv import EcsvOutputter\n    from astropy.io.ascii.core import Column\n\n    col1 = Column(name='col1', str_vals=['1', '2', '3'], dtype='int32')\n    col2 = Column(name='col2', str_vals=['1.1', '2.2', '3.3'], dtype='float64')\n    col3 = Column(name='col3', str_vals=['true', 'false', 'true'], dtype='bool')\n    col4 = Column(name='col4', str_vals=['\"a\"', '\"b\"', '\"c\"'], dtype='str')\n    col5 = Column(name='col5', str_vals=['{\"key\": \"value\"}', '{\"key2\": \"value2\"}', '{\"key3\": \"value3\"}'], dtype='str', subtype='json')\n\n    cols = [col1, col2, col3, col4, col5]\n    outputter = EcsvOutputter()\n\n    # Call the method to convert values\n    outputter._convert_vals(cols)\n\n    # Check the converted values\n    assert np.array_equal(col1.data, np.array([1, 2, 3], dtype='int32'))\n    assert np.array_equal(col2.data, np.array([1.1, 2.2, 3.3], dtype='float64'))\n    assert np.array_equal(col3.data, np.array([True, False, True], dtype='bool'))\n    assert np.array_equal(col4.data, np.array(['a', 'b', 'c'], dtype='str'))\n    assert col5.data[0]['key'] == 'value'\n    assert col5.data[1]['key2'] == 'value2'\n    assert col5.data[2]['key3'] == 'value3'\n", "def test_multidim_string_array():\n    \"\"\"Test a multidimensional string array column for both writing and reading.\"\"\"\n    t = Table()\n    t['a'] = np.array([[[\"one\", \"two\"], [\"three\", \"four\"]], [[\"five\", \"six\"], [\"seven\", \"eight\"]]], dtype='U5')\n\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n\n    assert np.all(t2['a'] == t['a'])\n    assert t2['a'].shape == t['a'].shape\n    assert t2['a'].dtype == t['a'].dtype\n", "def test_ecsv_with_units_and_meta():\n    \"\"\"\n    Test that units and metadata are correctly round-tripped in an ECSV file.\n    \"\"\"\n    t = Table()\n    t['a'] = Column([1, 2, 3], unit='m', description='column a', meta={'key': 'value'})\n    t['b'] = Column([4.5, 5.5, 6.5], unit='s', description='column b', meta={'info': 'additional info'})\n    \n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    \n    assert t2['a'].unit == 'm'\n    assert t2['a'].description == 'column a'\n    assert t2['a'].meta == {'key': 'value'}\n    \n    assert t2['b'].unit == 's'\n    assert t2['b'].description == 'column b'\n    assert t2['b'].meta == {'info': 'additional info'}\n    \n    assert np.all(t2['a'] == t['a'])\n    assert np.all(t2['b'] == t['b'])\n", "def test_write_and_read_multidim_arrays_with_different_dtypes():\n    \"\"\"\n    Test writing and reading back a table with multi-dimensional arrays \n    of different dtypes to ensure proper serialization and deserialization.\n    \"\"\"\n    t = Table()\n    t['int_array'] = np.arange(24).reshape(2, 3, 4).astype(np.int32)\n    t['float_array'] = np.arange(24).reshape(2, 3, 4).astype(np.float64)\n    t['bool_array'] = (np.arange(24).reshape(2, 3, 4) % 2 == 0).astype(np.bool_)\n    t['str_array'] = np.array([\n        [['a', 'b', 'c', 'd'], ['e', 'f', 'g', 'h'], ['i', 'j', 'k', 'l']],\n        [['m', 'n', 'o', 'p'], ['q', 'r', 's', 't'], ['u', 'v', 'w', 'x']]\n    ])\n\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n\n    assert t.colnames == t2.colnames\n    for col in t.colnames:\n        assert np.array_equal(t[col], t2[col])\n        assert t[col].dtype == t2[col].dtype\n\n        if t[col].dtype.kind == 'b':\n            assert t2[col].dtype.kind == 'b'\n        elif t[col].dtype.kind == 'f':\n            assert t2[col].dtype.kind == 'f'\n        elif t[col].dtype.kind == 'i':\n            assert t2[col].dtype.kind == 'i'\n        elif t[col].dtype.kind == 'U':\n            assert t2[col].dtype.kind == 'U'\n"], "sample_1": ["def test_separability_matrix():\n    result = separability_matrix(sh1 & sh2)\n    assert_allclose(result, np.array([[True, False], [False, True]]))\n\n    result = separability_matrix(sh1 & rot)\n    assert_allclose(result, np.array([[True, False, False], [False, True, True]]))\n\n    result = separability_matrix(rot & sh1)\n    assert_allclose(result, np.array([[True, True, False], [True, True, False], [False, False, True]]))\n\n    result = separability_matrix(rot & sh1 & sh2)\n    assert_allclose(result, cm_4d_expected[1])\n\n    result = separability_matrix(map1 | sh1 & sh2)\n    assert_allclose(result, np.array([[True, False], [False, True], [True, False], [False, True]]))\n\n    result = separability_matrix(p2)\n    assert_allclose(result, np.array([[True, True]]))\n", "def test_compute_n_outputs():\n    # Test with Model instances\n    left_model = models.Gaussian1D()\n    right_model = models.Gaussian1D()\n    noutp = _compute_n_outputs(left_model, right_model)\n    assert noutp == left_model.n_outputs + right_model.n_outputs\n\n    # Test with ndarray inputs\n    left_array = np.zeros((2, 3))\n    right_array = np.zeros((4, 3))\n    noutp = _compute_n_outputs(left_array, right_array)\n    assert noutp == left_array.shape[0] + right_array.shape[0]\n", "def test_separability_matrix():\n    result = separability_matrix(sh1 & sh2 | rot & scl1)\n    assert_allclose(result, np.array([[True, False], [True, False], [False, True], [False, True]]))\n\n    result = separability_matrix(map3 | sh1 & sh2)\n    assert_allclose(result, np.array([[True, False], [True, False]]))\n\n    result = separability_matrix(rot & (sh1 & sh2))\n    assert_allclose(result, np.array([[True, True, False], [True, True, False], [False, False, True]]))\n\n    result = separability_matrix(sh1 & sh2 | map2 | rot & scl1)\n    assert_allclose(result, np.array([[True, False], [True, False], [False, True]]))\n\n    with pytest.raises(ModelDefinitionError,\n                       match=r\"Models cannot be combined with the \\\"|\\\" operator; .*\"):\n        separability_matrix(sh1 | map1)\n", "def test_compute_n_outputs():\n    # Test with Model instances\n    model1 = models.Shift(1)\n    model2 = models.Scale(2)\n    noutp = _compute_n_outputs(model1, model2)\n    assert noutp == 2\n\n    # Test with numpy arrays\n    array1 = np.array([[1, 2], [3, 4]])\n    array2 = np.array([[5, 6], [7, 8], [9, 10]])\n    noutp = _compute_n_outputs(array1, array2)\n    assert noutp == 5\n\n    # Mixed test with Model and numpy array\n    noutp = _compute_n_outputs(model1, array2)\n    assert noutp == 4\n", "def test_separability_matrix_with_simple_models():\n    # Testing separability_matrix function with simple models\n    result = separability_matrix(sh1 & sh2)\n    assert_allclose(result, np.array([[True, False], [False, True]]))\n\n    result = separability_matrix(scl1 & scl2)\n    assert_allclose(result, np.array([[True, False], [False, True]]))\n\n    result = separability_matrix(rot)\n    assert_allclose(result, np.array([[True, True], [True, True]]))\n\n    result = separability_matrix(p2)\n    assert_allclose(result, np.array([[True, True], [False, False]]))\n"], "sample_2": ["def test_initialize_with_variance_uncertainty():\n    data = np.ones((5, 5))\n    uncertainty = VarianceUncertainty(data * 0.1)\n    ccd = CCDData(data, unit='adu', uncertainty=uncertainty)\n    assert isinstance(ccd.uncertainty, VarianceUncertainty)\n    np.testing.assert_array_equal(ccd.uncertainty.array, data * 0.1)\n    assert ccd.uncertainty.parent_nddata is ccd\n", "def test_ccddata_init_with_nddataarray():\n    nddata = NDDataArray(data=np.ones((5, 5)), unit=\"adu\")\n    ccd = CCDData(nddata)\n    assert np.array_equal(ccd.data, nddata.data)\n    assert ccd.unit == nddata.unit\n", "def test_ccddata_arithmetic_with_and_without_units():\n    ccd_data = create_ccd_data()\n    ccd_data_without_unit = CCDData(ccd_data.data)\n    \n    with pytest.raises(ValueError):\n        ccd_data.add(ccd_data_without_unit)\n    \n    with pytest.raises(ValueError):\n        ccd_data.subtract(ccd_data_without_unit)\n    \n    with pytest.raises(ValueError):\n        ccd_data.multiply(ccd_data_without_unit)\n    \n    with pytest.raises(ValueError):\n        ccd_data.divide(ccd_data_without_unit)\n\n    ccd_data_without_unit.add(ccd_data_without_unit)\n    ccd_data_without_unit.subtract(ccd_data_without_unit)\n    ccd_data_without_unit.multiply(ccd_data_without_unit)\n    ccd_data_without_unit.divide(ccd_data_without_unit)\n", "def test_set_invalid_wcs_type():\n    ccd_data = create_ccd_data()\n    with pytest.raises(TypeError) as excinfo:\n        ccd_data.wcs = 'invalid_wcs'\n    assert \"the wcs must be a WCS instance.\" in str(excinfo.value)\n", "def test_arithmetic_mismatch_unit_multiplication():\n    ccd_data = create_ccd_data()\n    ccd_data.unit = u.m\n    operand = CCDData(np.ones((10, 10)), unit=u.s)\n    with pytest.raises(u.UnitConversionError):\n        ccd_data.multiply(operand)\n"], "sample_5": ["def test_parameter_initialization():\n    # Test different cases of parameter initialization and their expected behaviors\n\n    # Test initialization with a valid default Quantity and unit\n    param = Parameter(name='test_param', default=5 * u.m, unit=u.m)\n    assert param.default == 5\n    assert param.unit == u.m\n    assert param.value == 5.0\n\n    # Test initialization with a valid default float value and no unit\n    param = Parameter(name='test_param', default=5.0)\n    assert param.default == 5.0\n    assert param.unit is None\n    assert param.value == 5.0\n\n    # Test initialization with mismatched unit and default Quantity\n    with pytest.raises(ParameterDefinitionError):\n        Parameter(name='test_param', default=5 * u.s, unit=u.m)\n\n    # Test initialization with a boolean value (should raise InputParameterError)\n    with pytest.raises(InputParameterError):\n        param = Parameter(name='test_param', default=True)\n\n    # Test initialization with array of float values\n    param = Parameter(name='test_param', default=[1.0, 2.0, 3.0])\n    assert np.array_equal(param.default, np.array([1.0, 2.0, 3.0]))\n    assert param.unit is None\n    assert np.array_equal(param.value, np.array([1.0, 2.0, 3.0]))\n\n    # Test initialization with array of Quantity values\n    param = Parameter(name='test_param', default=[1 * u.m, 2 * u.m, 3 * u.m], unit=u.m)\n    assert np.array_equal(param.default, np.array([1.0, 2.0, 3.0]))\n    assert param.unit == u.m\n    assert np.array_equal(param.value, np.array([1.0, 2.0, 3.0]))\n\n    # Test initialization with incompatible array of Quantity values\n    with pytest.raises(InputParameterError):\n        param = Parameter(name='test_param', default=[1 * u.m, 2 * u.s, 3 * u.m], unit=u.m)\n\n    # Test initialization with getter and setter\n    param = Parameter(name='test_param', default=5.0, getter=lambda x: x * 2, setter=lambda x: x / 2)\n    assert param.value == 10.0\n    param.value = 20.0", "def test_parameter_initialization():\n    from astropy.units import Unit\n    from astropy.modeling.parameters import Parameter, ParameterDefinitionError\n    \n    # Test basic initialization\n    param = Parameter(name='test_param', description='A test parameter', default=10.0)\n    assert param.name == 'test_param'\n    assert param.description == 'A test parameter'\n    assert param.default == 10.0\n    assert param.value == 10.0\n\n    # Test initialization with units\n    param_with_unit = Parameter(name='test_param_unit', default=5.0 * u.m, unit=u.m)\n    assert param_with_unit.unit == u.m\n    assert param_with_unit.default == 5.0\n    assert param_with_unit.value == 5.0\n\n    # Test initialization with incompatible units\n    with pytest.raises(ParameterDefinitionError):\n        Parameter(name='test_param_incompatible_unit', default=5.0 * u.s, unit=u.m)\n\n    # Test initialization with Quantity default and no unit specified\n    param_quantity_default = Parameter(name='test_param_quantity_default', default=15.0 * u.kg)\n    assert param_quantity_default.unit == u.kg\n    assert param_quantity_default.default == 15.0\n    assert param_quantity_default.value == 15.0\n\n    # Test initialization with a bool value (should raise InputParameterError)\n    with pytest.raises(InputParameterError):\n        Parameter(name='test_param_bool', default=True)\n\n    # Test initialization with a non-numeric type (should raise InputParameterError)\n    with pytest.raises(InputParameterError):\n        Parameter(name='test_param_non_numeric', default='string_value')\n", "def test_parameter_initialization():\n    # Test parameter initialization with various valid inputs\n    param = Parameter(name='test', description='test parameter', default=5.0)\n    assert param.name == 'test'\n    assert param.description == 'test parameter'\n    assert param.value == 5.0\n\n    param = Parameter(name='test', description='test parameter', default=5 * u.m)\n    assert param.unit == u.m\n    assert param.value == 5.0\n\n    param = Parameter(name='test', description='test parameter', default=5.0, unit=u.m)\n    assert param.unit == u.m\n    assert param.value == 5.0\n\n    with pytest.raises(ParameterDefinitionError):\n        Parameter(name='test', description='test parameter', default=5 * u.m, unit=u.s)\n\n    with pytest.raises(InputParameterError):\n        Parameter(name='test', description='test parameter', default=True)\n\n    with pytest.raises(ValueError):\n        Parameter(name='test', description='test parameter', default=5.0, min=10, max=1)\n\n    with pytest.raises(TypeError):\n        Parameter(name='test', description='test parameter', default=5.0, bounds=(10, 'a'))\n", "def test_parameter_copy():\n    param = Parameter(name='test', description='test description', default=1.0, unit=u.m,\n                      getter=None, setter=None, fixed=True, tied=False, min=0.0, max=2.0)\n    param_copy = param.copy(name='test_copy', description='copied description', default=2.0, unit=u.cm,\n                            fixed=False, min=0.1, max=2.5)\n\n    assert param_copy.name == 'test_copy'\n    assert param_copy.description == 'copied description'\n    assert param_copy.default == 2.0\n    assert param_copy.unit == u.cm\n    assert param_copy.fixed is False\n    assert param_copy.min == 0.1\n    assert param_copy.max == 2.5\n    assert param_copy.bounds == (0.1, 2.5)\n", "def test_tofloat(value, expected):\n    result = _tofloat(value)\n    if isinstance(expected, Quantity):\n        assert isinstance(result, Quantity)\n        assert_quantity_allclose(result, expected)\n    else:\n        assert np.allclose(result, expected)\n"], "sample_6": ["def test_wrap_at():\n    \"\"\"\n    Test the wrap_at method of Angle, Latitude, and Longitude classes.\n    \"\"\"\n    # Test for Angle\n    a = Angle([-20.0, 150.0, 350.0] * u.deg)\n    wrapped_a = a.wrap_at(360 * u.deg)\n    npt.assert_almost_equal(wrapped_a.degree, [340.0, 150.0, 350.0])\n\n    a.wrap_at(180 * u.deg, inplace=True)\n    npt.assert_almost_equal(a.degree, [-20.0, 150.0, -10.0])\n\n    # Test for Longitude\n    l = Longitude([-190.0, 190.0, 350.0] * u.deg)\n    wrapped_l = l.wrap_at(180 * u.deg)\n    npt.assert_almost_equal(wrapped_l.degree, [170.0, -170.0, -10.0])\n\n    l.wrap_at(360 * u.deg, inplace=True)\n    npt.assert_almost_equal(l.degree, [170.0, 190.0, 350.0])\n\n    # Test for Latitude (should raise ValueError if out of bounds)\n    with pytest.raises(ValueError):\n        Latitude([-100.0, 45.0, 100.0] * u.deg)\n", "def test_angle_to_string():\n    \"\"\"\n    Test the to_string method for Angle objects.\n    \"\"\"\n    a1 = Angle(10.2345, unit=u.deg)\n    assert a1.to_string() == '10d14m04.2s'\n    assert a1.to_string(decimal=True) == '10.2345'\n    assert a1.to_string(sep=':', precision=2) == '10:14:04.20'\n    assert a1.to_string(unit=u.hourangle) == '0h40m56.28s'\n    assert a1.to_string(format='latex') == r'$10^\\circ14{}^\\prime04.2{}^{\\prime\\prime}$'\n    assert a1.to_string(format='unicode') == '10\u00b014\u203204.2\u2033'\n\n    a2 = Angle(['10.2345d', '-20d'])\n    assert np.array_equal(a2.to_string(), ['10d14m04.2s', '-20d00m00s'])\n\n    a3 = Angle('1:2:30.43 degrees')\n    assert a3.to_string(sep=':', precision=2) == '1:02:30.43'\n    assert a3.to_string(unit=u.hourangle) == '0h04m10.71s'\n\n    a4 = Angle('1 2 0 hours')\n    assert a4.to_string() == '1h02m00s'\n    assert a4.to_string(unit=u.deg) == '15d30m00s'\n    assert a4.to_string(format='latex') == r'$1^{\\mathrm{h}}02^{\\mathrm{m}}00^{\\mathrm{s}}$'\n\n    a5 = Angle('1\u00b02\u20323\u2033')\n    assert a5.to_string() == '1d02m03s'\n    assert a5.to_string(unit=u.hourangle) == '0h04m08.2s'\n\n    a6 = Angle('-1h2.5m')\n    assert a6.to_string() == '-1h02m30s'\n    assert a6.to_string(unit=u.deg) == '-15d37m30s'\n    assert a6.to_string(alwayssign=True) == '-1h02m30s'\n\n    a7 = Angle('-1h2.5mW')\n    assert a7.to_string() == '1h02m30s'\n    assert a", "def test_angle_wrapping():\n    \"\"\"\n    Test the wrap_at method of the Angle class.\n    \"\"\"\n    angles = Angle([-20.0, 150.0, 350.0] * u.deg)\n\n    # Wrap into range 0 to 360 degrees\n    wrapped_angles_360 = angles.wrap_at(360 * u.deg)\n    npt.assert_almost_equal(wrapped_angles_360.degree, [340.0, 150.0, 350.0])\n\n    # Wrap into range -180 to 180 degrees\n    wrapped_angles_180 = angles.wrap_at(180 * u.deg)\n    npt.assert_almost_equal(wrapped_angles_180.degree, [-20.0, 150.0, -10.0])\n\n    # In-place wrapping\n    angles.wrap_at(180 * u.deg, inplace=True)\n    npt.assert_almost_equal(angles.degree, [-20.0, 150.0, -10.0])\n\n    # Wrap with angles in radians\n    angles_rad = Angle([-np.pi, np.pi/2, 3*np.pi] * u.rad)\n    wrapped_angles_rad = angles_rad.wrap_at(2 * np.pi * u.rad)\n    npt.assert_almost_equal(wrapped_angles_rad.rad, [-np.pi, np.pi/2, np.pi])\n\n    wrapped_angles_rad_180 = angles_rad.wrap_at(np.pi * u.rad)\n    npt.assert_almost_equal(wrapped_angles_rad_180.rad, [-np.pi, np.pi/2, -np.pi])\n", "def test_angle_to_string():\n    a1 = Angle(10.2345 * u.deg)\n    assert a1.to_string() == '10d14m4.2s'\n    assert a1.to_string(decimal=True) == '10.2345'\n    assert a1.to_string(sep=':', precision=2) == '10:14:04.20'\n    assert a1.to_string(format='latex') == '$10^{\\circ}14{}^{\\prime}4.2{}^{\\prime\\prime}$'\n    assert a1.to_string(format='unicode') == '10\u00b014\u20324.2\u2033'\n\n    a2 = Angle('-1h2m3s')\n    assert a2.to_string() == '-1h2m3s'\n    assert a2.to_string(decimal=True) == '-1.0341666666666667'\n    assert a2.to_string(sep=':', precision=1) == '-1:02:03.0'\n    assert a2.to_string(format='latex') == '$-1^{\\mathrm{h}}2^{\\mathrm{m}}3^{\\mathrm{s}}$'\n    assert a2.to_string(format='unicode') == '-1\u02b02\u1d503\u02e2'\n", "def test_angle_creation():\n    \"\"\"\n    Test creating Angle objects with various input formats.\n    \"\"\"\n    a1 = Angle('10.2345d')\n    assert a1.value == 10.2345\n    assert a1.unit == u.degree\n\n    a2 = Angle(['10.2345d', '-20d'])\n    npt.assert_almost_equal(a2.value, [10.2345, -20.0])\n    assert a2.unit == u.degree\n\n    a3 = Angle('1:2:30.43 degrees')\n    npt.assert_almost_equal(a3.value, 1.04178611)\n    assert a3.unit == u.degree\n\n    a4 = Angle('1 2 0 hours')\n    npt.assert_almost_equal(a4.value, 1.03333333)\n    assert a4.unit == u.hourangle\n\n    a5 = Angle(np.arange(1, 8), unit=u.deg)\n    npt.assert_almost_equal(a5.value, [1., 2., 3., 4., 5., 6., 7.])\n    assert a5.unit == u.degree\n\n    a6 = Angle('1\u00b02\u20323\u2033')\n    npt.assert_almost_equal(a6.value, 1.03416667)\n    assert a6.unit == u.degree\n\n    a7 = Angle('1d2m3.4s')\n    npt.assert_almost_equal(a7.value, 1.03427778)\n    assert a7.unit == u.degree\n\n    a8 = Angle('-1h2m3s')\n    npt.assert_almost_equal(a8.value, -1.03416667)\n    assert a8.unit == u.hourangle\n\n    a9 = Angle('-1:2.5', unit=u.deg)\n    npt.assert_almost_equal(a9.value, -1.04166667)\n    assert a9.unit == u.degree\n\n    a10 = Angle(10.2345 * u.deg)\n    assert a10.value == 10.2345\n    assert a10.unit == u.degree\n\n    a11 = Angle(Angle(10.2345 * u.deg))\n    assert a11.value == 10.2345\n    assert a11.unit == u.degree\n"], "sample_7": ["def test_column_copy():\n    \"\"\"\n    Test that copying a Column instance retains all relevant attributes\n    and data, and that modifications to the copy do not affect the original.\n    \"\"\"\n    c = table.Column([1, 2, 3], name='a', dtype=int, unit='mJy', format='%i', description='test column', meta={'c': 8, 'd': 12})\n    c_copy = c.copy()\n\n    # Check that the copied column has the same attributes as the original\n    assert c_copy.name == 'a'\n    assert c_copy.dtype == int\n    assert c_copy.unit == 'mJy'\n    assert c_copy.format == '%i'\n    assert c_copy.description == 'test column'\n    assert c_copy.meta == {'c': 8, 'd': 12}\n    assert np.all(c_copy == np.array([1, 2, 3]))\n\n    # Modify the copy and ensure the original is unaffected\n    c_copy[0] = 10\n    c_copy.name = 'b'\n    c_copy.unit = 'Jy'\n    c_copy.format = '%d'\n    c_copy.description = 'modified column'\n    c_copy.meta = {'e': 10, 'f': 14}\n\n    assert np.all(c == np.array([1, 2, 3]))\n    assert c.name == 'a'\n    assert c.unit == 'mJy'\n    assert c.format == '%i'\n    assert c.description == 'test column'\n    assert c.meta == {'c': 8, 'd': 12}\n\n    assert np.all(c_copy == np.array([10, 2, 3]))\n    assert c_copy.name == 'b'\n    assert c_copy.unit == 'Jy'\n    assert c_copy.format == '%d'\n    assert c_copy.description == 'modified column'\n    assert c_copy.meta == {'e': 10, 'f': 14}\n", "def test_column_info_dict_representation():\n    \"\"\"\n    Test the _represent_as_dict method for the ColumnInfo class.\n    Ensure that the dictionary representation contains all the expected keys.\n    \"\"\"\n    c = table.Column([1, 2, 3], name='a', description='b', unit='m', format=\"%i\", meta={'c': 8})\n    info_dict = c.info._represent_as_dict()\n\n    expected_keys = {'name', 'unit', 'format', 'description', 'meta', 'dtype', 'shape'}\n    assert set(info_dict.keys()) == expected_keys\n\n    assert info_dict['name'] == 'a'\n    assert info_dict['unit'] == 'm'\n    assert info_dict['format'] == '%i'\n    assert info_dict['description'] == 'b'\n    assert info_dict['meta'] == {'c': 8}\n    assert info_dict['dtype'] == 'int64'\n    assert info_dict['shape'] == []\n\n    # Test for structured columns\n    c2 = table.Column([(1, 2), (3, 4)], dtype=[('x', 'i4'), ('y', 'f4')])\n    info_dict = c2.info._represent_as_dict()\n    assert 'data' in info_dict\n    assert isinstance(info_dict['data'], dict)\n    assert set(info_dict['data'].keys()) == {'x', 'y'}\n", "def test_col_copy():\n    \"\"\"Test the col_copy function for both Column and MaskedColumn.\"\"\"\n    col = table.Column(data=[1, 2, 3], name='a', unit='m', format='%i', description='desc', meta={'key': 'value'})\n    col_copy_inst = col_copy(col)\n    assert isinstance(col_copy_inst, table.Column)\n    assert col_copy_inst.name == 'a'\n    assert col_copy_inst.unit == u.m\n    assert col_copy_inst.format == '%i'\n    assert col_copy_inst.description == 'desc'\n    assert col_copy_inst.meta == {'key': 'value'}\n    assert np.all(col_copy_inst == col)\n\n    mcol = table.MaskedColumn(data=[1, 2, 3], mask=[False, True, False], name='b', unit='s', format='%d', description='masked desc', meta={'masked_key': 'masked_value'})\n    mcol_copy_inst = col_copy(mcol)\n    assert isinstance(mcol_copy_inst, table.MaskedColumn)\n    assert mcol_copy_inst.name == 'b'\n    assert mcol_copy_inst.unit == u.s\n    assert mcol_copy_inst.format == '%d'\n    assert mcol_copy_inst.description == 'masked desc'\n    assert mcol_copy_inst.meta == {'masked_key': 'masked_value'}\n    assert np.all(mcol_copy_inst.data == mcol.data)\n    assert np.all(mcol_copy_inst.mask == mcol.mask)\n", "def test_col_copy_attributes():\n    \"\"\"\n    Test that `col_copy` function correctly copies attributes of a column,\n    including info attributes and indices.\n    \"\"\"\n    c = table.Column([1, 2, 3], name='a', unit='m', format='%d', description='test column',\n                     meta={'key': 'value'})\n    c.info.indices = [1, 2, 3]\n\n    new_c = col_copy(c)\n    \n    assert new_c.name == 'a'\n    assert new_c.unit == u.m\n    assert new_c.format == '%d'\n    assert new_c.description == 'test column'\n    assert new_c.meta == {'key': 'value'}\n    assert new_c.info.indices == [1, 2, 3]\n    assert np.all(new_c == c)\n\n    # Test for MaskedColumn\n    mc = table.MaskedColumn([1, 2, 3], name='b', unit='km', format='%d', description='masked column',\n                            meta={'another_key': 'another_value'}, mask=[False, True, False])\n    mc.info.indices = [1, 2, 3]\n\n    new_mc = col_copy(mc)\n    \n    assert new_mc.name == 'b'\n    assert new_mc.unit == u.km\n    assert new_mc.format == '%d'\n    assert new_mc.description == 'masked column'\n    assert new_mc.meta == {'another_key': 'another_value'}\n    assert new_mc.info.indices == [1, 2, 3]\n    assert np.all(new_mc == mc)\n    assert np.all(new_mc.mask == mc.mask)\n", "def test_convert_sequence_data_to_array():\n    \"\"\"Test _convert_sequence_data_to_array function with various inputs\"\"\"\n\n    # Test with a list of integers\n    data = [1, 2, 3]\n    result = _convert_sequence_data_to_array(data)\n    assert isinstance(result, np.ndarray)\n    assert result.dtype.kind == 'i'\n    assert np.array_equal(result, np.array(data))\n\n    # Test with a list containing np.ma.masked\n    data = [1, np.ma.masked, 3]\n    result = _convert_sequence_data_to_array(data)\n    assert isinstance(result, np.ma.MaskedArray)\n    assert result.dtype.kind == 'f'\n    assert np.array_equal(result.data, np.array([1, np.nan, 3]))\n    assert np.array_equal(result.mask, [False, True, False])\n\n    # Test with a list of lists with masked values\n    data = [[1, 2], [np.ma.masked, 4]]\n    result = _convert_sequence_data_to_array(data)\n    assert isinstance(result, np.ma.MaskedArray)\n    assert result.dtype.kind == 'f'\n    assert np.array_equal(result.data, np.array([[1, 2], [np.nan, 4]]))\n    assert np.array_equal(result.mask, [[False, False], [True, False]])\n\n    # Test with a list of masked arrays\n    data = [np.ma.array([1, 2], mask=[False, True]), np.ma.array([3, 4], mask=[False, False])]\n    result = _convert_sequence_data_to_array(data)\n    assert isinstance(result, np.ma.MaskedArray)\n    assert result.dtype.kind == 'i'\n    assert np.array_equal(result.data, np.array([[1, 2], [3, 4]]))\n    assert np.array_equal(result.mask, np.array([[False, True], [False, False]]))\n\n    # Test with a nested list of strings\n    data = [['a', 'b'], ['c', 'd']]\n    result = _convert_sequence_data_to_array(data)\n    assert isinstance(result, np.ndarray)\n    assert result.dtype.kind == 'U'\n    assert np.array_equal(result, np.array(data))\n\n    # Test with mixed data types\n    data = [1, 'a', 3.0]\n    result = _convert_sequence_data_to_array(data)\n    assert isinstance(result, np.ndarray)\n    assert result.dtype.kind == 'O'\n"], "sample_8": ["    def test_apply_to_both_functions(self):\n        from astropy.utils.masked import Masked\n\n        a = Masked(np.array([1, 2, 3]), mask=[False, True, False])\n        b = Masked(np.array([4, 5, 6]), mask=[False, False, True])\n        data_args, mask_args, kwargs, out = masked_a_helper(a, b)\n\n        assert_array_equal(data_args[0], a.unmasked)\n        assert_array_equal(data_args[1], b.unmasked)\n        assert_array_equal(mask_args[0], a.mask)\n        assert_array_equal(mask_args[1], b.mask)\n        assert kwargs == {}\n        assert out is None\n", "    def setup_method(self):\n        self.data = np.array([1, 2, 3, 4, 5], dtype=float)\n        self.mask = np.array([False, True, False, True, False])\n        self.masked_array = Masked(self.data, self.mask)\n", "    def test_dispatched_functions(self, func):\n        ma_func = getattr(func, '__self__', func)\n        if func in [np.outer, np.concatenate, np.append, np.broadcast_arrays, np.insert, np.choose, np.select, np.piecewise]:\n            if func in [np.outer]:\n                result = ma_func(self.ma, self.mb)\n            elif func in [np.concatenate, np.append]:\n                result = ma_func([self.ma, self.mb], axis=0)\n            elif func in [np.broadcast_arrays]:\n                result = ma_func(self.ma, self.mb)\n            elif func in [np.insert, np.choose, np.select, np.piecewise]:\n                result = ma_func(self.ma, 1, self.mb)\n        elif func in [np.unpackbits, np.packbits]:\n            result = ma_func(self.ma.astype(np.uint8))\n        else:\n            result = ma_func(self.ma)\n\n        assert isinstance(result, Masked)\n        assert_array_equal(result.mask, self.ma.mask)\n", "    def test_broadcast_to(self):\n        shape = (3, 2, 3)\n        mb_broadcast = np.broadcast_to(self.mb, shape)\n        expected_data = np.broadcast_to(self.b, shape)\n        expected_mask = np.broadcast_to(self.mask_b, shape)\n        assert_array_equal(mb_broadcast.unmasked, expected_data)\n        assert_array_equal(mb_broadcast.mask, expected_mask)\n", "    def test_datetime_as_string(self):\n        dates = np.array(['2021-01-01', '2021-01-02'], dtype='datetime64[D]')\n        mask = np.array([False, True])\n        mdates = Masked(dates, mask)\n        result = datetime_as_string(mdates)\n        expected_data = np.datetime_as_string(dates)\n        expected_mask = mask.copy()\n        assert_array_equal(result.unmasked, expected_data)\n        assert_array_equal(result.mask, expected_mask)\n"], "sample_9": ["def test_html_header_set_cols_from_names():\n    \"\"\"\n    Test to ensure that HTMLHeader._set_cols_from_names correctly handles\n    setting columns from header names, including handling multicolumn headers.\n    \"\"\"\n    header = html.HTMLHeader()\n    \n    # Single column names\n    header.names = ['A', 'B', 'C']\n    header._set_cols_from_names()\n    assert len(header.cols) == 3\n    assert [col.name for col in header.cols] == ['A', 'B', 'C']\n    \n    # Multicolumn headers\n    header.names = [('A', 2), 'B', ('C', 3)]\n    header._set_cols_from_names()\n    assert len(header.cols) == 6\n    assert [col.name for col in header.cols] == ['A', '', 'B', 'C', '', '']\n    assert header.cols[0].colspan == 2\n    assert header.cols[3].colspan == 3\n", "def test_htmlheader_set_cols_from_names():\n    \"\"\"\n    Test to ensure that _set_cols_from_names method of HTMLHeader\n    handles both simple and multicolumn headers appropriately.\n    \"\"\"\n\n    # Simple header names\n    header = html.HTMLHeader()\n    header.names = ['A', 'B', 'C']\n    header._set_cols_from_names()\n    assert [col.name for col in header.cols] == ['A', 'B', 'C']\n\n    # Multicolumn headers\n    header.names = [('A', '2'), 'B', ('C', '3')]\n    header._set_cols_from_names()\n    expected_names = ['A', '', 'B', 'C', '', '']\n    assert [col.name for col in header.cols] == expected_names\n    assert header.names == expected_names  # Check that header.names is updated correctly\n\n    # Verify the colspan attribute is set correctly for multicolumns\n    assert header.cols[0].colspan == 2\n    assert header.cols[3].colspan == 3\n\n    # Verify dummy columns are added correctly\n    assert isinstance(header.cols[1], core.Column)\n    assert isinstance(header.cols[4], core.Column)\n    assert isinstance(header.cols[5], core.Column)\n", "def test_htmlheader_set_cols_from_names():\n    \"\"\"\n    Test to ensure that the HTMLHeader correctly sets columns\n    from header names, especially handling multicolumns appropriately.\n    \"\"\"\n\n    header = html.HTMLHeader()\n    \n    # Single column names\n    header.names = ['A', 'B', 'C']\n    header._set_cols_from_names()\n    assert len(header.cols) == 3\n    assert header.cols[0].name == 'A'\n    assert header.cols[1].name == 'B'\n    assert header.cols[2].name == 'C'\n\n    # Multicolumn names\n    header.names = [('A', '2'), 'B', ('C', '3')]\n    header._set_cols_from_names()\n    assert len(header.cols) == 6\n    assert header.cols[0].name == 'A'\n    assert header.cols[0].colspan == 2\n    assert header.cols[1].name == ''\n    assert header.cols[2].name == 'B'\n    assert header.cols[3].name == 'C'\n    assert header.cols[3].colspan == 3\n    assert header.cols[4].name == ''\n    assert header.cols[5].name == ''\n", "def test_htmlheader_set_cols_from_names():\n    \"\"\"\n    Test to ensure that HTMLHeader sets columns from header names\n    correctly, handling multicolumns appropriately.\n    \"\"\"\n    header = html.HTMLHeader()\n    \n    # Test with single columns\n    header.names = ['A', 'B', 'C']\n    header._set_cols_from_names()\n    assert len(header.cols) == 3\n    assert [col.name for col in header.cols] == ['A', 'B', 'C']\n    \n    # Test with multicolumns\n    header.names = [('A', 2), 'B', ('C', 3)]\n    header._set_cols_from_names()\n    assert len(header.cols) == 6\n    assert [col.name for col in header.cols] == ['A', '', 'B', 'C', '', '']\n    assert header.cols[0].colspan == 2\n    assert header.cols[3].colspan == 3\n\n    # Ensure no dummy columns are added for single columns\n    header.names = ['A', 'B', 'C']\n    header._set_cols_from_names()\n    assert len(header.cols) == 3\n    assert [col.name for col in header.cols] == ['A', 'B', 'C']\n", "def test_htmlheader_set_cols_from_names():\n    \"\"\"\n    Test to ensure that HTMLHeader sets the columns correctly\n    from header names, handling multicolumns appropriately.\n    \"\"\"\n\n    header = html.HTMLHeader()\n    header.names = ['A', ('B', '2'), 'C']\n    header._set_cols_from_names()\n\n    assert len(header.cols) == 4\n    assert header.cols[0].name == 'A'\n    assert header.cols[1].name == 'B'\n    assert header.cols[1].colspan == 2\n    assert header.cols[2].name == ''\n    assert header.cols[3].name == 'C'\n\n    assert header.names == ['A', 'B', '', 'C']\n"], "sample_10": ["def test_setitem_with_complex_keys(table_types):\n    \"\"\"Test setting items using complex keys like tuples and slices.\"\"\"\n    self._setup(table_types)\n    t = table_types.Table([self.a, self.b, self.c])\n\n    # Set multiple columns using a tuple of column names\n    t['a', 'b'] = [[10, 20, 30], [40, 50, 60]]\n    assert np.all(t['a'] == [10, 20, 30])\n    assert np.all(t['b'] == [40, 50, 60])\n\n    # Set using a slice\n    t['a'][1:3] = [100, 200]\n    assert np.all(t['a'] == [10, 100, 200])\n\n    # Set using a list of indices\n    t['b'][[0, 2]] = [400, 600]\n    assert np.all(t['b'] == [400, 50, 600])\n\n    # Set using a single index\n    t['c'][1] = 800\n    assert t['c'][1] == 800\n\n    # Test set item with a tuple of slices (e.g., multi-dimensional slicing)\n    t['d'] = table_types.Column(np.arange(12).reshape(3, 2, 2))\n    t['d'][1, :, 1] = [900, 1000]\n    assert np.all(t['d'][1, :, 1] == [900, 1000])\n", "    def test_warn_replace_column(self, table_types):\n        self._setup(table_types)\n        t = table_types.Table([self.a, self.b])\n        \n        with pytest.warns(TableReplaceWarning, match=\"replaced column 'a'\"):\n            t.replace_column('a', [10, 20, 30])\n", "def test_add_columns_with_mixin():\n    \"\"\"Test adding columns to table with existing mixin columns\"\"\"\n    from astropy.time import Time\n\n    t = table.Table([[1, 2]], names=['a'])\n    t['time'] = Time(['2000-01-01', '2000-01-02'])\n    t.add_columns([[3, 4], [5, 6]], names=['b', 'c'])\n\n    assert t.colnames == ['a', 'time', 'b', 'c']\n    assert np.all(t['a'] == [1, 2])\n    assert np.all(t['time'].iso == ['2000-01-01 00:00:00.000', '2000-01-02 00:00:00.000'])\n    assert np.all(t['b'] == [3, 4])\n    assert np.all(t['c'] == [5, 6])\n", "def test_setitem_inplace():\n    \"\"\"Test the `_setitem_inplace` attribute functionality in `Table` that\n    allows in-place modification of columns.\"\"\"\n    t = table.Table([[1, 2, 3], [4, 5, 6]], names=['a', 'b'])\n    \n    # Normal behavior, should replace the column\n    t['a'] = [7, 8, 9]\n    assert np.all(t['a'] == [7, 8, 9])\n    \n    # Now setting the _setitem_inplace attribute to True\n    t._setitem_inplace = True\n    ta = t['a']\n    t['a'] = [10, 11, 12]\n    \n    # In this case, it should update in-place instead of replacing\n    assert t['a'] is ta\n    assert np.all(t['a'] == [10, 11, 12])\n\n    # Also verify that setting `conf.replace_inplace` to True has the same effect\n    with table.conf.set_temp('replace_inplace', True):\n        t['b'] = [13, 14, 15]\n        assert t['b'] is not t['b']  # Column reference should remain the same\n        assert np.all(t['b'] == [13, 14, 15])\n", "    def test_group_by_single_column(self, table_types):\n        self._setup(table_types)\n        t = table_types.Table([self.a, self.d])\n        grouped = t.group_by('d')\n        assert np.array_equal(grouped.groups.keys['d'], [1, 3, 7])\n        assert np.array_equal(grouped['a'], [3, 2, 1])\n        assert np.array_equal(grouped['d'], [1, 3, 7])\n"], "sample_12": ["def test_angle_to_string_format():\n    \"\"\"\n    Test various formatting options for Angle to_string method\n    \"\"\"\n    a = Angle(10.2345, u.deg)\n    \n    # Test different units\n    assert a.to_string(unit=u.hour) == '0h40m56.28s'\n    assert a.to_string(unit=u.radian) == '0.178627rad'\n    \n    # Test decimal representation\n    assert a.to_string(unit=u.deg, decimal=True, precision=4) == '10.2345'\n    assert a.to_string(unit=u.hour, decimal=True, precision=2) == '0.68'\n    \n    # Test different separators\n    assert a.to_string(unit=u.deg, sep=':', precision=1) == '10:14:04.2'\n    assert a.to_string(unit=u.deg, sep='-', precision=2) == '10-14-04.20'\n    assert a.to_string(unit=u.deg, sep=('d', 'm', 's'), precision=3) == '10d14m04.200s'\n    \n    # Test padding and always sign\n    assert a.to_string(unit=u.deg, pad=True, precision=1) == '10d14m04.2s'\n    assert a.to_string(unit=u.deg, alwayssign=True) == '+10d14m04.2s'\n    \n    # Test different formats\n    assert a.to_string(unit=u.deg, format='latex') == '10^\\circ14{}^\\prime04.2{}^{\\prime\\prime}'\n    assert a.to_string(unit=u.deg, format='unicode') == '10\u00b014\u203204.2\u2033'\n    \n    # Test fields\n    assert a.to_string(unit=u.deg, fields=1) == '10d'\n    assert a.to_string(unit=u.deg, fields=2) == '10d14m'\n    assert a.to_string(unit=u.deg, fields=3) == '10d14m04.2s'\n", "def test_angle_invalid_tuple():\n    \"\"\"\n    Test initialization of Angle with invalid tuples\n    \"\"\"\n    with pytest.raises(ValueError, match=\"cannot be interpreted\"):\n        Angle(('10', '20', '30'), unit=u.deg)\n    \n    with pytest.raises(ValueError, match=\"cannot be interpreted\"):\n        Angle((10, 20), unit=u.hour)\n    \n    with pytest.raises(ValueError, match=\"cannot be interpreted\"):\n        Angle(('10', '20'), unit=u.deg)\n    \n    with pytest.raises(ValueError, match=\"cannot be interpreted\"):\n        Angle(('10d', '20m'), unit=u.deg)\n    \n    with pytest.raises(ValueError, match=\"cannot be interpreted\"):\n        Angle(('10h', '20m'))\n", "def test_longitude_wrap_at():\n    \"\"\"\n    Test Longitude wrap_at method with various wrap angles and ensure correct wrapping\n    \"\"\"\n    # Testing wrap_at with default wrap_angle 360 degrees\n    lon = Longitude([370, -10, 720, -730] * u.deg)\n    wrapped_lon = lon.wrap_at(360 * u.deg)\n    assert_array_equal(wrapped_lon.degree, [10, 350, 360, 350])\n\n    # Testing wrap_at with wrap_angle 180 degrees\n    wrapped_lon = lon.wrap_at(180 * u.deg)\n    assert_array_equal(wrapped_lon.degree, [10, -10, 0, -10])\n\n    # Testing wrap_at with wrap_angle 90 degrees\n    wrapped_lon = lon.wrap_at(90 * u.deg)\n    assert_array_equal(wrapped_lon.degree, [-80, -10, 0, -10])\n\n    # Testing wrap_at with wrap_angle -180 degrees\n    wrapped_lon = lon.wrap_at(-180 * u.deg)\n    assert_array_equal(wrapped_lon.degree, [-170, 350, 0, 350])\n\n    # Testing wrap_at with wrap_angle 0 degrees\n    wrapped_lon = lon.wrap_at(0 * u.deg)\n    assert_array_equal(wrapped_lon.degree, [10, -10, 0, -10])\n\n    # Test wrapping a scalar Longitude\n    lon_scalar = Longitude('370d')\n    wrapped_lon_scalar = lon_scalar.wrap_at('180d')\n    assert wrapped_lon_scalar == Longitude('10d')\n\n    # Ensure that original Longitude object is not modified when inplace=False\n    lon = Longitude([370, -10, 720, -730] * u.deg)\n    wrapped_lon_copy = lon.wrap_at(180 * u.deg)\n    assert_array_equal(wrapped_lon_copy.degree, [10, -10, 0, -10])\n    assert_array_equal(lon.degree, [370, -10, 720, -730])\n\n    # Ensure that original Longitude object is modified when inplace=True\n    lon.wrap_at(180 * u.deg, inplace=True)\n    assert_array_equal(lon.degree, [10, -10, 0, -10])\n", "def test_angle_wrapping():\n    \"\"\"\n    Tests the wrap_at functionality for Angle objects with various wrap angles\n    \"\"\"\n    angles = Angle([0, 90, 180, 270, 360, 450] * u.deg)\n\n    wrapped_360 = angles.wrap_at(360 * u.deg)\n    assert np.all(wrapped_360.degree == np.array([0, 90, 180, 270, 0, 90]))\n\n    wrapped_180 = angles.wrap_at(180 * u.deg)\n    assert np.all(wrapped_180.degree == np.array([0, 90, -180, -90, 0, 90]))\n\n    wrapped_neg180 = angles.wrap_at(-180 * u.deg)\n    assert np.all(wrapped_neg180.degree == np.array([0, 90, -180, -90, 0, 90]))\n\n    wrapped_90 = angles.wrap_at(90 * u.deg)\n    assert np.all(wrapped_90.degree == np.array([0, 90, -90, 0, 0, 90]))\n\n    wrapped_45 = angles.wrap_at(45 * u.deg)\n    assert np.all(wrapped_45.degree == np.array([0, 45, 0, 45, 0, 45]))\n\n    # Test wrapping with angle in radians\n    wrapped_pi = angles.wrap_at(np.pi * u.rad)\n    assert np.all(wrapped_pi.degree == np.array([0, 90, -180, -90, 0, 90]))\n\n    wrapped_2pi = angles.wrap_at(2 * np.pi * u.rad)\n    assert np.all(wrapped_2pi.degree == np.array([0, 90, 180, 270, 0, 90]))\n\n    # Test wrapping with an angle in hours\n    angles_in_hours = Angle([0, 6, 12, 18, 24, 30] * u.hourangle)\n    wrapped_24h = angles_in_hours.wrap_at(24 * u.hourangle)\n    assert np.all(wrapped_24h.hour == np.array([0, 6, 12, 18, 0, 6]))\n\n    wrapped_12h = angles_in_hours.wrap_at(12 * u.hourangle)\n    assert np.all(wrapped_12h.hour == np.array([0, 6, -12, -6, 0", "def test_angle_to_string_formats():\n    \"\"\"\n    Test the `to_string` method of Angle for different formats.\n    \"\"\"\n    angle = Angle(\"54.12412\", unit=u.degree)\n    # Check LaTeX format\n    assert angle.to_string(format='latex') == r'$54^\\circ07{}^\\prime26.8320{}^{\\prime\\prime}$'\n    # Check LaTeX inline format\n    assert angle.to_string(format='latex_inline') == r'$54^\\circ07{}^\\prime26.8320{}^{\\prime\\prime}$'\n    # Check Unicode format\n    assert angle.to_string(format='unicode') == '54\u00b007\u203226.8320\u2033'\n    # Check invalid format\n    with pytest.raises(ValueError):\n        angle.to_string(format='invalid_format')\n"], "sample_13": ["def test_angle_to_string_with_precision():\n    \"\"\"\n    Test the Angle.to_string() method with different levels of precision.\n    \"\"\"\n\n    angle = Angle(123.456789, unit=u.degree)\n\n    assert angle.to_string(precision=0) == '123d27m24s'\n    assert angle.to_string(precision=1) == '123d27m24.4s'\n    assert angle.to_string(precision=2) == '123d27m24.44s'\n    assert angle.to_string(precision=3) == '123d27m24.444s'\n    assert angle.to_string(precision=4) == '123d27m24.4444s'\n    assert angle.to_string(precision=5) == '123d27m24.44444s'\n    assert angle.to_string(precision=6) == '123d27m24.444444s'\n    assert angle.to_string(precision=7) == '123d27m24.4444444s'\n    assert angle.to_string(precision=8) == '123d27m24.44444444s'\n", "def test_angle_to_string_format():\n    \"\"\"\n    Test the `to_string` method with different formats (latex, unicode) and separators.\n    \"\"\"\n\n    # Test Latex format\n    angle = Angle(\"54.12412d\")\n    assert angle.to_string(format='latex') == r'54^\\circ07{}^\\prime26.832{}^{\\prime\\prime}'\n    assert angle.to_string(unit=u.hourangle, format='latex') == r'3^{\\mathrm{h}}36^{\\mathrm{m}}29.7888^{\\mathrm{s}}'\n\n    # Test Unicode format\n    assert angle.to_string(format='unicode') == '54\u00b007\u203226.832\u2033'\n    assert angle.to_string(unit=u.hourangle, format='unicode') == '3\u02b036\u1d5029.7888\u02e2'\n\n    # Test multiple separators in sexagesimal representation\n    assert angle.to_string(sep=['h', 'm', 's']) == '54h07m26.832s'\n    assert angle.to_string(sep=['-', '|', ':']) == '54-07|26.832:'\n    assert angle.to_string(unit=u.hourangle, sep=['h', 'm', 's']) == '3h36m29.7888s'\n\n    # Test precision and padding\n    assert angle.to_string(unit=u.degree, sep=':', precision=2, pad=True) == '54:07:26.83'\n    assert angle.to_string(unit=u.hourangle, sep=':', precision=3, pad=True) == '3:36:29.789'\n\n    # Test separator with degrees and radian units\n    assert angle.to_string(unit=u.degree, sep=' ') == '54d07m26.832s'\n    assert angle.to_string(unit=u.radian, decimal=True, precision=5) == '0.94464'\n", "def test_angle_to_string_unicode():\n    \"\"\"\n    Test the to_string method with unicode format.\n    \"\"\"\n    angle = Angle(\"54.12412\", unit=u.degree)\n    \n    res = 'Angle as DMS: 54\u00b007\u203226.831999999992036\u2033'\n    assert f\"Angle as DMS: {angle.to_string(format='unicode')}\" == res\n    \n    res = 'Angle as HMS: 3\u02b036\u1d5029.78879999999947\u02e2'\n    assert f\"Angle as HMS: {angle.to_string(unit=u.hour, format='unicode')}\" == res\n    \n    angle = Angle(\"-54.12412\", unit=u.degree)\n    res = 'Angle as DMS: -54\u00b007\u203226.831999999992036\u2033'\n    assert f\"Angle as DMS: {angle.to_string(format='unicode')}\" == res\n    \n    angle = Angle(\"-3.60827466667\", unit=u.hour)\n    res = 'Angle as HMS: -3\u02b036\u1d5029.788800012\u02e2'\n    assert f\"Angle as HMS: {angle.to_string(unit=u.hour, format='unicode')}\" == res\n", "def test_angle_to_string_with_separators():\n    \"\"\"\n    Test the `to_string` method of Angle with various separators, precision, and padding.\n    \"\"\"\n\n    angle = Angle(\"45.123456\", unit=u.degree)\n\n    # Test default separator (fromunit)\n    assert angle.to_string(unit=u.degree) == \"45d07m24.4416s\"\n\n    # Test custom single character separators\n    assert angle.to_string(unit=u.degree, sep=':') == \"45:07:24.4416\"\n    assert angle.to_string(unit=u.degree, sep=' ', precision=3) == \"45 07 24.442\"\n    assert angle.to_string(unit=u.degree, sep='-', precision=2, pad=True) == \"45-07-24.44\"\n\n    # Test custom tuple separators\n    assert angle.to_string(unit=u.degree, sep=(\"d\", \"m\", \"s\"), precision=1) == \"45d07m24.4s\"\n    assert angle.to_string(unit=u.degree, sep=(\"d\", \"m\", \"s\"), precision=0, pad=True) == \"45d07m24s\"\n\n    # Test precision\n    assert angle.to_string(unit=u.degree, sep=':', precision=5) == \"45:07:24.44160\"\n    assert angle.to_string(unit=u.degree, sep=':', precision=8) == \"45:07:24.44160000\"\n\n    # Test alwayssign\n    assert angle.to_string(unit=u.degree, sep=':', alwayssign=True) == \"+45:07:24.4416\"\n\n    # Test pad\n    assert angle.to_string(unit=u.degree, sep=':', pad=True) == \"45:07:24.4416\"\n    assert angle.to_string(unit=u.degree, sep='-', pad=True) == \"45-07-24.4416\"\n\n    # Test different formats\n    assert angle.to_string(unit=u.degree, format='latex') == r\"$45^\\circ07{}^\\prime24.4416{}^{\\prime\\prime}$\"\n    assert angle.to_string(unit=u.degree, format='unicode') == \"45\u00b007\u203224.4416\u2033\"\n\n    # Test invalid format\n    with pytest.raises(ValueError):\n        angle.to_string(unit=u.degree, format='invalidformat')\n", "def test_angle_to_string_with_custom_format():\n    \"\"\"\n    Tests the to_string method with various custom formats\n    \"\"\"\n    a = Angle(45.6789, unit=u.deg)\n\n    # Test with unicode format\n    assert a.to_string(format='unicode') == '45\u00b040\u203244.04\u2033'\n\n    # Test with latex format\n    assert a.to_string(format='latex') == '$45^\\circ40{}^\\prime44.04{}^{\\prime\\prime}$'\n\n    # Test with decimal format\n    assert a.to_string(decimal=True, precision=4) == '45.6789'\n\n    # Test with different separators\n    assert a.to_string(sep=':', unit=u.deg, precision=2) == '45:40:44.04'\n    assert a.to_string(sep='-', unit=u.deg, precision=2) == '45-40-44.04'\n\n    # Test with alwayssign and pad options\n    assert a.to_string(alwayssign=True, pad=True) == '+45d40m44.04s'\n    assert a.to_string(alwayssign=False, pad=True) == '45d40m44.04s'\n    assert a.to_string(alwayssign=True, pad=False) == '+45d40m44.04s'\n\n    # Test with different field options\n    assert a.to_string(fields=1) == '45d'\n    assert a.to_string(fields=2) == '45d40m'\n    assert a.to_string(fields=3) == '45d40m44.04s'\n"], "sample_14": ["def test_angle_to_string_precision():\n    \"\"\"\n    Test the precision parameter of the Angle.to_string() method\n    \"\"\"\n\n    angle = Angle(10.123456789, unit=u.deg)\n\n    # Test various levels of precision\n    assert angle.to_string(precision=0) == '10d07m00s'\n    assert angle.to_string(precision=1) == '10d07m24.4s'\n    assert angle.to_string(precision=2) == '10d07m24.44s'\n    assert angle.to_string(precision=3) == '10d07m24.444s'\n    assert angle.to_string(precision=4) == '10d07m24.4444s'\n    assert angle.to_string(precision=5) == '10d07m24.44444s'\n    assert angle.to_string(precision=6) == '10d07m24.444444s'\n\n    # Test precision for hourangle units\n    angle_hour = Angle(1.23456789, unit=u.hourangle)\n    assert angle_hour.to_string(precision=0) == '1h14m00s'\n    assert angle_hour.to_string(precision=1) == '1h14m04.4s'\n    assert angle_hour.to_string(precision=2) == '1h14m04.44s'\n    assert angle_hour.to_string(precision=3) == '1h14m04.444s'\n    assert angle_hour.to_string(precision=4) == '1h14m04.4444s'\n    assert angle_hour.to_string(precision=5) == '1h14m04.44444s'\n    assert angle_hour.to_string(precision=6) == '1h14m04.444444s'\n\n    # Test precision for radians\n    angle_rad = Angle(np.pi / 4, unit=u.rad)\n    assert angle_rad.to_string(unit=u.rad, precision=0) == '0.785398rad'\n    assert angle_rad.to_string(unit=u.rad, precision=1) == '0.8rad'\n    assert angle_rad.to_string(unit=u.rad, precision=2) == '0.79rad'\n    assert angle_rad.to_string(unit=u.rad, precision=3) == '0.785rad'\n    assert angle_rad.to_string(unit=u.rad, precision=4) == '0.7854rad'\n    assert angle", "def test_angle_arithmetic_operations():\n    \"\"\"\n    Test arithmetic operations involving Angle objects and scalars.\n    This test covers addition, subtraction, multiplication, and division.\n    \"\"\"\n\n    # Addition\n    a1 = Angle(30, u.deg)\n    a2 = Angle(45, u.deg)\n    a3 = a1 + a2\n    assert a3.degree == 75\n\n    # Subtraction\n    a4 = a2 - a1\n    assert a4.degree == 15\n\n    # Multiplication\n    a5 = a1 * 2\n    assert a5.degree == 60\n\n    # Division\n    a6 = a2 / 3\n    assert a6.degree == 15\n\n    # Ensure operations with unit quantities give correct types\n    q1 = a1 * 2 * u.meter\n    assert type(q1) is u.Quantity\n    assert q1.unit == u.deg * u.meter\n    assert_allclose(q1.value, 60)\n\n    q2 = a2 / (2 * u.second)\n    assert type(q2) is u.Quantity\n    assert q2.unit == u.deg / u.second\n    assert_allclose(q2.value, 22.5)\n\n    # Ensure type consistency for scalar multiplication and division\n    assert type(a5) is Angle\n    assert type(a6) is Angle\n\n    # Ensure arithmetic with incompatible units raises errors\n    with pytest.raises(u.UnitTypeError):\n        a1 + 2 * u.meter\n    with pytest.raises(u.UnitTypeError):\n        a2 - 2 * u.second\n\n    # Ensure operations with array angles give correct results\n    a7 = Angle([10, 20, 30], u.deg)\n    a8 = a7 + a1\n    assert_allclose(a8.degree, [40, 50, 60])\n    \n    a9 = a7 * 2\n    assert_allclose(a9.degree, [20, 40, 60])\n\n    a10 = a7 / 2\n    assert_allclose(a10.degree, [5, 10, 15])\n", "def test_longitude_wrap_at():\n    \"\"\"\n    Test Longitude wrapping at different wrap angles.\n    \"\"\"\n    lon = Longitude([10, 190, 370] * u.deg)\n    \n    # Default wrap_angle is 360 degrees\n    assert np.all(lon.wrap_at(360 * u.deg).degree == np.array([10., 190., 10.]))\n\n    # Wrap at 180 degrees\n    assert np.all(lon.wrap_at(180 * u.deg).degree == np.array([10., -170., 10.]))\n\n    # Wrap at 90 degrees\n    assert np.all(lon.wrap_at(90 * u.deg).degree == np.array([10., -170., 10.]))\n\n    # Wrap at 270 degrees\n    assert np.all(lon.wrap_at(270 * u.deg).degree == np.array([10., 190., 370.]))\n\n    # Wrap at 0 degrees\n    assert np.all(lon.wrap_at(0 * u.deg).degree == np.array([10., 190., 10.]))\n\n    # Test with wrap_angle attribute\n    lon.wrap_angle = 180 * u.deg\n    assert np.all(lon.degree == np.array([10., -170., 10.]))\n", "def test_angle_to_string_custom_format():\n    \"\"\"\n    Test the to_string method with a custom format.\n    \"\"\"\n    angle = Angle(45.1234, unit=u.deg)\n    assert angle.to_string(format='unicode') == '45\u00b007\u203224.24\u2033'\n    assert angle.to_string(format='latex') == r'$45^\\circ07{}^\\prime24.24{}^{\\prime\\prime}$'\n    assert angle.to_string(format='latex_inline') == r'$45^\\circ07{}^\\prime24.24{}^{\\prime\\prime}$'\n\n    angle_hour = Angle(1.54321, unit=u.hour)\n    assert angle_hour.to_string(format='unicode') == '1\u02b032\u1d5035.556\u2033'\n    assert angle_hour.to_string(format='latex') == r'$1^{\\mathrm{h}}32^{\\mathrm{m}}35.556^{\\mathrm{s}}$'\n    assert angle_hour.to_string(format='latex_inline') == r'$1^{\\mathrm{h}}32^{\\mathrm{m}}35.556^{\\mathrm{s}}$'\n", "def test_angle_to_string_different_units():\n    \"\"\"\n    Test the `to_string` method for various units to ensure correct string representation.\n    \"\"\"\n    a = Angle(45.123456, unit=u.degree)\n    assert a.to_string(unit=u.degree) == '45d07m24.4416s'\n    assert a.to_string(unit=u.hourangle) == '3h00m29.6096s'\n    assert a.to_string(unit=u.radian) == '0.787552rad'\n\n    a = Angle(1.5, unit=u.hour)\n    assert a.to_string(unit=u.hour) == '1h30m00s'\n    assert a.to_string(unit=u.degree) == '22d30m00s'\n    assert a.to_string(unit=u.radian) == '0.392699rad'\n\n    a = Angle(0.5, unit=u.radian)\n    assert a.to_string(unit=u.radian) == '0.5rad'\n    assert a.to_string(unit=u.degree) == '28d38m20.0000s'\n    assert a.to_string(unit=u.hourangle) == '1h54m33.3333s'\n\n    # Test with different separators and precision\n    a = Angle(45.123456, unit=u.degree)\n    assert a.to_string(unit=u.degree, sep=':', precision=2) == '45:07:24.44'\n    assert a.to_string(unit=u.hourangle, sep=':', precision=1) == '3:00:29.6'\n    assert a.to_string(unit=u.radian, sep='', precision=5) == '0.78755rad'\n"], "sample_16": ["    def test_function_helper(self):\n        @function_helper\n            return x + y\n\n        np_func = FUNCTION_HELPERS.get(np.add)\n        assert np_func is test_func\n\n        result = test_func(5, y=15)\n        self.assertEqual(result, 20)\n", "    def test_logspace(self):\n        start = 1 * u.m\n        stop = 1000 * u.m\n        num = 4\n        out = np.logspace(start, stop, num=num)\n        expected = np.logspace(np.log10(start.value), np.log10(stop.value), num=num) * u.m\n        assert np.allclose(out.value, expected.value)  # Checking values ignoring slight floating point discrepancies\n        assert out.unit == expected.unit\n", "    def test_inv_unit_error(self):\n        q = np.array([[1.0, 2.0], [3.0, 4.0]]) * u.s\n        with pytest.raises(u.UnitsError):\n            np.linalg.inv(q)\n", "    def test_logspace(self):\n        q1 = 1 * u.m\n        q2 = 1000 * u.m\n        base = 10\n        num = 4\n        out = np.logspace(q1, q2, num=num, base=base)\n        expected = np.logspace(1, 1000, num=num, base=base) * u.m\n        assert np.all(out == expected)\n", "    def test_mean_with_axis(self):\n        self.check(np.mean, axis=1)\n"], "sample_17": ["    def test_put(self):\n        q = np.arange(3.0) * u.m\n        np.put(q, [0, 2], [50, 150] * u.cm)\n        expected = np.array([0.5, 1.0, 1.5]) * u.m\n        assert np.all(q == expected)\n        with pytest.raises(u.UnitsError):\n            np.put(q, [0, 2], [1, 2] * u.s)\n", "    def test_add(self):\n        q2 = np.ones_like(self.q) * u.m\n        out = np.add(self.q, q2)\n        expected = np.add(self.q.value, q2.value) * self.q.unit\n        assert np.all(out == expected)\n        ", "    def test_logspace(self):\n        q_start = 1 * u.km / u.s\n        q_stop = 10 * u.km / u.s\n        self.check(np.logspace, q_start, q_stop, 5)\n        self.check(np.logspace, q_start, q_stop, 5, base=2)\n        self.check(np.logspace, q_start, q_stop, 5, dtype=np.float32)\n        # Check that logspace works with LogQuantity\n        q_start_log = 1 * u.dex(u.km / u.s)\n        q_stop_log = 2 * u.dex(u.km / u.s)\n        out = np.logspace(q_start_log, q_stop_log, 5)\n        expected = np.logspace(1, 2, 5) * u.km / u.s\n        assert np.all(out == expected)\n", "    def test_array_function_enabled(self):\n        assert ARRAY_FUNCTION_ENABLED == getattr(np.core.overrides, \"ENABLE_ARRAY_FUNCTION\", True)\n", "    def test_asarray(self):\n        self.check(np.asarray)\n"], "sample_18": ["def test_flat_iterator(self):\n    flat_iter = self.q.flat\n    for i, q in enumerate(flat_iter):\n        assert_no_info(q)\n        assert q == self.q[i]\n    assert len(flat_iter) == len(self.q)\n", "    def test_flat_iterator(self):\n        flat_iter = self.q.flat\n        assert isinstance(flat_iter, u.QuantityIterator)\n        assert_info_equal(flat_iter.base, self.q)\n        for i, q in enumerate(flat_iter):\n            assert_no_info(q)\n            assert q == self.q[i]\n\n        # Test setting values through the flat iterator\n        new_values = [10, 20, 30, 40] * u.m / u.s\n        for i, val in enumerate(new_values):\n            flat_iter[i] = val\n\n        assert np.all(self.q.value == [10, 20, 30, 40])\n        assert_info_equal(self.q, self.q)  # Ensure info is preserved\n", "def test_flatten(self):\n    q_flat = self.q.flatten()\n    assert_info_equal(q_flat, self.q)\n", "    def test_quantity_iterator(self):\n        q_iter = self.q.flat\n        for q in q_iter:\n            assert_no_info(q)\n        assert len(q_iter) == self.q.size\n        q_iter_copy = q_iter.copy()\n        assert np.array_equal(q_iter_copy, self.q.flatten())\n", "    def test_quantity_equality(self):\n        q1 = u.Quantity([1, 2, 3], \"m\")\n        q2 = u.Quantity([1, 2, 3], \"m\")\n        q3 = u.Quantity([100, 200, 300], \"cm\")\n        q4 = u.Quantity([1, 2, 4], \"m\")\n\n        assert q1 == q2\n        assert q1 == q3\n        assert q1 != q4\n"], "sample_19": ["def test_nonstandard_units():\n    \"\"\"\n    Test for non-standard units in WCS headers.\n    \"\"\"\n    header = fits.Header()\n    header[\"CTYPE1\"] = \"RA---TAN\"\n    header[\"CTYPE2\"] = \"DEC--TAN\"\n    header[\"CUNIT1\"] = \"deg\"\n    header[\"CUNIT2\"] = \"deg\"\n    header[\"CRPIX1\"] = 0.5\n    header[\"CRPIX2\"] = 0.5\n    header[\"CRVAL1\"] = 0.0\n    header[\"CRVAL2\"] = 0.0\n    header[\"CD1_1\"] = -7.3055555555555e-05\n    header[\"CD1_2\"] = 0.0\n    header[\"CD2_1\"] = 0.0\n    header[\"CD2_2\"] = 7.3055555555555e-05\n    # Add non-standard units\n    header[\"CUNIT1\"] = \"degrees\"\n    header[\"CUNIT2\"] = \"degrees\"\n\n    with pytest.warns(wcs.FITSFixedWarning, match=\"unitfix\"):\n        w = wcs.WCS(header)\n        assert w.wcs.cunit[0] == \"deg\"\n        assert w.wcs.cunit[1] == \"deg\"\n\n    header[\"CUNIT1\"] = \"radians\"\n    header[\"CUNIT2\"] = \"radians\"\n    with pytest.raises(wcs.InvalidTransformError):\n        wcs.WCS(header)\n", "def test_parse_keysel():\n    \"\"\"\n    Test the _parse_keysel function with different key selections.\n    \"\"\"\n\n    # Test with no keysel provided\n    keysel = None\n    result = _parse_keysel(keysel)\n    assert result == -1, \"Expected -1 for no keysel\"\n\n    # Test with 'image' keysel\n    keysel = [\"image\"]\n    result = _parse_keysel(keysel)\n    assert result == _wcs.WCSHDR_IMGHEAD, \"Expected flag for 'image'\"\n\n    # Test with 'binary' keysel\n    keysel = [\"binary\"]\n    result = _parse_keysel(keysel)\n    assert result == _wcs.WCSHDR_BIMGARR, \"Expected flag for 'binary'\"\n\n    # Test with 'pixel' keysel\n    keysel = [\"pixel\"]\n    result = _parse_keysel(keysel)\n    assert result == _wcs.WCSHDR_PIXLIST, \"Expected flag for 'pixel'\"\n\n    # Test with multiple keysel values\n    keysel = [\"image\", \"binary\"]\n    result = _parse_keysel(keysel)\n    expected = _wcs.WCSHDR_IMGHEAD | _wcs.WCSHDR_BIMGARR\n    assert result == expected, \"Expected combined flags for 'image' and 'binary'\"\n\n    # Test with invalid keysel value\n    keysel = [\"invalid\"]\n    with pytest.raises(ValueError, match=\"keysel must be a list of 'image', 'binary' and/or 'pixel'\"):\n        _parse_keysel(keysel)\n", "def test_calc_footprint_with_header():\n    \"\"\"\n    Test calc_footprint with header to ensure it calculates correctly.\n    \"\"\"\n    header = get_pkg_data_contents(\"data/sip.fits\", encoding=\"binary\")\n    with pytest.warns(wcs.FITSFixedWarning):\n        w = wcs.WCS(header)\n\n    ref = np.array(\n        [\n            [202.39314493, 47.17753352],\n            [202.71885939, 46.94630488],\n            [202.94631893, 47.15855022],\n            [202.72053428, 47.37893142],\n        ]\n    )\n    footprint = w.calc_footprint()\n    assert_allclose(footprint, ref)\n", "def test_sliced_wcs():\n    \"\"\"\n    Test slicing functionality for WCS object.\n    \"\"\"\n    header = get_pkg_data_contents(\"data/sip.fits\", encoding=\"binary\")\n    with pytest.warns(wcs.FITSFixedWarning):\n        w = wcs.WCS(header)\n\n    # Define a slice\n    slice_obj = (slice(None, None, 2), slice(None, None, 2))\n\n    # Apply slice to WCS\n    w_sliced = w.slice(slice_obj)\n\n    # Ensure the sliced WCS has the same dimensionality\n    assert w_sliced.naxis == w.naxis\n\n    # Ensure CRPIX values are updated correctly\n    assert_allclose(\n        w_sliced.wcs.crpix, [crpix / 2 for crpix in w.wcs.crpix]\n    )\n\n    # Ensure CDELT values are updated correctly\n    assert_allclose(\n        w_sliced.wcs.cdelt, [cdelt * 2 for cdelt in w.wcs.cdelt]\n    )\n\n    # Ensure pixel shapes are updated correctly\n    assert w_sliced.pixel_shape == tuple(np.ceil(np.array(w.pixel_shape) / 2).astype(int))\n\n    # Test round-tripping of slicing\n    test_pix = np.array([[0, 0], [100, 100], [200, 200]])\n    world_coords = w.all_pix2world(test_pix, 1)\n    new_pix = w_sliced.all_world2pix(world_coords, 1)\n    assert np.allclose(new_pix * 2, test_pix, atol=1)\n\n    # Test for no changes to celestial axes\n    assert w_sliced.wcs.ctype[:2] == w.wcs.ctype[:2]\n", "def test_wcs_with_invalid_values():\n    \"\"\"\n    Test WCS transformation with invalid values to ensure proper handling.\n    \"\"\"\n    header = {\n        \"CTYPE1\": \"RA---TAN\",\n        \"CTYPE2\": \"DEC--TAN\",\n        \"CRPIX1\": 0,\n        \"CRPIX2\": 0,\n        \"CRVAL1\": 0,\n        \"CRVAL2\": 0,\n        \"CDELT1\": 1,\n        \"CDELT2\": 1,\n    }\n    w = wcs.WCS(header)\n\n    # Test with NaN values\n    result = w.wcs_pix2world([[np.nan, np.nan]], 1)\n    assert np.isnan(result).all()\n\n    # Test with Inf values\n    result = w.wcs_pix2world([[np.inf, np.inf]], 1)\n    assert np.isnan(result).all()\n\n    # Test with mixed NaN and Inf values\n    result = w.wcs_pix2world([[np.nan, np.inf]], 1)\n    assert np.isnan(result).all()\n\n    # Test with very large values\n    result = w.wcs_pix2world([[1e20, 1e20]], 1)\n    assert np.isnan(result).all()\n"], "sample_20": ["def test_mixin_column_with_meta(tmp_path):\n    \"\"\"\n    Test to ensure that mixin columns with metadata are properly written to and read from a FITS file.\n    This includes ensuring that complex metadata structures are preserved.\n    \"\"\"\n    filename = tmp_path / \"test_mixin_meta.fits\"\n    complex_meta = {'key1': [1, 2, 3], 'key2': {'subkey': 'value'}, 'key3': 'string'}\n\n    t1 = QTable()\n    t1['col1'] = [1, 2, 3] * u.m\n    t1['col1'].info.meta = complex_meta\n    t1.write(filename, overwrite=True)\n\n    t2 = QTable.read(filename)\n    assert equal_data(t1, t2)\n    assert t1['col1'].info.meta == t2['col1'].info.meta\n", "def test_fits_time_column(tmp_path):\n    \"\"\"Test that FITS Time columns are correctly serialized and deserialized.\"\"\"\n    filename = tmp_path / \"test_fits_time_column.fits\"\n    \n    t1 = Table()\n    t1[\"time\"] = Time([\"2000-01-01T00:00:00\", \"2001-01-01T00:00:00\"], format='isot', scale='utc')\n\n    t1.write(filename, overwrite=True)\n    t2 = Table.read(filename, astropy_native=True)\n    \n    assert t1.colnames == t2.colnames\n    assert t1[\"time\"].format == t2[\"time\"].format\n    assert t1[\"time\"].scale == t2[\"time\"].scale\n    assert np.all(t1[\"time\"] == t2[\"time\"])\n", "def test_read_table_fits_memmap_mask_invalid(tmp_path):\n    \"\"\"\n    Test read_table_fits function with memmap=True and mask_invalid=True.\n    \"\"\"\n    filename = tmp_path / \"test_memmap_mask_invalid.fits\"\n    data = np.array(\n        list(zip([1, 2, 3, 4], [\"a\", \"b\", \"c\", \"d\"], [2.3, 4.5, 6.7, 8.9])),\n        dtype=[(\"a\", int), (\"b\", \"U1\"), (\"c\", float)],\n    )\n    t1 = Table(data)\n    t1.write(filename, overwrite=True)\n\n    # Read the table with memmap=True and mask_invalid=True\n    t2 = Table.read(filename, memmap=True, mask_invalid=True)\n\n    # Ensure the data is read correctly\n    assert equal_data(t1, t2)\n\n    # Ensure that mask is None since memmap=True should deactivate masking\n    assert t2.mask is None\n\n    # Clean up to avoid issues with open files\n    del t1, t2\n    gc.collect()\n", "def test_is_column_keyword():\n    assert is_column_keyword(\"TTYPE1\")\n    assert is_column_keyword(\"TFORM2\")\n    assert not is_column_keyword(\"NAXIS\")\n    assert not is_column_keyword(\"EXTEND\")\n", "def test_read_table_fits_with_hdu(tmp_path):\n    filename = tmp_path / \"test_read_table_fits_with_hdu.fits\"\n    primary_hdu = PrimaryHDU()\n    bin_table_hdu = BinTableHDU(self.data, name=\"first\")\n    hdulist = HDUList([primary_hdu, bin_table_hdu])\n    hdulist.writeto(filename)\n    \n    # Read table with explicit HDU specified\n    t = Table.read(filename, hdu=\"first\")\n    assert equal_data(t, self.data)\n    \n    # Read table without specifying HDU, should raise a warning and read first table\n    with pytest.warns(\n        AstropyUserWarning,\n        match=r\"hdu= was not specified but multiple tables \"\n        r\"are present, reading in first available \"\n        r\"table \\(hdu=1\\)\",\n    ):\n        t2 = Table.read(filename)\n    assert equal_data(t2, self.data)\n"], "sample_21": ["def test_interpret_err_lines():\n    # Test with no error specifications\n    col_in = ['MJD', 'Rate']\n    cols = _interpret_err_lines(None, 2, names=col_in)\n    assert cols == ['MJD', 'Rate']\n\n    # Test with symmetric and two-sided error specifications\n    err_specs = {'serr': [2], 'terr': [1]}\n    ncols = 5\n    cols = _interpret_err_lines(err_specs, ncols, names=col_in)\n    assert cols == ['MJD', 'MJD_perr', 'MJD_nerr', 'Rate', 'Rate_err']\n\n    # Test inconsistent number of input colnames\n    with pytest.raises(ValueError, match=\"Inconsistent number of input colnames\"):\n        _interpret_err_lines(err_specs, 6, names=col_in)\n\n    # Test without specifying names\n    cols = _interpret_err_lines(err_specs, ncols, names=None)\n    assert cols == ['col1', 'col1_perr', 'col1_nerr', 'col2', 'col2_err']\n", "def test_interpret_err_lines():\n    # Testing when err_specs is None\n    col_in = ['MJD', 'Rate']\n    cols = _interpret_err_lines(None, 2, names=col_in)\n    assert cols == ['MJD', 'Rate']\n\n    # Testing with symmetric and two-sided errors\n    err_specs = {'terr': [1], 'serr': [2]}\n    cols = _interpret_err_lines(err_specs, 5, names=col_in)\n    expected_cols = ['MJD', 'MJD_perr', 'MJD_nerr', 'Rate', 'Rate_err']\n    assert cols == expected_cols\n\n    # Testing inconsistent number of input colnames\n    with pytest.raises(ValueError, match=\"Inconsistent number of input colnames\"):\n        _interpret_err_lines(err_specs, 6, names=col_in)\n\n    # Testing with no names\n    cols = _interpret_err_lines(err_specs, 5)\n    expected_cols = ['col1', 'col1_perr', 'col1_nerr', 'col2', 'col2_err']\n    assert cols == expected_cols\n", "def test_interpret_err_lines():\n    # Case with no error specifications\n    colnames = _interpret_err_lines(None, 4, names=[\"a\", \"b\", \"c\", \"d\"])\n    assert colnames == [\"a\", \"b\", \"c\", \"d\"]\n\n    # Case with symmetric and two-sided errors\n    err_specs = {'serr': [2], 'terr': [1]}\n    colnames = _interpret_err_lines(err_specs, 5, names=[\"a\", \"b\", \"c\"])\n    assert colnames == [\"a\", \"a_perr\", \"a_nerr\", \"b\", \"b_err\"]\n\n    # Case with inconsistent column names\n    with pytest.raises(ValueError, match=\"Inconsistent number of input colnames\"):\n        _interpret_err_lines(err_specs, 6, names=[\"a\", \"b\", \"c\"])\n\n    # Case with default column names\n    colnames = _interpret_err_lines(err_specs, 5)\n    assert colnames == [\"col1\", \"col1_perr\", \"col1_nerr\", \"col2\", \"col2_err\"]\n", "def test_interpret_err_lines():\n    # Test with no error specifications\n    col_in = ['MJD', 'Rate']\n    cols = _interpret_err_lines(None, 2, names=col_in)\n    assert cols[0] == 'MJD'\n    assert cols[1] == 'Rate'\n\n    # Test with symmetric and two-sided error specifications\n    err_specs = {'terr': [1], 'serr': [2]}\n    ncols = 5\n    cols = _interpret_err_lines(err_specs, ncols, names=col_in)\n    assert cols[0] == 'MJD'\n    assert cols[2] == 'MJD_nerr'\n    assert cols[4] == 'Rate_err'\n    \n    # Test error handling for inconsistent number of columns\n    with pytest.raises(ValueError, match=\"Inconsistent number of input colnames\"):\n        _interpret_err_lines(err_specs, 6, names=col_in)\n", "def test_interpret_err_lines():\n    # Test without err_specs and names\n    err_specs = None\n    ncols = 2\n    colnames = _interpret_err_lines(err_specs, ncols)\n    assert colnames == [\"col1\", \"col2\"]\n\n    # Test with only serr columns\n    err_specs = {\"serr\": [1]}\n    ncols = 3\n    colnames = _interpret_err_lines(err_specs, ncols)\n    assert colnames == [\"col1\", \"col1_err\", \"col2\"]\n\n    # Test with only terr columns\n    err_specs = {\"terr\": [1]}\n    ncols = 4\n    colnames = _interpret_err_lines(err_specs, ncols)\n    assert colnames == [\"col1\", \"col1_perr\", \"col1_nerr\", \"col2\"]\n\n    # Test with both serr and terr columns\n    err_specs = {\"serr\": [3], \"terr\": [1]}\n    ncols = 6\n    colnames = _interpret_err_lines(err_specs, ncols)\n    assert colnames == [\n        \"col1\",\n        \"col1_perr\",\n        \"col1_nerr\",\n        \"col2\",\n        \"col3\",\n        \"col3_err\",\n    ]\n\n    # Test with names parameter\n    names = [\"MJD\", \"Rate\"]\n    err_specs = {\"serr\": [2], \"terr\": [1]}\n    ncols = 5\n    colnames = _interpret_err_lines(err_specs, ncols, names=names)\n    assert colnames == [\n        \"MJD\",\n        \"MJD_perr\",\n        \"MJD_nerr\",\n        \"Rate\",\n        \"Rate_err\",\n    ]\n\n    # Test with inconsistent number of input colnames\n    with pytest.raises(ValueError, match=\"Inconsistent number of input colnames\"):\n        _interpret_err_lines(err_specs, 6, names=names)\n"], "sample_22": ["def test_matrix_transpose():\n    m1 = rotation_matrix(45 * u.deg, \"x\")\n    m1_transposed = matrix_transpose(m1)\n    assert_allclose(m1_transposed, m1.swapaxes(-2, -1))\n\n    m2 = rotation_matrix(90 * u.deg, \"y\")\n    m2_transposed = matrix_transpose(m2)\n    assert_allclose(m2_transposed, m2.swapaxes(-2, -1))\n\n    # Test with stack of matrices\n    matrices = np.stack((m1, m2))\n    matrices_transposed = matrix_transpose(matrices)\n    assert_allclose(matrices_transposed, matrices.swapaxes(-2, -1))\n", "def test_matrix_transpose():\n    # Test single matrix\n    matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    transposed_matrix = matrix_transpose(matrix)\n    expected_transpose = np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])\n    assert_array_equal(transposed_matrix, expected_transpose)\n\n    # Test stack of matrices\n    matrices = np.stack([matrix, 2 * matrix])\n    transposed_matrices = matrix_transpose(matrices)\n    expected_transpose_stack = np.stack([expected_transpose, 2 * expected_transpose])\n    assert_array_equal(transposed_matrices, expected_transpose_stack)\n\n    # Test identity matrix\n    identity_matrix = np.eye(3)\n    transposed_identity = matrix_transpose(identity_matrix)\n    assert_array_equal(transposed_identity, identity_matrix)\n\n    # Test single element matrix\n    single_element_matrix = np.array([[1]])\n    transposed_single_element = matrix_transpose(single_element_matrix)\n    assert_array_equal(transposed_single_element, single_element_matrix)\n", "def test_matrix_transpose():\n    m1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    m1_transposed = np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])\n    assert_array_equal(matrix_transpose(m1), m1_transposed)\n\n    m2 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    m2_transposed = np.array([[[1, 3], [2, 4]], [[5, 7], [6, 8]]])\n    assert_array_equal(matrix_transpose(m2), m2_transposed)\n\n    m3 = np.random.rand(3, 4, 5)\n    m3_transposed = np.swapaxes(m3, -2, -1)\n    assert_array_equal(matrix_transpose(m3), m3_transposed)\n", "def test_matrix_transpose():\n    m1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    expected_transpose_m1 = np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])\n    assert_array_equal(matrix_transpose(m1), expected_transpose_m1)\n\n    m2 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    expected_transpose_m2 = np.array([[[1, 3], [2, 4]], [[5, 7], [6, 8]]])\n    assert_array_equal(matrix_transpose(m2), expected_transpose_m2)\n\n    m3 = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n    expected_transpose_m3 = np.array([[[1, 4], [2, 5], [3, 6]], [[7, 10], [8, 11], [9, 12]]])\n    assert_array_equal(matrix_transpose(m3), expected_transpose_m3)\n", "def test_matrix_transpose():\n    # Test single matrix transpose\n    m = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    expected_transpose = np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])\n    assert_array_equal(matrix_transpose(m), expected_transpose)\n\n    # Test stack of matrices transpose\n    stack_matrices = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    expected_stack_transpose = np.array([[[1, 3], [2, 4]], [[5, 7], [6, 8]]])\n    assert_array_equal(matrix_transpose(stack_matrices), expected_stack_transpose)\n\n    # Test identity matrix transpose\n    identity_matrix = np.eye(3)\n    assert_array_equal(matrix_transpose(identity_matrix), identity_matrix)\n\n    # Test higher dimensional array transpose\n    high_dim_matrix = np.random.rand(2, 3, 4, 4)\n    transposed_high_dim = matrix_transpose(high_dim_matrix)\n    assert transposed_high_dim.shape == (2, 3, 4, 4)\n    assert_array_equal(transposed_high_dim[0, 0], high_dim_matrix[0, 0].swapaxes(-2, -1))\n"], "sample_23": ["def test_angle_initialization_with_array_of_strings():\n    \"\"\"Test Angle initialization with an array of strings representing angles.\"\"\"\n    angles = [\"45.0d\", \"90.0d\", \"135.0d\", \"180.0d\"]\n    angle_obj = Angle(angles)\n    expected_values = [45.0, 90.0, 135.0, 180.0]\n\n    assert_array_equal(angle_obj.degree, expected_values)\n    assert angle_obj.unit == u.degree\n\n    hour_angles = [\"1h\", \"2h\", \"3h\", \"4h\"]\n    hour_angle_obj = Angle(hour_angles)\n    expected_hour_values = [15.0, 30.0, 45.0, 60.0]  # 1h = 15\u00b0, 2h = 30\u00b0, etc.\n\n    assert_array_equal(hour_angle_obj.degree, expected_hour_values)\n    assert hour_angle_obj.unit == u.hourangle\n", "def test_latitude_limits_degrees():\n    \"\"\"\n    Test that the validation of the Latitude value range in degrees works\n    correctly.\n    \"\"\"\n    # Valid latitudes\n    valid_latitudes = [90, -90, 45, -45, 0]\n    for val in valid_latitudes:\n        lat = Latitude(val, u.deg)\n        assert lat.value == val\n        assert lat.unit == u.deg\n\n    # Invalid latitudes\n    invalid_latitudes = [90.0001, -90.0001, 180, -180]\n    for val in invalid_latitudes:\n        with pytest.raises(ValueError, match=r\"Latitude angle\\(s\\) must be within.*\"):\n            Latitude(val, u.deg)\n", "def test_angle_wrap_at_edge_cases():\n    \"\"\"\n    Test the wrap_at method for edge cases like angles exactly at wrap boundary,\n    and large angles which need multiple wraps.\n    \"\"\"\n    angles = Angle([360, 720, -360, -720] * u.deg)\n    wrapped_angles = angles.wrap_at(360 * u.deg)\n    assert np.all(wrapped_angles.degree == np.array([0.0, 0.0, 0.0, 0.0]))\n\n    wrapped_angles = angles.wrap_at(180 * u.deg)\n    assert np.all(wrapped_angles.degree == np.array([0.0, 0.0, 0.0, 0.0]))\n\n    angles = Angle([180, -180, 540, -540] * u.deg)\n    wrapped_angles = angles.wrap_at(180 * u.deg)\n    assert np.all(wrapped_angles.degree == np.array([180.0, -180.0, -180.0, 180.0]))\n\n    angles = Angle([450, -450, 810, -810] * u.deg)\n    wrapped_angles = angles.wrap_at(360 * u.deg)\n    assert np.all(wrapped_angles.degree == np.array([90.0, 270.0, 90.0, 270.0]))\n", "def test_angle_wrap_at_with_negative_values():\n    \"\"\"\n    Tests the wrap_at method with negative wrap angles.\n    Ensures that the angles are properly wrapped within the specified range.\n    \"\"\"\n\n    # Test with negative wrap angle -180 degrees\n    a = Angle([-20, 150, 350, 360, -370] * u.deg)\n    wrapped_a = a.wrap_at(\"-180d\")\n    expected_values = np.array([-20.0, 150.0, -10.0, 0.0, -10.0])\n    assert np.allclose(wrapped_a.degree, expected_values)\n\n    # Test with negative wrap angle -90 degrees\n    wrapped_a = a.wrap_at(\"-90d\")\n    expected_values = np.array([340.0, 60.0, 260.0, 270.0, 260.0])\n    assert np.allclose(wrapped_a.degree, expected_values)\n\n    # Test with negative wrap angle -360 degrees\n    wrapped_a = a.wrap_at(\"-360d\")\n    expected_values = np.array([-380.0, -210.0, -10.0, 0.0, -370.0])\n    assert np.allclose(wrapped_a.degree, expected_values)\n", "def test_angle_copy():\n    \"\"\"\n    Test copying of Angle objects and ensuring attributes are preserved.\n    \"\"\"\n    a1 = Angle(\"54.12412d\")\n    a2 = a1.copy()\n    assert a1 == a2\n    assert a1.unit == a2.unit\n    assert a1.dtype == a2.dtype\n    assert a1.value == a2.value\n    assert type(a1) is type(a2)\n    \n    # Modify the copy and check original remains unchanged\n    a2 += Angle(\"10d\")\n    assert a1 != a2\n    assert a1.value != a2.value\n    assert a1.unit == a2.unit\n"], "sample_24": ["    def setup_class(self):\n        self.a = np.array([0.0, 1.0, 0.0, -1.0])\n        self.mask_a = np.array([False, True, False, True])\n        self.ma = Masked(self.a, mask=self.mask_a)\n", "    def test_logspace(self):\n        start = np.log10(1)\n        stop = np.log10(100)\n        num = 5\n        result = np.logspace(start, stop, num)\n        expected = np.logspace(start, stop, num)\n        assert_array_equal(result, expected)\n", "    def test_fft(self):\n        self.check(np.fft.fft)\n", "    def setup_class(self):\n        self.a = np.array([[1.0, 2.0], [3.0, 4.0]])\n        self.mask_a = np.array([[False, True], [False, False]])\n        self.ma = Masked(self.a, mask=self.mask_a)\n", "    def test_fft(self):\n        self.check(np.fft.fft)\n"], "sample_25": ["def test_card_format_image():\n    \"\"\"Test that the card image is correctly formatted for various types of cards.\"\"\"\n\n    # Test a simple keyword with an integer value\n    c = fits.Card(\"SIMPLE\", True)\n    assert c.image == \"SIMPLE  =                    T                                                   \"\n\n    # Test a keyword with a string value\n    c = fits.Card(\"OBJECT\", \"M31\")\n    assert c.image == \"OBJECT  = 'M31     '                                                             \"\n\n    # Test a keyword with a float value\n    c = fits.Card(\"EXPOSURE\", 1500.0)\n    assert c.image == \"EXPOSURE=                1500.0                                                  \"\n\n    # Test a keyword with a complex value\n    c = fits.Card(\"COMPLEX\", 1.5 + 2.5j)\n    assert c.image == \"COMPLEX =           (1.5, 2.5)                                                   \"\n\n    # Test a keyword with a long string value that requires CONTINUE cards\n    long_string = \"This is a very long string value that will require the use of CONTINUE cards to be fully stored in the FITS header.\"\n    c = fits.Card(\"LONGSTR\", long_string)\n    expected_image = (\n        \"LONGSTR = 'This is a very long string value that will require the use of CONTINUE &'            \"\n        \"CONTINUE  'cards to be fully stored in the FITS header.'                                        \"\n    )\n    assert c.image == expected_image\n\n    # Test a keyword with a comment\n    c = fits.Card(\"COMMENT\", \"This is a comment\")\n    assert c.image == \"COMMENT This is a comment                                                        \"\n\n    # Test a keyword with a blank value and a comment\n    c = fits.Card(\"\", \"       / EXPOSURE INFORMATION\")\n    assert c.image == \"               / EXPOSURE INFORMATION                                           \"\n\n    # Test a keyword with an undefined value\n    c = fits.Card(\"UNDEF\", fits.card.UNDEFINED)\n    assert c.image == \"UNDEF   =                                                                      \"\n\n    # Test HIERARCH keyword\n    c = fits.Card(\"HIERARCH ESO DET CHIP PXSPACE\", 5e6)\n    assert c.image == \"HIERARCH ESO DET CHIP PXSPACE = 5.0E+06                                          \"\n", "def test_card_invalid_keyword_setter(self):\n    \"\"\"\n    Test the keyword setter of the Card class with various invalid inputs.\n    This helps ensure that inappropriate keyword values are correctly\n    rejected with appropriate exceptions raised.\n    \"\"\"\n    # Initialize a Card with a valid keyword\n    c = fits.Card(\"VALIDKW\", 123)\n\n    # Try setting a keyword to an invalid value (not a string)\n    with pytest.raises(ValueError, match=\"Keyword name '123' is not a string.\"):\n        c.keyword = 123\n\n    # Try setting a keyword to a string with illegal characters\n    with pytest.raises(ValueError, match=\"Illegal keyword name: 'INVALID*KW'.\"):\n        c.keyword = \"INVALID*KW\"\n\n    # Try setting a keyword to a string with only spaces\n    with pytest.raises(ValueError, match=\"Illegal keyword name: '        '.\"):\n        c.keyword = \"        \"\n\n    # Try setting a keyword to the reserved keyword 'END'\n    with pytest.raises(ValueError, match=\"Keyword 'END' not allowed.\"):\n        c.keyword = \"END\"\n\n    # Try setting a keyword that is too long without using 'HIERARCH'\n    with pytest.warns(AstropyUserWarning, match=\"Keyword name 'TOOLONGKEYWORD' is greater than 8 characters\"):\n        c.keyword = \"TOOLONGKEYWORD\"\n\n    # Try setting a keyword that requires HIERARCH and ensure it sets correctly\n    c.keyword = \"HIERARCH TOOLONGKEYWORD\"\n    assert c.keyword == \"TOOLONGKEYWORD\"\n", "def test_keyword_with_special_characters():\n    \"\"\"\n    Test that Card correctly handles keywords with special characters,\n    and that these characters trigger the creation of HIERARCH cards.\n    \"\"\"\n\n    # Special characters not allowed in standard FITS keywords\n    special_chars = [\"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"(\", \")\", \"+\", \"=\", \"[\", \"]\", \"{\", \"}\", \"|\", \"\\\\\", \":\", \";\", \"'\", '\"', \"<\", \">\", \",\", \".\", \"?\", \"/\", \"~\", \"`\"]\n\n    for char in special_chars:\n        keyword = f\"KEY{char}WORD\"\n        with pytest.warns(AstropyUserWarning, match=\"a HIERARCH card will be created\"):\n            c = fits.Card(keyword, \"value\")\n        assert str(c).startswith(\"HIERARCH\")\n        assert c.keyword == keyword\n        assert c.value == \"value\"\n", "def test_card_invalid_keyword_assignment():\n    \"\"\"\n    Test that assigning an invalid keyword value raises the appropriate error and that\n    keywords with invalid characters are handled correctly.\n    \"\"\"\n\n    # Attempt to assign a keyword with invalid characters\n    c = fits.Card()\n    with pytest.raises(ValueError, match=\"Illegal keyword name\"):\n        c.keyword = \"123INVALID\"\n\n    with pytest.raises(ValueError, match=\"Illegal keyword name\"):\n        c.keyword = \"abc+\"\n\n    # Check that a HIERARCH card is created for illegal characters in keyword\n    with pytest.warns(AstropyUserWarning) as w:\n        c = fits.Card(\"abc+def\", 42)\n    assert len(w) == 1\n    assert c.keyword == \"abc+def\"\n    assert str(c).startswith(\"HIERARCH abc+def\")\n\n    # Ensure that modifying the card image retains the correct keyword handling\n    c.value = 43\n    assert c.keyword == \"abc+def\"\n    assert str(c).startswith(\"HIERARCH abc+def =                   43\")\n", "def test_card_value_with_leading_zeros():\n    \"\"\"Test that Card can handle numbers with leading zeros correctly.\"\"\"\n\n    # Test leading zeros in integer\n    c = fits.Card(\"ZEROINT\", \"000123\")\n    assert c.keyword == \"ZEROINT\"\n    assert c.value == 123  # Should parse as an integer without leading zeros\n\n    # Test leading zeros in float\n    c = fits.Card(\"ZEROFLT\", \"000123.456\")\n    assert c.keyword == \"ZEROFLT\"\n    assert c.value == 123.456  # Should parse as a float without leading zeros\n\n    # Test leading zeros in scientific notation\n    c = fits.Card(\"ZEROSCI\", \"0001.23e02\")\n    assert c.keyword == \"ZEROSCI\"\n    assert c.value == 123.0  # Should parse as a float without leading zeros in scientific notation\n\n    # Test leading zeros in complex number\n    c = fits.Card(\"ZEROCPLX\", \"(0001.23e02, 0004.56e01)\")\n    assert c.keyword == \"ZEROCPLX\"\n    assert c.value == (123.0 + 45.6j)  # Should parse as a complex number with no leading zeros\n\n    # Test invalid leading zero number (should retain the original string as value)\n    c = fits.Card(\"ZEROERR\", \"000123abc\")\n    assert c.keyword == \"ZEROERR\"\n    assert c.value == \"000123abc\"  # Should retain the original string as value\n\n    # Ensure string values with leading zeros are preserved as is\n    c = fits.Card(\"ZEROSTR\", \"000123abc\")\n    assert c.keyword == \"ZEROSTR\"\n    assert c.value == \"000123abc\"\n"], "sample_26": ["    def test_set_data_with_dask_array(self):\n        \"\"\"\n        Test data assignment with dask array - issue #5087 extension\n        \"\"\"\n\n        import dask.array as da\n\n        im = fits.ImageHDU()\n        ar = da.arange(12, chunks=3)\n        im.data = ar\n\n        assert_equal(im.data.compute(), np.arange(12))\n", "    def test_image_hdu_data_assignment(self):\n        \"\"\"Test assignment of different data types to ImageHDU.\"\"\"\n        \n        # Test assigning integer data\n        int_data = np.arange(100, dtype=np.int32).reshape(10, 10)\n        hdu = fits.ImageHDU(data=int_data)\n        assert np.array_equal(hdu.data, int_data)\n        assert hdu.data.dtype == np.int32\n\n        # Test assigning float data\n        float_data = np.random.rand(10, 10).astype(np.float64)\n        hdu.data = float_data\n        assert np.array_equal(hdu.data, float_data)\n        assert hdu.data.dtype == np.float64\n\n        # Test assigning unsigned integer data\n        uint_data = np.arange(100, dtype=np.uint16).reshape(10, 10)\n        hdu.data = uint_data\n        assert np.array_equal(hdu.data, uint_data)\n        assert hdu.data.dtype == np.uint16\n\n        # Test assigning boolean data\n        bool_data = (np.arange(100) % 2 == 0).reshape(10, 10)\n        hdu.data = bool_data\n        assert np.array_equal(hdu.data, bool_data)\n        assert hdu.data.dtype == np.bool_\n\n        # Test assigning complex data\n        complex_data = np.random.rand(10, 10) + 1j * np.random.rand(10, 10)\n        hdu.data = complex_data\n        assert np.array_equal(hdu.data, complex_data)\n        assert hdu.data.dtype == np.complex128\n\n        # Ensure header updates correctly\n        assert hdu.header['BITPIX'] == -64  # For complex128\n        assert hdu.header['NAXIS'] == 2\n        assert hdu.header['NAXIS1'] == 10\n        assert hdu.header['NAXIS2'] == 10\n", "def test_image_hdu_data_assignment(self):\n    \"\"\"\n    Test that data can be assigned to an ImageHDU after initialization and\n    that the header is updated correctly.\n    \"\"\"\n    data = np.arange(100).reshape(10, 10)\n    hdu = fits.ImageHDU()\n    hdu.data = data\n\n    assert np.array_equal(hdu.data, data)\n    assert hdu.header['BITPIX'] == -64  # Since np.arange(100) defaults to int64 which is BITPIX = -64\n    assert hdu.header['NAXIS'] == 2\n    assert hdu.header['NAXIS1'] == 10\n    assert hdu.header['NAXIS2'] == 10\n\n    # Ensure that modifying the data updates the header correctly\n    new_data = np.zeros((20, 20))\n    hdu.data = new_data\n\n    assert np.array_equal(hdu.data, new_data)\n    assert hdu.header['BITPIX'] == -64  # Since np.zeros((20, 20)) defaults to float64 which is BITPIX = -64\n    assert hdu.header['NAXIS'] == 2\n    assert hdu.header['NAXIS1'] == 20\n    assert hdu.header['NAXIS2'] == 20\n", "def test_primary_hdu_attributes():\n    \"\"\"\n    Test that the PrimaryHDU class correctly initializes and sets attributes.\n    \"\"\"\n\n    # Test creating a PrimaryHDU with no arguments\n    hdu = fits.PrimaryHDU()\n    assert hdu.name == \"PRIMARY\"\n    assert \"EXTNAME\" not in hdu.header\n    assert hdu.ver == 1\n    assert \"EXTVER\" not in hdu.header\n\n    # Test creating a PrimaryHDU with data and header\n    data = np.zeros((5, 5))\n    header = fits.Header([(\"BITPIX\", 16), (\"NAXIS\", 2), (\"NAXIS1\", 5), (\"NAXIS2\", 5)])\n    hdu = fits.PrimaryHDU(data=data, header=header)\n    assert np.array_equal(hdu.data, data)\n    assert hdu.header[\"BITPIX\"] == 16\n    assert hdu.header[\"NAXIS\"] == 2\n    assert hdu.header[\"NAXIS1\"] == 5\n    assert hdu.header[\"NAXIS2\"] == 5\n\n    # Test setting the name and version\n    hdu.name = \"TEST\"\n    hdu.ver = 2\n    assert hdu.name == \"TEST\"\n    assert hdu.header[\"EXTNAME\"] == \"TEST\"\n    assert hdu.ver == 2\n    assert hdu.header[\"EXTVER\"] == 2\n\n    # Test modifying the header directly\n    hdu.header[\"BITPIX\"] = 32\n    hdu.header[\"NAXIS\"] = 3\n    hdu.header[\"NAXIS3\"] = 10\n    assert hdu.header[\"BITPIX\"] == 32\n    assert hdu.header[\"NAXIS\"] == 3\n    assert hdu.header[\"NAXIS3\"] == 10\n\n    # Test the shape property\n    assert hdu.shape == (5, 5)\n    hdu.data = np.zeros((10, 10, 10))\n    assert hdu.shape == (10, 10, 10)\n\n    # Test the update_header method updates the header correctly\n    hdu.update_header()\n    assert hdu.header[\"BITPIX\"] == 32\n    assert hdu.header[\"NAXIS\"] == 3\n    assert hdu.header[\"NAXIS1\"] == 10\n    assert hdu.header[\"N", "def test_image_hdu_data_assignment():\n    \"\"\"\n    Test that data assignment to an ImageHDU updates the header\n    correctly and the data is properly assigned.\n    \"\"\"\n    # Create initial data and header\n    initial_data = np.zeros((10, 10), dtype=np.float32)\n    hdr = fits.Header()\n    hdr['BITPIX'] = -32\n    hdr['NAXIS'] = 2\n    hdr['NAXIS1'] = 10\n    hdr['NAXIS2'] = 10\n\n    # Create an ImageHDU with the initial data and header\n    hdu = fits.ImageHDU(data=initial_data, header=hdr)\n\n    # Assign new data to the HDU\n    new_data = np.arange(100, dtype=np.float32).reshape((10, 10))\n    hdu.data = new_data\n\n    # Verify that the new data has been assigned correctly\n    np.testing.assert_array_equal(hdu.data, new_data)\n\n    # Verify that the header has been updated correctly\n    assert hdu.header['BITPIX'] == -32\n    assert hdu.header['NAXIS'] == 2\n    assert hdu.header['NAXIS1'] == 10\n    assert hdu.header['NAXIS2'] == 10\n    assert hdu.header['BSCALE'] == 1\n    assert hdu.header['BZERO'] == 0\n"], "sample_28": ["def test_card_creation_with_special_characters_in_value():\n    \"\"\"Test Card creation with special characters in the value.\"\"\"\n\n    c = fits.Card(\"KEY1\", \"Value with special chars: !@#$%^&*()_+=-[]{}|;:,.<>/?`~\")\n    assert str(c) == _pad(\"KEY1    = 'Value with special chars: !@#$%^&*()_+=-[]{}|;:,.<>/?`~'\")\n    assert c.keyword == \"KEY1\"\n    assert c.value == \"Value with special chars: !@#$%^&*()_+=-[]{}|;:,.<>/?`~\"\n    assert c.comment == \"\"\n\n    c = fits.Card(\"KEY2\", \"Another special value: \\n\\t\\b\\f\\r\")\n    assert str(c) == _pad(\"KEY2    = 'Another special value: \\\\n\\\\t\\\\b\\\\f\\\\r'\")\n    assert c.keyword == \"KEY2\"\n    assert c.value == \"Another special value: \\n\\t\\b\\f\\r\"\n    assert c.comment == \"\"\n\n    with pytest.raises(ValueError, match=\"Illegal value\"):\n        fits.Card(\"KEY3\", \"Non-printable: \\x01\\x02\\x03\")\n", "def test_rvkc_invalid_value_handling():\n    \"\"\"\n    Test handling of RVKC with invalid values to ensure proper exceptions are raised and values are handled correctly.\n    \"\"\"\n\n    # Invalid value in RVKC\n    with pytest.raises(ValueError, match=r\"Illegal value: 'invalid'.\"):\n        fits.Card(\"DP1.NAXIS\", \"invalid\")\n\n    # Invalid value format in RVKC\n    c = fits.Card(\"DP1.NAXIS\", \"AXIS: invalid\")\n    assert c.keyword == \"DP1\"\n    assert c.value == \"AXIS: invalid\"\n    assert c.field_specifier is None\n\n    # Invalid field-specifier in RVKC\n    with pytest.raises(ValueError, match=r\"Illegal value: 'AXIS..: 2'.\"):\n        fits.Card(\"DP1\", \"AXIS..: 2\")\n\n    # Ensure that adding an invalid RVKC to a header raises an exception\n    header = fits.Header()\n    with pytest.raises(ValueError, match=r\"Illegal value: 'AXIS: invalid'.\"):\n        header.set(\"DP1\", \"AXIS: invalid\")\n\n    # Ensure that malformed RVKC cannot be created\n    malformed_rvkc_str = \"DP1     = 'AXIS.1: invalid_value'\"\n    c = fits.Card.fromstring(malformed_rvkc_str)\n    assert c.keyword == \"DP1\"\n    assert c.value == \"AXIS.1: invalid_value\"\n    assert c.field_specifier is None\n", "def test_card_invalid_floating_point_parsing():\n    \"\"\"\n    Test that Cards with invalid floating point values are handled correctly,\n    preserving the invalid format and raising appropriate warnings during verification.\n    \"\"\"\n\n    # Case with a floating point number with a missing exponent sign\n    c = fits.Card.fromstring(\"INVALID = 1.23E+1\")\n    assert c.value == 12.3\n    assert str(c) == _pad(\"INVALID = 1.23E+1\")\n    with pytest.warns(fits.verify.VerifyWarning, match=r\"Verification reported errors\"):\n        assert str(c) == _pad(\"INVALID = 1.23E+01\")\n\n    # Case with a floating point number with a missing mantissa\n    c = fits.Card.fromstring(\"INVALID = +.23E+2\")\n    assert c.value == 23.0\n    assert str(c) == _pad(\"INVALID = +.23E+2\")\n    with pytest.warns(fits.verify.VerifyWarning, match=r\"Verification reported errors\"):\n        assert str(c) == _pad(\"INVALID = +.23E+02\")\n\n    # Case with a floating point number with incorrect spacing\n    c = fits.Card.fromstring(\"INVALID = 1.23 E+3\")\n    assert c.value == 1230.0\n    assert str(c) == _pad(\"INVALID = 1.23 E+3\")\n    with pytest.warns(fits.verify.VerifyWarning, match=r\"Verification reported errors\"):\n        assert str(c) == _pad(\"INVALID = 1.23E+03\")\n", "def test_card_modify_after_fromstring():\n    \"\"\"\n    Test modifying a Card object after creating it from a string using\n    Card.fromstring to ensure proper behavior of setters and image formatting.\n    \"\"\"\n    c = fits.Card.fromstring(\"TESTKEY = 'initial value' / initial comment\")\n    assert c.keyword == \"TESTKEY\"\n    assert c.value == \"initial value\"\n    assert c.comment == \"initial comment\"\n    assert str(c).rstrip() == \"TESTKEY = 'initial value' / initial comment\"\n\n    # Modify the value and comment\n    c.value = \"new value\"\n    c.comment = \"new comment\"\n    assert c.value == \"new value\"\n    assert c.comment == \"new comment\"\n    assert str(c).rstrip() == \"TESTKEY = 'new value' / new comment\"\n\n    # Modify the keyword\n    with pytest.raises(AttributeError):\n        c.keyword = \"NEWKEY\"\n\n    # Ensure that modifications are reflected in the card image\n    assert str(c).rstrip() == \"TESTKEY = 'new value' / new comment\"\n", "def test_card_creation_with_invalid_keyword():\n    \"\"\"\n    Test Card creation with invalid keyword names, ensuring a ValueError is raised.\n    \"\"\"\n\n    # Keyword with special characters which aren't allowed\n    with pytest.raises(ValueError, match=\"Illegal keyword name: 'INVALID@KEY'.\"):\n        fits.Card(\"INVALID@KEY\", \"value\")\n\n    # Keyword with lowercase letters which aren't allowed\n    with pytest.raises(ValueError, match=\"Illegal keyword name: 'invalidkey'.\"):\n        fits.Card(\"invalidkey\", \"value\")\n\n    # Keyword exceeding the standard length of 8 characters without HIERARCH\n    with pytest.raises(ValueError, match=\"Illegal keyword name: 'TOO_LONG_KEYWORD'.\"):\n        fits.Card(\"TOO_LONG_KEYWORD\", \"value\")\n"], "sample_29": ["    def test_write_latex_no_latex_names(self, write, tmp_path):\n        \"\"\"Test writing a LaTeX file without LaTeX formatted parameter names.\"\"\"\n        fp = tmp_path / \"test_write_latex_no_latex_names.tex\"\n        write(fp, format=\"latex\", latex_names=False)\n        tbl = QTable.read(fp)\n        # asserts each column name is in the original parameter names\n        cosmo_cls = type(write.__self__)\n        for column_name in tbl.colnames[2:]:  # skip metadata columns\n            assert column_name in cosmo_cls.__parameters__\n", "    def test_write_latex_custom_parameters(self, write, tmp_path, cosmo):\n        \"\"\"Test writing LaTeX with custom parameter names.\"\"\"\n        custom_format_table = {\n            \"H0\": \"Hubble Constant\",\n            \"Om0\": \"Matter Density\",\n            \"Ode0\": \"Dark Energy Density\",\n            \"Tcmb0\": \"CMB Temperature\",\n            \"Neff\": \"Effective Neutrino Number\",\n            \"m_nu\": \"Neutrino Mass\",\n            \"Ob0\": \"Baryon Density\",\n            \"w0\": \"Equation of State w0\",\n            \"wa\": \"Equation of State wa\",\n            \"wz\": \"Equation of State wz\",\n            \"wp\": \"Equation of State wp\",\n            \"zp\": \"Redshift zp\",\n        }\n        \n        original_format_table = _FORMAT_TABLE.copy()\n        _FORMAT_TABLE.update(custom_format_table)\n        \n        try:\n            fp = tmp_path / \"test_write_latex_custom_parameters.tex\"\n            write(fp, format=\"latex\", latex_names=True)\n            tbl = QTable.read(fp)\n            for column_name in tbl.colnames[2:]:\n                assert column_name in custom_format_table.values()\n        finally:\n            _FORMAT_TABLE.clear()\n            _FORMAT_TABLE.update(original_format_table)\n", "    def test_write_latex_kwargs(self, write, tmp_path):\n        \"\"\"Test that kwargs are properly passed to the table write method.\"\"\"\n        fp = tmp_path / \"test_write_latex_kwargs.tex\"\n        write(fp, format=\"latex\", overwrite=True, units=\"none\")\n        tbl = QTable.read(fp)\n        # Verify that the table was written with the given kwargs.\n        assert tbl.meta['units'] == 'none'\n", "    def test_write_latex_custom_kwargs(self, write, tmp_path):\n        \"\"\"Test writing a LaTeX file with custom kwargs.\"\"\"\n        fp = tmp_path / \"test_write_latex_custom_kwargs.tex\"\n        write(fp, format=\"latex\", overwrite=True, latexdict={'preamble': '\\\\usepackage{amsmath}'})\n        tbl = QTable.read(fp)\n        # Check if the table was written correctly with custom kwargs\n        assert tbl.meta['preamble'] == '\\\\usepackage{amsmath}'\n", "    def test_write_latex_with_kwargs(self, write, tmp_path):\n        \"\"\"Test writing LaTeX with additional kwargs passed to the table writer.\"\"\"\n        fp = tmp_path / \"test_write_latex_with_kwargs.tex\"\n        write(fp, format=\"latex\", latex_names=True, overwrite=True, caption=\"Cosmology Parameters\")\n        tbl = QTable.read(fp)\n        # Check if the table has been written correctly with the additional kwargs\n        assert tbl.meta['caption'] == \"Cosmology Parameters\"\n"], "sample_30": ["def test_check_astroyear():\n    # Test valid astronomical years\n    assert tree.check_astroyear(\"J2000\", \"test_field\")\n    assert tree.check_astroyear(\"B1950\", \"test_field\")\n    assert tree.check_astroyear(\"2000.5\", \"test_field\")\n    \n    # Test invalid astronomical years\n    assert not tree.check_astroyear(\"2000A\", \"test_field\")\n    assert not tree.check_astroyear(\"test\", \"test_field\")\n\n    # Test with None\n    assert tree.check_astroyear(None, \"test_field\")\n", "def test_lookup_by_attr_factory():\n    class TestClass:\n        iterator_data = [\n            {\"ID\": \"1\", \"name\": \"test1\"},\n            {\"ID\": \"2\", \"name\": \"test2\"},\n            {\"ID\": \"3\", \"name\": \"test3\"},\n        ]\n\n            for item in self.iterator_data:\n                yield type(\"Element\", (object,), item)\n\n    test_instance = TestClass()\n\n    # Testing unique attribute lookup\n    lookup_unique = _lookup_by_attr_factory(\"ID\", True, \"iterator\", \"Test Element\", \"Test docstring\")\n    element = lookup_unique(test_instance, \"2\")\n    assert element.ID == \"2\"\n    assert element.name == \"test2\"\n\n    # Testing non-unique attribute lookup\n    lookup_non_unique = _lookup_by_attr_factory(\"name\", False, \"iterator\", \"Test Element\", \"Test docstring\")\n    elements = list(lookup_non_unique(test_instance, \"test1\"))\n    assert len(elements) == 1\n    assert elements[0].ID == \"1\"\n    assert elements[0].name == \"test1\"\n\n    # Testing not found scenario\n    with pytest.raises(KeyError):\n        lookup_unique(test_instance, \"4\")\n", "def test_validate_resource_structure(tmp_path):\n    # Create a VOTable file with resources and nested resources\n    votable = tree.VOTableFile()\n\n    # Create top-level resources\n    resource1 = tree.Resource(ID=\"res1\", name=\"Resource 1\")\n    resource2 = tree.Resource(ID=\"res2\", name=\"Resource 2\")\n\n    # Create nested resources\n    nested_resource1 = tree.Resource(ID=\"nested_res1\", name=\"Nested Resource 1\")\n    nested_resource2 = tree.Resource(ID=\"nested_res2\", name=\"Nested Resource 2\")\n\n    # Create tables for resources\n    table1 = tree.Table(votable, ID=\"table1\", name=\"Table 1\")\n    table2 = tree.Table(votable, ID=\"table2\", name=\"Table 2\")\n    nested_table1 = tree.Table(votable, ID=\"nested_table1\", name=\"Nested Table 1\")\n    nested_table2 = tree.Table(votable, ID=\"nested_table2\", name=\"Nested Table 2\")\n\n    # Add tables to resources\n    resource1.tables.append(table1)\n    resource2.tables.append(table2)\n    nested_resource1.tables.append(nested_table1)\n    nested_resource2.tables.append(nested_table2)\n\n    # Nest resources\n    resource1.resources.append(nested_resource1)\n    resource2.resources.append(nested_resource2)\n\n    # Add top-level resources to votable\n    votable.resources.append(resource1)\n    votable.resources.append(resource2)\n\n    # Write to a file\n    votable.to_xml(str(tmp_path / \"resource_structure.xml\"))\n\n    # Parse the written file\n    parsed_votable = parse(str(tmp_path / \"resource_structure.xml\"))\n\n    # Validate the structure\n    assert len(parsed_votable.resources) == 2\n    assert len(parsed_votable.resources[0].tables) == 1\n    assert len(parsed_votable.resources[1].tables) == 1\n    assert len(parsed_votable.resources[0].resources) == 1\n    assert len(parsed_votable.resources[1].resources) == 1\n    assert len(parsed_votable.resources[0].resources[0].tables) == 1\n    assert len(parsed_votable.resources[1].resources[0].tables) == 1\n\n    assert parsed_votable.resources[0].ID == \"res1\"\n    assert parsed_votable.resources[", "def test_info_element():\n    info = tree.Info(\n        ID=\"test_info\",\n        name=\"test_name\",\n        value=\"test_value\",\n        xtype=\"test_xtype\",\n        ref=\"test_ref\",\n        unit=\"m\",\n        ucd=\"test_ucd\",\n        utype=\"test_utype\",\n        content=\"test_content\",\n    )\n\n    assert info.ID == \"test_info\"\n    assert info.name == \"test_name\"\n    assert info.value == \"test_value\"\n    assert info.xtype == \"test_xtype\"\n    assert info.ref == \"test_ref\"\n    assert info.unit.to_string() == \"m\"\n    assert info.ucd == \"test_ucd\"\n    assert info.utype == \"test_utype\"\n    assert info.content == \"test_content\"\n\n    info_dict = {\n        \"ID\": info.ID,\n        \"name\": info.name,\n        \"value\": info.value,\n        \"xtype\": info.xtype,\n        \"ref\": info.ref,\n        \"unit\": info.unit.to_string(),\n        \"ucd\": info.ucd,\n        \"utype\": info.utype,\n    }\n\n    assert set(info_dict.keys()) == {\"ID\", \"name\", \"value\", \"xtype\", \"ref\", \"unit\", \"ucd\", \"utype\"}\n\n    buf = io.StringIO()\n    info.to_xml(tree.XMLWriter(buf))\n    xml_output = buf.getvalue()\n    assert 'ID=\"test_info\"' in xml_output\n    assert 'name=\"test_name\"' in xml_output\n    assert 'value=\"test_value\"' in xml_output\n    assert 'xtype=\"test_xtype\"' in xml_output\n    assert 'ref=\"test_ref\"' in xml_output\n    assert 'unit=\"m\"' in xml_output\n    assert 'ucd=\"test_ucd\"' in xml_output\n    assert 'utype=\"test_utype\"' in xml_output\n", "def test_parse_votable_with_link():\n    votable = parse(get_pkg_data_filename(\"data/votable_with_link.xml\"))\n    table = votable.get_first_table()\n    link = table.links[0]\n\n    assert isinstance(link, tree.Link)\n    assert link.content_role == \"query\"\n    assert link.content_type == \"application/json\"\n    assert link.href == \"http://example.com/query\"\n    assert link.action == \"submit\"\n"], "sample_31": ["    def test_write_latex_with_kwargs(self, write, tmp_path, format):\n        \"\"\"Test passing additional kwargs to the write function.\"\"\"\n        fp = tmp_path / \"test_write_latex_with_kwargs.tex\"\n        additional_kwargs = {\"caption\": \"Test Caption\", \"latexdict\": {\"preamble\": \"\\\\usepackage{amsmath}\"}}\n        write(fp, format=format, **additional_kwargs)\n        # Read the file back to ensure it was written correctly with kwargs\n        with open(fp, \"r\") as f:\n            content = f.read()\n            assert \"Test Caption\" in content\n            assert \"\\\\usepackage{amsmath}\" in content\n", "    def test_write_latex_no_latex_names(self, write, tmp_path, format, latex_names):\n        \"\"\"Test writing LaTeX file with and without LaTeX names.\"\"\"\n        fp = tmp_path / f\"test_write_latex_no_latex_names_{latex_names}.tex\"\n        write(fp, format=format, latex_names=latex_names)\n        tbl = QTable.read(fp)\n        expected_columns = (\n            _FORMAT_TABLE.values() if latex_names else self.cosmo.__parameters__\n        )\n        for column_name in tbl.colnames[2:]:\n            assert column_name in expected_columns\n", "    def test_write_latex_no_latex_names(self, write, tmp_path, format, latex_names):\n        \"\"\"Test writing a LaTeX file with and without LaTeX names for the parameters\"\"\"\n        fp = tmp_path / f\"test_write_latex_no_latex_names_{latex_names}.tex\"\n        write(fp, format=format, latex_names=latex_names)\n        tbl = QTable.read(fp)\n        if latex_names:\n            # If latex_names is True, check the column names are in LaTeX format\n            for column_name in tbl.colnames[2:]:\n                assert column_name in _FORMAT_TABLE.values()\n        else:\n            # If latex_names is False, check the column names are not in LaTeX format\n            for column_name in tbl.colnames[2:]:\n                assert column_name not in _FORMAT_TABLE.values()\n                assert column_name in _FORMAT_TABLE.keys()\n", "    def test_write_latex_with_kwargs(self, write, tmp_path, format):\n        \"\"\"Test write_latex with additional kwargs\"\"\"\n        fp = tmp_path / \"test_write_latex_with_kwargs.tex\"\n        write(fp, format=format, latex_names=True, overwrite=True, overwrite_kwarg=True)\n        tbl = QTable.read(fp)\n        # Verify the table has been written with the additional kwargs\n        for column_name in tbl.colnames[2:]:\n            assert column_name in _FORMAT_TABLE.values()\n", "    def test_write_latex_no_latex_names(self, write, tmp_path, format, latex_names):\n        \"\"\"Test writing LaTeX with and without LaTeX formatted names\"\"\"\n        fp = tmp_path / \"test_write_latex_no_latex_names.tex\"\n        write(fp, format=format, latex_names=latex_names)\n        tbl = QTable.read(fp)\n        if latex_names:\n            for column_name in tbl.colnames[2:]:\n                assert column_name in _FORMAT_TABLE.values()\n        else:\n            for column_name in tbl.colnames[2:]:\n                assert column_name in _FORMAT_TABLE.keys()\n"], "sample_32": ["def test_de_densityscale_scalar(self, cosmo):\n    \"\"\"Test de_density_scale with scalar input.\"\"\"\n    assert u.allclose(cosmo.de_density_scale(0.5), 1.15234885, rtol=1e-4)\n    assert u.allclose(cosmo.de_density_scale(1.0), 2.40022841, rtol=1e-4)\n", "def test_Tcmb0_initialization(H0, Om0, Ode0, w0, wz, Tcmb0, expected):\n    \"\"\"Test initialization of Tcmb0 parameter.\"\"\"\n    cosmo = w0wzCDM(H0=H0, Om0=Om0, Ode0=Ode0, w0=w0, wz=wz, Tcmb0=Tcmb0)\n    assert cosmo.Tcmb0 == expected\n\n    flatcosmo = Flatw0wzCDM(H0=H0, Om0=Om0, w0=w0, wz=wz, Tcmb0=Tcmb0)\n    assert flatcosmo.Tcmb0 == expected\n", "def test_w():\n    cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=-1.0, wz=0.5)\n    assert u.allclose(cosmo.w(0.0), -1.0)\n    assert u.allclose(cosmo.w(1.0), -0.5)\n    assert u.allclose(cosmo.w(2.0), 0.0)\n    assert u.allclose(cosmo.w([0.0, 0.5, 1.0, 1.5, 2.3]), [-1.0, -0.75, -0.5, -0.25, 0.15])\n\n    cosmo_flat = Flatw0wzCDM(H0=70, Om0=0.3, w0=-1.0, wz=0.5)\n    assert u.allclose(cosmo_flat.w(0.0), -1.0)\n    assert u.allclose(cosmo_flat.w(1.0), -0.5)\n    assert u.allclose(cosmo_flat.w(2.0), 0.0)\n    assert u.allclose(cosmo_flat.w([0.0, 0.5, 1.0, 1.5, 2.3]), [-1.0, -0.75, -0.5, -0.25, 0.15])\n", "def test_w_parameter_validation():\n    \"\"\"Test that the w0 and wz parameters are properly validated.\"\"\"\n    # Create a valid instance first\n    cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=-1.0, wz=0.5)\n\n    # Invalid w0 values\n    with pytest.raises(ValueError, match=\"must be a float\"):\n        w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=\"invalid\", wz=0.5)\n\n    with pytest.raises(ValueError, match=\"must be a float\"):\n        w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=None, wz=0.5)\n\n    # Invalid wz values\n    with pytest.raises(ValueError, match=\"must be a float\"):\n        w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=-1.0, wz=\"invalid\")\n\n    with pytest.raises(ValueError, match=\"must be a float\"):\n        w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=-1.0, wz=None)\n\n    # Valid w0 and wz values with units\n    cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=-1.0 * u.one, wz=0.5 * u.one)\n    assert cosmo.w0 == -1.0\n    assert cosmo.wz == 0.5\n", "def test_w0wzCDM_w():\n    \"\"\"Test the w method of the w0wzCDM class.\"\"\"\n    cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=-1.0, wz=0.5)\n    \n    # Test the w function for various redshift values\n    assert u.allclose(cosmo.w(0), -1.0)\n    assert u.allclose(cosmo.w(0.5), -0.75)\n    assert u.allclose(cosmo.w(1), -0.5)\n    assert u.allclose(cosmo.w(2), 0.0)\n    assert u.allclose(cosmo.w(3), 0.5)\n\n    # Test with an array input\n    z = np.array([0, 0.5, 1, 2, 3])\n    expected_w = np.array([-1.0, -0.75, -0.5, 0.0, 0.5])\n    assert np.allclose(cosmo.w(z), expected_w)\n"], "sample_33": ["def test_indent():\n    # Test for a single line string\n    assert misc.indent(\"Hello, World!\", 1, 4) == \"    Hello, World!\"\n    \n    # Test for a multi-line string\n    input_string = \"Hello,\\nWorld!\"\n    expected_output = \"    Hello,\\n    World!\"\n    assert misc.indent(input_string, 1, 4) == expected_output\n\n    # Test with different shift values\n    input_string = \"Hello,\\nWorld!\"\n    expected_output = \"        Hello,\\n        World!\"\n    assert misc.indent(input_string, 2, 4) == expected_output\n\n    # Test with different width values\n    input_string = \"Hello,\\nWorld!\"\n    expected_output = \"  Hello,\\n  World!\"\n    assert misc.indent(input_string, 1, 2) == expected_output\n\n    # Test preserving the trailing newline\n    input_string = \"Hello,\\nWorld!\\n\"\n    expected_output = \"    Hello,\\n    World!\\n\"\n    assert misc.indent(input_string, 1, 4) == expected_output\n", "def test_silence():\n    from io import StringIO\n\n    # Capture the output\n    captured_output = StringIO()\n    sys.stdout = captured_output\n    sys.stderr = captured_output\n\n    # Write something to stdout and stderr\n    print(\"This should be silenced.\")\n    sys.stderr.write(\"This should also be silenced.\\n\")\n\n    # Use the silence context manager\n    with misc.silence():\n        print(\"This should not appear.\")\n        sys.stderr.write(\"Nor should this.\\n\")\n\n    # Write something to stdout and stderr again\n    print(\"This should be visible.\")\n    sys.stderr.write(\"This should also be visible.\\n\")\n\n    # Restore stdout and stderr\n    sys.stdout = sys.__stdout__\n    sys.stderr = sys.__stderr__\n\n    # Check the captured output\n    output = captured_output.getvalue()\n    assert \"This should be silenced.\" in output\n    assert \"This should also be silenced.\\n\" in output\n    assert \"This should be visible.\" in output\n    assert \"This should also be visible.\\n\" in output\n    assert \"This should not appear.\" not in output\n    assert \"Nor should this.\\n\" not in output\n", "def test_indent():\n    text = \"This is a test.\"\n    expected_indented_text = \"    This is a test.\"\n    assert misc.indent(text) == expected_indented_text\n    \n    text_multiline = \"This is a test.\\nWith multiple lines.\"\n    expected_multiline_indented_text = \"    This is a test.\\n    With multiple lines.\"\n    assert misc.indent(text_multiline) == expected_multiline_indented_text\n    \n    text_multiline_trailing_newline = \"This is a test.\\nWith multiple lines.\\n\"\n    expected_multiline_trailing_newline_indented_text = \"    This is a test.\\n    With multiple lines.\\n\"\n    assert misc.indent(text_multiline_trailing_newline) == expected_multiline_trailing_newline_indented_text\n\n    text_multiline_empty_lines = \"This is a test.\\n\\nWith multiple lines.\"\n    expected_multiline_empty_lines_indented_text = \"    This is a test.\\n\\n    With multiple lines.\"\n    assert misc.indent(text_multiline_empty_lines) == expected_multiline_empty_lines_indented_text\n", "def test_indent():\n    input_text = \"line1\\nline2\\nline3\"\n    expected_output = \"    line1\\n    line2\\n    line3\"\n    assert misc.indent(input_text) == expected_output\n\n    # Test with a different shift\n    expected_output_shift2 = \"        line1\\n        line2\\n        line3\"\n    assert misc.indent(input_text, shift=2) == expected_output_shift2\n\n    # Test with empty line\n    input_text_with_empty = \"line1\\n\\nline3\"\n    expected_output_with_empty = \"    line1\\n\\n    line3\"\n    assert misc.indent(input_text_with_empty) == expected_output_with_empty\n\n    # Test with a trailing newline\n    input_text_with_newline = \"line1\\nline2\\nline3\\n\"\n    expected_output_with_newline = \"    line1\\n    line2\\n    line3\\n\"\n    assert misc.indent(input_text_with_newline) == expected_output_with_newline\n", "def test_indent():\n    text = \"Line1\\nLine2\\nLine3\"\n    expected_indent_1 = \"    Line1\\n    Line2\\n    Line3\"\n    expected_indent_2 = \"        Line1\\n        Line2\\n        Line3\"\n    expected_indent_with_trailing_newline = \"    Line1\\n    Line2\\n    Line3\\n\"\n    \n    assert misc.indent(text) == expected_indent_1\n    assert misc.indent(text, shift=2) == expected_indent_2\n    assert misc.indent(text + '\\n') == expected_indent_with_trailing_newline\n"], "sample_34": ["def test_unit_conversion_with_equivalencies():\n    eq = [(u.J, u.erg, lambda x: x * 1e7, lambda x: x / 1e7)]\n    assert_allclose(u.J.to(u.erg, 1, equivalencies=eq), 1e7)\n    assert_allclose(u.erg.to(u.J, 1e7, equivalencies=eq), 1)\n    assert u.J.is_equivalent(u.erg, equivalencies=eq)\n", "def test_set_enabled_units():\n    from .. import si\n\n    # Test setting enabled units within a context manager\n    with u.set_enabled_units([si]):\n        assert 'm' in u.get_current_unit_registry().registry\n        assert 's' in u.get_current_unit_registry().registry\n        assert 'kg' in u.get_current_unit_registry().registry\n\n    # Ensure units are reset after context\n    assert 'm' not in u.get_current_unit_registry().registry\n    assert 's' not in u.get_current_unit_registry().registry\n    assert 'kg' not in u.get_current_unit_registry().registry\n\n    # Test permanently setting enabled units\n    u.get_current_unit_registry().set_enabled_units(si)\n    assert 'm' in u.get_current_unit_registry().registry\n    assert 's' in u.get_current_unit_registry().registry\n    assert 'kg' in u.get_current_unit_registry().registry\n", "def test_unit_registry_reset():\n    \"\"\"\n    Test the `_reset_units` and `_reset_equivalencies` methods\n    of `_UnitRegistry` to ensure they properly reset the registry.\n    \"\"\"\n    registry = u._UnitRegistry()\n    initial_units = registry.all_units\n    initial_registry = registry.registry\n    initial_non_prefix_units = registry.non_prefix_units\n    initial_by_physical_type = registry._by_physical_type\n\n    # Add a unit and equivalency to the registry\n    registry.add_enabled_units([u.m])\n    registry.add_enabled_equivalencies([(u.m, u.cm, lambda x: x * 100, lambda x: x / 100)])\n\n    assert u.m in registry.all_units\n    assert \"m\" in registry.registry\n\n    # Reset the units and equivalencies\n    registry._reset_units()\n    registry._reset_equivalencies()\n\n    assert registry.all_units == set()\n    assert registry.registry == {}\n    assert registry.non_prefix_units == set()\n    assert registry._by_physical_type == {}\n    assert registry.equivalencies == []\n", "def test_unit_registry_get_units_with_physical_type():\n    \"\"\"\n    Test the get_units_with_physical_type method in _UnitRegistry.\n    \"\"\"\n    meter = u.Unit(\"meter\")\n    second = u.Unit(\"second\")\n    new_registry = u._UnitRegistry(init=[meter, second])\n\n    same_physical_type_units = new_registry.get_units_with_physical_type(meter)\n    assert meter in same_physical_type_units\n    assert second not in same_physical_type_units\n\n    different_physical_type_units = new_registry.get_units_with_physical_type(second)\n    assert second in different_physical_type_units\n    assert meter not in different_physical_type_units\n", "def test_unit_arithmetic():\n    # Test the arithmetic operations with units to ensure consistency\n\n    # Multiplication\n    assert (u.m * u.s).to_string() == \"m s\"\n    assert (u.kg * u.m / u.s ** 2).to_string() == \"kg m / s2\"\n\n    # Division\n    assert (u.m / u.s).to_string() == \"m / s\"\n    assert (u.kg / u.m ** 3).to_string() == \"kg / m3\"\n\n    # Powers\n    assert (u.m ** 2).to_string() == \"m2\"\n    assert (u.s ** -1).to_string() == \"1 / s\"\n    assert (u.kg ** 0.5).to_string() == \"kg(1/2)\"\n\n    # Composite units\n    composite = (u.m ** 2 * u.s ** -2)\n    assert composite.decompose().to_string() == \"m2 / s2\"\n    assert composite.is_equivalent(u.J / u.kg)\n\n    # Negative powers and reciprocals\n    assert (u.m ** -1).to_string() == \"1 / m\"\n    assert (1 / u.s).to_string() == \"1 / s\"\n\n    # Mixed operations\n    mixed = u.kg * (u.m / u.s) ** 2\n    assert mixed.decompose().to_string() == \"kg m2 / s2\"\n    assert mixed.is_equivalent(u.J)\n\n    # Ensure unity works correctly in operations\n    assert (u.one * u.m) == u.m\n    assert (u.m / u.one) == u.m\n\n    # Ensure operations with unrecognized units raise errors\n    with pytest.raises(ValueError):\n        u.m * u.Unit(\"FOO\", parse_strict='silent')\n    with pytest.raises(ValueError):\n        u.m / u.Unit(\"FOO\", parse_strict='silent')\n"], "sample_35": ["def test_resolve_name():\n    # Test resolving a simple module\n    assert resolve_name('os') == __import__('os')\n    \n    # Test resolving a function within a module\n    assert resolve_name('os.path.join') == __import__('os.path').join\n    \n    # Test resolving with additional parts\n    assert resolve_name('os', 'path', 'join') == __import__('os.path').join\n    \n    # Test resolving a non-existent module\n    with pytest.raises(ImportError):\n        resolve_name('non_existent_module')\n    \n    # Test resolving a non-existent function within a module\n    with pytest.raises(ImportError):\n        resolve_name('os.non_existent_function')\n", "def test_resolve_name():\n    import astropy.utils.introspection as introspection_mod\n\n    # Test resolving a function within the same module\n    resolved = introspection.resolve_name('astropy.utils.introspection.resolve_name')\n    assert resolved is introspection_mod.resolve_name\n\n    # Test resolving a function with additional parts\n    resolved = introspection.resolve_name('astropy', 'utils', 'introspection', 'resolve_name')\n    assert resolved is introspection_mod.resolve_name\n\n    # Test resolving a module\n    resolved = introspection.resolve_name('astropy.utils')\n    import astropy.utils\n    assert resolved is astropy.utils\n\n    # Test resolving a non-existent function\n    with pytest.raises(ImportError):\n        introspection.resolve_name('astropy.utils.introspection.nonexistent_function')\n", "def test_resolve_name():\n    import astropy\n    import astropy.utils.introspection as introspection_module\n\n    # Test resolution of a module\n    assert resolve_name('astropy') is astropy\n\n    # Test resolution of a function in the module\n    assert resolve_name('astropy.utils.introspection.resolve_name') is introspection.resolve_name\n\n    # Test resolution with additional parts\n    assert resolve_name('astropy', 'utils', 'introspection', 'resolve_name') is introspection.resolve_name\n\n    # Test resolution of an attribute in the module\n    assert resolve_name('astropy.utils.introspection.__all__') == introspection_module.__all__\n\n    with pytest.raises(ImportError):\n        resolve_name('nonexistent.module')\n", "def test_isinstancemethod():\n    class MyClass:\n            pass\n        \n        @classmethod\n            pass\n        \n        @staticmethod\n            pass\n\n    assert isinstancemethod(MyClass, MyClass.instance_method) is True\n    assert isinstancemethod(MyClass, MyClass.class_method) is False\n    assert isinstancemethod(MyClass, MyClass.static_method) is False\n\n    with pytest.raises(AttributeError):\n        isinstancemethod(MyClass, MyClass.non_existent_method)\n", "def test_resolve_name():\n    # Test resolve_name with a direct function reference\n    result = introspection.resolve_name('astropy.utils.introspection.resolve_name')\n    assert result is introspection.resolve_name\n\n    # Test resolve_name with additional parts\n    result = introspection.resolve_name('astropy', 'utils', 'introspection', 'resolve_name')\n    assert result is introspection.resolve_name\n\n    # Test resolve_name with a class reference\n    result = introspection.resolve_name('collections.namedtuple')\n    assert result is namedtuple\n\n    # Test resolve_name with invalid name\n    with pytest.raises(ImportError):\n        introspection.resolve_name('non.existent.module')\n"], "sample_36": ["def test_biweight_location_with_initial_guess():\n    \"\"\"Test biweight_location with an initial guess for location M.\"\"\"\n    data = np.array([10, 20, 30, 40, 50])\n    M = 25  # initial guess for location\n    cbl = biweight_location(data, M=M)\n    expected = 30.0  # verified with manual calculation\n    assert_allclose(cbl, expected, rtol=1e-2)\n", "def test_biweight_location_with_nan():\n    \"\"\"Test biweight_location with NaN values in the input data.\"\"\"\n    data = np.array([1, 2, np.nan, 4, 5])\n    cbl = biweight_location(data)\n    # Expected result should ignore NaN and compute on [1, 2, 4, 5]\n    assert abs(cbl - 3.0) < 1e-3\n\n    # Test with axis specified\n    data = np.array([[1, 2, np.nan], [4, 5, 6]])\n    cbl = biweight_location(data, axis=1)\n    assert_allclose(cbl, [1.5, 5.0], atol=1e-3)\n", "def test_biweight_midcorrelation_edge_cases():\n    \"\"\"Test biweight_midcorrelation with edge case inputs.\"\"\"\n    x = []\n    y = []\n    with pytest.raises(ValueError) as e:\n        biweight_midcorrelation(x, y)\n    assert 'x and y must have the same shape.' in str(e.value)\n    \n    x = [1]\n    y = [2]\n    with pytest.raises(ValueError) as e:\n        biweight_midcorrelation(x, y)\n    assert 'x and y must have at least 2 elements.' in str(e.value)\n\n    x = [1, 2, 3, 4, 5]\n    y = [2, 3, 4, 5, 6]\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n    \n    x = [1, 2, 3, 4, 5]\n    y = [5, 4, 3, 2, 1]\n    assert_allclose(biweight_midcorrelation(x, y), -1.0)\n", "def test_biweight_scale_axis():\n    \"\"\"Test biweight_scale with the axis keyword.\"\"\"\n    with NumpyRNGContext(12345):\n        ny = 100\n        nx = 200\n        data = normal(5, 2, (ny, nx))\n\n        bs = biweight_scale(data, axis=0)\n        bsi = []\n        for i in range(nx):\n            bsi.append(biweight_scale(data[:, i]))\n        bsi = np.array(bsi)\n        assert_allclose(bs, bsi)\n\n        bs = biweight_scale(data, axis=1)\n        bsi = []\n        for i in range(ny):\n            bsi.append(biweight_scale(data[i, :]))\n        bsi = np.array(bsi)\n        assert_allclose(bs, bsi)\n", "def test_biweight_location_with_different_M():\n    data = [1, 2, 3, 4, 5, 100]\n    M = 3\n    cbl = biweight_location(data, M=M)\n    assert abs(cbl - 3.015) < 1e-3\n\n    M = np.array([3, 3, 3, 3, 3, 3])\n    cbl = biweight_location(data, M=M)\n    assert abs(cbl - 3.015) < 1e-3\n"], "sample_37": ["def test_invalid_transform_error():\n    \"\"\"\n    Test that an InvalidTransformError is raised when an invalid transformation is applied.\n    \"\"\"\n    header = get_pkg_data_contents('data/invalid_transform.hdr', encoding='binary')\n\n    with pytest.raises(wcs.InvalidTransformError):\n        w = wcs.WCS(header)\n        w.wcs_pix2world([100, 200], [100, 200], 0)\n", "def test_wcs_init_with_invalid_header():\n    \"\"\"\n    Test WCS initialization with an invalid header.\n    \"\"\"\n    invalid_header = \"\"\"\n    SIMPLE  =                    T / conforms to FITS standard\n    BITPIX  =                   16 / array data type\n    NAXIS   =                    0 / number of array dimensions\n    EXTEND  =                    T\n    \"\"\"\n    header = fits.Header.fromstring(invalid_header.strip())\n\n    with pytest.raises(ValueError) as exc:\n        w = wcs.WCS(header)\n    assert \"Invalid WCS transformation parameters\" in str(exc.value)\n", "def test_invalid_header():\n    \"\"\"\n    Test for invalid header that should raise an error when creating WCS object.\n    \"\"\"\n    invalid_header = \"\"\"\n    SIMPLE  =                    T / conforms to FITS standard\n    BITPIX  =                    8 / array data type\n    NAXIS   =                    0 / number of array dimensions\n    EXTEND  =                    T / FITS dataset may contain extensions\n    \"\"\"\n    with pytest.raises(ValueError) as exc:\n        w = wcs.WCS(invalid_header)\n    assert \"header must be a string or astropy.io.fits.Header object\" in str(exc.value)\n", "def test_wcs_copy():\n    \"\"\"Test copying WCS objects.\"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.crval = [30, 40]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crpix = [1, 1]\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n\n    w_copy = w.copy()\n    assert w.wcs.compare(w_copy.wcs)\n\n    w_deepcopy = w.deepcopy()\n    assert w.wcs.compare(w_deepcopy.wcs)\n\n    # Modify original and ensure copies don't change\n    w.wcs.crval = [50, 60]\n    assert not w.wcs.compare(w_copy.wcs)\n    assert not w.wcs.compare(w_deepcopy.wcs)\n\n    w_copy.wcs.crval = [70, 80]\n    assert not w_copy.wcs.compare(w_deepcopy.wcs)\n", "def test_wcs_copy():\n    \"\"\"\n    Test the copy and deepcopy methods of the WCS class.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n    \n    # Test shallow copy\n    w_copy = w.copy()\n    assert w.wcs.compare(w_copy.wcs)\n    assert w.sip.a == w_copy.sip.a\n    assert w.sip.b == w_copy.sip.b\n    \n    # Test deep copy\n    w_deepcopy = w.deepcopy()\n    assert w.wcs.compare(w_deepcopy.wcs)\n    assert np.array_equal(w.sip.a, w_deepcopy.sip.a)\n    assert np.array_equal(w.sip.b, w_deepcopy.sip.b)\n    \n    # Modify the original WCS and ensure the deepcopy is unaffected\n    w.sip.a[0, 0] += 1\n    assert not np.array_equal(w.sip.a, w_deepcopy.sip.a)\n"], "sample_38": ["def test_crpix_update():\n    \"\"\"\n    Test that updating CRPIX after WCS initialization correctly updates the WCS transformation.\n    \"\"\"\n\n    header = get_pkg_data_contents(\n        'data/sip.fits', encoding='binary')\n    w = wcs.WCS(header)\n\n    # Original CRPIX values\n    original_crpix1 = w.wcs.crpix[0]\n    original_crpix2 = w.wcs.crpix[1]\n\n    # New CRPIX values\n    new_crpix1 = original_crpix1 + 100\n    new_crpix2 = original_crpix2 + 200\n\n    # Update CRPIX values\n    w.wcs.crpix = [new_crpix1, new_crpix2]\n\n    # Check if the CRPIX values are updated\n    assert w.wcs.crpix[0] == new_crpix1\n    assert w.wcs.crpix[1] == new_crpix2\n\n    # Check if the transformation results are different due to the updated CRPIX\n    orig_world_coords = w.wcs_pix2world([[original_crpix1, original_crpix2]], 1)\n    new_world_coords = w.wcs_pix2world([[new_crpix1, new_crpix2]], 1)\n\n    assert not np.allclose(orig_world_coords, new_world_coords)\n", "def test_sip_order_parsing():\n    \"\"\"\n    Test parsing of SIP distortion coefficients with different orders.\n    \"\"\"\n    header = \"\"\"", "def test_wcs_sub():\n    \"\"\"\n    Test the WCS.sub() method to ensure it correctly extracts subsets of the WCS object.\n    \"\"\"\n    hdr = get_pkg_data_contents('data/sip.fits', encoding='binary')\n    w = wcs.WCS(hdr)\n    \n    # Test extracting celestial WCS\n    celestial_wcs = w.sub([WCSSUB_CELESTIAL])\n    assert celestial_wcs.naxis == 2\n    assert 'RA---TAN-SIP' in celestial_wcs.wcs.ctype\n    assert 'DEC--TAN-SIP' in celestial_wcs.wcs.ctype\n    \n    # Test extracting specific axis using integer\n    axis_wcs = w.sub([1])\n    assert axis_wcs.naxis == 1\n    assert axis_wcs.wcs.ctype[0] == 'RA---TAN-SIP'\n    \n    # Test extracting specific axis using list of integers\n    axes_wcs = w.sub([1, 2])\n    assert axes_wcs.naxis == 2\n    assert 'RA---TAN-SIP' in axes_wcs.wcs.ctype\n    assert 'DEC--TAN-SIP' in axes_wcs.wcs.ctype\n", "def test_no_convergence_exception():\n    \"\"\"\n    Test NoConvergence exception and its attributes.\n    \"\"\"\n    class FakeWCS:\n            raise wcs.NoConvergence(\n                \"Failed to converge\",\n                best_solution=np.array([[0, 0], [1, 1]]),\n                accuracy=np.array([[0.1, 0.1], [0.1, 0.1]]),\n                niter=5,\n                divergent=np.array([1]),\n                slow_conv=np.array([0])\n            )\n\n    w = FakeWCS()\n\n    with pytest.raises(wcs.NoConvergence) as excinfo:\n        w.all_world2pix([[1, 2]], 1)\n    \n    e = excinfo.value\n    assert e.best_solution is not None\n    assert e.accuracy is not None\n    assert e.niter == 5\n    assert e.divergent is not None\n    assert e.slow_conv is not None\n", "def test_wcs_copy():\n    \"\"\"\n    Test the copy and deepcopy methods of WCS class.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n\n    # Test shallow copy\n    w_copy = w.copy()\n    assert isinstance(w_copy, wcs.WCS)\n    assert w.wcs.compare(w_copy.wcs)\n    assert w.sip.a_order == w_copy.sip.a_order\n    assert w.sip.b_order == w_copy.sip.b_order\n    assert np.array_equal(w.sip.a, w_copy.sip.a)\n    assert np.array_equal(w.sip.b, w_copy.sip.b)\n\n    # Test deep copy\n    w_deepcopy = w.deepcopy()\n    assert isinstance(w_deepcopy, wcs.WCS)\n    assert w.wcs.compare(w_deepcopy.wcs)\n    assert w.sip.a_order == w_deepcopy.sip.a_order\n    assert w.sip.b_order == w_deepcopy.sip.b_order\n    assert np.array_equal(w.sip.a, w_deepcopy.sip.a)\n    assert np.array_equal(w.sip.b, w_deepcopy.sip.b)\n"], "sample_39": ["def test_no_convergence():\n    \"\"\"\n    Test to ensure NoConvergence exception is properly raised and handled.\n    \"\"\"\n    header = get_pkg_data_contents('data/sip-broken.hdr', encoding='binary')\n    w = wcs.WCS(header)\n    \n    # Setting an impossible transformation to ensure convergence will fail\n    w.wcs.crpix = [1e10, 1e10]\n    \n    # Attempt to convert a pixel coordinate to world coordinate\n    world_coords = w.all_pix2world([[100, 100]], 1)\n    \n    # Attempt to convert back to pixel coordinates\n    with pytest.raises(wcs.NoConvergence) as excinfo:\n        w.all_world2pix(world_coords, 1, tolerance=1e-10, maxiter=1)\n    \n    assert \"failed to converge\" in str(excinfo.value)\n    assert excinfo.value.niter == 1\n    assert excinfo.value.best_solution is not None\n    assert excinfo.value.accuracy is not None\n", "def test_wcs_sub():\n    \"\"\"\n    Test the sub method of the WCS class to ensure it correctly selects\n    specific axes from the WCS object.\n    \"\"\"\n    w = wcs.WCS(naxis=3)\n    w.wcs.crval = [40, 50, 60]\n    w.wcs.cdelt = [0.1, 0.1, 0.1]\n    w.wcs.crpix = [1, 1, 1]\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'VELO-LSR']\n\n    # Create a sub-WCS object with only the celestial axes (RA and DEC)\n    sub_wcs = w.sub([1, 2])\n    assert sub_wcs.naxis == 2\n    assert sub_wcs.wcs.ctype.tolist() == ['RA---TAN', 'DEC--TAN']\n    assert sub_wcs.wcs.crval.tolist() == [40, 50]\n    assert sub_wcs.wcs.cdelt.tolist() == [0.1, 0.1]\n    assert sub_wcs.wcs.crpix.tolist() == [1, 1]\n\n    # Create a sub-WCS object with only the spectral axis (VELO-LSR)\n    sub_wcs = w.sub([3])\n    assert sub_wcs.naxis == 1\n    assert sub_wcs.wcs.ctype.tolist() == ['VELO-LSR']\n    assert sub_wcs.wcs.crval.tolist() == [60]\n    assert sub_wcs.wcs.cdelt.tolist() == [0.1]\n    assert sub_wcs.wcs.crpix.tolist() == [1]\n\n    # Test for non-existent axis (should raise an error)\n    with pytest.raises(ValueError):\n        sub_wcs = w.sub([4])\n", "def test_pix2foc():\n    \"\"\"\n    Test the pix2foc transformation with various inputs.\n    \"\"\"\n    hdr_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(hdr_name)\n\n    # Test with a single point\n    x_foc, y_foc = w.pix2foc(200, 300, 0)\n    assert isinstance(x_foc, float)\n    assert isinstance(y_foc, float)\n\n    # Test with an array of points\n    x_pix = np.array([200, 250, 300])\n    y_pix = np.array([300, 350, 400])\n    x_foc, y_foc = w.pix2foc(x_pix, y_pix, 0)\n    assert x_foc.shape == x_pix.shape\n    assert y_foc.shape == y_pix.shape\n\n    # Test with invalid shape\n    with pytest.raises(ValueError) as exc:\n        w.pix2foc(np.array([[200, 250], [300, 350]]), 0)\n    assert exc.value.args[0] == 'When providing two arguments, the array must be of shape (N, 2)'\n\n    # Test with empty arrays\n    x_foc, y_foc = w.pix2foc(np.array([]), np.array([]), 0)\n    assert x_foc.shape == (0,)\n    assert y_foc.shape == (0,)\n", "def test_wcs_copy():\n    \"\"\"\n    Test the copy and deepcopy methods of the WCS class to ensure they\n    create correct copies of WCS objects.\n    \"\"\"\n    header = get_pkg_data_contents('data/sip.fits', encoding='binary')\n    original_wcs = wcs.WCS(header)\n\n    shallow_copy_wcs = original_wcs.copy()\n    deep_copy_wcs = original_wcs.deepcopy()\n\n    # Check that the shallow copy has the same attributes\n    assert original_wcs.wcs.compare(shallow_copy_wcs.wcs)\n    assert original_wcs.sip == shallow_copy_wcs.sip\n    assert original_wcs.cpdis1 == shallow_copy_wcs.cpdis1\n    assert original_wcs.cpdis2 == shallow_copy_wcs.cpdis2\n    assert original_wcs.det2im1 == shallow_copy_wcs.det2im1\n    assert original_wcs.det2im2 == shallow_copy_wcs.det2im2\n\n    # Check that the deep copy has the same attributes\n    assert original_wcs.wcs.compare(deep_copy_wcs.wcs)\n    assert original_wcs.sip == deep_copy_wcs.sip\n    assert original_wcs.cpdis1 == deep_copy_wcs.cpdis1\n    assert original_wcs.cpdis2 == deep_copy_wcs.cpdis2\n    assert original_wcs.det2im1 == deep_copy_wcs.det2im1\n    assert original_wcs.det2im2 == deep_copy_wcs.det2im2\n\n    # Modify the original WCS and ensure the copies are unaffected\n    original_wcs.wcs.crval = [30, 30]\n    original_wcs.sip.a[0, 0] = 0\n\n    assert not original_wcs.wcs.compare(shallow_copy_wcs.wcs)\n    assert not original_wcs.wcs.compare(deep_copy_wcs.wcs)\n    assert original_wcs.sip != shallow_copy_wcs.sip\n    assert original_wcs.sip != deep_copy_wcs.sip\n", "def test_invalid_units():\n    \"\"\"\n    Test that an appropriate warning or error is raised for invalid units in WCS headers.\n    \"\"\"\n    header = get_pkg_data_contents('data/invalid_units.hdr', encoding='binary')\n    with pytest.raises(wcs.InvalidTransformError):\n        w = wcs.WCS(header)\n\n    header = get_pkg_data_contents('data/invalid_units2.hdr', encoding='binary')\n    with catch_warnings(wcs.FITSFixedWarning) as w:\n        wcs.WCS(header)\n    \n    assert len(w) > 0\n    for item in w:\n        assert 'Invalid unit string' in str(item.message)\n"], "sample_40": ["def test_mass_energy_density():\n    # Test the mass-energy equivalency for mass density units\n    mass_density = 1 * u.kg / u.m**3\n    energy_density = (1 * u.kg / u.m**3).to(u.J / u.m**3, equivalencies=u.mass_energy())\n    expected_energy_density = (1 * u.kg).to(u.J, equivalencies=u.mass_energy()) / u.m**3\n\n    assert_allclose(energy_density.value, expected_energy_density.value)\n\n    # Test the inverse conversion\n    mass_density_converted_back = energy_density.to(u.kg / u.m**3, equivalencies=u.mass_energy())\n    assert_allclose(mass_density_converted_back.value, mass_density.value)\n", "def test_temperature_conversion():\n    # Kelvin to Celsius and back\n    kelvin = 300 * u.K\n    celsius = kelvin.to(u.deg_C, equivalencies=u.temperature())\n    assert_allclose(celsius.value, 26.85, atol=1e-2)\n    assert_allclose(celsius.to_value(u.K, equivalencies=u.temperature()), 300, atol=1e-2)\n\n    # Celsius to Fahrenheit and back\n    fahrenheit = celsius.to(u.imperial.deg_F, equivalencies=u.temperature())\n    assert_allclose(fahrenheit.value, 80.33, atol=1e-2)\n    assert_allclose(fahrenheit.to_value(u.deg_C, equivalencies=u.temperature()), 26.85, atol=1e-2)\n\n    # Kelvin to Fahrenheit and back\n    fahrenheit = kelvin.to(u.imperial.deg_F, equivalencies=u.temperature())\n    assert_allclose(fahrenheit.value, 80.33, atol=1e-2)\n    assert_allclose(fahrenheit.to_value(u.K, equivalencies=u.temperature()), 300, atol=1e-2)\n", "def test_mass_energy_derived_units():\n    # Test equivalence for surface density (kg/m^2 to J/m^2)\n    surface_density_kg_m2 = 5 * u.kg / u.m**2\n    surface_density_j_m2 = surface_density_kg_m2.to(u.J / u.m**2, equivalencies=u.mass_energy())\n    assert_allclose(surface_density_j_m2.value, 5 * (_si.c.value ** 2), rtol=1e-7)\n\n    # Test equivalence for energy density (kg/m^3 to J/m^3)\n    energy_density_kg_m3 = 3 * u.kg / u.m**3\n    energy_density_j_m3 = energy_density_kg_m3.to(u.J / u.m**3, equivalencies=u.mass_energy())\n    assert_allclose(energy_density_j_m3.value, 3 * (_si.c.value ** 2), rtol=1e-7)\n\n    # Test equivalence for power density (kg/s to J/s)\n    power_density_kg_s = 2 * u.kg / u.s\n    power_density_j_s = power_density_kg_s.to(u.J / u.s, equivalencies=u.mass_energy())\n    assert_allclose(power_density_j_s.value, 2 * (_si.c.value ** 2), rtol=1e-7)\n", "def test_mass_energy_density():\n    mass_density = 1 * u.kg / u.m**3\n    energy_density = mass_density.to(u.J / u.m**3, equivalencies=u.mass_energy())\n    assert_quantity_allclose(energy_density, 1 * u.J / u.m**3)\n\n    energy_density_back = energy_density.to(u.kg / u.m**3, equivalencies=u.mass_energy())\n    assert_quantity_allclose(energy_density_back, mass_density)\n", "def test_mass_energy_equivalency():\n    # Test the conversion between mass and energy for various masses\n    mass_kg = np.array([1, 2, 3]) * u.kg\n    energy_J = mass_kg.to(u.J, equivalencies=u.mass_energy())\n    expected_energy_J = np.array([1, 2, 3]) * u.kg * constants.c**2\n    assert_quantity_allclose(energy_J, expected_energy_J)\n\n    # Test the inverse conversion from energy to mass\n    energy_J = np.array([1, 2, 3]) * u.J\n    mass_kg = energy_J.to(u.kg, equivalencies=u.mass_energy())\n    expected_mass_kg = energy_J / (constants.c**2)\n    assert_quantity_allclose(mass_kg, expected_mass_kg)\n\n    # Test conversion for energy densities\n    energy_density_J_m3 = np.array([1, 2, 3]) * u.J / u.m**3\n    mass_density_kg_m3 = energy_density_J_m3.to(u.kg / u.m**3, equivalencies=u.mass_energy())\n    expected_mass_density_kg_m3 = energy_density_J_m3 / (constants.c**2)\n    assert_quantity_allclose(mass_density_kg_m3, expected_mass_density_kg_m3)\n"], "sample_41": ["def test_unit_registry_equivalencies():\n    # Test setting, adding, and retrieving equivalencies in the registry\n    equivalencies = [(u.cm, u.inch, lambda x: x / 2.54, lambda x: x * 2.54)]\n    registry = u.get_current_unit_registry()\n    original_equivalencies = registry.equivalencies\n\n    try:\n        # Set equivalencies\n        registry.set_enabled_equivalencies(equivalencies)\n        assert registry.equivalencies == equivalencies\n\n        # Add another equivalency and check if both are present\n        new_equivalency = (u.m, u.yd, lambda x: x / 0.9144, lambda x: x * 0.9144)\n        registry.add_enabled_equivalencies([new_equivalency])\n        assert len(registry.equivalencies) == 2\n        assert new_equivalency in registry.equivalencies\n\n    finally:\n        # Restore original equivalencies\n        registry.set_enabled_equivalencies(original_equivalencies)\n", "def test_reset_unit_registry():\n    registry = u.get_current_unit_registry()\n    assert len(registry.all_units) > 0\n    assert len(registry.registry) > 0\n\n    registry._reset_units()\n    assert len(registry.all_units) == 0\n    assert len(registry.registry) == 0\n\n    registry.add_enabled_units([u.m])\n    assert len(registry.all_units) == 1\n    assert 'm' in registry.registry\n\n    registry._reset_units()\n    assert len(registry.all_units) == 0\n    assert len(registry.registry) == 0\n", "def test_flatten_units_collection():\n    \"\"\"\n    Test _flatten_units_collection for various input types and values.\n    \"\"\"\n    from astropy.units.core import _flatten_units_collection\n\n    # Single unit\n    assert _flatten_units_collection(u.m) == {u.m}\n\n    # List of units\n    assert _flatten_units_collection([u.m, u.s]) == {u.m, u.s}\n\n    # Dictionary of units\n    assert _flatten_units_collection({'distance': u.m, 'time': u.s}) == {u.m, u.s}\n\n    # Iterable of units\n    assert _flatten_units_collection((u.m, u.s)) == {u.m, u.s}\n\n    # Module containing units\n    import astropy.units.si as si\n    assert 'm' in _flatten_units_collection(si)\n\n    # Mixed types\n    assert _flatten_units_collection([u.m, {'time': u.s}, (u.kg, u.s)]) == {u.m, u.s, u.kg}\n", "def test_set_enabled_units():\n    # Create a custom unit\n    custom_unit = u.def_unit('custom_unit', u.m)\n\n    # Set the enabled units to only the custom unit\n    with u.set_enabled_units([custom_unit]):\n        assert 'custom_unit' in u.get_current_unit_registry().registry\n        assert u.get_current_unit_registry().all_units == {custom_unit}\n\n    # Ensure the custom unit is not in the registry after the context ends\n    assert 'custom_unit' not in u.get_current_unit_registry().registry\n    assert custom_unit not in u.get_current_unit_registry().all_units\n\n", "def test_set_enabled_units():\n    unit1 = u.def_unit('unit1', represents=10 * u.m)\n    unit2 = u.def_unit('unit2', represents=20 * u.s)\n    \n    with u.set_enabled_units([unit1, unit2]):\n        assert 'unit1' in u.get_current_unit_registry().registry\n        assert 'unit2' in u.get_current_unit_registry().registry\n        assert u.m.find_equivalent_units() == u.UnitBase.EquivalentUnitsList([u.dimensionless_unscaled, unit1])\n        assert u.s.find_equivalent_units() == u.UnitBase.EquivalentUnitsList([u.dimensionless_unscaled, unit2])\n    \n    assert 'unit1' not in u.get_current_unit_registry().registry\n    assert 'unit2' not in u.get_current_unit_registry().registry\n"], "sample_42": ["def test_logarithmic_equivalency():\n    # Check conversion between dex and dimensionless values\n    dex_val = 2 * u.dex\n    dimensionless_val = dex_val.to(u.dimensionless_unscaled, equivalencies=u.logarithmic())\n    expected_dimensionless_val = 10 ** 2\n    assert np.allclose(dimensionless_val, expected_dimensionless_val)\n\n    # Reverse conversion\n    dex_val_reversed = dimensionless_val * u.dimensionless_unscaled\n    dex_val_converted = dex_val_reversed.to(u.dex, equivalencies=u.logarithmic())\n    assert np.allclose(dex_val_converted, dex_val)\n\n    # Test with array\n    dex_array = np.array([1, 2, 3]) * u.dex\n    dimensionless_array = dex_array.to(u.dimensionless_unscaled, equivalencies=u.logarithmic())\n    expected_dimensionless_array = 10 ** np.array([1, 2, 3])\n    assert np.allclose(dimensionless_array, expected_dimensionless_array)\n\n    # Reverse conversion for array\n    dex_array_reversed = dimensionless_array * u.dimensionless_unscaled\n    dex_array_converted = dex_array_reversed.to(u.dex, equivalencies=u.logarithmic())\n    assert np.allclose(dex_array_converted, dex_array)\n", "def test_dimensionless_angles_high_power():\n    # Test dimensionless_angles equivalency with a higher power of radians\n    rad2 = u.dimensionless_angles()\n    assert (1 * u.radian ** 2).to_value(1, equivalencies=rad2) == 1.\n    assert (1 * u.radian ** 3).to_value(1, equivalencies=rad2) == 1.\n    assert (1 * u.radian ** 2).to_value(u.deg ** 2, equivalencies=rad2) == (u.radian ** 2).to_value(u.deg ** 2)\n    assert (1 * u.radian ** 3).to_value(u.deg ** 3, equivalencies=rad2) == (u.radian ** 3).to_value(u.deg ** 3)\n    assert (1 * u.deg ** 2).to_value(1, equivalencies=rad2) == (u.deg ** 2).to_value(u.radian ** 2)\n    assert (1 * u.deg ** 3).to_value(1, equivalencies=rad2) == (u.deg ** 3).to_value(u.radian ** 3)\n", "def test_mass_energy_density():\n    # Test conversion between mass density (kg/m^3) and energy density (J/m^3)\n    mass_density = 1 * u.kg / u.m**3\n    energy_density = mass_density.to(u.J / u.m**3, equivalencies=u.mass_energy())\n    assert_quantity_allclose(energy_density.value, 8.9875517873681764e16)\n    \n    # Test inverse conversion\n    converted_mass_density = energy_density.to(u.kg / u.m**3, equivalencies=u.mass_energy())\n    assert_quantity_allclose(converted_mass_density.value, mass_density.value)\n", "def test_mass_energy_density():\n    # Test conversion from mass density to energy density\n    mass_density = 1.0 * u.kg / u.m**3\n    energy_density = mass_density.to(u.J / u.m**3, equivalencies=u.mass_energy())\n    assert_quantity_allclose(energy_density, _si.c.value**2 * u.J / u.m**3)\n\n    # Test conversion from energy density to mass density\n    converted_mass_density = energy_density.to(u.kg / u.m**3, equivalencies=u.mass_energy())\n    assert_quantity_allclose(converted_mass_density, mass_density)\n", "def test_brightness_temperature_default_beam_area():\n    # Test brightness temperature without specifying beam area\n    nu = 5 * u.GHz\n    tb = 3.526295144567176 * u.K\n    jysr = (1 * u.Jy / u.sr).to(u.K, equivalencies=u.brightness_temperature(nu))\n    assert_quantity_allclose(jysr, tb)\n\n    jy = tb.to(u.Jy / u.sr, equivalencies=u.brightness_temperature(nu))\n    assert_quantity_allclose(jy, 1 * u.Jy / u.sr)\n"], "sample_43": ["def test_custom_fitness_function():\n    class CustomFitness(FitnessFunc):\n            return N_k * (np.log(N_k) - np.log(T_k)) - a_k + b_k - c_k\n\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n    x = np.sin(2 * np.pi * t)\n    sigma = 0.1\n\n    bins = bayesian_blocks(t, x, sigma, fitness=CustomFitness)\n    assert len(bins) > 1  # Ensure it segments the data\n", "def test_fitness_function_custom_class():\n    \"\"\"Test using a custom fitness function derived from FitnessFunc.\"\"\"\n    class CustomFitnessFunc(FitnessFunc):\n            return N_k * np.log(N_k / T_k)\n\n    rng = np.random.RandomState(42)\n    t = rng.randn(100)\n    edges = bayesian_blocks(t, fitness=CustomFitnessFunc(p0=0.05))\n    assert_allclose(edges, [-2.6197451, -0.71094865, 0.36866702, 1.85227818])\n", "def test_custom_fitness_function():\n    \"\"\"Test bayesian_blocks with a custom fitness function.\"\"\"\n    class CustomFitness(FitnessFunc):\n            return N_k * np.log(T_k + 1)\n\n    rng = np.random.RandomState(123)\n    t = rng.rand(50)\n    x = np.ones_like(t)\n    custom_fitness_instance = CustomFitness()\n\n    # Using the custom fitness function instance directly\n    bins = bayesian_blocks(t, x, fitness=custom_fitness_instance)\n    assert len(bins) > 1\n\n    # Using the custom fitness function class directly\n    bins = bayesian_blocks(t, x, fitness=CustomFitness)\n    assert len(bins) > 1\n", "def test_custom_fitness_function():\n    class CustomFitness(FitnessFunc):\n            return N_k * np.log(T_k + 1)\n\n    rng = np.random.RandomState(0)\n    t = rng.randn(100)\n    \n    custom_fitness = CustomFitness(p0=0.05)\n    edges = bayesian_blocks(t, fitness=custom_fitness)\n    \n    assert len(edges) > 1  # Ensure there are multiple bins\n    assert edges[0] <= t.min() and edges[-1] >= t.max()  # Ensure edges cover the data range\n\n    # Test that the custom fitness function can be used with keyword arguments\n    custom_fitness_with_args = CustomFitness(gamma=0.1)\n    edges_with_args = bayesian_blocks(t, fitness=custom_fitness_with_args)\n\n    assert_allclose(edges, edges_with_args, rtol=1e-2)\n", "def test_point_measures_with_missing_data():\n    rng = np.random.RandomState(42)\n    t = np.linspace(0, 10, 50)\n    x = np.sin(t) + 0.5 * rng.randn(len(t))\n    sigma = 0.1 * np.ones_like(x)\n\n    # Introduce NaN values into the data\n    x[5] = np.nan\n    sigma[10] = np.nan\n\n    # Remove NaN values\n    mask = ~np.isnan(x) & ~np.isnan(sigma)\n    t = t[mask]\n    x = x[mask]\n    sigma = sigma[mask]\n\n    # Fit the Bayesian Blocks model\n    edges = bayesian_blocks(t, x, sigma, fitness='measures')\n\n    assert len(edges) > 1\n    assert edges[0] == t[0]\n    assert edges[-1] == t[-1]\n"], "sample_44": ["def test_function_unit_equivalencies():\n    \"\"\"Check that equivalencies between function units and physical units\n    are correctly applied.\"\"\"\n    lu = u.mag(u.Jy)\n    value = np.array([0, -2.5, -5, -7.5, -10])\n    physical_value = np.array([1, 10, 100, 1000, 10000]) * u.Jy\n\n    # Ensure equivalency directly\n    assert_allclose(lu.to(u.Jy, value), physical_value)\n    assert_allclose(u.Jy.to(lu, physical_value), value)\n\n    # Use equivalency in conversion functions\n    assert_allclose(lu.to(u.Jy, value, equivalencies=lu.equivalencies), physical_value)\n    assert_allclose(u.Jy.to(lu, physical_value, equivalencies=lu.equivalencies), value)\n\n    # Check if equivalencies are recognized in is_equivalent\n    assert lu.is_equivalent(u.Jy, equivalencies=lu.equivalencies)\n    assert u.Jy.is_equivalent(lu, equivalencies=lu.equivalencies)\n", "    def test_default_instantiation(self):\n        \"\"\"Test default instantiation of a FunctionUnitBase subclass.\"\"\"\n        class TestUnit(FunctionUnitBase):\n            _default_function_unit = u.mag\n            _quantity_class = u.Magnitude\n\n                return 2 * x\n\n                return x / 2\n\n        tu = TestUnit()\n        assert tu.physical_unit == u.dimensionless_unscaled\n        assert tu.function_unit == u.mag\n", "def test_function_unit_initialization():\n    \"\"\"Test initialization of FunctionUnitBase and its properties.\"\"\"\n    class TestUnit(FunctionUnitBase):\n        @property\n            return u.Unit('test_unit')\n\n        @property\n            return FunctionQuantity\n\n            return x\n\n            return x\n\n    physical_unit = u.m\n    func_unit = TestUnit(physical_unit)\n    \n    assert func_unit.physical_unit == physical_unit\n    assert func_unit.function_unit == u.Unit('test_unit')\n    assert func_unit._quantity_class is FunctionQuantity\n\n    func_unit_copy = func_unit._copy()\n    assert func_unit_copy.physical_unit == physical_unit\n    assert func_unit_copy.function_unit == u.Unit('test_unit')\n", "def test_function_unit_repr():\n    \"\"\"Ensure that FunctionUnitBase's __repr__ method works correctly for various cases.\"\"\"\n    # Create a subclass to test the abstract class behavior\n    class TestFunctionUnit(FunctionUnitBase):\n        @property\n            return u.Unit('dB')\n        \n        @property\n            return u.Quantity\n        \n            return x\n        \n            return x\n    \n    unit = TestFunctionUnit(u.m)\n    assert repr(unit) == 'TestFunctionUnit(\"m\", unit=\"dB\")'\n    \n    unit_with_function_unit = TestFunctionUnit(u.m, function_unit=u.Unit('2 dB'))\n    assert repr(unit_with_function_unit) == 'TestFunctionUnit(\"m\", unit=\"2 dB\")'\n    \n    dimensionless_unit = TestFunctionUnit(u.dimensionless_unscaled)\n    assert repr(dimensionless_unit) == 'TestFunctionUnit(\"1\", unit=\"dB\")'\n    \n    unit_with_physical_unit_in_si = TestFunctionUnit(u.kg)\n    assert repr(unit_with_physical_unit_in_si.si) == 'TestFunctionUnit(\"kg\", unit=\"dB\")'\n", "def test_functionunitbase_init():\n    \"\"\"Test initialization of FunctionUnitBase and its exceptions.\"\"\"\n    class TestFunctionUnit(FunctionUnitBase):\n        _default_function_unit = u.mag\n        _quantity_class = u.Magnitude\n\n            return x\n\n            return x\n\n    # Test valid initialization\n    test_unit = TestFunctionUnit(physical_unit=u.m)\n    assert test_unit.physical_unit == u.m\n    assert test_unit.function_unit == u.mag\n\n    # Test initialization with invalid physical unit\n    with pytest.raises(ValueError):\n        TestFunctionUnit(physical_unit=u.mag)\n\n    # Test initialization with invalid function unit\n    with pytest.raises(ValueError):\n        TestFunctionUnit(physical_unit=u.m, function_unit=u.s)\n"], "sample_45": ["    def test_now_func(self):\n        \"\"\"\n        Test the Now function to ensure it produces a SQL statement for\n        the current timestamp.\n        \"\"\"\n        now_instance = Now()\n        self.assertIsInstance(now_instance.output_field, DateTimeField)\n        compiler = DTModel.objects.all().query.get_compiler('default')\n        sql, params = now_instance.as_sql(compiler, connection=compiler.connection)\n        \n        # Check that the default SQL template is 'CURRENT_TIMESTAMP'\n        self.assertEqual(sql, 'CURRENT_TIMESTAMP')\n        self.assertEqual(params, [])\n\n        # Test PostgreSQL-specific behavior\n        postgresql_sql, postgresql_params = now_instance.as_postgresql(compiler, connection=compiler.connection)\n        self.assertEqual(postgresql_sql, 'STATEMENT_TIMESTAMP()')\n        self.assertEqual(postgresql_params, [])\n", "    def test_extract_invalid_type(self):\n        \"\"\"\n        Test Extract with invalid output_field type.\n        \"\"\"\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        msg = \"Tried to Extract from an invalid type.\"\n        with self.assertRaisesMessage(AssertionError, msg):\n            list(DTModel.objects.annotate(extracted=Extract('start_datetime', 'year', output_field=IntegerField())))\n", "    def test_extract_invalid_lookup_name(self):\n        \"\"\"\n        Test that using an invalid lookup name with the Extract function raises the appropriate error.\n        \"\"\"\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        \n        with self.assertRaisesMessage(ValueError, 'lookup_name must be provided'):\n            Extract('start_datetime')\n\n        with self.assertRaisesMessage(ValueError, \"Invalid lookup name 'invalid_lookup'\"):\n            list(DTModel.objects.annotate(extracted=Extract('start_datetime', 'invalid_lookup')))\n", "    def test_now_func(self):\n        # Test the Now function to ensure it returns the current timestamp.\n        now = timezone.now()\n        # Create a model with the current timestamp\n        self.create_model(now, now + timedelta(hours=1))\n\n        # Annotate the queryset with the Now function\n        qs = DTModel.objects.annotate(current_time=Now()).order_by('start_datetime')\n        model = qs.first()\n\n        # Check that the current_time is close to the actual current time (within a reasonable delta)\n        self.assertAlmostEqual(model.current_time.timestamp(), now.timestamp(), delta=2)\n", "    def test_now_func(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        with self.assertRaisesMessage(TypeError, \"TruncBase expressions require an output field.\"):\n            Now('start_datetime', tzinfo=timezone.utc).resolve_expression()\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(current_time=Now()).order_by('start_datetime'),\n            [(start_datetime, datetime.now()), (end_datetime, datetime.now())],\n            transform=lambda m: (m.start_datetime, m.current_time)\n        )\n        \n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(current_time=Now(tzinfo=timezone.utc)).order_by('start_datetime'),\n            [(start_datetime, datetime.now(timezone.utc)), (end_datetime, datetime.now(timezone.utc))],\n            transform=lambda m: (m.start_datetime, m.current_time)\n        )\n"], "sample_46": ["    def setUpTestData(cls):\n        cls.date_objs = [\n            NullableUUIDModel.objects.create(field=uuid.uuid4(), date_field=\"2021-01-01\"),\n            NullableUUIDModel.objects.create(field=uuid.uuid4(), date_field=\"2022-05-15\"),\n            NullableUUIDModel.objects.create(field=uuid.uuid4(), date_field=\"2023-09-30\"),\n        ]\n", "    def setUpTestData(cls):\n        cls.obj1 = NullableUUIDModel.objects.create(field=uuid.uuid4(), datetime_field=\"2023-01-01 12:00:00\")\n        cls.obj2 = NullableUUIDModel.objects.create(field=uuid.uuid4(), datetime_field=\"2022-01-01 12:00:00\")\n        cls.obj3 = NullableUUIDModel.objects.create(field=uuid.uuid4(), datetime_field=\"2021-01-01 12:00:00\")\n", "    def setUpTestData(cls):\n        cls.dates = [\n            DateTimeField.objects.create(field='2023-01-01 12:00:00'),\n            DateTimeField.objects.create(field='2022-05-15 12:00:00'),\n            DateTimeField.objects.create(field='2021-08-25 12:00:00'),\n        ]\n", "    def setUpTestData(cls):\n        cls.dates = [\n            YearModel.objects.create(date_field='2023-01-01'),\n            YearModel.objects.create(date_field='2022-01-01'),\n            YearModel.objects.create(date_field='2021-01-01'),\n        ]\n", "    def setUpTestData(cls):\n        cls.date_objs = [\n            models.DateField().from_db_value('2021-01-01', None, None, None),\n            models.DateField().from_db_value('2022-05-20', None, None, None),\n        ]\n        cls.datetime_objs = [\n            models.DateTimeField().from_db_value('2023-06-15 12:34:56', None, None, None),\n            models.DateTimeField().from_db_value('2024-11-23 08:00:00', None, None, None),\n        ]\n"], "sample_47": ["    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_cleanse_special_types_multivaluedict(self):\n        \"\"\"\n        Ensure that cleanse_special_types method correctly cleanses sensitive data in MultiValueDict.\n        \"\"\"\n        filter = SafeExceptionReporterFilter()\n        request = self.rf.post('/test_view/', data={'password': 'secret'})\n        multivaluedict = MultiValueDict({'password': ['secret'], 'username': ['john']})\n        cleansed = filter.get_cleansed_multivaluedict(request, multivaluedict)\n        self.assertEqual(cleansed['password'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed['username'], ['john'])\n", "    def test_get_post_parameters(self):\n        \"\"\"\n        Test SafeExceptionReporterFilter.get_post_parameters to ensure sensitive POST\n        parameters are replaced with stars.\n        \"\"\"\n        filter = SafeExceptionReporterFilter()\n        request = RequestFactory().post('/some_url/', data={\n            'regular': 'value',\n            'sensitive': 'secret_value',\n        })\n        request.sensitive_post_parameters = ['sensitive']\n        \n        cleansed_data = filter.get_post_parameters(request)\n        self.assertEqual(cleansed_data['regular'], 'value')\n        self.assertEqual(cleansed_data['sensitive'], CLEANSED_SUBSTITUTE)\n        ", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n        self.rf = RequestFactory()\n", "    def test_get_post_parameters(self):\n        \"\"\"\n        Ensure that sensitive POST parameters are cleansed.\n        \"\"\"\n        request = self.rf.post('/some_url/', {\n            'username': 'admin',\n            'password': 'super_secret',\n            'token': '12345',\n        })\n        request.sensitive_post_parameters = ['password', 'token']\n        filter = SafeExceptionReporterFilter()\n        params = filter.get_post_parameters(request)\n        self.assertEqual(params['username'], 'admin')\n        self.assertEqual(params['password'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(params['token'], CLEANSED_SUBSTITUTE)\n"], "sample_48": ["    def test_stddev_variance(self):\n        books = Book.objects.aggregate(\n            stddev_pop=StdDev('price', sample=False),\n            stddev_samp=StdDev('price', sample=True),\n            var_pop=Variance('price', sample=False),\n            var_samp=Variance('price', sample=True)\n        )\n        self.assertAlmostEqual(books['stddev_pop'], Decimal('23.77'), places=2)\n        self.assertAlmostEqual(books['stddev_samp'], Decimal('25.94'), places=2)\n        self.assertAlmostEqual(books['var_pop'], Decimal('565.35'), places=2)\n        self.assertAlmostEqual(books['var_samp'], Decimal('672.92'), places=2)\n", "    def test_variance_aggregate(self):\n        # Testing Variance with sample=False\n        vals = Author.objects.aggregate(var_age=Variance(\"age\"))\n        self.assertAlmostEqual(vals[\"var_age\"], 89.36, places=2)\n\n        # Testing Variance with sample=True\n        vals = Author.objects.aggregate(var_age_sample=Variance(\"age\", sample=True))\n        self.assertAlmostEqual(vals[\"var_age_sample\"], 100.32, places=2)\n", "    def test_stddev_variance(self):\n        # Testing StdDev with sample=True and sample=False\n        books = Book.objects.aggregate(\n            stddev_sample=StdDev(\"rating\", sample=True),\n            stddev_population=StdDev(\"rating\", sample=False)\n        )\n        self.assertAlmostEqual(books['stddev_sample'], 0.64, places=2)\n        self.assertAlmostEqual(books['stddev_population'], 0.58, places=2)\n\n        # Testing Variance with sample=True and sample=False\n        books = Book.objects.aggregate(\n            variance_sample=Variance(\"rating\", sample=True),\n            variance_population=Variance(\"rating\", sample=False)\n        )\n        self.assertAlmostEqual(books['variance_sample'], 0.41, places=2)\n        self.assertAlmostEqual(books['variance_population'], 0.34, places=2)\n", "    def test_count_with_filter(self):\n        # Test Count aggregate with a filter to count only books with a rating greater than 4\n        vals = Book.objects.aggregate(high_rating_count=Count('rating', filter=Q(rating__gt=4)))\n        self.assertEqual(vals, {\"high_rating_count\": 2})\n\n        # Test Count aggregate with a filter to count only books with a rating less than 4\n        vals = Book.objects.aggregate(low_rating_count=Count('rating', filter=Q(rating__lt=4)))\n        self.assertEqual(vals, {\"low_rating_count\": 1})\n\n        # Test Count aggregate with a filter to count only books with a rating equal to 4\n        vals = Book.objects.aggregate(equal_rating_count=Count('rating', filter=Q(rating=4)))\n        self.assertEqual(vals, {\"equal_rating_count\": 3})\n", "    def test_invalid_distinct_usage(self):\n        msg = \"Count does not allow distinct.\"\n        with self.assertRaisesMessage(TypeError, msg):\n            Author.objects.aggregate(Count('id', distinct=True, invalid_kwarg=True))\n"], "sample_49": ["    def test_clearable_file_input(self):\n        class MyForm(Form):\n            file = CharField(widget=ClearableFileInput())\n\n        # Initial render with an initial file set\n        form = MyForm(initial={'file': 'testfile.txt'})\n        self.assertHTMLEqual(\n            str(form['file']),\n            '<div><input type=\"file\" name=\"file\">'\n            '<input type=\"checkbox\" name=\"file-clear\" id=\"file_clear_id\">'\n            '<label for=\"file_clear_id\">Clear</label></div>'\n        )\n\n        # Render with no initial file set and the clear checkbox checked\n        form = MyForm(data={'file-clear': 'on'}, files={})\n        self.assertHTMLEqual(\n            str(form['file']),\n            '<div><input type=\"file\" name=\"file\">'\n            '<input type=\"checkbox\" name=\"file-clear\" id=\"file_clear_id\">'\n            '<label for=\"file_clear_id\">Clear</label></div>'\n        )\n        self.assertEqual(form.cleaned_data['file'], False)\n\n        # Render with a new file uploaded and the clear checkbox checked\n        form = MyForm(data={'file-clear': 'on'}, files={'file': 'newfile.txt'})\n        self.assertHTMLEqual(\n            str(form['file']),\n            '<div><input type=\"file\" name=\"file\">'\n            '<input type=\"checkbox\" name=\"file-clear\" id=\"file_clear_id\">'\n            '<label for=\"file_clear_id\">Clear</label></div>'\n        )\n        self.assertEqual(form.cleaned_data['file'], FILE_INPUT_CONTRADICTION)\n", "    def test_media_repr_with_empty_css_js(self):\n        # Test Media __repr__ with empty css and js\n        m = Media(css={}, js=[])\n        self.assertEqual(\n            repr(m),\n            \"Media(css={}, js=[])\"\n        )\n", "    def test_media_absolute_path(self):\n        media = Media(js=['relative/path.js', 'http://example.com/absolute/path.js', '/another/absolute/path.js'])\n        self.assertEqual(\n            media.absolute_path('relative/path.js'),\n            'http://media.example.com/static/relative/path.js'\n        )\n        self.assertEqual(\n            media.absolute_path('http://example.com/absolute/path.js'),\n            'http://example.com/absolute/path.js'\n        )\n        self.assertEqual(\n            media.absolute_path('/another/absolute/path.js'),\n            '/another/absolute/path.js'\n        )\n", "    def test_widget_render(self):\n        class CustomRenderer:\n                return f\"Rendered: {template_name} with context: {context}\"\n\n        widget = TextInput(attrs={'class': 'test-class'})\n        self.assertEqual(widget.render('test', 'value', {'placeholder': 'Enter text'}), \n                         'Rendered: django/forms/widgets/text.html with context: {\\'widget\\': {\\'name\\': \\'test\\', \\'is_hidden\\': False, \\'required\\': False, \\'value\\': \\'value\\', \\'attrs\\': {\\'class\\': \\'test-class\\', \\'placeholder\\': \\'Enter text\\'}, \\'template_name\\': \\'django/forms/widgets/text.html\\'}}')\n\n        widget_with_renderer = TextInput(attrs={'class': 'test-class'})\n        self.assertEqual(widget_with_renderer.render('test', 'value', {'placeholder': 'Enter text'}, renderer=CustomRenderer()), \n                         'Rendered: django/forms/widgets/text.html with context: {\\'widget\\': {\\'name\\': \\'test\\', \\'is_hidden\\': False, \\'required\\': False, \\'value\\': \\'value\\', \\'attrs\\': {\\'class\\': \\'test-class\\', \\'placeholder\\': 'Enter text\\'}, \\'template_name\\': \\'django/forms/widgets/text.html\\'}}')\n", "    def test_checkbox_input(self):\n        # Test CheckboxInput widget functionality\n        widget = CheckboxInput()\n        self.assertEqual(widget.format_value(True), None)\n        self.assertEqual(widget.format_value('true'), 'true')\n        self.assertEqual(widget.format_value('false'), 'false')\n        self.assertEqual(widget.format_value('some_value'), 'some_value')\n        \n        context = widget.get_context('test_name', True, {'class': 'test-class'})\n        self.assertIn('checked', context['widget']['attrs'])\n        self.assertEqual(context['widget']['attrs']['class'], 'test-class')\n\n        self.assertFalse(widget.value_from_datadict({}, {}, 'test_name'))\n        self.assertTrue(widget.value_from_datadict({'test_name': 'true'}, {}, 'test_name'))\n        self.assertFalse(widget.value_from_datadict({'test_name': 'false'}, {}, 'test_name'))\n        self.assertTrue(widget.value_from_datadict({'test_name': 'True'}, {}, 'test_name'))\n        self.assertFalse(widget.value_from_datadict({'test_name': 'False'}, {}, 'test_name'))\n        self.assertTrue(widget.value_from_datadict({'test_name': '1'}, {}, 'test_name'))\n        self.assertFalse(widget.value_from_datadict({'test_name': '0'}, {}, 'test_name'))\n\n        self.assertFalse(widget.value_omitted_from_data({'test_name': 'value'}, {}, 'test_name'))\n        self.assertFalse(widget.value_omitted_from_data({}, {}, 'test_name'))\n"], "sample_50": ["    def test_no_host_no_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                'somepassword',\n            )\n        )\n", "    def test_no_host_no_port(self):\n        \"\"\"Test case when both host and port are not provided.\"\"\"\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                'somepassword',\n            )\n        )\n", "    def test_no_host_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                'somepassword',\n            )\n        )\n", "    def test_no_host_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                'somepassword',\n            )\n        )\n", "    def test_no_host_no_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                'somepassword',\n            )\n        )\n"], "sample_51": ["    def test_invalid_parse_duration(self):\n        invalid_durations = [\n            'P-4D',  # negative days in ISO 8601\n            'P5Y',  # unsupported years in ISO 8601\n            'P2DT3H4M5S',  # mixed date and time parts not supported\n            '5H',  # hours without 'PT' prefix\n            '10:15:30:40',  # too many colons in standard format\n            '15:30:-1',  # negative seconds in standard format\n            '10:61:30',  # invalid minute value\n            '10:30:61',  # invalid second value\n            '10:-30:30',  # negative minute value\n            '10::30',  # missing minute value\n            '10:30:',  # trailing colon\n            'days 15:30',  # missing number for days\n        ]\n        for source in invalid_durations:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_invalid_parse_duration(self):\n        # Invalid duration formats that should return None\n        test_values = (\n            '10::30',  # Invalid hours:minutes:seconds format\n            '5:60:00',  # Invalid minutes value\n            '10:30:-1',  # Negative seconds value in standard format\n            '4 days 25:00:00',  # Invalid hours value in PostgreSQL format\n            'PT5M5S',  # Invalid ISO 8601 format\n            'P-4D',  # Invalid negative days in ISO 8601 format\n            'P4DT-5H',  # Invalid mixed positive and negative values in ISO 8601 format\n            '1 day 0:00:60',  # Invalid seconds value in PostgreSQL format\n        )\n        for source in test_values:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_invalid_inputs(self):\n        invalid_inputs = [\n            'invalid',\n            '2012-04-23T25:61:00',\n            '2012-04-23T09:15:61',\n            '2012-04-23T09:15:00+25:00',\n            '2012-04-23T09:15:00-25:00',\n            '2012-04-23T09:15:00+14:61',\n            '2012-04-23T09:15:00Zextra',\n            'P-4D',  # Invalid ISO 8601\n            '1 day extra',  # Invalid PostgreSQL format\n            '10:15:30 extra',  # Invalid standard duration format\n        ]\n        for value in invalid_inputs:\n            with self.subTest(value=value):\n                self.assertIsNone(parse_date(value))\n                self.assertIsNone(parse_time(value))\n                self.assertIsNone(parse_datetime(value))\n                self.assertIsNone(parse_duration(value))\n", "def test_invalid_duration(self):\n    invalid_durations = [\n        \"P\",  # Incomplete ISO 8601 duration\n        \"P-4D\",  # Negative sign not allowed in ISO 8601 duration\n        \"10:60\",  # Invalid minutes\n        \"25:00:00\",  # Invalid hours\n        \"99:99:99.9999999\",  # Invalid fractional seconds\n        \"4 days 25:00:00\",  # Invalid hours in PostgreSQL format\n        \"4 days 00:60:00\",  # Invalid minutes in PostgreSQL format\n        \"4 days 00:00:60\",  # Invalid seconds in PostgreSQL format\n        \"4 days 00:00:00.0000001\",  # Invalid microseconds in PostgreSQL format\n        \"1:1\",  # Invalid time format\n        \"1:60:00\",  # Invalid minutes\n        \"1:00:60\",  # Invalid seconds\n        \"1.1\",  # Invalid format\n        \"random string\",  # Completely invalid string\n    ]\n    for source in invalid_durations:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n", "    def test_invalid_duration_formats(self):\n        # Invalid duration formats\n        invalid_durations = [\n            '4 days 25:61:61',  # invalid hour, minute, second\n            'P4X',  # invalid ISO 8601 format\n            '5:60',  # invalid minute\n            '25:00:00',  # invalid hour\n            '15:30:-1',  # negative seconds not allowed in this format\n            '15:30:60',  # invalid seconds\n            '2 days 25:00',  # invalid hour in PostgreSQL format\n            '4D',  # invalid duration format\n            'P-4D',  # negative sign in ISO 8601 format should be before 'P'\n            '-P4D',  # invalid negative ISO 8601 format\n        ]\n        for value in invalid_durations:\n            with self.subTest(value=value):\n                self.assertIsNone(parse_duration(value))\n"], "sample_52": ["    def test_modelchoicefield_empty_initial(self):\n        \"\"\"\n        Test ModelChoiceField with an empty initial value.\n        \"\"\"\n        f = forms.ModelChoiceField(Category.objects.all(), required=False)\n        self.assertIsNone(f.clean(''))\n        self.assertIsNone(f.clean(None))\n        self.assertEqual(f.clean(self.c1.pk).name, 'Entertainment')\n        \n        c4 = Category.objects.create(name='Fourth', slug='4th', url='4th')\n        f = forms.ModelChoiceField(Category.objects.all(), initial=c4.pk)\n        self.assertEqual(f.clean(c4.pk).name, 'Fourth')\n        \n        Category.objects.get(url='4th').delete()\n        msg = \"['Select a valid choice. That choice is not one of the available choices.']\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(c4.pk)\n", "    def test_fields_for_model(self):\n        # Create a temporary model for testing\n        from django.db import models\n\n        class TempModel(models.Model):\n            name = models.CharField(max_length=100)\n            slug = models.SlugField()\n            description = models.TextField(blank=True)\n\n        fields = fields_for_model(TempModel, fields=['name', 'slug'])\n        self.assertIn('name', fields)\n        self.assertIn('slug', fields)\n        self.assertNotIn('description', fields)\n\n        exclude = fields_for_model(TempModel, exclude=['description'])\n        self.assertIn('name', exclude)\n        self.assertIn('slug', exclude)\n        self.assertNotIn('description', exclude)\n", "    def test_model_to_dict(self):\n        article = Article.objects.create(\n            pub_date=datetime.date(1988, 1, 4),\n            writer=Writer.objects.create(name='Test writer'),\n        )\n        category1 = Category.objects.create(name='cat1')\n        category2 = Category.objects.create(name='cat2')\n        article.categories.set([category1, category2])\n\n        data = model_to_dict(article)\n        self.assertEqual(data['pub_date'], datetime.date(1988, 1, 4))\n        self.assertEqual(data['writer'], article.writer.pk)\n        self.assertEqual(set(data['categories']), {category1.pk, category2.pk})\n", "def test_modelform_factory_fields_exclude(self):\n    # Test fields parameter\n    AuthorForm = forms.modelform_factory(Author, fields=['name'])\n    form = AuthorForm()\n    self.assertIn('name', form.fields)\n    self.assertNotIn('birth_date', form.fields)\n\n    # Test exclude parameter\n    AuthorForm = forms.modelform_factory(Author, exclude=['birth_date'])\n    form = AuthorForm()\n    self.assertIn('name', form.fields)\n    self.assertNotIn('birth_date', form.fields)\n", "    def test_modelmultiplechoicefield_clean(self):\n        f = forms.ModelMultipleChoiceField(Category.objects.all())\n        self.assertEqual(f.clean([self.c1.pk, self.c2.pk]), [self.c1, self.c2])\n        self.assertEqual(f.clean([]), [])\n        with self.assertRaises(ValidationError):\n            f.clean([0])\n        with self.assertRaises(ValidationError):\n            f.clean(['invalid'])\n        # Test invalid type that requires TypeError to be caught.\n        with self.assertRaises(ValidationError):\n            f.clean(['invalid_type'])\n        with self.assertRaises(ValidationError):\n            f.clean([{'foo': 'bar'}])\n\n        # Add a Category object *after* the ModelMultipleChoiceField has already been\n        # instantiated. This proves clean() checks the database during clean()\n        # rather than caching it at instantiation time.\n        c4 = Category.objects.create(name='Fourth', url='4th')\n        self.assertEqual(f.clean([c4.pk]), [c4])\n\n        # Delete a Category object *after* the ModelMultipleChoiceField has already been\n        # instantiated. This proves clean() checks the database during clean()\n        # rather than caching it at instantiation time.\n        Category.objects.get(url='4th').delete()\n        msg = \"['Select a valid choice. That choice is not one of the available choices.']\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean([c4.pk])\n"], "sample_54": ["def test_file_response_with_custom_filename(self):\n    response = FileResponse(io.BytesIO(b'custom filename content'), filename='custom_name.txt')\n    self.assertEqual(response['Content-Length'], '23')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.txt\"')\n    self.assertEqual(list(response), [b'custom filename content'])\n", "    def test_set_headers_with_filelike_object(self):\n        content = b'binary content'\n        filelike = io.BytesIO(content)\n        filelike.name = \"test_file.txt\"\n\n        response = FileResponse(filelike)\n        self.assertEqual(response['Content-Length'], str(len(content)))\n        self.assertEqual(response['Content-Type'], 'text/plain')\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"test_file.txt\"')\n        self.assertEqual(list(response), [content])\n", "    def test_file_response_with_custom_filename(self):\n        with tempfile.NamedTemporaryFile(suffix=\".txt\") as tmp:\n            tmp.write(b\"sample content\")\n            tmp.seek(0)  # Reset file pointer to the beginning\n            response = FileResponse(tmp, filename=\"custom_name.txt\")\n            self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.txt\"')\n            self.assertEqual(response['Content-Type'], 'text/plain')\n            self.assertEqual(list(response), [b\"sample content\"])\n", "def test_file_response_with_custom_filename(self):\n    with tempfile.NamedTemporaryFile() as tmp:\n        response = FileResponse(tmp, filename=\"custom_name.txt\")\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.txt\"')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        response.close()\n", "    def test_file_from_named_buffer_with_filename(self):\n        buffer = io.BytesIO(b'binary content')\n        buffer.name = 'example.bin'\n        response = FileResponse(buffer)\n        self.assertEqual(response['Content-Length'], '14')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"example.bin\"')\n        self.assertEqual(list(response), [b'binary content'])\n"], "sample_53": ["def test_file_input_format_value(self):\n    file_input = FileInput()\n    self.assertIsNone(file_input.format_value('somefile.txt'))\n", "    def test_media_addition(self):\n        css1 = {'all': ['style1.css']}\n        js1 = ['script1.js']\n        media1 = Media(css=css1, js=js1)\n\n        css2 = {'all': ['style2.css']}\n        js2 = ['script2.js']\n        media2 = Media(css=css2, js=js2)\n\n        combined_media = media1 + media2\n\n        self.assertEqual(combined_media._css, {\n            'all': ['style1.css', 'style2.css'],\n        })\n        self.assertEqual(combined_media._js, ['script1.js', 'script2.js'])\n", "    def test_media_addition(self):\n        \"\"\"Test the addition of media objects.\"\"\"\n        media1 = Media(css={'all': ['styles1.css']}, js=['script1.js'])\n        media2 = Media(css={'all': ['styles2.css']}, js=['script2.js'])\n        combined_media = media1 + media2\n        self.assertEqual(combined_media._css, {'all': [['styles1.css'], ['styles2.css']]})\n        self.assertEqual(combined_media._js, ['script1.js', 'script2.js'])\n", "    def test_media_addition(self):\n        media1 = Media(css={'all': ['styles1.css']}, js=['script1.js'])\n        media2 = Media(css={'all': ['styles2.css']}, js=['script2.js'])\n        combined_media = media1 + media2\n        \n        self.assertEqual(combined_media._css, {'all': ['styles1.css', 'styles2.css']})\n        self.assertEqual(combined_media._js, ['script1.js', 'script2.js'])\n", "def test_media_absolute_path(self):\n        media_instance = Media(js=['admin/js/vendor/jquery/jquery.min.js'])\n        absolute_path = media_instance.absolute_path('admin/js/vendor/jquery/jquery.min.js')\n        self.assertEqual(absolute_path, '/static/admin/js/vendor/jquery/jquery.min.js')\n\n        media_instance_with_http = Media(js=['http://example.com/js/script.js'])\n        absolute_path_with_http = media_instance_with_http.absolute_path('http://example.com/js/script.js')\n        self.assertEqual(absolute_path_with_http, 'http://example.com/js/script.js')\n\n        media_instance_with_https = Media(js=['https://example.com/js/script.js'])\n        absolute_path_with_https = media_instance_with_https.absolute_path('https://example.com/js/script.js')\n        self.assertEqual(absolute_path_with_https, 'https://example.com/js/script.js')\n"], "sample_55": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.model_admin = mock.MagicMock()\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.actor = Actor.objects.create(name='John Doe', age=45)\n        cls.actor_with_html = Actor.objects.create(name='<b>Jane Doe</b>', age=30)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.city = City.objects.create(name='Test City')\n        cls.restaurant = Restaurant.objects.create(name='Test Restaurant', city=cls.city)\n        cls.worker1 = Worker.objects.create(name='Worker 1', surname='Surname 1', work_at=cls.restaurant)\n        cls.worker2 = Worker.objects.create(name='Worker 2', surname='Surname 2', work_at=cls.restaurant)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.readonly_post = Post.objects.create(\n            title=\"Readonly Post\", content=\"This is a readonly content.\",\n            readonly_content=\"readonly content\",\n            posted=datetime.date.today()\n        )\n"], "sample_58": ["    def test_custom_prefix_addition(self):\n        class CustomPrefixForm(Form):\n            first_name = CharField()\n            last_name = CharField()\n\n                return 'custom_prefix_%s' % field_name\n\n        # Testing form without data\n        form = CustomPrefixForm()\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p><label for=\"id_custom_prefix_first_name\">First name:</label> '\n            '<input type=\"text\" name=\"custom_prefix_first_name\" id=\"id_custom_prefix_first_name\" required></p>\\n'\n            '<p><label for=\"id_custom_prefix_last_name\">Last name:</label> '\n            '<input type=\"text\" name=\"custom_prefix_last_name\" id=\"id_custom_prefix_last_name\" required></p>'\n        )\n        \n        # Testing form with data\n        data = {\n            'custom_prefix_first_name': 'John',\n            'custom_prefix_last_name': 'Doe',\n        }\n        form = CustomPrefixForm(data=data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['first_name'], 'John')\n        self.assertEqual(form.cleaned_data['last_name'], 'Doe')\n", "    def test_order_fields(self):\n        # Test the `order_fields` method to ensure fields are ordered correctly.\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n            field5 = CharField()\n\n        form = TestForm()\n        self.assertEqual(list(form.fields.keys()), ['field1', 'field2', 'field3', 'field4', 'field5'])\n        \n        # Reorder fields and check the new order\n        form.order_fields(['field5', 'field4', 'field3'])\n        self.assertEqual(list(form.fields.keys()), ['field5', 'field4', 'field3', 'field1', 'field2'])\n        \n        # Test with non-existing field names in the order list\n        form.order_fields(['field3', 'nonexistent', 'field1'])\n        self.assertEqual(list(form.fields.keys()), ['field3', 'field1', 'field5', 'field4', 'field2'])\n", "    def test_order_fields(self):\n        # Test custom field ordering.\n        class CustomOrderForm(Form):\n            first_name = CharField()\n            last_name = CharField()\n            age = IntegerField()\n            field_order = ['last_name', 'first_name', 'age']\n\n        f = CustomOrderForm()\n        self.assertEqual(list(f.fields.keys()), ['last_name', 'first_name', 'age'])\n\n        # Test partial custom field ordering.\n        class PartialOrderForm(Form):\n            first_name = CharField()\n            last_name = CharField()\n            age = IntegerField()\n            field_order = ['last_name', 'first_name']\n\n        f = PartialOrderForm()\n        self.assertEqual(list(f.fields.keys()), ['last_name', 'first_name', 'age'])\n\n        # Test no custom field ordering.\n        class NoOrderForm(Form):\n            first_name = CharField()\n            last_name = CharField()\n            age = IntegerField()\n\n        f = NoOrderForm()\n        self.assertEqual(list(f.fields.keys()), ['first_name', 'last_name', 'age'])\n\n        # Test invalid field name in field_order.\n        class InvalidOrderForm(Form):\n            first_name = CharField()\n            last_name = CharField()\n            age = IntegerField()\n            field_order = ['last_name', 'invalid_field']\n\n        f = InvalidOrderForm()\n        self.assertEqual(list(f.fields.keys()), ['first_name', 'last_name', 'age'])  # Invalid field should be ignored\n", "    def test_order_fields_function(self):\n        class SampleForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n\n        form = SampleForm()\n        # Initial order\n        self.assertEqual(list(form.fields.keys()), ['field1', 'field2', 'field3', 'field4'])\n        \n        # Order with specific field order\n        form.order_fields(['field4', 'field3', 'field1', 'field2'])\n        self.assertEqual(list(form.fields.keys()), ['field4', 'field3', 'field1', 'field2'])\n\n        # Order with partial field list (remaining fields should keep their original order)\n        form.order_fields(['field2', 'field1'])\n        self.assertEqual(list(form.fields.keys()), ['field2', 'field1', 'field4', 'field3'])\n\n        # Order with empty field order (should retain the original order)\n        form.order_fields([])\n        self.assertEqual(list(form.fields.keys()), ['field2', 'field1', 'field4', 'field3'])\n\n        # Order with None (should retain the original order)\n        form.order_fields(None)\n        self.assertEqual(list(form.fields.keys()), ['field2', 'field1', 'field4', 'field3'])\n", "    def test_order_fields(self):\n        class CustomOrderForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n\n                field_order = kwargs.pop('field_order', None)\n                super().__init__(*args, **kwargs)\n                self.order_fields(field_order)\n\n        # Test with custom field order\n        form = CustomOrderForm(field_order=['field3', 'field1', 'field4', 'field2'])\n        self.assertEqual(list(form.fields.keys()), ['field3', 'field1', 'field4', 'field2'])\n\n        # Test with default order\n        form = CustomOrderForm()\n        self.assertEqual(list(form.fields.keys()), ['field1', 'field2', 'field3', 'field4'])\n\n        # Test with partially provided order\n        form = CustomOrderForm(field_order=['field2', 'field3'])\n        self.assertEqual(list(form.fields.keys()), ['field2', 'field3', 'field1', 'field4'])\n\n        # Test with an invalid field in the order (should be ignored)\n        form = CustomOrderForm(field_order=['field2', 'field5', 'field3'])\n        self.assertEqual(list(form.fields.keys()), ['field2', 'field3', 'field1', 'field4'])\n"], "sample_56": ["def test_invalid_ordering(self):\n    \"\"\"\n    Test invalid ordering configuration that includes non-existing fields.\n    \"\"\"\n    class SongAdmin(admin.ModelAdmin):\n        ordering = ('nonexistent_field',)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'ordering[0]' refers to 'nonexistent_field', which is not an attribute of 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E033',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "    def test_custom_ordering_field(self):\n        class SongAdmin(admin.ModelAdmin):\n            ordering = ['invalid_field']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'ordering[0]' refers to 'invalid_field', which is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E033',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_field_in_fieldsets_not_in_model(self):\n        class SongAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ('title', 'nonexistent_field'),\n                }),\n            ]\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets[0][1][\\\"fields\\\"]' refers to 'nonexistent_field', \"\n                \"which is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E030',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_model_admin_check_autocomplete_fields(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = ['nonexistent']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields[0]' refers to 'nonexistent', \"\n                \"which is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E037',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_autocomplete_fields_not_list_or_tuple(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = 'test'\n\n        self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n            checks.Error(\n                \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                obj=SongAdmin,\n                id='admin.E036',\n            )\n        ])\n"], "sample_57": ["    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context('password', None, {})\n        self.assertIn('summary', context)\n        self.assertEqual(context['summary'], [{'label': 'No password set.'}])\n", "    def test_get_context_with_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context('password', None, {})\n        self.assertIn('summary', context)\n        self.assertEqual(context['summary'], [{'label': 'No password set.'}])\n", "    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context('password', None, {})\n        self.assertEqual(context['summary'], [{'label': str(_(\"No password set.\"))}])\n", "    def test_get_context_no_password_set(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context('password', None, {})\n        self.assertEqual(context['summary'], [{'label': str(_(\"No password set.\"))}])\n", "    def test_read_only_password_help_text(self):\n        user = User.objects.get(username='testclient')\n        form = UserChangeForm(instance=user)\n        self.assertIn(\n            'Raw passwords are not stored, so there is no way to see this user\\'s password, but you can change the password using <a href=\"../password/\">this form</a>.',\n            form.fields['password'].help_text\n        )\n"], "sample_59": ["    def test_model_equality_same_instance(self):\n        \"\"\"\n        Test that two instances of the same model with the same primary key are equal.\n        \"\"\"\n        dept1 = Department.objects.create(id=1, name=\"HR\")\n        dept2 = Department.objects.get(id=1)\n        self.assertEqual(dept1, dept2)\n    ", "    def test_model_init_with_kwargs(self):\n        \"\"\"\n        Test initializing a model with keyword arguments.\n        \"\"\"\n        d = Department.objects.create(id=20, name=\"HR\")\n        w = Worker(department=d, name=\"Part-time\")\n        self.assertEqual(w.department.name, \"HR\")\n        self.assertEqual(w.name, \"Part-time\")\n", "    def test_model_equality_and_hash(self):\n        \"\"\"\n        Test __eq__ and __hash__ methods for Model instances.\n        \"\"\"\n        d1 = Department.objects.create(id=1, name=\"HR\")\n        d2 = Department.objects.create(id=2, name=\"Engineering\")\n        w1 = Worker.objects.create(id=1, department=d1, name=\"Alice\")\n        w2 = Worker.objects.create(id=2, department=d2, name=\"Bob\")\n\n        # Test equality\n        self.assertEqual(w1, w1)\n        self.assertNotEqual(w1, w2)\n        self.assertNotEqual(w1, d1)  # Different model instances\n\n        # Test hash\n        with self.assertRaises(TypeError):\n            hash(Worker())\n\n        self.assertEqual(hash(w1), hash(w1.pk))\n        self.assertNotEqual(hash(w1), hash(w2))\n", "    def test_modelbase_init_without_parents(self):\n        \"\"\"\n        Test that ModelBase.__new__ returns a new class without parents.\n        \"\"\"\n        class NoParentModel(metaclass=ModelBase):\n            pass\n\n        self.assertTrue(hasattr(NoParentModel, '__module__'))\n        self.assertEqual(NoParentModel.__module__, __name__)\n", "def test_model_eq_and_hash(self):\n    \"\"\"\n    Test the __eq__ and __hash__ methods of the Model class.\n    \"\"\"\n    d1 = Department.objects.create(id=1, name=\"IT\")\n    d2 = Department.objects.create(id=2, name=\"HR\")\n    w1 = Worker.objects.create(id=1, department=d1, name=\"Alice\")\n    w2 = Worker.objects.create(id=2, department=d1, name=\"Bob\")\n    w3 = Worker.objects.create(id=3, department=d2, name=\"Charlie\")\n\n    # Test __eq__\n    self.assertTrue(w1 == w1)  # Same instance\n    self.assertFalse(w1 == w2)  # Different instances\n    self.assertFalse(w1 == d1)  # Different model types\n\n    # Test __hash__\n    with self.assertRaises(TypeError):\n        hash(Worker())  # Unsaved instance should raise TypeError\n\n    self.assertEqual(hash(w1), hash(w1.pk))\n    self.assertEqual(hash(w2), hash(w2.pk))\n    self.assertNotEqual(hash(w1), hash(w2.pk))\n"], "sample_60": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.site = AdminSite()\n        self.factory = RequestFactory()\n        self.model = Episode\n        self.admin = ModelAdmin(self.model, self.site)\n        self.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n"], "sample_61": ["    def test_unicode_validator_edge_cases(self):\n        valid_usernames = ['a'*150, '@.+-_']  # Testing long usernames and special characters at the edge\n        invalid_usernames = ['']  # Empty username should be invalid\n        v = validators.UnicodeUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_unicode_validator_message(self):\n        invalid_username = \"o'connell\"\n        expected_message = (\n            'Enter a valid username. This value may contain only letters, '\n            'numbers, and @/./+/-/_ characters.'\n        )\n        v = validators.UnicodeUsernameValidator()\n        with self.assertRaises(ValidationError) as cm:\n            v(invalid_username)\n        self.assertEqual(cm.exception.message, expected_message)\n", "    def test_unicode_validator_with_mixed_characters(self):\n        valid_usernames = ['user123', 'test.user', 'my_email@example.com', 'username+plus', 'user-name']\n        invalid_usernames = ['user!name', 'username#', 'user$%name', 'user^&name', 'user*name']\n        v = validators.UnicodeUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def setUp(self):\n        self.validator = validators.ASCIIUsernameValidator()\n", "    def test_ascii_validator_with_non_ascii(self):\n        invalid_usernames = ['Ren\u00e9', '\u00c9ric', '\u0623\u062d\u0645\u062f', '\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30', 'trailingnewline\\n']\n        v = validators.ASCIIUsernameValidator()\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n"], "sample_62": ["    def setUp(self):\n        self.site = admin.AdminSite()\n        self.site.register(Person, NameAdmin)\n", "def test_unregister_model(self):\n    \"Unregistering a registered model should succeed.\"\n    self.site.register(Person)\n    self.assertTrue(self.site.is_registered(Person))\n    self.site.unregister(Person)\n    self.assertFalse(self.site.is_registered(Person))\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "def test_unregister_model(self):\n    \"Unregistering a registered model should work correctly.\"\n    self.site.register(Person)\n    self.assertTrue(self.site.is_registered(Person))\n    self.site.unregister(Person)\n    self.assertFalse(self.site.is_registered(Person))\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n"], "sample_63": ["    def test_template_does_not_exist(self):\n        engine = Engine(dirs=[TEMPLATE_DIR])\n        with self.assertRaises(TemplateDoesNotExist) as cm:\n            engine.get_template('non_existent_template.html')\n        self.assertEqual(cm.exception.args[0], 'non_existent_template.html')\n", "    def test_custom_library_loading(self):\n        custom_libraries = {\n            'custom_tags': 'path.to.custom.library',\n        }\n        engine = Engine(dirs=[TEMPLATE_DIR], libraries=custom_libraries)\n        self.assertIn('custom_tags', engine.template_libraries)\n        self.assertEqual(engine.template_libraries['custom_tags'], import_library('path.to.custom.library'))\n", "    def test_default_initialization(self):\n        engine = Engine()\n        self.assertEqual(engine.dirs, [])\n        self.assertEqual(engine.app_dirs, False)\n        self.assertEqual(engine.autoescape, True)\n        self.assertEqual(engine.context_processors, [])\n        self.assertEqual(engine.debug, False)\n        self.assertEqual(engine.loaders, [('django.template.loaders.cached.Loader', ['django.template.loaders.filesystem.Loader'])])\n        self.assertEqual(engine.string_if_invalid, '')\n        self.assertEqual(engine.file_charset, 'utf-8')\n        self.assertEqual(engine.libraries, {})\n        self.assertEqual(engine.builtins, [\n            'django.template.defaulttags',\n            'django.template.defaultfilters',\n            'django.template.loader_tags',\n        ])\n", "    def test_import_library_success(self):\n        libraries = {'my_lib': 'django.template.defaulttags'}\n        engine = Engine(libraries=libraries)\n        self.assertIn('my_lib', engine.template_libraries)\n", "    def test_get_template_libraries(self):\n        libraries = {\n            'custom_library': 'path.to.custom.library',\n        }\n        engine = Engine(libraries=libraries)\n        self.assertIn('custom_library', engine.template_libraries)\n        self.assertEqual(engine.template_libraries['custom_library'].__name__, 'custom_library_module')  # assuming the imported library's module name\n"], "sample_64": ["    def test_bad_request(self):\n        response = HttpResponseBadRequest()\n        self.assertEqual(response.status_code, 400)\n        # Ensure content can be set and retrieved correctly\n        response.content = 'Bad request'\n        self.assertEqual(response.content, b'Bad request')\n", "    def test_http_response_permanent_redirect(self):\n        response = HttpResponsePermanentRedirect('/permanent-redirect/')\n        self.assertEqual(response.status_code, 301)\n        # Standard HttpResponse init args can be used\n        response = HttpResponsePermanentRedirect(\n            '/permanent-redirect/',\n            content='The resource has permanently moved',\n            content_type='text/html',\n        )\n        self.assertContains(response, 'The resource has permanently moved', status_code=301)\n        self.assertEqual(response.url, response['Location'])\n        ", "    def test_file_response(self):\n        filename = os.path.join(os.path.dirname(__file__), 'abc.txt')\n        # Create a temporary file for testing.\n        with open(filename, 'wb') as f:\n            f.write(b'Test file content')\n\n        file = open(filename, 'rb')\n        response = FileResponse(file)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(response['Content-Length'], str(os.path.getsize(filename)))\n\n        content = b''.join(response)\n        self.assertEqual(content, b'Test file content')\n\n        response.close()\n        self.assertTrue(file.closed)\n\n        os.remove(filename)\n", "    def test_file_response(self):\n        file_content = b\"Hello, World!\"\n        filelike = io.BytesIO(file_content)\n        response = FileResponse(filelike, as_attachment=True, filename=\"greeting.txt\")\n\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response[\"Content-Type\"], \"application/octet-stream\")\n        self.assertEqual(response[\"Content-Disposition\"], 'attachment; filename=\"greeting.txt\"')\n\n        # Verify content is streamed correctly\n        streamed_content = b''.join(response)\n        self.assertEqual(streamed_content, file_content)\n", "    def test_http_response_not_found(self):\n        response = HttpResponseNotFound(content='Page not found', content_type='text/plain')\n        self.assertEqual(response.status_code, 404)\n        self.assertEqual(response.content, b'Page not found')\n        self.assertEqual(response['Content-Type'], 'text/plain')\n"], "sample_65": ["    def test_get_catalog_with_invalid_package(self):\n        \"\"\"\n        JavaScriptCatalog raises a ValueError if an invalid package is provided.\n        \"\"\"\n        view = JavaScriptCatalog.as_view()\n        request = RequestFactory().get('/')\n        msg = 'Invalid package(s) provided to JavaScriptCatalog: invalid_package'\n        with self.assertRaisesMessage(ValueError, msg):\n            view(request, packages='invalid_package')\n", "    def _get_inactive_language_code(self):\n        \"\"\"Return language code for a language which is not activated.\"\"\"\n        current_language = get_language()\n        return [code for code, name in settings.LANGUAGES if not code == current_language][0]\n", "    def test_setlang_invalid_language_code(self):\n        \"\"\"\n        The set_language view should not change the language if an invalid\n        language code is provided.\n        \"\"\"\n        current_language = get_language()\n        post_data = {'language': 'invalid_code', 'next': '/'}\n        response = self.client.post('/i18n/setlang/', post_data)\n        self.assertRedirects(response, '/')\n        # The language should remain unchanged.\n        self.assertEqual(get_language(), current_language)\n        self.assertNotIn(LANGUAGE_SESSION_KEY, self.client.session)\n        self.assertNotIn(settings.LANGUAGE_COOKIE_NAME, self.client.cookies)\n", "    def test_get_paths_valid_packages(self):\n        \"\"\"Test get_paths with valid packages.\"\"\"\n        view = JavaScriptCatalog()\n        packages = ['django.contrib.admin', 'django.contrib.auth']\n        paths = view.get_paths(packages)\n        expected_paths = [\n            os.path.join(app_config.path, 'locale')\n            for app_config in apps.get_app_configs()\n            if app_config.name in packages\n        ]\n        self.assertEqual(paths, expected_paths)\n", "    def test_get_plural_string(self):\n        \"\"\"\n        Test if the plural string is correctly fetched from the catalog.\n        \"\"\"\n        catalog = JavaScriptCatalog()\n        catalog.translation = DjangoTranslation('de')\n        catalog.translation._catalog = {\n            '': 'Plural-Forms: nplurals=2; plural=(n != 1);\\n'\n        }\n        self.assertEqual(catalog._plural_string, 'nplurals=2; plural=(n != 1);')\n"], "sample_67": ["    def test_modelform_options_initialization(self):\n        class MockOptions:\n            model = 'TestModel'\n            fields = ['field1', 'field2']\n            exclude = ['field3']\n            widgets = {'field1': 'Widget1'}\n            localized_fields = ['field1']\n            labels = {'field1': 'Label1'}\n            help_texts = {'field1': 'Help1'}\n            error_messages = {'field1': {'error1': 'Error message 1'}}\n            field_classes = {'field1': 'FieldClass1'}\n\n        options = MockOptions()\n        modelform_options = ModelFormOptions(options)\n        \n        self.assertEqual(modelform_options.model, 'TestModel')\n        self.assertEqual(modelform_options.fields, ['field1', 'field2'])\n        self.assertEqual(modelform_options.exclude, ['field3'])\n        self.assertEqual(modelform_options.widgets, {'field1': 'Widget1'})\n        self.assertEqual(modelform_options.localized_fields, ['field1'])\n        self.assertEqual(modelform_options.labels, {'field1': 'Label1'})\n        self.assertEqual(modelform_options.help_texts, {'field1': 'Help1'})\n        self.assertEqual(modelform_options.error_messages, {'field1': {'error1': 'Error message 1'}})\n        self.assertEqual(modelform_options.field_classes, {'field1': 'FieldClass1'})\n", "    def test_model_form_options_initialization(self):\n        class TestMeta:\n            model = 'TestModel'\n            fields = ['field1', 'field2']\n            exclude = ['exclude_field']\n            widgets = {'field1': forms.TextInput()}\n            localized_fields = ['field1']\n            labels = {'field1': 'Field 1'}\n            help_texts = {'field1': 'Help text for field 1'}\n            error_messages = {'field1': {'required': 'Field 1 is required'}}\n            field_classes = {'field1': forms.CharField}\n\n        options = ModelFormOptions(TestMeta)\n        self.assertEqual(options.model, 'TestModel')\n        self.assertEqual(options.fields, ['field1', 'field2'])\n        self.assertEqual(options.exclude, ['exclude_field'])\n        self.assertEqual(options.widgets, {'field1': forms.TextInput()})\n        self.assertEqual(options.localized_fields, ['field1'])\n        self.assertEqual(options.labels, {'field1': 'Field 1'})\n        self.assertEqual(options.help_texts, {'field1': 'Help text for field 1'})\n        self.assertEqual(options.error_messages, {'field1': {'required': 'Field 1 is required'}})\n        self.assertEqual(options.field_classes, {'field1': forms.CharField})\n", "    def test_modelform_options_instantiation(self):\n        class MetaOptions:\n            model = Writer\n            fields = '__all__'\n            exclude = ['name']\n            widgets = {'name': forms.TextInput}\n            localized_fields = ['name']\n            labels = {'name': 'Writer Name'}\n            help_texts = {'name': 'Enter the full name of the writer.'}\n            error_messages = {'name': {'required': 'This field is mandatory.'}}\n            field_classes = {'name': forms.CharField}\n\n        opts = ModelFormOptions(MetaOptions)\n        self.assertEqual(opts.model, Writer)\n        self.assertEqual(opts.fields, '__all__')\n        self.assertEqual(opts.exclude, ['name'])\n        self.assertEqual(opts.widgets, {'name': forms.TextInput})\n        self.assertEqual(opts.localized_fields, ['name'])\n        self.assertEqual(opts.labels, {'name': 'Writer Name'})\n        self.assertEqual(opts.help_texts, {'name': 'Enter the full name of the writer.'})\n        self.assertEqual(opts.error_messages, {'name': {'required': 'This field is mandatory.'}})\n        self.assertEqual(opts.field_classes, {'name': forms.CharField})\n", "    def setUpTestData(cls):\n        cls.character1 = Character.objects.create(\n            username='character1',\n            last_action=datetime.datetime.today() - datetime.timedelta(days=5),\n        )\n        cls.character2 = Character.objects.create(\n            username='character2',\n            last_action=datetime.datetime.today() + datetime.timedelta(days=5),\n        )\n        cls.character3 = Character.objects.create(\n            username='character3',\n            last_action=datetime.datetime.today() - datetime.timedelta(days=1),\n        )\n", "    def test_model_form_options_initialization(self):\n        class Meta:\n            model = Category\n            fields = ['name', 'slug']\n            exclude = ['url']\n            widgets = {'name': forms.TextInput(attrs={'class': 'name-input'})}\n            localized_fields = ['name']\n            labels = {'name': 'Category Name'}\n            help_texts = {'name': 'Enter the category name'}\n            error_messages = {'name': {'required': 'This field is required'}}\n            field_classes = {'name': forms.CharField}\n\n        options = ModelFormOptions(Meta)\n        self.assertEqual(options.model, Category)\n        self.assertEqual(options.fields, ['name', 'slug'])\n        self.assertEqual(options.exclude, ['url'])\n        self.assertEqual(options.widgets, {'name': forms.TextInput(attrs={'class': 'name-input'})})\n        self.assertEqual(options.localized_fields, ['name'])\n        self.assertEqual(options.labels, {'name': 'Category Name'})\n        self.assertEqual(options.help_texts, {'name': 'Enter the category name'})\n        self.assertEqual(options.error_messages, {'name': {'required': 'This field is required'}})\n        self.assertEqual(options.field_classes, {'name': forms.CharField})\n"], "sample_66": ["    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES['signed_cookie'] = signing.get_cookie_signer(salt='test_salt').sign('cookie_value')\n\n        # Test valid signed cookie\n        self.assertEqual(request.get_signed_cookie('signed_cookie', salt='test_salt'), 'cookie_value')\n\n        # Test missing signed cookie with default value\n        self.assertEqual(request.get_signed_cookie('missing_cookie', default='default_value'), 'default_value')\n\n        # Test missing signed cookie without default value\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('missing_cookie')\n\n        # Test invalid signed cookie signature\n        request.COOKIES['signed_cookie'] = 'invalid_signature'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('signed_cookie', salt='test_salt')\n", "    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES['my_cookie'] = signing.get_cookie_signer(salt='my_cookie').sign('my_value')\n\n        # Correct signature\n        self.assertEqual(request.get_signed_cookie('my_cookie'), 'my_value')\n\n        # Bad signature\n        request.COOKIES['my_cookie'] = 'bad_signature'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('my_cookie')\n\n        # Default value on bad signature\n        self.assertEqual(request.get_signed_cookie('my_cookie', default='default_value'), 'default_value')\n        \n        # Missing cookie\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('missing_cookie')\n            \n        self.assertEqual(request.get_signed_cookie('missing_cookie', default='default_value'), 'default_value')\n", "    def test_http_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES['valid_signed_cookie'] = signing.get_cookie_signer(salt='salt').sign('cookie_value')\n        request.COOKIES['invalid_signed_cookie'] = 'invalid_cookie_value'\n        request.COOKIES['expired_signed_cookie'] = signing.get_cookie_signer(salt='salt').sign('cookie_value')\n        \n        # Test valid signed cookie\n        self.assertEqual(request.get_signed_cookie('valid_signed_cookie', salt='salt'), 'cookie_value')\n        \n        # Test invalid signed cookie\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('invalid_signed_cookie', salt='salt')\n        \n        # Test expired signed cookie with default value\n        with override_settings(SECRET_KEY='new_secret_key'):\n            self.assertEqual(request.get_signed_cookie('expired_signed_cookie', default='default_value', salt='salt'), 'default_value')\n        \n        # Test missing signed cookie with default value\n        self.assertEqual(request.get_signed_cookie('missing_signed_cookie', default='default_value', salt='salt'), 'default_value')\n        \n        # Test missing signed cookie without default value\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('missing_signed_cookie', salt='salt')\n", "    def test_get_signed_cookie(self):\n        # Create a request with a signed cookie\n        request = HttpRequest()\n        cookie_signer = signing.get_cookie_signer(salt='test_salt')\n        signed_value = cookie_signer.sign('cookie_value')\n        request.COOKIES['test_cookie'] = signed_value\n\n        # Verify that the signed cookie is correctly retrieved\n        self.assertEqual(request.get_signed_cookie('test_cookie', salt='test_salt'), 'cookie_value')\n\n        # Test with invalid signature\n        request.COOKIES['invalid_cookie'] = 'invalid_value'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('invalid_cookie', salt='test_salt')\n\n        # Test with expired cookie\n        expired_signer = signing.get_cookie_signer(salt='test_salt', max_age=-1)\n        expired_value = expired_signer.sign('cookie_value')\n        request.COOKIES['expired_cookie'] = expired_value\n        with self.assertRaises(signing.SignatureExpired):\n            request.get_signed_cookie('expired_cookie', salt='test_salt')\n\n        # Test with default value\n        self.assertEqual(request.get_signed_cookie('nonexistent_cookie', default='default_value'), 'default_value')\n", "    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        # Set up a signed cookie\n        cookie_key = 'my_cookie'\n        cookie_value = signing.get_cookie_signer(salt=cookie_key).sign('cookie_value')\n        request.COOKIES[cookie_key] = cookie_value\n\n        # Retrieve the signed cookie\n        self.assertEqual(request.get_signed_cookie(cookie_key), 'cookie_value')\n\n        # Attempt to retrieve a non-existent signed cookie with default value\n        self.assertEqual(request.get_signed_cookie('non_existent', default='default_value'), 'default_value')\n\n        # Attempt to retrieve a signed cookie with a bad signature\n        request.COOKIES[cookie_key] = 'invalid_signature'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie(cookie_key)\n\n        # Attempt to retrieve a signed cookie with a bad signature and default value\n        self.assertEqual(request.get_signed_cookie(cookie_key, default='default_value'), 'default_value')\n"], "sample_69": ["    def test_ensure_echo_on_enabled(self, mocked_stdin, mocked_termios):\n        # Mock termios to simulate ECHO being enabled\n        mocked_stdin.isatty.return_value = True\n        attrs = [None] * 4\n        attrs[3] = mocked_termios.ECHO\n        mocked_termios.tcgetattr.return_value = attrs\n\n        autoreload.ensure_echo_on()\n\n        # Ensure that tcsetattr is not called since ECHO is already enabled\n        mocked_termios.tcsetattr.assert_not_called()\n", "    def test_ensure_echo_on(self, mocked_stdin, mocked_termios):\n        # Mock isatty to return True, indicating it's a TTY.\n        mocked_stdin.isatty.return_value = True\n        # Mock tcgetattr to return attributes without ECHO flag.\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        \n        autoreload.ensure_echo_on()\n        \n        # Check if tcgetattr and tcsetattr were called\n        mocked_termios.tcgetattr.assert_called_once_with(mocked_stdin)\n        mocked_termios.tcsetattr.assert_called_once_with(\n            mocked_stdin, mocked_termios.TCSANOW, [0, 0, 0, mocked_termios.ECHO]\n        )\n        ", "    def test_trigger_reload(self, mock_logger, mock_exit):\n        autoreload.trigger_reload('test_file.py')\n        mock_logger.info.assert_called_with('test_file.py changed, reloading.')\n        mock_exit.assert_called_with(3)\n", "    def test_ensure_echo_on_enabled(self, mocked_isatty, mocked_termios):\n        # Mock the termios.tcgetattr to return attributes with ECHO disabled.\n        attrs = [0, 0, 0, 0]\n        attrs[3] = 0  # ECHO disabled\n        mocked_termios.tcgetattr.return_value = attrs\n\n        autoreload.ensure_echo_on()\n\n        # Ensure ECHO is enabled.\n        self.assertTrue(attrs[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called_once_with(mocked_termios.tcgetattr.return_value, mocked_termios.TCSANOW, attrs)\n", "    def test_echo_on_enabled(self, mock_isatty, mock_termios):\n        # Mock the termios attributes\n        attrs = [0, 1, 2, 0b1000]\n        mock_termios.tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        self.assertTrue(attrs[3] & mock_termios.ECHO)\n        mock_termios.tcsetattr.assert_called_once_with(\n            sys.stdin, mock_termios.TCSANOW, attrs\n        )\n"], "sample_70": ["    def test_set_callable(self):\n            return \"custom_value\"\n\n        a = create_a('set_callable')\n        a.set_callable = custom_value\n        a.save()\n        a.set_callable.delete()\n        a = A.objects.get(pk=a.pk)\n        self.assertEqual(a.set_callable, \"custom_value\")\n", "    def test_protected_error_initialization(self):\n        protected_objects = ['obj1', 'obj2']\n        error_message = \"Cannot delete some instances\"\n        error = ProtectedError(error_message, protected_objects)\n        self.assertEqual(error.protected_objects, protected_objects)\n        self.assertEqual(str(error), error_message)\n", "    def test_collect_with_keep_parents(self):\n        parent = R.objects.create()\n        child = RChild.objects.create(r_ptr=parent)\n        grandchild = RChild.objects.create(r_ptr=child)\n        collector = Collector(using='default')\n        collector.collect([grandchild], keep_parents=True)\n        self.assertIn(RChild, collector.data)\n        self.assertNotIn(R, collector.data)\n        self.assertEqual(collector.data[RChild], {grandchild})\n        self.assertFalse(R.objects.filter(pk=parent.pk).exists())\n", "    def test_protected_error_message(self):\n        \"\"\"\n        Test the ProtectedError exception to ensure it carries the correct message\n        and protected objects.\n        \"\"\"\n        r = R.objects.create()\n        a = A.objects.create(name='protected', protect=r)\n        try:\n            r.delete()\n        except IntegrityError as e:\n            self.assertIsInstance(e, ProtectedError)\n            self.assertEqual(\n                str(e),\n                \"Cannot delete some instances of model 'R' because they are \"\n                \"referenced through a protected foreign key: 'A.protect'\"\n            )\n            self.assertEqual(e.protected_objects, [a])\n        else:\n            self.fail(\"ProtectedError not raised\")\n", "    def test_protected_error_message(self):\n        a = create_a('protect_error')\n        try:\n            a.protect.delete()\n        except IntegrityError as e:\n            self.assertIn(\"Cannot delete some instances of model 'R' because they are \"\n                          \"referenced through a protected foreign key: 'A.protect'\", str(e))\n            self.assertIsInstance(e, ProtectedError)\n            self.assertEqual(e.protected_objects, [a.protect])\n"], "sample_71": ["    def test_format_edge_cases(self):\n        self.assertEqual(nformat(0, '.'), '0')\n        self.assertEqual(nformat(-0, '.'), '0')\n        self.assertEqual(nformat('', '.'), '0')\n        self.assertEqual(nformat(None, '.'), 'None')\n        self.assertEqual(nformat(' ', '.'), ' ')\n        self.assertEqual(nformat(float('inf'), '.'), 'inf')\n        self.assertEqual(nformat(float('-inf'), '.'), '-inf')\n        self.assertEqual(nformat(float('nan'), '.'), 'nan')\n", "    def test_format_with_non_integer_grouping(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,7890')\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 1), thousand_sep=','), '1,23,45,67,890')\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2), thousand_sep=','), '12,34,567,890')\n", "    def test_edge_cases(self):\n        # Test the smallest positive Decimal value\n        self.assertEqual(nformat(Decimal('1e-10000'), '.', decimal_pos=10), '0.0000000000')\n        # Test the largest possible integer\n        self.assertEqual(nformat(10**200, '.', grouping=3, thousand_sep=',', force_grouping=True), '1,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000')\n        # Test a Decimal with both integer and fractional parts having multiple zeros\n        self.assertEqual(nformat(Decimal('1000000.00001'), '.', decimal_pos=5, grouping=3, thousand_sep=',', force_grouping=True), '1,000,000.00001')\n        # Test large Decimal with scientific notation and grouping\n        self.assertEqual(nformat(Decimal('1e50'), '.', decimal_pos=2, grouping=3, thousand_sep=',', force_grouping=True), '100,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000.00')\n", "    def test_scientific_notation_conversion(self):\n        # Tests for numbers converted to scientific notation to save memory\n        self.assertEqual(\n            nformat(Decimal('1' + '0' * 201), '.'), \n            '1.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000e+201'\n        )\n        self.assertEqual(\n            nformat(Decimal('1e-201'), '.'), \n            '1.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000e-201'\n        )\n", "    def test_format_with_various_grouping(self):\n        # Test non-uniform digit grouping\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,7890')\n        self.assertEqual(nformat(1234567890, '.', grouping=(2, 3, 0), thousand_sep=','), '1,23,456,7890')\n        \n        # Test edge cases with grouping\n        self.assertEqual(nformat(1234567890, '.', grouping=(1, 0), thousand_sep=','), '1,2,3,4,5,6,7,8,9,0')\n        self.assertEqual(nformat(123, '.', grouping=(0,), thousand_sep=','), '123')\n        \n        # Test grouping with no thousand separator\n        self.assertEqual(nformat(1234567890, '.', grouping=(3,), thousand_sep=''), '1234567890')\n"], "sample_72": ["    def test_serialize_custom_class(self):\n        \"\"\"\n        Test serialization of a custom class that implements the `deconstruct` method.\n        \"\"\"\n        class CustomClass:\n                self.name = name\n                self.value = value\n\n                return (\n                    f'{self.__class__.__module__}.{self.__class__.__name__}',\n                    [self.name, self.value],\n                    {}\n                )\n\n        instance = CustomClass('test_name', 'test_value')\n        self.assertSerializedResultEqual(\n            instance,\n            (\"migrations.test_writer.CustomClass('test_name', 'test_value')\", {'import migrations.test_writer'})\n        )\n        self.assertSerializedEqual(instance)\n", "    def test_serialize_iterable(self):\n        # Test serialization of an iterable (e.g., a generator)\n        iterable = (x for x in range(3))\n        serialized_value = self.serialize_round_trip(iterable)\n        self.assertEqual(serialized_value, (0, 1, 2))\n", "    def test_serialize_deconstructable_serializer(self):\n        @deconstructible\n        class DeconstructableClass:\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n                return (\n                    'migrations.test_writer.DeconstructableClass',\n                    [self.arg1, self.arg2],\n                    {}\n                )\n\n        instance = DeconstructableClass('arg1_value', 'arg2_value')\n        self.assertSerializedResultEqual(\n            instance,\n            (\n                \"migrations.test_writer.DeconstructableClass('arg1_value', 'arg2_value')\",\n                {'import migrations.test_writer'}\n            )\n        )\n        serialized_instance = self.serialize_round_trip(instance)\n        self.assertEqual(serialized_instance.arg1, 'arg1_value')\n        self.assertEqual(serialized_instance.arg2, 'arg2_value')\n", "    def test_serialize_model_field(self):\n        \"\"\"\n        Tests that a model field with custom deconstruction can be serialized and deserialized correctly.\n        \"\"\"\n        class CustomField(models.CharField):\n                name, path, args, kwargs = super().deconstruct()\n                kwargs['custom'] = 'value'\n                return name, path, args, kwargs\n\n        field = CustomField(max_length=255)\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(\n            string,\n            \"migrations.test_writer.CustomField(max_length=255, custom='value')\"\n        )\n        self.serialize_round_trip(field)\n", "    def test_serialize_deconstructable_serializer(self):\n        @deconstructible\n        class CustomClass:\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n                return (\n                    'migrations.test_writer.CustomClass',\n                    [self.arg1, self.arg2],\n                    {}\n                )\n        \n        instance = CustomClass(1, \"test\")\n        self.assertSerializedResultEqual(\n            instance,\n            (\"migrations.test_writer.CustomClass(1, 'test')\", {'import migrations.test_writer'})\n        )\n"], "sample_73": ["    def test_load_manifest_invalid_version(self):\n        \"\"\"\n        Test that an invalid manifest version raises an error.\n        \"\"\"\n        class InvalidVersionManifestStorage(ManifestFilesMixin, StaticFilesStorage):\n                return '{\"version\": \"invalid\", \"paths\": {}}'\n\n        storage = InvalidVersionManifestStorage()\n        with self.assertRaises(ValueError):\n            storage.load_manifest()\n", "    def test_load_manifest_invalid_json(self):\n        storage_obj = ManifestFilesMixin()\n        storage_obj.read_manifest = lambda: '{\"invalid_json\":'  # Mock invalid JSON\n\n        with self.assertRaises(ValueError):\n            storage_obj.load_manifest()\n", "    def test_hashed_name_with_non_existent_file(self):\n        \"\"\"\n        Test that hashed_name raises a ValueError when a non-existent file is passed.\n        \"\"\"\n        with self.assertRaisesMessage(ValueError, \"The file 'nonexistent.png' could not be found with\"):\n            storage.staticfiles_storage.hashed_name('nonexistent.png')\n", "    def setUp(self):\n        super().setUp()\n        self.temp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.temp_dir)\n", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.manifest_name = 'staticfiles.json'\n        self.storage = storage.ManifestFilesMixin(location=self.temp_dir)\n"], "sample_75": ["    def test_invalid_foreign_key_value(self):\n        \"\"\"\n        Ensure that assigning an invalid foreign key value raises a validation error.\n        \"\"\"\n        author = Author.objects.create(name='InvalidAuthor')\n        # Setting an invalid book id (one that does not exist)\n        author.first_book_id = 9999\n        with self.assertRaises(ValidationError) as cm:\n            author.full_clean()  # Trigger the validation\n        \n        self.assertEqual(\n            cm.exception.message_dict,\n            {'first_book': ['Book instance with id 9999 does not exist.']}\n        )\n", "    def test_swappable_setting(self):\n        \"\"\"\n        Test the swappable setting functionality of ForeignKey and OneToOneField.\n        \"\"\"\n        from django.conf import settings\n\n        # Create a mock model to test swappable settings\n        class MockModel(models.Model):\n            name = models.CharField(max_length=255)\n\n        # Define a ForeignKey with swappable setting\n        fk = ForeignKey(\n            to='MockModel',\n            on_delete=models.CASCADE,\n            swappable=True\n        )\n        self.assertEqual(\n            fk.swappable_setting, \n            settings.SWAPPABLE_MODEL_NAME % 'MockModel'\n        )\n\n        # Define a OneToOneField with swappable setting\n        o2o = OneToOneField(\n            to='MockModel',\n            on_delete=models.CASCADE,\n            swappable=True\n        )\n        self.assertEqual(\n            o2o.swappable_setting, \n            settings.SWAPPABLE_MODEL_NAME % 'MockModel'\n        )\n", "    def test_related_name_is_valid(self):\n        class TestModel(models.Model):\n            test_field = ForeignKey('self', on_delete=models.CASCADE, related_name='invalid name')\n\n        field = TestModel._meta.get_field('test_field')\n        errors = field.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'fields.E306')\n", "    def test_related_name_is_valid(self):\n        field = RelatedField()\n        field.remote_field = mock.Mock()\n        field.remote_field.related_name = 'invalid-name'\n        field.model = mock.Mock()\n        field.model._meta.object_name = 'TestModel'\n        field.name = 'test_field'\n        errors = field._check_related_name_is_valid()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'fields.E306')\n", "    def setUp(self):\n        self.book1 = Book.objects.create(title='Book1')\n        self.book2 = Book.objects.create(title='Book2')\n        self.author1 = Author.objects.create(name='Author1', first_book=self.book1)\n        self.author2 = Author.objects.create(name='Author2', first_book=self.book2)\n"], "sample_74": ["    def test_no_user_host_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'password': 'somepassword',\n            }), (\n                ['psql', 'dbname'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_no_host_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_no_host_no_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_no_host_and_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_no_host_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n"], "sample_76": ["def test_language_code_in_languages(self):\n    # Test when LANGUAGE_CODE is in LANGUAGES\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English'), ('fr', 'French')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    # Test when LANGUAGE_CODE is not in LANGUAGES\n    with self.settings(LANGUAGE_CODE='de', LANGUAGES=[('en', 'English'), ('fr', 'French')]):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(\n                'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.',\n                id='translation.E004'\n            ),\n        ])\n", "    def test_consistent_language_settings(self):\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English'), ('fr', 'French')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English'), ('fr', 'French')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings(self):\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English'), ('fr', 'French')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings(self):\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n    with self.settings(LANGUAGE_CODE='fr-CA', LANGUAGES=[('fr-CA', 'French (Canada)')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n    with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_77": ["    def test_avoid_wrapping(self):\n        items = (\n            ('Hello world', 'Hello\\xa0world'),\n            ('No wrapping here', 'No\\xa0wrapping\\xa0here'),\n            ('Check non-breaking spaces', 'Check\\xa0non-breaking\\xa0spaces'),\n            ('Django & Python', 'Django\\xa0&\\xa0Python'),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        tests = (\n            (\"hello world\", \"hello\\xa0world\"),\n            (\"keep this together\", \"keep\\xa0this\\xa0together\"),\n            (\"multiple words to test\", \"multiple\\xa0words\\xa0to\\xa0test\"),\n            (\"no wrapping here\", \"no\\xa0wrapping\\xa0here\"),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            ('Hello world', 'Hello\\xa0world'),\n            ('Avoid wrapping', 'Avoid\\xa0wrapping'),\n            ('This is a test', 'This\\xa0is\\xa0a\\xa0test'),\n            ('Non breaking spaces', 'Non\\xa0breaking\\xa0spaces'),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        tests = (\n            ('Hello world', 'Hello\\xa0world'),\n            ('Multiple spaces test', 'Multiple\\xa0spaces\\xa0test'),\n            ('Preserve\\twhitespace', 'Preserve\\twhitespace'),\n            ('Newline\\ncharacter', 'Newline\\ncharacter'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            ('no wrapping', 'no wrapping'),\n            ('avoid text wrapping', 'avoid\\xa0text\\xa0wrapping'),\n            ('multiple  spaces', 'multiple\\xa0\\xa0spaces'),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.assertEqual(avoid_wrapping(value), output)\n                self.assertEqual(avoid_wrapping(lazystr(value)), output)\n"], "sample_78": ["    def test_no_translations_decorator(self):\n        \"\"\"\n        Test the no_translations decorator to ensure it deactivates translations\n        during the execution of the decorated function and restores it afterwards.\n        \"\"\"\n        @no_translations\n            self.assertIsNone(translation.get_language())\n\n        current_locale = translation.get_language()\n        with translation.override('de'):\n            dummy_command()\n            self.assertEqual(translation.get_language(), 'de')\n        self.assertEqual(translation.get_language(), current_locale)\n", "def test_command_parser_missing_args_message(self):\n        \"\"\"\n        Test that CommandParser raises an error with a custom missing_args_message\n        if no arguments are provided.\n        \"\"\"\n        parser = CommandParser(missing_args_message=\"Missing required arguments.\")\n        with self.assertRaises(CommandError) as cm:\n            parser.parse_args([])\n        self.assertEqual(str(cm.exception), \"Error: Missing required arguments.\")\n", "    def test_command_error_handling(self):\n        class ErrorCommand(BaseCommand):\n                raise CommandError(\"This is a test error\")\n\n        with self.assertRaises(CommandError) as cm:\n            management.call_command(ErrorCommand())\n        self.assertEqual(str(cm.exception), \"This is a test error\")\n", "def test_handle_default_options(self):\n        \"\"\"\n        Test the `handle_default_options` function to ensure it correctly sets\n        DJANGO_SETTINGS_MODULE and inserts pythonpath into sys.path.\n        \"\"\"\n        class Options:\n            settings = 'test_settings'\n            pythonpath = '/test/path'\n\n        options = Options()\n        original_env = os.environ.copy()\n        original_sys_path = sys.path.copy()\n\n        try:\n            handle_default_options(options)\n            self.assertEqual(os.environ['DJANGO_SETTINGS_MODULE'], 'test_settings')\n            self.assertIn('/test/path', sys.path)\n        finally:\n            os.environ.clear()\n            os.environ.update(original_env)\n            sys.path = original_sys_path\n", "    def test_output_wrapper_style_func(self):\n        \"\"\"Test the OutputWrapper's style_func property.\"\"\"\n        out = StringIO()\n        wrapper = OutputWrapper(out)\n        \n        # Test default style_func\n        self.assertEqual(wrapper.style_func(\"test\"), \"test\")\n        \n        # Test setting a new style_func\n        wrapper.style_func = lambda x: x.upper()\n        self.assertEqual(wrapper.style_func(\"test\"), \"TEST\")\n        \n        # Test setting style_func to None\n        wrapper.style_func = None\n        self.assertEqual(wrapper.style_func(\"test\"), \"test\")\n"], "sample_79": ["    def test_non_string_types(self):\n        self.assertEqual(pluralize(None), 's')\n        self.assertEqual(pluralize(True), '')\n        self.assertEqual(pluralize(False), 's')\n        self.assertEqual(pluralize({}, 'es'), 'es')\n        self.assertEqual(pluralize([], 'es'), 'es')\n", "    def test_invalid_arguments(self):\n        self.assertEqual(pluralize(1, 'es,error'), '')\n        self.assertEqual(pluralize('invalid', 'y,es'), '')\n        self.assertEqual(pluralize(None, 'y,es'), 'es')\n", "    def test_add(self):\n        self.check_values(\n            (('5', 3), '8'),\n            (('5', '3'), '8'),\n            (('5', 'a'), '5a'),\n            (('a', '3'), 'a3'),\n            (('a', 'b'), 'ab'),\n            (('5', None), ''),\n        )\n", "    def test_filesizeformat(self):\n        self.assertEqual(filesizeformat(0), '0 bytes')\n        self.assertEqual(filesizeformat(1023), '1023 bytes')\n        self.assertEqual(filesizeformat(1024), '1.0 KB')\n        self.assertEqual(filesizeformat(1048576), '1.0 MB')\n        self.assertEqual(filesizeformat(1073741824), '1.0 GB')\n        self.assertEqual(filesizeformat(1099511627776), '1.0 TB')\n        self.assertEqual(filesizeformat(1125899906842624), '1.0 PB')\n        self.assertEqual(filesizeformat(-1024), '-1.0 KB')\n        self.assertEqual(filesizeformat(\"invalid\"), '0 bytes')\n", "def test_invalid_suffix(self):\n    self.check_values(('word', 'item'), ('', 'item'), (None, 'item'))\n"], "sample_82": ["    def test_render_with_initial_value(self):\n        \"\"\"\n        Test rendering with an initial value set.\n        \"\"\"\n        widget = SelectDateWidget(years=('2020', '2021', '2022'))\n        self.check_html(widget, 'mydate', '2021-05-15', html=(\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\" selected>May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\" selected>15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"19\">19</option>\n                <option value=\"20\">20</option>\n                <option value=\"21\">21</option>\n                <option value=\"22\">22</option>\n                <option value=\"23\">23</option>\n                <option", "    def test_render_with_initial_value(self):\n        \"\"\"\n        Test rendering the widget with an initial value set.\n        \"\"\"\n        initial_widget = SelectDateWidget(\n            years=('2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016'),\n        )\n        self.check_html(initial_widget, 'mydate', '2012-05-10', html=(\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\" selected>May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\" selected>10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"19\">19</option>\n                <option value=\"20\">20</option>\n               ", "def test_render_with_existing_value(self):\n        \"\"\"\n        Rendering the widget with an existing value should pre-select the correct options.\n        \"\"\"\n        self.check_html(self.widget, 'mydate', '2009-05-20', html=(\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\" selected>May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"19\">19</option>\n                <option value=\"20\" selected>20</option>\n                <option value=\"21\">21</option>\n                <option value=\"22\">22</option>\n                <option value=\"23\">23</option>\n                <option value=\"24\">24</option>\n                <option value=\"25", "def test_render_with_initial_value(self):\n    widget = SelectDateWidget(years=('2015', '2016', '2017'))\n    self.check_html(widget, 'mydate', date(2016, 5, 20), html=(\n        \"\"\"\n        <select name=\"mydate_month\" id=\"id_mydate_month\">\n            <option value=\"\">---</option>\n            <option value=\"1\">January</option>\n            <option value=\"2\">February</option>\n            <option value=\"3\">March</option>\n            <option value=\"4\">April</option>\n            <option value=\"5\" selected>May</option>\n            <option value=\"6\">June</option>\n            <option value=\"7\">July</option>\n            <option value=\"8\">August</option>\n            <option value=\"9\">September</option>\n            <option value=\"10\">October</option>\n            <option value=\"11\">November</option>\n            <option value=\"12\">December</option>\n        </select>\n        \n        <select name=\"mydate_day\" id=\"id_mydate_day\">\n            <option value=\"\">---</option>\n            <option value=\"1\">1</option>\n            <option value=\"2\">2</option>\n            <option value=\"3\">3</option>\n            <option value=\"4\">4</option>\n            <option value=\"5\">5</option>\n            <option value=\"6\">6</option>\n            <option value=\"7\">7</option>\n            <option value=\"8\">8</option>\n            <option value=\"9\">9</option>\n            <option value=\"10\">10</option>\n            <option value=\"11\">11</option>\n            <option value=\"12\">12</option>\n            <option value=\"13\">13</option>\n            <option value=\"14\">14</option>\n            <option value=\"15\">15</option>\n            <option value=\"16\">16</option>\n            <option value=\"17\">17</option>\n            <option value=\"18\">18</option>\n            <option value=\"19\">19</option>\n            <option value=\"20\" selected>20</option>\n            <option value=\"21\">21</option>\n            <option value=\"22\">22</option>\n            <option value=\"23\">23</option>\n            <option value=\"24\">24</option>\n            <", "    def test_render_empty_with_custom_attrs(self):\n        widget = SelectDateWidget(attrs={'class': 'custom', 'data-custom': 'value'})\n        self.check_html(widget, 'mydate', '', html=(\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\" class=\"custom\" data-custom=\"value\">\n                <option selected value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\" class=\"custom\" data-custom=\"value\">\n                <option selected value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"19\">19</option>\n                <option value=\"20\">20</option>\n                <option value=\"21\">21</option>\n                <option value=\"22\">22</option>\n                <option value=\"23\">23</option>\n                <option value=\"24"], "sample_81": ["    def test_match(self):\n            pass\n        pattern = URLPattern(RegexPattern(r'^foo/(?P<param>\\d+)/$'), dummy_view)\n        match = pattern.resolve('/foo/123/')\n        self.assertIsNotNone(match)\n        self.assertEqual(match.func, dummy_view)\n        self.assertEqual(match.kwargs, {'param': '123'})\n", "    def setUp(self):\n        self.pattern = RegexPattern(r'^test/')\n        self.urlconf_name = 'urlpatterns.path_urls'\n        self.resolver = URLResolver(self.pattern, self.urlconf_name)\n", "    def test_repr(self):\n        pattern = RegexPattern('^foo/$')\n        callback = lambda request: None\n        url_pattern = URLPattern(pattern, callback, name=\"foo\")\n        self.assertEqual(repr(url_pattern), \"<URLPattern 'foo/'>\")\n        ", "    def setUp(self):\n        from django.urls import path\n\n            pass\n\n        self.urlpatterns = [\n            path('test/', test_view, name='test_view'),\n            path('nested/', include([\n                path('test/', test_view, name='nested_test_view'),\n            ], namespace='nested')),\n        ]\n        self.resolver = URLResolver(RegexPattern(r'^/'), self.urlpatterns)\n", "    def test_resolve(self):\n            return \"Hello, World!\"\n\n        pattern = RoutePattern('hello/')\n        url_pattern = URLPattern(pattern, my_view, name='hello')\n\n        match = url_pattern.resolve('hello/')\n        self.assertIsNotNone(match)\n        self.assertEqual(match.func, my_view)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {})\n        self.assertEqual(match.url_name, 'hello')\n        self.assertEqual(match.route, 'hello/')\n"], "sample_83": ["    def test_import_library_valid(self):\n        # Mocking a module with a 'register' attribute\n        module_name = 'mock_module'\n        with self.modify_settings(INSTALLED_APPS={'append': 'tests.mock_module'}):\n            module = import_library(module_name)\n        self.assertTrue(hasattr(module, 'filters'))\n        self.assertTrue(hasattr(module, 'tags'))\n", "def test_tag_wrapped(self):\n    @self.library.tag\n    @functools.lru_cache(maxsize=32)\n        return Node()\n    func_wrapped = self.library.tags['func'].__wrapped__\n    self.assertIs(func_wrapped, func)\n    self.assertTrue(hasattr(func_wrapped, 'cache_info'))\n", "    def test_import_library_valid(self):\n        # Create a mock module with a 'register' attribute\n        import types\n        module = types.ModuleType('test_module')\n        module.register = Library()\n        sys.modules['test_module'] = module\n\n        try:\n            library = import_library('test_module')\n            self.assertIsInstance(library, Library)\n        finally:\n            del sys.modules['test_module']\n", "    def test_import_library_valid(self):\n        with self.modify_settings(INSTALLED_APPS={'append': 'django.template.defaulttags'}):\n            library = import_library('django.template.defaulttags')\n            self.assertIsInstance(library, Library)\n", "    def test_import_library_success(self):\n        module_code = \"\"\""], "sample_85": ["    def test_related_field_check_methods(self):\n        \"\"\"\n        Test the check methods of RelatedField to ensure proper validation.\n        \"\"\"\n        class RelatedModel(models.Model):\n            name = models.CharField(max_length=100)\n        \n        class TestModel(models.Model):\n            related_field = models.ForeignKey(RelatedModel, on_delete=models.CASCADE, related_name='+' * 10)\n        \n        field = TestModel._meta.get_field('related_field')\n        errors = field.check()\n        \n        self.assertIn(\n            checks.Error(\n                \"The name '++++++++++' is invalid related_name for field TestModel.related_field\",\n                hint=\"Related name must be a valid Python identifier or end with a '+'\",\n                obj=field,\n                id='fields.E306',\n            ),\n            errors\n        )\n\n        class TestModelInvalidQueryName(models.Model):\n            related_field = models.ForeignKey(RelatedModel, on_delete=models.CASCADE, related_query_name='invalid_query_name_')\n        \n        field_invalid_query = TestModelInvalidQueryName._meta.get_field('related_field')\n        errors_invalid_query = field_invalid_query.check()\n        \n        self.assertIn(\n            checks.Error(\n                \"Reverse query name 'invalid_query_name_' must not end with an underscore.\",\n                hint=\"Add or change a related_name or related_query_name argument for this field.\",\n                obj=field_invalid_query,\n                id='fields.E308',\n            ),\n            errors_invalid_query\n        )\n\n        class TestModelInvalidLookupSep(models.Model):\n            related_field = models.ForeignKey(RelatedModel, on_delete=models.CASCADE, related_query_name='invalid__query__name')\n        \n        field_invalid_lookup_sep = TestModelInvalidLookupSep._meta.get_field('related_field')\n        errors_invalid_lookup_sep = field_invalid_lookup_sep.check()\n\n        self.assertIn(\n            checks.Error(\n                \"Reverse query name 'invalid__query__name' must not contain '__'.\",\n                hint=\"Add or change a related_name or related_query_name argument for this field.\",\n                obj=field_invalid_lookup_sep,\n                id='fields.E309',\n            ),\n            errors_invalid_lookup_sep\n        )\n\n        class TestModelMissingRelated(models.Model):\n            related_field = models.ForeignKey('nonexistentapp.RelatedModel', on_delete=models.CASCADE)\n        \n        field_missing_related = TestModelMissingRelated._meta.get_field('related_field')\n        errors_missing_related = field_missing_related.check()\n        \n        self.assertIn(\n            checks.Error(\n                \"Field defines a relation with model 'nonexistent", "    def test_related_name_validity(self):\n        class TestModel(models.Model):\n            related_field = models.ForeignKey('self', on_delete=models.CASCADE, related_name='valid_related_name')\n        field = TestModel._meta.get_field('related_field')\n        self.assertEqual(field._check_related_name_is_valid(), [])\n", "    def test_related_name_is_valid(self):\n        field = models.ForeignKey('self', on_delete=models.CASCADE, related_name='valid_related_name')\n        self.assertEqual(field._check_related_name_is_valid(), [])\n", "    def setUp(self):\n        self.model_a = A.objects.create(name=\"TestA\")\n        self.model_r = R.objects.create()\n", "    def test_foreign_object_initialization(self):\n        with self.assertRaises(ValueError):\n            ForeignObject(to='InvalidModel', on_delete=models.CASCADE, from_fields=['self'], to_fields=['id'])\n        with self.assertRaises(ValueError):\n            ForeignObject(to='self', on_delete=models.CASCADE, from_fields=[], to_fields=['id'])\n"], "sample_84": ["    def test_valid_dates(self):\n        dates = [\n            ('Sun, 06 Nov 1994 08:49:37 GMT', 784111777),\n            ('Sunday, 06-Nov-94 08:49:37 GMT', 784111777),\n            ('Sun Nov  6 08:49:37 1994', 784111777),\n        ]\n        for date, expected in dates:\n            with self.subTest(date=date):\n                self.assertEqual(parse_http_date_safe(date), expected)\n", "    def test_valid_dates(self):\n        valid_dates = [\n            ('Sun, 06 Nov 1994 08:49:37 GMT', datetime(1994, 11, 6, 8, 49, 37)),\n            ('Sunday, 06-Nov-94 08:49:37 GMT', datetime(1994, 11, 6, 8, 49, 37)),\n            ('Sun Nov  6 08:49:37 1994', datetime(1994, 11, 6, 8, 49, 37))\n        ]\n        for date_str, expected_date in valid_dates:\n            with self.subTest(date_str=date_str):\n                parsed = parse_http_date_safe(date_str)\n                self.assertEqual(datetime.utcfromtimestamp(parsed), expected_date)\n", "    def test_basic_parsing(self):\n        self.assertEqual(limited_parse_qsl('a=1&b=2'), [('a', '1'), ('b', '2')])\n        ", "    def test_unquote_space(self):\n        self.assertEqual(urlunquote('hello%20world'), 'hello world')\n", "    def test_parse_http_date_safe_valid(self):\n        valid_dates = [\n            'Sun, 06 Nov 1994 08:49:37 GMT',  # RFC1123\n            'Sunday, 06-Nov-94 08:49:37 GMT',  # RFC850\n            'Sun Nov  6 08:49:37 1994',  # ASCTIME\n        ]\n        for date in valid_dates:\n            with self.subTest(date=date):\n                parsed = parse_http_date_safe(date)\n                self.assertIsNotNone(parsed)\n                self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(1994, 11, 6, 8, 49, 37))\n"], "sample_88": ["    def test_emailmessage_custom_headers(self):\n        \"\"\"\n        Test that custom headers are correctly included in the email message.\n        \"\"\"\n        custom_headers = {\n            'X-Custom-Header': 'CustomHeaderValue',\n            'X-Another-Header': 'AnotherValue',\n        }\n        email = EmailMessage(\n            'Subject', 'Content', 'from@example.com', ['to@example.com'],\n            headers=custom_headers\n        )\n        message = email.message()\n        self.assertEqual(message['X-Custom-Header'], 'CustomHeaderValue')\n        self.assertEqual(message['X-Another-Header'], 'AnotherValue')\n", "    def test_safe_mime_text_with_long_lines(self):\n        \"\"\"\n        Test that SafeMIMEText correctly handles long lines by using\n        Quoted-Printable encoding for UTF-8 content when necessary.\n        \"\"\"\n        long_body = 'A' * (RFC5322_EMAIL_LINE_LENGTH_LIMIT + 1)\n        email = EmailMessage('Subject', long_body, 'from@example.com', ['to@example.com'])\n        message = email.message()\n        self.assertEqual(message['Subject'], 'Subject')\n        self.assertEqual(message['From'], 'from@example.com')\n        self.assertEqual(message['To'], 'to@example.com')\n        self.assertMessageHasHeaders(message, {\n            ('MIME-Version', '1.0'),\n            ('Content-Type', 'text/plain; charset=\"utf-8\"'),\n            ('Content-Transfer-Encoding', 'quoted-printable'),\n        })\n        self.assertTrue(len(message.get_payload(decode=True)) > RFC5322_EMAIL_LINE_LENGTH_LIMIT)\n", "    def test_multiple_attachments(self):\n        \"\"\"\n        Test email with multiple attachments of various types.\n        \"\"\"\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        email.attach('file1.txt', 'Text content 1', 'text/plain')\n        email.attach('file2.html', '<p>HTML content</p>', 'text/html')\n        email.attach('file3.png', b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00', 'image/png')\n\n        self.assertEqual(len(email.attachments), 3)\n\n        email.send()\n        attachments = self.get_decoded_attachments(email)\n\n        self.assertEqual(len(attachments), 3)\n        self.assertEqual(attachments[0][0], 'file1.txt')\n        self.assertEqual(attachments[0][1], b'Text content 1')\n        self.assertEqual(attachments[0][2], 'text/plain')\n\n        self.assertEqual(attachments[1][0], 'file2.html')\n        self.assertEqual(attachments[1][1], b'<p>HTML content</p>')\n        self.assertEqual(attachments[1][2], 'text/html')\n\n        self.assertEqual(attachments[2][0], 'file3.png')\n        self.assertEqual(attachments[2][1], b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00')\n        self.assertEqual(attachments[2][2], 'image/png')\n", "    def test_attach_file_with_invalid_mimetype(self):\n        \"\"\"\n        Attach a file with an invalid mimetype and ensure it falls back to\n        the default attachment mimetype.\n        \"\"\"\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n            tmp_file.write(b'file content')\n            tmp_file_path = tmp_file.name\n\n        try:\n            email.attach_file(tmp_file_path, mimetype='invalid/mimetype')\n            self.assertEqual(len(email.attachments), 1)\n            filename, content, mimetype = self.get_decoded_attachments(email)[0]\n            self.assertEqual(filename, Path(tmp_file_path).name)\n            self.assertEqual(content, b'file content')\n            self.assertEqual(mimetype, 'application/octet-stream')\n        finally:\n            os.remove(tmp_file_path)\n", "    def test_sanitize_address_invalid_domain(self):\n        \"\"\"\n        Test that sanitize_address raises ValueError for addresses with invalid domain parts.\n        \"\"\"\n        invalid_addresses = [\n            'user@invalid_domain',\n            'user@.com',\n            'user@com.',\n            'user@com..com',\n            'user@-example.com',\n            'user@example-.com'\n        ]\n        for address in invalid_addresses:\n            with self.assertRaises(ValueError):\n                sanitize_address(address, encoding='utf-8')\n"], "sample_87": ["    def test_ensure_echo_on(self, mocked_isatty, mocked_termios):\n        mocked_termios.tcgetattr.return_value = [None, None, None, 0]\n        autoreload.ensure_echo_on()\n        # Ensure tcgetattr was called on sys.stdin\n        mocked_termios.tcgetattr.assert_called_once_with(sys.stdin)\n        # Ensure tcsetattr was not called since echo was already on\n        mocked_termios.tcsetattr.assert_not_called()\n", "    def test_ensure_echo_on_disabled(self, mock_isatty, mock_termios):\n        # Simulate echo being disabled\n        mock_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        mock_termios.ECHO = 8  # Assume echo is the 4th bit set to 8\n        mock_termios.tcgetattr.return_value[3] = 0\n\n        autoreload.ensure_echo_on()\n\n        mock_termios.tcgetattr.assert_called_once_with(sys.stdin)\n        self.assertTrue(mock_termios.tcgetattr.return_value[3] & mock_termios.ECHO)\n        mock_termios.tcsetattr.assert_called_once_with(sys.stdin, mock_termios.TCSANOW, mock_termios.tcgetattr.return_value)\n", "    def test_echo_already_on(self, mocked_isatty, mocked_termios):\n        # Mock tcgetattr to return attributes with ECHO already set\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, mocked_termios.ECHO]\n        autoreload.ensure_echo_on()\n        # tcsetattr should not be called since ECHO is already on\n        mocked_termios.tcsetattr.assert_not_called()\n", "    def test_echo_on_enabled(self, mocked_isatty, mocked_termios):\n        # Simulate echo already enabled\n        mocked_termios.tcgetattr.return_value = [None, None, None, mocked_termios.ECHO]\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_enabled(self, mocked_isatty, mocked_termios):\n        mocked_termios.tcgetattr.return_value = [None, None, None, 0b00000000]\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcsetattr.called)\n        self.assertEqual(mocked_termios.tcsetattr.call_args[0][2][3] & mocked_termios.ECHO, mocked_termios.ECHO)\n"], "sample_89": ["    def test_ensure_echo_on(self, mock_isatty, mock_termios):\n        # Mock the termios attributes\n        mock_termios.TCSANOW = mock.MagicMock()\n        mock_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        mock_termios.ECHO = 8\n\n        # Call the function\n        autoreload.ensure_echo_on()\n\n        # Check if the attributes were set correctly\n        self.assertTrue(mock_termios.tcsetattr.called)\n        args = mock_termios.tcsetattr.call_args[0]\n        self.assertEqual(args[2][3] & mock_termios.ECHO, mock_termios.ECHO)\n", "    def test_ensure_echo_on_no_tty(self, mocked_tcsetattr, mocked_tcgetattr, mocked_isatty):\n        autoreload.ensure_echo_on()\n        mocked_isatty.assert_called_once()\n        mocked_tcgetattr.assert_not_called()\n        mocked_tcsetattr.assert_not_called()\n", "    def test_echo_mode_enabled(self, mocked_stdin, mocked_termios):\n        # Simulate the stdin being a tty and echo mode being off\n        mocked_stdin.isatty.return_value = True\n        term_attr = [0, 0, 0, 0]\n        term_attr[3] = term_attr[3] & ~mocked_termios.ECHO\n        mocked_termios.tcgetattr.return_value = term_attr\n\n        autoreload.ensure_echo_on()\n\n        # Check if ECHO flag was added back\n        term_attr[3] |= mocked_termios.ECHO\n        mocked_termios.tcsetattr.assert_called_with(mocked_stdin, mocked_termios.TCSANOW, term_attr)\n", "    def test_ensure_echo_on_enabled(self, mocked_isatty, mocked_termios):\n        # Mock termios to simulate echo being disabled\n        attrs = [0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attrs\n        attrs[3] = 0  # Disable ECHO\n\n        autoreload.ensure_echo_on()\n\n        # Ensure that ECHO is enabled\n        self.assertTrue(attrs[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called_once_with(\n            sys.stdin, mocked_termios.TCSANOW, attrs\n        )\n", "    def test_ensure_echo_on_enabled(self, mocked_stdin, mocked_termios):\n        mocked_stdin.isatty.return_value = True\n        attrs = [0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_termios.tcsetattr.called)\n"], "sample_90": ["    def test_construct_instance_excludes_non_editable_fields(self):\n        \"\"\"\n        Test that construct_instance excludes non-editable fields from being \n        set on the model instance.\n        \"\"\"\n        class NonEditableModel(models.Model):\n            name = models.CharField(max_length=50)\n            created_at = models.DateTimeField(auto_now_add=True)\n\n        class NonEditableModelForm(forms.ModelForm):\n            class Meta:\n                model = NonEditableModel\n                fields = '__all__'\n\n        form_data = {'name': 'Test'}\n        form = NonEditableModelForm(data=form_data)\n        self.assertTrue(form.is_valid())\n        instance = NonEditableModel()\n        instance = construct_instance(form, instance)\n        \n        # Ensure non-editable field is not set by the form\n        self.assertIsNone(instance.created_at)\n        self.assertEqual(instance.name, 'Test')\n", "    def test_model_form_options_initialization(self):\n        class Meta:\n            model = Article\n            fields = ['headline', 'pub_date']\n            exclude = ['slug']\n            widgets = {'headline': forms.Textarea}\n            localized_fields = ['headline']\n            labels = {'headline': 'Article Title'}\n            help_texts = {'headline': 'Enter the title of the article.'}\n            error_messages = {'headline': {'required': 'This field is required.'}}\n            field_classes = {'headline': forms.CharField}\n\n        options = ModelFormOptions(Meta)\n        self.assertEqual(options.model, Article)\n        self.assertEqual(options.fields, ['headline', 'pub_date'])\n        self.assertEqual(options.exclude, ['slug'])\n        self.assertEqual(options.widgets, {'headline': forms.Textarea})\n        self.assertEqual(options.localized_fields, ['headline'])\n        self.assertEqual(options.labels, {'headline': 'Article Title'})\n        self.assertEqual(options.help_texts, {'headline': 'Enter the title of the article.'})\n        self.assertEqual(options.error_messages, {'headline': {'required': 'This field is required.'}})\n        self.assertEqual(options.field_classes, {'headline': forms.CharField})\n", "    def setUp(self):\n        self.char = Character.objects.create(username='user1', last_action=datetime.datetime.today())\n        self.post = Post.objects.create(title='Test Title', slug='test-title', subtitle='Test Subtitle', posted=datetime.date.today())\n", "    def test_modelform_options_initialization(self):\n        class MetaOptions:\n            model = Article\n            fields = ['headline', 'slug']\n            exclude = ['pub_date']\n            widgets = {'headline': forms.Textarea}\n            localized_fields = ['headline']\n            labels = {'headline': 'Article Headline'}\n            help_texts = {'headline': 'Enter the headline of the article.'}\n            error_messages = {'headline': {'required': 'Headline is required.'}}\n            field_classes = {'headline': forms.CharField}\n\n        options = ModelFormOptions(MetaOptions)\n        self.assertEqual(options.model, Article)\n        self.assertEqual(options.fields, ['headline', 'slug'])\n        self.assertEqual(options.exclude, ['pub_date'])\n        self.assertEqual(options.widgets, {'headline': forms.Textarea})\n        self.assertEqual(options.localized_fields, ['headline'])\n        self.assertEqual(options.labels, {'headline': 'Article Headline'})\n        self.assertEqual(options.help_texts, {'headline': 'Enter the headline of the article.'})\n        self.assertEqual(options.error_messages, {'headline': {'required': 'Headline is required.'}})\n        self.assertEqual(options.field_classes, {'headline': forms.CharField})\n", "    def test_apply_limit_choices_to_to_formfield(self):\n        \"\"\"Test that limit_choices_to is correctly applied to a formfield's queryset.\"\"\"\n        field = forms.ModelChoiceField(queryset=Writer.objects.all())\n        field.get_limit_choices_to = mock.Mock(return_value={'name__icontains': 'Royko'})\n        apply_limit_choices_to_to_formfield(field)\n        self.assertEqual(str(field.queryset.query), str(Writer.objects.filter(name__icontains='Royko').query))\n"], "sample_91": ["def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n", "    def test_custom_403_template(self):\n        \"\"\"\n        403.html template is picked by the permission_denied handler.\n        \"\"\"\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception(\"Test 403 error\"))\n        self.assertContains(response, \"test template for a 403 error\", status_code=403)\n        self.assertContains(response, \"exception: Test 403 error\", status_code=403)\n", "    def test_permission_denied(self):\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception('Permission denied'))\n        self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n        self.assertIn(b'Permission denied', response.content)\n", "    def test_permission_denied(self):\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception(\"Permission Denied\"))\n        self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n        self.assertIn(b'Permission Denied', response.content)\n", "def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception(\"Custom permission error\"))\n    self.assertEqual(response.status_code, 403)\n    self.assertIn(b'<h1>403 Forbidden</h1>', response.content)\n    self.assertIn(b'Custom permission error', response.content)\n"], "sample_92": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user(username='test', email='test@example.com', password='test')\n        cls.inactive_user = User.objects.create_user(username='inactive', email='inactive@example.com', password='test', is_active=False)\n", "    def setUp(self):\n        self.user = User.objects.create_user('test', 'test@example.com', 'test')\n        self.backend = RemoteUserBackend()\n", "    def setUpTestData(cls):\n        cls.remote_user = 'remoteuser'\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(username=cls.remote_user, email='remote@test.com', is_active=False)\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username=cls.remote_user, email='test_remote@example.com', is_active=False\n        )\n"], "sample_93": ["def test_combined_expression(self):\n    # Testing arithmetic operations with CombinedExpression\n    combined_expr = CombinedExpression(F('age'), CombinedExpression.ADD, Value(5))\n    authors = Author.objects.annotate(combined_age=combined_expr).order_by('pk')\n    self.assertQuerysetEqual(\n        authors, [\n            ('Adrian Holovaty', 39),\n            ('Jacob Kaplan-Moss', 40),\n            ('Brad Dayley', 50),\n            ('James Bennett', 34),\n            ('Jeffrey Forcier', 42),\n            ('Paul Bissex', 34),\n            ('Wesley J. Chun', 30),\n            ('Peter Norvig', 62),\n            ('Stuart Russell', 51),\n        ],\n        lambda a: (a.name, a.combined_age)\n    )\n\n    # Testing bitwise operations with CombinedExpression\n    combined_expr_bitand = CombinedExpression(F('age'), CombinedExpression.BITAND, Value(1))\n    authors = Author.objects.annotate(combined_age=combined_expr_bitand).order_by('pk')\n    self.assertQuerysetEqual(\n        authors, [\n            ('Adrian Holovaty', 0),\n            ('Jacob Kaplan-Moss', 1),\n            ('Brad Dayley', 1),\n            ('James Bennett', 1),\n            ('Jeffrey Forcier', 1),\n            ('Paul Bissex', 1),\n            ('Wesley J. Chun', 1),\n            ('Peter Norvig', 1),\n            ('Stuart Russell', 0),\n        ],\n        lambda a: (a.name, a.combined_age)\n    )\n\n    combined_expr_bitor = CombinedExpression(F('age'), CombinedExpression.BITOR, Value(1))\n    authors = Author.objects.annotate(combined_age=combined_expr_bitor).order_by('pk')\n    self.assertQuerysetEqual(\n        authors, [\n            ('Adrian Holovaty', 35),\n            ('Jacob Kaplan-Moss', 35),\n            ('Brad Dayley', 45),\n            ('James Bennett', 29),\n            ('Jeffrey Forcier', 37),\n            ('Paul Bissex', 29),\n            ('Wesley J. Chun', 25),\n            ('Peter Norvig', 57),\n            ('Stuart Russell', 47),\n        ],\n        lambda a: (a.name, a.combined_age)\n    )\n", "    def test_combined_expression_output_field(self):\n        combined_expr = CombinedExpression(F('age'), CombinedExpression.ADD, F('friends__age'))\n        with self.assertRaises(FieldError):\n            combined_expr.output_field\n\n        combined_expr = CombinedExpression(F('age'), CombinedExpression.ADD, F('friends__age'), output_field=IntegerField())\n        self.assertEqual(combined_expr.output_field, IntegerField())\n", "def test_combinable_expressions(self):\n    # Test arithmetic operations\n    expr1 = F('rating') + F('pages')\n    expr2 = F('rating') - F('pages')\n    expr3 = F('rating') * F('pages')\n    expr4 = F('rating') / F('pages')\n    expr5 = F('rating') % F('pages')\n    expr6 = F('rating') ** 2\n\n    book = Book.objects.annotate(\n        add_expr=expr1,\n        sub_expr=expr2,\n        mul_expr=expr3,\n        div_expr=expr4,\n        mod_expr=expr5,\n        pow_expr=expr6,\n    ).first()\n\n    self.assertEqual(book.add_expr, book.rating + book.pages)\n    self.assertEqual(book.sub_expr, book.rating - book.pages)\n    self.assertEqual(book.mul_expr, book.rating * book.pages)\n    self.assertEqual(book.div_expr, book.rating / book.pages)\n    self.assertEqual(book.mod_expr, book.rating % book.pages)\n    self.assertEqual(book.pow_expr, book.rating ** 2)\n\n    # Test bitwise operations\n    expr7 = F('pages').bitand(128)\n    expr8 = F('pages').bitor(128)\n    expr9 = F('pages').bitleftshift(2)\n    expr10 = F('pages').bitrightshift(2)\n\n    book = Book.objects.annotate(\n        bitand_expr=expr7,\n        bitor_expr=expr8,\n        bitleftshift_expr=expr9,\n        bitrightshift_expr=expr10,\n    ).first()\n\n    self.assertEqual(book.bitand_expr, book.pages & 128)\n    self.assertEqual(book.bitor_expr, book.pages | 128)\n    self.assertEqual(book.bitleftshift_expr, book.pages << 2)\n    self.assertEqual(book.bitrightshift_expr, book.pages >> 2)\n", "    def test_expression_combination_with_case(self):\n        # Test combining expressions with a Case statement\n        books = Book.objects.annotate(\n            combined_expression=Case(\n                When(price__gt=Decimal('50.00'), then=F('pages') * 2),\n                default=F('pages') + 100,\n                output_field=IntegerField(),\n            )\n        )\n        expected_results = [\n            ('The Definitive Guide to Django: Web Development Done Right', 547),\n            ('Sams Teach Yourself Django in 24 Hours', 628),\n            ('Practical Django Projects', 400),\n            ('Python Web Development with Django', 450),\n            ('Artificial Intelligence: A Modern Approach', 2264),\n            ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 1892),\n        ]\n        self.assertQuerysetEqual(\n            books.order_by('name'),\n            expected_results,\n            lambda b: (b.name, b.combined_expression)\n        )\n", "def test_case_expression(self):\n    \"\"\"\n    Test the Case and When expressions.\n    \"\"\"\n    cases = [\n        When(pages__lt=400, then=Value('Short')),\n        When(pages__gte=400, pages__lt=800, then=Value('Medium')),\n        When(pages__gte=800, then=Value('Long')),\n    ]\n    books = Book.objects.annotate(length_category=Case(*cases, default=Value('Unknown'), output_field=IntegerField()))\n    expected_results = {\n        'The Definitive Guide to Django: Web Development Done Right': 'Medium',\n        'Sams Teach Yourself Django in 24 Hours': 'Medium',\n        'Practical Django Projects': 'Short',\n        'Python Web Development with Django': 'Short',\n        'Artificial Intelligence: A Modern Approach': 'Long',\n        'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp': 'Long'\n    }\n    for book in books:\n        self.assertEqual(book.length_category, expected_results[book.name])\n"], "sample_94": ["    def test_unique_email_field(self):\n        \"\"\"\n        A superuser should be created successfully when using a custom user model\n        with a unique email field.\n        \"\"\"\n        new_io = StringIO()\n        unique_email = 'unique@somewhere.org'\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            email=unique_email,\n            date_of_birth=\"1980-05-01\",\n            first_name='Unique',\n            stdout=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        u = CustomUserWithUniqueEmail._default_manager.get(email=unique_email)\n        self.assertEqual(u.email, unique_email)\n        self.assertEqual(u.first_name, 'Unique')\n\n        # Ensure the created password is unusable\n        self.assertFalse(u.has_usable_password())\n", "    def test_interactive_superuser_creation(self):\n        new_io = StringIO()\n        call_command(\n            'createsuperuser',\n            interactive=True,\n            stdout=new_io,\n            stdin=MockTTY(),\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        user = User.objects.get(username='alice')\n        self.assertEqual(user.email, 'alice@example.com')\n        self.assertTrue(user.check_password('password123'))\n", "    def test_handle_with_invalid_username(self):\n        \"\"\"\n        Test that an invalid username raises a CommandError.\n        \"\"\"\n        new_io = StringIO()\n        invalid_username = 'invalid!username'\n        \n        @mock_inputs({\n            'username': invalid_username,\n            'email': 'test@example.com',\n            'password': 'password123',\n        })\n            with self.assertRaisesMessage(CommandError, \"Enter a valid username.\"):\n                call_command(\n                    'createsuperuser',\n                    interactive=True,\n                    stdin=MockTTY(),\n                    stdout=new_io,\n                    stderr=new_io,\n                )\n        \n        test(self)\n", "    def test_not_running_in_tty_exception(self, mock_stderr, mock_stdout, mock_stdin):\n        # Simulate the command being run in a non-TTY environment.\n        mock_stdin.isatty = lambda: False\n        with self.assertRaises(NotRunningInTTYException):\n            call_command('createsuperuser', interactive=True, stdin=mock_stdin, stdout=mock_stdout, stderr=mock_stderr)\n\n        # Check the expected output\n        self.assertIn(\n            \"Superuser creation skipped due to not running in a TTY. \"\n            \"You can run `manage.py createsuperuser` in your project to create one manually.\",\n            mock_stdout.getvalue()\n        )\n", "    def test_invalid_username_interactive(self):\n        \"\"\"Creation should fail if the username contains invalid characters in interactive mode.\"\"\"\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, 'Enter a valid username.'):\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdin=MockTTY(),\n                stdout=new_io,\n                stderr=new_io,\n            )\n"], "sample_98": ["    def test_broken_pipe_error_handling(self):\n        \"\"\"\n        Simulate a broken pipe error to ensure it is handled gracefully and \n        logged correctly.\n        \"\"\"\n        conn = HTTPConnection(LiveServerErrorHandling.server_thread.host, LiveServerErrorHandling.server_thread.port)\n        try:\n            conn.request('GET', '/example_view/')\n            # Close the connection abruptly to simulate a broken pipe error\n            conn.sock.close()\n\n            with self.assertLogs('django.server', level='INFO') as log:\n                conn.getresponse()\n            self.assertIn(\"Broken pipe from\", log.output[0])\n        except Exception as e:\n            # Expected to raise an exception due to broken pipe\n            self.assertTrue(isinstance(e, (socket.error, ConnectionResetError, BrokenPipeError)))\n        finally:\n            conn.close()\n", "    def test_handle_broken_pipe_error(self):\n        \"\"\"\n        Test that the server handles BrokenPipeError gracefully and logs the correct message.\n        \"\"\"\n        with self.assertLogs('django.server', level='INFO') as log:\n            conn = HTTPConnection(self.server_thread.host, self.server_thread.port, timeout=1)\n            try:\n                conn.request('GET', '/example_view/')\n                # Simulate broken pipe by closing the connection prematurely\n                conn.sock.shutdown(socket.SHUT_RDWR)\n                conn.sock.close()\n            except OSError:\n                pass  # Ignore the OSError caused by the forced shutdown\n            \n            # Check the logs for the broken pipe message\n            self.assertTrue(any(\"Broken pipe from\" in message for message in log.output))\n", "    def test_ipv6_support(self):\n        \"\"\"\n        Test that the WSGIServer can handle IPv6 addresses.\n        \"\"\"\n        server_cls = type('TestWSGIServer', (WSGIServer,), {'address_family': socket.AF_INET6})\n        server = server_cls(('::1', 0), WSGIRequestHandler, ipv6=True)\n        self.assertEqual(server.address_family, socket.AF_INET6)\n        server.server_close()\n", "    def test_broken_pipe_error_handling(self):\n        \"\"\"\n        Test that broken pipe errors are handled gracefully and logged correctly.\n        \"\"\"\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port)\n        try:\n            conn.request('GET', '/example_view/')\n            conn.sock.shutdown(socket.SHUT_RDWR)  # Force a broken pipe error\n            conn.close()\n        except socket.error as e:\n            self.assertEqual(e.errno, errno.EPIPE, \"Expected a broken pipe error.\")\n        finally:\n            conn.close()\n", "    def test_log_message_on_404(self):\n        \"\"\"\n        Ensure that a 404 status code is properly logged by the server.\n        \"\"\"\n        with self.assertLogs(logger, level='INFO') as cm:\n            with self.assertRaises(HTTPError):\n                self.urlopen('/non_existent_view/')\n        self.assertIn('WARNING:django.server:404', cm.output[0])\n"], "sample_96": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 'name'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n"], "sample_99": ["    def test_now_function(self):\n        if settings.USE_TZ:\n            current_time = timezone.now()\n        else:\n            current_time = datetime.now()\n\n        self.create_model(current_time, current_time + timedelta(days=1))\n\n        qs = DTModel.objects.annotate(now=Now()).order_by('start_datetime')\n        self.assertEqual(qs.count(), 1)\n        annotated_model = qs.first()\n        self.assertEqual(annotated_model.start_datetime, current_time)\n        self.assertAlmostEqual(annotated_model.now, current_time, delta=timedelta(seconds=1))\n\n        # PostgreSQL specific check\n        with self.settings(DATABASES={\n            'default': {\n                'ENGINE': 'django.db.backends.postgresql',\n                'NAME': 'test',\n                'USER': 'test',\n                'PASSWORD': 'test',\n                'HOST': 'localhost',\n                'PORT': '',\n            }\n        }):\n            qs = DTModel.objects.annotate(now=Now()).order_by('start_datetime')\n            annotated_model = qs.first()\n            self.assertEqual(annotated_model.start_datetime, current_time)\n            self.assertAlmostEqual(annotated_model.now, current_time, delta=timedelta(seconds=1))\n", "    def test_now_func(self):\n        \"\"\"\n        Tests the Now function to ensure it returns the current timestamp.\n        \"\"\"\n        # Create model instances with different start and end datetimes\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        # Fetch the model instances and annotate with Now function\n        now = timezone.now()\n        qs = DTModel.objects.annotate(current_timestamp=Now())\n        \n        # Ensure current timestamp is within an acceptable range (1 second)\n        for instance in qs:\n            self.assertAlmostEqual(instance.current_timestamp, now, delta=timedelta(seconds=1))\n\n        # Test Now function with PostgreSQL-specific behavior\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT STATEMENT_TIMESTAMP();\")\n            statement_timestamp = cursor.fetchone()[0]\n            self.assertAlmostEqual(now, statement_timestamp, delta=timedelta(seconds=1))\n", "    def test_trunc_func_invalid_kind(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        with self.assertRaisesMessage(ValueError, 'Trunc only valid on DateField, TimeField, or DateTimeField.'):\n            list(DTModel.objects.annotate(truncated=Trunc('start_datetime', 'invalid_kind', output_field=DateTimeField())))\n", "    def test_extract_func_invalid_tzinfo(self):\n        start_datetime = datetime(2015, 6, 15, 23, 30, 1, 321)\n        end_datetime = datetime(2015, 6, 16, 13, 11, 27, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        invalid_tzinfo = \"Invalid/Timezone\"\n        \n        with self.assertRaisesMessage(ValueError, \"Invalid timezone specified\"):\n            DTModel.objects.annotate(\n                day_invalid_tz=Extract('start_datetime', 'day', tzinfo=invalid_tzinfo)\n            ).order_by('start_datetime').get()\n", "    def test_trunc_date_to_timefield(self):\n        \"\"\"\n        Test that attempting to truncate a DateField to a TimeField raises a ValueError.\n        \"\"\"\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        with self.assertRaisesMessage(ValueError, \"Cannot truncate DateField 'start_date' to TimeField\"):\n            list(DTModel.objects.annotate(truncated=Trunc('start_date', 'second', output_field=TimeField())))\n"], "sample_100": ["    def test_ensure_echo_on_no_change_needed(self, mocked_tcsetattr, mocked_tcgetattr, mocked_isatty):\n        autoreload.ensure_echo_on()\n        mocked_tcgetattr.assert_called_once_with(sys.stdin)\n        mocked_tcsetattr.assert_not_called()\n", "    def test_echo_on_already_enabled(self, mocked_termios, mocked_isatty):\n        attrs = [0, 0, 0, mocked_termios.ECHO]\n        mocked_termios.tcgetattr.return_value = attrs\n\n        autoreload.ensure_echo_on()\n        mocked_termios.tcgetattr.assert_called_once_with(sys.stdin)\n        mocked_termios.tcsetattr.assert_not_called()\n", "    def test_ensure_echo_on_enabled(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        attrs = [0, 0, 0, termios.ECHO]\n        mock_tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        self.assertFalse(mock_tcsetattr.called)\n", "    def test_ensure_echo_on(self, mock_isatty, mock_termios):\n        # Mock termios.tcgetattr to return a list where ECHO is off\n        mock_termios.tcgetattr.return_value = [None, None, None, 0]\n        # Call the function\n        autoreload.ensure_echo_on()\n        # Ensure tcsetattr was called to set ECHO on\n        self.assertTrue(mock_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_enabled(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        mock_tcgetattr.return_value = [0, 0, 0, 0x0008]  # ECHO flag is off\n        autoreload.ensure_echo_on()\n        self.assertTrue(mock_tcsetattr.called)\n"], "sample_102": ["    def test_unsupported_bulk_create_on_combined_qs(self):\n        qs = Number.objects.all()\n        msg = \"Calling QuerySet.bulk_create() after %s() is not supported.\"\n        combinators = ['union']\n        if connection.features.supports_select_difference:\n            combinators.append('difference')\n        if connection.features.supports_select_intersection:\n            combinators.append('intersection')\n        for combinator in combinators:\n            with self.subTest(combinator=combinator):\n                with self.assertRaisesMessage(\n                    NotSupportedError,\n                    msg % combinator,\n                ):\n                    getattr(qs, combinator)(qs).bulk_create([Number(num=10, other_num=0)])\n", "    def test_combining_qs_with_different_model_fields(self):\n        ReservedName.objects.create(name='test', order=1)\n        qs1 = Number.objects.filter(num=1).values('num', 'other_num')\n        qs2 = ReservedName.objects.values('name', 'order')\n        # We expect an error because the fields in the combined querysets do not match.\n        with self.assertRaisesMessage(DatabaseError, \"SELECT statements used in a UNION query must have an equal number of columns\"):\n            list(qs1.union(qs2))\n", "    def test_chain_method(self):\n        qs1 = Number.objects.filter(num__lte=1)\n        qs2 = qs1._chain()\n        self.assertEqual(len(qs2), len(qs1))\n        self.assertNumbersEqual(qs2, [0, 1])\n", "    def test_union_with_different_fields(self):\n        qs1 = Number.objects.values('num')\n        qs2 = ReservedName.objects.values('name')\n        union_qs = qs1.union(qs2)\n        results = list(union_qs)\n        expected_results = [{'num': i} for i in range(10)] + [{'name': 'a'}]\n        for result in expected_results:\n            self.assertIn(result, results)\n", "    def test_union_with_annotations(self):\n        qs1 = Number.objects.filter(num=1).annotate(\n            double_num=F('num') * 2,\n        ).values('num', 'double_num')\n        qs2 = Number.objects.filter(num=2).annotate(\n            triple_num=F('num') * 3,\n        ).values('num', 'triple_num')\n        result = qs1.union(qs2).order_by('num')\n        self.assertEqual(\n            list(result),\n            [{'num': 1, 'double_num': 2}, {'num': 2, 'triple_num': 6}]\n        )\n"], "sample_101": ["    def test_wsgi_request_get_scheme(self):\n        \"\"\"\n        Test that WSGIRequest correctly returns the URL scheme.\n        \"\"\"\n        environ = self.request_factory._base_environ()\n        environ['wsgi.url_scheme'] = 'https'\n        \n        request = WSGIRequest(environ)\n        \n        self.assertEqual(request._get_scheme(), 'https')\n", "    def test_wsgi_request_initialization(self):\n        \"\"\"\n        Test the initialization of WSGIRequest and its attributes.\n        \"\"\"\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/test/\",\n            REQUEST_METHOD=\"POST\",\n            CONTENT_LENGTH=\"10\",\n            CONTENT_TYPE=\"application/json\",\n            HTTP_COOKIE=\"cookie1=value1; cookie2=value2\",\n        )\n        environ['wsgi.input'] = BytesIO(b'{\"key\": \"value\"}')\n\n        request = WSGIRequest(environ)\n\n        self.assertEqual(request.path_info, \"/test/\")\n        self.assertEqual(request.method, \"POST\")\n        self.assertEqual(request.META['CONTENT_LENGTH'], \"10\")\n        self.assertEqual(request.META['CONTENT_TYPE'], \"application/json\")\n        self.assertEqual(request.COOKIES, {'cookie1': 'value1', 'cookie2': 'value2'})\n        self.assertEqual(request._stream.read(), b'{\"key\": \"value\"}')\n", "    def test_limited_stream_read(self):\n        \"\"\"\n        LimitedStream.read() correctly limits reading to the specified byte count.\n        \"\"\"\n        data = b\"Hello World!\"\n        stream = BytesIO(data)\n        limited_stream = LimitedStream(stream, limit=5)\n        \n        result = limited_stream.read()\n        \n        self.assertEqual(result, b\"Hello\")\n        self.assertEqual(limited_stream.remaining, 0)\n", "def test_limited_stream_read(self):\n    \"\"\"\n    LimitedStream read method works correctly for different sizes.\n    \"\"\"\n    stream = BytesIO(b\"abcdefghijklmnopqrstuvwxyz\")\n    limited_stream = LimitedStream(stream, limit=10)\n\n    # Read 5 bytes\n    result = limited_stream.read(5)\n    self.assertEqual(result, b\"abcde\")\n\n    # Read 3 more bytes\n    result = limited_stream.read(3)\n    self.assertEqual(result, b\"fgh\")\n\n    # Read the remaining bytes\n    result = limited_stream.read()\n    self.assertEqual(result, b\"ij\")\n\n    # Try to read past the limit\n    result = limited_stream.read()\n    self.assertEqual(result, b\"\")\n", "    def test_limited_stream_read(self):\n        \"\"\"\n        LimitedStream.read() should correctly limit the number of bytes read from the stream.\n        \"\"\"\n        stream = BytesIO(b\"abcdefghijklmnopqrstuvwxyz\")\n        limited_stream = LimitedStream(stream, limit=10)\n\n        # Read the first 5 bytes\n        result = limited_stream.read(5)\n        self.assertEqual(result, b\"abcde\")\n        self.assertEqual(limited_stream.remaining, 5)\n\n        # Read the next 5 bytes (reaching the limit)\n        result = limited_stream.read(5)\n        self.assertEqual(result, b\"fghij\")\n        self.assertEqual(limited_stream.remaining, 0)\n\n        # Attempt to read past the limit\n        result = limited_stream.read(5)\n        self.assertEqual(result, b\"\")\n        self.assertEqual(limited_stream.remaining, 0)\n"], "sample_103": ["    def test_distinct_filter_combination(self):\n        # Test distinct and filter combination with Count aggregate.\n        with self.assertRaisesMessage(ValueError, 'Star cannot be used with filter. Please specify a field.'):\n            Book.objects.aggregate(total=Count('*', filter=Q(rating__gt=4.0)))\n\n        # Test distinct and filter combination with Avg aggregate.\n        avg_price = Book.objects.aggregate(avg_price=Avg('price', distinct=True, filter=Q(rating__gt=4.0)))\n        self.assertEqual(avg_price['avg_price'], Decimal('52.8975'))\n\n        # Test distinct and filter combination with Sum aggregate.\n        sum_pages = Book.objects.aggregate(sum_pages=Sum('pages', distinct=True, filter=Q(rating__gt=4.0)))\n        self.assertEqual(sum_pages['sum_pages'], 447 + 350 + 946)\n", "    def test_stddev_variance(self):\n        # Test sample standard deviation (STDDEV_SAMP)\n        vals = Book.objects.filter(rating__lte=4.5).aggregate(stddev=StdDev('price', sample=True))\n        self.assertAlmostEqual(vals['stddev'], 27.682, places=3)\n\n        # Test population standard deviation (STDDEV_POP)\n        vals = Book.objects.filter(rating__lte=4.5).aggregate(stddev=StdDev('price'))\n        self.assertAlmostEqual(vals['stddev'], 25.327, places=3)\n\n        # Test sample variance (VAR_SAMP)\n        vals = Book.objects.filter(rating__lte=4.5).aggregate(variance=Variance('price', sample=True))\n        self.assertAlmostEqual(vals['variance'], 766.203, places=3)\n\n        # Test population variance (VAR_POP)\n        vals = Book.objects.filter(rating__lte=4.5).aggregate(variance=Variance('price'))\n        self.assertAlmostEqual(vals['variance'], 641.644, places=3)\n", "    def test_variance(self):\n        vals = Author.objects.aggregate(variance=Variance('age'))\n        # Verify that the variance of the authors' ages is approximately correct.\n        self.assertAlmostEqual(vals['variance'], 92.6944, places=4)\n", "def test_custom_stddev_variance(self):\n        # Test custom StdDev (sample and population)\n        vals = Book.objects.filter(rating__lt=4.5).aggregate(\n            stddev_pop=StdDev('price'),\n            stddev_samp=StdDev('price', sample=True)\n        )\n        self.assertAlmostEqual(vals['stddev_pop'], 24.874, places=3)\n        self.assertAlmostEqual(vals['stddev_samp'], 27.038, places=3)\n\n        # Test custom Variance (sample and population)\n        vals = Book.objects.filter(rating__lt=4.5).aggregate(\n            variance_pop=Variance('price'),\n            variance_samp=Variance('price', sample=True)\n        )\n        self.assertAlmostEqual(vals['variance_pop'], 618.747, places=3)\n        self.assertAlmostEqual(vals['variance_samp'], 731.246, places=3)\n", "    def test_stddev_variance(self):\n        # Test StdDev with and without sample\n        vals = Book.objects.aggregate(StdDev('pages'))\n        self.assertAlmostEqual(vals['pages__stddev'], 301.38, places=2)\n        vals = Book.objects.aggregate(StdDev('pages', sample=True))\n        self.assertAlmostEqual(vals['pages__stddev'], 334.99, places=2)\n\n        # Test Variance with and without sample\n        vals = Book.objects.aggregate(Variance('pages'))\n        self.assertAlmostEqual(vals['pages__variance'], 90830.16, places=2)\n        vals = Book.objects.aggregate(Variance('pages', sample=True))\n        self.assertAlmostEqual(vals['pages__variance'], 101394.45, places=2)\n"], "sample_104": ["def test_missing_manifest_entry(self):\n    \"\"\"\n    Test to ensure a ValueError is raised when a file is not found in the manifest.\n    \"\"\"\n    configured_storage = storage.staticfiles_storage\n    missing_file_name = 'missing/entry.css'\n    configured_storage.hashed_files.clear()\n    \n    with self.assertRaisesMessage(ValueError, \"Missing staticfiles manifest entry for '%s'\" % missing_file_name):\n        configured_storage.stored_name(missing_file_name)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self.storage = ManifestStaticFilesStorage()\n", "compilation error", "    def test_storage_initialization_with_defaults(self):\n        \"\"\"\n        Test initialization of StaticFilesStorage with default parameters.\n        \"\"\"\n        storage_instance = storage.StaticFilesStorage()\n        self.assertEqual(storage_instance.location, settings.STATIC_ROOT)\n        self.assertEqual(storage_instance.base_url, settings.STATIC_URL)\n"], "sample_107": ["    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n        self.rf = RequestFactory()\n", "    def test_get_post_parameters(self):\n        \"\"\"\n        Ensure that sensitive post parameters are cleansed by SafeExceptionReporterFilter.\n        \"\"\"\n        request = self.rf.post('/some_url/', {\n            'password': 'super_secret',\n            'username': 'test_user',\n        })\n        request.sensitive_post_parameters = ['password']\n        filter = SafeExceptionReporterFilter()\n        post_params = filter.get_post_parameters(request)\n        self.assertEqual(post_params['password'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(post_params['username'], 'test_user')\n", "    def test_is_active(self):\n        \"\"\"The filter should be active when settings.DEBUG is False.\"\"\"\n        with self.settings(DEBUG=False):\n            request = self.rf.get('/some_url/')\n            filter = SafeExceptionReporterFilter()\n            self.assertTrue(filter.is_active(request))\n", "    def test_cleanse_setting_with_non_dict_callable(self):\n        \"\"\"Test that a non-dict callable setting is wrapped.\"\"\"\n            return \"This should not be displayed\"\n\n        cleansed = cleanse_setting('CALLABLE_SETTING', test_callable)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n", "    def test_is_active(self):\n        \"\"\"Test if the filter is active based on the DEBUG setting\"\"\"\n        request = RequestFactory().get('/')\n        with self.settings(DEBUG=True):\n            filter = SafeExceptionReporterFilter()\n            self.assertFalse(filter.is_active(request))\n        with self.settings(DEBUG=False):\n            filter = SafeExceptionReporterFilter()\n            self.assertTrue(filter.is_active(request))\n"], "sample_106": ["    def test_patch_cache_control_empty(self):\n        response = HttpResponse()\n        patch_cache_control(response)\n        self.assertEqual(response.get('Cache-Control'), '')\n", "    def test_patch_cache_control_sets_new_cache_control_header(self):\n        response = HttpResponse()\n        patch_cache_control(response, public=True, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'public, max-age=3600')\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, public=True, max_age=3600)\n        self.assertIn('Cache-Control', response)\n        self.assertIn('public', response['Cache-Control'])\n        self.assertIn('max-age=3600', response['Cache-Control'])\n", "    def setUp(self):\n        self.response = HttpResponse()\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, public=True)\n        self.assertIn('Cache-Control', response)\n        self.assertEqual(response['Cache-Control'], 'public')\n\n        patch_cache_control(response, max_age=60)\n        self.assertIn('Cache-Control', response)\n        self.assertIn('max-age=60', response['Cache-Control'])\n\n        patch_cache_control(response, no_cache=True, no_store=True)\n        self.assertIn('Cache-Control', response)\n        self.assertIn('no-cache', response['Cache-Control'])\n        self.assertIn('no-store', response['Cache-Control'])\n"], "sample_105": ["    def test_redirect_with_invalid_pattern_name(self):\n        \"\"\"\n        Test that a RedirectView with an invalid pattern_name raises an error.\n        \"\"\"\n        msg = \"Reverse for 'invalid_pattern' not found. 'invalid_pattern' is not a valid view function or pattern name.\"\n        with self.assertRaisesMessage(NoReverseMatch, msg):\n            RedirectView.as_view(pattern_name='invalid_pattern')(self.rf.get('/foo/'))\n", "    def test_redirect_view_with_pattern_name_and_query_string(self):\n        \"\"\"\n        Test RedirectView with pattern_name and query_string parameters.\n        \"\"\"\n        response = RedirectView.as_view(pattern_name='artist_detail', query_string=True)(\n            self.rf.get('/foo/?extra_param=value'), pk=1)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/detail/artist/1/?extra_param=value')\n", "    def test_get_context_data_includes_view(self):\n        \"\"\"\n        Test that the context data includes the 'view' key referencing the view instance.\n        \"\"\"\n        class TestContextView(View):\n                context = super().get_context_data(**kwargs)\n                context['test'] = 'value'\n                return context\n\n        view_instance = TestContextView()\n        view_instance.setup(self.rf.get('/'))\n        context = view_instance.get_context_data()\n        self.assertIn('view', context)\n        self.assertIs(context['view'], view_instance)\n        self.assertEqual(context['test'], 'value')\n", "    def test_render_to_response(self):\n        \"\"\"\n        Test that render_to_response returns a TemplateResponse with the correct context.\n        \"\"\"\n        class TestTemplateView(TemplateResponseMixin, View):\n            template_name = 'generic_views/test.html'\n\n                context = self.get_context_data()\n                return self.render_to_response(context)\n\n        request = self.rf.get('/')\n        view = TestTemplateView.as_view()\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.template_name, ['generic_views/test.html'])\n        self.assertEqual(response.context_data, {'view': response.context_data['view']})\n", "    def test_redirect_with_missing_kwargs(self):\n        \"\"\"Test RedirectView with missing kwargs should raise KeyError\"\"\"\n        view = RedirectView.as_view(url='/bar/%(missing_kwarg)s/')\n        with self.assertRaises(KeyError):\n            view(self.rf.get('/foo/'))\n"], "sample_109": ["    def test_autocomplete_select_multiple_initial_value(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        form = AlbumForm(initial={'featuring': [beatles.pk, who.pk]})\n        widget = form['featuring'].field.widget\n        context = widget.get_context(name='featuring', value=[beatles.pk, who.pk], attrs={})\n        self.assertIn('The Beatles', context['widget']['optgroups'][0][1][0]['label'])\n        self.assertIn('The Who', context['widget']['optgroups'][0][1][1]['label'])\n        self.assertTrue(context['widget']['optgroups'][0][1][0]['selected'])\n        self.assertTrue(context['widget']['optgroups'][0][1][1]['selected'])\n", "    def test_foreignkeyrawidwidget_get_context_with_related_url(self):\n        class MockRel:\n            model = Band\n            limit_choices_to = {}\n                return Album._meta.get_field('band')\n\n        rel = MockRel()\n        widget = ForeignKeyRawIdWidget(rel, admin.site)\n        band = Band.objects.create(name='The Beatles', style='rock')\n        attrs = {'id': 'id_band'}\n        context = widget.get_context(name='band', value=band.pk, attrs=attrs)\n\n        self.assertIn('related_url', context)\n        self.assertIn('link_title', context)\n        self.assertEqual(context['widget']['attrs']['class'], 'vForeignKeyRawIdAdminField')\n", "    def test_autocomplete_select_multiple_widget(self):\n        class MultipleBandForm(forms.Form):\n            bands = forms.ModelMultipleChoiceField(\n                queryset=Band.objects.all(),\n                widget=AutocompleteSelectMultiple(\n                    Album._meta.get_field('featuring').remote_field,\n                    admin.site\n                )\n            )\n\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n\n        form = MultipleBandForm(initial={'bands': [beatles.pk, who.pk]})\n        output = form.as_table()\n        selected_option_beatles = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        selected_option_who = '<option value=\"%s\" selected>The Who</option>' % who.pk\n\n        self.assertIn(selected_option_beatles, output)\n        self.assertIn(selected_option_who, output)\n\n        attrs = form['bands'].field.widget.get_context(name='bands', value=[beatles.pk, who.pk], attrs={})['widget']['attrs']\n        self.assertEqual(attrs['class'], 'admin-autocomplete')\n        self.assertEqual(attrs['data-ajax--url'], '/admin_widgets/band/autocomplete/')\n", "    def test_url_params_from_lookup_dict(self):\n        # Test with a standard dictionary\n        lookups = {'foo': 'bar', 'baz': [1, 2, 3], 'qux': True}\n        params = url_params_from_lookup_dict(lookups)\n        self.assertEqual(params, {'foo': 'bar', 'baz': '1,2,3', 'qux': '1'})\n\n        # Test with a dictionary containing a callable\n        lookups = {'foo': lambda: 'bar'}\n        params = url_params_from_lookup_dict(lookups)\n        self.assertEqual(params, {'foo': 'bar'})\n\n        # Test with a dictionary containing a boolean False\n        lookups = {'foo': False}\n        params = url_params_from_lookup_dict(lookups)\n        self.assertEqual(params, {'foo': '0'})\n\n        # Test with an empty dictionary\n        lookups = {}\n        params = url_params_from_lookup_dict(lookups)\n        self.assertEqual(params, {})\n\n        # Test with a None value\n        lookups = None\n        params = url_params_from_lookup_dict(lookups)\n        self.assertEqual(params, {})\n", "    def test_autocomplete_select_multiple(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        class AlbumMultipleForm(forms.ModelForm):\n            class Meta:\n                model = Album\n                fields = ['featuring']\n                widgets = {\n                    'featuring': AutocompleteSelectMultiple(\n                        Album._meta.get_field('featuring').remote_field,\n                        admin.site,\n                    )\n                }\n\n        form = AlbumMultipleForm(initial={'featuring': [beatles.pk, who.pk]})\n        output = form.as_table()\n        selected_option_beatles = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        selected_option_who = '<option value=\"%s\" selected>The Who</option>' % who.pk\n        self.assertIn(selected_option_beatles, output)\n        self.assertIn(selected_option_who, output)\n\n        # Test that the widget class is correct\n        attrs = form['featuring'].field.widget.get_context(name='featuring', value=None, attrs={})['widget']['attrs']\n        self.assertEqual(attrs['class'], 'admin-autocomplete')\n\n        # Test build_attrs for AutocompleteSelectMultiple\n        attrs = form['featuring'].field.widget.build_attrs({})\n        self.assertJSONEqual(attrs['data-allow-clear'], 'true')\n"], "sample_111": ["    def test_get_filters_params(self):\n        \"\"\"\n        Test ChangeList.get_filters_params() to ensure it correctly removes\n        IGNORED_PARAMS from the params dictionary.\n        \"\"\"\n        m = BandAdmin(Band, custom_site)\n        request = self.factory.get('/band/', data={\n            ALL_VAR: '1',\n            ORDER_VAR: 'name',\n            ORDER_TYPE_VAR: 'asc',\n            SEARCH_VAR: 'test',\n            IS_POPUP_VAR: '1',\n            TO_FIELD_VAR: 'id',\n            'custom_param': 'value',\n        })\n        request.user = self.superuser\n        cl = m.get_changelist_instance(request)\n        filters_params = cl.get_filters_params()\n\n        # Ensure IGNORED_PARAMS are removed\n        self.assertNotIn(ALL_VAR, filters_params)\n        self.assertNotIn(ORDER_VAR, filters_params)\n        self.assertNotIn(ORDER_TYPE_VAR, filters_params)\n        self.assertNotIn(SEARCH_VAR, filters_params)\n        self.assertNotIn(IS_POPUP_VAR, filters_params)\n        self.assertNotIn(TO_FIELD_VAR, filters_params)\n        \n        # Ensure custom parameters remain\n        self.assertIn('custom_param', filters_params)\n        self.assertEqual(filters_params['custom_param'], 'value')\n", "    def test_get_filters_params(self):\n        \"\"\"\n        Ensure get_filters_params correctly removes ignored parameters from the query string.\n        \"\"\"\n        request = self.factory.get('/band/', data={\n            ALL_VAR: '1',\n            ORDER_VAR: '1',\n            ORDER_TYPE_VAR: 'asc',\n            SEARCH_VAR: 'test',\n            IS_POPUP_VAR: '1',\n            TO_FIELD_VAR: '1',\n            'custom_param': 'custom_value',\n        })\n        request.user = self.superuser\n\n        class SimpleBandAdmin(admin.ModelAdmin):\n            pass\n\n        m = SimpleBandAdmin(Band, custom_site)\n        cl = m.get_changelist_instance(request)\n        \n        # get_filters_params should remove all IGNORED_PARAMS but keep others\n        filtered_params = cl.get_filters_params()\n        expected_params = {'custom_param': 'custom_value'}\n        self.assertEqual(filtered_params, expected_params)\n", "    def test_get_filters_params_excludes_ignored_params(self):\n        \"\"\"\n        Test ChangeList.get_filters_params() to ensure it excludes IGNORED_PARAMS from the parameters.\n        \"\"\"\n        class TestModelAdmin(admin.ModelAdmin):\n                return Band.objects.all()\n        \n        model_admin = TestModelAdmin(Band, custom_site)\n        request = self.factory.get('/band/', data={\n            SEARCH_VAR: 'test',\n            ORDER_VAR: '1',\n            ORDER_TYPE_VAR: 'asc',\n            IS_POPUP_VAR: '1',\n            TO_FIELD_VAR: 'id',\n            'other_param': 'value',\n        })\n        request.user = self.superuser\n        cl = model_admin.get_changelist_instance(request)\n        filters_params = cl.get_filters_params()\n\n        # The resulting filters_params should exclude the IGNORED_PARAMS\n        expected_params = {'other_param': 'value'}\n        self.assertEqual(filters_params, expected_params)\n", "    def test_get_filters_params(self):\n        \"\"\"\n        Test ChangeList.get_filters_params() to ensure it removes IGNORED_PARAMS\n        from the query parameters.\n        \"\"\"\n        request = self.factory.get('/band/', data={\n            ALL_VAR: '1',\n            ORDER_VAR: 'name',\n            ORDER_TYPE_VAR: 'asc',\n            SEARCH_VAR: 'test',\n            IS_POPUP_VAR: '1',\n            TO_FIELD_VAR: 'name',\n            'custom_param': 'value'\n        })\n        request.user = self.superuser\n        m = BandAdmin(Band, custom_site)\n        cl = m.get_changelist_instance(request)\n        params = cl.get_filters_params()\n        self.assertNotIn(ALL_VAR, params)\n        self.assertNotIn(ORDER_VAR, params)\n        self.assertNotIn(ORDER_TYPE_VAR, params)\n        self.assertNotIn(SEARCH_VAR, params)\n        self.assertNotIn(IS_POPUP_VAR, params)\n        self.assertNotIn(TO_FIELD_VAR, params)\n        self.assertIn('custom_param', params)\n        self.assertEqual(params['custom_param'], 'value')\n", "    def test_get_filters_params_excludes_ignored_params(self):\n        class FilteredBandAdmin(admin.ModelAdmin):\n            list_display = ['name', 'genres']\n            list_filter = ['name']\n\n        m = FilteredBandAdmin(Band, custom_site)\n        request = self.factory.get('/band/', data={SEARCH_VAR: 'test', ORDER_VAR: 'name', ORDER_TYPE_VAR: 'asc', PAGE_VAR: 1, ALL_VAR: '', IS_POPUP_VAR: '1', TO_FIELD_VAR: 'id'})\n        request.user = self.superuser\n        cl = m.get_changelist_instance(request)\n        filtered_params = cl.get_filters_params()\n\n        # Ensure that all IGNORED_PARAMS are removed from the filtered_params\n        for param in IGNORED_PARAMS:\n            self.assertNotIn(param, filtered_params)\n\n        # Ensure that non-ignored params are still present\n        self.assertIn('name', filtered_params)\n"], "sample_110": ["    def test_combined_expression_add(self):\n        expr1 = F('field1')\n        expr2 = F('field2')\n        combined_expr = expr1 + expr2\n        self.assertIsInstance(combined_expr, CombinedExpression)\n        self.assertEqual(combined_expr.connector, Combinable.ADD)\n        self.assertEqual(str(combined_expr), \"field1 + field2\")\n", "def test_combined_expression_addition(self):\n    group1 = Group.objects.create(name='Group 1')\n    group2 = Group.objects.create(name='Group 2')\n    event1 = Event.objects.create(title='Event 1', group=group1)\n    event2 = Event.objects.create(title='Event 2', group=group2)\n\n    combined_expr = F('id') + 1\n    events = Event.objects.annotate(new_id=combined_expr).values('id', 'new_id')\n\n    for event in events:\n        self.assertEqual(event['id'] + 1, event['new_id'])\n", "    def test_combined_expression_add(self):\n        expr1 = F('field1')\n        expr2 = F('field2')\n        combined_expr = expr1 + expr2\n        self.assertIsInstance(combined_expr, CombinedExpression)\n        self.assertEqual(combined_expr.connector, CombinedExpression.ADD)\n        self.assertEqual(str(combined_expr), 'field1 + field2')\n", "    def test_combined_expression_add(self):\n        expr1 = F('field1')\n        expr2 = F('field2')\n        combined = expr1 + expr2\n        self.assertIsInstance(combined, CombinedExpression)\n        self.assertEqual(combined.connector, '+')\n        self.assertEqual(combined.lhs, expr1)\n        self.assertEqual(combined.rhs, expr2)\n", "    def test_combined_expression_addition(self):\n        \"\"\"Test the addition operation in CombinedExpression\"\"\"\n        g = Group.objects.create(name='Test Group')\n        e1 = Event.objects.create(title='Event 1', group=g)\n        e2 = Event.objects.create(title='Event 2', group=g)\n        \n        combined_expr = CombinedExpression(F('id'), CombinedExpression.ADD, F('group_id'))\n        events = Event.objects.annotate(combined_id=models.Value(combined_expr)).order_by('combined_id')\n        \n        self.assertSequenceEqual(list(events.values_list('combined_id', flat=True)), [e1.id + e1.group_id, e2.id + e2.group_id])\n"], "sample_112": ["    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js should correctly format the prepopulated fields for both admin form and inlines.\n        \"\"\"\n        class MockField:\n                self.auto_id = auto_id\n                self.name = name\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        class MockInlineAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n                self.original = original\n\n        context = {\n            'adminform': MockAdminForm([\n                {\"field\": MockField(\"id_title\", \"title\"), \"dependencies\": [MockField(\"id_slug\", \"slug\")]}\n            ]),\n            'inline_admin_formsets': [\n                [MockInlineAdminForm([\n                    {\"field\": MockField(\"id_description\", \"description\", 100, True), \"dependencies\": [MockField(\"id_short_description\", \"short_description\")]}\n                ])]\n            ]\n        }\n\n        result_context = prepopulated_fields_js(context)\n\n        expected_prepopulated_fields_json = json.dumps([\n            {\n                \"id\": \"#id_title\",\n                \"name\": \"title\",\n                \"dependency_ids\": [\"#id_slug\"],\n                \"dependency_list\": [\"slug\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": False\n            },\n            {\n                \"id\": \"#id_description\",\n                \"name\": \"description\",\n                \"dependency_ids\": [\"#id_short_description\"],\n                \"dependency_list\": [\"short_description\"],\n                \"maxLength\": 100,\n                \"allowUnicode\": True\n            }\n        ])\n\n        self.assertEqual(result_context['prepopulated_fields_json'], expected_prepopulated_fields_json)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js template tag should update context with prepopulated fields JSON.\n        \"\"\"\n        article = Article.objects.all()[0]\n        request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[article.pk]))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        extra_context = {\n            'adminform': admin.get_form(request)(),\n            'inline_admin_formsets': []\n        }\n        response = admin.change_view(request, str(article.pk), extra_context=extra_context)\n        template_context = prepopulated_fields_js(response.context_data)\n        self.assertIn('prepopulated_fields', template_context)\n        self.assertIn('prepopulated_fields_json', template_context)\n        self.assertIsInstance(template_context['prepopulated_fields_json'], str)\n        self.assertEqual(json.loads(template_context['prepopulated_fields_json']), [])\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def test_cell_count(self):\n        \"\"\"\n        Test the cell_count filter to ensure it returns the correct number of cells\n        for a given inline_admin_form.\n        \"\"\"\n        class MockField:\n                self.name = name\n\n        class MockLine:\n                self.fields = fields\n\n        class MockFieldset:\n                self.lines = lines\n\n        class MockFormSet:\n                self.can_delete = can_delete\n\n        class MockInlineAdminForm:\n                self.fieldsets = fieldsets\n                self.formset = formset\n\n                return iter(self.fieldsets)\n\n        # Define mock data for the inline form\n        field1 = MockField(\"field1\")\n        field2 = MockField(\"field2\")\n        field3 = MockField(\"field3\")\n        line1 = MockLine([field1, field2])\n        line2 = MockLine([field3])\n        fieldset1 = MockFieldset([line1, line2])\n        formset = MockFormSet(can_delete=True)\n        inline_admin_form = MockInlineAdminForm([fieldset1], formset)\n\n        # Call the cell_count function with the mock data\n        count = cell_count(inline_admin_form)\n\n        # There should be 1 hidden cell + 3 fields + 1 delete checkbox = 5\n        self.assertEqual(count, 5)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js should correctly serialize prepopulated fields to JSON.\n        \"\"\"\n        class MockField:\n                self.auto_id = auto_id\n                self.name = name\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        context = {\n            'adminform': MockAdminForm([\n                {\n                    \"field\": MockField(\"id_title\", \"title\", max_length=200),\n                    \"dependencies\": [MockField(\"id_slug\", \"slug\")]\n                }\n            ])\n        }\n        result_context = prepopulated_fields_js(context)\n        prepopulated_fields_json = json.loads(result_context['prepopulated_fields_json'])\n        expected_json = [\n            {\n                \"id\": \"#id_title\",\n                \"name\": \"title\",\n                \"dependency_ids\": [\"#id_slug\"],\n                \"dependency_list\": [\"slug\"],\n                \"maxLength\": 200,\n                \"allowUnicode\": False\n            }\n        ]\n        self.assertEqual(prepopulated_fields_json, expected_json)\n"], "sample_113": ["    def test_trim_docstring(self):\n        tests = [\n            (\"\"\"\n            A test docstring.\n            \"\"\", \"A test docstring.\"),\n            (\"\"\"\n            A test docstring with\n\n            a second paragraph.\n            \"\"\", \"A test docstring with\\n\\na second paragraph.\"),\n            (\"\", \"\"),\n            (\"     \", \"\"),\n        ]\n        for docstring, output in tests:\n            with self.subTest(docstring=docstring):\n                self.assertEqual(utils.trim_docstring(docstring), output)\n    ", "    def test_trim_docstring(self):\n        tests = [\n            (None, ''),\n            (\"\", ''),\n            (\"    \", ''),\n            (\"Simple docstring.\", \"Simple docstring.\"),\n            (\"    Simple docstring with leading spaces.\", \"Simple docstring with leading spaces.\"),\n            (\n                \"    First line.\\n    Second line.\\n\\n    Third line.\",\n                \"First line.\\nSecond line.\\n\\nThird line.\"\n            ),\n            (\n                \"    First line.\\n\\tSecond line with tabs.\\n    Third line.\",\n                \"First line.\\nSecond line with tabs.\\nThird line.\"\n            ),\n            (\n                \"\\tFirst line with tabs.\\n\\tSecond line with tabs.\\n\\n\\tThird line.\",\n                \"First line with tabs.\\nSecond line with tabs.\\n\\nThird line.\"\n            ),\n        ]\n        for docstring, expected in tests:\n            with self.subTest(docstring=docstring):\n                self.assertEqual(trim_docstring(docstring), expected)\n", "    def test_get_view_name(self):\n            pass\n        dummy_view.__module__ = \"dummy_module\"\n        view_name = utils.get_view_name(dummy_view)\n        self.assertEqual(view_name, \"dummy_module.dummy_view\")\n", "    def test_trim_docstring(self):\n        docstring = \"\"\"\n        This is a test docstring.\n\n        It has multiple lines, some of which are indented.\n        \n            This line is indented.\n        \"\"\"\n        expected_output = \"This is a test docstring.\\n\\nIt has multiple lines, some of which are indented.\\n\\n    This line is indented.\"\n        self.assertEqual(utils.trim_docstring(docstring), expected_output)\n", "    def test_trim_docstring(self):\n        docstring = \"\"\"\n            This is a test docstring.\n\n            It has multiple lines.\n            \n            And some whitespace.\n        \"\"\"\n        expected_output = \"This is a test docstring.\\n\\nIt has multiple lines.\\n\\nAnd some whitespace.\"\n        self.assertEqual(utils.trim_docstring(docstring), expected_output)\n"], "sample_114": ["def test_deep_deconstruct_partial(self):\n    \"\"\"\n    Ensure deep_deconstruct correctly handles functools.partial objects.\n    \"\"\"\n        return f\"{arg1}, {arg2}, {kwarg1}, {kwarg2}\"\n\n    partial_obj = functools.partial(sample_function, \"arg1_value\", kwarg1=\"kwarg1_value\")\n    original_obj = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"field\", models.CharField(max_length=200, default=partial_obj)),\n    ])\n    new_obj = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"field\", models.CharField(max_length=200, default=partial_obj)),\n    ])\n\n    changes = self.get_changes([original_obj], [new_obj])\n    self.assertEqual(len(changes), 0)\n\n    modified_partial_obj = functools.partial(sample_function, \"arg1_value\", kwarg1=\"kwarg1_modified\")\n    modified_obj = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"field\", models.CharField(max_length=200, default=modified_partial_obj)),\n    ])\n\n    changes = self.get_changes([original_obj], [modified_obj])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"field\")\n", "    def test_alter_field_with_default(self):\n        \"\"\"Tests changing a field with a default value.\"\"\"\n        changes = self.get_changes([self.author_name], [self.author_name_default])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\", preserve_default=True)\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default='Ada Lovelace')\n", "    def test_deep_deconstruct_partial_function(self):\n        \"\"\"\n        Test deep deconstruction of a partial function in a model field.\n        \"\"\"\n            return a + b\n\n        partial_func = functools.partial(sample_func, a=2)\n        before = ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('value', models.IntegerField(default=partial_func)),\n        ])\n        after = ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('value', models.IntegerField(default=partial_func)),\n        ])\n        changes = self.get_changes([before], [after])\n        self.assertEqual(changes, {})\n\n        altered_partial_func = functools.partial(sample_func, a=3)\n        after = ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('value', models.IntegerField(default=altered_partial_func)),\n        ])\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n        value = changes['testapp'][0].operations[0].field.default\n        self.assertEqual(\n            (sample_func, (), {'a': 3}),\n            (value.func, value.args, value.keywords)\n        )\n", "    def test_detect_changes_with_existing_apps(self):\n        \"\"\"\n        Test the detection of changes where the `existing_apps` attribute\n        is populated.\n        \"\"\"\n        # Create initial project states\n        before = self.make_project_state([self.author_empty])\n        after = self.make_project_state([self.author_name])\n\n        # Mock a MigrationQuestioner instance\n        questioner = MigrationQuestioner()\n\n        # Initialize MigrationAutodetector with existing_apps set\n        autodetector = MigrationAutodetector(before, after, questioner=questioner)\n        autodetector.existing_apps = {\"testapp\"}\n\n        # Detect changes\n        changes = autodetector._detect_changes()\n\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n", "    def test_alter_field_with_new_default(self):\n        \"\"\"\n        Tests that altering a field to have a new default value is correctly detected.\n        \"\"\"\n        before = [\n            ModelState('app', 'Author', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'Author', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200, default='Anonymous')),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name', preserve_default=True)\n        self.assertOperationFieldAttributes(changes, 'app', 0, 0, default='Anonymous')\n"], "sample_115": ["    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n    ", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n    ", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_cleanse_setting_callable(self):\n        \"\"\"Test that a callable setting is wrapped with CallableSettingWrapper.\"\"\"\n        reporter_filter = SafeExceptionReporterFilter()\n\n            return \"sensitive data\"\n\n        cleansed = reporter_filter.cleanse_setting('CALLABLE_SETTING', some_callable)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed), repr(some_callable))\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n"], "sample_116": ["    def test_empty_fragment_name(self):\n        key = make_template_fragment_key('', ['abc'])\n        self.assertEqual(key, 'template.cache..493e283d571a73056196f1a68efd0f66')\n", "    def test_with_special_characters_vary_on(self):\n        key = make_template_fragment_key('special', ['@#$', '^&*', '()'])\n        self.assertEqual(key, 'template.cache.special.3a0f74539a2c5b1a1f3dc8d2c9e8d8f8')\n", "    def test_with_empty_fragment_name(self):\n        key = make_template_fragment_key('')\n        self.assertEqual(key, 'template.cache..d41d8cd98f00b204e9800998ecf8427e')\n        ", "    def test_empty_fragment_name(self):\n        key = make_template_fragment_key('')\n        self.assertEqual(key, 'template.cache..d41d8cd98f00b204e9800998ecf8427e')\n", "    def test_with_mixed_vary_on(self):\n        key = make_template_fragment_key('mixed', ['abc', 123, 'def', 456])\n        self.assertEqual(key, 'template.cache.mixed.72d4a41b4a27b9a7b647a6e8a1c4fdbb')\n"], "sample_118": ["    def test_year_lookups(self):\n        # YearExact lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year=2005),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ],\n            ordered=False\n        )\n        \n        # YearGt lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__gt=2005),\n            [],\n            ordered=False\n        )\n\n        # YearGte lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__gte=2005),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ],\n            ordered=False\n        )\n\n        # YearLt lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__lt=2005),\n            [],\n            ordered=False\n        )\n\n        # YearLte lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__lte=2005),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ],\n            ordered=False\n        )\n", "    def test_year_lookups(self):\n        # Create some articles with specific publication years for testing year lookups\n        for a in Article.objects.all():\n            a.delete()\n        now = datetime.now()\n        article_2020 = Article.objects.create(pub_date=datetime(2020, 1, 1), headline='Article 2020')\n        article_2021 = Article.objects.create(pub_date=datetime(2021, 1, 1), headline='Article 2021')\n        article_2022 = Article.objects.create(pub_date=datetime(2022, 1, 1), headline='Article 2022')\n\n        # Test YearExact lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year=2021),\n            ['<Article: Article 2021>']\n        )\n        \n        # Test YearGt lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__gt=2020),\n            ['<Article: Article 2021>', '<Article: Article 2022>'],\n            ordered=False\n        )\n        \n        # Test YearGte lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__gte=2021),\n            ['<Article: Article 2021>', '<Article: Article 2022>'],\n            ordered=False\n        )\n        \n        # Test YearLt lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__lt=2022),\n            ['<Article: Article 2020>', '<Article: Article 2021>'],\n            ordered=False\n        )\n        \n        # Test YearLte lookup\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__lte=2021),\n            ['<Article: Article 2020>', '<Article: Article 2021>'],\n            ordered=False\n        )\n", "    def test_year_lookups(self):\n        # Test year lookup bounds for DateTimeField and DateField.\n        # Articles from 2005\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year=2005),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ],\n            ordered=False\n        )\n        # Articles before 2005\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__lt=2005),\n            [],\n        )\n        # Articles after 2005\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__gt=2005),\n            [],\n        )\n        # Articles in or after 2005\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__gte=2005),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ],\n            ordered=False\n        )\n        # Articles in or before 2005\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__lte=2005),\n            [\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n            ],\n            ordered=False\n        )\n", "    def test_year_lookups(self):\n        # Create Seasons with specific years\n        season_2019 = Season.objects.create(year=2019)\n        season_2020 = Season.objects.create(year=2020)\n        season_2021 = Season.objects.create(year=2021)\n        season_2022 = Season.objects.create(year=2022)\n\n        # Testing YearExact lookup\n        self.assertQuerysetEqual(\n            Season.objects.filter(year__exact=2020),\n            ['<Season: 2020>'],\n            transform=repr\n        )\n\n        # Testing YearGt lookup\n        self.assertQuerysetEqual(\n            Season.objects.filter(year__gt=2020),\n            ['<Season: 2021>', '<Season: 2022>'],\n            transform=repr,\n            ordered=False\n        )\n\n        # Testing YearGte lookup\n        self.assertQuerysetEqual(\n            Season.objects.filter(year__gte=2020),\n            ['<Season: 2020>', '<Season: 2021>', '<Season: 2022>'],\n            transform=repr,\n            ordered=False\n        )\n\n        # Testing YearLt lookup\n        self.assertQuerysetEqual(\n            Season.objects.filter(year__lt=2021),\n            ['<Season: 2019>', '<Season: 2020>'],\n            transform=repr,\n            ordered=False\n        )\n\n        # Testing YearLte lookup\n        self.assertQuerysetEqual(\n            Season.objects.filter(year__lte=2021),\n            ['<Season: 2019>', '<Season: 2020>', '<Season: 2021>'],\n            transform=repr,\n            ordered=False\n        )\n\n        # Testing YearIn lookup\n        self.assertQuerysetEqual(\n            Season.objects.filter(year__in=[2019, 2021]),\n            ['<Season: 2019>', '<Season: 2021>'],\n            transform=repr,\n            ordered=False\n        )\n", "def test_year_exact_lookup(self):\n    # Create some Articles with different publication years\n    a1 = Article.objects.create(headline='Article 8', pub_date=datetime(2010, 5, 1), author=self.au1, slug='a8')\n    a2 = Article.objects.create(headline='Article 9', pub_date=datetime(2011, 6, 1), author=self.au1, slug='a9')\n    a3 = Article.objects.create(headline='Article 10', pub_date=datetime(2011, 7, 1), author=self.au2, slug='a10')\n    a4 = Article.objects.create(headline='Article 11', pub_date=datetime(2012, 8, 1), author=self.au2, slug='a11')\n\n    # Test the YearExact lookup\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year=2011),\n        ['<Article: Article 9>', '<Article: Article 10>'],\n        ordered=False\n    )\n\n    # Test with YearGt and YearGte lookups\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__gt=2010),\n        ['<Article: Article 9>', '<Article: Article 10>', '<Article: Article 11>'],\n        ordered=False\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__gte=2011),\n        ['<Article: Article 9>', '<Article: Article 10>', '<Article: Article 11>'],\n        ordered=False\n    )\n\n    # Test with YearLt and YearLte lookups\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__lt=2012),\n        ['<Article: Article 8>', '<Article: Article 9>', '<Article: Article 10>'],\n        ordered=False\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__lte=2011),\n        ['<Article: Article 8>', '<Article: Article 9>', '<Article: Article 10>'],\n        ordered=False\n    )\n"], "sample_119": ["    def test_explain_query(self):\n        query = Query(Author)\n        explain_output = query.explain(using='default', format='text')\n        self.assertIsInstance(explain_output, str)\n        self.assertIn('EXPLAIN', explain_output.upper())\n", "def test_query_annotations(self):\n    query = Query(Author)\n    query.add_annotation(Count('num'), alias='num_count', is_summary=True)\n    where = query.build_where(Q(num_count__gt=1))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertIsInstance(lookup.lhs, SimpleCol)\n    self.assertEqual(lookup.rhs, 1)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n    self.assertEqual(query.annotations['num_count'].source_expressions[0].name, 'num')\n", "def test_query_with_annotation(self):\n    query = Query(Author)\n    query.add_annotation(Count('num'), alias='num_count')\n    where = query.build_where(Q(num_count__gt=1))\n    \n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 1)\n    self.assertIsInstance(lookup.lhs, SimpleCol)\n    self.assertEqual(lookup.lhs.alias, 'num_count')\n", "def test_chain_and_clone_methods(self):\n    query = Query(Author)\n    query.add_select_related(['creator'])\n\n    # Test chain method\n    new_query = query.chain()\n    self.assertNotEqual(id(query), id(new_query))\n    self.assertEqual(query.model, new_query.model)\n    self.assertEqual(query.select_related, new_query.select_related)\n\n    # Test clone method\n    cloned_query = query.clone()\n    self.assertNotEqual(id(query), id(cloned_query))\n    self.assertEqual(query.model, cloned_query.model)\n    self.assertEqual(query.select_related, cloned_query.select_related)\n\n    # Modify the cloned query and check original query remains unchanged\n    cloned_query.add_select_related(['note'])\n    self.assertNotEqual(query.select_related, cloned_query.select_related)\n    self.assertEqual(query.select_related, {'creator': {}})\n    self.assertEqual(cloned_query.select_related, {'creator': {}, 'note': {}})\n", "    def test_query_set_empty(self):\n        query = Query(Author)\n        query.set_empty()\n        self.assertTrue(query.is_empty())\n        self.assertIsInstance(query.where.children[0], NothingNode)\n"], "sample_120": ["    def test_serialize_partial_method_reference(self):\n        \"\"\"A reference to a partial method within a class can be serialized.\"\"\"\n        class TestModel3:\n                return x\n\n            partial_method = functools.partialmethod(method, 42)\n\n        self.serialize_round_trip(TestModel3.partial_method)\n", "    def test_serialize_builtin_function_type(self):\n        # Test serialization of a built-in function\n        self.assertSerializedResultEqual(\n            len,\n            (\"len\", set())\n        )\n\n        # Test serialization of a function defined in a module\n        self.assertSerializedResultEqual(\n            math.sqrt,\n            (\"math.sqrt\", {'import math'})\n        )\n\n        # Test serialization of a method of a built-in type\n        self.assertSerializedResultEqual(\n            str.lower,\n            (\"str.lower\", set())\n        )\n", "    def test_serialize_model_field(self):\n        field = models.CharField(max_length=100, unique=True, null=True)\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, \"models.CharField(max_length=100, null=True, unique=True)\")\n        self.assertEqual(imports, {\"from django.db import models\"})\n        self.assertSerializedFieldEqual(field)\n", "    def test_serialize_partial_function(self):\n        \"\"\"Test serialization of a partial function.\"\"\"\n            return a + b + c\n\n        partial_func = functools.partial(my_func, 1, 2)\n        string, imports = MigrationWriter.serialize(partial_func)\n        self.assertEqual(string, \"functools.partial(migrations.test_writer.my_func, 1, 2)\")\n        self.assertEqual(imports, {'import functools', 'import migrations.test_writer'})\n        self.serialize_round_trip(partial_func)\n", "    def test_serialize_model_field(self):\n        field = models.CharField(max_length=100, null=True, unique=True)\n        string, imports = MigrationWriter.serialize(field)\n        self.assertIn(\"models.CharField(max_length=100, null=True, unique=True)\", string)\n        self.assertIn(\"from django.db import models\", imports)\n"], "sample_121": ["    def test_model_state_fields_cache_descriptor(self):\n        class TestModel(Model):\n            field = models.IntegerField()\n\n        instance = TestModel()\n        # Initially, fields_cache should be empty\n        self.assertEqual(instance._state.fields_cache, {})\n\n        # Accessing fields_cache again should return the same empty dictionary\n        self.assertIs(instance._state.fields_cache, instance._state.fields_cache)\n\n        # Set a value in fields_cache and check if it persists\n        instance._state.fields_cache['field'] = 42\n        self.assertEqual(instance._state.fields_cache['field'], 42)\n", "    def test_unsaved_foreign_key_prohibited(self):\n        class Parent(models.Model):\n            pass\n\n        class Child(models.Model):\n            parent = models.ForeignKey(Parent, models.CASCADE)\n\n        parent = Parent()\n        child = Child(parent=parent)\n\n        with self.assertRaises(ValueError) as cm:\n            child.save()\n\n        self.assertEqual(\n            str(cm.exception),\n            \"save() prohibited to prevent data loss due to unsaved related object 'parent'.\"\n        )\n", "    def test_model_base_abstract_class(self):\n        class AbstractBase(models.Model):\n            class Meta:\n                abstract = True\n\n        class ConcreteModel(AbstractBase):\n            field1 = models.CharField(max_length=100)\n\n        self.assertFalse(AbstractBase._meta.abstract)\n        self.assertFalse(hasattr(AbstractBase, 'DoesNotExist'))\n        self.assertFalse(hasattr(AbstractBase, 'MultipleObjectsReturned'))\n        self.assertTrue(hasattr(ConcreteModel, 'DoesNotExist'))\n        self.assertTrue(hasattr(ConcreteModel, 'MultipleObjectsReturned'))\n", "    def test_proxy_model_with_fields(self):\n        class ProxyModelBase(models.Model):\n            field1 = models.IntegerField()\n\n            class Meta:\n                abstract = True\n\n        class ProxyModel(ProxyModelBase):\n            field2 = models.IntegerField()\n\n            class Meta:\n                proxy = True\n\n        self.assertEqual(ProxyModel.check(), [\n            Error(\n                \"Proxy model 'ProxyModel' contains model fields.\",\n                obj=ProxyModel,\n                id='models.E017',\n            ),\n        ])\n    ", "    def test_abstract_model_instantiation(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        with self.assertRaises(TypeError):\n            AbstractModel()\n"], "sample_122": ["    def setUp(self):\n        self.factory = RequestFactory()\n", "    def test_patch_cache_control_update_existing_header(self):\n        \"\"\"\n        Ensure patch_cache_control properly updates an existing Cache-Control header.\n        \"\"\"\n        response = HttpResponse()\n        response['Cache-Control'] = 'max-age=60, private'\n        patch_cache_control(response, max_age=120, public=True)\n        parts = set(response['Cache-Control'].split(', '))\n        expected_parts = {'max-age=120', 'public'}\n        self.assertEqual(parts, expected_parts)\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, public=True, max_age=3600)\n        self.assertIn('Cache-Control', response)\n        self.assertIn('public', response['Cache-Control'])\n        self.assertIn('max-age=3600', response['Cache-Control'])\n", "    def test_patch_cache_control_no_cache(self):\n        response = HttpResponse()\n        patch_cache_control(response, no_cache=True)\n        self.assertIn('Cache-Control', response)\n        self.assertIn('no-cache', response['Cache-Control'])\n", "    def test_patch_cache_control_initial_values(self):\n        \"\"\"Test patch_cache_control with initial values in Cache-Control header.\"\"\"\n        response = HttpResponse()\n        response['Cache-Control'] = 'max-age=3600, public'\n        patch_cache_control(response, max_age=7200)\n        self.assertIn('max-age=3600', response['Cache-Control'])\n        self.assertIn('public', response['Cache-Control'])\n"], "sample_123": ["    def test_limited_parse_qsl_with_limit(self):\n        qs = 'a=1&b=2&c=3&d=4'\n        result = limited_parse_qsl(qs, fields_limit=3)\n        self.assertEqual(result, [('a', '1'), ('b', '2'), ('c', '3')])\n", "    def test_urlparse(self):\n        url = 'http://example.com/path;params?query#fragment'\n        result = _urlparse(url)\n        expected = ('http', 'example.com', '/path', 'params', 'query', 'fragment')\n        self.assertEqual(result, expected)\n", "    def test_limited_parse_qsl(self):\n        qs = 'a=1&b=2&c=3'\n        self.assertEqual(limited_parse_qsl(qs, fields_limit=3), [('a', '1'), ('b', '2'), ('c', '3')])\n", "    def test_urlquote_plus_safe(self):\n        # Test with safe characters\n        self.assertEqual(urlquote_plus('foo bar', safe=''), 'foo+bar')\n        self.assertEqual(urlquote_plus('foo/bar', safe='/'), 'foo/bar')\n        self.assertEqual(urlquote_plus('foo&bar', safe='&'), 'foo&bar')\n", "    def test_basic_parsing(self):\n        self.assertEqual(limited_parse_qsl('a=1&b=2&c=3'), [('a', '1'), ('b', '2'), ('c', '3')])\n"], "sample_124": ["    def test_decimal_field(self):\n        class ProductForm(Form):\n            price = DecimalField(max_digits=5, decimal_places=2)\n\n        # Valid data\n        form = ProductForm(data={'price': '123.45'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['price'], Decimal('123.45'))\n\n        # Invalid data: more than allowed digits\n        form = ProductForm(data={'price': '123456.78'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['price'], ['Ensure that there are no more than 5 digits in total.'])\n\n        # Invalid data: more than allowed decimal places\n        form = ProductForm(data={'price': '123.456'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['price'], ['Ensure that there are no more than 2 decimal places.'])\n\n        # Invalid data: non-numeric input\n        form = ProductForm(data={'price': 'abc'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['price'], ['Enter a number.'])\n\n        # Valid data: empty value for non-required field\n        class OptionalProductForm(Form):\n            price = DecimalField(max_digits=5, decimal_places=2, required=False)\n\n        form = OptionalProductForm(data={'price': ''})\n        self.assertTrue(form.is_valid())\n        self.assertIsNone(form.cleaned_data['price'])\n", "    def test_boolean_field(self):\n        class SurveyForm(Form):\n            agree = BooleanField(label='Do you agree?')\n\n        # Test form with no data (unbound)\n        form = SurveyForm()\n        self.assertFalse(form.is_bound)\n        self.assertIsNone(form['agree'].value())\n        \n        # Test form with data indicating agreement\n        form = SurveyForm({'agree': 'True'})\n        self.assertTrue(form.is_bound)\n        self.assertTrue(form.is_valid())\n        self.assertTrue(form.cleaned_data['agree'])\n        self.assertHTMLEqual(\n            str(form['agree']),\n            '<input checked type=\"checkbox\" name=\"agree\" required>'\n        )\n\n        # Test form with data indicating disagreement\n        form = SurveyForm({'agree': 'False'})\n        self.assertTrue(form.is_bound)\n        self.assertTrue(form.is_valid())\n        self.assertFalse(form.cleaned_data['agree'])\n        self.assertHTMLEqual(\n            str(form['agree']),\n            '<input type=\"checkbox\" name=\"agree\" required>'\n        )\n\n        # Test form with no data (form should be invalid)\n        form = SurveyForm({'agree': ''})\n        self.assertTrue(form.is_bound)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['agree'], ['This field is required.'])\n\n        # Test form with initial data\n        form = SurveyForm(initial={'agree': True})\n        self.assertFalse(form.is_bound)\n        self.assertTrue(form['agree'].value())\n        self.assertHTMLEqual(\n            str(form['agree']),\n            '<input checked type=\"checkbox\" name=\"agree\" required>'\n        )\n", "    def test_integer_field(self):\n        class TestForm(Form):\n            age = IntegerField()\n\n        form = TestForm({'age': '25'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['age'], 25)\n\n        form = TestForm({'age': '25.0'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['age'], ['Enter a whole number.'])\n\n        form = TestForm({'age': 'abc'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['age'], ['Enter a whole number.'])\n\n        form = TestForm({'age': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['age'], ['This field is required.'])\n\n        form = TestForm({'age': None})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['age'], ['This field is required.'])\n", "    def test_field_prepare_value(self):\n        field = Field()\n        self.assertEqual(field.prepare_value('test'), 'test')\n        self.assertEqual(field.prepare_value(123), 123)\n        self.assertEqual(field.prepare_value(None), None)\n", "    def test_clean_field(self):\n        # Test clean method of Field.\n        f = Field()\n        with self.assertRaises(ValidationError):\n            f.clean('')\n        self.assertEqual(f.clean('hello'), 'hello')\n        \n        # Test clean method of CharField with max_length and strip.\n        f = CharField(max_length=5, strip=True)\n        self.assertEqual(f.clean(' hello '), 'hello')\n        with self.assertRaises(ValidationError):\n            f.clean('123456')\n\n        # Test clean method of IntegerField with min_value.\n        f = IntegerField(min_value=5)\n        self.assertEqual(f.clean(10), 10)\n        with self.assertRaises(ValidationError):\n            f.clean(1)\n        \n        # Test clean method of FloatField with not a number.\n        f = FloatField()\n        with self.assertRaises(ValidationError):\n            f.clean('abc')\n        \n        # Test clean method of DecimalField with max_digits and decimal_places.\n        f = DecimalField(max_digits=5, decimal_places=2)\n        self.assertEqual(f.clean('123.45'), Decimal('123.45'))\n        with self.assertRaises(ValidationError):\n            f.clean('12345.678')\n        \n        # Test clean method of BooleanField with a string representing False.\n        f = BooleanField()\n        self.assertFalse(f.clean('false'))\n        \n        # Test clean method of DateField with a string date.\n        f = DateField()\n        self.assertEqual(f.clean('2020-01-01'), datetime.date(2020, 1, 1))\n        \n        # Test clean method of TimeField with a string time.\n        f = TimeField()\n        self.assertEqual(f.clean('12:34:56'), datetime.time(12, 34, 56))\n        \n        # Test clean method of DateTimeField with a string datetime.\n        f = DateTimeField()\n        self.assertEqual(f.clean('2020-01-01 12:34:56'), datetime.datetime(2020, 1, 1, 12, 34, 56))\n        \n        # Test clean method of DurationField with a string duration.\n        f = DurationField()\n        self.assertEqual(f.clean('1 0:00:00'), datetime.timedelta(days=1))\n        \n        # Test clean method of RegexField with a regex pattern.\n        f = RegexField(r'^\\d+$')\n        self.assertEqual(f.clean('123"], "sample_125": ["    def test_json_response_with_dict(self):\n        \"\"\"JsonResponse works with a dict.\"\"\"\n        data = {'key': 'value'}\n        response = JsonResponse(data)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'application/json')\n        self.assertJSONEqual(response.content, json.dumps(data))\n", "    def test_reason_phrase_property(self):\n        \"\"\"Test the reason_phrase property of HttpResponse.\"\"\"\n        response = HttpResponse(status=404)\n        self.assertEqual(response.reason_phrase, 'Not Found')\n        custom_reason = 'Custom Reason'\n        response.reason_phrase = custom_reason\n        self.assertEqual(response.reason_phrase, custom_reason)\n", "    def test_reason_phrase_default(self):\n        \"\"\"HttpResponse should return default reason phrase if not set.\"\"\"\n        response = HttpResponse()\n        self.assertEqual(response.reason_phrase, 'OK')\n", "    def test_reason_phrase_default(self):\n        \"\"\"Default reason phrase is set correctly based on status code.\"\"\"\n        response = HttpResponse(status=200)\n        self.assertEqual(response.reason_phrase, \"OK\")\n", "    def test_init_with_status(self):\n        \"\"\"HttpResponse initializes with the correct status code.\"\"\"\n        response = HttpResponse(status=201)\n        self.assertEqual(response.status_code, 201)\n"], "sample_126": ["    def test_alter_field_with_function_default(self):\n        \"\"\"Tests autodetection of field alterations with function defaults.\"\"\"\n        before = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default='default_name')),\n            ]),\n        ]\n        after = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default=lambda: 'new_default')),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True)\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=lambda: 'new_default')\n", "    def test_create_model_with_custom_deconstructible(self):\n        \"\"\"\n        Test creation of a new model with a custom deconstructible object in its fields.\n        \"\"\"\n        custom_object = DeconstructibleObject('custom_arg', kwarg='value')\n        custom_model = ModelState('otherapp', 'CustomModel', [\n            ('id', models.AutoField(primary_key=True)),\n            ('custom_field', models.CharField(max_length=200, default=custom_object)),\n        ])\n        changes = self.get_changes([], [custom_model])\n        # Right number of migrations?\n        self.assertEqual(len(changes['otherapp']), 1)\n        # Right number of actions?\n        migration = changes['otherapp'][0]\n        self.assertEqual(len(migration.operations), 1)\n        # Right actions order?\n        self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='CustomModel')\n        self.assertEqual(migration.operations[0].fields[1][1].default.deconstruct(), (\n            custom_object.__module__ + '.' + custom_object.__class__.__name__,\n            ('custom_arg',),\n            {'kwarg': 'value'}\n        ))\n", "    def test_alter_model_managers_with_custom_manager(self):\n        \"\"\"\n        Adding a custom manager should add a new operation.\n        \"\"\"\n        initial_state = ModelState(\"otherapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], managers=[\n            ('objects', models.Manager()),\n        ])\n        modified_state = ModelState(\"otherapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], managers=[\n            ('objects', models.Manager()),\n            ('custom_manager', FoodManager('x', 'y')),\n        ])\n        changes = self.get_changes([initial_state], [modified_state])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterModelManagers\"])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"author\")\n        self.assertEqual([name for name, mgr in changes['otherapp'][0].operations[0].managers],\n                         ['objects', 'custom_manager'])\n        self.assertEqual(changes['otherapp'][0].operations[0].managers[1][1].args, ('x', 'y'))\n", "    def test_deep_deconstruct_composite_object(self):\n        \"\"\"\n        Test that deep_deconstruct correctly handles a composite object with nested structures.\n        \"\"\"\n        class CompositeObject:\n                self.args = args\n                self.kwargs = kwargs\n\n                return (\n                    self.__module__ + '.' + self.__class__.__name__,\n                    self.args,\n                    self.kwargs\n                )\n\n        nested_composite_1 = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=CompositeObject(\n                [CompositeObject(1), CompositeObject(2)],\n                {\"key\": CompositeObject(3)}\n            ))),\n        ])\n        \n        nested_composite_2 = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=CompositeObject(\n                [CompositeObject(1), CompositeObject(2)],\n                {\"key\": CompositeObject(3)}\n            ))),\n        ])\n        \n        changes = self.get_changes([nested_composite_1], [nested_composite_2])\n        self.assertEqual(changes, {})\n        \n        nested_composite_changed = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=CompositeObject(\n                [CompositeObject(1), CompositeObject(4)],\n                {\"key\": CompositeObject(3)}\n            ))),\n        ])\n        \n        changes = self.get_changes([nested_composite_1], [nested_composite_changed])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n", "def test_deep_deconstruct_partial_function(self):\n        \"\"\"\n        Tests the deep_deconstruct method for partial functions.\n        \"\"\"\n            return arg1 + arg2\n        \n        partial_func = functools.partial(example_function, 1, arg2=2)\n        obj = MigrationAutodetector(None, None)\n        deconstructed = obj.deep_deconstruct(partial_func)\n        \n        expected_deconstructed = (\n            example_function, \n            [1], \n            {\"arg2\": 2}\n        )\n        \n        self.assertEqual(deconstructed, expected_deconstructed)\n"], "sample_127": ["    def setUp(self):\n        self.country = Country.objects.create(name=\"CountryA\", iso_two_letter=\"CA\")\n        self.state = State.objects.create(two_letter_code=\"ST\")\n        self.pizzeria = Pizzeria.objects.create(name=\"PizzeriaA\")\n        self.restaurant = Restaurant.objects.create(name=\"RestaurantA\")\n        self.no_fields = NoFields.objects.create()\n        self.two_fields = TwoFields.objects.create(f1=1, f2=2)\n        self.nullable_fields = NullableFields.objects.create()\n", "    def test_bulk_update(self):\n        country_us = Country.objects.create(name=\"United States of America\", iso_two_letter=\"US\")\n        country_nl = Country.objects.create(name=\"The Netherlands\", iso_two_letter=\"NL\")\n        country_de = Country.objects.create(name=\"Germany\", iso_two_letter=\"DE\")\n        country_cz = Country.objects.create(name=\"Czech Republic\", iso_two_letter=\"CZ\")\n        \n        Country.objects.bulk_update(\n            [country_us, country_nl, country_de, country_cz],\n            fields=[\"iso_two_letter\"]\n        )\n        \n        self.assertQuerysetEqual(\n            Country.objects.order_by(\"name\"),\n            [\"Czech Republic\", \"Germany\", \"The Netherlands\", \"United States of America\"],\n            attrgetter(\"name\")\n        )\n", "    def setUp(self):\n        self.country1 = Country.objects.create(name=\"Country1\", iso_two_letter=\"C1\")\n        self.country2 = Country.objects.create(name=\"Country2\", iso_two_letter=\"C2\")\n", "def test_bulk_create_with_expressions(self):\n    \"\"\"\n    Test bulk_create with expressions as field values.\n    \"\"\"\n    Restaurant.objects.bulk_create([\n        Restaurant(name=Lower(Value(\"Sam's Shake Shack\"))),\n        Restaurant(name=Lower(Value(\"Betty's Beetroot Bar\"))),\n    ])\n    self.assertQuerysetEqual(Restaurant.objects.order_by('name'), [\n        \"betty's beetroot bar\", \"sam's shake shack\"\n    ], attrgetter(\"name\"))\n", "    def setUp(self):\n        self.countries = [\n            Country(name=\"United States of America\", iso_two_letter=\"US\"),\n            Country(name=\"The Netherlands\", iso_two_letter=\"NL\"),\n            Country(name=\"Germany\", iso_two_letter=\"DE\"),\n            Country(name=\"Czech Republic\", iso_two_letter=\"CZ\")\n        ]\n        Country.objects.bulk_create(self.countries)\n"], "sample_128": ["    def test_clone_index(self):\n        \"\"\"Test that clone method returns an identical copy of the index.\"\"\"\n        index = Index(\n            fields=['headline', 'pub_date'],\n            name='test_index',\n            db_tablespace='pg_default',\n            opclasses=['varchar_pattern_ops', 'text_pattern_ops'],\n            condition=Q(headline__contains='China'),\n            include=['published'],\n        )\n        cloned_index = index.clone()\n        self.assertEqual(index, cloned_index)\n        self.assertIsNot(index, cloned_index)\n", "    def test_set_name_with_model(self):\n        \"\"\"\n        Test that set_name_with_model method correctly generates a unique name for the index.\n        \"\"\"\n        from .models import Article\n        index = Index(fields=['headline', 'pub_date'])\n        index.set_name_with_model(Article)\n        self.assertEqual(len(index.name), 30)\n        self.assertTrue(index.name.startswith('indexes_article'))\n        self.assertIn('idx', index.name)\n", "    def test_index_deconstruct(self):\n        index = Index(\n            fields=['headline', 'pub_date'],\n            name='test_idx',\n            db_tablespace='pg_default',\n            opclasses=['varchar_pattern_ops', 'date_ops'],\n            condition=Q(published=True),\n            include=['author'],\n        )\n        path, args, kwargs = index.deconstruct()\n        self.assertEqual(path, 'django.db.models.Index')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs['fields'], ['headline', 'pub_date'])\n        self.assertEqual(kwargs['name'], 'test_idx')\n        self.assertEqual(kwargs['db_tablespace'], 'pg_default')\n        self.assertEqual(kwargs['opclasses'], ['varchar_pattern_ops', 'date_ops'])\n        self.assertEqual(kwargs['condition'], Q(published=True))\n        self.assertEqual(kwargs['include'], ('author',))\n", "    def test_index_creation_with_valid_parameters(self):\n        index = Index(\n            name='test_index',\n            fields=['field1', 'field2'],\n            db_tablespace='pg_default',\n            opclasses=['opclass1', 'opclass2'],\n            condition=Q(field1__isnull=False),\n            include=['field3', 'field4'],\n        )\n        self.assertEqual(index.name, 'test_index')\n        self.assertEqual(index.fields, ['field1', 'field2'])\n        self.assertEqual(index.db_tablespace, 'pg_default')\n        self.assertEqual(index.opclasses, ['opclass1', 'opclass2'])\n        self.assertEqual(index.condition, Q(field1__isnull=False))\n        self.assertEqual(index.include, ('field3', 'field4'))\n", "    def test_deconstruct(self):\n        index = Index(\n            name='test_index',\n            fields=['field1', 'field2'],\n            db_tablespace='pg_default',\n            opclasses=['text_pattern_ops', 'varchar_pattern_ops'],\n            condition=Q(field1__isnull=False),\n            include=['field3']\n        )\n        path, args, kwargs = index.deconstruct()\n        self.assertEqual(path, 'your_module.Index')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {\n            'name': 'test_index',\n            'fields': ['field1', 'field2'],\n            'db_tablespace': 'pg_default',\n            'opclasses': ['text_pattern_ops', 'varchar_pattern_ops'],\n            'condition': Q(field1__isnull=False),\n            'include': ('field3',)\n        })\n"], "sample_129": ["    def test_addslashes(self):\n        self.assertEqual(addslashes('O\\'Reilly'), \"O\\\\'Reilly\")\n        self.assertEqual(addslashes('\"Hello\"'), '\\\\\"Hello\\\\\"')\n        self.assertEqual(addslashes('\\\\Backslash'), '\\\\\\\\Backslash')\n", "    def test_lower(self):\n        from django.template.defaultfilters import lower\n        self.assertEqual(lower('TEST'), 'test')\n        self.assertEqual(lower('Test'), 'test')\n        self.assertEqual(lower('test'), 'test')\n        self.assertEqual(lower('123TEST'), '123test')\n        self.assertEqual(lower('test123'), 'test123')\n", "    def test_addslashes(self):\n        self.assertEqual(addslashes('He said, \"Hello!\"'), 'He said, \\\\\"Hello!\\\\\"')\n        self.assertEqual(addslashes(\"It's a test\"), \"It\\\\'s a test\")\n        self.assertEqual(addslashes('Back\\\\slash'), 'Back\\\\\\\\slash')\n", "    def test_capfirst(self):\n        self.assertEqual(capfirst(\"hello world\"), \"Hello world\")\n        self.assertEqual(capfirst(\"Hello World\"), \"Hello World\")\n        self.assertEqual(capfirst(\"\"), \"\")\n        self.assertEqual(capfirst(\"h\"), \"H\")\n        self.assertEqual(capfirst(123), \"123\")\n", "    def test_escapejs_filter(self):\n        self.assertEqual(escapejs_filter('This is a \"test\" string.'), 'This is a \\\\\"test\\\\\" string.')\n        self.assertEqual(escapejs_filter(\"This is a 'test' string.\"), \"This is a \\\\'test\\\\' string.\")\n        self.assertEqual(escapejs_filter(\"Special characters: <>&\"), \"Special characters: \\\\u003C\\\\u003E\\\\u0026\")\n"], "sample_130": ["    def test_chain_clone(self):\n        query = Query(Author)\n        chained_query = query.chain(using='default')\n        self.assertIsInstance(chained_query, Query)\n        self.assertNotEqual(id(query), id(chained_query))\n        self.assertEqual(chained_query.using, 'default')\n        self.assertEqual(chained_query.sql, query.sql)\n        self.assertEqual(chained_query.params, query.params)\n", "def test_combined_queries(self):\n    query1 = Query(Author)\n    query1.add_filter(Q(num__gt=2))\n    query2 = Query(Author)\n    query2.add_filter(Q(num__lt=5))\n    query1.combine(query2, connector=OR)\n    \n    combined_where = query1.where\n    self.assertEqual(combined_where.connector, OR)\n    \n    num_gt_lookup = combined_where.children[0].children[0]\n    num_lt_lookup = combined_where.children[1].children[0]\n    self.assertIsInstance(num_gt_lookup, GreaterThan)\n    self.assertIsInstance(num_lt_lookup, LessThan)\n    self.assertEqual(num_gt_lookup.rhs, 2)\n    self.assertEqual(num_lt_lookup.rhs, 5)\n    self.assertEqual(num_gt_lookup.lhs.target, Author._meta.get_field('num'))\n    self.assertEqual(num_lt_lookup.lhs.target, Author._meta.get_field('num'))\n", "    def test_rawquery_get_columns(self):\n        query = RawQuery(\"SELECT * FROM mytable\", using='default')\n        connection = connections['default']\n        connection.ops.introspection.identifier_converter = lambda x: x.upper()\n        cursor_mock = connection.cursor()\n        cursor_mock.description = [('col1',), ('col2',)]\n        query.cursor = cursor_mock\n        columns = query.get_columns()\n        self.assertEqual(columns, ['COL1', 'COL2'])\n", "def test_query_clone_with_annotations(self):\n    query = Query(Author)\n    query.add_annotation(Count('id'), alias='total')\n    clone = query.clone()\n    self.assertEqual(clone.annotations, query.annotations)\n    self.assertEqual(clone.annotation_select_mask, query.annotation_select_mask)\n    self.assertEqual(clone._annotation_select_cache, query._annotation_select_cache)\n", "    def test_add_annotation(self):\n        query = Query(Author)\n        query.add_annotation(Count('id'), alias='id_count', is_summary=True)\n        self.assertIn('id_count', query.annotations)\n        self.assertIsInstance(query.annotations['id_count'], Count)\n        self.assertEqual(query.annotations['id_count'].source_expressions[0].name, 'id')\n"], "sample_131": ["    def test_serialize_db_to_string(self):\n        creation = connection.creation_class(connection)\n        with mock.patch('django.apps.apps.get_app_configs', return_value=[]):\n            serialized_data = creation.serialize_db_to_string()\n            self.assertEqual(serialized_data, '[]')\n", "    def test_serialize_db_to_string(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                with mock.patch('django.core.serializers.serialize', return_value='serialized_data'):\n                    serialized_data = creation.serialize_db_to_string()\n                    self.assertEqual(serialized_data, 'serialized_data')\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_serialize_db_to_string(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'), \\\n                 mock.patch('django.core.serializers.serialize'), \\\n                 mock.patch.object(creation, 'serialize_db_to_string', return_value='{}') as mock_serialize:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n                mock_serialize.assert_called_once()\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_create_test_db_with_serialize(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'), \\\n                 mock.patch.object(creation, 'serialize_db_to_string', return_value='{}') as mocked_serialize:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n            mocked_serialize.assert_called_once()\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_serialize_db_to_string(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        with mock.patch.object(creation, 'serialize_db_to_string', return_value='{}') as mocked_serialize:\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n        mocked_serialize.assert_called_once()\n"], "sample_132": ["    def test_get_safe_settings(self):\n        \"\"\"\n        Test that get_safe_settings returns a dictionary with sensitive settings cleansed.\n        \"\"\"\n        with self.settings(SECRET_KEY='super_secret', API_KEY='api_key_value', NON_SENSITIVE='non_sensitive_value'):\n            filter = SafeExceptionReporterFilter()\n            safe_settings = filter.get_safe_settings()\n            self.assertEqual(safe_settings['SECRET_KEY'], filter.cleansed_substitute)\n            self.assertEqual(safe_settings['API_KEY'], filter.cleansed_substitute)\n            self.assertEqual(safe_settings['NON_SENSITIVE'], 'non_sensitive_value')\n", "    def setUp(self):\n        self.reporter_filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.reporter_filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.reporter_filter = SafeExceptionReporterFilter()\n"], "sample_135": ["    def test_iso_8601_year(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        self.assertEqual(dateformat.format(my_birthday, 'o'), '1979')\n", "    def test_iso_8601_year_number(self):\n        dt = datetime(2023, 1, 2)  # This is in the first ISO week of 2023\n        self.assertEqual(dateformat.format(dt, 'o'), '2023')\n        \n        dt = datetime(2022, 12, 31)  # This is in the last ISO week of 2022\n        self.assertEqual(dateformat.format(dt, 'o'), '2022')\n        \n        dt = datetime(2023, 1, 1)  # This is in the last ISO week of 2022\n        self.assertEqual(dateformat.format(dt, 'o'), '2022')\n", "    def test_invalid_date_format_specifiers(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30)\n\n        for specifier in ['b', 'c', 'd', 'D', 'E', 'F', 'j', 'l', 'L', 'm', 'M', 'n', 'N', 'o', 't', 'U', 'w', 'W', 'y', 'Y', 'z']:\n            msg = (\n                \"The format for time objects may not contain date-related \"\n                \"format specifiers (found '%s').\" % specifier\n            )\n            with self.assertRaisesMessage(TypeError, msg):\n                dateformat.time_format(dt.time(), specifier)\n", "    def test_iso_8601_format(self):\n        dt = datetime(2023, 3, 15, 14, 30, 45, 678910)\n        self.assertEqual(dateformat.format(dt, 'c'), '2023-03-15T14:30:45.678910')\n", "def test_iso_year_number(self):\n    # Test ISO 8601 year number matching the ISO week number\n    dt = datetime(2021, 1, 1)  # ISO week 53 of 2020\n    self.assertEqual(dateformat.format(dt, 'o'), '2020')\n    \n    dt = datetime(2021, 1, 4)  # ISO week 1 of 2021\n    self.assertEqual(dateformat.format(dt, 'o'), '2021')\n\n    dt = datetime(2022, 1, 1)  # ISO week 52 of 2021\n    self.assertEqual(dateformat.format(dt, 'o'), '2021')\n\n    dt = datetime(2022, 1, 3)  # ISO week 1 of 2022\n    self.assertEqual(dateformat.format(dt, 'o'), '2022')\n"], "sample_134": ["    def test_serialize_custom_class(self):\n        class CustomClass:\n                self.value = value\n\n                return (\n                    '%s.%s' % (self.__class__.__module__, self.__class__.__name__),\n                    [self.value],\n                    {}\n                )\n\n        instance = CustomClass('test')\n        string, imports = MigrationWriter.serialize(instance)\n        self.assertEqual(string, \"migrations.test_writer.CustomClass('test')\")\n        self.assertEqual(imports, {'import migrations.test_writer'})\n        self.assertSerializedEqual(instance)\n", "    def test_serialize_deconstructable(self):\n        @deconstructible\n        class DeconstructibleWithArguments:\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n                path = \"%s.%s\" % (self.__class__.__module__, self.__class__.__name__)\n                return (path, [self.arg1, self.arg2], {})\n\n        instance = DeconstructibleWithArguments(\"value1\", \"value2\")\n        self.assertSerializedResultEqual(\n            instance,\n            (\n                \"migrations.test_writer.DeconstructibleWithArguments('value1', 'value2')\",\n                {'import migrations.test_writer'},\n            )\n        )\n        self.serialize_round_trip(instance)\n", "    def test_serialize_custom_deconstructable_instance(self):\n        @deconstructible\n        class CustomClass:\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n                return (\n                    'migrations.test_writer.CustomClass',\n                    [self.arg1, self.arg2],\n                    {}\n                )\n\n        custom_instance = CustomClass('value1', 'value2')\n        self.assertSerializedResultEqual(\n            custom_instance,\n            (\n                \"migrations.test_writer.CustomClass('value1', 'value2')\",\n                {'import migrations.test_writer'}\n            )\n        )\n        self.assertSerializedEqual(custom_instance)\n", "    def test_serialize_function_type_method(self):\n        class TestClass:\n            @staticmethod\n                return \"static\"\n            \n            @classmethod\n                return \"class\"\n        \n        # Test serializing a static method\n        self.assertSerializedResultEqual(\n            TestClass.static_method,\n            (\"migrations.test_writer.TestClass.static_method\", {\"import migrations.test_writer\"})\n        )\n        \n        # Test serializing a class method\n        self.assertSerializedResultEqual(\n            TestClass.class_method,\n            (\"migrations.test_writer.TestClass.class_method\", {\"import migrations.test_writer\"})\n        )\n", "    def test_serialize_custom_deconstructable_instance(self):\n        class CustomDeconstructable:\n                return ('CustomDeconstructable', [], {})\n        \n        value = CustomDeconstructable()\n        string, imports = MigrationWriter.serialize(value)\n        self.assertEqual(string, \"CustomDeconstructable()\")\n        self.assertEqual(imports, set())\n"], "sample_136": ["    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        signer = signing.get_cookie_signer(salt='key_salt')\n        signed_cookie = signer.sign('cookie_value')\n        request.COOKIES['test_cookie'] = signed_cookie\n        \n        # Test retrieving a valid signed cookie\n        self.assertEqual(request.get_signed_cookie('test_cookie', salt='salt'), 'cookie_value')\n        \n        # Test retrieving a non-existing cookie with a default value\n        self.assertEqual(request.get_signed_cookie('non_existing_cookie', default='default_value'), 'default_value')\n        \n        # Test retrieving a non-existing cookie without a default value should raise KeyError\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('non_existing_cookie')\n        \n        # Test retrieving an invalid signed cookie should raise signing.BadSignature\n        request.COOKIES['invalid_cookie'] = 'invalid_value'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('invalid_cookie', salt='salt')\n", "    def test_get_signed_cookie(self):\n        # Signed cookie with valid signature\n        request = HttpRequest()\n        request.COOKIES['my_cookie'] = signing.get_cookie_signer(salt='my_cookie').sign('cookie_value')\n        self.assertEqual(request.get_signed_cookie('my_cookie'), 'cookie_value')\n\n        # Signed cookie with invalid signature raises BadSignature\n        request.COOKIES['my_cookie'] = 'invalid_signature'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('my_cookie')\n\n        # Signed cookie with invalid signature returns default value\n        self.assertEqual(request.get_signed_cookie('my_cookie', default='default_value'), 'default_value')\n\n        # Signed cookie with expired signature raises BadSignature\n        expired_cookie = signing.get_cookie_signer(salt='my_cookie').sign('cookie_value')\n        request.COOKIES['my_cookie'] = expired_cookie\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('my_cookie', max_age=-1)  # Negative max_age to force expiry\n\n        # Signed cookie with expired signature returns default value\n        self.assertEqual(request.get_signed_cookie('my_cookie', default='default_value', max_age=-1), 'default_value')\n", "    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        signer = signing.get_cookie_signer(salt='test_salt')\n        signed_cookie = signer.sign('cookie_value')\n        \n        # Test signed cookie retrieval\n        request.COOKIES['test_cookie'] = signed_cookie\n        self.assertEqual(request.get_signed_cookie('test_cookie', salt='test_salt'), 'cookie_value')\n        \n        # Test signed cookie with incorrect signature\n        request.COOKIES['test_cookie'] = signed_cookie + 'tampered'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('test_cookie', salt='test_salt')\n        \n        # Test signed cookie with missing cookie\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('missing_cookie', salt='test_salt')\n        \n        # Test signed cookie with default value on missing cookie\n        self.assertEqual(request.get_signed_cookie('missing_cookie', default='default_value', salt='test_salt'), 'default_value')\n        \n        # Test signed cookie with expired max_age\n        signed_cookie = signer.sign('cookie_value', max_age=1)\n        request.COOKIES['test_cookie'] = signed_cookie\n        with self.assertRaises(signing.SignatureExpired):\n            request.get_signed_cookie('test_cookie', salt='test_salt', max_age=1)\n", "    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES['test_cookie'] = signing.get_cookie_signer(salt='test').sign('cookie_value')\n        self.assertEqual(request.get_signed_cookie('test_cookie', salt='test'), 'cookie_value')\n\n        # Test with default value\n        self.assertEqual(request.get_signed_cookie('nonexistent_cookie', default='default_value'), 'default_value')\n\n        # Test with an invalid signature\n        request.COOKIES['bad_cookie'] = 'bad_value'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('bad_cookie', salt='test')\n\n        # Test with expired cookie\n        signed_cookie = signing.get_cookie_signer(salt='test').sign('cookie_value', max_age=-1)\n        request.COOKIES['expired_cookie'] = signed_cookie\n        with self.assertRaises(signing.SignatureExpired):\n            request.get_signed_cookie('expired_cookie', salt='test', max_age=10)\n", "    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES['my_cookie'] = signing.get_cookie_signer(salt='my_cookie').sign('cookie_value')\n        self.assertEqual(request.get_signed_cookie('my_cookie'), 'cookie_value')\n\n        # Test with bad signature\n        request.COOKIES['my_cookie'] = 'bad_value'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('my_cookie')\n\n        # Test with default value\n        self.assertEqual(request.get_signed_cookie('non_existent_cookie', default='default_value'), 'default_value')\n        \n        # Test with exception raised when default is not provided\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('non_existent_cookie')\n"], "sample_139": ["    def test_has_module_permission(self):\n        \"\"\"\n        Test the has_module_permission method for ModelAdmin.\n        \"\"\"\n        class TestAdmin(ModelAdmin):\n            model = Parent\n\n        ma = TestAdmin(Parent, admin.site)\n\n        # Create a regular user\n        user = User.objects.create_user(username='testuser', password='password', email='user@example.com')\n\n        # Regular user should not have module permission\n        request = self.factory.get('/')\n        request.user = user\n        self.assertFalse(ma.has_module_permission(request))\n\n        # Superuser should have module permission\n        request.user = self.superuser\n        self.assertTrue(ma.has_module_permission(request))\n", "def test_formfield_for_foreignkey_with_autocomplete(self):\n        \"\"\"\n        Test that formfield_for_foreignkey returns the correct widget when the\n        field is in the autocomplete_fields list.\n        \"\"\"\n        class AutocompleteBandAdmin(admin.ModelAdmin):\n            autocomplete_fields = ['genres']\n\n        m = AutocompleteBandAdmin(Band, custom_site)\n        request = self.factory.get('/band/')\n        request.user = self.superuser\n        db_field = Band._meta.get_field('genres')\n\n        form_field = m.formfield_for_foreignkey(db_field, request)\n        self.assertIsInstance(form_field.widget, AutocompleteSelect)\n\n        \"\"\"\n        Test that formfield_for_foreignkey returns the correct widget when the\n        field is in the raw_id_fields list.\n        \"\"\"\n        class RawIdBandAdmin(admin.ModelAdmin):\n            raw_id_fields = ['genres']\n\n        m = RawIdBandAdmin(Band, custom_site)\n        request = self.factory.get('/band/')\n        request.user = self.superuser\n        db_field = Band._meta.get_field('genres')\n\n        form_field = m.formfield_for_foreignkey(db_field, request)\n        self.assertIsInstance(form_field.widget, widgets.ForeignKeyRawIdWidget)\n\n        \"\"\"\n        Test that formfield_for_foreignkey returns the correct widget when the\n        field is in the radio_fields dict.\n        \"\"\"\n        class RadioFieldsBandAdmin(admin.ModelAdmin):\n            radio_fields = {'genres': admin.HORIZONTAL}\n\n        m = RadioFieldsBandAdmin(Band, custom_site)\n        request = self.factory.get('/band/')\n        request.user = self.superuser\n        db_field = Band._meta.get_field('genres')\n\n        form_field = m.formfield_for_foreignkey(db_field, request)\n        self.assertIsInstance(form_field.widget, widgets.AdminRadioSelect)\n", "    def test_get_view_on_site_url(self):\n        \"\"\"\n        Test if get_view_on_site_url returns the correct URL or None.\n        \"\"\"\n        parent = Parent.objects.create(name='Test Parent')\n        custom_site.register(Parent, ParentAdmin)\n        admin_instance = custom_site._registry[Parent]\n        request = self.factory.get('/parent/')\n        request.user = self.superuser\n\n        # Test when view_on_site is True and object has get_absolute_url method.\n        obj_url = reverse('admin:view_on_site', kwargs={\n            'content_type_id': get_content_type_for_model(parent).pk,\n            'object_id': parent.pk\n        })\n        self.assertEqual(admin_instance.get_view_on_site_url(parent), obj_url)\n\n        # Test when view_on_site is a callable.\n        admin_instance.view_on_site = lambda obj: f'/custom_url/{obj.pk}/'\n        self.assertEqual(admin_instance.get_view_on_site_url(parent), f'/custom_url/{parent.pk}/')\n\n        # Test when view_on_site is False.\n        admin_instance.view_on_site = False\n        self.assertIsNone(admin_instance.get_view_on_site_url(parent))\n\n        # Test when object is None.\n        self.assertIsNone(admin_instance.get_view_on_site_url())\n", "def test_inline_model_admin_get_formset(self):\n    \"\"\"\n    Test InlineModelAdmin's get_formset method for correct behavior.\n    \"\"\"\n    parent = Parent.objects.create(name='parent')\n    child = Child.objects.create(name='child', parent=parent)\n    request = self.factory.get('/child/')\n    request.user = self.superuser\n\n    class TestInlineAdmin(admin.TabularInline):\n        model = Child\n        fk_name = 'parent'\n        extra = 1\n\n    inline_admin = TestInlineAdmin(Parent, custom_site)\n    formset_class = inline_admin.get_formset(request)\n    formset = formset_class(instance=parent)\n    \n    self.assertEqual(formset.model, Child)\n    self.assertEqual(formset.fk.name, 'parent')\n    self.assertEqual(formset.extra, 1)\n    self.assertEqual(len(formset.forms), 1)\n", "def test_formfield_for_dbfield_with_formfield_overrides(self):\n    \"\"\"\n    Test that formfield_for_dbfield correctly applies formfield_overrides.\n    \"\"\"\n    class MyModel(models.Model):\n        name = models.CharField(max_length=100)\n        age = models.IntegerField()\n\n    class MyModelAdmin(admin.ModelAdmin):\n        formfield_overrides = {\n            models.CharField: {'widget': forms.Textarea},\n            models.IntegerField: {'widget': forms.HiddenInput},\n        }\n\n    model_admin = MyModelAdmin(MyModel, custom_site)\n    request = self.factory.get('/mymodel/')\n    request.user = self.superuser\n\n    name_field = MyModel._meta.get_field('name')\n    age_field = MyModel._meta.get_field('age')\n\n    name_formfield = model_admin.formfield_for_dbfield(name_field, request)\n    age_formfield = model_admin.formfield_for_dbfield(age_field, request)\n\n    self.assertIsInstance(name_formfield.widget, forms.Textarea)\n    self.assertIsInstance(age_formfield.widget, forms.HiddenInput)\n"], "sample_137": ["    def test_replace_named_groups(self):\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)$'\n        replaced_pattern = replace_named_groups(pattern)\n        expected_pattern = '^<a>/b/<c>$'\n        self.assertEqual(replaced_pattern, expected_pattern)\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n        replaced_pattern = replace_named_groups(pattern)\n        self.assertEqual(replaced_pattern, '^<a>/b/<c>/$')\n", "    def test_replace_named_groups(self):\n        # Test case 1\n        pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n        expected_output = r'^<a>/b/(\\w+)$'\n        self.assertEqual(replace_named_groups(pattern), expected_output)\n\n        # Test case 2\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n        expected_output = r'^<a>/b/<c>/$'\n        self.assertEqual(replace_named_groups(pattern), expected_output)\n\n        # Test case 3\n        pattern = r'^(?P<a>\\w+)/b/(\\w+)'\n        expected_output = r'^<a>/b/(\\w+)'\n        self.assertEqual(replace_named_groups(pattern), expected_output)\n\n        # Test case 4\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n        expected_output = r'^<a>/b/<c>'\n        self.assertEqual(replace_named_groups(pattern), expected_output)\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<name>\\w+)/(?P<age>\\d+)$'\n        expected_output = '^<name>/<age>$'\n        self.assertEqual(replace_named_groups(pattern), expected_output)\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>(x|y))/(\\w+)$'\n        expected_output = r'^<a>/b/<c>/(\\w+)$'\n        self.assertEqual(replace_named_groups(pattern), expected_output)\n"], "sample_140": ["    def test_sensitive_variables_all(self):\n        @sensitive_variables()\n            return password, credit_card\n\n        wrapper = test_func.__wrapped__\n        self.assertEqual(test_func.sensitive_variables, '__ALL__')\n        self.assertEqual(wrapper.__name__, 'test_func')\n        self.assertEqual(test_func('secret_password', '1234-5678-9012-3456'), ('secret_password', '1234-5678-9012-3456'))\n", "    def test_sensitive_variables_decorator_with_variable_names(self):\n        \"\"\"\n        Test that the sensitive_variables decorator properly marks specified variables as sensitive.\n        \"\"\"\n        @sensitive_variables('password', 'credit_card')\n            password = user.pass_word\n            credit_card = user.credit_card_number\n            return password, credit_card\n\n        wrapper = test_func.__wrapped__\n        self.assertEqual(wrapper.sensitive_variables, ('password', 'credit_card'))\n", "    def test_sensitive_variables_with_specified_variables(self):\n        @sensitive_variables('password', 'secret')\n            return password, secret, non_sensitive\n\n        wrapped_func = test_func\n        self.assertEqual(wrapped_func.sensitive_variables, ('password', 'secret'))\n        result = wrapped_func('pass123', 'secret123', 'not_secret')\n        self.assertEqual(result, ('pass123', 'secret123', 'not_secret'))\n", "    def test_sensitive_variables_with_specific_vars(self):\n        @sensitive_variables('password', 'credit_card')\n            return f\"{username}, {password}, {credit_card}\"\n        \n        wrapped_func = test_func.__wrapped__\n        self.assertEqual(test_func.sensitive_variables, ('password', 'credit_card'))\n        self.assertEqual(wrapped_func('user', 'pass', '1234-5678-9876-5432'), \"user, pass, 1234-5678-9876-5432\")\n    ", "    def test_sensitive_variables_decorator_with_specific_variables(self):\n        @sensitive_variables('password', 'credit_card')\n            password = user['password']\n            credit_card = user['credit_card']\n            raise ValueError(\"Sensitive variables test\")\n\n        user = {'password': 'secret', 'credit_card': '1234-5678-9012-3456'}\n        try:\n            test_func(user)\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn('password', html)\n        self.assertIn('credit_card', html)\n        self.assertNotIn('secret', html)\n        self.assertNotIn('1234-5678-9012-3456', html)\n"], "sample_141": ["    def test_progress_bar_update(self):\n        output = StringIO()\n        progress_bar = ProgressBar(output, total_count=100)\n        \n        # Update progress bar to 50%\n        progress_bar.update(50)\n        self.assertIn('[..................................................                         ]', output.getvalue())\n        \n        # Update progress bar to 100%\n        output = StringIO()\n        progress_bar = ProgressBar(output, total_count=100)\n        progress_bar.update(100)\n        self.assertIn('[...........................................................................]', output.getvalue())\n        self.assertIn('\\n', output.getvalue())\n", "    def test_progress_bar_update(self):\n        \"\"\"\n        Test the ProgressBar update function to ensure it properly updates\n        the progress bar output.\n        \"\"\"\n        import io\n        output = io.StringIO()\n        progress_bar = ProgressBar(output, total_count=4)\n        progress_bar.update(1)\n        progress_bar.update(2)\n        progress_bar.update(3)\n        progress_bar.update(4)\n        output_value = output.getvalue()\n        expected_output = \"[.................]\".replace(\".\", \" \") + \"[....              ]\".replace(\".\", \" \") + \"[........         ]\".replace(\".\", \" \") + \"[...............]\\n\".replace(\".\", \" \")\n        self.assertEqual(output_value, expected_output)\n", "    def test_progress_bar_updates_correctly(self):\n        output = StringIO()\n        total_count = 10\n        progress_bar = ProgressBar(output, total_count)\n        \n        # Call update for each count and check the output\n        for count in range(1, total_count + 1):\n            progress_bar.update(count)\n            output.seek(0)\n            expected_done = count * progress_bar.progress_width // total_count\n            expected_output = '[' + '.' * expected_done + ' ' * (progress_bar.progress_width - expected_done) + ']'\n            if count == total_count:\n                expected_output += '\\n'\n            self.assertIn(expected_output, output.getvalue())\n            output.truncate(0)\n            output.seek(0)\n            ", "    def test_progress_bar_update(self):\n        \"\"\"\n        Test the ProgressBar update method to ensure correct updating and output.\n        \"\"\"\n        import io\n        output = io.StringIO()\n        progress_bar = ProgressBar(output, 100)\n        progress_bar.update(10)\n        self.assertEqual(output.getvalue(), '[...........                                                                   ]')\n        \n        output.truncate(0)\n        output.seek(0)\n        \n        progress_bar.update(50)\n        self.assertEqual(output.getvalue().strip(), '[..........................................................                ]')\n        \n        output.truncate(0)\n        output.seek(0)\n        \n        progress_bar.update(100)\n        self.assertEqual(output.getvalue().strip(), '[...........................................................................]')\n", "    def test_deserialized_object_repr(self):\n        \"\"\"\n        Test the __repr__ method of the DeserializedObject class to ensure it returns the correct string representation.\n        \"\"\"\n        class MockModel:\n            _meta = type('meta', (), {'label': 'mockapp.mockmodel', 'pk': 1})\n                self.pk = pk\n\n        mock_instance = MockModel(1)\n        deserialized_obj = DeserializedObject(mock_instance)\n        self.assertEqual(repr(deserialized_obj), \"<DeserializedObject: mockapp.mockmodel(pk=1)>\")\n"], "sample_142": ["def test_modelform_factory_no_fields_no_exclude(self):\n    with self.assertRaises(ImproperlyConfigured):\n        modelform_factory(Song)\n", "    def test_model_form_factory_missing_fields_and_exclude(self):\n        with self.assertRaises(ImproperlyConfigured) as cm:\n            modelform_factory(Book)\n        self.assertEqual(\n            str(cm.exception),\n            \"Calling modelform_factory without defining 'fields' or 'exclude' explicitly is prohibited.\"\n        )\n", "    def test_construct_instance(self):\n        from django.db import models\n\n        class TestModel(models.Model):\n            name = models.CharField(max_length=50)\n            age = models.IntegerField()\n            description = models.TextField()\n            file = models.FileField(upload_to='uploads/')\n\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = TestModel\n                fields = ['name', 'age', 'description', 'file']\n\n        instance = TestModel(name=\"Initial\", age=25, description=\"Initial Description\")\n        form_data = {'name': 'Updated', 'age': 30, 'description': 'Updated Description'}\n        file_mock = forms.FileField()\n        form = TestForm(data=form_data, files={'file': file_mock})\n\n        form.is_valid()\n        updated_instance = construct_instance(form, instance)\n\n        self.assertEqual(updated_instance.name, 'Updated')\n        self.assertEqual(updated_instance.age, 30)\n        self.assertEqual(updated_instance.description, 'Updated Description')\n", "    def test_construct_instance(self):\n        from django.test import TestCase\n        from .models import Author, Book\n        from .forms import construct_instance\n\n        class BookForm(forms.ModelForm):\n            class Meta:\n                model = Book\n                fields = ['title', 'author']\n\n        class ConstructInstanceTestCase(TestCase):\n                author = Author.objects.create(name=\"Author Name\")\n                book = Book()\n                form_data = {'title': 'Book Title', 'author': author.id}\n                form = BookForm(data=form_data)\n                self.assertTrue(form.is_valid())\n                instance = construct_instance(form, book)\n                self.assertEqual(instance.title, 'Book Title')\n                self.assertEqual(instance.author, author)\n\n                author = Author.objects.create(name=\"Author Name\")\n                book = Book()\n                form_data = {'title': 'Book Title', 'author': author.id}\n                form = BookForm(data=form_data)\n                self.assertTrue(form.is_valid())\n                instance = construct_instance(form, book, exclude=['author'])\n                self.assertEqual(instance.title, 'Book Title')\n                self.assertIsNone(instance.author)\n\n                from django.core.files.uploadedfile import SimpleUploadedFile\n                from .models import BookWithFile\n\n                class BookWithFileForm(forms.ModelForm):\n                    class Meta:\n                        model = BookWithFile\n                        fields = ['title', 'file']\n\n                book = BookWithFile()\n                file_data = SimpleUploadedFile(\"file.txt\", b\"file_content\")\n                form_data = {'title': 'Book Title'}\n                form = BookWithFileForm(data=form_data, files={'file': file_data})\n                self.assertTrue(form.is_valid())\n                instance = construct_instance(form, book)\n                self.assertEqual(instance.title, 'Book Title')\n                self.assertEqual(instance.file.name, 'file.txt')\n", "def test_modelform_factory_with_valid_model(self):\n    \"\"\"\n    Test modelform_factory with valid model to ensure it correctly creates\n    a ModelForm class with the specified fields.\n    \"\"\"\n    from django.db import models\n    from django import forms\n    from django.forms.models import modelform_factory\n\n    class Author(models.Model):\n        name = models.CharField(max_length=100)\n        birth_date = models.DateField()\n\n    AuthorForm = modelform_factory(Author, fields=[\"name\", \"birth_date\"])\n\n    form = AuthorForm(data={\"name\": \"John Doe\", \"birth_date\": \"1980-01-01\"})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data[\"name\"], \"John Doe\")\n    self.assertEqual(form.cleaned_data[\"birth_date\"], models.DateField().to_python(\"1980-01-01\"))\n"], "sample_143": ["def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces(\"CamelCase\"), \"camel case\")\n        self.assertEqual(text.camel_case_to_spaces(\"camelCaseToSpaces\"), \"camel case to spaces\")\n        self.assertEqual(text.camel_case_to_spaces(\"CamelCaseWithAcronymsABC\"), \"camel case with acronyms abc\")\n        self.assertEqual(text.camel_case_to_spaces(\"\"), \"\")\n        self.assertEqual(text.camel_case_to_spaces(\"singleWord\"), \"singleword\")\n        self.assertEqual(text.camel_case_to_spaces(\"MultipleWordsInCamelCase\"), \"multiple words in camel case\")\n        self.assertEqual(text.camel_case_to_spaces(lazystr(\"CamelCaseToTestLazy\")), \"camel case to test lazy\")\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('CamelCaseString'), 'camel case string')\n        self.assertEqual(text.camel_case_to_spaces('camelCaseString'), 'camel case string')\n        self.assertEqual(text.camel_case_to_spaces('CamelCamelCase'), 'camel camel case')\n        self.assertEqual(text.camel_case_to_spaces('Camel2Camel2Case'), 'camel2 camel2 case')\n        self.assertEqual(text.camel_case_to_spaces('getHTTPResponseCode'), 'get http response code')\n        self.assertEqual(text.camel_case_to_spaces('get2HTTPResponseCode'), 'get2 http response code')\n        self.assertEqual(text.camel_case_to_spaces('HTTPResponseCodeXYZ'), 'http response code xyz')\n        self.assertEqual(text.camel_case_to_spaces(''), '')\n        self.assertEqual(text.camel_case_to_spaces(lazystr('CamelCaseString')), 'camel case string')\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst('hello'), 'Hello')\n        self.assertEqual(text.capfirst('Hello'), 'Hello')\n        self.assertEqual(text.capfirst(''), '')\n        self.assertEqual(text.capfirst('123'), '123')\n        self.assertEqual(text.capfirst('hELLO'), 'HELLO')\n        self.assertEqual(text.capfirst(lazystr('hello')), 'Hello')\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst('hello'), 'Hello')\n        self.assertEqual(text.capfirst('Hello'), 'Hello')\n        self.assertEqual(text.capfirst(''), '')\n        self.assertEqual(text.capfirst('h'), 'H')\n        self.assertEqual(text.capfirst('1hello'), '1hello')\n        self.assertEqual(text.capfirst(lazystr('hello')), 'Hello')\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst(\"hello\"), \"Hello\")\n        self.assertEqual(text.capfirst(\"Hello\"), \"Hello\")\n        self.assertEqual(text.capfirst(\"\"), \"\")\n        self.assertEqual(text.capfirst(\"123\"), \"123\")\n        self.assertEqual(text.capfirst(\" hello\"), \" hello\")\n        self.assertEqual(text.capfirst(lazystr(\"lazy\")), \"Lazy\")\n"], "sample_144": ["    def test_model_unpickle(self):\n        \"\"\"\n        Test unpickling of a model with deferred fields.\n        \"\"\"\n        place = Place.objects.create(name=\"Some Place\", address=\"123 Main St\")\n        pickled_place = model_unpickle((place._meta.app_label, place._meta.object_name)).__reduce__()\n        self.assertEqual(pickled_place[0], model_unpickle)\n        self.assertEqual(pickled_place[1], ((place._meta.app_label, place._meta.object_name),))\n        self.assertIn(DJANGO_VERSION_PICKLE_KEY, pickled_place[2])\n\n        unpickled_place = model_unpickle((place._meta.app_label, place._meta.object_name)).__new__(Place)\n        unpickled_place.__setstate__(pickled_place[2])\n        self.assertEqual(unpickled_place.name, place.name)\n        self.assertEqual(unpickled_place.address, place.address)\n", "    def test_model_save_with_unsaved_related_object(self):\n        \"\"\"\n        Test that saving a model instance with an unsaved related object raises\n        a ValueError to prevent data loss.\n        \"\"\"\n        place = Place(name=\"Unsaved Place\", address=\"123 Nowhere\")\n        restaurant = Restaurant(\n            place_ptr=place,\n            serves_hot_dogs=True,\n            serves_pizza=True,\n        )\n        with self.assertRaises(ValueError) as context:\n            restaurant.save()\n        self.assertEqual(\n            str(context.exception),\n            \"save() prohibited to prevent data loss due to unsaved related object 'place_ptr'.\"\n        )\n", "    def test_model_repr(self):\n        \"\"\"\n        Test the __repr__ method of the Model class.\n        \"\"\"\n        place = Place(name=\"Test Place\", address=\"123 Test Street\")\n        place.save()\n        self.assertEqual(repr(place), '<Place: Place object (%s)>' % place.pk)\n\n        restaurant = Restaurant(place_ptr=place, serves_hot_dogs=True, serves_pizza=False)\n        restaurant.save()\n        self.assertEqual(repr(restaurant), '<Restaurant: Restaurant object (%s)>' % restaurant.pk)\n\n        italian_restaurant = ItalianRestaurant(restaurant_ptr=restaurant, serves_gnocchi=True)\n        italian_restaurant.save()\n        self.assertEqual(repr(italian_restaurant), '<ItalianRestaurant: ItalianRestaurant object (%s)>' % italian_restaurant.pk)\n", "    def test_model_from_db(self):\n        \"\"\"\n        Test `from_db` method for loading model instances from database values.\n        \"\"\"\n        place = Place.objects.create(name=\"Test Place\", address=\"Test Address\")\n        restaurant = Restaurant.objects.create(\n            place_ptr=place, serves_hot_dogs=True, serves_pizza=False\n        )\n\n        # Simulate loading the restaurant instance from the database.\n        loaded_restaurant = Restaurant.from_db(\n            db='default',\n            field_names=['id', 'place_ptr_id', 'serves_hot_dogs', 'serves_pizza'],\n            values=[restaurant.id, restaurant.place_ptr_id, restaurant.serves_hot_dogs, restaurant.serves_pizza]\n        )\n\n        self.assertEqual(loaded_restaurant.pk, restaurant.pk)\n        self.assertEqual(loaded_restaurant.place_ptr_id, restaurant.place_ptr_id)\n        self.assertEqual(loaded_restaurant.serves_hot_dogs, restaurant.serves_hot_dogs)\n        self.assertEqual(loaded_restaurant.serves_pizza, restaurant.serves_pizza)\n        self.assertFalse(loaded_restaurant._state.adding)\n        self.assertEqual(loaded_restaurant._state.db, 'default')\n", "    def test_model_state_fields_cache_descriptor(self):\n        class TestModel(Model):\n            class Meta:\n                app_label = 'tests'\n                abstract = True\n\n        instance = TestModel()\n        # Access fields_cache to initialize it\n        self.assertEqual(instance._state.fields_cache, {})\n        # Ensure the same dictionary is returned on subsequent accesses\n        self.assertIs(instance._state.fields_cache, instance._state.fields_cache)\n"], "sample_145": ["    def test_not_list_or_tuple(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 'name'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_list_or_tuple(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n"], "sample_146": ["    def test_inconsistent_language_settings_with_i18n_disabled(self):\n        # When USE_I18N is False, LANGUAGE_CODE consistency should not raise E004\n        for tag in ['fr', 'fr-CA', 'fr-357']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_valid_language_code_in_languages_setting(self):\n        \"\"\"Ensure LANGUAGE_CODE is valid and present in LANGUAGES setting.\"\"\"\n        valid_tags = ['en', 'fr', 'es-419', 'zh-Hans']\n        for tag in valid_tags:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag, LANGUAGES=[(tag, tag)]):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_inconsistent_language_settings_with_use_i18n_false(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        with self.settings(USE_I18N=False, LANGUAGE_CODE='fr-CA'):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_mixed_valid_invalid_languages(self):\n        msg = 'You have provided an invalid language code in the LANGUAGES setting: %r.'\n        mixed_tags = self.valid_tags + self.invalid_tags\n        for tag in mixed_tags:\n            with self.subTest(tag), self.settings(LANGUAGES=[(tag, tag)]):\n                expected_errors = []\n                if tag in self.invalid_tags:\n                    expected_errors.append(Error(msg % tag, id='translation.E002'))\n                self.assertEqual(check_setting_languages(None), expected_errors)\n", "    def test_valid_consistent_language_settings(self):\n        # Testing LANGUAGE_CODE that is consistent with LANGUAGES setting.\n        tests = [\n            'en',  # Simple language code.\n            'fr',  # Simple language code.\n            'es',  # Simple language code.\n        ]\n        for tag in tests:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_147": ["    def test_combined_queryset_with_aggregates(self):\n        qs1 = Number.objects.filter(num__lte=5)\n        qs2 = Number.objects.filter(num__gte=5)\n        combined_qs = qs1.union(qs2)\n        with self.assertRaisesMessage(\n            NotSupportedError,\n            'Calling QuerySet.aggregate() after union() is not supported.'\n        ):\n            combined_qs.aggregate(max_num=Max('num'))\n", "    def test_union_with_empty_and_non_empty_qs(self):\n        qs1 = Number.objects.filter(num__lte=5)\n        qs2 = Number.objects.none()\n        self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 4, 5], ordered=False)\n        qs3 = Number.objects.filter(num__gte=5)\n        self.assertNumbersEqual(qs1.union(qs3), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], ordered=False)\n", "    def test_combined_qs_with_values(self):\n        qs1 = Number.objects.filter(num__lte=3).values('num')\n        qs2 = Number.objects.filter(num__gte=7).values('num')\n        combined_qs = qs1.union(qs2)\n        self.assertEqual(list(combined_qs), [{'num': 0}, {'num': 1}, {'num': 2}, {'num': 3}, {'num': 7}, {'num': 8}, {'num': 9}])\n", "    def test_bulk_create(self):\n        Number.objects.bulk_create([\n            Number(num=10, other_num=0),\n            Number(num=11, other_num=-1),\n        ])\n        self.assertNumbersEqual(Number.objects.filter(num__gte=10), [10, 11], ordered=False)\n", "    def test_union_with_raw_sql(self):\n        Number.objects.bulk_create([\n            Number(num=10, other_num=0),\n            Number(num=11, other_num=0),\n        ])\n        raw_qs = Number.objects.raw('SELECT * FROM testapp_number WHERE num >= 10')\n        qs = Number.objects.filter(num__lte=1)\n        union_qs = qs.union(raw_qs)\n        self.assertNumbersEqual(union_qs.order_by('num'), [0, 1, 10, 11])\n"], "sample_148": ["    def test_prepare_lookup_value(self):\n        self.assertEqual(prepare_lookup_value('field__in', '1,2,3'), ['1', '2', '3'])\n        self.assertEqual(prepare_lookup_value('field__isnull', ''), False)\n        self.assertEqual(prepare_lookup_value('field__isnull', 'false'), False)\n        self.assertEqual(prepare_lookup_value('field__isnull', '0'), False)\n        self.assertEqual(prepare_lookup_value('field__isnull', 'true'), True)\n        self.assertEqual(prepare_lookup_value('field__isnull', '1'), True)\n        self.assertEqual(prepare_lookup_value('field__exact', 'value'), 'value')\n", "    def test_prepare_lookup_value(self):\n        \"\"\"\n        Test the prepare_lookup_value function to ensure it correctly processes\n        different types of lookup keys and values.\n        \"\"\"\n        test_data = [\n            ('field__in', 'value1,value2,value3', ['value1', 'value2', 'value3']),\n            ('field__isnull', 'False', False),\n            ('field__isnull', '', False),\n            ('field__isnull', 'false', False),\n            ('field__isnull', '0', False),\n            ('field__isnull', 'TRUE', True),\n            ('field__isnull', 'true', True),\n            ('field__isnull', '1', True),\n            ('field', 'value', 'value'),\n        ]\n        for key, value, expected in test_data:\n            with self.subTest(key=key, value=value):\n                self.assertEqual(prepare_lookup_value(key, value), expected)\n", "    def test_unquote(self):\n        self.assertEqual(unquote('something_0Aor_0Aother'), 'something\\nor\\nother')\n        self.assertEqual(unquote('hello_20world'), 'hello world')\n        self.assertEqual(unquote('simple_test_21'), 'simple_test!')\n        self.assertEqual(unquote('path_3A_to_3A_file'), 'path::to::file')\n        self.assertEqual(unquote('_5B_example_5D'), '[example]')\n", "    def test_prepare_lookup_value(self):\n        \"\"\"\n        Tests for `prepare_lookup_value` utility function.\n        \"\"\"\n        self.assertEqual(prepare_lookup_value('field__in', '1,2,3'), ['1', '2', '3'])\n        self.assertEqual(prepare_lookup_value('field__isnull', ''), False)\n        self.assertEqual(prepare_lookup_value('field__isnull', 'false'), False)\n        self.assertEqual(prepare_lookup_value('field__isnull', '0'), False)\n        self.assertEqual(prepare_lookup_value('field__isnull', 'true'), True)\n        self.assertEqual(prepare_lookup_value('field__isnull', '1'), True)\n        self.assertEqual(prepare_lookup_value('field', 'value'), 'value')\n", "    def test_unquote(self):\n        self.assertEqual(unquote('something_0Aor_0Aother'), 'something\\nor\\nother')\n        self.assertEqual(unquote('path_2Fto_2Ffile'), 'path/to/file')\n        self.assertEqual(unquote('some_3Avalue'), 'some:value')\n        self.assertEqual(unquote('escaped_5Fcharacter'), 'escaped_character')\n"], "sample_151": ["    def test_generate_altered_db_table(self):\n        \"\"\"\n        Test if generate_altered_db_table correctly detects changes in db_table.\n        \"\"\"\n        before_state = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], options={\"db_table\": \"author_old\"})\n\n        after_state = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], options={\"db_table\": \"author_new\"})\n\n        changes = self.get_changes([before_state], [after_state])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=\"author_new\")\n", "    def test_field_with_regex_validator(self):\n        \"\"\"Tests detection of changes to a field's regex validator.\"\"\"\n        from_state = ModelState(\n            \"testapp\", \"model\", [(\"id\", models.AutoField(primary_key=True, validators=[\n                RegexValidator(\n                    re.compile('^[a-z]+\\\\Z'),\n                    'Enter a valid \u201cslug\u201d consisting of letters, numbers, underscores or hyphens.',\n                    'invalid'\n                )\n            ]))]\n        )\n        to_state = ModelState(\n            \"testapp\", \"model\", [(\"id\", models.AutoField(primary_key=True, validators=[\n                RegexValidator(\n                    re.compile('^[A-Z]+\\\\Z'),\n                    'Enter a valid \u201cslug\u201d consisting of uppercase letters only.',\n                    'invalid'\n                )\n            ]))]\n        )\n        changes = self.get_changes([from_state], [to_state])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, validators=[\n            RegexValidator(\n                re.compile('^[A-Z]+\\\\Z'),\n                'Enter a valid \u201cslug\u201d consisting of uppercase letters only.',\n                'invalid'\n            )\n        ])\n", "    def test_fk_on_rename_model(self):\n        \"\"\"\n        When renaming a model that is referenced by a ForeignKey, the FK's\n        model name should be updated accordingly.\n        \"\"\"\n        before = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n            ]),\n            ModelState(\"otherapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            ]),\n        ]\n        after = [\n            ModelState(\"testapp\", \"Writer\", [\n                (\"id\", models.AutoField(primary_key=True)),\n            ]),\n            ModelState(\"otherapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Writer\", models.CASCADE)),\n            ]),\n        ]\n        changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename_model\": True}))\n        # Right number/type of migrations for model rename?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"Writer\")\n        # Right number/type of migrations for FK update?\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, model_name=\"book\", name=\"author\")\n", "    def test_generate_deleted_proxies(self):\n        \"\"\"Tests deletion of proxy models.\"\"\"\n        changes = self.get_changes([self.author_proxy, self.author_empty], [self.author_empty])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"DeleteModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"AuthorProxy\")\n", "    def test_generate_altered_db_table(self):\n        \"\"\"Tests altering the db_table of a model.\"\"\"\n        changes = self.get_changes([self.author_with_db_table_options], [self.author_with_new_db_table_options])\n        # Check if the right number/type of migrations are created\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTable'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', table='author_two')\n\n        changes = self.get_changes([self.author_with_new_db_table_options], [self.author_with_db_table_options])\n        # Check if the right number/type of migrations are created\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTable'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', table='author_one')\n"], "sample_149": ["    def test_builtin_permission_name_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'a' * 250  # Create a verbose name that will cause the permission name to exceed 255 characters.\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_custom_permission_codename_clashing_with_default(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Can add checked (duplicate with default)'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin permission \"\n                \"for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n", "    def test_duplicate_builtin_and_custom_permissions(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Can add permission (duplicate with builtin)'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n", "    def test_custom_permissions_clashing_with_builtin(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Permission that clashes with builtin add permission'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n", "    def test_no_builtin_permissions_exceeding_name_length(self):\n        class Checked(models.Model):\n            name = models.CharField(max_length=30)\n\n            class Meta:\n                permissions = _get_builtin_permissions(Checked._meta)\n\n        Permission = apps.get_model('auth', 'Permission')\n        permission_name_max_length = Permission._meta.get_field('name').max_length\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertFalse(any(\n            error.id == 'auth.E007'\n            for error in errors\n        ))\n"], "sample_152": ["    def test_cascade_with_protected_related(self):\n        \"\"\"\n        Test that a protected related object prevents the deletion of a parent\n        object with a cascading relationship.\n        \"\"\"\n        a = create_a('cascade_with_protected')\n        protected_related = R.objects.create(protect=a.protect)\n        msg = (\n            \"Cannot delete some instances of model 'R' because they are \"\n            \"referenced through protected foreign keys: 'A.protect'.\"\n        )\n        with self.assertRaisesMessage(ProtectedError, msg):\n            a.protect.delete()\n        self.assertTrue(R.objects.filter(pk=protected_related.pk).exists())\n", "    def test_set_callable(self):\n            return self.DEFAULT\n        \n        a = create_a('set_callable')\n        a.set_callable = custom_callable\n        a.save()\n        \n        a.set_callable.delete()\n        a.refresh_from_db()\n        self.assertEqual(a.set_callable, custom_callable())\n", "    def test_set_custom_value_callable(self):\n            return R.objects.create()\n\n        a = create_a('set_custom_value_callable', custom_value)\n        a.setcustomvalue.delete()\n        a = A.objects.get(pk=a.pk)\n        self.assertEqual(custom_value(), a.setcustomvalue)\n", "    def test_protect_multiple_deep(self):\n        a = create_a('protect_deep')\n        b1 = B.objects.create(protect=a.protect)\n        b2 = B.objects.create(protect=a.protect)\n        c1 = Child.objects.create(parent_ptr=b1.protect)\n        c2 = Child.objects.create(parent_ptr=b2.protect)\n        msg = (\n            \"Cannot delete some instances of model 'R' because they are \"\n            \"referenced through protected foreign keys: 'A.protect', \"\n            \"'B.protect', 'Child.parent_ptr'.\"\n        )\n        with self.assertRaisesMessage(ProtectedError, msg):\n            a.protect.delete()\n", "    def test_set_callable(self):\n        callable_set_value = lambda: R.objects.create()\n        a = create_a('set_callable')\n        a.set_callable = callable_set_value()\n        a.save()\n        a.set_callable.delete()\n        a = A.objects.get(pk=a.pk)\n        self.assertEqual(a.set_callable, callable_set_value())\n"], "sample_150": ["    def test_missing_args_message(self):\n        parser = CommandParser(missing_args_message=\"Missing required argument\", called_from_command_line=True)\n        with self.assertRaises(SystemExit):\n            parser.parse_args([])\n", "    def setUp(self):\n        self.stdout = mock.MagicMock()\n        self.stderr = mock.MagicMock()\n", "    def test_missing_args_message(self):\n        parser = CommandParser(missing_args_message=\"Missing argument\", called_from_command_line=True)\n        with self.assertRaises(SystemExit) as cm:\n            parser.parse_args([])\n        self.assertEqual(cm.exception.code, 2)\n        ", "    def test_command_error_message(self):\n        with self.assertRaises(CommandError) as cm:\n            raise CommandError(\"This is a command error.\")\n        self.assertEqual(str(cm.exception), \"This is a command error.\")\n", "    def test_missing_args_message(self):\n        parser = CommandParser(missing_args_message=\"Missing arguments\")\n        with self.assertRaises(CommandError) as cm:\n            parser.parse_args([])\n        self.assertEqual(str(cm.exception), \"Error: Missing arguments\")\n"], "sample_153": ["    def test_model_base_new_with_no_parents(self):\n        class TestModel(metaclass=ModelBase):\n            pass\n\n        self.assertIsInstance(TestModel, ModelBase)\n", "    def test_model_base_creation(self):\n        class TestModel(metaclass=ModelBase):\n            class Meta:\n                app_label = 'test_app'\n\n        self.assertIsInstance(TestModel, ModelBase)\n        self.assertEqual(TestModel._meta.app_label, 'test_app')\n", "    def test_model_initialization_with_args(self):\n        class TestModel(Model):\n            field1 = IntegerField()\n            field2 = IntegerField()\n\n            class Meta:\n                app_label = 'test_app'\n\n        obj = TestModel(1, 2)\n        self.assertEqual(obj.field1, 1)\n        self.assertEqual(obj.field2, 2)\n", "    def test_model_check_swappable(self):\n        class SwappableModel(Model):\n            class Meta:\n                swappable = 'test_app.TestModel'\n\n        with mock.patch('django.apps.apps.get_model', return_value=None) as mocked_get_model:\n            errors = SwappableModel.check()\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'models.E002')\n            mocked_get_model.assert_called_once_with('test_app.TestModel')\n", "    def test_model_initialization(self):\n        class TestModel(Model):\n            class Meta:\n                app_label = 'testapp'\n\n        instance = TestModel()\n        self.assertIsInstance(instance, TestModel)\n        self.assertTrue(hasattr(instance, '_state'))\n        self.assertTrue(instance._state.adding)\n        self.assertIsNone(instance._state.db)\n"], "sample_154": ["    def test_check_database_backends_with_no_databases(self, mocked_connections):\n        # Test when no databases argument is provided\n        result = check_database_backends()\n        self.assertEqual(result, [])\n        mocked_connections.__getitem__.assert_not_called()\n", "    def test_no_issues_when_no_databases_provided(self, mocked_connections):\n        result = check_database_backends()\n        self.assertEqual(result, [])\n        self.assertFalse(mocked_connections.__getitem__.called)\n", "    def test_no_databases_provided(self, mocked_connections):\n        result = check_database_backends(databases=None)\n        self.assertEqual(result, [])\n        self.assertFalse(mocked_connections.__getitem__.called)\n", "    def test_no_databases_provided(self, mocked_connections):\n        result = check_database_backends(databases=[])\n        self.assertEqual(result, [])\n        self.assertFalse(mocked_connections.__getitem__.called)\n", "    def test_no_issues_with_valid_connections(self, mocked_connections):\n        # Mock validation check to return no issues\n        mocked_connections.__getitem__.return_value.validation.check.return_value = []\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, [])\n"], "sample_155": ["    def test_file_response_with_custom_block_size(self):\n        \"\"\"\n        Test FileResponse with a custom block size to ensure it properly\n        streams the content in specified block sizes.\n        \"\"\"\n        class CustomFile(io.BytesIO):\n                super().__init__(*args, **kwargs)\n                self.read_calls = 0\n\n                self.read_calls += 1\n                return super().read(size)\n\n        content = b'binary content with enough data to span multiple blocks'\n        custom_file = CustomFile(content)\n        response = FileResponse(custom_file)\n        response.block_size = 10  # Set custom block size\n        result = b''.join(response)\n\n        self.assertEqual(result, content)\n        self.assertEqual(custom_file.read_calls, len(content) // response.block_size + 1)\n        self.assertEqual(response['Content-Length'], str(len(content)))\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n", "def test_file_response_with_custom_block_size(self):\n    class CustomBlockSizeFileResponse(FileResponse):\n        block_size = 8192\n\n    response = CustomBlockSizeFileResponse(io.BytesIO(b'a' * 10000))\n    self.assertEqual(response.block_size, 8192)\n    self.assertEqual(response['Content-Length'], '10000')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertFalse(response.has_header('Content-Disposition'))\n    self.assertEqual(len(list(response)), 2)  # Check that content is read in 2 chunks of 8192 and 1808 bytes\n", "def test_file_response_with_custom_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename=\"custom_name.txt\")\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "    def test_file_response_with_custom_block_size(self):\n        # Create a temporary file with a known content\n        with tempfile.NamedTemporaryFile(delete=False) as tmp:\n            tmp.write(b'A' * 8192)  # 8KB of data\n            tmp_filename = tmp.name\n\n        try:\n            response = FileResponse(open(tmp_filename, 'rb'))\n            response.block_size = 2048  # Custom block size\n\n            # Read the content from the response\n            content = b''.join(response)\n            self.assertEqual(content, b'A' * 8192)\n            self.assertEqual(response['Content-Length'], '8192')\n            self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        finally:\n            os.remove(tmp_filename)\n", "    def test_file_response_with_custom_filename(self):\n        response = FileResponse(io.BytesIO(b'binary content'), filename='custom_name.bin')\n        self.assertEqual(response['Content-Length'], '14')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.bin\"')\n        self.assertEqual(list(response), [b'binary content'])\n"], "sample_156": ["    def test_order_fields(self):\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n\n        form = TestForm()\n        self.assertEqual(list(form.fields.keys()), ['field1', 'field2', 'field3', 'field4'])\n\n        form.order_fields(['field3', 'field1'])\n        self.assertEqual(list(form.fields.keys()), ['field3', 'field1', 'field2', 'field4'])\n\n        form.order_fields(['field4', 'field2'])\n        self.assertEqual(list(form.fields.keys()), ['field4', 'field2', 'field3', 'field1'])\n\n        # Test with unknown field\n        form.order_fields(['unknown_field', 'field3', 'field1'])\n        self.assertEqual(list(form.fields.keys()), ['field3', 'field1', 'field4', 'field2'])\n\n        # Test with None\n        form.order_fields(None)\n        self.assertEqual(list(form.fields.keys()), ['field3', 'field1', 'field4', 'field2'])\n", "    def test_hidden_fields_behavior(self):\n        class HiddenFieldForm(Form):\n            hidden_field = CharField(widget=HiddenInput, initial='initial_value')\n            visible_field = CharField()\n\n        form = HiddenFieldForm()\n        self.assertEqual(form['hidden_field'].value(), 'initial_value')\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p><input type=\"hidden\" name=\"hidden_field\" value=\"initial_value\" id=\"id_hidden_field\"></p>\\n'\n            '<p><label for=\"id_visible_field\">Visible field:</label> '\n            '<input type=\"text\" name=\"visible_field\" id=\"id_visible_field\" required></p>'\n        )\n\n        data = {\n            'visible_field': 'visible_value'\n        }\n        form = HiddenFieldForm(data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('hidden_field', form.errors)\n        self.assertEqual(\n            form.errors['hidden_field'], \n            ['This field is required.']\n        )\n\n        data = {\n            'hidden_field': 'submitted_value',\n            'visible_field': 'visible_value'\n        }\n        form = HiddenFieldForm(data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['hidden_field'], 'submitted_value')\n        self.assertEqual(form.cleaned_data['visible_field'], 'visible_value')\n\n        data = {\n            'visible_field': 'visible_value'\n        }\n        form = HiddenFieldForm(data, initial={'hidden_field': 'initial_value'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['hidden_field'], 'initial_value')\n        self.assertEqual(form.cleaned_data['visible_field'], 'visible_value')\n", "    def test_order_fields(self):\n        class OrderForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n\n        # Default order\n        f = OrderForm()\n        self.assertEqual(list(f.fields.keys()), ['field1', 'field2', 'field3', 'field4'])\n\n        # Custom order\n        f.order_fields(['field3', 'field1', 'field4'])\n        self.assertEqual(list(f.fields.keys()), ['field3', 'field1', 'field4', 'field2'])\n\n        # Include non-existent field in custom order, should ignore it\n        f.order_fields(['field3', 'nonexistent', 'field1'])\n        self.assertEqual(list(f.fields.keys()), ['field3', 'field1', 'field2', 'field4'])\n", "def test_baseform_order_fields_with_invalid_field_order(self):\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n\n    p = TestForm()\n    # Reorder fields incorrectly by specifying a non-existent field.\n    p.order_fields(['field2', 'field4', 'field1'])\n    self.assertEqual(list(p.fields), ['field2', 'field1', 'field3'])\n\n    # Check the rendered form output to ensure field ordering is correct.\n    self.assertHTMLEqual(\n        p.as_table(),\n        \"\"\"<tr><th><label for=\"id_field2\">Field2:</label></th>", "    def test_form_order_fields(self):\n        # Test the form order_fields method\n        class OrderedPerson(Form):\n            first_name = CharField()\n            last_name = CharField()\n            birthday = DateField()\n            email = EmailField()\n\n        p = OrderedPerson()\n        p.order_fields(['last_name', 'first_name', 'birthday'])\n        \n        self.assertEqual(list(p.fields.keys()), ['last_name', 'first_name', 'birthday', 'email'])\n\n        # Reorder the fields again\n        p.order_fields(['birthday', 'email'])\n        self.assertEqual(list(p.fields.keys()), ['birthday', 'email', 'last_name', 'first_name'])\n"], "sample_157": ["    def test_create_test_db(self, mocked_call_command, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                test_db_name = creation.create_test_db(verbosity=1, autoclobber=True, serialize=True, keepdb=False)\n            self.assertEqual(test_db_name, creation._get_test_db_name())\n            mocked_call_command.assert_any_call('migrate', verbosity=0, interactive=False, database=test_connection.alias, run_syncdb=True)\n            mocked_call_command.assert_any_call('createcachetable', database=test_connection.alias)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        suffix = 'clone1'\n\n        with mock.patch.object(creation, '_clone_test_db') as mocked_clone_test_db:\n            creation.clone_test_db(suffix, verbosity=0, autoclobber=True, keepdb=False)\n            mocked_clone_test_db.assert_called_once_with(suffix, verbosity=0, keepdb=False)\n", "    def test_create_test_db(self, mocked_ensure_connection, mocked_call_command):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                with mock.patch.object(creation, 'serialize_db_to_string', return_value='{}'):\n                    test_db_name = creation.create_test_db(verbosity=1, autoclobber=True, serialize=True)\n                    self.assertEqual(test_db_name, creation._get_test_db_name())\n                    mocked_call_command.assert_any_call('migrate', verbosity=0, interactive=False, database=test_connection.alias, run_syncdb=True)\n                    self.assertTrue(hasattr(test_connection, '_test_serialized_contents'))\n                    self.assertEqual(test_connection._test_serialized_contents, '{}')\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_clone_test_db(self, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch.object(creation, '_clone_test_db'):\n            creation.clone_test_db(suffix='1', verbosity=0, autoclobber=True, keepdb=False)\n            creation._clone_test_db.assert_called_once_with('1', verbosity=0, keepdb=False)\n", "    def test_create_test_db(self, *args):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        try:\n            with mock.patch.object(creation, '_create_test_db'), \\\n                    mock.patch.object(creation, 'log'), \\\n                    mock.patch('django.core.management.call_command') as mocked_call_command:\n                test_db_name = creation.create_test_db(verbosity=1, autoclobber=True, serialize=True, keepdb=False)\n                self.assertIn(TEST_DATABASE_PREFIX, test_db_name)\n                mocked_call_command.assert_any_call('migrate', verbosity=0, interactive=False, database=creation.connection.alias, run_syncdb=True)\n                mocked_call_command.assert_any_call('createcachetable', database=creation.connection.alias)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(verbosity=0)\n"], "sample_158": ["    def test_foreign_key_missing_related_name(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(Target, models.CASCADE)\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [])\n", "    def test_foreign_object_with_valid_fields(self):\n        class Person(models.Model):\n            country_id = models.IntegerField(unique=True)\n            city_id = models.IntegerField(unique=True)\n\n        class Membership(models.Model):\n            person_country_id = models.IntegerField()\n            person_city_id = models.IntegerField()\n            person = models.ForeignObject(\n                Person,\n                on_delete=models.CASCADE,\n                from_fields=['person_country_id', 'person_city_id'],\n                to_fields=['country_id', 'city_id'],\n            )\n\n        field = Membership._meta.get_field('person')\n        self.assertEqual(field.check(), [])\n", "    def test_foreign_key_with_multiple_target_fields(self):\n        class Target(models.Model):\n            country_id = models.IntegerField()\n            city_id = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['country_id', 'city_id'], name='unique_country_city')\n                ]\n\n        class Model(models.Model):\n            country_id = models.IntegerField()\n            city_id = models.IntegerField()\n            foreign_key = models.ForeignObject(\n                Target,\n                on_delete=models.CASCADE,\n                from_fields=['country_id', 'city_id'],\n                to_fields=['country_id', 'city_id']\n            )\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [])\n", "    def test_unique_constraint_on_onetoonefield(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            one_to_one = models.OneToOneField(Target, models.CASCADE)\n\n        field = Model._meta.get_field('one_to_one')\n        self.assertEqual(field.check(), [])\n", "    def test_related_field_descriptor(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(Target, models.CASCADE)\n\n        field = Model._meta.get_field('foreign_key')\n        descriptor = field.descriptor_class(field)\n        instance = Model()\n        target_instance = Target(id=1)\n        \n        # Test that the descriptor correctly sets and gets the related instance\n        descriptor.__set__(instance, target_instance)\n        self.assertEqual(descriptor.__get__(instance), target_instance)\n\n        # Test that the descriptor raises a ValueError when attempting to set an invalid related instance\n        with self.assertRaises(ValueError):\n            descriptor.__set__(instance, \"invalid_instance\")\n"], "sample_159": ["    def test_non_empty_default_permissions(self):\n        class Checked(models.Model):\n            class Meta:\n                default_permissions = ('add', 'change')\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_custom_permission_name_length_with_builtin_permissions(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'checked'\n                permissions = [\n                    ('view_checked', 'Can view checked'),\n                    ('another_permission', 'Can do something else'),\n                ]\n        # Ensure verbose_name is not too long for builtin permission names\n        self.assertLessEqual(len('Can view checked'), 255)\n        self.assertLessEqual(len('Can do something else'), 255)\n        \n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_builtin_permission_name_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'short name'\n\n        with self.settings(AUTH_USER_MODEL='auth_tests.Checked'):\n            errors = checks.run_checks(self.apps.get_app_configs())\n            self.assertEqual(errors, [])\n", "    def test_custom_permission_codename_clashing_with_builtin_permission(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Clashing with builtin permission'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin permission \"\n                \"for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n", "    def test_custom_permission_codename_clashes_with_builtin_permissions(self):\n        \"\"\"\n        Test that a custom permission codename that clashes with a builtin permission\n        raises the correct error.\n        \"\"\"\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Can add Checked (clashes with builtin)'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin permission \"\n                \"for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n"], "sample_160": ["    def test_invalid_inputs(self):\n        with self.assertRaises(TypeError):\n            nformat(None, '.')\n        with self.assertRaises(ValueError):\n            nformat(\"invalid\", '.', decimal_pos=-1)\n        with self.assertRaises(ValueError):\n            nformat(1234, '.', grouping=-1)\n        with self.assertRaises(ValueError):\n            nformat(1234, '.', grouping=(3, -2))\n", "    def test_invalid_inputs(self):\n        with self.assertRaises(ValueError):\n            nformat('invalid', '.')\n        with self.assertRaises(ValueError):\n            nformat(None, '.')\n        with self.assertRaises(ValueError):\n            nformat({}, '.')\n        with self.assertRaises(ValueError):\n            nformat([], '.')\n        with self.assertRaises(ValueError):\n            nformat(True, '.')\n", "    def test_edge_cases(self):\n        # Test with zero value\n        self.assertEqual(nformat(0, '.'), '0')\n        self.assertEqual(nformat(0, '.', decimal_pos=2), '0.00')\n\n        # Test with leading zeros\n        self.assertEqual(nformat('0001234', '.'), '1234')\n        self.assertEqual(nformat('0001234.567', '.', decimal_pos=3), '1234.567')\n\n        # Test with different decimal separators\n        self.assertEqual(nformat(1234.567, ',', decimal_pos=2), '1234,56')\n\n        # Test with negative zero\n        self.assertEqual(nformat(-0.0, '.'), '0.0')\n        self.assertEqual(nformat(-0.0, '.', decimal_pos=2), '0.00')\n\n        # Test with very small float using scientific notation\n        self.assertEqual(nformat(1e-300, '.', decimal_pos=302), '0.' + '0' * 299 + '1')\n\n        # Test with special characters in thousand separator\n        self.assertEqual(nformat(1234567, '.', grouping=3, thousand_sep='*'), '1*234*567')\n\n        # Test with large exponent value in Decimal\n        self.assertEqual(nformat(Decimal('1e400'), '.', decimal_pos=3), '1.000e+400')\n\n        # Test with mixed grouping sequence\n        self.assertEqual(nformat(1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,567')\n", "    def test_invalid_inputs(self):\n        # Test with invalid string input\n        with self.assertRaises(ValueError):\n            nformat('invalid', '.')\n        # Test with invalid decimal separator\n        with self.assertRaises(ValueError):\n            nformat(1234, '')\n        # Test with invalid decimal positions\n        with self.assertRaises(ValueError):\n            nformat(1234, '.', decimal_pos=-1)\n        with self.assertRaises(ValueError):\n            nformat(1234, '.', decimal_pos='two')\n        # Test with invalid grouping parameter\n        with self.assertRaises(TypeError):\n            nformat(1234, '.', grouping='three')\n        # Test with invalid thousand separator\n        with self.assertRaises(TypeError):\n            nformat(1234, '.', thousand_sep=None)\n", "    def test_decimal_with_exponent(self):\n        self.assertEqual(nformat(Decimal('1.23e4'), '.'), '12300')\n        self.assertEqual(nformat(Decimal('1.23e4'), '.', decimal_pos=2), '12300.00')\n        self.assertEqual(nformat(Decimal('1.23e4'), '.', grouping=3, thousand_sep=','), '12,300')\n        self.assertEqual(nformat(Decimal('1.23e4'), '.', grouping=3, thousand_sep=',', force_grouping=True), '12,300')\n        self.assertEqual(nformat(Decimal('-1.23e4'), '.', decimal_pos=1), '-12300.0')\n        self.assertEqual(nformat(Decimal('1.23e4'), '.', decimal_pos=2, grouping=3, thousand_sep=',', force_grouping=True), '12,300.00')\n"], "sample_161": ["    def test_recursive_relationship(self):\n        class Person(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, related_name='children')\n\n        field = Person._meta.get_field('parent')\n        self.assertEqual(field.check(), [])\n", "    def test_foreign_key_to_self(self):\n        class Model(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, null=True, blank=True)\n\n        field = Model._meta.get_field('parent')\n        self.assertEqual(field.check(), [])\n", "    def test_reverse_accessor_with_parent_link(self):\n        class Parent(models.Model):\n            pass\n\n        class Child(Parent):\n            sibling = models.OneToOneField(Parent, models.CASCADE, related_name='sibling_link')\n\n        self.assertEqual(Child.check(), [\n            Error(\n                \"Reverse accessor for 'Child.sibling' clashes with reverse accessor for 'Child.sibling_link'.\",\n                hint=(\n                    \"Add or change a related_name argument to the definition \"\n                    \"for 'Child.sibling' or 'Child.sibling_link'.\"\n                ),\n                obj=Child._meta.get_field('sibling'),\n                id='fields.E304',\n            ),\n            Error(\n                \"Reverse query name for 'Child.sibling' clashes with reverse query name for 'Child.sibling_link'.\",\n                hint=(\n                    \"Add or change a related_name argument to the definition \"\n                    \"for 'Child.sibling' or 'Child.sibling_link'.\"\n                ),\n                obj=Child._meta.get_field('sibling'),\n                id='fields.E305',\n            ),\n        ])\n", "    def test_foreign_key_related_name_clash(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            field = models.ForeignKey(Target, models.CASCADE, related_name='clash')\n\n        class AnotherModel(models.Model):\n            clash = models.IntegerField()\n\n        field = Model._meta.get_field('field')\n        self.assertEqual(field.check(), [\n            Error(\n                \"Reverse accessor for 'Model.field' clashes with field name 'AnotherModel.clash'.\",\n                hint=(\"Rename field 'AnotherModel.clash', or add/change a related_name \"\n                      \"argument to the definition for field 'Model.field'.\"),\n                obj=field,\n                id='fields.E302',\n            ),\n            Error(\n                \"Reverse query name for 'Model.field' clashes with field name 'AnotherModel.clash'.\",\n                hint=(\"Rename field 'AnotherModel.clash', or add/change a related_name \"\n                      \"argument to the definition for field 'Model.field'.\"),\n                obj=field,\n                id='fields.E303',\n            ),\n        ])\n", "    def test_fk_to_swapped_model(self):\n        class Replacement(models.Model):\n            pass\n\n        class SwappedModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPED_MODEL'\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(SwappedModel, models.CASCADE)\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [\n            Error(\n                \"Field defines a relation with the model 'invalid_models_tests.SwappedModel', which has been swapped out.\",\n                hint=\"Update the relation to point at 'settings.TEST_SWAPPED_MODEL'.\",\n                obj=field,\n                id='fields.E301',\n            ),\n        ])\n"], "sample_162": ["    def setUp(self):\n        self.command = mock.MagicMock()\n        self.command.gettext_version = (0, 18, 2)\n        self.translatable_file = TranslatableFile('dir', 'file.py', 'locale_dir')\n        self.build_file = BuildFile(self.command, 'django', self.translatable_file)\n", "    def setUp(self):\n        self.test_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.test_dir)\n        self.translatable_file = TranslatableFile(self.test_dir, 'test_file.html', self.test_dir)\n        self.command = MakeMessagesCommand()\n", "    def setUp(self):\n        super().setUp()\n        self.translatable = MakeMessagesCommand().translatable_file_class('dirpath', 'test.py', 'locale_dir')\n        self.command = MakeMessagesCommand()\n        self.build_file = BuildFile(self.command, 'django', self.translatable)\n", "    def setUp(self):\n        self.command = MakeMessagesCommand()\n        self.translatable = TranslatableFile('/path/to/dir', 'test.html', '/path/to/locale')\n        self.build_file = BuildFile(self.command, 'django', self.translatable)\n", "    def test_check_programs(self):\n        # Ensure check_programs raises an error if a required program is missing.\n        with mock.patch('django.core.management.utils.find_command', return_value=None):\n            with self.assertRaises(CommandError) as cm:\n                check_programs('msguniq')\n            self.assertEqual(\n                str(cm.exception),\n                \"Can't find msguniq. Make sure you have GNU gettext tools 0.15 or newer installed.\"\n            )\n        "], "sample_163": ["    def test_redirect_authenticated_user(self):\n        \"\"\"\n        Ensure that an authenticated user is redirected away from the login page\n        if redirect_authenticated_user is set to True.\n        \"\"\"\n        self.login()\n        response = self.client.get(\"/login/redirect_authenticated_user/\")\n        self.assertRedirects(response, settings.LOGIN_REDIRECT_URL, fetch_redirect_response=False)\n", "    def test_login_redirect_authenticated_user_loop(self):\n        \"\"\"Test redirection loop detection for authenticated user.\"\"\"\n        self.login()\n        msg = (\n            \"Redirection loop for authenticated user detected. Check that \"\n            \"your LOGIN_REDIRECT_URL doesn't point to a login page.\"\n        )\n        with self.settings(LOGIN_REDIRECT_URL=reverse(\"login\")):\n            with self.assertRaisesMessage(ValueError, msg):\n                self.client.get(reverse(\"login\"))\n", "    def test_login_redirect_loop_detection(self):\n        \"\"\"\n        Test to ensure that a ValueError is raised if a redirection loop is detected.\n        \"\"\"\n        with self.settings(LOGIN_REDIRECT_URL=\"/login/redirect_loop/\"):\n            self.login()\n            msg = (\n                \"Redirection loop for authenticated user detected. Check that \"\n                \"your LOGIN_REDIRECT_URL doesn't point to a login page.\"\n            )\n            with self.assertRaisesMessage(ValueError, msg):\n                self.client.get(\"/login/redirect_loop/\")\n", "    def test_redirect_to_login_then_logout(self):\n        \"\"\"Test redirecting to login, logging in, then logging out.\"\"\"\n        login_redirect_response = redirect_to_login(next=\"/else/where/\")\n        expected_login_url = \"/login/?next=/else/where/\"\n        self.assertEqual(expected_login_url, login_redirect_response.url)\n\n        # Log in with user credentials.\n        response = self.client.post(\n            expected_login_url,\n            {\n                \"username\": \"testclient\",\n                \"password\": \"password\",\n            },\n        )\n        self.assertIn(SESSION_KEY, self.client.session)\n        self.assertRedirects(response, \"/else/where/\", fetch_redirect_response=False)\n\n        # Log out and check redirection to login page.\n        response = self.client.post(\"/logout/\")\n        self.assertNotIn(SESSION_KEY, self.client.session)\n        self.assertContains(response, \"Logged out\")\n", "    def test_password_reset_email_subject_template(self):\n        \"\"\"\n        Ensure the email subject is rendered using the correct template.\n        \"\"\"\n        response = self.client.post(\n            \"/password_reset/\",\n            {\"email\": \"staffmember@example.com\"},\n        )\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertIn(\"Password reset on testserver\", mail.outbox[0].subject)\n"], "sample_164": ["    def setUp(self):\n        super().setUp()\n        self.request_factory = RequestFactory()\n", "    def test_configure_logging_with_old_settings(self):\n        configure_logging('django.utils.log.configure_logging', OLD_LOGGING)\n        logger = logging.getLogger('django.request')\n        self.assertEqual(logger.level, logging.ERROR)\n        self.assertEqual(len(logger.handlers), 1)\n        self.assertIsInstance(logger.handlers[0], AdminEmailHandler)\n", "    def test_configure_logging_with_default_settings(self):\n        configure_logging(None, None)\n        self.assertTrue(dictConfig.called)\n    ", "    def setUp(self):\n        super().setUp()\n        self.logger = logging.getLogger('django.request')\n        self.request_factory = RequestFactory()\n", "    def setUp(self):\n        self.request_factory = RequestFactory()\n        self.logger = logging.getLogger('django.request')\n"], "sample_165": ["    def test_construct_instance(self):\n        class DummyModel:\n                self.pk = 1\n                self.name = None\n                self.age = None\n                self.file_field = None\n\n            class _meta:\n                fields = [\n                    type('Field', (), {'name': 'name', 'editable': True, 'has_default': lambda: False, 'save_form_data': lambda instance, data: setattr(instance, 'name', data)}),\n                    type('Field', (), {'name': 'age', 'editable': True, 'has_default': lambda: False, 'save_form_data': lambda instance, data: setattr(instance, 'age', data)}),\n                    type('Field', (), {'name': 'file_field', 'editable': True, 'has_default': lambda: False, 'save_form_data': lambda instance, data: setattr(instance, 'file_field', data), '__class__': type('FileField', (), {})})\n                ]\n\n        class DummyForm:\n            cleaned_data = {\n                'name': 'John Doe',\n                'age': 30,\n                'file_field': 'dummy_file.txt'\n            }\n            data = {}\n            files = {}\n                return field_name\n                return self\n\n        form = DummyForm()\n        instance = DummyModel()\n        updated_instance = construct_instance(form, instance)\n        self.assertEqual(updated_instance.name, 'John Doe')\n        self.assertEqual(updated_instance.age, 30)\n        self.assertEqual(updated_instance.file_field, 'dummy_file.txt')\n", "    def test_modelform_factory(self):\n        from django.db import models\n\n        class TestModel(models.Model):\n            char_field = models.CharField(max_length=100)\n            int_field = models.IntegerField()\n\n        # Create ModelForm using modelform_factory\n        TestModelForm = modelform_factory(TestModel, fields=['char_field', 'int_field'])\n\n        # Verify form fields\n        form = TestModelForm()\n        self.assertIn('char_field', form.fields)\n        self.assertIn('int_field', form.fields)\n\n        # Test form validation\n        form = TestModelForm(data={'char_field': 'test', 'int_field': '123'})\n        self.assertTrue(form.is_valid())\n\n        form = TestModelForm(data={'char_field': '', 'int_field': 'abc'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('char_field', form.errors)\n        self.assertIn('int_field', form.errors)\n", "    def test_modelform_factory(self):\n        class TestModel(models.Model):\n            char_field = models.CharField(max_length=100)\n            int_field = models.IntegerField()\n\n        TestModelForm = modelform_factory(TestModel, fields=['char_field', 'int_field'])\n\n        self.assertTrue(issubclass(TestModelForm, ModelForm))\n        self.assertIn('char_field', TestModelForm.base_fields)\n        self.assertIn('int_field', TestModelForm.base_fields)\n\n        form = TestModelForm(data={'char_field': 'test', 'int_field': '123'})\n        self.assertTrue(form.is_valid())\n        instance = form.save()\n        self.assertEqual(instance.char_field, 'test')\n        self.assertEqual(instance.int_field, 123)\n", "    def test_save_new_instance(self):\n        class TestModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ['name']\n\n        form = TestModelForm(data={'name': 'new_choice'})\n        self.assertTrue(form.is_valid())\n        instance = form.save()\n        self.assertIsNotNone(instance.pk)\n        self.assertEqual(instance.name, 'new_choice')\n", "    def test_modelform_factory(self):\n        from ..models import ChoiceModel\n        from django.forms.models import modelform_factory\n\n        class CustomChoiceForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ['name']\n\n        CustomForm = modelform_factory(ChoiceModel, form=CustomChoiceForm, fields=['name'])\n        form = CustomForm(data={'name': 'test'})\n\n        self.assertTrue(form.is_valid())\n        instance = form.save()\n        self.assertEqual(instance.name, 'test')\n"], "sample_166": ["    def test_get_random_string_length(self):\n        for length in [0, 1, 10, 100, 1000]:\n            with self.subTest(length=length):\n                result = get_random_string(length)\n                self.assertEqual(len(result), length)\n", "    def test_get_random_string_length(self):\n        for length in range(1, 101):  # Test lengths from 1 to 100\n            result = get_random_string(length=length)\n            self.assertEqual(len(result), length)\n", "    def test_get_random_string_length(self):\n        # Test that get_random_string returns a string of the correct length\n        for length in [1, 5, 10, 20, 50, 100]:\n            with self.subTest(length=length):\n                random_string = get_random_string(length)\n                self.assertEqual(len(random_string), length)\n", "    def test_get_random_string_length(self):\n        for length in [0, 1, 12, 50, 100]:\n            random_string = get_random_string(length)\n            self.assertEqual(len(random_string), length)\n            self.assertTrue(all(c in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789' for c in random_string))\n", "    def test_get_random_string_length(self):\n        lengths = [1, 10, 100, 1000]\n        for length in lengths:\n            with self.subTest(length=length):\n                result = get_random_string(length=length)\n                self.assertEqual(len(result), length)\n"], "sample_167": ["    def test_intcomma_non_integer(self):\n        test_list = (\n            '1000.234567', '2000.1234', '3000.56789', '4000.0000',\n            '5000.1', '6000.12', '7000.123', '8000.12345678901234567890'\n        )\n        result_list = (\n            '1,000.234567', '2,000.1234', '3,000.56789', '4,000.0000',\n            '5,000.1', '6,000.12', '7,000.123', '8,000.12345678901234567890'\n        )\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'intcomma')\n", "    def test_naturaltime_far_past_future(self):\n        \"\"\"\n        Test naturaltime for dates far in the past and far in the future to ensure proper formatting.\n        \"\"\"\n        far_past = now - datetime.timedelta(days=365 * 100 + 30)  # 100 years and 30 days ago\n        far_future = now + datetime.timedelta(days=365 * 100 + 30)  # 100 years and 30 days from now\n\n        test_list = [\n            far_past,\n            far_future,\n        ]\n        result_list = [\n            '100\\xa0years, 1\\xa0month ago',\n            '100\\xa0years, 1\\xa0month from now',\n        ]\n\n        orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n        try:\n            with translation.override('en'):\n                self.humanize_tester(test_list, result_list, 'naturaltime')\n        finally:\n            humanize.datetime = orig_humanize_datetime\n", "    def test_naturaltime_with_naive_and_aware_datetimes(self):\n        naive_datetime = datetime.datetime(2012, 3, 9, 22, 30)\n        aware_datetime = datetime.datetime(2012, 3, 9, 22, 30, tzinfo=utc)\n\n        test_list = [\n            naive_datetime,\n            aware_datetime,\n            aware_datetime - datetime.timedelta(days=1),\n            aware_datetime + datetime.timedelta(days=1),\n        ]\n\n        result_list = [\n            'now',\n            'now',\n            '1\\xa0day ago',\n            '1\\xa0day from now',\n        ]\n\n        orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n        try:\n            with translation.override('en'):\n                self.humanize_tester(test_list, result_list, 'naturaltime')\n        finally:\n            humanize.datetime = orig_humanize_datetime\n", "    def test_invalid_inputs(self):\n        test_list = ['abc', None, [], {}, 5.5, Decimal('10.5')]\n        result_list = ['abc', None, [], {}, '5.5', '10.5']\n        \n        # Test ordinal with invalid inputs\n        self.humanize_tester(test_list, result_list, 'ordinal', lambda x: x)\n        \n        # Test intcomma with invalid inputs\n        intcomma_result_list = ['abc', None, '[]', '{}', '5.5', '10.5']\n        self.humanize_tester(test_list, intcomma_result_list, 'intcomma', lambda x: x)\n        \n        # Test intword with invalid inputs\n        self.humanize_tester(test_list, result_list, 'intword', lambda x: x)\n        \n        # Test apnumber with invalid inputs\n        self.humanize_tester(test_list, result_list, 'apnumber', lambda x: x)\n        \n        # Test naturalday with invalid inputs\n        naturalday_result_list = ['abc', None, '[]', '{}', '5.5', '10.5']\n        self.humanize_tester(test_list, naturalday_result_list, 'naturalday', lambda x: x)\n        \n        # Test naturaltime with invalid inputs\n        self.humanize_tester(test_list, result_list, 'naturaltime', lambda x: x)\n", "    def test_apnumber_out_of_range(self):\n        test_list = ['0', '10', '11', '20', '100', '-1', '3.5', 'not a number']\n        result_list = ['0', '10', '11', '20', '100', '-1', '3.5', 'not a number']\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'apnumber')\n"], "sample_168": ["    def test_no_migration_allowed(self):\n        \"\"\"Test that no content types are deleted if migration is not allowed.\"\"\"\n        with mock.patch('django.db.router.allow_migrate_model', return_value=False):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', verbosity=2)\n        self.assertNotIn(\"Deleting stale content type\", stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "    def test_no_migration_allowed_for_contenttype(self):\n        \"\"\"Test that no content types are removed if migrations are not allowed.\"\"\"\n        with mock.patch('django.db.router.allow_migrate_model', return_value=False):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', verbosity=2)\n        self.assertNotIn('Deleting stale content type', stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "    def test_handle_non_default_database(self):\n        \"\"\"Command should handle content types in a non-default database.\"\"\"\n        # Create a content type in the non-default database\n        ContentType.objects.using('non_default').create(app_label='contenttypes_tests', model='Fake')\n        with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', database='non_default', verbosity=2)\n        self.assertIn(\"Deleting stale content type 'contenttypes_tests | Fake'\", stdout.getvalue())\n        self.assertEqual(ContentType.objects.using('non_default').count(), 0)\n", "    def test_non_interactive_no_stale_contenttypes(self):\n        \"\"\"non-interactive mode does nothing if there are no stale content types.\"\"\"\n        with mock.patch('builtins.input', return_value='yes'):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n        self.assertNotIn('Deleting stale content type', stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "    def test_non_interactive_with_include_stale_apps(self):\n        \"\"\"non-interactive mode deletes stale content types including those from stale apps.\"\"\"\n        ContentType.objects.create(app_label='empty_models', model='Fake 1')\n        ContentType.objects.create(app_label='no_models', model='Fake 2')\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', include_stale_apps=True, interactive=False, verbosity=2)\n        self.assertIn(\n            \"Deleting stale content type 'empty_models | Fake 1'\",\n            stdout.getvalue(),\n        )\n        self.assertIn(\n            \"Deleting stale content type 'no_models | Fake 2'\",\n            stdout.getvalue(),\n        )\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n"], "sample_169": ["    def test_xml_serialization_with_fk(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n            class Meta:\n                app_label = 'test_app'\n\n        class Book(models.Model):\n            title = models.CharField(max_length=255)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n            class Meta:\n                app_label = 'test_app'\n\n        author = Author(name='John Doe')\n        book = Book(title='Test Book', author=author)\n\n        # Mock the get_model function to return our test models\n        with mock.patch('django.apps.apps.get_model') as get_model_mock:\n            get_model_mock.side_effect = lambda x: Author if x == 'test_app.author' else Book\n\n            xml_serializer = serializers.get_serializer(\"xml\")()\n            xml_output = xml_serializer.serialize([book], indent=2)\n\n            expected_xml = (\n                '<django-objects version=\"1.0\">\\n'\n                '  <object model=\"test_app.book\" pk=\"None\">\\n'\n                '    <field name=\"title\" type=\"CharField\">Test Book</field>\\n'\n                '    <field name=\"author\" rel=\"ManyToOneRel\" to=\"test_app.author\">None</field>\\n'\n                '  </object>\\n'\n                '</django-objects>'\n            )\n\n            self.assertXMLEqual(xml_output, expected_xml)\n", "    def test_deserialize_valid_data(self):\n        xml_data = (\n            '<django-objects version=\"1.0\">'\n            '<object model=\"model_fields.nullablejsonmodel\" pk=\"1\">'\n            '<field name=\"value\" type=\"JSONField\">{\"a\": \"b\"}</field>'\n            '</object></django-objects>'\n        )\n        deserialized_objects = list(serializers.deserialize('xml', xml_data))\n        self.assertEqual(len(deserialized_objects), 1)\n        obj = deserialized_objects[0].object\n        self.assertEqual(obj.pk, 1)\n        self.assertEqual(obj.value, {\"a\": \"b\"})\n", "    def test_defused_expat_parser_forbids_dtd(self):\n        parser = DefusedExpatParser()\n        with self.assertRaises(DTDForbidden):\n            parser.start_doctype_decl(\"name\", \"sysid\", \"pubid\", False)\n", "    def setUp(self):\n        self.test_xml_data = (\n            '<django-objects version=\"1.0\">'\n            '<object model=\"model_fields.nullablejsonmodel\">'\n            '<field name=\"value\" type=\"JSONField\">%s</field>'\n            '</object></django-objects>'\n        )\n", "    def test_xml_serialization(self):\n        test_xml_data = (\n            '<django-objects version=\"1.0\">'\n            '<object model=\"model_fields.jsonmodel\">'\n            '<field name=\"value\" type=\"JSONField\">%s'\n            '</field></object></django-objects>'\n        )\n        test_values = (\n            # (Python value, serialized value),\n            ({'a': 'b', 'c': None}, '{\"a\": \"b\", \"c\": null}'),\n            ('abc', '\"abc\"'),\n            ('{\"a\": \"a\"}', '\"{\\\\\"a\\\\\": \\\\\"a\\\\\"}\"'),\n        )\n        for value, serialized in test_values:\n            with self.subTest(value=value):\n                instance = JSONModel(value=value)\n                data = serializers.serialize('xml', [instance], fields=['value'])\n                self.assertXMLEqual(data, test_xml_data % serialized)\n                new_instance = list(serializers.deserialize('xml', data))[0].object\n                self.assertEqual(new_instance.value, instance.value)\n"], "sample_171": ["    def test_migrate_with_fake_option(self):\n        \"\"\"\n        Tests the --fake option of the migrate command.\n        \"\"\"\n        # Ensure no tables are created\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        # Run migrations with the --fake option\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', '0001', fake=True, verbosity=1, stdout=stdout, no_color=True)\n        stdout = stdout.getvalue()\n        self.assertIn('Faking apply of migrations.0001_initial', stdout)\n        # Ensure tables are still not created\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        # Check that the migration is recorded as applied\n        applied_migrations = MigrationRecorder(connection).applied_migrations()\n        self.assertIn((\"migrations\", \"0001_initial\"), applied_migrations)\n        # Cleanup by unmigrating everything\n        call_command(\"migrate\", \"migrations\", \"zero\", verbosity=0)\n", "    def test_migrate_with_fake(self):\n        \"\"\"\n        Tests the --fake option of the migrate command.\n        \"\"\"\n        # Make sure no tables are created\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n\n        # Fake apply initial migration\n        stdout = io.StringIO()\n        call_command(\"migrate\", \"migrations\", \"0001\", verbosity=1, fake=True, stdout=stdout, no_color=True)\n        stdout = stdout.getvalue()\n        self.assertIn('Applying migrations.0001_initial... FAKED', stdout)\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n\n        # Fake apply second migration\n        stdout = io.StringIO()\n        call_command(\"migrate\", \"migrations\", \"0002\", verbosity=1, fake=True, stdout=stdout, no_color=True)\n        stdout = stdout.getvalue()\n        self.assertIn('Applying migrations.0002_second... FAKED', stdout)\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n\n        # Unmigrate everything\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', 'zero', verbosity=1, fake=True, stdout=stdout, no_color=True)\n        stdout = stdout.getvalue()\n        self.assertIn('Unapplying migrations.0002_second... FAKED', stdout)\n        self.assertIn('Unapplying migrations.0001_initial... FAKED', stdout)\n        # Tables are still not created\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        self.assertTableNotExists(\"migrations_book\")\n", "    def test_migrate_with_fake(self):\n        \"\"\"\n        Test the --fake option with the migrate command.\n        \"\"\"\n        # Make sure no tables are created\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        # Run the migrations to 0001 only\n        call_command(\"migrate\", \"migrations\", \"0001\", fake=True, verbosity=0)\n        # Check if the migrations are marked as applied\n        recorder = MigrationRecorder(connection)\n        self.assertIn((\"migrations\", \"0001_initial\"), recorder.applied_migrations())\n        # The tables should still not exist\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        # Unapply the migrations\n        call_command(\"migrate\", \"migrations\", \"zero\", fake=True, verbosity=0)\n        self.assertNotIn((\"migrations\", \"0001_initial\"), recorder.applied_migrations())\n", "    def test_migrate_check_unapplied_with_plan(self):\n        \"\"\"\n        Tests the --check and --plan options together for the migrate command.\n        \"\"\"\n        out = io.StringIO()\n        with self.assertRaises(SystemExit):\n            call_command(\n                'migrate',\n                'migrations',\n                '0002',\n                check_unapplied=True,\n                plan=True,\n                stdout=out,\n                no_color=True,\n            )\n        self.assertIn(\n            'Planned operations:\\n'\n            'migrations.0001_initial\\n'\n            '    Create model Salamander\\n'\n            '    Raw Python operation -> Grow salamander tail.\\n'\n            'migrations.0002_second\\n'\n            '    Create model Book\\n'\n            \"    Raw SQL operation -> ['SELECT * FROM migrations_book']\\n\",\n            out.getvalue(),\n        )\n", "    def test_migrate_with_fake_and_fake_initial(self):\n        \"\"\"\n        Tests the behavior of migrate command when both --fake and --fake-initial options are used.\n        \"\"\"\n        # Ensure no tables exist initially\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        \n        # Run the initial migration with --fake-initial flag\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', '0001', fake_initial=True, verbosity=1, stdout=stdout, no_color=True)\n        stdout = stdout.getvalue()\n        self.assertIn('migrations.0001_initial... faked', stdout)\n        \n        # Check that tables are not actually created\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n\n        # Run the initial migration with --fake flag\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', '0001', fake=True, verbosity=1, stdout=stdout, no_color=True)\n        stdout = stdout.getvalue()\n        self.assertIn('migrations.0001_initial... faked', stdout)\n\n        # Check that tables are still not created\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n\n        # Apply the next migration with --fake flag\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', '0002', fake=True, verbosity=1, stdout=stdout, no_color=True)\n        stdout = stdout.getvalue()\n        self.assertIn('migrations.0002_second... faked', stdout)\n\n        # Ensure that the tables for the second migration are not created\n        self.assertTableNotExists(\"migrations_book\")\n\n        # Finally, unapply all migrations with --fake flag\n        stdout = io.StringIO()\n        call_command('migrate', 'migrations', 'zero', fake=True, verbosity=1, stdout=stdout, no_color=True)\n        stdout = stdout.getvalue()\n        self.assertIn('Unapplying migrations.0002_second... faked', stdout)\n        self.assertIn('Unapplying migrations.0001_initial... faked', stdout)\n\n        # Ensure that no tables exist\n        self.assertTableNot"], "sample_170": ["    def test_cleanse_setting_with_list(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        initial = ['password', 'secret_key', 'normal_value']\n        cleansed = [\n            reporter_filter.cleansed_substitute,\n            reporter_filter.cleansed_substitute,\n            'normal_value'\n        ]\n        self.assertEqual(\n            reporter_filter.cleanse_setting('SETTING_NAME', initial),\n            cleansed\n        )\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n"], "sample_172": ["    def test_get_content_type_for_model(self):\n        \"\"\"\n        Test that get_content_type_for_model returns the correct content type.\n        \"\"\"\n        from django.contrib.contenttypes.models import ContentType\n        band = Band(name='Test Band')\n        content_type = get_content_type_for_model(band)\n        expected_content_type = ContentType.objects.get_for_model(Band, for_concrete_model=False)\n        self.assertEqual(content_type, expected_content_type)\n", "    def test_get_content_type_for_model(self):\n        class DummyModel:\n            _meta = type('Meta', (), {'app_label': 'dummy', 'model_name': 'dummy'})\n\n                return '/dummy/'\n\n        obj = DummyModel()\n        content_type = get_content_type_for_model(obj)\n        self.assertEqual(content_type.app_label, 'dummy')\n        self.assertEqual(content_type.model, 'dummy')\n", "    def setUp(self):\n        self.user = User.objects.create_user(username='testuser', password='secret')\n        self.superuser = User.objects.create_superuser(username='superuser', password='secret', email=None)\n        self.car = Car.objects.create(owner=self.superuser, make='Toyota', model='Corolla')\n        self.client.force_login(self.superuser)\n", "    def test_get_extra(self):\n        \"\"\"\n        Test InlineModelAdmin's get_extra method for customizing the number of extra inline forms.\n        \"\"\"\n        class MyInline(admin.TabularInline):\n            model = Member\n            extra = 2\n\n        inline = MyInline(Member, admin.site)\n        self.assertEqual(inline.get_extra(request=None), 2)\n", "    def setUp(self):\n        super().setUp()\n        self.model_admin = admin.ModelAdmin(Event, admin.site)\n"], "sample_173": ["    def test_prepare_sql_script(self):\n        sql_script = \"SELECT 1; -- Comment\\nSELECT 2; /* Block Comment */\\nSELECT 3;\"\n        expected_statements = [\"SELECT 1;\", \"SELECT 2;\", \"SELECT 3;\"]\n        self.assertEqual(self.ops.prepare_sql_script(sql_script), expected_statements)\n", "    def test_last_executed_query(self):\n        sql = \"SELECT * FROM my_table WHERE column = %s\"\n        params = ['value']\n        expected_query = \"QUERY = %r - PARAMS = %r\" % (sql, tuple(params))\n        self.assertEqual(self.ops.last_executed_query(None, sql, params), expected_query)\n", "    def test_force_no_ordering(self):\n        self.assertEqual(self.ops.force_no_ordering(), [])\n", "    def test_random_function_sql(self):\n        self.assertEqual(self.ops.random_function_sql(), 'RANDOM()')\n", "    def test_random_function_sql(self):\n        self.assertEqual(self.ops.random_function_sql(), 'RANDOM()')\n"], "sample_174": ["    def test_bulk_batch_size(self):\n        fields = ['field1', 'field2']\n        objs = [object(), object(), object()]\n        self.assertEqual(self.ops.bulk_batch_size(fields, objs), 3)\n", "compilation error", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "def test_force_no_ordering(self):\n    self.assertEqual(self.ops.force_no_ordering(), [])\n", "compilation error"], "sample_175": ["    def test_protect_with_multiple_references(self):\n        # Create multiple instances and link them to the same protected instance.\n        a1 = create_a('protect_multi1')\n        a2 = create_a('protect_multi2')\n        protected_instance = a1.protect\n\n        # Attempt to delete the protected instance.\n        msg = (\n            \"Cannot delete some instances of model 'R' because they are \"\n            \"referenced through protected foreign keys: 'A.protect', \"\n            \"'A.protect'.\"\n        )\n        with self.assertRaisesMessage(ProtectedError, msg):\n            protected_instance.delete()\n", "    def test_on_delete_set_callable(self):\n            return self.DEFAULT\n\n        a = create_a('set_callable', on_delete=models.SET(get_default_value))\n        a.set_callable.delete()\n        a = A.objects.get(pk=a.pk)\n        self.assertEqual(self.DEFAULT, a.set_callable.pk)\n", "    def test_set_callable(self):\n            return self.DEFAULT\n\n        a = create_a('set_callable')\n        on_delete_callable = models.SET(custom_value)\n        related_obj = R.objects.create(set_callable=a)\n        related_obj.set_callable.on_delete = on_delete_callable\n        related_obj.save()\n        related_obj.delete()\n        a.refresh_from_db()\n        self.assertEqual(a.set_callable, self.DEFAULT)\n", "    def test_set_callable(self):\n            return self.DEFAULT\n\n        a = create_a('set_callable')\n        a.set_callable = callable_value\n        a.save()\n        a.set_callable.delete()\n        a = A.objects.get(pk=a.pk)\n        self.assertEqual(self.DEFAULT, a.set_callable.pk)\n", "    def test_set_callable(self):\n            return self.DEFAULT\n\n        a = create_a('set_callable')\n        a.set_callable = dynamic_value\n        a.set_callable.delete()\n        a = A.objects.get(pk=a.pk)\n        self.assertEqual(self.DEFAULT, a.set_callable)\n"], "sample_176": ["    def test_add_operation_with_dependencies(self):\n        \"\"\"Test the add_operation method with dependencies.\"\"\"\n        # Initial states with no models\n        before_states = []\n        after_states = [self.author_empty]\n\n        # Create the MigrationAutodetector instance\n        autodetector = MigrationAutodetector(\n            self.make_project_state(before_states),\n            self.make_project_state(after_states),\n        )\n\n        # Simulate adding an operation with dependencies\n        dependencies = [(\"testapp\", \"0001_initial\")]\n        autodetector.add_operation(\"testapp\", operations.CreateModel(name=\"Author\", fields=[(\"id\", models.AutoField(primary_key=True))]), dependencies=dependencies)\n\n        # Ensure the operation is added correctly with the dependencies\n        self.assertIn(\"testapp\", autodetector.generated_operations)\n        self.assertEqual(len(autodetector.generated_operations[\"testapp\"]), 1)\n        self.assertEqual(autodetector.generated_operations[\"testapp\"][0]._auto_deps, dependencies)\n", "    def test_rename_model_with_db_table_change(self):\n        \"\"\"\n        Tests autodetection of renamed models with a change in db_table.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_db_table_options],\n            [self.author_renamed_with_new_db_table_options],\n            MigrationQuestioner({\"ask_rename_model\": True})\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"AlterModelTable\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"NewAuthor\")\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"newauthor\", table=\"author_three\")\n", "def test_unique_together_change_addition(self):\n    \"\"\"\n    Test changing unique_together constraint by adding a new unique_together constraint.\n    \"\"\"\n    author_with_unique_together = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n        (\"email\", models.EmailField()),\n    ], options={\n        \"unique_together\": {(\"name\", \"email\")},\n    })\n    changes = self.get_changes([self.author_name], [author_with_unique_together])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", unique_together={(\"name\", \"email\")})\n", "    def test_generate_altered_order_with_respect_to(self):\n        \"\"\"\n        Test that an order_with_respect_to change is detected and generates\n        the correct migration operations.\n        \"\"\"\n        before = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n            ])\n        ]\n        after = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n            ], options={\"order_with_respect_to\": \"book\"})\n        ]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterOrderWithRespectTo\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", order_with_respect_to=\"book\")\n", "    def test_alter_field_preserve_default(self):\n        \"\"\"Tests that altering a field preserves the default value correctly.\"\"\"\n        before = [\n            ModelState('app', 'Model', [\n                ('id', models.AutoField(primary_key=True)),\n                ('field', models.CharField(max_length=200, default='default_value')),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'Model', [\n                ('id', models.AutoField(primary_key=True)),\n                ('field', models.CharField(max_length=400, default='default_value')),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='field', preserve_default=True)\n"], "sample_177": ["    def test_reload_model_clears_cache(self):\n        \"\"\"\n        Ensures that reloading a model clears the cache.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"Tag\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n            ],\n        ))\n\n        # Render the project state to apps\n        project_state.apps\n\n        # Change the model state\n        new_state = project_state.clone()\n        new_state.models['migrations', 'tag'].fields['name'] = models.CharField(max_length=200)\n\n        # Reload the model and ensure cache is cleared\n        with self.assertNumQueries(2):\n            new_state.reload_model('migrations', 'tag', delay=True)\n        \n        # Ensure the change was applied\n        Tag = new_state.apps.get_model(\"migrations\", \"Tag\")\n        self.assertEqual(Tag._meta.get_field(\"name\").max_length, 200)\n", "    def test_reload_model_with_proxy(self):\n        \"\"\"\n        Tests that reloading a model with a proxy correctly reloads its related models.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"BaseModel\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n        ))\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"ProxyModel\",\n            fields=[],\n            bases=(\"migrations.BaseModel\",),\n            options={\"proxy\": True},\n        ))\n\n        project_state.apps  # Render project state.\n        BaseModel = project_state.apps.get_model('migrations', 'BaseModel')\n        ProxyModel = project_state.apps.get_model('migrations', 'ProxyModel')\n\n        # Ensure the models are related as expected.\n        self.assertIs(ProxyModel._meta.proxy_for_model, BaseModel)\n        self.assertIn(ProxyModel, _get_related_models(BaseModel))\n\n        # Reload the base model and ensure the proxy model is also reloaded.\n        project_state.reload_model('migrations', 'basemodel', delay=True)\n        BaseModel_reloaded = project_state.apps.get_model('migrations', 'BaseModel')\n        ProxyModel_reloaded = project_state.apps.get_model('migrations', 'ProxyModel')\n\n        self.assertIsNot(BaseModel, BaseModel_reloaded)\n        self.assertIsNot(ProxyModel, ProxyModel_reloaded)\n        self.assertIs(ProxyModel_reloaded._meta.proxy_for_model, BaseModel_reloaded)\n        self.assertIn(ProxyModel_reloaded, _get_related_models(BaseModel_reloaded))\n", "    def test_related_model_tuples(self):\n        \"\"\"\n        Tests that get_related_models_tuples returns the correct related models tuples.\n        \"\"\"\n        new_apps = Apps([\"migrations\"])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class Book(models.Model):\n            title = models.CharField(max_length=1000)\n            author = models.ForeignKey(Author, models.CASCADE)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class Publisher(models.Model):\n            name = models.CharField(max_length=255)\n            books = models.ManyToManyField(Book)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        author_model = new_apps.get_model('migrations', 'Author')\n        book_model = new_apps.get_model('migrations', 'Book')\n        publisher_model = new_apps.get_model('migrations', 'Publisher')\n\n        author_related = get_related_models_tuples(author_model)\n        book_related = get_related_models_tuples(book_model)\n        publisher_related = get_related_models_tuples(publisher_model)\n\n        self.assertEqual(author_related, {('migrations', 'book')})\n        self.assertEqual(book_related, {('migrations', 'author'), ('migrations', 'publisher')})\n        self.assertEqual(publisher_related, {('migrations', 'book')})\n", "    def test_remove_model(self):\n        \"\"\"\n        Tests that removing a model from ProjectState works correctly and related models are also updated.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState('migrations', 'A', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=255)),\n        ]))\n        project_state.add_model(ModelState('migrations', 'B', [\n            ('id', models.AutoField(primary_key=True)),\n            ('a', models.ForeignKey('A', models.CASCADE)),\n        ]))\n\n        # Ensure models are present initially\n        self.assertIn(('migrations', 'a'), project_state.models)\n        self.assertIn(('migrations', 'b'), project_state.models)\n\n        # Remove model B\n        project_state.remove_model('migrations', 'b')\n\n        # Ensure B is removed and A is unaffected\n        self.assertIn(('migrations', 'a'), project_state.models)\n        self.assertNotIn(('migrations', 'b'), project_state.models)\n\n        # Ensure related models are updated\n        A = project_state.apps.get_model('migrations.A')\n        with self.assertRaises(LookupError):\n            project_state.apps.get_model('migrations.B')\n        self.assertEqual(A._meta.get_field('id').name, 'id')\n        self.assertEqual(A._meta.get_field('name').max_length, 255)\n", "    def test_reload_model_with_multiple_relationships(self):\n        \"\"\"\n        Tests reloading a model with multiple relationships to check if all related models are properly reloaded.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState('migrations', 'A', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=50)),\n        ]))\n        project_state.add_model(ModelState('migrations', 'B', [\n            ('id', models.AutoField(primary_key=True)),\n            ('a', models.ForeignKey('migrations.A', models.CASCADE)),\n        ]))\n        project_state.add_model(ModelState('migrations', 'C', [\n            ('id', models.AutoField(primary_key=True)),\n            ('b', models.ForeignKey('migrations.B', models.CASCADE)),\n            ('name', models.CharField(max_length=50)),\n        ]))\n        project_state.add_model(ModelState('migrations', 'D', [\n            ('id', models.AutoField(primary_key=True)),\n            ('c', models.ForeignKey('migrations.C', models.CASCADE)),\n            ('a', models.ForeignKey('migrations.A', models.CASCADE)),\n        ]))\n\n        # Render the initial state.\n        project_state.apps\n\n        # Modify model C and check if all related models are reloaded.\n        operation = AlterField(\n            model_name='C',\n            name='name',\n            field=models.CharField(max_length=100),\n        )\n        operation.state_forwards('migrations', project_state)\n        project_state.reload_model('migrations', 'c', delay=True)\n\n        A = project_state.apps.get_model('migrations.A')\n        B = project_state.apps.get_model('migrations.B')\n        C = project_state.apps.get_model('migrations.C')\n        D = project_state.apps.get_model('migrations.D')\n\n        # Ensure all models are reloaded and relationships are consistent.\n        self.assertIs(B._meta.get_field('a').related_model, A)\n        self.assertIs(C._meta.get_field('b').related_model, B)\n        self.assertIs(D._meta.get_field('c').related_model, C)\n        self.assertIs(D._meta.get_field('a').related_model, A)\n"], "sample_178": ["    def test_formset_with_custom_error_class(self):\n        \"\"\"Test formset with a custom error class applied to forms.\"\"\"\n        class CustomErrorList(ErrorList):\n            pass\n\n        class CustomErrorForm(Form):\n            name = CharField()\n\n        CustomErrorFormSet = formset_factory(CustomErrorForm, extra=2, formset=BaseFormSet)\n        formset = CustomErrorFormSet(error_class=CustomErrorList)\n        self.assertEqual(formset.error_class, CustomErrorList)\n        for form in formset:\n            self.assertEqual(form.error_class, CustomErrorList)\n\n        data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '0',\n            'form-0-name': '',\n            'form-1-name': '',\n        }\n        formset = CustomErrorFormSet(data)\n        self.assertFalse(formset.is_valid())\n        for form in formset:\n            self.assertIsInstance(form.errors, CustomErrorList)\n", "    def test_formset_custom_ordering_widget(self):\n        \"\"\"\n        Custom ordering widget specified in the formset class should be used\n        instead of the default ordering widget.\n        \"\"\"\n        class CustomOrderingWidgetFormSet(BaseFormSet):\n            ordering_widget = HiddenInput(attrs={'class': 'custom-ordering'})\n\n        CustomOrderingFormSet = formset_factory(Choice, formset=CustomOrderingWidgetFormSet, can_order=True)\n        formset = CustomOrderingFormSet(auto_id=False, prefix='choices')\n\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\"></li>", "    def test_formset_cleaned_data_access(self):\n        \"\"\"\n        Accessing formset.cleaned_data when formset is invalid should raise an AttributeError.\n        \"\"\"\n        formset = self.make_choiceformset([('Calexico', '')])\n        self.assertFalse(formset.is_valid())\n        with self.assertRaises(AttributeError):\n            _ = formset.cleaned_data\n", "    def test_absolute_max_limit_with_deletion(self):\n        \"\"\"\n        Ensure that formsets respect the absolute_max limit, even when forms are marked for deletion.\n        \"\"\"\n        initial = [{'name': f'Drink{i}'} for i in range(10)]\n        data = {\n            'form-TOTAL_FORMS': '12',  # the number of forms rendered\n            'form-INITIAL_FORMS': '10',  # the number of forms with initial data\n            'form-MIN_NUM_FORMS': '0',  # min number of forms\n            'form-MAX_NUM_FORMS': '0',  # max number of forms\n        }\n        for i in range(10):\n            data[f'form-{i}-name'] = f'Drink{i}'\n            data[f'form-{i}-DELETE'] = 'on' if i % 2 == 0 else ''\n        for i in range(10, 12):\n            data[f'form-{i}-name'] = f'Drink{i}'\n\n        LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, extra=2, absolute_max=11, can_delete=True)\n        formset = LimitedFavoriteDrinkFormSet(data=data)\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(len(formset.forms), 11)\n        self.assertEqual(len(formset.deleted_forms), 5)\n        self.assertEqual([form.cleaned_data for form in formset.deleted_forms], [\n            {'name': 'Drink0', 'DELETE': True},\n            {'name': 'Drink2', 'DELETE': True},\n            {'name': 'Drink4', 'DELETE': True},\n            {'name': 'Drink6', 'DELETE': True},\n            {'name': 'Drink8', 'DELETE': True},\n        ])\n", "    def test_formset_invalid_initial_forms(self):\n        \"\"\"Test that formset raises an error for invalid initial form count.\"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '3',  # Invalid: more initial forms than total forms\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        with self.assertRaises(ValidationError) as context:\n            formset.is_valid()\n        self.assertEqual(str(context.exception), 'ManagementForm data is missing or has been tampered with')\n"], "sample_180": ["    def test_modelbase_deferred_field(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=255)\n            age = models.IntegerField()\n\n        deferred_instance = TestModel(name=DEFERRED, age=30)\n        self.assertEqual(repr(deferred_instance.name), '<Deferred field>')\n        self.assertEqual(str(deferred_instance.name), '<Deferred field>')\n        self.assertEqual(deferred_instance.age, 30)\n        self.assertIn('name', deferred_instance.get_deferred_fields())\n        self.assertNotIn('age', deferred_instance.get_deferred_fields())\n", "    def test_modelbase_new_abstract_no_app_label(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        self.assertIsInstance(AbstractModel, models.Model)\n", "    def test_deferred_field_repr(self):\n        self.assertEqual(repr(DEFERRED), '<Deferred field>')\n", "    def test_model_name_starting_with_digit(self):\n        class _123Model(models.Model):\n            pass\n\n        self.assertEqual(_123Model.check(), [\n            Error(\n                \"The model name '_123Model' cannot start with an underscore \"\n                \"or a number.\",\n                obj=_123Model,\n                id='models.E023',\n            )\n        ])\n", "    def test_abstract_model_with_ordering(self):\n        class AbstractModel(models.Model):\n            field = models.IntegerField()\n\n            class Meta:\n                abstract = True\n                ordering = ['field']\n\n        self.assertEqual(AbstractModel.check(), [])\n"], "sample_179": ["    def test_proxy_model_with_fields(self):\n        class BaseModel(models.Model):\n            field1 = models.IntegerField()\n\n        class ProxyModel(BaseModel):\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                proxy = True\n\n        self.assertEqual(ProxyModel.check(), [\n            Error(\n                \"Proxy model 'ProxyModel' contains model fields.\",\n                obj=ProxyModel,\n                id='models.E017',\n            ),\n        ])\n", "    def test_name_starting_with_underscore(self):\n        class _InvalidModelName(models.Model):\n            field = models.CharField(max_length=10)\n\n        self.assertEqual(_InvalidModelName.check(), [\n            Error(\n                \"The model name '_InvalidModelName' cannot start or end with an underscore \"\n                \"as it collides with the query lookup syntax.\",\n                obj=_InvalidModelName,\n                id='models.E023',\n            )\n        ])\n", "    def test_model_state_fields_cache_descriptor(self):\n        \"\"\"\n        Test that ModelStateFieldsCacheDescriptor correctly initializes\n        the fields_cache attribute on the model instance.\n        \"\"\"\n        class Model(models.Model):\n            field = models.IntegerField()\n\n        instance = Model()\n        self.assertFalse(hasattr(instance, 'fields_cache'))\n        # Access the fields_cache attribute to trigger the descriptor.\n        cache = instance._state.fields_cache\n        self.assertTrue(hasattr(instance, 'fields_cache'))\n        self.assertEqual(cache, {})\n        self.assertEqual(instance.fields_cache, {})\n", "    def test_abstract_model_without_meta(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        self.assertEqual(AbstractModel.check(), [])\n", "    def test_property_name_related_field_accessor_clash(self):\n        class Parent(models.Model):\n            pass\n\n        class Child(Parent):\n            parent = models.ForeignKey(Parent, models.CASCADE)\n\n            @property\n                return self.parent.pk\n\n        self.assertEqual(Child.check(), [\n            Error(\n                \"The property 'parent_id' clashes with a related field accessor.\",\n                obj=Child,\n                id='models.E025',\n            )\n        ])\n"], "sample_182": ["    def test_combining_with_annotated_querysets(self):\n        qs1 = Number.objects.annotate(double_num=F('num') * 2).filter(double_num__lte=2)\n        qs2 = Number.objects.annotate(double_num=F('num') * 2).filter(double_num__gte=16)\n        self.assertNumbersEqual(qs1.union(qs2).order_by('double_num'), [0, 1, 8, 9])\n", "    def test_union_with_named_values_list(self):\n        ReservedName.objects.create(name='named_test', order=5)\n        qs1 = ReservedName.objects.all().values_list('name', 'order', named=True)\n        qs2 = ReservedName.objects.none().values_list('name', 'order', named=True)\n        result = qs1.union(qs2).first()\n        self.assertEqual(result.name, 'named_test')\n        self.assertEqual(result.order, 5)\n", "    def test_union_with_prefetch_related(self):\n        qs1 = Number.objects.prefetch_related('reservedname_set').filter(num__lte=1)\n        qs2 = Number.objects.prefetch_related('reservedname_set').filter(num__gte=8)\n        union_qs = qs1.union(qs2)\n        self.assertEqual(union_qs.count(), 4)\n        for number in union_qs:\n            self.assertTrue(hasattr(number, '_prefetched_objects_cache'))\n            self.assertIn('reservedname_set', number._prefetched_objects_cache)\n", "    def test_complex_aggregation(self):\n        qs1 = Number.objects.annotate(max_num=F('num')).filter(max_num__gte=3)\n        qs2 = Number.objects.annotate(max_num=F('num')).filter(max_num__lte=6)\n        combined_qs = qs1.union(qs2)\n        result = combined_qs.aggregate(max_value=Max('max_num'), min_value=Min('max_num'))\n        self.assertEqual(result['max_value'], 9)\n        self.assertEqual(result['min_value'], 0)\n", "    def test_combined_query_on_same_queryset(self):\n        qs = Number.objects.filter(num__lte=5)\n        combined_qs = qs.difference(qs)\n        self.assertEqual(len(combined_qs), 0)\n"], "sample_181": ["    def test_filtered_aggregate_with_f_expression(self):\n        agg = Sum(F('age') + 10, filter=Q(name__startswith='test'))\n        self.assertEqual(Author.objects.aggregate(age=agg)['age'], 230)\n", "def test_raw_query_get_columns(self):\n        raw_sql = \"SELECT * FROM %s\" % Book._meta.db_table\n        raw_query = RawQuery(raw_sql, using='default')\n        columns = raw_query.get_columns()\n        expected_columns = ['id', 'isbn', 'name', 'pages', 'rating', 'price', 'contact_id', 'publisher_id', 'pubdate']\n        self.assertEqual(columns, expected_columns)\n", "    def test_raw_query_clone(self):\n        raw_query = RawQuery(\"SELECT * FROM my_table WHERE id = %s\", \"default\", params=(1,))\n        cloned_query = raw_query.clone(using=\"default\")\n        self.assertEqual(cloned_query.sql, raw_query.sql)\n        self.assertEqual(cloned_query.params, raw_query.params)\n        self.assertEqual(cloned_query.using, raw_query.using)\n", "def test_clone_query(self):\n    original_query = Author.objects.filter(name__startswith='test').query\n    cloned_query = original_query.clone()\n    self.assertEqual(str(original_query), str(cloned_query))\n", "    def test_rawquery_get_columns(self):\n        raw_query = RawQuery(\"SELECT * FROM book\", using='default')\n        columns = raw_query.get_columns()\n        expected_columns = ['id', 'isbn', 'name', 'pages', 'rating', 'price', 'contact_id', 'publisher_id', 'pubdate']\n        self.assertEqual(columns, expected_columns)\n"], "sample_183": ["    def test_combined_expression_addition(self):\n        expr1 = Value(2, output_field=IntegerField())\n        expr2 = Value(3, output_field=IntegerField())\n        combined_expr = CombinedExpression(expr1, CombinedExpression.ADD, expr2)\n        self.assertEqual(str(combined_expr), \"2 + 3\")\n", "    def test_combined_expression_addition(self):\n        expr1 = Value(1, output_field=IntegerField())\n        expr2 = Value(2, output_field=IntegerField())\n        combined_expr = CombinedExpression(expr1, CombinedExpression.ADD, expr2, output_field=IntegerField())\n        self.assertEqual(str(combined_expr), \"1 + 2\")\n", "    def test_expression_combination_addition(self):\n        expr1 = Value(1)\n        expr2 = Value(2)\n        combined_expr = expr1 + expr2\n        self.assertEqual(str(combined_expr), \"1 + 2\")\n", "    def test_combined_expression_sqlite(self):\n        lhs = Value(5, output_field=IntegerField())\n        rhs = Value(10, output_field=IntegerField())\n        combined_expr = CombinedExpression(lhs, CombinedExpression.ADD, rhs, output_field=IntegerField())\n        sql, params = combined_expr.as_sqlite(None, connection)\n        self.assertEqual(sql, 'CAST((%s + %s) AS NUMERIC)')\n        self.assertEqual(params, [5, 10])\n", "    def test_value_expression(self):\n        value = Value(10, output_field=IntegerField())\n        sql, params = value.as_sql(None, None)\n        self.assertEqual(sql, '%s')\n        self.assertEqual(params, [10])\n"], "sample_184": ["    def test_model_without_app_label(self):\n        with self.assertRaisesMessage(RuntimeError, \"Model class invalid_models_tests.ModelWithoutAppLabel doesn't declare an explicit app_label and isn't in an application in INSTALLED_APPS.\"):\n            class ModelWithoutAppLabel(models.Model):\n                class Meta:\n                    app_label = None\n", "    def test_meta_options_ordering(self):\n        class Parent(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                abstract = True\n\n        class Child(Parent):\n            pass\n\n            class Meta:\n                ordering = ['name']\n\n        self.assertEqual(Child._meta.ordering, ['name'])\n", "    def test_modelbase_creation(self):\n        class AbstractBaseModel(models.Model):\n            class Meta:\n                abstract = True\n\n        class ConcreteModel(AbstractBaseModel):\n            field1 = models.CharField(max_length=100)\n\n        model_instance = ConcreteModel(field1='Test')\n        self.assertEqual(model_instance.field1, 'Test')\n", "    def test_model_state_initialization(self):\n        class ModelWithFields(models.Model):\n            char_field = models.CharField(max_length=100)\n            int_field = models.IntegerField()\n\n        instance = ModelWithFields(char_field=\"test\", int_field=1)\n        state = instance._state\n\n        self.assertIsInstance(state, ModelState)\n        self.assertTrue(state.adding)\n        self.assertIsNone(state.db)\n        self.assertEqual(state.fields_cache, {})\n        ", "    def test_modelbase_creation(self):\n        class BaseModel(models.Model, metaclass=ModelBase):\n            class Meta:\n                abstract = True\n\n        class ConcreteModel(BaseModel):\n            name = models.CharField(max_length=100)\n\n        self.assertEqual(ConcreteModel._meta.abstract, False)\n        self.assertEqual(ConcreteModel._meta.concrete_model, ConcreteModel)\n        self.assertIn('name', [f.name for f in ConcreteModel._meta.local_fields])\n"], "sample_185": ["    def test_date_format(self):\n        dt = datetime.date(2020, 1, 1)\n        self.assertEqual(date_format(dt), 'Jan. 1, 2020')\n        self.assertEqual(date_format(dt, format='SHORT_DATE_FORMAT'), '01/01/2020')\n        self.assertEqual(date_format(dt, format='YEAR_MONTH_FORMAT'), 'January 2020')\n        self.assertEqual(date_format(dt, format='MONTH_DAY_FORMAT'), 'January 1')\n    ", "    def test_reset_format_cache(self):\n        # Set some values in the caches.\n        _format_cache[('DATE_FORMAT', 'en')] = 'cached_value'\n        _format_modules_cache['en'] = 'cached_modules'\n        reset_format_cache()\n        self.assertEqual(_format_cache, {})\n        self.assertEqual(_format_modules_cache, {})\n", "    def setUp(self):\n        super().setUp()\n        self.format_cache_backup = _format_cache.copy()\n        self.format_modules_cache_backup = _format_modules_cache.copy()\n", "    def test_get_format(self):\n        with self.settings(\n            USE_L10N=True,\n            FORMAT_MODULE_PATH='i18n.other.locale',\n            DATE_FORMAT='Y-m-d',\n            DECIMAL_SEPARATOR='.',\n            DATE_INPUT_FORMATS=['%d-%m-%Y'],\n        ):\n            with translation.override('fr', deactivate=True):\n                self.assertEqual(get_format('DATE_FORMAT'), 'd/m/Y')\n                self.assertEqual(get_format('DECIMAL_SEPARATOR'), ',')\n                self.assertEqual(get_format('DATE_INPUT_FORMATS'), ['%d-%m-%Y', '%Y-%m-%d'])\n                self.assertEqual(get_format('UNKNOWN_FORMAT', 'en'), 'UNKNOWN_FORMAT')\n                self.assertEqual(get_format('UNKNOWN_FORMAT', 'fr'), 'UNKNOWN_FORMAT')\n", "    def test_get_format(self):\n        \"\"\"\n        Test the get_format function for various format types.\n        \"\"\"\n        with patch_formats('en', DATE_FORMAT='d/m/Y', TIME_FORMAT='H:i'):\n            self.assertEqual(get_format('DATE_FORMAT', lang='en'), 'd/m/Y')\n            self.assertEqual(get_format('TIME_FORMAT', lang='en'), 'H:i')\n        "], "sample_186": ["def test_valid_autocomplete_fields(self):\n    class ValidAutocompleteAdmin(admin.ModelAdmin):\n        autocomplete_fields = [\"author\"]\n\n    errors = ValidAutocompleteAdmin(Book, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class InvalidAutocompleteAdmin(admin.ModelAdmin):\n        autocomplete_fields = [\"nonexistent\"]\n\n    errors = InvalidAutocompleteAdmin(Book, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' refers to 'nonexistent', \"\n            \"which is not an attribute of 'admin_checks.Book'.\",\n            obj=InvalidAutocompleteAdmin,\n            id='admin.E037',\n        )\n    ]\n    self.assertEqual(errors, expected)\n\n    class InvalidAutocompleteFieldTypeAdmin(admin.ModelAdmin):\n        autocomplete_fields = [\"name\"]  # This is not a ForeignKey or ManyToManyField\n\n    errors = InvalidAutocompleteFieldTypeAdmin(Book, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' must be a foreign key or a many-to-many field.\",\n            obj=InvalidAutocompleteFieldTypeAdmin,\n            id='admin.E038',\n        )\n    ]\n    self.assertEqual(errors, expected)\n\n    class AutocompleteFieldWithoutSearchFieldsAdmin(admin.ModelAdmin):\n        autocomplete_fields = [\"author\"]\n\n    class AuthorAdmin(admin.ModelAdmin):\n        search_fields = []\n\n    admin.site.register(Author, AuthorAdmin)\n    try:\n        errors = AutocompleteFieldWithoutSearchFieldsAdmin(Book, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"AuthorAdmin must define \\\"search_fields\\\", because it's referenced by \"\n                \"AutocompleteFieldWithoutSearchFieldsAdmin.autocomplete_fields.\",\n                obj=AutocompleteFieldWithoutSearchFieldsAdmin,\n                id='admin.E040',\n            )\n        ]\n        self.assertEqual(errors, expected)\n    finally:\n        admin.site.unregister(Author)\n", "    def test_list_display_non_callable_or_attribute(self):\n        \"\"\"\n        Ensure list_display items are either callable, an attribute of the ModelAdmin,\n        or an attribute of the model.\n        \"\"\"\n        class SongAdmin(admin.ModelAdmin):\n            list_display = ['non_existent_attribute']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_display[0]' refers to 'non_existent_attribute', \"\n                \"which is not a callable, an attribute of 'SongAdmin', \"\n                \"or an attribute or method on 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E108',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_autocomplete_fields_not_list_or_tuple(self):\n        \"\"\"\n        Test that 'autocomplete_fields' is either a list or tuple.\n        \"\"\"\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = \"title\"\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                obj=SongAdmin,\n                id='admin.E036',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_prepopulated_fields_not_dict(self):\n        class SongAdmin(admin.ModelAdmin):\n            prepopulated_fields = ['title']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'prepopulated_fields' must be a dictionary.\",\n                obj=SongAdmin,\n                id='admin.E026',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_valid_autocomplete_fields(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = ['album']\n\n        class AlbumAdmin(admin.ModelAdmin):\n            search_fields = ['title']\n\n        admin.site.register(Song, SongAdmin)\n        admin.site.register(Album, AlbumAdmin)\n        try:\n            errors = SongAdmin(Song, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            admin.site.unregister(Song)\n            admin.site.unregister(Album)\n"], "sample_187": ["    def test_capfirst(self):\n        self.assertEqual(text.capfirst('hello'), 'Hello')\n        self.assertEqual(text.capfirst('Hello'), 'Hello')\n        self.assertEqual(text.capfirst(''), '')\n        self.assertEqual(text.capfirst('h'), 'H')\n        self.assertEqual(text.capfirst(' hello'), ' hello')\n        self.assertEqual(text.capfirst(lazystr('hello')), 'Hello')\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst('hello'), 'Hello')\n        self.assertEqual(text.capfirst('Hello'), 'Hello')\n        self.assertEqual(text.capfirst('123hello'), '123hello')\n        self.assertEqual(text.capfirst(''), '')\n        self.assertEqual(text.capfirst(lazystr('hello')), 'Hello')\n", "    def test_camel_case_to_spaces(self):\n        testdata = [\n            ('CamelCase', 'camel case'),\n            ('camelCase', 'camel case'),\n            ('CamelCaseTest', 'camel case test'),\n            ('Camel2Case', 'camel2 case'),\n            ('CamelCase2', 'camel case2'),\n            ('camelCaseTest', 'camel case test'),\n            ('', ''),\n            ('lowercase', 'lowercase'),\n            ('ALLCAPS', 'a l l c a p s'),  # This will split each letter\n            ('_leadingUnderscore', '_leading underscore'),\n            ('trailingUnderscore_', 'trailing underscore_'),\n        ]\n        for test, expected in testdata:\n            self.assertEqual(text.camel_case_to_spaces(test), expected)\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst('hello'), 'Hello')\n        self.assertEqual(text.capfirst('Hello'), 'Hello')\n        self.assertEqual(text.capfirst(''), '')\n        self.assertEqual(text.capfirst('123'), '123')\n        self.assertEqual(text.capfirst('hELLO'), 'HELLO')\n        self.assertEqual(text.capfirst(lazystr('hello')), 'Hello')\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('CamelCaseTest'), 'camel case test')\n        self.assertEqual(text.camel_case_to_spaces('lowerCamelCaseTest'), 'lower camel case test')\n        self.assertEqual(text.camel_case_to_spaces('NoChange'), 'no change')\n        self.assertEqual(text.camel_case_to_spaces('camelCaseToSpaces'), 'camel case to spaces')\n        self.assertEqual(text.camel_case_to_spaces(''), '')\n        self.assertEqual(text.camel_case_to_spaces('simple'), 'simple')\n"], "sample_188": ["    def test_as_sqlite_with_decimal_field(self):\n        class DecimalFieldMock:\n                return 'DecimalField'\n\n        mixin = SQLiteNumericMixin()\n        mixin.output_field = DecimalFieldMock()\n        sql, params = mixin.as_sqlite(None, None, sql='SELECT 1', params=())\n        self.assertEqual(sql, 'CAST(SELECT 1 AS NUMERIC)')\n        self.assertEqual(params, ())\n", "    def test_as_sqlite_with_decimal_field(self):\n        # Mocking an expression with DecimalField as output_field.\n        class DecimalFieldMock:\n                return 'DecimalField'\n\n        expression = mock.Mock()\n        expression.output_field = DecimalFieldMock()\n        expression.as_sql.return_value = ('original_sql', ['param1', 'param2'])\n\n        mixin = SQLiteNumericMixin()\n        sql, params = mixin.as_sqlite(expression, None, None)\n\n        self.assertEqual(sql, 'CAST(original_sql AS NUMERIC)')\n        self.assertEqual(params, ['param1', 'param2'])\n", "    def test_combined_expression_as_sql(self):\n        lhs = Value(3, output_field=IntegerField())\n        rhs = Value(5, output_field=IntegerField())\n        expr = CombinedExpression(lhs, CombinedExpression.ADD, rhs)\n        with mock.patch('django.db.backends.base.operations.BaseDatabaseOperations.combine_expression', return_value=\"%s + %s\") as mock_combine_expression:\n            sql, params = expr.as_sql(mock.MagicMock(), mock.MagicMock())\n            self.assertEqual(sql, \"(%s + %s)\")\n            self.assertEqual(params, [3, 5])\n            mock_combine_expression.assert_called_once_with('+', ['%s', '%s'])\n", "    def test_combined_expression_repr(self):\n        expr = CombinedExpression(Value(1), '+', Value(2))\n        self.assertEqual(repr(expr), \"<CombinedExpression: 1 + 2>\")\n", "    def test_as_sqlite_with_decimal_field(self):\n        class DummyField:\n                return 'DecimalField'\n\n        class DummyExpression(CombinedExpression):\n                return 'DUMMY SQL', []\n\n        expression = DummyExpression(lhs=Value(5, output_field=DummyField()), connector='+', rhs=Value(10, output_field=DummyField()))\n        sql, params = expression.as_sqlite(compiler=None, connection=None)\n        self.assertEqual(sql, 'CAST(DUMMY SQL AS NUMERIC)')\n        self.assertEqual(params, [])\n"], "sample_189": ["    def test_make_key(self):\n        cache_instance = caches['default']\n        key = cache_instance.make_key('test-key')\n        self.assertEqual(key, 'cacheprefix:1:test-key')\n", "    def setUp(self):\n        self.cache = BaseCache(params={})\n", "    def test_custom_key_func_implementation(self):\n        params = {\n            'KEY_FUNCTION': 'cache.tests.custom_key_func',\n            'KEY_PREFIX': 'prefix',\n            'VERSION': 1,\n        }\n        cache = BaseCache(params)\n        key = cache.make_key('my_key')\n        self.assertEqual(key, 'CUSTOM-prefix-1-my_key')\n", "    def test_default_key_func(self):\n        self.assertEqual(default_key_func('my_key', 'prefix', 1), 'prefix:1:my_key')\n", "    def test_validate_key_valid(self):\n        # Test that validate_key does not raise warnings for valid keys\n        key = 'valid_key_123'\n        with self.assertRaises(AssertionError):\n            with self.assertWarns(CacheKeyWarning):\n                cache.validate_key(key)\n"], "sample_190": ["def test_pattern_lookups(self):\n    # Test various pattern lookups (startswith, endswith, contains, icontains) using both direct values and field references\n    Article.objects.create(headline='Pattern Match 1', pub_date=datetime(2005, 12, 1))\n    Article.objects.create(headline='Pattern Match 2', pub_date=datetime(2005, 12, 2))\n    Article.objects.create(headline='Pattern Match 3', pub_date=datetime(2005, 12, 3))\n\n    self.assertQuerysetEqual(\n        Article.objects.filter(headline__startswith='Pattern'),\n        [\n            '<Article: Pattern Match 1>',\n            '<Article: Pattern Match 2>',\n            '<Article: Pattern Match 3>',\n        ]\n    )\n\n    self.assertQuerysetEqual(\n        Article.objects.filter(headline__endswith='Match 1'),\n        ['<Article: Pattern Match 1>']\n    )\n\n    self.assertQuerysetEqual(\n        Article.objects.filter(headline__contains='Match'),\n        [\n            '<Article: Pattern Match 1>',\n            '<Article: Pattern Match 2>',\n            '<Article: Pattern Match 3>',\n        ]\n    )\n\n    self.assertQuerysetEqual(\n        Article.objects.filter(headline__icontains='pattern'),\n        [\n            '<Article: Pattern Match 1>',\n            '<Article: Pattern Match 2>',\n            '<Article: Pattern Match 3>',\n        ]\n    )\n", "def test_uuid_lookups(self):\n    import uuid\n    # Create a few UUID-based Articles.\n    uuid1 = uuid.uuid4()\n    uuid2 = uuid.uuid4()\n    uuid3 = uuid.uuid4()\n    article1 = Article.objects.create(headline='UUID Article 1', author=self.au1, slug='uuid1', uuid=uuid1)\n    article2 = Article.objects.create(headline='UUID Article 2', author=self.au2, slug='uuid2', uuid=uuid2)\n    article3 = Article.objects.create(headline='UUID Article 3', author=self.au1, slug='uuid3', uuid=uuid3)\n    \n    self.assertQuerysetEqual(\n        Article.objects.filter(uuid__exact=uuid1),\n        ['<Article: UUID Article 1>']\n    )\n    \n    self.assertQuerysetEqual(\n        Article.objects.filter(uuid__iexact=str(uuid2).replace('-', '')),\n        ['<Article: UUID Article 2>']\n    )\n    \n    self.assertQuerysetEqual(\n        Article.objects.filter(uuid__contains=str(uuid3)[0:8]),\n        ['<Article: UUID Article 3>']\n    )\n    \n    self.assertQuerysetEqual(\n        Article.objects.filter(uuid__icontains=str(uuid3).replace('-', '')),\n        ['<Article: UUID Article 3>']\n    )\n    \n    self.assertQuerysetEqual(\n        Article.objects.filter(uuid__startswith=str(uuid1)[0:8]),\n        ['<Article: UUID Article 1>']\n    )\n    \n    self.assertQuerysetEqual(\n        Article.objects.filter(uuid__istartswith=str(uuid2).replace('-', '')[0:8]),\n        ['<Article: UUID Article 2>']\n    )\n    \n    self.assertQuerysetEqual(\n        Article.objects.filter(uuid__endswith=str(uuid3)[-8:]),\n        ['<Article: UUID Article 3>']\n    )\n    \n    self.assertQuerysetEqual(\n        Article.objects.filter(uuid__iendswith=str(uuid1).replace('-', '')[-8:]),\n        ['<Article: UUID Article 1>']\n    )\n", "    def test_uuid_lookups(self):\n        import uuid\n        uuid1 = uuid.uuid4()\n        uuid2 = uuid.uuid4()\n        uuid3 = uuid.uuid4()\n\n        # Create Authors with UUID fields\n        au1 = Author.objects.create(name='Author with UUID', alias=uuid1)\n        au2 = Author.objects.create(name='Another Author with UUID', alias=uuid2)\n\n        # Test UUID exact lookup\n        self.assertEqual(Author.objects.filter(alias__exact=uuid1).get(), au1)\n        self.assertEqual(Author.objects.filter(alias__exact=uuid2).get(), au2)\n\n        # Test UUID IExact lookup\n        self.assertEqual(Author.objects.filter(alias__iexact=str(uuid1).upper()).get(), au1)\n        self.assertEqual(Author.objects.filter(alias__iexact=str(uuid2).lower()).get(), au2)\n\n        # Test UUID Contains lookup\n        self.assertEqual(Author.objects.filter(alias__contains=str(uuid1)[4:8]).get(), au1)\n        self.assertEqual(Author.objects.filter(alias__contains=str(uuid2)[4:8]).get(), au2)\n\n        # Test UUID IContains lookup\n        self.assertEqual(Author.objects.filter(alias__icontains=str(uuid1).upper()[4:8]).get(), au1)\n        self.assertEqual(Author.objects.filter(alias__icontains=str(uuid2).lower()[4:8]).get(), au2)\n\n        # Test UUID StartsWith lookup\n        self.assertEqual(Author.objects.filter(alias__startswith=str(uuid1)[:8]).get(), au1)\n        self.assertEqual(Author.objects.filter(alias__startswith=str(uuid2)[:8]).get(), au2)\n\n        # Test UUID IStartsWith lookup\n        self.assertEqual(Author.objects.filter(alias__istartswith=str(uuid1).upper()[:8]).get(), au1)\n        self.assertEqual(Author.objects.filter(alias__istartswith=str(uuid2).lower()[:8]).get(), au2)\n\n        # Test UUID EndsWith lookup\n        self.assertEqual(Author.objects.filter(alias__endswith=str(uuid1)[-12:]).get(), au1)\n        self.assertEqual(Author.objects.filter(alias__endswith=str(uuid2)[-12:]).get(), au2)\n\n        # Test UUID IEndsWith lookup\n        self.assertEqual(Author.objects.filter(alias__iendswith=str(uuid1).upper()[-12:]).get(),", "    def test_bilateral_transforms(self):\n        class DummyTransform(Transform):\n            bilateral = True\n\n        class DummyField(Field):\n                return [DummyTransform]\n\n        field = DummyField()\n        value = 'dummy_value'\n        lookup = BuiltinLookup(field, value)\n        self.assertEqual(lookup.bilateral_transforms, [DummyTransform])\n        self.assertEqual(lookup.apply_bilateral_transforms(Value(value)).source_expressions[0].value, value)\n", "    def test_year_lookups(self):\n        # Create articles with specific years for testing year lookups\n        article_2000 = Article.objects.create(\n            headline='Article 2000',\n            pub_date=datetime(2000, 1, 1),\n            author=self.au1,\n            slug='a2000',\n        )\n        article_2010 = Article.objects.create(\n            headline='Article 2010',\n            pub_date=datetime(2010, 1, 1),\n            author=self.au1,\n            slug='a2010',\n        )\n        article_2020 = Article.objects.create(\n            headline='Article 2020',\n            pub_date=datetime(2020, 1, 1),\n            author=self.au1,\n            slug='a2020',\n        )\n\n        # Test YearExact\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__exact=2010),\n            ['<Article: Article 2010>']\n        )\n        \n        # Test YearGt\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__gt=2005),\n            ['<Article: Article 2010>', '<Article: Article 2020>']\n        )\n        \n        # Test YearGte\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__gte=2010),\n            ['<Article: Article 2010>', '<Article: Article 2020>']\n        )\n        \n        # Test YearLt\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__lt=2010),\n            ['<Article: Article 2000>']\n        )\n        \n        # Test YearLte\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__lte=2010),\n            ['<Article: Article 2000>', '<Article: Article 2010>']\n        )\n"], "sample_191": ["    def test_echo_already_on(self, mocked_tcsetattr, mocked_tcgetattr, mocked_isatty):\n        autoreload.ensure_echo_on()\n        mocked_tcgetattr.assert_called_once()\n        mocked_tcsetattr.assert_not_called()\n", "    def test_echo_on_enabled(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        attrs = [0, 0, 0, 0]\n        attrs[3] = attrs[3] & ~termios.ECHO\n        mock_tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        self.assertTrue(attrs[3] & termios.ECHO)\n        mock_tcsetattr.assert_called_once_with(sys.stdin, termios.TCSANOW, attrs)\n", "    def test_ensure_echo_on_enabled(self, mocked_termios, mocked_isatty):\n        attrs = [0, 0, 0, 0, 0, 0]\n        attrs[3] |= mocked_termios.ECHO  # Set the ECHO flag\n        mocked_termios.tcgetattr.return_value = attrs\n\n        autoreload.ensure_echo_on()\n\n        mocked_termios.tcgetattr.assert_called_once_with(sys.stdin)\n        mocked_termios.tcsetattr.assert_not_called()  # No need to set if already enabled\n", "    def test_ensure_echo_on_with_tty(self, mock_stdin, mock_termios):\n        mock_stdin.isatty.return_value = True\n        attrs = [0, 0, 0, 0, 0]\n        mock_termios.tcgetattr.return_value = attrs\n        attrs[3] = 0  # ECHO off\n        autoreload.ensure_echo_on()\n        self.assertTrue(attrs[3] & mock_termios.ECHO)\n        mock_termios.tcsetattr.assert_called_once_with(mock_stdin, mock_termios.TCSANOW, attrs)\n", "    def test_ensure_echo_on_non_tty(self, mock_isatty):\n        # Ensure nothing happens if stdin is not a TTY\n        autoreload.ensure_echo_on()\n        mock_isatty.assert_called_once()\n"], "sample_192": ["    def test_formset_with_invalid_management_form(self):\n        \"\"\"\n        If the management form is invalid, the formset should raise a\n        ValidationError.\n        \"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '0',\n            'choices-0-choice': 'Calexico',\n            'choices-0-votes': '100',\n        }\n        # Tamper with management form data to make it invalid\n        data['choices-TOTAL_FORMS'] = 'invalid'\n        \n        with self.assertRaises(ValidationError) as context:\n            ChoiceFormSet(data, auto_id=False, prefix='choices').is_valid()\n        \n        self.assertEqual(\n            str(context.exception),\n            \"[{'code': 'missing_management_form', 'message': 'ManagementForm data is missing or has been tampered with'}]\"\n        )\n", "    def test_formset_with_custom_validation_error(self):\n        \"\"\"Formset's custom validation raises a ValidationError.\"\"\"\n        class CustomErrorForm(Form):\n            name = CharField()\n            age = IntegerField()\n\n        class CustomErrorFormSet(BaseFormSet):\n                if any(form.cleaned_data.get('age') < 0 for form in self.forms if form.cleaned_data):\n                    raise ValidationError(\"Age cannot be negative.\")\n\n        CustomErrorFormSet = formset_factory(CustomErrorForm, formset=CustomErrorFormSet, extra=1)\n        data = {\n            'form-TOTAL_FORMS': '1',\n            'form-INITIAL_FORMS': '0',\n            'form-MIN_NUM_FORMS': '0',\n            'form-MAX_NUM_FORMS': '0',\n            'form-0-name': 'John Doe',\n            'form-0-age': '-1',\n        }\n        formset = CustomErrorFormSet(data, prefix='form')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['Age cannot be negative.'])\n", "    def test_management_form_initial_values(self):\n        \"\"\"\n        Test that the ManagementForm initial values are set correctly when the formset is not bound.\n        \"\"\"\n        ChoiceFormSet = formset_factory(Choice, extra=3)\n        formset = ChoiceFormSet()\n        management_form = formset.management_form\n        self.assertEqual(management_form.initial[TOTAL_FORM_COUNT], '3')\n        self.assertEqual(management_form.initial[INITIAL_FORM_COUNT], '0')\n        self.assertEqual(management_form.initial[MIN_NUM_FORM_COUNT], '0')\n        self.assertEqual(management_form.initial[MAX_NUM_FORM_COUNT], '1000')\n", "    def test_management_form_validation(self):\n        \"\"\"\n        Ensure that the management form is validated properly and raises\n        ValidationError when the management form data is tampered with.\n        \"\"\"\n        # Missing TOTAL_FORMS\n        data = {\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1000',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        with self.assertRaises(ValidationError) as cm:\n            formset.is_valid()\n        self.assertEqual(\n            str(cm.exception),\n            \"[{'message': 'ManagementForm data is missing or has been tampered with', 'code': 'missing_management_form'}]\"\n        )\n\n        # Tampered TOTAL_FORMS\n        data = {\n            'choices-TOTAL_FORMS': 'invalid',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1000',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        with self.assertRaises(ValidationError) as cm:\n            formset.is_valid()\n        self.assertEqual(\n            str(cm.exception),\n            \"[{'message': 'ManagementForm data is missing or has been tampered with', 'code': 'missing_management_form'}]\"\n        )\n", "    def test_custom_management_form_fields(self):\n        \"\"\"\n        Custom fields in the ManagementForm should be processed correctly.\n        \"\"\"\n        class CustomManagementForm(ManagementForm):\n            custom_field = CharField(widget=HiddenInput, initial='custom_value')\n\n        class CustomManagementFormset(BaseFormSet):\n            @property\n                if self.is_bound:\n                    form = CustomManagementForm(self.data, auto_id=self.auto_id, prefix=self.prefix)\n                    if not form.is_valid():\n                        raise ValidationError(\n                            _('ManagementForm data is missing or has been tampered with'),\n                            code='missing_management_form',\n                        )\n                else:\n                    form = CustomManagementForm(auto_id=self.auto_id, prefix=self.prefix, initial={\n                        TOTAL_FORM_COUNT: self.total_form_count(),\n                        INITIAL_FORM_COUNT: self.initial_form_count(),\n                        MIN_NUM_FORM_COUNT: self.min_num,\n                        MAX_NUM_FORM_COUNT: self.max_num,\n                        'custom_field': 'custom_value'\n                    })\n                return form\n\n        CustomFormSet = formset_factory(Choice, formset=CustomManagementFormset)\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1000',\n            'choices-0-choice': 'Calexico',\n            'choices-0-votes': '100',\n            'choices-custom_field': 'custom_value',\n        }\n        formset = CustomFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.management_form.cleaned_data['custom_field'], 'custom_value')\n"], "sample_193": ["    def test_resolve_relation(self):\n        \"\"\"\n        Test the resolve_relation function for various input cases.\n        \"\"\"\n        from migrations.models import Author, Book\n\n        # Case 1: relation = \"self\"\n        resolved_relation = resolve_relation(Book, \"self\")\n        self.assertEqual(resolved_relation, Book)\n\n        # Case 2: relation = \"Author\"\n        resolved_relation = resolve_relation(Book, \"Author\")\n        self.assertEqual(resolved_relation, \"migrations.Author\")\n\n        # Case 3: relation = \"migrations.Author\"\n        resolved_relation = resolve_relation(Book, \"migrations.Author\")\n        self.assertEqual(resolved_relation, \"migrations.Author\")\n\n        # Case 4: relation = Author (model class)\n        resolved_relation = resolve_relation(Book, Author)\n        self.assertEqual(resolved_relation, Author)\n", "    def test_resolve_relation(self):\n        \"\"\"\n        Test the resolve_relation function to ensure it correctly resolves\n        various forms of the relation argument.\n        \"\"\"\n        class Model(models.Model):\n            class Meta:\n                app_label = \"migrations\"\n                apps = Apps([\"migrations\"])\n\n        model_instance = Model()\n        model_instance._meta.app_label = \"app_label\"\n        model_instance._meta.model_name = \"ModelName\"\n\n        # Test RECURSIVE_RELATIONSHIP_CONSTANT\n        self.assertEqual(resolve_relation(model_instance, RECURSIVE_RELATIONSHIP_CONSTANT), model_instance)\n\n        # Test bare model name\n        self.assertEqual(resolve_relation(model_instance, \"AnotherModel\"), \"app_label.AnotherModel\")\n\n        # Test \"app_label.ModelName\" string\n        self.assertEqual(resolve_relation(model_instance, \"another_app.AnotherModel\"), \"another_app.AnotherModel\")\n\n        # Test model class\n        self.assertEqual(resolve_relation(model_instance, Model), Model)\n", "    def test_resolve_relation(self):\n        \"\"\"\n        Test resolve_relation to ensure it correctly transforms relations\n        relative to the scope model.\n        \"\"\"\n        class ModelA(models.Model):\n            class Meta:\n                app_label = 'migrations'\n                apps = Apps()\n\n        class ModelB(models.Model):\n            class Meta:\n                app_label = 'migrations'\n                apps = Apps()\n\n        # Relative relation without app label\n        self.assertEqual(resolve_relation(ModelA, 'ModelB'), 'migrations.ModelB')\n\n        # Relative relation with app label\n        self.assertEqual(resolve_relation(ModelA, 'another_app.ModelC'), 'another_app.ModelC')\n\n        # Recursive relation constant\n        self.assertEqual(resolve_relation(ModelA, 'self'), ModelA)\n\n        # Model class\n        self.assertEqual(resolve_relation(ModelA, ModelB), ModelB)\n", "    def test_related_name_validation(self):\n        \"\"\"\n        Tests the validation of invalid related_name in RelatedField.\n        \"\"\"\n        new_apps = Apps(['migrations'])\n\n        class InvalidModel(models.Model):\n            related_name = 'invalid name'\n            valid_name = 'valid_name'\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class RelatedModel(models.Model):\n            invalid_fk = models.ForeignKey(InvalidModel, models.CASCADE, related_name=InvalidModel.related_name)\n            valid_fk = models.ForeignKey(InvalidModel, models.CASCADE, related_name=InvalidModel.valid_name)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state = ProjectState()\n        project_state.add_model(ModelState.from_model(InvalidModel))\n        project_state.add_model(ModelState.from_model(RelatedModel))\n\n        invalid_fk_field = RelatedModel._meta.get_field('invalid_fk')\n        valid_fk_field = RelatedModel._meta.get_field('valid_fk')\n\n        self.assertEqual(invalid_fk_field.check()[0].id, 'fields.E306')\n        self.assertEqual(valid_fk_field.check(), [])\n", "    def test_related_field_check(self):\n        \"\"\"\n        Ensure RelatedField's check methods properly validate field configurations.\n        \"\"\"\n        new_apps = Apps([\"migrations\"])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n            author = models.ForeignKey(Author, models.CASCADE, related_name='books', related_query_name='author_books')\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class InvalidBook(models.Model):\n            title = models.CharField(max_length=100)\n            author = models.ForeignKey(Author, models.CASCADE, related_name='invalid books', related_query_name='author_books_')\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        project_state = ProjectState.from_apps(new_apps)\n        book_state = project_state.models['migrations', 'book']\n        invalid_book_state = project_state.models['migrations', 'invalidbook']\n\n        # Valid Book model should have no related field errors\n        book_field = book_state.fields['author']\n        self.assertEqual(book_field.check(), [])\n\n        # Invalid Book model should report related field name errors\n        invalid_book_field = invalid_book_state.fields['author']\n        errors = invalid_book_field.check()\n        self.assertEqual(len(errors), 2)\n        self.assertIn('fields.E306', [e.id for e in errors])  # Invalid related_name\n        self.assertIn('fields.E308', [e.id for e in errors])  # Invalid related_query_name\n"], "sample_194": ["def test_deferrable_enum_values(self):\n    self.assertEqual(str(Deferrable.DEFERRED), 'deferred')\n    self.assertEqual(str(Deferrable.IMMEDIATE), 'immediate')\n", "    def test_constraint_sql(self):\n        check = models.Q(price__gt=models.F('discounted_price'))\n        constraint = models.CheckConstraint(check=check, name='price_gt_discounted_price')\n        mock_schema_editor = mock.Mock()\n        mock_schema_editor._check_sql.return_value = 'CHECK (price > discounted_price)'\n        constraint_sql = constraint.constraint_sql(Product, mock_schema_editor)\n        self.assertEqual(constraint_sql, 'CHECK (price > discounted_price)')\n", "    def test_invalid_fields_argument(self):\n        msg = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(fields=[], name='no_fields')\n", "def test_deconstruction_with_multiple_options(self):\n    fields = ['foo', 'bar']\n    name = 'unique_fields'\n    condition = models.Q(foo=models.F('bar'))\n    include = ['baz_1', 'baz_2']\n    opclasses = ['varchar_pattern_ops', 'text_pattern_ops']\n    deferrable = models.Deferrable.DEFERRED\n    constraint = models.UniqueConstraint(\n        fields=fields, name=name, condition=condition,\n        include=include, opclasses=opclasses, deferrable=deferrable\n    )\n    path, args, kwargs = constraint.deconstruct()\n    self.assertEqual(path, 'django.db.models.UniqueConstraint')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {\n        'fields': tuple(fields),\n        'name': name,\n        'condition': condition,\n        'include': tuple(include),\n        'opclasses': opclasses,\n        'deferrable': deferrable,\n    })\n", "    def test_invalid_fields_argument(self):\n        msg = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(name='unique', fields=[])\n"], "sample_195": ["    def setUp(self):\n        self.ops = DatabaseOperations(connection=connection)\n", "    def test_bulk_batch_size(self):\n        # Case when there's only a single field to insert\n        fields = ['field1']\n        objs = ['obj1'] * 600  # More than 500 to test the limit\n        self.assertEqual(self.ops.bulk_batch_size(fields, objs), 500)\n\n        # Case when there are multiple fields\n        fields = ['field1', 'field2']\n        self.assertEqual(self.ops.bulk_batch_size(fields, objs), 499)  # 999 // 2\n\n        # Case when there are no fields\n        fields = []\n        self.assertEqual(self.ops.bulk_batch_size(fields, objs), len(objs))\n", "    def setUp(self):\n        self.ops = connection.ops\n", "    def setUp(self):\n        self.ops = DatabaseOperations(connection=connection)\n", "    def test_bulk_batch_size(self):\n        # Mock a connection with a feature max_query_params set to 999\n        class MockConnection:\n            class Features:\n                max_query_params = 999\n            features = Features()\n\n        self.ops.connection = MockConnection()\n\n        # Test case with a single field\n        fields = ['field1']\n        objs = ['obj1'] * 10  # Number of objects is irrelevant for this test\n        self.assertEqual(self.ops.bulk_batch_size(fields, objs), 500)\n\n        # Test case with multiple fields\n        fields = ['field1', 'field2']\n        objs = ['obj1', 'obj2'] * 10  # Number of objects is irrelevant for this test\n        self.assertEqual(self.ops.bulk_batch_size(fields, objs), 499)\n\n        # Test case with no fields\n        fields = []\n        objs = ['obj1'] * 10\n        self.assertEqual(self.ops.bulk_batch_size(fields, objs), len(objs))\n"], "sample_196": ["    def test_adapt_ipaddressfield_value(self):\n        self.assertEqual(self.ops.adapt_ipaddressfield_value('192.168.0.1'), '192.168.0.1')\n        self.assertIsNone(self.ops.adapt_ipaddressfield_value(''))\n        self.assertIsNone(self.ops.adapt_ipaddressfield_value(None))\n", "    def test_force_no_ordering(self):\n        self.assertEqual(self.ops.force_no_ordering(), [])\n", "    def test_adapt_ipaddressfield_value(self):\n        self.assertEqual(self.ops.adapt_ipaddressfield_value('127.0.0.1'), '127.0.0.1')\n        self.assertIsNone(self.ops.adapt_ipaddressfield_value(''))\n        self.assertIsNone(self.ops.adapt_ipaddressfield_value(None))\n", "def test_adapt_ipaddressfield_value(self):\n    self.assertEqual(self.ops.adapt_ipaddressfield_value('127.0.0.1'), '127.0.0.1')\n    self.assertEqual(self.ops.adapt_ipaddressfield_value(''), None)\n    self.assertEqual(self.ops.adapt_ipaddressfield_value(None), None)\n", "    def test_cache_key_culling_sql(self):\n        expected_sql = \"SELECT cache_key FROM %s ORDER BY cache_key LIMIT 1 OFFSET %%s\"\n        self.assertEqual(self.ops.cache_key_culling_sql(), expected_sql)\n"], "sample_198": ["    def test_func_expression_with_numeric_output_field(self):\n        \"\"\"\n        Test that Func expressions with numeric output fields are correctly handled.\n        \"\"\"\n        Number.objects.create(integer=10, float=20.5)\n        avg_value = Func('integer', function='AVG', output_field=FloatField())\n        result = Number.objects.annotate(avg_int=avg_value).values('avg_int').first()\n        self.assertEqual(result['avg_int'], 10.0)\n", "    def test_as_sqlite_cast_decimal_field(self):\n        mixin = SQLiteNumericMixin()\n        mixin.as_sql = mock.Mock(return_value=('SELECT 1', []))\n        mixin.output_field = mock.Mock()\n        mixin.output_field.get_internal_type.return_value = 'DecimalField'\n        sql, params = mixin.as_sqlite(None, None)\n        self.assertEqual(sql, 'CAST(SELECT 1 AS NUMERIC)')\n        self.assertEqual(params, [])\n", "    def test_as_sqlite_with_decimal_field(self):\n        expr = Func(Value(1), function='test_function', output_field=DecimalField())\n        compiler = mock.Mock()\n        connection = mock.Mock()\n        connection.vendor = 'sqlite'\n        sql, params = expr.as_sqlite(compiler, connection)\n        self.assertIn('CAST(test_function(%s) AS NUMERIC)', sql)\n        self.assertEqual(params, [1])\n", "        def as_sql(self, compiler, connection, **extra_context):\n            return \"%s\", []\n", "    def test_resolve_expression(self):\n        expr = BaseExpression(output_field=IntegerField())\n        resolved_expr = expr.resolve_expression()\n        self.assertIsInstance(resolved_expr, BaseExpression)\n        self.assertEqual(resolved_expr.output_field, IntegerField())\n"], "sample_197": ["def test_custom_time_strings(self):\n        \"\"\" Test with custom time strings. \"\"\"\n        custom_time_strings = {\n            'year': npgettext_lazy('custom', '%d yr', '%d yrs'),\n            'month': npgettext_lazy('custom', '%d mo', '%d mos'),\n            'week': npgettext_lazy('custom', '%d wk', '%d wks'),\n            'day': npgettext_lazy('custom', '%d dy', '%d dys'),\n            'hour': npgettext_lazy('custom', '%d hr', '%d hrs'),\n            'minute': npgettext_lazy('custom', '%d min', '%d mins'),\n        }\n\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1\\xa0yr')\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1\\xa0mo')\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), '1\\xa0wk')\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), '1\\xa0dy')\n        self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), '1\\xa0hr')\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), '1\\xa0min')\n", "    def test_custom_time_strings(self):\n        \"\"\" Custom time strings should be used if provided. \"\"\"\n        custom_time_strings = {\n            'year': 'custom %d year',\n            'month': 'custom %d month',\n            'week': 'custom %d week',\n            'day': 'custom %d day',\n            'hour': 'custom %d hour',\n            'minute': 'custom %d minute',\n        }\n        t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour + self.oneminute\n        self.assertEqual(\n            timesince(self.t, t, time_strings=custom_time_strings),\n            'custom 1 year, custom 1 month'\n        )\n        self.assertEqual(\n            timesince(self.t, t, time_strings=custom_time_strings, depth=6),\n            'custom 1 year, custom 1 month, custom 1 week, custom 1 day, custom 1 hour, custom 1 minute'\n        )\n", "    def test_custom_time_strings(self):\n        \"\"\" Custom time strings. \"\"\"\n        custom_time_strings = {\n            'year': npgettext_lazy('custom', '%d yr', '%d yrs'),\n            'month': npgettext_lazy('custom', '%d mo', '%d mos'),\n            'week': npgettext_lazy('custom', '%d wk', '%d wks'),\n            'day': npgettext_lazy('custom', '%d dy', '%d dys'),\n            'hour': npgettext_lazy('custom', '%d hr', '%d hrs'),\n            'minute': npgettext_lazy('custom', '%d min', '%d mins'),\n        }\n        self.assertEqual(\n            timesince(self.t, self.t + 2 * self.oneday + 6 * self.onehour, time_strings=custom_time_strings),\n            '2\\xa0dys, 6\\xa0hrs'\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneyear + 3 * self.onemonth, time_strings=custom_time_strings),\n            '1\\xa0yr, 3\\xa0mos'\n        )\n", "    def test_custom_time_strings(self):\n        \"\"\" Custom time strings should be used instead of default. \"\"\"\n        custom_time_strings = {\n            'year': npgettext_lazy('custom', '%d yr', '%d yrs'),\n            'month': npgettext_lazy('custom', '%d mo', '%d mos'),\n            'week': npgettext_lazy('custom', '%d wk', '%d wks'),\n            'day': npgettext_lazy('custom', '%d d', '%d ds'),\n            'hour': npgettext_lazy('custom', '%d hr', '%d hrs'),\n            'minute': npgettext_lazy('custom', '%d min', '%d mins'),\n        }\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1\\xa0yr')\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1\\xa0mo')\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), '1\\xa0wk')\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), '1\\xa0d')\n        self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), '1\\xa0hr')\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), '1\\xa0min')\n", "    def test_custom_time_strings(self):\n        \"\"\" Test custom time strings. \"\"\"\n        custom_time_strings = {\n            'year': npgettext_lazy('custom', '%d yr', '%d yrs'),\n            'month': npgettext_lazy('custom', '%d mo', '%d mos'),\n            'week': npgettext_lazy('custom', '%d wk', '%d wks'),\n            'day': npgettext_lazy('custom', '%d dy', '%d dys'),\n            'hour': npgettext_lazy('custom', '%d hr', '%d hrs'),\n            'minute': npgettext_lazy('custom', '%d min', '%d mins'),\n        }\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), '1\\xa0min')\n        self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), '1\\xa0hr')\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), '1\\xa0dy')\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), '1\\xa0wk')\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1\\xa0mo')\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1\\xa0yr')\n"], "sample_199": ["    def test_combined_expression_with_datetime_and_decimal(self):\n        test = self.b1\n        duration = datetime.timedelta(days=2)\n        b = Book.objects.annotate(\n            combined=ExpressionWrapper(F('pubdate') + Value(duration), output_field=DateTimeField())\n        ).get(isbn=test.isbn)\n        combined_date = test.pubdate + duration\n        self.assertEqual(b.combined, combined_date)\n\n        b = Book.objects.annotate(\n            combined=ExpressionWrapper(F('price') + Value(Decimal('10.00')), output_field=DecimalField())\n        ).get(isbn=test.isbn)\n        combined_price = test.price + Decimal('10.00')\n        self.assertEqual(b.combined, combined_price)\n", "    def test_combined_expression_with_datetime(self):\n        ticket = Ticket.objects.create(\n            active_at=datetime.datetime(2023, 1, 1, 12, 0, 0),\n            duration=datetime.timedelta(days=5, hours=3)\n        )\n        ticket_with_expiration = Ticket.objects.annotate(\n            expires=ExpressionWrapper(F('active_at') + F('duration'), output_field=DateTimeField())\n        ).get(pk=ticket.pk)\n        expected_expiration = datetime.datetime(2023, 1, 6, 15, 0, 0)\n        self.assertEqual(ticket_with_expiration.expires, expected_expiration)\n", "def test_combined_expression_with_arithmetic_operators(self):\n    book = Book.objects.annotate(\n        add_result=ExpressionWrapper(F('pages') + 100, output_field=IntegerField()),\n        sub_result=ExpressionWrapper(F('pages') - 50, output_field=IntegerField()),\n        mul_result=ExpressionWrapper(F('rating') * 2, output_field=FloatField()),\n        div_result=ExpressionWrapper(F('pages') / 2, output_field=FloatField()),\n        mod_result=ExpressionWrapper(F('pages') % 100, output_field=IntegerField()),\n        pow_result=ExpressionWrapper(F('rating') ** 2, output_field=FloatField())\n    ).get(pk=self.b1.pk)\n    self.assertEqual(book.add_result, self.b1.pages + 100)\n    self.assertEqual(book.sub_result, self.b1.pages - 50)\n    self.assertEqual(book.mul_result, self.b1.rating * 2)\n    self.assertEqual(book.div_result, self.b1.pages / 2)\n    self.assertEqual(book.mod_result, self.b1.pages % 100)\n    self.assertEqual(book.pow_result, self.b1.rating ** 2)\n", "    def test_combined_bitwise_operations(self):\n        # Test combined bitwise AND operation\n        books = Book.objects.annotate(\n            combined_bitwise_and=ExpressionWrapper(F('pages').bitand(Value(300)), output_field=IntegerField())\n        )\n        for book in books:\n            self.assertEqual(book.combined_bitwise_and, book.pages & 300)\n\n        # Test combined bitwise OR operation\n        books = Book.objects.annotate(\n            combined_bitwise_or=ExpressionWrapper(F('pages').bitor(Value(300)), output_field=IntegerField())\n        )\n        for book in books:\n            self.assertEqual(book.combined_bitwise_or, book.pages | 300)\n\n        # Test combined bitwise XOR operation\n        books = Book.objects.annotate(\n            combined_bitwise_xor=ExpressionWrapper(F('pages').bitxor(Value(300)), output_field=IntegerField())\n        )\n        for book in books:\n            self.assertEqual(book.combined_bitwise_xor, book.pages ^ 300)\n\n        # Test combined bitwise left shift operation\n        books = Book.objects.annotate(\n            combined_left_shift=ExpressionWrapper(F('pages').bitleftshift(Value(2)), output_field=IntegerField())\n        )\n        for book in books:\n            self.assertEqual(book.combined_left_shift, book.pages << 2)\n\n        # Test combined bitwise right shift operation\n        books = Book.objects.annotate(\n            combined_right_shift=ExpressionWrapper(F('pages').bitrightshift(Value(2)), output_field=IntegerField())\n        )\n        for book in books:\n            self.assertEqual(book.combined_right_shift, book.pages >> 2)\n", "    def test_combined_expression_with_different_fields(self):\n        book = Book.objects.annotate(\n            combined_expression=ExpressionWrapper(F('pages') * F('price'), output_field=DecimalField())\n        ).filter(isbn='159059725').first()\n        self.assertEqual(book.combined_expression, Decimal(book.pages) * book.price)\n"], "sample_200": ["def test_sanitize_address_with_punycode(self):\n    \"\"\"\n    Test sanitize_address with domain names that require punycode encoding.\n    \"\"\"\n    tests = [\n        ('user@b\u00fccher.com', 'user@xn--bcher-kva.com'),\n        ('user@ex\u00e4mple.com', 'user@xn--exmple-cua.com'),\n        ('user@t\u00e9st.com', 'user@xn--tst-qla.com'),\n        ('user@\u043f\u0440\u0438\u043c\u0435\u0440\u044b.com', 'user@xn--e1afmkfd.xn--com-9ka'),\n        ('user@\u4f8b\u5b50.com', 'user@xn--fsq.xn--com-9ka'),\n    ]\n    for original, expected in tests:\n        with self.subTest(original=original):\n            self.assertEqual(sanitize_address(original, 'utf-8'), expected)\n\n    # Test with display names\n    self.assertEqual(\n        sanitize_address(('T\u00e9st User', 'user@ex\u00e4mple.com'), 'utf-8'),\n        'T\u00e9st User <user@xn--exmple-cua.com>'\n    )\n    self.assertEqual(\n        sanitize_address('\"T\u00e9st User\" <user@ex\u00e4mple.com>', 'utf-8'),\n        'T\u00e9st User <user@xn--exmple-cua.com>'\n    )\n", "    def test_attach_multipart_mixed(self):\n        \"\"\"\n        Test attaching files and multipart/alternative content to a mixed multipart email.\n        \"\"\"\n        subject = \"Test subject\"\n        body = \"This is a text body.\"\n        from_email = \"from@example.com\"\n        to_email = [\"to@example.com\"]\n        html_content = '<p>This is an <strong>HTML</strong> body.</p>'\n        \n        msg = EmailMultiAlternatives(subject, body, from_email, to_email)\n        msg.attach_alternative(html_content, \"text/html\")\n        msg.attach(\"attachment.txt\", \"This is the content of the attachment.\", \"text/plain\")\n        \n        message = msg.message()\n        \n        # Ensure the message is multipart/mixed\n        self.assertTrue(message.is_multipart())\n        self.assertEqual(message.get_content_subtype(), \"mixed\")\n        \n        # Ensure the multipart content\n        payload = message.get_payload()\n        self.assertEqual(len(payload), 2)  # Two parts: the multipart/alternative and the attachment\n        \n        # Check the first part (should be multipart/alternative)\n        alternative_part = payload[0]\n        self.assertEqual(alternative_part.get_content_subtype(), \"alternative\")\n        self.assertTrue(alternative_part.is_multipart())\n        self.assertEqual(len(alternative_part.get_payload()), 2)  # Text and HTML parts\n        \n        # Check the text part\n        text_part = alternative_part.get_payload(0)\n        self.assertEqual(text_part.get_payload(), body)\n        self.assertEqual(text_part.get_content_type(), \"text/plain\")\n        \n        # Check the HTML part\n        html_part = alternative_part.get_payload(1)\n        self.assertEqual(html_part.get_payload(), html_content)\n        self.assertEqual(html_part.get_content_type(), \"text/html\")\n        \n        # Check the attachment part\n        attachment_part = payload[1]\n        self.assertEqual(attachment_part.get_content_type(), \"text/plain\")\n        self.assertEqual(attachment_part.get_payload(decode=True), b\"This is the content of the attachment.\")\n        self.assertEqual(attachment_part.get_filename(), \"attachment.txt\")\n", "    def test_set_payload_utf8(self):\n        \"\"\"Test that set_payload correctly handles utf-8 payloads with long lines.\"\"\"\n        long_payload = 'a' * 1000  # Single line longer than RFC5322_EMAIL_LINE_LENGTH_LIMIT\n        email = EmailMessage('Subject', long_payload, 'from@example.com', ['to@example.com'])\n        message = email.message()\n        payload = message.get_payload()\n        self.assertGreater(len(long_payload), RFC5322_EMAIL_LINE_LENGTH_LIMIT)\n        self.assertEqual(payload, long_payload)\n        self.assertEqual(message.get_content_charset(), 'utf-8')\n        self.assertEqual(message.get('Content-Transfer-Encoding'), 'quoted-printable')\n", "    def test_invalid_email_address(self):\n        \"\"\"\n        Test that an invalid email address raises a ValueError.\n        \"\"\"\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['invalid-email'])\n        with self.assertRaises(ValueError):\n            email.message()\n", "def test_attach_file_with_non_ascii_name(self):\n        \"\"\"\n        Ensure attach_file handles filenames with non-ASCII characters correctly.\n        \"\"\"\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        with tempfile.NamedTemporaryFile(delete=False, suffix='\u00fcn\u00ee\u00e7\u00f8d\u00eb.txt') as temp_file:\n            temp_file.write(b'File content with non-ASCII name.')\n\n        try:\n            email.attach_file(temp_file.name)\n            filename, content, mimetype = self.get_decoded_attachments(email)[0]\n            self.assertEqual(filename, '\u00fcn\u00ee\u00e7\u00f8d\u00eb.txt')\n            self.assertEqual(content, b'File content with non-ASCII name.')\n            self.assertEqual(mimetype, 'text/plain')\n        finally:\n            os.remove(temp_file.name)\n"], "sample_201": ["    def test_store_and_retrieve_empty_message(self):\n        \"\"\"\n        Ensure that storing and retrieving an empty message list works correctly.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.update(response)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n        self.assertEqual(list(storage), [])\n", "def test_empty_message_storage(self):\n    \"\"\"\n    Test that storing and retrieving an empty list of messages works correctly.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    \n    # Storing empty messages\n    storage._store([], response)\n    \n    # Check if the cookie is set correctly with empty value\n    cookie = response.cookies.get(storage.cookie_name)\n    self.assertIsNotNone(cookie)\n    self.assertEqual(cookie.value, '')\n    \n    # Retrieving empty messages\n    messages, all_retrieved = storage._get()\n    self.assertEqual(messages, [])\n    self.assertTrue(all_retrieved)\n", "    def test_empty_messages(self):\n        \"\"\"\n        Test that storing and retrieving an empty list of messages works correctly.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        # Store empty messages\n        storage._store([], response)\n        # Retrieve messages\n        self.assertEqual(list(storage), [])\n        # Check if the cookie is set to empty\n        self.assertEqual(response.cookies['messages'].value, '')\n", "    def test_update_cookie_empty(self):\n        \"\"\"\n        Test that the cookie is deleted when storing with an empty value.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add a message to ensure the cookie is set first.\n        storage.add(constants.INFO, 'test')\n        storage.update(response)\n        self.assertIn(storage.cookie_name, response.cookies)\n        \n        # Now, simulate the consumption of the message.\n        for m in storage:\n            pass  # Iterate through the storage to consume messages.\n        \n        # Update the response and check if the cookie is deleted.\n        storage.update(response)\n        self.assertEqual(response.cookies[storage.cookie_name].value, '')\n        self.assertEqual(response.cookies[storage.cookie_name]['max-age'], 0)\n        self.assertEqual(response.cookies[storage.cookie_name]['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n", "    def test_empty_message_handling(self):\n        \"\"\"\n        Ensure that storing and retrieving an empty message list works correctly.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        \n        # Store empty message list\n        storage.update(response)\n        \n        # Ensure cookie is set with empty value\n        self.assertEqual(response.cookies['messages'].value, '')\n        \n        # Retrieve messages\n        self.assertEqual(list(storage), [])\n"], "sample_202": ["    def test_cookie_hashing(self):\n        \"\"\"\n        Ensure that the cookie hashing mechanism works correctly and consistently.\n        \"\"\"\n        storage = self.get_storage()\n        messages = [Message(constants.INFO, 'Test message for hashing')]\n        \n        # Encode messages to get the hash\n        encoded_data = storage._encode(messages)\n        hash, _ = encoded_data.split('$', 1)\n        \n        # Verify the hash is as expected\n        self.assertEqual(hash, storage._hash(json.dumps(messages, cls=MessageEncoder, separators=(',', ':'))))\n\n        # Decode the data and ensure it's intact\n        decoded_messages = storage._decode(encoded_data)\n        self.assertEqual(messages, decoded_messages)\n", "    def test_empty_cookie(self):\n        \"\"\"\n        If the cookie is empty, the storage should return an empty list of messages.\n        \"\"\"\n        request = self.get_request()\n        request.COOKIES = {CookieStorage.cookie_name: ''}\n        storage = self.storage_class(request)\n        self.assertEqual(list(storage), [])\n", "    def test_not_finished_sentinel(self):\n        \"\"\"\n        Messages containing the not_finished sentinel are correctly identified\n        as incomplete and handled appropriately.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add messages and simulate storage\n        messages = [Message(constants.INFO, 'message %s' % x) for x in range(4)]\n        set_cookie_data(storage, messages + [CookieStorage.not_finished])\n\n        # Retrieve messages, should recognize the not_finished sentinel and remove it\n        retrieved_messages, all_retrieved = storage._get()\n        self.assertEqual(len(retrieved_messages), 4)\n        self.assertFalse(all_retrieved)\n\n        # Store messages back and check the response\n        unstored_messages = storage._store(retrieved_messages, response)\n        self.assertEqual(len(unstored_messages), 0)\n\n        # Ensure the sentinel was added back to indicate incomplete retrieval\n        self.assertIn(CookieStorage.not_finished, storage._decode(response.cookies['messages'].value))\n", "    def test_invalid_json_format(self):\n        \"\"\"\n        If the stored cookie contains invalid JSON data, the message storage\n        should handle it gracefully and return no messages.\n        \"\"\"\n        request = self.get_request()\n        storage = self.storage_class(request)\n        # Manually set an invalid JSON cookie value.\n        storage.request.COOKIES[CookieStorage.cookie_name] = 'invalid_json_data'\n        self.assertEqual(list(storage), [])\n", "    def test_not_finished_sentinel(self):\n        \"\"\"\n        Ensure that the not_finished sentinel value is correctly handled\n        when retrieving messages.\n        \"\"\"\n        storage = self.storage_class(self.get_request())\n        example_messages = ['test1', 'test2', self.storage_class.not_finished]\n        set_cookie_data(storage, example_messages)\n        messages, all_retrieved = storage._get()\n        self.assertEqual(messages, ['test1', 'test2'])\n        self.assertFalse(all_retrieved)\n"], "sample_203": ["    def test_valid_ipv4_address(self):\n        valid_ipv4 = '192.168.1.1'\n        try:\n            validators.validate_ipv4_address(valid_ipv4)\n        except ValidationError:\n            self.fail(f\"validate_ipv4_address raised ValidationError unexpectedly for {valid_ipv4}\")\n", "    def test_valid_ipv4_address(self):\n        valid_ipv4 = '192.168.0.1'\n        try:\n            validators.validate_ipv4_address(valid_ipv4)\n        except ValidationError:\n            self.fail(f\"validate_ipv4_address() raised ValidationError unexpectedly for {valid_ipv4}!\")\n", "    def test_custom_message_with_ip_validators(self):\n        cases = [\n            (validators.validate_ipv4_address, '999.999.999.999', 'invalid'),\n            (validators.validate_ipv6_address, 'invalid_ip', 'invalid'),\n            (validators.validate_ipv46_address, 'invalid_ip', 'invalid'),\n        ]\n        for validator, value, code in cases:\n            with self.subTest(type(validator).__name__, value=value):\n                class MyForm(forms.Form):\n                    field = forms.CharField(\n                        validators=[validator],\n                        error_messages={code: '%(value)s'},\n                    )\n\n                form = MyForm({'field': value})\n                self.assertIs(form.is_valid(), False)\n                self.assertEqual(form.errors, {'field': [value]})\n", "    def test_validate_ipv4_address(self):\n        with self.assertRaises(ValidationError):\n            validate_ipv4_address('999.999.999.999')\n        try:\n            validate_ipv4_address('192.168.1.1')\n        except ValidationError:\n            self.fail(\"validate_ipv4_address raised ValidationError unexpectedly!\")\n", "    def test_prohibit_null_characters_validator(self):\n        class MyForm(forms.Form):\n            field = forms.CharField(\n                validators=[validators.ProhibitNullCharactersValidator()],\n            )\n\n        form = MyForm({'field': 'a\\0b'})\n        self.assertIs(form.is_valid(), False)\n        self.assertEqual(form.errors, {'field': ['Null characters are not allowed.']})\n"], "sample_204": ["    def test_detect_conflicts(self):\n        \"\"\"\n        Tests detecting conflicts in the migration graph.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        conflicts = migration_loader.detect_conflicts()\n        expected_conflicts = {\n            \"migrations\": [\"0003_conflicting\", \"0003_alternate\"]\n        }\n        self.assertEqual(conflicts, expected_conflicts)\n", "    def test_loading_cyclic_dependencies(self):\n        \"\"\"\n        Tests that cyclic dependencies between migrations are detected and\n        an error is raised.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        with self.assertRaisesMessage(ValueError, \"Cyclic dependency detected\"):\n            loader.build_graph()\n", "    def test_inconsistent_migration_history(self):\n        \"\"\"\n        Tests that InconsistentMigrationHistory exception is raised\n        when there are unapplied dependencies in the applied migrations.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n        # Record an applied migration which has an unapplied dependency\n        recorder.record_applied(\"migrations\", \"0002_second\")\n        with self.assertRaises(InconsistentMigrationHistory):\n            migration_loader.check_consistent_history(connection)\n", "    def test_ambiguous_migration_prefix(self):\n        \"\"\"\n        Tests that AmbiguityError is raised when there are multiple migrations\n        with the same prefix.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        with self.assertRaises(AmbiguityError):\n            migration_loader.get_migration_by_prefix(\"migrations\", \"000\")\n", "    def test_migrations_module(self):\n        \"\"\"\n        Tests the migrations_module class method to ensure it returns the correct path \n        and explicit flag based on settings.MIGRATION_MODULES.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        module_path, explicit = migration_loader.migrations_module('migrations')\n        self.assertEqual(module_path, 'migrations.test_migrations')\n        self.assertTrue(explicit)\n\n        module_path, explicit = migration_loader.migrations_module('nonexistent_app')\n        self.assertEqual(module_path, 'nonexistent_app.migrations')\n        self.assertFalse(explicit)\n"], "sample_205": ["def test_str(self):\n    error1 = ValidationError('message')\n    error2 = ValidationError({'field1': 'message1', 'field2': 'message2'})\n    error3 = ValidationError(['message1', 'message2'])\n\n    self.assertEqual(str(error1), \"['message']\")\n    self.assertEqual(str(error2), \"{'field1': ['message1'], 'field2': ['message2']}\")\n    self.assertEqual(str(error3), \"['message1', 'message2']\")\n", "def test_iter(self):\n    error_dict = {\n        'field1': ValidationError('error1'),\n        'field2': ValidationError('error2'),\n    }\n    error = ValidationError(error_dict)\n    self.assertEqual(\n        list(error),\n        [('field1', ['error1']), ('field2', ['error2'])],\n    )\n\n    error_list = [\n        ValidationError('error1'),\n        ValidationError('error2'),\n    ]\n    error = ValidationError(error_list)\n    self.assertEqual(\n        list(error),\n        ['error1', 'error2'],\n    )\n\n    error = ValidationError('single error')\n    self.assertEqual(\n        list(error),\n        ['single error'],\n    )\n", "    def test_update_error_dict(self):\n        error_dict = {\n            'field1': ['error1'],\n            NON_FIELD_ERRORS: ['non-field error1'],\n        }\n        validation_error = ValidationError({'field2': 'error2'})\n        updated_error_dict = validation_error.update_error_dict(error_dict)\n\n        self.assertIn('field2', updated_error_dict)\n        self.assertEqual(updated_error_dict['field2'], ['error2'])\n        self.assertIn('field1', updated_error_dict)\n        self.assertEqual(updated_error_dict['field1'], ['error1'])\n        self.assertIn(NON_FIELD_ERRORS, updated_error_dict)\n        self.assertEqual(updated_error_dict[NON_FIELD_ERRORS], ['non-field error1'])\n        \n        validation_error = ValidationError('error3')\n        updated_error_dict = validation_error.update_error_dict(error_dict)\n        self.assertIn(NON_FIELD_ERRORS, updated_error_dict)\n        self.assertEqual(updated_error_dict[NON_FIELD_ERRORS], ['non-field error1', 'error3'])\n", "def test_str(self):\n    error1 = ValidationError('message')\n    error2 = ValidationError({'field1': 'message1', 'field2': 'message2'})\n    error3 = ValidationError(['message1', 'message2', 'message3'])\n\n    self.assertEqual(str(error1), \"['message']\")\n    self.assertEqual(\n        str(error2),\n        \"{'field1': ['message1'], 'field2': ['message2']}\",\n    )\n    self.assertEqual(\n        str(error3),\n        \"['message1', 'message2', 'message3']\",\n    )\n", "    def test_single_string_error(self):\n        error = ValidationError('Single error message')\n        self.assertEqual(error.message, 'Single error message')\n        self.assertEqual(error.code, None)\n        self.assertEqual(error.params, None)\n        self.assertEqual(list(error), ['Single error message'])\n        self.assertEqual(str(error), \"['Single error message']\")\n        self.assertEqual(repr(error), \"ValidationError(['Single error message'])\")\n"], "sample_206": ["    def test_file_field_generate_filename(self):\n        \"\"\"\n        Test the generate_filename method to ensure it correctly applies the\n        upload_to pattern to the filename.\n        \"\"\"\n        class TestModel(models.Model):\n            myfile = FileField(upload_to='uploads/%Y/%m/%d')\n\n        instance = TestModel()\n        field = instance._meta.get_field('myfile')\n        filename = 'test_file.txt'\n        generated_filename = field.generate_filename(instance, filename)\n        date_path = datetime.datetime.now().strftime('uploads/%Y/%m/%d/')\n        self.assertTrue(generated_filename.startswith(date_path))\n        self.assertTrue(generated_filename.endswith('test_file.txt'))\n", "def test_fieldfile_equality(self):\n        \"\"\"\n        Test that FieldFile equality comparison works correctly.\n        \"\"\"\n        d1 = Document(myfile='file1.txt')\n        d2 = Document(myfile='file1.txt')\n        d3 = Document(myfile='file2.txt')\n        self.assertEqual(d1.myfile, d2.myfile)\n        self.assertNotEqual(d1.myfile, d3.myfile)\n\n        \"\"\"\n        Test that FieldFile hashing works correctly.\n        \"\"\"\n        d = Document(myfile='file.txt')\n        self.assertEqual(hash(d.myfile), hash('file.txt'))\n", "    def test_generate_filename_callable(self):\n        \"\"\"\n        FileField.generate_filename should correctly apply a callable upload_to.\n        \"\"\"\n            return f\"callable_upload_dir/{filename}\"\n\n        class CallableUploadToDocument(models.Model):\n            myfile = models.FileField(upload_to=callable_upload_to)\n\n        document = CallableUploadToDocument(myfile='test_file.py')\n        filename = document.myfile.field.generate_filename(document, 'test_file.py')\n        self.assertTrue(filename.startswith('callable_upload_dir/'))\n        self.assertTrue(filename.endswith('test_file.py'))\n", "    def test_file_field_generate_filename(self):\n        \"\"\"\n        FileField.generate_filename() applies the upload_to function or path correctly.\n        \"\"\"\n        class TestModel(models.Model):\n            myfile = FileField(upload_to='test_upload/%Y/%m/%d')\n\n        instance = TestModel()\n        field = instance._meta.get_field('myfile')\n        filename = field.generate_filename(instance, 'filename.txt')\n        self.assertTrue(filename.startswith('test_upload/'))\n        self.assertTrue(filename.endswith('filename.txt'))\n\n        class CallableUploadToModel(models.Model):\n            myfile = FileField(upload_to=lambda instance, filename: f'callable_upload/{filename}')\n\n        instance = CallableUploadToModel()\n        field = instance._meta.get_field('myfile')\n        filename = field.generate_filename(instance, 'filename.txt')\n        self.assertEqual(filename, 'callable_upload/filename.txt')\n", "    def test_generate_filename_callable(self):\n        \"\"\"\n        FileField.generate_filename() should correctly process a callable upload_to argument.\n        \"\"\"\n            return 'custom_path/' + filename\n\n        class CustomUploadToDocument(models.Model):\n            myfile = models.FileField(upload_to=custom_upload_to)\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            with override_settings(MEDIA_ROOT=tmp_dir):\n                document = CustomUploadToDocument(myfile=ContentFile(b\"content\", name=\"myfile.txt\"))\n                document.save()\n                expected_path = os.path.join(tmp_dir, 'custom_path/myfile.txt')\n                self.assertTrue(os.path.exists(expected_path))\n                # Clean up\n                os.remove(expected_path)\n"], "sample_207": ["    def test_get_prep_value(self):\n        field = models.JSONField()\n        value = {'name': 'John', 'age': 30}\n        self.assertEqual(field.get_prep_value(value), json.dumps(value))\n", "    def test_key_transform_exact(self):\n        key_transform = KeyTransform('key_name', 'value')\n        lookup = KeyTransformExact(key_transform, 'expected_value')\n        self.assertIsInstance(lookup, KeyTransformExact)\n        self.assertEqual(lookup.lhs, KeyTextTransform('key_name', 'value'))\n        self.assertEqual(lookup.rhs, 'expected_value')\n", "    def test_compile_json_path(self):\n        test_cases = [\n            ([], True, '$'),\n            ([], False, ''),\n            (['a', 'b', 'c'], True, '$.\"a\".\"b\".\"c\"'),\n            (['a', '1', 'b'], True, '$.\"a\"[1].\"b\"'),\n            (['0', '1', '2'], True, '$[0][1][2]'),\n            (['a', 'b', 'c'], False, '.\"a\".\"b\".\"c\"'),\n            (['a', '1', 'b'], False, '.\"a\"[1].\"b\"'),\n            (['0', '1', '2'], False, '[0][1][2]'),\n        ]\n        for key_transforms, include_root, expected in test_cases:\n            with self.subTest(key_transforms=key_transforms, include_root=include_root):\n                self.assertEqual(\n                    compile_json_path(key_transforms, include_root),\n                    expected\n                )\n", "    def test_from_db_value_with_native_json_field_and_no_decoder(self):\n        field = models.JSONField()\n        connection.features.has_native_json_field = True\n        self.assertEqual(field.from_db_value('{\"a\": \"b\"}', None, connection), '{\"a\": \"b\"}')\n", "    def test_compile_json_path(self):\n        cases = [\n            (['a', 'b', 'c'], '$.\"a\".\"b\".\"c\"'),\n            (['a', 1, 'c'], '$.\"a\"[1].\"c\"'),\n            ([], '$'),\n            (['a'], '$.\"a\"'),\n            (['a', 'b', 'c', 2], '$.\"a\".\"b\".\"c\"[2]'),\n        ]\n        for keys, expected in cases:\n            with self.subTest(keys=keys):\n                self.assertEqual(compile_json_path(keys), expected)\n"], "sample_208": ["    def test_alter_model_table_and_field(self):\n        \"\"\"Test detection of AlterModelTable and AlterField in the same migration.\"\"\"\n        before = [\n            ModelState('app', 'Model', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'Model', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=255)),\n            ], options={'db_table': 'new_model_table'}),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterModelTable', 'AlterField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='model', table='new_model_table')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='name')\n", "def test_deep_deconstruct():\n    \"\"\"\n    Tests the deep_deconstruct method of MigrationAutodetector.\n    Ensures it correctly deconstructs various types of objects.\n    \"\"\"\n    deconstructible_obj = DeconstructibleObject(1, a='A')\n    partial_func = functools.partial(lambda x: x, 2)\n    regex = re.compile('test')\n\n    obj_list = [deconstructible_obj, partial_func, regex, 123]\n    obj_tuple = (deconstructible_obj, partial_func, regex, 123)\n    obj_dict = {'key1': deconstructible_obj, 'key2': partial_func, 'key3': regex, 'key4': 123}\n\n    autodetector = MigrationAutodetector(None, None)\n    deconstructed_list = autodetector.deep_deconstruct(obj_list)\n    deconstructed_tuple = autodetector.deep_deconstruct(obj_tuple)\n    deconstructed_dict = autodetector.deep_deconstruct(obj_dict)\n\n    self.assertEqual(deconstructed_list[0][1], (1,))\n    self.assertEqual(deconstructed_tuple[1][1], ((), {'x': 2}))\n    self.assertIsInstance(deconstructed_dict['key3'], RegexObject)\n    self.assertEqual(deconstructed_list[3], 123)\n", "    def test_alter_field_default(self):\n        \"\"\"Tests autodetection of default value changes in fields.\"\"\"\n        changes = self.get_changes([self.author_name], [self.author_name_default])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True)\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=\"Ada Lovelace\")\n", "    def test_alter_model_managers_to_empty(self):\n        \"\"\"\n        Changing the model managers to an empty list should add a new operation.\n        \"\"\"\n        before = [\n            ModelState('otherapp', 'Pony', [\n                ('id', models.AutoField(primary_key=True)),\n            ], managers=[\n                ('food_qs', FoodQuerySet.as_manager()),\n                ('food_mgr', FoodManager('a', 'b')),\n            ]),\n        ]\n        after = [\n            ModelState('otherapp', 'Pony', [\n                ('id', models.AutoField(primary_key=True)),\n            ], managers=[]),\n        ]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterModelManagers\"])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"pony\")\n        self.assertEqual([name for name, mgr in changes['otherapp'][0].operations[0].managers], [])\n", "    def test_alter_model_managers_empty(self):\n        \"\"\"\n        Tests changing a model to have no managers.\n        \"\"\"\n        changes = self.get_changes([self.other_pony_food], [self.other_pony])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterModelManagers\"])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"pony\")\n        self.assertEqual(changes['otherapp'][0].operations[0].managers, [])\n"], "sample_209": ["    def test_field_clash_detection(self):\n        \"\"\"\n        Test for detection of field clashes in model inheritance.\n        \"\"\"\n        class ParentModel(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                abstract = True\n\n        class ChildModel(ParentModel):\n            name = models.IntegerField()\n\n        errors = ChildModel.check()\n        expected_error = (\n            \"The field 'name' clashes with the field 'name' \"\n            \"from model 'ParentModel'.\"\n        )\n        self.assertIn(expected_error, str(errors))\n", "    def test_model_state_fields_cache_descriptor(self):\n        # Test the ModelStateFieldsCacheDescriptor to ensure it initializes correctly.\n        department = Department.objects.create(name=\"Finance\")\n        worker = Worker.objects.create(name=\"Temporary\", department=department)\n        self.assertFalse(hasattr(worker._state, 'fields_cache'))\n        _ = worker._state.fields_cache  # Accessing the fields_cache should initialize it.\n        self.assertTrue(hasattr(worker._state, 'fields_cache'))\n        self.assertEqual(worker._state.fields_cache, {})\n", "    def test_proxy_model_inheritance(self):\n        \"\"\"\n        Test that proxy models inherit methods and properties correctly from the base model.\n        \"\"\"\n        class BaseModel(Model):\n            class Meta:\n                abstract = True\n\n                return \"base method\"\n\n        class ProxyModel(BaseModel):\n            class Meta:\n                proxy = True\n\n        self.assertTrue(issubclass(ProxyModel, BaseModel))\n        self.assertEqual(ProxyModel().base_method(), \"base method\")\n", "    def test_deepcopy_model_instance(self):\n        \"\"\"\n        Test that deepcopy works properly on a model instance,\n        including correctly copying the state and related fields.\n        \"\"\"\n        department = Department.objects.create(id=1, name='HR')\n        worker = Worker.objects.create(name='Jane Doe', department=department)\n        worker_copy = copy.deepcopy(worker)\n\n        self.assertEqual(worker.name, worker_copy.name)\n        self.assertEqual(worker.department, worker_copy.department)\n        self.assertIsNot(worker, worker_copy)\n        self.assertIsNot(worker._state, worker_copy._state)\n        self.assertIsNot(worker._state.fields_cache, worker_copy._state.fields_cache)\n        self.assertEqual(worker._state.db, worker_copy._state.db)\n        self.assertEqual(worker._state.adding, worker_copy._state.adding)\n\n        # Modify the original instance and ensure changes do not reflect on the deepcopy\n        worker.name = 'John Doe'\n        self.assertNotEqual(worker.name, worker_copy.name)\n", "    def test_model_state_initialization(self):\n        \"\"\"\n        Test that the ModelState is initialized correctly.\n        \"\"\"\n        state = ModelState()\n        self.assertIsNone(state.db)\n        self.assertTrue(state.adding)\n        self.assertEqual(state.fields_cache, {})\n        "], "sample_210": ["    def test_redirect_with_pattern_name_and_kwargs(self):\n        \"\"\"\n        Ensure that RedirectView correctly handles URL reversing with pattern_name and kwargs.\n        \"\"\"\n        # Assuming 'artist_detail' URL pattern is defined as '/detail/artist/<int:pk>/'\n        response = RedirectView.as_view(pattern_name='artist_detail')(self.rf.get('/foo/'), pk=1)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/detail/artist/1/')\n", "    def test_setup_get_request(self):\n        \"\"\"\n        Ensure `setup` correctly initializes `get` and `head` methods.\n        \"\"\"\n        class CustomView(View):\n                return HttpResponse('Custom GET response')\n\n        view_instance = CustomView()\n        self.assertFalse(hasattr(view_instance, 'head'))\n        view_instance.setup(self.rf.get('/'))\n        self.assertTrue(hasattr(view_instance, 'head'))\n        self.assertEqual(view_instance.head, view_instance.get)\n", "    def test_template_response_mixin_render_to_response(self):\n        \"\"\"\n        Test that TemplateResponseMixin's render_to_response method returns a \n        correct TemplateResponse.\n        \"\"\"\n        class TestTemplateView(TemplateView):\n            template_name = 'generic_views/about.html'\n\n        request = self.rf.get('/')\n        view = TestTemplateView()\n        view.setup(request)\n        context = {'foo': 'bar'}\n        response = view.render_to_response(context)\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.template_name, ['generic_views/about.html'])\n        self.assertEqual(response.context_data['foo'], 'bar')\n", "    def test_permanent_redirect_override(self):\n        \"\"\"Test that overriding the permanent attribute in an instance works correctly.\"\"\"\n        class CustomRedirectView(RedirectView):\n            permanent = True\n\n        response = CustomRedirectView.as_view(url='/bar/')(self.rf.get('/foo/'))\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response.url, '/bar/')\n", "    def test_render_to_response_with_context(self):\n        \"\"\"\n        Test that render_to_response correctly uses the provided context and response_kwargs.\n        \"\"\"\n        class TestView(TemplateResponseMixin, View):\n            template_name = 'generic_views/about.html'\n\n        request = self.rf.get('/')\n        view = TestView()\n        view.setup(request)\n        context = {'key': 'value'}\n        response_kwargs = {'content_type': 'text/plain'}\n        response = view.render_to_response(context, **response_kwargs)\n\n        self.assertEqual(response.context_data, context)\n        self.assertEqual(response['Content-Type'], 'text/plain')\n        self.assertEqual(response.status_code, 200)\n"], "sample_211": ["    def test_invalid_method_name(self):\n        \"\"\"\n        Test that passing an invalid method name as a keyword argument raises a TypeError.\n        \"\"\"\n        msg = \"The method name 'invalid_method' is not accepted as a keyword argument to SimpleView().\"\n        with self.assertRaisesMessage(TypeError, msg):\n            SimpleView.as_view(invalid_method=\"value\")\n", "    def test_redirect_url_with_kwargs(self):\n        \"\"\"\n        A RedirectView can handle URL with kwargs and generate the appropriate redirect URL.\n        \"\"\"\n        response = RedirectView.as_view(url='/bar/%(param)s/')(self.rf.get('/foo/'), param='baz')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/baz/')\n", "    def test_redirect_view_with_query_string(self):\n        \"\"\"\n        Test RedirectView when query_string is set to True.\n        \"\"\"\n        response = RedirectView.as_view(url='/bar/', query_string=True)(self.rf.get('/foo/?name=John&age=30'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/?name=John&age=30')\n", "    def test_redirect_with_invalid_url(self):\n        \"\"\"\n        Test that a RedirectView with an invalid URL pattern raises an error.\n        \"\"\"\n        class InvalidRedirectView(RedirectView):\n            url = '/bar/%(nonexistent_param)d/'\n\n        with self.assertRaises(KeyError):\n            InvalidRedirectView.as_view()(self.rf.get('/foo/'))\n", "    def test_reverse_url_with_args_kwargs(self):\n        \"Named pattern parameter should reverse to the matching pattern with args and kwargs\"\n        response = RedirectView.as_view(pattern_name='artist_detail')(self.rf.get('/foo/'), pk=1, slug='test-slug')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/detail/artist/1/test-slug/')\n"], "sample_213": ["    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.storage = FileSystemStorage(location=self.temp_dir)\n", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.storage = FileSystemStorage(location=self.temp_dir)\n", "    def setUp(self):\n        self.storage = FileSystemStorage()\n        self.content_file = ContentFile(\"test content\")\n        self.field = FileField(upload_to='uploads/')\n        self.instance = Storage()\n        self.field_file = FieldFile(instance=self.instance, field=self.field, name=\"uploads/test.txt\")\n", "    def setUp(self):\n        self.instance = type('TestModel', (object,), {})()\n        self.field = FileField(upload_to='uploads/')\n        self.field.set_attributes_from_name('file')\n        self.storage = FileSystemStorage(location=tempfile.mkdtemp())\n        self.field.storage = self.storage\n", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.storage = FileSystemStorage(location=self.temp_dir, base_url='/test_media_url/')\n    "], "sample_212": ["def test_session_middleware_process_request(self):\n    \"\"\"\n    Test that process_request correctly initializes the session from the cookie.\n    \"\"\"\n    class MockSessionStore:\n            self.session_key = session_key\n            self.accessed = False\n            self.modified = False\n            self._session = {}\n\n    with self.settings(SESSION_ENGINE='mock_session_engine', SESSION_COOKIE_NAME='sessionid'):\n            return {'SessionStore': MockSessionStore}\n\n        with unittest.mock.patch('importlib.import_module', mock_import_module):\n            middleware = SessionMiddleware()\n            request = HttpRequest()\n            request.COOKIES['sessionid'] = 'testsessionkey'\n            middleware.process_request(request)\n            self.assertEqual(request.session.session_key, 'testsessionkey')\n", "def test_session_middleware_process_request(self):\n    \"\"\"\n    Test that process_request correctly initializes the session from the cookie.\n    \"\"\"\n    class MockSessionStore:\n            self.session_key = session_key\n\n    class MockSettings:\n        SESSION_COOKIE_NAME = 'sessionid'\n        SESSION_ENGINE = 'django.contrib.sessions.backends.signed_cookies'\n\n    request = HttpRequest()\n    request.COOKIES['sessionid'] = 'testsessionkey'\n\n    original_settings = settings\n    try:\n        settings = MockSettings()\n        middleware = SessionMiddleware()\n        middleware.SessionStore = MockSessionStore\n\n        middleware.process_request(request)\n        self.assertEqual(request.session.session_key, 'testsessionkey')\n    finally:\n        settings = original_settings\n", "def test_session_middleware_process_request_and_response(self):\n    \"\"\"\n    Test the process_request and process_response methods of SessionMiddleware.\n    \"\"\"\n    class MockSession:\n            self.session_key = session_key\n            self.accessed = False\n            self.modified = False\n\n            self.modified = True\n\n            return 3600\n\n            return False\n\n            return False\n\n        return HttpResponse()\n\n    middleware = SessionMiddleware(get_response)\n\n    request = HttpRequest()\n    request.COOKIES[settings.SESSION_COOKIE_NAME] = 'testsessionkey'\n    request.session = MockSession('testsessionkey')\n\n    # Test process_request\n    middleware.process_request(request)\n    self.assertEqual(request.session.session_key, 'testsessionkey')\n\n    # Test process_response with a modified session\n    request.session.accessed = True\n    request.session.modified = True\n    response = middleware.process_response(request, HttpResponse())\n    self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n    self.assertTrue(request.session.modified)\n", "def test_session_middleware_process_request(self):\n    \"\"\"\n    Test that process_request correctly initializes the session.\n    \"\"\"\n    request = HttpRequest()\n    request.COOKIES[settings.SESSION_COOKIE_NAME] = 'test_session_key'\n    \n    middleware = SessionMiddleware(lambda req: HttpResponse())\n    middleware.process_request(request)\n    \n    self.assertEqual(request.session.session_key, 'test_session_key')\n", "def test_session_cookie_set_on_response(self):\n    \"\"\"\n    Test that the session cookie is set correctly in the response when the session is modified.\n    \"\"\"\n    request = HttpRequest()\n    request.COOKIES[settings.SESSION_COOKIE_NAME] = 'test_session_key'\n    request.session = self.SessionStore('test_session_key')\n    request.session.modified = True\n    request.session.get_expire_at_browser_close = lambda: False\n    request.session.get_expiry_age = lambda: 3600\n\n    response = HttpResponse()\n    middleware = SessionMiddleware()\n    response = middleware.process_response(request, response)\n\n    self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n    cookie = response.cookies[settings.SESSION_COOKIE_NAME]\n    self.assertEqual(cookie['max-age'], 3600)\n    self.assertEqual(cookie['path'], settings.SESSION_COOKIE_PATH)\n    self.assertEqual(cookie['domain'], settings.SESSION_COOKIE_DOMAIN)\n    self.assertEqual(cookie['secure'], settings.SESSION_COOKIE_SECURE or False)\n    self.assertEqual(cookie['httponly'], settings.SESSION_COOKIE_HTTPONLY or False)\n    self.assertEqual(cookie['samesite'], settings.SESSION_COOKIE_SAMESITE)\n"], "sample_214": ["    def test_compile_json_path_with_include_root(self):\n        key_transforms = ['a', 'b', 'c']\n        path = compile_json_path(key_transforms, include_root=True)\n        self.assertEqual(path, '$.\"a\".\"b\".\"c\"')\n", "    def test_check_supported(self):\n        field = models.JSONField()\n        with mock.patch('django.db.connections') as mock_connections:\n            mock_connections.databases = {'default': 'default_db'}\n            mock_connections['default'].features.supports_json_field = False\n            errors = field.check(databases=['default'])\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'fields.E180')\n            self.assertIn('default_db does not support JSONFields.', errors[0].msg)\n", "    def test_compile_json_path(self):\n        tests = [\n            (['key1', 'key2'], True, '$.\"key1\".\"key2\"'),\n            (['key1', '2'], True, '$.\"key1\"[2]'),\n            (['key1', 'key2'], False, '.\"key1\".\"key2\"'),\n            (['key1', '2'], False, '.\"key1\"[2]'),\n            (['1', 'key2'], True, '$[1].\"key2\"'),\n            (['1', '2'], True, '$[1][2]'),\n            (['1', '2'], False, '[1][2]')\n        ]\n        for key_transforms, include_root, expected in tests:\n            with self.subTest(key_transforms=key_transforms, include_root=include_root):\n                self.assertEqual(compile_json_path(key_transforms, include_root), expected)\n", "    def test_preprocess_lhs(self):\n        # Test if preprocess_lhs correctly constructs the JSON path.\n        key_transform = KeyTransform('key', KeyTransform('nested_key'))\n        compiler_mock = mock.MagicMock()\n        connection_mock = mock.MagicMock()\n        lhs, params, key_transforms = key_transform.preprocess_lhs(compiler_mock, connection_mock)\n        self.assertEqual(key_transforms, ['key', 'nested_key'])\n", "    def test_deconstruct_with_required_args(self):\n        field = models.JSONField(null=True, blank=True, default=dict)\n        name, path, args, kwargs = field.deconstruct()\n        self.assertEqual(path, 'django.db.models.JSONField')\n        self.assertEqual(args, [])\n        self.assertEqual(kwargs['null'], True)\n        self.assertEqual(kwargs['blank'], True)\n        self.assertEqual(kwargs['default'], dict)\n"], "sample_215": ["    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_cleanse_setting_callable(self):\n        \"\"\"\n        The cleanse_setting method should wrap callable settings with CallableSettingWrapper.\n        \"\"\"\n        reporter_filter = SafeExceptionReporterFilter()\n        initial = {'CALLABLE_SETTING': lambda: \"secret_value\"}\n        cleansed = reporter_filter.cleanse_setting('SETTING_NAME', initial)\n        self.assertIsInstance(cleansed['CALLABLE_SETTING'], CallableSettingWrapper)\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_cleanse_special_types_with_multivaluedict(self):\n        \"\"\"\n        Ensure that MultiValueDict is cleansed properly.\n        \"\"\"\n        reporter_filter = SafeExceptionReporterFilter()\n        multivaluedict = MultiValueDict({\n            'username': ['user1'],\n            'password': ['super_secret']\n        })\n        cleansed = reporter_filter.cleanse_special_types(None, multivaluedict)\n        self.assertEqual(cleansed['username'], ['user1'])\n        self.assertEqual(cleansed['password'], [reporter_filter.cleansed_substitute])\n", "    def test_cleanse_special_types_multivaluedict(self):\n        \"\"\"\n        Test that cleanse_special_types correctly cleanses MultiValueDicts.\n        \"\"\"\n        reporter_filter = SafeExceptionReporterFilter()\n        request = RequestFactory().post('/test_view/', data={'password': 'secret'})\n        multivaluedict = MultiValueDict({'password': ['secret']})\n        cleansed = reporter_filter.cleanse_special_types(request, multivaluedict)\n        self.assertEqual(cleansed['password'], reporter_filter.cleansed_substitute)\n"], "sample_216": ["def test_resolve_relation(self):\n    \"\"\"Test the resolve_relation function with various inputs.\"\"\"\n    # Test with a model class.\n    class MockModel:\n        class _meta:\n            app_label = 'testapp'\n            model_name = 'mockmodel'\n\n    self.assertEqual(resolve_relation(MockModel), ('testapp', 'mockmodel'))\n    \n    # Test with a string model reference.\n    self.assertEqual(resolve_relation('app.Model'), ('app', 'model'))\n    \n    # Test with a string model reference without app_label.\n    with self.assertRaises(TypeError):\n        resolve_relation('Model')\n    \n    # Test with RECURSIVE_RELATIONSHIP_CONSTANT.\n    with self.assertRaises(TypeError):\n        resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT)\n    \n    self.assertEqual(\n        resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, 'app_label', 'model_name'),\n        ('app_label', 'model_name')\n    )\n    \n    # Test with a string model reference with app_label.\n    self.assertEqual(resolve_relation('Model', 'app_label'), ('app_label', 'model'))\n    \n    # Test with invalid string model reference.\n    with self.assertRaises(TypeError):\n        resolve_relation('InvalidModel', None)\n", "def test_resolve_relation(self):\n    \"\"\"Tests resolve_relation function for various cases.\"\"\"\n    # Case 1: Model as string with RECURSIVE_RELATIONSHIP_CONSTANT\n    with self.assertRaises(TypeError):\n        resolve_relation('self')\n    self.assertEqual(resolve_relation('self', 'app', 'model'), ('app', 'model'))\n\n    # Case 2: Model as string with app_label\n    self.assertEqual(resolve_relation('app.Model'), ('app', 'model'))\n    with self.assertRaises(TypeError):\n        resolve_relation('Model')\n\n    # Case 3: Model as string with app_label provided\n    self.assertEqual(resolve_relation('Model', 'app'), ('app', 'model'))\n\n    # Case 4: Model as model class\n    MockModel = type('MockModel', (object,), {'_meta': type('Meta', (object,), {'app_label': 'app', 'model_name': 'model'})})\n    self.assertEqual(resolve_relation(MockModel), ('app', 'model'))\n", "    def test_field_is_referenced(self):\n        \"\"\"\n        Tests the field_is_referenced function to check if a field is referenced by any state models.\n        \"\"\"\n        state = ProjectState()\n        state.add_model(self.author_empty.clone())\n        state.add_model(self.book.clone())\n\n        # Add a foreign key field to the author model\n        field = models.ForeignKey('otherapp.Book', models.CASCADE)\n        state.models['testapp.Author'].fields['book'] = field\n\n        # Check if the book field is referenced by the author model\n        self.assertTrue(field_is_referenced(state, ('otherapp', 'book'), ('testapp', 'author', 'book')))\n\n        # Check if a non-existing field is not referenced\n        self.assertFalse(field_is_referenced(state, ('otherapp', 'book'), ('testapp', 'author', 'non_existent_field')))\n\n        # Remove the foreign key field and check again\n        del state.models['testapp.Author'].fields['book']\n        self.assertFalse(field_is_referenced(state, ('otherapp', 'book'), ('testapp', 'author', 'book')))\n", "def test_field_references_recursive_relationship(self):\n    \"\"\"\n    Test field_references with a recursive relationship to make sure it\n    correctly resolves and returns the appropriate FieldReference.\n    \"\"\"\n    remote_field_mock = mock.Mock()\n    remote_field_mock.model = RECURSIVE_RELATIONSHIP_CONSTANT\n    remote_field_mock.to_fields = None\n    remote_field_mock.through = None\n    field_mock = mock.Mock()\n    field_mock.remote_field = remote_field_mock\n\n    model_tuple = ('app', 'Model')\n    reference_model_tuple = ('app', 'model')\n    reference_field_name = None\n    reference_field = None\n\n    with self.assertRaises(TypeError):\n        field_references(\n            model_tuple,\n            field_mock,\n            reference_model_tuple,\n            reference_field_name,\n            reference_field,\n        )\n", "    def test_resolve_relation_recursive(self):\n        \"\"\"Tests resolve_relation with recursive relationship.\"\"\"\n        # Define a fake model with _meta attributes\n        FakeModel = namedtuple('FakeModel', '_meta')\n        FakeMeta = namedtuple('FakeMeta', 'app_label model_name')\n        \n        model = FakeModel(FakeMeta('testapp', 'TestModel'))\n        \n        # Test resolving a recursive relationship\n        resolved = resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, 'app_label', 'model_name')\n        self.assertEqual(resolved, ('app_label', 'model_name'))\n        \n        # Test resolving a normal model reference\n        resolved = resolve_relation('otherapp.OtherModel')\n        self.assertEqual(resolved, ('otherapp', 'othermodel'))\n        \n        # Test resolving a model class reference\n        resolved = resolve_relation(model)\n        self.assertEqual(resolved, ('testapp', 'testmodel'))\n        \n        # Test resolve unscoped model relationship\n        with self.assertRaises(TypeError):\n            resolve_relation('OtherModel', None, None)\n"], "sample_217": ["    def test_widget_format_value(self):\n        # Test the format_value method of various widgets.\n        text_widget = TextInput()\n        number_widget = NumberInput()\n        email_widget = EmailInput()\n        url_widget = URLInput()\n\n        self.assertEqual(text_widget.format_value(None), None)\n        self.assertEqual(text_widget.format_value(''), None)\n        self.assertEqual(text_widget.format_value('test'), 'test')\n\n        self.assertEqual(number_widget.format_value(None), None)\n        self.assertEqual(number_widget.format_value(123), '123')\n\n        self.assertEqual(email_widget.format_value(None), None)\n        self.assertEqual(email_widget.format_value('test@example.com'), 'test@example.com')\n\n        self.assertEqual(url_widget.format_value(None), None)\n        self.assertEqual(url_widget.format_value('http://example.com'), 'http://example.com')\n", "    def test_custom_widgets(self):\n        ###############################################################\n        # Custom widget media handling\n        ###############################################################\n\n        # A custom widget can define its own media assets\n        class CustomWidget(TextInput):\n            class Media:\n                css = {\n                    'all': ('/custom/css1', '/custom/css2')\n                }\n                js = ('/custom/js1', 'http://media.custom.com/path/to/js2', 'https://secure.custom.com/path/to/js3')\n\n        custom_widget = CustomWidget()\n        self.assertEqual(\n            str(custom_widget.media),\n            \"\"\"<link href=\"/custom/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_splitdatetimewidget(self):\n    class MyForm(Form):\n        field1 = CharField(widget=SplitDateTimeWidget())\n\n    form = MyForm()\n    self.assertHTMLEqual(\n        str(form['field1']),\n        \"\"\"<input type=\"text\" name=\"field1_0\" required id=\"id_field1_0\">", "    def test_media_repr_and_str(self):\n        # Test __repr__ and __str__ methods of Media class for better coverage\n        media = Media(css={'all': ['path/to/css1', 'path/to/css2']}, js=['/path/to/js1'])\n        self.assertEqual(\n            repr(media),\n            \"Media(css={'all': ['path/to/css1', 'path/to/css2']}, js=['/path/to/js1'])\"\n        )\n        self.assertEqual(\n            str(media),\n            \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_widget_render(self):\n        class TestWidget(TextInput):\n            template_name = 'django/forms/widgets/text.html'\n\n        widget = TestWidget(attrs={'id': 'test_id', 'class': 'test_class'})\n        output = widget.render(name='test_name', value='test_value')\n        self.assertIn('id=\"test_id\"', output)\n        self.assertIn('class=\"test_class\"', output)\n        self.assertIn('name=\"test_name\"', output)\n        self.assertIn('value=\"test_value\"', output)\n"], "sample_218": ["    def test_trunc_quarter_func_boundaries_with_timezone(self):\n        \"\"\"\n        Test truncation to quarter boundaries with timezone applied.\n        \"\"\"\n        start_datetime = datetime(2016, 4, 1, 14, 30, 50, 321)  # Second quarter start\n        end_datetime = datetime(2016, 6, 30, 14, 10, 50, 123)  # Second quarter end\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n        \n        model = DTModel.objects.annotate(\n            melb_quarter=TruncQuarter('start_datetime', tzinfo=melb),\n        ).order_by('start_datetime').get()\n\n        melb_start_datetime = start_datetime.astimezone(melb)\n        self.assertEqual(model.start_datetime, start_datetime)\n        self.assertEqual(model.melb_quarter, truncate_to(start_datetime, 'quarter', melb))\n\n        # Test for transition between quarters\n        start_datetime = datetime(2016, 7, 1, 0, 0, 0, 0)  # Third quarter start\n        end_datetime = datetime(2016, 9, 30, 23, 59, 59, 999999)  # Third quarter end\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        model = DTModel.objects.annotate(\n            melb_quarter=TruncQuarter('start_datetime', tzinfo=melb),\n        ).order_by('start_datetime').get()\n\n        melb_start_datetime = start_datetime.astimezone(melb)\n        self.assertEqual(model.start_datetime, start_datetime)\n        self.assertEqual(model.melb_quarter, truncate_to(start_datetime, 'quarter', melb))\n", "def test_trunc_func_with_explicit_tzinfo(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_date', kind, output_field=DateField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.date(), kind)),\n                (end_datetime, truncate_to(end_datetime.date(), kind))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_time', kind, output_field=TimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.time(), kind)),\n                (end_datetime, truncate_to(end_datetime.time(), kind))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_date_kind('year')\n    test_date_kind('quarter')\n    test_date_kind('month')\n    test_date_kind('week')\n    test_date_kind('day')\n    test_time_kind('hour')\n    test_time_kind('minute')\n    test_time_kind('second')\n    test_datetime_kind('year')\n    test_datetime_kind('quarter')\n    test", "    def test_now_func(self):\n        # Test Now function without timezone\n        now = datetime.now()\n        if settings.USE_TZ:\n            now = timezone.make_aware(now, is_dst=False)\n\n        DTModel.objects.create(\n            name=\"Test Now\",\n            start_datetime=now,\n            end_datetime=now + timedelta(days=1),\n            start_date=now.date(),\n            end_date=(now + timedelta(days=1)).date(),\n            start_time=now.time(),\n            end_time=(now + timedelta(days=1)).time(),\n            duration=timedelta(days=1),\n        )\n\n        now_annotated = DTModel.objects.annotate(current_time=Now()).first()\n        self.assertIsInstance(now_annotated.current_time, datetime)\n\n        with self.assertRaisesMessage(ValueError, \"Cannot truncate DateField 'start_date' to DateTimeField\"):\n            list(DTModel.objects.annotate(truncated=TruncDate('start_date', output_field=DateTimeField())))\n", "    def test_extract_daylight_saving_transition(self):\n        # Test for correct behavior during daylight saving time transitions\n        melb = pytz.timezone('Australia/Melbourne')\n        # Daylight saving start\n        dst_start = datetime(2016, 10, 2, 1, 30)\n        # Daylight saving end\n        dst_end = datetime(2016, 4, 3, 1, 30)\n        dst_start = timezone.make_aware(dst_start, melb, is_dst=True)\n        dst_end = timezone.make_aware(dst_end, melb, is_dst=False)\n        self.create_model(dst_start, dst_end)\n        \n        # Extract hour should reflect the correct hour after transition\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=ExtractHour('start_datetime')).order_by('start_datetime'),\n            [(dst_start, 1)],\n            lambda m: (m.start_datetime, m.extracted)\n        )\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=ExtractHour('end_datetime')).order_by('end_datetime'),\n            [(dst_end, 1)],\n            lambda m: (m.end_datetime, m.extracted)\n        )\n        \n        # Extract day should reflect the correct day after transition\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=ExtractDay('start_datetime')).order_by('start_datetime'),\n            [(dst_start, 2)],\n            lambda m: (m.start_datetime, m.extracted)\n        )\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(extracted=ExtractDay('end_datetime')).order_by('end_datetime'),\n            [(dst_end, 3)],\n            lambda m: (m.end_datetime, m.extracted)\n        )\n", "    def test_trunc_hour_with_dst_transition(self):\n        \"\"\"\n        Test the TruncHour function during a daylight saving transition.\n        \"\"\"\n        melb = pytz.timezone('Australia/Melbourne')\n        start_datetime = datetime(2021, 4, 4, 2, 30)  # DST ends at 3:00 AM on April 4, 2021\n        end_datetime = datetime(2021, 4, 4, 3, 30)\n        start_datetime = timezone.make_aware(start_datetime, melb, is_dst=True)\n        end_datetime = timezone.make_aware(end_datetime, melb, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        qs = DTModel.objects.annotate(truncated=TruncHour('start_datetime')).order_by('start_datetime')\n\n        self.assertQuerysetEqual(\n            qs,\n            [\n                (start_datetime, truncate_to(start_datetime, 'hour')),\n                (end_datetime, truncate_to(end_datetime, 'hour'))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n"], "sample_219": ["        def get_internal_type():\n            return 'DecimalField'\n", "    def test_func_with_custom_function_name(self):\n        class MyFunc(Func):\n            function = 'MY_CUSTOM_FUNCTION'\n\n        expr = MyFunc(Value(1), Value(2))\n        self.assertEqual(expr.function, 'MY_CUSTOM_FUNCTION')\n        self.assertEqual(expr.template, '%(function)s(%(expressions)s)')\n        self.assertEqual(expr.arg_joiner, ', ')\n", "    def test_func_output_field(self):\n        # Test that Func correctly infers its output_field based on the first provided expression.\n        func = Func(Value(1, output_field=IntegerField()))\n        self.assertIsInstance(func.output_field, IntegerField)\n        \n        func = Func(Value(3.14, output_field=FloatField()))\n        self.assertIsInstance(func.output_field, FloatField)\n        \n        func = Func(Value(\"test\", output_field=CharField()))\n        self.assertIsInstance(func.output_field, CharField)\n        ", "        def __init__(self, output_field):\n            super().__init__(output_field=output_field)\n", "    def test_func_instantiation(self):\n        func = Func(Value(1), Value(2), function='MY_FUNC', output_field=IntegerField())\n        self.assertEqual(func.function, 'MY_FUNC')\n        self.assertEqual(func.template, '%(function)s(%(expressions)s)')\n        self.assertEqual(func.arg_joiner, ', ')\n        self.assertEqual(func.arity, None)\n        self.assertEqual(func.get_source_expressions(), [Value(1), Value(2)])\n        self.assertIsInstance(func.output_field, IntegerField)\n"], "sample_220": ["    def test_http_response_init(self):\n        \"\"\"HttpResponse initializes with given content and headers.\"\"\"\n        response = HttpResponse(\"Hello, World!\")\n        self.assertEqual(response.content, b\"Hello, World!\")\n        self.assertEqual(response['Content-Type'], \"text/html; charset=utf-8\")\n", "    def test_basic_response_initialization(self):\n        \"\"\"HttpResponse initializes with default values properly.\"\"\"\n        response = HttpResponse()\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/html; charset=utf-8')\n        self.assertFalse(response.streaming)\n        self.assertEqual(response.content, b'')\n    ", "    def test_json_response_with_dict(self):\n        \"\"\"JsonResponse should serialize a dictionary to JSON.\"\"\"\n        data = {'key': 'value'}\n        response = JsonResponse(data)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'application/json')\n        self.assertEqual(response.content, b'{\"key\": \"value\"}')\n", "    def test_http_response_init_defaults(self):\n        \"\"\"HttpResponse initializes with default content, status code, and headers.\"\"\"\n        response = HttpResponse()\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/html; charset=utf-8')\n        self.assertEqual(response.content, b'')\n", "    def test_reason_phrase_default(self):\n        \"\"\"HttpResponse uses default reason phrase for status code.\"\"\"\n        response = HttpResponse(status=200)\n        self.assertEqual(response.reason_phrase, 'OK')\n        response = HttpResponse(status=404)\n        self.assertEqual(response.reason_phrase, 'Not Found')\n"], "sample_221": ["    def setUpTestData(cls):\n        cls.group = Group.objects.create(name='Group 1')\n        cls.event1 = Event.objects.create(title='Event 1', group=cls.group)\n        cls.event2 = Event.objects.create(title='Event 2', group=cls.group)\n        cls.happening = Happening.objects.create(name='Test Happening')\n", "    def setUpTestData(cls):\n        cls.group1 = Group.objects.create(name='Group 1')\n        cls.group2 = Group.objects.create(name='Group 2')\n        cls.event1 = Event.objects.create(title='Event 1', group=cls.group1)\n        cls.event2 = Event.objects.create(title='Event 2', group=cls.group2)\n        cls.happening = Happening.objects.create(name='Happening 1', when=datetime.datetime.now())\n", "    def setUpTestData(cls):\n        cls.group = Group.objects.create(name=\"Test Group\")\n", "    def test_queryset_len(self):\n        \"\"\"\n        Test the __len__ method of the QuerySet.\n        \"\"\"\n        group = Group.objects.create(name='Test Group')\n        Event.objects.create(title='Event 1', group=group)\n        Event.objects.create(title='Event 2', group=group)\n        qs = Event.objects.filter(group=group)\n        self.assertEqual(len(qs), 2)\n", "    def setUpTestData(cls):\n        cls.group1 = Group.objects.create(name='Group 1')\n        cls.group2 = Group.objects.create(name='Group 2')\n        cls.event1 = Event.objects.create(title='Event 1', group=cls.group1)\n        cls.event2 = Event.objects.create(title='Event 2', group=cls.group2)\n"], "sample_222": ["    def test_fcntl_lock_and_unlock(self):\n        with tempfile.NamedTemporaryFile() as temp_file:\n            fd = temp_file.fileno()\n            self.assertTrue(locks.lock(temp_file, locks.LOCK_EX))\n            self.assertTrue(locks.unlock(temp_file))\n", "def test_lock_unlock_nonexistent_file(self):\n    nonexistent_file_path = 'nonexistent_file.txt'\n    with self.assertRaises(FileNotFoundError):\n        with open(nonexistent_file_path, 'r') as f:\n            locks.lock(f, locks.LOCK_EX)\n            locks.unlock(f)\n", "def test_non_blocking_shared_lock(self):\n    file_path = Path(__file__).parent / 'test.png'\n    with open(file_path) as f1, open(file_path) as f2:\n        self.assertIs(locks.lock(f1, locks.LOCK_SH | locks.LOCK_NB), True)\n        self.assertIs(locks.lock(f2, locks.LOCK_SH | locks.LOCK_NB), True)\n        self.assertIs(locks.unlock(f1), True)\n        self.assertIs(locks.unlock(f2), True)\n", "    def test_lock_unlock_shared(self):\n        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n            try:\n                self.assertTrue(locks.lock(temp_file, locks.LOCK_SH))\n                self.assertTrue(locks.unlock(temp_file))\n            finally:\n                os.unlink(temp_file.name)\n", "    def test_lock_unlock_non_existent_file(self):\n        # Test locking and unlocking a non-existent file\n        with self.assertRaises(FileNotFoundError):\n            with open('non_existent_file.txt', 'wb') as f:\n                locks.lock(f, locks.LOCK_EX)\n                "], "sample_223": ["    def test_bulk_update_with_expression(self):\n        \"\"\"\n        Test bulk_update method with an expression.\n        \"\"\"\n        n1 = Note.objects.create(note='note1', misc='misc1')\n        n2 = Note.objects.create(note='note2', misc='misc2')\n        Note.objects.bulk_update(\n            [n1, n2],\n            fields=['note'],\n            batch_size=2\n        )\n        self.assertEqual(Note.objects.get(id=n1.id).note, 'note1')\n        self.assertEqual(Note.objects.get(id=n2.id).note, 'note2')\n", "    def test_queryset_deepcopy(self):\n        \"\"\"\n        Test that deepcopy of a QuerySet doesn't populate the result cache.\n        \"\"\"\n        author = Author.objects.create(name='author')\n        qs = Author.objects.all()\n        with self.assertNumQueries(0):\n            qs_copy = copy.deepcopy(qs)\n            self.assertIsNone(qs_copy._result_cache)\n            self.assertEqual(list(qs_copy), [author])\n", "    def setUpTestData(cls):\n        cls.model_a = ModelA.objects.create(name='model_a')\n        cls.model_b = ModelB.objects.create(name='model_b', c=ModelC.objects.create(name='model_c'))\n        cls.model_a_with_b = ModelA.objects.create(name='model_a_with_b', b=cls.model_b)\n        cls.model_d = ModelD.objects.create(name='model_d')\n        cls.model_a_with_d = ModelA.objects.create(name='model_a_with_d', d=cls.model_d)\n", "    def test_get_or_create(self):\n        \"\"\"Test the get_or_create method of QuerySet.\"\"\"\n        obj, created = ExtraInfo.objects.get_or_create(info='e3', defaults={'note': None, 'value': 100})\n        self.assertTrue(created)\n        self.assertEqual(obj.info, 'e3')\n        self.assertEqual(obj.value, 100)\n        self.assertIsNone(obj.note)\n        # Test retrieval\n        obj2, created = ExtraInfo.objects.get_or_create(info='e3', defaults={'note': None, 'value': 200})\n        self.assertFalse(created)\n        self.assertEqual(obj2, obj)\n        self.assertEqual(obj2.value, 100)  # value should not change\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name='Author1', num=1001)\n        cls.author2 = Author.objects.create(name='Author2', num=1002)\n        cls.item1 = Item.objects.create(name='Item1', creator=cls.author1)\n        cls.item2 = Item.objects.create(name='Item2', creator=cls.author2)\n        cls.item3 = Item.objects.create(name='Item3', creator=cls.author2)\n"], "sample_224": ["    def test_bulk_create(self):\n        new_authors = [\n            Author(name='Author1', age=40),\n            Author(name='Author2', age=50),\n            Author(name='Author3', age=60)\n        ]\n        Author.objects.bulk_create(new_authors)\n        self.assertEqual(Author.objects.count(), 12)\n\n        author_names = Author.objects.values_list('name', flat=True)\n        self.assertIn('Author1', author_names)\n        self.assertIn('Author2', author_names)\n        self.assertIn('Author3', author_names)\n", "def test_bulk_create(self):\n    authors = [\n        Author(name='Test Author 1', age=50),\n        Author(name='Test Author 2', age=60),\n        Author(name='Test Author 3', age=70),\n    ]\n    Author.objects.bulk_create(authors)\n    self.assertEqual(Author.objects.count(), 12)  # 9 existing authors + 3 new authors\n\n    # Check if the authors were actually created\n    author_names = list(Author.objects.values_list('name', flat=True))\n    self.assertIn('Test Author 1', author_names)\n    self.assertIn('Test Author 2', author_names)\n    self.assertIn('Test Author 3', author_names)\n", "    def test_bulk_create(self):\n        Author.objects.all().delete()\n        authors = [Author(name=f'Author {i}', age=30 + i) for i in range(5)]\n        created_authors = Author.objects.bulk_create(authors)\n        self.assertEqual(Author.objects.count(), 5)\n        for author in created_authors:\n            self.assertIn(author, Author.objects.all())\n", "    def test_bulk_create(self):\n        publishers = [\n            Publisher(name='Publisher1', num_awards=2),\n            Publisher(name='Publisher2', num_awards=5),\n        ]\n        Publisher.objects.bulk_create(publishers)\n        self.assertEqual(Publisher.objects.count(), 7)\n        self.assertQuerysetEqual(\n            Publisher.objects.order_by('name'), \n            ['Apress', 'Expensive Publisher', 'Jonno\\'s House of Books', 'Morgan Kaufmann', 'Prentice Hall', 'Publisher1', 'Publisher2', 'Sams'],\n            lambda p: p.name\n        )\n", "    def test_bulk_create(self):\n        # Test bulk_create with regular fields\n        authors = [\n            Author(name='Author 1', age=50),\n            Author(name='Author 2', age=60),\n            Author(name='Author 3', age=70)\n        ]\n        created_authors = Author.objects.bulk_create(authors)\n        self.assertEqual(len(created_authors), 3)\n        self.assertQuerysetEqual(\n            Author.objects.filter(name__in=['Author 1', 'Author 2', 'Author 3']).order_by('name'),\n            ['Author 1', 'Author 2', 'Author 3'],\n            lambda a: a.name\n        )\n\n        # Test bulk_create with ignoring conflicts\n        Publisher.objects.create(name='Test Publisher', num_awards=5)\n        publishers = [\n            Publisher(name='Test Publisher', num_awards=5),  # This should be ignored\n            Publisher(name='Unique Publisher', num_awards=10)\n        ]\n        created_publishers = Publisher.objects.bulk_create(publishers, ignore_conflicts=True)\n        self.assertEqual(len(created_publishers), 2)\n        self.assertQuerysetEqual(\n            Publisher.objects.filter(name__in=['Test Publisher', 'Unique Publisher']).order_by('name'),\n            ['Test Publisher', 'Unique Publisher'],\n            lambda p: p.name\n        )\n"], "sample_225": ["    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite(name=\"test_adminsite\")\n        self.factory = RequestFactory()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n        self.site.register(User)\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n"], "sample_226": ["    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n\n        with mock.patch.object(creation, '_clone_test_db') as mock_clone_test_db:\n            creation.clone_test_db(suffix='clone', verbosity=0, autoclobber=True, keepdb=False)\n        \n        mock_clone_test_db.assert_called_once_with('clone', verbosity=0, keepdb=False)\n", "    def test_create_destroy_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db') as mock_create_test_db:\n                with mock.patch.object(creation, '_destroy_test_db') as mock_destroy_test_db:\n                    test_db_name = creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n                    creation.destroy_test_db(old_database_name, verbosity=0, keepdb=False)\n                    mock_create_test_db.assert_called_once()\n                    mock_destroy_test_db.assert_called_once_with(test_db_name, verbosity=0)\n        finally:\n            # Clean up in case of test failure\n            creation.connection.settings_dict[\"NAME\"] = old_database_name\n", "    def test_create_test_db_keepdb(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db') as mock_create_db, \\\n                 mock.patch.object(creation, 'log') as mock_log:\n                creation.create_test_db(verbosity=1, autoclobber=True, serialize=False, keepdb=True)\n                mock_create_db.assert_called_once_with(1, True, True)\n                mock_log.assert_called_with(\"Using existing test database for alias 'default'...\")\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_create_test_db(self, mocked_call_command):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                test_db_name = creation.create_test_db(verbosity=1, autoclobber=True, serialize=True)\n                self.assertTrue(test_db_name.startswith(TEST_DATABASE_PREFIX))\n                # Ensure the database name has been set correctly.\n                self.assertEqual(test_connection.settings_dict['NAME'], test_db_name)\n                # Ensure the database has been migrated.\n                mocked_call_command.assert_any_call(\n                    'migrate',\n                    verbosity=0,\n                    interactive=False,\n                    database=test_connection.alias,\n                    run_syncdb=True,\n                )\n                # Ensure the cache table has been created.\n                mocked_call_command.assert_any_call('createcachetable', database=test_connection.alias)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0, keepdb=False)\n", "    def setUp(self):\n        self.test_connection = get_connection_copy()\n        self.creation = BaseDatabaseCreation(self.test_connection)\n        self.old_database_name = self.test_connection.settings_dict['NAME']\n"], "sample_227": ["    def test_generic_foreign_key_check_methods(self):\n        \"\"\"\n        Test the check methods (_check_field_name, _check_object_id_field,\n        _check_content_type_field) of the GenericForeignKey class.\n        \"\"\"\n        class TestModel(models.Model):\n            content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n            object_id = models.PositiveIntegerField()\n            gfk = GenericForeignKey('content_type', 'object_id')\n\n        field = TestModel._meta.get_field('gfk')\n        self.assertEqual(field.check(), [])\n\n        # Test invalid field name ending with underscore\n        field.name = 'invalid_field_name_'\n        self.assertEqual(\n            field._check_field_name(),\n            [checks.Error('Field names must not end with an underscore.', obj=field, id='fields.E001')]\n        )\n\n        # Test invalid object_id field\n        field.name = 'gfk'\n        field.fk_field = 'nonexistent_field'\n        self.assertEqual(\n            field._check_object_id_field(),\n            [checks.Error(\n                \"The GenericForeignKey object ID references the nonexistent field 'nonexistent_field'.\",\n                obj=field, id='contenttypes.E001'\n            )]\n        )\n\n        # Test invalid content_type field (not a ForeignKey)\n        field.fk_field = 'object_id'\n        field.ct_field = 'object_id'\n        self.assertEqual(\n            field._check_content_type_field(),\n            [checks.Error(\n                \"'TestModel.object_id' is not a ForeignKey.\",\n                hint=(\n                    \"GenericForeignKeys must use a ForeignKey to \"\n                    \"'contenttypes.ContentType' as the 'content_type' field.\"\n                ),\n                obj=field, id='contenttypes.E003'\n            )]\n        )\n\n        # Test invalid content_type field (ForeignKey to wrong model)\n        field.ct_field = 'content_type'\n        TestModel._meta.get_field('content_type').remote_field.model = TestModel\n        self.assertEqual(\n            field._check_content_type_field(),\n            [checks.Error(\n                \"'TestModel.content_type' is not a ForeignKey to 'contenttypes.ContentType'.\",\n                hint=(\n                    \"GenericForeignKeys must use a ForeignKey to \"\n                    \"'contenttypes.ContentType' as the 'content_type' field.\"\n                ),\n                obj=field, id='contenttypes.E004'\n            )]\n        )\n", "    def test_generic_foreign_key_get_forward_related_filter(self):\n        class TestModel(models.Model):\n            content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n            object_id = models.PositiveIntegerField()\n            gfk = GenericForeignKey('content_type', 'object_id')\n\n        obj = TestModel.objects.create(content_type=ContentType.objects.get_for_model(User), object_id=self.alfred.pk)\n        gfk_field = obj._meta.get_field('gfk')\n        filter_kwargs = gfk_field.get_forward_related_filter(self.alfred)\n        \n        self.assertEqual(filter_kwargs, {\n            'content_type': ContentType.objects.get_for_model(User).pk,\n            'object_id': self.alfred.pk,\n        })\n", "    def test_genericforeignkey_get_filter_kwargs_for_object(self):\n        class DummyModel(models.Model):\n            content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n            object_id = models.PositiveIntegerField()\n            gfk = GenericForeignKey('content_type', 'object_id')\n\n        content_type = ContentType.objects.get_for_model(Book)\n        dummy_instance = DummyModel(content_type=content_type, object_id=self.bio_book.pk)\n        \n        expected_filter_kwargs = {\n            'content_type': content_type.pk,\n            'object_id': self.bio_book.pk,\n        }\n        \n        self.assertEqual(dummy_instance.gfk.get_filter_kwargs_for_object(dummy_instance), expected_filter_kwargs)\n", "    def test_generic_foreign_key_get_filter_kwargs_for_object(self):\n        \"\"\"\n        Ensure get_filter_kwargs_for_object returns correct filter arguments.\n        \"\"\"\n        obj = self.bio_book\n        generic_foreign_key = GenericForeignKey('author', 'pk')\n        filter_kwargs = generic_foreign_key.get_filter_kwargs_for_object(obj)\n        self.assertEqual(filter_kwargs, {\n            'pk': obj.pk,\n            'author': obj.author.pk,\n        })\n", "    def test_genericforeignkey_contribute_to_class(self):\n        \"\"\"\n        Test that GenericForeignKey properly contributes to the model class.\n        \"\"\"\n        class ModelWithGFK(models.Model):\n            content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n            object_id = models.PositiveIntegerField()\n            content_object = GenericForeignKey('content_type', 'object_id')\n\n            class Meta:\n                app_label = 'myapp'\n\n        self.assertIn('content_object', ModelWithGFK._meta.private_fields)\n        self.assertTrue(isinstance(ModelWithGFK._meta.get_field('content_object'), GenericForeignKey))\n        self.assertEqual(ModelWithGFK.content_object.ct_field, 'content_type')\n        self.assertEqual(ModelWithGFK.content_object.fk_field, 'object_id')\n"], "sample_228": ["def test_absolute_max_zero(self):\n        \"\"\"\n        If absolute_max is set to 0, no forms should be instantiated.\n        \"\"\"\n        ChoiceFormSet = formset_factory(Choice, absolute_max=0)\n        formset = ChoiceFormSet(data={\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-0-choice': 'Test Choice',\n            'choices-0-votes': '10',\n        }, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(len(formset.forms), 0)\n        self.assertEqual(formset.non_form_errors(), ['Please submit at most 0 forms.'])\n", "    def test_custom_management_form_fields(self):\n        \"\"\"\n        Custom fields in the management form (MIN_NUM_FORMS and MAX_NUM_FORMS)\n        should be correctly rendered and handled.\n        \"\"\"\n        class CustomManagementFormSet(BaseFormSet):\n            min_num = 2\n            max_num = 5\n        \n        CustomFormSet = formset_factory(FavoriteDrinkForm, formset=CustomManagementFormSet, extra=1)\n        formset = CustomFormSet(auto_id=False, prefix='drinks')\n        management_form_html = (\n            '<input type=\"hidden\" name=\"drinks-TOTAL_FORMS\" value=\"1\">'\n            '<input type=\"hidden\" name=\"drinks-INITIAL_FORMS\" value=\"0\">'\n            '<input type=\"hidden\" name=\"drinks-MIN_NUM_FORMS\" value=\"2\">'\n            '<input type=\"hidden\" name=\"drinks-MAX_NUM_FORMS\" value=\"5\">'\n        )\n        \n        self.assertHTMLEqual(\n            str(formset.management_form),\n            management_form_html\n        )\n        \n        # Now, submit data with less than the minimum number of forms\n        data = {\n            'drinks-TOTAL_FORMS': '1',\n            'drinks-INITIAL_FORMS': '0',\n            'drinks-MIN_NUM_FORMS': '2',\n            'drinks-MAX_NUM_FORMS': '5',\n            'drinks-0-name': 'Mojito',\n        }\n        formset = CustomFormSet(data, auto_id=False, prefix='drinks')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['Please submit at least 2 forms.'])\n        \n        # Now, submit data with more than the maximum number of forms\n        data['drinks-TOTAL_FORMS'] = '6'\n        for i in range(1, 6):\n            data[f'drinks-{i}-name'] = f'Drink {i}'\n        \n        formset = CustomFormSet(data, auto_id=False, prefix='drinks')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['Please submit at most 5 forms.'])\n", "def test_management_form_initialization(self):\n        \"\"\"\n        Test initialization of ManagementForm with various configurations.\n        \"\"\"\n        # Test with default initialization\n        form = ManagementForm()\n        self.assertIn(TOTAL_FORM_COUNT, form.fields)\n        self.assertIn(INITIAL_FORM_COUNT, form.fields)\n        self.assertIn(MIN_NUM_FORM_COUNT, form.fields)\n        self.assertIn(MAX_NUM_FORM_COUNT, form.fields)\n        self.assertEqual(form.fields[TOTAL_FORM_COUNT].widget.__class__, HiddenInput)\n        self.assertEqual(form.fields[INITIAL_FORM_COUNT].widget.__class__, HiddenInput)\n        self.assertEqual(form.fields[MIN_NUM_FORM_COUNT].widget.__class__, HiddenInput)\n        self.assertEqual(form.fields[MAX_NUM_FORM_COUNT].widget.__class__, HiddenInput)\n\n        # Test initialization with custom initial values\n        initial_data = {\n            TOTAL_FORM_COUNT: 5,\n            INITIAL_FORM_COUNT: 3,\n            MIN_NUM_FORM_COUNT: 1,\n            MAX_NUM_FORM_COUNT: 10,\n        }\n        form = ManagementForm(initial=initial_data)\n        self.assertEqual(form.initial[TOTAL_FORM_COUNT], 5)\n        self.assertEqual(form.initial[INITIAL_FORM_COUNT], 3)\n        self.assertEqual(form.initial[MIN_NUM_FORM_COUNT], 1)\n        self.assertEqual(form.initial[MAX_NUM_FORM_COUNT], 10)\n", "    def test_custom_management_form(self):\n        \"\"\"\n        Test if the custom ManagementForm works correctly with custom fields and widgets.\n        \"\"\"\n        class CustomManagementForm(ManagementForm):\n                super().__init__(*args, **kwargs)\n                self.base_fields['custom_field'] = CharField(widget=HiddenInput)\n\n        class CustomFormSet(BaseFormSet):\n            @cached_property\n                \"\"\"Return the CustomManagementForm instance for this FormSet.\"\"\"\n                if self.is_bound:\n                    form = CustomManagementForm(self.data, auto_id=self.auto_id, prefix=self.prefix)\n                    if not form.is_valid():\n                        raise ValidationError(\n                            _('ManagementForm data is missing or has been tampered with'),\n                            code='missing_management_form',\n                        )\n                else:\n                    form = CustomManagementForm(auto_id=self.auto_id, prefix=self.prefix, initial={\n                        TOTAL_FORM_COUNT: self.total_form_count(),\n                        INITIAL_FORM_COUNT: self.initial_form_count(),\n                        MIN_NUM_FORM_COUNT: self.min_num,\n                        MAX_NUM_FORM_COUNT: self.max_num,\n                        'custom_field': 'custom_value'\n                    })\n                return form\n\n        CustomFormSetFactory = formset_factory(Choice, formset=CustomFormSet)\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1000',\n            'choices-0-choice': 'Test Choice',\n            'choices-0-votes': '10',\n            'choices-custom_field': 'custom_value'\n        }\n        formset = CustomFormSetFactory(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.management_form.cleaned_data['custom_field'], 'custom_value')\n", "    def test_formset_custom_error_list(self):\n        \"\"\"\n        Custom error lists can be used in formsets and propagate correctly.\n        \"\"\"\n        class CustomErrorList(ErrorList):\n            pass\n\n        class CustomErrorForm(Form):\n            name = CharField()\n\n        CustomErrorFormSet = formset_factory(CustomErrorForm, error_class=CustomErrorList)\n        data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '0',\n            'form-0-name': '',\n            'form-1-name': 'Test',\n        }\n        formset = CustomErrorFormSet(data)\n        self.assertFalse(formset.is_valid())\n        self.assertIsInstance(formset.errors[0]['name'], CustomErrorList)\n        self.assertEqual(str(formset.errors[0]['name']), '<ul class=\"errorlist\"><li>This field is required.</li></ul>')\n"], "sample_229": ["    def test_intersection_with_deferred_fields(self):\n        # Create additional data.\n        ReservedName.objects.create(name='name1', order=1)\n        ReservedName.objects.create(name='name2', order=2)\n\n        # Define querysets with deferred fields.\n        qs1 = ReservedName.objects.defer('name')\n        qs2 = ReservedName.objects.defer('order')\n\n        # Perform intersection and check results.\n        result = qs1.intersection(qs2)\n        self.assertEqual(len(result), 2)\n        self.assertEqual(result[0].name, 'name1')\n        self.assertEqual(result[0].order, 1)\n        self.assertEqual(result[1].name, 'name2')\n        self.assertEqual(result[1].order, 2)\n", "    def test_union_with_ordering_by_related_model_field(self):\n        reserved_name = ReservedName.objects.create(name='test_name', order=5)\n        qs1 = Number.objects.filter(num=1)\n        qs2 = Number.objects.filter(num=2)\n        qs3 = Number.objects.filter(num=reserved_name.order)\n        # Test union with ordering by related model field.\n        union_qs = qs1.union(qs2, qs3).order_by('reservedname__order')\n        self.assertEqual(len(list(union_qs)), 3)\n        self.assertEqual(list(union_qs)[0].num, 1)\n        self.assertEqual(list(union_qs)[1].num, 2)\n        self.assertEqual(list(union_qs)[2].num, 5)\n", "    def test_combining_qs_with_different_fields(self):\n        qs1 = Number.objects.filter(num__lte=1).values('num')\n        qs2 = Number.objects.filter(num__gte=8).values('other_num')\n        combined_qs = qs1.union(qs2)\n        results = list(combined_qs)\n        self.assertEqual(len(results), 4)\n        self.assertIn({'num': 0}, results)\n        self.assertIn({'num': 1}, results)\n        self.assertIn({'other_num': 8}, results)\n        self.assertIn({'other_num': 9}, results)\n", "    def test_union_with_ordering_and_limit(self):\n        qs1 = Number.objects.filter(num__lte=1).order_by('num')\n        qs2 = Number.objects.filter(num__gte=8).order_by('-num')\n        combined_qs = qs1.union(qs2).order_by('num')[:3]\n        self.assertNumbersEqual(combined_qs, [0, 1, 8], ordered=True)\n", "def test_get_none(self):\n    qs = Number.objects.none()\n    with self.assertRaises(Number.DoesNotExist):\n        qs.get(num=1)\n"], "sample_230": ["    def test_invalid_json_structure(self):\n        field = JSONField()\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean('{\"a\": [1, 2, invalid]}')\n", "    def test_invalid_json_input_type(self):\n        field = JSONField()\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean(b'\\x80\\x03]q\\x00(K\\x01K\\x02K\\x03e.')  # non-JSON binary input\n", "    def test_custom_empty_value(self):\n        class CustomJSONField(JSONField):\n                if value in self.empty_values:\n                    return 'custom empty'\n                return super().to_python(value)\n        \n        field = CustomJSONField(required=False)\n        self.assertEqual(field.clean(''), 'custom empty')\n        self.assertEqual(field.clean(None), 'custom empty')\n", "def test_bound_data(self):\n    field = JSONField()\n    self.assertEqual(field.bound_data('{\"a\": \"b\"}', None), {'a': 'b'})\n    self.assertEqual(field.bound_data('{\"a\": \"b\"}', {'a': 'c'}), {'a': 'b'})\n    self.assertEqual(field.bound_data(None, {'a': 'c'}), {'a': 'c'})\n    self.assertEqual(field.bound_data(InvalidJSONInput('invalid json'), None), InvalidJSONInput('invalid json'))\n", "    def test_bound_data(self):\n        field = JSONField()\n        self.assertEqual(field.bound_data('{\"a\": 1}', None), {\"a\": 1})\n        self.assertEqual(field.bound_data('invalid json', None), InvalidJSONInput('invalid json'))\n        self.assertEqual(field.bound_data('{\"a\": 1}', '{\"b\": 2}'), {\"a\": 1})\n        self.assertEqual(field.bound_data('invalid json', '{\"b\": 2}'), InvalidJSONInput('invalid json'))\n"], "sample_231": ["    def test_get_post_parameters_sensitive_all(self):\n        \"\"\"Test that all POST parameters are cleansed when '__ALL__' is specified.\"\"\"\n        request = RequestFactory().post('/some_url/', {\n            'username': 'john_doe',\n            'password': 'secret',\n        })\n        request.sensitive_post_parameters = '__ALL__'\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed = reporter_filter.get_post_parameters(request)\n        self.assertEqual(cleansed['username'], reporter_filter.cleansed_substitute)\n        self.assertEqual(cleansed['password'], reporter_filter.cleansed_substitute)\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "def test_safe_exception_reporter_filter_cleanses_callable_settings(self):\n    \"\"\"\n    Callable settings should be cleansed by SafeExceptionReporterFilter.\n    \"\"\"\n    reporter_filter = SafeExceptionReporterFilter()\n    \n        return \"Sensitive data\"\n    \n    cleansed_setting = reporter_filter.cleanse_setting('API_KEY_CALLABLE', sensitive_callable)\n    self.assertIsInstance(cleansed_setting, CallableSettingWrapper)\n    self.assertEqual(repr(cleansed_setting), repr(sensitive_callable))\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.reporter_filter = SafeExceptionReporterFilter()\n        "], "sample_232": ["    def test_compile_json_path(self):\n        tests = [\n            ([], '$'),\n            (['a'], '$.\"a\"'),\n            ([1], '$[1]'),\n            (['a', 'b'], '$.\"a\".\"b\"'),\n            ([1, 'a'], '$[1].\"a\"'),\n            (['a', 1], '$.\"a\"[1]'),\n            (['a', 'b', 2, 'c'], '$.\"a\".\"b\"[2].\"c\"'),\n        ]\n        for key_transforms, expected in tests:\n            with self.subTest(key_transforms=key_transforms):\n                self.assertEqual(compile_json_path(key_transforms), expected)\n", "    def test_check_supported(self):\n        field = JSONField()\n        databases = ['default']\n        with mock.patch('django.db.models.JSONField.model', new=mock.Mock()):\n            with mock.patch('django.db.models.JSONField.model._meta.required_db_features', new=['supports_json_field']):\n                errors = field._check_supported(databases)\n                self.assertEqual(errors, [])\n            with mock.patch('django.db.models.JSONField.model._meta.required_db_features', new=[]):\n                with mock.patch('django.db.models.JSONField.model._meta.model_name', new='testmodel'):\n                    with mock.patch('django.db.models.JSONField.model._meta.app_label', new='testapp'):\n                        with mock.patch('django.db.models.JSONField.model._meta.db_table', new='test_table'):\n                            connection = mock.Mock()\n                            connection.features.supports_json_field = False\n                            connections['default'] = connection\n                            errors = field._check_supported(databases)\n                            self.assertEqual(len(errors), 1)\n                            self.assertEqual(errors[0].id, 'fields.E180')\n                            self.assertIn('does not support JSONFields.', errors[0].msg)\n", "    def test_compile_json_path(self):\n        # Test cases with expected outcomes\n        test_cases = [\n            ([], '$'),  # No transforms, include_root = True\n            (['key'], '$.\"key\"'),\n            (['key', 'subkey'], '$.\"key\".\"subkey\"'),\n            (['0'], '$[0]'),\n            (['key', '0', 'subkey'], '$.\"key\"[0].\"subkey\"'),\n            (['key_with_special_chars!@#'], '$.\"key_with_special_chars!@#\"'),\n        ]\n        for key_transforms, expected_path in test_cases:\n            with self.subTest(key_transforms=key_transforms):\n                self.assertEqual(compile_json_path(key_transforms), expected_path)\n\n        # Test cases without root\n        test_cases_no_root = [\n            ([], ''),  # No transforms, include_root = False\n            (['key'], '\"key\"'),\n            (['key', 'subkey'], '\"key\".\"subkey\"'),\n            (['0'], '[0]'),\n            (['key', '0', 'subkey'], '\"key\"[0].\"subkey\"'),\n            (['key_with_special_chars!@#'], '\"key_with_special_chars!@#\"'),\n        ]\n        for key_transforms, expected_path in test_cases_no_root:\n            with self.subTest(key_transforms=key_transforms):\n                self.assertEqual(compile_json_path(key_transforms, include_root=False), expected_path)\n", "    def test_key_transform_factory(self):\n        key_name = 'test_key'\n        factory = KeyTransformFactory(key_name)\n        self.assertIsInstance(factory, KeyTransformFactory)\n        transform = factory('value')\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, key_name)\n", "    def test_check_supported(self):\n        field = models.JSONField()\n        with mock.patch('django.db.models.fields.json.connections') as mock_connections, \\\n                mock.patch('django.db.models.fields.json.router') as mock_router:\n            mock_router.allow_migrate_model.return_value = True\n            mock_connections.__getitem__.return_value.features.supports_json_field = False\n            errors = field._check_supported(['default'])\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'fields.E180')\n            self.assertIn('does not support JSONFields.', errors[0].msg)\n"], "sample_233": ["    def test_token_with_incorrect_timestamp(self):\n        \"\"\"\n        A token generated with an incorrect timestamp should not validate.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        valid_token = p0.make_token(user)\n        ts_b36, hash_string = valid_token.split('-')\n        incorrect_ts_b36 = int_to_base36(base36_to_int(ts_b36) + 1000)  # Modify the timestamp\n        invalid_token = f\"{incorrect_ts_b36}-{hash_string}\"\n        self.assertIs(p0.check_token(user, invalid_token), False)\n", "    def test_token_with_different_password(self):\n        \"\"\"\n        Changing the user's password invalidates the token.\n        \"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'oldpassword')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        \n        user.set_password('newpassword')\n        user.save()\n        \n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_invalidation_after_password_change(self):\n        \"\"\"Changing the user's password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_changed_password(self):\n        \"\"\"Changing the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_changed_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n"], "sample_234": ["    def test_union_with_different_field_types(self):\n        ReservedName.objects.create(name='test', order=1)\n        qs1 = Number.objects.filter(num=1).values_list('num', flat=True)\n        qs2 = ReservedName.objects.filter(order=1).values_list('name', flat=True)\n        result = list(qs1.union(qs2))\n        self.assertIn(1, result)\n        self.assertIn('test', result)\n", "    def test_intersection_with_annotation(self):\n        qs1 = Number.objects.annotate(is_even=F('num') % 2).filter(is_even=0)\n        qs2 = Number.objects.annotate(is_even=F('num') % 2).filter(is_even=1)\n        qs3 = Number.objects.annotate(is_even=F('num') % 2).filter(is_even=0)\n        self.assertNumbersEqual(qs1.intersection(qs3), [0, 2, 4, 6, 8], ordered=False)\n        self.assertNumbersEqual(qs1.intersection(qs2), [], ordered=False)\n", "    def test_bulk_create_and_update(self):\n        # Test bulk_create\n        objects_to_create = [Number(num=i, other_num=10 - i) for i in range(10, 15)]\n        created_objects = Number.objects.bulk_create(objects_to_create)\n        self.assertEqual(len(created_objects), 5)\n        self.assertNumbersEqual(Number.objects.filter(num__gte=10), list(range(10, 15)), ordered=False)\n\n        # Test bulk_update\n        for obj in created_objects:\n            obj.other_num = obj.num * 2\n        Number.objects.bulk_update(created_objects, ['other_num'])\n        updated_objects = Number.objects.filter(num__gte=10)\n        for obj in updated_objects:\n            self.assertEqual(obj.other_num, obj.num * 2)\n", "    def test_update_on_combined_qs(self):\n        qs1 = Number.objects.filter(num__lte=1)\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3)\n        combined_qs = qs1.union(qs2)\n        with self.assertRaisesMessage(NotSupportedError, 'Calling QuerySet.update() after union() is not supported.'):\n            combined_qs.update(num=5)\n", "    def test_union_with_annotate(self):\n        qs1 = Number.objects.filter(num__lte=1).annotate(new_field=F('num') * 2)\n        qs2 = Number.objects.filter(num__gte=8).annotate(new_field=F('num') * 2)\n        expected_results = [\n            {'num': 0, 'new_field': 0},\n            {'num': 1, 'new_field': 2},\n            {'num': 8, 'new_field': 16},\n            {'num': 9, 'new_field': 18},\n        ]\n        self.assertQuerysetEqual(\n            qs1.union(qs2).order_by('num'),\n            expected_results,\n            transform=lambda x: {'num': x.num, 'new_field': x.new_field},\n            ordered=True\n        )\n"], "sample_235": ["def test_set_autocommit_resets_run_commit_hooks_on_set_autocommit_on(self):\n        with transaction.atomic():\n            self.do(1)\n            connection.set_autocommit(True)\n            self.assertTrue(connection.run_commit_hooks_on_set_autocommit_on)\n\n        self.assertDone([1])\n", "    def test_run_and_clear_commit_hooks_clears_hooks(self):\n        with transaction.atomic():\n            self.do(1)\n            connection.run_and_clear_commit_hooks()\n            self.assertNotified([1])\n\n        with transaction.atomic():\n            self.do(2)\n            self.assertNotified([1])  # Ensure no new hooks run prematurely\n\n        self.assertDone([1, 2])\n", "    def test_connection_timezone_properties(self):\n        with self.settings(USE_TZ=True, TIME_ZONE='UTC'):\n            self.assertEqual(connection.timezone_name, 'UTC')\n            self.assertEqual(connection.timezone.zone, 'UTC')\n        \n        with self.settings(USE_TZ=True, TIME_ZONE='Europe/London'):\n            self.assertEqual(connection.timezone_name, 'Europe/London')\n            self.assertEqual(connection.timezone.zone, 'Europe/London')\n\n        with self.settings(USE_TZ=False):\n            self.assertEqual(connection.timezone_name, settings.TIME_ZONE)\n            self.assertIsNone(connection.timezone)\n", "def test_transaction_hooks_with_multiple_savepoints(self):\n    with transaction.atomic():\n        self.do(1)\n        with transaction.atomic():\n            self.do(2)\n            with transaction.atomic():\n                self.do(3)\n            self.assertNotified([])\n        self.assertNotified([])\n    self.assertDone([1, 2, 3])\n", "    def test_set_rollback_flag(self):\n        \"\"\"Test setting the rollback flag manually within an atomic block.\"\"\"\n        with transaction.atomic():\n            self.do(1)\n            connection.set_rollback(True)\n            self.assertTrue(connection.get_rollback())\n            connection.set_rollback(False)\n            self.assertFalse(connection.get_rollback())\n\n        self.assertDone([1])\n"], "sample_236": ["def test_protect_custom_message(self):\n        custom_message = (\n            \"Deletion restricted: Cannot delete instances of model '%s' because they are \"\n            \"referenced through a protected foreign key: '%s.%s'\"\n        )\n        a = create_a('protect_custom')\n        msg = custom_message % (R.__name__, A.__name__, 'protect')\n        with self.assertRaisesMessage(ProtectedError, msg) as cm:\n            a.protect.delete()\n        self.assertEqual(cm.exception.protected_objects, {a})\n", "    def test_set_callable(self):\n        \"\"\"\n        Test the SET callable to ensure it correctly sets the field value using the callable result on delete.\n        \"\"\"\n            return \"Custom Value\"\n\n        set_callable = models.ForeignKey(\n            R, on_delete=models.SET(get_custom_value), related_name='set_callable'\n        )\n        a = create_a('set_callable')\n        a.set_callable.delete()\n        a = A.objects.get(pk=a.pk)\n        self.assertEqual(a.set_callable, get_custom_value())\n", "    def test_protected_error_initialization(self):\n        objs = [A.objects.create(name='test')]\n        msg = \"Protected objects present\"\n        error = ProtectedError(msg, objs)\n        self.assertEqual(error.protected_objects, objs)\n        self.assertEqual(str(error), msg)\n", "    def test_no_related_objects_to_collect(self):\n        \"\"\"\n        Test the scenario where there are no related objects to collect for deletion.\n        \"\"\"\n        r = R.objects.create()\n        s = S.objects.create(r=r)\n        initial_count = S.objects.count()\n        collector = Collector(using='default')\n        collector.collect(objs=[s], source=r, collect_related=False)\n        self.assertEqual(len(collector.data), 1)\n        self.assertEqual(len(collector.data[S]), 1)\n        self.assertEqual(S.objects.count(), initial_count)\n", "    def test_protected_error_initialization(self):\n        protected_objs = [A.objects.create(name='protected_obj1'), A.objects.create(name='protected_obj2')]\n        msg = \"Test protected error message\"\n        error = ProtectedError(msg, protected_objs)\n        self.assertEqual(error.protected_objects, protected_objs)\n        self.assertEqual(str(error), msg)\n"], "sample_237": ["    def test_builtin_permission_name_max_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'valid_name'\n\n        with self.settings(\n            AUTH_USER_MODEL='auth_tests.Checked',\n            DEFAULT_PERMISSION_NAME_MAX_LENGTH=10,\n            DEFAULT_PERMISSION_CODENAME_MAX_LENGTH=20,\n        ):\n            errors = checks.run_checks(self.apps.get_app_configs())\n            self.assertEqual(errors, [])\n", "    def test_builtin_permission_name_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'short name'\n        \n        # Mock the _get_builtin_permissions function to return a permission name longer than 255 characters\n        with self.settings(\n            INSTALLED_APPS=['django.contrib.auth'],\n            AUTHENTICATION_BACKENDS=['django.contrib.auth.backends.ModelBackend']\n        ):\n            original_function = apps.get_model('auth', 'Permission')._meta.get_field('name').max_length\n            \n            try:\n                apps.get_model('auth', 'Permission')._meta.get_field('name').max_length = 10\n                errors = checks.run_checks(self.apps.get_app_configs())\n                self.assertEqual(errors, [\n                    checks.Error(\n                        \"The verbose_name of model 'auth_tests.Checked' must be at most 10 \"\n                        \"characters for its builtin permission names to be at most 10 characters.\",\n                        obj=Checked,\n                        id='auth.E007',\n                    ),\n                ])\n            finally:\n                apps.get_model('auth', 'Permission')._meta.get_field('name').max_length = original_function\n", "    def test_builtin_permission_name_max_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'a' * 256\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 255 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_custom_permission_clashing_builtin(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Custom add permission clashing with builtin')\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin permission \"\n                \"for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n", "    def test_custom_permission_name_clashes_with_default(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Custom add permission'),\n                    ('change_checked', 'Custom change permission'),\n                    ('delete_checked', 'Custom delete permission'),\n                    ('view_checked', 'Custom view permission'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n            checks.Error(\n                \"The permission codenamed 'change_checked' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n            checks.Error(\n                \"The permission codenamed 'delete_checked' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n            checks.Error(\n                \"The permission codenamed 'view_checked' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n"], "sample_238": ["    def test_custom_transforms(self):\n        \"\"\"\n        Test the custom mathematical transforms on the Book model.\n        \"\"\"\n        # Abs Transform\n        books = Book.objects.annotate(abs_rating=Abs(F('rating')))\n        for book in books:\n            self.assertEqual(book.abs_rating, abs(book.rating))\n\n        # ACos Transform\n        books = Book.objects.filter(rating__lte=1).annotate(acos_rating=ACos(F('rating')))\n        for book in books:\n            self.assertEqual(book.acos_rating, math.acos(book.rating))\n\n        # ASin Transform\n        books = Book.objects.filter(rating__lte=1).annotate(asin_rating=ASin(F('rating')))\n        for book in books:\n            self.assertEqual(book.asin_rating, math.asin(book.rating))\n\n        # ATan Transform\n        books = Book.objects.annotate(atan_price=ATan(F('price')))\n        for book in books:\n            self.assertEqual(book.atan_price, math.atan(book.price))\n\n        # ATan2 Transform\n        books = Book.objects.annotate(atan2_price=ATan2(F('price'), Value(1)))\n        for book in books:\n            self.assertEqual(book.atan2_price, math.atan2(book.price, 1))\n\n        # Ceil Transform\n        books = Book.objects.annotate(ceil_price=Ceil(F('price')))\n        for book in books:\n            self.assertEqual(book.ceil_price, math.ceil(book.price))\n\n        # Cos Transform\n        books = Book.objects.annotate(cos_price=Cos(F('price')))\n        for book in books:\n            self.assertEqual(book.cos_price, math.cos(book.price))\n\n        # Cot Transform\n        books = Book.objects.annotate(cot_price=Cot(F('price')))\n        for book in books:\n            self.assertEqual(book.cot_price, 1 / math.tan(book.price))\n\n        # Degrees Transform\n        books = Book.objects.annotate(degrees_price=Degrees(F('price')))\n        for book in books:\n            self.assertEqual(book.degrees_price, math.degrees(book.price))\n\n        # Exp Transform\n        books = Book.objects.annotate(exp_price=Exp(F('price')))\n        for book in books:\n            self.assertEqual(book.exp_price, math.exp(book.price))\n\n        # Floor Transform\n        books = Book.objects.annotate(floor_price=Floor(F('price')))\n        for book in books:\n            self.assertEqual(book.floor_price, math.floor(book.price))\n\n", "    def test_math_functions(self):\n        # Test the ABS function\n        author = Author.objects.create(name='Test Author', age=-42)\n        result = Author.objects.annotate(abs_age=Abs(F('age'))).get(pk=author.pk)\n        self.assertEqual(result.abs_age, 42)\n\n        # Test the ACOS function\n        author = Author.objects.create(name='Test Author', age=1)\n        result = Author.objects.annotate(acos_age=ACos(F('age'))).get(pk=author.pk)\n        self.assertAlmostEqual(result.acos_age, math.acos(1), places=5)\n\n        # Test the ASIN function\n        result = Author.objects.annotate(asin_age=ASin(F('age'))).get(pk=author.pk)\n        self.assertAlmostEqual(result.asin_age, math.asin(1), places=5)\n\n        # Test the ATAN function\n        author = Author.objects.create(name='Test Author', age=1)\n        result = Author.objects.annotate(atan_age=ATan(F('age'))).get(pk=author.pk)\n        self.assertAlmostEqual(result.atan_age, math.atan(1), places=5)\n\n        # Test the ATAN2 function\n        result = Author.objects.annotate(atan2_age=ATan2(F('age'), Value(1))).get(pk=author.pk)\n        self.assertAlmostEqual(result.atan2_age, math.atan2(1, 1), places=5)\n\n        # Test the CEIL function\n        author = Author.objects.create(name='Test Author', age=-42.7)\n        result = Author.objects.annotate(ceil_age=Ceil(F('age'))).get(pk=author.pk)\n        self.assertEqual(result.ceil_age, -42)\n\n        # Test the COS function\n        author = Author.objects.create(name='Test Author', age=0)\n        result = Author.objects.annotate(cos_age=Cos(F('age'))).get(pk=author.pk)\n        self.assertAlmostEqual(result.cos_age, math.cos(0), places=5)\n\n        # Test the COT function\n        author = Author.objects.create(name='Test Author', age=1)\n        result = Author.objects.annotate(cot_age=Cot(F('age'))).get(pk=author.pk)\n        self.assertAlmostEqual(result.cot_age, 1 / math.tan(1), places=5)\n\n        # Test the DEGREES function\n       ", "    def test_trigonometric_functions(self):\n        # Ensure trigonometric functions can be used in queries and return expected results\n        from django.db.models import Func\n\n        class TrigFunc(Func):\n            function = 'SIN'\n            template = '%(function)s(%(expressions)s)'\n\n        # SIN function\n        author_sin_age = Author.objects.annotate(sin_age=TrigFunc(F('age'))).values('name', 'sin_age')\n        for author in author_sin_age:\n            self.assertAlmostEqual(author['sin_age'], math.sin(Author.objects.get(name=author['name']).age))\n\n        # COS function\n        class CosFunc(Func):\n            function = 'COS'\n            template = '%(function)s(%(expressions)s)'\n\n        author_cos_age = Author.objects.annotate(cos_age=CosFunc(F('age'))).values('name', 'cos_age')\n        for author in author_cos_age:\n            self.assertAlmostEqual(author['cos_age'], math.cos(Author.objects.get(name=author['name']).age))\n\n        # TAN function\n        class TanFunc(Func):\n            function = 'TAN'\n            template = '%(function)s(%(expressions)s)'\n\n        author_tan_age = Author.objects.annotate(tan_age=TanFunc(F('age'))).values('name', 'tan_age')\n        for author in author_tan_age:\n            self.assertAlmostEqual(author['tan_age'], math.tan(Author.objects.get(name=author['name']).age))\n", "    def test_log_function(self):\n        with self.assertNumQueries(1) as ctx:\n            Book.objects.annotate(log_price=Log(Value(10), F('price'))).get(pk=self.b1.pk)\n        sql = ctx.captured_queries[0]['sql']\n        self.assertIn('LOG', sql)\n\n        book = Book.objects.annotate(log_price=Log(Value(10), F('price'))).get(pk=self.b1.pk)\n        self.assertAlmostEqual(book.log_price, math.log10(float(self.b1.price)), places=5)\n\n        # Check for SQLite specific behavior.\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT sqlite_version()')\n            sqlite_version = cursor.fetchone()[0]\n        if sqlite_version < '5.0.0':\n            with self.assertNumQueries(1) as ctx:\n                Book.objects.annotate(log_price=Log(Value(10), F('price'))).get(pk=self.b1.pk)\n            sql = ctx.captured_queries[0]['sql']\n            self.assertIn('LOG', sql)\n        else:\n            with self.assertNumQueries(1) as ctx:\n                Book.objects.annotate(log_price=Log(Value(10), F('price'))).get(pk=self.b1.pk)\n            sql = ctx.captured_queries[0]['sql']\n            self.assertIn('LOG', sql)\n", "    def test_transform_functions(self):\n        with CaptureQueriesContext(connection) as ctx:\n            Author.objects.annotate(\n                abs_age=Abs('age'),\n                acos_age=ACos('age'),\n                asin_age=ASin('age'),\n                atan_age=ATan('age'),\n                atan2_age=ATan2('age', 'age'),\n                ceil_age=Ceil('age'),\n                cos_age=Cos('age'),\n                cot_age=Cot('age'),\n                degrees_age=Degrees('age'),\n                exp_age=Exp('age'),\n                floor_age=Floor('age'),\n                ln_age=Ln('age'),\n                log_age=Log('age', 'age'),\n                mod_age=Mod('age', Value(10)),\n                pi_value=Pi(),\n                power_age=Power('age', 2),\n                radians_age=Radians('age'),\n                random_value=Random(),\n                round_age=Round('age'),\n                sign_age=Sign('age'),\n                sin_age=Sin('age'),\n                sqrt_age=Sqrt('age'),\n                tan_age=Tan('age')\n            ).first()\n\n        sql = ctx.captured_queries[0]['sql']\n        self.assertIn('ABS(', sql)\n        self.assertIn('ACOS(', sql)\n        self.assertIn('ASIN(', sql)\n        self.assertIn('ATAN(', sql)\n        self.assertIn('ATAN2(', sql)\n        self.assertIn('CEILING(', sql)\n        self.assertIn('COS(', sql)\n        self.assertIn('COT(', sql)\n        self.assertIn('DEGREES(', sql)\n        self.assertIn('EXP(', sql)\n        self.assertIn('FLOOR(', sql)\n        self.assertIn('LN(', sql)\n        self.assertIn('LOG(', sql)\n        self.assertIn('MOD(', sql)\n        self.assertIn('PI(', sql)\n        self.assertIn('POWER(', sql)\n        self.assertIn('RADIANS(', sql)\n        self.assertIn('RANDOM(', sql)\n        self.assertIn('ROUND(', sql)\n        self.assertIn('SIGN(', sql)\n        self.assertIn('SIN(', sql)\n        self.assertIn('SQRT(', sql)\n        self.assertIn('TAN(', sql)\n"], "sample_239": ["    def test_custom_kwarg_formset_with_initial_data(self):\n        \"\"\"\n        Test CustomKwargForm formset with initial data and custom_kwarg.\n        \"\"\"\n        initial_data = [{'custom_kwarg': 'initial_value'}]\n        FormSet = formset_factory(CustomKwargForm, extra=1)\n        formset = FormSet(initial=initial_data, form_kwargs={'custom_kwarg': 'custom_value'})\n        self.assertTrue(hasattr(formset.forms[0], 'custom_kwarg'))\n        self.assertEqual(formset.forms[0].custom_kwarg, 'custom_value')\n        self.assertTrue(hasattr(formset.forms[1], 'custom_kwarg'))\n        self.assertEqual(formset.forms[1].custom_kwarg, 'custom_value')\n", "    def test_formset_with_custom_management_form(self):\n        \"\"\"\n        A custom management form can be used with a formset.\n        \"\"\"\n        class CustomManagementForm(ManagementForm):\n            custom_field = CharField()\n\n        class CustomManagementFormSet(BaseFormSet):\n            @property\n                \"\"\"Return the CustomManagementForm instance for this FormSet.\"\"\"\n                if self.is_bound:\n                    form = CustomManagementForm(self.data, auto_id=self.auto_id, prefix=self.prefix)\n                    if not form.is_valid():\n                        raise ValidationError(\n                            _(\n                                'ManagementForm data is missing or has been tampered '\n                                'with. Missing fields: %(field_names)s'\n                            ) % {\n                                'field_names': ', '.join(\n                                    form.add_prefix(field_name) for field_name in form.errors\n                                ),\n                            },\n                            code='missing_management_form',\n                        )\n                else:\n                    form = CustomManagementForm(auto_id=self.auto_id, prefix=self.prefix, initial={\n                        TOTAL_FORM_COUNT: self.total_form_count(),\n                        INITIAL_FORM_COUNT: self.initial_form_count(),\n                        MIN_NUM_FORM_COUNT: self.min_num,\n                        MAX_NUM_FORM_COUNT: self.max_num,\n                        'custom_field': 'custom_value'\n                    })\n                return form\n\n        CustomFormSet = formset_factory(Choice, formset=CustomManagementFormSet)\n        formset = CustomFormSet(data={\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1000',\n            'choices-0-choice': 'Calexico',\n            'choices-0-votes': '100',\n            'choices-custom_field': 'custom_value'\n        }, auto_id=False, prefix='choices')\n\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.management_form.cleaned_data['custom_field'], 'custom_value')\n", "    def test_management_form_validation(self):\n        \"\"\"Test validation of the ManagementForm within a formset.\"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1000',\n            'choices-0-choice': 'Calexico',\n            'choices-0-votes': '100',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n\n        # Tamper with the management form data\n        data['choices-TOTAL_FORMS'] = '2'\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        with self.assertRaises(ValidationError):\n            formset.management_form.is_valid()\n", "    def test_management_form_initial_values(self):\n        \"\"\"\n        ManagementForm should be populated with initial values if formset is not bound.\n        \"\"\"\n        formset = ChoiceFormSet()\n        management_form = formset.management_form\n        self.assertEqual(management_form.initial[TOTAL_FORM_COUNT], 1)\n        self.assertEqual(management_form.initial[INITIAL_FORM_COUNT], 0)\n        self.assertEqual(management_form.initial[MIN_NUM_FORM_COUNT], 0)\n        self.assertEqual(management_form.initial[MAX_NUM_FORM_COUNT], 1000)\n", "    def test_formset_with_min_num_and_extra(self):\n        \"\"\"\n        Formsets with min_num and extra ensure that a minimum number of forms\n        are displayed, with extra forms being empty.\n        \"\"\"\n        ChoiceFormSet = formset_factory(Choice, min_num=2, extra=2)\n        formset = ChoiceFormSet(auto_id=False, prefix='choices')\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\"></li>"], "sample_240": ["    def test_token_invalid_after_password_change(self):\n        \"\"\"Changing the user's password invalidates the token.\"\"\"\n        user = User.objects.create_user('passwordchangeuser', 'test5@example.com', 'initialpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        # Change the user's password\n        user.set_password('newpassword')\n        user.save()\n        # Token should now be invalid\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_changed_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_after_password_change(self):\n        \"\"\"Changing the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepwuser', 'test5@example.com', 'oldpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        # Change the user's password\n        user.set_password('newpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_changed_password(self):\n        \"\"\"Updating the user's password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'oldpassword')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_different_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n"], "sample_241": ["    def setUpTestData(cls):\n        cls.raw_query = RawQuery(\"SELECT * FROM company WHERE name = %s\", using='default', params=[\"Example Inc.\"])\n", "    def setUp(self):\n        self.raw_query = RawQuery(\"SELECT * FROM test_table WHERE id = %s\", using=\"default\", params=(1,))\n", "    def test_raw_query_initialization(self):\n        query = RawQuery(\"SELECT * FROM my_table WHERE id = %s\", \"default\", params=(1,))\n        self.assertEqual(query.sql, \"SELECT * FROM my_table WHERE id = %s\")\n        self.assertEqual(query.using, \"default\")\n        self.assertEqual(query.params, (1,))\n        self.assertEqual(query.low_mark, 0)\n        self.assertIsNone(query.high_mark)\n        self.assertEqual(query.extra_select, {})\n        self.assertEqual(query.annotation_select, {})\n", "    def setUp(self):\n        self.query = RawQuery(\"SELECT * FROM test_table WHERE id = %s\", \"default\", params=(1,))\n", "    def setUp(self):\n        self.sql = \"SELECT * FROM test_table WHERE id = %s\"\n        self.using = DEFAULT_DB_ALIAS\n        self.params = (1,)\n        self.raw_query = RawQuery(self.sql, self.using, self.params)\n"], "sample_242": ["    def test_as_sql(self, mock_process_rhs, mock_process_lhs):\n        lookup = BuiltinLookup(Value(1), Value(2))\n        mock_compiler = mock.Mock()\n        mock_connection = mock.Mock()\n        mock_connection.operators = {lookup.lookup_name: '='}\n\n        sql, params = lookup.as_sql(mock_compiler, mock_connection)\n\n        self.assertEqual(sql, 'lhs_sql = rhs_sql')\n        self.assertEqual(params, ['lhs_param', 'rhs_param'])\n        mock_process_lhs.assert_called_once_with(mock_compiler, mock_connection)\n        mock_process_rhs.assert_called_once_with(mock_compiler, mock_connection)\n", "    def test_process_lhs(self):\n        from django.db import connection\n        from django.db.models import IntegerField\n        compiler = mock.Mock()\n        field = IntegerField()\n        lhs_value = Value(1, output_field=field)\n        lookup = BuiltinLookup(lhs=lhs_value, rhs=2)\n        lhs_sql, params = lookup.process_lhs(compiler, connection)\n        self.assertIn('%s', lhs_sql)\n        self.assertEqual(params, [1])\n", "    def test_get_rhs_op(self):\n        class TestConnection:\n            operators = {'exact': '='}\n\n        lookup = BuiltinLookup(Value(1), Value(2))\n        connection = TestConnection()\n        rhs = '%s'\n        self.assertEqual(lookup.get_rhs_op(connection, rhs), rhs)\n", "    def test_builtin_lookup_as_sql(self):\n        lookup = BuiltinLookup(Value(1), Value(2))\n        compiler = mock.Mock()\n        connection = mock.Mock()\n        connection.ops.field_cast_sql = lambda db_type, field_internal_type: '%s'\n        connection.ops.lookup_cast = lambda lookup_name, field_internal_type: '%s'\n        connection.operators = {'exact': '='}\n        \n        sql, params = lookup.as_sql(compiler, connection)\n        \n        self.assertEqual(sql, '%s = %s')\n        self.assertEqual(params, [1, 2])\n", "    def test_get_prep_lookup(self):\n        # Test with a direct value\n        lookup = Lookup(Value(1), 2)\n        self.assertEqual(lookup.rhs, 2)\n\n        # Test with a value that has a 'resolve_expression' method\n        mock_rhs = mock.Mock()\n        mock_rhs.resolve_expression.return_value = 'resolved'\n        lookup = Lookup(Value(1), mock_rhs)\n        self.assertEqual(lookup.rhs, 'resolved')\n"], "sample_243": ["def test_subquery(self):\n    subquery = Query(Author)\n    subquery.add_filter(Q(name='foo'))\n\n    query = Query(Item)\n    where = query.build_where(Q(creator__in=subquery))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.rhs, Query)\n    self.assertEqual(str(lookup.rhs), str(subquery))\n\n    query_str, params = query.sql_with_params()\n    self.assertIn('SELECT', query_str)\n    self.assertIn('FROM', query_str)\n    self.assertIn('WHERE', query_str)\n", "def test_rawquery_get_columns(self):\n    raw_query = RawQuery(\"SELECT id, name FROM mytable\", \"default\")\n    raw_query.cursor = connections['default'].cursor()\n    raw_query.cursor.description = [(\"id\",), (\"name\",)]\n    columns = raw_query.get_columns()\n    self.assertEqual(columns, [\"id\", \"name\"])\n", "def test_clone_query_with_extra(self):\n        query = Query(Item)\n        query.add_extra(select={'extra_field': '1'}, select_params=(), where=['note IS NOT NULL'], params=(), tables=(), order_by=['extra_field'])\n        clone = query.clone()\n        self.assertEqual(query.extra, clone.extra)\n        self.assertEqual(query.extra_order_by, clone.extra_order_by)\n        self.assertEqual(query.where.children, clone.where.children)\n", "    def test_clone_with_alias_refcount(self):\n        query = Query(Author)\n        query.alias_refcount = {'a1': 2, 'a2': 1}\n        clone = query.clone()\n        self.assertEqual(clone.alias_refcount, {'a1': 2, 'a2': 1})\n", "def test_output_field_property(self):\n    query = Query(Author)\n    query.add_fields(['name'])\n    self.assertEqual(query.output_field, Author._meta.get_field('name'))\n\n    query = Query(Author)\n    query.add_annotation(Count('id'), 'num_authors')\n    self.assertEqual(query.output_field, query.annotations['num_authors'].output_field)\n\n    query = Query(Author)\n    query.add_fields(['name'])\n    query.add_annotation(Count('id'), 'num_authors')\n    self.assertIsNone(query.output_field)\n"], "sample_244": ["def test_management_form_clean(self):\n    \"\"\"ManagementForm.clean() should set default values for TOTAL_FORM_COUNT and INITIAL_FORM_COUNT.\"\"\"\n    form = ManagementForm(data={})\n    form.full_clean()\n    self.assertEqual(form.cleaned_data[TOTAL_FORM_COUNT], 0)\n    self.assertEqual(form.cleaned_data[INITIAL_FORM_COUNT], 0)\n", "def test_management_form_clean(self):\n    \"\"\"\n    Test that the ManagementForm's clean() method correctly sets default values\n    for TOTAL_FORM_COUNT and INITIAL_FORM_COUNT if they are missing.\n    \"\"\"\n    formset = self.make_choiceformset([('Calexico', '100')], total_forms=1, initial_forms=1)\n    self.assertTrue(formset.management_form.is_valid())\n    self.assertEqual(formset.management_form.cleaned_data[TOTAL_FORM_COUNT], 1)\n    self.assertEqual(formset.management_form.cleaned_data[INITIAL_FORM_COUNT], 1)\n    \n    # Simulate missing TOTAL_FORM_COUNT and INITIAL_FORM_COUNT in data\n    data = {\n        'choices-INITIAL_FORMS': '1',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    formset.management_form.full_clean()\n    self.assertEqual(formset.management_form.cleaned_data[TOTAL_FORM_COUNT], 0)\n    self.assertEqual(formset.management_form.cleaned_data[INITIAL_FORM_COUNT], 0)\n", "    def test_management_form_missing_fields(self):\n        \"\"\"\n        If management form is missing required fields, formset should raise error.\n        \"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            # 'choices-MIN_NUM_FORMS' is missing\n            # 'choices-MAX_NUM_FORMS' is missing\n            'choices-0-choice': 'Calexico',\n            'choices-0-votes': '100',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.non_form_errors(),\n            [\n                'ManagementForm data is missing or has been tampered with. '\n                'Missing fields: choices-MIN_NUM_FORMS, choices-MAX_NUM_FORMS. '\n                'You may need to file a bug report if the issue persists.'\n            ],\n        )\n", "    def test_formset_with_custom_ordering_widget(self):\n        \"\"\"\n        Test that a formset can use a custom ordering widget.\n        \"\"\"\n        class CustomOrderingForm(Form):\n            field = IntegerField()\n\n        class CustomOrderingFormSet(BaseFormSet):\n                return HiddenInput(attrs={'class': 'custom-ordering'})\n\n        CustomFormSet = formset_factory(CustomOrderingForm, formset=CustomOrderingFormSet, can_order=True)\n        formset = CustomFormSet(auto_id=False)\n        self.assertHTMLEqual(\n            '\\n'.join(form.as_ul() for form in formset.forms),\n            \"\"\"<li>Field: <input type=\"number\" name=\"form-0-field\"></li>", "    def test_management_form_cleaned_data_default(self):\n        \"\"\"\n        Ensure that the cleaned_data of the management form includes default \n        values for TOTAL_FORM_COUNT and INITIAL_FORM_COUNT when not bound.\n        \"\"\"\n        # Create an unbound management form\n        formset = ChoiceFormSet()\n        management_form = formset.management_form\n\n        # Ensure the cleaned_data includes default values\n        management_form.full_clean()\n        self.assertEqual(management_form.cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(management_form.cleaned_data[INITIAL_FORM_COUNT], 0)\n"], "sample_245": ["    def test_preprocess_djangojs(self):\n        build_file = BuildFile(MakeMessagesCommand(), 'djangojs', TranslatableFile('test_dir', 'test.js', 'locale'))\n        build_file.preprocess()\n        expected_file_path = 'test_dir/test.js.c'\n        self.assertTrue(os.path.exists(expected_file_path))\n        with open(expected_file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n            self.assertIn('prepared JS content', content)\n        os.remove(expected_file_path)\n", "    def test_is_templatized_djangojs(self):\n        \"\"\"\n        BuildFile should correctly identify if a file needs to be templatized\n        for djangojs domain.\n        \"\"\"\n        mock_command = mock.MagicMock()\n        mock_command.gettext_version = (0, 18, 2)\n        translatable = MakeMessagesCommand().translatable_file_class('.', 'script.js', NO_LOCALE_DIR)\n        build_file = MakeMessagesCommand().build_file_class(mock_command, 'djangojs', translatable)\n        self.assertTrue(build_file.is_templatized)\n", "    def test_no_default_ignore(self):\n        \"\"\"\n        Test the --no-default-ignore option, ensuring the default ignore patterns\n        are not used when specified.\n        \"\"\"\n        out, po_contents = self._run_makemessages(no_default_ignore=True)\n        self.assertNotIn(\"ignoring directory\", out)\n        self.assertNotIn(\"ignoring file\", out)\n        # Ensure files that would normally be ignored are now included\n        self.assertMsgId('This should be included even if it matches default ignore patterns.', po_contents)\n", "    def setUp(self):\n        super().setUp()\n        self.translatable_file = self.translatable_file_class('.', 'test.py', './locale')\n        self.build_file = self.build_file_class(Command(), 'django', self.translatable_file)\n", "    def setUp(self):\n        super().setUp()\n        self.build_file = self.build_file_class(\n            command=MakeMessagesCommand(),\n            domain='django',\n            translatable=self.translatable_file_class('.', 'template.html', NO_LOCALE_DIR)\n        )\n        self.build_file.preprocess()\n"], "sample_246": ["    def setUp(self):\n        super().setUp()\n        self.command = MakeMessagesCommand()\n        self.translatable = TranslatableFile(os.path.join(self.test_dir, 'templates'), 'test.html', 'locale')\n        self.build_file = BuildFile(self.command, 'django', self.translatable)\n", "    def setUp(self):\n        self.command = MakeMessagesCommand()\n        self.translatable_file = TranslatableFile(\"dirpath\", \"filename.html\", \"locale_dir\")\n        self.build_file = BuildFile(self.command, \"django\", self.translatable_file)\n", "    def test_locale_regex(self):\n        \"\"\"\n        Verify that the regular expression used for detecting locale directories is correct.\n        \"\"\"\n        cmd = MakeMessagesCommand()\n        looks_like_locale = re.compile(r'[a-z]{2}')\n\n        # Valid locale directories\n        self.assertTrue(looks_like_locale.match('en'))\n        self.assertTrue(looks_like_locale.match('de'))\n        self.assertTrue(looks_like_locale.match('fr'))\n\n        # Invalid locale directories\n        self.assertFalse(looks_like_locale.match('_invalid'))\n        self.assertFalse(looks_like_locale.match('.hidden'))\n        self.assertFalse(looks_like_locale.match('123'))\n        self.assertFalse(looks_like_locale.match('locale'))\n", "    def setUp(self):\n        super().setUp()\n        self.locale_dir = os.path.join(self.test_dir, 'locale')\n        self.addCleanup(shutil.rmtree, self.locale_dir, ignore_errors=True)\n    ", "    def setUp(self):\n        self.command = MakeMessagesCommand()\n        self.translatable = TranslatableFile('some_dir', 'some_file.py', 'some_locale_dir')\n        self.build_file = BuildFile(self.command, 'django', self.translatable)\n"], "sample_247": ["def test_rawquery_get_columns(self):\n    raw_query = RawQuery(\"SELECT id, name FROM annotations_author\", using=\"default\")\n    columns = raw_query.get_columns()\n    self.assertEqual(columns, [\"id\", \"name\"])\n", "def test_clone_query(self):\n        books = Book.objects.annotate(is_book=Value(1))\n        cloned_books = books.query.clone()\n        self.assertEqual(str(books.query), str(cloned_books))\n        self.assertEqual(len(books), len(list(cloned_books.get_compiler(connection=connection).execute_sql())))\n", "    def test_raw_query_execution(self):\n        query = RawQuery(\"SELECT id FROM annotations_author WHERE age > %s\", 'default', params=(40,))\n        results = list(query)\n        expected_ids = list(Author.objects.filter(age__gt=40).values_list('id', flat=True))\n        self.assertEqual([row[0] for row in results], expected_ids)\n", "    def test_raw_query_execution(self):\n        raw_query = RawQuery(\"SELECT id, name FROM annotations_author WHERE age > %s\", \"default\", params=(30,))\n        results = list(raw_query)\n        self.assertGreater(len(results), 0)\n        for result in results:\n            self.assertIn('id', result)\n            self.assertIn('name', result)\n            ", "def test_rawquery_clone(self):\n    raw_query = RawQuery(\"SELECT * FROM annotations_book WHERE rating > %s\", using='default', params=(4,))\n    cloned_query = raw_query.clone(using='default')\n    self.assertEqual(raw_query.sql, cloned_query.sql)\n    self.assertEqual(raw_query.params, cloned_query.params)\n    self.assertEqual(raw_query.using, cloned_query.using)\n"], "sample_248": ["    def test_shell_with_python_fallback(self, select):\n        select.return_value = ([], [], [])\n        with mock.patch('code.interact') as mock_interact:\n            call_command('shell')\n            mock_interact.assert_called_once()\n", "    def test_shell_with_ipython_installed(self, select):\n        select.return_value = ([], [], [])\n        with captured_stdout() as stdout:\n            call_command('shell', interface='ipython')\n        self.assertIn('IPython', sys.modules)\n", "    def test_plain_python_shell(self, mock_interact):\n        call_command('shell', interface='python')\n        self.assertTrue(mock_interact.called)\n", "    def test_default_to_python_shell_when_others_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with mock.patch('code.interact') as mock_code_interact:\n            call_command('shell')\n            mock_code_interact.assert_called_once()\n", "    def test_shell_with_python_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n            call_command('shell', interface='python')\n"], "sample_249": ["    def test_create_test_db(self, mock_call_command):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = BaseDatabaseCreation(test_connection)\n        with mock.patch.object(creation, '_create_test_db'):\n            with mock.patch.object(creation, 'log'):\n                test_db_name = creation.create_test_db(verbosity=2, autoclobber=True, serialize=True, keepdb=False)\n                self.assertTrue(test_db_name.startswith(TEST_DATABASE_PREFIX))\n                mock_call_command.assert_any_call(\n                    'migrate',\n                    verbosity=1,\n                    interactive=False,\n                    database=test_connection.alias,\n                    run_syncdb=True,\n                )\n                mock_call_command.assert_any_call('createcachetable', database=test_connection.alias)\n                self.assertTrue(hasattr(test_connection, '_test_serialized_contents'))\n", "    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        \n        with mock.patch.object(creation, '_clone_test_db') as mocked_clone_test_db:\n            creation.clone_test_db(suffix='clone', verbosity=1, autoclobber=True, keepdb=True)\n            mocked_clone_test_db.assert_called_with('clone', 1, True)\n", "    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch.object(creation, '_clone_test_db') as mocked_clone_db:\n            suffix = '001'\n            creation.clone_test_db(suffix, verbosity=0, autoclobber=True, keepdb=False)\n            mocked_clone_db.assert_called_once_with(suffix, verbosity=0, keepdb=False)\n", "    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        suffix = 'clone'\n        try:\n            with mock.patch.object(creation, '_clone_test_db') as mock_clone_test_db:\n                creation.clone_test_db(suffix, verbosity=0, autoclobber=True, keepdb=True)\n                mock_clone_test_db.assert_called_once_with(suffix, verbosity=0, keepdb=True)\n                clone_settings = creation.get_test_db_clone_settings(suffix)\n                self.assertEqual(clone_settings['NAME'], f\"{old_database_name}_{suffix}\")\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_get_test_db_name_with_custom_test_name(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'original_db'\n        test_connection.settings_dict['TEST'] = {'NAME': 'custom_test_db'}\n        creation = BaseDatabaseCreation(test_connection)\n        self.assertEqual(creation._get_test_db_name(), 'custom_test_db')\n"], "sample_250": ["    def test_month_formats(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        alt_month_name = datetime(1979, 2, 8, 22, 00)  # February\n\n        self.assertEqual(dateformat.format(my_birthday, 'F'), 'July')\n        self.assertEqual(dateformat.format(alt_month_name, 'E'), 'Febr.')  # Alternative month name\n        self.assertEqual(dateformat.format(my_birthday, 'M'), 'Jul')\n        self.assertEqual(dateformat.format(my_birthday, 'b'), 'jul')\n        self.assertEqual(dateformat.format(my_birthday, 'N'), 'July')\n", "    def test_iso_week_year(self):\n        test_dates = [\n            (datetime(2023, 1, 1), '2022'),  # Jan 1, 2023 falls in the last week of 2022 in ISO calendar\n            (datetime(2023, 1, 2), '2023'),  # Jan 2, 2023 falls in the first week of 2023\n            (datetime(2023, 12, 31), '2023'),  # Dec 31, 2023 falls in the last week of 2023\n        ]\n        for test_date, expected in test_dates:\n            with self.subTest(test_date=test_date):\n                self.assertEqual(dateformat.format(test_date, 'o'), expected)\n", "    def test_weekday_abbreviation(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        self.assertEqual(dateformat.format(my_birthday, 'D'), 'Sun')\n", "    def test_ordinal_suffix(self):\n        days_with_suffixes = {\n            1: 'st', 2: 'nd', 3: 'rd', 4: 'th', 11: 'th', 12: 'th', 13: 'th', 21: 'st', 22: 'nd', 23: 'rd', 31: 'st'\n        }\n        for day, expected_suffix in days_with_suffixes.items():\n            with self.subTest(day=day):\n                dt = datetime(2023, 10, day)\n                self.assertEqual(dateformat.format(dt, 'S'), expected_suffix)\n", "    def test_alternative_month_names(self):\n        # Testing the 'E' format for alternative month names\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        self.assertEqual(dateformat.format(my_birthday, 'E'), 'July')\n"], "sample_251": ["def test_combined_expression_with_decimal_annotation(self):\n        book = Book.objects.annotate(\n            combined=ExpressionWrapper(Value(10) + F('price'), output_field=DecimalField())\n        ).get(isbn=self.b1.isbn)\n        combined_value = Decimal('40.00')\n        self.assertEqual(book.combined, combined_value)\n", "def test_combined_expression_with_custom_func(self):\n    class Multiply(Func):\n        function = 'MULTIPLY'\n        template = '%(function)s(%(expressions)s)'\n\n    Book.objects.create(\n        isbn='123456789', name='New Book',\n        pages=200, rating=4.0, price=Decimal('20.00'), contact=self.a1, publisher=self.p1,\n        pubdate=datetime.date(2022, 1, 1)\n    )\n\n    book = Book.objects.annotate(\n        combined=ExpressionWrapper(\n            Multiply(F('price'), F('rating')),\n            output_field=DecimalField()\n        )\n    ).get(isbn='123456789')\n    \n    expected_combined_value = book.price * book.rating\n    self.assertEqual(book.combined, expected_combined_value)\n", "    def test_combined_expression_nested_functions(self):\n        book = Book.objects.annotate(\n            combined=ExpressionWrapper(Func(F('price') + Value(10), function='ROUND', output_field=IntegerField()), output_field=IntegerField())\n        ).get(isbn='159059725')\n        combined = round(book.price + 10)\n        self.assertEqual(book.combined, combined)\n", "def test_annotation_with_combined_expression(self):\n    books = Book.objects.annotate(\n        combined_price=ExpressionWrapper(F('price') + Value(10), output_field=DecimalField())\n    )\n    for book in books:\n        self.assertEqual(book.combined_price, book.price + Decimal(10))\n", "def test_combined_expression_with_multiplication_division(self):\n    test = self.b1\n    b = Book.objects.annotate(\n        combined=ExpressionWrapper((F('pages') * F('price')) / F('rating'), output_field=DecimalField())\n    ).get(isbn=test.isbn)\n    combined = Decimal(test.pages * test.price / test.rating)\n    self.assertEqual(b.combined, combined)\n"], "sample_252": ["    def test_default_initialization(self):\n        field = models.JSONField()\n        self.assertIsNone(field.encoder)\n        self.assertIsNone(field.decoder)\n", "    def test_compile_json_path(self):\n        test_cases = [\n            (['a', 'b', 1], True, '$.\"a\".\"b\"[1]'),\n            (['key', '0', 'subkey'], False, '.\"key\".\"0\".\"subkey\"'),\n            ([3, 'name'], True, '$.[3].\"name\"'),\n            ([], True, '$'),\n            ([], False, ''),\n        ]\n        for key_transforms, include_root, expected in test_cases:\n            with self.subTest(key_transforms=key_transforms, include_root=include_root):\n                self.assertEqual(compile_json_path(key_transforms, include_root), expected)\n", "    def test_custom_json_encoder(self):\n        class CustomEncoder(json.JSONEncoder):\n                if isinstance(obj, uuid.UUID):\n                    return str(obj)\n                return super().default(obj)\n\n        field = models.JSONField(encoder=CustomEncoder)\n        value = {'uuid': uuid.UUID('d85e2076-b67c-4ee7-8c3a-2bf5a2cc2475')}\n        json_value = field.get_prep_value(value)\n        expected_json = json.dumps(value, cls=CustomEncoder)\n        self.assertEqual(json_value, expected_json)\n", "    def test_key_transform_lt(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__c__lt=15),\n            [self.objs[3], self.objs[4]]\n        )\n", "    def test_compile_json_path(self):\n        tests = [\n            ([], True, '$'),\n            ([], False, ''),\n            (['a'], True, '$.\"a\"'),\n            (['a', 'b', 1], True, '$.\"a\".\"b\"[1]'),\n            (['a', 2, 'b'], False, '.\"a\"[2].\"b\"'),\n            (['key with spaces', 3], True, '$.\"key with spaces\"[3]'),\n        ]\n        for key_transforms, include_root, expected in tests:\n            with self.subTest(key_transforms=key_transforms, include_root=include_root):\n                self.assertEqual(compile_json_path(key_transforms, include_root), expected)\n"], "sample_253": ["    def test_echo_on(self, mocked_tcsetattr, mocked_tcgetattr, mocked_isatty):\n        mocked_tcgetattr.return_value = [0, 0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_tcsetattr.call_count, 1)\n        self.assertTrue(mocked_tcgetattr.called)\n        ", "    def test_ensure_echo_on(self, mock_stdin, mock_termios):\n        # Mock termios attributes\n        mock_termios.ECHO = 8\n        mock_termios.TCSANOW = 0\n        # Mock stdin attributes\n        mock_stdin.isatty.return_value = True\n        attr_list = [0, 1, 2, 0]\n        mock_termios.tcgetattr.return_value = attr_list\n\n        autoreload.ensure_echo_on()\n\n        # Ensure termios.tcsetattr was called to enable echo\n        attr_list[3] |= mock_termios.ECHO\n        mock_termios.tcsetattr.assert_called_once_with(mock_stdin, mock_termios.TCSANOW, attr_list)\n", "    def test_ensure_echo_on_when_disabled(self, mocked_tcsetattr, mocked_tcgetattr, mocked_isatty):\n        attrs = [0, 0, 0, 0]\n        mocked_tcgetattr.return_value = attrs\n        with mock.patch('signal.signal') as mock_signal:\n            mock_signal.return_value = signal.SIG_IGN\n            attrs[3] &= ~termios.ECHO\n            autoreload.ensure_echo_on()\n            attrs[3] |= termios.ECHO\n            self.assertTrue(mocked_tcsetattr.called)\n            mock_signal.assert_called_with(signal.SIGTTOU, signal.SIG_IGN)\n", "    def test_ensure_echo_on_with_tty(self, mocked_stdin, mocked_termios):\n        # Mock a tty stdin\n        mocked_stdin.isatty.return_value = True\n        attrs = [0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attrs\n        attrs[3] = attrs[3] & ~mocked_termios.ECHO  # Ensure ECHO is off\n        autoreload.ensure_echo_on()\n        # Ensure ECHO is on\n        self.assertTrue(attrs[3] & mocked_termios.ECHO)\n", "    def test_ensure_echo_on_when_disabled(self, mocked_isatty, mocked_termios):\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        mocked_termios.ECHO = 8\n        mocked_termios.SIGTTOU = signal.SIGTTOU\n\n        autoreload.ensure_echo_on()\n        \n        mocked_termios.tcgetattr.assert_called_once_with(sys.stdin)\n        mocked_termios.tcsetattr.assert_called_once()\n        self.assertTrue(mocked_termios.tcsetattr.call_args[0][2][3] & mocked_termios.ECHO)\n"], "sample_254": ["    def test_inline_foreignkey_widget(self):\n        \"\"\"\n        Ensure that the proper widget is used for ForeignKey fields in inlines.\n        \"\"\"\n        parent = Parent.objects.create(name='Parent1')\n        child = Child.objects.create(name='Child1', parent=parent)\n        response = self.client.get(reverse('admin:admin_inlines_parent_change', args=(parent.pk,)))\n        # Ensure that the widget for the ForeignKey field is rendered correctly.\n        self.assertContains(response, '<select name=\"child_set-0-parent\"')\n        self.assertContains(response, f'<option value=\"{parent.pk}\" selected>{parent}</option>')\n", "    def setUpTestData(cls):\n        super().setUpTestData()\n        cls.holder = Holder.objects.create(dummy=13)\n        cls.inner = Inner.objects.create(dummy=42, holder=cls.holder)\n", "    def test_get_queryset_uses_ordering(self):\n        \"\"\"\n        Test that get_queryset uses ordering specified in ModelAdmin.\n        \"\"\"\n        class TestModelAdmin(ModelAdmin):\n            ordering = ['name']\n        \n        modeladmin = TestModelAdmin(Author, admin_site)\n        request = self.factory.get(reverse('admin:admin_inlines_author_changelist'))\n        request.user = self.superuser\n        queryset = modeladmin.get_queryset(request)\n        self.assertEqual(list(queryset.query.order_by), ['name'])\n\n        class TestModelAdmin2(ModelAdmin):\n            ordering = ['-name']\n        \n        modeladmin = TestModelAdmin2(Author, admin_site)\n        queryset = modeladmin.get_queryset(request)\n        self.assertEqual(list(queryset.query.order_by), ['-name'])\n\n        class TestModelAdmin3(ModelAdmin):\n            ordering = None\n        \n        modeladmin = TestModelAdmin3(Author, admin_site)\n        queryset = modeladmin.get_queryset(request)\n        self.assertEqual(list(queryset.query.order_by), [])\n", "    def test_formfield_overrides(self):\n        \"\"\"\n        Test that formfield overrides in ModelAdmin work correctly.\n        \"\"\"\n        class CustomAdmin(ModelAdmin):\n            formfield_overrides = {\n                models.CharField: {'widget': forms.Textarea(attrs={'cols': 80, 'rows': 20})},\n                models.IntegerField: {'widget': forms.TextInput(attrs={'size': '20'})},\n            }\n\n        ma = CustomAdmin(Author, admin_site)\n        request = self.factory.get(reverse('admin:admin_inlines_author_add'))\n        request.user = self.superuser\n\n        form = ma.get_form(request)()\n        self.assertIsInstance(form.fields['name'].widget, forms.Textarea)\n        self.assertEqual(form.fields['name'].widget.attrs['cols'], 80)\n        self.assertEqual(form.fields['name'].widget.attrs['rows'], 20)\n        self.assertIsInstance(form.fields['age'].widget, forms.TextInput)\n        self.assertEqual(form.fields['age'].widget.attrs['size'], '20')\n", "    def test_formfield_for_dbfield_foreignkey(self):\n        \"\"\"\n        Test formfield_for_dbfield method with ForeignKey fields.\n        \"\"\"\n        class ForeignKeyModel(models.Model):\n            name = models.CharField(max_length=50)\n\n        class TestModel(models.Model):\n            foreign_key = models.ForeignKey(ForeignKeyModel, on_delete=models.CASCADE)\n\n        class TestModelAdmin(ModelAdmin):\n            formfield_overrides = {\n                models.ForeignKey: {'widget': forms.TextInput(attrs={'size': '10'})}\n            }\n\n        foreign_key_model = ForeignKeyModel.objects.create(name='Foreign Key Object')\n        request = self.factory.get('/')\n        request.user = self.superuser\n        model_admin = TestModelAdmin(TestModel, admin_site)\n        formfield = model_admin.formfield_for_dbfield(TestModel._meta.get_field('foreign_key'), request)\n        self.assertIsInstance(formfield.widget, forms.TextInput)\n        self.assertEqual(formfield.widget.attrs['size'], '10')\n"], "sample_256": ["    def test_unicode_ci_compare_case_insensitive(self):\n        self.assertTrue(_unicode_ci_compare('test', 'TEST'))\n        self.assertTrue(_unicode_ci_compare('Stra\u00dfe', 'strasse'))\n        self.assertFalse(_unicode_ci_compare('test', 'tesTing'))\n        self.assertFalse(_unicode_ci_compare('Stra\u00dfe', 'strassen'))\n", "    def test_unicode_ci_compare_equal(self):\n        self.assertTrue(_unicode_ci_compare('TeSt', 'tEsT'))\n        self.assertTrue(_unicode_ci_compare('Stra\u00dfe', 'strasse'))\n        self.assertTrue(_unicode_ci_compare('caf\u00e9', 'CAFE'))\n", "    def test_unicode_ci_compare_equal_strings(self):\n        self.assertTrue(_unicode_ci_compare('test', 'TEST'))\n        self.assertTrue(_unicode_ci_compare('tEsT', 'TeSt'))\n        self.assertTrue(_unicode_ci_compare('\u00df', 'ss'))\n", "    def test_unicode_ci_compare_equal(self):\n        self.assertTrue(_unicode_ci_compare('test', 'TEST'))\n        self.assertTrue(_unicode_ci_compare('Stra\u00dfe', 'strasse'))\n        self.assertTrue(_unicode_ci_compare('\u03a3', '\u03c3'))\n", "    def test_unicode_ci_compare(self):\n        # Identical strings with different cases.\n        self.assertTrue(_unicode_ci_compare(\"Test\", \"test\"))\n        # Identical strings with different unicode normalization forms.\n        self.assertTrue(_unicode_ci_compare(\"\u00c5\", \"A\u030a\"))\n        # Different strings.\n        self.assertFalse(_unicode_ci_compare(\"Test\", \"Best\"))\n        # Identical unicode strings.\n        self.assertTrue(_unicode_ci_compare(\"Stra\u00dfe\", \"strasse\"))\n        # Identical strings with special characters.\n        self.assertTrue(_unicode_ci_compare(\"Hello\u00a9\", \"hello\u00a9\"))\n"], "sample_255": ["    def test_handle_one_request(self):\n        \"\"\"Test WSGIRequestHandler.handle_one_request method.\"\"\"\n            \"\"\"A WSGI app that returns a basic response.\"\"\"\n            start_response('200 OK', [('Content-Type', 'text/plain')])\n            return [b'Hello World']\n\n        rfile = BytesIO()\n        rfile.write(b\"GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n\")\n        rfile.seek(0)\n\n        wfile = BytesIO()\n\n            if mode == 'rb':\n                return rfile\n            elif mode == 'wb':\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        handler = WSGIRequestHandler(request, '192.168.0.2', server)\n        \n        # Prevent logging from appearing in test output.\n        with self.assertLogs('django.server', 'INFO'):\n            handler.handle_one_request()\n\n        wfile.seek(0)\n        response = wfile.read()\n\n        self.assertIn(b'HTTP/1.1 200 OK', response)\n        self.assertIn(b'Hello World', response)\n", "    def test_handle_error_non_broken_pipe(self):\n        \"\"\"WSGIServer handles non-broken pipe errors.\"\"\"\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        client_address = ('192.168.2.0', 8080)\n        class CustomException(Exception):\n            pass\n        \n        with self.assertRaises(CustomException):\n            try:\n                server = WSGIServer(('localhost', 0), WSGIRequestHandler)\n                try:\n                    raise CustomException()\n                except Exception:\n                    with captured_stderr() as err:\n                        server.handle_error(request, client_address)\n            finally:\n                server.server_close()\n", "    def test_handle_one_request(self):\n        \"\"\"WSGIRequestHandler.handle_one_request reads and parses the request correctly.\"\"\"\n            start_response('200 OK', [('Content-Type', 'text/plain')])\n            return [b'Hello, World!']\n\n        rfile = BytesIO()\n        rfile.write(b\"GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n\")\n        rfile.seek(0)\n\n        wfile = BytesIO()\n\n            if mode == 'rb':\n                return rfile\n            elif mode == 'wb':\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        handler = WSGIRequestHandler(request, '192.168.0.2', server)\n        handler.handle_one_request()\n\n        wfile.seek(0)\n        response = wfile.getvalue().split(b'\\r\\n')\n\n        self.assertIn(b'HTTP/1.1 200 OK', response)\n        self.assertIn(b'Hello, World!', response)\n", "    def test_handle_one_request_with_long_request_line(self):\n        \"\"\"WSGIRequestHandler properly handles a request line that is too long.\"\"\"\n        rfile = BytesIO()\n        long_request_line = b\"GET \" + b\"/\" * 65537 + b\" HTTP/1.1\\r\\n\"\n        rfile.write(long_request_line)\n        rfile.seek(0)\n        \n        wfile = BytesIO()\n            if mode == 'rb':\n                return rfile\n            elif mode == 'wb':\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: None)\n\n        handler = WSGIRequestHandler(request, '192.168.0.2', server)\n        handler.handle_one_request()\n\n        wfile.seek(0)\n        response = wfile.read()\n        \n        self.assertIn(b\"HTTP/1.1 414 Request-URI Too Long\", response)\n", "    def test_handle_one_request(self):\n        \"\"\"WSGIRequestHandler correctly handles one request and closes the connection.\"\"\"\n            start_response('200 OK', [('Content-Type', 'text/plain')])\n            return [b'Hello, World!']\n\n        rfile = BytesIO()\n        rfile.write(b\"GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n\")\n        rfile.seek(0)\n\n        wfile = BytesIO()\n\n            if mode == 'rb':\n                return rfile\n            elif mode == 'wb':\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        handler = WSGIRequestHandler(request, '192.168.0.2', server)\n        handler.handle()\n\n        wfile.seek(0)\n        response = wfile.read()\n\n        self.assertIn(b'HTTP/1.1 200 OK', response)\n        self.assertIn(b'Hello, World!', response)\n        self.assertTrue(handler.close_connection)\n"], "sample_257": ["    def test_compile_json_path(self):\n        tests = [\n            (['key1', 'key2', 'key3'], '$.key1.key2.key3'),\n            (['0', '1', '2'], '$[0][1][2]'),\n            (['key1', '1', 'key3', '2'], '$.key1[1].key3[2]'),\n            ([], '$'),\n            (['single_key'], '$.single_key'),\n        ]\n        for key_transforms, expected in tests:\n            with self.subTest(key_transforms=key_transforms):\n                self.assertEqual(compile_json_path(key_transforms), expected)\n", "    def test_compile_json_path_with_root(self):\n        self.assertEqual(compile_json_path(['a', 'b', 1]), '$.\"a\".\"b\"[1]')\n", "    def test_compile_json_path_empty(self):\n        self.assertEqual(compile_json_path([]), '$')\n", "    def test_key_transform_factory(self):\n        key_transform_factory = KeyTransformFactory('test_key')\n        transform = key_transform_factory('lhs')\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, 'test_key')\n        self.assertEqual(transform.lhs, 'lhs')\n", "    def test_key_transform_gt(self):\n        field = models.JSONField()\n        key_transform = KeyTransform('key_name', field)\n        lookup = KeyTransformGt(key_transform, 10)\n        self.assertIsInstance(lookup, KeyTransformGt)\n        "], "sample_258": ["    def test_connect_with_strong_reference(self):\n        receiver_1 = Callable()\n        a_signal.connect(receiver_1, weak=False)\n        result = a_signal.send(sender=self, val='test')\n        self.assertEqual(result, [(receiver_1, 'test')])\n        self.assertEqual(len(a_signal.receivers), 1)\n        a_signal.disconnect(receiver_1)\n        self.assertTestIsClean(a_signal)\n", "def test_disconnect_non_existent_receiver(self):\n    receiver_1 = Callable()\n    receiver_2 = Callable()\n    a_signal.connect(receiver_1)\n    disconnected_non_existent = a_signal.disconnect(receiver_2)\n    self.assertFalse(disconnected_non_existent)\n    self.assertTrue(a_signal.has_listeners())\n    a_signal.disconnect(receiver_1)\n    self.assertTestIsClean(a_signal)\n", "    def test_connect_with_dispatch_uid(self):\n            return val\n\n            return val\n\n        dispatch_uid = \"unique_id\"\n        a_signal.connect(receiver_1, dispatch_uid=dispatch_uid)\n        a_signal.connect(receiver_2, dispatch_uid=dispatch_uid)\n\n        # Ensure only the first receiver connected with the same dispatch_uid is used\n        result = a_signal.send(sender=self, val='test')\n        self.assertEqual(result, [(receiver_1, 'test')])\n\n        # Ensure the second receiver is not connected\n        a_signal.disconnect(receiver_2, dispatch_uid=dispatch_uid)\n        result = a_signal.send(sender=self, val='test')\n        self.assertEqual(result, [(receiver_1, 'test')])\n\n        # Cleanup\n        a_signal.disconnect(receiver_1, dispatch_uid=dispatch_uid)\n        self.assertTestIsClean(a_signal)\n", "    def test_disconnect_by_dispatch_uid(self):\n            pass\n\n            pass\n\n        a_signal.connect(receiver_1, dispatch_uid=\"uid1\")\n        a_signal.connect(receiver_2, dispatch_uid=\"uid2\")\n        \n        self.assertTrue(a_signal.disconnect(dispatch_uid=\"uid1\"))\n        self.assertFalse(any(uid == \"uid1\" for uid, _ in a_signal.receivers))\n        \n        self.assertTrue(a_signal.disconnect(dispatch_uid=\"uid2\"))\n        self.assertFalse(any(uid == \"uid2\" for uid, _ in a_signal.receivers))\n        \n        self.assertTestIsClean(a_signal)\n", "    def test_disconnect_with_dispatch_uid(self):\n            pass\n\n            pass\n\n        a_signal.connect(uid_based_receiver_1, dispatch_uid=\"uid_1\")\n        a_signal.connect(uid_based_receiver_2, dispatch_uid=\"uid_2\")\n        self.assertEqual(len(a_signal.receivers), 2)\n        a_signal.disconnect(dispatch_uid=\"uid_1\")\n        self.assertEqual(len(a_signal.receivers), 1)\n        a_signal.disconnect(dispatch_uid=\"uid_2\")\n        self.assertTestIsClean(a_signal)\n"], "sample_259": ["def test_queryset_values(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects(\n            [book1],\n            Prefetch('authors', queryset=Author.objects.values('id', 'name'))\n        )\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book1.authors.values('id', 'name'), [{'id': self.author1.id, 'name': self.author1.name}, {'id': self.author2.id, 'name': self.author2.name}, {'id': self.author3.id, 'name': self.author3.name}])\n", "    def test_bulk_create(self):\n        new_books = [\n            Book(title='New Book 1'),\n            Book(title='New Book 2'),\n            Book(title='New Book 3'),\n        ]\n        created_books = Book.objects.bulk_create(new_books)\n        self.assertEqual(len(created_books), 3)\n        self.assertTrue(Book.objects.filter(title='New Book 1').exists())\n        self.assertTrue(Book.objects.filter(title='New Book 2').exists())\n        self.assertTrue(Book.objects.filter(title='New Book 3').exists())\n", "def test_select_related_foreignkey(self):\n        authors = Author.objects.select_related('first_book').all()\n        with self.assertNumQueries(1):\n            for author in authors:\n                self.assertIsNotNone(author.first_book)\n\n        with self.assertNumQueries(0):\n            [author.first_book for author in authors]\n", "def test_get_or_create(self):\n    defaults = {'first_book': self.book2}\n    author, created = Author.objects.get_or_create(name='Charlotte', defaults=defaults)\n    self.assertFalse(created)\n    self.assertEqual(author.first_book, self.book1)\n\n    author, created = Author.objects.get_or_create(name='New Author', defaults=defaults)\n    self.assertTrue(created)\n    self.assertEqual(author.first_book, self.book2)\n", "def test_related_populators(self):\n    book = Book.objects.create(title='Test Book')\n    author = Author.objects.create(name='Test Author', first_book=book)\n    book.authors.add(author)\n\n    queryset = Book.objects.select_related('first_book').prefetch_related('authors')\n    results = list(queryset)\n\n    self.assertEqual(results[0].title, 'Test Book')\n    self.assertEqual(results[0].first_book, book)\n    self.assertEqual(results[0].authors.first(), author)\n"], "sample_260": ["    def test_create_model_alter_unique_together(self):\n        \"\"\"\n        AlterUniqueTogether should optimize into CreateModel.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n                migrations.AlterUniqueTogether(\"Foo\", [[\"name\"]]),\n            ],\n            [\n                migrations.CreateModel(\"Foo\", [\n                    (\"name\", models.CharField(max_length=255)),\n                ], options={\"unique_together\": {(\"name\",)}}),\n            ],\n        )\n", "    def test_create_model_add_constraint(self):\n        \"\"\"\n        AddConstraint should optimize into CreateModel.\n        \"\"\"\n        constraint = models.UniqueConstraint(fields=['name'], name='unique_name')\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={'verbose_name': 'Foo'},\n                ),\n                migrations.AddConstraint(\"Foo\", constraint),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                    options={\n                        'verbose_name': 'Foo',\n                        'constraints': [constraint]\n                    },\n                ),\n            ],\n        )\n", "    def test_create_model_add_index(self):\n        \"\"\"\n        AddIndex should optimize into CreateModel.\n        \"\"\"\n        index = models.Index(fields=['name'], name='name_idx')\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={'verbose_name': 'Foo'},\n                ),\n                migrations.AddIndex(\"Foo\", index),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={'verbose_name': 'Foo', 'indexes': [index]},\n                ),\n            ],\n        )\n", "    def test_create_model_alter_model_managers(self):\n        \"\"\"\n        AlterModelManagers should optimize into CreateModel.\n        \"\"\"\n        managers1 = [('objects', EmptyManager()), ('manager1', EmptyManager())]\n        managers2 = [('objects', EmptyManager()), ('manager2', EmptyManager())]\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                    managers=managers1,\n                ),\n                migrations.AlterModelManagers(name='Foo', managers=managers2),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                    managers=managers2,\n                ),\n            ],\n        )\n", "    def test_create_model_add_constraint(self):\n        \"\"\"\n        AddConstraint should optimize into CreateModel.\n        \"\"\"\n        constraint = models.UniqueConstraint(fields=['name'], name='unique_name')\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                ),\n                migrations.AddConstraint(\"Foo\", constraint),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\n                        'verbose_name': 'Foo',\n                        'constraints': [constraint],\n                    },\n                    bases=(UnicodeModel,),\n                ),\n            ],\n        )\n"], "sample_261": ["def test_parse_duration_invalid_formats(self):\n    # Invalid formats\n    invalid_values = [\n        '10:15:30:40',  # Too many colons\n        '1 days 10:15:30',  # Incorrect day format\n        '10:15:30.abcdef',  # Invalid fractional seconds\n        '10:15:30,-5',  # Invalid negative fractional seconds\n        'P4Y2M',  # ISO 8601 invalid year-month combination\n        'P4W2D',  # ISO 8601 invalid week-day combination\n        'PT5H5M5S5',  # Extra digit in ISO 8601\n        '1:60:60',  # Invalid hour, minute, and second\n        '1::60',  # Missing minute\n        '10.5:30',  # Invalid hour with decimal\n    ]\n    for value in invalid_values:\n        with self.subTest(value=value):\n            self.assertIsNone(parse_duration(value))\n", "    def test_invalid_parse_duration(self):\n        invalid_inputs = [\n            '',  # Empty string\n            'random string',  # Completely invalid string\n            '1:60',  # Invalid minute value\n            '24:00:00',  # Invalid hour value\n            '1 25:00:00',  # Invalid hour value with days\n            '1:1:1:1',  # Too many colons\n            'P-1D',  # Invalid ISO 8601 representation\n            'P4Y',  # Unsupported ISO 8601 representation\n            'P4W',  # Unsupported ISO 8601 representation\n            'PT-5S',  # Invalid negative ISO 8601 representation without leading dash\n            '4 days 25:00:00',  # Invalid time in PostgreSQL format\n            '4 days 10:60:00',  # Invalid minute in PostgreSQL format\n            '4 days 10:10:60',  # Invalid second in PostgreSQL format\n            '4 days -10:10:10',  # Invalid negative hour in PostgreSQL format\n        ]\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_parse_invalid_duration(self):\n        invalid_values = [\n            'not a duration',    # Completely invalid string\n            'P4Y5M',             # Unsupported combination of years and months\n            'P4W5D',             # Unsupported weeks and days combination\n            'PT5H5M',            # Incorrect format for hours and minutes\n            '4 days 25:00:00',   # Invalid time (25 hours)\n            '4 days -25:00:00',  # Invalid negative time\n            '4 25:00:00',        # Invalid time without \"days\"\n            'P4DT25H',           # Invalid ISO format (25 hours)\n            'P-4D',              # Invalid ISO negative format\n        ]\n        for value in invalid_values:\n            with self.subTest(value=value):\n                self.assertIsNone(parse_duration(value))\n", "    def test_parse_time_invalid_fractional_seconds(self):\n        # Invalid fractional seconds\n        invalid_inputs = [\n            '10:20:30.4000000',  # too many digits in fractional seconds\n            '10:20:30,4000000',  # too many digits in fractional seconds with comma\n            '10:20:30.40a',      # non-numeric character in fractional seconds\n            '10:20:30,40a',      # non-numeric character in fractional seconds with comma\n        ]\n        for invalid_input in invalid_inputs:\n            with self.subTest(invalid_input=invalid_input):\n                self.assertIsNone(parse_time(invalid_input))\n", "    def test_parse_invalid_duration(self):\n        # Invalid duration formats\n        invalid_durations = [\n            'random text',\n            '10 hours',\n            'P1Y2M3DT4H5M',  # Year and Month not supported\n            'P1DT',  # Missing time part after 'T'\n            'PT5H4M',  # Missing minutes separator\n            '5:30.0000001',  # More than 6 microseconds\n            '15:30:',  # Trailing colon\n            '4 day 15:30',  # Incorrect day format\n            'P4YT5H',  # Mixed year and time\n        ]\n        for invalid in invalid_durations:\n            with self.subTest(invalid=invalid):\n                self.assertIsNone(parse_duration(invalid))\n"], "sample_262": ["    def test_lazy_pickle_support(self):\n        import pickle\n\n        lazy_int = lazy(lambda: 10, int)\n        lazy_instance = lazy_int()\n\n        # Pickle the lazy instance\n        pickled_lazy_instance = pickle.dumps(lazy_instance)\n\n        # Unpickle the lazy instance\n        unpickled_lazy_instance = pickle.loads(pickled_lazy_instance)\n\n        # Ensure the unpickled instance works as expected\n        self.assertEqual(unpickled_lazy_instance, 10)\n", "    def test_lazy_deepcopy(self):\n        \"\"\"\n        Ensure that deepcopy works with lazy proxy objects.\n        \"\"\"\n        class Klazz:\n                self.value = value\n\n                return self.value == other.value\n\n        lazy_obj = lazy(lambda: Klazz(10), Klazz)()\n        deepcopy_lazy_obj = copy.deepcopy(lazy_obj)\n        self.assertIsNot(deepcopy_lazy_obj, lazy_obj)\n        self.assertEqual(deepcopy_lazy_obj, lazy_obj)\n", "    def test_lazy_mod_operator(self):\n        \"\"\"\n        Test the behavior of the % (modulus) operator on lazy objects.\n        \"\"\"\n        lazy_text = lazy(lambda: \"Hello %s\", str)\n        self.assertEqual(lazy_text() % \"World\", \"Hello World\")\n\n        lazy_number = lazy(lambda: 10, int)\n        self.assertEqual(lazy_number() % 3, 1)\n\n        lazy_number_text = lazy(lambda: \"Number: %d\", str)\n        self.assertEqual(lazy_number_text() % 42, \"Number: 42\")\n", "    def test_lazy_deepcopy(self):\n        \"\"\"Ensure that lazy objects can be deep copied correctly.\"\"\"\n        lazy_obj = lazy(lambda: [1, 2, 3], list)\n        copied_obj = copy.deepcopy(lazy_obj())\n        self.assertEqual(copied_obj, [1, 2, 3])\n        self.assertIsNot(copied_obj, lazy_obj())\n", "    def test_lazy_mod(self):\n        lazy_str = lazy(lambda: \"test\", str)\n        self.assertEqual(lazy_str() % 'ing', 'testing')\n\n        lazy_int = lazy(lambda: 10, int)\n        self.assertEqual(lazy_int() % 3, 1)\n"], "sample_263": ["    def test_dumpdata_with_invalid_format(self):\n        \"\"\"\n        A CommandError should be raised if an invalid serialization format is specified.\n        \"\"\"\n        management.call_command('loaddata', 'fixture1.json', verbosity=0)\n        with self.assertRaisesMessage(CommandError, \"Unknown serialization format: invalid_format\"):\n            self._dumpdata_assert(\n                ['fixtures'],\n                '',\n                format='invalid_format'\n            )\n", "    def test_handle_unknown_format(self):\n        \"\"\"\n        Test that handle raises a CommandError for an unknown serialization format.\n        \"\"\"\n        with self.assertRaisesMessage(CommandError, \"Unknown serialization format: unknown_format\"):\n            management.call_command('dumpdata', 'fixtures', format='unknown_format', verbosity=0)\n", "    def test_compressed_loading_invalid_extension(self):\n        # Load fixture with an unsupported compressed file extension\n        msg = \"Unsupported file extension (.tar). Fixtures saved in 'fixture_invalid_extension.json'.\"\n        with self.assertWarnsMessage(RuntimeWarning, msg):\n            management.call_command('loaddata', 'fixture_invalid_extension.tar', verbosity=0)\n        self.assertEqual(\n            Article.objects.get().headline,\n            'Expected headline from invalid extension fixture',\n        )\n", "    def test_dumpdata_invalid_format(self):\n        \"\"\"\n        Test that an error is raised if an unknown serialization format is specified.\n        \"\"\"\n        management.call_command('loaddata', 'fixture1.json', verbosity=0)\n        with self.assertRaisesMessage(CommandError, \"Unknown serialization format: invalid_format\"):\n            self._dumpdata_assert(\n                ['fixtures'],\n                '',\n                format='invalid_format'\n            )\n", "    def test_invalid_format_error(self):\n        \"\"\"\n        Test that an unknown serialization format raises a CommandError.\n        \"\"\"\n        with self.assertRaisesMessage(CommandError, \"Unknown serialization format: invalid_format\"):\n            management.call_command('dumpdata', 'fixtures', format='invalid_format')\n"], "sample_264": ["    def test_empty_messages(self):\n        \"\"\"\n        An empty list of messages should be handled correctly by the storage.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Store an empty list of messages.\n        messages = []\n        unstored_messages = storage._store(messages, response)\n\n        # Ensure no messages are stored.\n        cookie_storing = self.stored_messages_count(storage, response)\n        self.assertEqual(cookie_storing, 0)\n        self.assertEqual(len(unstored_messages), 0)\n\n        # Check the cookie is properly deleted.\n        self.assertEqual(response.cookies['messages'].value, '')\n        self.assertEqual(response.cookies['messages']['max-age'], 0)\n    ", "    def test_empty_message_storage(self):\n        \"\"\"\n        Ensure that storing an empty list of messages sets the cookie to an empty value.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        \n        # Add no messages to storage\n        storage.update(response)\n        \n        # Check that the cookie is set to an empty value\n        self.assertEqual(response.cookies['messages'].value, '')\n        self.assertEqual(response.cookies['messages']['domain'], '.example.com')\n        self.assertEqual(response.cookies['messages']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(response.cookies['messages']['samesite'], settings.SESSION_COOKIE_SAMESITE)\n", "    def test_empty_message_list(self):\n        \"\"\"\n        When an empty message list is encoded and decoded, it should be \n        handled gracefully and return an empty list.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Encode and decode an empty message list.\n        empty_messages = []\n        encoded = storage._encode(empty_messages)\n        decoded = storage._decode(encoded)\n\n        self.assertEqual(decoded, empty_messages)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "    def test_empty_message_storage(self):\n        \"\"\"\n        Test that no messages are stored when an empty list is provided.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        # Store an empty list of messages\n        unstored_messages = storage._store([], response)\n        cookie_storing = self.stored_messages_count(storage, response)\n        \n        # Ensure no messages are stored and unstored_messages is empty\n        self.assertEqual(cookie_storing, 0)\n        self.assertEqual(len(unstored_messages), 0)\n        self.assertEqual(response.cookies['messages'].value, '')\n", "    def test_empty_message_handling(self):\n        \"\"\"\n        Test handling of empty messages to ensure they are properly encoded/decoded.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add an empty message\n        empty_message = \"\"\n        storage.add(constants.INFO, empty_message)\n\n        # Encode and decode the message\n        encoded = storage._encode([Message(constants.INFO, empty_message)])\n        decoded = storage._decode(encoded)\n\n        # Ensure the decoded message matches the original empty message\n        self.assertEqual(len(decoded), 1)\n        self.assertEqual(decoded[0].message, empty_message)\n\n        # Update the response and check the cookie\n        storage.update(response)\n        self.assertEqual(self.stored_messages_count(storage, response), 1)\n"], "sample_265": ["    def test_template_does_not_exist(self):\n        \"\"\"\n        Ensure that TemplateDoesNotExist is raised with the correct\n        template name when a template cannot be found.\n        \"\"\"\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n\n        with self.assertRaises(TemplateDoesNotExist) as cm:\n            engine.get_template('nonexistent.html')\n        \n        self.assertEqual(cm.exception.args[0], 'nonexistent.html')\n", "    def test_get_template_template_does_not_exist(self):\n        \"\"\"\n        Test that get_template raises TemplateDoesNotExist when the template is not found.\n        \"\"\"\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.get_template('non_existent_template.html')\n", "    def test_template_does_not_exist(self):\n        \"\"\"Test that attempting to load a non-existent template raises TemplateDoesNotExist.\"\"\"\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.get_template('non_existent_template.html')\n", "    def test_get_template_non_existent(self):\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.get_template('non_existent_template.html')\n", "    def test_get_template_template_does_not_exist(self):\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.get_template('non_existent_template.html')\n"], "sample_266": ["    def test_detect_conflicts(self):\n        \"\"\"\n        Test detection of conflicting leaf migrations in the graph.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        # Simulate a conflict by adding a new migration directly to the graph\n        migration_loader.graph.add_node(('migrations', '0003_conflict'), object())\n        conflicts = migration_loader.detect_conflicts()\n        self.assertIn('migrations', conflicts)\n        self.assertIn('0003_conflict', conflicts['migrations'])\n        self.assertIn('0002_second', conflicts['migrations'])\n", "    def test_missing_migration_class(self):\n        \"\"\"\n        Tests that a missing Migration class in a migration file raises a BadMigrationError.\n        \"\"\"\n        with self.assertRaisesMessage(BadMigrationError, \"Migration 0001_initial in app migrations has no Migration class\"):\n            MigrationLoader(connection)\n", "    def test_check_key(self):\n        \"\"\"\n        Tests that check_key correctly resolves special cases like '__first__' and '__latest__'.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.build_graph()\n        migration_key_first = ('migrations', '__first__')\n        migration_key_latest = ('migrations', '__latest__')\n        \n        resolved_first = loader.check_key(migration_key_first, 'migrations')\n        resolved_latest = loader.check_key(migration_key_latest, 'migrations')\n        \n        self.assertEqual(resolved_first, ('migrations', '0001_initial'))\n        self.assertEqual(resolved_latest, ('migrations', '0002_second'))\n\n        # Test for unmigrated apps\n        loader.unmigrated_apps.add('unmigrated_app')\n        unmigrated_key = ('unmigrated_app', '__first__')\n        self.assertIsNone(loader.check_key(unmigrated_key, 'migrations'))\n\n        # Test for unknown apps\n        with self.assertRaises(ValueError):\n            loader.check_key(('unknown_app', '__first__'), 'migrations')\n", "    def test_collect_sql(self):\n        \"\"\"\n        Tests the collect_sql method to ensure it returns the correct SQL statements.\n        \"\"\"\n        migration_loader = MigrationLoader(connection)\n        plan = migration_loader.graph.forwards_plan((\"migrations\", \"0002_second\"))\n        sql_statements = migration_loader.collect_sql(plan)\n        \n        # Ensure the collected SQL statements are not empty\n        self.assertTrue(sql_statements)\n        # Just a basic check on the first few statements to make sure SQL is collected\n        self.assertIn(\"CREATE TABLE\", sql_statements[0])\n        self.assertIn(\"migrations_author\", sql_statements[0])\n        self.assertIn(\"INSERT INTO\", sql_statements[1])\n", "    def test_detect_conflicts(self):\n        \"\"\"\n        Test MigrationLoader's detect_conflicts method to ensure it correctly\n        identifies conflicting migrations.\n        \"\"\"\n        # Load the migrations\n        migration_loader = MigrationLoader(connection)\n        \n        # Simulate conflicts by adding leaf nodes with the same app label but different migration names\n        migration_loader.graph.add_node((\"migrations\", \"0003_conflict1\"), None)\n        migration_loader.graph.add_node((\"migrations\", \"0004_conflict2\"), None)\n        \n        # Detect conflicts\n        conflicts = migration_loader.detect_conflicts()\n        \n        # Assert the conflicts dictionary contains the expected conflicts\n        self.assertIn(\"migrations\", conflicts)\n        self.assertEqual(conflicts[\"migrations\"], [\"0003_conflict1\", \"0004_conflict2\"])\n"], "sample_267": ["    def setUp(self):\n        self.settings_dict = {'NAME': ':memory:'}\n        self.wrapper = DatabaseWrapper(self.settings_dict)\n", "    def test_get_connection_params(self):\n        from django.db.backends.sqlite3.base import DatabaseWrapper\n\n        settings_dict = {\n            'NAME': 'test_db.sqlite3',\n            'OPTIONS': {\n                'timeout': 20,\n            },\n        }\n        db_wrapper = DatabaseWrapper(settings_dict)\n        conn_params = db_wrapper.get_connection_params()\n        \n        self.assertEqual(conn_params['database'], 'test_db.sqlite3')\n        self.assertEqual(conn_params['detect_types'], dbapi2.PARSE_DECLTYPES | dbapi2.PARSE_COLNAMES)\n        self.assertEqual(conn_params['timeout'], 20)\n        self.assertFalse(conn_params['check_same_thread'])\n        self.assertTrue(conn_params['uri'])\n", "    def test_get_connection_params(self):\n        from django.db.backends.sqlite3.base import DatabaseWrapper\n        settings_dict = {\n            'NAME': 'test.db',\n            'OPTIONS': {\n                'timeout': 20,\n            }\n        }\n        wrapper = DatabaseWrapper(settings_dict)\n        conn_params = wrapper.get_connection_params()\n        expected_params = {\n            'database': 'test.db',\n            'detect_types': dbapi2.PARSE_DECLTYPES | dbapi2.PARSE_COLNAMES,\n            'timeout': 20,\n            'check_same_thread': False,\n            'uri': True\n        }\n        self.assertEqual(conn_params, expected_params)\n", "    def test_get_connection_params(self):\n        settings_dict = {\n            'NAME': 'test_db',\n            'OPTIONS': {\n                'timeout': 20,\n            },\n        }\n        wrapper = DatabaseWrapper(settings_dict)\n        conn_params = wrapper.get_connection_params()\n        expected_params = {\n            'database': 'test_db',\n            'detect_types': Database.PARSE_DECLTYPES | Database.PARSE_COLNAMES,\n            'timeout': 20,\n            'check_same_thread': False,\n            'uri': True,\n        }\n        self.assertEqual(conn_params, expected_params)\n", "    def setUp(self):\n        self.cursor = connection.cursor()\n        self.cursor.execute(\"CREATE TABLE referenced_table (id INTEGER PRIMARY KEY)\")\n        self.cursor.execute(\"CREATE TABLE referencing_table (id INTEGER PRIMARY KEY, ref_id INTEGER, FOREIGN KEY(ref_id) REFERENCES referenced_table(id))\")\n        self.cursor.execute(\"INSERT INTO referenced_table (id) VALUES (1)\")\n"], "sample_268": ["    def test_ensure_echo_on_enabled(self, mocked_isatty, mocked_termios):\n        attr_list = [0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attr_list\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_termios.tcsetattr.called)  # No call if ECHO is already enabled\n", "    def test_ensure_echo_on(self, mock_isatty, mock_tcsetattr, mock_tcgetattr, mock_stdin):\n        mock_tcgetattr.return_value = [0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        self.assertEqual(mock_tcsetattr.call_count, 1)\n", "    def test_ensure_echo_on_enabled(self, mocked_tcsetattr, mocked_tcgetattr, mocked_isatty):\n        mocked_tcgetattr.return_value = [None, None, None, 0b00000000, None, None]\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_tcgetattr.called)\n        self.assertTrue(mocked_tcsetattr.called)\n", "    def test_ensure_echo_on_disabled(self, mock_isatty, mock_termios):\n        mock_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        attrs = mock_termios.tcgetattr.return_value\n        self.assertFalse(attrs[3] & mock_termios.ECHO)\n        mock_termios.tcsetattr.assert_not_called()\n", "    def test_echo_on(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        attr_list = [0, 0, 0, 0b10000]  # ECHO bit is not set\n        mock_tcgetattr.return_value = attr_list\n\n        autoreload.ensure_echo_on()\n\n        self.assertTrue(attr_list[3] & termios.ECHO)\n        mock_tcsetattr.assert_called_once_with(sys.stdin, termios.TCSANOW, attr_list)\n"], "sample_269": ["    def test_get_paths(self):\n        \"\"\"Test that get_paths returns correct locales paths for valid packages.\"\"\"\n        view = JavaScriptCatalog()\n        packages = ['admin', 'auth']\n        expected_paths = [os.path.join(app.path, 'locale') for app in apps.get_app_configs() if app.name in packages]\n        self.assertEqual(view.get_paths(packages), expected_paths)\n", "    def test_catalog_with_custom_domain(self):\n        \"\"\"\n        Test JavaScriptCatalog with a custom gettext domain.\n        \"\"\"\n        view = JavaScriptCatalog.as_view(domain='custom_domain')\n        request = RequestFactory().get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('\"custom_domain translation\"', response.content.decode())\n", "    def test_jsi18n_with_multiple_locale_paths(self):\n        \"\"\"\n        The JavaScriptCatalog should correctly handle multiple locale paths.\n        \"\"\"\n        with override('es'):\n            response = self.client.get('/jsi18n/')\n            self.assertContains(response, 'esto tiene que ser traducido')\n            self.assertNotContains(response, 'untranslated string')\n", "    def test_setlang_get_request(self):\n        \"\"\"\n        The set_language view does not change the language if accessed via GET,\n        and redirects to the 'next' argument if provided.\n        \"\"\"\n        lang_code = self._get_inactive_language_code()\n        get_data = {'language': lang_code, 'next': '/'}\n        response = self.client.get('/i18n/setlang/', data=get_data)\n        self.assertRedirects(response, '/')\n        # The language is not set in a cookie.\n        self.assertNotIn(settings.LANGUAGE_COOKIE_NAME, self.client.cookies)\n", "    def test_js_catalog_with_invalid_package(self):\n        \"\"\"\n        JavaScriptCatalog should raise an error if an invalid package is provided.\n        \"\"\"\n        view = JavaScriptCatalog.as_view()\n        request = RequestFactory().get('/')\n        with self.assertRaises(ValueError, msg='Invalid package(s) provided to JavaScriptCatalog: invalid_package'):\n            view(request, packages='invalid_package')\n"], "sample_270": ["    def test_modelbase_create_non_model_subclass(self):\n        class NonModelBaseClass:\n            pass\n\n        with self.assertRaises(TypeError):\n            class InvalidModel(models.Model, NonModelBaseClass):\n                pass\n", "    def test_non_abstract_model_requires_app_label(self):\n        \"\"\"\n        Test that non-abstract models must have an app_label defined or belong to an application\n        in INSTALLED_APPS.\n        \"\"\"\n        class Meta:\n            pass\n\n        attrs = {\n            '__module__': 'invalid_models_tests',\n            'Meta': Meta,\n        }\n\n        with self.assertRaises(RuntimeError) as context:\n            ModelBase('NonAbstractModel', (models.Model,), attrs)\n        \n        self.assertEqual(\n            str(context.exception),\n            \"Model class invalid_models_tests.NonAbstractModel doesn't declare an explicit app_label and isn't in an application in INSTALLED_APPS.\"\n        )\n", "    def test_swappable_model_custom(self):\n        class CustomModel(models.Model):\n            class Meta:\n                app_label = 'invalid_models_tests'\n\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPED_MODEL'\n\n        self.assertEqual(SwappableModel._meta.swapped, 'invalid_models_tests.CustomModel')\n        self.assertEqual(SwappableModel.check(), [])\n", "    def test_model_base_creation(self):\n        class MyModel(metaclass=ModelBase):\n            class Meta:\n                app_label = 'invalid_models_tests'\n\n        self.assertEqual(MyModel.__name__, 'MyModel')\n        self.assertEqual(MyModel._meta.app_label, 'invalid_models_tests')\n", "    def test_field_type_int(self):\n        class Model(models.Model):\n            field = models.IntegerField()\n\n        self.assertEqual(Model.check(), [])\n"], "sample_271": ["    def test_ensure_echo_on(self, mocked_stdin, mocked_termios):\n        # Simulate a TTY\n        mocked_stdin.isatty.return_value = True\n        # Simulate echo being off\n        attrs = [0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attrs\n        attrs[3] = attrs[3] & ~mocked_termios.ECHO\n        mocked_termios.TCSANOW = mock.Mock()\n        \n        autoreload.ensure_echo_on()\n        \n        # Ensure ECHO is set back on\n        self.assertTrue(attrs[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called_with(mocked_stdin, mocked_termios.TCSANOW, attrs)\n", "    def test_ensure_echo_on(self, mocked_tcsetattr, mocked_tcgetattr, mocked_isatty):\n        mocked_tcgetattr.return_value = [0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_tcsetattr.call_count, 1)\n", "    def test_echo_on_enabled(self, mock_isatty, mock_termios):\n        # Mock the attributes and methods of termios to simulate echo being enabled\n        attr_list = [0, 1, 2, 3, 4]\n        attr_list[3] = mock_termios.ECHO\n        mock_termios.tcgetattr.return_value = attr_list\n\n        autoreload.ensure_echo_on()\n\n        # ensure echo is enabled, so tcsetattr should not be called\n        mock_termios.tcgetattr.assert_called_once()\n        mock_termios.tcsetattr.assert_not_called()\n", "    def test_termios_not_imported(self):\n        # Should return immediately if termios is None.\n        autoreload.ensure_echo_on()\n", "    def test_trigger_reload(self, mock_exit, mock_logger):\n        filename = 'test_file.py'\n        autoreload.trigger_reload(filename)\n        mock_logger.info.assert_called_once_with('%s changed, reloading.', filename)\n        mock_exit.assert_called_once_with(3)\n"], "sample_272": ["    def test_migrate_with_custom_router(self):\n        \"\"\"\n        Tests running migrations with a custom database router.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Define a simple migration plan\n        plan = executor.migration_plan([(\"migrations\", \"0001_initial\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            ],\n        )\n        # Ensure tables do not exist before migration\n        self.assertTableNotExists(\"migrations_author\")\n        # Run the migration\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        # Verify the table is created\n        self.assertTableExists(\"migrations_author\")\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Undo the migration\n        plan = executor.migration_plan([(\"migrations\", None)])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n            ],\n        )\n        executor.migrate([(\"migrations\", None)])\n        # Ensure the table is removed\n        self.assertTableNotExists(\"migrations_author\")\n", "    def test_invalid_migration_plan(self):\n        \"\"\"\n        Tests that an InvalidMigrationPlan is raised for mixed migration plans.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Apply initial migrations\n        executor.migrate([\n            (\"migrations\", \"0001_initial\"),\n        ])\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n\n        # Generate a mixed plan with both forwards and backwards migrations\n        plan = [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ]\n        with self.assertRaises(InvalidMigrationPlan):\n            executor.migrate(plan=plan)\n\n        # Cleanup by unapplying the migration\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n", "    def test_mixed_migration_state(self):\n        \"\"\"\n        Test the case where some migrations in the middle of the plan are already applied.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Fake apply the initial migration\n        executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n\n        # Check the plan to apply the next migration\n        plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n            ],\n        )\n\n        # Apply the next migration\n        executor.migrate([(\"migrations\", \"0002_second\")])\n        # Ensure the tables are created\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_book\")\n\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n\n        # Fake-apply the last migration\n        executor.migrate([(\"migrations\", \"0003_third\")], fake=True)\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n\n        # Plan to migrate to the last migration\n        plan = executor.migration_plan([(\"migrations\", \"0003_third\")])\n        self.assertEqual(plan, [])\n\n        # Now plan to revert back to the second migration\n        plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0003_third\"], True),\n            ],\n        )\n\n        # Revert back to the second migration\n        executor.migrate([(\"migrations\", \"0002_second\")])\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_book\")\n        self.assertTableNotExists(\"migrations_tribble\")\n\n        # Finally, clean up by reverting back to no migrations\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        self.assertTableNotExists(\"migrations_tribble\")\n", "    def test_mixed_direction_plan_error_message(self):\n        \"\"\"\n        Tests that the error message for a mixed direction migration plan is correctly raised.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Apply initial migrations\n        executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n        executor.migrate([(\"migrations\", \"0002_second\")], fake=True)\n        # Generate a mixed direction plan\n        plan = [\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], True),\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ]\n        msg = (\n            'Migration plans with both forwards and backwards migrations are '\n            'not supported. Please split your migration process into separate '\n            'plans of only forwards OR backwards migrations.'\n        )\n        with self.assertRaisesMessage(InvalidMigrationPlan, msg) as cm:\n            executor.migrate(None, plan)\n        self.assertEqual(\n            cm.exception.args[1],\n            plan,\n        )\n", "    def test_mixed_plan_with_fake_initial(self):\n        \"\"\"\n        Tests that a mixed plan with --fake-initial option works correctly.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Apply initial migration\n        executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_tribble\")\n        # Check the plan for mixed migrations\n        plan = executor.migration_plan([\n            (\"migrations\", \"0002_second\"),\n            (\"migrations\", None),\n        ])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n            ],\n        )\n        # Run mixed plan with fake_initial=True\n        executor.migrate(None, plan, fake_initial=True)\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_tribble\")\n        self.assertIn(('migrations', '0002_second'), executor.loader.applied_migrations)\n        self.assertNotIn(('migrations', '0001_initial'), executor.loader.applied_migrations)\n        # Cleanup\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n"], "sample_273": ["    def test_property_name_clashes_with_related_field(self):\n        class RelatedModel(models.Model):\n            name = models.CharField(max_length=255)\n\n        class Model(models.Model):\n            related_model = models.ForeignKey(RelatedModel, on_delete=models.CASCADE)\n\n            @property\n                return self.related_model_id\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The property 'related_model_id' clashes with a related field accessor.\",\n                obj=Model,\n                id='models.E025',\n            ),\n        ])\n", "    def test_field_name_clash_with_parent(self):\n        class Parent(models.Model):\n            name = models.CharField(max_length=50)\n        \n        class Child(Parent):\n            name = models.IntegerField()\n        \n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The field 'name' from parent model 'check_framework.Parent' clashes with the field 'name' from parent model 'check_framework.Child'.\",\n                obj=Child,\n                id='models.E005',\n            ),\n        ])\n", "    def test_field_name_clash(self):\n        class ParentModel(models.Model):\n            common_field = models.CharField(max_length=20)\n\n        class ChildModel(ParentModel):\n            common_field = models.IntegerField()\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The field 'common_field' from parent model 'check_framework.ParentModel' clashes with the field 'common_field' from parent model 'check_framework.ChildModel'.\",\n                obj=ChildModel,\n                id='models.E005',\n            )\n        ])\n", "    def test_ordering_with_nonexistent_field(self):\n        class Model(models.Model):\n            class Meta:\n                ordering = ['nonexistent_field']\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"'ordering' refers to the nonexistent field, related field, or lookup 'nonexistent_field'.\",\n                obj=Model,\n                id='models.E015',\n            ),\n        ])\n", "    def test_invalid_ordering_type(self):\n        class Model(models.Model):\n            class Meta:\n                ordering = 'invalid_ordering'\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"'ordering' must be a tuple or list (even if you want to order by only one field).\",\n                obj=Model,\n                id='models.E014',\n            ),\n        ])\n"], "sample_274": ["    def test_modelform_factory(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=50)\n            age = models.IntegerField()\n\n            class Meta:\n                app_label = 'tests'\n\n        form = modelform_factory(TestModel, fields=['name', 'age'])\n        self.assertEqual(list(form.base_fields), ['name', 'age'])\n\n        form = modelform_factory(TestModel, exclude=['age'])\n        self.assertEqual(list(form.base_fields), ['name'])\n\n        with self.assertRaises(ImproperlyConfigured):\n            modelform_factory(TestModel)\n", "    def test_construct_instance(self):\n        from django.db import models\n\n        class TestModel(models.Model):\n            char_field = models.CharField(max_length=100)\n            int_field = models.IntegerField()\n            bool_field = models.BooleanField()\n\n        class TestForm(ModelForm):\n            class Meta:\n                model = TestModel\n                fields = ['char_field', 'int_field', 'bool_field']\n\n        form_data = {\n            'char_field': 'Test String',\n            'int_field': 42,\n            'bool_field': True\n        }\n\n        form = TestForm(data=form_data)\n        self.assertTrue(form.is_valid())\n        instance = construct_instance(form, TestModel())\n\n        self.assertEqual(instance.char_field, 'Test String')\n        self.assertEqual(instance.int_field, 42)\n        self.assertEqual(instance.bool_field, True)\n", "    def setUp(self):\n        # Create a sample model instance and form class for testing\n        from django.db import models\n\n        class SampleModel(models.Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField()\n\n        class SampleForm(ModelForm):\n            class Meta:\n                model = SampleModel\n                fields = ['name', 'age']\n\n        self.SampleModel = SampleModel\n        self.SampleForm = SampleForm\n", "def test_construct_instance(self):\n    from django.forms.models import construct_instance\n    from ..models import ChoiceModel\n\n    class TestForm(Form):\n        name = CharField()\n\n    instance = ChoiceModel(name='initial')\n    form = TestForm({'name': 'updated'})\n    instance = construct_instance(form, instance, fields=['name'])\n\n    self.assertEqual(instance.name, 'updated')\n\n    instance = ChoiceModel(name='initial')\n    form = TestForm({'name': ''})\n    instance = construct_instance(form, instance, fields=['name'])\n\n    self.assertEqual(instance.name, '')\n", "    def test_modelform_factory(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField()\n\n        form_class = modelform_factory(TestModel, fields=['name', 'age'])\n        self.assertIn('name', form_class.base_fields)\n        self.assertIn('age', form_class.base_fields)\n\n        form = form_class(data={'name': 'John Doe', 'age': 30})\n        self.assertTrue(form.is_valid())\n        instance = form.save(commit=False)\n        self.assertEqual(instance.name, 'John Doe')\n        self.assertEqual(instance.age, 30)\n"], "sample_275": ["    def test_queryset_len(self):\n        Book.objects.bulk_create([\n            Book(id=1, pagecount=100),\n            Book(id=2, pagecount=200),\n            Book(id=3, pagecount=300),\n        ])\n        qs = Book.objects.all()\n        self.assertEqual(len(qs), 3)\n", "    def test_queryset_deepcopy(self):\n        \"\"\"\n        Ensure deepcopy does not populate the QuerySet cache.\n        \"\"\"\n        queryset = Book.objects.all()\n        # Trigger cache population\n        list(queryset)\n        # Perform deepcopy\n        copied_queryset = copy.deepcopy(queryset)\n        self.assertIsNone(copied_queryset._result_cache)\n", "    def setUp(self):\n        self.book1 = Book.objects.create(id=1, pagecount=100)\n        self.book2 = Book.objects.create(id=2, pagecount=200)\n        self.book3 = Book.objects.create(id=3, pagecount=300)\n", "    def test_bulk_create(self):\n        \"\"\"\n        Ensure that bulk_create can insert multiple objects at once.\n        \"\"\"\n        books = [\n            Book(pagecount=150),\n            Book(pagecount=250),\n            Book(pagecount=350),\n        ]\n        created_books = Book.objects.bulk_create(books)\n        self.assertEqual(len(created_books), 3)\n        self.assertEqual(Book.objects.count(), 3)\n        self.assertEqual(Book.objects.filter(pagecount=150).count(), 1)\n        self.assertEqual(Book.objects.filter(pagecount=250).count(), 1)\n        self.assertEqual(Book.objects.filter(pagecount=350).count(), 1)\n", "    def test_query_set_getitem(self):\n        \"\"\"\n        Test retrieving an item or slice from the set of results using __getitem__.\n        \"\"\"\n        Book.objects.bulk_create([\n            Book(id=1, pagecount=100),\n            Book(id=2, pagecount=200),\n            Book(id=3, pagecount=300),\n        ])\n\n        # Retrieve single item\n        book = Book.objects.all()[1]\n        self.assertEqual(book.pagecount, 200)\n\n        # Retrieve slice\n        books = Book.objects.all()[1:3]\n        self.assertEqual(len(books), 2)\n        self.assertEqual(books[0].pagecount, 200)\n        self.assertEqual(books[1].pagecount, 300)\n\n        # Test step in slice\n        books = Book.objects.all()[0:3:2]\n        self.assertEqual(len(books), 2)\n        self.assertEqual(books[0].pagecount, 100)\n        self.assertEqual(books[1].pagecount, 300)\n"], "sample_276": ["    def test_template_detail_view(self):\n        template_name = 'test_template.html'\n        template_path = Path(settings.TEMPLATES[0]['DIRS'][0]) / template_name\n        template_path.write_text('{% block content %}Test Template Content{% endblock %}')\n\n        url = reverse('django-admindocs-templates', args=[template_name])\n        response = self.client.get(url)\n        \n        self.assertContains(response, '<h1>Template: <q>test_template.html</q></h1>', html=True)\n        self.assertContains(response, '<pre>{% block content %}Test Template Content{% endblock %}</pre>', html=True)\n        self.assertContains(response, '<td>Order</td>')\n        self.assertContains(response, '<td>File</td>')\n        self.assertContains(response, '<td>Contents</td>')\n        template_path.unlink()\n", "    def test_template_detail_template_not_found(self):\n        \"\"\"\n        If the template is not found in the template directories, it should\n        return an appropriate response indicating the template does not exist.\n        \"\"\"\n        response = self.client.get(reverse('django-admindocs-templates', args=['non_existent_template.html']))\n        self.assertContains(response, '<h1>Template: <q>non_existent_template.html</q></h1>', html=True)\n        self.assertContains(response, '<p>The template does not exist.</p>', html=True)\n", "    def test_model_detail_view_no_related_fields(self):\n        \"\"\"\n        Models with no related fields or methods should display an appropriate message.\n        \"\"\"\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'SimpleModel']))\n        self.assertContains(response, \"<h3>Fields</h3>\")\n        self.assertContains(response, \"<p>No fields available.</p>\")\n        self.assertContains(response, \"<h3>Methods with arguments</h3>\")\n        self.assertContains(response, \"<p>No methods with arguments available.</p>\")\n        self.assertContains(response, \"<h3>Related fields</h3>\")\n        self.assertContains(response, \"<p>No related fields available.</p>\")\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n"], "sample_277": ["    def test_invert(self):\n        q = Q(price__gt=F('discounted_price'))\n        inverted_q = ~q\n        self.assertTrue(inverted_q.negated)\n        self.assertEqual(inverted_q.connector, Q.AND)\n        self.assertEqual(inverted_q.children, [q])\n", "    def test_invert(self):\n        q = Q(price__gt=F('discounted_price'))\n        inverted_q = ~q\n        self.assertTrue(inverted_q.negated)\n        self.assertEqual(inverted_q.connector, 'AND')\n        self.assertEqual(inverted_q.children, [q])\n", "    def test_invert(self):\n        q = Q(price__gt=F('discounted_price'))\n        inverted_q = ~q\n        self.assertTrue(inverted_q.negated)\n        self.assertEqual(inverted_q.connector, 'AND')\n        self.assertEqual(inverted_q.children, [q])\n", "    def test_combine_negated(self):\n        q1 = Q(price__gt=F('discounted_price'))\n        q2 = Q(price=F('discounted_price'))\n        q = q1 & ~q2\n        path, args, kwargs = q.deconstruct()\n        self.assertEqual(args, (\n            ('price__gt', F('discounted_price')),\n            ('price', F('discounted_price')),\n        ))\n        self.assertEqual(kwargs, {'_negated': True})\n", "def test_invert(self):\n    q = Q(price__gt=F('discounted_price'))\n    inverted_q = ~q\n    self.assertTrue(inverted_q.negated)\n    self.assertEqual(inverted_q.connector, Q.AND)\n    self.assertEqual(inverted_q.children, [q])\n"], "sample_278": ["    def test_filtered_relation_init(self):\n        condition = Q(name=\"test\")\n        relation = FilteredRelation(\"test_relation\", condition=condition)\n        self.assertEqual(relation.relation_name, \"test_relation\")\n        self.assertEqual(relation.condition, condition)\n        self.assertIsNone(relation.alias)\n        self.assertEqual(relation.path, [])\n", "    def test_register_lookup(self):\n        class TestLookup:\n            lookup_name = 'test'\n\n        self.TestField.register_lookup(TestLookup)\n        self.assertIn('test', self.TestField.get_lookups())\n        self.assertIs(self.TestField.get_lookup('test'), TestLookup)\n", "    def test_register_lookup(self):\n        RegisterLookupMixinTests.TestLookup.register_lookup(RegisterLookupMixinTests.DummyLookup)\n        self.assertIn('dummy', RegisterLookupMixinTests.TestLookup.get_lookups())\n", "    def test_q_combination(self):\n        q1 = Q(name='example')\n        q2 = Q(num_employees__gt=10)\n        combined_q = q1 & q2\n        self.assertEqual(combined_q.connector, Q.AND)\n        self.assertIn(q1, combined_q.children)\n        self.assertIn(q2, combined_q.children)\n", "    def test_deferred_loading_field(self):\n        \"\"\"\n        Test that DeferredAttribute properly loads and caches the field value.\n        \"\"\"\n        emp = Employee.objects.create(firstname=\"Test\", lastname=\"User\", salary=100)\n        emp_id = emp.id\n        ceo_field = Company._meta.get_field(\"ceo\")\n        deferred_attr = DeferredAttribute(ceo_field)\n\n        company = Company.objects.create(\n            name=\"Test Company\", num_employees=1, num_chairs=1, ceo=emp\n        )\n        company_id = company.id\n        \n        # Simulate deferred loading\n        del company.ceo\n        self.assertNotIn(\"ceo\", company.__dict__)\n        \n        # Access deferred attribute and check if it loads and caches correctly\n        loaded_ceo = deferred_attr.__get__(company, Company)\n        self.assertEqual(loaded_ceo, emp)\n        self.assertIn(\"ceo\", company.__dict__)\n        self.assertEqual(company.__dict__[\"ceo\"], emp)\n        \n        # Ensure accessing again returns cached value\n        cached_ceo = deferred_attr.__get__(company, Company)\n        self.assertIs(cached_ceo, emp)\n        self.assertIs(cached_ceo, company.__dict__[\"ceo\"])\n"], "sample_279": ["    def test_invalid_fields_argument(self):\n        message = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, message):\n            models.UniqueConstraint(fields=[], name='invalid')\n", "    def test_invalid_fields_argument(self):\n        msg = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=[],\n                name='invalid_fields',\n            )\n", "    def test_clone(self):\n        original_constraint = models.UniqueConstraint(\n            fields=['foo', 'bar'],\n            name='unique_constraint',\n            condition=models.Q(foo=models.F('bar')),\n            deferrable=models.Deferrable.DEFERRED,\n            include=['baz_1', 'baz_2'],\n            opclasses=['text_pattern_ops', 'varchar_pattern_ops'],\n        )\n        cloned_constraint = original_constraint.clone()\n        self.assertEqual(original_constraint, cloned_constraint)\n        self.assertIsNot(original_constraint, cloned_constraint)\n        self.assertEqual(\n            repr(cloned_constraint),\n            \"<UniqueConstraint: fields=('foo', 'bar') name='unique_constraint' \"\n            \"condition=(AND: ('foo', F(bar))) \"\n            \"deferrable=Deferrable.DEFERRED include=('baz_1', 'baz_2') \"\n            \"opclasses=['text_pattern_ops', 'varchar_pattern_ops']>\"\n        )\n", "    def test_invalid_fields_argument(self):\n        msg = 'At least one field is required to define a unique constraint.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(name='unique_fields', fields=[])\n", "def test_unique_constraint_invalid_field(self):\n    message = 'At least one field is required to define a unique constraint.'\n    with self.assertRaisesMessage(ValueError, message):\n        models.UniqueConstraint(fields=[], name='invalid_unique')\n"], "sample_280": ["    def test_stddev_sample(self):\n        \"\"\"\n        Test the StdDev aggregate function with the sample parameter set to True.\n        \"\"\"\n        # Sample standard deviation of author ages\n        vals = Author.objects.aggregate(stddev_sample=StdDev('age', sample=True))\n        expected_stddev = math.sqrt(sum((x - 37.4) ** 2 for x in [34, 35, 45, 29, 37, 29, 25, 57, 46]) / 8)  # Sample stddev\n        self.assertAlmostEqual(vals['stddev_sample'], expected_stddev, places=6)\n", "    def test_aggregate_distinct_behavior(self):\n        # Testing distinct behavior in aggregate functions with different scenarios.\n        \n        # Distinct behavior with Sum aggregate\n        vals = Book.objects.aggregate(distinct_sum=Sum('rating', distinct=True))\n        self.assertEqual(vals, {'distinct_sum': 21.5})  # 3.0, 4.0, 4.5, 5.0, 4.0, 4.0 -> 3.0, 4.0, 4.5, 5.0 -> 21.5\n\n        # Distinct behavior with Avg aggregate\n        vals = Book.objects.aggregate(distinct_avg=Avg('rating', distinct=True))\n        self.assertEqual(vals, {'distinct_avg': 4.125})  # (3.0 + 4.0 + 4.5 + 5.0) / 4 = 4.125\n\n        # Distinct behavior with Count aggregate\n        vals = Book.objects.aggregate(distinct_count=Count('rating', distinct=True))\n        self.assertEqual(vals, {'distinct_count': 4})  # 3.0, 4.0, 4.5, 5.0\n\n        # Distinct behavior with Min aggregate\n        vals = Book.objects.aggregate(distinct_min=Min('rating', distinct=True))\n        self.assertEqual(vals, {'distinct_min': 3.0})  # Minimum distinct rating is 3.0\n\n        # Distinct behavior with Max aggregate\n        vals = Book.objects.aggregate(distinct_max=Max('rating', distinct=True))\n        self.assertEqual(vals, {'distinct_max': 5.0})  # Maximum distinct rating is 5.0\n\n        # Distinct behavior with Variance aggregate\n        vals = Book.objects.aggregate(distinct_variance=Variance('rating', distinct=True))\n        self.assertAlmostEqual(vals['distinct_variance'], 0.875, places=3)  # Variance of distinct ratings: 3.0, 4.0, 4.5, 5.0\n\n        # Distinct behavior with StdDev aggregate\n        vals = Book.objects.aggregate(distinct_stddev=StdDev('rating', distinct=True))\n        self.assertAlmostEqual(vals['distinct_stddev'], 0.935, places=3)  # StdDev of distinct ratings: 3.0, 4.0, 4.5, 5.", "    def test_aggregate_with_default(self):\n        # Test Sum with default value\n        result = Author.objects.filter(age__gt=100).aggregate(total_age=Sum('age', default=10))\n        self.assertEqual(result['total_age'], 10)\n\n        # Test Avg with default value\n        result = Author.objects.filter(age__gt=100).aggregate(average_age=Avg('age', default=20))\n        self.assertEqual(result['average_age'], 20)\n\n        # Test Max with default value\n        result = Author.objects.filter(age__gt=100).aggregate(max_age=Max('age', default=30))\n        self.assertEqual(result['max_age'], 30)\n\n        # Test Min with default value\n        result = Author.objects.filter(age__gt=100).aggregate(min_age=Min('age', default=40))\n        self.assertEqual(result['min_age'], 40)\n\n        # Test StdDev with default value\n        result = Author.objects.filter(age__gt=100).aggregate(stddev_age=StdDev('age', default=50))\n        self.assertEqual(result['stddev_age'], 50)\n\n        # Test Variance with default value\n        result = Author.objects.filter(age__gt=100).aggregate(var_age=Variance('age', default=60))\n        self.assertEqual(result['var_age'], 60)\n", "    def test_count_with_distinct(self):\n        books = Book.objects.filter(pages__gt=300).aggregate(distinct_authors=Count('authors', distinct=True))\n        self.assertEqual(books['distinct_authors'], 7)\n", "    def test_aggregate_with_distinct(self):\n        # Test distinct functionality for Avg, Count, and Sum aggregates.\n        vals = Author.objects.aggregate(distinct_avg_age=Avg(\"age\", distinct=True))\n        self.assertEqual(vals, {\"distinct_avg_age\": Approximate(37.4, places=1)})\n\n        vals = Author.objects.aggregate(distinct_count_age=Count(\"age\", distinct=True))\n        self.assertEqual(vals, {\"distinct_count_age\": 9})\n\n        vals = Author.objects.aggregate(distinct_sum_age=Sum(\"age\", distinct=True))\n        self.assertEqual(vals, {\"distinct_sum_age\": 337})\n"], "sample_281": ["    def test_invalid_app_label(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid_app', 'model_name': 'Answer', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_invalid_app_label(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid_app', 'model_name': 'Answer', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_invalid_app_label(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid', 'model_name': 'answer', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_invalid_model(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid_app', 'model_name': 'invalid_model', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_invalid_app_label(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid', 'model_name': 'answer', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_282": ["def test_boundfield_as_widget(self):\n    class SampleForm(Form):\n        sample_field = CharField(widget=TextInput(attrs={'class': 'custom-class'}))\n\n    form = SampleForm()\n    bound_field = form['sample_field']\n    widget_html = bound_field.as_widget()\n    self.assertIn('class=\"custom-class\"', widget_html)\n    self.assertIn('name=\"sample_field\"', widget_html)\n    self.assertIn('type=\"text\"', widget_html)\n", "    def test_bound_field_label_tag(self):\n        form = PartiallyRequiredForm({'f_0': 'Hello', 'f_1': ''})\n        bound_field = form['f']\n        label = bound_field.label_tag()\n        self.assertInHTML('<label for=\"id_f_0\">F:</label>', label)\n        \n        label_with_content = bound_field.label_tag(contents=\"Custom Label\")\n        self.assertInHTML('<label for=\"id_f_0\">Custom Label:</label>', label_with_content)\n        \n        label_with_attrs = bound_field.label_tag(attrs={'class': 'custom-class'})\n        self.assertInHTML('<label for=\"id_f_0\" class=\"custom-class\">F:</label>', label_with_attrs)\n        \n        label_with_suffix = bound_field.label_tag(label_suffix=\"?\")\n        self.assertInHTML('<label for=\"id_f_0\">F?:</label>', label_with_suffix)\n", "def test_boundfield_label_tag(self):\n    class SampleForm(Form):\n        field1 = CharField(label='Sample Label', required=True)\n\n    form = SampleForm()\n    bound_field = form['field1']\n    rendered_label = bound_field.label_tag()\n    self.assertInHTML('<label for=\"id_field1\">Sample Label:</label>', rendered_label)\n\n    # Test with label_suffix\n    form.label_suffix = '*'\n    rendered_label = bound_field.label_tag()\n    self.assertInHTML('<label for=\"id_field1\">Sample Label*:</label>', rendered_label)\n\n    # Test with custom contents\n    rendered_label = bound_field.label_tag(contents='Custom Label')\n    self.assertInHTML('<label for=\"id_field1\">Custom Label:</label>', rendered_label)\n\n    # Test with extra attributes\n    rendered_label = bound_field.label_tag(attrs={'class': 'custom-class'})\n    self.assertInHTML('<label for=\"id_field1\" class=\"custom-class\">Sample Label*:</label>', rendered_label)\n", "    def test_boundfield_css_classes(self):\n        class TestForm(Form):\n            name = CharField()\n        form = TestForm()\n        bound_field = form['name']\n        self.assertEqual(bound_field.css_classes(), '')\n        form.error_css_class = 'error'\n        form.required_css_class = 'required'\n        self.assertEqual(bound_field.css_classes(), 'required')\n        form.add_error('name', 'error')\n        self.assertEqual(bound_field.css_classes(), 'error required')\n", "def test_bound_field_label_tag(self):\n    form = ComplexFieldForm()\n    bound_field = form['field1']\n    self.assertHTMLEqual(\n        bound_field.label_tag(),\n        '<label for=\"id_field1_0\">Field1:</label>'\n    )\n    self.assertHTMLEqual(\n        bound_field.label_tag(contents='Custom Label'),\n        '<label for=\"id_field1_0\">Custom Label:</label>'\n    )\n    self.assertHTMLEqual(\n        bound_field.label_tag(attrs={'class': 'custom-class'}),\n        '<label for=\"id_field1_0\" class=\"custom-class\">Field1:</label>'\n    )\n"], "sample_283": ["def test_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {},\n        )\n    )\n", "def test_no_name_no_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'PASSWORD': 'somepassword',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n", "def test_default_dbname_when_no_name_or_service(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\n                'USER': 'someuser',\n                'PASSWORD': 'somepassword',\n                'HOST': 'somehost',\n                'PORT': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_no_db_name_or_service(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\n                'USER': 'someuser',\n                'PASSWORD': 'somepassword',\n                'HOST': 'somehost',\n                'PORT': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "def test_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {},\n        )\n    )\n"], "sample_284": ["    def test_manifest_save_load(self):\n        \"\"\"\n        Ensure that saving and loading the manifest works correctly.\n        \"\"\"\n        # Initial run to create manifest\n        self.run_collectstatic()\n        initial_manifest = storage.staticfiles_storage.load_manifest()\n        self.assertNotEqual(initial_manifest, {})\n\n        # Clear in-memory manifest and reload from file\n        storage.staticfiles_storage.hashed_files = {}\n        reloaded_manifest = storage.staticfiles_storage.load_manifest()\n        self.assertEqual(reloaded_manifest, initial_manifest)\n\n        # Modify manifest in-memory and save it\n        modified_manifest = initial_manifest.copy()\n        modified_manifest['new/file.txt'] = 'new/file.hash.txt'\n        storage.staticfiles_storage.hashed_files = modified_manifest\n        storage.staticfiles_storage.save_manifest()\n\n        # Reload from file and check it matches modified manifest\n        storage.staticfiles_storage.hashed_files = {}\n        reloaded_modified_manifest = storage.staticfiles_storage.load_manifest()\n        self.assertEqual(reloaded_modified_manifest, modified_manifest)\n", "    def test_hashed_name_with_no_content(self):\n        \"\"\"\n        Test that hashed_name returns the original name when no content is provided.\n        \"\"\"\n        storage_instance = storage.staticfiles_storage\n        original_name = 'test/no_content.css'\n        hashed_name = storage_instance.hashed_name(original_name)\n        self.assertEqual(original_name, hashed_name)\n", "    def setUp(self):\n        self.storage = ManifestFilesMixin()\n", "    def test_file_hash_with_empty_content(self):\n        mixin = HashedFilesMixin()\n        result = mixin.file_hash('dummy_file', content=None)\n        self.assertIsNone(result)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(self._temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, self._temp_dir)\n"], "sample_285": ["    def test_dirs_with_absolute_paths(self):\n        static_dir1 = Path('/absolute/path/to/static')\n        static_dir2 = Path('/another/absolute/path')\n        with self.settings(STATIC_ROOT='/static/root', STATICFILES_DIRS=[static_dir1, static_dir2]):\n            self.assertEqual(check_finders(None), [])\n            finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n            self.assertIn(str(static_dir1), [str(location[1]) for location in finder.locations])\n            self.assertIn(str(static_dir2), [str(location[1]) for location in finder.locations])\n", "    def test_dirs_existing_path_with_prefix(self, mock_isdir):\n        error1 = Error(\n            \"The prefix 'prefix/' in the STATICFILES_DIRS setting must \"\n            \"not end with a slash.\",\n            id='staticfiles.E003',\n        )\n        error2 = Warning(\n            \"The directory '/existing/path' in the STATICFILES_DIRS setting \"\n            \"does not exist.\",\n            id='staticfiles.W004',\n        )\n        self.assertEqual(check_finders(None), [])\n        finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n        self.assertEqual(finder.locations, [('prefix', '/existing/path')])\n        self.assertTrue('/existing/path' in finder.storages)\n", "    def test_valid_directories(self):\n        with mock.patch('os.path.isdir', return_value=True):\n            self.assertEqual(check_finders(None), [])\n            finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n            self.assertTrue(list(finder.list(None)))\n", "    def test_find_in_nonexistent_directories(self):\n        finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n        self.assertEqual(finder.find('somefile.txt'), None)\n        self.assertEqual(finder.find('somefile.txt', all=True), [])\n", "    def test_find_static_file_in_filesystem_finder(self):\n        static_dir = Path(TEST_ROOT) / 'project' / 'documents'\n        static_file = static_dir / 'test.txt'\n        with self.settings(STATICFILES_DIRS=[static_dir]):\n            if not static_dir.exists():\n                static_dir.mkdir(parents=True)\n            static_file.touch()\n            finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n            found = finder.find('test.txt')\n            self.assertEqual(found, str(static_file))\n            static_file.unlink()\n            static_dir.rmdir()\n"], "sample_286": ["    def test_model_creation_with_missing_app_label(self):\n        # Test that creating a model without an app_label in Meta raises an error.\n        with self.assertRaisesMessage(RuntimeError, \"Model class tests.ModelTestModel doesn't declare an explicit app_label and isn't in an application in INSTALLED_APPS.\"):\n            class ModelTestModel(models.Model):\n                class Meta:\n                    pass\n", "    def test_model_creation_without_explicit_app_label(self):\n        \"\"\"\n        Test that creating a model without an explicit app label raises a\n        RuntimeError if the model isn't in an installed app.\n        \"\"\"\n        class Meta:\n            pass\n        \n        attrs = {\n            '__module__': 'uninstalled_app',\n            'Meta': Meta,\n        }\n\n        with self.assertRaisesMessage(RuntimeError, \"Model class uninstalled_app.ModelWithoutAppLabel doesn't declare an explicit app_label and isn't in an application in INSTALLED_APPS.\"):\n            ModelBase('ModelWithoutAppLabel', (models.Model,), attrs)\n", "    def test_initialize_model_with_missing_required_fields(self):\n        \"\"\"\n        Attempt to initialize a model instance without required fields should raise an error.\n        \"\"\"\n        with self.assertRaisesMessage(TypeError, \"__init__() missing 2 required positional arguments: 'headline' and 'pub_date'\"):\n            Article()\n", "    def test_from_db_creates_instance_correctly(self):\n        Article.objects.create(headline=\"From DB\", pub_date=datetime(2023, 10, 5))\n        article = Article.objects.get(headline=\"From DB\")\n        article_from_db = Article.from_db(\"default\", [\"id\", \"headline\", \"pub_date\"], [article.id, \"From DB\", datetime(2023, 10, 5)])\n        self.assertEqual(article, article_from_db)\n        self.assertEqual(article_from_db.headline, \"From DB\")\n        self.assertEqual(article_from_db.pub_date, datetime(2023, 10, 5))\n        self.assertFalse(article_from_db._state.adding)\n        self.assertEqual(article_from_db._state.db, \"default\")\n", "    def test_modelbase_new_creates_class(self):\n        class TestModel(metaclass=ModelBase):\n            class Meta:\n                app_label = 'test'\n\n        self.assertIsInstance(TestModel, ModelBase)\n        self.assertEqual(TestModel.__name__, 'TestModel')\n        self.assertEqual(TestModel._meta.app_label, 'test')\n"], "sample_287": ["    def test_check_autocomplete_fields_not_list_or_tuple(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = 'test'\n\n        self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n            checks.Error(\n                \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                obj=SongAdmin,\n                id='admin.E036',\n            )\n        ])\n", "    def test_invalid_field_in_autocomplete_fields(self):\n        \"\"\"\n        Test that an error is raised when 'autocomplete_fields' contains a non-existent field.\n        \"\"\"\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = ['nonexistent_field']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields[0]' refers to 'nonexistent_field', \"\n                \"which is not a field of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E037',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_raw_id_fields(self):\n        class AlbumAdmin(admin.ModelAdmin):\n            raw_id_fields = ['author']\n\n        class AuthorAdmin(admin.ModelAdmin):\n            pass\n\n        admin.site.register(Author, AuthorAdmin)\n        try:\n            errors = AlbumAdmin(Album, AdminSite()).check()\n            expected = [\n                checks.Error(\n                    \"The value of 'raw_id_fields[0]' refers to 'author', which is not a field of 'admin_checks.Album'.\",\n                    obj=AlbumAdmin,\n                    id='admin.E002',\n                )\n            ]\n            self.assertEqual(errors, expected)\n        finally:\n            admin.site.unregister(Author)\n", "    def test_list_display_with_callable(self):\n        \"\"\"\n        Ensure that list_display can contain callables.\n        \"\"\"\n        class SongAdmin(admin.ModelAdmin):\n                return obj.title\n\n            list_display = (\"my_custom_display\",)\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "def test_prepopulated_fields_not_dict(self):\n    \"\"\"\n    Check that a non-dictionary 'prepopulated_fields' raises a system check error.\n    \"\"\"\n    class SongAdmin(admin.ModelAdmin):\n        prepopulated_fields = 'test'\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'prepopulated_fields' must be a dictionary.\",\n            obj=SongAdmin,\n            id='admin.E026',\n        )\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_288": ["    def test_compile_json_path(self):\n        self.assertEqual(compile_json_path(['a', 'b', 1, 'c']), '$.a.b[1].\"c\"')\n        self.assertEqual(compile_json_path(['a', 'b', 1, 'c'], include_root=False), 'a.b[1].\"c\"')\n        self.assertEqual(compile_json_path(['a']), '$.a')\n        self.assertEqual(compile_json_path([]), '$')\n        self.assertEqual(compile_json_path([], include_root=False), '')\n", "    def test_check_supported(self):\n        model_field = JSONField()\n        errors = model_field._check_supported(['default'])\n        self.assertEqual(errors, [])\n", "    def test_compile_json_path_integers(self):\n        key_transforms = [0, 1, 2]\n        expected = '$[0][1][2]'\n        self.assertEqual(compile_json_path(key_transforms), expected)\n", "    def test_compile_json_path_include_root(self):\n        key_transforms = ['a', 'b', 1, 'c']\n        expected_path = '$.\"a\".\"b\"[1].\"c\"'\n        self.assertEqual(compile_json_path(key_transforms), expected_path)\n", "    def test_key_transform_numeric_lookup(self):\n        field = models.JSONField()\n        transform = field.get_transform('c')\n        self.assertIsInstance(transform, KeyTransformFactory)\n        \n        instance = JSONModel(value={'c': 10})\n        instance.save()\n        instance.refresh_from_db()\n        \n        self.assertSequenceEqual(\n            JSONModel.objects.filter(value__c__lt=15),\n            [instance],\n        )\n        self.assertSequenceEqual(\n            JSONModel.objects.filter(value__c__lte=10),\n            [instance],\n        )\n        self.assertSequenceEqual(\n            JSONModel.objects.filter(value__c__gt=5),\n            [instance],\n        )\n        self.assertSequenceEqual(\n            JSONModel.objects.filter(value__c__gte=10),\n            [instance],\n        )\n"], "sample_289": ["    def test_setlistdefault(self):\n        d = MultiValueDict({'key1': ['value1']})\n        default_list = ['default1', 'default2']\n        self.assertEqual(d.setlistdefault('key1', default_list), ['value1'])\n        self.assertEqual(d.setlistdefault('key2', default_list), default_list)\n        self.assertEqual(d.getlist('key2'), default_list)\n", "    def test_immutablelist_basic(self):\n        d = ImmutableList([1, 2, 3, 4])\n        self.assertEqual(d[0], 1)\n        self.assertEqual(d[-1], 4)\n        self.assertEqual(d[1:3], (2, 3))\n        self.assertEqual(len(d), 4)\n        self.assertTrue(isinstance(d, tuple))\n", "    def test_empty_initialization(self):\n        s = OrderedSet()\n        self.assertEqual(len(s), 0)\n        self.assertFalse(s)\n        self.assertEqual(list(s), [])\n", "    def test_update_with_invalid_iterable_elements(self):\n        x = MultiValueDict()\n        msg = 'dictionary update sequence element #0 has length 1; 2 is required.'\n        with self.assertRaisesMessage(ValueError, msg):\n            x.update([('key',)])\n", "    def test_immutablelist_methods(self):\n        d = ImmutableList([1, 2, 3])\n\n        # Test immutable list methods\n        with self.assertRaisesMessage(AttributeError, 'ImmutableList object is immutable.'):\n            d.__delitem__(1)\n\n        with self.assertRaisesMessage(AttributeError, 'ImmutableList object is immutable.'):\n            d.__delslice__(0, 1)\n\n        with self.assertRaisesMessage(AttributeError, 'ImmutableList object is immutable.'):\n            d.__iadd__([4, 5])\n\n        with self.assertRaisesMessage(AttributeError, 'ImmutableList object is immutable.'):\n            d.__imul__(2)\n\n        with self.assertRaisesMessage(AttributeError, 'ImmutableList object is immutable.'):\n            d.append(4)\n\n        with self.assertRaisesMessage(AttributeError, 'ImmutableList object is immutable.'):\n            d.extend([5, 6])\n\n        with self.assertRaisesMessage(AttributeError, 'ImmutableList object is immutable.'):\n            d.insert(1, 7)\n\n        with self.assertRaisesMessage(AttributeError, 'ImmutableList object is immutable.'):\n            d.pop()\n\n        with self.assertRaisesMessage(AttributeError, 'ImmutableList object is immutable.'):\n            d.remove(3)\n\n        with self.assertRaisesMessage(AttributeError, 'ImmutableList object is immutable.'):\n            d.reverse()\n\n        self.assertEqual(list(d), [1, 2, 3])\n"], "sample_290": ["    def test_mutate_state_preserve_false(self):\n        \"\"\"Tests the mutate_state method with preserve=False.\"\"\"\n        project_state = self.make_project_state([self.author_empty])\n        migration = migrations.Migration(\"0001_initial\", \"testapp\")\n        migration.operations = [migrations.AddField(\"Author\", \"name\", models.CharField(max_length=200))]\n        new_state = migration.mutate_state(project_state, preserve=False)\n        # Ensure the original state is mutated\n        self.assertEqual(new_state.models['testapp', 'author'].fields['name'].max_length, 200)\n        self.assertIn(('name', models.CharField), new_state.models['testapp', 'author'].fields)\n        # Ensure the original state was not preserved\n        self.assertIs(project_state, new_state)\n", "def test_apply_migration(self):\n    \"\"\"\n    Tests the apply method of the Migration class.\n    \"\"\"\n    class TestOperation:\n            self.name = name\n            self.atomic = atomic\n            self.reversible = reversible\n\n            pass\n\n            schema_editor.collected_sql.append(f\"-- Applying {self.name}\")\n\n            return f\"Operation {self.name}\"\n\n        @property\n            return True\n\n    project_state = mock.Mock()\n    schema_editor = mock.Mock()\n    schema_editor.collected_sql = []\n    schema_editor.atomic_migration = True\n    schema_editor.connection.alias = 'default'\n\n    migration = migrations.Migration('test_migration', 'testapp')\n    migration.operations = [TestOperation('op1'), TestOperation('op2', atomic=False)]\n\n    result_state = migration.apply(project_state, schema_editor, collect_sql=True)\n    \n    self.assertEqual(result_state, project_state)\n    self.assertEqual(schema_editor.collected_sql, [\n        \"--\", \"-- Operation op1\", \"--\", \n        \"--\", \"-- Operation op2\", \"--\"\n    ])\n", "def test_mutate_state_preserve_false(self):\n    \"\"\"Tests the mutate_state method with preserve=False.\"\"\"\n    class MockOperation:\n            state.operations_applied.append(self)\n\n    class MockState:\n            return MockState()\n\n            self.operations_applied = []\n\n    migration = migrations.Migration('0001_initial', 'test_app')\n    mock_operations = [MockOperation(), MockOperation()]\n    migration.operations = mock_operations\n    project_state = MockState()\n\n    new_state = migration.mutate_state(project_state, preserve=False)\n    self.assertEqual(new_state.operations_applied, mock_operations)\n    self.assertNotEqual(new_state, project_state)\n", "    def test_suggest_name_with_replaces(self):\n        \"\"\"Test suggest_name when migration replaces other migrations.\"\"\"\n        class Migration(migrations.Migration):\n            replaces = [\"0001_initial\", \"0002_auto_20210101_1234\"]\n\n        migration = Migration('0003_auto_20210101_5678', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'auto_%s' % get_migration_name_timestamp())\n", "def test_mutate_state(self):\n        \"\"\"Tests the mutate_state method of Migration class.\"\"\"\n        project_state = ProjectState()\n        new_model_state = ModelState(\n            app_label=\"testapp\",\n            name=\"NewModel\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n        )\n        project_state.add_model(new_model_state)\n\n        class AddFieldOperation:\n            reversible = True\n            reduces_to_sql = True\n            atomic = True\n\n                model_state = state.models[app_label, \"newmodel\"]\n                model_state.fields.append((\"new_field\", models.CharField(max_length=200)))\n                state.reload_model(app_label, \"NewModel\")\n\n                pass\n\n                pass\n\n                return \"AddFieldOperation\"\n\n        migration = Migration(\"some_migration\", \"testapp\")\n        migration.operations = [AddFieldOperation()]\n        mutated_state = migration.mutate_state(project_state)\n\n        new_model_state_after = mutated_state.models[\"testapp\", \"newmodel\"]\n        self.assertEqual(len(new_model_state_after.fields), 2)\n        self.assertEqual(new_model_state_after.fields[1][0], \"new_field\")\n        self.assertIsInstance(new_model_state_after.fields[1][1], models.CharField)\n"], "sample_291": ["    def test_render_to_response(self):\n        class TestView(TemplateResponseMixin, View):\n            template_name = 'generic_views/about.html'\n\n        view = TestView()\n        view.setup(self.rf.get('/'))\n        context = {'key': 'value'}\n        response = view.render_to_response(context)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context_data['key'], 'value')\n        self.assertEqual(response.template_name, ['generic_views/about.html'])\n", "    def test_redirect_with_invalid_pattern_name(self):\n        \"\"\"\n        Ensure RedirectView raises an error when an invalid pattern name is provided.\n        \"\"\"\n        with self.assertRaises(NoReverseMatch):\n            RedirectView.as_view(pattern_name='invalid_pattern')(self.rf.get('/foo/'))\n", "    def test_redirect_view_with_parameters(self):\n        \"\"\"\n        Test that RedirectView correctly formats the URL with given parameters.\n        \"\"\"\n        response = RedirectView.as_view(url='/bar/%(slug)s/', query_string=True)(self.rf.get('/foo/?key=value'), slug='example')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/example/?key=value')\n", "    def test_redirect_with_pattern_and_args(self):\n        \"\"\"\n        Test RedirectView with pattern_name and arguments.\n        \"\"\"\n        response = RedirectView.as_view(pattern_name='artist_detail', query_string=True)(self.rf.get('/foo/', {'id': 10}))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/detail/artist/10/?id=10')\n", "    def test_template_name_as_string(self):\n        \"\"\"\n        Test that a template view with a single template name string works correctly.\n        \"\"\"\n        class SingleTemplateNameView(TemplateView):\n            template_name = 'generic_views/single_template.html'\n\n        response = SingleTemplateNameView.as_view()(self.rf.get('/single_template/'))\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.template_name, ['generic_views/single_template.html'])\n"], "sample_292": ["    def test_https_good_origin_with_wildcard_referer(self):\n        \"\"\"\n        A POST HTTPS request with a good origin that matches a wildcard referer\n        is accepted.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_ORIGIN'] = 'https://sub.example.com'\n        req.META['HTTP_REFERER'] = 'https://sub.example.com/somepage'\n        with self.settings(ALLOWED_HOSTS=['www.example.com'], CSRF_TRUSTED_ORIGINS=['https://*.example.com']):\n            mw = CsrfViewMiddleware(post_form_view)\n            mw.process_request(req)\n            resp = mw.process_view(req, post_form_view, (), {})\n            self.assertIsNone(resp)\n", "    def test_https_good_referer_subdomain_allowed(self):\n        \"\"\"\n        A POST HTTPS request with a referer from a subdomain allowed by\n        CSRF_COOKIE_DOMAIN should be accepted.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_REFERER'] = 'https://sub.example.com/'\n        req.META['SERVER_PORT'] = '443'\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(response)\n", "    def test_https_wildcard_trusted_origin_subdomain_allowed(self):\n        \"\"\"\n        A POST request with an origin that matches a CSRF_TRUSTED_ORIGINS\n        wildcard subdomain is accepted.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_ORIGIN'] = 'https://sub.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), True)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(response)\n        self.assertEqual(mw.allowed_origins_exact, set())\n        self.assertEqual(mw.allowed_origin_subdomains, {'https': ['.example.com']})\n", "    def test_custom_csrf_cookie_name(self):\n        \"\"\"\n        Test that the middleware works with a custom CSRF_COOKIE_NAME setting.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        req.COOKIES['customcsrftoken'] = self._csrf_id_cookie\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_request(req)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        self._check_token_present(resp)\n        csrf_cookie = resp.cookies.get('customcsrftoken')\n        self.assertTrue(csrf_cookie)\n        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n", "    def test_csrf_cookie_secure_flag(self):\n        \"\"\"\n        Ensure the CSRF cookie has the secure flag set when settings.CSRF_COOKIE_SECURE is True.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        with self.settings(CSRF_COOKIE_SECURE=True):\n            mw = CsrfViewMiddleware(token_view)\n            mw.process_view(req, token_view, (), {})\n            resp = mw(req)\n            csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, False)\n            self.assertTrue(csrf_cookie)\n            self.assertTrue(csrf_cookie.secure)\n"], "sample_293": ["    def test_regex_compilation(self):\n        \"\"\"\n        Test that RegexPattern correctly compiles regex patterns and raises\n        an ImproperlyConfigured exception for invalid regex.\n        \"\"\"\n        valid_pattern = RegexPattern(r'^valid_pattern/$')\n        self.assertIsInstance(valid_pattern.regex, re.Pattern)\n\n        invalid_pattern = r'^invalid_pattern/(['\n        with self.assertRaises(ImproperlyConfigured):\n            RegexPattern(invalid_pattern)\n", "    def test_pattern_startswith_slash(self):\n        pattern = RegexPattern(r'^/test/')\n        warnings = pattern.check()\n        self.assertEqual(len(warnings), 1)\n        self.assertEqual(warnings[0].id, 'urls.W002')\n        self.assertIn(\"has a route beginning with a '/'\", warnings[0].msg)\n", "    def test_locale_regex_descriptor_with_string_pattern(self):\n        class MockPattern:\n            _regex = r'^test/$'\n            _regex_dict = {}\n            _compile = re.compile\n\n        instance = MockPattern()\n        descriptor = LocaleRegexDescriptor('_regex')\n        self.assertEqual(descriptor.__get__(instance), re.compile(r'^test/$'))\n", "    def test_check_pattern_startswith_slash(self):\n        # Test when the pattern starts with a slash and APPEND_SLASH is True\n        with override_settings(APPEND_SLASH=True):\n            pattern = RegexPattern(r'^/test/pattern')\n            warnings = pattern.check()\n            self.assertEqual(len(warnings), 1)\n            self.assertEqual(warnings[0].id, \"urls.W002\")\n\n        # Test when the pattern does not start with a slash and APPEND_SLASH is True\n        with override_settings(APPEND_SLASH=True):\n            pattern = RegexPattern(r'^test/pattern')\n            warnings = pattern.check()\n            self.assertEqual(len(warnings), 0)\n\n        # Test when APPEND_SLASH is False\n        with override_settings(APPEND_SLASH=False):\n            pattern = RegexPattern(r'^/test/pattern')\n            warnings = pattern.check()\n            self.assertEqual(len(warnings), 0)\n", "    def test_custom_error_handlers(self):\n        resolver = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse.custom_error_handlers')\n        messages = resolver._check_custom_error_handlers()\n        self.assertEqual(len(messages), 0)\n"], "sample_294": ["    def test_https_invalid_referer(self):\n        \"\"\"\n        A POST HTTPS request with an invalid referer is rejected.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_REFERER'] = 'https://invalid-referer'\n        mw = CsrfViewMiddleware(post_form_view)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(response.status_code, 403)\n        self.assertContains(\n            response,\n            'Referer checking failed - Referer is malformed.',\n            status_code=403,\n        )\n", "    def test_get_token_sets_csrf_cookie(self):\n        \"\"\"\n        If get_token() is called, the CSRF cookie is set.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        token = get_token(req)\n        self.assertIsNotNone(token)\n        self.assertIn('CSRF_COOKIE', req.META)\n        self.assertTrue(req.META.get(\"CSRF_COOKIE_USED\", False))\n        self.assertEqual(len(req.META['CSRF_COOKIE']), CSRF_TOKEN_LENGTH)\n", "    def test_reject_untrusted_origin(self):\n        \"\"\"\n        A POST request with an untrusted origin in CSRF_TRUSTED_ORIGINS is rejected.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_ORIGIN'] = 'https://untrusted.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), False)\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            response = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(response.status_code, 403)\n        msg = REASON_BAD_ORIGIN % req.META['HTTP_ORIGIN']\n        self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % msg)\n", "    def test_bad_origin_subdomain(self):\n        \"\"\"A request with an origin that is a subdomain of an allowed domain is rejected.\"\"\"\n        req = self._get_POST_request_with_token()\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_ORIGIN'] = 'https://subdomain.evil.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), False)\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            response = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(response.status_code, 403)\n        msg = REASON_BAD_ORIGIN % req.META['HTTP_ORIGIN']\n        self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % msg)\n", "    def test_process_request_no_csrf_cookie_https(self):\n        \"\"\"\n        If no CSRF cookie is present in an HTTPS request, the middleware rejects the request.\n        \"\"\"\n        req = self._get_POST_no_csrf_cookie_request()\n        req._is_secure_override = True\n        req.META['SERVER_PORT'] = '443'\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            resp = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(403, resp.status_code)\n        self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % REASON_NO_CSRF_COOKIE)\n"], "sample_295": ["    def test_as_sql(self):\n        lhs = Expression(IntegerField())\n        rhs = Expression(FloatField())\n        combined_expr = CombinedExpression(lhs, Combinable.ADD, rhs)\n        compiler = mock.Mock()\n        connection = mock.Mock()\n        connection.ops.combine_expression = lambda connector, expressions: f\"({expressions[0]} {connector} {expressions[1]})\"\n        compiler.compile.side_effect = lambda expr: (f\"compiled_{expr}\", [])\n        \n        sql, params = combined_expr.as_sql(compiler, connection)\n        self.assertEqual(sql, \"(compiled_<Expression: <__main__.Expression object at\")  # Check if the SQL is correctly formatted\n        self.assertEqual(params, [])  # Check if parameters are correctly set\n", "    def test_as_sqlite_with_decimal_field(self):\n        class MockField:\n                return 'DecimalField'\n        \n        class MockExpression(SQLiteNumericMixin):\n                return 'mock_sql', ['mock_param']\n        \n        expression = MockExpression()\n        expression.output_field = MockField()\n        sql, params = expression.as_sqlite(None, None)\n        self.assertEqual(sql, 'CAST(mock_sql AS NUMERIC)')\n        self.assertEqual(params, ['mock_param'])\n", "    def test_sql_function_call(self):\n        # Testing basic SQL function call with Func\n        class LowerFunc(Func):\n            function = 'LOWER'\n            template = '%(function)s(%(expressions)s)'\n\n        Employee.objects.create(firstname='JOHN', lastname='DOE')\n        Employee.objects.create(firstname='jane', lastname='SMITH')\n\n        lower_firstname = LowerFunc(F('firstname'))\n        employees = Employee.objects.annotate(lowered_firstname=lower_firstname).order_by('lowered_firstname')\n\n        self.assertQuerysetEqual(\n            employees,\n            ['jane', 'JOHN'],\n            lambda e: e.lowered_firstname\n        )\n", "    def test_raw_sql_as_sql(self):\n        raw_sql = RawSQL(\"SELECT * FROM my_table WHERE column = %s\", params=[\"my_value\"])\n        qs = Company.objects.annotate(my_col=raw_sql).values(\"my_col\")\n        self.assertIn(\"SELECT * FROM my_table WHERE column = \", str(qs.query))\n        self.assertIn(\"my_value\", str(qs.query))\n", "    def test_combined_expression_output_field_resolution(self):\n        lhs = Value(10, output_field=IntegerField())\n        rhs = Value(5.5, output_field=FloatField())\n        expr = CombinedExpression(lhs, Combinable.ADD, rhs)\n        resolved_expr = expr.resolve_expression()\n        self.assertIsInstance(resolved_expr.output_field, FloatField)\n        self.assertEqual(str(resolved_expr), \"10 + 5.5\")\n"], "sample_296": ["    def test_store_empty_message(self):\n        \"\"\"\n        Test that storing an empty message does not set a cookie.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Store an empty message.\n        storage.add(constants.INFO, '')\n        storage.update(response)\n\n        # Check that no messages are stored in the cookie.\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n        self.assertNotIn('messages', response.cookies)\n", "    def test_message_serializer(self):\n        \"\"\"\n        Test the MessageSerializer's dumps and loads methods.\n        \"\"\"\n        serializer = CookieStorage(self.get_request()).signer._serializer\n        messages = [\n            Message(constants.INFO, 'test message 1'),\n            Message(constants.WARNING, 'test message 2', extra_tags='extra')\n        ]\n        dumped_messages = serializer.dumps(messages)\n        loaded_messages = serializer.loads(dumped_messages)\n\n        self.assertEqual(len(loaded_messages), 2)\n        self.assertEqual(loaded_messages[0].message, 'test message 1')\n        self.assertEqual(loaded_messages[0].level, constants.INFO)\n        self.assertEqual(loaded_messages[1].message, 'test message 2')\n        self.assertEqual(loaded_messages[1].level, constants.WARNING)\n        self.assertEqual(loaded_messages[1].extra_tags, 'extra')\n", "    def test_empty_message_encoding(self):\n        \"\"\"\n        Test that encoding and decoding an empty message list returns an empty list.\n        \"\"\"\n        storage = self.get_storage()\n        empty_messages = []\n        encoded = storage._encode(empty_messages)\n        decoded = storage._decode(encoded)\n        self.assertEqual(decoded, empty_messages)\n", "    def test_empty_message_encoding(self):\n        \"\"\"\n        Ensure that an empty list of messages is properly encoded and decoded.\n        \"\"\"\n        storage = self.get_storage()\n        messages = []\n        encoded = storage._encode(messages, encode_empty=True)\n        decoded = storage._decode(encoded)\n        self.assertEqual(messages, decoded)\n", "def test_empty_messages(self):\n    \"\"\"\n    Test that storing and retrieving an empty list of messages works correctly.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    \n    # Store an empty list of messages\n    storage._store([], response)\n    \n    # Retrieve messages from the response\n    stored_messages = storage._decode(response.cookies[storage.cookie_name].value)\n    \n    # Assert that the stored messages are an empty list\n    self.assertEqual(stored_messages, [])\n"], "sample_297": ["    def setUpTestData(cls):\n        cls.sql = \"SELECT * FROM queries_note WHERE note = %s\"\n        cls.params = ['n1']\n        cls.raw_query = RawQuery(cls.sql, using=DEFAULT_DB_ALIAS, params=cls.params)\n", "    def test_raw_query_get_columns(self):\n        \"\"\"\n        Test that RawQuery.get_columns() correctly fetches column names.\n        \"\"\"\n        connection = connections[DEFAULT_DB_ALIAS]\n        cursor = connection.cursor()\n        cursor.execute(\"CREATE TABLE test_table (id INT, name VARCHAR(100));\")\n        cursor.execute(\"INSERT INTO test_table (id, name) VALUES (1, 'test');\")\n        raw_query = RawQuery(\"SELECT * FROM test_table\", using=DEFAULT_DB_ALIAS)\n        raw_query._execute_query()\n        columns = raw_query.get_columns()\n        self.assertEqual(columns, ['id', 'name'])\n        cursor.execute(\"DROP TABLE test_table;\")\n", "    def test_query_clone(self):\n        \"\"\"Test that cloning a Query object works correctly.\"\"\"\n        query = Query(Author)\n        cloned_query = query.clone()\n\n        self.assertEqual(query.model, cloned_query.model)\n        self.assertEqual(query.alias_prefix, cloned_query.alias_prefix)\n        self.assertEqual(query.alias_map, cloned_query.alias_map)\n        self.assertEqual(query.select, cloned_query.select)\n        self.assertEqual(query.where.__class__, cloned_query.where.__class__)\n        self.assertEqual(query.group_by, cloned_query.group_by)\n        self.assertEqual(query.order_by, cloned_query.order_by)\n        self.assertEqual(query.low_mark, cloned_query.low_mark)\n        self.assertEqual(query.high_mark, cloned_query.high_mark)\n        self.assertEqual(query.distinct, cloned_query.distinct)\n        self.assertEqual(query.distinct_fields, cloned_query.distinct_fields)\n        self.assertEqual(query.select_for_update, cloned_query.select_for_update)\n        self.assertEqual(query.select_for_update_nowait, cloned_query.select_for_update_nowait)\n        self.assertEqual(query.select_for_update_skip_locked, cloned_query.select_for_update_skip_locked)\n        self.assertEqual(query.select_for_update_of, cloned_query.select_for_update_of)\n        self.assertEqual(query.select_for_no_key_update, cloned_query.select_for_no_key_update)\n        self.assertEqual(query.select_related, cloned_query.select_related)\n        self.assertEqual(query.max_depth, cloned_query.max_depth)\n        self.assertEqual(query.values_select, cloned_query.values_select)\n        self.assertEqual(query.annotations, cloned_query.annotations)\n        self.assertEqual(query.annotation_select_mask, cloned_query.annotation_select_mask)\n        self.assertEqual(query.extra, cloned_query.extra)\n        self.assertEqual(query.extra_select_mask, cloned_query.extra_select_mask)\n        self.assertEqual(query.extra_tables, cloned_query.extra_tables)\n        self.assertEqual(query.extra_order_by, cloned_query.extra_order_by)\n        self.assertEqual(query.deferred_loading, cloned_query.deferred_loading)\n        self.assertEqual(query._filtered_relations, cloned_query._filtered_relations)\n        self.assertEqual(query.explain_query, cloned_query.explain_query)\n        self.assertEqual(query.explain_format, cloned_query.explain_format)\n        self.assertEqual(query.explain_options, cloned_query.explain_options)\n", "    def test_ticket_25000(self):\n        # Test filtering on a complex nested Q-object with multiple OR conditions\n        # involving multiple joins. This ensures the query handles aliasing and\n        # join promotion correctly.\n        a1 = Author.objects.create(name='Author1', num=1)\n        a2 = Author.objects.create(name='Author2', num=2)\n        report1 = Report.objects.create(name='Report1', creator=a1)\n        report2 = Report.objects.create(name='Report2', creator=a2)\n        note1 = Note.objects.create(note='Note1', misc='Misc1')\n        note2 = Note.objects.create(note='Note2', misc='Misc2')\n        ann1 = Annotation.objects.create(name='Ann1', tag=Tag.objects.create(name='Tag1'))\n        ann1.notes.add(note1)\n        ann2 = Annotation.objects.create(name='Ann2', tag=Tag.objects.create(name='Tag2'))\n        ann2.notes.add(note2)\n        \n        q_obj = (\n            Q(report__creator__name='Author1') |\n            Q(annotation__notes__note='Note1') |\n            Q(annotation__tag__name='Tag2')\n        )\n        \n        qs = Author.objects.filter(q_obj).distinct()\n        self.assertEqual(str(qs.query).count('LEFT OUTER JOIN'), 3)\n        self.assertIn(a1, qs)\n        self.assertIn(a2, qs)\n\n        q_obj_nested = (\n            Q(report__creator__name='Author1') &\n            (Q(annotation__notes__note='Note1') | Q(annotation__tag__name='Tag2'))\n        )\n        \n        qs_nested = Author.objects.filter(q_obj_nested).distinct()\n        self.assertEqual(str(qs_nested.query).count('LEFT OUTER JOIN'), 3)\n        self.assertIn(a1, qs_nested)\n        self.assertNotIn(a2, qs_nested)\n", "    def setUp(self):\n        self.MockOpts = namedtuple('MockOpts', ['get_fields'])\n        self.MockField = namedtuple('MockField', ['name', 'attname', 'concrete'])\n"], "sample_298": ["    def test_token_with_changed_password(self):\n        \"\"\"\n        Changing the user's password invalidates the token.\n        \"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_invalid_after_password_change(self):\n        \"\"\"Changing the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        # Change the user's password\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_invalid_timestamp(self):\n        \"\"\"\n        Test that a token with an invalid timestamp is rejected.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        # Manually tamper with the timestamp to make it invalid\n        invalid_token = 'invalidtimestamp-' + tk1.split('-')[1]\n        self.assertIs(p0.check_token(user, invalid_token), False)\n", "    def test_token_with_invalid_timestamp(self):\n        \"\"\"\n        Tokens with tampered timestamps should not validate.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        parts = tk1.split(\"-\")\n        # Tamper with the timestamp part of the token\n        tampered_token = f\"{parts[0][:-1]}0-{parts[1]}\"\n        self.assertIs(p0.check_token(user, tampered_token), False)\n", "    def test_make_token_consistency(self):\n        \"\"\"\n        Ensure that the token generated for a user is consistent if called multiple times\n        within a short period.\n        \"\"\"\n        user = User.objects.create_user('consistencytestuser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        tk2 = p0.make_token(user)\n        self.assertEqual(tk1, tk2)\n"], "sample_299": ["    def test_no_cache_setting(self):\n        \"\"\"\n        Ensure no errors occur if CACHES setting is empty.\n        \"\"\"\n        with self.settings(CACHES={}):\n            self.assertEqual(check_default_cache_is_configured(None), [E001])\n            self.assertEqual(check_cache_location_not_exposed(None), [])\n            self.assertEqual(check_file_based_cache_is_absolute(None), [])\n", "    def test_cache_with_non_filebased_backend(self):\n        \"\"\"\n        Ensure that non-FileBasedCache backends do not raise warnings\n        for relative paths or location conflicts.\n        \"\"\"\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n                'LOCATION': 'relative/path/does/not/matter',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [])\n            self.assertEqual(check_cache_location_not_exposed(None), [])\n", "    def test_multiple_caches(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache_default',\n            },\n            'secondary': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache_secondary',\n            },\n        }):\n            self.assertEqual(check_default_cache_is_configured(None), [])\n            self.assertEqual(check_file_based_cache_is_absolute(None), [])\n            self.assertEqual(check_cache_location_not_exposed(None), [])\n", "def test_non_file_based_cache(self):\n    with self.settings(CACHES={\n        'default': {\n            'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            'LOCATION': 'cache',\n        },\n    }):\n        self.assertEqual(check_file_based_cache_is_absolute(None), [])\n", "def test_non_file_based_cache(self):\n    with self.settings(CACHES={\n        'default': {\n            'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            'LOCATION': 'irrelevant',\n        },\n    }):\n        self.assertEqual(check_file_based_cache_is_absolute(None), [])\n"], "sample_300": ["    def test_query_with_annotation(self):\n        query = Query(Item)\n        query.add_annotation(Count('id'), alias='item_count')\n        where = query.build_where(Q(item_count__gt=2))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, GreaterThan)\n        self.assertEqual(lookup.rhs, 2)\n        self.assertEqual(lookup.lhs.target.output_field, Item._meta.get_field('id').output_field)\n", "def test_rawquery_get_columns(self):\n        # Mock the connection and cursor behavior\n        class MockCursor:\n                self.description = [(\"column1\",), (\"column2\",)]\n\n        class MockConnection:\n            introspection = type('introspection', (), {'identifier_converter': lambda x: x.upper()})\n            cursor = MockCursor\n            ops = type('ops', (), {'adapt_unknown_value': lambda val: val})\n\n        connections['test'] = MockConnection()\n\n        raw_query = RawQuery(\"SELECT * FROM test_table\", \"test\")\n        columns = raw_query.get_columns()\n        self.assertEqual(columns, [\"COLUMN1\", \"COLUMN2\"])\n", "    def test_aggregate_annotation(self):\n        query = Query(Item)\n        query.add_annotation(Count('id'), alias='total_count', is_summary=True, select=True)\n        self.assertIn('total_count', query.annotations)\n        self.assertIsInstance(query.annotations['total_count'], Count)\n        self.assertEqual(query.annotations['total_count'].source_expressions[0].name, 'id')\n", "    def test_subquery_annotation(self):\n        subquery = Query(Item)\n        subquery.add_annotation(Count('id'), alias='item_count')\n        main_query = Query(Ranking)\n        main_query.add_annotation(Exists(subquery), alias='has_items')\n        where = main_query.build_where(Q(has_items=True))\n        exists_condition = where.children[0]\n        self.assertIsInstance(exists_condition, Exists)\n        self.assertEqual(exists_condition.rhs.alias, 'item_count')\n", "def test_raw_query_execution(self):\n    query = RawQuery(\"SELECT * FROM my_table WHERE id = %s\", using=DEFAULT_DB_ALIAS, params=(1,))\n    with self.assertRaises(RuntimeError):\n        query.params = [1, 2, 3]  # Simulate an unexpected params type\n        query._execute_query()\n"], "sample_301": ["    def test_ensure_echo_on(self, mock_tcsetattr, mock_tcgetattr, mock_isatty, mock_termios):\n        # Simulate ECHO being off\n        attributes = [0, 0, 0, 0]\n        mock_tcgetattr.return_value = attributes\n        mock_termios.ECHO = 0x0008\n        attributes[3] = 0\n\n        autoreload.ensure_echo_on()\n\n        # ECHO should be turned on\n        self.assertEqual(attributes[3], 0x0008)\n        mock_tcsetattr.assert_called_once_with(sys.stdin, mock_termios.TCSANOW, attributes)\n", "    def test_ensure_echo_on_disabled(self, mock_signal, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        attrs = [None, None, None, 0]\n        mock_tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        self.assertTrue(attrs[3] & autoreload.termios.ECHO)\n        mock_tcsetattr.assert_called_once()\n", "    def test_trigger_reload_logs_and_exits(self, mock_logger_info, mock_sys_exit):\n        filename = 'changed_file.py'\n        autoreload.trigger_reload(filename)\n        mock_logger_info.assert_called_once_with('%s changed, reloading.', filename)\n        mock_sys_exit.assert_called_once_with(3)\n", "    def test_ensure_echo_on(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        mock_tcgetattr.return_value = [None, None, None, 0]\n        \n        autoreload.ensure_echo_on()\n        \n        mock_tcgetattr.assert_called_once_with(sys.stdin)\n        self.assertEqual(mock_tcsetattr.call_count, 0)\n", "    def test_ensure_echo_on_enabled(self, mock_signal, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        # Mock the tcgetattr to return a list where ECHO is not set.\n        attributes = [0, 0, 0, 0, 0, 0, 0]\n        attributes[3] &= ~termios.ECHO  # Clear the ECHO bit.\n        mock_tcgetattr.return_value = attributes\n\n        autoreload.ensure_echo_on()\n\n        # Check if ECHO bit is set.\n        attributes[3] |= termios.ECHO\n        mock_tcsetattr.assert_called_once_with(sys.stdin, termios.TCSANOW, attributes)\n        mock_signal.assert_called_once_with(signal.SIGTTOU, signal.SIG_IGN)\n"], "sample_302": ["def test_database_client_runshell_sigint_handling(self):\n    original_handler = signal.getsignal(signal.SIGINT)\n    with mock.patch('signal.signal') as mock_signal:\n        mock_signal.side_effect = [None, original_handler]  # Mimic the setting and restoring of handler\n        client = DatabaseClient(connection=connection)\n        with mock.patch.object(DatabaseClient, 'runshell', return_value=None) as mock_runshell:\n            client.runshell([])\n            mock_signal.assert_any_call(signal.SIGINT, signal.SIG_IGN)\n            mock_signal.assert_any_call(signal.SIGINT, original_handler)\n        mock_runshell.assert_called_once_with([])\n", "    def test_settings_to_cmd_args_env_with_full_settings(self):\n        settings_dict = {\n            'HOST': 'localhost',\n            'PORT': 5432,\n            'NAME': 'testdb',\n            'USER': 'testuser',\n            'PASSWORD': 'password123',\n            'OPTIONS': {\n                'passfile': '/path/to/passfile',\n                'service': 'testservice',\n                'sslmode': 'require',\n                'sslrootcert': '/path/to/rootcert',\n                'sslcert': '/path/to/cert',\n                'sslkey': '/path/to/key',\n            },\n        }\n        parameters = ['--no-password']\n\n        expected_args = [\n            'psql',\n            '-U', 'testuser',\n            '-h', 'localhost',\n            '-p', '5432',\n            'testdb',\n            '--no-password',\n        ]\n\n        expected_env = {\n            'PGPASSWORD': 'password123',\n            'PGPASSFILE': '/path/to/passfile',\n            'PGSERVICE': 'testservice',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': '/path/to/rootcert',\n            'PGSSLCERT': '/path/to/cert',\n            'PGSSLKEY': '/path/to/key',\n        }\n\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n        self.assertEqual(args, expected_args)\n        self.assertEqual(env, expected_env)\n", "compilation error", "compilation error", "def test_settings_to_cmd_args_env_with_full_settings(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'testdb',\n        'USER': 'testuser',\n        'PASSWORD': 'password',\n        'OPTIONS': {\n            'passfile': '/path/to/passfile',\n            'service': 'service_name',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/rootcert',\n            'sslcert': '/path/to/cert',\n            'sslkey': '/path/to/key',\n        },\n    }\n    parameters = ['-a', '-b']\n    expected_args = [\n        'psql', '-U', 'testuser', '-h', 'localhost', '-p', '5432', 'testdb', '-a', '-b'\n    ]\n    expected_env = {\n        'PGPASSWORD': 'password',\n        'PGPASSFILE': '/path/to/passfile',\n        'PGSERVICE': 'service_name',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/rootcert',\n        'PGSSLCERT': '/path/to/cert',\n        'PGSSLKEY': '/path/to/key',\n    }\n    args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n    self.assertEqual(args, expected_args)\n    self.assertEqual(env, expected_env)\n"], "sample_303": ["    def test_runshell_with_custom_env(self):\n        custom_env = {'CUSTOM_VAR': 'value'}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=(['some_command'], custom_env),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once_with(['some_command'], env={**os.environ, **custom_env}, check=True)\n", "compilation error", "    def test_runshell_with_custom_env(self):\n        custom_env = {'CUSTOM_ENV_VAR': 'value'}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=(['some_command'], custom_env),\n            ):\n                self.client.runshell(None)\n            expected_env = {**os.environ, **custom_env}\n            run.assert_called_once_with(['some_command'], env=expected_env, check=True)\n", "    def test_runshell_with_custom_env(self):\n        custom_env = {'CUSTOM_VAR': 'value'}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], custom_env),\n            ):\n                self.client.runshell(None)\n            expected_env = {**os.environ, **custom_env}\n            run.assert_called_once_with([], env=expected_env, check=True)\n", "    def test_runshell_with_custom_env(self):\n        custom_env = {'CUSTOM_ENV_VAR': 'value'}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=(['custom_command'], custom_env),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once_with(['custom_command'], env={**os.environ, **custom_env}, check=True)\n"], "sample_304": ["def test_ip_address_validators(self):\n    with self.assertRaises(ValueError):\n        ip_address_validators('invalid_protocol', False)\n\n    with self.assertRaises(ValueError):\n        ip_address_validators('both', True)\n\n    self.assertEqual(\n        ip_address_validators('both', False),\n        ([validate_ipv46_address], _('Enter a valid IPv4 or IPv6 address.'))\n    )\n    self.assertEqual(\n        ip_address_validators('ipv4', False),\n        ([validate_ipv4_address], _('Enter a valid IPv4 address.'))\n    )\n    self.assertEqual(\n        ip_address_validators('ipv6', False),\n        ([validate_ipv6_address], _('Enter a valid IPv6 address.'))\n    )\n", "    def test_int_list_validator_custom_separator(self):\n        validator = int_list_validator(sep=';', allow_negative=True)\n        self.assertIsNone(validator('-1;2;-3'))\n        with self.assertRaises(ValidationError):\n            validator('1,2;3')\n        with self.assertRaises(ValidationError):\n            validator('1;2;3;')\n", "    def test_file_extension_validator_case_insensitivity(self):\n        # File extension validator should treat extensions case-insensitively\n        validator = FileExtensionValidator(allowed_extensions=['jpg', 'png'])\n        self.assertIsNone(validator(ContentFile('image data', name='image.JPG')))\n        self.assertIsNone(validator(ContentFile('image data', name='image.PNG')))\n        self.assertRaises(ValidationError, validator, ContentFile('image data', name='image.TXT'))\n", "    def test_valid_urls(self):\n        valid_urls = [\n            'http://www.djangoproject.com/',\n            'https://www.djangoproject.com/',\n            'ftp://ftp.is.co.za/rfc/rfc1808.txt',\n            'ftps://ftp.is.co.za/rfc/rfc1808.txt',\n            'http://localhost/',\n            'http://127.0.0.1/',\n            'http://[::1]/',  # IPv6\n            'http://[2001:db8::1]/',  # IPv6\n        ]\n        for url in valid_urls:\n            with self.subTest(url=url):\n                try:\n                    URLValidator()(url)\n                except ValidationError:\n                    self.fail(f'URLValidator raised ValidationError unexpectedly for {url}')\n", "    def test_ip_address_validators(self):\n        validators, message = ip_address_validators('both', False)\n        self.assertEqual(validators[0].__name__, 'validate_ipv46_address')\n        self.assertEqual(message, 'Enter a valid IPv4 or IPv6 address.')\n\n        validators, message = ip_address_validators('ipv4', False)\n        self.assertEqual(validators[0].__name__, 'validate_ipv4_address')\n        self.assertEqual(message, 'Enter a valid IPv4 address.')\n\n        validators, message = ip_address_validators('ipv6', False)\n        self.assertEqual(validators[0].__name__, 'validate_ipv6_address')\n        self.assertEqual(message, 'Enter a valid IPv6 address.')\n\n        with self.assertRaises(ValueError):\n            ip_address_validators('ipv4', True)\n\n        with self.assertRaises(ValueError):\n            ip_address_validators('unknown', False)\n"], "sample_305": ["    def test_uuid_exact_lookup(self):\n        # Create a UUID field and a model instance with a specific UUID\n        uuid_value = uuid.uuid4()\n        obj = Publisher.objects.create(name='Test Publisher', num_awards=1, uuid=uuid_value)\n        \n        # Test Exact lookup\n        result = Publisher.objects.filter(uuid__exact=uuid_value)\n        self.assertEqual(result.count(), 1)\n        self.assertEqual(result.first().uuid, uuid_value)\n        ", "    def test_uuid_field_lookups(self):\n        # Test exact match\n        uuid = '12345678123456781234567812345678'\n        obj = UUIDField.objects.create(uuid=uuid)\n        self.assertTrue(UUIDField.objects.filter(uuid__exact=uuid).exists())\n        self.assertTrue(UUIDField.objects.filter(uuid__iexact=uuid).exists())\n\n        # Test contains\n        self.assertTrue(UUIDField.objects.filter(uuid__contains='12345678').exists())\n        self.assertTrue(UUIDField.objects.filter(uuid__icontains='12345678').exists())\n\n        # Test startswith\n        self.assertTrue(UUIDField.objects.filter(uuid__startswith='12345678').exists())\n        self.assertTrue(UUIDField.objects.filter(uuid__istartswith='12345678').exists())\n\n        # Test endswith\n        self.assertTrue(UUIDField.objects.filter(uuid__endswith='12345678').exists())\n        self.assertTrue(UUIDField.objects.filter(uuid__iendswith='12345678').exists())\n        \n        # Test invalid UUID lookup\n        with self.assertRaises(ValueError):\n            UUIDField.objects.filter(uuid__exact='invalid_uuid')\n", "    def test_exact_lookup(self):\n        self.assertTrue(Author.objects.filter(name__exact='Adrian Holovaty').exists())\n        self.assertFalse(Author.objects.filter(name__exact='Nonexistent Author').exists())\n", "    def test_exact_lookup(self):\n        # Testing Exact lookup\n        qs = Book.objects.filter(id__exact=self.b1.id)\n        self.assertEqual(list(qs), [self.b1])\n", "    def test_integer_field_float_rounding(self):\n        \"\"\"\n        Test IntegerField lookups with floating point values to ensure they\n        round correctly.\n        \"\"\"\n        # Create an author with a floating point age\n        author = Author.objects.create(name='Float Age Author', age=35.6)\n        \n        # Test GreaterThanOrEqual lookup with a float value\n        qs = Author.objects.filter(age__gte=35.6)\n        self.assertIn(author, qs)\n        \n        # Test LessThan lookup with a float value\n        qs = Author.objects.filter(age__lt=35.7)\n        self.assertIn(author, qs)\n        \n        # Test that exact integer values do not match with the float value\n        qs = Author.objects.filter(age__exact=36)\n        self.assertNotIn(author, qs)\n"], "sample_306": ["    def test_invalid_parse_duration(self):\n        invalid_inputs = [\n            'not a duration',\n            'P5Y10M',  # ISO 8601 format with year and month, not supported\n            '5 days 25:00:00',  # Invalid time component\n            'P5W5D',  # ISO 8601 format with weeks and days combined, not supported\n            'P5DT25H',  # ISO 8601 format with invalid hour\n            '5.5:30',  # Invalid hours\n            '5:60:30',  # Invalid minutes\n            '5:30:60',  # Invalid seconds\n            '5:30:30.1000000',  # Too many microseconds\n            '5:30:30,1000000',  # Too many microseconds with comma\n        ]\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_invalid_duration_formats(self):\n        invalid_durations = [\n            'P-',  # incomplete ISO 8601\n            'PT-',  # incomplete ISO 8601 time component\n            'P1DT-',  # incomplete negative duration\n            'P4Y1D',  # unsupported year in ISO 8601\n            'P4M1D',  # unsupported month in ISO 8601\n            'P4W1D',  # unsupported week in ISO 8601\n            '4 days 25:00:00',  # invalid hour\n            '4 days 00:60:00',  # invalid minute\n            '4 days 00:00:60',  # invalid second\n            'invalid format',  # completely invalid format\n        ]\n        for duration in invalid_durations:\n            with self.subTest(duration=duration):\n                self.assertIsNone(parse_duration(duration))\n", "    def test_parse_time_invalid_formats(self):\n        # Invalid time formats\n        invalid_times = [\n            '24:00',         # Invalid hour\n            '12:60',         # Invalid minute\n            '12:30:60',      # Invalid second\n            '12:30:30:30',   # Extra component\n            '12:30:30.1234567',  # Too many microseconds\n            '12:30:30,1234567',  # Too many microseconds (comma)\n            '12:30:30.',     # Trailing dot\n            '12:30:30,',     # Trailing comma\n            '12:',           # Missing minute\n            '12:30:',        # Missing second\n            '12:30:30.Z',    # Invalid character\n        ]\n        for source in invalid_times:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_time(source))\n", "    def test_invalid_durations(self):\n        invalid_values = [\n            '',\n            'P',\n            'PT',\n            'P-',\n            'PT-',\n            '4D',\n            'T5H',\n            'P4YT',\n            'P4DT-',\n            'P4DT5',\n            'PT5M-',\n            'PT5S-',\n            'P4Y2M',\n            '5H',\n            '5M',\n            '5S',\n            '5',\n            '1D 5H',\n            '1 day 25:00:00',\n            '1 day -25:00:00',\n        ]\n        for value in invalid_values:\n            with self.subTest(value=value):\n                self.assertIsNone(parse_duration(value))\n", "    def test_parse_duration_invalid(self):\n        invalid_values = [\n            'not a duration',\n            '5 minutes',\n            'P-4D',\n            'PT-5H',\n            '5days',\n            'P4DT-1H',\n            '4 10:15:30:45',\n            '10:15:30.400.500',\n            'T15:30',\n            'PT1.2H',\n            '5:60:30',\n            '15:60',\n            '25:30',\n            '1:30:30',\n        ]\n        for value in invalid_values:\n            with self.subTest(value=value):\n                self.assertIsNone(parse_duration(value))\n"], "sample_307": ["def test_iso_8601_year_number(self):\n        # Testing ISO 8601 year number matching the ISO week number (o)\n        dt = datetime(2019, 12, 30)  # ISO year 2020, week 1\n        self.assertEqual(dateformat.format(dt, 'o'), '2020')\n", "def test_rfc_5322_format(self):\n        dt = datetime(2021, 5, 15, 12, 45, 30)\n        self.assertEqual(\n            dateformat.format(dt, 'r'),\n            'Sat, 15 May 2021 12:45:30 +0000'\n        )\n", "def test_iso_week_year(self):\n    # ISO week year is different from the Gregorian year if the week overlaps two years.\n    iso_year_date = datetime(2020, 1, 1)\n    self.assertEqual(dateformat.format(iso_year_date, 'o'), '2020')\n    iso_year_date = datetime(2019, 12, 31)\n    self.assertEqual(dateformat.format(iso_year_date, 'o'), '2020')\n\n    iso_year_date = datetime(2021, 1, 1)\n    self.assertEqual(dateformat.format(iso_year_date, 'o'), '2020')\n    iso_year_date = datetime(2021, 12, 31)\n    self.assertEqual(dateformat.format(iso_year_date, 'o'), '2021')\n", "    def test_iso_8601_year_format(self):\n        dt = datetime(2023, 10, 5)\n        self.assertEqual(dateformat.format(dt, 'o'), '2023')\n", "    def test_iso_year_number(self):\n        test_date = datetime(2023, 1, 2)  # ISO week 1 of 2023\n        self.assertEqual(dateformat.format(test_date, 'o'), '2023')\n"], "sample_308": ["    def test_format_with_escape_characters(self):\n        dt = datetime(2023, 10, 5, 14, 45, 30)\n        self.assertEqual(dateformat.format(dt, r'\\\\Y \\i\\s Y'), r'\\Y is 2023')\n        self.assertEqual(dateformat.format(dt, r'\\\\H\\H \\a\\n\\d \\m\\i\\\\n\\u\\\\t\\e\\\\s'), r'\\HH and minutes')\n", "def test_timezone_name(self):\n        tz = get_fixed_timezone(330)  # +05:30 timezone\n        dt = datetime(2023, 3, 10, 12, 0, 0, tzinfo=tz)\n        self.assertEqual(dateformat.format(dt, 'e'), 'IST')\n", "    def test_iso8601_year(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30)\n        self.assertEqual(dateformat.format(dt, 'o'), '2009')\n", "    def test_iso_8601_format(self):\n        dt = datetime(2023, 10, 4, 14, 23, 45, 678901)\n        self.assertEqual(dateformat.format(dt, 'c'), '2023-10-04T14:23:45.678901')\n", "    def test_iso_year_number(self):\n        dates = [\n            (datetime(2005, 1, 1), '2004'),\n            (datetime(2005, 1, 2), '2004'),\n            (datetime(2005, 1, 3), '2004'),\n            (datetime(2005, 1, 4), '2005'),\n            (datetime(2005, 12, 31), '2005'),\n            (datetime(2006, 1, 1), '2005'),\n            (datetime(2006, 1, 2), '2006'),\n        ]\n        for dt, expected in dates:\n            with self.subTest(dt=dt):\n                self.assertEqual(dateformat.format(dt, 'o'), expected)\n"], "sample_309": ["    def test_valid_dates(self):\n        valid_dates = [\n            'Sun, 06 Nov 1994 08:49:37 GMT',\n            'Sunday, 06-Nov-94 08:49:37 GMT',\n            'Sun Nov  6 08:49:37 1994',\n        ]\n        for date in valid_dates:\n            with self.subTest(date=date):\n                self.assertIsNotNone(parse_http_date_safe(date))\n", "    def test_valid_date(self):\n        date_str = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        expected_timestamp = parse_http_date(date_str)\n        self.assertEqual(parse_http_date_safe(date_str), expected_timestamp)\n", "    def test_valid_date(self):\n        self.assertEqual(\n            parse_http_date_safe('Sun, 06 Nov 1994 08:49:37 GMT'),\n            784111777\n        )\n    ", "    def test_valid_dates(self):\n        valid_dates = [\n            'Sun, 06 Nov 1994 08:49:37 GMT',\n            'Sunday, 06-Nov-94 08:49:37 GMT',\n            'Sun Nov  6 08:49:37 1994',\n        ]\n        expected_timestamps = [\n            784111777,\n            784111777,\n            784111777,\n        ]\n        for date_str, expected_timestamp in zip(valid_dates, expected_timestamps):\n            with self.subTest(date_str=date_str):\n                self.assertEqual(parse_http_date_safe(date_str), expected_timestamp)\n", "    def test_valid_date(self):\n        date_str = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        parsed = parse_http_date_safe(date_str)\n        self.assertEqual(\n            datetime.fromtimestamp(parsed, timezone.utc),\n            datetime(1994, 11, 6, 8, 49, 37, tzinfo=timezone.utc),\n        )\n"], "sample_310": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_get_view_func_valid_view(self):\n        view = \"django.contrib.admindocs.views.BaseAdminDocsView\"\n        view_func = views.ViewDetailView._get_view_func(view)\n        self.assertIsNotNone(view_func)\n        self.assertTrue(inspect.isclass(view_func))\n", "    def test_extract_views_from_urlpatterns(self):\n        urlpatterns = [\n            path('test/view/', views.BaseAdminDocsView.as_view(), name='test-view'),\n            path('test/include/', include([\n                path('nested/', views.BookmarkletsView.as_view(), name='nested-view')\n            ]))\n        ]\n        result = extract_views_from_urlpatterns(urlpatterns)\n        self.assertEqual(len(result), 2)\n        self.assertEqual(result[0][0], views.BaseAdminDocsView.as_view())\n        self.assertEqual(result[0][1], '/test/view/')\n        self.assertEqual(result[0][3], 'test-view')\n        self.assertEqual(result[1][0], views.BookmarkletsView.as_view())\n        self.assertEqual(result[1][1], '/test/include/nested/')\n        self.assertEqual(result[1][3], 'nested-view')\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n"], "sample_312": ["    def test_add_with_squash_false(self):\n        node = Node(['a', 'b'], 'OR')\n        node.add('c', 'OR', squash=False)\n        self.assertEqual(len(node), 3)\n        self.assertIn('c', node)\n        self.assertEqual(node.connector, 'OR')\n        self.assertEqual(str(node), \"(OR: a, b, c)\")\n", "def test_add_squash_false(self):\n    node = Node(['a', 'b'], 'OR')\n    self.assertEqual(node.add('c', 'OR', squash=False), 'c')\n    self.assertEqual(node, Node(['a', 'b', 'c'], 'OR'))\n", "def test_add_squash_false(self):\n        node = Node(['a', 'b'], 'AND')\n        added_data = 'c'\n        # When squash is False, the added_data should be appended to children\n        self.assertEqual(node.add(added_data, 'AND', squash=False), added_data)\n        self.assertEqual(node.children, ['a', 'b', 'c'])\n        # Make sure the original connector and negated properties remain unchanged\n        self.assertEqual(node.connector, 'AND')\n        self.assertFalse(node.negated)\n", "compilation error", "def test_add_squash(self):\n        # Test squashing behavior when adding nodes\n        node = Node(['a', 'b'], 'AND')\n        other_node = Node(['c', 'd'], 'AND')\n        # When the connectors are the same and squash is True, children should be merged\n        node.add(other_node, 'AND', squash=True)\n        self.assertEqual(str(node), \"(AND: a, b, c, d)\")\n\n        # When squash is False, other_node should just be added as a child\n        node = Node(['a', 'b'], 'AND')\n        other_node = Node(['c', 'd'], 'AND')\n        node.add(other_node, 'AND', squash=False)\n        self.assertEqual(str(node), \"(AND: a, b, (AND: c, d))\")\n"], "sample_311": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUp(self):\n        self.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        self.client.force_login(self.superuser)\n        self.site = AdminSite(name='test_admin')\n", "    def test_add_action(self):\n        \"\"\"\n        Ensure that we can add a custom action to the AdminSite and that it appears in the list of actions.\n        \"\"\"\n            pass\n\n        site.add_action(custom_action, 'custom_action')\n        self.assertIn('custom_action', site._global_actions)\n        self.assertEqual(site._global_actions['custom_action'], custom_action)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='admin', password='secret', email='admin@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.s1 = Section.objects.create(name='Test section')\n"], "sample_313": ["    def test_template_changed_ignored_django_path(self, mock_is_django_path):\n        template_path = Path(__file__).parent / 'templates' / 'django_template.html'\n        self.assertIsNone(autoreload.template_changed(None, template_path))\n        mock_is_django_path.assert_called_once_with(str(template_path))\n", "    def test_get_template_directories_no_django_backend(self, mock_engines_all):\n        mock_engines_all.return_value = [\n            mock.MagicMock(spec=DjangoTemplates, engine=mock.MagicMock(template_loaders=[])),\n            mock.MagicMock(),  # Non-DjangoTemplates backend\n        ]\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            set()\n        )\n", "    def test_template_dirs_with_cached_loader(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            set()\n        )\n", "    def test_get_template_directories_with_custom_loader(self, mock_engines_all):\n        mock_loader_with_get_dirs = mock.Mock()\n        mock_loader_with_get_dirs.get_dirs.return_value = ['custom_template_dir']\n        mock_loader_without_get_dirs = mock.Mock()\n        del mock_loader_without_get_dirs.get_dirs\n\n        mock_backend = mock.Mock(spec=DjangoTemplates)\n        mock_backend.engine.template_loaders = [\n            mock_loader_with_get_dirs, \n            mock_loader_without_get_dirs\n        ]\n        mock_backend.engine.dirs = []\n        mock_engines_all.return_value = [mock_backend]\n\n        expected_directories = {Path.cwd() / 'custom_template_dir'}\n        self.assertSetEqual(autoreload.get_template_directories(), expected_directories)\n", "    def test_get_template_directories_with_custom_loader(self, mock_get_dirs):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates_extra',\n            }\n        )\n        mock_get_dirs.assert_called_once()\n"], "sample_314": ["    def test_unicode_ci_compare_equal(self):\n        # Test that _unicode_ci_compare correctly identifies equal strings\n        self.assertTrue(_unicode_ci_compare('test', 'TEST'))\n        self.assertTrue(_unicode_ci_compare('stra\u00dfe', 'strasse'))  # \u00df and ss are considered equivalent\n", "    def test_username_field_normalization(self):\n        # Test that the UsernameField normalizes its input\n        field = UsernameField()\n        value = 'test\u2126'  # U+2126 OHM SIGN\n        normalized_value = field.clean(value)\n        self.assertEqual(normalized_value, 'test\u03a9')  # U+03A9 GREEK CAPITAL LETTER OMEGA\n", "    def test_unicode_ci_compare(self):\n        self.assertTrue(_unicode_ci_compare('abc', 'ABC'))\n        self.assertTrue(_unicode_ci_compare('Stra\u00dfe', 'strasse'))  # German sharp S\n        self.assertFalse(_unicode_ci_compare('Python', 'Java'))\n        self.assertTrue(_unicode_ci_compare('\u03a3', '\u03c3'))  # Greek Sigma in uppercase and lowercase\n        self.assertTrue(_unicode_ci_compare('\u2126', '\u03a9'))  # Ohm sign vs Greek capital letter omega\n", "    def test_unicode_ci_compare_equal(self):\n        self.assertTrue(_unicode_ci_compare('Test', 'test'))\n        self.assertTrue(_unicode_ci_compare('TeSt', 'tEsT'))\n        self.assertTrue(_unicode_ci_compare('\u03a9', '\u2126'))  # Greek Capital Letter Omega vs. Ohm Sign\n", "    def test_unicode_ci_compare(self):\n        self.assertTrue(_unicode_ci_compare('A', 'a'))\n        self.assertTrue(_unicode_ci_compare('\u2126', '\u03a9'))  # U+2126 OHM SIGN and U+03A9 GREEK CAPITAL LETTER OMEGA\n        self.assertFalse(_unicode_ci_compare('A', 'B'))\n        self.assertTrue(_unicode_ci_compare('Stra\u00dfe', 'strasse'))  # German sharp s (\u00df) equivalence\n        self.assertTrue(_unicode_ci_compare('CAF\u00c9', 'cafe'))  # Case and accent insensitive comparison\n"], "sample_315": ["    def test_process_request_sets_language_code(self):\n        request = RequestFactory().get('/en/some_path/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        with translation.override('en'):\n            middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'en')\n", "    def test_default_language_activation(self):\n        request = RequestFactory().get('/foo/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        with translation.override('nl'):\n            middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'en')\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.middleware = LocaleMiddleware(lambda req: HttpResponse())\n", "    def test_process_request_sets_language_code(self):\n        request = RequestFactory().get('/nl/some-url/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'nl')\n", "    def test_invalid_language_code(self):\n        # Test with an invalid language code\n        response = self.client.get('/xx/account/register/', HTTP_ACCEPT_LANGUAGE='xx')\n        self.assertEqual(response.status_code, 404)\n"], "sample_316": ["    def test_image_file_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, 800)  # Replace with actual width of test.png\n            self.assertEqual(image_file.height, 600)  # Replace with actual height of test.png\n", "    def test_imagefile_width_height(self):\n        \"\"\"\n        Test that ImageFile correctly returns the width and height of an image.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            with mock.patch('PIL.ImageFile.Parser.image', new_callable=mock.PropertyMock) as mock_image:\n                mock_image.return_value.size = (640, 480)\n                self.assertEqual(image_file.width, 640)\n                self.assertEqual(image_file.height, 480)\n", "    def test_image_file_dimensions(self):\n        \"\"\"\n        Test the width and height properties of the ImageFile class.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, 540)\n            self.assertEqual(image_file.height, 405)\n", "    def test_image_file_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            img_file = images.ImageFile(fh)\n            self.assertEqual(img_file.width, 540)\n            self.assertEqual(img_file.height, 405)\n", "    def test_image_file_properties(self):\n        \"\"\"\n        Test the width and height properties of the ImageFile class.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            with mock.patch('django.core.files.images.get_image_dimensions', return_value=(800, 600)):\n                self.assertEqual(image.width, 800)\n                self.assertEqual(image.height, 600)\n"], "sample_317": ["    def test_enclosure_creation(self):\n        \"\"\"\n        Test the creation of an RSS enclosure and ensure its attributes are correctly set.\n        \"\"\"\n        enclosure = Enclosure(url='http://example.com/podcast.mp3', length='12345', mime_type='audio/mpeg')\n        self.assertEqual(enclosure.url, 'http://example.com/podcast.mp3')\n        self.assertEqual(enclosure.length, '12345')\n        self.assertEqual(enclosure.mime_type, 'audio/mpeg')\n", "    def test_rfc2822_date_function(self):\n        \"\"\"\n        Test the rfc2822_date function to ensure proper formatting of dates.\n        \"\"\"\n        date = datetime.datetime(2023, 10, 1, 12, 0, 0)\n        formatted_date = rfc2822_date(date)\n        self.assertEqual(formatted_date, 'Sun, 01 Oct 2023 12:00:00 -0000')\n\n        date = datetime.date(2023, 10, 1)\n        formatted_date = rfc2822_date(date)\n        self.assertEqual(formatted_date, 'Sun, 01 Oct 2023 00:00:00 -0000')\n", "    def test_rss2_feed_no_items(self):\n        \"\"\"\n        Test if RSS feed is correctly generated when no items are present.\n        \"\"\"\n        response = self.client.get('/syndication/rss2/no-items/')\n        doc = minidom.parseString(response.content)\n\n        feed_elem = doc.getElementsByTagName('rss')\n        self.assertEqual(len(feed_elem), 1)\n        feed = feed_elem[0]\n        self.assertEqual(feed.getAttribute('version'), '2.0')\n        \n        chan_elem = feed.getElementsByTagName('channel')\n        self.assertEqual(len(chan_elem), 1)\n        chan = chan_elem[0]\n\n        self.assertChildNodes(\n            chan, [\n                'title', 'link', 'description', 'language', 'lastBuildDate',\n                'atom:link', 'ttl', 'copyright', 'category',\n            ]\n        )\n        self.assertEqual(len(chan.getElementsByTagName('item')), 0)\n", "    def test_rfc2822_date(self):\n        \"\"\"\n        Test rfc2822_date function for various datetime inputs.\n        \"\"\"\n        dt_naive = datetime.datetime(2023, 10, 1, 12, 0, 0)\n        dt_aware = timezone.make_aware(datetime.datetime(2023, 10, 1, 12, 0, 0), TZ)\n        dt_date = datetime.date(2023, 10, 1)\n\n        self.assertEqual(rfc2822_date(dt_naive), 'Sun, 01 Oct 2023 12:00:00 -0000')\n        self.assertEqual(rfc2822_date(dt_aware), 'Sun, 01 Oct 2023 12:00:00 -0000')\n        self.assertEqual(rfc2822_date(dt_date), 'Sun, 01 Oct 2023 00:00:00 -0000')\n", "def test_rfc2822_date_naive_datetime(self):\n    \"\"\"\n    Test rfc2822_date function with naive datetime.\n    \"\"\"\n    naive_datetime = datetime.datetime(2023, 10, 1, 10, 0, 0)\n    formatted_date = rfc2822_date(naive_datetime)\n    self.assertEqual(formatted_date, 'Sun, 01 Oct 2023 10:00:00 -0000')\n"], "sample_318": ["    def test_compile_valid_regex(self):\n        pattern = RegexPattern(r'^valid/$')\n        compiled_regex = pattern._compile(r'^valid/$')\n        self.assertTrue(compiled_regex.match('valid/'))\n", "    def test_resolver_match_repr_partial_func(self):\n            return HttpResponse(\"Test view\")\n\n        partial_func = functools.partial(test_view)\n        match = ResolverMatch(partial_func, args=(), kwargs={}, url_name='partial-func')\n        expected_repr = (\n            \"ResolverMatch(func=functools.partial(<function test_view at 0x\"\n        )\n        self.assertTrue(repr(match).startswith(expected_repr), repr(match))\n", "    def test_urlpattern_repr(self):\n        \"\"\"\n        Test __repr__ method of URLPattern to ensure it returns the expected string.\n        \"\"\"\n        pattern = URLPattern(RegexPattern(r'^test/$'), views.empty_view, name='test-view')\n        self.assertEqual(repr(pattern), \"<URLPattern '^test/$' [name='test-view']>\")\n", "    def test_locale_regex_descriptor(self):\n        class TestPattern:\n            _regex = r'^test/$'\n            regex = LocaleRegexDescriptor('_regex')\n\n                return re.compile(regex)\n        \n        pattern_instance = TestPattern()\n        compiled_regex = pattern_instance.regex\n        self.assertTrue(compiled_regex.match('test/'))\n", "    def test_urlpattern_resolve_with_default_args(self):\n        \"\"\"\n        Test URLPattern resolve with default arguments.\n        \"\"\"\n        pattern = URLPattern(RegexPattern(r'^default_args/(?P<arg1>[0-9]+)/$'), views.empty_view, default_args={'arg2': 'default'})\n        match = pattern.resolve('/default_args/123/')\n        self.assertIsNotNone(match)\n        self.assertEqual(match.func, views.empty_view)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'arg1': '123', 'arg2': 'default'})\n        "], "sample_319": ["    def test_remove_model_with_index(self):\n        \"\"\"Tests deletion of model with index.\"\"\"\n        book_with_index = ModelState(\n            \"otherapp\",\n            \"Book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n            {\n                \"indexes\": [\n                    models.Index(fields=[\"title\"], name=\"book_title_idx\"),\n                ]\n            },\n        )\n        changes = self.get_changes([book_with_index, self.author_empty], [self.author_empty])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"RemoveIndex\", \"DeleteModel\"])\n        self.assertOperationAttributes(\n            changes, \"otherapp\", 0, 0, model_name=\"book\", name=\"book_title_idx\"\n        )\n        self.assertOperationAttributes(\n            changes, \"otherapp\", 0, 1, name=\"Book\"\n        )\n", "    def test_deep_deconstruct_regex_object(self):\n        \"\"\"\n        Test deep deconstruction of RegexObject instances.\n        \"\"\"\n        regex = re.compile(r'^test$')\n        regex_object = RegexObject(regex)\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pattern\", models.CharField(max_length=200, default=regex_object)),\n                ],\n            )\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pattern\", models.CharField(max_length=200, default=regex_object)),\n                ],\n            )\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 0)\n\n        # Change the regex pattern\n        new_regex = re.compile(r'^new_test$')\n        new_regex_object = RegexObject(new_regex)\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pattern\", models.CharField(max_length=200, default=new_regex_object)),\n                ],\n            )\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationFieldAttributes(\n            changes, \"testapp\", 0, 0, default=new_regex_object\n        )\n", "    def test_alter_field_non_schema(self):\n        \"\"\"\n        Tests that altering a field with non-schema changes does not generate a migration.\n        \"\"\"\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200, verbose_name=\"Author's name\")),\n                ],\n            )\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200, verbose_name=\"Writer's name\")),\n                ],\n            )\n        ]\n        changes = self.get_changes(before, after)\n        self.assertEqual(len(changes), 0)\n", "def test_alter_model_managers_with_empty_managers(self):\n    \"\"\"\n    Changing the model managers to an empty list should be detected and handled.\n    \"\"\"\n    # Initial state with managers.\n    initial_state = ModelState(\n        \"otherapp\",\n        \"Pony\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        managers=[\n            (\"food_qs\", FoodQuerySet.as_manager()),\n            (\"food_mgr\", FoodManager(\"a\", \"b\")),\n        ],\n    )\n    # Modified state with no managers.\n    modified_state = ModelState(\n        \"otherapp\",\n        \"Pony\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        managers=[],\n    )\n    changes = self.get_changes([initial_state], [modified_state])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterModelManagers\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"pony\")\n    self.assertEqual(changes[\"otherapp\"][0].operations[0].managers, [])\n", "    def test_resolve_dependency_non_swappable(self):\n        \"\"\"\n        Test that _resolve_dependency correctly resolves non-swappable dependencies.\n        \"\"\"\n        autodetector = MigrationAutodetector(None, None)\n        dependency = (\"myapp\", \"MyModel\", None, True)\n        resolved_dependency, is_swappable = autodetector._resolve_dependency(dependency)\n        self.assertEqual(resolved_dependency, dependency)\n        self.assertFalse(is_swappable)\n"], "sample_320": ["    def test_create_model_with_references_model(self):\n        \"\"\"\n        Tests the CreateModel operation with references_model functionality.\n        \"\"\"\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n                (\"rider\", models.ForeignKey(\"test_crmrm.Rider\", models.CASCADE)),\n            ],\n        )\n        self.assertEqual(operation.describe(), \"Create model Pony\")\n        self.assertEqual(operation.migration_name_fragment, \"pony\")\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmrm\", new_state)\n        self.assertEqual(new_state.models[\"test_crmrm\", \"pony\"].name, \"Pony\")\n        self.assertEqual(len(new_state.models[\"test_crmrm\", \"pony\"].fields), 3)\n        # Test the database alteration\n        self.assertTableNotExists(\"test_crmrm_pony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_crmrm\", editor, project_state, new_state)\n        self.assertTableExists(\"test_crmrm_pony\")\n        # Test references_model\n        self.assertTrue(operation.references_model(\"Rider\", \"test_crmrm\"))\n        self.assertFalse(operation.references_model(\"Horse\", \"test_crmrm\"))\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crmrm\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_crmrm_pony\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"CreateModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"fields\", \"name\"])\n", "    def test_delete_model_with_constraints(self):\n        \"\"\"\n        Test DeleteModel operation respects constraints.\n        \"\"\"\n        project_state = self.set_up_test_model(\n            \"test_del_model_constraint\",\n            constraints=[\n                models.CheckConstraint(check=models.Q(pink__gt=2), name=\"pink_gt_2\"),\n                models.UniqueConstraint(fields=[\"pink\"], name=\"unique_pink\"),\n            ],\n        )\n        operation = migrations.DeleteModel(\"Pony\")\n        self.assertEqual(operation.describe(), \"Delete model Pony\")\n        self.assertEqual(operation.migration_name_fragment, \"delete_pony\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_del_model_constraint\", new_state)\n        self.assertNotIn((\"test_del_model_constraint\", \"pony\"), new_state.models)\n        self.assertTableExists(\"test_del_model_constraint_pony\")\n        self.assertConstraintExists(\"test_del_model_constraint_pony\", \"pink_gt_2\")\n        self.assertConstraintExists(\"test_del_model_constraint_pony\", \"unique_pink\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_del_model_constraint\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_del_model_constraint_pony\")\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_del_model_constraint\", editor, new_state, project_state)\n        self.assertTableExists(\"test_del_model_constraint_pony\")\n        self.assertConstraintExists(\"test_del_model_constraint_pony\", \"pink_gt_2\")\n        self.assertConstraintExists(\"test_del_model_constraint_pony\", \"unique_pink\")\n", "    def test_create_model_with_duplicate_field_name_in_m2m(self):\n        with self.assertRaisesMessage(\n            ValueError, \"Found duplicate value ponies in CreateModel fields argument.\"\n        ):\n            migrations.CreateModel(\n                \"Stable\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"ponies\", models.ManyToManyField(\"Pony\", related_name=\"stables\")),\n                    (\"ponies\", models.ManyToManyField(\"Pony\", related_name=\"stables\")),\n                ],\n            )\n", "    def test_delete_model_mti_with_fk(self):\n        \"\"\"\n        Tests the DeleteModel operation on a multi-table inheritance setup with a foreign key.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_dlmtimfk\", mti_model=True, related_model=True)\n        # Add a foreign key to the related model pointing to the inherited model\n        operation = migrations.AddField(\n            \"Rider\",\n            \"shetland_pony\",\n            models.ForeignKey(\"test_dlmtimfk.ShetlandPony\", models.CASCADE),\n        )\n        # Apply the AddField operation\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_dlmtimfk\", new_state)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_dlmtimfk\", editor, project_state, new_state)\n        \n        # Test the DeleteModel operation\n        operation = migrations.DeleteModel(\"ShetlandPony\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_dlmtimfk\", new_state)\n        self.assertIn((\"test_dlmtimfk\", \"rider\"), project_state.models)\n        self.assertIn((\"test_dlmtimfk\", \"shetlandpony\"), project_state.models)\n        self.assertNotIn((\"test_dlmtimfk\", \"shetlandpony\"), new_state.models)\n        # Test the database alteration\n        self.assertTableExists(\"test_dlmtimfk_shetlandpony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_dlmtimfk\", editor, project_state, new_state)\n        self.assertTableExists(\"test_dlmtimfk_pony\")\n        self.assertTableNotExists(\"test_dlmtimfk_shetlandpony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_dlmtimfk\", editor, new_state, project_state)\n        self.assertTableExists(\"test_dlmtimfk_shetlandpony\")\n        self.assertTableExists(\"test_dlmtimfk_pony\")\n", "    def test_alter_model_table_with_m2m_fields(self):\n        \"\"\"\n        Tests the AlterModelTable operation when the model has ManyToMany fields.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_altertmtblm2m\", second_model=True)\n        # Add a ManyToMany field\n        operation = migrations.AddField(\n            \"Pony\", \"stables\", models.ManyToManyField(\"Stable\", related_name=\"ponies\")\n        )\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_altertmtblm2m\", new_state)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_altertmtblm2m\", editor, project_state, new_state)\n        \n        # Rename the table\n        operation = migrations.AlterModelTable(\"Pony\", \"test_altertmtblm2m_pony_renamed\")\n        self.assertEqual(\n            operation.describe(), \"Rename table for Pony to test_altertmtblm2m_pony_renamed\"\n        )\n        new_state = new_state.clone()\n        operation.state_forwards(\"test_altertmtblm2m\", new_state)\n        \n        self.assertTableExists(\"test_altertmtblm2m_pony_stables\")\n        self.assertTableNotExists(\"test_altertmtblm2m_pony_renamed_stables\")\n        \n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_altertmtblm2m\", editor, project_state, new_state)\n        \n        self.assertTableNotExists(\"test_altertmtblm2m_pony_stables\")\n        self.assertTableExists(\"test_altertmtblm2m_pony_renamed_stables\")\n        \n        # Test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_altertmtblm2m\", editor, new_state, project_state)\n        \n        self.assertTableExists(\"test_altertmtblm2m_pony_stables\")\n        self.assertTableNotExists(\"test_altertmtblm2m_pony_renamed_stables\")\n"], "sample_321": ["def test_process_request_invalid_session_key(self):\n    \"\"\"\n    If CSRF_USE_SESSIONS is enabled and the session key is invalid, a new\n    CSRF token should be generated.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    req.session[CSRF_SESSION_KEY] = 'invalid_session_key'\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_request(req)\n    new_token = get_token(req)\n    self.assertIsNotNone(new_token)\n    self.assertNotEqual(new_token, 'invalid_session_key')\n", "def test_process_response_sets_vary_cookie_header(self):\n    \"\"\"\n    The process_response() method sets the 'Vary: Cookie' header to the response \n    if the CSRF token is used in the request.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    req.META[\"CSRF_COOKIE_USED\"] = True\n    mw = CsrfViewMiddleware(lambda req: HttpResponse())\n    resp = mw.process_response(req, HttpResponse())\n    self.assertIn('Cookie', resp.get('Vary', ''))\n", "    def test_rotate_token_resets_csrf_cookie(self):\n        \"\"\"\n        rotate_token() should reset the CSRF token in the request and mark the\n        csrf_cookie_needs_reset attribute to True.\n        \"\"\"\n        req = self._get_GET_csrf_cookie_request()\n        mw = CsrfViewMiddleware(lambda req: HttpResponse())\n        mw.process_request(req)\n        \n        # Verify initial token\n        initial_token = req.META['CSRF_COOKIE']\n        \n        # Rotate the token\n        rotate_token(req)\n        \n        # Verify token has been rotated\n        self.assertNotEqual(initial_token, req.META['CSRF_COOKIE'])\n        self.assertTrue(req.csrf_cookie_needs_reset)\n        \n        # Process response and verify new token is set in cookies\n        resp = mw.process_response(req, HttpResponse())\n        new_token = req.META['CSRF_COOKIE']\n        self.assertEqual(resp.cookies[settings.CSRF_COOKIE_NAME].value, new_token)\n        self.assertNotEqual(initial_token, new_token)\n", "def test_mask_and_unmask_cipher_secret(self):\n    \"\"\"\n    Test masking and unmasking of a CSRF secret to ensure that the original\n    secret can be correctly retrieved.\n    \"\"\"\n    original_secret = 'a' * CSRF_SECRET_LENGTH\n    masked_token = _mask_cipher_secret(original_secret)\n    self.assertEqual(len(masked_token), CSRF_TOKEN_LENGTH)\n    unmasked_secret = _unmask_cipher_token(masked_token)\n    self.assertEqual(unmasked_secret, original_secret)\n", "def test_process_response_sets_vary_header(self):\n    \"\"\"\n    The CsrfViewMiddleware should set the Vary header to 'Cookie' when setting\n    the CSRF cookie.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    self.assertIn('Cookie', resp.get('Vary', ''))\n"], "sample_322": ["    def test_create_project_state_with_applied_migrations(self):\n        \"\"\"\n        Tests creating a project state with applied migrations.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Migrate forwards\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        executor.loader.build_graph()\n        state = executor._create_project_state(with_applied_migrations=True)\n        # Verify that the state includes the applied migration\n        self.assertIn((\"migrations\", \"author\"), state.models)\n        self.assertIn((\"migrations\", \"tribble\"), state.models)\n        # Migrate back to clean up the database\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n", "    def test_migrate_all_forwards_with_progress_callback(self):\n        \"\"\"\n        Test the _migrate_all_forwards method with a progress callback.\n        \"\"\"\n        state = {\"callbacks\": []}\n\n            state[\"callbacks\"].append((phase, args))\n        \n        executor = MigrationExecutor(connection, progress_callback=fake_progress_callback)\n        \n        initial_state = executor._create_project_state(with_applied_migrations=True)\n        plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n        full_plan = executor.migration_plan(executor.loader.graph.leaf_nodes(), clean_start=True)\n\n        final_state = executor._migrate_all_forwards(initial_state, plan, full_plan, fake=False, fake_initial=False)\n        \n        self.assertEqual(\n            state[\"callbacks\"],\n            [\n                (\"render_start\", ()),\n                (\"render_success\", ()),\n                (\"apply_start\", (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False)),\n                (\"apply_success\", (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False)),\n                (\"apply_start\", (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False)),\n                (\"apply_success\", (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False)),\n            ],\n        )\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_book\")\n        \n        # Cleanup\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n", "    def test_conflicting_migrations(self):\n        \"\"\"\n        Test handling of conflicting migrations where two branches of migrations\n        conflict and need to be resolved.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Initial plan to apply migrations up to the conflict point\n        plan = executor.migration_plan([(\"migrations\", \"0002_conflict1\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n                (executor.loader.graph.nodes[\"migrations\", \"0002_conflict1\"], False),\n            ],\n        )\n        # Apply up to the conflict point\n        executor.migrate([(\"migrations\", \"0002_conflict1\")])\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_book\")\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n\n        # Now attempt to apply the conflicting migration\n        plan = executor.migration_plan([(\"migrations\", \"0002_conflict2\")])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_conflict2\"], False),\n            ],\n        )\n        # Apply the conflicting migration\n        executor.migrate([(\"migrations\", \"0002_conflict2\")])\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_book\")\n        self.assertTableExists(\"migrations_conflict_resolution\")\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n\n        # Clean up by migrating back to zero\n        plan = executor.migration_plan([(\"migrations\", None)])\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_conflict2\"], True),\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n            ],\n        )\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        self.assertTableNotExists(\"migrations_conflict_resolution\")\n", "    def test_apply_migration_with_fake_initial(self):\n        \"\"\"\n        Test applying a migration with the fake_initial option set to True.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        \n            if migration and migration.name == \"0001_initial\":\n                self.assertTrue(fake)\n        \n        executor.progress_callback = fake_storer\n        # Ensure the tables do not exist before running the migration\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n        # Apply the initial migration with fake_initial=True\n        executor.migrate([(\"migrations\", \"0001_initial\")], fake_initial=True)\n        # Ensure the tables are created\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_tribble\")\n        # Unapply the migration to clean up\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_tribble\")\n", "    def test_check_replacements(self):\n        \"\"\"\n        Tests that check_replacements correctly marks replacement migrations as applied.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        recorder = executor.recorder\n        # Simulate the state where the replaced migrations are partially applied: 0001 is applied, 0002 is not.\n        recorder.record_applied(\"migrations\", \"0001_initial\")\n        executor.loader.build_graph()\n\n        # Initially, the replacement migration should not be marked as applied.\n        self.assertNotIn(\n            (\"migrations\", \"0001_squashed_0002\"),\n            recorder.applied_migrations(),\n        )\n\n        # Apply the remaining migration\n        executor.migrate([(\"migrations\", \"0002_second\")], fake=True)\n\n        # Run the check_replacements method\n        executor.check_replacements()\n\n        # The replacement migration should now be marked as applied\n        self.assertIn(\n            (\"migrations\", \"0001_squashed_0002\"),\n            recorder.applied_migrations(),\n        )\n\n        # Clean up by unapplying all migrations\n        executor.migrate([(\"migrations\", None)])\n        self.assertNotIn(\n            (\"migrations\", \"0001_squashed_0002\"),\n            recorder.applied_migrations(),\n        )\n"], "sample_323": ["    def test_detect_soft_applied_with_proxy_and_unmanaged_models(self):\n        \"\"\"\n        Tests detection of initial migrations already having been applied\n        with proxy and unmanaged models.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Were the tables there before?\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_unmanaged\")\n        self.assertTableNotExists(\"migrations_proxy\")\n        # Run it normally\n        self.assertEqual(\n            executor.migration_plan([(\"migrations\", \"0001_initial\")]),\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            ],\n        )\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        # Are the tables there now?\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_unmanaged\")\n        self.assertTableNotExists(\"migrations_proxy\")  # Proxy models don't create tables\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Fake-reverse that\n        executor.migrate([(\"migrations\", None)], fake=True)\n        # Are the tables still there?\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_unmanaged\")\n        self.assertTableNotExists(\"migrations_proxy\")  # Proxy models don't create tables\n        # Make sure that was faked\n        executor.migrate([(\"migrations\", None)], fake=True)\n        # Finally, migrate forwards; this should fake-apply our initial migration\n        executor.loader.build_graph()\n        self.assertEqual(\n            executor.migration_plan([(\"migrations\", \"0001_initial\")]),\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            ],\n        )\n        executor.migrate([(\"migrations\", \"0001_initial\")], fake_initial=True)\n        # And migrate back to clean up the database\n        executor.loader.build_graph()\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_unmanaged\")\n", "    def test_mixed_plan_with_dependencies(self):\n        \"\"\"\n        Tests that migrations with dependencies handle mixed plans correctly.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Apply initial migration\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_book\")\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Apply second migration\n        executor.migrate([(\"migrations\", \"0002_second\")])\n        self.assertTableExists(\"migrations_tribble\")\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        # Create a mixed plan with dependencies\n        plan = executor.migration_plan([\n            (\"migrations\", None),\n            (\"migrations2\", \"0001_initial\"),\n        ])\n        msg = (\n            'Migration plans with both forwards and backwards migrations are '\n            'not supported. Please split your migration process into separate '\n            'plans of only forwards OR backwards migrations.'\n        )\n        with self.assertRaisesMessage(InvalidMigrationPlan, msg) as cm:\n            executor.migrate(None, plan)\n        self.assertEqual(\n            cm.exception.args[1],\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], True),\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n                (executor.loader.graph.nodes[\"migrations2\", \"0001_initial\"], False),\n            ],\n        )\n        # Cleanup\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        self.assertTableNotExists(\"migrations_tribble\")\n", "    def test_migration_plan_clean_start(self):\n        \"\"\"\n        Tests the migration_plan method with clean_start=True.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Apply the initial migration so that we have an applied migration\n        executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n        executor.loader.build_graph()\n\n        # Plan migration with clean_start=True\n        plan = executor.migration_plan([(\"migrations\", \"0002_second\")], clean_start=True)\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n            ],\n        )\n\n        # Plan migration without clean_start\n        plan = executor.migration_plan([(\"migrations\", \"0002_second\")], clean_start=False)\n        self.assertEqual(\n            plan,\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n            ],\n        )\n", "    def test_apply_migration(self):\n        \"\"\"\n        Tests the apply_migration function to ensure that it correctly applies a migration\n        and records it as applied.\n        \"\"\"\n        state = {\"applied\": False}\n        executor = MigrationExecutor(connection)\n\n        class ApplyMigration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name=\"TestModel\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=100)),\n                    ],\n                ),\n            ]\n\n            if phase == \"apply_success\":\n                state[\"applied\"] = True\n\n        # Use a progress callback to monitor the application of the migration\n        executor.progress_callback = apply_callback\n\n        # Ensure the migration table does not exist before applying migration\n        self.assertTableNotExists(\"testmodel\")\n\n        # Apply the migration\n        migration = ApplyMigration(\"0001_initial\", \"test_app\")\n        executor.apply_migration(ProjectState(), migration)\n\n        # Check if the migration was applied\n        self.assertTrue(state[\"applied\"])\n        self.assertTableExists(\"testmodel\")\n\n        # Check if the migration was recorded\n        self.assertIn(\n            (\"test_app\", \"0001_initial\"),\n            executor.recorder.applied_migrations(),\n        )\n\n        # Cleanup by reverting the migration and deleting the table\n        executor.unapply_migration(ProjectState(), migration)\n        self.assertTableNotExists(\"testmodel\")\n        executor.recorder.record_unapplied(\"test_app\", \"0001_initial\")\n", "    def test_apply_migration(self):\n        \"\"\"\n        Tests the application of an individual migration.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n        state = executor.loader.project_state()\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n\n        # Apply the migration\n        state = executor.apply_migration(state, migration)\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_book\")\n\n        # Ensure that the migration is recorded as applied\n        self.assertIn((\"migrations\", \"0001_initial\"), executor.recorder.applied_migrations())\n\n        # Unapply the migration\n        state = executor.unapply_migration(state, migration)\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n\n        # Ensure that the migration is recorded as unapplied\n        self.assertNotIn((\"migrations\", \"0001_initial\"), executor.recorder.applied_migrations())\n"], "sample_324": ["    def test_reject_request_reason(self):\n        \"\"\"\n        If RejectRequest is raised, the reason is properly handled.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        mw = CsrfViewMiddleware(post_form_view)\n\n        class MockRejectRequest(Exception):\n                self.reason = reason\n\n            raise MockRejectRequest(\"Mock rejection reason\")\n\n        mw._check_token = mock_check_token\n\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            resp = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(403, resp.status_code)\n        self.assertEqual(cm.records[0].getMessage(), 'Forbidden (Mock rejection reason): ')\n", "    def test_https_good_referer_without_trailing_slash(self):\n        \"\"\"\n        A POST HTTPS request with a good referer without a trailing slash is accepted.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_REFERER'] = 'https://www.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "    def test_rotate_token_resets_token(self):\n        \"\"\"\n        rotate_token() should reset the CSRF token in the request.\n        \"\"\"\n        req = self._get_GET_csrf_cookie_request()\n        original_token = get_token(req)\n        rotate_token(req)\n        new_token = get_token(req)\n        self.assertNotEqual(original_token, new_token, \"CSRF token was not rotated\")\n", "    def test_invalid_origin_in_request(self):\n        \"\"\"\n        A request with an invalid origin format is rejected.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_ORIGIN'] = 'invalid-origin'\n        mw = CsrfViewMiddleware(post_form_view)\n        self._check_referer_rejects(mw, req)\n        self.assertIs(mw._origin_verified(req), False)\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            response = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(response.status_code, 403)\n        msg = REASON_BAD_ORIGIN % req.META['HTTP_ORIGIN']\n        self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % msg)\n", "    def test_rotate_token_functionality(self):\n        \"\"\"\n        Test that rotate_token changes the CSRF token in use for a request.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        # Initial token setup\n        initial_token = get_token(req)\n        self.assertIsNotNone(initial_token)\n        # Rotate token\n        rotate_token(req)\n        new_token = req.META.get(\"CSRF_COOKIE\")\n        self.assertIsNotNone(new_token)\n        self.assertNotEqual(initial_token, new_token, \"Token should be rotated and hence different\")\n"], "sample_325": ["    def test_boundfield_css_classes(self):\n        class CustomForm(Form):\n            first_name = CharField()\n            last_name = CharField(required=False)\n\n        form = CustomForm()\n        self.assertEqual(form['first_name'].css_classes(), '')\n        self.assertEqual(form['first_name'].css_classes(extra_classes='foo bar'), 'foo bar')\n        self.assertEqual(form['last_name'].css_classes(), '')\n        self.assertEqual(form['last_name'].css_classes(extra_classes='foo bar'), 'foo bar')\n\n        # Adding error_css_class and required_css_class to form\n        form.error_css_class = 'error'\n        form.required_css_class = 'required'\n        form = CustomForm(data={'first_name': ''})\n\n        self.assertEqual(form['first_name'].css_classes(), 'required error')\n        self.assertEqual(form['first_name'].css_classes(extra_classes='foo bar'), 'foo bar required error')\n        self.assertEqual(form['last_name'].css_classes(), '')\n", "def test_boundwidget_rendering(self):\n    class SampleForm(Form):\n        choices = ChoiceField(choices=[('1', 'One'), ('2', 'Two')], widget=RadioSelect)\n\n    form = SampleForm()\n    bound_widget = list(form['choices'])[0]\n\n    # Test rendering of the bound widget\n    rendered_tag = bound_widget.tag()\n    self.assertHTMLEqual(\n        rendered_tag,\n        '<input type=\"radio\" name=\"choices\" value=\"1\" id=\"id_choices_0\" required>'\n    )\n\n    # Test rendering of the bound widget with wrap_label=True\n    rendered_tag_with_label = bound_widget.tag(wrap_label=True)\n    self.assertHTMLEqual(\n        rendered_tag_with_label,\n        '<label for=\"id_choices_0\"><input type=\"radio\" name=\"choices\" value=\"1\" id=\"id_choices_0\" required> One</label>'\n    )\n\n    # Test template name propagation\n    self.assertEqual(bound_widget.template_name, 'django/forms/widgets/radio.html')\n    \n    # Test id_for_label and choice_label properties\n    self.assertEqual(bound_widget.id_for_label, 'id_choices_0')\n    self.assertEqual(bound_widget.choice_label, 'One')\n\n    # Ensure custom template name is used if provided in widget data\n    bound_widget.data['template_name'] = 'custom_template.html'\n    self.assertEqual(bound_widget.template_name, 'custom_template.html')\n", "def test_boundfield_label_tag_custom_attrs(self):\n    \"\"\"\n    Ensure BoundField label_tag can accept custom attributes and override default ones.\n    \"\"\"\n    class CustomForm(Form):\n        name = CharField()\n\n    form = CustomForm()\n    boundfield = form['name']\n\n    # Test with additional custom attributes\n    self.assertHTMLEqual(\n        boundfield.label_tag(attrs={'class': 'my-class', 'id': 'my-id'}),\n        '<label for=\"id_name\" class=\"my-class\" id=\"my-id\">Name:</label>'\n    )\n\n    # Test with overriding default attribute\n    self.assertHTMLEqual(\n        boundfield.label_tag(attrs={'for': 'custom_id'}),\n        '<label for=\"custom_id\">Name:</label>'\n    )\n\n    # Test with multiple attributes\n    self.assertHTMLEqual(\n        boundfield.label_tag(attrs={'class': 'my-class', 'data-custom': 'custom-value'}),\n        '<label for=\"id_name\" class=\"my-class\" data-custom=\"custom-value\">Name:</label>'\n    )\n\n    # Test without attributes\n    self.assertHTMLEqual(\n        boundfield.label_tag(attrs={}),\n        '<label for=\"id_name\">Name:</label>'\n    )\n", "def test_boundfield_as_widget_with_custom_widget(self):\n    # Test that a custom widget passed to as_widget is used instead of the field's widget.\n    class CustomWidget(TextInput):\n            return format_html('<input type=\"text\" name=\"{}\" value=\"{}\" class=\"custom-widget\">', name, value)\n\n    class CustomForm(Form):\n        name = CharField()\n\n    form = CustomForm()\n    rendered_widget = form['name'].as_widget(widget=CustomWidget())\n    self.assertHTMLEqual(rendered_widget, '<input type=\"text\" name=\"name\" value=\"\" class=\"custom-widget\">')\n\n    # Also test with initial data\n    form = CustomForm(initial={'name': 'John Doe'})\n    rendered_widget = form['name'].as_widget(widget=CustomWidget())\n    self.assertHTMLEqual(rendered_widget, '<input type=\"text\" name=\"name\" value=\"John Doe\" class=\"custom-widget\">')\n", "    def test_boundfield_build_widget_attrs(self):\n        class CustomForm(Form):\n            name = CharField()\n            age = IntegerField()\n\n        form = CustomForm()\n        field = form['name']\n        \n        # Test required attribute\n        attrs = field.build_widget_attrs({'class': 'custom-class'})\n        self.assertEqual(attrs, {'class': 'custom-class', 'required': True})\n        \n        # Test disabled attribute\n        form.fields['age'].disabled = True\n        field = form['age']\n        attrs = field.build_widget_attrs({'class': 'custom-class'})\n        self.assertEqual(attrs, {'class': 'custom-class', 'disabled': True})\n        \n        # Test required attribute with use_required_attribute set to False\n        form.use_required_attribute = False\n        field = form['name']\n        attrs = field.build_widget_attrs({'class': 'custom-class'})\n        self.assertEqual(attrs, {'class': 'custom-class'})\n"], "sample_326": ["    def test_avoid_wrapping(self):\n        tests = (\n            ('Hello World', 'Hello\\xa0World'),\n            ('This is a test', 'This\\xa0is\\xa0a\\xa0test'),\n            ('Preserve non-breaking spaces', 'Preserve\\xa0non-breaking\\xa0spaces'),\n            ('Multiple   spaces', 'Multiple\\xa0\\xa0\\xa0spaces'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n            (\"Avoid wrapping\", \"Avoid\\xa0wrapping\"),\n            (\"Multiple  spaces\", \"Multiple\\xa0\\xa0spaces\"),\n            (\"Text with\\nnewlines\", \"Text\\xa0with\\nnewlines\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n            (\"Avoid text wrapping\", \"Avoid\\xa0text\\xa0wrapping\"),\n            (\"Already\\xa0wrapped text\", \"Already\\xa0wrapped\\xa0text\"),\n            (lazystr(\"Lazy text wrapping\"), \"Lazy\\xa0text\\xa0wrapping\"),\n            (\"Multiple   spaces\", \"Multiple\\xa0\\xa0\\xa0spaces\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        tests = (\n            ('This is a test', 'This\\xa0is\\xa0a\\xa0test'),\n            ('Hello World!', 'Hello\\xa0World!'),\n            ('Multiple    spaces', 'Multiple\\xa0\\xa0\\xa0\\xa0spaces'),\n            ('Tab\\tseparated', 'Tab\\tseparated'),  # Tabs should remain unchanged\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        tests = (\n            ('Hello world', 'Hello\\xa0world'),\n            ('This is a test', 'This\\xa0is\\xa0a\\xa0test'),\n            ('Avoid wrapping', 'Avoid\\xa0wrapping'),\n            ('Multiple  spaces', 'Multiple\\xa0\\xa0spaces'),\n            ('End with space ', 'End\\xa0with\\xa0space\\xa0'),\n            (' Start with space', '\\xa0Start\\xa0with\\xa0space'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.check_output(avoid_wrapping, value, output)\n"], "sample_327": ["def test_custom_error_message(self):\n    field = JSONField(error_messages={'invalid': 'Custom error message.'})\n    with self.assertRaisesMessage(ValidationError, 'Custom error message.'):\n        field.clean('{some badly formed: json}')\n", "def test_invalid_with_custom_error_messages(self):\n    custom_error_messages = {\n        'invalid': 'Custom invalid JSON message.',\n    }\n    field = JSONField(error_messages=custom_error_messages)\n    with self.assertRaisesMessage(ValidationError, 'Custom invalid JSON message.'):\n        field.clean('{some badly formed: json}')\n", "def test_bound_data(self):\n    field = JSONField()\n    self.assertEqual(field.bound_data('{\"a\": \"b\"}', None), {'a': 'b'})\n    self.assertEqual(field.bound_data(None, {'a': 'b'}), {'a': 'b'})\n    self.assertEqual(field.bound_data('{invalid: json}', {'a': 'b'}), InvalidJSONInput('{invalid: json}'))\n", "    def test_custom_error_messages(self):\n        field = JSONField(error_messages={'invalid': 'Custom invalid message.'})\n        with self.assertRaisesMessage(ValidationError, 'Custom invalid message.'):\n            field.clean('{invalid json}')\n", "def test_booleanfield(self):\n        field = BooleanField()\n        self.assertTrue(field.clean('true'))\n        self.assertFalse(field.clean('false'))\n        self.assertFalse(field.clean('0'))\n        self.assertTrue(field.clean('1'))\n        self.assertTrue(field.clean(True))\n        self.assertFalse(field.clean(False))\n        with self.assertRaisesMessage(ValidationError, 'This field is required.'):\n            field.clean(None)\n\n        field = BooleanField(disabled=True, required=True)\n        self.assertIsNone(field.clean(None))\n        self.assertFalse(field.has_changed(False, None))\n"], "sample_328": ["def test_update_multiple_objects(self):\n    notes = [\n        Note.objects.create(note='note-1', misc='misc-1'),\n        Note.objects.create(note='note-2', misc='misc-2'),\n        Note.objects.create(note='note-3', misc='misc-3'),\n    ]\n    for note in notes:\n        note.note = 'updated-' + note.note\n        note.misc = 'updated-' + note.misc\n    rows_updated = Note.objects.bulk_update(notes, ['note', 'misc'])\n    self.assertEqual(rows_updated, 3)\n    self.assertCountEqual(\n        Note.objects.values_list('note', flat=True),\n        ['updated-note-1', 'updated-note-2', 'updated-note-3']\n    )\n    self.assertCountEqual(\n        Note.objects.values_list('misc', flat=True),\n        ['updated-misc-1', 'updated-misc-2', 'updated-misc-3']\n    )\n", "    def test_ordering_with_bulk_update(self):\n        # Test if bulk_update respects queryset ordering\n        notes_ordered = Note.objects.order_by('-id')\n        for note in notes_ordered:\n            note.note = 'ordered-%s' % note.id\n        with self.assertNumQueries(1):\n            Note.objects.bulk_update(notes_ordered, ['note'])\n        self.assertEqual(\n            list(Note.objects.order_by('-id').values_list('note', flat=True)),\n            ['ordered-%s' % note.id for note in notes_ordered]\n        )\n", "    def test_update_with_annotations(self):\n        Note.objects.bulk_create([\n            Note(note='Note %s' % i, misc='Misc %s' % i)\n            for i in range(5)\n        ])\n        notes = list(Note.objects.annotate(note_length=Lower('note')).all())\n        for note in notes:\n            note.misc = 'Updated %s' % note.note_length\n        Note.objects.bulk_update(notes, ['misc'])\n        self.assertCountEqual(\n            Note.objects.values_list('misc', flat=True),\n            ['Updated note %s' % i for i in range(5)]\n        )\n", "    def test_bulk_update_with_empty_queryset(self):\n        # Test bulk_update with an empty queryset should result in no updates and no errors.\n        notes = Note.objects.filter(note='non_existent_note')\n        self.assertEqual(notes.count(), 0)\n        rows_updated = notes.bulk_update(notes, ['note'])\n        self.assertEqual(rows_updated, 0)\n", "    def setUp(self):\n        self.note1 = Note.objects.create(note='note1', misc='misc1')\n        self.note2 = Note.objects.create(note='note2', misc='misc2')\n"], "sample_329": ["    def test_serialize_function_with_class_instance(self):\n        class MyClass:\n                return \"method call\"\n\n        instance = MyClass()\n        self.assertSerializedResultEqual(\n            instance.method,\n            (\"migrations.test_writer.WriterTests.test_serialize_function_with_class_instance.<locals>.MyClass.method\",\n             {'import migrations.test_writer'})\n        )\n", "    def test_serialize_partial_bound_method(self):\n        instance = TestModel1()\n        partial_bound_method = functools.partial(instance.upload_to)\n        with self.assertRaisesMessage(ValueError, \"Cannot serialize function\"):\n            self.serialize_round_trip(partial_bound_method)\n", "    def test_serialize_function_type(self):\n            return \"sample\"\n        \n        self.assertSerializedEqual(sample_function)\n        self.assertSerializedResultEqual(\n            sample_function,\n            (\"migrations.test_writer.sample_function\", {\"import migrations.test_writer\"})\n        )\n", "    def test_serialize_function_type_serializer_class_method(self):\n        class MyClass:\n            @classmethod\n                return \"class method\"\n\n        self.assertSerializedResultEqual(\n            MyClass.my_class_method,\n            (\"migrations.test_writer.MyClass.my_class_method\", {\"import migrations.test_writer\"})\n        )\n", "    def test_serialize_deconstructable(self):\n        class CustomClass:\n                return (\n                    '%s.%s' % (self.__class__.__module__, self.__class__.__name__),\n                    [],\n                    {'arg1': 'value1'}\n                )\n        \n        instance = CustomClass()\n        string, imports = MigrationWriter.serialize(instance)\n        self.assertEqual(string, \"migrations.test_writer.CustomClass(arg1='value1')\")\n        self.assertEqual(imports, {'import migrations.test_writer'})\n        self.serialize_round_trip(instance)\n"], "sample_330": ["    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2023-10-05'), datetime.date(2023, 10, 5))\n        self.assertIsNone(typecast_date(''))\n", "    def test_typecast_date(self):\n        # Test valid date string\n        self.assertEqual(typecast_date(\"2023-10-05\"), datetime.date(2023, 10, 5))\n        # Test None input\n        self.assertIsNone(typecast_date(None))\n        # Test empty string\n        self.assertIsNone(typecast_date(\"\"))\n", "    def test_typecast_date(self):\n        self.assertEqual(typecast_date(\"2023-10-01\"), datetime.date(2023, 10, 1))\n        self.assertEqual(typecast_date(\"\"), None)\n", "    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2023-10-01'), datetime.date(2023, 10, 1))\n        self.assertEqual(typecast_date(''), None)\n        self.assertEqual(typecast_date(None), None)\n", "    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2023-10-01'), datetime.date(2023, 10, 1))\n        self.assertIsNone(typecast_date(''))\n        self.assertIsNone(typecast_date(None))\n"], "sample_331": ["    def test_invalid_durations(self):\n        invalid_values = [\n            '1 day 25:61:61',    # Invalid time component\n            'P1Y2M3DT4H5M60S',   # Invalid seconds component\n            'P-1D',              # Invalid negative period\n            'T5H',               # Missing 'P' prefix\n            '1 1:1:1:1',         # Too many time components\n            '10:10:10:10',       # Invalid time format\n            'P1DT',              # Missing time component after 'T'\n            'P0.5D0.5H',         # Invalid mixed fraction format\n            'P0.5DT0.5H',        # Invalid fraction in both date and time\n            'P1DT1H1M1S1',       # Extra digit in seconds\n            '00:00:00:00',       # Too many zero components\n            '1day',              # Invalid format with \"day\"\n            'P1Y2M3',            # Missing 'D' in ISO 8601 format\n            'P1Y2M3D4H',         # Missing 'T' in ISO 8601 format\n            '-P1DT-1H',          # Invalid negative format in ISO 8601\n        ]\n        for source in invalid_values:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_parse_invalid_duration(self):\n        invalid_durations = [\n            'P4Y', 'P4M', 'P4W', 'P-1D', 'PT5H5M', 'PT5M5S', 'PT5H5S',\n            'P5D5H', 'PT', 'P', '5H', '5M', '5S', 'T5H', 'T5M', 'T5S',\n            'P5H5M', 'P5M5S', 'P5H5S', '5D5H', 'P5DT5M', 'P5DT5S',\n            'PT5M5H', 'PT5S5H', 'PT5M5D', 'PT5S5D', 'P5DT5H5M', 'P5DT5H5S',\n            'P5DT5M5S', 'P5H5M5S', 'P5D5H5M5S'\n        ]\n        for invalid_duration in invalid_durations:\n            with self.subTest(invalid_duration=invalid_duration):\n                self.assertIsNone(parse_duration(invalid_duration))\n", "    def test_parse_invalid_date_format(self):\n        # Ensure invalid date formats return None\n        invalid_dates = [\n            '2023-13-01',  # Invalid month\n            '2023-02-29',  # Invalid day\n            '2023-00-10',  # Invalid month\n            '2023-01-00',  # Invalid day\n            '2023-01-32',  # Invalid day\n            'abcd-ef-gh',  # Non-numeric\n            '23-01-01',    # Year not fully specified\n        ]\n        for invalid_date in invalid_dates:\n            with self.subTest(invalid_date=invalid_date):\n                self.assertIsNone(parse_date(invalid_date))\n", "    def test_parse_invalid_duration(self):\n        invalid_values = (\n            '4 days 25:00:00',  # Invalid time in duration\n            'P4Y5D',  # Unsupported ISO 8601 format (years)\n            'P-4D',  # Invalid ISO 8601 negative format\n            'P4DT-1H',  # Invalid ISO 8601 mixed negative format\n            'P5',  # Incomplete ISO 8601 format\n            '4 days 0:60:00',  # Invalid minute in duration\n            '4 days 0:00:60',  # Invalid second in duration\n            '10:61',  # Invalid minute without hours\n            'PT',  # Incomplete ISO 8601 format\n        )\n        for value in invalid_values:\n            with self.subTest(value=value):\n                self.assertIsNone(parse_duration(value))\n", "    def test_parse_date_edge_cases(self):\n        # Edge cases for date parsing\n        self.assertEqual(parse_date('0001-01-01'), date(1, 1, 1))\n        self.assertEqual(parse_date('9999-12-31'), date(9999, 12, 31))\n        self.assertIsNone(parse_date('2012-02-30'))  # Invalid date\n        self.assertIsNone(parse_date('2012-13-01'))  # Invalid month\n        self.assertIsNone(parse_date('2012-00-10'))  # Invalid month\n        self.assertIsNone(parse_date('2012-01-00'))  # Invalid day\n        self.assertIsNone(parse_date('2012-01-32'))  # Invalid day\n        self.assertIsNone(parse_date('abcd-ef-gh'))  # Non-numeric date\n"], "sample_332": ["def test_formset_with_invalid_initial_data(self):\n        \"\"\"\n        A FormSet with invalid initial data should correctly report errors.\n        \"\"\"\n        initial = [{'choice': 'Calexico', 'votes': 'invalid'}]\n        formset = self.make_choiceformset(initial=initial)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [{'votes': ['Enter a whole number.']}])\n", "    def test_management_form_clean(self):\n        \"\"\"\n        Test the ManagementForm clean method to ensure TOTAL_FORM_COUNT and INITIAL_FORM_COUNT\n        defaults to 0 when not present in cleaned_data.\n        \"\"\"\n        class TestManagementForm(ManagementForm):\n                cleaned_data = super().clean()\n                self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 0)\n                self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n                return cleaned_data\n        \n        data = {\n            'form-TOTAL_FORMS': 'invalid',\n            'form-INITIAL_FORMS': 'invalid',\n        }\n        form = TestManagementForm(data, auto_id=False, prefix='form')\n        form.full_clean()\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(form.cleaned_data[INITIAL_FORM_COUNT], 0)\n", "    def test_initial_form_count(self):\n        \"\"\"\n        Tests the initial form count calculation for different scenarios.\n        \"\"\"\n        # Case 1: No initial data, expected initial count is 0\n        formset = ChoiceFormSet()\n        self.assertEqual(formset.initial_form_count(), 0)\n\n        # Case 2: With initial data, expected initial count is length of initial data\n        initial = [{'choice': 'Choice1', 'votes': 1}, {'choice': 'Choice2', 'votes': 2}]\n        formset = ChoiceFormSet(initial=initial)\n        self.assertEqual(formset.initial_form_count(), 2)\n\n        # Case 3: With initial data and bound formset\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '2',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '2',\n            'choices-0-choice': 'Choice1',\n            'choices-0-votes': '1',\n            'choices-1-choice': 'Choice2',\n            'choices-1-votes': '2',\n        }\n        formset = ChoiceFormSet(data=data, initial=initial)\n        self.assertEqual(formset.initial_form_count(), 2)\n", "    def test_management_form_cleaned_data(self):\n        \"\"\"\n        Test that ManagementForm cleaned_data contains default values for\n        TOTAL_FORM_COUNT and INITIAL_FORM_COUNT even if management form data\n        is invalid.\n        \"\"\"\n        data = {\n            'choices-TOTAL_FORMS': 'invalid',  # invalid total form count\n            'choices-INITIAL_FORMS': '0',  # valid initial form count\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(formset.is_valid(), False)\n        self.assertEqual(formset.management_form.cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(formset.management_form.cleaned_data[INITIAL_FORM_COUNT], 0)\n", "    def test_max_num_with_initial_data_above_absolute_max(self):\n        \"\"\"\n        The number of initial forms plus extra forms should not exceed absolute_max.\n        \"\"\"\n        initial = [{'choice': 'Choice1', 'votes': 10}, {'choice': 'Choice2', 'votes': 20}]\n        data = {\n            'choices-TOTAL_FORMS': '4',  # the number of forms rendered\n            'choices-INITIAL_FORMS': '2',  # the number of forms with initial data\n            'choices-MIN_NUM_FORMS': '0',  # min number of forms\n            'choices-MAX_NUM_FORMS': '0',  # max number of forms\n            'choices-0-choice': 'Choice1',\n            'choices-0-votes': '10',\n            'choices-1-choice': 'Choice2',\n            'choices-1-votes': '20',\n            'choices-2-choice': 'Choice3',\n            'choices-2-votes': '30',\n            'choices-3-choice': 'Choice4',\n            'choices-3-votes': '40',\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=2, max_num=4, absolute_max=3)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices', initial=initial)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.non_form_errors(),\n            ['Please submit at most 3 forms.'],\n        )\n"], "sample_333": ["    def test_declarative_fields_metaclass(self):\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n\n        # Check if declared_fields and base_fields are set correctly\n        self.assertIn('field1', TestForm.declared_fields)\n        self.assertIn('field2', TestForm.declared_fields)\n        self.assertIn('field1', TestForm.base_fields)\n        self.assertIn('field2', TestForm.base_fields)\n\n        # Check if modifications in class attributes are reflected\n        class TestFormModified(TestForm):\n            field3 = CharField()\n\n        self.assertIn('field3', TestFormModified.declared_fields)\n        self.assertIn('field1', TestFormModified.declared_fields)\n        self.assertIn('field2', TestFormModified.declared_fields)\n        self.assertNotIn('field3', TestForm.declared_fields)\n\n        # Ensure shadowing works\n        class TestFormShadow(TestForm):\n            field1 = None\n\n        self.assertNotIn('field1', TestFormShadow.declared_fields)\n        self.assertIn('field2', TestFormShadow.declared_fields)\n        self.assertNotIn('field1', TestFormShadow.base_fields)\n        self.assertIn('field2', TestFormShadow.base_fields)\n", "    def test_form_with_custom_field_order(self):\n        class CustomOrderForm(Form):\n            field_a = CharField()\n            field_b = CharField()\n            field_c = CharField()\n\n                super().__init__(*args, **kwargs)\n                self.order_fields(['field_c', 'field_a', 'field_b'])\n\n        form = CustomOrderForm()\n        self.assertEqual(list(form.fields.keys()), ['field_c', 'field_a', 'field_b'])\n\n        self.assertHTMLEqual(\n            form.as_table(),\n            '<tr><th><label for=\"id_field_c\">Field c:</label></th><td>'\n            '<input type=\"text\" name=\"field_c\" id=\"id_field_c\" required></td></tr>'\n            '<tr><th><label for=\"id_field_a\">Field a:</label></th><td>'\n            '<input type=\"text\" name=\"field_a\" id=\"id_field_a\" required></td></tr>'\n            '<tr><th><label for=\"id_field_b\">Field b:</label></th><td>'\n            '<input type=\"text\" name=\"field_b\" id=\"id_field_b\" required></td></tr>'\n        )\n\n        self.assertHTMLEqual(\n            form.as_ul(),\n            '<li><label for=\"id_field_c\">Field c:</label> '\n            '<input type=\"text\" name=\"field_c\" id=\"id_field_c\" required></li>'\n            '<li><label for=\"id_field_a\">Field a:</label> '\n            '<input type=\"text\" name=\"field_a\" id=\"id_field_a\" required></li>'\n            '<li><label for=\"id_field_b\">Field b:</label> '\n            '<input type=\"text\" name=\"field_b\" id=\"id_field_b\" required></li>'\n        )\n\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p><label for=\"id_field_c\">Field c:</label> '\n            '<input type=\"text\" name=\"field_c\" id=\"id_field_c\" required></p>'\n            '<p><label for=\"id_field_a\">Field a:</label> '\n            '<input type=\"text\" name=\"field_a\" id=\"id_field_a\" required></p>'\n            '<p><label for=\"id_field_b\">Field b:</label> '\n            '<input type=\"text\" name=\"field_b\" id=\"", "    def test_dynamic_field_addition_in_init(self):\n        # Test dynamic addition of fields in __init__\n        class DynamicForm(Form):\n                super().__init__(*args, **kwargs)\n                self.fields['dynamic_field'] = CharField()\n\n        form = DynamicForm()\n        self.assertIn('dynamic_field', form.fields)\n\n        form = DynamicForm({'dynamic_field': 'test_value'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['dynamic_field'], 'test_value')\n", "    def test_form_order_fields_method(self):\n        class OrderedForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n            field5 = CharField()\n\n        form = OrderedForm()\n        form.order_fields(['field3', 'field1', 'field4'])\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field4', 'field2', 'field5'])\n\n        # Test ordering with unknown fields\n        form = OrderedForm()\n        form.order_fields(['unknown', 'field2', 'field4'])\n        self.assertEqual(list(form.fields), ['field2', 'field4', 'field1', 'field3', 'field5'])\n\n        # Test ordering with no field_order\n        form = OrderedForm()\n        form.order_fields(None)\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3', 'field4', 'field5'])\n", "    def test_form_with_ordered_fields(self):\n        class OrderedForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field_order = ['field3', 'field1', 'field2']\n\n        form = OrderedForm()\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field2'])\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p><label for=\"id_field3\">Field3:</label> <input id=\"id_field3\" name=\"field3\" type=\"text\" required></p>'\n            '<p><label for=\"id_field1\">Field1:</label> <input id=\"id_field1\" name=\"field1\" type=\"text\" required></p>'\n            '<p><label for=\"id_field2\">Field2:</label> <input id=\"id_field2\" name=\"field2\" type=\"text\" required></p>'\n        )\n"], "sample_334": ["def test_order_fields(self):\n    # Test ordering of fields using order_fields method.\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field4 = CharField()\n        field5 = CharField()\n\n    form = TestForm()\n    self.assertEqual(list(form.fields), ['field1', 'field2', 'field3', 'field4', 'field5'])\n\n    # Reorder fields using order_fields\n    form.order_fields(['field5', 'field3', 'field1'])\n    self.assertEqual(list(form.fields), ['field5', 'field3', 'field1', 'field2', 'field4'])\n\n    # Reorder fields with some missing ones\n    form.order_fields(['field4', 'field2'])\n    self.assertEqual(list(form.fields), ['field4', 'field2', 'field5', 'field3', 'field1'])\n\n    # Reorder with all fields present\n    form.order_fields(['field3', 'field1', 'field5', 'field4', 'field2'])\n    self.assertEqual(list(form.fields), ['field3', 'field1', 'field5', 'field4', 'field2'])\n\n    # Reorder with empty list should keep the original order\n    form.order_fields([])\n    self.assertEqual(list(form.fields), ['field3', 'field1', 'field5', 'field4', 'field2'])\n", "    def test_form_with_explicit_ordering(self):\n        # Test form field ordering with explicit order specified in the form definition.\n        class OrderedPerson(Form):\n            field_order = ['last_name', 'first_name', 'birthday']\n            first_name = CharField()\n            last_name = CharField()\n            birthday = DateField()\n\n        p = OrderedPerson()\n        self.assertEqual(list(p.fields), ['last_name', 'first_name', 'birthday'])\n\n        # Ensure HTML output respects the order\n        self.assertHTMLEqual(\n            p.as_p(),\n            \"\"\"<p><label for=\"id_last_name\">Last name:</label> <input type=\"text\" name=\"last_name\" id=\"id_last_name\" required></p>", "    def test_boundfield_css_classes_combined(self):\n        class CustomForm(Form):\n            field1 = CharField()\n            field2 = CharField(required=False)\n            field3 = CharField(widget=Textarea)\n            field4 = ChoiceField(choices=[('P', 'Python'), ('J', 'Java')])\n\n        form = CustomForm()\n        field1 = form['field1']\n        field2 = form['field2']\n        field3 = form['field3']\n        field4 = form['field4']\n\n        self.assertEqual(field1.css_classes(), '')\n        self.assertEqual(field1.css_classes(extra_classes='required'), 'required')\n        self.assertEqual(field2.css_classes(extra_classes='required'), 'required')\n        self.assertEqual(field3.css_classes(extra_classes='required'), 'required')\n        self.assertEqual(field4.css_classes(extra_classes='required'), 'required')\n\n        self.assertEqual(field1.css_classes(extra_classes=' class1 class2'), 'class1 class2')\n        self.assertEqual(field2.css_classes(extra_classes=' class1 class2'), 'class1 class2')\n        self.assertEqual(field3.css_classes(extra_classes=' class1 class2'), 'class1 class2')\n        self.assertEqual(field4.css_classes(extra_classes=' class1 class2'), 'class1 class2')\n", "    def test_order_fields_custom_order(self):\n        class CustomOrderForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n            field5 = CharField()\n\n        form = CustomOrderForm()\n        form.order_fields(['field5', 'field4', 'field1'])\n\n        self.assertEqual(list(form.fields), ['field5', 'field4', 'field1', 'field2', 'field3'])\n\n        form = CustomOrderForm()\n        form.order_fields(['field2'])\n\n        self.assertEqual(list(form.fields), ['field2', 'field1', 'field3', 'field4', 'field5'])\n\n        form = CustomOrderForm()\n        form.order_fields(['field3', 'field1', 'field2', 'field5', 'field4'])\n\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field2', 'field5', 'field4'])\n", "    def test_non_field_error(self):\n        # Test adding non-field errors\n        class TestForm(Form):\n            field1 = CharField(max_length=10)\n            field2 = CharField(max_length=10)\n\n                raise ValidationError('Non-field error message')\n\n        form = TestForm({'field1': 'abc', 'field2': 'def'})\n        self.assertFalse(form.is_valid())\n        self.assertIn(NON_FIELD_ERRORS, form.errors)\n        self.assertEqual(form.errors[NON_FIELD_ERRORS], ['Non-field error message'])\n        self.assertHTMLEqual(\n            str(form.non_field_errors()),\n            '<ul class=\"errorlist nonfield\"><li>Non-field error message</li></ul>'\n        )\n"], "sample_335": ["def test_decimalfield_prepare_value(self):\n    f = DecimalField(max_digits=6, decimal_places=2)\n    self.assertEqual(f.prepare_value(decimal.Decimal(\"123.45\")), \"123.45\")\n    self.assertEqual(f.prepare_value(\"123.45\"), \"123.45\")\n    self.assertEqual(f.prepare_value(None), None)\n    self.assertEqual(f.prepare_value(123.45), \"123.45\")\n", "    def test_charfield_valid_input(self):\n        f = CharField(max_length=10, min_length=2)\n        self.assertEqual(f.clean('abc'), 'abc')\n        self.assertEqual(f.clean(' a '), 'a')\n        self.assertEqual(f.clean(' '), '')\n    ", "    def test_decimalfield_invalid_cases(self):\n        f = DecimalField(max_digits=5, decimal_places=2)\n        invalid_values = [\n            'abc', '123.456', '12.3456', '12345.67', '12..34', '12,34', '--12.34'\n        ]\n        for value in invalid_values:\n            with self.subTest(value=value), self.assertRaises(ValidationError):\n                f.clean(value)\n", "def test_decimalfield_invalid_input(self):\n    f = DecimalField(max_digits=5, decimal_places=2)\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean('abc')\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean('123abc')\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean('1.2.3')\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean('1,23')\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean('12.34.56')\n", "def test_decimalfield_invalid_values(self):\n    f = DecimalField(max_digits=5, decimal_places=3)\n    invalid_values = [\n        'abc',  # Non-numeric input\n        '12345.6789',  # Exceeds max_digits\n        '123.4567',  # Exceeds decimal_places\n        '--123.45',  # Invalid double negative\n        '12..34',  # Invalid double dot\n    ]\n    for value in invalid_values:\n        with self.subTest(value=value), self.assertRaisesMessage(ValidationError, \"'Enter a valid number.'\"):\n            f.clean(value)\n"], "sample_336": ["    def test_urlpattern_repr(self):\n        \"\"\"\n        Test repr of URLPattern, ensuring it correctly represents the pattern.\n        \"\"\"\n        pattern = URLPattern(RegexPattern(r'^foo/$'), views.empty_view, name='foo')\n        self.assertEqual(\n            repr(pattern),\n            \"<URLPattern 'foo' [name='foo']>\"\n        )\n", "    def test_language_prefix_default_language(self):\n        \"\"\"Test language prefix for default language.\"\"\"\n        pattern = LocalePrefixPattern()\n        with self.settings(LANGUAGE_CODE='en', USE_I18N=True):\n            self.assertEqual(pattern.language_prefix, 'en/')\n", "    def test_regex_pattern_match(self):\n        pattern = RegexPattern(r'^places/(?P<id>\\d+)/$')\n        self.assertEqual(pattern.match('places/42/'), ('', (), {'id': '42'}))\n        self.assertIsNone(pattern.match('places/'))\n", "    def test_valid_regex_pattern_match(self):\n        \"\"\"\n        Test that a RegexPattern correctly matches a given URL path.\n        \"\"\"\n        pattern = RegexPattern(r'^test/(?P<param>\\d+)/$')\n        match = pattern.match('test/123/')\n        self.assertIsNotNone(match)\n        self.assertEqual(match[1], ())\n        self.assertEqual(match[2], {'param': '123'})\n", "    def test_urlpattern_repr(self):\n        \"\"\"\n        Test the __repr__ method of URLPattern to ensure it produces the correct string representation.\n        \"\"\"\n        pattern = URLPattern(RegexPattern(r'^test/$'), views.empty_view, name='test-view')\n        expected_repr = \"<URLPattern 'test-view'>\"\n        self.assertEqual(repr(pattern), expected_repr)\n"], "sample_337": ["    def test_invalid_referer_matches_allowed_host(self):\n        \"\"\"\n        A POST HTTPS request with a malformed Referer that matches ALLOWED_HOSTS is rejected.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_REFERER'] = 'https://www.example.com\\\\somepage'\n        req.META['HTTP_HOST'] = 'www.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        self._check_referer_rejects(mw, req)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertContains(\n            response,\n            'Referer checking failed - Referer is malformed.',\n            status_code=403,\n        )\n", "    def test_check_referer_with_malformed_referer(self):\n        \"\"\"\n        Test that the middleware correctly identifies and rejects malformed referers.\n        \"\"\"\n        malformed_referers = [\n            'http://http://www.example.com/',\n            '',\n            '\u00d8B\u00f6I\u00df',\n            '//example.com/',\n            'https://',\n            'https://[',\n        ]\n        for referer in malformed_referers:\n            with self.subTest(referer=referer):\n                req = self._get_POST_request_with_token()\n                req._is_secure_override = True\n                req.META['HTTP_REFERER'] = referer\n                mw = CsrfViewMiddleware(post_form_view)\n                with self.assertRaises(RejectRequest):\n                    mw._check_referer(req)\n                response = mw.process_view(req, post_form_view, (), {})\n                self.assertEqual(response.status_code, 403)\n                self.assertIn('Referer checking failed', response.content.decode())\n", "    def test_csrf_exempt_view_with_valid_token(self):\n        \"\"\"\n        A view decorated with csrf_exempt should bypass CSRF checks, even if\n        a valid token is present in the request.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        mw = CsrfViewMiddleware(csrf_exempt(post_form_view))\n        mw.process_request(req)\n        resp = mw.process_view(req, csrf_exempt(post_form_view), (), {})\n        self.assertIsNone(resp)\n", "    def test_csrf_secret_length_correct(self):\n        \"\"\"\n        Ensures that the CSRF secret and token lengths are correctly defined.\n        \"\"\"\n        self.assertEqual(CSRF_SECRET_LENGTH, 32)\n        self.assertEqual(CSRF_TOKEN_LENGTH, 64)\n", "    def test_rotate_token_on_login(self):\n        \"\"\"\n        Test that the CSRF token is rotated upon user login.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        mw = CsrfViewMiddleware(post_form_view)\n\n        # Simulate a login by calling rotate_token()\n        rotate_token(req)\n        new_token = req.META[\"CSRF_COOKIE\"]\n\n        # Process the request and view with the new token\n        mw.process_request(req)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n\n        # Ensure the CSRF token was actually rotated\n        rotated_token = req.META.get(\"CSRF_COOKIE\")\n        self.assertIsNotNone(rotated_token)\n        self.assertNotEqual(rotated_token, self._csrf_id_cookie)\n        self.assertTrue(equivalent_tokens(new_token, rotated_token))\n\n        # Check if the response contains the new token\n        response = mw.process_response(req, HttpResponse())\n        self.assertIn(settings.CSRF_COOKIE_NAME, response.cookies)\n        self.assertEqual(response.cookies[settings.CSRF_COOKIE_NAME].value, new_token)\n"], "sample_338": ["    def test_rename_model_with_altered_field(self):\n        \"\"\"\n        Tests autodetection of renamed models while simultaneously altering one\n        of the fields that relate to the renamed model.\n        \"\"\"\n        before = [\n            self.author_with_book,\n            self.book,\n        ]\n        after = [\n            self.author_renamed_with_book,\n            ModelState(\"otherapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Writer\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=400)),\n            ]),\n        ]\n        changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename_model\": True}))\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Writer\")\n        # Right number/type of migrations for related field alteration?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name=\"book\", name=\"title\", max_length=400)\n", "    def test_generate_altered_db_table(self):\n        \"\"\"\n        Tests detection for changing db_table in model's options' when keeping renamed model intact.\n        \"\"\"\n        before = self.make_project_state([self.author_with_db_table_options])\n        after = self.make_project_state([self.author_renamed_with_db_table_options])\n        autodetector = MigrationAutodetector(before, after, MigrationQuestioner({\"ask_rename_model\": True}))\n        changes = autodetector._detect_changes()\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"NewAuthor\")\n        # Ensure no AlterModelTable operation is created since db_table remains same\n        self.assertEqual(len(changes['testapp'][0].operations), 1)\n", "    def test_no_changes_deep_deconstruct(self):\n        \"\"\"Test deep_deconstruct with no changes between states.\"\"\"\n        before = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=DeconstructibleObject(\n                DeconstructibleObject(1),\n                (DeconstructibleObject('t1'), DeconstructibleObject('t2'),),\n                a=DeconstructibleObject('A'),\n                b=DeconstructibleObject(B=DeconstructibleObject('c')),\n            ))),\n        ])\n        after = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=DeconstructibleObject(\n                DeconstructibleObject(1),\n                (DeconstructibleObject('t1'), DeconstructibleObject('t2'),),\n                a=DeconstructibleObject('A'),\n                b=DeconstructibleObject(B=DeconstructibleObject('c')),\n            ))),\n        ])\n        changes = self.get_changes([before], [after])\n        # No changes should be detected\n        self.assertEqual(len(changes), 0)\n", "    def test_alter_field_with_preserve_default_false(self):\n        \"\"\"\n        #XXXXX - Tests altering a field with preserve_default set to False.\n        \"\"\"\n        before = ModelState(\"testapp\", \"Book\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"title\", models.CharField(max_length=200)),\n        ])\n        after = ModelState(\"testapp\", \"Book\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"title\", models.CharField(max_length=255)),\n        ])\n        changes = self.get_changes([before], [after])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"title\", preserve_default=False)\n", "def test_deep_deconstruct_with_partial_function(self):\n    \"\"\"\n    Test that the deep_deconstruct method correctly handles\n    functools.partial objects.\n    \"\"\"\n        return arg1 + arg2\n\n    partial_obj = functools.partial(sample_function, 10, arg2=20)\n    expected_result = (\n        sample_function,\n        [10],\n        {'arg2': 20}\n    )\n\n    autodetector = MigrationAutodetector(None, None)\n    deconstructed = autodetector.deep_deconstruct(partial_obj)\n    self.assertEqual(deconstructed, expected_result)\n\n"], "sample_339": ["    def test_fields_for_model_with_all_fields(self):\n        \"\"\"\n        Test fields_for_model when 'ALL_FIELDS' is passed.\n        \"\"\"\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=50)\n            field2 = models.IntegerField()\n            field3 = models.DateField()\n            field4 = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        fields = fields_for_model(TestModel, fields=ALL_FIELDS)\n        self.assertEqual(set(fields.keys()), {'field1', 'field2', 'field3', 'field4'})\n", "    def test_options_initialization(self):\n        class Meta:\n            model = Author\n            fields = ['name']\n            exclude = ['id']\n            widgets = {'name': forms.TextInput(attrs={'class': 'author'})}\n            localized_fields = ['name']\n            labels = {'name': 'Author Name'}\n            help_texts = {'name': 'Enter the author name.'}\n            error_messages = {'name': {'required': 'This field is required.'}}\n            field_classes = {'name': forms.CharField}\n\n        options = ModelFormOptions(Meta)\n\n        self.assertEqual(options.model, Author)\n        self.assertEqual(options.fields, ['name'])\n        self.assertEqual(options.exclude, ['id'])\n        self.assertEqual(options.widgets, {'name': forms.TextInput(attrs={'class': 'author'})})\n        self.assertEqual(options.localized_fields, ['name'])\n        self.assertEqual(options.labels, {'name': 'Author Name'})\n        self.assertEqual(options.help_texts, {'name': 'Enter the author name.'})\n        self.assertEqual(options.error_messages, {'name': {'required': 'This field is required.'}})\n        self.assertEqual(options.field_classes, {'name': forms.CharField})\n", "    def test_exclusion_of_non_editable_fields(self):\n        class AuthorForm(forms.ModelForm):\n            class Meta:\n                model = Author\n                fields = '__all__'\n                exclude = ['id']\n\n        author = Author.objects.create(name='Charles Baudelaire')\n        AuthorFormSet = modelformset_factory(Author, form=AuthorForm, can_delete=True)\n\n        data = {\n            'form-TOTAL_FORMS': '1',\n            'form-INITIAL_FORMS': '1',\n            'form-MAX_NUM_FORMS': '0',\n            'form-0-id': str(author.id),\n            'form-0-name': 'Charles Baudelaire',\n        }\n\n        formset = AuthorFormSet(data, queryset=Author.objects.all())\n        self.assertTrue(formset.is_valid())\n\n        data['form-0-name'] = 'Arthur Rimbaud'\n        formset = AuthorFormSet(data, queryset=Author.objects.all())\n        self.assertTrue(formset.is_valid())\n\n        # Ensure that the instance is updated correctly\n        saved_instances = formset.save()\n        self.assertEqual(len(saved_instances), 1)\n        author.refresh_from_db()\n        self.assertEqual(author.name, 'Arthur Rimbaud')\n", "    def test_construct_instance_with_all_fields(self):\n        \"\"\"\n        Test the construct_instance function with all fields provided.\n        \"\"\"\n        author = Author.objects.create(name=\"Charles Dickens\")\n        BookForm = modelform_factory(Book, fields=\"__all__\")\n        \n        # Create form data for a book\n        data = {\n            'title': 'Great Expectations',\n            'author': author.id\n        }\n        form = BookForm(data)\n        \n        self.assertTrue(form.is_valid())\n        book = form.save(commit=False)\n\n        # Construct a new instance using the form's cleaned data\n        instance = construct_instance(form, book, fields=['title', 'author'])\n        \n        self.assertEqual(instance.title, 'Great Expectations')\n        self.assertEqual(instance.author, author)\n", "    def test_formfield_callback(self):\n            if db_field.name == 'title':\n                return forms.CharField(max_length=200, widget=forms.Textarea)\n            return db_field.formfield(**kwargs)\n\n        BookForm = modelform_factory(Book, formfield_callback=formfield_for_dbfield)\n        form = BookForm()\n        self.assertIsInstance(form.fields['title'].widget, forms.Textarea)\n"], "sample_340": ["    def test_bad_migration_error(self):\n        \"\"\"\n        Test that a BadMigrationError is raised when a migration file doesn't \n        have a Migration class.\n        \"\"\"\n        with self.assertRaisesMessage(BadMigrationError, \"Migration 0001_initial in app migrations has no Migration class\"):\n            MigrationLoader(connection)\n", "    def test_circular_dependencies(self):\n        \"\"\"\n        Test handling of circular dependencies between apps.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n\n        # Load with nothing applied, should detect the circular dependency\n        with self.assertRaises(ValueError):\n            loader.build_graph()\n\n        # Fake-apply one migration to break the circular dependency\n        self.record_applied(recorder, 'app1', '0001_initial')\n        loader.build_graph()\n        plan = set(loader.graph.forwards_plan(('app2', '0001_initial')))\n        plan = plan - loader.applied_migrations.keys()\n        expected_plan = {('app2', '0001_initial')}\n        self.assertEqual(plan, expected_plan)\n", "    def test_replacement_migrations(self):\n        \"\"\"\n        Tests if replacing migrations are properly loaded and applied.\n        \"\"\"\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n        \n        # Load initial migrations\n        migration_loader = MigrationLoader(connection)\n        self.assertEqual(\n            len([x for x in migration_loader.graph.nodes if x[0] == \"migrations\"]),\n            1,\n        )\n        \n        # Apply a replacing migration\n        self.record_applied(recorder, 'migrations', '0001_initial')\n        migration_loader.build_graph()\n        self.assertIn(('migrations', '0002_replacement'), migration_loader.disk_migrations)\n        self.assertNotIn(('migrations', '0002_initial'), migration_loader.disk_migrations)\n        ", "    def test_load_invalid_migration_class(self):\n        \"\"\"\n        Tests that a BadMigrationError is raised if a migration file does not have a Migration class.\n        \"\"\"\n        with self.assertRaisesMessage(BadMigrationError, \"Migration 0001_initial in app migrations has no Migration class\"):\n            MigrationLoader(connection)\n", "    def test_invalid_migration_class(self):\n        \"\"\"\n        Tests that a migration file without a Migration class raises BadMigrationError.\n        \"\"\"\n        with self.assertRaisesMessage(BadMigrationError, \"Migration invalid_migration in app migrations has no Migration class\"):\n            MigrationLoader(connection)\n"], "sample_341": ["    def test_management_form_not_bound(self):\n        \"\"\"\n        Test that the management form is not bound when data is not provided.\n        \"\"\"\n        formset = ChoiceFormSet()\n        self.assertFalse(formset.management_form.is_bound)\n        self.assertEqual(formset.management_form.initial[TOTAL_FORM_COUNT], 1)\n        self.assertEqual(formset.management_form.initial[INITIAL_FORM_COUNT], 0)\n        self.assertEqual(formset.management_form.initial[MIN_NUM_FORM_COUNT], 0)\n        self.assertEqual(formset.management_form.initial[MAX_NUM_FORM_COUNT], 1000)\n", "    def test_empty_formset_with_custom_clean(self):\n        \"\"\"\n        A formset with an empty form and a custom clean method that raises\n        a validation error should be invalid.\n        \"\"\"\n        class CustomCleanFormSet(BaseFormSet):\n                raise ValidationError('Custom formset level validation error')\n\n        CustomFormSet = formset_factory(FavoriteDrinkForm, formset=CustomCleanFormSet, extra=0)\n        formset = CustomFormSet(data={'form-TOTAL_FORMS': '0', 'form-INITIAL_FORMS': '0'})\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['Custom formset level validation error'])\n", "    def test_custom_error_messages(self):\n        \"\"\"\n        Custom error messages set at formset initialization should be used.\n        \"\"\"\n        error_messages = {\n            'missing_management_form': 'Custom management form error message.'\n        }\n        data = {\n            'choices-TOTAL_FORMS': '1',  # the number of forms rendered\n            'choices-INITIAL_FORMS': '0',  # the number of forms with initial data\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices', error_messages=error_messages)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['Custom management form error message.'])\n", "def test_management_form_clean_data(self):\n    \"\"\"\n    Test that the ManagementForm returns correct cleaned data.\n    \"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '3',\n        'choices-INITIAL_FORMS': '2',\n        'choices-MIN_NUM_FORMS': '1',\n        'choices-MAX_NUM_FORMS': '10',\n    }\n    formset = self.make_choiceformset([], data)\n    management_form = formset.management_form\n    cleaned_data = management_form.clean()\n    self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 3)\n    self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 2)\n    self.assertEqual(cleaned_data[MIN_NUM_FORM_COUNT], 1)\n    self.assertEqual(cleaned_data[MAX_NUM_FORM_COUNT], 10)\n", "    def test_cleaned_data_property(self):\n        \"\"\"Test cleaned_data property raises AttributeError when formset is invalid.\"\"\"\n        formset = self.make_choiceformset([('Calexico', '')])\n        with self.assertRaises(AttributeError):\n            _ = formset.cleaned_data\n"], "sample_342": ["    def test_process_request_invalid_app_model_field(self):\n        \"\"\"\n        Ensure that PermissionDenied is raised when the request contains invalid app_label,\n        model_name, or field_name.\n        \"\"\"\n        invalid_opts = {\n            'app_label': 'invalid_app',\n            'model_name': 'invalid_model',\n            'field_name': 'invalid_field'\n        }\n        request = self.factory.get(self.url, {'term': 'is', **invalid_opts})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid', 'model_name': 'Answer', 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_app_label_does_not_exist(self):\n        opts = {\n            'app_label': 'nonexistent_app',\n            'model_name': Answer._meta.model_name,\n            'field_name': 'question',\n        }\n        request = self.factory.get(self.url, {'term': 'is', **opts})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_invalid_app_label(self):\n        request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid', 'model_name': 'Answer', 'field_name': 'question'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_permission_denied_invalid_model_admin(self):\n    # Creating request with invalid model admin configuration\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'admin_views', 'model_name': 'invalid_model', 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_343": ["    def test_check_generic_foreign_key_existence(self):\n        class TaggedItem(models.Model):\n            content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n            object_id = models.PositiveIntegerField()\n            content_object = GenericForeignKey('content_type', 'object_id')\n\n        class Post(models.Model):\n            tags = GenericRelation(TaggedItem)\n\n        errors = TaggedItem._meta.get_field('content_object').check()\n        self.assertEqual(errors, [])\n", "    def test_contribute_to_class(self):\n        class Model(models.Model):\n            field = GenericForeignKey()\n\n        model_instance = Model()\n        self.assertTrue(hasattr(model_instance, 'field'))\n        self.assertIsInstance(model_instance.field, GenericForeignKey)\n", "    def test_check_field_name(self):\n        class Model(models.Model):\n            field = GenericForeignKey()\n\n        field_with_underscore = GenericForeignKey()\n        field_with_underscore.name = \"invalid_field_\"\n        field_with_underscore.model = Model\n\n        errors = field_with_underscore.check()\n        self.assertGreater(len(errors), 0)\n        self.assertEqual(errors[0].id, 'fields.E001')\n", "    def test_check_field_name(self):\n        class Model(models.Model):\n            field = GenericForeignKey()\n\n        # Test valid field name\n        self.assertEqual(Model.field._check_field_name(), [])\n\n        # Test invalid field name\n        class InvalidModel(models.Model):\n            invalid_field_ = GenericForeignKey()\n\n        self.assertEqual(\n            InvalidModel.invalid_field_._check_field_name(),\n            [checks.Error(\n                'Field names must not end with an underscore.',\n                obj=InvalidModel.invalid_field_,\n                id='fields.E001',\n            )]\n        )\n", "    def test_generic_foreign_key_get_forward_related_filter(self):\n        question = Question.objects.create(text='Who?')\n        post = Post.objects.create(title='Answer', parent=question)\n\n        expected_filter = {\n            'object_id': question.pk,\n            'content_type': question._get_pk_val(),\n        }\n        self.assertEqual(Post.parent.get_forward_related_filter(question), expected_filter)\n"], "sample_344": ["    def test_rename_model(self):\n        \"\"\"\n        Tests renaming a model and ensuring all references are updated.\n        \"\"\"\n        new_apps = Apps([\"migrations\"])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n            author = models.ForeignKey(Author, models.CASCADE)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        project_state = ProjectState.from_apps(new_apps)\n        project_state.rename_model('migrations', 'Author', 'Writer')\n        writer_state = project_state.models['migrations', 'writer']\n        book_state = project_state.models['migrations', 'book']\n\n        self.assertEqual(writer_state.name, 'Writer')\n        self.assertEqual(writer_state.fields['name'].max_length, 255)\n        self.assertEqual(book_state.fields['author'].remote_field.model, 'migrations.Writer')\n", "    def test_clone_project_state(self):\n        \"\"\"\n        Test cloning a ProjectState to ensure that the cloned state is an exact copy.\n        \"\"\"\n        new_apps = Apps(['migrations'])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class Book(models.Model):\n            title = models.CharField(max_length=255)\n            author = models.ForeignKey(Author, models.CASCADE)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state = ProjectState.from_apps(new_apps)\n        cloned_state = project_state.clone()\n\n        self.assertEqual(project_state, cloned_state)\n        self.assertIsNot(project_state, cloned_state)\n        self.assertEqual(project_state.models, cloned_state.models)\n        self.assertIsNot(project_state.models, cloned_state.models)\n\n        # Ensure the cloned state's apps property is also an exact copy\n        self.assertNotEqual(id(project_state.apps), id(cloned_state.apps))\n        self.assertEqual(len(project_state.apps.get_models()), len(cloned_state.apps.get_models()))\n", "    def test_complex_relation_chain(self):\n        \"\"\"\n        Tests handling of complex relation chains involving multiple levels\n        of foreign keys and proxy models.\n        \"\"\"\n        new_apps = Apps(['migrations'])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class Book(models.Model):\n            title = models.CharField(max_length=255)\n            author = models.ForeignKey(Author, models.CASCADE)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class EBook(Book):\n            file_format = models.CharField(max_length=10)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n                proxy = True\n\n        class Publisher(models.Model):\n            name = models.CharField(max_length=255)\n            books = models.ManyToManyField(Book)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state = ProjectState()\n        project_state.add_model(ModelState.from_model(Author))\n        project_state.add_model(ModelState.from_model(Book))\n        project_state.add_model(ModelState.from_model(EBook))\n        project_state.add_model(ModelState.from_model(Publisher))\n\n        self.assertEqual(len(project_state.apps.get_models()), 4)\n\n        author_state = project_state.models['migrations', 'author']\n        book_state = project_state.models['migrations', 'book']\n        ebook_state = project_state.models['migrations', 'ebook']\n        publisher_state = project_state.models['migrations', 'publisher']\n\n        self.assertEqual(author_state.app_label, \"migrations\")\n        self.assertEqual(author_state.name, \"Author\")\n        self.assertEqual(list(author_state.fields), [\"id\", \"name\"])\n\n        self.assertEqual(book_state.app_label, \"migrations\")\n        self.assertEqual(book_state.name, \"Book\")\n        self.assertEqual(list(book_state.fields), [\"id\", \"title\", \"author\"])\n\n        self.assertEqual(ebook_state.app_label, \"migrations\")\n        self.assertEqual(ebook_state.name, \"EBook\")\n        self.assertEqual(ebook_state.fields, {})\n        self.assertEqual(\n            ebook_state.options,\n            {\"proxy\": True, \"indexes\": [], \"constraints\": []},\n        )\n        self.assertEqual(ebook_state.bases, (\"migrations.book\",))\n\n        self.assertEqual(publisher_state.app_label, \"migrations\")\n        self", "    def test_add_and_remove_field(self):\n        \"\"\"\n        Test adding and removing a field from a model state.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"TestModel\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n            ],\n        ))\n\n        # Add a new field to the model\n        new_field = models.IntegerField(default=0)\n        project_state.add_field(\"migrations\", \"testmodel\", \"age\", new_field, preserve_default=True)\n        test_model_state = project_state.models[\"migrations\", \"testmodel\"]\n        self.assertIn(\"age\", test_model_state.fields)\n        self.assertEqual(test_model_state.fields[\"age\"].default, 0)\n\n        # Remove the added field\n        project_state.remove_field(\"migrations\", \"testmodel\", \"age\")\n        test_model_state = project_state.models[\"migrations\", \"testmodel\"]\n        self.assertNotIn(\"age\", test_model_state.fields)\n", "    def test_project_state_clone(self):\n        \"\"\"\n        Test that cloning a ProjectState creates an exact copy.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"Example\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=255)),\n            ],\n        ))\n\n        cloned_state = project_state.clone()\n        self.assertEqual(project_state.models, cloned_state.models)\n        self.assertEqual(project_state.real_apps, cloned_state.real_apps)\n        self.assertEqual(project_state.is_delayed, cloned_state.is_delayed)\n"], "sample_345": ["    def test_ensure_echo_on_not_tty(self, mocked_signal, mocked_stdin, mocked_termios):\n        mocked_stdin.isatty.return_value = False\n        autoreload.ensure_echo_on()\n        mocked_termios.tcgetattr.assert_not_called()\n        mocked_termios.tcsetattr.assert_not_called()\n", "    def test_enable_echo(self, mock_termios, mock_isatty):\n        mock_termios.tcgetattr.return_value = [None, None, None, 0, None, None, None]\n        mock_termios.ECHO = 0x00000008  # Example value for ECHO flag\n        autoreload.ensure_echo_on()\n        self.assertTrue(mock_termios.tcsetattr.called)\n", "    def test_echo_on_when_disabled(self, mocked_termios, mocked_isatty):\n        attrs = [0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attrs\n\n            nonlocal attrs\n            attrs = attributes\n\n        mocked_termios.tcsetattr.side_effect = set_attr\n        attrs[3] = attrs[3] & ~mocked_termios.ECHO\n\n        autoreload.ensure_echo_on()\n\n        self.assertTrue(attrs[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called()\n", "    def test_ensure_echo_on_enabled(self, mock_signal, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mock_isatty.called)\n        self.assertTrue(mock_tcgetattr.called)\n        self.assertTrue(mock_tcsetattr.called)\n        self.assertTrue(mock_signal.called)\n", "    def test_ensure_echo_on_enabled(self, mock_tcsetattr, mock_tcgetattr, mock_isatty):\n        mock_tcgetattr.return_value = [0, 0, 0, 0b100]  # ECHO bit is set\n        autoreload.ensure_echo_on()\n        mock_tcsetattr.assert_not_called()\n"], "sample_346": ["    def test_middleware_decorator_process_request(self):\n        class TestMiddleware:\n                self.get_response = get_response\n\n                return HttpResponse(\"processed request\")\n\n        @decorator_from_middleware(TestMiddleware)\n            return HttpResponse(\"view response\")\n\n        request = HttpRequest()\n        response = my_view(request)\n        self.assertEqual(response.content, b\"processed request\")\n", "    def test_decorator_from_middleware(self):\n        class TestMiddleware:\n                self.get_response = get_response\n\n                response = self.get_response(request)\n                response['X-Test-Middleware'] = 'middleware applied'\n                return response\n\n        @decorator_from_middleware(TestMiddleware)\n            return HttpResponse()\n\n        request = HttpRequest()\n        response = a_view(request)\n        self.assertEqual(response['X-Test-Middleware'], 'middleware applied')\n", "    def test_multi_decorate_with_single_decorator(self):\n        \"\"\"\n        _multi_decorate should correctly apply a single decorator to a method.\n        \"\"\"\n                return \"decorated \" + func(*args, **kwargs)\n            return wrapper\n\n        class TestClass:\n            @_multi_decorate(simple_decorator, name='method')\n                return \"method called\"\n\n        obj = TestClass()\n        self.assertEqual(obj.method(), \"decorated method called\")\n", "    def test_classonlymethod_on_class(self):\n        class TestClass:\n            @classonlymethod\n                return \"class method called\"\n\n        self.assertEqual(TestClass.class_method(), \"class method called\")\n", "    def test_decorator_from_middleware(self):\n        class TestMiddleware:\n                self.get_response = get_response\n\n                response = self.get_response(request)\n                response['X-Test-Middleware'] = 'Middleware Applied'\n                return response\n\n        @decorator_from_middleware(TestMiddleware)\n            return HttpResponse(\"Middleware Test\")\n\n        request = HttpRequest()\n        response = my_view(request)\n        self.assertEqual(response['X-Test-Middleware'], 'Middleware Applied')\n        self.assertEqual(response.content.decode(), \"Middleware Test\")\n"], "sample_347": ["    def test_template_localtime(self):\n        naive = datetime.datetime(2022, 1, 1, 12, 0, 0)\n        aware = datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=EAT)\n\n        with override_settings(USE_TZ=True):\n            # Naive datetime should not be converted\n            self.assertEqual(timezone.template_localtime(naive), naive)\n            # Aware datetime should be converted\n            self.assertEqual(timezone.template_localtime(aware), aware.astimezone(timezone.get_current_timezone()))\n\n        with override_settings(USE_TZ=False):\n            # Naive datetime should not be converted\n            self.assertEqual(timezone.template_localtime(naive), naive)\n            # Aware datetime should not be converted\n            self.assertEqual(timezone.template_localtime(aware), aware)\n\n        with override_settings(USE_TZ=True):\n            # Overriding `use_tz` to False\n            self.assertEqual(timezone.template_localtime(aware, use_tz=False), aware)\n            # Overriding `use_tz` to True\n            self.assertEqual(timezone.template_localtime(naive, use_tz=True), naive)\n", "    def test_localtime(self):\n        naive = datetime.datetime(2015, 1, 1, 0, 0, 1)\n        with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n            timezone.localtime(naive)\n        with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n            timezone.localtime(naive, timezone=EAT)\n\n        aware = datetime.datetime(2015, 1, 1, 0, 0, 1, tzinfo=ICT)\n        self.assertEqual(timezone.localtime(aware, timezone=EAT), datetime.datetime(2014, 12, 31, 21, 0, 1, tzinfo=EAT))\n        with timezone.override(EAT):\n            self.assertEqual(timezone.localtime(aware), datetime.datetime(2014, 12, 31, 21, 0, 1, tzinfo=EAT))\n\n        with mock.patch('django.utils.timezone.now', return_value=aware):\n            self.assertEqual(timezone.localtime(timezone=EAT), datetime.datetime(2014, 12, 31, 21, 0, 1, tzinfo=EAT))\n            with timezone.override(EAT):\n                self.assertEqual(timezone.localtime(), datetime.datetime(2014, 12, 31, 21, 0, 1, tzinfo=EAT))\n", "    def test_get_fixed_timezone(self):\n        self.assertEqual(\n            timezone.get_fixed_timezone(datetime.timedelta(hours=3)).utcoffset(None),\n            datetime.timedelta(hours=3)\n        )\n        self.assertEqual(\n            timezone.get_fixed_timezone(-60).utcoffset(None),\n            datetime.timedelta(minutes=-60)\n        )\n        self.assertEqual(\n            timezone.get_fixed_timezone(120).utcoffset(None),\n            datetime.timedelta(minutes=120)\n        )\n", "    def test_get_fixed_timezone(self):\n        offset = datetime.timedelta(hours=5)\n        tz = timezone.get_fixed_timezone(offset)\n        self.assertEqual(tz.utcoffset(None), offset)\n        self.assertEqual(tz.tzname(None), '+0500')\n\n        offset = 330  # 5 hours 30 minutes\n        tz = timezone.get_fixed_timezone(offset)\n        self.assertEqual(tz.utcoffset(None), datetime.timedelta(minutes=330))\n        self.assertEqual(tz.tzname(None), '+0530')\n\n        offset = -120  # -2 hours\n        tz = timezone.get_fixed_timezone(offset)\n        self.assertEqual(tz.utcoffset(None), datetime.timedelta(minutes=-120))\n        self.assertEqual(tz.tzname(None), '-0200')\n", "    def test_get_fixed_timezone(self):\n        delta = datetime.timedelta(hours=5)\n        fixed_tz = timezone.get_fixed_timezone(delta)\n        self.assertEqual(fixed_tz.utcoffset(None), delta)\n        self.assertEqual(fixed_tz.tzname(None), \"+0500\")\n        \n        negative_delta = datetime.timedelta(hours=-3, minutes=-30)\n        fixed_tz_negative = timezone.get_fixed_timezone(negative_delta)\n        self.assertEqual(fixed_tz_negative.utcoffset(None), negative_delta)\n        self.assertEqual(fixed_tz_negative.tzname(None), \"-0330\")\n\n        with self.assertRaises(ValueError):\n            timezone.get_fixed_timezone(\"invalid_offset\")\n"], "sample_348": ["    def test_invalid_model(self):\n        class InvalidModelAdmin(ModelAdmin):\n            class Meta:\n                model = 'not a model'\n\n        self.assertIsInvalid(\n            InvalidModelAdmin, ValidationTestModel,\n            \"The value of 'model' must be a Model.\",\n            'admin.E201',\n            invalid_obj=InvalidModelAdmin.Meta\n        )\n", "    def test_model_form_options_initialization(self):\n        class Meta:\n            model = ValidationTestModel\n            fields = ['name', 'state']\n            exclude = ['description']\n            widgets = {'name': forms.TextInput()}\n            localized_fields = ['name']\n            labels = {'name': 'Band Name'}\n            help_texts = {'name': 'Enter the band name'}\n            error_messages = {'name': {'required': 'Please enter the band name'}}\n            field_classes = {'name': forms.CharField}\n\n        options = ModelFormOptions(Meta)\n        self.assertEqual(options.model, ValidationTestModel)\n        self.assertEqual(options.fields, ['name', 'state'])\n        self.assertEqual(options.exclude, ['description'])\n        self.assertEqual(options.widgets, {'name': forms.TextInput()})\n        self.assertEqual(options.localized_fields, ['name'])\n        self.assertEqual(options.labels, {'name': 'Band Name'})\n        self.assertEqual(options.help_texts, {'name': 'Enter the band name'})\n        self.assertEqual(options.error_messages, {'name': {'required': 'Please enter the band name'}})\n        self.assertEqual(options.field_classes, {'name': forms.CharField})\n", "    def test_save_as_new_not_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            save_as_new = 1\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'save_as_new' must be a boolean.\",\n            'admin.E132'\n        )\n", "    def test_formfield_callback_invalid_type(self):\n        class TestModelAdmin(ModelAdmin):\n            formfield_callback = \"not_callable\"\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'formfield_callback' must be a function or callable.\",\n            'admin.E025'\n        )\n", "    def test_formfield_callback_not_callable(self):\n        class TestModelForm(forms.ModelForm):\n            class Meta:\n                model = ValidationTestModel\n                fields = '__all__'\n                formfield_callback = 'not_callable'\n\n        with self.assertRaises(TypeError):\n            TestModelForm()\n"], "sample_349": ["    def test_admin_date_widget_media(self):\n        widget = AdminDateWidget()\n        self.assertIn('admin/js/calendar.js', widget.media._js)\n        self.assertIn('admin/js/admin/DateTimeShortcuts.js', widget.media._js)\n", "    def test_get_context_with_related_url(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        rel = Album._meta.get_field('band').remote_field\n        widget = ForeignKeyRawIdWidget(rel, admin.site)\n        context = widget.get_context('band', beatles.pk, {})\n        self.assertIn('related_url', context)\n        self.assertIn('link_title', context)\n        self.assertEqual(context['related_url'], '/admin/admin_widgets/band/')\n        self.assertEqual(context['link_title'], 'Lookup')\n        self.assertIn('vForeignKeyRawIdAdminField', context['widget']['attrs']['class'])\n", "    def test_autocomplete_select_multiple_build_attrs(self):\n        form = AlbumForm()\n        widget = AutocompleteSelectMultiple(\n            Album._meta.get_field('featuring'),\n            admin.site,\n            attrs={'class': 'my-class'},\n        )\n        attrs = widget.get_context(name='featuring', value=None, attrs={})['widget']['attrs']\n        self.assertEqual(attrs, {\n            'class': 'my-class admin-autocomplete',\n            'data-ajax--cache': 'true',\n            'data-ajax--delay': 250,\n            'data-ajax--type': 'GET',\n            'data-ajax--url': '/autocomplete/',\n            'data-theme': 'admin-autocomplete',\n            'data-allow-clear': 'false',\n            'data-app-label': 'admin_widgets',\n            'data-field-name': 'featuring',\n            'data-model-name': 'album',\n            'data-placeholder': '',\n            'lang': 'en',\n        })\n", "    def test_autocomplete_select_multiple(self):\n        form = AlbumForm()\n        widget = form.fields['featuring'].widget\n        self.assertIsInstance(widget, AutocompleteSelectMultiple)\n        attrs = widget.get_context(name='featuring', value=None, attrs={})['widget']['attrs']\n        self.assertEqual(attrs['class'], 'admin-autocomplete')\n        self.assertEqual(attrs['data-ajax--url'], '/autocomplete/')\n        self.assertJSONEqual(attrs['data-allow-clear'], 'true')\n", "    def test_autocomplete_select_multiple_render_options(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        form = AlbumForm(initial={'featuring': [beatles.pk, who.pk]})\n        output = form.as_table()\n        selected_option_beatles = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        selected_option_who = '<option value=\"%s\" selected>The Who</option>' % who.pk\n        self.assertIn(selected_option_beatles, output)\n        self.assertIn(selected_option_who, output)\n"], "sample_350": ["    def test_union_with_prefetch_related(self):\n        celebrity = Celebrity.objects.create(name='Celebrity 1')\n        reserved_name = ReservedName.objects.create(name='Reserved 1', order=1)\n        qs1 = Celebrity.objects.filter(pk=celebrity.pk).prefetch_related('reservedname_set')\n        qs2 = ReservedName.objects.filter(pk=reserved_name.pk).prefetch_related('celebrity_set')\n        union_qs = qs1.union(qs2)\n        self.assertEqual(union_qs.count(), 2)\n        self.assertIn(celebrity, list(union_qs)[0].reservedname_set.all())\n        self.assertIn(reserved_name, list(union_qs)[1].celebrity_set.all())\n", "    def test_bulk_create(self):\n        Number.objects.bulk_create([Number(num=10), Number(num=11)])\n        self.assertEqual(Number.objects.count(), 12)\n        self.assertTrue(Number.objects.filter(num=10).exists())\n        self.assertTrue(Number.objects.filter(num=11).exists())\n", "    def test_combined_qs_with_different_orderings(self):\n        qs1 = Number.objects.order_by('num')\n        qs2 = Number.objects.order_by('-num')\n        combined_qs = qs1.union(qs2)\n        self.assertNumbersEqual(combined_qs.order_by('num'), list(range(10)))\n        self.assertNumbersEqual(combined_qs.order_by('-num'), list(range(9, -1, -1)))\n", "def test_bulk_create_with_ignore_conflicts(self):\n    # Create a few instances to ensure some conflicts occur.\n    Number.objects.create(num=1, other_num=9)\n    Number.objects.create(num=2, other_num=8)\n    \n    # Create instances with conflicting `num` values.\n    objs = [\n        Number(num=1, other_num=10),\n        Number(num=2, other_num=20),\n        Number(num=3, other_num=30),\n    ]\n    \n    # Attempt to bulk_create with ignore_conflicts=True.\n    created_objs = Number.objects.bulk_create(objs, ignore_conflicts=True)\n    \n    # Only the non-conflicting object should be created.\n    self.assertEqual(len(created_objs), 1)\n    self.assertEqual(created_objs[0].num, 3)\n    \n    # Check the final state of the database.\n    final_qs = Number.objects.order_by('num')\n    self.assertEqual(list(final_qs.values_list('num', 'other_num')), [\n        (1, 9), (2, 8), (3, 30)\n    ])\n", "    def test_difference_with_multiple_subqueries(self):\n        qs1 = Number.objects.filter(num__gte=0, num__lte=5)\n        qs2 = Number.objects.filter(num__gte=2, num__lte=4)\n        qs3 = Number.objects.filter(num=3)\n        self.assertNumbersEqual(qs1.difference(qs2.difference(qs3)), [0, 1, 5], ordered=False)\n"], "sample_351": ["    def test_modelmultiplechoicefield_initial(self):\n        f = forms.ModelMultipleChoiceField(Category.objects.all(), initial=[self.c1.pk, self.c3.pk])\n        self.assertEqual(list(f.initial), [self.c1.pk, self.c3.pk])\n        self.assertEqual(list(f.choices), [\n            ('', '---------'),\n            (self.c1.pk, 'Entertainment'),\n            (self.c2.pk, 'A test'),\n            (self.c3.pk, 'Third'),\n        ])\n        self.assertEqual(f.prepare_value([self.c1.pk, self.c3.pk]), [str(self.c1.pk), str(self.c3.pk)])\n", "    def test_model_to_dict(self):\n        writer = Writer.objects.create(name='Test writer')\n        article = Article.objects.create(\n            title='Test Article',\n            pub_date=datetime.date(2023, 10, 10),\n            writer=writer\n        )\n        article_data = model_to_dict(article)\n        self.assertEqual(article_data['title'], 'Test Article')\n        self.assertEqual(article_data['pub_date'], datetime.date(2023, 10, 10))\n        self.assertEqual(article_data['writer'], writer.pk)\n\n        # Test with fields parameter\n        article_data = model_to_dict(article, fields=['title', 'pub_date'])\n        self.assertEqual(article_data, {\n            'title': 'Test Article',\n            'pub_date': datetime.date(2023, 10, 10)\n        })\n\n        # Test with exclude parameter\n        article_data = model_to_dict(article, exclude=['pub_date'])\n        self.assertEqual(article_data['title'], 'Test Article')\n        self.assertNotIn('pub_date', article_data)\n", "def test_model_to_dict(self):\n        author = Author.objects.create(name='Test Author')\n        book = Book.objects.create(author=author, title='Test Book')\n        expected_dict = {\n            'id': book.id,\n            'author': author.id,\n            'title': 'Test Book',\n        }\n        self.assertEqual(model_to_dict(book), expected_dict)\n\n        # Testing fields parameter\n        expected_dict_fields = {\n            'title': 'Test Book',\n        }\n        self.assertEqual(model_to_dict(book, fields=['title']), expected_dict_fields)\n\n        # Testing exclude parameter\n        expected_dict_exclude = {\n            'id': book.id,\n            'author': author.id,\n        }\n        self.assertEqual(model_to_dict(book, exclude=['title']), expected_dict_exclude)\n", "def test_model_to_dict(self):\n    author = Author.objects.create(name='Test Author')\n    book = Book.objects.create(author=author, title='Test Book', published_date=datetime.date(2022, 1, 1))\n    data = model_to_dict(book)\n    self.assertEqual(data, {\n        'id': book.id,\n        'author': author.id,\n        'title': 'Test Book',\n        'published_date': datetime.date(2022, 1, 1),\n    })\n    \n    data = model_to_dict(book, fields=['title'])\n    self.assertEqual(data, {\n        'title': 'Test Book',\n    })\n    \n    data = model_to_dict(book, exclude=['author'])\n    self.assertEqual(data, {\n        'id': book.id,\n        'title': 'Test Book',\n        'published_date': datetime.date(2022, 1, 1),\n    })\n    \n    data = model_to_dict(book, fields=['author', 'title'], exclude=['title'])\n    self.assertEqual(data, {\n        'author': author.id,\n    })\n", "    def test_model_to_dict(self):\n        article = Article.objects.create(\n            title=\"Test Article\",\n            pub_date=datetime.date(2023, 1, 1),\n            writer=Writer.objects.create(name=\"Test Writer\")\n        )\n        article.categories.add(self.c1, self.c2)\n\n        data = model_to_dict(article)\n        self.assertEqual(data['title'], \"Test Article\")\n        self.assertEqual(data['pub_date'], datetime.date(2023, 1, 1))\n        self.assertEqual(data['writer'], article.writer.pk)\n        self.assertEqual(list(data['categories']), [self.c1.pk, self.c2.pk])\n\n        data_with_fields = model_to_dict(article, fields=['title', 'pub_date'])\n        self.assertEqual(data_with_fields, {\n            'title': \"Test Article\",\n            'pub_date': datetime.date(2023, 1, 1)\n        })\n\n        data_with_exclude = model_to_dict(article, exclude=['title'])\n        self.assertNotIn('title', data_with_exclude)\n        self.assertIn('pub_date', data_with_exclude)\n"], "sample_352": ["    def test_split_having(self):\n        # Create a mock where node with a mix of aggregate and non-aggregate children\n        class MockChild:\n                self.contains_aggregate = contains_aggregate\n\n                if self.contains_aggregate:\n                    return None, self\n                return self, None\n\n        # Non-aggregate child\n        child1 = MockChild(False)\n        # Aggregate child\n        child2 = MockChild(True)\n\n        node = WhereNode(children=[child1, child2], connector=AND, negated=False)\n\n        where_node, having_node = node.split_having()\n\n        self.assertIsNotNone(where_node)\n        self.assertIsNotNone(having_node)\n        self.assertIn(child1, where_node.children)\n        self.assertIn(child2, having_node.children)\n", "    def test_split_having_no_aggregates(self):\n        # Test split_having method when there are no aggregates\n        node = WhereNode(children=[WhereNodeTest.DummyNode(), WhereNodeTest.DummyNode()])\n        where, having = node.split_having()\n        self.assertEqual(where, node)\n        self.assertIsNone(having)\n", "    def test_split_having_no_aggregate(self):\n        where_node = WhereNode()\n        where_node.contains_aggregate = False\n        where_clause, having_clause = where_node.split_having()\n        self.assertIs(where_clause, where_node)\n        self.assertIs(having_clause, None)\n", "    def test_split_having(self):\n        # Test split_having for nodes without aggregates\n        node = WhereNode()\n        self.assertEqual(node.split_having(), (node, None))\n\n        # Test split_having for nodes with aggregates\n        child_node_with_aggregate = WhereNode()\n        child_node_with_aggregate.children = [NothingNode()]\n        node_with_aggregate = WhereNode(children=[child_node_with_aggregate])\n        node_with_aggregate.children[0].children[0].contains_aggregate = True\n        self.assertEqual(node_with_aggregate.split_having(), (None, node_with_aggregate))\n\n        # Test split_having with mixed children\n        child_node_without_aggregate = WhereNode()\n        child_node_without_aggregate.children = [NothingNode()]\n        node_mixed = WhereNode(children=[child_node_with_aggregate, child_node_without_aggregate])\n        where_node, having_node = node_mixed.split_having()\n        self.assertIsNotNone(where_node)\n        self.assertIsNotNone(having_node)\n        self.assertEqual(len(where_node.children), 1)\n        self.assertEqual(len(having_node.children), 1)\n", "    def test_split_having_without_aggregate(self):\n        # Split_having should return the node itself and None if it doesn't contain aggregates.\n        wn = WhereNode()\n        wn.children.append(WhereNodeTest.DummyNode())\n        where_node, having_node = wn.split_having()\n        self.assertIs(where_node, wn)\n        self.assertIsNone(having_node)\n"], "sample_353": ["    def test_command_error_on_invalid_required_field(self):\n        \"\"\"\n        A CommandError should be raised if a required field with a many-to-many\n        relation through model is specified.\n        \"\"\"\n        class FakeUser:\n            REQUIRED_FIELDS = ['groups']\n            USERNAME_FIELD = 'username'\n\n            class _meta:\n                @staticmethod\n                    class FakeField:\n                        many_to_many = True\n\n                        class remote_field:\n                            through = None\n                            _meta = type('meta', (object,), {'auto_created': False})\n\n                    if name == 'groups':\n                        return FakeField()\n                    raise exceptions.FieldDoesNotExist\n\n        with mock.patch('django.contrib.auth.get_user_model', return_value=FakeUser):\n            with self.assertRaisesMessage(CommandError, \"Required field 'groups' specifies a many-to-many relation through model, which is not supported.\"):\n                call_command('createsuperuser', interactive=False, username='joe', email='joe@somewhere.org')\n", "    def test_non_interactive_superuser_creation(self):\n        out = StringIO()\n        call_command('createsuperuser', interactive=False, verbosity=1, stdout=out)\n        self.assertIn(\"Superuser created successfully.\", out.getvalue())\n        user = User.objects.get(username='superuser')\n        self.assertEqual(user.email, 'super@user.com')\n        self.assertTrue(user.check_password('super_password'))\n", "    def setUp(self):\n        self.stdout = StringIO()\n        self.stderr = StringIO()\n", "    def test_required_field_with_m2m_through(self):\n        \"\"\"\n        Test that CommandError is raised when required field specifies a many-to-many\n        relation through model.\n        \"\"\"\n        class FakeUserModel:\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = ['groups']\n            _meta = mock.Mock()\n            _meta.get_field.side_effect = [\n                mock.Mock(\n                    many_to_many=True,\n                    remote_field=mock.Mock(\n                        through=mock.Mock(\n                            _meta=mock.Mock(auto_created=False)\n                        )\n                    )\n                )\n            ]\n\n        with mock.patch('django.contrib.auth.get_user_model', return_value=FakeUserModel):\n            command = createsuperuser.Command()\n            parser = mock.Mock()\n            with self.assertRaisesMessage(CommandError, \"Required field 'groups' specifies a many-to-many relation through model, which is not supported.\"):\n                command.add_arguments(parser)\n", "    def test_createsuperuser_without_password_interactive(self):\n        \"\"\"\n        Test creating a superuser interactively without providing a password.\n        \"\"\"\n        new_io = StringIO()\n        call_command(\n            'createsuperuser',\n            interactive=True,\n            stdin=MockTTY(),\n            stdout=new_io,\n            stderr=new_io,\n        )\n        self.assertIn(\"Superuser created successfully.\", new_io.getvalue().strip())\n        user = User.objects.get(username='alice')\n        self.assertEqual(user.email, 'alice@example.com')\n        self.assertFalse(user.has_usable_password())\n"], "sample_354": ["    def test_invalid_username_special_characters(self):\n        \"\"\"\n        Creation fails if the username contains special characters.\n        \"\"\"\n        new_io = StringIO()\n        entered_passwords = ['password', 'password']\n        invalid_username = \"invalid@username\"\n\n            return entered_passwords.pop(0)\n\n        @mock_inputs({'password': return_passwords, 'username': invalid_username, 'email': ''})\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdin=MockTTY(),\n                stdout=new_io,\n                stderr=new_io,\n            )\n            expected_error = 'Enter a valid username. This value may contain only letters, numbers, and @/./+/-/_ characters.'\n            self.assertIn(expected_error, new_io.getvalue().strip())\n            self.assertFalse(User.objects.filter(username=invalid_username).exists())\n\n        test(self)\n", "    def test_invalid_username_non_interactive(self):\n        \"\"\"Creation fails if the username is invalid in non-interactive mode.\"\"\"\n        user_field = User._meta.get_field(User.USERNAME_FIELD)\n        invalid_username = ('x' * user_field.max_length) + 'y'\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, \"Ensure this value has at most %s characters (it has %s).\" % (user_field.max_length, len(invalid_username))):\n            call_command(\n                'createsuperuser',\n                username=invalid_username,\n                email='test@example.com',\n                interactive=False,\n                stdout=new_io,\n            )\n", "    def test_multiple_superusers_same_username(self):\n        \"Multiple superusers with the same username should be created successfully\"\n        new_io = StringIO()\n\n        @mock_inputs({\n            'username': 'joe',\n            'password': 'nopasswd',\n            'email': 'joe1@somewhere.org',\n        })\n            call_command(\n                \"createsuperuser\",\n                interactive=True,\n                stdout=new_io,\n                stdin=MockTTY(),\n            )\n            command_output = new_io.getvalue().strip()\n            self.assertEqual(command_output, 'Superuser created successfully.')\n\n        createsuperuser()\n\n        @mock_inputs({\n            'username': 'joe',\n            'password': 'nopasswd',\n            'email': 'joe2@somewhere.org',\n        })\n            call_command(\n                \"createsuperuser\",\n                interactive=True,\n                stdout=new_io,\n                stdin=MockTTY(),\n            )\n            command_output = new_io.getvalue().strip()\n            self.assertEqual(command_output, 'Superuser created successfully.')\n\n        createsuperuser()\n\n        users = CustomUserNonUniqueUsername.objects.filter(username=\"joe\")\n        self.assertEqual(users.count(), 2)\n        self.assertEqual(users[0].email, 'joe1@somewhere.org')\n        self.assertEqual(users[1].email, 'joe2@somewhere.org')\n", "    def test_validate_username_success(self):\n        command = createsuperuser.Command()\n        default_username = get_default_username()\n        self.assertIsNone(command._validate_username(default_username, \"username\", \"default\"))\n", "    def test_password_validation_exception(self, mock_validate_password, mock_getpass):\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, 'This is a test error.'):\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                username='testuser',\n                email='test@example.com',\n                stdin=MockTTY(),\n                stdout=new_io,\n            )\n        self.assertIn('This is a test error.', new_io.getvalue())\n"], "sample_355": ["    def setUp(self):\n        self.content_type = ContentType.objects.create(app_label='test_app', model='test_model')\n        self.permission = Permission.objects.create(\n            name='Can test',\n            content_type=self.content_type,\n            codename='can_test',\n        )\n", "    def test_create_user(self):\n        user = User.objects.create_user(username='testuser', email='testuser@example.com', password='testpassword')\n        self.assertEqual(user.username, 'testuser')\n        self.assertEqual(user.email, 'testuser@example.com')\n        self.assertTrue(user.check_password('testpassword'))\n        self.assertFalse(user.is_staff)\n        self.assertFalse(user.is_superuser)\n", "    def test_create_user_without_username(self):\n        with self.assertRaisesMessage(ValueError, 'The given username must be set'):\n            User.objects.create_user(username=None, email='test@example.com', password='test')\n", "    def test_create_user_without_username(self):\n        with self.assertRaises(ValueError) as context:\n            User.objects.create_user(username='', email='test@example.com', password='testpassword')\n        self.assertEqual(str(context.exception), 'The given username must be set')\n", "    def test_create_user(self):\n        user = User.objects.create_user(username='normaluser', email='normal@example.com', password='testpass123')\n        self.assertEqual(user.username, 'normaluser')\n        self.assertEqual(user.email, 'normal@example.com')\n        self.assertFalse(user.is_staff)\n        self.assertFalse(user.is_superuser)\n"], "sample_356": ["    def test_alter_model_managers_options(self):\n        \"\"\"\n        Changing the model managers along with other options should add new operations.\n        \"\"\"\n        pony_with_managers = ModelState(\"otherapp\", \"Pony\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], managers=[\n            ('food_qs', FoodQuerySet.as_manager()),\n            ('food_mgr', FoodManager('a', 'b')),\n            ('food_mgr_kwargs', FoodManager('x', 'y', 3, 4)),\n        ])\n\n        pony_with_managers_and_options = ModelState(\"otherapp\", \"Pony\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], managers=[\n            ('food_qs', FoodQuerySet.as_manager()),\n            ('food_mgr', FoodManager('a', 'b')),\n            ('food_mgr_kwargs', FoodManager('x', 'y', 3, 4)),\n        ], options={\n            'permissions': [('can_fly', 'Can Fly')],\n        })\n\n        changes = self.get_changes([pony_with_managers], [pony_with_managers_and_options])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterModelManagers\", \"AlterModelOptions\"])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"pony\")\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, name=\"pony\", options={\n            'permissions': [('can_fly', 'Can Fly')],\n        })\n", "def test_rename_model_with_db_table(self):\n    \"\"\"\n    Ensure renaming a model with a custom db_table is handled correctly.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_db_table_options],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"NewAuthor\")\n    # Ensure the db_table stays the same\n    self.assertNotIn(\"AlterModelTable\", [op.__class__.__name__ for op in changes['testapp'][0].operations])\n", "    def test_generate_created_models(self):\n        \"\"\"\n        Test the `generate_created_models` method to ensure it correctly\n        generates creation operations for new models including their related\n        fields, indexes, constraints, and options.\n        \"\"\"\n        before = self.make_project_state([])\n        after = self.make_project_state([self.author_with_book, self.book, self.publisher])\n\n        autodetector = MigrationAutodetector(before, after)\n        autodetector._detect_changes()\n\n        # Generate created models\n        autodetector.generate_created_models()\n\n        # Ensure the correct operations are created\n        generated_operations = autodetector.generated_operations\n\n        self.assertIn('testapp', generated_operations)\n        self.assertIn('otherapp', generated_operations)\n\n        # Check that the 'Author' model creation is detected correctly\n        author_ops = generated_operations['testapp']\n        self.assertEqual(len(author_ops), 2)\n        self.assertIsInstance(author_ops[0], operations.CreateModel)\n        self.assertEqual(author_ops[0].name, 'Author')\n        self.assertIsInstance(author_ops[1], operations.AddField)\n        self.assertEqual(author_ops[1].name, 'book')\n\n        # Check that the 'Book' model creation is detected correctly\n        book_ops = generated_operations['otherapp']\n        self.assertEqual(len(book_ops), 1)\n        self.assertIsInstance(book_ops[0], operations.CreateModel)\n        self.assertEqual(book_ops[0].name, 'Book')\n\n        # Check that the 'Publisher' model creation is detected correctly\n        publisher_ops = generated_operations['testapp']\n        self.assertEqual(len(publisher_ops), 2)  # 'Author' and 'Publisher'\n        self.assertIsInstance(publisher_ops[1], operations.CreateModel)\n        self.assertEqual(publisher_ops[1].name, 'Publisher')\n", "    def test_deep_deconstruct_partial(self):\n        \"\"\"\n        Tests deep_deconstruct method for functools.partial.\n        \"\"\"\n            return a + b\n\n        partial_func = functools.partial(some_function, 1, b=3)\n        autodetector = MigrationAutodetector(None, None)\n        deconstructed = autodetector.deep_deconstruct(partial_func)\n        self.assertEqual(deconstructed, (\n            some_function,\n            (1,),\n            {'b': 3}\n        ))\n", "def test_deep_deconstruct_with_regex(self):\n        \"\"\"Test deep_deconstruct handling of regex objects.\"\"\"\n        regex = re.compile(r'^test_.*$')\n        from_state = ModelState(\n            \"testapp\", \"model\", [(\"name\", models.CharField(primary_key=True, validators=[RegexValidator(regex)]))]\n        )\n        to_state = ModelState(\n            \"testapp\", \"model\", [(\"name\", models.CharField(primary_key=True, validators=[RegexValidator(regex)]))]\n        )\n        changes = self.get_changes([from_state], [to_state])\n        # No changes should be detected as the regex is the same\n        self.assertNumberMigrations(changes, \"testapp\", 0)\n"], "sample_357": ["def test_deep_deconstruct_partial_function(self):\n        \"\"\"\n        Test the deep_deconstruct method with functools.partial objects.\n        \"\"\"\n            return x + y\n\n        partial_func = functools.partial(example_function, 1, y=2)\n        deconstructed = MigrationAutodetector.deep_deconstruct(self, partial_func)\n        self.assertEqual(deconstructed, (example_function, (1,), {'y': 2}))\n\n        # Test with nested partials\n        nested_partial_func = functools.partial(partial_func, y=3)\n        deconstructed_nested = MigrationAutodetector.deep_deconstruct(self, nested_partial_func)\n        self.assertEqual(deconstructed_nested, (example_function, (1,), {'y': 3}))\n", "    def test_field_renamed_and_model_renamed(self):\n        \"\"\"\n        Tests autodetection when a field and its related model are both renamed.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_book, self.book],\n            [self.author_renamed_with_book, self.book_with_field_and_author_renamed],\n            MigrationQuestioner({\"ask_rename\": True, \"ask_rename_model\": True}),\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Writer\")\n        # Right number/type of migrations for related field rename?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, old_name=\"author\", new_name=\"writer\")\n", "    def test_deep_deconstruct_with_partial(self):\n        \"\"\"Test deep deconstruction for functools.partial objects.\"\"\"\n        # Create a partial object\n            pass\n\n        partial_obj = functools.partial(test_func, 1, c=2)\n        autodetector = MigrationAutodetector(None, None)\n\n        # Perform deep deconstruct\n        result = autodetector.deep_deconstruct(partial_obj)\n\n        # Expected deconstruction\n        expected = (test_func, (1,), {\"c\": 2})\n\n        self.assertEqual(result, expected)\n", "    def test_deep_deconstruct_dict_with_regex(self):\n        \"\"\"Test deep_deconstruct method with dict containing a regex object.\"\"\"\n        state_with_regex = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"data\", models.JSONField(default={\n                'pattern': re.compile(r'^\\w+$')\n            })),\n        ])\n        state_with_same_regex = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"data\", models.JSONField(default={\n                'pattern': re.compile(r'^\\w+$')\n            })),\n        ])\n        changes = self.get_changes([state_with_regex], [state_with_same_regex])\n        self.assertEqual(len(changes), 0)\n", "    def test_alter_model_managers_with_different_order(self):\n        \"\"\"\n        Changing the order of model managers should add a new operation.\n        \"\"\"\n        pony_food_mgr_reordered = ModelState(\"otherapp\", \"Pony\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], managers=[\n            ('food_mgr', FoodManager('a', 'b')),\n            ('food_qs', FoodQuerySet.as_manager()),\n            ('food_mgr_kwargs', FoodManager('x', 'y', 3, 4)),\n        ])\n\n        changes = self.get_changes([self.other_pony_food], [pony_food_mgr_reordered])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterModelManagers\"])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"pony\")\n        self.assertEqual([name for name, mgr in changes['otherapp'][0].operations[0].managers],\n                         ['food_mgr', 'food_qs', 'food_mgr_kwargs'])\n        self.assertEqual(changes['otherapp'][0].operations[0].managers[0][1].args, ('a', 'b', 1, 2))\n        self.assertEqual(changes['otherapp'][0].operations[0].managers[2][1].args, ('x', 'y', 3, 4))\n"], "sample_358": ["    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), opclasses=['opclass1', 'opclass2']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), opclasses=['opclass1', 'opclass2']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), opclasses=['opclass1', 'opclass2']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table',\n            ['first_column', 'second_column'],\n            quote_name=lambda column: column.upper(),\n            col_suffixes=['ASC', 'DESC'],\n            opclasses=['varchar_pattern_ops', 'text_pattern_ops'],\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), opclasses=['opclass1', 'opclass2']\n        )\n"], "sample_359": ["    def test_add_constraint_with_expression_index(self):\n        \"\"\"\n        Tests the AddConstraint operation with an expression index.\n        \"\"\"\n        app_label = \"test_addconstraint_expr\"\n        project_state = self.set_up_test_model(app_label)\n        constraint = models.CheckConstraint(\n            check=models.Q(pink__gt=models.F('weight') * 2),\n            name=\"test_constraint_check_expr\"\n        )\n        operation = migrations.AddConstraint(\"Pony\", constraint)\n        self.assertEqual(\n            operation.describe(), \"Create constraint test_constraint_check_expr on model Pony\"\n        )\n        self.assertEqual(\n            operation.migration_name_fragment,\n            'pony_test_constraint_check_expr',\n        )\n        # Test the state alteration\n        new_state = project_state.clone()\n        operation.state_forwards(app_label, new_state)\n        self.assertEqual(len(new_state.models[app_label, \"pony\"].options[\"constraints\"]), 1)\n        Pony = new_state.apps.get_model(app_label, \"Pony\")\n        self.assertEqual(len(Pony._meta.constraints), 1)\n        # Test the database alteration\n        with connection.schema_editor() as editor:\n            operation.database_forwards(app_label, editor, project_state, new_state)\n        with self.assertRaises(IntegrityError), transaction.atomic():\n            Pony.objects.create(pink=1, weight=1.0)\n        with self.assertRaises(IntegrityError), transaction.atomic():\n            Pony.objects.create(pink=2, weight=1.0)\n        Pony.objects.create(pink=3, weight=1.0)\n        # Test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(app_label, editor, new_state, project_state)\n        Pony.objects.create(pink=1, weight=1.0)\n        # Test deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AddConstraint\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(definition[2], {'model_name': 'Pony', 'constraint': constraint})\n", "    def test_delete_model_with_constraint(self):\n        \"\"\"\n        Test the DeleteModel operation to ensure constraints are appropriately handled.\n        \"\"\"\n        app_label = 'test_delmodelconstraint'\n        project_state = self.set_up_test_model(app_label)\n        \n        # Add a check constraint to the Pony model\n        check_constraint = models.CheckConstraint(check=models.Q(pink__gt=2), name='pony_pink_gt_2')\n        add_constraint_operation = migrations.AddConstraint(\n            \"Pony\", \n            check_constraint\n        )\n        \n        # Apply the constraint\n        new_state = project_state.clone()\n        add_constraint_operation.state_forwards(app_label, new_state)\n        with connection.schema_editor() as editor:\n            add_constraint_operation.database_forwards(app_label, editor, project_state, new_state)\n        \n        # Ensure the constraint is in place\n        Pony = new_state.apps.get_model(app_label, \"Pony\")\n        with self.assertRaises(IntegrityError), transaction.atomic():\n            Pony.objects.create(pink=1, weight=1.0)\n        \n        # Delete the model\n        operation = migrations.DeleteModel(\"Pony\")\n        self.assertEqual(operation.describe(), \"Delete model Pony\")\n        self.assertEqual(operation.migration_name_fragment, 'delete_pony')\n        \n        # Update state and apply operation\n        final_state = new_state.clone()\n        operation.state_forwards(app_label, final_state)\n        with connection.schema_editor() as editor:\n            operation.database_forwards(app_label, editor, new_state, final_state)\n        \n        # Ensure model table is deleted\n        self.assertTableNotExists(f'{app_label}_pony')\n        \n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(app_label, editor, final_state, new_state)\n        \n        # Ensure model table is restored\n        self.assertTableExists(f'{app_label}_pony')\n        \n        # Ensure the constraint is also restored\n        with self.assertRaises(IntegrityError), transaction.atomic():\n            Pony.objects.create(pink=1, weight=1.0)\n", "    def test_create_model_with_duplicate_options(self):\n        \"\"\"\n        Tests the CreateModel operation with duplicate options in unique_together and index_together.\n        \"\"\"\n        with self.assertRaisesMessage(ValueError, 'Found duplicate value pink in CreateModel unique_together argument.'):\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(default=1)),\n                ],\n                options={\n                    'unique_together': {('pink', 'pink')},\n                }\n            )\n\n        with self.assertRaisesMessage(ValueError, 'Found duplicate value pink in CreateModel index_together argument.'):\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(default=1)),\n                ],\n                options={\n                    'index_together': {('pink', 'pink')},\n                }\n            )\n", "    def test_delete_model_with_custom_table_name(self):\n        \"\"\"\n        Tests the DeleteModel operation on a model with a custom table name.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_dlwctn\", table_name=\"custom_table_name\")\n        # Test the state alteration\n        operation = migrations.DeleteModel(\"Pony\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_dlwctn\", new_state)\n        self.assertNotIn((\"test_dlwctn\", \"pony\"), new_state.models)\n        # Test the database alteration\n        self.assertTableExists(\"custom_table_name\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_dlwctn\", editor, project_state, new_state)\n        self.assertTableNotExists(\"custom_table_name\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_dlwctn\", editor, new_state, project_state)\n        self.assertTableExists(\"custom_table_name\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"DeleteModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(list(definition[2]), [\"name\"])\n", "    def test_create_model_with_bases(self):\n        \"\"\"\n        Tests the CreateModel operation with specified bases.\n        \"\"\"\n        operation = migrations.CreateModel(\n            \"DerivedPony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"color\", models.CharField(max_length=30)),\n            ],\n            bases=(\"test_crb.Pony\",),\n        )\n        self.assertEqual(operation.describe(), \"Create model DerivedPony\")\n        self.assertEqual(operation.migration_name_fragment, 'derivedpony')\n        # Test the state alteration\n        project_state = self.set_up_test_model(\"test_crb\", with_base_model=True)\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crb\", new_state)\n        self.assertEqual(new_state.models[\"test_crb\", \"derivedpony\"].name, \"DerivedPony\")\n        self.assertEqual(len(new_state.models[\"test_crb\", \"derivedpony\"].fields), 2)\n        # Test the database alteration\n        self.assertTableNotExists(\"test_crb_derivedpony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_crb\", editor, project_state, new_state)\n        self.assertTableExists(\"test_crb_derivedpony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crb\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_crb_derivedpony\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"CreateModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"bases\", \"fields\", \"name\"])\n"], "sample_360": ["    def test_should_update_cache(self):\n        middleware = UpdateCacheMiddleware(empty_response)\n        request = HttpRequest()\n        request._cache_update_cache = True\n\n        response = HttpResponse('test response')\n        self.assertTrue(middleware._should_update_cache(request, response))\n\n        request._cache_update_cache = False\n        self.assertFalse(middleware._should_update_cache(request, response))\n", "    def tearDown(self):\n        cache.clear()\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.middleware = UpdateCacheMiddleware(lambda req: HttpResponse(\"test response\"))\n", "    def tearDown(self):\n        cache.clear()\n", "    def setUp(self):\n        self.middleware = UpdateCacheMiddleware(lambda req: HttpResponse(\"Content\"))\n"], "sample_361": ["    def test_avoid_wrapping(self):\n        items = (\n            (\"Hello world\", \"Hello\\xa0world\"),\n            (\"Avoid wrapping in the middle\", \"Avoid\\xa0wrapping\\xa0in\\xa0the\\xa0middle\"),\n            (\"Multiple  spaces\", \"Multiple\\xa0\\xa0spaces\"),\n            (\"NoSpacesHere\", \"NoSpacesHere\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"no wrap\", \"no\\xa0wrap\"),\n            (\"wrap me\", \"wrap\\xa0me\"),\n            (\"Django utils\", \"Django\\xa0utils\"),\n            (\"HTML utilities\", \"HTML\\xa0utilities\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "def test_avoid_wrapping(self):\n    items = (\n        (\"Avoid text wrapping\", \"Avoid\\xa0text\\xa0wrapping\"),\n        (\"Space    and\\tTab\", \"Space\\xa0\\xa0\\xa0\\xa0and\\tTab\"),\n        (\"Line\\nbreak\", \"Line\\nbreak\"),\n        (\"Normal text\", \"Normal\\xa0text\"),\n        (lazystr(\"Lazy string with spaces\"), \"Lazy\\xa0string\\xa0with\\xa0spaces\"),\n    )\n    for value, output in items:\n        with self.subTest(value=value, output=output):\n            self.check_output(avoid_wrapping, value, output)\n", "    def test_avoid_wrapping(self):\n        tests = (\n            ('Hello world', 'Hello\\xa0world'),\n            ('This is a test', 'This\\xa0is\\xa0a\\xa0test'),\n            ('No spaces', 'No\\xa0spaces'),\n            ('Multiple   spaces', 'Multiple\\xa0\\xa0\\xa0spaces'),\n            (lazystr('Lazy string test'), 'Lazy\\xa0string\\xa0test'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"no wrapping\", \"no wrapping\"),\n            (\"avoid wrapping text\", \"avoid\\xa0wrapping\\xa0text\"),\n            (\"multiple  spaces\", \"multiple\\xa0\\xa0spaces\"),\n            (\"leading space\", \"leading\\xa0space\"),\n            (\"trailing space \", \"trailing\\xa0space\\xa0\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n"], "sample_362": ["    def test_rename_model_with_fk_to_new_model(self):\n        \"\"\"\n        Tests autodetection of renamed models while simultaneously adding a new\n        model with a foreign key to the renamed model.\n        \"\"\"\n        before = [self.author_with_book, self.book]\n        after = [\n            self.author_renamed_with_book, \n            self.book_with_author_renamed, \n            ModelState(\"testapp\", \"NewModel\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Writer\", models.CASCADE)),\n            ])\n        ]\n        changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename_model\": True}))\n        \n        # Check the number and types of migrations\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"CreateModel\"])\n        \n        # Verify the rename model operation\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Writer\")\n        \n        # Verify the new model creation with the foreign key\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"NewModel\")\n        self.assertEqual(\n            [f for f in changes['testapp'][0].operations[1].fields if f[0] == 'author'][0][1].remote_field.model,\n            'testapp.Writer'\n        )\n", "    def test_remove_m2m_and_create_new(self):\n        \"\"\"\n        Removing an m2m and creating a new one should be detected correctly.\n        \"\"\"\n        changes = self.get_changes(\n            [self.book_with_multiple_authors, self.author_name],\n            [self.book_with_multiple_authors_through_attribution, self.author_name, self.attribution]\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, ['RemoveField', 'CreateModel', 'AddField'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name='book', name='authors')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, name='Attribution')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 2, model_name='book', name='authors')\n", "    def test_adding_field_with_foreign_key(self):\n        \"\"\"Tests autodetection when adding a field with a ForeignKey.\"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_book, self.book])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"book\")\n        self.assertMigrationDependencies(changes, 'testapp', 0, [(\"otherapp\", \"__first__\")])\n", "    def test_generate_renamed_models(self):\n        \"\"\"Test detection of renamed models.\"\"\"\n        changes = self.get_changes(\n            [self.author_with_book, self.book],\n            [self.author_renamed_with_book, self.book_with_author_renamed],\n            MigrationQuestioner({\"ask_rename_model\": True}),\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Writer\")\n        # Make sure related models are updated correctly.\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name='book', name='author')\n", "    def test_deep_deconstruct_with_partial_functions(self):\n        \"\"\"\n        Ensure deep_deconstruct handles functools.partial objects correctly.\n        \"\"\"\n        # Create a partial function object\n            return a + b + c\n\n        partial_func = functools.partial(sample_function, a=1, b=2)\n\n        # Use deep_deconstruct to deconstruct the partial function object\n        autodetector = MigrationAutodetector(None, None)\n        deconstructed = autodetector.deep_deconstruct(partial_func)\n\n        # Assert the deconstructed form is as expected\n        self.assertEqual(deconstructed, (sample_function, (), {'a': 1, 'b': 2, 'c': None}))\n\n        # Now create a model state that uses this partial function as a default value\n        model_state = ModelState('app', 'model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('value', models.IntegerField(default=partial_func)),\n        ])\n\n        # Ensure that no changes are detected if the state remains the same\n        changes = self.get_changes([model_state], [model_state])\n        self.assertNumberMigrations(changes, 'app', 0)\n\n        # Modify the partial function and ensure changes are detected\n        modified_partial_func = functools.partial(sample_function, a=1, b=3)\n        modified_model_state = ModelState('app', 'model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('value', models.IntegerField(default=modified_partial_func)),\n        ])\n        changes = self.get_changes([model_state], [modified_model_state])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterField'])\n"], "sample_363": ["    def test_get_url(self):\n        rel = Album._meta.get_field('band').remote_field\n        w = widgets.AutocompleteSelect(rel, widget_admin_site)\n        self.assertEqual(\n            w.get_url(),\n            reverse('admin:autocomplete', current_app=widget_admin_site.name)\n        )\n", "    def test_attrs(self):\n        w = widgets.AdminTextInputWidget()\n        self.assertHTMLEqual(\n            w.render('test', 'sample text'),\n            '<input value=\"sample text\" type=\"text\" class=\"vTextField\" name=\"test\">',\n        )\n        w = widgets.AdminTextInputWidget(attrs={'class': 'customTextField'})\n        self.assertHTMLEqual(\n            w.render('test', 'sample text'),\n            '<input value=\"sample text\" type=\"text\" class=\"customTextField\" name=\"test\">',\n        )\n", "    def test_attrs(self):\n        w = widgets.AdminEmailInputWidget()\n        self.assertHTMLEqual(\n            w.render('test', 'user@example.com'),\n            '<input type=\"email\" class=\"vTextField\" name=\"test\" value=\"user@example.com\">'\n        )\n        w = widgets.AdminEmailInputWidget(attrs={'class': 'customEmailInput'})\n        self.assertHTMLEqual(\n            w.render('test', 'user@example.com'),\n            '<input type=\"email\" class=\"customEmailInput\" name=\"test\" value=\"user@example.com\">'\n        )\n", "    def test_attrs(self):\n        w = widgets.AdminTextInputWidget()\n        self.assertHTMLEqual(\n            w.render('test', 'Sample text'),\n            '<input value=\"Sample text\" type=\"text\" class=\"vTextField\" name=\"test\">',\n        )\n        # pass attrs to widget\n        w = widgets.AdminTextInputWidget(attrs={'size': 20, 'class': 'myTextField'})\n        self.assertHTMLEqual(\n            w.render('test', 'Sample text'),\n            '<input value=\"Sample text\" type=\"text\" class=\"myTextField\" name=\"test\" size=\"20\">',\n        )\n", "    def test_attrs(self):\n        w = widgets.AdminIntegerFieldWidget()\n        self.assertHTMLEqual(\n            w.render('test', 12345),\n            '<input value=\"12345\" type=\"number\" class=\"vIntegerField\" name=\"test\">',\n        )\n        w = widgets.AdminIntegerFieldWidget(attrs={'class': 'myIntegerField'})\n        self.assertHTMLEqual(\n            w.render('test', 12345),\n            '<input value=\"12345\" type=\"number\" class=\"myIntegerField\" name=\"test\">',\n        )\n"], "sample_364": ["    def test_include_with_invalid_tuple_length(self):\n        msg = 'Passing a 3-tuple to include() is not supported. Pass a 2-tuple containing the list of patterns and app_name, and provide the namespace argument to include() instead.'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            include(([], 'app_name', 'extra_value'))\n", "    def test_include_with_invalid_tuple_length(self):\n        msg = 'Passing a 3-tuple to include() is not supported. Pass a 2-tuple containing the list of patterns and app_name, and provide the namespace argument to include() instead.'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            include((None, 'app_name', 'extra'))\n", "def test_include_with_invalid_tuple(self):\n        msg = 'Passing a 3-tuple to include() is not supported. Pass a 2-tuple containing the list of patterns and app_name, and provide the namespace argument to include() instead.'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            include((1, 2, 3))\n\n        urlpatterns = [path('test/', empty_view, name='test')]\n        urlconf_tuple = (urlpatterns, 'app_name')\n        module, app_name, namespace = include(urlconf_tuple, namespace='custom_ns')\n        self.assertEqual(module, urlpatterns)\n        self.assertEqual(app_name, 'app_name')\n        self.assertEqual(namespace, 'custom_ns')\n\n        with self.assertRaises(ImproperlyConfigured):\n            include('some.module', namespace='namespace')\n", "def test_include_with_invalid_tuple_length(self):\n    msg = 'Passing a 3-tuple to include() is not supported. Pass a 2-tuple containing the list of patterns and app_name, and provide the namespace argument to include() instead.'\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        include(('module', 'app_name', 'extra'))\n", "    def test_include_with_tuple_argument(self):\n        from django.urls import include, path\n\n            pass\n\n        urlpatterns = [\n            path('app/', include(('dummy_module', 'dummy_app_name')))\n        ]\n\n        self.assertEqual(urlpatterns[0].app_name, 'dummy_app_name')\n        self.assertEqual(urlpatterns[0].urlconf_module, 'dummy_module')\n"], "sample_365": ["compilation error", "    def test_partition(self):\n        self.assertEqual(partition(lambda x: x > 3, range(5)), ([0, 1, 2, 3], [4]))\n        self.assertEqual(partition(lambda x: x % 2 == 0, range(5)), ([1, 3], [0, 2, 4]))\n        self.assertEqual(partition(lambda x: True, range(5)), ([], [0, 1, 2, 3, 4]))\n        self.assertEqual(partition(lambda x: False, range(5)), ([0, 1, 2, 3, 4], []))\n", "    def test_lazy_decorator_with_arguments(self):\n        @keep_lazy(str)\n            return val.upper()\n\n        # Testing with immediate evaluation\n        self.assertEqual(test_func(\"hello\"), \"HELLO\")\n\n        # Testing with lazy evaluation\n        lazy_val = lazy(lambda: \"world\", str)\n        self.assertEqual(test_func(lazy_val()), \"WORLD\")\n", "    def test_lazy_deepcopy(self):\n        \"\"\"Ensure that lazy instances can be deep copied.\"\"\"\n        lazy_obj = lazy(lambda: [1, 2, 3], list)()\n        copied_lazy_obj = copy.deepcopy(lazy_obj)\n        self.assertEqual(lazy_obj, copied_lazy_obj)\n        self.assertIsNot(lazy_obj, copied_lazy_obj)\n", "    def test_lazy_deepcopy(self):\n        \"\"\"\n        Lazy objects should support deepcopy properly.\n        \"\"\"\n        lazy_obj = lazy(lambda: [1, 2, 3], list)\n        original = lazy_obj()\n        copied = copy.deepcopy(original)\n        self.assertEqual(original, copied)\n        self.assertIsNot(original, copied)\n"], "sample_366": ["    def test_parse_datetime_invalid_formats(self):\n        invalid_inputs = (\n            '2012-04-23T09:15:60',  # Invalid second\n            '2012-04-23T09:61:00',  # Invalid minute\n            '2012-13-23T09:15:00',  # Invalid month\n            '2012-04-31T09:15:00',  # Invalid day\n            '2012-04-23T25:15:00',  # Invalid hour\n            '2012-04-23T09:15:00+25:00',  # Invalid time zone offset\n            '2012-04-23T09:15:00-25:00',  # Invalid time zone offset\n            '2012-04-23T09:15:00+04:60',  # Invalid time zone offset minute\n            '2012-04-23T09:15:00-04:60',  # Invalid time zone offset minute\n            '2012-04-23 09:15:00',  # Missing 'T' separator\n            '20120423T091500',  # Missing separators\n            '2012-04-23T09:15:00+04',  # Incomplete time zone offset\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_datetime(source))\n", "    def test_parse_incomplete_iso_8601(self):\n        test_values = (\n            ('PT', None),\n            ('P', None),\n            ('PT5', None),\n            ('PT5H5', None),\n            ('PT5H5M5', None),\n            ('PT5H5M5S5', None),\n            ('PT5H5M5S5.5', None),\n            ('P-5D', None),\n            ('PT5H-5M', None),\n            ('PT5H5M-5S', None),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_invalid_parse_date(self):\n        invalid_inputs = (\n            '2012-04-31',  # April has only 30 days\n            '2012-02-30',  # February has only 28 or 29 days\n            '2011-02-29',  # 2011 is not a leap year\n            '2012-00-10',  # Month zero is invalid\n            '2012-13-01',  # There is no 13th month\n            '2012-01-32',  # January has only 31 days\n            '0000-01-01',  # Year zero is invalid in datetime.date\n            '10000-01-01',  # Year is out of valid range for datetime.date\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_date(source))\n", "    def test_parse_invalid_durations(self):\n        invalid_durations = [\n            'invalid',\n            '1 day 25:00:00',  # invalid hour\n            '1 day 00:61:00',  # invalid minute\n            '1 day 00:00:61',  # invalid second\n            '1 day 00:00:00.0000001',  # invalid microsecond\n            'P5Y',  # unsupported ISO 8601 year format\n            'P5M',  # unsupported ISO 8601 month format\n            '5 days 25:61:61',  # completely invalid time\n        ]\n        for invalid in invalid_durations:\n            with self.subTest(invalid=invalid):\n                self.assertIsNone(parse_duration(invalid))\n", "    def test_parse_invalid_formats(self):\n        # Invalid date formats\n        self.assertIsNone(parse_date('April 23, 2012'))\n        self.assertIsNone(parse_date('23/04/2012'))\n        self.assertIsNone(parse_date('2012/04/23'))\n        # Invalid time formats\n        self.assertIsNone(parse_time('24:00:00'))\n        self.assertIsNone(parse_time('12:60:00'))\n        self.assertIsNone(parse_time('12:00:60'))\n        self.assertIsNone(parse_time('12:00:00.1234567'))\n        self.assertIsNone(parse_time('12:00:00,1234567'))\n        # Invalid datetime formats\n        self.assertIsNone(parse_datetime('2012-04-23T24:00:00'))\n        self.assertIsNone(parse_datetime('2012-04-23 12:60:00'))\n        self.assertIsNone(parse_datetime('2012-04-23 12:00:60'))\n        self.assertIsNone(parse_datetime('2012-04-23T12:00:00.1234567'))\n        self.assertIsNone(parse_datetime('2012-04-23T12:00:00,1234567'))\n        # Invalid duration formats\n        self.assertIsNone(parse_duration('P4M3D'))\n        self.assertIsNone(parse_duration('P4D3M'))\n        self.assertIsNone(parse_duration('P4DT3M'))\n        self.assertIsNone(parse_duration('PT5H4D'))\n        self.assertIsNone(parse_duration('P1Y2M3W4DT5H6M7.8S'))\n"], "sample_367": ["    def test_cache_page_with_invalid_request(self):\n            return HttpResponse(\"response\")\n\n        my_view_cached = cache_page(123)(my_view)\n        \n        class InvalidRequest:\n            pass\n\n        invalid_request = InvalidRequest()\n        msg = (\n            \"cache_control didn't receive an HttpRequest. If you are \"\n            \"decorating a classmethod, be sure to use @method_decorator.\"\n        )\n\n        with self.assertRaisesMessage(TypeError, msg):\n            my_view_cached(invalid_request)\n", "    def test_cache_page_with_invalid_request(self):\n        \"\"\"\n        Test cache_page decorator to ensure it raises TypeError when not given an HttpRequest.\n        \"\"\"\n            return HttpResponse(\"OK\")\n\n        my_view_cached = cache_page(123)(my_view)\n        msg = (\n            \"cache_page didn't receive an HttpRequest. If you are \"\n            \"decorating a classmethod, be sure to use @method_decorator.\"\n        )\n\n        with self.assertRaisesMessage(TypeError, msg):\n            my_view_cached(\"invalid_request\")\n", "    def test_cache_page_decorator_http_request(self):\n        class MyClass:\n            @cache_page(60 * 15)\n                return HttpResponse()\n\n        request = HttpRequest()\n        response = MyClass().a_view(request)\n        self.assertEqual(response.status_code, 200)\n", "    def test_cache_control_decorator_headers(self):\n        @cache_control(max_age=3600, public=True)\n            return HttpResponse()\n\n        request = HttpRequest()\n        response = a_view(request)\n        self.assertEqual(response.headers['Cache-Control'], 'max-age=3600, public')\n", "    def test_cache_control_decorator_headers(self):\n        @cache_control(no_cache=True, no_store=True, must_revalidate=True, max_age=3600)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertIn('Cache-Control', response.headers)\n        self.assertEqual(response.headers['Cache-Control'], 'no-cache, no-store, must-revalidate, max-age=3600')\n"], "sample_368": ["    def test_mixed_plan_with_dependencies(self):\n        \"\"\"\n        Tests that migrations with dependencies are handled correctly in mixed plans.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Fake apply a set of migrations\n        executor.migrate([\n            (\"migrations\", \"0001_initial\"),\n            (\"migrations\", \"0002_second\"),\n        ], fake=True)\n        executor.loader.build_graph()\n        \n        # Introduce a new migration with dependencies\n        new_migration = migrations.Migration(\n            \"0003_third\",\n            \"migrations\",\n            dependencies=[\n                (\"migrations\", \"0002_second\"),\n                (\"migrations2\", \"0001_initial\"),\n            ],\n            operations=[\n                migrations.CreateModel(\n                    name=\"ThirdModel\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                    ],\n                ),\n            ],\n        )\n        executor.loader.graph.add_node((\"migrations\", \"0003_third\"), new_migration)\n\n        # Create a plan that involves both forwards and backwards migrations\n        plan = executor.migration_plan([\n            (\"migrations\", None),\n            (\"migrations2\", \"0001_initial\"),\n        ])\n        msg = (\n            'Migration plans with both forwards and backwards migrations are '\n            'not supported. Please split your migration process into separate '\n            'plans of only forwards OR backwards migrations.'\n        )\n        with self.assertRaisesMessage(InvalidMigrationPlan, msg) as cm:\n            executor.migrate(None, plan)\n        self.assertEqual(\n            cm.exception.args[1],\n            [\n                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], True),\n                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n                (executor.loader.graph.nodes[\"migrations2\", \"0001_initial\"], False),\n            ],\n        )\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n        executor.migrate([\n            (\"migrations\", None),\n            (\"migrations2\", None),\n        ])\n        # Are the tables gone?\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        self.assertTableNotExists(\"migrations2_otherauthor\")\n        self.assertTableNotExists(\"migrations_thirdmodel\")\n", "    def test_migration_plan_with_replacement(self):\n        \"\"\"\n        Tests that the migration plan is recalculated correctly when replacements exist.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Mock the loader to have a replacement scenario\n        original_loader = executor.loader\n        with mock.patch.object(executor, 'loader', wraps=original_loader) as mocked_loader:\n            mocked_loader.replace_migrations = True\n            # Initial plan that should cause a recalculation due to replacements\n            plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n            self.assertEqual(\n                plan,\n                [\n                    (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n                    (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n                ],\n            )\n            # Ensure the migration plan was recalculated due to replacements\n            self.assertTrue(mocked_loader.build_graph.called)\n", "    def test_create_project_state_with_applied_migrations(self):\n        \"\"\"\n        Tests creating a project state with applied migrations included.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        # Apply initial migrations\n        executor.migrate([(\"migrations\", \"0001_initial\")])\n        state = executor._create_project_state(with_applied_migrations=True)\n        self.assertIn(('migrations', 'author'), state.models)\n        self.assertIn(('migrations', 'book'), state.models)\n        # Cleanup by migrating back to the initial state\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n", "    def test_apply_migration(self):\n        \"\"\"\n        Tests the apply_migration method of the MigrationExecutor class.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n        state = executor._create_project_state()\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n        \n        # Apply the migration\n        new_state = executor.apply_migration(state, migration)\n        \n        # Check if the migration was applied correctly\n        self.assertTableExists(\"migrations_author\")\n        self.assertTableExists(\"migrations_book\")\n        \n        # Validate the state after migration\n        self.assertIn(('migrations', 'author'), new_state.models)\n        self.assertIn(('migrations', 'book'), new_state.models)\n        \n        # Cleanup by migrating back to None\n        executor.loader.build_graph()\n        executor.migrate([(\"migrations\", None)])\n        self.assertTableNotExists(\"migrations_author\")\n        self.assertTableNotExists(\"migrations_book\")\n", "    def test_custom_user_related_migration(self):\n        \"\"\"\n        Tests applying migrations where a custom user model has related models.\n        \"\"\"\n        with isolate_lru_cache(global_apps.get_swappable_settings_name):\n            executor = MigrationExecutor(connection)\n            self.assertTableNotExists('migrations_customuser')\n            self.assertTableNotExists('migrations_userprofile')\n            # Migrate forwards\n            executor.migrate([('migrations', '0001_initial')])\n            self.assertTableExists('migrations_customuser')\n            self.assertTableExists('migrations_userprofile')\n            # Ensure the related model is correctly linked to the custom user model\n            migrations_apps = executor.loader.project_state(\n                ('migrations', '0001_initial'),\n            ).apps\n            CustomUser = migrations_apps.get_model('migrations', 'CustomUser')\n            UserProfile = migrations_apps.get_model('migrations', 'UserProfile')\n            user = CustomUser.objects.create(username='testuser')\n            profile = UserProfile.objects.create(user=user)\n            self.assertEqual(profile.user, user)\n            # Migrate back to clean up the database\n            executor.loader.build_graph()\n            executor.migrate([('migrations', None)])\n            self.assertTableNotExists('migrations_customuser')\n            self.assertTableNotExists('migrations_userprofile')\n"], "sample_369": ["    def test_alter_index_together(self):\n        \"\"\"Test altering index_together constraint.\"\"\"\n        initial_state = ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n            ('author', models.CharField(max_length=200)),\n        ], {\n            'index_together': {('author', 'title')},\n        })\n        altered_state = ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n            ('author', models.CharField(max_length=200)),\n        ], {\n            'index_together': {('title', 'author')},\n        })\n        changes = self.get_changes([initial_state], [altered_state])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterIndexTogether'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='book', index_together={('title', 'author')})\n", "    def test_create_and_delete_model(self):\n        \"\"\"Test combination of creating and then deleting a model.\"\"\"\n        # Define states for the test\n        model_state_initial = ModelState(\"testapp\", \"TemporaryModel\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n\n        # Step 1: Create the model\n        changes = self.get_changes([], [model_state_initial])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"CreateModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"TemporaryModel\")\n\n        # Step 2: Delete the model\n        changes = self.get_changes([model_state_initial], [])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"DeleteModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"TemporaryModel\")\n", "    def test_create_model_with_unique_together_and_index_together(self):\n        \"\"\"Test creation of new model with unique_together and index_together already defined.\"\"\"\n        author = ModelState('otherapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ], {\n            'unique_together': {('id', 'name')},\n            'index_together': {('name', 'id')},\n        })\n        changes = self.get_changes([], [author])\n        # Right number of migrations?\n        self.assertEqual(len(changes['otherapp']), 1)\n        # Right number of actions?\n        migration = changes['otherapp'][0]\n        self.assertEqual(len(migration.operations), 2)\n        # Right actions order?\n        self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel', 'AlterUniqueTogether', 'AlterIndexTogether'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Author')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, name='author', unique_together={('id', 'name')})\n        self.assertOperationAttributes(changes, 'otherapp', 0, 2, name='author', index_together={('name', 'id')})\n", "def test_create_model_with_unique_together_and_index_together(self):\n    \"\"\"Test creation of a new model with unique_together and index_together already defined.\"\"\"\n    author = ModelState('otherapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n    ], {\n        'unique_together': {('id', 'name')},\n        'index_together': {('name',)},\n    })\n    changes = self.get_changes([], [author])\n    # Right number of migrations?\n    self.assertEqual(len(changes['otherapp']), 1)\n    # Right number of actions?\n    migration = changes['otherapp'][0]\n    self.assertEqual(len(migration.operations), 4)\n    # Right actions order?\n    self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel', 'AlterUniqueTogether', 'AlterIndexTogether'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Author')\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, name='author', unique_together={('id', 'name')})\n    self.assertOperationAttributes(changes, 'otherapp', 0, 2, name='author', index_together={('name',)})\n", "    def test_create_model_with_constraints_and_indexes(self):\n        \"\"\"Test creation of new model with constraints and indexes already defined.\"\"\"\n        author_with_constraints_indexes = ModelState('otherapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ], {\n            'constraints': [models.CheckConstraint(check=models.Q(name__contains='Bob'), name='name_contains_bob')],\n            'indexes': [models.Index(fields=['name'], name='create_model_with_indexes_idx')]\n        })\n        changes = self.get_changes([], [author_with_constraints_indexes])\n        added_constraint = models.CheckConstraint(check=models.Q(name__contains='Bob'), name='name_contains_bob')\n        added_index = models.Index(fields=['name'], name='create_model_with_indexes_idx')\n        # Right number of migrations?\n        self.assertEqual(len(changes['otherapp']), 1)\n        # Right number of actions?\n        migration = changes['otherapp'][0]\n        self.assertEqual(len(migration.operations), 3)\n        # Right actions order?\n        self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel', 'AddConstraint', 'AddIndex'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Author')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, model_name='author', constraint=added_constraint)\n        self.assertOperationAttributes(changes, 'otherapp', 0, 2, model_name='author', index=added_index)\n"], "sample_370": ["    def test_forward_one_to_one_descriptor_get_object(self):\n        author = Author.objects.create(name='Test Author', first_book=self.book1)\n        bio = Bio.objects.create(author=author)\n        descriptor = ForwardOneToOneDescriptor(Bio._meta.get_field('author'))\n        \n        # Ensure that the descriptor retrieves the correct object\n        self.assertEqual(descriptor.get_object(bio), author)\n        ", "def test_reverse_onetoone_forward_access(self):\n        \"\"\"\n        Test that a related object can be accessed from the reverse side of a\n        one-to-one relation using the forward accessor.\n        \"\"\"\n        author = AuthorWithAge.objects.create(name='Mark', age=40, first_book=self.book1)\n        with self.assertNumQueries(1):\n            fetched_author = AuthorWithAge.objects.select_related('author').get(pk=author.pk)\n        with self.assertNumQueries(0):\n            self.assertEqual(fetched_author.author.name, 'Mark')\n", "    def setUpTestData(cls):\n        cls.book = Book.objects.create(title='1984')\n        cls.book_with_year = BookWithYear.objects.create(book=cls.book, published_year=1949)\n    ", "    def test_forward_many_to_one_descriptor_get(self):\n        \"\"\"\n        Test the ForwardManyToOneDescriptor's __get__ method\n        for retrieving the related instance through the forward relation.\n        \"\"\"\n        with self.assertNumQueries(1):\n            parent = self.author1.first_book\n        self.assertEqual(parent, self.book1)\n", "def test_forward_many_to_one_descriptor(self):\n    \"\"\"\n    Test the __get__ method of ForwardManyToOneDescriptor to ensure it correctly\n    fetches and caches the related object.\n    \"\"\"\n    author = Author.objects.create(name='George', first_book=self.book1)\n    author.first_book = self.book2  # Change related book\n    descriptor = ForwardManyToOneDescriptor(author._meta.get_field('first_book'))\n\n    # Ensure that the related object is correctly fetched and cached\n    with self.assertNumQueries(1):\n        related_book = descriptor.__get__(author)\n        self.assertEqual(related_book, self.book2)\n\n    # Ensure that the related object is fetched from cache on subsequent access\n    with self.assertNumQueries(0):\n        cached_related_book = descriptor.__get__(author)\n        self.assertEqual(cached_related_book, self.book2)\n\n    # Ensure that the correct exception is raised when the related object does not exist\n    author.first_book_id = 9999  # Set a non-existent related object ID\n    with self.assertRaises(descriptor.RelatedObjectDoesNotExist):\n        descriptor.__get__(author)\n"], "sample_371": ["    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n        self.request = HttpRequest()\n        self.request.method = 'POST'\n        self.request.POST = MultiValueDict({\n            'password': ['super_secret'],\n            'username': ['john_doe'],\n            'nested': [{'api_key': '123456', 'value': 'visible_value'}],\n        })\n", "    def test_get_safe_settings(self):\n        settings_dict = {\n            'DEBUG': True,\n            'SECRET_KEY': 'super_secret_key',\n            'PASSWORD': 'super_secret_password',\n            'API_KEY': 'super_secret_api_key',\n            'NON_SENSITIVE_KEY': 'visible_value',\n        }\n        with self.settings(**settings_dict):\n            reporter_filter = SafeExceptionReporterFilter()\n            safe_settings = reporter_filter.get_safe_settings()\n            self.assertEqual(safe_settings['DEBUG'], True)\n            self.assertEqual(safe_settings['NON_SENSITIVE_KEY'], 'visible_value')\n            self.assertEqual(safe_settings['SECRET_KEY'], reporter_filter.cleansed_substitute)\n            self.assertEqual(safe_settings['PASSWORD'], reporter_filter.cleansed_substitute)\n            self.assertEqual(safe_settings['API_KEY'], reporter_filter.cleansed_substitute)\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_cleanse_special_types_multivaluedict(self):\n        filter = SafeExceptionReporterFilter()\n        request = HttpRequest()\n        request.POST = MultiValueDict({'sensitive': ['sensitive_value']})\n        cleansed_value = filter.cleanse_special_types(request, request.POST)\n        self.assertEqual(cleansed_value['sensitive'], filter.cleansed_substitute)\n", "    def test_cleanse_setting_callable(self):\n        \"\"\"\n        Callable settings should be wrapped in CallableSettingWrapper when cleansed.\n        \"\"\"\n            return \"callable_value\"\n        \n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed = reporter_filter.cleanse_setting('CALLABLE_SETTING', some_callable)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed), repr(some_callable))\n"], "sample_372": ["    def test_regex_pattern_match(self):\n        pattern = RegexPattern(r'^test/(?P<param>\\d+)/$')\n        match = pattern.match('test/123/')\n        self.assertIsNotNone(match)\n        self.assertEqual(match[0], '')\n        self.assertEqual(match[1], ())\n        self.assertEqual(match[2], {'param': '123'})\n", "    def test_compile_invalid_regex(self):\n        \"\"\"\n        Test that an improperly configured regex raises an ImproperlyConfigured exception.\n        \"\"\"\n        with self.assertRaises(ImproperlyConfigured):\n            RegexPattern('invalid[')\n", "    def test_regex_pattern_match(self):\n        pattern = RegexPattern(r'^places/([0-9]+)/$')\n        self.assertEqual(pattern.match('places/42/'), ('', ('42',), {}))\n        self.assertIsNone(pattern.match('places/abc/'))\n", "    def test_resolver_match(self):\n        \"\"\"\n        Test that URLResolver's resolve method returns the correct ResolverMatch object\n        for given URL patterns.\n        \"\"\"\n        patterns = [\n            path('test1/', views.empty_view, name='test1'),\n            path('test2/<int:param>/', views.empty_view, name='test2'),\n            path('nested/', include([\n                path('test3/', views.empty_view, name='nested_test3'),\n            ], namespace='nested'))\n        ]\n        resolver = URLResolver(RegexPattern(r'^/'), patterns)\n\n        # Test simple path resolution\n        match = resolver.resolve('/test1/')\n        self.assertEqual(match.func, views.empty_view)\n        self.assertEqual(match.url_name, 'test1')\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {})\n\n        # Test path with parameter resolution\n        match = resolver.resolve('/test2/42/')\n        self.assertEqual(match.func, views.empty_view)\n        self.assertEqual(match.url_name, 'test2')\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'param': 42})\n\n        # Test nested path resolution\n        match = resolver.resolve('/nested/test3/')\n        self.assertEqual(match.func, views.empty_view)\n        self.assertEqual(match.url_name, 'nested_test3')\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {})\n        self.assertEqual(match.namespaces, ['nested'])\n", "    def test_regex_pattern_match(self):\n        \"\"\"\n        Test the match method of RegexPattern class.\n        \"\"\"\n        pattern = RegexPattern(r'^test/(?P<id>\\d+)/(?P<slug>[\\w-]+)/$', is_endpoint=True)\n        match_result = pattern.match('test/123/some-slug/')\n        self.assertIsNotNone(match_result)\n        self.assertEqual(match_result[1], ())\n        self.assertEqual(match_result[2], {'id': '123', 'slug': 'some-slug'})\n"], "sample_373": ["    def test_template_detail_non_existent(self):\n        response = self.client.get(reverse('django-admindocs-templates', args=['non_existent_template.html']))\n        self.assertContains(response, '<h1>Template: <q>non_existent_template.html</q></h1>', html=True)\n        self.assertContains(response, '<p>Template does not exist.</p>', html=True)\n", "    def test_get_view_func_success(self):\n        view = 'django.contrib.admindocs.views.BaseAdminDocsView.dispatch'\n        view_func = views.ViewDetailView._get_view_func(view)\n        self.assertTrue(inspect.isfunction(view_func))\n        self.assertEqual(view_func.__name__, 'dispatch')\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_get_readable_field_data_type_with_custom_description(self):\n        \"\"\"\n        Ensure get_readable_field_data_type correctly processes fields with custom descriptions.\n        \"\"\"\n        class CustomFieldWithDescription(models.Field):\n            description = \"Custom field with description: %(custom)s\"\n\n        field = CustomFieldWithDescription()\n        field.custom = \"custom_value\"\n        expected_description = \"Custom field with description: custom_value\"\n        self.assertEqual(get_readable_field_data_type(field), expected_description)\n"], "sample_374": ["    def setUp(self):\n        self.book1 = Book.objects.create(title='Book1')\n        self.book2 = Book.objects.create(title='Book2')\n        self.author1 = Author.objects.create(name='Author1', first_book=self.book1)\n        self.author2 = Author.objects.create(name='Author2', first_book=self.book2)\n        self.book1.authors.add(self.author1)\n        self.book2.authors.add(self.author2)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.book2 = Book.objects.create(title='Jane Eyre')\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Emily', first_book=cls.book1)\n        cls.author3 = Author.objects.create(name='Anne', first_book=cls.book2)\n        cls.book1.authors.add(cls.author1, cls.author2)\n        cls.book2.authors.add(cls.author3)\n", "    def test_queryset_deepcopy(self):\n        qs = Book.objects.all()\n        qs_copy = copy.deepcopy(qs)\n        self.assertEqual(list(qs), list(qs_copy))\n", "    def setUp(self):\n        self.book1 = Book.objects.create(title='Book 1')\n        self.book2 = Book.objects.create(title='Book 2')\n        self.book3 = Book.objects.create(title='Book 3')\n", "    def test_prefetch_with_custom_queryset(self):\n        \"\"\"\n        Test that prefetch_related works correctly with a custom queryset that\n        includes annotations and filters.\n        \"\"\"\n        annotated_authors = Author.objects.annotate(\n            num_books=Count('books')\n        ).filter(num_books__gt=0)\n\n        with self.assertNumQueries(2):\n            books = Book.objects.prefetch_related(\n                Prefetch('authors', queryset=annotated_authors)\n            )\n            book_list = list(books)\n\n        with self.assertNumQueries(0):\n            for book in book_list:\n                for author in book.authors.all():\n                    self.assertGreater(author.num_books, 0)\n"], "sample_375": ["    def test_add_and_remove_index(self):\n        \"\"\"\n        Test adding and removing an index on a model.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label='migrations',\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=100)),\n            ],\n        ))\n        index = models.Index(fields=['name'], name='testmodel_name_idx')\n        \n        # Add index\n        project_state.add_index('migrations', 'testmodel', index)\n        test_model_state = project_state.models['migrations', 'testmodel']\n        self.assertIn(index, test_model_state.options['indexes'])\n        \n        # Remove index\n        project_state.remove_index('migrations', 'testmodel', 'testmodel_name_idx')\n        test_model_state = project_state.models['migrations', 'testmodel']\n        self.assertNotIn(index, test_model_state.options['indexes'])\n", "    def test_rename_model_field_relations(self):\n        \"\"\"\n        Test the impact of renaming a field on the relations between models.\n        \"\"\"\n        new_apps = Apps(['migrations'])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class Book(models.Model):\n            author = models.ForeignKey(Author, models.CASCADE)\n            title = models.CharField(max_length=1000)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state = ProjectState()\n        project_state.add_model(ModelState.from_model(Author))\n        project_state.add_model(ModelState.from_model(Book))\n\n        # Initial relations check\n        self.assertEqual(\n            list(project_state.relations['migrations', 'author']),\n            [('migrations', 'book')],\n        )\n        self.assertEqual(\n            list(project_state.relations['migrations', 'book']),\n            [],\n        )\n\n        # Rename the field 'author' in Book to 'writer'\n        project_state.rename_field('migrations', 'book', 'author', 'writer')\n\n        # Check relations after renaming the field\n        self.assertEqual(\n            list(project_state.relations['migrations', 'author']),\n            [('migrations', 'book')],\n        )\n        self.assertEqual(\n            project_state.relations['migrations', 'author']['migrations', 'book'],\n            {'writer': project_state.models['migrations', 'book'].fields['writer']},\n        )\n", "    def test_remove_field_with_index(self):\n        \"\"\"\n        Test removal of a field that has an index applied to it.\n        \"\"\"\n        project_state = ProjectState()\n        index_field = models.CharField(max_length=100, db_index=True)\n        project_state.add_model(ModelState(\n            app_label='migrations',\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('indexed_field', index_field),\n            ],\n        ))\n\n        self.assertIn('indexed_field', project_state.models['migrations', 'testmodel'].fields)\n        self.assertEqual(len(project_state.models['migrations', 'testmodel'].options['indexes']), 1)\n\n        # Remove the indexed field\n        project_state.remove_field('migrations', 'testmodel', 'indexed_field')\n        self.assertNotIn('indexed_field', project_state.models['migrations', 'testmodel'].fields)\n        self.assertEqual(len(project_state.models['migrations', 'testmodel'].options['indexes']), 0)\n", "    def test_update_model_field_relation(self):\n        \"\"\"\n        Tests updating model field relation in ProjectState.\n        \"\"\"\n        new_apps = Apps()\n\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        class Book(models.Model):\n            title = models.CharField(max_length=255)\n            author = models.ForeignKey(Author, models.CASCADE)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        project_state = ProjectState()\n        project_state.add_model(ModelState.from_model(Author))\n        project_state.add_model(ModelState.from_model(Book))\n\n        # Initial check\n        self.assertEqual(len(project_state.relations['migrations', 'author']), 1)\n        self.assertEqual(\n            list(project_state.relations['migrations', 'author'].keys())[0],\n            ('migrations', 'book')\n        )\n\n        # Modify the field relation\n        new_field = models.ForeignKey('migrations.Author', models.CASCADE, related_name='books')\n        project_state.update_model_field_relation(\n            'migrations.book', ('migrations', 'book'), 'author', new_field, {}\n        )\n        \n        # Check updated relation\n        self.assertEqual(len(project_state.relations['migrations', 'author']), 1)\n        self.assertEqual(\n            list(project_state.relations['migrations', 'author'].keys())[0],\n            ('migrations', 'book')\n        )\n        self.assertEqual(\n            project_state.relations['migrations', 'author'][('migrations', 'book')],\n            {'author': new_field}\n        )\n", "    def test_clone_project_state(self):\n        \"\"\"\n        Ensure that cloning a ProjectState instance results in an exact copy,\n        including models, real apps, and the delayed state.\n        \"\"\"\n        new_apps = Apps([\"migrations\"])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n        \n        class Book(models.Model):\n            title = models.CharField(max_length=1000)\n            author = models.ForeignKey(Author, models.CASCADE)\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        project_state = ProjectState.from_apps(new_apps)\n        project_state.is_delayed = True  # Set delayed state\n        cloned_state = project_state.clone()\n\n        self.assertEqual(project_state, cloned_state)\n        self.assertEqual(project_state.models, cloned_state.models)\n        self.assertEqual(project_state.real_apps, cloned_state.real_apps)\n        self.assertEqual(project_state.is_delayed, cloned_state.is_delayed)\n        self.assertIsNot(project_state.models, cloned_state.models)\n        self.assertIsNot(project_state.real_apps, cloned_state.real_apps)\n        self.assertIsNot(project_state.apps, cloned_state.apps)\n"], "sample_376": ["    def test_store_retrieve_empty_message(self):\n        \"\"\"\n        Test storing and retrieving an empty message.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        empty_message = ''\n        storage.add(constants.INFO, empty_message)\n        storage.update(response)\n        retrieved_messages = list(storage)\n        self.assertEqual(len(retrieved_messages), 1)\n        self.assertEqual(retrieved_messages[0].message, empty_message)\n", "    def test_store_empty_message(self):\n        \"\"\"\n        Storing an empty message list should result in deletion of the messages cookie.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Store an empty message list\n        storage._store([], response)\n        \n        # Check that the cookie has been deleted\n        self.assertEqual(response.cookies['messages'].value, '')\n        self.assertEqual(response.cookies['messages']['max-age'], 0)\n", "    def test_empty_message_storage(self):\n        \"\"\"\n        Ensure that storage properly handles empty messages list.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        unstored_messages = storage.update(response)\n        \n        # Ensure no messages are stored in the cookie\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n        self.assertEqual(len(unstored_messages), 0)\n        self.assertEqual(response.cookies['messages'].value, '')\n        self.assertEqual(response.cookies['messages']['max-age'], 0)\n        self.assertEqual(response.cookies['messages']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n", "    def test_signing_error(self):\n        \"\"\"\n        Test that a BadSignature or binascii.Error during cookie decoding \n        results in no messages being returned and sets the used flag to True.\n        \"\"\"\n        request = self.get_request()\n        storage = self.storage_class(request)\n        \n        # Manually create a malformed cookie value to simulate BadSignature\n        request.COOKIES[CookieStorage.cookie_name] = 'malformed_cookie_value'\n        \n        # Check that no messages are returned and used flag is set to True\n        self.assertEqual(list(storage), [])\n        self.assertTrue(storage.used)\n\n        # Manually create a base64-encoded but invalid binascii value\n        request.COOKIES[CookieStorage.cookie_name] = 'invalid_b64_value=='\n        \n        # Check again that no messages are returned and used flag is set to True\n        self.assertEqual(list(storage), [])\n        self.assertTrue(storage.used)\n", "    def test_update_cookie(self):\n        \"\"\"\n        Test that _update_cookie method correctly sets and deletes cookies\n        based on the presence of encoded_data.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        \n        # Test setting the cookie with data\n        messages = ['test', 'me']\n        encoded_data = storage._encode(messages)\n        storage._update_cookie(encoded_data, response)\n        self.assertIn(storage.cookie_name, response.cookies)\n        self.assertEqual(storage._decode(response.cookies[storage.cookie_name].value), messages)\n        \n        # Test deleting the cookie\n        storage._update_cookie(None, response)\n        self.assertIn(storage.cookie_name, response.cookies)\n        self.assertEqual(response.cookies[storage.cookie_name]['max-age'], 0)\n        self.assertEqual(response.cookies[storage.cookie_name]['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n"], "sample_377": ["    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.request = HttpRequest()\n        self.filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n        self.request = mock.MagicMock()\n        self.request.sensitive_post_parameters = []\n        self.request.POST = MultiValueDict()\n", "    def test_get_safe_settings(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        with override_settings(SECRET_KEY=\"super_secret\", DEBUG=False):\n            safe_settings = reporter_filter.get_safe_settings()\n            self.assertEqual(\n                safe_settings[\"SECRET_KEY\"], reporter_filter.cleansed_substitute\n            )\n            self.assertFalse(\"DEBUG\" in safe_settings)\n", "    def test_cleanse_special_types_multivaluedict(self):\n        \"\"\"\n        Test that cleanse_special_types works correctly for MultiValueDict instances.\n        \"\"\"\n        request = self.rf.post(\"/some_url/\", self.breakfast_data)\n        multivaluedict = MultiValueDict(request.POST)\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed_dict = reporter_filter.cleanse_special_types(request, multivaluedict)\n        for key in multivaluedict.keys():\n            self.assertEqual(\n                cleansed_dict[key],\n                reporter_filter.cleansed_substitute if key in request.sensitive_post_parameters else multivaluedict[key]\n            )\n            "], "sample_378": ["    def test_update_multiple_objects_with_different_fields(self):\n        articles = [\n            Article.objects.create(name='Article %s' % i, created=datetime.datetime.today())\n            for i in range(5)\n        ]\n        for i, article in enumerate(articles):\n            article.name = 'Updated Article %s' % i\n            article.created = datetime.datetime(1991, 10, 31, i, 0)\n        Article.objects.bulk_update(articles, ['name', 'created'])\n        self.assertCountEqual(\n            Article.objects.values_list('name', flat=True),\n            ['Updated Article %s' % i for i in range(5)]\n        )\n        self.assertCountEqual(\n            Article.objects.values_list('created', flat=True),\n            [datetime.datetime(1991, 10, 31, i, 0) for i in range(5)]\n        )\n", "    def test_bulk_create_with_empty_objects(self):\n        Note.objects.bulk_create([])\n        rows = Note.objects.all()\n        self.assertEqual(rows.count(), 0)\n", "    def test_bulk_create_with_empty_list(self):\n        with self.assertNumQueries(0):\n            created_objects = Note.objects.bulk_create([])\n        self.assertEqual(created_objects, [])\n", "    def setUpTestData(cls):\n        cls.articles = [\n            Article.objects.create(name=f'Article {i}', created=datetime.datetime.today())\n            for i in range(5)\n        ]\n", "    def test_bulk_update_with_expressions(self):\n        notes = [\n            Note.objects.create(note='initial', misc='initial')\n            for _ in range(10)\n        ]\n        for note in notes:\n            note.note = F('misc')\n            note.misc = F('note') + F('misc')\n        Note.objects.bulk_update(notes, ['note', 'misc'])\n        for note in notes:\n            note.refresh_from_db()\n            self.assertEqual(note.note, 'initial')\n            self.assertEqual(note.misc, 'initialinitial')\n"], "sample_379": ["    def test_safe_string_addition(self):\n        \"\"\"\n        Test the addition of SafeString with both SafeData and non-SafeData.\n        \"\"\"\n        safe_str1 = SafeString('Hello')\n        safe_str2 = SafeString(' World')\n        non_safe_str = '!!!'\n\n        # Adding two SafeString instances\n        result = safe_str1 + safe_str2\n        self.assertIsInstance(result, SafeString)\n        self.assertEqual(result, 'Hello World')\n\n        # Adding SafeString and a non-safe string\n        result = safe_str1 + non_safe_str\n        self.assertNotIsInstance(result, SafeString)\n        self.assertEqual(result, 'Hello!!!')\n", "def test_safe_string_concatenation(self):\n    \"\"\"\n    Ensure concatenating SafeString with regular strings and other SafeStrings\n    behaves as expected.\n    \"\"\"\n    safe_str1 = mark_safe('a&b')\n    safe_str2 = mark_safe('<c>d')\n    regular_str = 'e&f'\n\n    # Concatenate SafeString with SafeString\n    result = safe_str1 + safe_str2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, 'a&b<c>d')\n\n    # Concatenate SafeString with regular string\n    result = safe_str1 + regular_str\n    self.assertIsInstance(result, str)\n    self.assertEqual(result, 'a&be&f')\n\n    # Concatenate regular string with SafeString\n    result = regular_str + safe_str1\n    self.assertIsInstance(result, str)\n    self.assertEqual(result, 'e&fa&b')\n", "    def test_safe_string_add_non_safe_string(self):\n        \"\"\"\n        Adding a non-safe string to a SafeString instance should result in a non-safe string.\n        \"\"\"\n        safe_str = SafeString('safe')\n        result = safe_str + ' not safe'\n        self.assertNotIsInstance(result, SafeString)\n        self.assertEqual(result, 'safe not safe')\n", "    def test_safe_string_addition(self):\n        \"\"\"\n        Test that adding SafeString instances results in a SafeString.\n        \"\"\"\n        s1 = mark_safe('a&b')\n        s2 = mark_safe('<c>d')\n        result = s1 + s2\n\n        self.assertIsInstance(result, SafeString)\n        self.assertEqual(result, 'a&b<c>d')\n\n        # Adding a regular string should result in a regular string\n        s3 = 'e&f'\n        result = s1 + s3\n        self.assertNotIsInstance(result, SafeString)\n        self.assertEqual(result, 'a&be&f')\n", "def test_safe_string_concatenation_with_safe_string(self):\n        s1 = mark_safe('a&b')\n        s2 = mark_safe('<c>d')\n        result = s1 + s2\n        self.assertIsInstance(result, SafeString)\n        self.assertEqual(result, 'a&b<c>d')\n"], "sample_380": ["    def test_count_with_filter(self):\n        \"\"\"\n        Test Count with a filter to ensure that the filter is applied correctly.\n        \"\"\"\n        count = Book.objects.aggregate(filtered_count=Count('rating', filter=Q(price__gt=30)))\n        self.assertEqual(count, {\"filtered_count\": 2})\n", "    def test_count_distinct(self):\n        # Test Count with distinct=True\n        vals = Book.objects.aggregate(distinct_authors=Count('authors', distinct=True))\n        self.assertEqual(vals, {\"distinct_authors\": 9})\n\n        # Test Count with distinct=False\n        vals = Book.objects.aggregate(authors=Count('authors', distinct=False))\n        self.assertEqual(vals, {\"authors\": 11})\n", "    def test_aggregate_with_default_distinct(self):\n        \"\"\"\n        Test the use of the 'distinct' parameter in aggregate functions with defaults.\n        \"\"\"\n        books_with_distinct_ratings = Book.objects.aggregate(\n            distinct_ratings=Avg('rating', distinct=True, default=0),\n        )\n        self.assertEqual(books_with_distinct_ratings['distinct_ratings'], 4.125)\n\n        books_with_default_distinct_sum = Book.objects.filter(rating__lt=3.0).aggregate(\n            distinct_sum=Sum('price', distinct=True, default=Decimal('0.00')),\n        )\n        self.assertEqual(books_with_default_distinct_sum['distinct_sum'], Decimal('0.00'))\n\n        books_with_default_distinct_avg = Book.objects.filter(rating__lt=3.0).aggregate(\n            distinct_avg=Avg('price', distinct=True, default=Decimal('10.00')),\n        )\n        self.assertEqual(books_with_default_distinct_avg['distinct_avg'], Decimal('10.00'))\n", "    def test_aggregate_with_filter_and_default(self):\n        result = Author.objects.filter(age__gt=40).aggregate(\n            total_books=Count('book', default=Value(0), filter=Q(book__rating__gt=4.0))\n        )\n        self.assertEqual(result['total_books'], 2)\n", "    def test_stddev_variance_samples(self):\n        authors = Author.objects.aggregate(\n            stddev_sample=StdDev('age', sample=True),\n            stddev_population=StdDev('age', sample=False),\n            variance_sample=Variance('age', sample=True),\n            variance_population=Variance('age', sample=False),\n        )\n        self.assertAlmostEqual(authors['stddev_sample'], 9.456, places=3)\n        self.assertAlmostEqual(authors['stddev_population'], 8.946, places=3)\n        self.assertAlmostEqual(authors['variance_sample'], 89.456, places=3)\n        self.assertAlmostEqual(authors['variance_population'], 80.002, places=3)\n"], "sample_381": ["def test_rename_field_preserved_db_column_with_constraint(self):\n    \"\"\"\n    RenameField is used if a field is renamed and db_column equal to the\n    old field's column is added, even when constraints are involved.\n    \"\"\"\n    before = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.IntegerField()),\n        ], options={\n            'constraints': [\n                models.CheckConstraint(check=models.Q(field__gte=0), name='field_gte_0')\n            ]\n        }),\n    ]\n    after = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('renamed_field', models.IntegerField(db_column='field')),\n        ], options={\n            'constraints': [\n                models.CheckConstraint(check=models.Q(renamed_field__gte=0), name='field_gte_0')\n            ]\n        }),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterField', 'RenameField'])\n    self.assertOperationAttributes(\n        changes, 'app', 0, 0, model_name='foo', name='field',\n    )\n    self.assertEqual(changes['app'][0].operations[0].field.deconstruct(), (\n        'field', 'django.db.models.IntegerField', [], {'db_column': 'field'},\n    ))\n    self.assertOperationAttributes(\n        changes, 'app', 0, 1, model_name='foo', old_name='field',\n        new_name='renamed_field',\n    )\n", "def test_alter_field_with_regex_validator(self):\n    \"\"\"\n    Test altering a field's regex validator to ensure changes are detected\n    correctly.\n    \"\"\"\n    before = [ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, validators=[\n            RegexValidator(\n                regex=r'^[a-z]+$',\n                message='Enter a valid value.',\n                code='invalid'\n            )\n        ])),\n    ])]\n    after = [ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, validators=[\n            RegexValidator(\n                regex=r'^[a-zA-Z]+$',\n                message='Enter a valid value.',\n                code='invalid'\n            )\n        ])),\n    ])]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n    self.assertOperationFieldAttributes(\n        changes, \"testapp\", 0, 0, validators=[\n            RegexValidator(\n                regex=re.compile(r'^[a-zA-Z]+$'),\n                message='Enter a valid value.',\n                code='invalid'\n            )\n        ]\n    )\n", "def test_handle_unresolved_dependencies(self):\n    \"\"\"\n    Test handling of unresolved dependencies.\n    \"\"\"\n    class UnresolvedDependencyMigration(migrations.Migration):\n        dependencies = [('nonexistentapp', '0001_initial')]\n\n    before = self.make_project_state([])\n    after = self.make_project_state([self.author_empty])\n    autodetector = MigrationAutodetector(before, after)\n\n    with self.assertRaises(ValueError) as cm:\n        autodetector.arrange_for_graph({'testapp': [UnresolvedDependencyMigration('auto_1', 'testapp')]}, MigrationGraph())\n\n    self.assertIn(\"Cannot resolve operation dependencies: \", str(cm.exception))\n", "def test_delete_and_create_model_with_fk(self):\n    \"\"\"\n    Tests that deleting a model with a ForeignKey and creating a new model with the same name and ForeignKey\n    in a single migration works correctly.\n    \"\"\"\n    before = [\n        ModelState('app', 'OldModel', [\n            ('id', models.AutoField(primary_key=True)),\n            ('fk', models.ForeignKey('app.OtherModel', models.CASCADE)),\n        ]),\n        ModelState('app', 'OtherModel', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'NewModel', [\n            ('id', models.AutoField(primary_key=True)),\n            ('fk', models.ForeignKey('app.OtherModel', models.CASCADE)),\n        ]),\n        ModelState('app', 'OtherModel', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n    ]\n    changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='OldModel')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='NewModel')\n", "def test_deep_deconstruct_partial_function(self):\n    \"\"\"\n    Tests the deep deconstruction of functools.partial functions.\n    \"\"\"\n        return x + y\n\n    partial_func_1 = functools.partial(test_func, 1)\n    partial_func_2 = functools.partial(test_func, 1)\n    partial_func_3 = functools.partial(test_func, 2)\n\n    result_1 = MigrationAutodetector(None, None).deep_deconstruct(partial_func_1)\n    result_2 = MigrationAutodetector(None, None).deep_deconstruct(partial_func_2)\n    result_3 = MigrationAutodetector(None, None).deep_deconstruct(partial_func_3)\n\n    self.assertEqual(result_1, result_2)\n    self.assertNotEqual(result_1, result_3)\n"], "sample_382": ["    def test_get_template_directories_with_filesystem_loader(self, mock_get_dirs):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                EXTRA_TEMPLATES_DIR,\n            }\n        )\n        mock_get_dirs.assert_called_once()\n", "    def test_reset_cached_loaders(self, mock_reset):\n        autoreload.reset_loaders()\n        mock_reset.assert_called_once()\n", "    def test_reset_cached_loaders(self, mock_filesystem_reset, mock_app_directories_reset):\n        autoreload.reset_loaders()\n        mock_filesystem_reset.assert_called_once()\n        mock_app_directories_reset.assert_called_once()\n", "    def test_get_template_directories_with_no_loaders(self, mock_loaders, mock_dirs):\n        mock_dirs.return_value = [EXTRA_TEMPLATES_DIR]\n        mock_loaders.return_value = []\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates_extra',\n            }\n        )\n", "    def test_template_directories_with_no_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            set()\n        )\n"], "sample_383": ["    def setUp(self):\n        self.field = Tag._meta.get_field(\"name\")\n        self.targets = [self.field]\n        self.sources = [self.field]\n        self.alias = \"tag\"\n        self.source = MultiColSource(self.alias, self.targets, self.sources, self.field)\n", "    def test_multicolsource_init(self):\n        alias = \"test_alias\"\n        targets = [\"target1\", \"target2\"]\n        sources = [\"source1\", \"source2\"]\n        field = \"test_field\"\n        multicolsource = MultiColSource(alias, targets, sources, field)\n        self.assertEqual(multicolsource.alias, alias)\n        self.assertEqual(multicolsource.targets, targets)\n        self.assertEqual(multicolsource.sources, sources)\n        self.assertEqual(multicolsource.field, field)\n        self.assertEqual(multicolsource.output_field, field)\n", "    def test_relatedin_prep_lookup(self):\n        # Testing the RelatedIn.get_prep_lookup() method for single-column relations.\n        r = RelatedIn(lhs=self.r1._meta.get_field('creator'), rhs=[self.a1.id])\n        r.get_prep_lookup()\n        self.assertEqual(r.rhs, [self.a1.id])\n\n        # Testing for multi-column relations.\n        targets = [self.a1._meta.get_field('id'), self.a1._meta.get_field('extra')]\n        sources = [self.a1._meta.get_field('id'), self.a1._meta.get_field('num')]\n        mcs = MultiColSource('creator', targets, sources, self.a1._meta.get_field('id'))\n        r = RelatedIn(lhs=mcs, rhs=[(self.a1.id, self.a1.extra_id)])\n        r.get_prep_lookup()\n        self.assertEqual(r.rhs, [(self.a1.id, self.a1.extra_id)])\n        ", "    def test_multicolsource_repr(self):\n        field = RelatedObject._meta.get_field(\"single\")\n        alias = \"alias\"\n        targets = [\"target1\", \"target2\"]\n        sources = [\"source1\", \"source2\"]\n        mcs = MultiColSource(alias, targets, sources, field)\n        self.assertEqual(\n            repr(mcs),\n            \"MultiColSource(alias, RelatedObject.single)\"\n        )\n", "    def setUpTestData(cls):\n        cls.tag1 = Tag.objects.create(name=\"Tag1\")\n        cls.tag2 = Tag.objects.create(name=\"Tag2\")\n        cls.annotation1 = Annotation.objects.create(name=\"Annotation1\", tag=cls.tag1)\n        cls.annotation2 = Annotation.objects.create(name=\"Annotation2\", tag=cls.tag2)\n        cls.note1 = Note.objects.create(note=\"Note1\", misc=\"misc1\")\n        cls.note2 = Note.objects.create(note=\"Note2\", misc=\"misc2\")\n        cls.annotation1.notes.add(cls.note1)\n        cls.annotation2.notes.add(cls.note2)\n"], "sample_384": ["    def setUp(self):\n        self.notes = [Note.objects.create(note=str(i), misc=str(i)) for i in range(10)]\n", "    def setUpTestData(cls):\n        cls.notes = [Note.objects.create(note=str(i), misc=str(i)) for i in range(5)]\n        cls.tags = [Tag.objects.create(name=str(i)) for i in range(5)]\n", "    def setUpTestData(cls):\n        cls.notes = [Note.objects.create(note=\"note-%s\" % i, misc=\"misc-%s\" % i) for i in range(10)]\n", "    def setUp(self):\n        self.note = Note.objects.create(note=\"test_note\", misc=\"test_misc\")\n", "    def test_queryset_repr(self):\n        notes = [Note.objects.create(note=str(i), misc=str(i)) for i in range(10)]\n        qs = Note.objects.all()\n        self.assertIn(\"<QuerySet [\", repr(qs))\n        self.assertIn(\"Note\", repr(qs))\n"], "sample_385": ["def test_related_field_widget_wrapper_context(self):\n    rel = Album._meta.get_field(\"band\").remote_field\n    widget = RelatedFieldWidgetWrapper(\n        forms.TextInput(), rel, admin.site, can_add_related=True, can_change_related=True, can_delete_related=True, can_view_related=True\n    )\n    context = widget.get_context(name=\"band\", value=\"\", attrs={})\n    self.assertIn(\"add_related_url\", context)\n    self.assertIn(\"change_related_template_url\", context)\n    self.assertIn(\"delete_related_template_url\", context)\n    self.assertTrue(context[\"can_add_related\"])\n    self.assertTrue(context[\"can_change_related\"])\n    self.assertTrue(context[\"can_delete_related\"])\n    self.assertTrue(context[\"can_view_related\"])\n", "def test_url_params_from_lookup_dict(self):\n    lookups = {\n        \"is_active\": True,\n        \"group__in\": [1, 2, 3],\n        \"username\": \"test_user\",\n        \"status\": False,\n        \"get_name\": lambda: \"dynamic_name\",\n    }\n    expected_params = {\n        \"is_active\": \"1\",\n        \"group__in\": \"1,2,3\",\n        \"username\": \"test_user\",\n        \"status\": \"0\",\n        \"get_name\": \"dynamic_name\",\n    }\n    self.assertEqual(url_params_from_lookup_dict(lookups), expected_params)\n", "    def test_url_params_from_lookup_dict(self):\n        lookups = {\n            \"key1\": \"value1\",\n            \"key2\": [\"list\", \"of\", \"values\"],\n            \"key3\": True,\n            \"key4\": lambda: \"callable_value\",\n        }\n        expected = {\n            \"key1\": \"value1\",\n            \"key2\": \"list,of,values\",\n            \"key3\": \"1\",\n            \"key4\": \"callable_value\",\n        }\n        result = url_params_from_lookup_dict(lookups)\n        self.assertEqual(result, expected)\n", "    def test_autocomplete_mixin_optgroups(self):\n        \"\"\"Test that optgroups function returns the correct selected options.\"\"\"\n        beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n        who = Band.objects.create(name=\"The Who\", style=\"rock\")\n        form = AlbumForm()\n        widget = form.fields['band'].widget\n\n        # Simulate selected values\n        selected_value = [beatles.pk, who.pk]\n        optgroups = widget.optgroups(name='band', value=selected_value)\n\n        # Extract the options from the optgroups\n        options = optgroups[0][1]\n        option_values = [option['value'] for option in options]\n        option_labels = [option['label'] for option in options]\n\n        self.assertIn(str(beatles.pk), option_values)\n        self.assertIn(str(who.pk), option_values)\n        self.assertIn('The Beatles', option_labels)\n        self.assertIn('The Who', option_labels)\n", "def test_admin_date_widget_media(self):\n        widget = AdminDateWidget()\n        self.assertEqual(\n            widget.media._js,\n            [\"admin/js/calendar.js\", \"admin/js/admin/DateTimeShortcuts.js\"]\n        )\n"], "sample_386": ["    def test_safe_string_concatenation(self):\n        \"\"\"\n        SafeString concatenation with another SafeString should result in a SafeString.\n        Concatenation with a regular string should not result in a SafeString.\n        \"\"\"\n        safe_str1 = SafeString(\"hello\")\n        safe_str2 = SafeString(\"world\")\n        regular_str = \"!\"\n\n        concatenated_safe = safe_str1 + safe_str2\n        self.assertIsInstance(concatenated_safe, SafeString)\n        self.assertEqual(concatenated_safe, \"helloworld\")\n\n        concatenated_unsafe = safe_str1 + regular_str\n        self.assertNotIsInstance(concatenated_unsafe, SafeString)\n        self.assertEqual(concatenated_unsafe, \"hello!\")\n", "def test_safe_string_concat_non_safe_string(self):\n    \"\"\"\n    Concatenating a SafeString with a non-SafeString should return a normal string.\n    \"\"\"\n    safe_s = SafeString(\"safe\")\n    non_safe_s = \" not safe\"\n    result = safe_s + non_safe_s\n    self.assertIsInstance(result, str)\n    self.assertNotIsInstance(result, SafeData)\n    self.assertEqual(result, \"safe not safe\")\n", "    def test_safe_string_addition(self):\n        \"\"\"\n        Adding a SafeString instance to another SafeString or SafeData should\n        result in a SafeString. Adding a SafeString to a regular string should\n        not result in a SafeString.\n        \"\"\"\n        safe_str1 = mark_safe(\"a&b\")\n        safe_str2 = mark_safe(\"c&d\")\n        regular_str = \"e&f\"\n\n        result1 = safe_str1 + safe_str2\n        result2 = safe_str1 + regular_str\n        result3 = regular_str + safe_str1\n\n        self.assertIsInstance(result1, SafeString)\n        self.assertNotIsInstance(result2, SafeString)\n        self.assertNotIsInstance(result3, SafeString)\n        self.assertEqual(result1, \"a&bc&d\")\n        self.assertEqual(result2, \"a&be&f\")\n        self.assertEqual(result3, \"e&fa&b\")\n", "def test_safe_string_concatenation(self):\n    \"\"\"\n    Concatenating SafeString instances should result in another SafeString.\n    Concatenating a SafeString with a regular string should result in a regular string.\n    \"\"\"\n    s1 = mark_safe(\"safe\")\n    s2 = mark_safe(\"string\")\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, \"safestring\")\n\n    s3 = \"unsafe\"\n    result = s1 + s3\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, \"safeunsafe\")\n", "def test_safe_string_concat(self):\n        \"\"\"\n        Concatenating a SafeString with another SafeString or SafeData \n        should result in a SafeString. Concatenating with an unsafe string\n        should result in a regular string.\n        \"\"\"\n        safe_str1 = mark_safe(\"hello\")\n        safe_str2 = mark_safe(\" world\")\n        unsafe_str = \"!\"\n\n        # Concatenate SafeString with SafeString\n        result = safe_str1 + safe_str2\n        self.assertIsInstance(result, SafeString)\n        self.assertEqual(result, \"hello world\")\n\n        # Concatenate SafeString with unsafe string\n        result = safe_str1 + unsafe_str\n        self.assertIsInstance(result, str)\n        self.assertNotIsInstance(result, SafeString)\n        self.assertEqual(result, \"hello!\")\n"], "sample_387": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_get_empty_value_display(self):\n        class TestAdmin(admin.ModelAdmin):\n            empty_value_display = \"N/A\"\n\n        ma = TestAdmin(Event, admin.site)\n        self.assertEqual(ma.get_empty_value_display(), \"N/A\")\n", "    def test_merge_formfield_overrides(self):\n        \"\"\"\n        Test that formfield_overrides in BaseModelAdmin are correctly merged with\n        FORMFIELD_FOR_DBFIELD_DEFAULTS.\n        \"\"\"\n        class MyModelAdmin(BaseModelAdmin):\n            formfield_overrides = {\n                models.CharField: {\"widget\": forms.TextInput(attrs={\"size\": \"20\"})},\n                models.IntegerField: {\"widget\": forms.NumberInput(attrs={\"min\": \"0\"})},\n            }\n\n        admin_instance = MyModelAdmin()\n\n        self.assertIn(models.CharField, admin_instance.formfield_overrides)\n        self.assertIn(models.IntegerField, admin_instance.formfield_overrides)\n        self.assertIn(models.DateField, admin_instance.formfield_overrides)\n\n        char_field_overrides = admin_instance.formfield_overrides[models.CharField]\n        self.assertEqual(char_field_overrides[\"widget\"].attrs[\"size\"], \"20\")\n        \n        int_field_overrides = admin_instance.formfield_overrides[models.IntegerField]\n        self.assertEqual(int_field_overrides[\"widget\"].attrs[\"min\"], \"0\")\n        \n        date_field_overrides = admin_instance.formfield_overrides[models.DateField]\n        self.assertIsInstance(date_field_overrides[\"widget\"], widgets.AdminDateWidget)\n", "    def test_inline_model_admin_initialization(self):\n        \"\"\"\n        Test the initialization of InlineModelAdmin and check attributes.\n        \"\"\"\n        class InlineModel(admin.StackedInline):\n            model = Member\n\n        inline_instance = InlineModel(Member, admin.site)\n\n        self.assertEqual(inline_instance.model, Member)\n        self.assertEqual(inline_instance.verbose_name, Member._meta.verbose_name)\n        self.assertEqual(inline_instance.verbose_name_plural, Member._meta.verbose_name_plural)\n", "    def test_UUIDField(self):\n        self.assertFormfield(Event, \"identifier\", widgets.AdminUUIDInputWidget)\n"], "sample_388": ["    def setUp(self):\n        self.backend = ModelBackend()\n        self.user = User.objects.create_user(username='testuser', password='password', is_active=True)\n        self.inactive_user = User.objects.create_user(username='inactiveuser', password='password', is_active=False)\n", "    def test_inactive_user_does_not_authenticate(self):\n        \"\"\"\n        Ensure that inactive users are not authenticated even if they exist in the database.\n        \"\"\"\n        User.objects.create(username=\"inactiveuser\", is_active=False)\n        response = self.client.get(\"/remote_user/\", **{self.header: \"inactiveuser\"})\n        self.assertTrue(response.context[\"user\"].is_anonymous)\n", "    def setUp(self):\n        self.backend = ModelBackend()\n        self.user = User.objects.create_user(username=\"testuser\", password=\"password\")\n        self.user.is_active = True\n        self.user.save()\n", "    def setUp(self):\n        self.user = User.objects.create(username=\"testuser\", is_active=False)\n        self.backend_instance = AllowAllUsersModelBackend()\n", "    def clean_username(self, username):\n        \"\"\"\n        Intentionally produce an invalid username format.\n        \"\"\"\n        return \"invalid@username@format\"\n"], "sample_389": ["    def test_get_raw_host_without_http_host(self):\n        req = HttpRequest()\n        req.META = {\n            \"SERVER_NAME\": \"example.com\",\n            \"SERVER_PORT\": \"80\"\n        }\n        with override_settings(USE_X_FORWARDED_HOST=False):\n            self.assertEqual(req._get_raw_host(), \"example.com\")\n", "    def setUp(self):\n        self.request = HttpRequest()\n", "    def setUp(self):\n        self.request = HttpRequest()\n", "    def test_get_host(self):\n        req = HttpRequest()\n        req.META = {\"HTTP_HOST\": \"www.example.com\"}\n        self.assertEqual(req.get_host(), \"www.example.com\")\n", "    def test_get_host_with_valid_host(self):\n        req = HttpRequest()\n        req.META = {\n            \"HTTP_HOST\": \"example.com\",\n            \"SERVER_NAME\": \"example.com\",\n            \"SERVER_PORT\": \"80\",\n        }\n        with self.settings(ALLOWED_HOSTS=[\"example.com\"]):\n            self.assertEqual(req.get_host(), \"example.com\")\n"], "sample_390": ["    def test_directory_index_not_allowed(self):\n        \"\"\"The static view should return 404 if directory indexes are not allowed.\"\"\"\n        with self.assertRaises(Http404):\n            self.client.get(\"/%s/\" % self.prefix, show_indexes=False)\n", "    def test_directory_index_template_not_found(self):\n        \"\"\"Test that the default directory index template is used if the custom template is not found\"\"\"\n        with override_settings(TEMPLATES=[{\n            \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n            \"OPTIONS\": {\n                \"loaders\": [\n                    (\"django.template.loaders.locmem.Loader\", {}),\n                ],\n            },\n        }]):\n            response = self.client.get(\"/%s/subdir/\" % self.prefix)\n            self.assertContains(response, \"Index of subdir/\")\n            self.assertIn(\"visible\", response.context[\"file_list\"])\n", "    def test_directory_index_nonexistent_template(self):\n        \"\"\"\n        Test that the directory index view falls back to the default template\n        if the custom template doesn't exist.\n        \"\"\"\n        response = self.client.get(\"/%s/\" % self.prefix)\n        self.assertContains(response, \"Index of ./\")\n        self.assertTemplateNotUsed(response, \"static/directory_index.html\")\n        self.assertIn(\"subdir/\", response.context[\"file_list\"])\n", "    def test_directory_indexes_not_allowed(self):\n        \"\"\"Test that a 404 is raised when directory indexes are not allowed.\"\"\"\n        response = self.client.get(\"/%s/\" % self.prefix, {'show_indexes': False})\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, \"Directory indexes are not allowed here.\", status_code=404)\n", "    def test_directory_index_not_allowed(self):\n        \"\"\"Test that accessing a directory without show_indexes raises Http404\"\"\"\n        response = self.client.get(\"/%s/subdir/\" % self.prefix, {\"show_indexes\": \"False\"})\n        self.assertEqual(response.status_code, 404)\n"], "sample_391": ["    def test_add_index(self):\n        \"\"\"\n        AddIndex should optimize into CreateModel.\n        \"\"\"\n        index = models.Index(fields=[\"name\"])\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                ),\n                migrations.AddIndex(\"Foo\", index),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"indexes\": [index]},\n                ),\n            ],\n        )\n", "    def test_create_add_constraint(self):\n        \"\"\"\n        AddConstraint should optimize into CreateModel.\n        \"\"\"\n        constraint = models.UniqueConstraint(fields=[\"name\"], name=\"unique_name_constraint\")\n        managers = [(\"objects\", EmptyManager())]\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n                migrations.AddConstraint(\"Foo\", constraint),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                    constraints=[constraint],\n                ),\n            ],\n        )\n", "    def test_create_model_and_add_index(self):\n        \"\"\"\n        AddIndex should optimize into CreateModel.\n        \"\"\"\n        index = models.Index(fields=[\"name\"], name=\"name_idx\")\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                    managers=[(\"objects\", EmptyManager())],\n                ),\n                migrations.AddIndex(\"Foo\", index),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                    options={\n                        \"verbose_name\": \"Foo\",\n                        \"indexes\": [index],\n                    },\n                    bases=(UnicodeModel,),\n                    managers=[(\"objects\", EmptyManager())],\n                ),\n            ],\n        )\n", "    def test_create_model_with_duplicate_field_names(self):\n        \"\"\"\n        Ensure CreateModel raises ValueError when there are duplicate field names.\n        \"\"\"\n        with self.assertRaisesMessage(\n            ValueError, \"Found duplicate value name in CreateModel fields argument.\"\n        ):\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                    (\"name\", models.IntegerField()),\n                ],\n            )\n", "    def test_create_model_add_index(self):\n        \"\"\"\n        AddIndex should optimize into CreateModel.\n        \"\"\"\n        managers = [(\"objects\", EmptyManager())]\n        index = models.Index(fields=[\"name\"], name=\"name_idx\")\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n                migrations.AddIndex(\"Foo\", index),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                    options={'indexes': [index]},\n                ),\n            ],\n        )\n"], "sample_392": ["    def test_compile_json_path(self):\n        test_cases = [\n            ([], True, \"$\"),\n            ([], False, \"\"),\n            ([\"key1\"], True, '$.\"key1\"'),\n            ([\"key1\", \"key2\"], True, '$.\"key1\".\"key2\"'),\n            ([\"key1\", 2], True, '$.\"key1\"[2]'),\n            ([\"key1\", \"key2\", 3], True, '$.\"key1\".\"key2\"[3]'),\n        ]\n        for key_transforms, include_root, expected in test_cases:\n            with self.subTest(key_transforms=key_transforms, include_root=include_root):\n                self.assertEqual(compile_json_path(key_transforms, include_root), expected)\n", "    def test_compile_json_path_with_root(self):\n        key_transforms = [\"key1\", \"key2\", 3, \"key4\"]\n        expected_path = '$.\"key1\".\"key2\"[3].\"key4\"'\n        self.assertEqual(compile_json_path(key_transforms), expected_path)\n", "    def test_compile_json_path(self):\n        # Test with include_root=True\n        key_transforms = [\"a\", 1, \"b\"]\n        expected_path = '$.\"a\"[1].\"b\"'\n        self.assertEqual(compile_json_path(key_transforms), expected_path)\n        \n        # Test with include_root=False\n        expected_path = '.\"a\"[1].\"b\"'\n        self.assertEqual(compile_json_path(key_transforms, include_root=False), expected_path)\n", "    def test_compile_json_path(self):\n        tests = [\n            ([], \"$\"),\n            ([\"a\"], \"$.a\"),\n            ([\"a\", \"b\"], \"$.a.b\"),\n            ([\"a\", 1, \"b\"], \"$.a[1].b\"),\n            ([0], \"$[0]\"),\n            ([\"a\", \"1\"], '$.a.\"1\"'),\n        ]\n        for key_transforms, expected in tests:\n            with self.subTest(key_transforms=key_transforms):\n                self.assertEqual(compile_json_path(key_transforms), expected)\n", "    def test_compile_json_path(self):\n        test_cases = [\n            ([], \"$\"),\n            ([\"key1\"], '$.\"key1\"'),\n            ([\"key1\", \"key2\"], '$.\"key1\".\"key2\"'),\n            ([\"key1\", 2, \"key3\"], '$.\"key1\"[2].\"key3\"'),\n            ([2, \"key1\"], \"$[2].\\\"key1\\\"\"),\n        ]\n        for key_transforms, expected in test_cases:\n            with self.subTest(key_transforms=key_transforms):\n                self.assertEqual(compile_json_path(key_transforms), expected)\n"], "sample_393": ["    def setUp(self):\n        self.command = MakeMessagesCommand()\n        self.domain = \"django\"\n        self.translatable = TranslatableFile(\"dirpath\", \"testfile.html\", \"locale_dir\")\n        self.build_file = BuildFile(self.command, self.domain, self.translatable)\n    ", "    def test_handle_extensions_with_commas(self):\n        \"\"\"\n        handle_extensions should properly handle extensions separated by commas.\n        \"\"\"\n        extensions = \"html,txt,py\"\n        expected = {\".html\", \".txt\", \".py\"}\n        self.assertEqual(set(handle_extensions([extensions])), expected)\n", "    def setUp(self):\n        super().setUp()\n        self.command = MakeMessagesCommand()\n        self.translatable = self.command.translatable_file_class(\n            dirpath=self.test_dir, file_name=\"test.html\", locale_dir=self.test_dir\n        )\n        self.build_file = self.command.build_file_class(\n            command=self.command, domain=\"django\", translatable=self.translatable\n        )\n", "    def test_preprocess_for_django_domain(self):\n        test_file_content = \"{% trans 'Hello' %}\"\n        test_file_path = os.path.join(self.test_dir, \"templates\", \"test.html\")\n        os.makedirs(os.path.dirname(test_file_path), exist_ok=True)\n        with open(test_file_path, \"w\", encoding=\"utf-8\") as fp:\n            fp.write(test_file_content)\n        \n        translatable = TranslatableFile(os.path.dirname(test_file_path), \"test.html\", self.test_dir)\n        build_file = BuildFile(MakeMessagesCommand(), \"django\", translatable)\n        \n        build_file.preprocess()\n        with open(build_file.work_path, \"r\", encoding=\"utf-8\") as fp:\n            content = fp.read()\n        \n        self.assertIn(\"gettext('Hello')\", content)\n        build_file.cleanup()\n", "    def test_normalize_eols(self):\n        \"\"\"\n        Ensure that the normalize_eols function correctly normalizes\n        end-of-line characters to `\\n`.\n        \"\"\"\n        input_text = \"First line\\r\\nSecond line\\nThird line\\r\\n\"\n        expected_output = \"First line\\nSecond line\\nThird line\\n\"\n        normalized_text = normalize_eols(input_text)\n        self.assertEqual(normalized_text, expected_output)\n"], "sample_394": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.restaurant = Restaurant.objects.create(\n            city=City.objects.create(state=State.objects.create(name=\"Test State\"), name=\"Test City\"), \n            name=\"Test Restaurant\"\n        )\n", "    def test_get_object_with_invalid_id(self):\n        \"\"\"\n        The get_object method should return None if the object_id fails validation.\n        \"\"\"\n        admin = ModelAdmin(Article, site)\n        request = self.client.request().wsgi_request\n        obj = admin.get_object(request, \"invalid-id\")\n        self.assertIsNone(obj)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.p1 = Parent.objects.create(name=\"Parent\")\n        cls.c1 = Child.objects.create(parent=cls.p1, name=\"Child1\")\n        cls.c2 = Child.objects.create(parent=cls.p1, name=\"Child2\")\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def test_change_view_initial_data_preserved(self):\n        \"\"\"\n        The initial data provided in the GET parameters is preserved when the\n        form is re-rendered after a validation error.\n        \"\"\"\n        initial_data = {\n            \"title\": \"Initial Title\",\n            \"content\": \"Initial Content\",\n            \"date_0\": \"2023-01-01\",\n            \"date_1\": \"12:00:00\",\n        }\n        # GET request with initial data\n        response = self.client.get(reverse(\"admin:admin_views_article_add\"), initial_data)\n        self.assertContains(response, 'value=\"Initial Title\"', html=True)\n        self.assertContains(response, '>Initial Content</textarea>', html=True)\n        self.assertContains(response, 'value=\"2023-01-01\"', html=True)\n        self.assertContains(response, 'value=\"12:00:00\"', html=True)\n\n        # POST request with validation error (missing required field)\n        post_data = initial_data.copy()\n        post_data[\"content\"] = \"\"\n        response = self.client.post(reverse(\"admin:admin_views_article_add\"), post_data)\n        self.assertContains(response, \"This field is required.\", html=True)\n        self.assertContains(response, 'value=\"Initial Title\"', html=True)\n        self.assertContains(response, '>Initial Content</textarea>', html=True)\n        self.assertContains(response, 'value=\"2023-01-01\"', html=True)\n        self.assertContains(response, 'value=\"12:00:00\"', html=True)\n"], "sample_395": ["    def test_get_template_directories_with_cached_loader(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                EXTRA_TEMPLATES_DIR,\n            },\n        )\n", "    def test_get_template_directories_with_non_django_backend(self, mock_engines_all):\n        non_django_backend = mock.MagicMock()\n        non_django_backend.configure_mock(spec=[])\n        mock_engines_all.return_value = [non_django_backend]\n\n        self.assertEqual(autoreload.get_template_directories(), set())\n        non_django_backend.engine.template_loaders.assert_not_called()\n", "    def test_non_existing_template_directory(self):\n        self.assertSetEqual(autoreload.get_template_directories(), set())\n", "    def test_template_dirs_with_custom_loader(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                Path.cwd() / EXTRA_TEMPLATES_DIR,\n            },\n        )\n", "    def test_django_path_template_not_reset(self, mock_reset, mock_is_django_path):\n        template_path = EXTRA_TEMPLATES_DIR / \"index.html\"\n        self.assertTrue(autoreload.template_changed(None, template_path))\n        mock_reset.assert_not_called()\n"], "sample_396": ["    def test_ticket_25000(self):\n        \"\"\"\n        Ensure that the 'combine' method in Query handles complex query combination scenarios correctly.\n        \"\"\"\n        # Creating test data\n        author1 = Author.objects.create(name=\"Author1\", num=1)\n        author2 = Author.objects.create(name=\"Author2\", num=2)\n        author3 = Author.objects.create(name=\"Author3\", num=3)\n        report1 = Report.objects.create(name=\"Report1\", creator=author1)\n        report2 = Report.objects.create(name=\"Report2\", creator=author2)\n        report3 = Report.objects.create(name=\"Report3\", creator=author3)\n\n        # Creating two different querysets\n        qs1 = Report.objects.filter(creator__name__startswith=\"Author\")\n        qs2 = Report.objects.filter(name__contains=\"1\")\n\n        # Combining the querysets with an OR operation\n        combined_qs = qs1 | qs2\n\n        # The combined query should include all reports since 'OR' operation\n        self.assertCountEqual(combined_qs, [report1, report2, report3])\n\n        # Check that the SQL generated for the combined queryset is valid and does not contain unnecessary joins\n        sql_query = str(combined_qs.query)\n        self.assertIn(\"SELECT\", sql_query)\n        self.assertIn(\"FROM\", sql_query)\n        self.assertNotIn(\"UNION\", sql_query)\n        self.assertNotIn(\"LEFT JOIN\", sql_query)\n\n        # Combining the querysets with an AND operation\n        combined_qs = qs1 & qs2\n\n        # The combined query should only include reports that match both conditions\n        self.assertEqual(list(combined_qs), [report1])\n", "    def test_raw_query_clone(self):\n        sql = \"SELECT * FROM queries_note WHERE note = %s\"\n        params = [\"n1\"]\n        rq1 = RawQuery(sql, using=DEFAULT_DB_ALIAS, params=params)\n        rq2 = rq1.clone(using=DEFAULT_DB_ALIAS)\n\n        self.assertEqual(rq1.sql, rq2.sql)\n        self.assertEqual(rq1.params, rq2.params)\n        self.assertEqual(rq1.using, rq2.using)\n        self.assertIsNot(rq1, rq2)\n        self.assertIsNone(rq2.cursor)\n", "    def test_ticket_24605_with_multiple_filters(self):\n        \"\"\"\n        Ensure that combining multiple filters with Q objects works as expected.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n        \n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(alive=True) & Q(related_individual__isnull=False)\n            ),\n            [i1],\n        )\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(alive=False) | Q(related_individual__isnull=True)\n            ).order_by(\"pk\"),\n            [i2, i3, i4],\n        )\n", "    def test_raw_query_get_columns(self):\n        \"\"\"\n        Test that get_columns() returns the correct column names from a raw query.\n        \"\"\"\n        raw_query = RawQuery(\"SELECT id, name FROM queries_note\", DEFAULT_DB_ALIAS)\n        # Mock the cursor description to simulate database response\n        raw_query.cursor = type(\n            \"CursorMock\",\n            (),\n            {\"description\": [(\"id\",), (\"name\",)]},\n        )()\n        self.assertEqual(raw_query.get_columns(), [\"id\", \"name\"])\n", "    def test_raw_query_execution(self):\n        \"\"\"Test raw SQL query execution and cursor behavior.\"\"\"\n        sql = \"SELECT * FROM queries_note WHERE note = %s\"\n        params = [\"n1\"]\n        raw_query = RawQuery(sql, DEFAULT_DB_ALIAS, params)\n        \n        # Test initial cursor state\n        self.assertIsNone(raw_query.cursor)\n\n        # Execute query and test cursor state\n        results = list(raw_query)\n        self.assertIsNotNone(raw_query.cursor)\n\n        # Test results\n        self.assertEqual(len(results), 1)\n        self.assertEqual(results[0].note, \"n1\")\n"], "sample_397": ["    def test_find_template_loader_invalid_value(self):\n        \"\"\"\n        Test that ImproperlyConfigured is raised when an invalid value\n        is passed to find_template_loader.\n        \"\"\"\n        engine = Engine()\n        with self.assertRaisesMessage(\n            ImproperlyConfigured, \"Invalid value in template loaders configuration: 'invalid.loader'\"\n        ):\n            engine.find_template_loader(\"invalid.loader\")\n", "    def test_string_if_invalid(self):\n        \"\"\"\n        Test that the string_if_invalid option is respected.\n        \"\"\"\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"string_if_invalid\": \"INVALID\",\n                },\n            }\n        )\n        template = engine.from_string(\"Hello, {{ name }}!\")\n        self.assertEqual(template.render({}), \"Hello, INVALID!\")\n        self.assertEqual(template.render({\"name\": \"world\"}), \"Hello, world!\")\n", "    def test_find_template_loader_invalid_value(self):\n        \"\"\"Test find_template_loader with invalid loader value.\"\"\"\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {},\n            }\n        )\n        invalid_loader = 123  # Invalid loader value\n        msg = \"Invalid value in template loaders configuration: 123\"\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            engine.engine.find_template_loader(invalid_loader)\n", "    def test_find_template_loader_invalid_value(self):\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"loaders\": [\"invalid.loader.path\"],\n                },\n            }\n        )\n        with self.assertRaisesMessage(ImproperlyConfigured, \"Invalid value in template loaders configuration: 'invalid.loader.path'\"):\n            engine.engine.get_template_loaders(engine.engine.loaders)\n", "    def test_find_template_loader_invalid_value(self):\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {},\n            }\n        ).engine\n        invalid_loader = 123  # Invalid loader value\n        msg = f\"Invalid value in template loaders configuration: {repr(invalid_loader)}\"\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            engine.find_template_loader(invalid_loader)\n"], "sample_398": ["    def test_password_reset_done_view(self):\n        response = self.client.get(\"/password_reset/done/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/password_reset_done.html\")\n        self.assertContains(response, \"Password reset sent\")\n", "    def test_get_context_data(self):\n        class MockView(PasswordContextMixin, TemplateView):\n            template_name = \"registration/password_reset_done.html\"\n            title = \"Test Title\"\n\n        view = MockView()\n        request = HttpRequest()\n        request.session = {}\n        response = view.get(request)\n        context = view.get_context_data()\n\n        self.assertEqual(context[\"title\"], \"Test Title\")\n        self.assertEqual(context[\"subtitle\"], None)\n", "    def test_logout_then_login_with_safe_next(self):\n        self.login()\n        req = HttpRequest()\n        req.method = \"POST\"\n        csrf_token = get_token(req)\n        req.COOKIES[settings.CSRF_COOKIE_NAME] = csrf_token\n        req.POST = {\"csrfmiddlewaretoken\": csrf_token}\n        req.session = self.client.session\n        response = logout_then_login(req, login_url=\"/login/?next=/safe/\")\n        self.confirm_logged_out()\n        self.assertRedirects(response, \"/login/?next=/safe/\", fetch_redirect_response=False)\n", "    def test_password_reset_complete_view(self):\n        \"\"\"\n        Ensure that the password reset complete view renders correctly.\n        \"\"\"\n        response = self.client.get(reverse(\"password_reset_complete\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Password reset complete\")\n        self.assertContains(response, 'href=\"%s\"' % resolve_url(settings.LOGIN_URL))\n", "    def test_redirect_authenticated_user_false(self):\n        \"\"\"\n        If `redirect_authenticated_user` is False, an authenticated user should\n        stay on the login page.\n        \"\"\"\n        self.login()\n        response = self.client.get(\"/login/redirect_authenticated_user_false/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"This is the login page\")\n"], "sample_399": ["    def test_raw_query_clone(self):\n        raw_query = RawQuery(\"SELECT * FROM author\", using=\"default\")\n        cloned_query = raw_query.clone(using=\"default\")\n        \n        self.assertEqual(raw_query.sql, cloned_query.sql)\n        self.assertEqual(raw_query.params, cloned_query.params)\n        self.assertEqual(raw_query.using, cloned_query.using)\n        self.assertIsNone(raw_query.cursor)\n        self.assertIsNone(cloned_query.cursor)\n        \n        raw_query_with_params = RawQuery(\"SELECT * FROM author WHERE name = %s\", using=\"default\", params=(\"Adrian Holovaty\",))\n        cloned_query_with_params = raw_query_with_params.clone(using=\"default\")\n        \n        self.assertEqual(raw_query_with_params.sql, cloned_query_with_params.sql)\n        self.assertEqual(raw_query_with_params.params, cloned_query_with_params.params)\n        self.assertEqual(raw_query_with_params.using, cloned_query_with_params.using)\n        self.assertIsNone(raw_query_with_params.cursor)\n        self.assertIsNone(cloned_query_with_params.cursor)\n", "    def test_rawquery_get_columns(self):\n        raw_sql = \"SELECT name, age FROM aggregation_author\"\n        raw_query = RawQuery(raw_sql, using=DEFAULT_DB_ALIAS)\n        columns = raw_query.get_columns()\n        self.assertEqual(columns, [\"name\", \"age\"])\n", "    def test_rawquery_clone_method(self):\n        raw_query = RawQuery(sql=\"SELECT * FROM my_table\", using=\"default\")\n        cloned_query = raw_query.clone(using=\"other_db\")\n\n        self.assertEqual(cloned_query.sql, raw_query.sql)\n        self.assertEqual(cloned_query.params, raw_query.params)\n        self.assertEqual(cloned_query.using, \"other_db\")\n        self.assertIsNone(cloned_query.cursor)\n        self.assertEqual(cloned_query.low_mark, raw_query.low_mark)\n        self.assertEqual(cloned_query.high_mark, raw_query.high_mark)\n        self.assertEqual(cloned_query.extra_select, raw_query.extra_select)\n        self.assertEqual(cloned_query.annotation_select, raw_query.annotation_select)\n", "    def test_aggregate_with_clone(self):\n        qs = Book.objects.annotate(total_pages=Sum(\"pages\"))\n        cloned_qs = qs._clone()\n        self.assertEqual(len(cloned_qs), len(qs))\n        for book in cloned_qs:\n            original_book = qs.get(pk=book.pk)\n            self.assertEqual(book.total_pages, original_book.total_pages)\n", "    def test_raw_query_get_columns(self):\n        sql = \"SELECT id, name FROM aggregation_author\"\n        raw_query = RawQuery(sql, using=DEFAULT_DB_ALIAS)\n        columns = raw_query.get_columns()\n        self.assertEqual(columns, [\"id\", \"name\"])\n"], "sample_400": ["    def test_alter_field_with_custom_db_column(self):\n        \"\"\"\n        Tests altering a field to add a custom db_column.\n        \"\"\"\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                ],\n            )\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200, db_column=\"author_name\")),\n                ],\n            )\n        ]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n        self.assertOperationFieldAttributes(\n            changes, \"testapp\", 0, 0, db_column=\"author_name\"\n        )\n", "    def test_create_model_with_constraints(self):\n        \"\"\"Test creation of new model with multiple constraints already defined.\"\"\"\n        author = ModelState(\n            \"otherapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"constraints\": [\n                    models.CheckConstraint(\n                        check=models.Q(name__contains=\"Bob\"), name=\"name_contains_bob\"\n                    ),\n                    models.CheckConstraint(\n                        check=models.Q(age__gt=18), name=\"age_gt_18\"\n                    ),\n                ]\n            },\n        )\n        changes = self.get_changes([], [author])\n        added_constraint_1 = models.CheckConstraint(\n            check=models.Q(name__contains=\"Bob\"), name=\"name_contains_bob\"\n        )\n        added_constraint_2 = models.CheckConstraint(\n            check=models.Q(age__gt=18), name=\"age_gt_18\"\n        )\n        # Right number of migrations?\n        self.assertEqual(len(changes[\"otherapp\"]), 1)\n        # Right number of actions?\n        migration = changes[\"otherapp\"][0]\n        self.assertEqual(len(migration.operations), 3)\n        # Right actions order?\n        self.assertOperationTypes(\n            changes, \"otherapp\", 0, [\"CreateModel\", \"AddConstraint\", \"AddConstraint\"]\n        )\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"Author\")\n        self.assertOperationAttributes(\n            changes, \"otherapp\", 0, 1, model_name=\"author\", constraint=added_constraint_1\n        )\n        self.assertOperationAttributes(\n            changes, \"otherapp\", 0, 2, model_name=\"author\", constraint=added_constraint_2\n        )\n", "    def test_resolve_dependency(self):\n        \"\"\"Tests resolving dependencies, including swappable dependencies.\"\"\"\n        # Non-swappable dependency\n        dep = (\"app_label\", \"model_name\")\n        resolved_dep, is_swappable = MigrationAutodetector._resolve_dependency(dep)\n        self.assertEqual(resolved_dep, dep)\n        self.assertFalse(is_swappable)\n        \n        # Swappable dependency\n        with self.settings(AUTH_USER_MODEL=\"auth.User\"):\n            dep = (\"__setting__\", \"AUTH_USER_MODEL\")\n            resolved_dep, is_swappable = MigrationAutodetector._resolve_dependency(dep)\n            self.assertEqual(resolved_dep, (\"auth\", \"user\"))\n            self.assertTrue(is_swappable)\n", "    def test_alter_field_preserve_default_false(self):\n        \"\"\"\n        Test autodetection of altering a field with preserve_default set to False.\n        \"\"\"\n        author_name_default_preserve_false = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default=\"Ada Lovelace\")),\n            ],\n            options={\"preserve_default\": False},\n        )\n        changes = self.get_changes(\n            [self.author_name_default], [author_name_default_preserve_false]\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"name\", preserve_default=False\n        )\n", "    def test_generate_deleted_models(self):\n        \"\"\"\n        Tests autodetection of deleted models and ensures the correct\n        operations are generated in the right order.\n        \"\"\"\n        before = self.make_project_state([self.author_empty, self.publisher])\n        after = self.make_project_state([])\n        changes = self.get_changes(before, after)\n\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"DeleteModel\"])\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"DeleteModel\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Author\")\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"Publisher\")\n"], "sample_401": ["def test_invalid_initial_forms_count(self):\n    \"\"\"\n    When the initial forms count is higher than the total forms count, \n    formset should be invalid.\n    \"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"3\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-MAX_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Zero\",\n        \"choices-0-votes\": \"0\",\n        \"choices-1-choice\": \"One\",\n        \"choices-1-votes\": \"1\",\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\"ManagementForm data is missing or has been tampered with. Missing fields: choices-INITIAL_FORMS. You may need to file a bug report if the issue persists.\"],\n    )\n", "    def test_management_form_clean_defaults(self):\n        \"\"\"\n        ManagementForm should default TOTAL_FORM_COUNT and INITIAL_FORM_COUNT to 0 if\n        not provided in cleaned_data.\n        \"\"\"\n        form = ManagementForm(data={})\n        self.assertFalse(form.is_valid())\n        form.cleaned_data = {}\n        cleaned_data = form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n", "    def test_formset_with_custom_error_messages(self):\n        \"\"\"\n        Test that formset uses custom error messages for management form errors.\n        \"\"\"\n        CustomErrorMessageFormSet = formset_factory(\n            FavoriteDrinkForm,\n            error_messages={\n                \"missing_management_form\": \"Custom management form error\",\n                \"too_many_forms\": \"Custom too many forms error\",\n                \"too_few_forms\": \"Custom too few forms error\",\n            },\n        )\n        \n        # Test missing management form error\n        formset = CustomErrorMessageFormSet({})\n        self.assertIs(formset.is_valid(), False)\n        self.assertEqual(formset.non_form_errors(), [\"Custom management form error\"])\n        \n        # Test too many forms error\n        data_too_many = {\n            \"form-TOTAL_FORMS\": \"5\",\n            \"form-INITIAL_FORMS\": \"0\",\n            \"form-0-name\": \"Drink 1\",\n            \"form-1-name\": \"Drink 2\",\n            \"form-2-name\": \"Drink 3\",\n            \"form-3-name\": \"Drink 4\",\n            \"form-4-name\": \"Drink 5\",\n        }\n        formset = CustomErrorMessageFormSet(data_too_many, max_num=3, validate_max=True)\n        self.assertIs(formset.is_valid(), False)\n        self.assertEqual(formset.non_form_errors(), [\"Custom too many forms error\"])\n\n        # Test too few forms error\n        data_too_few = {\n            \"form-TOTAL_FORMS\": \"2\",\n            \"form-INITIAL_FORMS\": \"0\",\n            \"form-0-name\": \"Drink 1\",\n            \"form-1-name\": \"\",\n        }\n        formset = CustomErrorMessageFormSet(data_too_few, min_num=3, validate_min=True)\n        self.assertIs(formset.is_valid(), False)\n        self.assertEqual(formset.non_form_errors(), [\"Custom too few forms error\"])\n", "    def test_custom_error_messages(self):\n        \"\"\"\n        Ensure that custom error messages passed to the formset are used.\n        \"\"\"\n        data = {\n            \"form-TOTAL_FORMS\": \"2\",\n            \"form-INITIAL_FORMS\": \"1\",\n            \"form-0-title\": \"Test\",\n            \"form-0-pub_date\": \"1904-06-16\",\n            \"form-1-title\": \"\",\n            \"form-1-pub_date\": \"\",  # Missing title and pub_date.\n        }\n        CustomErrorFormSet = formset_factory(\n            ArticleForm,\n            error_messages={\n                \"missing_management_form\": \"custom management form error\",\n                \"too_many_forms\": \"custom too many forms error\",\n                \"too_few_forms\": \"custom too few forms error\"\n            },\n            validate_min=True,\n            min_num=2,\n            validate_max=True,\n            max_num=1\n        )\n        formset = CustomErrorFormSet(data)\n        self.assertFalse(formset.is_valid())\n        self.assertIn(\"custom too many forms error\", formset.non_form_errors())\n        self.assertEqual(\n            formset.errors,\n            [\n                {},\n                {\n                    \"title\": [\"This field is required.\"],\n                    \"pub_date\": [\"This field is required.\"],\n                },\n            ],\n        )\n\n        # Test too few forms error\n        data[\"form-TOTAL_FORMS\"] = \"1\"\n        formset = CustomErrorFormSet(data)\n        self.assertFalse(formset.is_valid())\n        self.assertIn(\"custom too few forms error\", formset.non_form_errors())\n", "    def test_formset_with_minimum_forms(self):\n        \"\"\"\n        Test that the formset respects the minimum number of forms (min_num)\n        and raises validation errors if not enough forms are submitted.\n        \"\"\"\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"2\",\n            \"choices-MAX_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"\",\n            \"choices-0-votes\": \"\",\n        }\n        ChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), [\"Please submit at least 2 forms.\"])\n        self.assertEqual(\n            str(formset.non_form_errors()),\n            '<ul class=\"errorlist nonform\"><li>Please submit at least 2 forms.</li></ul>',\n        )\n"], "sample_402": ["    def test_no_prepend_www(self):\n        \"\"\"\n        PREPEND_WWW set to False should not prepend 'www.' to URLs.\n        \"\"\"\n        request = self.rf.get(\"/no-prepend\")\n        r = CommonMiddleware(get_response_empty).process_request(request)\n        self.assertIsNone(r)\n", "def test_prepend_www_no_host(self):\n    \"\"\"\n    PREPEND_WWW should not redirect if the host is not present.\n    \"\"\"\n    request = self.rf.get(\"/path/\")\n    request.META[\"HTTP_HOST\"] = \"\"\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertIsNone(r)\n", "    def test_prepend_www_with_subdomain(self):\n        \"\"\"\n        PREPEND_WWW should not add 'www.' to URLs that already have a subdomain.\n        \"\"\"\n        request = self.rf.get(\"/path/\")\n        request.META[\"HTTP_HOST\"] = \"sub.testserver\"\n        r = CommonMiddleware(get_response_empty).process_request(request)\n        self.assertIsNone(r)\n", "    def test_append_slash_no_redirect_on_GET_with_trailing_slash(self):\n        \"\"\"\n        APPEND_SLASH should not redirect if the URL already ends with a slash.\n        \"\"\"\n        request = self.rf.get(\"/trailing-slash/\")\n        response = CommonMiddleware(get_response_empty)(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertIsNone(response.get(\"Location\"))\n", "    def test_prepend_www_no_host(self):\n        \"\"\"\n        PREPEND_WWW should not prepend 'www.' if the request has no host.\n        \"\"\"\n        request = self.rf.get(\"/path/\")\n        request.META[\"HTTP_HOST\"] = \"\"\n        r = CommonMiddleware(get_response_empty).process_request(request)\n        self.assertIsNone(r)\n"], "sample_403": ["    def test_create_model_with_duplicate_field_options(self):\n        \"\"\"\n        Tests the CreateModel operation when options have duplicate keys.\n        \"\"\"\n        duplicate_option_key = \"indexes\"\n        options = {\n            \"indexes\": [models.Index(fields=[\"pink\"], name=\"pony_pink_idx\")],\n            duplicate_option_key: [models.Index(fields=[\"weight\"], name=\"pony_weight_idx\")]\n        }\n        with self.assertRaisesMessage(\n            ValueError, f\"Found duplicate value {duplicate_option_key} in CreateModel options argument.\"\n        ):\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(default=1)),\n                    (\"weight\", models.IntegerField(default=2)),\n                ],\n                options=options\n            )\n", "    def test_create_model_with_options(self):\n        \"\"\"\n        Tests the CreateModel operation with additional options.\n        \"\"\"\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n            ],\n            options={\n                \"ordering\": [\"pink\"],\n                \"verbose_name\": \"Magic Pony\",\n                \"verbose_name_plural\": \"Magic Ponies\"\n            }\n        )\n        self.assertEqual(operation.describe(), \"Create model Pony\")\n        self.assertEqual(operation.migration_name_fragment, \"pony\")\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmo_options\", new_state)\n        self.assertEqual(new_state.models[\"test_crmo_options\", \"pony\"].name, \"Pony\")\n        self.assertEqual(len(new_state.models[\"test_crmo_options\", \"pony\"].fields), 2)\n        self.assertEqual(new_state.models[\"test_crmo_options\", \"pony\"].options[\"ordering\"], [\"pink\"])\n        self.assertEqual(new_state.models[\"test_crmo_options\", \"pony\"].options[\"verbose_name\"], \"Magic Pony\")\n        self.assertEqual(new_state.models[\"test_crmo_options\", \"pony\"].options[\"verbose_name_plural\"], \"Magic Ponies\")\n        # Test the database alteration\n        self.assertTableNotExists(\"test_crmo_options_pony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_crmo_options\", editor, project_state, new_state)\n        self.assertTableExists(\"test_crmo_options_pony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crmo_options\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_crmo_options_pony\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"CreateModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"fields\", \"name\", \"options\"])\n", "    def test_create_model_with_references_model(self):\n        \"\"\"\n        Tests the CreateModel operation with references to another model.\n        \"\"\"\n        operation1 = migrations.CreateModel(\n            \"Parent\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n            ],\n        )\n        operation2 = migrations.CreateModel(\n            \"Child\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"parent\", models.ForeignKey(\"test_crmr.Parent\", models.CASCADE)),\n            ],\n        )\n        self.assertEqual(operation1.describe(), \"Create model Parent\")\n        self.assertEqual(operation2.describe(), \"Create model Child\")\n        self.assertEqual(operation1.migration_name_fragment, \"parent\")\n        self.assertEqual(operation2.migration_name_fragment, \"child\")\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation1.state_forwards(\"test_crmr\", new_state)\n        self.assertEqual(new_state.models[\"test_crmr\", \"parent\"].name, \"Parent\")\n        self.assertEqual(len(new_state.models[\"test_crmr\", \"parent\"].fields), 2)\n        operation2.state_forwards(\"test_crmr\", new_state)\n        self.assertEqual(new_state.models[\"test_crmr\", \"child\"].name, \"Child\")\n        self.assertEqual(len(new_state.models[\"test_crmr\", \"child\"].fields), 2)\n        # Test the database alteration\n        self.assertTableNotExists(\"test_crmr_parent\")\n        self.assertTableNotExists(\"test_crmr_child\")\n        with connection.schema_editor() as editor:\n            operation1.database_forwards(\"test_crmr\", editor, project_state, new_state)\n        self.assertTableExists(\"test_crmr_parent\")\n        with connection.schema_editor() as editor:\n            operation2.database_forwards(\"test_crmr\", editor, project_state, new_state)\n        self.assertTableExists(\"test_crmr_child\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation2.database_backwards(\"test_crmr\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_crmr_child\")\n        with connection.schema_editor() as editor:\n            operation1.database_backwards(\"test_crmr\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_crmr_parent\")\n", "    def test_rename_model_references(self):\n        \"\"\"\n        Test the references_model method in the RenameModel operation.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_rename_model_references\")\n        # Create a RenameModel operation\n        operation = migrations.RenameModel(\"Pony\", \"Horse\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_rename_model_references\", new_state)\n        # Test references_model with old name\n        self.assertTrue(operation.references_model(\"Pony\", \"test_rename_model_references\"))\n        # Test references_model with new name\n        self.assertTrue(operation.references_model(\"Horse\", \"test_rename_model_references\"))\n        # Test references_model with a different model name\n        self.assertFalse(operation.references_model(\"Rider\", \"test_rename_model_references\"))\n        # Test the database alteration and reversal\n        self.assertTableExists(\"test_rename_model_references_pony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_rename_model_references\", editor, project_state, new_state)\n        self.assertTableExists(\"test_rename_model_references_horse\")\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_rename_model_references\", editor, new_state, project_state)\n        self.assertTableExists(\"test_rename_model_references_pony\")\n", "    def test_add_constraint(self):\n        \"\"\"\n        Test AddConstraint operation to ensure it correctly alters the database\n        and state for various constraint types.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_add_constraint\")\n        check_constraint = models.CheckConstraint(\n            check=models.Q(weight__gt=50), name=\"weight_gt_50\"\n        )\n        unique_constraint = models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n        constraint_operation = migrations.AddConstraint(\n            \"Pony\", check_constraint\n        )\n        unique_constraint_operation = migrations.AddConstraint(\n            \"Pony\", unique_constraint\n        )\n\n        new_state = project_state.clone()\n        constraint_operation.state_forwards(\"test_add_constraint\", new_state)\n        self.assertIn(\n            check_constraint, new_state.models[\"test_add_constraint\", \"pony\"].options[\"constraints\"]\n        )\n\n        with connection.schema_editor() as editor:\n            constraint_operation.database_forwards(\n                \"test_add_constraint\", editor, project_state, new_state\n            )\n\n        self.assertTableExists(\"test_add_constraint_pony\")\n        Pony = new_state.apps.get_model(\"test_add_constraint\", \"Pony\")\n        with self.assertRaises(IntegrityError):\n            Pony.objects.create(weight=30)\n\n        new_state = new_state.clone()\n        unique_constraint_operation.state_forwards(\"test_add_constraint\", new_state)\n        self.assertIn(\n            unique_constraint, new_state.models[\"test_add_constraint\", \"pony\"].options[\"constraints\"]\n        )\n\n        with connection.schema_editor() as editor:\n            unique_constraint_operation.database_forwards(\n                \"test_add_constraint\", editor, project_state, new_state\n            )\n\n        with self.assertRaises(IntegrityError):\n            Pony.objects.create(name=\"unique\", weight=60)\n            Pony.objects.create(name=\"unique\", weight=70)\n\n        with connection.schema_editor() as editor:\n            unique_constraint_operation.database_backwards(\n                \"test_add_constraint\", editor, new_state, project_state\n            )\n        self.assertNotIn(\n            unique_constraint, project_state.models[\"test_add_constraint\", \"pony\"].options[\"constraints\"]\n        )\n\n        with connection.schema_editor() as editor:\n            constraint_operation.database_backwards(\n                \"test_add_constraint\", editor, new_state, project_state\n            )\n        self.assertNotIn(\n            check_constraint, project_state.models[\"test_add_constraint\", \"pony\"].options[\"constraints\"]\n        )\n"], "sample_404": ["def test_text_node_render(self):\n    \"\"\"\n    Test that TextNode renders the text content correctly.\n    \"\"\"\n    text_content = \"This is a test string.\"\n    text_node = TextNode(text_content)\n    self.assertEqual(text_node.render(Context()), text_content)\n", "def test_text_node_render(self):\n    \"\"\"\n    Ensure TextNode renders correctly.\n    \"\"\"\n    engine = self._engine()\n    t = engine.from_string(\"This is a test.\")\n    c = Context()\n    self.assertEqual(t.render(c), \"This is a test.\")\n", "    def test_template_repr(self):\n        \"\"\"\n        Test the __repr__ method of the Template class.\n        \"\"\"\n        template_string = \"This is a test template\"\n        template = self._engine().from_string(template_string)\n        expected_repr = '<Template template_string=\"This is a test tem...\">'\n        self.assertEqual(repr(template), expected_repr)\n", "    def test_template_repr(self):\n        \"\"\"\n        Ensure the __repr__ method of the Template class works correctly.\n        \"\"\"\n        engine = self._engine()\n        template_string = \"short template\"\n        template = engine.from_string(template_string)\n        self.assertEqual(repr(template), '<Template template_string=\"short template...\">')\n", "    def test_template_repr(self):\n        \"\"\"\n        Ensure the Template class __repr__ method formats the template string correctly.\n        \"\"\"\n        template_string = \"Test template string\"\n        template = self._engine().from_string(template_string)\n        self.assertEqual(repr(template), '<Template template_string=\"Test template str...\">')\n"], "sample_405": ["    def test_create_model_with_options(self):\n        \"\"\"\n        Tests the CreateModel operation with additional options.\n        \"\"\"\n        options = {\n            \"verbose_name\": \"Pony Model\",\n            \"verbose_name_plural\": \"Ponies\",\n            \"ordering\": [\"pink\"],\n            \"unique_together\": {(\"pink\", \"weight\")},\n            \"index_together\": {(\"pink\", \"weight\")},\n            \"db_table\": \"pony_table\",\n        }\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n                (\"weight\", models.FloatField(default=1.0)),\n            ],\n            options=options,\n        )\n        self.assertEqual(operation.describe(), \"Create model Pony\")\n        self.assertEqual(operation.migration_name_fragment, \"pony\")\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmo\", new_state)\n        self.assertEqual(new_state.models[\"test_crmo\", \"pony\"].name, \"Pony\")\n        self.assertEqual(len(new_state.models[\"test_crmo\", \"pony\"].fields), 3)\n        self.assertEqual(new_state.models[\"test_crmo\", \"pony\"].options, options)\n        # Test the database alteration\n        self.assertTableNotExists(\"pony_table\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_crmo\", editor, project_state, new_state)\n        self.assertTableExists(\"pony_table\")\n        # Check indexes and unique constraints\n        self.assertIndexExists(\"pony_table\", [\"pink\", \"weight\"])\n        self.assertUniqueConstraintExists(\"pony_table\", [\"pink\", \"weight\"])\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crmo\", editor, new_state, project_state)\n        self.assertTableNotExists(\"pony_table\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"CreateModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2].keys()), [\"fields\", \"name\", \"options\"])\n        self.assertEqual(definition[2][\"options\"], options)\n", "    def test_create_model_with_duplicate_field_bases_and_managers(self):\n        \"\"\"\n        Tests the CreateModel operation raises ValueError for duplicate field names, \n        base classes, and manager names.\n        \"\"\"\n        with self.assertRaisesMessage(\n            ValueError, \"Found duplicate value id in CreateModel fields argument.\"\n        ):\n            migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"id\", models.IntegerField(default=1)),\n                ],\n            )\n\n        with self.assertRaisesMessage(\n            ValueError, \"Found duplicate value test_app.pony in CreateModel bases argument.\"\n        ):\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[],\n                bases=(\n                    \"test_app.Pony\",\n                    \"test_app.Pony\",\n                ),\n            )\n\n        with self.assertRaisesMessage(\n            ValueError, \"Found duplicate value objects in CreateModel managers argument.\"\n        ):\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[],\n                managers=[\n                    (\"objects\", models.Manager()),\n                    (\"objects\", models.Manager()),\n                ],\n            )\n", "    def test_delete_model_with_unique_and_index_together(self):\n        \"\"\"\n        Test DeleteModel operation with models having unique_together and index_together options.\n        \"\"\"\n        app_label = \"test_delmoduit\"\n        project_state = self.apply_operations(\n            app_label,\n            ProjectState(),\n            operations=[\n                migrations.CreateModel(\n                    \"Pony\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=30)),\n                        (\"weight\", models.FloatField()),\n                    ],\n                    options={\n                        \"unique_together\": {(\"name\", \"weight\")},\n                        \"index_together\": {(\"name\", \"weight\")},\n                    },\n                ),\n            ],\n        )\n        table_name = f\"{app_label}_pony\"\n        self.assertTableExists(table_name)\n        self.assertUniqueConstraintExists(table_name, [\"name\", \"weight\"])\n        self.assertIndexExists(table_name, [\"name\", \"weight\"])\n\n        operation = migrations.DeleteModel(\"Pony\")\n        self.assertEqual(operation.describe(), \"Delete model Pony\")\n        new_state = project_state.clone()\n        operation.state_forwards(app_label, new_state)\n        self.assertNotIn((app_label, \"pony\"), new_state.models)\n\n        with connection.schema_editor() as editor:\n            operation.database_forwards(app_label, editor, project_state, new_state)\n        self.assertTableNotExists(table_name)\n\n        with connection.schema_editor() as editor:\n            operation.database_backwards(app_label, editor, new_state, project_state)\n        self.assertTableExists(table_name)\n        self.assertUniqueConstraintExists(table_name, [\"name\", \"weight\"])\n        self.assertIndexExists(table_name, [\"name\", \"weight\"])\n", "def test_alter_field_with_constraint(self):\n    \"\"\"\n    Tests the AlterField operation on a field that has constraints.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_alflwc\", constraints=[\n        models.CheckConstraint(check=models.Q(pink__gte=0), name=\"check_pink_gte_0\"),\n    ])\n    # Test the state alteration\n    operation = migrations.AlterField(\n        \"Pony\", \"pink\", models.PositiveIntegerField()\n    )\n    self.assertEqual(operation.describe(), \"Alter field pink on Pony\")\n    self.assertEqual(operation.migration_name_fragment, \"alter_pony_pink\")\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_alflwc\", new_state)\n    self.assertIsInstance(\n        new_state.models[\"test_alflwc\", \"pony\"].fields[\"pink\"], models.PositiveIntegerField\n    )\n    # Test the database alteration\n    self.assertColumnExists(\"test_alflwc_pony\", \"pink\")\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_alflwc\", editor, project_state, new_state)\n    self.assertColumnExists(\"test_alflwc_pony\", \"pink\")\n    # Check that the constraint still works\n    Pony = new_state.apps.get_model(\"test_alflwc\", \"Pony\")\n    with self.assertRaises(IntegrityError):\n        with transaction.atomic():\n            Pony.objects.create(pink=-1, weight=1.0)\n    Pony.objects.create(pink=1, weight=1.0)\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_alflwc\", editor, new_state, project_state)\n    self.assertColumnExists(\"test_alflwc_pony\", \"pink\")\n    # And deconstruction\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"AlterField\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(sorted(definition[2]), [\"field\", \"model_name\", \"name\"])\n", "    def test_create_model_with_custom_options(self):\n        \"\"\"\n        Test the CreateModel operation with custom model options.\n        \"\"\"\n        custom_options = {\n            \"verbose_name\": \"Custom Pony\",\n            \"verbose_name_plural\": \"Custom Ponies\",\n            \"ordering\": [\"-id\"],\n        }\n        operation = migrations.CreateModel(\n            \"CustomPony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n            ],\n            options=custom_options,\n        )\n        self.assertEqual(operation.describe(), \"Create model CustomPony\")\n        self.assertEqual(operation.migration_name_fragment, \"custompony\")\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmco\", new_state)\n        self.assertEqual(new_state.models[\"test_crmco\", \"custompony\"].name, \"CustomPony\")\n        self.assertEqual(len(new_state.models[\"test_crmco\", \"custompony\"].fields), 2)\n        self.assertEqual(new_state.models[\"test_crmco\", \"custompony\"].options, custom_options)\n        # Test the database alteration\n        self.assertTableNotExists(\"test_crmco_custompony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_crmco\", editor, project_state, new_state)\n        self.assertTableExists(\"test_crmco_custompony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crmco\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_crmco_custompony\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"CreateModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"fields\", \"name\", \"options\"])\n        self.assertEqual(definition[2][\"options\"], custom_options)\n"], "sample_406": ["    def test_auto_created_manager(self):\n        class AutoCreatedManager(models.Manager):\n            auto_created = True\n\n        class MyModel(models.Model):\n            objects = AutoCreatedManager()\n\n        manager = MyModel._meta.managers_map['objects']\n        self.assertTrue(manager.auto_created)\n", "    def test_base_manager_deconstruct(self):\n        class CustomQuerySet(QuerySet):\n            pass\n\n        class CustomManager(BaseManager.from_queryset(CustomQuerySet)):\n            pass\n\n        manager = CustomManager()\n        self.assertEqual(manager.deconstruct(), (False, \"app.CustomManager\", None, manager._constructor_args[0], manager._constructor_args[1]))\n", "    def test_creation_counter_increment(self):\n        initial_counter = BaseManager.creation_counter\n        BaseManager()\n        self.assertEqual(BaseManager.creation_counter, initial_counter + 1)\n", "    def test_base_manager_deconstruct(self):\n        class CustomQuerySet(QuerySet):\n            pass\n\n        class CustomManager(BaseManager.from_queryset(CustomQuerySet)):\n            pass\n\n        manager = CustomManager()\n        self.assertEqual(\n            manager.deconstruct(),\n            (\n                False,  # as_manager\n                'path.to.module.CustomManager',  # Replace 'path.to.module' with the actual module path\n                None,  # qs_class\n                (),  # args\n                {},  # kwargs\n            )\n        )\n", "    def test_auto_created_default_value(self):\n        \"\"\"\n        Test that the default value of auto_created is False.\n        \"\"\"\n        manager = BaseManager()\n        self.assertFalse(manager.auto_created)\n"], "sample_407": ["    def test_related_model_creation(self):\n        # Test creating a related model instance using the default manager\n        reporter = Reporter.objects.create(first_name=\"Alice\", last_name=\"Johnson\", email=\"alice@example.com\")\n        article = reporter.article_set.create(headline=\"Alice's Story\", pub_date=datetime.date(2022, 1, 1))\n        self.assertEqual(article.reporter, reporter)\n        self.assertEqual(article.headline, \"Alice's Story\")\n        \n        # Test creating a related model instance using explicit foreign key assignment\n        new_article = Article.objects.create(headline=\"New Article\", pub_date=datetime.date(2022, 1, 2), reporter=reporter)\n        self.assertEqual(new_article.reporter, reporter)\n        self.assertEqual(new_article.headline, \"New Article\")\n        \n        # Test creating a related model instance using reporter's id\n        another_article = Article.objects.create(headline=\"Another Article\", pub_date=datetime.date(2022, 1, 3), reporter_id=reporter.id)\n        self.assertEqual(another_article.reporter, reporter)\n        self.assertEqual(another_article.headline, \"Another Article\")\n", "    def test_save_and_refresh_deferred_fields(self):\n        \"\"\"\n        Test saving and refreshing models with deferred fields.\n        \"\"\"\n        # Create a new Article with deferred fields\n        a2 = Article.objects.only(\"headline\").get(pk=self.a.pk)\n        self.assertEqual(a2.headline, \"This is a test\")\n\n        # Modify a deferred field and save\n        a2.headline = \"Updated headline\"\n        a2.save()\n\n        # Refresh from DB and ensure deferred fields are updated\n        a2.refresh_from_db()\n        self.assertEqual(a2.headline, \"Updated headline\")\n        self.assertEqual(a2.pub_date, datetime.date(2005, 7, 27))\n\n        # Use defer to load only some fields\n        a3 = Article.objects.defer(\"pub_date\").get(pk=self.a.pk)\n        self.assertEqual(a3.headline, \"Updated headline\")\n        with self.assertNumQueries(1):\n            self.assertEqual(a3.pub_date, datetime.date(2005, 7, 27))\n", "    def test_unique_constraint_violation(self):\n        \"\"\"\n        Test the unique constraint violation for a model with a unique field.\n        \"\"\"\n        reporter = Reporter.objects.create(\n            first_name=\"Unique\", last_name=\"Reporter\", email=\"unique@example.com\"\n        )\n        duplicate_reporter = Reporter(\n            first_name=\"Unique\", last_name=\"Reporter\", email=\"unique@example.com\"\n        )\n        with self.assertRaises(ValidationError):\n            duplicate_reporter.full_clean()  # This should raise a ValidationError due to unique constraint\n", "    def test_proxy_model_inheritance(self):\n        class ProxyReporter(Reporter):\n            class Meta:\n                proxy = True\n\n        proxy_reporter = ProxyReporter.objects.create(\n            first_name=\"Proxy\", last_name=\"Reporter\", email=\"proxy@example.com\"\n        )\n        self.assertTrue(Reporter.objects.filter(email=\"proxy@example.com\").exists())\n        self.assertTrue(ProxyReporter.objects.filter(email=\"proxy@example.com\").exists())\n\n        # Ensure that the proxy model cannot have fields defined.\n        with self.assertRaisesMessage(\n            TypeError, \"Proxy model 'ProxyReporter' contains model fields.\"\n        ):\n            class InvalidProxyReporter(Reporter):\n                extra_field = models.CharField(max_length=50)\n                class Meta:\n                    proxy = True\n", "    def test_modelbase_new(self):\n        class AbstractParent(models.Model):\n            class Meta:\n                abstract = True\n\n        class ConcreteChild(AbstractParent):\n            pass\n\n        self.assertTrue(issubclass(ConcreteChild, AbstractParent))\n"], "sample_408": ["def test_add_operation_with_dependencies(self):\n        \"\"\"Tests add_operation method with dependencies.\"\"\"\n        autodetector = MigrationAutodetector(ProjectState(), ProjectState())\n\n        operation = operations.AddField(\n            model_name=\"Author\",\n            name=\"new_field\",\n            field=models.CharField(max_length=200)\n        )\n        dependencies = [(\"testapp\", \"Author\", \"existing_field\", True)]\n\n        autodetector.add_operation(\"testapp\", operation, dependencies=dependencies)\n        \n        # Check that the operation is added correctly.\n        self.assertIn(\"testapp\", autodetector.generated_operations)\n        self.assertEqual(len(autodetector.generated_operations[\"testapp\"]), 1)\n        self.assertIsInstance(autodetector.generated_operations[\"testapp\"][0], operations.AddField)\n        self.assertEqual(autodetector.generated_operations[\"testapp\"][0]._auto_deps, dependencies)\n\n        \"\"\"Tests add_operation method without dependencies.\"\"\"\n        autodetector = MigrationAutodetector(ProjectState(), ProjectState())\n\n        operation = operations.AddField(\n            model_name=\"Author\",\n            name=\"new_field\",\n            field=models.CharField(max_length=200)\n        )\n\n        autodetector.add_operation(\"testapp\", operation)\n        \n        # Check that the operation is added correctly.\n        self.assertIn(\"testapp\", autodetector.generated_operations)\n        self.assertEqual(len(autodetector.generated_operations[\"testapp\"]), 1)\n        self.assertIsInstance(autodetector.generated_operations[\"testapp\"][0], operations.AddField)\n        self.assertEqual(autodetector.generated_operations[\"testapp\"][0]._auto_deps, [])\n", "    def test_rename_index_together_to_unique_together(self):\n        \"\"\"\n        Test renaming index_together to unique_together.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_empty, self.book_foo_together],\n            [self.author_empty, self.book_unique_together],\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes, \"otherapp\", 0, [\"RenameIndex\", \"AlterUniqueTogether\"]\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            0,\n            model_name=\"book\",\n            new_name=\"book_title_author_idx\",\n            old_fields=(\"author\", \"title\"),\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            1,\n            name=\"book\",\n            unique_together={(\"author\", \"title\")},\n        )\n", "    def test_deep_deconstruct_partial(self):\n        \"\"\"\n        Test the deep deconstruction of functools.partial objects.\n        \"\"\"\n            return x + y\n\n        partial_func = functools.partial(test_func, 5, y=10)\n        result = MigrationAutodetector(None, None).deep_deconstruct(partial_func)\n\n        self.assertEqual(result[0], test_func)\n        self.assertEqual(result[1], (5,))\n        self.assertEqual(result[2], {\"y\": 10})\n", "    def test_deep_deconstruct(self):\n        \"\"\"\n        Test deep_deconstruct method for various types of objects.\n        \"\"\"\n        autodetector = MigrationAutodetector(None, None)\n        \n        # Test deep deconstruction of a list\n        obj = [1, 2, [3, 4]]\n        self.assertEqual(autodetector.deep_deconstruct(obj), [1, 2, [3, 4]])\n        \n        # Test deep deconstruction of a tuple\n        obj = (1, 2, (3, 4))\n        self.assertEqual(autodetector.deep_deconstruct(obj), (1, 2, (3, 4)))\n        \n        # Test deep deconstruction of a dict\n        obj = {\"a\": 1, \"b\": {\"c\": 2}}\n        self.assertEqual(autodetector.deep_deconstruct(obj), {\"a\": 1, \"b\": {\"c\": 2}})\n        \n        # Test deep deconstruction of a functools.partial\n            return a + b\n        \n        obj = functools.partial(test_func, 1, b=2)\n        deconstructed = autodetector.deep_deconstruct(obj)\n        self.assertEqual(deconstructed[0], test_func)\n        self.assertEqual(deconstructed[1], (1,))\n        self.assertEqual(deconstructed[2], {\"b\": 2})\n        \n        # Test deep deconstruction of a compiled regex\n        obj = re.compile(r'\\d+')\n        deconstructed = autodetector.deep_deconstruct(obj)\n        self.assertIsInstance(deconstructed, RegexObject)\n        \n        # Test deep deconstruction of a type\n        obj = int\n        self.assertEqual(autodetector.deep_deconstruct(obj), int)\n        \n        # Test deep deconstruction of an object with deconstruct method\n        deconstructible_obj = DeconstructibleObject(1, 2, a=3)\n        deconstructed = autodetector.deep_deconstruct(deconstructible_obj)\n        self.assertEqual(deconstructed[0], 'test.models.DeconstructibleObject')\n        self.assertEqual(deconstructed[1], (1, 2))\n        self.assertEqual(deconstructed[2], {'a': 3})\n", "    def test_unique_together_with_rename_field(self):\n        \"\"\"\n        Tests detection and handling of unique_together constraint changes\n        when a field is renamed.\n        \"\"\"\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                    (\"age\", models.IntegerField()),\n                ],\n                {\n                    \"unique_together\": {(\"name\", \"age\")},\n                },\n            ),\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"fullname\", models.CharField(max_length=200)),\n                    (\"age\", models.IntegerField()),\n                ],\n                {\n                    \"unique_together\": {(\"fullname\", \"age\")},\n                },\n            ),\n        ]\n        changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename\": True}))\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes, \"testapp\", 0, [\"RenameField\", \"AlterUniqueTogether\"]\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, old_name=\"name\", new_name=\"fullname\"\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 1, name=\"author\", unique_together={(\"fullname\", \"age\")}\n        )\n"], "sample_409": ["    def test_get_available_languages(self):\n        \"\"\"Test the get_available_languages template tag.\"\"\"\n        with override_settings(LANGUAGES=[('en', 'English'), ('fr', 'French')]):\n            output = self.engine.render_to_string(\"template\")\n            self.assertIn(\"en: English\", output)\n            self.assertIn(\"fr: French\", output)\n", "    def test_get_available_languages(self):\n        \"\"\"Test the get_available_languages template tag.\"\"\"\n        with override_settings(LANGUAGES=[('en', 'English'), ('de', 'German')]):\n            output = self.engine.render_to_string(\"i18n_get_available_languages\")\n        self.assertEqual(output.strip(), \"en: English\\nde: German\")\n", "    def test_render(self):\n        context = {}\n        node = GetAvailableLanguagesNode(\"languages\")\n        node.render(context)\n        self.assertEqual(\n            context[\"languages\"],\n            [(\"en\", \"English\"), (\"de\", \"German\")]\n        )\n", "    def test_get_language_info_node(self):\n        lang_info_node = GetLanguageInfoNode(Variable(\"'en'\"), 'lang_info')\n        context = Context()\n        rendered = lang_info_node.render(context)\n        self.assertEqual(rendered, \"\")\n        self.assertIn('lang_info', context)\n        self.assertEqual(context['lang_info'], translation.get_language_info('en'))\n", "def test_get_available_languages(self):\n    \"\"\"Test the get_available_languages template tag.\"\"\"\n    with override_settings(LANGUAGES=[('en', 'English'), ('de', 'Deutsch')]):\n        output = self.engine.render_to_string(\"template\")\n        self.assertEqual(output, \"en: English de: Deutsch \")\n"], "sample_410": ["    def test_set_unusable_password(self):\n        user = AbstractBaseUser()\n        user.set_unusable_password()\n        self.assertFalse(user.has_usable_password())\n", "    def setUp(self):\n        self.user = AbstractBaseUser()\n", "    def setUp(self):\n        self.user = self.TestUser(username='testuser')\n", "    def test_set_unusable_password(self):\n        user = User(password=\"testpassword\")\n        user.set_unusable_password()\n        self.assertFalse(user.has_usable_password())\n        self.assertNotEqual(user.password, \"testpassword\")\n", "    def setUp(self):\n        self.user = AbstractBaseUser()\n"], "sample_411": ["    def test_handle_default_options(self):\n        \"\"\"\n        Test the handle_default_options() function to ensure it sets the environment\n        variables correctly.\n        \"\"\"\n        # Backup original values\n        original_settings = os.environ.get(\"DJANGO_SETTINGS_MODULE\")\n        original_pythonpath = sys.path[:]\n        \n        class Options:\n            settings = \"myproject.settings\"\n            pythonpath = \"/my/custom/path\"\n        \n        options = Options()\n        handle_default_options(options)\n        \n        self.assertEqual(os.environ[\"DJANGO_SETTINGS_MODULE\"], \"myproject.settings\")\n        self.assertIn(\"/my/custom/path\", sys.path)\n        \n        # Restore original values\n        if original_settings:\n            os.environ[\"DJANGO_SETTINGS_MODULE\"] = original_settings\n        else:\n            del os.environ[\"DJANGO_SETTINGS_MODULE\"]\n        \n        sys.path = original_pythonpath\n", "def test_no_color_and_force_color_cannot_be_used_together(self):\n    with self.assertRaises(CommandError) as cm:\n        management.call_command(\"dance\", no_color=True, force_color=True)\n    self.assertIn(\"'no_color' and 'force_color' can't be used together.\", str(cm.exception))\n", "def test_command_error_return_code(self):\n    \"\"\"Test that CommandError initializes with the correct return code.\"\"\"\n    error = CommandError(\"Test error\", returncode=5)\n    self.assertEqual(error.returncode, 5)\n", "def test_output_wrapper_style_func():\n    \"\"\"Test OutputWrapper's style_func property and its effect on output.\"\"\"\n    out = StringIO()\n    wrapper = OutputWrapper(out)\n\n    # Test default style_func (no styling)\n    wrapper.write(\"test message\")\n    self.assertEqual(out.getvalue(), \"test message\\n\")\n    out.truncate(0)\n    out.seek(0)\n\n    # Test custom style_func\n        return f\"styled: {message}\"\n\n    wrapper.style_func = custom_style_func\n    wrapper.write(\"test message\")\n    self.assertEqual(out.getvalue(), \"styled: test message\\n\")\n    out.truncate(0)\n    out.seek(0)\n\n    # Test style_func when isatty is False\n    with mock.patch.object(out, \"isatty\", return_value=False):\n        wrapper.style_func = custom_style_func\n        wrapper.write(\"test message\")\n        self.assertEqual(out.getvalue(), \"test message\\n\")\n", "def test_output_wrapper_color():\n    out = StringIO()\n    wrapper = management.base.OutputWrapper(out, ending=\"\")\n    \n    # Test with no color\n    wrapper.style_func = None\n    wrapper.write(\"no color\")\n    self.assertEqual(out.getvalue(), \"no color\")\n    out.truncate(0)\n    out.seek(0)\n    \n    # Test with color\n    wrapper.style_func = lambda x: f\"\\x1b[31m{x}\\x1b[0m\"  # Red color\n    wrapper.write(\"red color\")\n    self.assertEqual(out.getvalue(), \"\\x1b[31mred color\\x1b[0m\")\n"], "sample_412": ["    def test_avoid_wrapping(self):\n        items = (\n            (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n            (\"Hello world!\", \"Hello\\xa0world!\"),\n            (\"Django utils\", \"Django\\xa0utils\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"Hello world\", \"Hello\\xa0world\"),\n            (\"This is a test string\", \"This\\xa0is\\xa0a\\xa0test\\xa0string\"),\n            (\"No wrapping here\", \"No\\xa0wrapping\\xa0here\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"Hello world\", \"Hello\\xa0world\"),\n            (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n            (\"No wrapping here\", \"No\\xa0wrapping\\xa0here\"),\n            (\"Spaces should be replaced\", \"Spaces\\xa0should\\xa0be\\xa0replaced\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "def test_avoid_wrapping(self):\n    items = (\n        (\"hello world\", \"hello\\xa0world\"),\n        (\"multiple words with spaces\", \"multiple\\xa0words\\xa0with\\xa0spaces\"),\n        (\"no_spaces\", \"no_spaces\"),\n        (\" \", \"\\xa0\"),\n        (\"\", \"\"),\n    )\n    for value, output in items:\n        with self.subTest(value=value, output=output):\n            self.check_output(avoid_wrapping, value, output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"hello world\", \"hello\\xa0world\"),\n            (\"no wrapping here\", \"no\\xa0wrapping\\xa0here\"),\n            (\"123 456 789\", \"123\\xa0456\\xa0789\"),\n            (\"    leading spaces\", \"    leading\\xa0spaces\"),\n            (\"trailing spaces    \", \"trailing\\xa0spaces    \"),\n            (\"multiple    spaces\", \"multiple\\xa0\\xa0\\xa0\\xa0spaces\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n"], "sample_413": ["    def test_template_tags_with_no_libraries_defined(self):\n        with self.settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"OPTIONS\": {\n                        \"libraries\": {},\n                    },\n                },\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def test_no_template_tags(self):\n        with self.settings(TEMPLATES=[]):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def test_template_tags_with_same_library_name_different_case(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"Same_Tags\", \"same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_2.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [self.error_same_tags],\n            )\n", "    def test_template_tags_with_no_libraries_specified(self):\n        with self.settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"OPTIONS\": {},\n                }\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def test_no_libraries_in_templates(self):\n        with self.settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"OPTIONS\": {\n                        # No libraries specified\n                    },\n                }\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n"], "sample_414": ["    def setUp(self):\n        self.u1 = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        self.client.force_login(self.u1)\n", "    def setUp(self):\n        self.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        self.user = User.objects.create_user(username=\"user\", password=\"secret\")\n", "    def test_get_field_queryset(self):\n        \"\"\"\n        Ensure get_field_queryset respects ModelAdmin ordering.\n        \"\"\"\n        class MyModelAdmin(admin.ModelAdmin):\n            ordering = [\"name\"]\n\n        ma = MyModelAdmin(Student, admin.site)\n        request = None  # Using None for request as a placeholder\n        queryset = ma.get_field_queryset(\"default\", Student._meta.get_field(\"school\"), request)\n        self.assertIsNotNone(queryset)\n        self.assertEqual(queryset.query.order_by, ['name'])\n", "    def test_get_autocomplete_fields(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            autocomplete_fields = [\"field1\", \"field2\"]\n\n        ma = MyModelAdmin(Event, admin.site)\n        self.assertEqual(ma.get_autocomplete_fields(None), [\"field1\", \"field2\"])\n", "    def setUp(self):\n        self.superuser = User.objects.create_superuser(\n            username=\"superuser\", password=\"secret\", email=\"superuser@example.com\"\n        )\n        self.user = User.objects.create_user(username=\"user\", password=\"secret\")\n        self.staff_user = User.objects.create_user(username=\"staff\", password=\"secret\", is_staff=True)\n        self.model_admin = admin.ModelAdmin(Event, admin.site)\n"], "sample_415": ["    def test_validate_with_expressions_and_condition(self):\n        \"\"\"\n        Test validation with both expressions and condition.\n        \"\"\"\n        constraint = models.UniqueConstraint(\n            Lower(\"name\"),\n            name=\"name_lower_with_condition_uniq\",\n            condition=models.Q(color__isnull=True),\n        )\n        non_unique_product = UniqueConstraintProduct(name=self.p2.name.upper())\n        msg = \"Constraint \u201cname_lower_with_condition_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, non_unique_product)\n        # Values not matching condition are ignored.\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name, color=self.p1.color),\n        )\n        # Existing instances have their existing row excluded.\n        constraint.validate(UniqueConstraintProduct, self.p2)\n        # Unique field is excluded.\n        constraint.validate(\n            UniqueConstraintProduct,\n            non_unique_product,\n            exclude={\"name\"},\n        )\n        # Field from a condition is excluded.\n        constraint.validate(\n            UniqueConstraintProduct,\n            non_unique_product,\n            exclude={\"color\"},\n        )\n", "    def test_clone(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"foo\", \"bar\"],\n            name=\"unique_fields\",\n            condition=models.Q(foo=models.F(\"bar\")),\n            deferrable=models.Deferrable.DEFERRED,\n            include=[\"baz_1\", \"baz_2\"],\n            opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n        )\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint, cloned_constraint)\n        self.assertIsNot(constraint, cloned_constraint)\n", "    def test_clone(self):\n        constraint = models.UniqueConstraint(fields=[\"foo\", \"bar\"], name=\"unique\")\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint, cloned_constraint)\n", "    def test_unique_constraint_custom_violation_error_message(self):\n        custom_error_message = \"Custom error message for unique constraint\"\n        constraint = models.UniqueConstraint(\n            fields=[\"name\", \"color\"],\n            name=\"unique_name_color\",\n            violation_error_message=custom_error_message,\n        )\n        non_unique_product = UniqueConstraintProduct(\n            name=self.p1.name, color=self.p1.color\n        )\n        with self.assertRaisesMessage(ValidationError, custom_error_message):\n            constraint.validate(UniqueConstraintProduct, non_unique_product)\n", "    def test_unique_constraint_with_custom_violation_message(self):\n        custom_message = \"Custom violation message for unique constraint.\"\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"unique_name\",\n            violation_error_message=custom_message,\n        )\n        self.assertEqual(constraint.get_violation_error_message(), custom_message)\n        \n        # Ensure that custom violation message is used during validation\n        duplicate_product = UniqueConstraintProduct(name=self.p1.name)\n        with self.assertRaisesMessage(ValidationError, custom_message):\n            constraint.validate(UniqueConstraintProduct, duplicate_product)\n"], "sample_416": ["def test_default_dbname(self):\n    # Test case to ensure the default 'postgres' db is used when NAME and service are not provided\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"USER\": \"someuser\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n            None,\n        ),\n    )\n", "    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"someuser\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"444\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n                None,\n            ),\n        )\n", "    def test_no_dbname_or_service(self):\n        # When neither dbname nor service is provided, it should default to 'postgres'.\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"someuser\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"444\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n                None,\n            ),\n        )\n", "    def test_no_dbname_no_service(self):\n        # Test the case where both NAME and OPTIONS['service'] are not provided.\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"someuser\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"444\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n                None,\n            ),\n        )\n", "    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"someuser\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"444\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n                None,\n            ),\n        )\n"], "sample_417": ["    def test_edge_cases(self):\n        self.assertEqual(floatformat(\"1.23e3\", 2), \"1230.00\")\n        self.assertEqual(floatformat(\"1.23e-3\", 6), \"0.001230\")\n        self.assertEqual(floatformat(\"1.23e3\", -2), \"1230\")\n        self.assertEqual(floatformat(\"1.23e-3\", -6), \"0.001230\")\n        self.assertEqual(floatformat(\"1.23e3\"), \"1230\")\n        self.assertEqual(floatformat(\"1.23e-3\"), \"0.0012\")\n        self.assertEqual(floatformat(\"1.230000e-3\", 2), \"0.00\")\n        self.assertEqual(floatformat(\"1.230000e3\", -2), \"1230\")\n", "    def test_mark_safe_behavior(self):\n        # Test to ensure that mark_safe behavior is correctly applied\n        self.assertEqual(floatformat(mark_safe(\"12345.6789\"), 2), \"12345.68\")\n        self.assertEqual(floatformat(mark_safe(\"12345.0000\"), 2), \"12345.00\")\n        self.assertEqual(floatformat(mark_safe(\"12345.0000\"), -2), \"12345\")\n        self.assertEqual(floatformat(mark_safe(\"12345.6789\"), \"2u\"), \"12345.68\")\n", "    def test_addslashes(self):\n        self.assertEqual(addslashes('He said \"hello\"'), 'He said \\\\\"hello\\\\\"')\n        self.assertEqual(addslashes(\"It's a test\"), \"It\\\\'s a test\")\n        self.assertEqual(addslashes(\"Back\\\\slash\"), \"Back\\\\\\\\slash\")\n", "    def test_floatformat_special_cases(self):\n        self.assertEqual(floatformat(float('nan')), 'nan')\n        self.assertEqual(floatformat(float('inf')), 'inf')\n        self.assertEqual(floatformat(float('-inf')), '-inf')\n        self.assertEqual(floatformat('1.23.45'), '')\n        self.assertEqual(floatformat('0e0'), '0')\n        self.assertEqual(floatformat('1e-5'), '0.00001')\n        self.assertEqual(floatformat('1e5'), '100000')\n", "    def test_filesizeformat(self):\n        self.assertEqual(filesizeformat(1023), \"1023 bytes\")\n        self.assertEqual(filesizeformat(1024), \"1.0 KB\")\n        self.assertEqual(filesizeformat(1048576), \"1.0 MB\")\n        self.assertEqual(filesizeformat(1073741824), \"1.0 GB\")\n        self.assertEqual(filesizeformat(1099511627776), \"1.0 TB\")\n        self.assertEqual(filesizeformat(1125899906842624), \"1.0 PB\")\n        self.assertEqual(filesizeformat(-1024), \"-1.0 KB\")\n        self.assertEqual(filesizeformat(\"invalid\"), \"0 bytes\")\n"], "sample_418": ["    def test_floatformat_default(self):\n        output = self.engine.render_to_string(\"floatformat01\", {\"num1\": 34.23234})\n        self.assertEqual(output, \"34.2\")\n", "    def test_cut_filter01(self):\n        output = self.engine.render_to_string(\"cut_filter01\", {})\n        self.assertEqual(output, \"hell wrld\")\n", "    def test_add_integers(self):\n        output = self.engine.render_to_string(\"{{ 3|add:2 }}\", {})\n        self.assertEqual(output, \"5\")\n", "    def test_addslashes(self):\n        output = self.engine.render_to_string(\"addslashes01\", {\"value\": 'He said, \"Hello!\"'})\n        self.assertEqual(output, 'He said, \\\\\"Hello!\\\\\"')\n", "    def test_pluralize01(self):\n        output = self.engine.render_to_string(\"pluralize01\", {})\n        self.assertEqual(output, \"s\")\n"], "sample_419": ["    def test_custom_error_messages_in_formset_factory(self):\n        \"\"\"\n        Test if custom error messages provided in formset_factory are used.\n        \"\"\"\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Choice 1\",\n            \"choices-0-votes\": \"10\",\n            \"choices-1-choice\": \"Choice 2\",\n            \"choices-1-votes\": \"20\",\n        }\n        custom_error_messages = {\n            'too_many_forms': 'You have submitted too many forms.',\n            'too_few_forms': 'You have submitted too few forms.',\n            'missing_management_form': 'Custom management form error.',\n        }\n        ChoiceFormSet = formset_factory(\n            Choice, extra=1, max_num=1, min_num=3, validate_max=True, validate_min=True, error_messages=custom_error_messages\n        )\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), [\"You have submitted too many forms.\"])\n        \n        data[\"choices-TOTAL_FORMS\"] = \"1\"\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), [\"You have submitted too few forms.\"])\n\n        formset = ChoiceFormSet({}, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), [\"Custom management form error.\"])\n", "    def test_formset_custom_clean_method(self):\n        \"\"\"\n        Test that a custom clean method on a formset properly raises \n        ValidationError and the non_form_errors are accessible.\n        \"\"\"\n        class CustomFormset(BaseFormSet):\n                super().clean()\n                raise ValidationError(\"This is a custom formset error\")\n\n        CustomFormsetFactory = formset_factory(Choice, formset=CustomFormset)\n        \n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Calexico\",\n            \"choices-0-votes\": \"100\",\n        }\n\n        formset = CustomFormsetFactory(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), [\"This is a custom formset error\"])\n", "    def test_management_form_clean(self):\n        \"\"\"\n        Ensure the ManagementForm's clean method sets default values correctly\n        when management form is invalid.\n        \"\"\"\n        formset = self.make_choiceformset(\n            formset_data=[(\"Choice 1\", \"10\")],\n            total_forms=1,\n            initial_forms=0,\n        )\n        # Simulate invalid ManagementForm by tampering with TOTAL_FORMS\n        formset.management_form.cleaned_data[TOTAL_FORM_COUNT] = 2\n        self.assertEqual(formset.management_form.clean()[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(formset.management_form.clean()[INITIAL_FORM_COUNT], 0)\n", "    def test_management_form_clean(self):\n        \"\"\"\n        Test the clean method of the ManagementForm to ensure it sets the default\n        values for TOTAL_FORM_COUNT and INITIAL_FORM_COUNT if they are missing.\n        \"\"\"\n        class TestManagementForm(Form):\n            TOTAL_FORMS = IntegerField(widget=HiddenInput)\n            INITIAL_FORMS = IntegerField(widget=HiddenInput)\n\n                cleaned_data = super().clean()\n                cleaned_data.setdefault(TOTAL_FORM_COUNT, 0)\n                cleaned_data.setdefault(INITIAL_FORM_COUNT, 0)\n                return cleaned_data\n\n        form_data = {\n            \"TOTAL_FORMS\": \"5\",\n            \"INITIAL_FORMS\": \"3\"\n        }\n        form = TestManagementForm(data=form_data)\n        self.assertTrue(form.is_valid())\n        cleaned_data = form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 5)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 3)\n\n        form_data = {}  # missing fields\n        form = TestManagementForm(data=form_data)\n        self.assertTrue(form.is_valid())\n        cleaned_data = form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n", "def test_empty_form_invalid_data(self):\n        \"\"\"\n        An empty form in the formset with invalid data should be flagged as invalid.\n        \"\"\"\n        ChoiceFormSet = formset_factory(Choice)\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",  # the number of forms rendered\n            \"choices-INITIAL_FORMS\": \"0\",  # the number of forms with initial data\n            \"choices-MIN_NUM_FORMS\": \"0\",  # min number of forms\n            \"choices-MAX_NUM_FORMS\": \"0\",  # max number of forms\n            \"choices-0-choice\": \"\",  # empty data, but required field\n            \"choices-0-votes\": \"\",  # empty data, but required field\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [{\"choice\": [\"This field is required.\"], \"votes\": [\"This field is required.\"]}])\n"], "sample_420": ["    def setUp(self):\n        self.writer = Writer.objects.create(name=\"John Doe\")\n        self.article = Article.objects.create(\n            headline=\"Test Article\",\n            slug=\"test-article\",\n            pub_date=datetime.date(2020, 1, 1),\n            writer=self.writer,\n            article=\"This is a test article.\",\n        )\n", "    def test_model_form_options_initialization(self):\n        class MetaOptions:\n            model = Article\n            fields = [\"headline\", \"slug\"]\n            exclude = [\"pub_date\"]\n            widgets = {\"headline\": forms.Textarea}\n            localized_fields = [\"headline\"]\n            labels = {\"headline\": \"Article Headline\"}\n            help_texts = {\"slug\": \"Enter a unique slug\"}\n            error_messages = {\"headline\": {\"required\": \"This field is required\"}}\n            formfield_callback = lambda field: forms.CharField()\n\n        opts = ModelFormOptions(MetaOptions)\n        self.assertEqual(opts.model, Article)\n        self.assertEqual(opts.fields, [\"headline\", \"slug\"])\n        self.assertEqual(opts.exclude, [\"pub_date\"])\n        self.assertEqual(opts.widgets, {\"headline\": forms.Textarea})\n        self.assertEqual(opts.localized_fields, [\"headline\"])\n        self.assertEqual(opts.labels, {\"headline\": \"Article Headline\"})\n        self.assertEqual(opts.help_texts, {\"slug\": \"Enter a unique slug\"})\n        self.assertEqual(opts.error_messages, {\"headline\": {\"required\": \"This field is required\"}})\n        self.assertEqual(opts.formfield_callback, MetaOptions.formfield_callback)\n", "    def test_construct_instance_with_file_fields(self):\n        \"\"\"\n        Test `construct_instance` with a model that has file fields. Ensure that\n        file fields are handled correctly and deferred until after other fields.\n        \"\"\"\n        class FileModel(models.Model):\n            name = models.CharField(max_length=100)\n            file = models.FileField(upload_to='uploads/')\n\n        instance = FileModel(name=\"initial_name\", file=\"initial_file\")\n        form_data = {\"name\": \"updated_name\"}\n        file = SimpleUploadedFile(\"new_file.txt\", b\"new content\")\n        form_files = {\"file\": file}\n\n        class FileModelForm(forms.ModelForm):\n            class Meta:\n                model = FileModel\n                fields = \"__all__\"\n\n        form = FileModelForm(data=form_data, files=form_files, instance=instance)\n        self.assertTrue(form.is_valid())\n        updated_instance = construct_instance(form, instance)\n        self.assertEqual(updated_instance.name, \"updated_name\")\n        self.assertEqual(updated_instance.file, file)\n", "    def setUp(self):\n        self.writer = Writer.objects.create(name=\"Author\")\n        self.article = Article.objects.create(\n            headline=\"Original Headline\",\n            slug=\"original-headline\",\n            pub_date=datetime.date(2020, 1, 1),\n            writer=self.writer,\n            article=\"Original content\"\n        )\n        self.file_field = SimpleUploadedFile(\"file.txt\", b\"file_content\")\n", "    def test_construct_instance_with_fields_and_exclude(self):\n        \"\"\"\n        Test construct_instance with both fields and exclude parameters.\n        \"\"\"\n        # Create a writer instance to work with\n        writer = Writer.objects.create(name=\"Test Writer\")\n        \n        # Data that will populate the form\n        data = {\n            \"headline\": \"Test Headline\",\n            \"slug\": \"test-headline\",\n            \"pub_date\": \"2023-01-01\",\n            \"writer\": writer.pk,\n            \"article\": \"Test article content\",\n            \"status\": 1\n        }\n\n        # Create an ArticleForm instance with the data\n        form = ArticleForm(data=data)\n\n        # Ensure form is valid before constructing instance\n        self.assertTrue(form.is_valid())\n\n        # Call construct_instance with specified fields and exclude parameters\n        instance = construct_instance(form, Article(), fields=[\"headline\", \"slug\", \"pub_date\", \"writer\"], exclude=[\"status\"])\n\n        # Check that the fields were set correctly on the instance\n        self.assertEqual(instance.headline, \"Test Headline\")\n        self.assertEqual(instance.slug, \"test-headline\")\n        self.assertEqual(instance.pub_date, datetime.date(2023, 1, 1))\n        self.assertEqual(instance.writer, writer)\n\n        # Check that the excluded field was not set\n        self.assertIsNone(instance.status)\n"], "sample_421": ["    def test_combined_expression_basic_operations(self):\n        lhs = Value(10, output_field=IntegerField())\n        rhs = Value(5, output_field=IntegerField())\n\n        add_expr = CombinedExpression(lhs, Combinable.ADD, rhs)\n        sub_expr = CombinedExpression(lhs, Combinable.SUB, rhs)\n        mul_expr = CombinedExpression(lhs, Combinable.MUL, rhs)\n        div_expr = CombinedExpression(lhs, Combinable.DIV, rhs)\n        mod_expr = CombinedExpression(lhs, Combinable.MOD, rhs)\n        pow_expr = CombinedExpression(lhs, Combinable.POW, rhs)\n        bitand_expr = CombinedExpression(lhs, Combinable.BITAND, rhs)\n        bitor_expr = CombinedExpression(lhs, Combinable.BITOR, rhs)\n        bitxor_expr = CombinedExpression(lhs, Combinable.BITXOR, rhs)\n\n        self.assertEqual(add_expr.connector, '+')\n        self.assertEqual(sub_expr.connector, '-')\n        self.assertEqual(mul_expr.connector, '*')\n        self.assertEqual(div_expr.connector, '/')\n        self.assertEqual(mod_expr.connector, '%%')\n        self.assertEqual(pow_expr.connector, '^')\n        self.assertEqual(bitand_expr.connector, '&')\n        self.assertEqual(bitor_expr.connector, '|')\n        self.assertEqual(bitxor_expr.connector, '#')\n", "    def test_add_combined_expression(self):\n        expr1 = CombinedExpression(Value(2, output_field=IntegerField()), Combinable.ADD, Value(3, output_field=IntegerField()))\n        expr2 = CombinedExpression(Value(5, output_field=IntegerField()), Combinable.ADD, Value(7, output_field=IntegerField()))\n        self.assertNotEqual(hash(expr1), hash(expr2))\n        self.assertEqual(\n            str(expr1),\n            \"2 + 3\",\n        )\n        self.assertEqual(\n            str(expr2),\n            \"5 + 7\",\n        )\n", "    def test_combined_expression_addition(self):\n        expression = CombinedExpression(F('a'), Combinable.ADD, F('b'))\n        self.assertEqual(str(expression), 'a + b')\n", "    def test_combined_expression_with_mod(self):\n        self.assertQuerysetEqual(\n            CaseTestModel.objects.annotate(\n                test=Case(\n                    When(integer=1, then=Value(10)),\n                    When(integer=2, then=Value(20)),\n                    default=Value(30),\n                )\n                % 3,\n            ).order_by(\"pk\"),\n            [(1, 1), (2, 2), (3, 0), (2, 2), (3, 0), (3, 0), (4, 0)],\n            transform=attrgetter(\"integer\", \"test\"),\n        )\n", "    def test_combined_expression_add(self):\n        lhs = Value(5, output_field=IntegerField())\n        rhs = Value(3, output_field=IntegerField())\n        combined = CombinedExpression(lhs, Combinable.ADD, rhs)\n        self.assertEqual(str(combined), \"5 + 3\")\n"], "sample_422": ["    def test_forward_many_to_one_descriptor_get_object(self):\n        book = Book.objects.create(title=\"New Book\")\n        author = Author.objects.create(name=\"New Author\", first_book=book)\n        descriptor = ForwardManyToOneDescriptor(Author._meta.get_field(\"first_book\"))\n        self.assertEqual(descriptor.get_object(author), book)\n", "    def setUp(self):\n        self.book = Book.objects.create(title=\"Test Book\")\n        self.author = Author.objects.create(name=\"Test Author\", first_book=self.book)\n", "    def test_foreign_key_deferred_attribute_set(self):\n        \"\"\"\n        Test setting a value using ForeignKeyDeferredAttribute.\n        \"\"\"\n        class MockModel:\n                self.__dict__ = {}\n                self._state = mock.Mock()\n                self._state.db = None\n\n        field = mock.Mock()\n        field.attname = 'some_field'\n        field.is_cached.return_value = False\n        instance = MockModel()\n        fk_attr = ForeignKeyDeferredAttribute(field)\n\n        # Set value and check the dictionary\n        fk_attr.__set__(instance, 123)\n        self.assertEqual(instance.__dict__['some_field'], 123)\n\n        # Simulate cache and set a new value\n        field.is_cached.return_value = True\n        fk_attr.__set__(instance, 456)\n        self.assertEqual(instance.__dict__['some_field'], 456)\n", "    def test_foreignkey_deferred_attribute_set(self):\n        \"\"\"\n        Test that ForeignKeyDeferredAttribute correctly sets value and manages cache.\n        \"\"\"\n        class FakeField:\n            attname = 'parent_id'\n                self.remote_field = remote_field\n\n                return instance._cache\n\n                instance._cache = False\n\n        class FakeInstance:\n            _cache = True\n\n        class RemoteField:\n            multiple = False\n\n        field = FakeField(RemoteField())\n        attr = ForeignKeyDeferredAttribute(field)\n\n        instance = FakeInstance()\n        attr.__set__(instance, 1)\n        self.assertEqual(instance.__dict__['parent_id'], 1)\n        self.assertFalse(instance._cache)\n", "    def test_foreign_key_deferred_attribute_set(self):\n        \"\"\"\n        Test setting a value to ForeignKeyDeferredAttribute to ensure it updates\n        the instance's __dict__ properly and manages cache.\n        \"\"\"\n        class MockField:\n            attname = \"mock_field\"\n                return \"mock_field\" in instance.__dict__\n                instance.__dict__.pop(\"mock_field\", None)\n\n        class MockModel:\n                self.__dict__ = {}\n\n        instance = MockModel()\n        field = MockField()\n        fda = ForeignKeyDeferredAttribute(field)\n\n        # Initial setting of the value\n        fda.__set__(instance, 10)\n        self.assertEqual(instance.__dict__[\"mock_field\"], 10)\n\n        # Setting the same value should not trigger delete_cached_value\n        with mock.patch.object(field, 'delete_cached_value') as mocked_delete:\n            fda.__set__(instance, 10)\n            mocked_delete.assert_not_called()\n\n        # Setting a different value should trigger delete_cached_value\n        with mock.patch.object(field, 'delete_cached_value') as mocked_delete:\n            fda.__set__(instance, 20)\n            mocked_delete.assert_called_once()\n            self.assertEqual(instance.__dict__[\"mock_field\"], 20)\n"], "sample_423": ["def test_resolve_dependency():\n    \"\"\"\n    Tests the resolution of dependencies, especially for settings-based dependencies.\n    \"\"\"\n    before = self.make_project_state([])\n    after = self.make_project_state([self.author_empty])\n    autodetector = MigrationAutodetector(before, after)\n    \n    # Case 1: Normal dependency\n    dependency = ('app_label', 'model_name')\n    resolved_dep, is_swappable = autodetector._resolve_dependency(dependency)\n    self.assertEqual(resolved_dep, ('app_label', 'model_name'))\n    self.assertFalse(is_swappable)\n\n    # Case 2: Swappable dependency\n    with self.settings(AUTH_USER_MODEL='auth.User'):\n        dependency = ('__setting__', 'AUTH_USER_MODEL')\n        resolved_dep, is_swappable = autodetector._resolve_dependency(dependency)\n        self.assertEqual(resolved_dep, ('auth', 'user'))\n        self.assertTrue(is_swappable)\n\n    # Case 3: Non-swappable __setting__ dependency\n    with self.settings(CUSTOM_SETTING='app.Model'):\n        dependency = ('__setting__', 'CUSTOM_SETTING')\n        resolved_dep, is_swappable = autodetector._resolve_dependency(dependency)\n        self.assertEqual(resolved_dep, ('app', 'model'))\n        self.assertTrue(is_swappable)\n", "    def test_generate_altered_db_table(self):\n        \"\"\"Tests detection for altering db_table in model's options.\"\"\"\n        from_state = self.make_project_state([self.author_with_db_table_options])\n        to_state = self.make_project_state([self.author_with_new_db_table_options])\n        autodetector = MigrationAutodetector(from_state, to_state)\n        changes = autodetector._detect_changes()\n        \n        # Ensure the correct number/type of migrations\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n        \n        # Check the attributes of the migration operation\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"author\", table=\"author_two\"\n        )\n", "    def test_alter_field_with_function_default(self):\n        \"\"\"Tests altering a field with a function default.\"\"\"\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Article\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"published\", models.DateField(default=None)),\n                ],\n            ),\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Article\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"published\", models.DateField(default=models.DateField().default)),\n                ],\n            ),\n        ]\n        changes = self.get_changes(before, after)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"published\")\n", "    def test_migration_squash_name(self):\n        \"\"\"\n        Test that the parse_number method correctly identifies the second number in squashed migration names.\n        \"\"\"\n        squashed_name = \"0003_squashed_0004\"\n        self.assertEqual(MigrationAutodetector.parse_number(squashed_name), 4)\n", "    def test_alter_model_managers_with_different_managers(self):\n        \"\"\"Changing model managers with different arguments should be detected.\"\"\"\n        initial_state = ModelState(\n            \"testapp\",\n            \"Author\",\n            [(\"id\", models.AutoField(primary_key=True))],\n            managers=[(\"objects\", models.Manager())],\n        )\n        modified_state = ModelState(\n            \"testapp\",\n            \"Author\",\n            [(\"id\", models.AutoField(primary_key=True))],\n            managers=[(\"special_objects\", models.Manager())],\n        )\n        changes = self.get_changes([initial_state], [modified_state])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelManagers\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\")\n        self.assertEqual(\n            [name for name, mgr in changes[\"testapp\"][0].operations[0].managers],\n            [\"special_objects\"],\n        )\n"], "sample_424": ["    def test_delete_model_with_m2m(self):\n        \"\"\"\n        Tests the DeleteModel operation with a model that has a ManyToManyField.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_dlmo\", second_model=True)\n        # Test the state alteration\n        create_m2m_field_operation = migrations.AddField(\n            \"Pony\", \"stables\", models.ManyToManyField(\"Stable\", related_name=\"ponies\")\n        )\n        create_m2m_field_operation.state_forwards(\"test_dlmo\", project_state)\n        with connection.schema_editor() as editor:\n            create_m2m_field_operation.database_forwards(\n                \"test_dlmo\", editor, project_state, project_state\n            )\n        self.assertTableExists(\"test_dlmo_pony_stables\")\n        self.assertColumnNotExists(\"test_dlmo_pony\", \"stables\")\n        \n        # Now test deleting the model with the M2M field\n        operation = migrations.DeleteModel(\"Pony\")\n        self.assertEqual(operation.describe(), \"Delete model Pony\")\n        self.assertEqual(operation.migration_name_fragment, \"delete_pony\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_dlmo\", new_state)\n        self.assertNotIn((\"test_dlmo\", \"pony\"), new_state.models)\n        # Test the database alteration\n        self.assertTableExists(\"test_dlmo_pony\")\n        self.assertTableExists(\"test_dlmo_pony_stables\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_dlmo\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_dlmo_pony\")\n        self.assertTableNotExists(\"test_dlmo_pony_stables\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_dlmo\", editor, new_state, project_state)\n        self.assertTableExists(\"test_dlmo_pony\")\n        self.assertTableExists(\"test_dlmo_pony_stables\")\n", "    def test_alter_model_table_with_existing_table(self):\n        \"\"\"\n        Tests the AlterModelTable operation on an existing table.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_altermtwt\")\n        # Test the state alteration\n        operation = migrations.AlterModelTable(\"Pony\", \"test_altermtwt_pony_2\")\n        self.assertEqual(\n            operation.describe(), \"Rename table for Pony to test_altermtwt_pony_2\"\n        )\n        self.assertEqual(operation.migration_name_fragment, \"alter_pony_table\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_altermtwt\", new_state)\n        self.assertEqual(\n            new_state.models[\"test_altermtwt\", \"pony\"].options[\"db_table\"],\n            \"test_altermtwt_pony_2\",\n        )\n        # Ensure the table exists before alteration\n        self.assertTableExists(\"test_altermtwt_pony\")\n        # Test the database alteration\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_altermtwt\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_altermtwt_pony\")\n        self.assertTableExists(\"test_altermtwt_pony_2\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\n                \"test_altermtwt\", editor, new_state, project_state\n            )\n        self.assertTableExists(\"test_altermtwt_pony\")\n        self.assertTableNotExists(\"test_altermtwt_pony_2\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AlterModelTable\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(definition[2], {\"name\": \"Pony\", \"table\": \"test_altermtwt_pony_2\"})\n", "    def test_create_model_with_empty_options_and_bases(self):\n        \"\"\"\n        Tests the CreateModel operation with empty options and bases.\n        \"\"\"\n        operation = migrations.CreateModel(\n            \"EmptyOptionsAndBasesModel\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"attribute\", models.CharField(max_length=255)),\n            ],\n            options={},\n            bases=(),\n        )\n        self.assertEqual(operation.describe(), \"Create model EmptyOptionsAndBasesModel\")\n        self.assertEqual(operation.migration_name_fragment, \"emptyoptionsandbasesmodel\")\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_emptyoptsbases\", new_state)\n        self.assertEqual(new_state.models[\"test_emptyoptsbases\", \"emptyoptionsandbasesmodel\"].name, \"EmptyOptionsAndBasesModel\")\n        self.assertEqual(len(new_state.models[\"test_emptyoptsbases\", \"emptyoptionsandbasesmodel\"].fields), 2)\n        # Test the database alteration\n        self.assertTableNotExists(\"test_emptyoptsbases_emptyoptionsandbasesmodel\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_emptyoptsbases\", editor, project_state, new_state)\n        self.assertTableExists(\"test_emptyoptsbases_emptyoptionsandbasesmodel\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_emptyoptsbases\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_emptyoptsbases_emptyoptionsandbasesmodel\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"CreateModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"fields\", \"name\", \"options\", \"bases\"])\n", "    def test_create_model_with_duplicate_manager_name(self):\n        with self.assertRaisesMessage(\n            ValueError,\n            \"Found duplicate value objects in CreateModel managers argument.\",\n        ):\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[],\n                managers=[\n                    (\"objects\", models.Manager()),\n                    (\"objects\", models.Manager()),\n                ],\n            )\n", "    def test_create_model_with_options(self):\n        \"\"\"\n        Tests the CreateModel operation with various options set.\n        \"\"\"\n        options = {\n            \"verbose_name\": \"PonyVerbose\",\n            \"verbose_name_plural\": \"PoniesVerbose\",\n            \"db_table\": \"custom_pony_table\",\n            \"ordering\": [\"-weight\"],\n            \"get_latest_by\": \"weight\",\n        }\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n                (\"weight\", models.FloatField()),\n            ],\n            options=options,\n        )\n        self.assertEqual(operation.describe(), \"Create model Pony\")\n        self.assertEqual(operation.migration_name_fragment, \"pony\")\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmo_opt\", new_state)\n        self.assertEqual(new_state.models[\"test_crmo_opt\", \"pony\"].name, \"Pony\")\n        self.assertEqual(len(new_state.models[\"test_crmo_opt\", \"pony\"].fields), 3)\n        self.assertEqual(new_state.models[\"test_crmo_opt\", \"pony\"].options, options)\n        # Test the database alteration\n        self.assertTableNotExists(\"custom_pony_table\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_crmo_opt\", editor, project_state, new_state)\n        self.assertTableExists(\"custom_pony_table\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crmo_opt\", editor, new_state, project_state)\n        self.assertTableNotExists(\"custom_pony_table\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"CreateModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"fields\", \"name\", \"options\"])\n        self.assertEqual(definition[2][\"options\"], options)\n"], "sample_425": ["    def test_custom_serializer_registration(self):\n        class CustomObject:\n                self.name = name\n\n        class CustomObjectSerializer(BaseSerializer):\n                return f\"CustomObject({self.value.name!r})\", set()\n\n        # Register the custom serializer\n        Serializer.register(CustomObject, CustomObjectSerializer)\n        value = CustomObject(\"example\")\n        self.assertSerializedResultEqual(\n            value,\n            (\"CustomObject('example')\", set())\n        )\n\n        # Ensure the custom serializer works\n        self.assertEqual(\n            serializer_factory(value).serialize(),\n            (\"CustomObject('example')\", set())\n        )\n\n        # Unregister the custom serializer\n        Serializer.unregister(CustomObject)\n        with self.assertRaisesMessage(ValueError, \"Cannot serialize:\"):\n            serializer_factory(value).serialize()\n", "    def test_serialize_function_type(self):\n            return \"test\"\n\n        class SampleClass:\n                return \"method\"\n\n        # Serialize a regular function\n        self.assertSerializedResultEqual(\n            sample_function,\n            (\n                \"migrations.test_writer.WriterTests.test_serialize_function_type.\"\n                \"sample_function\",\n                {\"import migrations.test_writer\"},\n            ),\n        )\n\n        # Serialize a method bound to a class\n        self.assertSerializedResultEqual(\n            SampleClass.method,\n            (\n                \"migrations.test_writer.WriterTests.test_serialize_function_type.\"\n                \"SampleClass.method\",\n                {\"import migrations.test_writer\"},\n            ),\n        )\n\n        # Serialize a built-in function\n        self.assertSerializedResultEqual(\n            len,\n            (\"builtins.len\", {\"import builtins\"}),\n        )\n\n        # Test for error when trying to serialize a lambda\n        with self.assertRaisesMessage(ValueError, \"Cannot serialize function: lambda\"):\n            self.assertSerializedEqual(lambda x: x + 1)\n", "    def test_serialize_frozenset(self):\n        frozenset_value = frozenset([1, 2, 3])\n        string, imports = MigrationWriter.serialize(frozenset_value)\n        self.assertEqual(string, \"frozenset([1, 2, 3])\")\n        self.assertEqual(imports, set())\n        self.assertEqual(self.serialize_round_trip(frozenset_value), frozenset_value)\n", "    def test_serialize_custom_type(self):\n        class CustomType:\n                return (\n                    \"migrations.test_writer.CustomType\",\n                    [],\n                    {},\n                )\n        custom_type_instance = CustomType()\n        self.assertSerializedResultEqual(\n            custom_type_instance,\n            (\"migrations.test_writer.CustomType()\", {\"import migrations.test_writer\"}),\n        )\n", "    def test_serialize_dict_serializer(self):\n        value = {\"a\": 1, \"b\": 2.5, \"c\": \"hello\"}\n        result = self.serialize_round_trip(value)\n        self.assertEqual(result, value)\n"], "sample_426": ["def test_custom_time_strings(self):\n    \"\"\"\n    Test custom time strings.\n    \"\"\"\n    custom_time_strings = {\n        \"year\": \"yr\",\n        \"month\": \"mo\",\n        \"week\": \"wk\",\n        \"day\": \"dy\",\n        \"hour\": \"hr\",\n        \"minute\": \"min\",\n    }\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings),\n        \"1\\xa0yr\",\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings),\n        \"1\\xa0mo\",\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings),\n        \"1\\xa0wk\",\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings),\n        \"1\\xa0dy\",\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings),\n        \"1\\xa0hr\",\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings),\n        \"1\\xa0min\",\n    )\n", "    def test_custom_time_strings(self):\n        \"\"\"Test custom time strings.\"\"\"\n        custom_time_strings = {\n            \"year\": \"anno %(num)d\",\n            \"month\": \"mese %(num)d\",\n            \"week\": \"settimana %(num)d\",\n            \"day\": \"giorno %(num)d\",\n            \"hour\": \"ora %(num)d\",\n            \"minute\": \"minuto %(num)d\",\n        }\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), \"minuto 1\")\n        self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), \"ora 1\")\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), \"giorno 1\")\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), \"settimana 1\")\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), \"mese 1\")\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), \"anno 1\")\n", "def test_timesince_with_custom_time_strings(self):\n    \"\"\"Test timesince with custom time strings.\"\"\"\n    custom_time_strings = {\n        \"year\": npgettext_lazy(\"custom\", \"%(num)d yr\", \"%(num)d yrs\", \"num\"),\n        \"month\": npgettext_lazy(\"custom\", \"%(num)d mo\", \"%(num)d mos\", \"num\"),\n        \"week\": npgettext_lazy(\"custom\", \"%(num)d wk\", \"%(num)d wks\", \"num\"),\n        \"day\": npgettext_lazy(\"custom\", \"%(num)d dy\", \"%(num)d dys\", \"num\"),\n        \"hour\": npgettext_lazy(\"custom\", \"%(num)d hr\", \"%(num)d hrs\", \"num\"),\n        \"minute\": npgettext_lazy(\"custom\", \"%(num)d min\", \"%(num)d mins\", \"num\"),\n    }\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings),\n        \"1\\xa0min\"\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings),\n        \"1\\xa0hr\"\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings),\n        \"1\\xa0dy\"\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings),\n        \"1\\xa0wk\"\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings),\n        \"1\\xa0mo\"\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings),\n        \"1\\xa0yr\"\n    )\n", "    def test_custom_time_strings(self):\n        \"\"\"Test custom time strings.\"\"\"\n        custom_time_strings = {\n            \"year\": \"A year\",\n            \"month\": \"A month\",\n            \"week\": \"A week\",\n            \"day\": \"A day\",\n            \"hour\": \"An hour\",\n            \"minute\": \"A minute\",\n        }\n        tests = [\n            (self.t + self.oneminute, \"A minute\"),\n            (self.t + self.onehour, \"An hour\"),\n            (self.t + self.oneday, \"A day\"),\n            (self.t + self.oneweek, \"A week\"),\n            (self.t + self.onemonth, \"A month\"),\n            (self.t + self.oneyear, \"A year\"),\n        ]\n        for value, expected in tests:\n            with self.subTest():\n                self.assertEqual(timesince(self.t, value, time_strings=custom_time_strings), expected)\n                self.assertEqual(timeuntil(value, self.t, time_strings=custom_time_strings), expected)\n", "def test_custom_time_strings(self):\n    \"\"\"Test custom time strings.\"\"\"\n    custom_time_strings = {\n        \"year\": npgettext_lazy(\"custom\", \"%(num)d yr\", \"%(num)d yrs\", \"num\"),\n        \"month\": npgettext_lazy(\"custom\", \"%(num)d mo\", \"%(num)d mos\", \"num\"),\n        \"week\": npgettext_lazy(\"custom\", \"%(num)d wk\", \"%(num)d wks\", \"num\"),\n        \"day\": npgettext_lazy(\"custom\", \"%(num)d dy\", \"%(num)d dys\", \"num\"),\n        \"hour\": npgettext_lazy(\"custom\", \"%(num)d hr\", \"%(num)d hrs\", \"num\"),\n        \"minute\": npgettext_lazy(\"custom\", \"%(num)d min\", \"%(num)d mins\", \"num\"),\n    }\n    self.assertEqual(\n        timesince(self.t, self.t + 2 * self.oneday + 6 * self.onehour, time_strings=custom_time_strings),\n        \"2\\xa0dys, 6\\xa0hrs\",\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneyear + self.onemonth, time_strings=custom_time_strings),\n        \"1\\xa0yr, 1\\xa0mo\",\n    )\n    self.assertEqual(\n        timesince(self.t, self.t + 3 * self.oneweek + 2 * self.oneday, time_strings=custom_time_strings),\n        \"3\\xa0wks, 2\\xa0dys\",\n    )\n"], "sample_427": ["    def test_management_form_clean_method(self):\n        \"\"\"\n        Test that the `clean` method of the `ManagementForm` sets default values\n        for `TOTAL_FORM_COUNT` and `INITIAL_FORM_COUNT` if they are missing or invalid.\n        \"\"\"\n        # Case 1: `TOTAL_FORM_COUNT` and `INITIAL_FORM_COUNT` missing\n        data = {\n            \"form-MIN_NUM_FORMS\": \"1\",\n            \"form-MAX_NUM_FORMS\": \"5\"\n        }\n        management_form = ManagementForm(data)\n        cleaned_data = management_form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n\n        # Case 2: `TOTAL_FORM_COUNT` and `INITIAL_FORM_COUNT` invalid\n        data = {\n            \"form-TOTAL_FORMS\": \"invalid\",\n            \"form-INITIAL_FORMS\": \"invalid\",\n            \"form-MIN_NUM_FORMS\": \"1\",\n            \"form-MAX_NUM_FORMS\": \"5\"\n        }\n        with self.assertRaises(ValidationError):\n            management_form = ManagementForm(data)\n            management_form.full_clean()\n            cleaned_data = management_form.clean()\n", "    def test_management_form_clean(self):\n        \"\"\"\n        Test that the ManagementForm's clean method correctly sets default values.\n        \"\"\"\n        # Create a ManagementForm with no initial data.\n        form = ManagementForm()\n        cleaned_data = form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 0)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 0)\n\n        # Create a ManagementForm with initial data.\n        form = ManagementForm(initial={\n            TOTAL_FORM_COUNT: 3,\n            INITIAL_FORM_COUNT: 2,\n        })\n        cleaned_data = form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 3)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 2)\n", "    def test_management_form_initialization(self):\n        \"\"\"\n        Test that the ManagementForm initializes correctly with both bound and unbound data.\n        \"\"\"\n        # Test unbound ManagementForm initialization\n        formset = self.make_choiceformset()\n        management_form = formset.management_form\n        self.assertFalse(management_form.is_bound)\n        self.assertEqual(management_form.initial[TOTAL_FORM_COUNT], 1)\n        self.assertEqual(management_form.initial[INITIAL_FORM_COUNT], 0)\n        self.assertEqual(management_form.initial[MIN_NUM_FORM_COUNT], 0)\n        self.assertEqual(management_form.initial[MAX_NUM_FORM_COUNT], 1000)\n\n        # Test bound ManagementForm initialization with valid data\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"1000\",\n        }\n        formset = self.make_choiceformset(formset_data=[(\"Test\", \"1\")], total_forms=1)\n        management_form = formset.management_form\n        self.assertTrue(management_form.is_bound)\n        self.assertTrue(management_form.is_valid())\n        self.assertEqual(management_form.cleaned_data[TOTAL_FORM_COUNT], 1)\n        self.assertEqual(management_form.cleaned_data[INITIAL_FORM_COUNT], 0)\n", "    def test_management_form_clean(self):\n        \"\"\"\n        Ensure that the ManagementForm's clean method correctly sets default\n        values for TOTAL_FORM_COUNT and INITIAL_FORM_COUNT.\n        \"\"\"\n        data = {\n            \"form-TOTAL_FORMS\": \"3\",\n            \"form-INITIAL_FORMS\": \"1\",\n            \"form-MIN_NUM_FORMS\": \"0\",\n            \"form-MAX_NUM_FORMS\": \"10\",\n        }\n        management_form = ManagementForm(data)\n        self.assertTrue(management_form.is_valid())\n        cleaned_data = management_form.clean()\n        self.assertEqual(cleaned_data[TOTAL_FORM_COUNT], 3)\n        self.assertEqual(cleaned_data[INITIAL_FORM_COUNT], 1)\n", "def test_management_form_clean_data(self):\n    \"\"\"\n    Ensure the ManagementForm's clean method sets default values for\n    TOTAL_FORMS and INITIAL_FORMS.\n    \"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"\",\n        \"choices-INITIAL_FORMS\": \"\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-MAX_NUM_FORMS\": \"0\",\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.management_form.cleaned_data.get(TOTAL_FORM_COUNT), 0\n    )\n    self.assertEqual(\n        formset.management_form.cleaned_data.get(INITIAL_FORM_COUNT), 0\n    )\n"], "sample_428": ["    def test_format_zero(self):\n        # Test formatting for zero\n        self.assertEqual(nformat(0, \".\"), \"0\")\n        self.assertEqual(nformat(0, \".\", decimal_pos=2), \"0.00\")\n        self.assertEqual(nformat(0, \".\", grouping=3, thousand_sep=\",\", force_grouping=True), \"0\")\n        self.assertEqual(nformat(0, \".\", decimal_pos=3, grouping=3, thousand_sep=\",\", force_grouping=True), \"0.000\")\n", "    def test_non_standard_grouping(self):\n        self.assertEqual(\n            nformat(1234567, \".\", grouping=(3, 2, 0), thousand_sep=\",\"), \"12,34,567\"\n        )\n        self.assertEqual(\n            nformat(1234567890, \".\", grouping=(2, 3), thousand_sep=\" \"), \"1 234 567 890\"\n        )\n        self.assertEqual(\n            nformat(1234567890123, \".\", grouping=(4,), thousand_sep=\":\"), \"123:4567:8901:23\"\n        )\n", "    def test_invalid_inputs(self):\n        # Testing invalid inputs like strings that can't be converted to numbers\n        with self.assertRaises(ValueError):\n            nformat(\"abc\", \".\")\n        with self.assertRaises(ValueError):\n            nformat(\"123abc\", \".\")\n        # Testing invalid grouping values\n        with self.assertRaises(TypeError):\n            nformat(1234, \".\", grouping=\"invalid\", thousand_sep=\",\")\n        with self.assertRaises(TypeError):\n            nformat(1234, \".\", grouping=[3, \"invalid\"], thousand_sep=\",\")\n", "    def test_grouping_sequence(self):\n        self.assertEqual(nformat(12345678, \".\", grouping=(3, 2, 0), thousand_sep=\",\"), \"12,34,5678\")\n        self.assertEqual(nformat(123456789, \".\", grouping=(2, 3, 0), thousand_sep=\"-\"), \"1-23-456-789\")\n        self.assertEqual(nformat(1234567890, \".\", grouping=(1, 4, 3), thousand_sep=\".\"), \"1.2345.678.90\")\n", "    def test_edge_case_floats(self):\n        # Test edge case floats that are precisely on the threshold of decimal rounding.\n        self.assertEqual(nformat(0.00000005, \".\", decimal_pos=8), \"0.00000005\")\n        self.assertEqual(nformat(0.00000005, \".\", decimal_pos=7), \"0.0000000\")\n        self.assertEqual(nformat(0.00000005, \".\", decimal_pos=9), \"0.000000050\")\n        self.assertEqual(nformat(0.0000000500000001, \".\", decimal_pos=8), \"0.00000005\")\n        self.assertEqual(nformat(0.0000000500000001, \".\", decimal_pos=7), \"0.0000001\")\n        self.assertEqual(nformat(0.0000000500000001, \".\", decimal_pos=9), \"0.000000050\")\n"], "sample_429": ["    def test_url_validator_with_extended_schemes(self):\n        validator = URLValidator(schemes=EXTENDED_SCHEMES)\n        valid_extended_urls = [\n            \"file://localhost/path\",\n            \"git://example.com/\",\n            \"git+ssh://git@github.com/example/hg-git.git\",\n        ]\n        invalid_extended_urls = [\n            \"invalid-scheme://example.com/\",\n            \"git://-invalid.com\",\n        ]\n\n        for url in valid_extended_urls:\n            with self.subTest(url=url):\n                self.assertIsNone(validator(url))\n\n        for url in invalid_extended_urls:\n            with self.subTest(url=url):\n                with self.assertRaises(ValidationError):\n                    validator(url)\n", "    def test_valid_urls(self):\n        validator = URLValidator()\n        for url in VALID_URLS:\n            with self.subTest(url=url):\n                try:\n                    validator(url)\n                except ValidationError:\n                    self.fail(f\"URLValidator failed for valid URL: {url}\")\n", "    def test_valid_urls(self):\n        validator = URLValidator()\n        for url in VALID_URLS:\n            with self.subTest(url=url):\n                validator(url)\n", "def test_url_validator_ipv6_with_zone_id(self):\n    validator = URLValidator()\n    valid_urls = [\n        \"http://[fe80::1%25en0]\",\n        \"http://[fe80::1%25eth0]\",\n        \"http://[fe80::1%25wlan0]\",\n    ]\n    for url in valid_urls:\n        with self.subTest(url=url):\n            self.assertIsNone(validator(url))\n\n    invalid_urls = [\n        \"http://[fe80::1%25]\",\n        \"http://[fe80::1%25eth0:8080]\",\n    ]\n    for url in invalid_urls:\n        with self.subTest(url=url):\n            with self.assertRaises(ValidationError):\n                validator(url)\n", "    def test_url_validator_ipv6(self):\n        valid_ipv6_urls = [\n            \"http://[2001:db8::1]/\",\n            \"http://[::ffff:192.0.2.128]/\",\n            \"http://[::1]/\",\n            \"http://[fe80::1]/\",\n            \"http://[2001:0db8:85a3:0000:0000:8a2e:0370:7334]/\",\n            \"http://[::192.9.5.5]/ipng\"\n        ]\n        invalid_ipv6_urls = [\n            \"http://[2001:db8::12345]/\",\n            \"http://[2001:dg8::1]/\",\n            \"http://[::ffff:192.0.2.256]/\",\n            \"http://[::ffff:192.0.2.256\",\n            \"http://[::1::]/\"\n        ]\n\n        for url in valid_ipv6_urls:\n            with self.subTest(url=url):\n                validator = URLValidator()\n                try:\n                    validator(url)\n                except ValidationError:\n                    self.fail(f\"URLValidator raised ValidationError unexpectedly for {url}\")\n\n        for url in invalid_ipv6_urls:\n            with self.subTest(url=url):\n                validator = URLValidator()\n                with self.assertRaises(ValidationError):\n                    validator(url)\n"], "sample_430": ["def test_add_model_with_default_related_name(self):\n        \"\"\"\n        Test creation of new model with default_related_name option already defined.\n        \"\"\"\n        author_with_default_related_name = ModelState(\n            \"otherapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n            {\n                \"default_related_name\": \"related_author\",\n            },\n        )\n        changes = self.get_changes([], [author_with_default_related_name])\n        # Right number of migrations?\n        self.assertEqual(len(changes[\"otherapp\"]), 1)\n        # Right number of actions?\n        migration = changes[\"otherapp\"][0]\n        self.assertEqual(len(migration.operations), 1)\n        # Right actions order?\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\"])\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            0,\n            name=\"Author\",\n            options={\"default_related_name\": \"related_author\"},\n        )\n", "    def test_handle_dependency_resolution(self):\n        \"\"\"Tests proper dependency resolution in a complex scenario.\"\"\"\n        model_a = ModelState(\n            \"app_a\",\n            \"ModelA\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field_b\", models.ForeignKey(\"app_b.ModelB\", models.CASCADE)),\n            ],\n        )\n        model_b = ModelState(\n            \"app_b\",\n            \"ModelB\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field_a\", models.ForeignKey(\"app_a.ModelA\", models.CASCADE)),\n            ],\n        )\n        model_c = ModelState(\n            \"app_c\",\n            \"ModelC\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field_b\", models.ForeignKey(\"app_b.ModelB\", models.CASCADE)),\n                (\"field_a\", models.ForeignKey(\"app_a.ModelA\", models.CASCADE)),\n            ],\n        )\n        before = []\n        after = [model_a, model_b, model_c]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"app_a\", 1)\n        self.assertNumberMigrations(changes, \"app_b\", 1)\n        self.assertNumberMigrations(changes, \"app_c\", 1)\n        # Ensure dependencies are resolved correctly and there are no circular dependencies\n        self.assertMigrationDependencies(\n            changes, \"app_a\", 0, [(\"app_b\", \"__first__\")]\n        )\n        self.assertMigrationDependencies(\n            changes, \"app_b\", 0, [(\"app_a\", \"__first__\")]\n        )\n        self.assertMigrationDependencies(\n            changes, \"app_c\", 0, [(\"app_a\", \"auto_1\"), (\"app_b\", \"auto_1\")]\n        )\n", "    def test_deep_deconstruct_custom_object(self):\n        \"\"\"Test deep deconstruction of a custom deconstructible object.\"\"\"\n        custom_obj = DeconstructibleObject(\n            DeconstructibleObject(1, 2),\n            a=DeconstructibleObject(3, 4),\n            b={\"key\": DeconstructibleObject(5, 6)}\n        )\n        autodetector = MigrationAutodetector([], [])\n        deconstructed = autodetector.deep_deconstruct(custom_obj)\n        expected = (\n            \"DeconstructibleObject\",\n            ((\"DeconstructibleObject\", (1, 2), {}),),\n            {\n                \"a\": (\"DeconstructibleObject\", (3, 4), {}),\n                \"b\": {\"key\": (\"DeconstructibleObject\", (5, 6), {})},\n            },\n        )\n        self.assertEqual(deconstructed, expected)\n", "    def test_alter_field_with_preserve_default(self):\n        \"\"\"\n        #23420 - Changing a field's attributes while preserving its default should work.\n        \"\"\"\n        initial_state = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default=\"default name\")),\n            ],\n        )\n        altered_state = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=300, default=\"default name\")),\n            ],\n        )\n        changes = self.get_changes([initial_state], [altered_state])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"name\",\n            preserve_default=True,\n        )\n        self.assertOperationFieldAttributes(\n            changes, \"testapp\", 0, 0, max_length=300\n        )\n", "    def test_alter_model_managers_add(self):\n        \"\"\"\n        Adding a new manager should add a new operation.\n        \"\"\"\n        before = ModelState(\n            \"otherapp\",\n            \"Pony\",\n            [(\"id\", models.AutoField(primary_key=True))],\n            managers=[(\"objects\", models.Manager())],\n        )\n        after = ModelState(\n            \"otherapp\",\n            \"Pony\",\n            [(\"id\", models.AutoField(primary_key=True))],\n            managers=[\n                (\"objects\", models.Manager()),\n                (\"food_mgr\", FoodManager(\"a\", \"b\")),\n            ],\n        )\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterModelManagers\"])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"pony\")\n        self.assertEqual(\n            [name for name, mgr in changes[\"otherapp\"][0].operations[0].managers],\n            [\"objects\", \"food_mgr\"],\n        )\n        self.assertEqual(\n            changes[\"otherapp\"][0].operations[0].managers[1][1].args, (\"a\", \"b\", 1, 2)\n        )\n"], "sample_431": ["    def test_get_deferred_fields(self):\n        a = Article.objects.create(headline=\"Test headline\", pub_date=datetime.now())\n        a = Article.objects.only(\"headline\").get(pk=a.pk)\n        deferred_fields = a.get_deferred_fields()\n        self.assertIn(\"pub_date\", deferred_fields)\n        self.assertNotIn(\"headline\", deferred_fields)\n", "    def test_deferred_field_repr(self):\n        self.assertEqual(repr(DEFERRED), \"<Deferred field>\")\n", "    def test_abstract_model_instantiation(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        with self.assertRaisesMessage(TypeError, \"Abstract models cannot be instantiated.\"):\n            AbstractModel()\n", "    def test_initial_state(self):\n        \"\"\"Test the initial state of the ModelState instance.\"\"\"\n        state = ModelState()\n        self.assertIsNone(state.db)\n        self.assertTrue(state.adding)\n        self.assertEqual(state.fields_cache, {})\n        ", "    def test_abstract_model_instantiation_error(self):\n        class AbstractModel(Model):\n            class Meta:\n                abstract = True\n\n        with self.assertRaisesMessage(TypeError, \"Abstract models cannot be instantiated.\"):\n            AbstractModel()\n"], "sample_432": ["    def test_model_admin_str_representation(self):\n        \"\"\"\n        Test the __str__ method of ModelAdmin.\n        \"\"\"\n        parent_admin = ParentAdmin(Parent, custom_site)\n        self.assertEqual(str(parent_admin), \"admin_changelist.ParentAdmin\")\n        \n        child_admin = ChildAdmin(Child, custom_site)\n        self.assertEqual(str(child_admin), \"admin_changelist.ChildAdmin\")\n        \n        custom_pagination_admin = CustomPaginationAdmin(Child, custom_site)\n        self.assertEqual(str(custom_pagination_admin), \"admin_changelist.CustomPaginationAdmin\")\n", "def test_formfield_for_dbfield_override(self):\n        \"\"\"\n        Test that the formfield_for_dbfield method correctly applies overrides\n        from formfield_overrides.\n        \"\"\"\n        class CustomAdmin(ModelAdmin):\n            formfield_overrides = {\n                models.CharField: {'widget': widgets.AdminTextareaWidget},\n            }\n\n        model_admin = CustomAdmin(Child, custom_site)\n        request = self.factory.get(\"/child/\")\n        request.user = self.superuser\n        db_field = Child._meta.get_field('name')\n        form_field = model_admin.formfield_for_dbfield(db_field, request)\n        \n        self.assertIsInstance(form_field.widget, widgets.AdminTextareaWidget)\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.superuser = User.objects.create_superuser(\n            username=\"super\", email=\"a@b.com\", password=\"xxx\"\n        )\n        self.site = admin.AdminSite()\n        self.parent_model = Parent\n        self.inline_model = Child\n        self.model_admin = ModelAdmin(self.parent_model, self.site)\n    ", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.superuser = User.objects.create_superuser(\n            username=\"super\", email=\"a@b.com\", password=\"xxx\"\n        )\n        self.parent = Parent.objects.create(name=\"Parent\")\n        self.child = Child.objects.create(name=\"Child\", parent=self.parent)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", email=\"a@b.com\", password=\"xxx\"\n        )\n        cls.parent_model = Parent.objects.create(name=\"Parent\")\n        cls.child_model = Child.objects.create(name=\"Child\", parent=cls.parent_model)\n"], "sample_433": ["    def test_mutate_state(self):\n        \"\"\"\n        Test the mutate_state method of the Migration class.\n        \"\"\"\n        project_state = ProjectState()\n        project_state.add_model(self.author_empty.clone())\n        migration = migrations.Migration(\"0001_initial\", \"testapp\")\n        new_state = migration.mutate_state(project_state)\n        self.assertIn(\"testapp.author\", new_state.models)\n        self.assertEqual(new_state.models[\"testapp.author\"].name, \"Author\")\n        self.assertIsNot(new_state, project_state)\n", "def test_suggest_name_with_special_chars_in_multiple_operations(self):\n    \"\"\"\n    Tests that the suggest_name method handles special characters correctly\n    when there are multiple operations with suggest names.\n    \"\"\"\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.AddConstraint(\n                \"Person\",\n                models.UniqueConstraint(\n                    fields=[\"name\"], name=\"person.name-*~unique!\"\n                ),\n            ),\n            migrations.AddConstraint(\n                \"Animal\",\n                models.UniqueConstraint(\n                    fields=[\"species\"], name=\"animal.species-*~unique!\"\n                ),\n            ),\n        ]\n\n    migration = Migration(\"some_migration\", \"test_app\")\n    self.assertEqual(migration.suggest_name(), \"person_person_name_unique_animal_species_unique\")\n", "def test_migration_eq(self):\n    \"\"\"\n    Test the equality operator for the Migration class.\n    \"\"\"\n    migration1 = migrations.Migration(\"0001_initial\", \"testapp\")\n    migration2 = migrations.Migration(\"0001_initial\", \"testapp\")\n    migration3 = migrations.Migration(\"0002_second\", \"testapp\")\n    migration4 = migrations.Migration(\"0001_initial\", \"otherapp\")\n\n    self.assertEqual(migration1, migration2)\n    self.assertNotEqual(migration1, migration3)\n    self.assertNotEqual(migration1, migration4)\n", "    def test_swappable_dependency(self):\n        \"\"\"Tests the swappable_dependency function.\"\"\"\n        dependency = swappable_dependency(\"auth.User\")\n        self.assertIsInstance(dependency, SwappableTuple)\n        self.assertEqual(dependency, (\"auth\", \"__first__\"))\n        self.assertEqual(dependency.setting, \"auth.User\")\n", "def test_mutate_state_with_operations(self):\n    \"\"\"\n    Test the mutate_state method with operations.\n    \"\"\"\n    # Create a mock ProjectState and Operation\n    mock_project_state = mock.Mock()\n    mock_operation = mock.Mock()\n    mock_operation.state_forwards = mock.Mock()\n\n    class MigrationWithOperations(Migration):\n        operations = [mock_operation]\n\n    # Instantiate the MigrationWithOperations class\n    migration = MigrationWithOperations(name=\"test_migration\", app_label=\"test_app\")\n\n    # Call mutate_state with preserve=True\n    new_state = migration.mutate_state(mock_project_state, preserve=True)\n\n    # Assert that the clone method was called on the original project state\n    mock_project_state.clone.assert_called_once()\n\n    # Assert that state_forwards was called on the operation\n    mock_operation.state_forwards.assert_called_once_with(\"test_app\", new_state)\n\n    # Call mutate_state with preserve=False\n    new_state = migration.mutate_state(mock_project_state, preserve=False)\n\n    # Assert that clone method was not called again\n    self.assertEqual(mock_project_state.clone.call_count, 1)\n\n    # Assert that state_forwards was called again on the operation\n    self.assertEqual(mock_operation.state_forwards.call_count, 2)\n"], "sample_434": ["    def test_template_view_renders_correct_template(self):\n        class TestTemplateView(TemplateView):\n            template_name = \"test_template.html\"\n        \n        request = RequestFactory().get(\"/\")\n        view = TestTemplateView.as_view()\n        response = view(request)\n        \n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.template_name, [\"test_template.html\"])\n        ", "    def test_template_view_renders_correctly(self):\n        class TestTemplateView(TemplateView):\n            template_name = \"test_template.html\"\n\n                context = super().get_context_data(**kwargs)\n                context[\"test_key\"] = \"test_value\"\n                return context\n\n        request = RequestFactory().get(\"/\")\n        response = TestTemplateView.as_view()(request)\n\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(\"test_key\", response.context_data)\n        self.assertEqual(response.context_data[\"test_key\"], \"test_value\")\n", "    def test_template_view_renders_template(self):\n        class TestTemplateView(TemplateView):\n            template_name = 'test_template.html'\n\n                context = super().get_context_data(**kwargs)\n                context['test_key'] = 'test_value'\n                return context\n\n        request = RequestFactory().get('/')\n        response = TestTemplateView.as_view()(request)\n\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context_data['view'], response.context_data['view'])\n        self.assertEqual(response.context_data['test_key'], 'test_value')\n        self.assertTemplateUsed(response, 'test_template.html')\n", "    def test_template_view_renders_correct_template(self):\n        class TestTemplateView(TemplateView):\n            template_name = \"test_template.html\"\n\n        view = TestTemplateView.as_view()\n        request = RequestFactory().get(\"/\")\n\n        response = view(request)\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.template_name, [\"test_template.html\"])\n", "    def test_template_view_renders_correct_template(self):\n        request_factory = RequestFactory()\n        request = request_factory.get(\"/\")\n        \n        class MyTemplateView(TemplateView):\n            template_name = \"test_template.html\"\n        \n        view = MyTemplateView.as_view()\n        \n        with mock.patch(\"django.template.response.TemplateResponse.render\") as mock_render:\n            response = view(request)\n            mock_render.assert_called_once()\n            self.assertEqual(response.status_code, 200)\n"], "sample_435": ["    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(\"password\", None, {})\n        self.assertIn(\"summary\", context)\n        self.assertEqual(context[\"summary\"], [{\"label\": _(\"No password set.\")}])\n", "    def test_get_context_with_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(\"password\", None, {\"id\": \"id_password\"})\n        self.assertIn(\"summary\", context)\n        self.assertEqual(context[\"summary\"][0][\"label\"], \"No password set.\")\n", "    def test_get_context_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + \"test\"\n        context = widget.get_context(name=\"password\", value=value, attrs={})\n        self.assertIn(\"No password set.\", [item[\"label\"] for item in context[\"summary\"]])\n", "    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(name=\"password\", value=None, attrs={})\n        self.assertIn(\"No password set.\", context[\"summary\"][0][\"label\"])\n", "    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(name=\"password\", value=None, attrs={})\n        self.assertIn(\"summary\", context)\n        self.assertEqual(context[\"summary\"], [{\"label\": gettext(\"No password set.\")}])\n"], "sample_436": ["    def test_invalid_ipv4_address(self):\n        \"\"\"\n        Test that an invalid IPv4 address raises a CommandError with the correct message.\n        \"\"\"\n        self.write_settings(\n            \"settings.py\",\n            sdict={\n                \"ALLOWED_HOSTS\": [\"*\"],\n                \"DEBUG\": True,\n            },\n        )\n        out, err = self.run_manage([\"runserver\", \"999.999.999.999:8000\"])\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"999.999.999.999 is not a valid IPv4 address.\")\n", "    def setUp(self):\n        super().setUp()\n        self.write_settings(\n            \"settings.py\",\n            sdict={\n                \"DEBUG\": True,\n                \"ALLOWED_HOSTS\": [\"*\"],\n            },\n        )\n", "    def setUp(self):\n        super().setUp()\n        self.output = StringIO()\n        self.cmd = Command(stdout=self.output)\n", "    def setUp(self):\n        super().setUp()\n        self.write_settings(\n            \"settings.py\",\n            sdict={\n                \"ALLOWED_HOSTS\": [\"*\"],\n                \"DEBUG\": True,\n            },\n        )\n", "    def setUp(self):\n        super().setUp()\n        self.write_settings(\"settings.py\")\n        self.stdout = StringIO()\n        self.stderr = StringIO()\n        self.runserver_command = RunserverCommand(stdout=self.stdout, stderr=self.stderr)\n"], "sample_437": ["    def test_set_autocommit(self):\n        conn = connections[DEFAULT_DB_ALIAS]\n        with patch.object(conn, \"_set_autocommit\") as mocked_set_autocommit:\n            conn.set_autocommit(True)\n            self.assertTrue(conn.get_autocommit())\n            mocked_set_autocommit.assert_called_once_with(True)\n            conn.set_autocommit(False)\n            self.assertFalse(conn.get_autocommit())\n            mocked_set_autocommit.assert_called_with(False)\n            ", "    def test_savepoint_allowed(self):\n        with patch.object(connection.features, \"uses_savepoints\", True):\n            with patch.object(connection, \"get_autocommit\", return_value=False):\n                self.assertTrue(connection._savepoint_allowed())\n", "    def setUp(self):\n        self.conn = connections[DEFAULT_DB_ALIAS]\n        self.addCleanup(self.conn.close)\n", "    def test_validate_thread_sharing(self):\n        conn = connections[DEFAULT_DB_ALIAS]\n        conn.inc_thread_sharing()\n        original_thread_ident = conn._thread_ident\n\n        try:\n            # Simulate access from a different thread\n            with patch('django.db.backends.base.base._thread.get_ident', return_value=original_thread_ident + 1):\n                # Should not raise an error due to increased thread sharing count\n                conn.validate_thread_sharing()\n\n                # Decrease thread sharing count\n                conn.dec_thread_sharing()\n                # Should raise an error now as the thread sharing count is zero\n                with self.assertRaises(DatabaseError):\n                    conn.validate_thread_sharing()\n        finally:\n            # Ensure we restore the original state\n            conn._thread_ident = original_thread_ident\n            conn._thread_sharing_count = 0\n", "    def test_set_autocommit(self):\n        conn = connections[DEFAULT_DB_ALIAS]\n        with patch.object(conn, \"_set_autocommit\", return_value=None) as mocked_set_autocommit:\n            conn.set_autocommit(True)\n            self.assertTrue(conn.autocommit)\n            mocked_set_autocommit.assert_called_once_with(True)\n\n            conn.set_autocommit(False)\n            self.assertFalse(conn.autocommit)\n            mocked_set_autocommit.assert_called_with(False)\n"], "sample_438": ["    def test_create_model_with_deferred_field(self):\n        class MyModel(Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField(default=0)\n\n            class Meta:\n                app_label = 'contenttypes_tests'\n\n        # Create an instance with deferred fields\n        obj = MyModel(name='John', age=DEFERRED)\n        self.assertEqual(obj.name, 'John')\n        self.assertEqual(obj.age, DEFERRED)\n        self.assertIn('age', obj.get_deferred_fields())\n", "    def test_modelbase_creation(self):\n        class BaseModel(models.Model):\n            class Meta:\n                abstract = True\n\n        class MyModel(BaseModel):\n            field = models.CharField(max_length=100)\n\n        self.assertTrue(hasattr(MyModel, 'field'))\n        self.assertEqual(MyModel._meta.abstract, False)\n        self.assertEqual(MyModel._meta.concrete_model, MyModel)\n", "    def test_modelbase_new_without_meta(self):\n        class ModelWithoutMeta(metaclass=ModelBase):\n            pass\n\n        with self.assertRaises(RuntimeError):\n            ModelWithoutMeta()\n", "    def test_model_base_creation(self):\n        class TestModel(models.Model, metaclass=ModelBase):\n            class Meta:\n                app_label = \"test_app\"\n\n        self.assertTrue(issubclass(TestModel, models.Model))\n        self.assertTrue(hasattr(TestModel, \"_meta\"))\n        self.assertEqual(TestModel._meta.app_label, \"test_app\")\n", "    def test_modelbase_new_creation(self):\n        class TestModel(models.Model, metaclass=ModelBase):\n            class Meta:\n                app_label = \"contenttypes_tests\"\n\n        self.assertEqual(TestModel.__name__, \"TestModel\")\n        self.assertEqual(TestModel._meta.app_label, \"contenttypes_tests\")\n"], "sample_439": ["    def test_error_on_order_fields_with_invalid_field_name(self):\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = TestForm()\n        # Order fields with an invalid field name that doesn't exist in the form\n        form.order_fields([\"field1\", \"invalid_field\", \"field2\"])\n        # The invalid field should be ignored, and the remaining fields should be ordered correctly\n        self.assertEqual(list(form.fields.keys()), [\"field1\", \"field2\", \"field3\"])\n", "def test_baseform_order_fields(self):\n    class TestForm(BaseForm):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field4 = CharField()\n\n    form = TestForm()\n    self.assertEqual(list(form.fields.keys()), [\"field1\", \"field2\", \"field3\", \"field4\"])\n\n    form.order_fields([\"field3\", \"field1\"])\n    self.assertEqual(list(form.fields.keys()), [\"field3\", \"field1\", \"field2\", \"field4\"])\n\n    form.order_fields([\"field2\", \"field4\", \"field1\"])\n    self.assertEqual(list(form.fields.keys()), [\"field2\", \"field4\", \"field1\", \"field3\"])\n\n    form.order_fields(None)\n    self.assertEqual(list(form.fields.keys()), [\"field1\", \"field2\", \"field3\", \"field4\"])\n", "    def test_order_fields(self):\n        class CustomForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n\n        form = CustomForm()\n        # Original order\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3', 'field4'])\n        \n        # Reordering fields\n        form.order_fields(['field3', 'field1', 'field4'])\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field4', 'field2'])\n        \n        # Including non-existent field in the order should be ignored\n        form.order_fields(['field3', 'field1', 'non_existent', 'field4'])\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field4', 'field2'])\n\n        # Reordering with None should retain original order\n        form.order_fields(None)\n        self.assertEqual(list(form.fields), ['field3', 'field1', 'field4', 'field2'])\n", "def test_order_fields(self):\n    class OrderedForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n\n    form = OrderedForm(field_order=[\"field2\", \"field1\"])\n    self.assertEqual(list(form.fields), [\"field2\", \"field1\", \"field3\"])\n\n    form = OrderedForm(field_order=[\"field3\"])\n    self.assertEqual(list(form.fields), [\"field3\", \"field1\", \"field2\"])\n\n    form = OrderedForm(field_order=[\"field2\", \"field3\", \"field1\"])\n    self.assertEqual(list(form.fields), [\"field2\", \"field3\", \"field1\"])\n\n    form = OrderedForm(field_order=[])\n    self.assertEqual(list(form.fields), [\"field1\", \"field2\", \"field3\"])\n", "    def test_order_fields_method(self):\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n            field4 = CharField()\n\n        # Default field order\n        form = TestForm()\n        self.assertEqual(list(form.fields), [\"field1\", \"field2\", \"field3\", \"field4\"])\n\n        # Change field order\n        form.order_fields([\"field3\", \"field1\", \"field4\"])\n        self.assertEqual(list(form.fields), [\"field3\", \"field1\", \"field4\", \"field2\"])\n\n        # Ignore unknown fields in field_order\n        form.order_fields([\"field2\", \"unknown_field\"])\n        self.assertEqual(list(form.fields), [\"field2\", \"field3\", \"field1\", \"field4\"])\n"], "sample_440": ["def test_bulk_create_with_deferred_field(self):\n    \"\"\"\n    Test bulk_create with deferred fields to ensure that deferred fields do not \n    interfere with the creation and saving of the objects.\n    \"\"\"\n    Country.objects.bulk_create(self.data)\n    self.assertEqual(Country.objects.count(), 4)\n\n    new_data = [\n        Country(name=\"Australia\", iso_two_letter=\"AU\"),\n        Country(name=\"Japan\", iso_two_letter=\"JP\"),\n    ]\n    with self.assertNumQueries(1):\n        Country.objects.only(\"name\").bulk_create(new_data)\n    self.assertEqual(Country.objects.count(), 6)\n    self.assertQuerySetEqual(\n        Country.objects.order_by(\"name\").values_list(\"name\", flat=True),\n        [\"Australia\", \"Czech Republic\", \"Germany\", \"Japan\", \"The Netherlands\", \"United States of America\"]\n    )\n", "    def test_iterator(self):\n        \"\"\"\n        Test that the iterator method of QuerySet properly iterates over the results.\n        \"\"\"\n        Country.objects.bulk_create(self.data)\n        queryset = Country.objects.all()\n        country_names = [country.name for country in queryset.iterator()]\n        expected_names = [\"United States of America\", \"The Netherlands\", \"Germany\", \"Czech Republic\"]\n        self.assertCountEqual(country_names, expected_names)\n", "    def setUp(self):\n        self.country1 = Country.objects.create(name=\"Country1\", iso_two_letter=\"C1\")\n        self.country2 = Country.objects.create(name=\"Country2\", iso_two_letter=\"C2\")\n", "    def test_bulk_create_with_prefetch_related(self):\n        \"\"\"\n        Test bulk_create with prefetch_related to ensure that related objects\n        are correctly prefetched.\n        \"\"\"\n        country1 = Country.objects.create(name=\"Country1\", iso_two_letter=\"C1\")\n        country2 = Country.objects.create(name=\"Country2\", iso_two_letter=\"C2\")\n        \n        RelatedModel.objects.bulk_create(\n            [\n                RelatedModel(country=country1, name=\"Related1\"),\n                RelatedModel(country=country1, name=\"Related2\"),\n                RelatedModel(country=country2, name=\"Related3\"),\n                RelatedModel(country=country2, name=\"Related4\"),\n            ]\n        )\n        related_models = RelatedModel.objects.prefetch_related(\"country\").all()\n        with self.assertNumQueries(2):\n            for related in related_models:\n                self.assertIsNotNone(related.country)\n        self.assertEqual(len(related_models), 4)\n", "    def test_query_set_len(self):\n        countries = Country.objects.bulk_create(self.data)\n        self.assertEqual(len(countries), 4)\n        self.assertEqual(len(Country.objects.all()), 4)\n"], "sample_441": ["    def test_initial_data(self):\n        user = User.objects.create(username=\"testuser\")\n        user.set_password(\"testpassword\")\n        form = UserChangeForm(instance=user)\n        self.assertIn(\"password\", form.initial)\n        self.assertIn(\"password\", form.data)\n", "    def test_readonly_password_hash_field(self):\n        user = User.objects.get(username=\"testclient\")\n        form = UserChangeForm(instance=user)\n        password_field = form.fields[\"password\"]\n        self.assertIsInstance(password_field, ReadOnlyPasswordHashField)\n        self.assertIsInstance(password_field.widget, ReadOnlyPasswordHashWidget)\n        self.assertFalse(password_field.required)\n        self.assertTrue(password_field.disabled)\n", "    def test_get_context_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + \"random\"\n        context = widget.get_context('password', value, {})\n        self.assertIn('summary', context)\n        self.assertEqual(context['summary'][0]['label'], gettext(\"No password set.\"))\n", "    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(\"password\", None, {})\n        self.assertEqual(context[\"summary\"], [{\"label\": gettext(\"No password set.\")}])\n", "    def test_unicode_ci_compare_equal(self):\n        # Test case-insensitive comparison for equal strings\n        self.assertTrue(_unicode_ci_compare(\"hello\", \"HELLO\"))\n        self.assertTrue(_unicode_ci_compare(\"\u03a9\", \"\u2126\"))  # GREEK CAPITAL LETTER OMEGA vs OHM SIGN\n"], "sample_442": ["    def test_sign_unsign_with_different_serializers(self):\n        class CustomJSONSerializer:\n                return json.dumps(obj).encode(\"utf-8\")\n\n                return json.loads(data.decode(\"utf-8\"))\n\n        class AnotherSerializer:\n                return json.dumps(obj).encode(\"utf-16\")\n\n                return json.loads(data.decode(\"utf-16\"))\n\n        signer = signing.Signer(key=\"predictable-secret\")\n        custom_serializer = CustomJSONSerializer()\n        another_serializer = AnotherSerializer()\n\n        obj = {\"foo\": \"bar\"}\n\n        signed_obj_custom = signer.sign_object(obj, serializer=CustomJSONSerializer)\n        self.assertNotEqual(obj, signed_obj_custom)\n        self.assertEqual(obj, signer.unsign_object(signed_obj_custom, serializer=CustomJSONSerializer))\n\n        signed_obj_another = signer.sign_object(obj, serializer=AnotherSerializer)\n        self.assertNotEqual(obj, signed_obj_another)\n        self.assertEqual(obj, signer.unsign_object(signed_obj_another, serializer=AnotherSerializer))\n\n        # Ensure cross-serializer compatibility is not assumed\n        with self.assertRaises(json.JSONDecodeError):\n            signer.unsign_object(signed_obj_custom, serializer=AnotherSerializer)\n        with self.assertRaises(json.JSONDecodeError):\n            signer.unsign_object(signed_obj_another, serializer=CustomJSONSerializer)\n", "    def test_compression_flag(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        obj = \"This is a test string that will be compressed.\"\n        signed_obj = signer.sign_object(obj, compress=True)\n        self.assertTrue(signed_obj.startswith('.'))\n", "def test_sign_unsign_with_special_characters(self):\n    \"sign/unsign should handle strings with special characters\"\n    signer = signing.Signer(key=\"predictable-secret\")\n    examples = [\n        \"hello@world.com\",\n        \"password123!\",\n        \"!@#$%^&*()_+-=<>?/|\\\\{}[]`~\",\n        \"\u6f22\u5b57\u30c6\u30b9\u30c8\",\n        \"emoji test \ud83d\ude0a\ud83d\ude80\",\n    ]\n    for example in examples:\n        signed = signer.sign(example)\n        self.assertIsInstance(signed, str)\n        self.assertNotEqual(example, signed)\n        self.assertEqual(example, signer.unsign(signed))\n", "    def test_loads_with_max_age(self):\n        \"loads should raise SignatureExpired if the signed data is older than max_age\"\n        signer = signing.TimestampSigner(key=\"predictable-secret\")\n        with freeze_time(123456789):\n            value = {\"foo\": \"bar\"}\n            signed_value = signer.sign_object(value)\n        \n        with freeze_time(123456800):\n            self.assertEqual(value, signing.loads(signed_value, key=\"predictable-secret\", max_age=12))\n            with self.assertRaises(signing.SignatureExpired):\n                signing.loads(signed_value, key=\"predictable-secret\", max_age=10)\n", "    def test_sign_with_different_separators(self):\n        \"sign/unsign should be reversible with different separators\"\n        separators = [\"|\", \"#\", \"~\"]\n        for sep in separators:\n            signer = signing.Signer(key=\"predictable-secret\", sep=sep)\n            example = \"example-string\"\n            signed = signer.sign(example)\n            self.assertIsInstance(signed, str)\n            self.assertNotEqual(example, signed)\n            self.assertEqual(example, signer.unsign(signed))\n"], "sample_443": ["    def setUp(self):\n        self.cache = caches[\"default\"]\n        self.temp_dir = self.cache._dir\n", "    def test_is_expired(self):\n        # Create a cache instance\n        cache_instance = caches['default']\n        cache_key = 'test_key'\n        cache_value = 'test_value'\n        cache_instance.set(cache_key, cache_value, timeout=1)  # 1 second timeout\n\n        # Check that the cache value is present initially\n        self.assertEqual(cache_instance.get(cache_key), cache_value)\n\n        # Wait for the cache to expire\n        time.sleep(2)\n        \n        # Check that the cache value has expired\n        self.assertIsNone(cache_instance.get(cache_key))\n", "    def setUp(self):\n        super().setUp()\n        self.dirname = tempfile.mkdtemp()\n        for cache_params in settings.CACHES.values():\n            cache_params[\"LOCATION\"] = self.dirname\n        setting_changed.send(self.__class__, setting=\"CACHES\", enter=False)\n", "    def test_set_and_get_large_value(self):\n        large_value = \"x\" * (2 * 1024 * 1024)  # 2 MB\n        cache.set(\"large_key\", large_value)\n        self.assertEqual(cache.get(\"large_key\"), large_value)\n", "    def setUp(self):\n        super().setUp()\n        self.dirname = tempfile.mkdtemp()\n        settings.CACHES['default'] = {\n            'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n            'LOCATION': self.dirname,\n        }\n        self.cache = caches['default']\n"], "sample_444": ["    def test_hashed_name_with_nonexistent_file(self):\n        \"\"\"\n        Test the behavior of hashed_name() when the provided filename does not exist.\n        \"\"\"\n        storage_class = storage.staticfiles_storage.__class__\n        with self.assertRaises(ValueError) as cm:\n            storage_class().hashed_name('nonexistent/file.css')\n        self.assertEqual(\n            str(cm.exception),\n            \"The file 'nonexistent/file.css' could not be found with %r.\" % storage_class()\n        )\n", "    def test_manifest_load_invalid_json(self):\n        \"\"\"\n        Test loading a manifest with invalid JSON content.\n        \"\"\"\n        temp_manifest_storage = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, temp_manifest_storage)\n        manifest_file = Path(temp_manifest_storage) / storage.staticfiles_storage.manifest_name\n        manifest_file.write_text(\"invalid json content\")\n        storage.staticfiles_storage.manifest_storage = storage.StaticFilesStorage(location=temp_manifest_storage)\n        \n        with self.assertRaises(json.JSONDecodeError):\n            storage.staticfiles_storage.load_manifest()\n", "    def test_process_additional_patterns(self):\n        \"\"\"\n        Ensure that additional file patterns added to the storage are processed correctly.\n        \"\"\"\n        finders.get_finder.cache_clear()\n        relpath = self.hashed_file_path(\"additional/test_additional_patterns.css\")\n        self.assertEqual(relpath, \"additional/test_additional_patterns.1234567890ab.css\")\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            self.assertIn(b\"url('image.0987654321cd.png')\", content)\n            self.assertIn(b\"import url('styles.1234567890ab.css')\", content)\n        self.assertPostCondition()\n", "    def test_stored_name(self):\n        # Test known existing file\n        known_file = \"cached/styles.css\"\n        stored_name = storage.staticfiles_storage.stored_name(known_file)\n        self.assertIn(\".css\", stored_name)\n        self.assertNotEqual(stored_name, known_file)\n\n        # Test non-existent file\n        non_existent_file = \"cached/nonexistent.css\"\n        with self.assertRaisesMessage(ValueError, f\"Missing staticfiles manifest entry for '{non_existent_file}'\"):\n            storage.staticfiles_storage.stored_name(non_existent_file)\n\n        # Test manifest strict mode off\n        storage.staticfiles_storage.manifest_strict = False\n        try:\n            stored_name = storage.staticfiles_storage.stored_name(non_existent_file)\n            self.assertIn(\".css\", stored_name)\n        finally:\n            storage.staticfiles_storage.manifest_strict = True\n", "    def test_static_files_storage_path(self):\n        storage_instance = storage.StaticFilesStorage()\n        \n        # Test with valid STATIC_ROOT setting\n        with override_settings(STATIC_ROOT='/tmp/static_root'):\n            storage_instance.location = settings.STATIC_ROOT\n            path = storage_instance.path('test/file.txt')\n            self.assertEqual(path, os.path.join('/tmp/static_root', 'test/file.txt'))\n        \n        # Test with STATIC_ROOT set to None\n        with override_settings(STATIC_ROOT=None):\n            with self.assertRaises(ImproperlyConfigured):\n                storage_instance.path('test/file.txt')\n"], "sample_445": ["    def test_timesince_non_default_time_strings(self):\n        \"\"\"Test timesince with non-default time strings.\"\"\"\n        time_strings = {\n            \"year\": npgettext_lazy(\"custom\", \"%(num)d year\", \"%(num)d years\", \"num\"),\n            \"month\": npgettext_lazy(\"custom\", \"%(num)d month\", \"%(num)d months\", \"num\"),\n            \"week\": npgettext_lazy(\"custom\", \"%(num)d week\", \"%(num)d weeks\", \"num\"),\n            \"day\": npgettext_lazy(\"custom\", \"%(num)d day\", \"%(num)d days\", \"num\"),\n            \"hour\": npgettext_lazy(\"custom\", \"%(num)d hour\", \"%(num)d hours\", \"num\"),\n            \"minute\": npgettext_lazy(\"custom\", \"%(num)d minute\", \"%(num)d minutes\", \"num\"),\n        }\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneminute, time_strings=time_strings),\n            \"1\\xa0minute\"\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.onehour, time_strings=time_strings),\n            \"1\\xa0hour\"\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneday, time_strings=time_strings),\n            \"1\\xa0day\"\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneweek, time_strings=time_strings),\n            \"1\\xa0week\"\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.onemonth, time_strings=time_strings),\n            \"1\\xa0month\"\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneyear, time_strings=time_strings),\n            \"1\\xa0year\"\n        )\n", "def test_time_strings_override(self):\n    \"\"\"Test custom time_strings override.\"\"\"\n    custom_time_strings = {\n        \"year\": npgettext_lazy(\"custom\", \"%(num)d custom year\", \"%(num)d custom years\", \"num\"),\n        \"month\": npgettext_lazy(\"custom\", \"%(num)d custom month\", \"%(num)d custom months\", \"num\"),\n        \"week\": npgettext_lazy(\"custom\", \"%(num)d custom week\", \"%(num)d custom weeks\", \"num\"),\n        \"day\": npgettext_lazy(\"custom\", \"%(num)d custom day\", \"%(num)d custom days\", \"num\"),\n        \"hour\": npgettext_lazy(\"custom\", \"%(num)d custom hour\", \"%(num)d custom hours\", \"num\"),\n        \"minute\": npgettext_lazy(\"custom\", \"%(num)d custom minute\", \"%(num)d custom minutes\", \"num\"),\n    }\n    self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), \"1\\xa0custom minute\")\n    self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), \"1\\xa0custom hour\")\n    self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), \"1\\xa0custom day\")\n    self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), \"1\\xa0custom week\")\n    self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), \"1\\xa0custom month\")\n    self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), \"1\\xa0custom year\")\n", "    def test_custom_time_strings(self):\n        \"\"\"Test timesince with custom time strings.\"\"\"\n        custom_time_strings = {\n            \"year\": npgettext_lazy(\"custom\", \"%(num)d yr\", \"%(num)d yrs\", \"num\"),\n            \"month\": npgettext_lazy(\"custom\", \"%(num)d mo\", \"%(num)d mos\", \"num\"),\n            \"week\": npgettext_lazy(\"custom\", \"%(num)d wk\", \"%(num)d wks\", \"num\"),\n            \"day\": npgettext_lazy(\"custom\", \"%(num)d dy\", \"%(num)d dys\", \"num\"),\n            \"hour\": npgettext_lazy(\"custom\", \"%(num)d hr\", \"%(num)d hrs\", \"num\"),\n            \"minute\": npgettext_lazy(\"custom\", \"%(num)d min\", \"%(num)d mins\", \"num\"),\n        }\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), \"1\\xa0yr\")\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), \"1\\xa0mo\")\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), \"1\\xa0wk\")\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), \"1\\xa0dy\")\n        self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), \"1\\xa0hr\")\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), \"1\\xa0min\")\n", "def test_negative_depth(self):\n    \"\"\"Test handling of negative depth values.\"\"\"\n    msg = \"depth must be greater than 0.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        timesince(self.t, self.t, depth=-1)\n", "def test_custom_time_strings(self):\n        \"\"\"Test timesince with custom time strings.\"\"\"\n        custom_time_strings = {\n            \"year\": ngettext_lazy(\"%(num)d yr\", \"%(num)d yrs\", \"num\"),\n            \"month\": ngettext_lazy(\"%(num)d mth\", \"%(num)d mths\", \"num\"),\n            \"week\": ngettext_lazy(\"%(num)d wk\", \"%(num)d wks\", \"num\"),\n            \"day\": ngettext_lazy(\"%(num)d d\", \"%(num)d ds\", \"num\"),\n            \"hour\": ngettext_lazy(\"%(num)d hr\", \"%(num)d hrs\", \"num\"),\n            \"minute\": ngettext_lazy(\"%(num)d min\", \"%(num)d mins\", \"num\"),\n        }\n\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings),\n            \"1\\xa0yr\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings),\n            \"1\\xa0mth\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings),\n            \"1\\xa0wk\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings),\n            \"1\\xa0d\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings),\n            \"1\\xa0hr\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings),\n            \"1\\xa0min\",\n        )\n"], "sample_446": ["    def test_addslashes(self):\n        self.assertEqual(addslashes('test \"string\"'), 'test \\\\\"string\\\\\"')\n        self.assertEqual(addslashes(\"test 'string'\"), \"test \\\\'string\\\\'\")\n        self.assertEqual(addslashes(\"test \\\\string\\\\\"), \"test \\\\\\\\string\\\\\\\\\")\n    ", "    def test_invalid_inputs(self):\n        self.assertEqual(floatformat(\"abc\"), \"\")\n        self.assertEqual(floatformat(\"123.456\", \"invalid\"), \"123.456\")\n        self.assertEqual(floatformat([1, 2, 3]), \"\")\n        self.assertEqual(floatformat({\"key\": \"value\"}), \"\")\n", "    def test_special_float_strings(self):\n        self.assertEqual(floatformat(\"NaN\"), \"nan\")\n        self.assertEqual(floatformat(\"-NaN\"), \"nan\")\n        self.assertEqual(floatformat(\"Infinity\"), \"inf\")\n        self.assertEqual(floatformat(\"-Infinity\"), \"-inf\")\n", "    def test_string_inputs(self):\n        self.assertEqual(floatformat(\"7.7\"), \"7.7\")\n        self.assertEqual(floatformat(\"7.0\"), \"7\")\n        self.assertEqual(floatformat(\"0.7\"), \"0.7\")\n        self.assertEqual(floatformat(\"-0.7\"), \"-0.7\")\n        self.assertEqual(floatformat(\"0.07\"), \"0.1\")\n        self.assertEqual(floatformat(\"-0.07\"), \"-0.1\")\n        self.assertEqual(floatformat(\"0.007\"), \"0.0\")\n        self.assertEqual(floatformat(\"0.0\"), \"0\")\n        self.assertEqual(floatformat(\"7.7\", 0), \"8\")\n        self.assertEqual(floatformat(\"7.7\", 3), \"7.700\")\n        self.assertEqual(floatformat(\"6.000000\", 3), \"6.000\")\n        self.assertEqual(floatformat(\"6.200000\", 3), \"6.200\")\n        self.assertEqual(floatformat(\"6.200000\", -3), \"6.200\")\n        self.assertEqual(floatformat(\"13.1031\", -3), \"13.103\")\n        self.assertEqual(floatformat(\"11.1197\", -2), \"11.12\")\n        self.assertEqual(floatformat(\"11.0000\", -2), \"11\")\n        self.assertEqual(floatformat(\"11.000001\", -2), \"11.00\")\n        self.assertEqual(floatformat(\"8.2798\", 3), \"8.280\")\n        self.assertEqual(floatformat(\"5555.555\", 2), \"5555.56\")\n        self.assertEqual(floatformat(\"001.3000\", 2), \"1.30\")\n        self.assertEqual(floatformat(\"0.12345\", 2), \"0.12\")\n        self.assertEqual(floatformat(Decimal(\"555.555\"), 2), \"555.56\")\n        self.assertEqual(floatformat(Decimal(\"09.000\")), \"9\")\n        self.assertEqual(\n            floatformat(Decimal(\"123456.123456789012345678901\"), 21),\n            \"123456.123456789012345678901\",\n        )\n        self.assertEqual(floatformat(\"foo\"), \"\")\n        self.assertEqual(floatformat(\"13.1031\", \"bar\"), \"13.1031\")\n        self.assertEqual(floatformat(\"18.125\", 2), \"", "    def test_invalid_inputs(self):\n        self.assertEqual(floatformat(\"invalid\", 2), \"\")\n        self.assertEqual(floatformat(\"1.23\", \"invalid\"), \"1.23\")\n        self.assertEqual(floatformat(\"invalid\", \"invalid\"), \"\")\n        self.assertEqual(floatformat(None, \"invalid\"), \"\")\n"], "sample_447": ["    def test_combined_expression_with_datetime(self):\n        test_datetime = datetime.datetime(2023, 10, 5, 12, 30, 15)\n        test_timedelta = datetime.timedelta(days=5, hours=3)\n        test_combined = test_datetime + test_timedelta\n        Ticket.objects.create(active_at=test_datetime, duration=test_timedelta)\n        t = Ticket.objects.annotate(\n            combined=ExpressionWrapper(\n                F(\"active_at\") + F(\"duration\"), output_field=DateTimeField()\n            )\n        ).first()\n        self.assertEqual(t.combined, test_combined)\n", "    def test_combined_expressions_with_different_operators(self):\n        test = self.b1\n        book = Book.objects.annotate(\n            add_combined=ExpressionWrapper(F(\"pages\") + F(\"rating\"), output_field=IntegerField()),\n            sub_combined=ExpressionWrapper(F(\"pages\") - F(\"rating\"), output_field=IntegerField()),\n            mul_combined=ExpressionWrapper(F(\"pages\") * F(\"rating\"), output_field=FloatField()),\n            div_combined=ExpressionWrapper(F(\"pages\") / F(\"rating\"), output_field=FloatField()),\n        ).get(isbn=test.isbn)\n        \n        add_combined = int(test.pages + test.rating)\n        sub_combined = int(test.pages - test.rating)\n        mul_combined = test.pages * float(test.rating)\n        div_combined = test.pages / float(test.rating)\n        \n        self.assertEqual(book.add_combined, add_combined)\n        self.assertEqual(book.sub_combined, sub_combined)\n        self.assertEqual(book.mul_combined, mul_combined)\n        self.assertAlmostEqual(book.div_combined, div_combined, places=2)\n", "    def test_combined_expression_with_subquery(self):\n        authors_subquery = (\n            Author.objects.filter(\n                age__lt=30,\n            )\n            .annotate(younger_age=F(\"age\"))\n            .values(\"younger_age\")\n        )\n        books = Book.objects.annotate(\n            combined_value=ExpressionWrapper(\n                F(\"pages\") + Subquery(authors_subquery[:1]), output_field=IntegerField()\n            )\n        )\n        for book in books:\n            self.assertGreaterEqual(book.combined_value, book.pages)\n", "    def test_combined_expression_with_arithmetic_operations(self):\n        book = Book.objects.annotate(\n            combined=ExpressionWrapper(\n                F(\"pages\") + F(\"rating\") - F(\"price\") * 2 / F(\"rating\"), \n                output_field=FloatField()\n            )\n        ).get(isbn=self.b1.isbn)\n        combined = float(self.b1.pages + self.b1.rating - self.b1.price * 2 / self.b1.rating)\n        self.assertAlmostEqual(book.combined, combined, places=2)\n", "    def test_combined_expression_bitwise_operations(self):\n        # Create an Employee instance for testing.\n        employee = Employee.objects.create(\n            first_name=\"Alan\",\n            last_name=\"Turing\",\n            age=41,\n            salary=Decimal(\"100000.00\"),\n            store=self.s1,\n        )\n        \n        # Annotate the employee with a combined expression using bitwise operations.\n        employees = Employee.objects.annotate(\n            bitwise_and=F(\"age\").bitand(32),\n            bitwise_or=F(\"age\").bitor(32),\n            bitwise_xor=F(\"age\").bitxor(32),\n            bitwise_left_shift=F(\"age\").bitleftshift(2),\n            bitwise_right_shift=F(\"age\").bitrightshift(2),\n        )\n        \n        # Retrieve the annotated employee.\n        annotated_employee = employees.get(pk=employee.pk)\n        \n        # Calculate the expected results of the bitwise operations.\n        expected_bitwise_and = employee.age & 32\n        expected_bitwise_or = employee.age | 32\n        expected_bitwise_xor = employee.age ^ 32\n        expected_bitwise_left_shift = employee.age << 2\n        expected_bitwise_right_shift = employee.age >> 2\n        \n        # Verify the annotated values.\n        self.assertEqual(annotated_employee.bitwise_and, expected_bitwise_and)\n        self.assertEqual(annotated_employee.bitwise_or, expected_bitwise_or)\n        self.assertEqual(annotated_employee.bitwise_xor, expected_bitwise_xor)\n        self.assertEqual(annotated_employee.bitwise_left_shift, expected_bitwise_left_shift)\n        self.assertEqual(annotated_employee.bitwise_right_shift, expected_bitwise_right_shift)\n"], "sample_448": ["    def test_clone(self):\n        constraint = models.UniqueConstraint(fields=[\"foo\", \"bar\"], name=\"unique\")\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint, cloned_constraint)\n", "    def test_clone(self):\n        constraint = models.UniqueConstraint(fields=[\"foo\", \"bar\"], name=\"unique\")\n        cloned_constraint = constraint.clone()\n        self.assertEqual(cloned_constraint, constraint)\n        self.assertEqual(cloned_constraint.name, constraint.name)\n        self.assertEqual(cloned_constraint.fields, constraint.fields)\n        self.assertEqual(cloned_constraint.condition, constraint.condition)\n        self.assertEqual(cloned_constraint.deferrable, constraint.deferrable)\n        self.assertEqual(cloned_constraint.include, constraint.include)\n        self.assertEqual(cloned_constraint.opclasses, constraint.opclasses)\n        self.assertEqual(cloned_constraint.expressions, constraint.expressions)\n        self.assertEqual(cloned_constraint.violation_error_message, constraint.violation_error_message)\n", "def test_unique_constraint_deconstruct_with_all_options(self):\n    name = \"unique_constraint_full\"\n    fields = [\"foo\", \"bar\"]\n    condition = models.Q(foo=models.F(\"bar\"))\n    deferrable = models.Deferrable.DEFERRED\n    include = [\"baz_1\", \"baz_2\"]\n    opclasses = [\"text_pattern_ops\", \"varchar_pattern_ops\"]\n    expressions = [Lower(\"title\"), F(\"author\")]\n    \n    constraint = models.UniqueConstraint(\n        *expressions,\n        fields=fields,\n        name=name,\n        condition=condition,\n        deferrable=deferrable,\n        include=include,\n        opclasses=opclasses,\n        violation_error_message=\"custom error message\",\n    )\n    \n    path, args, kwargs = constraint.deconstruct()\n    self.assertEqual(path, \"django.db.models.UniqueConstraint\")\n    self.assertEqual(args, tuple(expressions))\n    self.assertEqual(\n        kwargs,\n        {\n            \"fields\": tuple(fields),\n            \"name\": name,\n            \"condition\": condition,\n            \"deferrable\": deferrable,\n            \"include\": tuple(include),\n            \"opclasses\": opclasses,\n            \"violation_error_message\": \"custom error message\",\n        },\n    )\n", "    def test_validate_with_excluded_field(self):\n        constraint = models.UniqueConstraint(fields=[\"name\", \"color\"], name=\"unique_name_color\")\n        p3 = UniqueConstraintProduct(name=\"p1\", color=\"blue\")\n        # Exclude one of the fields in the constraint.\n        constraint.validate(UniqueConstraintProduct, p3, exclude={\"color\"})\n        # If both fields are excluded, it should not raise an error.\n        constraint.validate(UniqueConstraintProduct, p3, exclude={\"name\", \"color\"})\n        # If neither field is excluded, it should raise a validation error.\n        msg = \"Unique constraint product with this Name and Color already exists.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, p3)\n", "    def test_repr(self):\n        self.assertEqual(repr(Deferrable.DEFERRED), 'Deferrable.DEFERRED')\n        self.assertEqual(repr(Deferrable.IMMEDIATE), 'Deferrable.IMMEDIATE')\n"], "sample_449": ["def test_handle_one_request(self):\n        \"\"\"A WSGI app that just returns a hello world.\"\"\"\n        start_response(\"200 OK\", [])\n        return [b\"Hello World\"]\n\n    rfile = BytesIO(b\"GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n\")\n    rfile.seek(0)\n    wfile = UnclosableBytesIO()\n\n        if mode == \"rb\":\n            return rfile\n        elif mode == \"wb\":\n            return wfile\n\n    request = Stub(makefile=makefile)\n    server = Stub(base_environ={}, get_app=lambda: test_app)\n\n    # Prevent logging from appearing in test output.\n    with self.assertLogs(\"django.server\", \"INFO\"):\n        handler = WSGIRequestHandler(request, \"127.0.0.1\", server)\n        handler.handle_one_request()\n\n    wfile.seek(0)\n    lines = list(wfile.readlines())\n    self.assertIn(b\"HTTP/1.1 200 OK\\r\\n\", lines)\n    self.assertIn(b\"Hello World\", lines)\n", "    def test_ipv6_support(self):\n        \"\"\"WSGIServer should support IPv6 if ipv6 argument is True.\"\"\"\n        addr = \"::1\"\n        port = 8000\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            httpd = WSGIServer((addr, port), WSGIRequestHandler, ipv6=True)\n            self.assertEqual(httpd.address_family, socket.AF_INET6)\n            httpd.server_close()  # Ensure the server is properly closed after the test\n", "compilation error", "compilation error", "    def test_threaded_server_request_handling(self):\n        \"\"\"ThreadedWSGIServer handles requests correctly and overrides connections.\"\"\"\n            start_response(\"200 OK\", [])\n            return [b\"Hello World\"]\n\n        rfile = BytesIO(b\"GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n\")\n        rfile.seek(0)\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = ThreadedWSGIServer(\n            (\"localhost\", 0), WSGIRequestHandler, connections_override={\"default\": \"mock_connection\"}\n        )\n        server.set_app(test_app)\n\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            server.process_request_thread(request, (\"127.0.0.1\", 0))\n\n        wfile.seek(0)\n        response = wfile.read()\n        self.assertIn(b\"Hello World\", response)\n        self.assertIn(b\"200 OK\", response)\n"], "sample_450": ["    def test_get_admin_log_node_render(self):\n        log_entry = LogEntry.objects.first()\n        context = {\n            \"log_entries\": LogEntry.objects.all(),\n            \"user\": self.user,\n            \"log_entry\": log_entry\n        }\n        \n        node = AdminLogNode(limit=\"5\", varname=\"filtered_log\", user=None)\n        node.render(context)\n        self.assertIn(\"filtered_log\", context)\n        self.assertEqual(list(context[\"filtered_log\"]), list(LogEntry.objects.all()[:5]))\n\n        node_user = AdminLogNode(limit=\"5\", varname=\"user_filtered_log\", user=str(self.user.pk))\n        node_user.render(context)\n        self.assertIn(\"user_filtered_log\", context)\n        self.assertEqual(list(context[\"user_filtered_log\"]), list(LogEntry.objects.filter(user__pk=self.user.pk)[:5]))\n", "    def test_get_admin_log_tag(self):\n        \"\"\"\n        Test the custom template tag 'get_admin_log' to ensure it returns the correct log entries.\n        \"\"\"\n        # Setup log entries in context\n        log_entries = LogEntry.objects.all()\n        context = template.Context({\"log_entries\": log_entries, \"user\": self.user})\n\n        # Test without user filter\n        node = get_admin_log(None, template.Token(token_type=template.TOKEN_BLOCK, contents=\"get_admin_log 10 as admin_log\"))\n        node.render(context)\n        self.assertEqual(context[\"admin_log\"].count(), min(10, log_entries.count()))\n\n        # Test with user filter\n        node = get_admin_log(None, template.Token(token_type=template.TOKEN_BLOCK, contents=\"get_admin_log 10 as user_log for_user user\"))\n        node.render(context)\n        self.assertEqual(context[\"user_log\"].count(), min(10, log_entries.filter(user=self.user).count()))\n\n        # Test with user filter using user ID\n        node = get_admin_log(None, template.Token(token_type=template.TOKEN_BLOCK, contents=\"get_admin_log 10 as user_log for_user %s\" % self.user.pk))\n        node.render(context)\n        self.assertEqual(context[\"user_log\"].count(), min(10, log_entries.filter(user=self.user).count()))\n\n        # Test invalid usage\n        with self.assertRaises(template.TemplateSyntaxError):\n            get_admin_log(None, template.Token(token_type=template.TOKEN_BLOCK, contents=\"get_admin_log as admin_log\"))\n", "    def test_get_admin_log_tag(self):\n        \"\"\"\n        Test the custom template tag 'get_admin_log' to ensure it populates\n        the context with the correct log entries.\n        \"\"\"\n        from django.template import Context, Template\n\n        content_type_pk = ContentType.objects.get_for_model(Article).pk\n\n        # Create additional log entries\n        for i in range(5):\n            LogEntry.objects.log_action(\n                self.user.pk,\n                content_type_pk,\n                self.a1.pk,\n                f\"Article changed {i}\",\n                CHANGE,\n                change_message=f\"Changed something {i}\",\n            )\n\n        # Test the template tag without user filter\n        template_string = '{% load your_template_tag_module %}{% get_admin_log 3 as recent_logs %}'\n        template = Template(template_string)\n        context = Context({\"log_entries\": LogEntry.objects.all()})\n        template.render(context)\n        self.assertEqual(len(context[\"recent_logs\"]), 3)\n        self.assertEqual(context[\"recent_logs\"][0].object_repr, \"Article changed 4\")\n\n        # Test the template tag with user filter by user ID\n        template_string = '{% load your_template_tag_module %}{% get_admin_log 2 as recent_logs for_user 1 %}'\n        template = Template(template_string)\n        context = Context({\"log_entries\": LogEntry.objects.all()})\n        template.render(context)\n        self.assertEqual(len(context[\"recent_logs\"]), 2)\n        self.assertEqual(context[\"recent_logs\"][0].object_repr, \"Article changed 4\")\n\n        # Test the template tag with user filter by context variable\n        context = Context({\n            \"log_entries\": LogEntry.objects.all(),\n            \"user\": self.user,\n        })\n        template_string = '{% load your_template_tag_module %}{% get_admin_log 4 as recent_logs for_user user %}'\n        template = Template(template_string)\n        template.render(context)\n        self.assertEqual(len(context[\"recent_logs\"]), 4)\n        self.assertEqual(context[\"recent_logs\"][0].object_repr, \"Article changed 4\")\n", "    def test_get_admin_log_tag(self):\n        \"\"\"\n        Test the custom template tag get_admin_log.\n        \"\"\"\n        from django.template import Context, Template\n\n        log_entries = LogEntry.objects.all()\n        context = Context({\"log_entries\": log_entries, \"user\": self.user})\n\n        # Test the tag without user\n        template_str = \"{% load custom_tags %} {% get_admin_log 5 as admin_logs %}\"\n        template = Template(template_str)\n        template.render(context)\n        self.assertEqual(len(context[\"admin_logs\"]), min(5, log_entries.count()))\n\n        # Test the tag with user as context variable\n        template_str = (\n            \"{% load custom_tags %} {% get_admin_log 5 as user_admin_logs for_user user %}\"\n        )\n        template = Template(template_str)\n        template.render(context)\n        user_logs = log_entries.filter(user=self.user)\n        self.assertEqual(len(context[\"user_admin_logs\"]), min(5, user_logs.count()))\n\n        # Test the tag with user as user ID\n        template_str = (\n            \"{% load custom_tags %} {% get_admin_log 5 as id_admin_logs for_user \"\n            + str(self.user.pk)\n            + \" %}\"\n        )\n        template = Template(template_str)\n        template.render(context)\n        self.assertEqual(len(context[\"id_admin_logs\"]), min(5, user_logs.count()))\n", "    def test_get_admin_log_tag(self):\n        \"\"\"\n        Test the get_admin_log template tag to ensure it correctly populates the\n        context variable with the filtered admin log entries.\n        \"\"\"\n        from django.template import Context, Template\n\n        # Prepare log entries\n        user2 = User.objects.create_user(username=\"user2\", password=\"secret\")\n        content_type_pk = ContentType.objects.get_for_model(Article).pk\n        LogEntry.objects.log_action(\n            user2.pk,\n            content_type_pk,\n            self.a1.pk,\n            repr(self.a1),\n            CHANGE,\n            change_message=\"Changed by user2\",\n        )\n        LogEntry.objects.log_action(\n            self.user.pk,\n            content_type_pk,\n            self.a1.pk,\n            repr(self.a1),\n            ADDITION,\n            change_message=\"Added by superuser\",\n        )\n        \n        # Context with log entries and user\n        context = Context({\n            \"log_entries\": LogEntry.objects.all(),\n            \"user\": self.user,\n            \"user2\": user2,\n        })\n\n        # Template rendering without user filter\n        template_code = \"{% load your_template_library %}{% get_admin_log 10 as admin_log %}\"\n        template = Template(template_code)\n        template.render(context)\n        self.assertEqual(len(context[\"admin_log\"]), 2)\n\n        # Template rendering with user filter\n        template_code = \"{% load your_template_library %}{% get_admin_log 10 as admin_log for_user user2 %}\"\n        template = Template(template_code)\n        template.render(context)\n        self.assertEqual(len(context[\"admin_log\"]), 1)\n        self.assertEqual(context[\"admin_log\"][0].change_message, \"Changed by user2\")\n"], "sample_451": ["    def test_replace_metacharacters(self):\n        pattern = r'abc\\?def\\*ghi\\+jkl\\^mno\\$pqr'\n        replaced_pattern = replace_metacharacters(pattern)\n        expected_pattern = 'abcdef*ghijklmno$pqr'\n        self.assertEqual(replaced_pattern, expected_pattern)\n", "def test_replace_named_groups(self):\n        pattern = r\"^(?P<a>\\w+)/b/(?P<c>\\w+)$\"\n        expected = r\"^<a>/b/<c>$\"\n        result = replace_named_groups(pattern)\n        self.assertEqual(result, expected)\n\n        pattern = r\"^(?P<a>\\w+)/b/(\\w+)$\"\n        expected = r\"^(?P<a>\\w+)/b/<var>$\"\n        result = replace_unnamed_groups(pattern)\n        self.assertEqual(result, expected)\n\n        pattern = r\"(?P<a>\\w+)/b/(?:\\w+)c(?:\\w+)\"\n        expected = r\"(?P<a>\\w+)/b/c\"\n        result = remove_non_capturing_groups(pattern)\n        self.assertEqual(result, expected)\n", "    def test_replace_metacharacters(self):\n        self.assertEqual(replace_metacharacters(r\"abc?def*ghi+\"), \"abcdefghi\")\n        self.assertEqual(replace_metacharacters(r\"abc\\?def\\*ghi\\+\"), \"abc?def*ghi+\")\n        self.assertEqual(replace_metacharacters(r\"^abc$\"), \"abc\")\n        self.assertEqual(replace_metacharacters(r\"\\\\A\\\\b\"), \"b\")\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)$'\n        expected_output = r'^<a>/b/<c>$'\n        self.assertEqual(replace_named_groups(pattern), expected_output)\n", "    def test_replace_metacharacters(self):\n        self.assertEqual(replace_metacharacters(r\"abc?def*ghi+\"), \"abcdefghi\")\n        self.assertEqual(replace_metacharacters(r\"abc\\?def\\*ghi\\+\"), r\"abc\\?def\\*ghi\\+\")\n        self.assertEqual(replace_metacharacters(r\"abc\\\\?def\"), r\"abc\\\\def\")\n        self.assertEqual(replace_metacharacters(r\"abc^def$ghi\"), \"abcdefghi\")\n        self.assertEqual(replace_metacharacters(r\"abc\\\\^def\\\\$ghi\"), r\"abc\\\\def\\\\ghi\")\n"], "sample_453": ["    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js should correctly update the context with prepopulated fields JSON.\n        \"\"\"\n        class MockField:\n                self.name = name\n                self.auto_id = auto_id\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        field1 = {\"field\": MockField(\"title\", \"id_title\"), \"dependencies\": [MockField(\"slug\", \"id_slug\")]}\n        field2 = {\"field\": MockField(\"description\", \"id_description\", 100), \"dependencies\": []}\n        \n        context = {\n            \"adminform\": MockAdminForm([field1, field2]),\n            \"inline_admin_formsets\": [],\n        }\n        \n        updated_context = prepopulated_fields_js(context)\n        prepopulated_fields_json = json.loads(updated_context[\"prepopulated_fields_json\"])\n\n        self.assertEqual(len(prepopulated_fields_json), 2)\n        self.assertEqual(prepopulated_fields_json[0][\"id\"], \"#id_title\")\n        self.assertEqual(prepopulated_fields_json[0][\"name\"], \"title\")\n        self.assertEqual(prepopulated_fields_json[0][\"dependency_ids\"], [\"#id_slug\"])\n        self.assertEqual(prepopulated_fields_json[0][\"dependency_list\"], [\"slug\"])\n        self.assertEqual(prepopulated_fields_json[0][\"maxLength\"], 50)\n        self.assertEqual(prepopulated_fields_json[0][\"allowUnicode\"], False)\n        \n        self.assertEqual(prepopulated_fields_json[1][\"id\"], \"#id_description\")\n        self.assertEqual(prepopulated_fields_json[1][\"name\"], \"description\")\n        self.assertEqual(prepopulated_fields_json[1][\"dependency_ids\"], [])\n        self.assertEqual(prepopulated_fields_json[1][\"dependency_list\"], [])\n        self.assertEqual(prepopulated_fields_json[1][\"maxLength\"], 100)\n        self.assertEqual(prepopulated_fields_json[1][\"allowUnicode\"], False)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js function should correctly process context and return updated context.\n        \"\"\"\n        class MockField:\n                self.name = name\n                self.auto_id = auto_id\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        field1 = MockField(name=\"name1\", auto_id=\"id_name1\")\n        field2 = MockField(name=\"name2\", auto_id=\"id_name2\", allow_unicode=True)\n        prepopulated_fields = [\n            {\"field\": field1, \"dependencies\": [field2]},\n            {\"field\": field2, \"dependencies\": [field1]},\n        ]\n        \n        context = {\n            \"adminform\": MockAdminForm(prepopulated_fields),\n            \"inline_admin_formsets\": []\n        }\n\n        updated_context = prepopulated_fields_js(context)\n        expected_json = json.dumps([\n            {\n                \"id\": \"#id_name1\",\n                \"name\": \"name1\",\n                \"dependency_ids\": [\"#id_name2\"],\n                \"dependency_list\": [\"name2\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": False,\n            },\n            {\n                \"id\": \"#id_name2\",\n                \"name\": \"name2\",\n                \"dependency_ids\": [\"#id_name1\"],\n                \"dependency_list\": [\"name1\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": True,\n            },\n        ])\n        \n        self.assertIn(\"prepopulated_fields\", updated_context)\n        self.assertIn(\"prepopulated_fields_json\", updated_context)\n        self.assertEqual(updated_context[\"prepopulated_fields\"], prepopulated_fields)\n        self.assertEqual(updated_context[\"prepopulated_fields_json\"], expected_json)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js should correctly serialize prepopulated fields into JSON.\n        \"\"\"\n        class MockField:\n                self.name = name\n                self.auto_id = auto_id\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        class MockInlineAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n                self.original = original\n\n        context = {\n            \"adminform\": MockAdminForm([\n                {\"field\": MockField(\"name\", \"id_name\", 50, True), \"dependencies\": [MockField(\"dep1\", \"id_dep1\"), MockField(\"dep2\", \"id_dep2\")]}\n            ]),\n            \"inline_admin_formsets\": [\n                [\n                    MockInlineAdminForm(\n                        [\n                            {\"field\": MockField(\"inline_name\", \"id_inline_name\", 50, False), \"dependencies\": [MockField(\"inline_dep1\", \"id_inline_dep1\")]}\n                        ]\n                    )\n                ]\n            ],\n        }\n\n        result_context = prepopulated_fields_js(context)\n        self.assertIn(\"prepopulated_fields_json\", result_context)\n        prepopulated_fields_json = json.loads(result_context[\"prepopulated_fields_json\"])\n        expected_json = [\n            {\n                \"id\": \"#id_name\",\n                \"name\": \"name\",\n                \"dependency_ids\": [\"#id_dep1\", \"#id_dep2\"],\n                \"dependency_list\": [\"dep1\", \"dep2\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": True,\n            },\n            {\n                \"id\": \"#id_inline_name\",\n                \"name\": \"inline_name\",\n                \"dependency_ids\": [\"#id_inline_dep1\"],\n                \"dependency_list\": [\"inline_dep1\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": False,\n            }\n        ]\n        self.assertEqual(prepopulated_fields_json, expected_json)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        Test prepopulated_fields_js template tag context generation.\n        \"\"\"\n        class MockField:\n                self.auto_id = auto_id\n                self.name = name\n                self.field = self\n                self.max_length = max_length\n                self.allow_unicode = allow_unicode\n\n        class MockAdminForm:\n                self.prepopulated_fields = prepopulated_fields\n\n        field1 = MockField(auto_id=\"id_field1\", name=\"field1\", max_length=100)\n        field2 = MockField(auto_id=\"id_field2\", name=\"field2\")\n        dependency1 = MockField(auto_id=\"id_dependency1\", name=\"dependency1\")\n        dependency2 = MockField(auto_id=\"id_dependency2\", name=\"dependency2\", allow_unicode=True)\n        \n        prepopulated_fields = [\n            {\"field\": field1, \"dependencies\": [dependency1, dependency2]},\n            {\"field\": field2, \"dependencies\": [dependency2]},\n        ]\n        \n        adminform = MockAdminForm(prepopulated_fields)\n        context = {\"adminform\": adminform}\n        \n        updated_context = prepopulated_fields_js(context)\n        \n        expected_json = json.dumps([\n            {\n                \"id\": \"#id_field1\",\n                \"name\": \"field1\",\n                \"dependency_ids\": [\"#id_dependency1\", \"#id_dependency2\"],\n                \"dependency_list\": [\"dependency1\", \"dependency2\"],\n                \"maxLength\": 100,\n                \"allowUnicode\": False,\n            },\n            {\n                \"id\": \"#id_field2\",\n                \"name\": \"field2\",\n                \"dependency_ids\": [\"#id_dependency2\"],\n                \"dependency_list\": [\"dependency2\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": False,\n            },\n        ])\n        \n        self.assertEqual(updated_context[\"prepopulated_fields\"], prepopulated_fields)\n        self.assertEqual(updated_context[\"prepopulated_fields_json\"], expected_json)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n"], "sample_452": ["    def test_create_model_with_existing_table(self):\n        \"\"\"\n        Tests the CreateModel operation when the table already exists.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_crmoextab\")\n        # Create the table manually\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                CREATE TABLE test_crmoextab_existingtable (\n                    id SERIAL PRIMARY KEY,\n                    data VARCHAR(100)\n                )\n                \"\"\"\n            )\n        operation = migrations.CreateModel(\n            \"ExistingTable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"data\", models.CharField(max_length=100)),\n            ],\n        )\n        self.assertEqual(operation.describe(), \"Create model ExistingTable\")\n        self.assertEqual(operation.migration_name_fragment, \"existingtable\")\n        # Test the state alteration\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_crmoextab\", new_state)\n        self.assertEqual(new_state.models[\"test_crmoextab\", \"existingtable\"].name, \"ExistingTable\")\n        self.assertEqual(len(new_state.models[\"test_crmoextab\", \"existingtable\"].fields), 2)\n        # Test the database alteration\n        self.assertTableExists(\"test_crmoextab_existingtable\")\n        with connection.schema_editor() as editor:\n            with self.assertRaises(IntegrityError):\n                operation.database_forwards(\"test_crmoextab\", editor, project_state, new_state)\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_crmoextab\", editor, new_state, project_state)\n        self.assertTableExists(\"test_crmoextab_existingtable\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"CreateModel\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(sorted(definition[2]), [\"fields\", \"name\"])\n", "def test_create_model_with_duplicate_option(self):\n    message = \"Found duplicate value key in CreateModel options argument.\"\n    with self.assertRaisesMessage(ValueError, message):\n        migrations.CreateModel(\n            \"Pony\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n            options={\"key\": \"value\", \"key\": \"another_value\"},\n        )\n    message = \"Found duplicate value unicode in CreateModel options argument.\"\n    with self.assertRaisesMessage(ValueError, message):\n        migrations.CreateModel(\n            \"Pony\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n            options={\"unicode\": \"value\", \"unicode\": \"another_value\"},\n        )\n", "    def test_rename_index_with_fields(self):\n        app_label = \"test_rinwf\"\n        project_state = self.set_up_test_model(app_label)\n        # Create the operation\n        create_operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n                (\"weight\", models.FloatField()),\n            ],\n        )\n        index_operation = migrations.AddIndex(\n            \"Pony\", models.Index(fields=[\"pink\"], name=\"pony_pink_idx\")\n        )\n        rename_operation = migrations.RenameIndex(\n            \"Pony\", new_name=\"new_pony_pink_idx\", old_fields=(\"pink\",)\n        )\n        remove_operation = migrations.RemoveIndex(\"Pony\", \"new_pony_pink_idx\")\n        # Apply initial operations\n        new_state = project_state.clone()\n        create_operation.state_forwards(app_label, new_state)\n        with connection.schema_editor() as editor:\n            create_operation.database_forwards(app_label, editor, project_state, new_state)\n        project_state = new_state.clone()\n        index_operation.state_forwards(app_label, new_state)\n        with connection.schema_editor() as editor:\n            index_operation.database_forwards(app_label, editor, project_state, new_state)\n        # Test the state alteration\n        rename_operation.state_forwards(app_label, new_state)\n        self.assertEqual(new_state.models[app_label, \"pony\"].options[\"indexes\"][0].name, \"new_pony_pink_idx\")\n        # Test the database alteration\n        with connection.schema_editor() as editor:\n            rename_operation.database_forwards(app_label, editor, project_state, new_state)\n        self.assertIndexNameExists(f\"{app_label}_pony\", \"new_pony_pink_idx\")\n        self.assertIndexNameNotExists(f\"{app_label}_pony\", \"pony_pink_idx\")\n        # Reversal\n        with connection.schema_editor() as editor:\n            rename_operation.database_backwards(app_label, editor, new_state, project_state)\n        self.assertIndexNameExists(f\"{app_label}_pony\", \"pony_pink_idx\")\n        self.assertIndexNameNotExists(f\"{app_label}_pony\", \"new_pony_pink_idx\")\n        # Clean up\n        new_state = project_state.clone()\n        remove_operation.state_forwards(app_label, new_state)\n        with connection.schema_editor() as editor:\n            remove_operation.database_forwards(app_label, editor, project_state, new_state", "    def test_alter_model_table_with_custom_db_table(self):\n        \"\"\"\n        Tests the AlterModelTable operation with a custom database table name.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_alterdbtable\")\n        # Test the state alteration\n        operation = migrations.AlterModelTable(\"Pony\", \"custom_pony_table\")\n        self.assertEqual(\n            operation.describe(), \"Rename table for Pony to custom_pony_table\"\n        )\n        self.assertEqual(operation.migration_name_fragment, \"alter_pony_table\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_alterdbtable\", new_state)\n        self.assertEqual(\n            new_state.models[\"test_alterdbtable\", \"pony\"].options[\"db_table\"],\n            \"custom_pony_table\",\n        )\n        # Test the database alteration\n        self.assertTableExists(\"test_alterdbtable_pony\")\n        self.assertTableNotExists(\"custom_pony_table\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_alterdbtable\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_alterdbtable_pony\")\n        self.assertTableExists(\"custom_pony_table\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_alterdbtable\", editor, new_state, project_state)\n        self.assertTableExists(\"test_alterdbtable_pony\")\n        self.assertTableNotExists(\"custom_pony_table\")\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AlterModelTable\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(definition[2], {\"name\": \"Pony\", \"table\": \"custom_pony_table\"})\n", "    def test_rename_model_with_db_table(self):\n        \"\"\"\n        Tests the RenameModel operation where the model has a custom db_table.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_rnmoddbt\", related_model=True)\n        # Test the state alteration\n        operation = migrations.RenameModel(\"Pony\", \"Horse\")\n        self.assertEqual(operation.describe(), \"Rename model Pony to Horse\")\n        self.assertEqual(operation.migration_name_fragment, \"rename_pony_horse\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_rnmoddbt\", new_state)\n        self.assertNotIn((\"test_rnmoddbt\", \"pony\"), new_state.models)\n        self.assertIn((\"test_rnmoddbt\", \"horse\"), new_state.models)\n        self.assertEqual(\n            new_state.models[\"test_rnmoddbt\", \"horse\"].options[\"db_table\"],\n            \"custom_pony_table\",\n        )\n        # Test the database alteration\n        self.assertTableExists(\"custom_pony_table\")\n        self.assertTableNotExists(\"test_rnmoddbt_horse\")\n        if connection.features.supports_foreign_keys:\n            self.assertFKExists(\n                \"test_rnmoddbt_rider\", [\"pony_id\"], (\"custom_pony_table\", \"id\")\n            )\n            self.assertFKNotExists(\n                \"test_rnmoddbt_rider\", [\"pony_id\"], (\"test_rnmoddbt_horse\", \"id\")\n            )\n        with connection.schema_editor(\n            atomic=connection.features.supports_atomic_references_rename\n        ) as editor:\n            operation.database_forwards(\"test_rnmoddbt\", editor, project_state, new_state)\n        self.assertTableExists(\"custom_pony_table\")\n        self.assertTableNotExists(\"test_rnmoddbt_pony\")\n        if connection.features.supports_foreign_keys:\n            self.assertFKExists(\n                \"test_rnmoddbt_rider\", [\"pony_id\"], (\"custom_pony_table\", \"id\")\n            )\n            self.assertFKNotExists(\n                \"test_rnmoddbt_rider\", [\"pony_id\"], (\"test_rnmoddbt_horse\", \"id\")\n            )\n        # And test reversal\n        with connection.schema_editor(\n            atomic=connection.features.supports_atomic_references_rename\n        ) as editor:\n            operation.database_backwards(\n                \"test_rnmoddbt\","], "sample_454": ["    def test_eq(self):\n        constraint1 = ExclusionConstraint(\n            name=\"exclude_test\",\n            expressions=[(\"field1\", \"=\"), (\"field2\", \"<>\")],\n            index_type=\"GIST\",\n        )\n        constraint2 = ExclusionConstraint(\n            name=\"exclude_test\",\n            expressions=[(\"field1\", \"=\"), (\"field2\", \"<>\")],\n            index_type=\"GIST\",\n        )\n        constraint3 = ExclusionConstraint(\n            name=\"exclude_test_diff\",\n            expressions=[(\"field1\", \"=\"), (\"field2\", \"<>\")],\n            index_type=\"GIST\",\n        )\n        self.assertEqual(constraint1, constraint2)\n        self.assertNotEqual(constraint1, constraint3)\n", "    def test_init_with_invalid_index_type(self):\n        msg = \"Exclusion constraints only support GiST or SP-GiST indexes.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            ExclusionConstraint(\n                name=\"exclude_invalid_index\",\n                expressions=[(\"field\", \"=\")],\n                index_type=\"btree\",\n            )\n", "    def test_eq(self):\n        constraint1 = ExclusionConstraint(\n            name=\"exclude_test\",\n            expressions=[(\"field1\", \"=\"), (\"field2\", \"&&\")],\n        )\n        constraint2 = ExclusionConstraint(\n            name=\"exclude_test\",\n            expressions=[(\"field1\", \"=\"), (\"field2\", \"&&\")],\n        )\n        self.assertEqual(constraint1, constraint2)\n        self.assertNotEqual(constraint1, mock.ANY)\n        constraint3 = ExclusionConstraint(\n            name=\"exclude_test2\",\n            expressions=[(\"field1\", \"=\"), (\"field2\", \"&&\")],\n        )\n        self.assertNotEqual(constraint1, constraint3)\n", "    def test_eq(self):\n        expressions1 = [(F(\"field1\"), \"=\"), (F(\"field2\"), \"=\")]\n        expressions2 = [(F(\"field3\"), \"=\"), (F(\"field4\"), \"=\")]\n        self.assertEqual(\n            ExclusionConstraint(name=\"exclusion\", expressions=expressions1),\n            ExclusionConstraint(name=\"exclusion\", expressions=expressions1),\n        )\n        self.assertEqual(\n            ExclusionConstraint(name=\"exclusion\", expressions=expressions1),\n            mock.ANY,\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(name=\"exclusion\", expressions=expressions1),\n            ExclusionConstraint(name=\"exclusion2\", expressions=expressions1),\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(name=\"exclusion\", expressions=expressions1),\n            ExclusionConstraint(name=\"exclusion\", expressions=expressions2),\n        )\n        self.assertNotEqual(\n            ExclusionConstraint(name=\"exclusion\", expressions=expressions1),\n            1,\n        )\n", "    def test_eq(self):\n        expr1 = (F(\"field1\"), \"=\")\n        expr2 = (F(\"field2\"), \"&&\")\n        expr3 = (F(\"field3\"), \"<>\")\n        constraint1 = ExclusionConstraint(name=\"exclude1\", expressions=[expr1, expr2])\n        constraint2 = ExclusionConstraint(name=\"exclude1\", expressions=[expr1, expr2])\n        constraint3 = ExclusionConstraint(name=\"exclude2\", expressions=[expr1, expr3])\n\n        self.assertEqual(constraint1, constraint2)\n        self.assertNotEqual(constraint1, constraint3)\n        self.assertEqual(constraint1, mock.ANY)\n        self.assertNotEqual(constraint1, 1)\n"], "sample_455": ["def test_validate_with_deferrable_expressions(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        name=\"unique_name_lower\",\n        deferrable=models.Deferrable.IMMEDIATE,\n    )\n    msg = \"UniqueConstraint with expressions cannot be deferred.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(\n            Lower(\"name\"),\n            name=\"unique_name_lower\",\n            deferrable=models.Deferrable.DEFERRED,\n        )\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"unique_name\"),\n    )\n    # Ensure the existing row is excluded in validation.\n    self.p1.name = \"unique_name\"\n    constraint.validate(UniqueConstraintProduct, self.p1)\n", "    def test_constraint_sql(self):\n        constraint = models.UniqueConstraint(fields=[\"foo\"], name=\"unique_foo\")\n        with mock.patch('django.db.backends.base.schema.BaseDatabaseSchemaEditor._unique_sql') as mock_unique_sql:\n            mock_unique_sql.return_value = \"SQL\"\n            model_mock = mock.Mock()\n            schema_editor_mock = mock.Mock()\n            self.assertEqual(constraint.constraint_sql(model_mock, schema_editor_mock), \"SQL\")\n            mock_unique_sql.assert_called_once()\n", "def test_unique_constraint_clone(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique_fields\",\n        condition=models.Q(foo=models.F(\"bar\")),\n        deferrable=models.Deferrable.IMMEDIATE,\n        include=[\"baz_1\", \"baz_2\"],\n        opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n    ).clone()\n    self.assertEqual(\n        repr(constraint),\n        \"<UniqueConstraint: fields=('foo', 'bar') name='unique_fields' \"\n        \"condition=(AND: ('foo', F(bar))) \"\n        \"deferrable=Deferrable.IMMEDIATE \"\n        \"include=('baz_1', 'baz_2') \"\n        \"opclasses=['text_pattern_ops', 'varchar_pattern_ops']>\"\n    )\n", "    def test_clone(self):\n        fields = [\"foo\", \"bar\"]\n        name = \"unique_fields\"\n        constraint = models.UniqueConstraint(fields=fields, name=name)\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint, cloned_constraint)\n        self.assertIsNot(constraint, cloned_constraint)\n        self.assertEqual(constraint.name, cloned_constraint.name)\n        self.assertEqual(constraint.fields, cloned_constraint.fields)\n        self.assertEqual(constraint.condition, cloned_constraint.condition)\n        self.assertEqual(constraint.deferrable, cloned_constraint.deferrable)\n        self.assertEqual(constraint.include, cloned_constraint.include)\n        self.assertEqual(constraint.opclasses, cloned_constraint.opclasses)\n        self.assertEqual(constraint.expressions, cloned_constraint.expressions)\n        self.assertEqual(constraint.violation_error_code, cloned_constraint.violation_error_code)\n        self.assertEqual(constraint.violation_error_message, cloned_constraint.violation_error_message)\n", "    def test_unique_constraint_violation_error_code_clone(self):\n        constraint = UniqueConstraint(\n            fields=[\"name\", \"color\"],\n            name=\"unique_name_color\",\n            violation_error_code=\"unique_error_code\",\n        )\n        cloned_constraint = constraint.clone()\n        self.assertEqual(cloned_constraint.violation_error_code, \"unique_error_code\")\n"], "sample_456": ["def test_formset_management_form_validation(self):\n    \"\"\"\n    Ensure that the management form validation works correctly and raises\n    appropriate errors when the management form is tampered with.\n    \"\"\"\n    # Missing TOTAL_FORMS and INITIAL_FORMS\n    data = {\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-MAX_NUM_FORMS\": \"1000\",\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\n            \"ManagementForm data is missing or has been tampered with. \"\n            \"Missing fields: choices-TOTAL_FORMS, choices-INITIAL_FORMS. \"\n            \"You may need to file a bug report if the issue persists.\",\n        ],\n    )\n    \n    # Invalid TOTAL_FORMS and INITIAL_FORMS values\n    data = {\n        \"choices-TOTAL_FORMS\": \"invalid\",\n        \"choices-INITIAL_FORMS\": \"invalid\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-MAX_NUM_FORMS\": \"1000\",\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\n            \"ManagementForm data is missing or has been tampered with. \"\n            \"Missing fields: choices-TOTAL_FORMS, choices-INITIAL_FORMS. \"\n            \"You may need to file a bug report if the issue persists.\",\n        ],\n    )\n", "    def test_formset_with_default_error_messages(self):\n        class CustomForm(Form):\n            name = CharField()\n\n        class CustomFormSet(BaseFormSet):\n                if any(form.cleaned_data.get(\"name\") == \"Invalid\" for form in self.forms):\n                    raise ValidationError(\"This is a non-form error\", code=\"custom_error\")\n\n        data = {\n            \"form-TOTAL_FORMS\": \"2\",\n            \"form-INITIAL_FORMS\": \"0\",\n            \"form-0-name\": \"Valid\",\n            \"form-1-name\": \"Invalid\",\n        }\n        CustomFormSetFactory = formset_factory(CustomForm, formset=CustomFormSet)\n        formset = CustomFormSetFactory(data, auto_id=False, prefix=\"form\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.non_form_errors(),\n            [\"This is a non-form error\"],\n        )\n        self.assertEqual(\n            str(formset.non_form_errors()),\n            '<ul class=\"errorlist nonform\"><li>This is a non-form error</li></ul>',\n        )\n", "def test_management_form_clean_method(self):\n        \"\"\"\n        Test that ManagementForm.clean() properly sets default values for\n        TOTAL_FORM_COUNT and INITIAL_FORM_COUNT.\n        \"\"\"\n        class CustomForm(Form):\n            field = CharField()\n\n        CustomFormSet = formset_factory(CustomForm)\n        data = {\n            \"form-TOTAL_FORMS\": \"5\",\n            \"form-INITIAL_FORMS\": \"2\",\n            \"form-MIN_NUM_FORMS\": \"1\",\n            \"form-MAX_NUM_FORMS\": \"10\",\n        }\n        formset = CustomFormSet(data)\n        management_form = formset.management_form\n        management_form.full_clean()\n        self.assertEqual(management_form.cleaned_data[TOTAL_FORM_COUNT], 5)\n        self.assertEqual(management_form.cleaned_data[INITIAL_FORM_COUNT], 2)\n", "    def test_formset_with_custom_error_message(self):\n        \"\"\"\n        Test formset with a custom error message for `too_few_forms` using `validate_min` flag.\n        \"\"\"\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"3\",\n            \"choices-MAX_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1, min_num=3, validate_min=True)\n        formset = ChoiceFormSet(\n            data,\n            auto_id=False,\n            prefix=\"choices\",\n            error_messages={\n                \"too_few_forms\": \"You must submit at least %(num)d forms.\"\n            },\n        )\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.non_form_errors(),\n            [\"You must submit at least 3 forms.\"],\n        )\n        self.assertEqual(\n            str(formset.non_form_errors()),\n            '<ul class=\"errorlist nonform\">'\n            \"<li>You must submit at least 3 forms.</li></ul>\",\n        )\n", "def test_formset_custom_clean_raises_validation_error(self):\n    \"\"\"\n    Ensure that a custom formset's clean method can raise a ValidationError and\n    that the error is properly captured in non_form_errors().\n    \"\"\"\n    class CustomCleanFormSet(BaseFormSet):\n            raise ValidationError(\"Custom validation error\")\n\n    CustomFormSet = formset_factory(FavoriteDrinkForm, formset=CustomCleanFormSet)\n    data = {\n        \"form-TOTAL_FORMS\": \"1\",\n        \"form-INITIAL_FORMS\": \"0\",\n        \"form-0-name\": \"Test Drink\",\n    }\n    formset = CustomFormSet(data)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [\"Custom validation error\"])\n"], "sample_457": ["    def test_validate_with_multiple_expressions(self):\n        constraint = models.UniqueConstraint(\n            Lower(\"name\"),\n            F(\"color\"),\n            name=\"name_color_lower_uniq\",\n        )\n        msg = \"Constraint \u201cname_color_lower_uniq\u201d is violated.\"\n        non_unique_product = UniqueConstraintProduct(name=self.p1.name, color=self.p1.color)\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, non_unique_product)\n        # Valid product with different color.\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name, color=\"blue\"),\n        )\n        # Existing instances have their existing row excluded.\n        constraint.validate(UniqueConstraintProduct, self.p1)\n        # Unique fields are excluded.\n        constraint.validate(\n            UniqueConstraintProduct,\n            non_unique_product,\n            exclude={\"name\"},\n        )\n        constraint.validate(\n            UniqueConstraintProduct,\n            non_unique_product,\n            exclude={\"color\"},\n        )\n        constraint.validate(\n            UniqueConstraintProduct,\n            non_unique_product,\n            exclude={\"name\", \"color\"},\n        )\n", "    def test_unique_constraint_with_empty_opclasses(self):\n        msg = \"UniqueConstraint.opclasses must be a list or tuple.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=[\"name\"],\n                name=\"name_opclasses_invalid\",\n                opclasses=\"\",\n            )\n", "    def test_clone_with_all_arguments(self):\n        constraint = models.UniqueConstraint(\n            Lower(\"name\"),\n            F(\"author\"),\n            name=\"book_func_uq\",\n            condition=models.Q(published=True),\n            deferrable=models.Deferrable.DEFERRED,\n            include=[\"genre\"],\n            opclasses=[\"text_pattern_ops\"]\n        ).clone()\n        self.assertEqual(\n            repr(constraint),\n            \"<UniqueConstraint: expressions=(Lower(F(name)), F(author)) \"\n            \"name='book_func_uq' condition=(AND: ('published', True)) \"\n            \"deferrable=Deferrable.DEFERRED include=('genre',) \"\n            \"opclasses=['text_pattern_ops']>\"\n        )\n", "    def test_default_violation_error_message(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"default_violation\",\n        )\n        self.assertEqual(\n            constraint.get_violation_error_message(),\n            \"Constraint \u201cdefault_violation\u201d is violated.\",\n        )\n", "    def test_deferrable_repr(self):\n        constraint = models.Deferrable.DEFERRED\n        self.assertEqual(repr(constraint), \"Deferrable.DEFERRED\")\n        constraint = models.Deferrable.IMMEDIATE\n        self.assertEqual(repr(constraint), \"Deferrable.IMMEDIATE\")\n"], "sample_458": ["    def test_addslashes(self):\n        self.assertEqual(addslashes('He said, \"Hello!\"'), 'He said, \\\\\"Hello!\\\\\"')\n        self.assertEqual(addslashes(\"It's a test\"), \"It\\\\'s a test\")\n        self.assertEqual(addslashes(\"Backslash \\\\ test\"), \"Backslash \\\\\\\\ test\")\n    ", "    def test_addslashes(self):\n        self.assertEqual(addslashes('He said \"hello\"'), 'He said \\\\\"hello\\\\\"')\n        self.assertEqual(addslashes(\"It's a test\"), \"It\\\\'s a test\")\n        self.assertEqual(addslashes(r\"Backslash\\\\\"), r\"Backslash\\\\\\\\\")\n", "    def test_floatformat_invalid_input(self):\n        self.assertEqual(floatformat(\"invalid\"), \"\")\n        self.assertEqual(floatformat(\"invalid\", 2), \"\")\n        self.assertEqual(floatformat(\"invalid\", \"g\"), \"\")\n        self.assertEqual(floatformat(\"invalid\", \"2u\"), \"\")\n        self.assertEqual(floatformat(\"invalid\", \"-3\"), \"\")\n", "    def test_force_grouping_with_mixed_types(self):\n        output = self.engine.render_to_string(\n            \"floatformat03\", {\"a\": 12345.6789, \"b\": mark_safe(\"54321.9876\"), \"arg\": \"2g\"}\n        )\n        self.assertEqual(output, \"12,345.68 54,321.99\")\n", "    def test_floatformat_large_values(self):\n        self.assertEqual(floatformat(1e10, -2), \"10000000000\")\n        self.assertEqual(floatformat(1e10, 2), \"10000000000.00\")\n        self.assertEqual(floatformat(-1e10, -2), \"-10000000000\")\n        self.assertEqual(floatformat(-1e10, 2), \"-10000000000.00\")\n        self.assertEqual(floatformat(Decimal(\"1e10\"), -2), \"10000000000\")\n        self.assertEqual(floatformat(Decimal(\"1e10\"), 2), \"10000000000.00\")\n"], "sample_459": ["    def test_uuid_exact_lookup(self):\n        instance = self.UUIDModel.objects.create(uuid=\"12345678-1234-5678-1234-567812345678\")\n        self.assertTrue(\n            self.UUIDModel.objects.filter(uuid__exact=\"12345678123456781234567812345678\").exists()\n        )\n        self.assertFalse(\n            self.UUIDModel.objects.filter(uuid__exact=\"87654321-4321-8765-4321-876543218765\").exists()\n        )\n", "    def test_uuidiexact(self):\n        field = models.UUIDField()\n        lookup = UUIDIExact(field.get_transform(\"uuidiexact\"), \"123e4567-e89b-12d3-a456-426614174000\")\n        sql, params = lookup.as_sql(MockCompiler(), connection)\n        self.assertIn(\"REPLACE\", sql)\n        self.assertEqual(params, [\"123e4567e89b12d3a456426614174000\"])\n", "    def test_uuidfield_contains(self):\n        instance = self.UUIDModel.objects.create(uuid=\"12345678123456781234567812345678\")\n        self.assertTrue(self.UUIDModel.objects.filter(uuid__contains=\"1234\").exists())\n        self.assertFalse(self.UUIDModel.objects.filter(uuid__contains=\"5678-\").exists())\n", "    def test_uuid_iexact_lookup(self):\n        from django.db.models import Q\n        from uuid import uuid4\n\n        class UUIDModel(models.Model):\n            uuid = models.UUIDField()\n\n        uuid_value = uuid4()\n        uuid_model = UUIDModel.objects.create(uuid=uuid_value)\n\n        self.assertEqual(\n            UUIDModel.objects.filter(uuid__iexact=uuid_value.hex).count(), 1\n        )\n        self.assertEqual(\n            UUIDModel.objects.filter(uuid__iexact=str(uuid_value)).count(), 1\n        )\n", "    def setUp(self):\n        self.model = models.UUIDField()\n"], "sample_460": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def test_is_registered(self):\n        \"\"\"\n        Ensure that is_registered function works as expected.\n        \"\"\"\n        model = Article\n        self.assertIs(site.is_registered(model), True)\n        site.unregister(model)\n        self.assertIs(site.is_registered(model), False)\n        site.register(model)\n        self.assertIs(site.is_registered(model), True)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUp(self):\n        self.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        self.client.force_login(self.superuser)\n        self.admin_site = AdminSite()\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.s1 = Section.objects.create(name=\"Test section\")\n        cls.a1 = Article.objects.create(\n            content=\"<p>Middle content</p>\",\n            date=datetime.datetime(2008, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.a2 = Article.objects.create(\n            content=\"<p>Oldest content</p>\",\n            date=datetime.datetime(2000, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n"], "sample_461": ["    def test_custom_validator(self):\n            if not value.startswith(\"https\"):\n                raise ValidationError(\"URL must start with 'https'.\")\n\n        f = URLField(validators=[custom_validator])\n        self.assertEqual(f.clean(\"https://secure.com\"), \"https://secure.com\")\n        \n        msg = \"'URL must start with 'https'.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"http://insecure.com\")\n", "    def test_urlfield_invalid_with_spaces(self):\n        f = URLField()\n        tests = [\n            \" http://example.com\",\n            \"http://example.com \",\n            \"http:// example .com\",\n            \"http://example. com\",\n        ]\n        msg = \"'Enter a valid URL.'\"\n        for value in tests:\n            with self.subTest(value=value):\n                with self.assertRaisesMessage(ValidationError, msg):\n                    f.clean(value)\n", "    def test_urlfield_invalid_schemes(self):\n        f = URLField()\n        invalid_scheme_urls = [\n            \"ftp://example.com\",\n            \"file://localhost/path\",\n            \"mailto:user@example.com\",\n            \"data:text/plain;base64,SGVsbG8sIFdvcmxkIQ==\",\n            \"ssh://example.com\",\n            \"chrome://settings\",\n            \"custom-scheme://example.com\",\n        ]\n        msg = \"'Enter a valid URL.'\"\n        for value in invalid_scheme_urls:\n            with self.subTest(value=value):\n                with self.assertRaisesMessage(ValidationError, msg):\n                    f.clean(value)\n", "    def test_urlfield_custom_error_messages(self):\n        error_messages = {\n            'invalid': 'Custom invalid message.',\n            'required': 'Custom required message.',\n        }\n        f = URLField(error_messages=error_messages)\n        with self.assertRaisesMessage(ValidationError, \"'Custom required message.'\"):\n            f.clean(\"\")\n        with self.assertRaisesMessage(ValidationError, \"'Custom invalid message.'\"):\n            f.clean(\"invalid-url\")\n", "    def test_integerfield_widget(self):\n        f = IntegerField()\n        self.assertWidgetRendersTo(f, '<input type=\"number\" name=\"f\" id=\"id_f\" required>')\n"], "sample_462": ["def test_choicefield_dynamic_choices_update(self):\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")])\n    f.choices = [(\"3\", \"Three\"), (\"4\", \"Four\")]\n    self.assertEqual(f.clean(\"3\"), \"3\")\n    msg = \"'Select a valid choice. 1 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"1\")\n", "def test_choicefield_invalid_choice(self):\n    f = ChoiceField(choices=[(\"A\", \"Apple\"), (\"B\", \"Banana\")])\n    msg = \"'Select a valid choice. C is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"C\")\n    msg = \"'Select a valid choice. D is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"D\")\n", "    def test_choicefield_with_empty_values(self):\n        empty_values = [None, \"\", [], (), {}]\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], required=False)\n        for empty_value in empty_values:\n            self.assertEqual(\"\", f.clean(empty_value))\n", "    def test_choicefield_invalid_type(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")])\n        msg = \"'Select a valid choice. [1, 2] is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean([1, 2])\n", "    def test_choicefield_invalid_coerce(self):\n            if value == \"invalid\":\n                raise ValueError(\"Invalid value\")\n            return value\n\n        f = TypedChoiceField(choices=[(\"J\", \"John\"), (\"P\", \"Paul\")], coerce=coerce_func)\n        self.assertEqual(\"J\", f.clean(\"J\"))\n        msg = \"'Select a valid choice. invalid is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"invalid\")\n"], "sample_463": ["def test_rename_index_together_to_index_together(self):\n    \"\"\"Test renaming an index_together to another index_together.\"\"\"\n    book_renamed_index_together = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"index_together\": {(\"author\", \"title_renamed\")},\n        },\n    )\n    changes = self.get_changes(\n        [AutodetectorTests.author_empty, self.book_index_together],\n        [AutodetectorTests.author_empty, book_renamed_index_together],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterIndexTogether\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book\",\n        index_together={(\"author\", \"title_renamed\")},\n    )\n", "    def test_deep_deconstruct_with_compiled_regex(self):\n        \"\"\"\n        Test deep_deconstruct with a compiled regex object.\n        \"\"\"\n        regex = re.compile(r\"^test$\")\n        before_state = ModelState(\n            \"testapp\",\n            \"TestModel\",\n            [(\"id\", models.AutoField(primary_key=True)), (\"regex_field\", models.CharField(max_length=200, validators=[RegexValidator(regex)]))],\n        )\n        after_state = ModelState(\n            \"testapp\",\n            \"TestModel\",\n            [(\"id\", models.AutoField(primary_key=True)), (\"regex_field\", models.CharField(max_length=200, validators=[RegexValidator(regex)]))],\n        )\n        changes = self.get_changes([before_state], [after_state])\n        self.assertEqual(changes, {})\n", "    def test_deep_deconstruct_functional_partial(self):\n        \"\"\"\n        Test deep_deconstruct for functools.partial objects to ensure\n        that they are correctly serialized and deserialized.\n        \"\"\"\n\n            return x + y\n\n        partial_func = functools.partial(sample_func, 3, y=4)\n        obj = {\n            \"field\": partial_func,\n            \"nested\": {\"field\": functools.partial(sample_func, 5, y=6)},\n        }\n\n        autodetector = MigrationAutodetector(None, None)\n        deconstructed = autodetector.deep_deconstruct(obj)\n        expected_deconstructed = {\n            \"field\": (\n                sample_func,\n                (3,),\n                {\"y\": 4},\n            ),\n            \"nested\": {\n                \"field\": (\n                    sample_func,\n                    (5,),\n                    {\"y\": 6},\n                )\n            },\n        }\n        self.assertEqual(deconstructed, expected_deconstructed)\n", "    def test_deep_deconstruct_with_regex(self):\n        \"\"\"\n        Tests deep deconstruction of a model with a regex field.\n        \"\"\"\n        regex_model_state = ModelState(\n            \"testapp\",\n            \"RegexModel\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"regex_field\",\n                    models.CharField(\n                        max_length=200,\n                        validators=[\n                            RegexValidator(\n                                re.compile(\"^[a-z]+\\\\Z\", re.IGNORECASE),\n                                \"Invalid value.\",\n                                \"invalid\",\n                            )\n                        ],\n                    ),\n                ),\n            ],\n        )\n        changes = self.get_changes([], [regex_model_state])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"RegexModel\")\n", "    def test_deep_deconstruct(self):\n        \"\"\"\n        Tests deep deconstruction of various types of objects.\n        \"\"\"\n        # Create an instance of MigrationAutodetector with dummy states\n        from_state = ProjectState()\n        to_state = ProjectState()\n        autodetector = MigrationAutodetector(from_state, to_state)\n\n        # Test list\n        obj = [1, 2, [3, 4]]\n        deconstructed = autodetector.deep_deconstruct(obj)\n        self.assertEqual(deconstructed, [1, 2, [3, 4]])\n\n        # Test tuple\n        obj = (1, 2, (3, 4))\n        deconstructed = autodetector.deep_deconstruct(obj)\n        self.assertEqual(deconstructed, (1, 2, (3, 4)))\n\n        # Test dict\n        obj = {'a': 1, 'b': {'c': 2, 'd': 3}}\n        deconstructed = autodetector.deep_deconstruct(obj)\n        self.assertEqual(deconstructed, {'a': 1, 'b': {'c': 2, 'd': 3}})\n\n        # Test functools.partial\n            return a + b\n\n        obj = functools.partial(sample_func, 1, b=2)\n        deconstructed = autodetector.deep_deconstruct(obj)\n        self.assertEqual(deconstructed, (sample_func, (1,), {'b': 2}))\n\n        # Test COMPILED_REGEX_TYPE\n        obj = re.compile(r'\\d+')\n        deconstructed = autodetector.deep_deconstruct(obj)\n        self.assertIsInstance(deconstructed, RegexObject)\n        self.assertEqual(deconstructed.pattern, r'\\d+')\n\n        # Test type\n        obj = models.CharField\n        deconstructed = autodetector.deep_deconstruct(obj)\n        self.assertEqual(deconstructed, models.CharField)\n\n        # Test object with deconstruct method\n        obj = models.CharField(max_length=255)\n        deconstructed = autodetector.deep_deconstruct(obj)\n        self.assertEqual(deconstructed[0], 'django.db.models.CharField')\n        self.assertEqual(deconstructed[1], [])\n        self.assertEqual(deconstructed[2], {'max_length': 255})\n"], "sample_464": ["def test_set_headers_without_content_length(self):\n        class NoTellFile:\n                self.content = content\n                self.read_pos = 0\n\n                if size == -1:\n                    size = len(self.content) - self.read_pos\n                start = self.read_pos\n                self.read_pos += size\n                return self.content[start : start + size]\n\n                if whence == io.SEEK_SET:\n                    self.read_pos = offset\n                elif whence == io.SEEK_CUR:\n                    self.read_pos += offset\n                elif whence == io.SEEK_END:\n                    self.read_pos = len(self.content) + offset\n\n            @property\n                return \"not_a_real_file.txt\"\n\n                pass\n\n        file = NoTellFile(b\"example content\")\n        response = FileResponse(file)\n        self.assertFalse(response.has_header(\"Content-Length\"))\n        self.assertEqual(response.headers[\"Content-Type\"], \"application/octet-stream\")\n", "def test_custom_content_type(self):\n    response = FileResponse(\n        io.BytesIO(b\"binary content\"), content_type=\"application/pdf\"\n    )\n    self.assertEqual(response.headers[\"Content-Type\"], \"application/pdf\")\n", "def test_file_response_async_iterator(self):\n    class AsyncBytesIO(io.BytesIO):\n        async def __aiter__(self):\n            for chunk in iter(lambda: self.read(4), b\"\"):\n                yield chunk\n\n    async def stream_data():\n        yield b\"binary \"\n        yield b\"content\"\n\n    test_cases = [\n        (\"AsyncBytesIO\", AsyncBytesIO),\n        (\"async generator\", stream_data)\n    ]\n\n    for class_name, data_source in test_cases:\n        with self.subTest(data_source=class_name):\n            if callable(data_source):\n                response = FileResponse(data_source())\n            else:\n                buffer = data_source(b\"binary content\")\n                response = FileResponse(buffer)\n            self.assertEqual(list(response.streaming_content), [b'binary ', b'content'])\n", "def test_json_response():\n    data = {\"key\": \"value\"}\n    response = JsonResponse(data)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.headers[\"Content-Type\"], \"application/json\")\n    self.assertEqual(response.content, b'{\"key\": \"value\"}')\n    with self.assertRaises(TypeError):\n        JsonResponse([1, 2, 3])\n    response = JsonResponse([1, 2, 3], safe=False)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.headers[\"Content-Type\"], \"application/json\")\n    self.assertEqual(response.content, b'[1, 2, 3]')\n", "def test_default_content_type(self):\n    \"\"\"\n    Test that FileResponse sets the default content type when none is provided.\n    \"\"\"\n    with tempfile.NamedTemporaryFile() as tmp:\n        response = FileResponse(tmp)\n        self.assertEqual(response.headers[\"Content-Type\"], \"application/octet-stream\")\n"], "sample_465": ["def test_formfield_overrides(self):\n    class BandAdmin(ModelAdmin):\n        formfield_overrides = {\n            models.CharField: {\"widget\": forms.Textarea(attrs={\"rows\": 1, \"cols\": 40})},\n            models.DateField: {\"widget\": forms.SelectDateWidget},\n        }\n\n    ma = BandAdmin(Band, self.site)\n    form = ma.get_form(request)()\n\n    # Check CharField widget override\n    self.assertIsInstance(form.fields[\"name\"].widget, forms.Textarea)\n    self.assertEqual(form.fields[\"name\"].widget.attrs, {\"rows\": 1, \"cols\": 40})\n\n    # Check DateField widget override\n    self.assertIsInstance(form.fields[\"sign_date\"].widget, forms.SelectDateWidget)\n\n    # Check that other fields are not affected\n    self.assertIsInstance(form.fields[\"bio\"].widget, forms.Textarea)\n", "    def setUp(self):\n        self.site = AdminSite()\n", "    def test_get_list_display(self):\n        \"\"\"\n        Test that the correct list of fields to be displayed on the changelist\n        is returned by get_list_display.\n        \"\"\"\n        class BandAdmin(ModelAdmin):\n            list_display = [\"name\", \"bio\"]\n\n        ma = BandAdmin(Band, self.site)\n        self.assertEqual(ma.get_list_display(request), [\"name\", \"bio\"])\n", "    def test_formfield_for_dbfield(self):\n        \"\"\"\n        Ensure formfield_for_dbfield returns the correct form field instances\n        based on db_field type and model admin configurations.\n        \"\"\"\n        class TestModelAdmin(ModelAdmin):\n            formfield_overrides = {\n                models.CharField: {\"widget\": forms.Textarea},\n                models.IntegerField: {\"widget\": forms.HiddenInput},\n            }\n\n        ma = TestModelAdmin(Band, self.site)\n\n        # Test for CharField with formfield_overrides.\n        db_field = Band._meta.get_field(\"name\")\n        form_field = ma.formfield_for_dbfield(db_field, request)\n        self.assertIsInstance(form_field.widget, forms.Textarea)\n\n        # Test for IntegerField with formfield_overrides.\n        class TestModel(models.Model):\n            number = models.IntegerField()\n\n        db_field = TestModel._meta.get_field(\"number\")\n        form_field = ma.formfield_for_dbfield(db_field, request)\n        self.assertIsInstance(form_field.widget, forms.HiddenInput)\n\n        # Test for ForeignKey with no formfield_overrides.\n        db_field = Concert._meta.get_field(\"main_band\")\n        form_field = ma.formfield_for_dbfield(db_field, request)\n        self.assertIsInstance(form_field.widget.widget, forms.Select)\n\n        # Test for ManyToManyField with no formfield_overrides.\n        class TestModel(models.Model):\n            bands = models.ManyToManyField(Band)\n\n        db_field = TestModel._meta.get_field(\"bands\")\n        form_field = ma.formfield_for_dbfield(db_field, request)\n        self.assertIsInstance(form_field.widget.widget, forms.SelectMultiple)\n", "    def setUpTestData(cls):\n        cls.band = Band.objects.create(\n            name=\"The Rolling Stones\",\n            bio=\"\",\n            sign_date=date(1962, 1, 1),\n        )\n        cls.concert = Concert.objects.create(\n            main_band=cls.band,\n            opening_band=cls.band,\n            day=1,\n            transport=1,\n        )\n"], "sample_466": ["    def test_migration_with_replaces(self):\n        \"\"\"\n        Tests serializing a migration with 'replaces' attribute.\n        \"\"\"\n        migration = type(\n            \"Migration\",\n            (migrations.Migration,),\n            {\n                \"operations\": [\n                    migrations.CreateModel(\n                        \"ReplacedModel\", \n                        [(\"field\", models.CharField(max_length=255))],\n                        {\"verbose_name\": \"Replaced model\"},\n                        (models.Model,),\n                    ),\n                ],\n                \"dependencies\": [(\"app1\", \"0001_initial\")],\n                \"replaces\": [(\"app1\", \"0002_replaced_migration\")],\n            },\n        )\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        result = self.safe_exec(output)\n        self.assertIn(\"Migration\", result)\n        self.assertIn(\"replaces\", output)\n        self.assertIn(\"0002_replaced_migration\", output)\n", "    def test_operation_writer_with_complex_dict(self):\n        operation = custom_migration_operations.operations.ExpandArgsOperation({\n            'key1': custom_migration_operations.operations.ArgsOperation(1, 2),\n            'key2': custom_migration_operations.operations.KwargsOperation(kwarg1=3),\n            'key3': [custom_migration_operations.operations.ArgsOperation(4, 5), 6]\n        })\n        buff, imports = OperationWriter(operation, indentation=0).serialize()\n        self.assertEqual(imports, {\"import custom_migration_operations.operations\"})\n        self.assertEqual(\n            buff,\n            \"custom_migration_operations.operations.ExpandArgsOperation(\\n\"\n            \"    arg={\\n\"\n            \"        'key1': custom_migration_operations.operations.ArgsOperation(\\n\"\n            \"            arg1=1,\\n\"\n            \"            arg2=2,\\n\"\n            \"        ),\\n\"\n            \"        'key2': custom_migration_operations.operations.KwargsOperation(\\n\"\n            \"            kwarg1=3,\\n\"\n            \"        ),\\n\"\n            \"        'key3': [\\n\"\n            \"            custom_migration_operations.operations.ArgsOperation(\\n\"\n            \"                arg1=4,\\n\"\n            \"                arg2=5,\\n\"\n            \"            ),\\n\"\n            \"            6,\\n\"\n            \"        ],\\n\"\n            \"    },\\n\"\n            \"),\",\n        )\n", "    def test_expand_args_dict_signature(self):\n        operation = custom_migration_operations.operations.ExpandArgsOperation(\n            arg={\"key1\": 1, \"key2\": 2}\n        )\n        buff, imports = OperationWriter(operation, indentation=0).serialize()\n        self.assertEqual(imports, {\"import custom_migration_operations.operations\"})\n        self.assertEqual(\n            buff,\n            \"custom_migration_operations.operations.ExpandArgsOperation(\\n\"\n            \"    arg={\\n\"\n            \"        'key1': 1,\\n\"\n            \"        'key2': 2,\\n\"\n            \"    },\\n\"\n            \"),\",\n        )\n", "    def test_migration_basedir(self):\n        \"\"\"\n        Test the basedir property of MigrationWriter to ensure it finds the correct directory.\n        \"\"\"\n        with mock.patch('django.db.migrations.loader.MigrationLoader.migrations_module') as mock_migrations_module:\n            # Scenario where MigrationLoader returns a valid migrations module name\n            mock_migrations_module.return_value = (\"testapp.migrations\", None)\n            migration = migrations.Migration(\"0001_initial\", \"testapp\")\n            writer = MigrationWriter(migration)\n            with mock.patch('django.utils.module_loading.module_dir') as mock_module_dir:\n                mock_module_dir.return_value = \"/path/to/testapp/migrations\"\n                self.assertEqual(writer.basedir, \"/path/to/testapp/migrations\")\n        \n            # Scenario where the migrations module cannot be imported and needs to be created\n            mock_migrations_module.return_value = (\"testapp.migrations\", None)\n            with mock.patch('django.apps.apps.get_app_config') as mock_get_app_config:\n                mock_get_app_config.return_value.path = \"/path/to/testapp\"\n                with mock.patch('os.makedirs') as mock_makedirs, \\\n                     mock.patch('builtins.open', mock.mock_open()):\n                    self.assertEqual(\n                        writer.basedir,\n                        \"/path/to/testapp/migrations\"\n                    )\n                    mock_makedirs.assert_called_with(\"/path/to/testapp/migrations\", exist_ok=True)\n\n            # Scenario where MIGRATION_MODULES setting is used and base module must be located\n            mock_migrations_module.return_value = (\"custom.migrations.path\", None)\n            with mock.patch('importlib.import_module') as mock_import_module:\n                mock_import_module.side_effect = [ImportError, mock.DEFAULT, mock.DEFAULT]\n                with mock.patch('django.utils.module_loading.module_dir') as mock_module_dir:\n                    mock_module_dir.side_effect = [ValueError, \"/path/to/custom\"]\n                    self.assertEqual(\n                        writer.basedir,\n                        \"/path/to/custom/path\"\n                    )\n                    mock_makedirs.assert_called_with(\"/path/to/custom/path\", exist_ok=True)\n", "    def test_serialize_migrations_dependency(self):\n        \"\"\"\n        Test serialization of migration dependencies including swappable dependencies.\n        \"\"\"\n        migration = migrations.Migration(\n            \"0001_initial\",\n            \"testapp\",\n            dependencies=[\n                (\"auth\", \"0001_initial\"),\n                (\"contenttypes\", \"0002_remove_content_type_name\"),\n                (\"__setting__\", \"AUTH_USER_MODEL\"),\n            ],\n        )\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"dependencies = [\", output)\n        self.assertIn(\"        ('auth', '0001_initial'),\", output)\n        self.assertIn(\"        ('contenttypes', '0002_remove_content_type_name'),\", output)\n        self.assertIn(\"        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\", output)\n        self.assertIn(\"from django.conf import settings\", output)\n"], "sample_467": ["    def test_render_with_initial_value(self):\n        \"\"\"\n        Test rendering with an initial value.\n        \"\"\"\n        self.check_html(\n            self.widget,\n            \"mydate\",\n            date(2011, 6, 21),\n            html=(\n                \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\" selected>June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"19\">19</option>\n                <option value=\"20\">20</option>\n                <option value=\"21\" selected>21</option>\n                <option value=\"22\">22</option>\n                <option value=\"23\">23</option>\n                <option value=\"24\">24</option>\n                <option value", "    def test_render_with_initial(self):\n        widget = SelectDateWidget(years=(\"2020\", \"2021\", \"2022\"))\n        self.check_html(\n            widget,\n            \"mydate\",\n            date(2021, 5, 21),\n            html=(\n                \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\" selected>May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"19\">19</option>\n                <option value=\"20\">20</option>\n                <option value=\"21\" selected>21</option>\n                <option value=\"22\">22</option>\n                <option value=\"23\">23</option>\n                <option value=\"24\">24</", "    def test_non_required_empty(self):\n        widget = SelectDateWidget(years=(\"2022\", \"2023\"), empty_label=\"Select\")\n        self.check_html(\n            widget,\n            \"mydate\",\n            \"\",\n            html=(\n                \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option selected value=\"\">Select</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option selected value=\"\">Select</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"19\">19</option>\n                <option value=\"20\">20</option>\n                <option value=\"21\">21</option>\n                <option value=\"22\">22</option>\n                <option value=\"23\">23</option>\n                <option value=\"24\">24</option>\n                <option value=\"25\">", "    def test_render_with_initial_value(self):\n        \"\"\"\n        Test rendering when the initial value is provided.\n        \"\"\"\n        widget = SelectDateWidget(\n            years=(\n                \"2007\",\n                \"2008\",\n                \"2009\",\n                \"2010\",\n                \"2011\",\n                \"2012\",\n                \"2013\",\n                \"2014\",\n                \"2015\",\n                \"2016\",\n            ),\n            initial=\"2012-05-21\"\n        )\n        self.check_html(\n            widget,\n            \"mydate\",\n            \"2012-05-21\",\n            html=(\n                \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\" selected>May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option", "    def test_render_selected_empty_label(self):\n        widget = SelectDateWidget(\n            years=(\"2014\",), empty_label=(\"Select year\", \"Select month\", \"Select day\")\n        )\n        self.check_html(\n            widget,\n            \"mydate\",\n            \"2014-03-15\",\n            html=(\n                \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option value=\"\">Select month</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\" selected>March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option value=\"\">Select day</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\" selected>15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"19\">19</option>\n                <option value=\"20\">20</option>\n                <option value=\"21\">21</option>\n                <option value=\"22\">22</option>\n                <option value=\"23\">23"], "sample_469": ["    def setUp(self):\n        self.query = RawQuery(\n            \"SELECT * FROM annotations_author WHERE name = %s\", \"default\", params=[\"Adrian Holovaty\"]\n        )\n", "    def setUp(self):\n        self.sql = \"SELECT * FROM annotations_author WHERE age > %s\"\n        self.params = (30,)\n        self.raw_query = RawQuery(self.sql, using=DEFAULT_DB_ALIAS, params=self.params)\n", "def test_get_field_names_from_opts(self):\n    # Test with opts having concrete and non-concrete fields\n    class MockField:\n            self.name = name\n            self.attname = attname\n            self.concrete = concrete\n    \n    class MockOpts:\n            return [\n                MockField(name='field1', attname='field1_attname', concrete=True),\n                MockField(name='field2', attname='field2_attname', concrete=True),\n                MockField(name='field3', attname='field3_attname', concrete=False),\n            ]\n    \n    opts = MockOpts()\n    field_names = get_field_names_from_opts(opts)\n    self.assertEqual(field_names, {'field1', 'field1_attname', 'field2', 'field2_attname', 'field3'})\n    \n    # Test with opts being None\n    field_names = get_field_names_from_opts(None)\n    self.assertEqual(field_names, set())\n", "def test_raw_query_get_columns(self):\n        raw_sql = \"SELECT id, name FROM annotations_author\"\n        raw_query = RawQuery(raw_sql, using=\"default\")\n        columns = raw_query.get_columns()\n        self.assertEqual(columns, [\"id\", \"name\"])\n", "def test_annotation_with_complex_expression(self):\n    complex_expr = ExpressionWrapper(F(\"rating\") * F(\"pages\") + Value(10), output_field=DecimalField())\n    books = Book.objects.annotate(complex_value=complex_expr)\n    for book in books:\n        expected_value = book.rating * book.pages + 10\n        self.assertAlmostEqual(book.complex_value, expected_value)\n"], "sample_468": ["    def test_contextdict_with_context_manager(self):\n        c = Context({\"a\": 1})\n        with ContextDict(c, {\"b\": 2}) as cd:\n            self.assertEqual(cd[\"b\"], 2)\n            cd[\"c\"] = 3\n            self.assertEqual(cd[\"c\"], 3)\n        self.assertNotIn(\"c\", c.flatten())\n        self.assertNotIn(\"b\", c.flatten())\n", "    def test_pop_empty_context(self):\n        c = Context()\n        with self.assertRaises(ContextPopException):\n            c.pop()\n", "def test_context_dict_enter_exit():\n    \"\"\"\n    Ensure that ContextDict correctly enters and exits, modifying the context stack.\n    \"\"\"\n    c = Context({\"a\": 1})\n    initial_len = len(c.dicts)\n    with ContextDict(c, {\"b\": 2}) as ctx_dict:\n        self.assertEqual(len(c.dicts), initial_len + 1)\n        self.assertEqual(c[\"b\"], 2)\n    self.assertEqual(len(c.dicts), initial_len)\n    with self.assertRaises(KeyError):\n        c[\"b\"]\n", "    def test_context_pop_exception(self):\n        c = Context({\"a\": 1})\n        c.pop()  # First pop should work fine\n        with self.assertRaises(ContextPopException):\n            c.pop()  # Second pop should raise ContextPopException\n", "    def test_context_pop_exception(self):\n        c = Context({\"a\": 1})\n        c.pop()  # Should work fine, as there's an initial context\n        with self.assertRaises(ContextPopException):\n            c.pop()  # Should raise ContextPopException, as there's only the builtins dict left\n"], "sample_470": ["    def test_lazy_object_copy(self):\n        class Klazz:\n                self.value = value\n\n        lazy_klazz = lazy(lambda: Klazz(42), Klazz)()\n        copied_lazy_klazz = copy.copy(lazy_klazz)\n        self.assertEqual(copied_lazy_klazz.value, 42)\n        self.assertNotEqual(id(lazy_klazz), id(copied_lazy_klazz))\n", "    def test_keep_lazy_decorator(self):\n        \"\"\"\n        Ensure the keep_lazy decorator works as expected and returns a lazy \n        evaluated result if any arguments are lazy.\n        \"\"\"\n        @keep_lazy(str)\n            return a + b\n\n        lazy_str1 = lazystr(\"Hello \")\n        lazy_str2 = lazystr(\"World\")\n        result = concat_strings(lazy_str1, lazy_str2)\n        \n        self.assertIsInstance(result, Promise)\n        self.assertEqual(str(result), \"Hello World\")\n\n        # Test with non-lazy arguments to ensure immediate evaluation\n        result = concat_strings(\"Hello \", \"World\")\n        self.assertEqual(result, \"Hello World\")\n", "    def test_partition(self):\n            return x % 2 == 0\n\n        result = partition(is_even, range(6))\n        self.assertEqual(result, ([1, 3, 5], [0, 2, 4]))\n", "    def test_partition(self):\n        \"\"\"Test partition function for splitting values based on a predicate.\"\"\"\n        values = [1, 2, 3, 4, 5]\n        predicate = lambda x: x > 3\n        true_values, false_values = partition(predicate, values)\n        self.assertEqual(true_values, [4, 5])\n        self.assertEqual(false_values, [1, 2, 3])\n", "    def test_partition_function(self):\n        \"\"\"\n        Test the partition function to ensure it splits values correctly based on predicate.\n        \"\"\"\n        # Test with a simple predicate\n        predicate = lambda x: x > 3\n        values = [0, 1, 2, 3, 4, 5]\n        result = partition(predicate, values)\n        self.assertEqual(result, ([0, 1, 2, 3], [4, 5]))\n\n        # Test with another predicate\n        predicate = lambda x: x % 2 == 0\n        values = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n        result = partition(predicate, values)\n        self.assertEqual(result, ([1, 3, 5, 7, 9], [0, 2, 4, 6, 8]))\n\n        # Test with all true predicate\n        predicate = lambda x: True\n        values = [0, 1, 2, 3]\n        result = partition(predicate, values)\n        self.assertEqual(result, ([], [0, 1, 2, 3]))\n\n        # Test with all false predicate\n        predicate = lambda x: False\n        values = [0, 1, 2, 3]\n        result = partition(predicate, values)\n        self.assertEqual(result, ([0, 1, 2, 3], []))\n"], "sample_471": ["def test_integerfield_custom_widget(self):\n    \"\"\"\n    Ensure a custom widget can be used with IntegerField.\n    \"\"\"\n    class CustomWidget(Textarea):\n        pass\n\n    f = IntegerField(widget=CustomWidget)\n    self.assertWidgetRendersTo(\n        f, '<textarea name=\"f\" id=\"id_f\" required></textarea>'\n    )\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(\"\")\n    self.assertEqual(5, f.clean(\"5\"))\n    self.assertIsInstance(f.clean(\"5\"), int)\n    with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n        f.clean(\"a\")\n", "def test_integerfield_invalid_values(self):\n    f = IntegerField()\n    # Test various invalid values and ensure ValidationError is raised with the correct message.\n    invalid_values = [\"abc\", \"1a2b\", \"@!#\", \"1.2.3\", \"100,000\", None]\n    for value in invalid_values:\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(value)\n", "def test_integerfield_unicode_fraction(self):\n    f = IntegerField()\n    with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n        f.clean(\"\uff11\uff0f\uff12\")\n", "def test_integerfield_invalid_input(self):\n    f = IntegerField()\n    invalid_inputs = [\"abc\", \"123abc\", \"12.34.56\", \"\", None, \"1e5\"]\n    for invalid_input in invalid_inputs:\n        with self.assertRaises(ValidationError):\n            f.clean(invalid_input)\n", "    def test_integerfield_invalid_localized_number(self):\n        \"\"\"\n        A localized IntegerField handles invalid localized number formats.\n        \"\"\"\n        f = IntegerField(localize=True)\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"1,234\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"1 234\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"1.234\")\n"], "sample_472": ["def test_non_iterable_object_list(self):\n        \"\"\"\n        Paginator with a non-iterable object_list should raise TypeError.\n        \"\"\"\n        with self.assertRaises(TypeError):\n            Paginator(12345, 5)\n", "    def test_get_page_validates_number(self):\n        \"\"\"\n        Paginator.get_page() should validate the page number before returning a page.\n        \"\"\"\n        paginator = Paginator([1, 2, 3, 4, 5, 6], 2)\n        \n        with unittest.mock.patch.object(paginator, 'validate_number', wraps=paginator.validate_number) as mock_validate:\n            page = paginator.get_page(2)\n            mock_validate.assert_called_once_with(2)\n            self.assertEqual(page.number, 2)\n            \n            page = paginator.get_page(-1)\n            mock_validate.assert_called_with(-1)\n            self.assertEqual(page.number, 3)  # Last page due to invalid page number\n            \n            page = paginator.get_page('invalid')\n            mock_validate.assert_called_with('invalid')\n            self.assertEqual(page.number, 1)  # First page due to invalid page number\n", "    def test_custom_error_messages_inheritance(self):\n        \"\"\"\n        Ensure that custom error messages are correctly inherited from the default\n        messages when only some custom messages are provided.\n        \"\"\"\n        custom_messages = {\"invalid_page\": \"Custom invalid page message\"}\n        paginator = Paginator([1, 2, 3], 2, error_messages=custom_messages)\n        \n        # Custom message should be used for invalid_page\n        msg = \"Custom invalid page message\"\n        with self.assertRaisesMessage(PageNotAnInteger, msg):\n            paginator.validate_number(\"invalid\")\n        \n        # Default messages should be used for other cases\n        msg = \"That page number is less than 1\"\n        with self.assertRaisesMessage(EmptyPage, msg):\n            paginator.validate_number(0)\n        \n        msg = \"That page contains no results\"\n        with self.assertRaisesMessage(EmptyPage, msg):\n            paginator.validate_number(3)\n", "    def test_elided_page_range_custom_arguments(self):\n        \"\"\"\n        Test the get_elided_page_range method with custom arguments.\n        \"\"\"\n        ELLIPSIS = Paginator.ELLIPSIS\n        paginator = Paginator(range(1500), 100)\n        tests = [\n            # Custom arguments for elision\n            (1, [1, 2, 3, ELLIPSIS, 14, 15], 2, 1),\n            (5, [1, 2, 3, 4, 5, 6, 7, ELLIPSIS, 14, 15], 2, 1),\n            (6, [1, ELLIPSIS, 4, 5, 6, 7, 8, ELLIPSIS, 14, 15], 2, 1),\n            (10, [1, ELLIPSIS, 8, 9, 10, 11, 12, ELLIPSIS, 14, 15], 2, 1),\n            (15, [1, ELLIPSIS, 12, 13, 14, 15], 2, 1),\n            # Custom arguments with no elision\n            (1, [1, 2, 3, 4, 5, 6], 4, 0),\n            (3, [1, 2, 3, 4, 5, 6], 4, 0),\n            (5, [1, 2, 3, 4, 5, 6], 4, 0),\n            (6, [1, 2, 3, 4, 5, 6], 4, 0),\n            (7, [3, 4, 5, 6, 7, 8], 4, 0),\n        ]\n        for number, expected, on_each_side, on_ends in tests:\n            with self.subTest(number=number, on_each_side=on_each_side, on_ends=on_ends):\n                page_range = paginator.get_elided_page_range(\n                    number,\n                    on_each_side=on_each_side,\n                    on_ends=on_ends,\n                )\n                self.assertIsInstance(page_range, collections.abc.Generator)\n                self.assertEqual(list(page_range), expected)\n", "    def test_paginator_with_callable_object_list(self):\n        class CallableList:\n                self.data = data\n\n                return len(self.data)\n\n                return self.data[index]\n\n                return len(self.data)\n\n        data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        callable_list = CallableList(data)\n        paginator = Paginator(callable_list, 3)\n        \n        self.assertEqual(paginator.count, len(data))\n        self.assertEqual(paginator.num_pages, 4)\n        self.assertEqual(list(paginator.page_range), [1, 2, 3, 4])\n\n        page1 = paginator.page(1)\n        self.assertEqual(page1.object_list, data[:3])\n        page2 = paginator.page(2)\n        self.assertEqual(page2.object_list, data[3:6])\n        page3 = paginator.page(3)\n        self.assertEqual(page3.object_list, data[6:9])\n        page4 = paginator.page(4)\n        self.assertEqual(page4.object_list, data[9:])\n"], "sample_473": ["compilation error", "compilation error", "        def receive_disconnect():\n            return {\"type\": \"http.disconnect\"}\n        ", "compilation error", "compilation error"], "sample_474": ["    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\", alias=None)\n", "    def test_char_with_sqlite(self):\n        authors = Author.objects.annotate(first_initial=Left(\"name\", 1))\n        with self.settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'}}):\n            self.assertCountEqual(\n                authors.filter(first_initial=Chr(ord(\"J\"))), [self.john]\n            )\n            self.assertCountEqual(\n                authors.exclude(first_initial=Chr(ord(\"J\"))), [self.elena, self.rhonda]\n            )\n", "    def test_multiple_annotations(self):\n        authors = Author.objects.annotate(\n            first_initial=Left(\"name\", 1),\n            second_initial=Left(\"name\", 2),\n            initial_chr=Chr(ord(\"J\")),\n        )\n        self.assertSequenceEqual(\n            authors.filter(first_initial=F(\"initial_chr\"), second_initial=F(\"initial_chr\") + Chr(ord(\"o\"))),\n            [self.john],\n        )\n", "    def test_multiple_initials(self):\n        authors = Author.objects.annotate(\n            first_initial=Left(\"name\", 1),\n            second_initial=Left(\"name\", 2),\n        )\n        self.assertCountEqual(\n            authors.filter(first_initial=Chr(ord(\"J\")), second_initial=Left(\"name\", 2)),\n            [self.john]\n        )\n        self.assertCountEqual(\n            authors.exclude(first_initial=Chr(ord(\"J\"))),\n            [self.elena, self.rhonda]\n        )\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\", alias=None)\n"], "sample_475": ["    def test_not_list_or_tuple(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"name\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_exclude_not_list_or_tuple(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = \"name\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            \"admin.E014\",\n        )\n", "    def test_readonly_fields_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n"], "sample_476": ["    def test_generate_filename(self):\n        \"\"\"\n        Test generate_filename to ensure correct filename generation.\n        \"\"\"\n        p = self.PersonModel(name=\"Joe\")\n        generated_filename = p.mugshot.field.generate_filename(p, \"test_image.jpg\")\n        self.assertTrue(generated_filename.startswith(datetime.datetime.now().strftime(str(p.mugshot.field.upload_to))))\n        self.assertTrue(generated_filename.endswith(\"test_image.jpg\"))\n", "def test_generate_filename(self):\n        \"\"\"\n        Test the generate_filename method to ensure it correctly generates\n        filenames using both callable and string upload_to parameters.\n        \"\"\"\n        # Define a callable for upload_to that appends '_custom' to filenames\n            return 'custom/' + filename\n\n        # Use a string upload_to parameter\n        field_with_string_upload_to = ImageField(upload_to='uploads/%Y/%m/%d')\n        p = self.PersonModel(name=\"Joe\")\n        generated_filename = field_with_string_upload_to.generate_filename(p, 'test.png')\n        self.assertTrue(generated_filename.startswith('uploads/'))\n        self.assertTrue(generated_filename.endswith('/test.png'))\n\n        # Use a callable upload_to parameter\n        field_with_callable_upload_to = ImageField(upload_to=custom_upload_to)\n        generated_filename_callable = field_with_callable_upload_to.generate_filename(p, 'test.png')\n        self.assertEqual(generated_filename_callable, 'custom/test.png')\n", "    def test_update_image(self):\n        \"\"\"\n        Tests updating an image through an ImageField.\n        \"\"\"\n        # Create a Person instance and save an initial image.\n        p = self.PersonModel(name=\"Alice\")\n        p.mugshot.save(\"mug\", self.file1)\n        self.check_dimensions(p, 4, 8)\n\n        # Update the image.\n        p.mugshot.save(\"mug\", self.file2)\n        self.check_dimensions(p, 8, 4)\n\n        # Verify that the old image is deleted.\n        self.assertFalse(os.path.exists(os.path.join(temp_storage_dir, \"mug\", \"4x8.png\")))\n        self.assertTrue(os.path.exists(os.path.join(temp_storage_dir, \"mug\", \"8x4.png\")))\n\n        # Fetch from the database and check dimensions again.\n        p = self.PersonModel.objects.get(name=\"Alice\")\n        self.check_dimensions(p, 8, 4)\n", "def test_file_field_generate_filename(self):\n    \"\"\"\n    Test the generate_filename method of FileField to ensure it correctly\n    applies the upload_to attribute and uses the storage backend to generate\n    the final filename.\n    \"\"\"\n    from django.core.files.storage import FileSystemStorage\n    from django.core.files.utils import validate_file_name\n\n    class TestStorage(FileSystemStorage):\n            return \"generated_\" + filename\n\n    class TestModel:\n        pass\n\n    # Create a FileField with a custom storage backend and upload_to attribute\n    field = FileField(upload_to=\"uploads/%Y/%m/%d\", storage=TestStorage())\n\n    # Create a mock instance of the model\n    instance = TestModel()\n\n    # Generate a filename\n    filename = field.generate_filename(instance, \"testfile.txt\")\n\n    # Ensure that the filename is correctly generated\n    self.assertTrue(filename.startswith(\"uploads/\"))\n    self.assertTrue(filename.endswith(\"testfile.txt\"))\n    self.assertIn(\"/generated_\", filename)\n\n    # Test with upload_to as a callable\n        return \"callable_upload/\" + filename\n\n    field.upload_to = upload_to_callable\n    filename = field.generate_filename(instance, \"testfile.txt\")\n    self.assertEqual(filename, \"callable_upload/generated_testfile.txt\")\n\n    # Test with upload_to as a string without a date format\n    field.upload_to = \"static_upload\"\n    filename = field.generate_filename(instance, \"testfile.txt\")\n    self.assertEqual(filename, \"static_upload/generated_testfile.txt\")\n\n    # Test with an invalid file name\n    invalid_filename = \"in\\\\valid/test:file.txt\"\n    valid_filename = validate_file_name(invalid_filename, allow_relative_path=True)\n    self.assertNotIn(\"\\\\\", valid_filename)\n    self.assertNotIn(\":\", valid_filename)\n", "    def setUp(self):\n        \"\"\"\n        Setup a temporary directory and a File instance for use in tests.\n        \"\"\"\n        if os.path.exists(temp_storage_dir):\n            shutil.rmtree(temp_storage_dir)\n        os.mkdir(temp_storage_dir)\n\n        self.file_path = os.path.join(os.path.dirname(__file__), \"test.txt\")\n        with open(self.file_path, \"w\") as f:\n            f.write(\"This is a test file.\")\n        self.file = File(open(self.file_path, \"rb\"))\n"], "sample_477": ["    def test_random03(self):\n        output = self.engine.render_to_string(\n            \"random03\", {\"c\": [\"<item1>\", mark_safe(\"<item2>\"), \"<item3>\"]}\n        )\n        self.assertIn(output, [\"&lt;item1&gt;\", \"<item2>\", \"&lt;item3&gt;\"])\n", "    def test_add_int(self):\n        output = self.engine.render_to_string(\"add_int\", {\"a\": 5, \"b\": 3})\n        self.assertEqual(output, \"8\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\")\n        self.assertEqual(output, \"9\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\")\n        self.assertEqual(output, \"7\")\n", "    def test_cut01(self):\n        output = self.engine.render_to_string(\"cut01\", {\"a\": \"banana\"})\n        self.assertEqual(output, \"bnn\")\n"], "sample_478": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"name\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            \"admin.E014\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n"], "sample_479": ["    def test_add_remove_constraint(self):\n        \"\"\"\n        RemoveConstraint should cancel AddConstraint.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.AddConstraint(\n                    \"Pony\",\n                    models.CheckConstraint(\n                        check=models.Q(weight__gte=0),\n                        name=\"pony_weight_gte_0\"\n                    ),\n                ),\n                migrations.RemoveConstraint(\"Pony\", \"pony_weight_gte_0\"),\n            ],\n            [],\n        )\n", "    def test_create_model_with_duplicate_field_names(self):\n        \"\"\"\n        Test CreateModel raises a ValueError for duplicate field names.\n        \"\"\"\n        with self.assertRaisesMessage(ValueError, \"Found duplicate value name in CreateModel fields argument.\"):\n            migrations.CreateModel(\n                \"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255)), (\"name\", models.IntegerField())],\n            )\n", "    def test_create_model_alter_table_comment(self):\n        \"\"\"\n        AlterModelTableComment should optimize into CreateModel.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                ),\n                migrations.AlterModelTableComment(\n                    name=\"Foo\", table_comment=\"This is a test comment.\"\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"db_table_comment\": \"This is a test comment.\"},\n                ),\n            ],\n        )\n", "    def test_create_model_add_constraint(self):\n        \"\"\"\n        AddConstraint should optimize into CreateModel.\n        \"\"\"\n        constraint = models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                ),\n                migrations.AddConstraint(\"Foo\", constraint),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    constraints=[constraint],\n                ),\n            ],\n        )\n", "    def test_create_model_alter_table(self):\n        \"\"\"\n        AlterModelTable should optimize into CreateModel.\n        \"\"\"\n        managers = [(\"objects\", EmptyManager())]\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n                migrations.AlterModelTable(\"Foo\", \"new_table_name\"),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\", \"db_table\": \"new_table_name\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n            ],\n        )\n"], "sample_480": ["def test_compile_json_path(self):\n    test_cases = [\n        ([\"key1\"], True, '$.\"key1\"'),\n        ([\"key1\", \"key2\"], True, '$.\"key1\".\"key2\"'),\n        ([\"key1\", \"2\"], True, '$.\"key1\"[2]'),\n        ([\"key1\", \"key2\"], False, '.\"key1\".\"key2\"'),\n        ([\"key1\", \"2\"], False, '.\"key1\"[2]'),\n    ]\n    \n    for key_transforms, include_root, expected in test_cases:\n        with self.subTest(key_transforms=key_transforms, include_root=include_root):\n            self.assertEqual(compile_json_path(key_transforms, include_root), expected)\n", "    def test_compile_json_path_with_root(self):\n        key_transforms = ['a', 'b', 'c']\n        result = compile_json_path(key_transforms, include_root=True)\n        self.assertEqual(result, '$.\"a\".\"b\".\"c\"')\n", "    def test_compile_json_path(self):\n        key_transforms = [\"name\", \"details\", 2, \"value\"]\n        expected_path = '$.\"name\".\"details\"[2].\"value\"'\n        self.assertEqual(compile_json_path(key_transforms), expected_path)\n", "    def test_compile_json_path(self):\n        self.assertEqual(compile_json_path([\"a\", \"b\", \"c\"]), '$.\"a\".\"b\".\"c\"')\n        self.assertEqual(compile_json_path([\"a\", 1, \"c\"]), '$.\"a\"[1].\"c\"')\n        self.assertEqual(compile_json_path([1, \"b\", 3], include_root=False), '[\"1\"].\"b\"[3]')\n", "    def test_compile_json_path(self):\n        tests = [\n            ([\"a\", \"b\", \"1\"], True, '$.\"a\".\"b\"[1]'),\n            ([\"a\", \"b\", \"1\"], False, '.\"a\".\"b\"[1]'),\n            ([\"key1\", \"key2\", \"3\"], True, '$.\"key1\".\"key2\"[3]'),\n            ([\"key1\", \"key2\", \"3\"], False, '.\"key1\".\"key2\"[3]'),\n        ]\n        for key_transforms, include_root, expected in tests:\n            with self.subTest(key_transforms=key_transforms, include_root=include_root):\n                self.assertEqual(compile_json_path(key_transforms, include_root), expected)\n"], "sample_481": ["    def test_join09(self):\n        output = self.engine.render_to_string(\n            \"join09\", {\"a\": [\"alpha\", \"beta & me\"], \"var\": \", \"}\n        )\n        self.assertEqual(output, \"ALPHA, BETA &AMP; ME\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta <br> me\"]})\n        self.assertEqual(output, \"alpha &lt;br&gt; beta &lt;br&gt; me\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta & me\"]})\n        self.assertEqual(output, \"alpha&lt;br&gt;beta &amp; me\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\n            \"join09\", {\"a\": [\"alpha\", \"beta & me\"], \"var\": \" & \"}\n        )\n        self.assertEqual(output, \"ALPHA &AMP; BETA &AMP; ME\")\n", "    def test_join_with_empty_list(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": []})\n        self.assertEqual(output, \"\")\n"], "sample_482": ["    def test_basic(self):\n        output = self.engine.render_to_string(\"cut_basic\", {\"a\": \"xaxbxcxd\"})\n        self.assertEqual(output, \"abcd\")\n", "    def test_lower(self):\n        output = self.engine.render_to_string(\"lower_filter\", {\"value\": \"Hello WORLD\"})\n        self.assertEqual(output, \"hello world\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\"force_escape_basic\", {\"a\": \"x&y <p>\"})\n        self.assertEqual(output, \"x&amp;y &lt;p&gt;\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\"slugify_basic\")\n        self.assertEqual(output, \"hello-world-this-is-a-test\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"safeseq_basic\",\n            {\"a\": [\"<x&y>\", \"<p>\"], \"b\": [mark_safe(\"<x&y>\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"<x&y>, <p> -- <x&y>, <p>\")\n"], "sample_483": ["    def test_autocomplete_fields_not_list_or_tuple(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = \"test\"\n\n        self.assertEqual(\n            SongAdmin(Song, AdminSite()).check(),\n            [\n                checks.Error(\n                    \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                    obj=SongAdmin,\n                    id=\"admin.E036\",\n                )\n            ],\n        )\n", "    def test_check_autocomplete_fields(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = \"title\"\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                obj=SongAdmin,\n                id=\"admin.E036\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n        class ValidSongAdmin(admin.ModelAdmin):\n            autocomplete_fields = [\"album\"]\n\n        errors = ValidSongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n\n        class InvalidSongAdmin(admin.ModelAdmin):\n            autocomplete_fields = [\"nonexistent\"]\n\n        errors = InvalidSongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields[0]' refers to 'nonexistent', which is not a field of 'admin_checks.Song'.\",\n                obj=InvalidSongAdmin,\n                id=\"admin.E037\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n        class InvalidTypeSongAdmin(admin.ModelAdmin):\n            autocomplete_fields = [\"title\"]\n\n        errors = InvalidTypeSongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'autocomplete_fields[0]' must be a foreign key or a many-to-many field.\",\n                obj=InvalidTypeSongAdmin,\n                id=\"admin.E038\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "def test_fields_not_list_or_tuple(self):\n    class SongAdmin(admin.ModelAdmin):\n        fields = \"test\"\n\n    self.assertEqual(\n        SongAdmin(Song, AdminSite()).check(),\n        [\n            checks.Error(\n                \"The value of 'fields' must be a list or tuple.\",\n                obj=SongAdmin,\n                id=\"admin.E004\",\n            )\n        ],\n    )\n", "    def test_list_display_contains_callable(self):\n        class SongAdmin(admin.ModelAdmin):\n                return obj.title\n\n            list_display = (\"custom_title\",)\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_autocomplete_fields_not_list_or_tuple(self):\n        class SongAdmin(admin.ModelAdmin):\n            autocomplete_fields = \"test\"\n\n        self.assertEqual(\n            SongAdmin(Song, AdminSite()).check(),\n            [\n                checks.Error(\n                    \"The value of 'autocomplete_fields' must be a list or tuple.\",\n                    obj=SongAdmin,\n                    id=\"admin.E036\",\n                )\n            ],\n        )\n"], "sample_484": ["    def setUpTestData(cls):\n        Author.objects.create(name=\"Jonathan\", alias=\"johnny\")\n        Author.objects.create(name=\"Elizabeth\", alias=\"lizzy\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"Alexander\", alias=\"alex\")\n        Author.objects.create(name=\"Beatrice\", alias=\"bea\")\n", "    def test_basic_concat(self):\n        authors = Author.objects.annotate(full_info=Concat(\"name\", Value(\" \"), \"alias\"))\n        self.assertQuerySetEqual(\n            authors.order_by(\"name\"),\n            [\"John Smith smithj\", \"Rhonda \"],\n            lambda a: a.full_info,\n        )\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John\", alias=\"johnny\")\n        Author.objects.create(name=\"Jane\", alias=\"jane\")\n        Author.objects.create(name=\"Bob\", alias=None)\n        ", "    def test_sql_generation(self):\n        author = Author.objects.annotate(name_part=Right(\"name\", 2)).first()\n        with connection.cursor() as cursor:\n            query, params = author.name_part.query.sql_with_params()\n            cursor.execute(query, params)\n            result = cursor.fetchall()\n            self.assertGreater(len(result), 0)\n"], "sample_485": ["    def test_avoid_wrapping(self):\n        tests = (\n            (\"No spaces here\", \"No spaces here\"),\n            (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n            (\"Multiple    spaces\", \"Multiple\\xa0\\xa0\\xa0\\xa0spaces\"),\n            (\"Leading spaces\", \"\\xa0Leading\\xa0spaces\"),\n            (\"Trailing spaces \", \"Trailing\\xa0spaces\\xa0\"),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n", "def test_avoid_wrapping(self):\n    tests = (\n        (\"hello world\", \"hello\\xa0world\"),\n        (\"multiple spaces here\", \"multiple\\xa0spaces\\xa0here\"),\n        (\"no wrapping needed\", \"no\\xa0wrapping\\xa0needed\"),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(avoid_wrapping(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"hello world\", \"hello\\xa0world\"),\n            (\"no wrap here\", \"no\\xa0wrap\\xa0here\"),\n            (\"leading space \", \"leading\\xa0space\\xa0\"),\n            (\" multiple  spaces \", \" multiple\\xa0\\xa0spaces\\xa0\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.check_output(avoid_wrapping, value, output)\n                self.check_output(avoid_wrapping, lazystr(value), output)\n", "    def test_avoid_wrapping(self):\n        items = (\n            (\"Hello world\", \"Hello\\xa0world\"),\n            (\"This is a test\", \"This\\xa0is\\xa0a\\xa0test\"),\n            (\"Django testing\", \"Django\\xa0testing\"),\n            (\"\", \"\"),\n            (\" \", \"\\xa0\"),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.check_output(avoid_wrapping, value, output)\n", "    def test_avoid_wrapping(self):\n        tests = (\n            (\"Avoid wrapping this text\", \"Avoid\\xa0wrapping\\xa0this\\xa0text\"),\n            (\"Multiple spaces   in a sentence\", \"Multiple\\xa0spaces\\xa0\\xa0\\xa0in\\xa0a\\xa0sentence\"),\n            (\"Leading spaces\", \"Leading\\xa0spaces\"),\n            (\"Trailing spaces \", \"Trailing\\xa0spaces\\xa0\"),\n            (\"No spaces\", \"No\\xa0spaces\"),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(avoid_wrapping(value), output)\n"], "sample_486": ["    def test_inlineformset_factory_save_commit_false(self):\n        \"\"\"\n        Test saving an inline formset with commit=False does not save the instance to the database,\n        but still allows for many-to-many relationships to be saved later.\n        \"\"\"\n        FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n        parent_instance = UUIDPKParent.objects.create()\n        formset = FormSet(\n            {\n                \"uuidpkchild_set-TOTAL_FORMS\": 1,\n                \"uuidpkchild_set-INITIAL_FORMS\": 0,\n                \"uuidpkchild_set-MAX_NUM_FORMS\": \"\",\n                \"uuidpkchild_set-0-name\": \"Test Child\",\n            },\n            instance=parent_instance,\n        )\n        self.assertTrue(formset.is_valid())\n        saved_instances = formset.save(commit=False)\n        self.assertEqual(len(saved_instances), 1)\n        self.assertIsNone(saved_instances[0].pk)\n        # Save the parent instance to ensure the child can be saved\n        parent_instance.save()\n        for instance in saved_instances:\n            instance.save()\n        self.assertIsNotNone(saved_instances[0].pk)\n", "    def test_inlineformset_factory_validation_error_messages(self):\n        \"\"\"\n        Ensure that custom validation error messages defined at the model level\n        are overridden by those defined at the form level.\n        \"\"\"\n        FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n        formset = FormSet(\n            {\n                \"uuidpkchild_set-TOTAL_FORMS\": 1,\n                \"uuidpkchild_set-INITIAL_FORMS\": 0,\n                \"uuidpkchild_set-MAX_NUM_FORMS\": \"\",\n                \"uuidpkchild_set-0-name\": \"\",\n            }\n        )\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.forms[0].errors[\"name\"], \n            [\"This field is required.\"]\n        )\n", "    def test_construct_instance_saves_all_fields_correctly(self):\n        \"\"\"\n        Test that construct_instance correctly saves all provided fields\n        to the instance, including file fields, while excluding specified fields.\n        \"\"\"\n        from django import forms\n        from .models import TestModel\n\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = TestModel\n                fields = ['char_field', 'int_field', 'file_field']\n\n        instance = TestModel()\n        form_data = {'char_field': 'test', 'int_field': 123}\n        file_data = {'file_field': SimpleUploadedFile('test.txt', b'This is a test file')}\n        form = TestForm(data=form_data, files=file_data)\n\n        instance = construct_instance(form, instance)\n        self.assertEqual(instance.char_field, 'test')\n        self.assertEqual(instance.int_field, 123)\n        self.assertEqual(instance.file_field.read(), b'This is a test file')\n", "    def test_inlineformset_factory_with_custom_widget(self):\n        \"\"\"\n        Ensure that inline formsets respect custom widgets passed to the factory.\n        \"\"\"\n        from django.forms.widgets import TextInput\n        FormSet = inlineformset_factory(\n            UUIDPKParent, UUIDPKChild, fields=\"__all__\", widgets={\"name\": TextInput(attrs={\"class\": \"custom-widget\"})}\n        )\n        formset = FormSet()\n        self.assertIsInstance(formset.forms[0].fields[\"name\"].widget, TextInput)\n        self.assertEqual(formset.forms[0].fields[\"name\"].widget.attrs[\"class\"], \"custom-widget\")\n", "    def test_inlineformset_factory_applies_widgets(self):\n        \"\"\"\n        Ensure that custom widgets are properly applied to inline form fields.\n        \"\"\"\n        from django.forms import TextInput\n        \n        FormSet = inlineformset_factory(\n            UUIDPKParent, UUIDPKChild, fields=\"__all__\", widgets={'name': TextInput(attrs={'class': 'custom'})}\n        )\n        formset = FormSet()\n        self.assertEqual(formset.forms[0].fields['name'].widget.attrs['class'], 'custom')\n"], "sample_487": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_invalid_field_type(self):\n        \"\"\"\n        The value of 'list_editable' must refer to an editable field.\n        \"\"\"\n\n        class ProductAdmin(ModelAdmin):\n            list_editable = [\"name\"]\n\n        self.assertIsInvalid(\n            ProductAdmin,\n            ValidationTestModel,\n            \"The value of 'list_editable[0]' refers to 'name', which is not editable through the admin.\",\n            id=\"admin.E125\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"not a list or tuple\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n"], "sample_488": ["    def test_get_registered_model(self):\n        \"\"\"\n        Tests apps.get_registered_model().\n        \"\"\"\n        self.assertEqual(apps.get_registered_model(\"admin\", \"LogEntry\"), LogEntry)\n        with self.assertRaises(LookupError):\n            apps.get_registered_model(\"admin\", \"LogExit\")\n\n        # App label is case-sensitive, Model name is case-insensitive.\n        self.assertEqual(apps.get_registered_model(\"admin\", \"loGentrY\"), LogEntry)\n        with self.assertRaises(LookupError):\n            apps.get_registered_model(\"Admin\", \"LogEntry\")\n", "    def test_register_model(self):\n        \"\"\"\n        Tests apps.register_model().\n        \"\"\"\n        # Create a new model dynamically\n        class DynamicModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        # Register the new model\n        apps.register_model(\"admin\", DynamicModel)\n        \n        # Verify that the new model is registered correctly\n        registered_model = apps.get_model(\"admin\", \"DynamicModel\")\n        self.assertEqual(registered_model, DynamicModel)\n\n        # Attempt to register the same model again and check for warning\n        with self.assertRaisesMessage(RuntimeWarning, \"Model 'admin.dynamicmodel' was already registered.\"):\n            apps.register_model(\"admin\", DynamicModel)\n\n        # Create a conflicting model with the same name but different module\n        class ConflictingModel(models.Model):\n            name = models.CharField(max_length=100)\n            class Meta:\n                app_label = \"admin\"\n                model_name = \"DynamicModel\"\n                module = \"conflicting_module\"\n\n        # Attempt to register the conflicting model and expect RuntimeError\n        with self.assertRaisesMessage(RuntimeError, \"Conflicting 'dynamicmodel' models in application 'admin':\"):\n            apps.register_model(\"admin\", ConflictingModel)\n", "    def test_get_registered_model(self):\n        \"\"\"\n        Test that get_registered_model returns the correct model or raises LookupError.\n        \"\"\"\n        new_apps = Apps([\"apps\"])\n        meta_contents = {\n            \"app_label\": \"apps\",\n            \"apps\": new_apps,\n        }\n        body = {}\n        body[\"Meta\"] = type(\"Meta\", (), meta_contents)\n        body[\"__module__\"] = TotallyNormal.__module__\n        TestModel = type(\"TestModel\", (models.Model,), body)\n        \n        # Register the model\n        new_apps.register_model(\"apps\", TestModel)\n        \n        # Check if the registered model can be retrieved\n        self.assertEqual(new_apps.get_registered_model(\"apps\", \"TestModel\"), TestModel)\n        \n        # Check for a non-existent model, expect LookupError\n        with self.assertRaises(LookupError):\n            new_apps.get_registered_model(\"apps\", \"NonExistentModel\")\n", "    def test_get_registered_model(self):\n        \"\"\"\n        Tests apps.get_registered_model().\n        \"\"\"\n        self.assertEqual(apps.get_registered_model(\"admin\", \"LogEntry\"), LogEntry)\n        with self.assertRaises(LookupError):\n            apps.get_registered_model(\"admin\", \"LogExit\")\n        with self.assertRaises(LookupError):\n            apps.get_registered_model(\"nonexistent_app\", \"LogEntry\")\n", "    def test_register_model(self):\n        \"\"\"\n        Tests the register_model method in the Apps class.\n        \"\"\"\n        new_apps = Apps([\"apps\"])\n        meta_contents = {\n            \"app_label\": \"apps\",\n            \"apps\": new_apps,\n        }\n\n        body = {}\n        body[\"Meta\"] = type(\"Meta\", (), meta_contents)\n        body[\"__module__\"] = TotallyNormal.__module__\n        TestModel = type(\"TestModel\", (models.Model,), body)\n\n        # Register the model and check if it's correctly added\n        new_apps.register_model(\"apps\", TestModel)\n        self.assertEqual(new_apps.get_registered_model(\"apps\", \"testmodel\"), TestModel)\n\n        # Register the same model again should raise a warning\n        with self.assertRaisesMessage(RuntimeWarning, \"Model 'apps.testmodel' was already registered.\"):\n            new_apps.register_model(\"apps\", TestModel)\n\n        # Register a different model with the same name should raise an error\n        body[\"__module__\"] = TotallyNormal.__module__ + \".new\"\n        DifferentTestModel = type(\"TestModel\", (models.Model,), body)\n        with self.assertRaisesMessage(RuntimeError, \"Conflicting 'testmodel' models in application 'apps'\"):\n            new_apps.register_model(\"apps\", DifferentTestModel)\n"], "sample_489": ["    def test_bulk_create_with_deferred_fields(self):\n        \"\"\"\n        Test bulk_create with deferred fields.\n        \"\"\"\n        deferred_queryset = Country.objects.defer('name')\n        deferred_countries = list(deferred_queryset)\n        Country.objects.bulk_create([\n            Country(name=\"Spain\", iso_two_letter=\"ES\"),\n            Country(name=\"Italy\", iso_two_letter=\"IT\"),\n        ])\n        self.assertEqual(Country.objects.count(), 2 + len(deferred_countries))\n        self.assertQuerySetEqual(\n            Country.objects.filter(name__in=[\"Spain\", \"Italy\"]).order_by(\"name\"),\n            [\"Italy\", \"Spain\"],\n            attrgetter(\"name\"),\n        )\n", "    def setUp(self):\n        self.country1 = Country.objects.create(name=\"Country1\", iso_two_letter=\"C1\")\n        self.country2 = Country.objects.create(name=\"Country2\", iso_two_letter=\"C2\")\n", "    def setUp(self):\n        self.data = [\n            Country(name=\"United States of America\", iso_two_letter=\"US\"),\n            Country(name=\"The Netherlands\", iso_two_letter=\"NL\"),\n            Country(name=\"Germany\", iso_two_letter=\"DE\"),\n            Country(name=\"Czech Republic\", iso_two_letter=\"CZ\"),\n        ]\n        Country.objects.bulk_create(self.data)\n", "    def test_bulk_create_with_prefetch_related(self):\n        parent1 = NoFields.objects.create()\n        parent2 = NoFields.objects.create()\n        children = [\n            NullableFields(auto_field=parent1, integer_field=1),\n            NullableFields(auto_field=parent1, integer_field=2),\n            NullableFields(auto_field=parent2, integer_field=3),\n            NullableFields(auto_field=parent2, integer_field=4),\n        ]\n        NullableFields.objects.bulk_create(children)\n        with self.assertNumQueries(2):\n            parents = NoFields.objects.prefetch_related(\"nullablefields_set\").all()\n            parent1_children = parents[0].nullablefields_set.all()\n            parent2_children = parents[1].nullablefields_set.all()\n            self.assertEqual(parent1_children.count(), 2)\n            self.assertEqual(parent2_children.count(), 2)\n            self.assertQuerySetEqual(parent1_children, [1, 2], attrgetter(\"integer_field\"))\n            self.assertQuerySetEqual(parent2_children, [3, 4], attrgetter(\"integer_field\"))\n", "    def test_bulk_create_with_empty_list(self):\n        \"\"\"\n        Test bulk_create with an empty list to ensure it doesn't cause any errors\n        and doesn't insert any data.\n        \"\"\"\n        initial_count = Country.objects.count()\n        created = Country.objects.bulk_create([])\n        self.assertEqual(created, [])\n        self.assertEqual(Country.objects.count(), initial_count)\n"], "sample_490": ["    def test_custom_violation_code_and_message(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"unique_with_custom_code_message\",\n            violation_error_code=\"custom_code\",\n            violation_error_message=\"Custom error message for %(name)s\",\n        )\n        p = UniqueConstraintProduct(name=\"test_product\")\n        p.save()\n        non_unique_product = UniqueConstraintProduct(name=p.name)\n        msg = \"Custom error message for unique_with_custom_code_message\"\n        with self.assertRaisesMessage(ValidationError, msg) as cm:\n            constraint.validate(UniqueConstraintProduct, non_unique_product)\n        self.assertEqual(cm.exception.code, \"custom_code\")\n", "    def test_unique_constraint_with_violation_error_message(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"unique_name\",\n            violation_error_message=\"Custom violation error\"\n        )\n        non_unique_product = UniqueConstraintProduct(name=self.p1.name)\n        msg = \"Custom violation error\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, non_unique_product)\n        # Ensure the custom error message is raised for a violation\n        constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=\"unique_name\"))\n", "    def test_clone(self):\n        fields = [\"foo\", \"bar\"]\n        name = \"unique_fields\"\n        constraint = models.UniqueConstraint(fields=fields, name=name)\n        cloned_constraint = constraint.clone()\n        self.assertEqual(cloned_constraint.name, constraint.name)\n        self.assertEqual(cloned_constraint.fields, constraint.fields)\n        self.assertEqual(cloned_constraint.violation_error_code, constraint.violation_error_code)\n        self.assertEqual(cloned_constraint.violation_error_message, constraint.violation_error_message)\n", "    def test_deferrable_enum_repr(self):\n        self.assertEqual(repr(Deferrable.DEFERRED), \"Deferrable.DEFERRED\")\n        self.assertEqual(repr(Deferrable.IMMEDIATE), \"Deferrable.IMMEDIATE\")\n", "    def test_clone(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"foo\", \"bar\"],\n            name=\"unique_fields\",\n            violation_error_message=\"custom error\",\n            violation_error_code=\"custom_code\",\n        )\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint, cloned_constraint)\n        self.assertIsNot(constraint, cloned_constraint)\n"], "sample_491": ["    def test_boundfield_as_widget(self):\n        class CustomForm(Form):\n            name = CharField(widget=TextInput(attrs={\"class\": \"name-input\"}))\n\n        form = CustomForm()\n        boundfield = form[\"name\"]\n\n        self.assertHTMLEqual(\n            boundfield.as_widget(),\n            '<input type=\"text\" name=\"name\" class=\"name-input\" required id=\"id_name\">'\n        )\n\n        # Test overriding widget attributes\n        self.assertHTMLEqual(\n            boundfield.as_widget(attrs={\"class\": \"custom-class\"}),\n            '<input type=\"text\" name=\"name\" class=\"custom-class\" required id=\"id_name\">'\n        )\n\n        # Test with a different widget type\n        self.assertHTMLEqual(\n            boundfield.as_widget(widget=Textarea(attrs={\"rows\": 3, \"cols\": 40})),\n            '<textarea name=\"name\" rows=\"3\" cols=\"40\" required id=\"id_name\"></textarea>'\n        )\n\n        # Test with only_initial set to True\n        form = CustomForm(initial={\"name\": \"Initial Name\"})\n        boundfield = form[\"name\"]\n        self.assertHTMLEqual(\n            boundfield.as_widget(only_initial=True),\n            '<input type=\"text\" name=\"initial-name\" value=\"Initial Name\" class=\"name-input\" required id=\"initial-id_name\">'\n        )\n", "    def test_boundfield_widget_type_custom_widget(self):\n        class CustomWidget(TextInput):\n            pass\n\n        class CustomForm(Form):\n            custom_field = CharField(widget=CustomWidget())\n\n        form = CustomForm()\n        self.assertEqual(form[\"custom_field\"].widget_type, \"custom\")\n", "def test_boundfield_as_hidden(self):\n    class HiddenFieldForm(Form):\n        hidden_field = CharField(widget=HiddenInput)\n\n    f = HiddenFieldForm({\"hidden_field\": \"secret\"})\n    self.assertHTMLEqual(\n        f[\"hidden_field\"].as_hidden(),\n        '<input type=\"hidden\" name=\"hidden_field\" value=\"secret\" id=\"id_hidden_field\">',\n    )\n", "    def test_boundfield_as_widget(self):\n        class CustomForm(Form):\n            text = CharField()\n            textarea = CharField(widget=Textarea)\n            hidden = CharField(widget=HiddenInput)\n\n        f = CustomForm(auto_id=\"id_%s\")\n        self.assertHTMLEqual(\n            f[\"text\"].as_widget(attrs={\"class\": \"custom-class\"}),\n            '<input type=\"text\" name=\"text\" class=\"custom-class\" required id=\"id_text\">',\n        )\n        self.assertHTMLEqual(\n            f[\"textarea\"].as_widget(attrs={\"rows\": 5, \"cols\": 20}),\n            '<textarea name=\"textarea\" rows=\"5\" cols=\"20\" required id=\"id_textarea\"></textarea>',\n        )\n        self.assertHTMLEqual(\n            f[\"hidden\"].as_widget(attrs={\"class\": \"hidden-class\"}),\n            '<input type=\"hidden\" name=\"hidden\" class=\"hidden-class\" id=\"id_hidden\">',\n        )\n", "    def test_bound_field_has_changed(self):\n        class MyForm(Form):\n            name = CharField()\n            age = IntegerField()\n\n        form_data = {\"name\": \"Alice\", \"age\": \"30\"}\n        f = MyForm(data=form_data)\n        f.is_valid()\n\n        # Test with unmodified data\n        self.assertFalse(f[\"name\"]._has_changed())\n        self.assertFalse(f[\"age\"]._has_changed())\n\n        # Modify the data and check for changes\n        f = MyForm(data={\"name\": \"Alice\", \"age\": \"31\"})\n        f.is_valid()\n        self.assertFalse(f[\"name\"]._has_changed())\n        self.assertTrue(f[\"age\"]._has_changed())\n\n        # Test with initial data\n        f = MyForm(initial={\"name\": \"Bob\", \"age\": 25})\n        self.assertFalse(f[\"name\"]._has_changed())\n        self.assertFalse(f[\"age\"]._has_changed())\n\n        # Check changed data with initial data\n        f = MyForm(data=form_data, initial={\"name\": \"Bob\", \"age\": 25})\n        f.is_valid()\n        self.assertTrue(f[\"name\"]._has_changed())\n        self.assertTrue(f[\"age\"]._has_changed())\n\n        # Test with showing hidden initial\n        class HiddenInitialForm(Form):\n            name = CharField(show_hidden_initial=True)\n            age = IntegerField(show_hidden_initial=True)\n\n        f = HiddenInitialForm(data=form_data, initial={\"name\": \"Alice\", \"age\": 30})\n        f.is_valid()\n        self.assertFalse(f[\"name\"]._has_changed())\n        self.assertFalse(f[\"age\"]._has_changed())\n\n        f = HiddenInitialForm(data={\"name\": \"Alice\", \"age\": \"31\", \"initial-age\": \"30\"})\n        f.is_valid()\n        self.assertFalse(f[\"name\"]._has_changed())\n        self.assertTrue(f[\"age\"]._has_changed())\n"], "sample_492": ["    def test_serialize_datetime_tzinfo(self):\n        \"\"\"\n        Test serialization of datetime with tzinfo attribute.\n        \"\"\"\n        dt = datetime.datetime(2023, 1, 1, 12, 0, tzinfo=datetime.timezone.utc)\n        self.assertSerializedResultEqual(\n            dt,\n            (\"datetime.datetime(2023, 1, 1, 12, 0, tzinfo=datetime.timezone.utc)\", {\"import datetime\"}),\n        )\n", "    def test_complex_migration(self):\n        \"\"\"\n        Tests serializing a complex migration with multiple operations.\n        \"\"\"\n        fields1 = {\n            \"charfield\": models.CharField(max_length=255),\n            \"datetimefield\": models.DateTimeField(default=datetime.datetime.now),\n        }\n\n        fields2 = {\n            \"name\": models.CharField(max_length=100),\n            \"price\": models.DecimalField(max_digits=10, decimal_places=2),\n        }\n\n        migration = type(\n            \"Migration\",\n            (migrations.Migration,),\n            {\n                \"operations\": [\n                    migrations.CreateModel(\n                        \"FirstModel\", tuple(fields1.items()), bases=(models.Model,)\n                    ),\n                    migrations.CreateModel(\n                        \"SecondModel\", tuple(fields2.items()), bases=(models.Model,)\n                    ),\n                    migrations.AddField(\n                        \"FirstModel\", \"newfield\", models.IntegerField(default=0)\n                    ),\n                    migrations.RemoveField(\n                        \"SecondModel\", \"price\"\n                    ),\n                    migrations.RenameField(\n                        \"FirstModel\", \"charfield\", \"renamed_charfield\"\n                    ),\n                    migrations.AlterField(\n                        \"FirstModel\", \"datetimefield\", models.DateField()\n                    ),\n                ],\n                \"dependencies\": [(\"testapp\", \"0001_initial\")],\n            },\n        )\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        result = self.safe_exec(output)\n        self.assertIn(\"Migration\", result)\n", "    def test_serialize_operation_dependencies(self):\n        \"\"\"\n        Tests serializing an operation with dependencies.\n        \"\"\"\n        operation = migrations.CreateModel(\n            \"TestModel\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=255)),\n            ],\n            options={\"ordering\": [\"name\"]},\n            bases=(models.Model,),\n        )\n        operation.dependencies = [(\"app1\", \"0001_initial\"), (\"app2\", \"0002_second\")]\n        buff, imports = OperationWriter(operation, indentation=0).serialize()\n        self.assertEqual(imports, {\"from django.db import models\", \"from django.db import migrations\"})\n        self.assertIn(\n            \"dependencies=[\\n\"\n            \"    ('app1', '0001_initial'),\\n\"\n            \"    ('app2', '0002_second'),\\n\"\n            \"]\",\n            buff,\n        )\n", "    def test_serialize_custom_datetime_subclass(self):\n        class CustomDateTime(datetime.datetime):\n            pass\n\n        custom_dt = CustomDateTime(2023, 10, 11, 15, 30)\n        string, imports = MigrationWriter.serialize(custom_dt)\n        self.assertEqual(\n            string,\n            \"datetime.datetime(2023, 10, 11, 15, 30)\",\n        )\n        self.assertEqual(imports, {\"import datetime\"})\n        self.assertEqual(custom_dt, self.serialize_round_trip(custom_dt))\n", "    def test_migration_filename(self):\n        \"\"\"\n        Tests the filename property of MigrationWriter.\n        \"\"\"\n        migration = type(\n            \"Migration\",\n            (migrations.Migration,),\n            {\n                \"name\": \"0001_initial\",\n                \"app_label\": \"testapp\",\n            },\n        )\n        writer = MigrationWriter(migration)\n        self.assertEqual(writer.filename, \"0001_initial.py\")\n"], "sample_493": ["    def test_rawquery_clone_and_get_columns(self):\n        raw_query = RawQuery(\"SELECT * FROM aggregation_author\", \"default\")\n        cloned_query = raw_query.clone(\"default\")\n        self.assertEqual(raw_query.sql, cloned_query.sql)\n        self.assertEqual(raw_query.using, cloned_query.using)\n        self.assertEqual(raw_query.params, cloned_query.params)\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT * FROM aggregation_author\")\n            raw_query.cursor = cursor\n            columns = raw_query.get_columns()\n            expected_columns = [col[0] for col in cursor.description]\n            self.assertEqual(columns, expected_columns)\n", "    def test_rawquery_clone_and_chain(self):\n        raw_query = RawQuery(\"SELECT * FROM my_table\", using=\"default\", params=(1, 2, 3))\n        cloned_query = raw_query.clone(using=\"other\")\n        self.assertEqual(cloned_query.sql, raw_query.sql)\n        self.assertEqual(cloned_query.params, raw_query.params)\n        self.assertEqual(cloned_query.using, \"other\")\n\n        chained_query = raw_query.chain(using=\"chained\")\n        self.assertEqual(chained_query.sql, raw_query.sql)\n        self.assertEqual(chained_query.params, raw_query.params)\n        self.assertEqual(chained_query.using, \"chained\")\n", "    def test_annotate_over_subquery_with_f(self):\n        \"\"\"\n        Test to verify that annotation over subquery with F expressions\n        works correctly and improves coverage for subquery handling.\n        \"\"\"\n        latest_book_pubdate_qs = (\n            Book.objects.filter(publisher=OuterRef(\"pk\"))\n            .order_by(\"-pubdate\")\n            .values(\"pubdate\")[:1]\n        )\n        publisher_qs = Publisher.objects.annotate(\n            latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n            books_count=Count(\"book\"),\n        ).annotate(\n            latest_book_pubdate_plus_count=F(\"latest_book_pubdate\") + F(\"books_count\")\n        )\n\n        with self.assertNumQueries(1) as ctx:\n            list(publisher_qs)\n        sql = ctx[0][\"sql\"].lower()\n        # Ensure the subquery and F expressions are handled correctly\n        self.assertIn(\"latest_book_pubdate\", sql)\n        self.assertIn(\"books_count\", sql)\n        self.assertIn(\"latest_book_pubdate_plus_count\", sql)\n", "    def test_annotate_with_filtered_relation(self):\n        # Test annotate with FilteredRelation.\n        authors = Author.objects.annotate(\n            filtered_books=Count(\"book\", filter=Q(book__rating__gte=4))\n        ).order_by(\"name\")\n        self.assertQuerySetEqual(\n            authors,\n            [\n                (\"Adrian Holovaty\", 1),\n                (\"Brad Dayley\", 0),\n                (\"Jacob Kaplan-Moss\", 1),\n                (\"James Bennett\", 0),\n                (\"Jeffrey Forcier\", 0),\n                (\"Paul Bissex\", 0),\n                (\"Peter Norvig\", 2),\n                (\"Stuart Russell\", 1),\n                (\"Wesley J. Chun\", 0),\n            ],\n            lambda a: (a.name, a.filtered_books),\n        )\n", "    def test_raw_query_get_columns(self):\n        raw_query = RawQuery(\n            \"SELECT name, age FROM aggregation_author WHERE age > %s\", \"default\", [30]\n        )\n        raw_query._execute_query()\n        columns = raw_query.get_columns()\n        self.assertEqual(columns, [\"name\", \"age\"])\n"], "sample_494": ["    def test_serialize_custom_deconstructable(self):\n        class CustomDeconstructable:\n                return (\n                    \"custom_module.CustomDeconstructable\",\n                    [],\n                    {\"arg1\": 10, \"arg2\": \"test\"}\n                )\n\n        instance = CustomDeconstructable()\n        string, imports = MigrationWriter.serialize(instance)\n        self.assertEqual(\n            string,\n            \"custom_module.CustomDeconstructable(arg1=10, arg2='test')\"\n        )\n        self.assertEqual(imports, {\"import custom_module\"})\n", "    def test_serialize_deconstructable_object(self):\n        deconstructable_obj = DeconstructibleInstances()\n        serialized_value, imports = MigrationWriter.serialize(deconstructable_obj)\n        self.assertEqual(\n            serialized_value,\n            \"migrations.test_writer.DeconstructibleInstances()\"\n        )\n        self.assertIn(\"import migrations.test_writer\", imports)\n\n        # Ensure it can be deserialized correctly\n        deserialized_obj = self.safe_exec(\n            \"%s\\ntest_value_result = %s\" % (\"\\n\".join(imports), serialized_value)\n        )[\"test_value_result\"]\n        self.assertIsInstance(deserialized_obj, DeconstructibleInstances)\n", "    def test_serialize_function_type(self):\n            return \"test\"\n\n        self.assertSerializedResultEqual(\n            sample_function,\n            (\n                \"migrations.test_writer.sample_function\",\n                {\"import migrations.test_writer\"},\n            ),\n        )\n        self.serialize_round_trip(sample_function)\n\n        with self.assertRaisesMessage(ValueError, \"Cannot serialize function: lambda\"):\n            self.assertSerializedEqual(lambda x: x)\n\n        with self.assertRaisesMessage(\n            ValueError, \"Cannot serialize function <function\"\n        ):\n            self.serialize_round_trip(test_serialize_function_type)\n", "    def test_serialize_regex_flag(self):\n        regex = re.compile(r\"^\\w+$\", re.IGNORECASE | re.MULTILINE)\n        self.assertSerializedEqual(regex)\n        string, imports = MigrationWriter.serialize(regex)\n        self.assertEqual(\n            string,\n            \"re.compile('^\\\\\\\\w+$', re.RegexFlag['IGNORECASE'] | re.RegexFlag['MULTILINE'])\",\n        )\n        self.assertIn(\"import re\", imports)\n", "    def test_serialize_path_like_serializer(self):\n        class CustomPath:\n                return \"/custom/path\"\n\n        custom_path = CustomPath()\n        serialized_value = serializer_factory(custom_path).serialize()\n        self.assertEqual(serialized_value, (repr(os.fspath(custom_path)), {}))\n"], "sample_495": ["    def test_validate_number(self):\n        \"\"\"\n        Tests Paginator.validate_number() for various edge cases.\n        \"\"\"\n        paginator = Paginator([1, 2, 3, 4, 5], 2)\n\n        # Valid integers within range\n        self.assertEqual(paginator.validate_number(1), 1)\n        self.assertEqual(paginator.validate_number(2), 2)\n        self.assertEqual(paginator.validate_number(3), 3)\n\n        # Valid integer equal to num_pages\n        self.assertEqual(paginator.validate_number(3), 3)\n\n        # Invalid page number less than 1\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(0)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(-1)\n\n        # Invalid page number greater than num_pages\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(4)\n\n        # Invalid types\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number({})\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number([])\n\n        # Float that is an integer\n        self.assertEqual(paginator.validate_number(2.0), 2)\n\n        # Float that is not an integer\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number(2.5)\n", "    def test_page_invalid_index(self):\n        \"\"\"\n        Tests that attempting to access a Page with an invalid index raises the appropriate exceptions.\n        \"\"\"\n        paginator = Paginator([1, 2, 3], 2)\n        page = paginator.page(1)\n        \n        # Test accessing with a negative index\n        with self.assertRaises(IndexError):\n            page[-1]\n        \n        # Test accessing with an index out of range\n        with self.assertRaises(IndexError):\n            page[10]\n\n        # Test slicing with a negative start index\n        self.assertEqual(page[-2:], [1, 2])\n        \n        # Test slicing with an end index out of range\n        self.assertEqual(page[:10], [1, 2])\n", "    def test_page_methods(self):\n        \"\"\"\n        Test methods on the Page object to ensure they return correct values.\n        \"\"\"\n        paginator = Paginator([1, 2, 3, 4, 5], 2)\n        page1 = paginator.page(1)\n        page2 = paginator.page(2)\n        page3 = paginator.page(3)\n\n        # has_next and has_previous methods\n        self.assertTrue(page1.has_next())\n        self.assertFalse(page1.has_previous())\n        self.assertTrue(page2.has_next())\n        self.assertTrue(page2.has_previous())\n        self.assertFalse(page3.has_next())\n        self.assertTrue(page3.has_previous())\n\n        # has_other_pages method\n        self.assertTrue(page1.has_other_pages())\n        self.assertTrue(page2.has_other_pages())\n        self.assertTrue(page3.has_other_pages())\n\n        # next_page_number and previous_page_number methods\n        self.assertEqual(page1.next_page_number(), 2)\n        with self.assertRaises(InvalidPage):\n            page1.previous_page_number()\n        self.assertEqual(page2.next_page_number(), 3)\n        self.assertEqual(page2.previous_page_number(), 1)\n        with self.assertRaises(InvalidPage):\n            page3.next_page_number()\n        self.assertEqual(page3.previous_page_number(), 2)\n\n        # start_index and end_index methods\n        self.assertEqual(page1.start_index(), 1)\n        self.assertEqual(page1.end_index(), 2)\n        self.assertEqual(page2.start_index(), 3)\n        self.assertEqual(page2.end_index(), 4)\n        self.assertEqual(page3.start_index(), 5)\n        self.assertEqual(page3.end_index(), 5)\n", "    def test_validate_number(self):\n        paginator = Paginator([1, 2, 3, 4, 5], 2)\n        # Valid page numbers\n        self.assertEqual(paginator.validate_number(1), 1)\n        self.assertEqual(paginator.validate_number(2), 2)\n        # Invalid page numbers\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number('invalid')\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number(1.5)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(-1)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(0)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(4)\n", "    def test_page_object_list_caching(self):\n        \"\"\"\n        Tests that the Page object caches the object_list after initial conversion to a list.\n        \"\"\"\n        class CustomList:\n                self.data = data\n                self._converted = False\n\n                if not self._converted:\n                    self.data = list(self.data)\n                    self._converted = True\n                return iter(self.data)\n\n                return len(self.data)\n\n        object_list = CustomList(range(10))\n        paginator = Paginator(object_list, 5)\n        page = paginator.page(1)\n\n        # Ensure object_list is not converted initially.\n        self.assertFalse(object_list._converted)\n        # Access object_list which should trigger conversion.\n        _ = page.object_list\n        self.assertTrue(object_list._converted)\n        # Access object_list again to verify caching.\n        _ = page.object_list\n        self.assertTrue(object_list._converted)\n"], "sample_496": ["    def setUp(self):\n        self.write_settings('settings.py', apps=['django.contrib.auth', 'django.contrib.contenttypes'])\n", "    def test_system_check(self):\n        \"\"\"\n        Test that the system check framework is called when `requires_system_checks` is True.\n        \"\"\"\n        class Command(BaseCommand):\n            requires_system_checks = True\n\n                self.stdout.write(\"System check passed\")\n\n        command = Command(stdout=StringIO())\n        with mock.patch.object(command, 'check', return_value=[]):\n            call_command(command)\n            self.assertIn(\"System check passed\", command.stdout.getvalue())\n", "    def setUp(self):\n        self.write_settings('settings.py')\n", "    def test_check_migrations_no_migrations(self):\n        \"\"\"Test check_migrations with no pending migrations.\"\"\"\n        self.write_settings('settings.py')\n        self.addCleanup(self.remove_settings, 'settings.py')\n        out, err = self.run_manage(['runserver'])\n        self.assertNoOutput(out)\n        self.assertNotInOutput(err, \"unapplied migration(s)\")\n", "    def setUp(self):\n        self.write_settings('settings.py')\n"], "sample_497": ["def test_tick_label_position_update():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    tick = mticker.MaxNLocator(nbins=5)\n    ax.xaxis.set_major_locator(tick)\n    ax.xaxis.set_major_formatter(mticker.ScalarFormatter())\n\n    # Initial positions before update\n    initial_label_positions = [label.get_position() for label in ax.xaxis.get_ticklabels()]\n\n    # Update tick positions and check if the label positions change\n    ax.set_xlim(0, 20)\n    updated_label_positions = [label.get_position() for label in ax.xaxis.get_ticklabels()]\n\n    assert initial_label_positions != updated_label_positions\n", "def test_tick_contains():\n    fig, ax = plt.subplots()\n    xtick = XTick(ax, 0, '')\n    ytick = YTick(ax, 0, '')\n\n    # Test XTick contains method\n    event_x = matplotlib.backend_bases.MouseEvent('button_press_event', fig.canvas, 0, 0)\n    event_y = matplotlib.backend_bases.MouseEvent('button_press_event', fig.canvas, 10, 10)\n    assert xtick.contains(event_x) == (False, {})\n    assert ytick.contains(event_x) == (False, {})\n\n    # Test YTick contains method\n    assert xtick.contains(event_y) == (False, {})\n    assert ytick.contains(event_y) == (False, {})\n", "def test_tick_initialization():\n    fig, ax = plt.subplots()\n    major_tick = ax.xaxis.get_major_ticks()[0]\n    minor_tick = ax.xaxis.get_minor_ticks()[0]\n    \n    # Test the initialization parameters for major tick\n    assert major_tick.label1.get_visible()\n    assert not major_tick.label2.get_visible()\n    assert major_tick.tick1line.get_visible()\n    assert major_tick.tick2line.get_visible()\n    assert major_tick.gridline.get_visible() == rcParams['axes.grid'] and rcParams['axes.grid.which'] in ('both', 'major')\n    \n    # Test the initialization parameters for minor tick\n    assert not minor_tick.label1.get_visible()\n    assert not minor_tick.label2.get_visible()\n    assert minor_tick.tick1line.get_visible()\n    assert minor_tick.tick2line.get_visible()\n    assert minor_tick.gridline.get_visible() == rcParams['axes.grid'] and rcParams['axes.grid.which'] in ('both', 'minor')\n", "def test_tick_label_position():\n    fig, ax = plt.subplots()\n    ax.xaxis.set_major_locator(mticker.MaxNLocator(nbins=4))\n    ax.xaxis.set_minor_locator(mticker.AutoMinorLocator())\n\n    # Setting tick labels manually\n    ax.set_xticks([0.25, 0.75])\n    ax.set_xticklabels(['Label1', 'Label2'], minor=False)\n\n    major_labels = [tick.get_text() for tick in ax.xaxis.get_majorticklabels()]\n    assert major_labels == ['Label1', 'Label2']\n\n    ax.set_xticks([0.1, 0.5, 0.9], minor=True)\n    ax.set_xticklabels(['mLabel1', 'mLabel2', 'mLabel3'], minor=True)\n\n    minor_labels = [tick.get_text() for tick in ax.xaxis.get_minorticklabels()]\n    assert minor_labels == ['mLabel1', 'mLabel2', 'mLabel3']\n", "    def test_tick_initialization(self):\n        fig, ax = plt.subplots()\n        tick = ax.xaxis.get_major_ticks()[0]\n        assert tick.get_loc() == 0\n        assert tick.tick1line.get_visible() is True\n        assert tick.tick2line.get_visible() is False\n        assert tick.label1.get_visible() is True\n        assert tick.label2.get_visible() is False\n        assert tick.gridline.get_visible() is False\n"], "sample_498": ["def test_legend_custom_handler_map():\n    \"\"\"Test custom handler map for the legend.\"\"\"\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    line, = ax.plot(x, y, label=\"sin(x)\")\n    \n    class CustomHandler(legend_handler.HandlerBase):\n                           xdescent, ydescent, width, height, fontsize, trans):\n            return [plt.Line2D([xdescent, xdescent + width], \n                               [ydescent + height / 2, ydescent + height / 2], \n                               color='red')]\n\n    custom_handler_map = {line: CustomHandler()}\n    legend = ax.legend(handler_map=custom_handler_map)\n    \n    # Check that the custom handler is used\n    assert isinstance(legend.legendHandles[0], plt.Line2D)\n    assert legend.legendHandles[0].get_color() == 'red'\n", "def test_draggable_legend():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='Test Line')\n    leg = ax.legend(draggable=True)\n    \n    # Ensure the legend is initially draggable\n    assert leg.get_draggable() is True\n    \n    # Test dragging and updating the location\n    initial_loc = leg._loc\n    draggable = leg.set_draggable(True)\n    draggable.finalize_offset()\n    \n    # Check if the location has changed after dragging\n    assert leg._loc != initial_loc\n    \n    # Disable dragging and ensure it's no longer draggable\n    leg.set_draggable(False)\n    assert leg.get_draggable() is False\n", "def test_draggable_legend_update_bbox():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], label=\"line\")\n    leg = ax.legend(draggable=True, loc='upper left')\n\n    draggable_leg = leg.get_draggable()\n    draggable_leg.finalize_offset()\n\n    assert leg._loc == 'upper left'\n    assert leg.get_bbox_to_anchor() is not None\n\n    draggable_leg._update_bbox_to_anchor((0.5, 0.5))\n    updated_bbox = leg.get_bbox_to_anchor()\n    assert updated_bbox.x0 == 0.5\n    assert updated_bbox.y0 == 0.5\n", "def test_draggable_legend_update_bbox():\n    # Test the draggable functionality with update='bbox'\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], label='line')\n    leg = ax.legend(draggable=True, loc='upper right', update='bbox')\n    \n    # Simulate dragging the legend\n    draggable = leg._draggable\n    draggable.start(0, 0)\n    draggable.stop(100, 100)\n    \n    # Verify the bbox_to_anchor has been updated\n    bbox = leg.get_bbox_to_anchor()\n    assert bbox.x0 == 100\n    assert bbox.y0 == 100\n", "def test_draggable_legend_update_loc():\n    \"\"\"Test DraggableLegend with update='loc' parameter\"\"\"\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='data')\n    leg = ax.legend()\n    draggable = leg.set_draggable(state=True, update='loc')\n    \n    # Mock the get_loc_in_canvas to return a fixed point\n    with mock.patch.object(draggable, 'get_loc_in_canvas', return_value=(0.5, 0.5)):\n        draggable.finalize_offset()\n    \n    # Ensure the loc parameter is updated correctly\n    assert leg._loc == (0.5, 0.5)\n"], "sample_499": ["def test_legend_custom_handler_map():\n    # Test custom handler map functionality\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    line1, = ax.plot(x, np.sin(x), label='sin')\n    line2, = ax.plot(x, np.cos(x), label='cos')\n\n    # Create a custom handler map\n    custom_handler_map = {line1: mlegend.HandlerLine2D(numpoints=2)}\n\n    with mock.patch('matplotlib.legend.Legend') as Legend:\n        ax.legend(handler_map=custom_handler_map)\n    \n    Legend.assert_called_with(ax, [line1, line2], ['sin', 'cos'], handler_map=custom_handler_map)\n", "def test_draggable_legend_update():\n    fig, ax = plt.subplots()\n    line, = ax.plot(range(10), label='Line')\n    leg = ax.legend()\n    draggable_legend = leg.set_draggable(True, update='bbox')\n    assert draggable_legend is not None\n    draggable_legend.finalize_offset()\n    assert leg._update == 'bbox'\n    draggable_legend = leg.set_draggable(True, update='loc')\n    assert draggable_legend is not None\n    draggable_legend.finalize_offset()\n    assert leg._update == 'loc'\n", "def test_draggable_legend_update_loc():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], label=\"line\")\n    leg = ax.legend()\n    draggable = leg.set_draggable(True, update=\"loc\")\n    assert draggable is not None\n    assert draggable._update == \"loc\"\n    draggable.finalize_offset()\n    assert leg._loc == draggable.get_loc_in_canvas()\n", "def test_legend_bboxtoanchor():\n    # Test setting and getting bbox_to_anchor with different inputs.\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend()\n\n    # Test setting bbox_to_anchor with a tuple of 2 floats.\n    leg.set_bbox_to_anchor((0.5, 0.5))\n    assert np.allclose(leg.get_bbox_to_anchor().bounds, [0.5, 0.5, 0, 0])\n\n    # Test setting bbox_to_anchor with a tuple of 4 floats.\n    leg.set_bbox_to_anchor((0.1, 0.2, 0.3, 0.4))\n    assert np.allclose(leg.get_bbox_to_anchor().bounds, [0.1, 0.2, 0.3, 0.4])\n\n    # Test setting bbox_to_anchor with a Bbox instance.\n    bbox = mpl.transforms.Bbox.from_bounds(0.2, 0.3, 0.4, 0.5)\n    leg.set_bbox_to_anchor(bbox)\n    assert leg.get_bbox_to_anchor().bounds == bbox.bounds\n\n    # Test setting bbox_to_anchor to None.\n    leg.set_bbox_to_anchor(None)\n    assert leg.get_bbox_to_anchor() == ax.bbox\n\n    # Test with a different transform.\n    transform = mpl.transforms.Affine2D().scale(2.0)\n    leg.set_bbox_to_anchor((0.1, 0.2, 0.3, 0.4), transform=transform)\n    assert np.allclose(leg.get_bbox_to_anchor().bounds, [0.1, 0.2, 0.3, 0.4])\n    assert leg.get_bbox_to_anchor().get_transform().get_matrix() == transform.get_matrix()\n", "def test_draggable_legend():\n    # Test the DraggableLegend functionality\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], label=\"line\")\n    leg = ax.legend()\n\n    # Enable draggable legend\n    draggable = leg.set_draggable(True)\n    assert leg.get_draggable()\n    assert isinstance(draggable, mlegend.DraggableLegend)\n\n    # Check if the draggable functionality updates the legend location\n    initial_loc = leg._loc\n    draggable.finalize_offset()\n    assert leg._loc != initial_loc\n\n    # Disable draggable legend\n    leg.set_draggable(False)\n    assert not leg.get_draggable()\n"], "sample_500": ["def test_colorbar_set_alpha():\n    \"\"\"Test the set_alpha method of the Colorbar class\"\"\"\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]], cmap='viridis')\n    cbar = fig.colorbar(im, ax=ax)\n    \n    # Test setting alpha to a scalar\n    cbar.set_alpha(0.5)\n    assert cbar.alpha == 0.5\n    \n    # Test setting alpha to an array\n    alpha_array = np.array([[0.5, 0.7], [0.9, 1.0]])\n    cbar.set_alpha(alpha_array)\n    assert cbar.alpha is None  # Should be None when an array is set\n    assert np.array_equal(cbar.mappable.get_alpha(), alpha_array)\n    \n    fig.canvas.draw()\n", "def test_colorbar_update_normal():\n    # Test the update_normal method of the Colorbar class\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    im = ax.imshow(data, cmap='viridis')\n    cbar = fig.colorbar(im, ax=ax)\n\n    # Update the norm of the mappable and call update_normal\n    new_norm = mcolors.Normalize(vmin=0.2, vmax=0.8)\n    im.set_norm(new_norm)\n    cbar.update_normal(im)\n\n    # Assert that the Colorbar's norm is updated\n    assert cbar.norm == new_norm\n    fig.canvas.draw()  # Ensure that the changes are rendered correctly\n\n    # Check the updated ticks and ticklabels\n    ticks = cbar.ax.yaxis.get_majorticklocs()\n    assert ticks[0] >= 0.2 and ticks[-1] <= 0.8\n", "def test_colorbar_invalid_extend():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError):\n        Colorbar(ax, extend='invalid')\n", "def test_colorbar_set_ticks_method():\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable(norm=mcolors.Normalize(vmin=0, vmax=1), cmap='viridis')\n    cbar = fig.colorbar(sm, ax=ax)\n    cbar.set_ticks([0.2, 0.5, 0.8])\n    cbar.set_ticklabels(['Low', 'Medium', 'High'])\n    fig.canvas.draw()\n    tick_labels = [tick.get_text() for tick in cbar.ax.yaxis.get_ticklabels()]\n    assert tick_labels == ['Low', 'Medium', 'High']\n\n    # Test minor ticks\n    cbar.set_ticks([0.1, 0.3, 0.7, 0.9], minor=True)\n    cbar.set_ticklabels(['m1', 'm2', 'm3', 'm4'], minor=True)\n    fig.canvas.draw()\n    minor_tick_labels = [tick.get_text() for tick in cbar.ax.yaxis.get_ticklabels(minor=True)]\n    assert minor_tick_labels == ['m1', 'm2', 'm3', 'm4']\n", "def test_colorbar_boundaries_values():\n    \"\"\"Test setting boundaries and values for a colorbar.\"\"\"\n    fig, ax = plt.subplots()\n    cmap = cm.get_cmap(\"viridis\", 5)\n    boundaries = [0, 1, 2, 3, 4, 5]\n    values = [0.5, 1.5, 2.5, 3.5, 4.5]\n    norm = BoundaryNorm(boundaries, cmap.N)\n\n    # Create a colorbar with specified boundaries and values\n    Colorbar(ax, cmap=cmap, norm=norm, boundaries=boundaries, values=values)\n\n    # Check if boundaries and values are set correctly\n    cbar = ax.collections[0].colorbar\n    np.testing.assert_array_equal(cbar._boundaries, boundaries)\n    np.testing.assert_array_equal(cbar._values, values)\n\n    plt.close(fig)\n"], "sample_501": ["def test_draggable_legend():\n    # Test the DraggableLegend feature.\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend()\n    leg.set_draggable(True)\n\n    assert leg.get_draggable()\n    draggable = leg._draggable\n    assert draggable._update == 'loc'\n    assert draggable.legend == leg\n\n    # Simulate a drag event to check finalization\n    draggable.start_drag(0, 0)\n    draggable.dragging(10, 10)\n    draggable.end_drag(10, 10)\n\n    assert leg._loc == ('axes', (0.05, 0.95))  # Default loc is best, so move to (0.05, 0.95)\n", "def test_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    lines = ax.plot(range(10), label='test')\n    leg = ax.legend()\n    # Test setting bbox to anchor with a 4-tuple\n    leg.set_bbox_to_anchor((0.1, 0.1, 0.5, 0.5))\n    assert leg.get_bbox_to_anchor().bounds == (0.1, 0.1, 0.5, 0.5)\n    # Test setting bbox to anchor with a 2-tuple\n    leg.set_bbox_to_anchor((0.2, 0.2))\n    assert leg.get_bbox_to_anchor().bounds == (0.2, 0.2, 0, 0)\n    # Test setting bbox to anchor with a BboxBase instance\n    bbox = mtransforms.Bbox.from_bounds(0.3, 0.3, 0.5, 0.5)\n    leg.set_bbox_to_anchor(bbox)\n    assert leg.get_bbox_to_anchor().bounds == (0.3, 0.3, 0.5, 0.5)\n    # Test setting bbox to anchor to None\n    leg.set_bbox_to_anchor(None)\n    assert leg.get_bbox_to_anchor() == ax.bbox\n", "def test_draggable_legend_update_loc():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend()\n    draggable = leg.set_draggable(True, update='loc')\n    draggable.finalize_offset()\n    assert leg._update == 'loc'\n    assert leg._draggable is not None\n    assert leg._loc is not None\n    assert draggable.legend == leg\n\n", "def test_legend_draggable_update_loc():\n    fig, ax = plt.subplots()\n    line, = ax.plot(range(10), label='line')\n    leg = ax.legend()\n    draggable = leg.set_draggable(True, use_blit=False, update='loc')\n    assert draggable is not None\n    assert leg.get_draggable()\n    draggable.finalize_offset()\n    assert leg._update == 'loc'\n    assert isinstance(draggable, mlegend.DraggableLegend)\n    leg.set_draggable(False)\n    assert not leg.get_draggable()\n", "def test_legend_update_bbox_to_anchor():\n    # Test for updating the bbox_to_anchor when dragging the legend\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    ax.plot(x, x**2, label='quadratic')\n    leg = ax.legend(loc='upper left', bbox_to_anchor=(0.5, 0.5))\n    \n    # Simulate dragging the legend\n    draggable = leg.set_draggable(True, update='bbox')\n    draggable.finalize_offset()\n    \n    # Check if the bbox_to_anchor is updated correctly\n    bbox_to_anchor = leg.get_bbox_to_anchor().bounds\n    assert bbox_to_anchor[0] != 0.5 or bbox_to_anchor[1] != 0.5, \"bbox_to_anchor not updated properly\"\n"], "sample_502": ["def test_set_loglevel():\n    # Test setting log level to different levels.\n    plt.set_loglevel(\"info\")\n    assert logging.getLogger('matplotlib').getEffectiveLevel() == logging.INFO\n    \n    plt.set_loglevel(\"warning\")\n    assert logging.getLogger('matplotlib').getEffectiveLevel() == logging.WARNING\n\n    plt.set_loglevel(\"error\")\n    assert logging.getLogger('matplotlib').getEffectiveLevel() == logging.ERROR\n\n    plt.set_loglevel(\"debug\")\n    assert logging.getLogger('matplotlib').getEffectiveLevel() == logging.DEBUG\n\n    # Reset to default level (warning)\n    plt.set_loglevel(\"warning\")\n    assert logging.getLogger('matplotlib').getEffectiveLevel() == logging.WARNING\n", "def test_switch_backend():\n    initial_backend = mpl.get_backend()\n    try:\n        plt.switch_backend('agg')\n        assert mpl.get_backend() == 'agg'\n        plt.switch_backend('svg')\n        assert mpl.get_backend() == 'svg'\n    finally:\n        plt.switch_backend(initial_backend)\n", "def test_install_uninstall_repl_displayhook():\n    # Ensure that interactive mode is enabled and hooks are installed\n    plt.ion()\n    assert mpl.is_interactive()\n    plt.install_repl_displayhook()\n    assert plt._IP_REGISTERED is not None or plt._INSTALL_FIG_OBSERVER\n\n    # Uninstall the hooks and check the state\n    plt.uninstall_repl_displayhook()\n    assert plt._IP_REGISTERED is None\n    assert not plt._INSTALL_FIG_OBSERVER\n\n    # Ensure uninstalling twice does not raise an error\n    plt.uninstall_repl_displayhook()\n    assert plt._IP_REGISTERED is None\n    assert not plt._INSTALL_FIG_OBSERVER\n", "def test_draw_all():\n    fig1 = plt.figure()\n    fig2 = plt.figure()\n    ax1 = fig1.add_subplot(111)\n    ax2 = fig2.add_subplot(111)\n    ax1.plot([1, 2, 3], [1, 4, 9])\n    ax2.plot([1, 2, 3], [1, 2, 3])\n    \n    # Trigger draw_all to ensure all figures are drawn\n    plt.draw_all()\n    \n    assert fig1.stale == False\n    assert fig2.stale == False\n", "def test_xkcd_mode():\n    # Check that xkcd mode sets the appropriate rcParams.\n    with plt.xkcd():\n        assert rcParams['font.family'] == ['xkcd', 'xkcd Script', 'Humor Sans', 'Comic Neue', 'Comic Sans MS']\n        assert rcParams['font.size'] == 14.0\n        assert rcParams['path.sketch'] == (1, 100, 2)\n        assert rcParams['axes.linewidth'] == 1.5\n        assert rcParams['lines.linewidth'] == 2.0\n        assert rcParams['figure.facecolor'] == 'white'\n        assert rcParams['grid.linewidth'] == 0.0\n        assert rcParams['axes.grid'] == False\n        assert rcParams['axes.unicode_minus'] == False\n        assert rcParams['axes.edgecolor'] == 'black'\n        assert rcParams['xtick.major.size'] == 8\n        assert rcParams['xtick.major.width'] == 3\n        assert rcParams['ytick.major.size'] == 8\n        assert rcParams['ytick.major.width'] == 3\n\n    # Check that rcParams are restored after exiting xkcd mode.\n    assert rcParams['font.family'] != ['xkcd', 'xkcd Script', 'Humor Sans', 'Comic Neue', 'Comic Sans MS']\n    assert rcParams['font.size'] != 14.0\n    assert rcParams['path.sketch'] != (1, 100, 2)\n    assert rcParams['axes.linewidth'] != 1.5\n    assert rcParams['lines.linewidth'] != 2.0\n    assert rcParams['figure.facecolor'] != 'white'\n    assert rcParams['grid.linewidth'] != 0.0\n    assert rcParams['axes.grid'] != False\n    assert rcParams['axes.unicode_minus'] != False\n    assert rcParams['axes.edgecolor'] != 'black'\n    assert rcParams['xtick.major.size'] != 8\n    assert rcParams['xtick.major.width'] != 3\n    assert rcParams['ytick.major.size'] != 8\n    assert rcParams['ytick.major.width'] != 3\n"], "sample_503": ["def test_set_markevery_invalid():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1, 2], marker=\"o\")\n\n    with pytest.raises(ValueError, match=\"`markevery` is a tuple but its len is not 2\"):\n        line.set_markevery((1, 2, 3))\n\n    with pytest.raises(ValueError, match=\"`markevery` is a tuple with len 2 and second element is an int, but the first element is not an int\"):\n        line.set_markevery((1.0, 2))\n\n    with pytest.raises(ValueError, match=\"`markevery` is a tuple with len 2 and second element is a float, but the first element is not a float or an int\"):\n        line.set_markevery((1, 0.5))\n\n    with pytest.raises(ValueError, match=\"markevery is specified relative to the axes size, but the line does not have a Axes as parent\"):\n        mlines.Line2D([0, 1], [0, 1], marker=\"o\", markevery=(0.5, 0.1))\n    \n    with pytest.raises(ValueError, match=\"markevery=.* is not a recognized value\"):\n        line.set_markevery(\"invalid\")\n", "def test_get_transformed_path():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 10)\n    y = np.sin(x)\n    line = mlines.Line2D(x, y)\n    ax.add_line(line)\n    fig.canvas.draw()\n    \n    tpath = line._get_transformed_path()\n    assert tpath is not None\n    assert isinstance(tpath, mtransforms.TransformedPath)\n    assert tpath.get_path().vertices.shape == (10, 2)\n", "def test_invalid_markevery_values():\n    line = mlines.Line2D([0, 1, 2], [0, 1, 2])\n    invalid_values = [\n        -1,\n        (0, -1),\n        (0.5, -0.1),\n        (0.5, 1, 2),  # tuple of length not 2\n        \"invalid_type\",\n        [1, 2, \"invalid\"],\n        slice(1, 2, -1)  # negative step in slice\n    ]\n    for val in invalid_values:\n        with pytest.raises(ValueError):\n            line.set_markevery(val)\n", "def test_segment_hits_single_point():\n    \"\"\"\n    Test the segment_hits function for a single point edge case.\n    \"\"\"\n    cx, cy = 0, 0\n    x, y = np.array([0]), np.array([0])\n    radius = 1\n    assert_array_equal(mlines.segment_hits(cx, cy, x, y, radius), [0])\n\n    cx, cy = 1, 1\n    x, y = np.array([0]), np.array([0])\n    radius = 1.5\n    assert_array_equal(mlines.segment_hits(cx, cy, x, y, radius), [0])\n\n    cx, cy = 2, 2\n    x, y = np.array([0]), np.array([0])\n    radius = 1.5\n    assert_array_equal(mlines.segment_hits(cx, cy, x, y, radius), [])\n", "def test_contains_method():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1, 2], [2, 1, 0], picker=True)\n    fig.canvas.draw()\n\n    # Check if point (1, 1) is on the line\n    mouse_event = SimpleNamespace(x=fig.bbox.width // 2, y=fig.bbox.height // 2)\n    found, indices = line.contains(mouse_event)\n    assert found\n    assert_array_equal(indices['ind'], [1])\n\n    # Check if point (0, 0) is not on the line\n    mouse_event = SimpleNamespace(x=0, y=0)\n    found, indices = line.contains(mouse_event)\n    assert not found\n"], "sample_504": ["def test_colorbar_custom_boundaries_values():\n    \"\"\"\n    Test creating a colorbar with custom boundaries and values, ensuring that\n    the colorbar is drawn correctly with the specified parameters.\n    \"\"\"\n    fig, ax = plt.subplots()\n    cmap = cm.get_cmap(\"viridis\", lut=6)\n    norm = BoundaryNorm(boundaries=[0, 1, 2, 3, 4, 5, 6], ncolors=cmap.N)\n    boundaries = [0, 1, 2, 3, 4, 5, 6]\n    values = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]\n    \n    # Creating the colorbar with custom boundaries and values\n    cbar = Colorbar(ax, cmap=cmap, norm=norm, boundaries=boundaries, values=values, orientation='vertical')\n    \n    # Verify colorbar boundaries and values\n    np.testing.assert_array_equal(cbar._boundaries, boundaries)\n    np.testing.assert_array_equal(cbar._values, values)\n    assert cbar.orientation == 'vertical'\n    \n    # Draw the figure to ensure the colorbar is rendered without errors\n    fig.canvas.draw()\n", "def test_colorbar_ticklocation():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    \n    # Test colorbar with 'auto' ticklocation\n    cbar_auto = fig.colorbar(im, ticklocation='auto')\n    assert cbar_auto.ticklocation == 'right'\n\n    # Test colorbar with 'left' ticklocation\n    cbar_left = fig.colorbar(im, ticklocation='left')\n    assert cbar_left.ticklocation == 'left'\n    assert cbar_left.ax.yaxis.get_ticks_position() == 'left'\n\n    # Test colorbar with 'right' ticklocation\n    cbar_right = fig.colorbar(im, ticklocation='right')\n    assert cbar_right.ticklocation == 'right'\n    assert cbar_right.ax.yaxis.get_ticks_position() == 'right'\n\n    # Test colorbar with 'top' ticklocation\n    cbar_top = fig.colorbar(im, orientation='horizontal', ticklocation='top')\n    assert cbar_top.ticklocation == 'top'\n    assert cbar_top.ax.xaxis.get_ticks_position() == 'top'\n\n    # Test colorbar with 'bottom' ticklocation\n    cbar_bottom = fig.colorbar(im, orientation='horizontal', ticklocation='bottom')\n    assert cbar_bottom.ticklocation == 'bottom'\n    assert cbar_bottom.ax.xaxis.get_ticks_position() == 'bottom'\n", "def test_colorbar_set_alpha():\n    \"\"\"\n    Test the set_alpha method of the Colorbar class to ensure proper setting of transparency.\n    \"\"\"\n    fig, ax = plt.subplots()\n    mappable = cm.ScalarMappable(norm=Normalize(), cmap='viridis')\n    cb = fig.colorbar(mappable, ax=ax)\n\n    # Test setting alpha to a scalar value\n    cb.set_alpha(0.5)\n    assert cb.alpha == 0.5\n\n    # Test setting alpha to an array (should set alpha to None)\n    alpha_array = np.array([0.2, 0.5, 0.8])\n    cb.set_alpha(alpha_array)\n    assert cb.alpha is None\n\n    # Test setting alpha back to a scalar\n    cb.set_alpha(1.0)\n    assert cb.alpha == 1.0\n\n    plt.close(fig)\n", "def test_colorbar_set_alpha():\n    \"\"\"\n    Test the set_alpha method of the Colorbar class.\n    \"\"\"\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im)\n\n    # Test with a scalar alpha value.\n    cbar.set_alpha(0.5)\n    assert cbar.alpha == 0.5\n\n    # Test with an array alpha value.\n    alpha_array = np.array([0.2, 0.4, 0.6, 0.8])\n    cbar.set_alpha(alpha_array)\n    assert cbar.alpha is None  # Should be set to None if an array is provided.\n\n    # Test with no alpha value (default).\n    cbar.set_alpha(None)\n    assert cbar.alpha is None\n", "def test_colorbar_set_alpha():\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable(norm=mcolors.Normalize(), cmap='viridis')\n    cb = fig.colorbar(sm)\n    \n    # Test setting alpha as a float\n    cb.set_alpha(0.5)\n    assert cb.alpha == 0.5\n\n    # Test setting alpha as an array\n    alpha_array = np.array([0.5, 0.6, 0.7])\n    cb.set_alpha(alpha_array)\n    assert cb.alpha is None\n    assert np.array_equal(cb.mappable.get_alpha(), alpha_array)\n\n    # Test setting alpha back to a float\n    cb.set_alpha(0.8)\n    assert cb.alpha == 0.8\n    assert cb.mappable.get_alpha() is None  # Alpha is not an array anymore\n"], "sample_505": ["def test_timezone_conversion():\n    # Test that num2date correctly handles timezone-aware datetime objects.\n    tz = dateutil.tz.gettz('Europe/Berlin')\n    d1 = datetime.datetime(2023, 3, 27, 12, 0, tzinfo=mdates.UTC)\n    d2 = datetime.datetime(2023, 3, 27, 14, 0, tzinfo=tz)\n\n    num1 = mdates.date2num(d1)\n    num2 = mdates.date2num(d2)\n\n    assert num1 == num2, f\"Expected {num1} to equal {num2} after timezone conversion\"\n\n    date1 = mdates.num2date(num1, tz=mdates.UTC)\n    date2 = mdates.num2date(num2, tz=tz)\n\n    assert date1 == date2, f\"Expected {date1} to equal {date2} after num2date conversion\"\n", "def test_drange_boundary_cases():\n    # Test drange with exact boundary case (start == end)\n    start = datetime.datetime(2023, 1, 1)\n    end = start\n    delta = datetime.timedelta(days=1)\n    assert len(mdates.drange(start, end, delta)) == 0\n\n    # Test drange with end just slightly after start\n    end = start + datetime.timedelta(microseconds=1)\n    result = mdates.drange(start, end, delta)\n    assert len(result) == 1\n    assert mdates.num2date(result[0]) == start\n\n    # Test drange with very small delta\n    delta = datetime.timedelta(microseconds=1)\n    result = mdates.drange(start, end, delta)\n    assert len(result) == 2\n    assert mdates.num2date(result[0]) == start\n    assert mdates.num2date(result[1]) == end\n", "def test_drangedate():\n    # Test the drange function with datetime.date objects\n    start = datetime.date(2011, 1, 1)\n    end = datetime.date(2011, 1, 2)\n    delta = datetime.timedelta(hours=1)\n    expected_length = 24  # 24 hours in the day\n    drange_result = mdates.drange(start, end, delta)\n    assert len(drange_result) == expected_length\n\n    # Verify the last element in the range is one hour before the end date\n    assert mdates.num2date(drange_result[-1]) == datetime.datetime(2011, 1, 1, 23, 0)\n", "def test_set_get_epoch():\n    # Reset epoch to default and ensure it's '1970-01-01T00:00:00'\n    mdates._reset_epoch_test_example()\n    assert mdates.get_epoch() == '1970-01-01T00:00:00'\n\n    # Change epoch to '0000-12-31' and verify\n    mdates.set_epoch('0000-12-31')\n    assert mdates.get_epoch() == '0000-12-31'\n\n    # Attempt to change epoch again (should raise RuntimeError)\n    with pytest.raises(RuntimeError):\n        mdates.set_epoch('1970-01-01')\n\n    # Reset epoch for further tests\n    mdates._reset_epoch_test_example()\n    assert mdates.get_epoch() == '1970-01-01T00:00:00'\n", "def test_custom_date_formatter():\n    # Test custom date formatting using DateFormatter class\n    d = datetime.datetime(2023, 10, 1, 12, 30)\n    d_num = mdates.date2num(d)\n    fmt = '%Y-%m-%d %H:%M:%S'\n    \n    formatter = mdates.DateFormatter(fmt)\n    formatted_date = formatter(d_num)\n    expected_date = d.strftime(fmt)\n    \n    assert formatted_date == expected_date\n\n    # Test custom date formatting using ConciseDateFormatter class\n    locator = mdates.AutoDateLocator()\n    concise_formatter = mdates.ConciseDateFormatter(locator)\n    formatted_date = concise_formatter(d_num)\n    expected_date = d.strftime('%Y')\n    \n    assert formatted_date == expected_date\n"], "sample_506": ["def test_spine_set_get_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    \n    # Test setting and getting bounds\n    spine.set_bounds(0.1, 0.9)\n    assert spine.get_bounds() == (0.1, 0.9)\n    \n    # Test setting and getting bounds with a tuple\n    spine.set_bounds((0.2, 0.8))\n    assert spine.get_bounds() == (0.2, 0.8)\n    \n    # Test setting bounds with one None value\n    spine.set_bounds(0.3, None)\n    assert spine.get_bounds() == (0.3, 0.8)\n    spine.set_bounds(None, 0.7)\n    assert spine.get_bounds() == (0.3, 0.7)\n    \n    # Test setting bounds with invalid spine type\n    spine.spine_type = 'circle'\n    with pytest.raises(ValueError, match='set_bounds.*circular spines'):\n        spine.set_bounds(0.1, 0.9)\n", "def test_spine_set_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines.left\n    spine.set_bounds(0.1, 0.9)\n    assert spine.get_bounds() == (0.1, 0.9)\n\n    spine.set_bounds(None, 0.8)\n    assert spine.get_bounds() == (0.1, 0.8)\n\n    spine.set_bounds(0.2, None)\n    assert spine.get_bounds() == (0.2, 0.8)\n\n    with pytest.raises(ValueError, match='set_bounds\\(\\) method incompatible with circular spines'):\n        circular_spine = ax.spines.circular_spine(ax, (0.5, 0.5), 1)\n        circular_spine.set_bounds(0.1, 0.9)\n", "def test_spine_set_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    \n    # Initial bounds should be None\n    assert spine.get_bounds() is None\n    \n    # Set bounds and check\n    spine.set_bounds(0.1, 0.9)\n    assert spine.get_bounds() == (0.1, 0.9)\n    \n    # Check bounds with one None value\n    spine.set_bounds(None, 0.8)\n    assert spine.get_bounds() == (0.1, 0.8)\n    \n    spine.set_bounds(0.2, None)\n    assert spine.get_bounds() == (0.2, 0.8)\n    \n    # Check if setting bounds to None resets to initial\n    spine.set_bounds(None, None)\n    assert spine.get_bounds() == (0.2, 0.8)  # last valid bounds\n    \n    # Test ValueError for circular spine\n    circular_spine = ax.spines['left']\n    circular_spine.set_patch_circle((0.5, 0.5), 0.5)\n    with pytest.raises(ValueError, match='set_bounds() method incompatible with circular spines'):\n        circular_spine.set_bounds(0.1, 0.9)\n", "def test_spine_set_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    \n    # Set bounds for a linear spine\n    spine.set_bounds(1, 3)\n    assert spine.get_bounds() == (1, 3), \"Failed to set bounds for linear spine\"\n    \n    # Verify bounds influence on path\n    low, high = spine.get_bounds()\n    v1 = spine.get_path().vertices\n    assert v1[0, 1] == low, \"Low bound not set correctly\"\n    assert v1[1, 1] == high, \"High bound not set correctly\"\n    \n    # Ensure ValueError raised for circular spine\n    with pytest.raises(ValueError, match='set_bounds() method incompatible with circular spines'):\n        spine_circular = ax.spines['left']\n        spine_circular.set_patch_circle((0.5, 0.5), 0.5)\n        spine_circular.set_bounds(0, 1)\n", "def test_spine_set_get_bounds():\n    \"\"\"Test setting and getting spine bounds.\"\"\"\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n\n    # Test default bounds (None)\n    assert spine.get_bounds() is None\n\n    # Set and get new bounds\n    spine.set_bounds(0, 1)\n    assert spine.get_bounds() == (0, 1)\n\n    # Update only one bound\n    spine.set_bounds(None, 2)\n    assert spine.get_bounds() == (0, 2)\n\n    spine.set_bounds(1, None)\n    assert spine.get_bounds() == (1, 2)\n\n    # Test setting bounds for circular spine raises ValueError\n    circular_spine = Spine.circular_spine(ax, (0.5, 0.5), 1)\n    with pytest.raises(ValueError, match='set_bounds.*circular spines'):\n        circular_spine.set_bounds(0, 1)\n"], "sample_507": ["def test_str_category_formatter_format_ticks():\n    unit = cat.UnitData([\"a\", \"b\", \"c\"])\n    formatter = cat.StrCategoryFormatter(unit._mapping)\n    assert formatter.format_ticks([0, 1, 2]) == [\"a\", \"b\", \"c\"]\n    assert formatter.format_ticks([0.5, 1.5, 2.5]) == [\"\", \"\", \"\"]\n", "def test_StrCategoryFormatter_bytes():\n    byte_data = [b'apple', b'banana', b'cherry']\n    unit = cat.UnitData(byte_data)\n    formatter = cat.StrCategoryFormatter(unit._mapping)\n    \n    for i, d in enumerate(byte_data):\n        assert formatter(i) == d.decode('utf-8')\n\n    # Check that formatting a value outside the range returns an empty string\n    assert formatter(len(byte_data)) == \"\"\n", "def test_default_units_with_bytes():\n    axis = FakeAxis(None)\n    data = [b'apple', b'banana', b'cherry']\n    units = cat.StrCategoryConverter.default_units(data, axis)\n    assert isinstance(units, cat.UnitData)\n    assert list(units._mapping.keys()) == [b'apple', b'banana', b'cherry']\n    assert list(units._mapping.values()) == [0, 1, 2]\n", "def test_UnitData_str_is_convertible():\n    \"\"\"Test the _str_is_convertible method of UnitData.\"\"\"\n    unit_data = cat.UnitData()\n\n    # Test with a string that can be converted to a float\n    assert unit_data._str_is_convertible(\"3.14\") is True\n\n    # Test with a string that can be parsed as a date\n    assert unit_data._str_is_convertible(\"2023-10-05\") is True\n\n    # Test with a string that cannot be converted to a float or parsed as a date\n    assert unit_data._str_is_convertible(\"hello world\") is False\n\n    # Test with an empty string\n    assert unit_data._str_is_convertible(\"\") is False\n\n    # Test with a string that has mixed characters\n    assert unit_data._str_is_convertible(\"3.14hello\") is False\n", "def test_StrCategoryConverter_validate_unit():\n    \"\"\"\n    Test the _validate_unit method of StrCategoryConverter.\n    \"\"\"\n    valid_unit = cat.UnitData([\"apple\", \"banana\", \"cherry\"])\n    invalid_unit = {\"apple\": 0, \"banana\": 1, \"cherry\": 2}  # Missing _mapping attribute\n\n    # Validate should pass for valid unit\n    try:\n        cat.StrCategoryConverter._validate_unit(valid_unit)\n    except ValueError:\n        pytest.fail(\"Unexpected ValueError for valid unit\")\n\n    # Validate should fail for invalid unit\n    with pytest.raises(ValueError, match=r'Provided unit.*is not valid for a categorical converter.*'):\n        cat.StrCategoryConverter._validate_unit(invalid_unit)\n"], "sample_508": ["def test_set_clip_path_invalid_arguments():\n    art = martist.Artist()\n    from matplotlib.patches import Rectangle\n\n    with pytest.raises(TypeError, match=\"Invalid arguments to set_clip_path\"):\n        art.set_clip_path(\"invalid_path\", \"invalid_transform\")\n    \n    with pytest.raises(TypeError, match=\"Invalid arguments to set_clip_path\"):\n        art.set_clip_path(Rectangle((0, 0), 1, 1), \"invalid_transform\")\n", "def test_allow_rasterization_decorator():\n    class TestArtist(martist.Artist):\n            super().__init__()\n            self._rasterized = False\n            self._agg_filter = None\n\n        @allow_rasterization\n            return \"drawn\"\n\n            return self._rasterized\n\n            return self._agg_filter\n\n    artist = TestArtist()\n    renderer = type('Renderer', (), {\n        '_raster_depth': 0,\n        '_rasterizing': False,\n        'start_rasterizing': lambda: None,\n        'stop_rasterizing': lambda: None,\n        'start_filter': lambda: None,\n        'stop_filter': lambda x: None\n    })()\n\n    assert artist.draw(renderer) == \"drawn\"\n    artist._rasterized = True\n    assert artist.draw(renderer) == \"drawn\"\n    artist._agg_filter = lambda: None\n    assert artist.draw(renderer) == \"drawn\"\n    artist._rasterized = False\n    assert artist.draw(renderer) == \"drawn\"\n", "def test_set_transform():\n    art = martist.Artist()\n    trans = mtransforms.Affine2D().rotate_deg(45)\n    art.set_transform(trans)\n    assert art.get_transform() == trans\n    assert art.is_transform_set()\n    art.set_transform(None)\n    assert not art.is_transform_set()\n    assert isinstance(art.get_transform(), mtransforms.IdentityTransform)\n", "def test_artist_get_set_stale():\n    art = martist.Artist()\n\n    # Initial state should be stale\n    assert art.stale\n\n    # Set stale to False and check\n    art.stale = False\n    assert not art.stale\n\n    # Set stale to True and check\n    art.stale = True\n    assert art.stale\n\n    # Change a property and check if it sets stale to True\n    art.set_alpha(0.5)\n    assert art.stale\n\n    # Set stale_callback and check if it's called\n        stale_callback.called = True\n        stale_callback.obj = obj\n        stale_callback.val = val\n\n    stale_callback.called = False\n    art.stale_callback = stale_callback\n\n    art.stale = True\n    assert stale_callback.called\n    assert stale_callback.obj is art\n    assert stale_callback.val is True\n", "def test_set_properties():\n    # Test setting multiple properties at once\n    art = martist.Artist()\n    art.set(alpha=0.5, visible=False, animated=True, label=\"test_label\")\n    assert art.get_alpha() == 0.5\n    assert not art.get_visible()\n    assert art.get_animated()\n    assert art.get_label() == \"test_label\"\n\n    # Test setting properties with invalid values\n    with pytest.raises(ValueError, match=\"outside 0-1 range\"):\n        art.set(alpha=1.5)\n    with pytest.raises(TypeError, match=\"alpha must be numeric or None\"):\n        art.set(alpha=\"string\")\n    with pytest.raises(ValueError, match=\"invalid property\"):\n        art.set(non_existent_property=1)\n\n    # Test updating properties using update method\n    art.update({\"alpha\": 0.8, \"visible\": True})\n    assert art.get_alpha() == 0.8\n    assert art.get_visible()\n"], "sample_509": ["def test_auto_date_locator_interval_multiples():\n    # Test AutoDateLocator with different interval multiples settings\n    d1 = datetime.datetime(2000, 1, 1)\n    d2 = datetime.datetime(2000, 1, 10)\n\n    locator = mdates.AutoDateLocator(interval_multiples=False)\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(mdates.date2num(d1), mdates.date2num(d2))\n    intervals_non_multiple = locator()\n\n    locator = mdates.AutoDateLocator(interval_multiples=True)\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(mdates.date2num(d1), mdates.date2num(d2))\n    intervals_multiple = locator()\n\n    assert len(intervals_non_multiple) <= len(intervals_multiple)\n", "def test_set_epoch():\n    mdates._reset_epoch_test_example()\n    mdates.set_epoch('1970-01-01T00:00:00')\n    assert mdates.get_epoch() == '1970-01-01T00:00:00'\n    with pytest.raises(RuntimeError, match='set_epoch must be called before dates plotted.'):\n        mdates.set_epoch('2000-01-01T00:00:00')\n    mdates._reset_epoch_test_example()\n    mdates.set_epoch('0000-12-31')\n    assert mdates.get_epoch() == '0000-12-31'\n    mdates._reset_epoch_test_example()\n", "def test_set_epoch():\n    mdates._reset_epoch_test_example()\n    # Test setting epoch to a valid date\n    mdates.set_epoch('2000-01-01')\n    assert mdates.get_epoch() == '2000-01-01'\n    # Test setting epoch again should raise RuntimeError\n    with pytest.raises(RuntimeError):\n        mdates.set_epoch('2010-01-01')\n    # Reset epoch and test setting to default\n    mdates._reset_epoch_test_example()\n    assert mdates.get_epoch() == '1970-01-01T00:00:00'\n    # Test setting epoch to an invalid date should raise ValueError\n    with pytest.raises(ValueError, match=\"is not a valid UTC date\"):\n        mdates.set_epoch('invalid-date')\n    # Test setting epoch to a valid datetime string\n    mdates._reset_epoch_test_example()\n    mdates.set_epoch('1970-01-01T01:00:00')\n    assert mdates.get_epoch() == '1970-01-01T01:00:00'\n    # Reset epoch after tests\n    mdates._reset_epoch_test_example()\n", "def test_num2date():\n    dates = [19002.0, 19033.0, 19064.0]  # Example Matplotlib date numbers\n    expected = [datetime.datetime(2022, 1, 10, tzinfo=mdates.UTC),\n                datetime.datetime(2022, 2, 10, tzinfo=mdates.UTC),\n                datetime.datetime(2022, 3, 10, tzinfo=mdates.UTC)]\n    result = mdates.num2date(dates, tz=mdates.UTC)\n    assert result == expected\n\n    # Testing with a single date number\n    single_date = 19002.0\n    expected_single = datetime.datetime(2022, 1, 10, tzinfo=mdates.UTC)\n    result_single = mdates.num2date(single_date, tz=mdates.UTC)\n    assert result_single == expected_single\n", "def test_set_and_get_epoch():\n    # Test setting and getting the epoch\n    mdates._reset_epoch_test_example()\n    epoch = '2000-01-01T00:00:00'\n    mdates.set_epoch(epoch)\n    assert mdates.get_epoch() == epoch\n\n    with pytest.raises(RuntimeError):\n        mdates.set_epoch('1990-01-01T00:00:00')\n    \n    mdates._reset_epoch_test_example()\n    default_epoch = '1970-01-01T00:00:00'\n    assert mdates.get_epoch() == default_epoch\n\n    # Test resetting epoch back to default\n    mdates.set_epoch(default_epoch)\n    assert mdates.get_epoch() == default_epoch\n\n    mdates._reset_epoch_test_example()\n"], "sample_510": ["def test_figure_creation():\n    fig1 = plt.figure()\n    fig2 = plt.figure()\n    assert fig1 != fig2, \"Each call to plt.figure() should create a new figure.\"\n\n    fig3 = plt.figure(num=1)\n    assert fig3 == fig1, \"plt.figure(num=1) should return the figure with number 1.\"\n\n    fig4 = plt.figure(num=2)\n    assert fig4 == fig2, \"plt.figure(num=2) should return the figure with number 2.\"\n\n    fig5 = plt.figure(num=\"Test Figure\")\n    assert fig5.get_label() == \"Test Figure\", \"Figure label should be set correctly.\"\n    assert fig5 == plt.figure(\"Test Figure\"), \"plt.figure('Test Figure') should return the figure with that label.\"\n\n    fig6 = plt.figure(figsize=(8, 6), dpi=200, facecolor='red', edgecolor='blue', frameon=False)\n    assert fig6.get_size_inches().tolist() == [8, 6], \"Figure size should be set correctly.\"\n    assert fig6.dpi == 200, \"Figure DPI should be set correctly.\"\n    assert fig6.get_facecolor() == 'red', \"Figure facecolor should be set correctly.\"\n    assert fig6.get_edgecolor() == 'blue', \"Figure edgecolor should be set correctly.\"\n    assert fig6.get_frameon() == False, \"Figure frameon should be set correctly.\"\n", "def test_figimage():\n    np.random.seed(0)\n    data = np.random.random((10, 10))\n    fig = plt.figure()\n    img = plt.figimage(data)\n    assert img.get_array().shape == (10, 10)\n    assert img.get_extent() == (0, 10, 0, 10)\n\n    img2 = plt.figimage(data, xo=100, yo=100)\n    assert img2.get_extent() == (100, 110, 100, 110)\n\n    img3 = plt.figimage(data, alpha=0.5)\n    assert img3.get_alpha() == 0.5\n\n    img4 = plt.figimage(data, norm=plt.Normalize(0, 1))\n    assert img4.norm.vmax == 1\n\n    img5 = plt.figimage(data, cmap=\"viridis\")\n    assert img5.cmap.name == \"viridis\"\n", "def test_switch_backend():\n    original_backend = mpl.get_backend()\n\n    # Test switching to a valid backend\n    new_backend = 'agg'\n    plt.switch_backend(new_backend)\n    assert mpl.get_backend().lower() == new_backend\n\n    # Test if switching to an invalid backend raises ImportError\n    with pytest.raises(ImportError):\n        plt.switch_backend('invalid_backend')\n\n    # Restore the original backend to avoid side effects on other tests\n    plt.switch_backend(original_backend)\n", "def test_switch_backend():\n    plt.switch_backend(\"agg\")\n    assert mpl.get_backend() == \"agg\"\n    with pytest.raises(ImportError):\n        plt.switch_backend(\"nonexistentbackend\")\n", "def test_figure_kwargs():\n    fig = plt.figure(num=42, figsize=(8, 6), dpi=150, facecolor='lightgray', edgecolor='blue', frameon=False)\n    assert fig.number == 42\n    assert fig.get_size_inches().tolist() == [8, 6]\n    assert fig.get_dpi() == 150\n    assert fig.get_facecolor() == 'lightgray'\n    assert fig.get_edgecolor() == 'blue'\n    assert not fig.get_frameon()\n\n    # Ensure that creating a figure with the same num reuses the same figure.\n    fig2 = plt.figure(num=42)\n    assert fig is fig2\n\n    # Ensure that setting clear=True clears the figure\n    fig2 = plt.figure(num=42, clear=True)\n    assert len(fig2.get_axes()) == 0\n"], "sample_511": ["def test_polar_plot_creation():\n    # Test that creating a polar plot sets the correct projection type\n    plt.figure()\n    ax = plt.axes(projection='polar')\n    assert isinstance(ax, PolarAxes)\n    assert ax.name == 'polar'\n    \n    # Test that polar plots can plot data correctly\n    theta = np.linspace(0, 2*np.pi, 100)\n    r = np.abs(np.sin(theta))\n    ax.plot(theta, r)\n    assert len(ax.lines) == 1  # One line should be plotted\n    assert np.all(ax.lines[0].get_xdata() == theta)\n    assert np.all(ax.lines[0].get_ydata() == r)\n", "def test_set_cmap():\n    plt.figure()\n    data = np.random.rand(10, 10)\n    plt.imshow(data)\n    assert plt.gci().get_cmap().name == 'viridis'  # Default colormap\n    plt.set_cmap('gray')\n    assert plt.gci().get_cmap().name == 'gray'\n", "def test_autumn_cmap():\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    cax = ax.imshow(data)\n    plt.autumn()\n    assert cax.get_cmap().name == 'autumn'\n", "def test_show_block_parameter():\n    plt.figure()\n    with pytest.raises(TypeError, match=\"show() got an unexpected keyword argument 'invalid_param'\"):\n        plt.show(invalid_param=True)\n        \n    # Test that 'block' parameter works without error\n    plt.figure()\n    plt.plot([1, 2, 3], [4, 5, 6])\n    plt.show(block=False)  # This should not block and should not raise an error\n    plt.close()\n", "def test_subplot_tool():\n    fig = plt.figure()\n    assert plt.subplot_tool(fig) is not None\n    plt.close(fig)\n"], "sample_512": ["def test_subplots_adjust():\n    # Test subplots_adjust by adjusting various parameters\n    fig, axs = plt.subplots(2, 2)\n    plt.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2, wspace=0.3, hspace=0.3)\n\n    for ax in axs.flat:\n        assert 0.2 <= ax.get_position().x0 <= 0.8\n        assert 0.2 <= ax.get_position().y0 <= 0.8\n        assert 0.2 <= ax.get_position().x1 <= 0.8\n        assert 0.2 <= ax.get_position().y1 <= 0.8\n\n    # Test that subplots_adjust maintains the figure layout with no arguments\n    initial_layout = [ax.get_position() for ax in axs.flat]\n    plt.subplots_adjust()\n    adjusted_layout = [ax.get_position() for ax in axs.flat]\n    for init_pos, adj_pos in zip(initial_layout, adjusted_layout):\n        np.testing.assert_almost_equal(init_pos.bounds, adj_pos.bounds)\n", "def test_figure_label():\n    plt.figure(\"test_fig\")\n    assert plt.gcf().get_label() == \"test_fig\"\n    plt.figure(1)\n    plt.figure(\"test_fig_2\")\n    assert plt.gcf().get_label() == \"test_fig_2\"\n    plt.close(\"test_fig\")\n    assert not plt.fignum_exists(\"test_fig\")\n", "def test_switch_backend():\n    plt.switch_backend(\"agg\")\n    assert mpl.get_backend() == \"agg\"\n    with pytest.raises(ImportError):\n        plt.switch_backend(\"nonexistent_backend\")\n    \n    # Test switching from interactive to non-interactive backends\n    interactive_backends = [\"TkAgg\", \"Qt5Agg\", \"MacOSX\"]\n    for backend in interactive_backends:\n        try:\n            plt.switch_backend(backend)\n            assert mpl.get_backend() == backend\n            plt.switch_backend(\"agg\")\n            assert mpl.get_backend() == \"agg\"\n        except ImportError:\n            pass  # backend not installed, skip test for this backend\n", "def test_switch_backend():\n    fig, ax = plt.subplots()\n    initial_backend = plt.get_backend()\n    # Test switching to a different backend\n    if initial_backend != 'agg':\n        plt.switch_backend('agg')\n        assert plt.get_backend() == 'agg'\n        plt.switch_backend(initial_backend)  # Switch back to initial\n        assert plt.get_backend() == initial_backend\n    else:\n        plt.switch_backend('pdf')\n        assert plt.get_backend() == 'pdf'\n        plt.switch_backend(initial_backend)  # Switch back to initial\n        assert plt.get_backend() == initial_backend\n", "def test_switch_backend():\n    initial_backend = plt.get_backend()\n    try:\n        plt.switch_backend('agg')\n        assert plt.get_backend() == 'agg'\n        \n        with pytest.raises(ImportError, match=\"Cannot load backend\"):\n            plt.switch_backend('nonexistent_backend')\n\n        # Ensure compatibility with event loops\n        plt.switch_backend('agg')\n        assert plt.get_backend() == 'agg'\n    finally:\n        plt.switch_backend(initial_backend)\n"], "sample_513": ["def test_draggable_legend_final_location():\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    ax.plot(x, x, label='line')\n    leg = ax.legend()\n    draggable_legend = leg.set_draggable(True, update='loc')\n    \n    assert draggable_legend is not None\n    draggable_legend.finalize_offset()\n    assert leg._loc == draggable_legend.get_loc_in_canvas()\n", "def test_draggable_legend_update_loc():\n    fig, ax = plt.subplots()\n    x = np.arange(100)\n    line, = ax.plot(x, 50 - x, 'o', label='y=1')\n    leg = ax.legend(loc='upper left', draggable=True, update='loc')\n    assert leg.get_draggable()\n    orig_loc = leg._loc\n    leg.set_draggable(True)\n    new_loc = (0.5, 0.5)\n    leg._draggable._update_loc(new_loc)\n    assert leg._loc == new_loc\n    assert leg._loc != orig_loc\n", "def test_set_get_loc():\n    \"\"\"Test setting and getting the legend location.\"\"\"\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    legend = ax.legend(loc='upper left')\n    assert legend._get_loc() == legend.codes['upper left']\n    legend.set_loc('lower right')\n    assert legend._get_loc() == legend.codes['lower right']\n    legend.set_loc((0.5, 0.5))\n    assert legend._get_loc() == (0.5, 0.5)\n", "def test_legend_update_handler_map():\n    # Test updating the default handler map and ensuring it's used\n    original_handler_map = mlegend.Legend.get_default_handler_map().copy()\n    \n    class CustomHandler(mlegend.legend_handler.HandlerBase):\n            return [mpl.patches.Rectangle((x0, y0), width, height, transform=trans)]\n\n    try:\n        # Update the default handler map\n        mlegend.Legend.update_default_handler_map({Line2D: CustomHandler()})\n\n        fig, ax = plt.subplots()\n        line, = ax.plot([0, 1], [0, 1], label='line')\n        leg = ax.legend()\n\n        # Ensure the custom handler is used\n        assert isinstance(leg.legendHandles[0], mpl.patches.Rectangle)\n    finally:\n        # Restore the original handler map to avoid side effects on other tests\n        mlegend.Legend.set_default_handler_map(original_handler_map)\n", "def test_legend_set_bbox_to_anchor():\n    # Test the set_bbox_to_anchor method of the Legend class.\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], label='line')\n    leg = ax.legend()\n    original_bbox = leg.get_bbox_to_anchor()\n    \n    new_bbox = (0.5, 0.5, 0.3, 0.3)\n    leg.set_bbox_to_anchor(new_bbox)\n    transformed_bbox = leg.get_bbox_to_anchor()\n\n    assert transformed_bbox.bounds == new_bbox\n    assert transformed_bbox != original_bbox\n"], "sample_514": ["def test_colorbar_ticks_update():\n    \"\"\"Test that updating the ticks reflects correctly on the colorbar.\"\"\"\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cbar = fig.colorbar(pc)\n    # Initial check for major ticks\n    initial_ticks = cbar.get_ticks()\n    assert len(initial_ticks) > 0  # There should be some initial ticks\n\n    # Update the ticks\n    new_ticks = [1, 2, 3, 4, 5]\n    cbar.set_ticks(new_ticks)\n    updated_ticks = cbar.get_ticks()\n    assert updated_ticks.tolist() == new_ticks\n\n    # Update the minor ticks\n    new_minor_ticks = [0.5, 1.5, 2.5, 3.5, 4.5]\n    cbar.set_ticks(new_minor_ticks, minor=True)\n    updated_minor_ticks = cbar.get_ticks(minor=True)\n    assert updated_minor_ticks.tolist() == new_minor_ticks\n", "def test_colorbar_scaling_and_aspect():\n    fig, ax = plt.subplots()\n    data = np.random.random((10, 10))\n    im = ax.imshow(data, cmap='viridis', aspect='auto')\n    cbar = fig.colorbar(im)\n    \n    # Check initial aspect ratio\n    cbar_aspect_initial = cbar.ax.get_box_aspect()\n    assert cbar_aspect_initial == 20, \"Initial aspect ratio should be 20\"\n\n    # Change aspect ratio and verify\n    cbar.ax.set_box_aspect(30)\n    cbar_aspect_new = cbar.ax.get_box_aspect()\n    assert cbar_aspect_new == 30, \"Aspect ratio should have been updated to 30\"\n\n    # Change colorbar scale and verify\n    cbar.set_ticks([0.2, 0.5, 0.8])\n    np.testing.assert_almost_equal(cbar.get_ticks(), [0.2, 0.5, 0.8])\n\n    cbar.set_ticks([0.3, 0.6, 0.9])\n    np.testing.assert_almost_equal(cbar.get_ticks(), [0.3, 0.6, 0.9])\n", "def test_colorbar_set_ticks_labels():\n    \"\"\"\n    Test the set_ticks() and set_ticklabels() methods of Colorbar.\n    \"\"\"\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh([[1, 2], [3, 4]])\n    cb = fig.colorbar(pc)\n\n    # Set and test major ticks\n    major_ticks = [1, 2, 3]\n    cb.set_ticks(major_ticks)\n    assert cb.get_ticks().tolist() == major_ticks\n\n    # Set and test major tick labels\n    major_labels = ['one', 'two', 'three']\n    cb.set_ticklabels(major_labels)\n    assert [tick.get_text() for tick in cb.ax.yaxis.get_ticklabels()] == major_labels\n\n    # Set and test minor ticks\n    minor_ticks = [1.5, 2.5]\n    cb.set_ticks(minor_ticks, minor=True)\n    assert cb.get_ticks(minor=True).tolist() == minor_ticks\n\n    # Set and test minor tick labels\n    minor_labels = ['one point five', 'two point five']\n    cb.set_ticklabels(minor_labels, minor=True)\n    assert [tick.get_text() for tick in cb.ax.yaxis.get_minorticklabels()] == minor_labels\n\n    # Test invalid tick setting\n    with pytest.raises(ValueError):\n        cb.set_ticklabels(['too', 'many', 'labels', 'for', 'ticks'])\n", "def test_colorbar_remove_and_readd():\n    # Test removing and re-adding a colorbar to ensure proper behavior\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    cax = fig.add_axes([0.85, 0.15, 0.03, 0.7])\n    im = ax.imshow(data)\n    cbar = fig.colorbar(im, cax=cax)\n    cbar.remove()\n    cax = fig.add_axes([0.85, 0.15, 0.03, 0.7])\n    cbar = fig.colorbar(im, cax=cax)\n    assert cbar.ax in fig.axes\n\n    # Ensure the colorbar can be properly reconfigured after re-adding\n    cbar.set_label('New Label')\n    assert cbar.ax.get_ylabel() == 'New Label'\n    cbar.set_ticks([0.2, 0.4, 0.6, 0.8])\n    assert cbar.get_ticks().tolist() == [0.2, 0.4, 0.6, 0.8]\n    cbar.set_alpha(0.5)\n    assert cbar.alpha == 0.5\n    plt.close(fig)\n", "def test_colorbar_set_alpha():\n    \"\"\"Test setting transparency of colorbar\"\"\"\n    fig, ax = plt.subplots()\n    cmap = plt.get_cmap(\"viridis\")\n    norm = Normalize(vmin=0, vmax=10)\n    sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n    cbar = fig.colorbar(sm, ax=ax)\n    \n    # Test setting a scalar alpha value\n    cbar.set_alpha(0.5)\n    assert cbar.alpha == 0.5\n    \n    # Test setting an array-like alpha value\n    alpha_array = np.linspace(0, 1, cmap.N)\n    cbar.set_alpha(alpha_array)\n    assert cbar.alpha is None  # Should reset to None when array is set\n\n    # Ensure the colorbar is drawn correctly with the specified alpha\n    fig.canvas.draw()\n"], "sample_515": ["def test_colorbar_set_alpha():\n    # Test setting alpha value for colorbars.\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    cmap = plt.cm.viridis\n\n    # Colorbar with alpha set to a scalar value\n    im = ax.imshow(data, cmap=cmap, alpha=0.7)\n    cbar = fig.colorbar(im)\n    assert cbar.alpha == 0.7\n\n    # Colorbar with alpha set to an array\n    alpha_array = np.linspace(0.3, 1.0, 10)\n    im = ax.imshow(data, cmap=cmap, alpha=alpha_array)\n    cbar = fig.colorbar(im)\n    assert cbar.alpha is None\n\n    # Update alpha after creation\n    cbar.set_alpha(0.5)\n    assert cbar.alpha == 0.5\n    fig.canvas.draw()\n    np.testing.assert_allclose(cbar.solids.get_alpha(), 0.5)\n", "def test_colorbar_locator_formatter():\n    # Test if locator and formatter set correctly and can be retrieved\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    \n    # Set and get major locator\n    loc = FixedLocator(np.arange(7))\n    cb.locator = loc\n    assert cb.locator is loc\n    assert cb._long_axis().get_major_locator() is loc\n\n    # Set and get minor locator\n    minor_loc = FixedLocator(np.arange(0, 7, 0.1))\n    cb.minorlocator = minor_loc\n    assert cb.minorlocator is minor_loc\n    assert cb._long_axis().get_minor_locator() is minor_loc\n\n    # Set and get major formatter\n    fmt = LogFormatter()\n    cb.formatter = fmt\n    assert cb.formatter is fmt\n    assert cb._long_axis().get_major_formatter() is fmt\n\n    # Set and get minor formatter\n    minor_fmt = LogFormatter()\n    cb.minorformatter = minor_fmt\n    assert cb.minorformatter is minor_fmt\n    assert cb._long_axis().get_minor_formatter() is minor_fmt\n\n    # Ensure setting through axis affects properties in Colorbar\n    cb.ax.yaxis.set_major_locator(FixedLocator([0, 1, 2, 3]))\n    assert cb.locator is cb.ax.yaxis.get_major_locator()\n    \n    cb.ax.yaxis.set_minor_locator(FixedLocator([0, 0.5, 1.5, 2.5]))\n    assert cb.minorlocator is cb.ax.yaxis.get_minor_locator()\n    \n    cb.ax.yaxis.set_major_formatter(LogFormatter())\n    assert cb.formatter is cb.ax.yaxis.get_major_formatter()\n    \n    cb.ax.yaxis.set_minor_formatter(LogFormatter())\n    assert cb.minorformatter is cb.ax.yaxis.get_minor_formatter()\n", "def test_colorbar_remove_and_add_again():\n    # Test removing a colorbar and adding it back to ensure consistency\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10), cmap='viridis')\n    cb = fig.colorbar(pc, ax=ax)\n    \n    # Ensure colorbar is added\n    assert cb.ax in fig.axes\n\n    # Remove the colorbar\n    cb.remove()\n    assert cb.ax not in fig.axes\n\n    # Add the colorbar again\n    cb = fig.colorbar(pc, ax=ax)\n    assert cb.ax in fig.axes\n    assert cb.ax.get_position() == ax.get_position()\n", "def test_colorbar_alpha_setter():\n    \"\"\"\n    Test the setter method for alpha in Colorbar.\n    Ensures that the alpha value is set correctly for both scalar and array inputs.\n    \"\"\"\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]], alpha=0.5)\n    cbar = fig.colorbar(im)\n    \n    # Test setting scalar alpha\n    cbar.set_alpha(0.8)\n    assert cbar.alpha == 0.8\n\n    # Test setting array alpha, should set cbar.alpha to None\n    alpha_array = np.array([[0.5, 0.5], [0.5, 0.5]])\n    cbar.set_alpha(alpha_array)\n    assert cbar.alpha is None\n\n    # Check mappable alpha\n    assert np.array_equal(im.get_alpha(), alpha_array)\n", "def test_colorbar_axes_locator():\n    \"\"\"Test the _ColorbarAxesLocator class and its functionality.\"\"\"\n    fig, ax = plt.subplots()\n    data = np.linspace(0, 1, 100).reshape(10, 10)\n    cmap = plt.get_cmap('viridis')\n    norm = plt.Normalize(vmin=0, vmax=1)\n    im = ax.imshow(data, cmap=cmap, norm=norm)\n    cbar = fig.colorbar(im, extend='both')\n\n    locator = cbar.ax._axes_locator\n    assert isinstance(locator, _ColorbarAxesLocator)\n\n    orig_pos = ax.get_position()\n    loc_pos = locator(ax, fig.canvas.get_renderer())\n    assert loc_pos == ax.get_position(original=True)\n\n    # Test proportional_y function\n    y, extendlen = locator._cbar._proportional_y()\n    assert len(y) == len(data) + 1\n    assert len(extendlen) == 2\n\n    # Test _extend_lower and _extend_upper functions\n    assert locator._cbar._extend_lower() is True\n    assert locator._cbar._extend_upper() is True\n\n    # Test get_subplotspec function\n    subplotspec = locator.get_subplotspec()\n    assert subplotspec is None or isinstance(subplotspec, mpl.gridspec.SubplotSpec)\n"], "sample_516": ["def test_create_pdf_info_dict():\n    # Test with valid metadata\n    metadata = {\n        'Title': 'Sample PDF',\n        'Author': 'Author Name',\n        'Subject': 'Sample Subject',\n        'Keywords': 'sample, pdf',\n        'Creator': 'Sample Creator',\n        'Producer': 'Sample Producer',\n        'CreationDate': datetime.datetime(2023, 1, 1, tzinfo=datetime.timezone.utc),\n        'ModDate': datetime.datetime(2023, 1, 2, tzinfo=datetime.timezone.utc),\n        'Trapped': 'True'\n    }\n    info_dict = _create_pdf_info_dict('pdf', metadata)\n    assert info_dict['Title'] == 'Sample PDF'\n    assert info_dict['Author'] == 'Author Name'\n    assert info_dict['Subject'] == 'Sample Subject'\n    assert info_dict['Keywords'] == 'sample, pdf'\n    assert info_dict['Creator'] == 'Sample Creator'\n    assert info_dict['Producer'] == 'Sample Producer'\n    assert info_dict['CreationDate'] == metadata['CreationDate']\n    assert info_dict['ModDate'] == metadata['ModDate']\n    assert info_dict['Trapped'] == Name('True')\n\n    # Test with minimal metadata\n    metadata = {}\n    info_dict = _create_pdf_info_dict('pdf', metadata)\n    assert 'Title' not in info_dict\n    assert 'Author' not in info_dict\n    assert 'Subject' not in info_dict\n    assert 'Keywords' not in info_dict\n    assert 'ModDate' not in info_dict\n    assert 'Trapped' not in info_dict\n    assert 'Creator' in info_dict\n    assert 'Producer' in info_dict\n    assert 'CreationDate' in info_dict\n\n    # Test with invalid metadata\n    metadata = {\n        'Title': 1234,  # Invalid type\n        'Trapped': 'foo',  # Invalid value\n        'foobar': 'invalid',  # Invalid key\n    }\n    with pytest.warns(UserWarning, match='not an instance of str.'):\n        info_dict = _create_pdf_info_dict('pdf', metadata)\n    with pytest.warns(UserWarning, match='not one of {\"True\", \"False\", \"Unknown\"}'):\n        info_dict = _create_pdf_info_dict('pdf', metadata)\n    with pytest.warns(UserWarning, match=\"Unknown infodict keyword: 'foobar'.\"):\n        info", "def test_pdf_repr():\n    import datetime\n    from matplotlib.transforms import Bbox\n\n    now = datetime.datetime.now()\n    bbox = Bbox.from_bounds(0, 0, 1, 1)\n    \n    assert pdfRepr(123) == b'123'\n    assert pdfRepr(123.456) == b'123.456'\n    assert pdfRepr('test') == b'(test)'\n    assert pdfRepr(b'test') == b'(test)'\n    assert pdfRepr(True) == b'true'\n    assert pdfRepr(False) == b'false'\n    assert pdfRepr(None) == b'null'\n    assert pdfRepr(now).startswith(b'D:')\n    assert pdfRepr(bbox) == b'0 0 1 1'\n    assert pdfRepr({'key': 'value'}) == b\"<< /key (value) >>\"\n    assert pdfRepr([1, 2, 3]) == b\"[1 2 3]\"\n\n    with pytest.raises(TypeError):\n        pdfRepr(set([1, 2, 3]))\n", "def test_pdf_repr():\n    # Test the pdfRepr function for various data types.\n\n    # Test with float\n    assert pdfRepr(0.123456789) == b'0.123456789'\n    assert pdfRepr(-1.23e-10) == b'-0.0000000001'\n\n    # Test with boolean\n    assert pdfRepr(True) == b'true'\n    assert pdfRepr(False) == b'false'\n\n    # Test with integer\n    assert pdfRepr(123) == b'123'\n\n    # Test with string\n    assert pdfRepr(\"Hello, World!\") == b'(Hello, World!)'\n    # Test with string containing special characters\n    assert pdfRepr(\"Hello\\nWorld!\") == b'(Hello\\\\nWorld!)'\n\n    # Test with bytes\n    assert pdfRepr(b'Hello, PDF!') == b'(Hello, PDF!)'\n\n    # Test with dictionary\n    assert pdfRepr({\"Key\": \"Value\"}) == b'<< /Key (Value) >>'\n\n    # Test with list\n    assert pdfRepr([1, 2, 3]) == b'[ 1 2 3 ]'\n\n    # Test with None\n    assert pdfRepr(None) == b'null'\n\n    # Test with datetime\n    dt = datetime.datetime(2023, 1, 1, 12, 0, 0)\n    assert pdfRepr(dt) == b'(D:20230101120000+00\\'00\\')'\n\n    # Test with BboxBase\n    bbox = BboxBase.from_bounds(0, 0, 1, 1)\n    assert pdfRepr(bbox) == b'0 0 1 1'\n", "def test_pdf_repr():\n    ref = Reference(1)\n    assert pdfRepr(ref) == b\"1 0 R\"\n    assert pdfRepr(3.14) == b\"3.14\"\n    assert pdfRepr(True) == b\"true\"\n    assert pdfRepr(False) == b\"false\"\n    assert pdfRepr(42) == b\"42\"\n    assert pdfRepr(\"Hello\") == b\"(Hello)\"\n    assert pdfRepr(b\"Hello\") == b\"(Hello)\"\n    assert pdfRepr({\"A\": 1, \"B\": 2}) == b\"<< /A 1 /B 2 >>\"\n    assert pdfRepr([1, 2, 3]) == b\"[ 1 2 3 ]\"\n    assert pdfRepr(None) == b\"null\"\n    date = datetime.datetime(2023, 10, 1, 12, 0)\n    assert pdfRepr(date) == b\"(D:20231001120000)\"\n    bbox = BboxBase([[0, 0], [1, 1]])\n    assert pdfRepr(bbox) == b\"0 0 1 1\"\n", "def test_pdf_repr():\n    assert pdfRepr(10) == b'10'\n    assert pdfRepr(10.123456789) == b'10.123456789'\n    assert pdfRepr(True) == b'true'\n    assert pdfRepr(False) == b'false'\n    assert pdfRepr(\"hello\") == b'(hello)'\n    assert pdfRepr(b\"hello\") == b'(hello)'\n    assert pdfRepr([1, 2, 3]) == b'[ 1 2 3 ]'\n    assert pdfRepr((1, 2, 3)) == b'[ 1 2 3 ]'\n    assert pdfRepr({\"key\": \"value\"}) == b'<< /key (value) >>'\n    assert pdfRepr(None) == b'null'\n\n    date = datetime.datetime(2023, 10, 1, 12, 34, 56)\n    assert pdfRepr(date) == b'(D:20231001123456+00\\'00\\')'\n\n    bbox = BboxBase([[0, 0], [1, 1]])\n    assert pdfRepr(bbox) == b'0.000000 0.000000 1.000000 1.000000'\n\n    with pytest.raises(TypeError):\n        pdfRepr(set([1, 2, 3]))\n"], "sample_517": ["def test_text_contains_bbox_patch():\n    fig, ax = plt.subplots()\n    txt = ax.text(0.5, 0.5, 'test', fontsize=30, bbox=dict(facecolor='red', alpha=0.5))\n    fig.canvas.draw()\n\n    event_inside = MouseEvent('button_press_event', fig.canvas, 0.5, 0.5, 1, None)\n    event_outside = MouseEvent('button_press_event', fig.canvas, 0.9, 0.9, 1, None)\n    \n    inside, _ = txt.contains(event_inside)\n    assert inside, \"Text with bbox should contain the point within bbox\"\n\n    inside, _ = txt.contains(event_outside)\n    assert not inside, \"Text with bbox should not contain the point outside bbox\"\n", "def test_get_rotation_mode():\n    text = Text(0, 0, 'foo', rotation_mode='anchor')\n    assert text.get_rotation_mode() == 'anchor'\n    text.set_rotation_mode('default')\n    assert text.get_rotation_mode() == 'default'\n    text.set_rotation_mode(None)\n    assert text.get_rotation_mode() is None\n", "def test_set_rotation_mode():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Test rotation mode', rotation=45, rotation_mode='anchor')\n    fig.canvas.draw()\n    assert text.get_rotation_mode() == 'anchor'\n    text.set_rotation_mode('default')\n    assert text.get_rotation_mode() == 'default'\n    with pytest.raises(ValueError, match=\"rotation_mode must be 'anchor', 'default' or None\"):\n        text.set_rotation_mode('invalid_mode')\n", "def test_text_str_rotation():\n    fig, ax = plt.subplots()\n    # Test string rotation values\n    text1 = ax.text(0.5, 0.5, \"Horizontal\", rotation='horizontal')\n    text2 = ax.text(0.5, 0.6, \"Vertical\", rotation='vertical')\n    fig.canvas.draw()\n    \n    assert text1.get_rotation() == 0.0\n    assert text2.get_rotation() == 90.0\n\n", "def test_get_text():\n    text = Text(0, 0, \"Hello, World!\")\n    assert text.get_text() == \"Hello, World!\"\n\n"], "sample_518": ["def test_patch_get_methods():\n    patch = Patch(edgecolor='blue', facecolor='red', linewidth=2.5, antialiased=True,\n                  linestyle='-', hatch='/', fill=False, capstyle='round', joinstyle='bevel')\n\n    assert patch.get_edgecolor() == mcolors.to_rgba('blue')\n    assert patch.get_facecolor() == mcolors.to_rgba('red', 0)\n    assert patch.get_linewidth() == 2.5\n    assert patch.get_antialiased() is True\n    assert patch.get_linestyle() == '-'\n    assert patch.get_hatch() == '/'\n    assert patch.get_fill() is False\n    assert patch.get_capstyle() == 'round'\n    assert patch.get_joinstyle() == 'bevel'\n", "def test_patch_contains():\n    # Test the contains method for different patches\n    fig, ax = plt.subplots()\n    rect = Rectangle((0, 0), 1, 1)\n    circle = mpatches.Circle((0.5, 0.5), 0.5)\n    ellipse = Ellipse((1, 1), 1, 0.5)\n    polygon = Polygon([[0, 0], [1, 0], [1, 1], [0, 1]])\n\n    ax.add_patch(rect)\n    ax.add_patch(circle)\n    ax.add_patch(ellipse)\n    ax.add_patch(polygon)\n\n    fig.canvas.draw()\n\n    for patch, points in [\n        (rect, [(0.5, 0.5), (1.5, 1.5)]),\n        (circle, [(0.5, 0.5), (1.5, 1.5)]),\n        (ellipse, [(1, 1), (1.5, 1.5)]),\n        (polygon, [(0.5, 0.5), (1.5, 1.5)])\n    ]:\n        for point in points:\n            contains, _ = patch.contains_point(point)\n            if point == (0.5, 0.5) or point == (1, 1):\n                assert contains, f\"{patch} should contain point {point}\"\n            else:\n                assert not contains, f\"{patch} should not contain point {point}\"\n", "def test_patch_contains():\n    fig, ax = plt.subplots()\n    rect = Rectangle((0.1, 0.1), 0.6, 0.6)\n    ax.add_patch(rect)\n    fig.canvas.draw()  # Force the renderer to draw the rectangle\n\n    # Test contains method\n    inside_event = type('test', (object,), {'x': 0.3, 'y': 0.3, 'inaxes': ax})\n    outside_event = type('test', (object,), {'x': 0.8, 'y': 0.8, 'inaxes': ax})\n    \n    assert rect.contains(inside_event)[0]\n    assert not rect.contains(outside_event)[0]\n\n    # Test contains_point method\n    assert rect.contains_point((0.3, 0.3))\n    assert not rect.contains_point((0.8, 0.8))\n\n    # Test contains_points method\n    points = [(0.3, 0.3), (0.8, 0.8), (0.5, 0.5)]\n    expected = [True, False, True]\n    result = rect.contains_points(points)\n    assert np.array_equal(result, expected)\n", "def test_patch_setters():\n    patch = Patch()\n\n    # Test setting edge color\n    patch.set_edgecolor('red')\n    assert patch.get_edgecolor() == mcolors.to_rgba('red')\n\n    # Test setting face color\n    patch.set_facecolor('blue')\n    assert patch.get_facecolor() == mcolors.to_rgba('blue')\n\n    # Test setting both edge and face color via set_color\n    patch.set_color('green')\n    assert patch.get_edgecolor() == mcolors.to_rgba('green')\n    assert patch.get_facecolor() == mcolors.to_rgba('green')\n\n    # Test setting linewidth\n    patch.set_linewidth(2.5)\n    assert patch.get_linewidth() == 2.5\n\n    # Test setting linestyle\n    patch.set_linestyle('--')\n    assert patch.get_linestyle() == '--'\n\n    # Test setting antialiased\n    patch.set_antialiased(False)\n    assert not patch.get_antialiased()\n\n    # Test setting hatch pattern\n    patch.set_hatch('x')\n    assert patch.get_hatch() == 'x'\n\n    # Test setting capstyle\n    patch.set_capstyle('round')\n    assert patch.get_capstyle() == 'round'\n\n    # Test setting joinstyle\n    patch.set_joinstyle('bevel')\n    assert patch.get_joinstyle() == 'bevel'\n\n    # Test setting alpha\n    patch.set_alpha(0.5)\n    assert patch.get_facecolor() == mcolors.to_rgba('green', 0.5)\n    assert patch.get_edgecolor() == mcolors.to_rgba('green', 0.5)\n", "def test_fancybboxpatch_path():\n    # Test the path generation for FancyBboxPatch\n    boxstyle = BoxStyle(\"Round\", pad=0.2)\n    fbbox = mpatches.FancyBboxPatch((1, 2), width=3, height=4, boxstyle=boxstyle, mutation_scale=2, mutation_aspect=1.5)\n    \n    path = fbbox.get_path()\n    assert isinstance(path, mpath.Path)\n    \n    # Check some properties of the generated path\n    assert path.vertices.shape[1] == 2  # vertices should be 2D points\n    assert path.codes is not None  # there should be path codes\n    assert len(path.vertices) == len(path.codes)  # vertices and codes should align\n    \n    # Check that the path was transformed correctly according to the mutation_scale and mutation_aspect\n    # We use the boxstyle attributes directly to check path transformation logic\n    assert np.allclose(fbbox.get_mutation_scale(), 2)\n    assert np.allclose(fbbox.get_mutation_aspect(), 1.5)\n    assert fbbox.get_boxstyle() == boxstyle\n\n    # Test with default boxstyle\n    fbbox.set_boxstyle(\"Square\", pad=0.3)\n    path = fbbox.get_path()\n    assert isinstance(path, mpath.Path)\n    \n    assert path.vertices.shape[1] == 2  # vertices should be 2D points\n    assert path.codes is not None  # there should be path codes\n    assert len(path.vertices) == len(path.codes)  # vertices and codes should align\n"], "sample_519": ["def test_figure_clear_with_suptitle():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    fig.suptitle(\"Main Title\")\n    ax1.set_title(\"Subplot Title\")\n\n    # Ensure the titles are set before clearing\n    assert fig._suptitle.get_text() == \"Main Title\"\n    assert ax1.get_title() == \"Subplot Title\"\n\n    fig.clear()\n\n    # Ensure the titles are cleared after clear\n    assert fig._suptitle is None\n    assert len(fig.axes) == 0\n", "def test_subplot_params_update():\n    subplot_params = SubplotParams(left=0.1, bottom=0.2, right=0.9, top=0.8, wspace=0.5, hspace=0.3)\n    assert subplot_params.left == 0.1\n    assert subplot_params.bottom == 0.2\n    assert subplot_params.right == 0.9\n    assert subplot_params.top == 0.8\n    assert subplot_params.wspace == 0.5\n    assert subplot_params.hspace == 0.3\n\n    subplot_params.update(left=0.15, hspace=0.35)\n    assert subplot_params.left == 0.15\n    assert subplot_params.bottom == 0.2\n    assert subplot_params.right == 0.9\n    assert subplot_params.top == 0.8\n    assert subplot_params.wspace == 0.5\n    assert subplot_params.hspace == 0.35\n\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        subplot_params.update(left=1.0)\n    \n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        subplot_params.update(bottom=1.0)\n", "def test_add_gridspec():\n    fig = plt.figure()\n\n    # Test creating a gridspec with default parameters\n    gs = fig.add_gridspec(2, 2)\n    assert isinstance(gs, gridspec.GridSpec)\n    assert gs.get_geometry() == (2, 2)\n\n    # Test adding subplots to the gridspec\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax3 = fig.add_subplot(gs[1, :])\n    assert ax1.get_gridspec() == gs\n    assert ax2.get_gridspec() == gs\n    assert ax3.get_gridspec() == gs\n\n    # Test that the subplots have been added to the figure\n    assert ax1 in fig.axes\n    assert ax2 in fig.axes\n    assert ax3 in fig.axes\n\n    # Test gridspec with specific width and height ratios\n    gs = fig.add_gridspec(2, 3, width_ratios=[1, 2, 1], height_ratios=[2, 1])\n    ax1 = fig.add_subplot(gs[0, :])\n    ax2 = fig.add_subplot(gs[1, 0])\n    ax3 = fig.add_subplot(gs[1, 1:])\n    assert ax1.get_gridspec() == gs\n    assert ax2.get_gridspec() == gs\n    assert ax3.get_gridspec() == gs\n\n    # Test that the subplots have been added to the figure\n    assert ax1 in fig.axes\n    assert ax2 in fig.axes\n    assert ax3 in fig.axes\n\n    # Ensure gridspec keyword arguments are being passed correctly\n    with pytest.raises(ValueError, match=\"'height_ratios' must not be defined both as parameter and as key in 'gridspec_kw'\"):\n        fig.add_gridspec(2, 2, height_ratios=[1, 2], gridspec_kw={'height_ratios': [1, 2]})\n\n    with pytest.raises(ValueError, match=\"'width_ratios' must not be defined both as parameter and as key in 'gridspec_kw'\"):\n        fig.add_gridspec(2, 2, width_ratios=[1, 2], gridspec_kw={'width_ratios': [1, 2]})\n", "def test_add_axes_existing():\n    fig = plt.figure()\n    ax1 = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    ax2 = fig.add_axes(ax1)  # Adding the same axes should not create a new one\n    assert ax1 is ax2\n    assert len(fig.axes) == 1\n", "def test_subfigure_update_from():\n    fig = plt.figure()\n    sub_fig1 = fig.add_subfigure(fig.add_gridspec(1, 2)[0])\n    sub_fig2 = fig.add_subfigure(fig.add_gridspec(1, 2)[1])\n    \n    sub_fig1.suptitle('SubFigure 1 Title')\n    sub_fig1.set_facecolor('blue')\n    \n    sub_fig2.update_from(sub_fig1)\n    \n    assert sub_fig2._suptitle.get_text() == 'SubFigure 1 Title'\n    assert sub_fig2.patch.get_facecolor() == sub_fig1.patch.get_facecolor()\n"], "sample_520": ["def test_text3d_set_get_properties(fig_test, fig_ref):\n    # Test setting and getting 3D text properties\n    ax_test = fig_test.add_subplot(projection='3d')\n    t3d = art3d.Text3D(1, 2, 3, 'Test Text', zdir='x', color='red')\n    ax_test.add_artist(t3d)\n    assert t3d.get_position_3d() == (1, 2, 3)\n    assert np.array_equal(t3d._dir_vec, [0, 0, 1])  # zdir='x' implies (0, 0, 1)\n    assert t3d.get_text() == 'Test Text'\n    assert t3d.get_color() == 'red'\n\n    t3d.set_position_3d((4, 5, 6), 'y')\n    t3d.set_text('New Text')\n    t3d.set_color('blue')\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    t3d_ref = art3d.Text3D(4, 5, 6, 'New Text', zdir='y', color='blue')\n    ax_ref.add_artist(t3d_ref)\n    assert t3d_ref.get_position_3d() == (4, 5, 6)\n    assert np.array_equal(t3d_ref._dir_vec, [0, 0, 0])  # zdir='y' implies (0, 0, 0)\n    assert t3d_ref.get_text() == 'New Text'\n    assert t3d_ref.get_color() == 'blue'\n", "def test_get_dir_vector():\n    # Test predefined directions\n    np.testing.assert_array_equal(art3d.get_dir_vector('x'), [1, 0, 0])\n    np.testing.assert_array_equal(art3d.get_dir_vector('y'), [0, 1, 0])\n    np.testing.assert_array_equal(art3d.get_dir_vector('z'), [0, 0, 1])\n    np.testing.assert_array_equal(art3d.get_dir_vector(None), [0, 0, 0])\n\n    # Test custom direction\n    np.testing.assert_array_equal(art3d.get_dir_vector((1, 1, 1)), [1, 1, 1])\n\n    # Test invalid input\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector('invalid')\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector((1, 1))\n\n    # Test iterable but not a valid 3-tuple\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector([1, 1, 1, 1])\n", "def test_text3d_set_get_position_3d():\n    text = art3d.Text3D(1, 2, 3, \"Test\")\n    assert text.get_position_3d() == (1, 2, 3)\n    text.set_position_3d((4, 5, 6))\n    assert text.get_position_3d() == (4, 5, 6)\n", "def test_get_dir_vector():\n    # Test predefined direction strings\n    np.testing.assert_array_equal(art3d.get_dir_vector('x'), [1, 0, 0])\n    np.testing.assert_array_equal(art3d.get_dir_vector('y'), [0, 1, 0])\n    np.testing.assert_array_equal(art3d.get_dir_vector('z'), [0, 0, 1])\n    np.testing.assert_array_equal(art3d.get_dir_vector(None), [0, 0, 0])\n\n    # Test custom 3-tuple\n    np.testing.assert_array_equal(art3d.get_dir_vector((1, 2, 3)), [1, 2, 3])\n\n    # Test invalid input\n    with pytest.raises(ValueError, match=\"expected\"):\n        art3d.get_dir_vector('a')\n    with pytest.raises(ValueError, match=\"expected\"):\n        art3d.get_dir_vector((1, 2))\n    with pytest.raises(ValueError, match=\"expected\"):\n        art3d.get_dir_vector([1, 2, 3, 4])\n", "def test_text3d_set_position_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    text = art3d.Text3D(1, 2, 3, 'Test Text')\n    ax.add_artist(text)\n\n    assert text.get_position_3d() == (1, 2, 3)\n\n    text.set_position_3d((4, 5, 6), zdir='x')\n    assert text.get_position_3d() == (4, 5, 6)\n    assert np.all(text._dir_vec == np.array([1, 0, 0]))\n\n    text.set_position_3d((7, 8, 9), zdir=(0.5, 0.5, 0.5))\n    assert text.get_position_3d() == (7, 8, 9)\n    assert np.all(text._dir_vec == np.array([0.5, 0.5, 0.5]))\n\n    with pytest.raises(ValueError):\n        text.set_position_3d((10, 11, 12), zdir='invalid')\n"], "sample_521": ["def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array([1, 2, 3]))\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector('invalid')\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector((1, 2))\n", "def test_norm_angle():\n    assert art3d._norm_angle(0) == 0\n    assert art3d._norm_angle(360) == 0\n    assert art3d._norm_angle(-180) == -180\n    assert art3d._norm_angle(540) == 180\n    assert art3d._norm_angle(-540) == -180\n", "def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array((1, 0, 0)))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array((0, 1, 0)))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array((0, 0, 1)))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array((0, 0, 0)))\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array((1, 2, 3)))\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector('a')\n", "def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector([1, 2, 3]), np.array([1, 2, 3]))\n\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector('invalid')\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector([1, 2])\n", "def test_line3d_collection():\n    segments = [\n        [[0, 0, 0], [1, 1, 1]],\n        [[1, 1, 1], [2, 2, 2]],\n        [[2, 2, 2], [3, 3, 3]]\n    ]\n\n    fig_test = plt.figure()\n    ax_test = fig_test.add_subplot(projection='3d')\n    line_collection_test = art3d.Line3DCollection(segments)\n    ax_test.add_collection(line_collection_test)\n    ax_test.set_xlim(0, 3)\n    ax_test.set_ylim(0, 3)\n    ax_test.set_zlim(0, 3)\n\n    fig_ref = plt.figure()\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    for seg in segments:\n        xs, ys, zs = zip(*seg)\n        ax_ref.plot(xs, ys, zs, color='b')\n    ax_ref.set_xlim(0, 3)\n    ax_ref.set_ylim(0, 3)\n    ax_ref.set_zlim(0, 3)\n"], "sample_522": ["def test_colorbar_set_label():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    label = 'Colorbar Label'\n    cb.set_label(label)\n    assert cb.ax.get_ylabel() == label\n\n    cb.set_label(None)\n    assert cb.ax.get_ylabel() == ''\n\n    new_label = 'New Label'\n    cb.set_label(new_label)\n    assert cb.ax.get_ylabel() == new_label\n\n    cb.ax.set_xlabel('X-axis label')\n    assert cb.ax.get_xlabel() == 'X-axis label'\n", "def test_colorbar_with_subfigures():\n    fig = plt.figure()\n    subfigs = fig.subfigures(1, 2)\n    \n    for subfig in subfigs:\n        ax = subfig.subplots()\n        data = np.linspace(0, 1, 100).reshape(10, 10)\n        cax = ax.imshow(data)\n        subfig.colorbar(cax)\n\n    fig.canvas.draw()\n    for subfig in subfigs:\n        assert len(subfig.axes) == 1\n        assert len(subfig.legends) == 0\n        assert all(isinstance(ax, plt.Axes) for ax in subfig.axes)\n\n    # Check that subfigures with colorbars do not interfere with each other\n    assert subfigs[0].axes[0].get_position() != subfigs[1].axes[0].get_position()\n", "def test_figure_base_autofmt_xdate():\n    fig = plt.figure()\n    ax = fig.add_subplot()\n    ax.plot([1, 2, 3], [1, 2, 3])\n    fig.autofmt_xdate(bottom=0.3, rotation=45, ha='center')\n    # Ensure that the bottom position of the subplots is set correctly\n    assert fig.subplotpars.bottom == 0.3\n    # Ensure that the rotation and alignment of xtick labels are set correctly\n    for label in ax.get_xticklabels():\n        assert label.get_rotation() == 45\n        assert label.get_ha() == 'center'\n", "def test_figure_add_axes():\n    fig = plt.figure()\n    ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    assert ax in fig.axes\n    assert fig.gca() == ax\n", "def test_figure_set_layout_engine():\n    fig = plt.figure()\n\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), mpl.layout_engine.TightLayoutEngine)\n\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), mpl.layout_engine.ConstrainedLayoutEngine)\n\n    fig.set_layout_engine('compressed')\n    engine = fig.get_layout_engine()\n    assert isinstance(engine, mpl.layout_engine.ConstrainedLayoutEngine)\n    assert engine.compress is True\n\n    fig.set_layout_engine('none')\n    assert fig.get_layout_engine() is None\n\n    fig.set_layout_engine(layout=None)\n    assert fig.get_layout_engine() is None\n\n    with pytest.raises(ValueError, match=\"Invalid value for 'layout'\"):\n        fig.set_layout_engine('invalid_layout')\n"], "sample_523": ["def test_legend_custom_handler_map():\n    \"\"\"Test custom legend handler map functionality\"\"\"\n    class CustomHandler:\n            return plt.Line2D([0, 1], [0, 1], color='green')\n\n    fig, ax = plt.subplots()\n    ln1, = ax.plot([0, 1], label='line1')\n    ln2, = ax.plot([0, 1], label='line2')\n    \n    custom_handler_map = {ln1: CustomHandler()}\n    legend = ax.legend(handler_map=custom_handler_map)\n    \n    assert isinstance(legend.legendHandles[0], plt.Line2D)\n    assert legend.legendHandles[0].get_color() == 'green'\n    assert isinstance(legend.legendHandles[1], plt.Line2D)\n    assert legend.legendHandles[1].get_color() != 'green'\n", "def test_draggable_legend():\n    # Test that a legend can be made draggable and its position can be updated.\n\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    line, = ax.plot(x, y, label=\"Sine Wave\")\n\n    leg = ax.legend()\n    assert not leg.get_draggable()\n\n    # Make the legend draggable\n    leg.set_draggable(True)\n    assert leg.get_draggable()\n\n    # Simulate dragging the legend to a new position\n    draggable = leg.get_draggable()\n    draggable.start(0, 0)\n    draggable.stop(200, 200)\n    draggable.finalize_offset()\n\n    # Check that the legend position has been updated\n    new_bbox = leg.get_window_extent()\n    assert new_bbox.x0 != 0\n    assert new_bbox.y0 != 0\n\n    # Test updating bbox_to_anchor parameter\n    leg.set_draggable(True, update='bbox')\n    draggable_bbox = leg.get_draggable()\n    draggable_bbox.start(0, 0)\n    draggable_bbox.stop(200, 200)\n    draggable_bbox.finalize_offset()\n\n    # Verify if bbox_to_anchor is updated\n    bbox_anchor = leg.get_bbox_to_anchor()\n    assert bbox_anchor.x0 != 0\n    assert bbox_anchor.y0 != 0\n", "def test_legend_set_get_ncols():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test1')\n    ax.plot(range(10, 20), label='test2')\n    leg = ax.legend(ncols=2)\n    assert leg._ncols == 2\n    leg.set_ncols(3)\n    assert leg._ncols == 3\n", "def test_legend_custom_handler():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], label='line')\n\n    class CustomHandler:\n            line = mlines.Line2D([0, 1], [0, 1], color='blue', linestyle='--')\n            handlebox.add_artist(line)\n            return line\n\n    handler_map = {line: CustomHandler()}\n    leg = ax.legend(handler_map=handler_map)\n    \n    assert isinstance(leg.legendHandles[0], mlines.Line2D)\n    assert leg.legendHandles[0].get_color() == 'blue'\n    assert leg.legendHandles[0].get_linestyle() == '--'\n", "def test_legend_set_ncols():\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    y = np.arange(10)\n    ax.plot(x, y, label='line1')\n    ax.plot(x, y*2, label='line2')\n    ax.plot(x, y*3, label='line3')\n    ax.plot(x, y*4, label='line4')\n    leg = ax.legend()\n    leg.set_ncols(2)\n    assert leg._ncols == 2  # Check if ncols is set correctly\n    leg._init_legend_box(leg.legendHandles, leg.get_texts(), markerfirst=True)\n    assert len(leg._legend_handle_box.get_children()) == 2  # Check if two columns are created\n"], "sample_524": ["def test_subfigure_creation():\n    fig = plt.figure()\n    subfigs = fig.subfigures(1, 2)\n    assert len(subfigs) == 2\n    assert isinstance(subfigs[0], mpl.figure.SubFigure)\n    assert isinstance(subfigs[1], mpl.figure.SubFigure)\n    \n    axsL = subfigs[0].subplots(1, 2)\n    axsR = subfigs[1].subplots(2, 1)\n    \n    assert len(axsL) == 2\n    assert len(axsR) == 2\n    assert all(isinstance(ax, mpl.axes.Axes) for ax in axsL)\n    assert all(isinstance(ax, mpl.axes.Axes) for ax in axsR)\n    \n    subfigs[0].suptitle('Left SubFigure')\n    subfigs[1].suptitle('Right SubFigure')\n    \n    assert subfigs[0]._suptitle.get_text() == 'Left SubFigure'\n    assert subfigs[1]._suptitle.get_text() == 'Right SubFigure'\n    \n    for ax in axsL:\n        ax.plot([1, 2, 3], [1, 4, 9])\n    \n    for ax in axsR:\n        ax.plot([1, 2, 3], [1, 4, 9])\n\n    fig.canvas.draw()\n", "def test_subplotparams_update():\n    # Test updating SubplotParams and ensuring validation works correctly\n    params = SubplotParams(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.5, hspace=0.5)\n    assert params.left == 0.1\n    assert params.bottom == 0.1\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.wspace == 0.5\n    assert params.hspace == 0.5\n\n    # Test invalid update where left >= right\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=0.9, right=0.1)\n\n    # Test invalid update where bottom >= top\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=0.9, top=0.1)\n    \n    # Test valid update\n    params.update(left=0.2, top=0.8)\n    assert params.left == 0.2\n    assert params.top == 0.8\n", "def test_figure_base_get_children():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    ax2 = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    text = fig.text(0.5, 0.5, 'Test')\n    line = fig.gca().plot([0, 1], [0, 1])[0]\n\n    children = fig.get_children()\n\n    assert fig.patch in children\n    assert ax1 in children\n    assert ax2 in children\n    assert text in children\n    assert line in children\n", "def test_subplot_params_update():\n    # Test SubplotParams.update method\n    params = SubplotParams(left=0.1, bottom=0.2, right=0.9, top=0.8, wspace=0.5, hspace=0.5)\n    assert params.left == 0.1\n    assert params.bottom == 0.2\n    assert params.right == 0.9\n    assert params.top == 0.8\n    assert params.wspace == 0.5\n    assert params.hspace == 0.5\n\n    params.update(left=0.15, bottom=0.25)\n    assert params.left == 0.15\n    assert params.bottom == 0.25\n    assert params.right == 0.9\n    assert params.top == 0.8\n    assert params.wspace == 0.5\n    assert params.hspace == 0.5\n\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=1.0)\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=1.0)\n", "def test_axes_stack_as_list():\n    stack = mpl.figure._AxesStack()\n    ax1 = plt.Axes(plt.figure(), [0.1, 0.1, 0.8, 0.8])\n    ax2 = plt.Axes(plt.figure(), [0.1, 0.1, 0.8, 0.8])\n    ax3 = plt.Axes(plt.figure(), [0.1, 0.1, 0.8, 0.8])\n    stack.add(ax1)\n    stack.add(ax2)\n    stack.add(ax3)\n    \n    assert stack.as_list() == [ax1, ax2, ax3], \"Axes not added correctly to stack\"\n"], "sample_525": ["def test_subplots_adjust():\n    fig, axs = plt.subplots(2, 2)\n    fig.subplots_adjust(left=0.2, bottom=0.3, right=0.8, top=0.9, wspace=0.4, hspace=0.5)\n\n    # Ensure subplot parameters are correctly set\n    assert fig.subplotpars.left == 0.2\n    assert fig.subplotpars.bottom == 0.3\n    assert fig.subplotpars.right == 0.8\n    assert fig.subplotpars.top == 0.9\n    assert fig.subplotpars.wspace == 0.4\n    assert fig.subplotpars.hspace == 0.5\n\n    # Ensure layout adjustments are applied correctly\n    for ax in axs.flat:\n        pos = ax.get_position()\n        assert pos.x0 >= 0.2\n        assert pos.y0 >= 0.3\n        assert pos.x1 <= 0.8\n        assert pos.y1 <= 0.9\n", "def test_subplot_params():\n    # Test SubplotParams initialization\n    sp = mpl.figure.SubplotParams(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.2, hspace=0.2)\n    assert sp.left == 0.1\n    assert sp.right == 0.9\n    assert sp.top == 0.9\n    assert sp.bottom == 0.1\n    assert sp.wspace == 0.2\n    assert sp.hspace == 0.2\n\n    # Test update method\n    sp.update(left=0.2)\n    assert sp.left == 0.2\n    sp.update(right=0.8)\n    assert sp.right == 0.8\n    sp.update(top=0.85)\n    assert sp.top == 0.85\n    sp.update(bottom=0.15)\n    assert sp.bottom == 0.15\n    sp.update(wspace=0.25)\n    assert sp.wspace == 0.25\n    sp.update(hspace=0.25)\n    assert sp.hspace == 0.25\n\n    # Test validation check\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        sp.update(left=0.9)\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        sp.update(bottom=0.9)\n", "def test_subplot_mosaic_empty_with_labels():\n    fig = plt.figure()\n    labels = [[\"A\", None], [None, \"B\"]]\n    grid_axes = fig.subplot_mosaic(labels, empty_sentinel=None)\n    \n    assert \"A\" in grid_axes\n    assert \"B\" in grid_axes\n    assert grid_axes[\"A\"] is not None\n    assert grid_axes[\"B\"] is not None\n    assert fig.axes == list(grid_axes.values())\n", "def test_figure_autoformat_xdate():\n    fig, ax = plt.subplots()\n    dates = mdates.drange(datetime(2020, 1, 1), datetime(2020, 1, 10), timedelta(days=1))\n    values = np.random.rand(len(dates))\n    \n    ax.plot_date(dates, values)\n    \n    # Testing autofmt_xdate without parameters (default)\n    fig.autofmt_xdate()\n    for label in ax.get_xticklabels(which='major'):\n        assert label.get_rotation() == 30\n        assert label.get_ha() == 'right'\n    \n    # Testing autofmt_xdate with different parameters\n    fig.autofmt_xdate(bottom=0.3, rotation=45, ha='center', which='both')\n    for label in ax.get_xticklabels(which='both'):\n        assert label.get_rotation() == 45\n        assert label.get_ha() == 'center'\n        assert label.get_visible() is True\n    assert fig.subplotpars.bottom == 0.3\n", "def test_subfigure_subplot():\n    # Test adding a subplot to a subfigure and ensuring proper structure\n    fig = plt.figure()\n    subfig = fig.add_subfigure(plt.GridSpec(1, 1)[0, 0])\n    ax = subfig.add_subplot(111)\n    \n    assert subfig in fig.subfigs\n    assert ax in subfig.axes\n    assert ax in fig.axes\n\n    # Add another subplot to the main figure and ensure it doesn't interfere\n    ax_main = fig.add_subplot(111)\n    assert ax_main in fig.axes\n    assert ax_main not in subfig.axes\n"], "sample_526": ["def test_set_epoch_before_dates():\n    fig, ax = plt.subplots()\n    # Set the epoch before plotting dates\n    mdates.set_epoch('2000-01-01')\n    \n    dates = [datetime.datetime(2001, 1, 1), datetime.datetime(2001, 1, 2)]\n    ax.plot(dates, [0, 1])\n    \n    assert mdates.get_epoch() == '2000-01-01'\n    \n    # Plot should correctly use the custom epoch\n    expected_x = [mdates.date2num(datetime.datetime(2001, 1, 1)),\n                  mdates.date2num(datetime.datetime(2001, 1, 2))]\n    np.testing.assert_allclose(ax.get_lines()[0].get_xdata(), expected_x)\n\n    # Reset epoch\n    mdates._reset_epoch_test_example()\n", "def test_rrulewrapper_set():\n    rule = mdates.rrulewrapper(YEARLY, byeaster=1)\n    # Initial check\n    assert rule._rrule._freq == YEARLY\n    assert rule._rrule._byeaster == 1\n    assert rule._rrule._interval == 1\n\n    # Modify the rule\n    rule.set(interval=5, byeaster=None)\n    assert rule._rrule._interval == 5\n    assert rule._rrule._byeaster is None\n\n    # Test time zone localization\n    dt = datetime.datetime(2020, 1, 1, tzinfo=mdates.UTC)\n    localized_dt = rule._attach_tzinfo(dt, dateutil.tz.gettz('Pacific/Kiritimati'))\n    assert localized_dt.tzname() == 'Pacific/Kiritimati'\n", "def test_set_epoch_error():\n    mdates._reset_epoch_test_example()\n    mdates.get_epoch()  # Set default.\n    mdates.date2num(datetime.datetime.now())  # Trigger date conversion\n    \n    with pytest.raises(RuntimeError, match='set_epoch must be called before dates plotted.'):\n        mdates.set_epoch('2000-01-01')\n        \n    mdates._reset_epoch_test_example()  # Reset for other tests\n", "def test_set_epoch_errors():\n    mdates._reset_epoch_test_example()\n    with pytest.raises(ValueError, match=\"is not a valid timezone\"):\n        mdates.set_epoch('invalid_epoch')\n    with pytest.raises(TypeError, match=\"must be string or tzinfo subclass\"):\n        mdates.set_epoch(12345)\n    with pytest.raises(RuntimeError, match=\"set_epoch must be called before dates plotted\"):\n        mdates.set_epoch('1970-01-01')\n        mdates.date2num(datetime.datetime(1970, 1, 1))\n        mdates.set_epoch('2000-01-01')\n    mdates._reset_epoch_test_example()\n", "def test_date_formatter_invalid_format():\n    # Test DateFormatter with an invalid date format string\n    with pytest.raises(ValueError, match=\"Invalid format string\"):\n        mdates.DateFormatter('%Q-%W-%E')\n"], "sample_527": ["def test_add_axes():\n    fig = plt.figure()\n    ax1 = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    assert ax1 in fig.axes\n    assert ax1.get_position().bounds == (0.1, 0.1, 0.8, 0.8)\n\n    # Adding an already existing Axes should not add a duplicate\n    fig.add_axes(ax1)\n    assert len(fig.axes) == 1\n    assert fig.axes[0] is ax1\n\n    with pytest.raises(ValueError):\n        fig.add_axes([0.1, 0.1, 0.8, 0.8], projection='polar', axes_class=Axes)\n\n    # Test removing an Axes\n    fig.delaxes(ax1)\n    assert len(fig.axes) == 0\n", "def test_subplots_creation():\n    fig = plt.figure()\n    axs = fig.subplots(2, 2)\n    assert len(axs) == 2\n    assert len(axs[0]) == 2\n    assert isinstance(axs[0, 0], plt.Axes)\n    assert isinstance(axs[1, 1], plt.Axes)\n    plt.close(fig)\n", "def test_subplots():\n    fig = plt.figure()\n    axs = fig.subplots(2, 2)\n    assert len(axs) == 2\n    assert len(axs[0]) == 2\n    assert len(axs[1]) == 2\n\n    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.5, hspace=0.5)\n    assert fig.subplotpars.left == 0.1\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.top == 0.9\n    assert fig.subplotpars.bottom == 0.1\n    assert fig.subplotpars.wspace == 0.5\n    assert fig.subplotpars.hspace == 0.5\n\n    axs[0, 0].plot([0, 1], [0, 1])\n    axs[0, 1].plot([0, 1], [1, 0])\n    axs[1, 0].plot([0, 1], [0.5, 0.5])\n    axs[1, 1].plot([0, 1], [0.25, 0.75])\n    fig.canvas.draw()\n", "def test_subplotparams_update():\n    params = SubplotParams()\n    \n    # Default values\n    assert params.left == mpl.rcParams[\"figure.subplot.left\"]\n    assert params.right == mpl.rcParams[\"figure.subplot.right\"]\n    assert params.bottom == mpl.rcParams[\"figure.subplot.bottom\"]\n    assert params.top == mpl.rcParams[\"figure.subplot.top\"]\n    assert params.wspace == mpl.rcParams[\"figure.subplot.wspace\"]\n    assert params.hspace == mpl.rcParams[\"figure.subplot.hspace\"]\n    \n    # Update values\n    params.update(left=0.2, right=0.8, bottom=0.1, top=0.9, wspace=0.5, hspace=0.3)\n    assert params.left == 0.2\n    assert params.right == 0.8\n    assert params.bottom == 0.1\n    assert params.top == 0.9\n    assert params.wspace == 0.5\n    assert params.hspace == 0.3\n    \n    # Test validation\n    with pytest.raises(ValueError):\n        params.update(left=0.8, right=0.2)\n    with pytest.raises(ValueError):\n        params.update(bottom=0.9, top=0.1)\n", "def test_axes_stack():\n    stack = _AxesStack()\n    ax1 = plt.subplot()\n    ax2 = plt.subplot()\n    \n    # Test adding axes\n    stack.add(ax1)\n    stack.add(ax2)\n    assert stack.as_list() == [ax1, ax2]\n    \n    # Test current method\n    assert stack.current() == ax2\n    \n    # Test bubble method\n    stack.bubble(ax1)\n    assert stack.current() == ax1\n    \n    # Test remove method\n    stack.remove(ax1)\n    assert stack.as_list() == [ax2]\n    \n    # Test adding the same axes again\n    stack.add(ax2)\n    assert stack.as_list() == [ax2]\n    \n    plt.close('all')\n"], "sample_528": ["def test_remove_blacklisted_style_params():\n    style_dict = {\n        'interactive': True,\n        'backend': 'TkAgg',\n        'figure.facecolor': 'blue',\n        'savefig.dpi': 300\n    }\n    expected_result = {\n        'figure.facecolor': 'blue',\n        'savefig.dpi': 300\n    }\n    result = mpl.style.core._remove_blacklisted_style_params(style_dict)\n    assert result == expected_result\n", "def test_remove_blacklisted_style_params():\n    test_dict = {\n        'interactive': True,\n        'backend': 'Agg',\n        PARAM: VALUE,\n        'timezone': 'UTC'\n    }\n    expected_dict = {PARAM: VALUE}\n    result = mpl.style.core._remove_blacklisted_style_params(test_dict)\n    assert result == expected_dict\n", "def test_use_invalid_style():\n    invalid_style = 'non_existent_style'\n    with pytest.raises(OSError, match=f\"{invalid_style!r} not found in the style library\"):\n        style.use(invalid_style)\n", "def test_reload_library():\n    with temp_style('test_reload', DUMMY_SETTINGS):\n        assert 'test_reload' in style.available\n        style.library.pop('test_reload')\n        assert 'test_reload' not in style.library\n        style.reload_library()\n        assert 'test_reload' in style.library\n", "def test_reload_library():\n    original_available_styles = set(style.available)\n    new_style_name = 'new_style'\n    new_style_settings = {PARAM: 'blue'}\n\n    with temp_style(new_style_name, new_style_settings):\n        # Ensure the new style is available after reload\n        assert new_style_name in style.available\n        assert set(style.available) == original_available_styles.union({new_style_name})\n\n    # Ensure the new style is removed after the context manager\n    assert set(style.available) == original_available_styles\n"], "sample_529": ["def test_draggable_legend_update_loc():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], label='line')\n    leg = ax.legend(draggable=True, loc='upper right')\n    draggable_leg = leg.get_draggable()\n    assert isinstance(draggable_leg, DraggableLegend)\n    \n    # Mock the location update to check if the loc parameter is updated correctly\n    with mock.patch.object(DraggableLegend, '_update_loc', wraps=draggable_leg._update_loc) as mock_update_loc:\n        draggable_leg.finalize_offset()\n        mock_update_loc.assert_called_once()\n    \n    # Ensure the legend's loc is updated correctly\n    assert leg._loc == draggable_leg.get_loc_in_canvas()\n", "def test_legend_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend()\n    original_bbox = leg.get_bbox_to_anchor()\n\n    # Set a new bbox_to_anchor and check if it's updated correctly\n    new_bbox = (0.1, 0.2, 0.3, 0.4)\n    leg.set_bbox_to_anchor(new_bbox)\n    updated_bbox = leg.get_bbox_to_anchor().bounds\n    assert updated_bbox == new_bbox\n\n    # Reset bbox_to_anchor to None and check if it falls back to parent bbox\n    leg.set_bbox_to_anchor(None)\n    reset_bbox = leg.get_bbox_to_anchor()\n    assert reset_bbox == ax.bbox\n\n    # Test with a BboxBase instance\n    from matplotlib.transforms import Bbox\n    bbox_instance = Bbox.from_bounds(0.5, 0.5, 0.2, 0.2)\n    leg.set_bbox_to_anchor(bbox_instance)\n    assert leg.get_bbox_to_anchor().bounds == bbox_instance.bounds\n", "def test_draggable_legend():\n    \"\"\"Test that draggable legend can be moved and updated properly.\"\"\"\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='movable line')\n    leg = ax.legend(draggable=True)\n\n    # Simulate dragging the legend to a new location\n    old_loc = leg._loc\n    new_loc = (0.5, 0.5)\n    draggable = leg.set_draggable(True)\n    draggable.finalize_offset = mock.Mock()\n    draggable.finalize_offset()\n    leg._update_loc(new_loc)\n\n    assert leg._loc != old_loc\n    assert leg._loc == new_loc\n", "def test_legend_draggable_finalize_offset():\n    # Test that the finalize_offset method updates the legend location correctly\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], label='test')\n    leg = ax.legend(loc='upper left', draggable=True)\n    \n    # Simulate dragging the legend to a new location\n    draggable = leg._draggable\n    draggable.start_drag(draggable._dragging_target, mock.Mock(x=50, y=50))\n    draggable.end_drag(mock.Mock(x=100, y=100))\n    \n    # Finalize the offset and check the new location\n    draggable.finalize_offset()\n    \n    assert leg.get_bbox_to_anchor().bounds[:2] == (100, 100)\n", "def test_legend_draggable_update_loc():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='draggable_test')\n    leg = ax.legend(draggable=True, update='loc')\n    assert leg.get_draggable()\n    assert leg._draggable._update == 'loc'\n    initial_loc = leg._loc\n    # Simulate dragging\n    leg._draggable.finalize_offset()\n    new_loc = leg._loc\n    assert initial_loc != new_loc  # Ensure the location has been updated\n"], "sample_530": ["def test_offsetbox_contains():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    anchored_box = AnchoredOffsetbox(\n        loc='center',\n        child=da,\n        pad=0.,\n        frameon=False,\n        bbox_to_anchor=(.5, .5),\n        bbox_transform=ax.transAxes,\n        borderpad=0.)\n    ax.add_artist(anchored_box)\n    \n    # Simulate a mouse event at the center of the box\n    event = MouseEvent(\"button_press_event\", fig.canvas, 0.5, 0.5, MouseButton.LEFT)\n    inside, details = anchored_box.contains(event)\n    assert inside\n\n    # Simulate a mouse event outside the box\n    event = MouseEvent(\"button_press_event\", fig.canvas, 0.1, 0.1, MouseButton.LEFT)\n    inside, details = anchored_box.contains(event)\n    assert not inside\n", "def test_offsetbox_set_offset_callable():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n\n        return width / 2, height / 2\n\n    da.set_offset(offset_callable)\n    anchored_box = AnchoredOffsetbox(loc='center', child=da, pad=0., frameon=False)\n    ax.add_artist(anchored_box)\n\n    fig.canvas.draw()\n    assert anchored_box.get_offset(100, 100, 0, 0, fig.canvas.get_renderer()) == (50, 50)\n", "def test_offsetbox_contains():\n    # Test if the contains method correctly delegates to children\n    fig, ax = plt.subplots()\n    size = 100\n    da = DrawingArea(size, size)\n    bg = mpatches.Rectangle((0, 0), size, size, facecolor='#CCCCCC', edgecolor='None', linewidth=0)\n    line = mlines.Line2D([0, size], [size/2, size/2], color='black', linewidth=10)\n    \n    da.add_artist(bg)\n    da.add_artist(line)\n    \n    ax.add_artist(da)\n    fig.canvas.draw()\n    \n    # Simulate a mouse event within the bounds of the line\n    event = MouseEvent('button_press_event', fig.canvas, x=size/2, y=size/2, button=MouseButton.LEFT)\n    \n    contains, _ = da.contains(event)\n    assert contains, \"The DrawingArea should contain the point within the line.\"\n    \n    # Simulate a mouse event outside the bounds of the DrawingArea\n    event = MouseEvent('button_press_event', fig.canvas, x=size*2, y=size*2, button=MouseButton.LEFT)\n    \n    contains, _ = da.contains(event)\n    assert not contains, \"The DrawingArea should not contain the point outside its bounds.\"\n", "def test_offsetbox_setters():\n    # Test OffsetBox setters and getters\n    fig, ax = plt.subplots()\n    size = 100\n    da = DrawingArea(size, size, clip=True)\n    \n    # Test setting and getting width\n    da.set_width(150)\n    assert da.width == 150\n    \n    # Test setting and getting height\n    da.set_height(200)\n    assert da.height == 200\n    \n    # Test setting and getting offset\n    da.set_offset((10, 20))\n    assert da.get_offset(0, 0, 0, 0, fig.canvas.get_renderer()) == (10, 20)\n    \n    # Test get_visible_children and get_children\n    rect = mpatches.Rectangle((0, 0), size, size, facecolor='#CCCCCC', edgecolor='None', linewidth=0)\n    da.add_artist(rect)\n    assert da.get_visible_children() == [rect]\n    assert da.get_children() == [rect]\n    \n    # Test PaddedBox\n    pb = PaddedBox(da, pad=10)\n    assert pb.pad == 10\n    pb.draw(fig.canvas.get_renderer())\n    \n    # Test TextArea\n    text_area = TextArea(\"Sample Text\", textprops={'fontsize': 10})\n    assert text_area.get_text() == \"Sample Text\"\n    text_area.set_text(\"New Text\")\n    assert text_area.get_text() == \"New Text\"\n    \n    # Check multiline baseline\n    text_area.set_multilinebaseline(True)\n    assert text_area.get_multilinebaseline() is True\n    text_area.set_multilinebaseline(False)\n    assert text_area.get_multilinebaseline() is False\n\n    # Test if OffsetBox.get_window_extent works without errors\n    ab = AnnotationBbox(text_area, (0.5, 0.5))\n    ax.add_artist(ab)\n    fig.canvas.draw()\n    assert ab.get_window_extent() is not None\n", "def test_offsetbox_set_offset_callable():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    anchored_box = AnchoredOffsetbox(loc='center', child=da)\n\n        return w / 2, h / 2\n\n    anchored_box.set_offset(custom_offset)\n    ax.add_artist(anchored_box)\n    fig.canvas.draw()\n\n    renderer = fig.canvas.get_renderer()\n    w, h, xd, yd = anchored_box.get_extent(renderer)\n    offset = anchored_box.get_offset(w, h, xd, yd, renderer)\n\n    assert offset == (w / 2, h / 2)\n"], "sample_531": ["def test_subplotparams_update():\n    # Test the SubplotParams update method with valid parameters\n    params = mpl.rcParams\n    params.update({\n        \"figure.subplot.left\": 0.1,\n        \"figure.subplot.bottom\": 0.1,\n        \"figure.subplot.right\": 0.9,\n        \"figure.subplot.top\": 0.9,\n        \"figure.subplot.wspace\": 0.2,\n        \"figure.subplot.hspace\": 0.2\n    })\n    sp = mpl.figure.SubplotParams()\n    sp.update(left=0.2, bottom=0.2, right=0.8, top=0.8, wspace=0.3, hspace=0.3)\n    \n    assert sp.left == 0.2\n    assert sp.bottom == 0.2\n    assert sp.right == 0.8\n    assert sp.top == 0.8\n    assert sp.wspace == 0.3\n    assert sp.hspace == 0.3\n\n    # Test the SubplotParams update method with invalid parameters\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        sp.update(left=0.9, right=0.8)\n\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        sp.update(bottom=0.9, top=0.8)\n", "def test_update_subplot_params():\n    params = SubplotParams()\n    \n    # Test valid updates\n    params.update(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n    assert params.left == 0.1\n    assert params.bottom == 0.1\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    # Test invalid updates where left >= right\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=1.0, right=0.5)\n\n    # Test invalid updates where bottom >= top\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=0.9, top=0.5)\n", "def test_subplot_mosaic_invalid_key():\n    fig = plt.figure()\n    with pytest.raises(KeyError, match=\"Invalid character in mosaic layout\"):\n        fig.subplot_mosaic(\"AAB;CCD\")\n", "def test_stale_figure_callback():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    assert fig.stale\n    fig.stale = False\n    _stale_figure_callback(ax, True)\n    assert fig.stale\n    _stale_figure_callback(ax, False)\n    assert not fig.stale\n", "def test_subplot_params_update():\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params = SubplotParams(left=0.8, right=0.2)\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params = SubplotParams(bottom=0.8, top=0.2)\n    \n    params = SubplotParams(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n    assert params.left == 0.1\n    assert params.bottom == 0.1\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    params.update(left=0.2, wspace=0.4)\n    assert params.left == 0.2\n    assert params.wspace == 0.4\n    assert params.right == 0.9  # unchanged\n    assert params.hspace == 0.2  # unchanged\n"], "sample_532": ["def test_clabeltext_get_rotation():\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    y = np.arange(10)\n    z = np.random.random((10, 10))\n    cs = ax.contour(x, y, z)\n    \n    # Use the deprecated ClabelText to ensure get_rotation works as expected\n    with pytest.deprecated_call():\n        cs.clabel(use_clabeltext=True)\n\n    assert isinstance(cs.labelTexts[0], ClabelText)\n    rotation = cs.labelTexts[0].get_rotation()\n    \n    # Assert that the rotation is a float (angle in degrees)\n    assert isinstance(rotation, float)\n", "def test_contour_label_props_deprecation_warning():\n    # Test that accessing the deprecated label properties issues a warning.\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    fig, ax = plt.subplots()\n    CS = ax.contour(X, Y, Z)\n    with pytest.warns(mpl.MatplotlibDeprecationWarning):\n        _ = CS.labelFontProps\n    with pytest.warns(mpl.MatplotlibDeprecationWarning):\n        _ = CS.labelFontSizeList\n    with pytest.warns(mpl.MatplotlibDeprecationWarning):\n        _ = CS.labelTextsList\n", "def test_clabel_text_rotation():\n    # Test ClabelText rotation to ensure it rotates properly with transformation.\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    y = np.arange(9)\n    z = np.random.random((9, 10))\n    cs = ax.contour(x, y, z)\n    # Add clabels using ClabelText with transform_rotates_text=True\n    cs.clabel(use_clabeltext=True)\n    # Check if the rotation is set correctly\n    for label in cs.labelTexts:\n        assert label.get_rotation_mode() == \"anchor\", \"Label rotation mode is not 'anchor'\"\n\n    # Add clabels using ClabelText with transform_rotates_text=False\n    cs.clabel(use_clabeltext=False)\n    # Check if the rotation is set correctly\n    for label in cs.labelTexts:\n        assert label.get_rotation_mode() == None, \"Label rotation mode should be None\"\n", "def test_ClabelText_rotation():\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, 'test', transform=ax.transAxes)\n    clabel_text = ClabelText(x=0.5, y=0.5, text='label', transform=ax.transData, rotation=45)\n    ax.add_artist(clabel_text)\n    renderer = fig.canvas.get_renderer()\n    fig.draw(renderer)\n    assert clabel_text.get_rotation() == 45\n", "def test_clabeltext_rotation():\n    # Test the deprecated ClabelText class and its get_rotation method\n    # Create a simple contour plot\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z = np.exp(-X**2 - Y**2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(X, Y, Z)\n\n    # Add a label and use ClabelText\n    cs.clabel(use_clabeltext=True)\n\n    # Access the ClabelText object\n    label_text = cs.labelTexts[0]\n    if isinstance(label_text, ClabelText):\n        rotation = label_text.get_rotation()\n        assert isinstance(rotation, float), \"Rotation should be a float value\"\n    else:\n        pytest.fail(\"Expected ClabelText instance\")\n"], "sample_533": ["def test_clabeltext_deprecated():\n    x = np.arange(10)\n    y = np.arange(10)\n    z = np.random.random((10, 10))\n\n    fig, ax = plt.subplots()\n    with pytest.warns(mpl.MatplotlibDeprecationWarning, match=\"cs.labelTexts[0].get_font()\"):\n        cs = ax.contour(x, y, z)\n        assert cs.labelFontProps\n        assert cs.labelFontSizeList\n        assert cs.labelTextsList\n", "def test_clabel_properties():\n    # Test deprecated properties of ContourLabeler\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n    \n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    clabels = cs.clabel()\n\n    with pytest.warns(mpl.MatplotlibDeprecationWarning):\n        font_props = cs.labelFontProps\n        assert isinstance(font_props, mpl.font_manager.FontProperties)\n\n    with pytest.warns(mpl.MatplotlibDeprecationWarning):\n        font_size_list = cs.labelFontSizeList\n        assert isinstance(font_size_list, list)\n        assert len(font_size_list) == len(cs.labelLevelList)\n        assert all(isinstance(size, (int, float)) for size in font_size_list)\n\n    with pytest.warns(mpl.MatplotlibDeprecationWarning):\n        texts_list = cs.labelTextsList\n        assert isinstance(texts_list, list)\n        assert len(texts_list) == len(cs.labelTexts)\n        assert all(isinstance(text, mpl.text.Text) for text in texts_list)\n", "def test_clabeltext_deprecated():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.linspace(0, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X) * np.cos(Y)\n    cs = ax.contour(X, Y, Z)\n\n    with pytest.warns(MatplotlibDeprecationWarning, match=\"ClabelText is deprecated\"):\n        t = ClabelText(x=5, y=5, text='test')\n        t.get_rotation()\n", "def test_contour_label_manual_single_point():\n    # Test manual label placement with a single point\n    x = np.arange(10)\n    y = np.arange(10)\n    z = np.random.random((10, 10))\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n\n    # Set manual label placement with a single point\n    pts = [(5.0, 5.0)]\n    labels = cs.clabel(manual=pts)\n\n    # Check if the label is placed correctly\n    assert len(labels) == 1\n    assert labels[0].get_position() == (5.0, 5.0)\n", "def test_clabel_rotations():\n    # Test to verify the correctness of rotation for labels\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    fig, ax = plt.subplots()\n    CS = ax.contour(X, Y, Z)\n\n    # Test with use_clabeltext=False\n    clabels1 = CS.clabel(use_clabeltext=False)\n    for label in clabels1:\n        assert not label.get_transform_rotates_text()\n\n    # Test with use_clabeltext=True\n    clabels2 = CS.clabel(use_clabeltext=True)\n    for label in clabels2:\n        assert label.get_transform_rotates_text()\n"], "sample_534": ["def test_clabel_text_deprecated_warning():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    \n    with pytest.warns(MatplotlibDeprecationWarning, match=\"Text.set_transform_rotates_text\"):\n        cs.clabel(use_clabeltext=True)\n", "def test_find_nearest_contour_transform():\n    # Create a rotated meshgrid to test transformations\n    x = np.linspace(-3.0, 3.0, 100)\n    y = np.linspace(-2.0, 2.0, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.exp(-0.5 * (X**2 + Y**2))\n\n    # Apply an affine transformation (rotation + translation)\n    tform = mtransforms.Affine2D().rotate_deg(45).translate(1, 1) + plt.gca().transData\n\n    fig, ax = plt.subplots()\n    ax.set_transform(tform)\n    CS = ax.contour(X, Y, Z)\n\n    # Test finding nearest contour point in data space\n    nearest_contour = CS.find_nearest_contour(1, 1, transform=plt.gca().transData)\n    assert nearest_contour is not None\n\n    # Test finding nearest contour point in display space\n    nearest_contour_disp = CS.find_nearest_contour(1, 1, transform=None)\n    assert nearest_contour_disp is not None\n\n    # Ensure the contour points found are different due to transformation\n    assert nearest_contour != nearest_contour_disp\n", "def test_clabeltext_deprecation():\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    y = np.arange(10)\n    xg, yg = np.meshgrid(x, y)\n    z = np.random.random((10, 10))\n    cs = ax.contour(xg, yg, z)\n\n    with pytest.warns(mpl.MatplotlibDeprecationWarning, match=\"ClabelText\"):\n        t = mpl.contour.ClabelText(x=0, y=0, text='test')\n        assert isinstance(t, mpl.contour.ClabelText)\n    \n    with pytest.warns(mpl.MatplotlibDeprecationWarning, match=\"labelFontProps\"):\n        _ = cs.labelFontProps\n\n    with pytest.warns(mpl.MatplotlibDeprecationWarning, match=\"labelFontSizeList\"):\n        _ = cs.labelFontSizeList\n\n    with pytest.warns(mpl.MatplotlibDeprecationWarning, match=\"labelTextsList\"):\n        _ = cs.labelTextsList\n\n    with pytest.warns(mpl.MatplotlibDeprecationWarning, match=\"set_label_props\"):\n        t = Text(x=0, y=0, text='test')\n        cs.set_label_props(t, 'new text', 'red')\n        assert t.get_text() == 'new text'\n        assert same_color(t.get_color(), 'red')\n", "def test_clabeltext_deprecation():\n    from matplotlib.contour import ClabelText\n    with pytest.warns(mpl.MatplotlibDeprecationWarning, match=\"ClabelText is deprecated\"):\n        text = ClabelText(0, 0, \"test\")\n    assert isinstance(text, Text)\n", "def test_clabel_text_deprecated():\n    fig, ax = plt.subplots()\n    x = np.linspace(-3.0, 3.0, 100)\n    y = np.linspace(-3.0, 3.0, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n\n    cs = ax.contour(X, Y, Z, levels=8)\n\n    with pytest.warns(mpl.MatplotlibDeprecationWarning,\n                      match=r\"ClabelText.* is deprecated since Matplotlib 3.7\"):\n        labels = cs.clabel(use_clabeltext=True)\n\n    assert isinstance(labels[0], ClabelText)\n\n    # Verify that the new method is also working correctly\n    with pytest.warns(mpl.MatplotlibDeprecationWarning,\n                      match=r\"ClabelText.* is deprecated since Matplotlib 3.7\"):\n        labels = cs.clabel(use_clabeltext=False)\n    assert not any(isinstance(label, ClabelText) for label in labels)\n"], "sample_535": ["def test_table_scale():\n    # Test the scale method to ensure it scales cells correctly\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    table.add_cell(0, 0, width=1, height=1, text='A')\n    table.add_cell(0, 1, width=1, height=1, text='B')\n    table.add_cell(1, 0, width=1, height=1, text='C')\n    table.add_cell(1, 1, width=1, height=1, text='D')\n    \n    # Scale cells by factor of 2\n    table.scale(2, 2)\n    \n    assert table[0, 0].get_width() == 2\n    assert table[0, 0].get_height() == 2\n    assert table[0, 1].get_width() == 2\n    assert table[0, 1].get_height() == 2\n    assert table[1, 0].get_width() == 2\n    assert table[1, 0].get_height() == 2\n    assert table[1, 1].get_width() == 2\n    assert table[1, 1].get_height() == 2\n", "def test_cell_text_position():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    cell = table.add_cell(0, 0, width=1, height=1, text='Test', loc='center')\n    renderer = fig.canvas.get_renderer()\n\n    # Initial text position\n    bbox_initial = cell.get_text().get_window_extent(renderer)\n    \n    # Manually update text position and check if updated correctly\n    cell._set_text_position(renderer)\n    bbox_updated = cell.get_text().get_window_extent(renderer)\n    \n    assert bbox_initial != bbox_updated\n\n    # Check if text is centered within the cell\n    cell_center = cell.get_window_extent(renderer).center\n    text_center = bbox_updated.center\n    assert np.isclose(cell_center[0], text_center[0], atol=1)\n    assert np.isclose(cell_center[1], text_center[1], atol=1)\n", "def test_set_fontsize():\n    # Create a table and set a specific font size\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    \n    cell1 = table.add_cell(0, 0, width=1, height=1, text=\"Test1\")\n    cell2 = table.add_cell(1, 0, width=1, height=1, text=\"Test2\")\n    \n    # Set font size for the entire table\n    table.set_fontsize(15)\n    \n    # Verify that the font size for individual cells matches the set font size\n    assert cell1.get_fontsize() == 15\n    assert cell2.get_fontsize() == 15\n", "def test_set_text_props():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n\n    # Add a cell and update its text properties\n    cell = table.add_cell(0, 0, 1, 1, text=\"Initial text\")\n    assert cell.get_text().get_text() == \"Initial text\"\n    cell.set_text_props(text=\"Updated text\", color='red', fontsize=15)\n\n    # Verify the updates\n    assert cell.get_text().get_text() == \"Updated text\"\n    assert cell.get_text().get_color() == 'red'\n    assert cell.get_text().get_fontsize() == 15\n", "def test_table_scaling():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    table.add_cell(0, 0, width=1, height=2, text='A')\n    table.add_cell(0, 1, width=2, height=2, text='B')\n    table.add_cell(1, 0, width=1, height=1, text='C')\n    table.add_cell(1, 1, width=2, height=1, text='D')\n\n    # Scale the table\n    table.scale(2, 0.5)\n    \n    # Check cell dimensions after scaling\n    assert table[0, 0].get_width() == 2\n    assert table[0, 0].get_height() == 1\n    assert table[0, 1].get_width() == 4\n    assert table[0, 1].get_height() == 1\n    assert table[1, 0].get_width() == 2\n    assert table[1, 0].get_height() == 0.5\n    assert table[1, 1].get_width() == 4\n    assert table[1, 1].get_height() == 0.5\n"], "sample_536": ["def test_lock_draw():\n    lock = widgets.LockDraw()\n    widget_a, widget_b = object(), object()\n\n    # Test initial state\n    assert lock.available(widget_a)\n    assert not lock.locked()\n    assert not lock.isowner(widget_a)\n\n    # Test acquiring the lock\n    lock(widget_a)\n    assert not lock.available(widget_b)\n    assert lock.locked()\n    assert lock.isowner(widget_a)\n\n    # Test releasing the lock\n    lock.release(widget_a)\n    assert lock.available(widget_b)\n    assert not lock.locked()\n    assert not lock.isowner(widget_a)\n\n    # Test acquiring the lock by another widget\n    lock(widget_b)\n    assert not lock.available(widget_a)\n    assert lock.locked()\n    assert lock.isowner(widget_b)\n\n    # Test releasing the lock by the wrong widget\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(widget_a)\n    assert lock.locked()\n    assert lock.isowner(widget_b)\n\n    # Test ValueError when acquiring a locked lock\n    with pytest.raises(ValueError, match='already locked'):\n        lock(widget_a)\n    assert lock.locked()\n    assert lock.isowner(widget_b)\n", "def test_lock_draw():\n    lock = widgets.LockDraw()\n    obj1 = object()\n    obj2 = object()\n\n    # Initially, lock should be available\n    assert lock.available(obj1)\n    assert lock.available(obj2)\n    assert not lock.locked()\n\n    # Lock the draw for obj1\n    lock(obj1)\n    assert lock.locked()\n    assert not lock.available(obj2)\n    assert lock.available(obj1)\n    assert lock.isowner(obj1)\n    assert not lock.isowner(obj2)\n\n    # Releasing the lock for obj2 should raise an error\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(obj2)\n\n    # Releasing the lock for obj1\n    lock.release(obj1)\n    assert not lock.locked()\n    assert lock.available(obj1)\n    assert lock.available(obj2)\n", "def test_lockdraw():\n    lock = widgets.LockDraw()\n    \n    obj1 = object()\n    obj2 = object()\n    \n    assert lock.available(obj1)\n    assert not lock.locked()\n    \n    lock(obj1)\n    assert not lock.available(obj2)\n    assert lock.isowner(obj1)\n    assert lock.locked()\n    \n    with pytest.raises(ValueError, match='already locked'):\n        lock(obj2)\n    \n    lock.release(obj1)\n    assert lock.available(obj2)\n    assert not lock.locked()\n    \n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(obj2)\n", "def test_lock_draw():\n    lock = widgets.LockDraw()\n    widget1 = object()\n    widget2 = object()\n\n    # Initially, the lock should be available.\n    assert lock.available(widget1)\n    assert not lock.locked()\n    assert not lock.isowner(widget1)\n\n    # Acquire lock for widget1\n    lock(widget1)\n    assert not lock.available(widget2)\n    assert lock.locked()\n    assert lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n\n    # Trying to acquire the lock for widget2 should raise a ValueError.\n    with pytest.raises(ValueError):\n        lock(widget2)\n\n    # Release the lock from widget1\n    lock.release(widget1)\n    assert not lock.locked()\n    assert lock.available(widget2)\n    assert not lock.isowner(widget1)\n", "def test_lock_draw():\n    lock = widgets.LockDraw()\n    widget1 = mock.Mock()\n    widget2 = mock.Mock()\n\n    # Test initial state\n    assert lock.available(widget1)\n    assert not lock.locked()\n\n    # Test acquiring lock\n    lock(widget1)\n    assert lock.locked()\n    assert not lock.available(widget2)\n    assert lock.available(widget1)\n    assert lock.isowner(widget1)\n\n    # Test releasing lock\n    lock.release(widget1)\n    assert not lock.locked()\n    assert lock.available(widget2)\n\n    # Test errors on invalid release\n    with pytest.raises(ValueError):\n        lock.release(widget2)\n\n    # Test errors on double lock\n    lock(widget1)\n    with pytest.raises(ValueError):\n        lock(widget2)\n"], "sample_537": ["def test_window_hanning():\n    \"\"\"Test window_hanning function to ensure it applies the Hanning window correctly.\"\"\"\n    data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    windowed_data = mlab.window_hanning(data)\n    expected_window = np.hanning(len(data))\n    expected_result = expected_window * data\n    assert_array_almost_equal_nulp(windowed_data, expected_result)\n", "def test_stride_windows_noverlap_greater_than_0():\n    x = np.arange(20)\n    n = 5\n    noverlap = 2\n    result = mlab.stride_windows(x, n, noverlap=noverlap)\n    expected_result = np.array([\n        [0, 3, 6, 9, 12, 15],\n        [1, 4, 7, 10, 13, 16],\n        [2, 5, 8, 11, 14, 17],\n        [3, 6, 9, 12, 15, 18],\n        [4, 7, 10, 13, 16, 19]\n    ])\n    assert_array_equal(result, expected_result)\n", "def test_detrend_custom_function():\n    \"\"\"Test the detrend function with a custom detrend function.\"\"\"\n        return x - np.median(x, axis=axis, keepdims=True)\n\n    x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n    expected = np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n    \n    result = mlab.detrend(x, key=custom_detrend)\n    assert_array_equal(result, expected)\n\n    x_2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    expected_2d = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n    \n    result_2d = mlab.detrend(x_2d, key=custom_detrend, axis=1)\n    assert_array_equal(result_2d, expected_2d)\n\n    # Test with axis=None for a 1D array\n    result_axis_none = mlab.detrend(x, key=custom_detrend, axis=None)\n    assert_array_equal(result_axis_none, expected)\n    \n    # Test with axis=None for a 2D array\n    result_2d_axis_none = mlab.detrend(x_2d, key=custom_detrend, axis=None)\n    expected_2d_axis_none = np.array([[-4, -3, -2], [1, 2, 3], [6, 7, 8]])\n    assert_array_equal(result_2d_axis_none, expected_2d_axis_none)\n", "def test_stride_windows_non_integer_steps():\n    x = np.arange(20)\n    with pytest.raises(ValueError):\n        mlab.stride_windows(x, 5, noverlap=2.5)\n", "def test_detrend_invalid_key():\n    x = np.arange(10)\n    with pytest.raises(ValueError, match=\"Unknown value for key: 'invalid'\"):\n        mlab.detrend(x, key=\"invalid\")\n"], "sample_538": ["def test_fully_contains():\n    bbox = mtransforms.Bbox.from_extents(1, 1, 4, 5)\n    \n    assert bbox.fully_contains(2, 3)\n    assert not bbox.fully_contains(1, 3)\n    assert not bbox.fully_contains(4, 3)\n    assert not bbox.fully_contains(2, 1)\n    assert not bbox.fully_contains(2, 5)\n", "def test_bbox_transformation():\n    bbox = mtransforms.Bbox([[0, 0], [2, 2]])\n    transform = mtransforms.Affine2D().scale(2, 3)\n    transformed_bbox = bbox.transformed(transform)\n\n    expected_points = np.array([[0, 0], [4, 6]])\n    assert_array_almost_equal(transformed_bbox.get_points(), expected_points)\n\n    # Verify transformation works with non-affine transformations\n    non_affine_transform = mtransforms.TransformedBbox(bbox, transform)\n    non_affine_transformed_bbox = bbox.transformed(non_affine_transform)\n    assert_array_almost_equal(non_affine_transformed_bbox.get_points(), expected_points)\n", "def test_bbox_expanded():\n    # Test the expanded method of Bbox\n    bbox = mtransforms.Bbox([[1, 1], [3, 4]])\n    expanded_bbox = bbox.expanded(2, 3)\n    expected_bbox = mtransforms.Bbox([[-1, -2.5], [5, 7.5]])\n    assert_bbox_eq(expanded_bbox, expected_bbox)\n\n    # Check that expanding with factor 1 doesn't change the bbox\n    expanded_bbox_same = bbox.expanded(1, 1)\n    assert_bbox_eq(expanded_bbox_same, bbox)\n\n    # Check expanding by a factor < 1\n    bbox2 = mtransforms.Bbox([[0, 0], [4, 4]])\n    expanded_bbox_small = bbox2.expanded(0.5, 0.5)\n    expected_bbox_small = mtransforms.Bbox([[1, 1], [3, 3]])\n    assert_bbox_eq(expanded_bbox_small, expected_bbox_small)\n", "def test_bounding_box_transformations():\n    # Test the transformations of bounding boxes\n    \n    bbox = mtransforms.Bbox.from_extents(0, 0, 1, 1)\n    trans = mtransforms.Affine2D().scale(2, 2).translate(1, 1)\n    \n    # Check transformation\n    trans_bbox = bbox.transformed(trans)\n    expected_points = np.array([[1, 1], [3, 3]])\n    assert_array_equal(trans_bbox.get_points(), expected_points)\n\n    # Check translation\n    trans = mtransforms.Affine2D().translate(2, 2)\n    trans_bbox = bbox.transformed(trans)\n    expected_points = np.array([[2, 2], [3, 3]])\n    assert_array_equal(trans_bbox.get_points(), expected_points)\n\n    # Check rotation\n    trans = mtransforms.Affine2D().rotate_deg(90)\n    trans_bbox = bbox.transformed(trans)\n    expected_points = np.array([[0, 0], [-1, 1]])\n    assert_array_almost_equal(trans_bbox.get_points(), expected_points, decimal=7)\n\n    # Check skew\n    trans = mtransforms.Affine2D().skew_deg(30, 0)\n    trans_bbox = bbox.transformed(trans)\n    expected_points = np.array([[0, 0], [1.57735027, 1]])\n    assert_array_almost_equal(trans_bbox.get_points(), expected_points, decimal=7)\n", "def test_lockable_bbox_no_lock():\n    orig = mtransforms.Bbox.unit()\n    locked = mtransforms.LockableBbox(orig)\n\n    # LockableBbox should have no locked elements initially.\n    assert locked.locked_x0 is None\n    assert locked.locked_y0 is None\n    assert locked.locked_x1 is None\n    assert locked.locked_y1 is None\n\n    # All elements should match the original Bbox.\n    assert locked.x0 == orig.x0\n    assert locked.y0 == orig.y0\n    assert locked.x1 == orig.x1\n    assert locked.y1 == orig.y1\n\n    # Changing the original Bbox should update all elements.\n    orig.set_points(orig.get_points() + 5)\n    assert locked.x0 == orig.x0\n    assert locked.y0 == orig.y0\n    assert locked.x1 == orig.x1\n    assert locked.y1 == orig.y1\n"], "sample_539": ["def test_lock_draw():\n    lock = widgets.LockDraw()\n    widget1 = mock.Mock()\n    widget2 = mock.Mock()\n    \n    # Initial state: no owner\n    assert lock.available(widget1)\n    assert not lock.locked()\n    \n    # Lock with widget1\n    lock(widget1)\n    assert not lock.available(widget2)\n    assert lock.locked()\n    assert lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n    \n    # Release lock with widget1\n    lock.release(widget1)\n    assert lock.available(widget2)\n    assert not lock.locked()\n    \n    # Attempt to release lock with a widget that doesn't own it\n    lock(widget1)\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(widget2)\n    \n    # Attempt to lock when already locked\n    with pytest.raises(ValueError, match='already locked'):\n        lock(widget2)\n", "def test_button_widget():\n    fig, ax = plt.subplots()\n    on_click = mock.Mock(spec=noop, return_value=None)\n    \n    button = widgets.Button(ax, label='Click me', color='0.85', hovercolor='0.95')\n    button.on_clicked(on_click)\n    \n    # Simulate button click\n    do_event(button, 'button_press_event', xdata=0.5, ydata=0.5, button=1)\n    do_event(button, 'button_release_event', xdata=0.5, ydata=0.5, button=1)\n    \n    on_click.assert_called_once()\n", "def test_lock_draw():\n    lock = widgets.LockDraw()\n\n    class DummyWidget:\n            self.name = name\n\n    widget1 = DummyWidget(\"widget1\")\n    widget2 = DummyWidget(\"widget2\")\n\n    assert lock.available(widget1)\n    assert lock.available(widget2)\n    assert not lock.locked()\n    assert not lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n\n    # Lock with widget1\n    lock(widget1)\n    assert not lock.available(widget2)\n    assert lock.locked()\n    assert lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n\n    # Release lock from widget1\n    lock.release(widget1)\n    assert lock.available(widget2)\n    assert not lock.locked()\n    assert not lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n\n    # Attempt to release lock from widget2 when it does not own it\n    with pytest.raises(ValueError, match=\"you do not own this lock\"):\n        lock.release(widget2)\n\n    # Attempt to acquire lock when already locked by widget1\n    lock(widget1)\n    with pytest.raises(ValueError, match=\"already locked\"):\n        lock(widget1)\n", "def test_slider_valfmt():\n    fig, ax = plt.subplots()\n    # Test with a string format\n    slider = widgets.Slider(ax=ax, label='Value', valmin=0, valmax=1, valinit=0.5, valfmt='%1.2f')\n    assert slider.valtext.get_text() == '0.50'\n    \n    slider.set_val(0.75)\n    assert slider.valtext.get_text() == '0.75'\n    \n    # Test with None, should use ScalarFormatter\n    slider = widgets.Slider(ax=ax, label='Value', valmin=0, valmax=1, valinit=0.5, valfmt=None)\n    slider.set_val(0.25)\n    assert slider.valtext.get_text() == '0.25'\n    \n    # Test with a different string format\n    slider = widgets.Slider(ax=ax, label='Value', valmin=0, valmax=100, valinit=50, valfmt='%1.1f')\n    assert slider.valtext.get_text() == '50.0'\n    \n    slider.set_val(75)\n    assert slider.valtext.get_text() == '75.0'\n", "def test_lock_draw():\n    lock_draw = widgets.LockDraw()\n\n    widget1 = mock.Mock()\n    widget2 = mock.Mock()\n\n    # Initially, the lock should be available\n    assert lock_draw.available(widget1)\n    assert lock_draw.available(widget2)\n\n    # Acquire the lock with widget1\n    lock_draw(widget1)\n    assert not lock_draw.available(widget2)\n    assert lock_draw.available(widget1)\n    assert lock_draw.isowner(widget1)\n    assert not lock_draw.isowner(widget2)\n\n    # Try to release the lock with widget2 (should raise an error)\n    with pytest.raises(ValueError, match=\"you do not own this lock\"):\n        lock_draw.release(widget2)\n\n    # Release the lock with widget1\n    lock_draw.release(widget1)\n    assert lock_draw.available(widget2)\n    assert not lock_draw.isowner(widget1)\n    assert not lock_draw.isowner(widget2)\n\n    # Try to acquire the lock again with widget2\n    lock_draw(widget2)\n    assert not lock_draw.available(widget1)\n    assert lock_draw.available(widget2)\n    assert lock_draw.isowner(widget2)\n    assert not lock_draw.isowner(widget1)\n\n    # Try to acquire the lock again with widget2 (should raise an error)\n    with pytest.raises(ValueError, match=\"already locked\"):\n        lock_draw(widget2)\n"], "sample_540": ["def test_movie_writer_registry_list():\n    \"\"\"Test listing available writers from the registry.\"\"\"\n    available_writers = animation.writers.list()\n    assert 'pillow' in available_writers  # PillowWriter should always be available\n    assert 'html' in available_writers  # HTMLWriter should always be available\n    # Checking the list of writers for common ones that should be registered\n    for writer in ['ffmpeg', 'imagemagick', 'ffmpeg_file', 'imagemagick_file']:\n        if animation.writers.is_available(writer):\n            assert writer in available_writers\n        else:\n            assert writer not in available_writers\n", "def test_adjusted_figsize():\n    # Test the adjusted_figsize function with different inputs\n    test_cases = [\n        ((8.0, 6.0, 100, 1), (8.0, 6.0)),\n        ((8.0, 6.0, 100, 2), (8.0, 6.0)),\n        ((8.1, 6.1, 100, 5), (8.1, 6.05)),\n        ((8.1, 6.1, 100, 10), (8.0, 6.0)),\n        ((8.0, 6.0, 96, 3), (8.0, 6.0)),\n    ]\n\n    for (w, h, dpi, n), (expected_w, expected_h) in test_cases:\n        result_w, result_h = adjusted_figsize(w, h, dpi, n)\n        assert np.isclose(result_w, expected_w), f\"Expected width {expected_w}, got {result_w}\"\n        assert np.isclose(result_h, expected_h), f\"Expected height {expected_h}, got {result_h}\"\n", "def test_adjusted_figsize():\n    # Test adjusted_figsize function for a few cases.\n    w, h = 10, 5\n    dpi = 100\n    n = 4\n\n    wnew, hnew = animation.adjusted_figsize(w, h, dpi, n)\n\n    assert wnew == 10.0\n    assert hnew == 5.0\n\n    w, h = 10.1, 5.1\n    wnew, hnew = animation.adjusted_figsize(w, h, dpi, n)\n    assert wnew == 10.08  # Adjusted to nearest multiple of 4 pixels\n    assert hnew == 5.12  # Adjusted to nearest multiple of 4 pixels\n\n    w, h = 10.1, 5.1\n    dpi = 101\n    wnew, hnew = animation.adjusted_figsize(w, h, dpi, n)\n    assert wnew != w  # Should be adjusted\n    assert hnew != h  # Should be adjusted\n", "def test_adjusted_figsize():\n    # Test adjusted_figsize function with various inputs.\n    w, h = 5, 5\n    dpi = 100\n    n = 2\n\n    wnew, hnew = animation.adjusted_figsize(w, h, dpi, n)\n    assert wnew == 5.0\n    assert hnew == 5.0\n\n    w, h = 5.1, 5.1\n    wnew, hnew = animation.adjusted_figsize(w, h, dpi, n)\n    assert wnew == 5.12\n    assert hnew == 5.12\n\n    w, h = 5.1, 5.1\n    n = 3\n    wnew, hnew = animation.adjusted_figsize(w, h, dpi, n)\n    assert wnew == 5.13\n    assert hnew == 5.13\n", "def test_adjusted_figsize():\n    # Test adjusted_figsize function with different parameters\n    w, h, dpi, n = 8.0, 6.0, 100.0, 10\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 8.0 and hnew == 6.0\n\n    # Test with values that would require adjustment\n    w, h, dpi, n = 8.1, 6.1, 100.0, 10\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew != 8.1 or hnew != 6.1\n\n    # Test with different multiples\n    w, h, dpi, n = 8.0, 6.0, 100.0, 5\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 8.0 and hnew == 6.0\n"], "sample_541": ["def test_button_widget(ax):\n    callback = mock.Mock()\n    # Create the button widget\n    button = widgets.Button(ax, 'Test Button', color='0.85', hovercolor='0.95')\n    button.on_clicked(callback)\n    \n    # Simulate button press and release\n    do_event(button, 'button_press_event', xdata=0.5, ydata=0.5, button=1)\n    do_event(button, 'button_release_event', xdata=0.5, ydata=0.5, button=1)\n    \n    # Check if callback was called once\n    callback.assert_called_once()\n    \n    # Change color and hovercolor and check if they are updated\n    button.ax.set_facecolor('0.75')\n    assert mcolors.same_color(button.ax.get_facecolor(), '0.75')\n    button.hovercolor = '0.85'\n    button._motion(mock_event(button.ax, xdata=0.5, ydata=0.5))\n    assert mcolors.same_color(button.ax.get_facecolor(), '0.85')\n    \n    # Disconnect the callback and ensure it's not called again\n    button.disconnect(callback)\n    do_event(button, 'button_press_event', xdata=0.5, ydata=0.5, button=1)\n    do_event(button, 'button_release_event', xdata=0.5, ydata=0.5, button=1)\n    callback.assert_called_once()  # Should still be called only once\n", "def test_lock_draw():\n    lock = widgets.LockDraw()\n    widget1 = object()\n    widget2 = object()\n\n    # Initially, the lock should be available\n    assert lock.available(widget1)\n    assert not lock.locked()\n\n    # Lock should be acquired by widget1\n    lock(widget1)\n    assert lock.locked()\n    assert lock.isowner(widget1)\n    assert not lock.available(widget2)\n\n    # Trying to acquire the lock by another widget should raise an error\n    with pytest.raises(ValueError, match='already locked'):\n        lock(widget2)\n\n    # Releasing the lock by a different widget should raise an error\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(widget2)\n\n    # Releasing the lock by the owner should make it available\n    lock.release(widget1)\n    assert not lock.locked()\n    assert lock.available(widget2)\n", "def test_lock_draw():\n    canvas_mock = mock.Mock()\n    lock = widgets.LockDraw()\n\n    assert lock.available(canvas_mock)\n    lock(canvas_mock)\n    assert lock.locked()\n    assert lock.isowner(canvas_mock)\n    assert not lock.available(mock.Mock())\n\n    with pytest.raises(ValueError, match='already locked'):\n        lock(mock.Mock())\n\n    lock.release(canvas_mock)\n    assert not lock.locked()\n    assert not lock.isowner(canvas_mock)\n    assert lock.available(canvas_mock)\n\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(mock.Mock())\n", "def test_lockdraw():\n    canvas = mock.Mock()\n    lockdraw = widgets.LockDraw()\n\n    # Test initial state\n    assert lockdraw._owner is None\n\n    # Test acquiring lock\n    lockdraw(canvas)\n    assert lockdraw._owner is canvas\n\n    # Test acquiring lock again by the same owner\n    lockdraw(canvas)\n    assert lockdraw._owner is canvas\n\n    # Test ValueError on lock acquire by a different owner\n    new_canvas = mock.Mock()\n    with pytest.raises(ValueError, match='already locked'):\n        lockdraw(new_canvas)\n\n    # Test releasing lock\n    lockdraw.release(canvas)\n    assert lockdraw._owner is None\n\n    # Test ValueError on release by a non-owner\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lockdraw.release(new_canvas)\n\n    # Test lock status and owner check\n    assert not lockdraw.locked()\n    assert not lockdraw.isowner(canvas)\n    lockdraw(canvas)\n    assert lockdraw.locked()\n    assert lockdraw.isowner(canvas)\n\n    # Test available method\n    assert lockdraw.available(canvas)\n    assert not lockdraw.available(new_canvas)\n", "def test_lockdraw():\n    lock = widgets.LockDraw()\n\n    # Check initial state of the lock\n    assert lock._owner is None\n    assert lock.locked() is False\n    assert lock.available(object()) is True\n\n    obj1 = object()\n    obj2 = object()\n\n    # Reserve the lock for obj1\n    lock(obj1)\n    assert lock._owner is obj1\n    assert lock.locked() is True\n    assert lock.available(obj1) is True\n    assert lock.available(obj2) is False\n\n    # Try to reserve the lock again for obj1 (should raise ValueError)\n    with pytest.raises(ValueError, match='already locked'):\n        lock(obj1)\n\n    # Try to reserve the lock for obj2 (should raise ValueError)\n    with pytest.raises(ValueError, match='already locked'):\n        lock(obj2)\n\n    # Release the lock from obj1\n    lock.release(obj1)\n    assert lock._owner is None\n    assert lock.locked() is False\n    assert lock.available(obj1) is True\n    assert lock.available(obj2) is True\n\n    # Try to release the lock from obj2 (should raise ValueError)\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(obj2)\n"], "sample_542": ["def test_get_rotation_mode():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    \n    assert text.get_rotation_mode() is None\n    \n    text.set_rotation_mode('anchor')\n    assert text.get_rotation_mode() == 'anchor'\n    \n    text.set_rotation_mode('default')\n    assert text.get_rotation_mode() == 'default'\n\n    with pytest.raises(ValueError, match=\"rotation_mode must be one of\"):\n        text.set_rotation_mode('invalid')\n", "def test_get_rotation_mode():\n    text = Text(0, 0, 'sample text', rotation_mode='anchor')\n    assert text.get_rotation_mode() == 'anchor'\n    text.set_rotation_mode('default')\n    assert text.get_rotation_mode() == 'default'\n    with pytest.raises(ValueError):\n        text.set_rotation_mode('invalid_mode')\n", "def test_get_text_metrics_with_cache():\n    class MockRenderer:\n            return len(text) * 10, 20, 5\n\n    renderer = MockRenderer()\n    fontprop = FontProperties()\n    text = \"Test\"\n    ismath = False\n    dpi = 72\n\n    # Call the function to cache the result\n    result1 = mpl.text._get_text_metrics_with_cache(renderer, text, fontprop, ismath, dpi)\n    result2 = mpl.text._get_text_metrics_with_cache(renderer, text, fontprop, ismath, dpi)\n\n    assert result1 == result2\n    assert mpl.text._get_text_metrics_with_cache_impl.cache_info().hits > 0\n", "def test_update_bbox_position_size():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Test', bbox=dict(facecolor='red', alpha=0.5))\n    fig.canvas.draw()\n\n    # Get initial bbox position and size\n    initial_bbox = text.get_bbox_patch().get_window_extent()\n\n    # Update position and size\n    text.update_bbox_position_size(fig.canvas.get_renderer())\n    updated_bbox = text.get_bbox_patch().get_window_extent()\n\n    # Ensure bbox position and size are updated correctly\n    assert_almost_equal(initial_bbox.x0, updated_bbox.x0)\n    assert_almost_equal(initial_bbox.y0, updated_bbox.y0)\n    assert_almost_equal(initial_bbox.width, updated_bbox.width)\n    assert_almost_equal(initial_bbox.height, updated_bbox.height)\n", "def test_text_get_bbox_patch():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Test BBox', bbox=dict(facecolor='red', alpha=0.5))\n    fig.canvas.draw()\n\n    bbox_patch = text.get_bbox_patch()\n    assert bbox_patch is not None\n    assert bbox_patch.get_facecolor() == (1.0, 0.0, 0.0, 0.5)  # red with alpha 0.5\n    assert bbox_patch.get_transform() == text.get_transform()\n"], "sample_543": ["def test_lockdraw():\n    canvas = mock.Mock()\n    widget = mock.Mock()\n    lockdraw = widgets.LockDraw()\n    \n    # Test initial state\n    assert lockdraw._owner is None\n    assert lockdraw.available(widget)\n    \n    # Test acquiring lock\n    lockdraw(widget)\n    assert lockdraw._owner == widget\n    assert not lockdraw.available(mock.Mock())\n    \n    # Test releasing lock\n    lockdraw.release(widget)\n    assert lockdraw._owner is None\n    assert lockdraw.available(widget)\n    \n    # Test acquiring lock when already locked\n    lockdraw(widget)\n    with pytest.raises(ValueError, match='already locked'):\n        lockdraw(widget)\n    \n    # Test releasing lock when not the owner\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lockdraw.release(mock.Mock())\n", "def test_lockdraw():\n    lock = widgets.LockDraw()\n    widget1 = object()\n    widget2 = object()\n\n    assert lock.available(widget1)\n    assert lock.available(widget2)\n    assert not lock.locked()\n\n    lock(widget1)\n    assert lock.locked()\n    assert lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n    assert lock.available(widget1)\n    assert not lock.available(widget2)\n\n    with pytest.raises(ValueError, match='already locked'):\n        lock(widget2)\n\n    lock.release(widget1)\n    assert not lock.locked()\n    assert lock.available(widget1)\n    assert lock.available(widget2)\n\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(widget2)\n", "def test_slider_snap_values(ax):\n    snap_values = np.array([0, 0.5, 1, 1.5, 2])\n\n    slider = widgets.Slider(ax=ax, label='test', valmin=0.0, valmax=2.0,\n                            valinit=0.75, snap_values=snap_values)\n    assert slider.val == 0.5  # Should snap to the nearest value\n\n    slider.set_val(1.25)\n    assert slider.val == 1.5  # Should snap to the nearest value\n\n    slider.set_val(0.9)\n    assert slider.val == 1.0  # Should snap to the nearest value\n\n    slider.set_val(-0.1)\n    assert slider.val == 0.0  # Should not go below valmin\n\n    slider.set_val(2.1)\n    assert slider.val == 2.0  # Should not go above valmax\n", "def test_lock_draw():\n    lock = widgets.LockDraw()\n    widget1 = object()\n    widget2 = object()\n\n    assert lock.available(widget1)\n    assert lock.available(widget2)\n\n    lock(widget1)\n    assert lock.isowner(widget1)\n    assert not lock.isowner(widget2)\n    assert lock.locked()\n    assert not lock.available(widget2)\n\n    with pytest.raises(ValueError, match='already locked'):\n        lock(widget2)\n\n    lock.release(widget1)\n    assert not lock.isowner(widget1)\n    assert not lock.locked()\n    assert lock.available(widget2)\n\n    with pytest.raises(ValueError, match='you do not own this lock'):\n        lock.release(widget2)\n", "def test_lock_draw():\n    # Create LockDraw instance\n    lock = widgets.LockDraw()\n\n    # Test lock availability\n    assert lock.available(None)\n\n    # Reserve the lock for an object\n    obj1 = object()\n    lock(obj1)\n    assert not lock.available(None)\n    assert lock.isowner(obj1)\n    assert lock.locked()\n\n    # Attempt to release the lock with a different object should raise ValueError\n    obj2 = object()\n    with pytest.raises(ValueError, match=\"you do not own this lock\"):\n        lock.release(obj2)\n\n    # Release the lock with the correct object\n    lock.release(obj1)\n    assert lock.available(None)\n    assert not lock.locked()\n\n    # Attempt to reserve the lock again with the same object\n    lock(obj1)\n    assert lock.isowner(obj1)\n    assert lock.locked()\n\n    # Attempt to reserve the lock with another object should raise ValueError\n    with pytest.raises(ValueError, match=\"already locked\"):\n        lock(obj2)\n\n    # Release the lock and ensure it's available\n    lock.release(obj1)\n    assert lock.available(None)\n    assert not lock.locked()\n"], "sample_544": ["def test_set_alpha():\n    fig, ax = plt.subplots()\n    im = AxesImage(ax)\n    data = np.random.rand(10, 10)\n    im.set_data(data)\n    im.set_alpha(0.5)\n    assert im._get_scalar_alpha() == 0.5\n    with pytest.raises(TypeError, match='alpha must be a float, two-dimensional array, or None'):\n        im.set_alpha([0.5])\n    # Test 2D alpha array\n    alpha_array = np.random.rand(10, 10)\n    im.set_alpha(alpha_array)\n    assert np.array_equal(im._alpha, alpha_array)\n\n    # Test None alpha\n    im.set_alpha(None)\n    assert im._get_scalar_alpha() == 1.0\n", "def test_composite_images_empty():\n    class MockRenderer:\n            return 1.0\n\n    class MockImage:\n            return None, 0, 0, None\n\n    renderer = MockRenderer()\n    images = [MockImage() for _ in range(5)]\n    output, offset_x, offset_y = composite_images(images, renderer)\n    assert output.shape == (0, 0, 4)\n    assert offset_x == 0\n    assert offset_y == 0\n", "def test_image_alpha_array():\n    fig, ax = plt.subplots()\n    img = np.random.rand(10, 10, 4)\n    img[..., 3] = np.linspace(0, 1, 10).reshape(10, 1)  # Vary alpha channel.\n    ax.imshow(img)\n    fig.canvas.draw()\n    assert_array_equal(ax.images[0]._A[..., 3], np.linspace(0, 1, 10).reshape(10, 1))\n", "def test_axesimage_set_alpha():\n    ax = plt.gca()\n    im = AxesImage(ax)\n    z = np.arange(12, dtype=float).reshape((4, 3))\n    im.set_data(z)\n\n    # Set a scalar alpha value\n    im.set_alpha(0.5)\n    assert im._get_scalar_alpha() == 0.5\n\n    # Set a 2D array alpha value\n    alpha_array = np.full((4, 3), 0.5)\n    im.set_alpha(alpha_array)\n    assert im._get_scalar_alpha() == 1.0  # Should return 1.0 as it has pixel-specific alpha values\n\n    # Test invalid alpha values\n    with pytest.raises(TypeError, match='alpha must be a float, two-dimensional array, or None'):\n        im.set_alpha([0.5])\n    with pytest.raises(TypeError, match='alpha must be a float, two-dimensional array, or None'):\n        im.set_alpha(np.full((4,), 0.5))\n", "def test_rgb_to_rgba():\n    # Test the _rgb_to_rgba function with different input arrays.\n    rgb_array = np.array([[[255, 0, 0], [0, 255, 0]], [[0, 0, 255], [255, 255, 0]]], dtype=np.uint8)\n    expected_rgba_array = np.array([[[255, 0, 0, 255], [0, 255, 0, 255]], [[0, 0, 255, 255], [255, 255, 0, 255]]], dtype=np.uint8)\n    result = mimage._rgb_to_rgba(rgb_array)\n    assert_array_equal(result, expected_rgba_array)\n\n    # Test with a float array\n    rgb_array_float = np.array([[[1.0, 0, 0], [0, 1.0, 0]], [[0, 0, 1.0], [1.0, 1.0, 0]]], dtype=np.float32)\n    expected_rgba_array_float = np.array([[[1.0, 0, 0, 1.0], [0, 1.0, 0, 1.0]], [[0, 0, 1.0, 1.0], [1.0, 1.0, 0, 1.0]]], dtype=np.float32)\n    result_float = mimage._rgb_to_rgba(rgb_array_float)\n    assert_array_equal(result_float, expected_rgba_array_float)\n"], "sample_545": ["def test_update_subplotparams():\n    # Test updating SubplotParams\n    params = mpl.figure.SubplotParams(left=0.1, right=0.9)\n    assert params.left == 0.1\n    assert params.right == 0.9\n    assert params.top == mpl.rcParams['figure.subplot.top']  # Default value\n    assert params.bottom == mpl.rcParams['figure.subplot.bottom']  # Default value\n\n    # Update values and check\n    params.update(top=0.8, bottom=0.2)\n    assert params.top == 0.8\n    assert params.bottom == 0.2\n\n    # Check invalid updates\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=0.95)\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=0.85)\n", "def test_subfigure_layout_engine():\n    fig = plt.figure(layout='constrained')\n    sub_fig = fig.subfigures(1, 2)\n\n    # Ensure that subfigures inherit layout engine\n    assert isinstance(sub_fig[0].get_layout_engine(), ConstrainedLayoutEngine)\n    assert isinstance(sub_fig[1].get_layout_engine(), ConstrainedLayoutEngine)\n\n    # Change layout engine of parent figure\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n    \n    # Ensure subfigures inherit new layout engine\n    assert isinstance(sub_fig[0].get_layout_engine(), TightLayoutEngine)\n    assert isinstance(sub_fig[1].get_layout_engine(), TightLayoutEngine)\n", "def test_figure_add_artist():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    line = plt.Line2D([0, 1], [0, 1], color='r', lw=2)\n    artist = fig.add_artist(line)\n    assert artist in fig.artists\n    assert artist.get_color() == 'r'\n    assert artist.get_linewidth() == 2\n    assert fig.gca() == ax\n    fig.canvas.draw()\n    fig.delaxes(ax)\n    assert artist in fig.artists\n    fig.clear()\n    assert artist not in fig.artists\n", "def test_update_subplotparams():\n    sp = SubplotParams(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.2, hspace=0.3)\n    assert sp.left == 0.1\n    assert sp.right == 0.9\n    assert sp.bottom == 0.1\n    assert sp.top == 0.9\n    assert sp.wspace == 0.2\n    assert sp.hspace == 0.3\n    \n    sp.update(left=0.2)\n    assert sp.left == 0.2\n    assert sp.right == 0.9\n    assert sp.bottom == 0.1\n    assert sp.top == 0.9\n    assert sp.wspace == 0.2\n    assert sp.hspace == 0.3\n    \n    sp.update(bottom=0.2, top=0.8)\n    assert sp.left == 0.2\n    assert sp.right == 0.9\n    assert sp.bottom == 0.2\n    assert sp.top == 0.8\n    assert sp.wspace == 0.2\n    assert sp.hspace == 0.3\n    \n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        sp.update(left=1.0)\n        \n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        sp.update(bottom=1.0)\n", "def test_invalid_subplot_params():\n    # Test SubplotParams constructor with invalid ranges\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        SubplotParams(left=0.6, right=0.5)\n\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        SubplotParams(bottom=0.6, top=0.5)\n\n    # Test SubplotParams update method with invalid ranges\n    sp = SubplotParams()\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        sp.update(left=0.6, right=0.5)\n\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        sp.update(bottom=0.6, top=0.5)\n\n    # Test SubplotParams defaults\n    sp = SubplotParams()\n    assert sp.left == mpl.rcParams[\"figure.subplot.left\"]\n    assert sp.right == mpl.rcParams[\"figure.subplot.right\"]\n    assert sp.bottom == mpl.rcParams[\"figure.subplot.bottom\"]\n    assert sp.top == mpl.rcParams[\"figure.subplot.top\"]\n    assert sp.wspace == mpl.rcParams[\"figure.subplot.wspace\"]\n    assert sp.hspace == mpl.rcParams[\"figure.subplot.hspace\"]\n"], "sample_546": ["def test_subplot_params_update():\n    sp = SubplotParams(left=0.1, right=0.9)\n    assert sp.left == 0.1\n    assert sp.right == 0.9\n    sp.update(left=0.2, top=0.8)\n    assert sp.left == 0.2\n    assert sp.top == 0.8\n    assert sp.right == 0.9\n    with pytest.raises(ValueError, match=\"left cannot be >= right\"):\n        sp.update(left=1.0)\n    with pytest.raises(ValueError, match=\"bottom cannot be >= top\"):\n        sp.update(bottom=1.0)\n", "def test_figure_suptitle():\n    fig = plt.figure()\n    title = \"Test Suptitle\"\n    suptitle = fig.suptitle(title, fontsize=16, fontweight='bold')\n\n    assert suptitle.get_text() == title\n    assert suptitle.get_fontsize() == 16\n    assert suptitle.get_fontweight() == 'bold'\n", "def test_suptitle_supxlabel_supylabel():\n    fig = plt.figure()\n    sup_title = fig.suptitle(\"Main Title\")\n    sup_xlabel = fig.suplabels(\"X Axis Label\", {\n        \"name\": \"_supxlabel\",\n        \"x0\": 0.5,\n        \"y0\": 0.01,\n        \"ha\": \"center\",\n        \"va\": \"bottom\",\n        \"rotation\": 0,\n        \"size\": \"figure.labelsize\",\n        \"weight\": \"figure.labelweight\"\n    })\n    sup_ylabel = fig.suplabels(\"Y Axis Label\", {\n        \"name\": \"_supylabel\",\n        \"x0\": 0.02,\n        \"y0\": 0.5,\n        \"ha\": \"left\",\n        \"va\": \"center\",\n        \"rotation\": \"vertical\",\n        \"rotation_mode\": \"anchor\",\n        \"size\": \"figure.labelsize\",\n        \"weight\": \"figure.labelweight\"\n    })\n\n    assert sup_title.get_text() == \"Main Title\"\n    assert sup_xlabel.get_text() == \"X Axis Label\"\n    assert sup_ylabel.get_text() == \"Y Axis Label\"\n    assert fig._suptitle == sup_title\n    assert fig._supxlabel == sup_xlabel\n    assert fig._supylabel == sup_ylabel\n", "def test_subplotparams_update():\n    sp = SubplotParams(left=0.1, right=0.9, top=0.9, bottom=0.1)\n    assert sp.left == 0.1\n    assert sp.right == 0.9\n    assert sp.top == 0.9\n    assert sp.bottom == 0.1\n\n    # Test valid update\n    sp.update(left=0.15, right=0.85)\n    assert sp.left == 0.15\n    assert sp.right == 0.85\n    assert sp.top == 0.9\n    assert sp.bottom == 0.1\n\n    # Test invalid update with left >= right\n    with pytest.raises(ValueError):\n        sp.update(left=0.9, right=0.1)\n\n    # Test invalid update with bottom >= top\n    with pytest.raises(ValueError):\n        sp.update(bottom=0.9, top=0.1)\n", "def test_subplot_params():\n    # Test default SubplotParams\n    params = SubplotParams()\n    assert params.left == mpl.rcParams['figure.subplot.left']\n    assert params.right == mpl.rcParams['figure.subplot.right']\n    assert params.bottom == mpl.rcParams['figure.subplot.bottom']\n    assert params.top == mpl.rcParams['figure.subplot.top']\n    assert params.wspace == mpl.rcParams['figure.subplot.wspace']\n    assert params.hspace == mpl.rcParams['figure.subplot.hspace']\n\n    # Test updating SubplotParams\n    params.update(left=0.1, bottom=0.2, right=0.9, top=0.8, wspace=0.5, hspace=0.5)\n    assert params.left == 0.1\n    assert params.right == 0.9\n    assert params.bottom == 0.2\n    assert params.top == 0.8\n    assert params.wspace == 0.5\n    assert params.hspace == 0.5\n\n    # Test invalid updates\n    with pytest.raises(ValueError):\n        params.update(left=0.9, right=0.1)\n    with pytest.raises(ValueError):\n        params.update(bottom=0.8, top=0.2)\n"], "sample_547": ["def test_offsetbox_contains():\n    fig, ax = plt.subplots()\n    size = 100\n    da = DrawingArea(size, size)\n    bg = mpatches.Rectangle((0, 0), size, size,\n                            facecolor='#CCCCCC',\n                            edgecolor='None',\n                            linewidth=0)\n    da.add_artist(bg)\n    ax.add_artist(da)\n\n    fig.canvas.draw()\n    event_inside = MouseEvent('button_press_event', fig.canvas, 50, 50, MouseButton.LEFT)\n    event_outside = MouseEvent('button_press_event', fig.canvas, 150, 150, MouseButton.LEFT)\n\n    assert da.contains(event_inside)[0]\n    assert not da.contains(event_outside)[0]\n", "def test_offsetbox_set_offset_callable():\n    fig, ax = plt.subplots()\n    size = 100\n    da = DrawingArea(size, size)\n\n        return width / 2, height / 2\n\n    da.set_offset(dynamic_offset)\n    assert callable(da.get_offset)\n\n    anchored_box = AnchoredOffsetbox(\n        loc='center',\n        child=da,\n        pad=0.,\n        frameon=False,\n        bbox_to_anchor=(.5, .5),\n        bbox_transform=ax.transAxes,\n        borderpad=0.)\n\n    ax.add_artist(anchored_box)\n    fig.canvas.draw()\n\n    renderer = fig.canvas.get_renderer()\n    bbox = da.get_bbox(renderer)\n    offset = da.get_offset(bbox, renderer)\n    assert offset == (bbox.width / 2, bbox.height / 2)\n", "def test_drawingarea_clip_children_property():\n    fig, ax = plt.subplots()\n    da = DrawingArea(50, 50, clip=True)\n    assert da.clip_children  # Check initial state\n\n    da.clip_children = False\n    assert not da.clip_children  # Check after setting to False\n\n    da.clip_children = True\n    assert da.clip_children  # Check after setting back to True\n", "def test_offsetbox_get_offset_callable():\n    fig, ax = plt.subplots()\n\n        return width / 2, height / 2\n\n    da = DrawingArea(100, 100)\n    da.set_offset(offset_callable)\n\n    assert callable(da._offset)\n    bbox = da.get_bbox(ax.figure._get_renderer())\n    offset = da.get_offset(bbox, ax.figure._get_renderer())\n\n    # Ensure that the offset is calculated correctly by the callable\n    assert offset == (bbox.width / 2, bbox.height / 2)\n\n    anchored_box = AnchoredOffsetbox(loc='center', child=da)\n    ax.add_artist(anchored_box)\n    fig.canvas.draw()\n", "def test_offsetbox_contains():\n    fig, ax = plt.subplots()\n    ta = TextArea(\"foo\")\n    ab = AnchoredOffsetbox('center', child=ta)\n    ax.add_artist(ab)\n\n    # Simulate a mouse event at the center of the axes\n    x, y = ax.transAxes.transform_point((0.5, 0.5))\n    event = MouseEvent(\"button_press_event\", fig.canvas, x, y, MouseButton.LEFT)\n\n    # Check that the offset box contains the event\n    assert ab.contains(event)[0] == True\n\n    # Simulate a mouse event outside the axes\n    event_outside = MouseEvent(\"button_press_event\", fig.canvas, -10, -10, MouseButton.LEFT)\n\n    # Check that the offset box does not contain the event outside\n    assert ab.contains(event_outside)[0] == False\n"], "sample_548": ["def test_colorbar_remove_callbacks():\n    \"\"\"Test that the callbacks are properly removed when a colorbar is removed.\"\"\"\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable(norm=mcolors.Normalize(), cmap='viridis')\n    cb = fig.colorbar(sm, ax=ax)\n\n    # Check that callbacks are connected\n    assert sm.callbacks.connect('changed', cb.update_normal) == cb.mappable.colorbar_cid\n    assert cb._extend_cid1 in cb.ax.callbacks.callbacks[\"xlim_changed\"]\n    assert cb._extend_cid2 in cb.ax.callbacks.callbacks[\"ylim_changed\"]\n\n    # Remove the colorbar\n    cb.remove()\n\n    # Check that callbacks are disconnected\n    assert sm.callbacks.connect('changed', cb.update_normal) is None\n    assert cb.mappable.colorbar is None\n    assert cb.mappable.colorbar_cid is None\n    assert cb._extend_cid1 not in cb.ax.callbacks.callbacks[\"xlim_changed\"]\n    assert cb._extend_cid2 not in cb.ax.callbacks.callbacks[\"ylim_changed\"]\n", "def test_colorbar_invalid_extend():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh([[0, 1], [2, 3]])\n    with pytest.raises(ValueError, match=\"Invalid value for extendfrac\"):\n        Colorbar(ax, pc, extend='invalid_extend')\n", "def test_make_axes_location_orientation_mutual_exclusive():\n    fig, ax = plt.subplots()\n    with pytest.raises(TypeError, match=\"location and orientation are mutually exclusive\"):\n        make_axes(ax, location='top', orientation='vertical')\n", "def test_colorbar_axes_locator():\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    im = ax.imshow(data)\n    cbar = fig.colorbar(im)\n\n    # Test that the _ColorbarAxesLocator is correctly set.\n    assert isinstance(cbar.ax._axes_locator, _ColorbarAxesLocator)\n    assert cbar.ax._axes_locator._cbar is cbar\n\n    # Test the locator call\n    renderer = fig.canvas.get_renderer()\n    pos = cbar.ax._axes_locator(ax, renderer)\n    assert pos == ax.get_position(original=True)\n\n    # Test that the locator handles the extensions properly.\n    cbar.extend = 'both'\n    y, extendlen = cbar._proportional_y()\n    pos_with_extend = cbar.ax._axes_locator(ax, renderer)\n    assert pos_with_extend.height < pos.height\n\n    cbar.extend = 'neither'\n    pos_no_extend = cbar.ax._axes_locator(ax, renderer)\n    assert pos_no_extend.height == pos.height\n", "def test_make_axes(location, orientation, fraction, shrink, aspect, pad):\n    fig, ax = plt.subplots()\n    cmap, norms = _get_cmap_norms()\n    norm = norms['both']\n    boundaries = values = norm.boundaries\n    values = values[:-1]\n    mappable = cm.ScalarMappable(norm=norm, cmap=cmap)\n    cax, kwargs = make_axes(ax, location=location, orientation=orientation,\n                            fraction=fraction, shrink=shrink, aspect=aspect, pad=pad)\n    \n    assert cax.get_position().bounds != ax.get_position().bounds\n    assert kwargs['orientation'] == orientation\n    assert kwargs['ticklocation'] == location\n"], "sample_549": ["def test_safe_masked_invalid():\n    x = np.array([1, 2, 3, np.inf, -np.inf, np.nan, 4, 5])\n    expected_masked = np.ma.masked_invalid(x)\n    actual_masked = cbook.safe_masked_invalid(x)\n    assert_array_equal(expected_masked.mask, actual_masked.mask)\n    assert_array_equal(expected_masked, actual_masked)\n", "def test_is_writable_file_like():\n    class WritableFileLike:\n            pass\n\n    class NonWritableFileLike:\n        pass\n\n    writable_obj = WritableFileLike()\n    non_writable_obj = NonWritableFileLike()\n\n    assert cbook.is_writable_file_like(writable_obj) is True\n    assert cbook.is_writable_file_like(non_writable_obj) is False\n", "def test_open_file_cm():\n    with cbook.open_file_cm(\"testfile.txt\", \"w\") as f:\n        f.write(\"test\")\n\n    with cbook.open_file_cm(\"testfile.txt\", \"r\") as f:\n        content = f.read()\n        assert content == \"test\"\n\n    with cbook.open_file_cm(\"testfile.txt\", \"a\") as f:\n        f.write(\" append\")\n\n    with cbook.open_file_cm(\"testfile.txt\", \"r\") as f:\n        content = f.read()\n        assert content == \"test append\"\n\n    os.remove(\"testfile.txt\")\n\n    with pytest.raises(ValueError, match=\"fname must be a PathLike or file handle\"):\n        cbook.open_file_cm(12345, \"r\")\n", "def test_file_requires_unicode():\n    class MockFile:\n            if isinstance(data, bytes):\n                raise TypeError(\"bytes data not allowed\")\n            return True\n\n    mock_file = MockFile()\n    assert cbook.file_requires_unicode(mock_file) == True\n\n    class MockFileBytes:\n            if isinstance(data, bytes):\n                return True\n            raise TypeError(\"non-bytes data not allowed\")\n\n    mock_file_bytes = MockFileBytes()\n    assert cbook.file_requires_unicode(mock_file_bytes) == False\n", "def test_flatten():\n    nested_list = [[1, 2, [3, 4, [5, 6]], 7], 8, [[9], 10]]\n    result = list(cbook.flatten(nested_list))\n    expected = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    assert result == expected\n\n    mixed_types_list = [['a', ['b', ['c']]], 'd', ['e', 'f'], 'g']\n    result = list(cbook.flatten(mixed_types_list))\n    expected = ['a', 'b', 'c', 'd', 'e', 'f', 'g']\n    assert result == expected\n\n    scalar_list = [1, 2, 3]\n    result = list(cbook.flatten(scalar_list))\n    expected = [1, 2, 3]\n    assert result == expected\n\n    empty_list = []\n    result = list(cbook.flatten(empty_list))\n    expected = []\n    assert result == expected\n"], "sample_550": ["def test_process_plot_format():\n    # Test different cases for _process_plot_format function\n    assert _process_plot_format('ko') == (None, 'o', (0.0, 0.0, 0.0, 1.0))  # Black circles\n    assert _process_plot_format('.b') == (None, '.', (0.0, 0.0, 1.0, 1.0))  # Blue dots\n    assert _process_plot_format('r--') == ('--', None, (1.0, 0.0, 0.0, 1.0))  # Red dashed lines\n    assert _process_plot_format('C2--') == ('--', None, (0.0, 0.584, 0.816, 1.0))  # Third color in the color cycle, dashed lines\n\n    # Test ambiguous format/data key detection\n    with pytest.raises(ValueError, match=\"two linestyle symbols\"):\n        _process_plot_format('r--b')\n    with pytest.raises(ValueError, match=\"two color symbols\"):\n        _process_plot_format('rb')\n    with pytest.raises(ValueError, match=\"two marker symbols\"):\n        _process_plot_format('oo')\n\n    # Test handling invalid format string\n    with pytest.raises(ValueError, match=\"unrecognized character 'x'\"):\n        _process_plot_format('x')", "def test_process_plot_format():\n    # Test valid format strings\n    assert _process_plot_format(\"ko\") == (None, 'o', (0, 0, 0, 1))\n    assert _process_plot_format(\".b\") == (None, '.', (0, 0, 1, 1))\n    assert _process_plot_format(\"r--\") == ('--', None, (1, 0, 0, 1))\n    assert _process_plot_format(\"C2--\") == ('--', None, mcolors.to_rgba('C2'))\n\n    # Test invalid format strings\n    with pytest.raises(ValueError, match=\"is not a valid format string\"):\n        _process_plot_format(\"invalid\")\n\n    with pytest.raises(ValueError, match=\"two linestyle symbols\"):\n        _process_plot_format(\"--r--\")\n\n    with pytest.raises(ValueError, match=\"two marker symbols\"):\n        _process_plot_format(\"oo\")\n\n    with pytest.raises(ValueError, match=\"two color symbols\"):\n        _process_plot_format(\"rr\")\n", "def test_process_plot_format():\n    # Test valid format strings\n    assert _process_plot_format('ko') == ('None', 'o', (0.0, 0.0, 0.0, 1.0))  # black circles\n    assert _process_plot_format('.b') == ('None', '.', (0.0, 0.0, 1.0, 1.0))  # blue dots\n    assert _process_plot_format('r--') == ('--', 'None', (1.0, 0.0, 0.0, 1.0))  # red dashed lines\n    assert _process_plot_format('C2--') == ('--', 'None', (0.0, 0.502, 0.0, 1.0))  # third color in cycle\n\n    # Test invalid format strings\n    with pytest.raises(ValueError, match=\"unrecognized character 'Q'\"):\n        _process_plot_format('r--Q')\n    with pytest.raises(ValueError, match=\"unrecognized character '!'\"):\n        _process_plot_format('!!')\n\n    # Test ambiguous format strings\n    with pytest.raises(ValueError, match=\"two linestyle symbols\"):\n        _process_plot_format('r--r--')\n    with pytest.raises(ValueError, match=\"two marker symbols\"):\n        _process_plot_format('oo')\n    with pytest.raises(ValueError, match=\"two color symbols\"):\n        _process_plot_format('rr')\n\n    # Test grayscale '1.0'\n    assert _process_plot_format('1.0') == ('None', 'None', (1.0, 1.0, 1.0, 1.0))\n\n    # Test empty format string\n    assert _process_plot_format('') == ('None', 'None', None)\n", "def test_axis_method_wrapper():\n    class TestAxis:\n            \"\"\"\n            This is a test docstring for this Axis.\n\n            Parameters\n            ----------\n            arg1 : int\n                An integer argument.\n            \"\"\"\n            return \"bar\"\n\n    class TestAxes:\n        xaxis = TestAxis()\n        get_foo = _axis_method_wrapper(\"xaxis\", \"get_bar\")\n\n    axes = TestAxes()\n    assert axes.get_foo() == \"bar\"\n    assert axes.get_foo.__doc__ == \"\"\"\n            This is a test docstring for the xaxis.\n\n            Parameters\n            ----------\n            arg1 : int\n                An integer argument.\n            \"\"\".strip()\n\n    # Test that missing substitutions raise ValueError\n    class TestAxisMissingDocstring:\n            return \"baz\"\n\n    class TestAxesMissingDocstring:\n        xaxis = TestAxisMissingDocstring()\n        get_foo = _axis_method_wrapper(\"xaxis\", \"get_baz\", doc_sub={\"nonexistent\": \"replacement\"})\n\n    with pytest.raises(ValueError, match=\"contains 'nonexistent' as substrings\"):\n        TestAxesMissingDocstring()\n", "def test_get_xaxis_text1_transform():\n    fig, ax = plt.subplots()\n    pad_points = 10\n    transform, valign, halign = ax.get_xaxis_text1_transform(pad_points)\n    assert isinstance(transform, transforms.Transform)\n    assert valign == \"top\"\n    assert halign == \"center\"\n"], "sample_551": ["def test_text3d_properties():\n    text = art3d.Text3D(1, 2, 3, \"Test Text\", zdir='y', color='blue')\n\n    # Test initial properties\n    assert text.get_position_3d() == (1, 2, 3)\n    assert text.get_text() == \"Test Text\"\n    assert same_color(text.get_color(), 'blue')\n\n    # Modify and test properties\n    text.set_position_3d((4, 5, 6), zdir='x')\n    assert text.get_position_3d() == (4, 5, 6)\n\n    text.set_color('red')\n    assert same_color(text.get_color(), 'red')\n\n    text.set_z(10)\n    assert text.get_position_3d() == (4, 5, 10)\n", "def test_text3d_properties():\n    # Test setting and getting 3D properties for Text3D object.\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    text = art3d.Text3D(x=1, y=2, z=3, text=\"Test Text\", zdir='x')\n    assert text.get_position_3d() == (1, 2, 3)\n    text.set_position_3d((4, 5, 6), zdir='y')\n    assert text.get_position_3d() == (4, 5, 6)\n    assert np.all(text._dir_vec == np.array([0, 1, 0]))\n    text.set_z(10)\n    assert text.get_position_3d()[2] == 10\n", "def test_text3d_set_get_properties():\n    text = art3d.Text3D(1, 2, 3, \"test\", zdir='x', fontsize=12, color='blue')\n    \n    # Check initial properties\n    assert text.get_position_3d() == (1, 2, 3)\n    assert text._dir_vec is not None\n    assert text._z == 3\n    \n    # Set new properties\n    text.set_position_3d((4, 5, 6), zdir='y')\n    assert text.get_position_3d() == (4, 5, 6)\n    assert np.array_equal(text._dir_vec, art3d.get_dir_vector('y'))\n    \n    # Set only z position\n    text.set_z(10)\n    assert text._z == 10\n    assert text.get_position_3d() == (4, 5, 10)\n    \n    # Check if stale is set correctly\n    assert text.stale\n\n    # Set 3D properties\n    text.set_3d_properties(7, zdir='z')\n    assert text._z == 7\n    assert np.array_equal(text._dir_vec, art3d.get_dir_vector('z'))\n    assert text.stale\n", "def test_text3d_properties():\n    text = art3d.Text3D(x=1, y=2, z=3, text='Hello', zdir='y')\n    assert text.get_position_3d() == (1, 2, 3)\n\n    text.set_position_3d((4, 5, 6), zdir='x')\n    assert text.get_position_3d() == (4, 5, 6)\n\n    text.set_z(7)\n    assert text.get_position_3d() == (4, 5, 7)\n    \n    text.set_3d_properties(z=8, zdir='z')\n    assert text.get_position_3d() == (4, 5, 8)\n", "def test_text3d_properties():\n    text = art3d.Text3D(1, 2, 3, \"Test text\", zdir='y', fontsize=12, color='red')\n    assert text.get_position_3d() == (1, 2, 3)\n    assert same_color(text.get_color(), 'red')\n    assert text.get_fontsize() == 12\n\n    text.set_position_3d((4, 5, 6), zdir='x')\n    assert text.get_position_3d() == (4, 5, 6)\n\n    text.set_z(10)\n    assert text.get_position_3d() == (4, 5, 10)\n\n    text.set_3d_properties(7, zdir='z')\n    assert text.get_position_3d() == (4, 5, 7)\n\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    ax.add_artist(text)\n    assert text in ax.artists\n"], "sample_552": ["def test_subfigure_add_artist():\n    fig = plt.figure()\n    subfig1, subfig2 = fig.subfigures(1, 2)\n    ax1 = subfig1.add_subplot(111)\n    ax2 = subfig2.add_subplot(111)\n    rect1 = plt.Rectangle((0.2, 0.2), 0.6, 0.6, fill=False, color=\"blue\")\n    rect2 = plt.Rectangle((0.4, 0.4), 0.2, 0.2, fill=True, color=\"red\")\n\n    subfig1.add_artist(rect1)\n    subfig2.add_artist(rect2)\n\n    assert rect1 in subfig1.artists\n    assert rect2 in subfig2.artists\n\n    buffer = io.BytesIO()\n    fig.savefig(buffer, format='png')\n    assert buffer.getbuffer().nbytes > 0\n", "def test_subplotparams_update():\n    params = SubplotParams(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.2, hspace=0.2)\n    assert params.left == 0.1\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.bottom == 0.1\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    params.update(left=0.2, bottom=0.2)\n    assert params.left == 0.2\n    assert params.bottom == 0.2\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=1.0)\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=1.0)\n", "def test_axes_stack():\n    stack = _AxesStack()\n    \n    ax1, ax2, ax3 = Axes(Figure()), Axes(Figure()), Axes(Figure())\n    assert stack.current() is None\n    \n    stack.add(ax1)\n    assert stack.current() is ax1\n    \n    stack.add(ax2)\n    assert stack.current() is ax2\n    \n    stack.bubble(ax1)\n    assert stack.current() is ax1\n    \n    with pytest.raises(ValueError):\n        stack.bubble(ax3)\n    \n    stack.add(ax1)  # Shouldn't add again\n    assert len(stack.as_list()) == 2\n    \n    stack.remove(ax1)\n    assert stack.current() is ax2\n    \n    stack.remove(ax2)\n    assert stack.current() is None\n", "def test_subplotparams_update():\n    # Test updating SubplotParams and verify exceptions\n    params = SubplotParams()\n    assert params.left == mpl.rcParams[\"figure.subplot.left\"]\n    assert params.right == mpl.rcParams[\"figure.subplot.right\"]\n    assert params.top == mpl.rcParams[\"figure.subplot.top\"]\n    assert params.bottom == mpl.rcParams[\"figure.subplot.bottom\"]\n    assert params.wspace == mpl.rcParams[\"figure.subplot.wspace\"]\n    assert params.hspace == mpl.rcParams[\"figure.subplot.hspace\"]\n    \n    # Test valid updates\n    params.update(left=0.2, right=0.8, top=0.9, bottom=0.1, wspace=0.3, hspace=0.4)\n    assert params.left == 0.2\n    assert params.right == 0.8\n    assert params.top == 0.9\n    assert params.bottom == 0.1\n    assert params.wspace == 0.3\n    assert params.hspace == 0.4\n    \n    # Test invalid updates that should raise exceptions\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=0.9, right=0.1)\n    \n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=0.9, top=0.1)\n", "def test_subplot_mosaic_invalid_layout():\n    fig = plt.figure()\n    invalid_layout = [[\"A\", \"B\"], [\"C\", [\"D\", \"E\"]]]\n    with pytest.raises(ValueError, match=\"All of the rows must be the same length\"):\n        fig.subplot_mosaic(invalid_layout)\n\n    nested_invalid_layout = [[\"A\", [\"B\", \"C\"]], [\"D\", \"E\"]]\n    with pytest.raises(ValueError, match=\"All of the rows must be the same length\"):\n        fig.subplot_mosaic(nested_invalid_layout)\n"], "sample_553": ["def test_adjusted_figsize():\n    # Test the adjusted_figsize function with various inputs\n    w, h = 5, 4\n    dpi = 100\n    n = 10\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 5.0\n    assert hnew == 4.0\n\n    w, h = 5.3, 4.7\n    n = 20\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 5.2\n    assert hnew == 4.8\n\n    w, h = 3.7, 2.9\n    n = 30\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert np.isclose(wnew, 3.6, atol=0.1)\n    assert np.isclose(hnew, 3.0, atol=0.1)\n", "def test_adjusted_figsize():\n    # Test the adjusted_figsize function\n    w, h = 5, 5\n    dpi = 100\n    n = 10\n\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    \n    # Check if the new sizes are correct multiples of n\n    assert (int(wnew * dpi) % n == 0), f\"Width not a multiple of {n}\"\n    assert (int(hnew * dpi) % n == 0), f\"Height not a multiple of {n}\"\n    \n    # Check if the new sizes are close to the original sizes\n    assert abs(wnew - w) < 0.1, f\"Width too different from original: {wnew} vs {w}\"\n    assert abs(hnew - h) < 0.1, f\"Height too different from original: {hnew} vs {h}\"\n", "def test_adjusted_figsize():\n    # Test the adjusted_figsize function for various inputs.\n    w, h, dpi, n = 5, 5, 100, 4\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 5.04\n    assert hnew == 5.04\n\n    w, h, dpi, n = 6.5, 3.3, 120, 5\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert np.isclose(wnew, 6.5, rtol=1e-2)\n    assert np.isclose(hnew, 3.3, rtol=1e-2)\n\n    w, h, dpi, n = 8.0, 4.0, 80, 2\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert np.isclose(wnew, 8.0, rtol=1e-2)\n    assert np.isclose(hnew, 4.0, rtol=1e-2)\n", "def test_adjusted_figsize():\n    # Test adjusted_figsize function with different inputs\n    w, h, dpi, n = 8, 6, 100, 2\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 8.0\n    assert hnew == 6.0\n\n    w, h, dpi, n = 7.5, 5.5, 96, 3\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 7.5\n    assert hnew == 5.5\n\n    w, h, dpi, n = 10, 8, 120, 4\n    wnew, hnew = adjusted_figsize(w, h, dpi, n)\n    assert wnew == 10.0\n    assert hnew == 8.0\n", "def test_adjusted_figsize():\n    w, h, dpi, n = 5.0, 5.0, 100.0, 2\n    wnew, hnew = animation.adjusted_figsize(w, h, dpi, n)\n    assert wnew == 5.0\n    assert hnew == 5.0\n\n    w, h, dpi, n = 5.1, 5.1, 100.0, 2\n    wnew, hnew = animation.adjusted_figsize(w, h, dpi, n)\n    assert wnew != 5.1  # Should adjust to the nearest multiple\n    assert hnew != 5.1\n"], "sample_554": ["def test_update_bbox_position_size():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Test text\", bbox=dict(facecolor='red', alpha=0.5))\n    fig.canvas.draw()\n\n    renderer = fig.canvas.get_renderer()\n    initial_bbox = text.get_bbox_patch().get_window_extent(renderer)\n    initial_pos = text.get_position()\n\n    # Update position and ensure bbox position and size is updated correctly\n    text.set_position((0.7, 0.7))\n    text.update_bbox_position_size(renderer)\n    fig.canvas.draw()\n    updated_bbox = text.get_bbox_patch().get_window_extent(renderer)\n    updated_pos = text.get_position()\n\n    assert initial_pos != updated_pos\n    assert initial_bbox != updated_bbox\n", "def test_wrap_with_rotation_modes():\n    fig, ax = plt.subplots()\n    s = 'Text with wrap and rotation modes.'\n    text_default = ax.text(0.5, 0.5, s, wrap=True, rotation_mode='default', rotation=45)\n    text_anchor = ax.text(0.5, 0.7, s, wrap=True, rotation_mode='anchor', rotation=45)\n    fig.canvas.draw()\n    assert text_default._get_wrapped_text() == ('Text with wrap and rotation\\nmodes.')\n    assert text_anchor._get_wrapped_text() == ('Text with wrap and rotation\\nmodes.')\n", "def test_text_contains_bbox():\n    fig, ax = plt.subplots()\n    txt = ax.text(0.5, 0.5, 'Test text with bbox', bbox=dict(facecolor='red', alpha=0.5))\n    fig.canvas.draw()\n    mevent = MouseEvent('button_press_event', fig.canvas, *ax.transData.transform((0.5, 0.5)))\n    \n    contains, _ = txt.contains(mevent)\n    assert contains, \"Mouse event should be within the text bbox\"\n\n    mevent.x, mevent.y = ax.transData.transform((0.5, 0.55))  # Move outside bbox\n    contains, _ = txt.contains(mevent)\n    assert not contains, \"Mouse event should be outside the text bbox\"\n", "def test_set_backgroundcolor():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Test background color')\n    assert text.get_bbox_patch() is None  # No bbox initially\n\n    # Set a background color\n    text.set_backgroundcolor('red')\n    bbox_patch = text.get_bbox_patch()\n    assert bbox_patch is not None  # Bbox should now exist\n    assert bbox_patch.get_facecolor() == mpl.colors.to_rgba('red')\n\n    # Change the background color\n    text.set_backgroundcolor('blue')\n    assert bbox_patch.get_facecolor() == mpl.colors.to_rgba('blue')\n\n    # Remove the background color\n    text.set_backgroundcolor(None)\n    assert text.get_bbox_patch() is None  # Bbox should be removed\n\n    # Ensure changes are reflected in the renderer\n    fig.canvas.draw()\n", "def test_set_wrap():\n    text = Text(0, 0, \"Some long text that needs to be wrapped.\")\n    assert not text.get_wrap()\n    text.set_wrap(True)\n    assert text.get_wrap()\n    text.set_wrap(False)\n    assert not text.get_wrap()\n"], "sample_555": ["def test_polygon_initialization():\n    # Test initialization of Polygon with different vertex configurations\n\n    # Case 1: Simple triangle\n    xy = [[0, 0], [1, 0], [0.5, 1]]\n    p = Polygon(xy)\n    assert p.get_closed()  # should be closed by default\n    assert_array_equal(p.get_xy(), np.array(xy + [xy[0]]))  # should close path\n\n    # Case 2: Simple quadrilateral\n    xy = [[0, 0], [1, 0], [1, 1], [0, 1]]\n    p = Polygon(xy, closed=True)\n    assert p.get_closed()\n    assert_array_equal(p.get_xy(), np.array(xy + [xy[0]]))\n\n    # Case 3: Open path\n    p = Polygon(xy, closed=False)\n    assert not p.get_closed()\n    assert_array_equal(p.get_xy(), np.array(xy))\n\n    # Case 4: Single point, should be treated as degenerate\n    xy = [[0, 0]]\n    p = Polygon(xy)\n    assert p.get_closed()\n    assert_array_equal(p.get_xy(), np.array(xy + xy))  # duplicate point\n\n    # Case 5: Two points, should be treated as degenerate\n    xy = [[0, 0], [1, 1]]\n    p = Polygon(xy)\n    assert p.get_closed()\n    assert_array_equal(p.get_xy(), np.array(xy + [xy[0]]))  # should close path\n", "def test_set_facecolor():\n    p = Patch()\n    assert p.get_facecolor() == mpl.rcParams['patch.facecolor']\n    p.set_facecolor('blue')\n    assert p.get_facecolor() == mcolors.to_rgba('blue')\n    p.set_facecolor(None)\n    assert p.get_facecolor() == mpl.rcParams['patch.facecolor']\n", "def test_patch_setters():\n    # Test various setters for Patch class and its derived classes\n\n    # Test Patch class\n    patch = Patch(edgecolor='blue', facecolor='red', linewidth=2)\n    assert patch.get_edgecolor() == mcolors.to_rgba('blue')\n    assert patch.get_facecolor() == mcolors.to_rgba('red')\n    assert patch.get_linewidth() == 2\n\n    patch.set_edgecolor('green')\n    patch.set_facecolor('yellow')\n    patch.set_linewidth(4)\n\n    assert patch.get_edgecolor() == mcolors.to_rgba('green')\n    assert patch.get_facecolor() == mcolors.to_rgba('yellow')\n    assert patch.get_linewidth() == 4\n\n    # Test Rectangle class\n    rect = Rectangle((0, 0), width=5, height=10, angle=45)\n    assert rect.get_x() == 0\n    assert rect.get_y() == 0\n    assert rect.get_width() == 5\n    assert rect.get_height() == 10\n    assert rect.get_angle() == 45\n\n    rect.set_x(1)\n    rect.set_y(2)\n    rect.set_width(6)\n    rect.set_height(12)\n    rect.set_angle(90)\n\n    assert rect.get_x() == 1\n    assert rect.get_y() == 2\n    assert rect.get_width() == 6\n    assert rect.get_height() == 12\n    assert rect.get_angle() == 90\n\n    # Test Ellipse class\n    ellipse = Ellipse((2, 3), width=4, height=8, angle=30)\n    assert ellipse.get_center() == (2, 3)\n    assert ellipse.get_width() == 4\n    assert ellipse.get_height() == 8\n    assert ellipse.get_angle() == 30\n\n    ellipse.set_center((5, 6))\n    ellipse.set_width(7)\n    ellipse.set_height(14)\n    ellipse.set_angle(60)\n\n    assert ellipse.get_center() == (5, 6)\n    assert ellipse.get_width() == 7\n    assert ellipse.get_height() == 14\n    assert ellipse.get_angle() == 60\n\n    # Test FancyBboxPatch class\n    fancy_bbox = FancyBboxPatch((0, 0), width=5, height=10, boxstyle=\"round,pad=0.5\")\n    assert fancy_bbox", "def test_patch_setters_getters():\n    \"\"\"\n    Test the setters and getters for various properties of Patch and its subclasses.\n    \"\"\"\n\n    # Test Patch class\n    patch = Patch()\n    patch.set_antialiased(True)\n    assert patch.get_antialiased() is True\n\n    patch.set_edgecolor('red')\n    assert patch.get_edgecolor() == mcolors.to_rgba('red')\n\n    patch.set_facecolor('blue')\n    assert patch.get_facecolor() == mcolors.to_rgba('blue')\n\n    patch.set_linewidth(2.5)\n    assert patch.get_linewidth() == 2.5\n\n    patch.set_linestyle('--')\n    assert patch.get_linestyle() == '--'\n\n    patch.set_fill(False)\n    assert patch.get_fill() is False\n\n    patch.set_hatch('//')\n    assert patch.get_hatch() == '//'\n\n    patch.set_capstyle('round')\n    assert patch.get_capstyle() == 'round'\n\n    patch.set_joinstyle('bevel')\n    assert patch.get_joinstyle() == 'bevel'\n\n    # Test Rectangle subclass\n    rect = Rectangle((0, 0), 2, 3, angle=45)\n    rect.set_x(1)\n    assert rect.get_x() == 1\n\n    rect.set_y(2)\n    assert rect.get_y() == 2\n\n    rect.set_width(4)\n    assert rect.get_width() == 4\n\n    rect.set_height(5)\n    assert rect.get_height() == 5\n\n    rect.set_angle(30)\n    assert rect.get_angle() == 30\n\n    rect.set_bounds(0, 0, 6, 7)\n    assert rect.get_x() == 0\n    assert rect.get_y() == 0\n    assert rect.get_width() == 6\n    assert rect.get_height() == 7\n\n    # Test Ellipse subclass\n    ellipse = Ellipse((1, 1), 4, 2, angle=30)\n    ellipse.set_center((2, 2))\n    assert ellipse.get_center() == (2, 2)\n\n    ellipse.set_width(5)\n    assert ellipse.get_width() == 5\n\n    ellipse.set_height(3)\n    assert ellipse.get_height() == 3\n\n    ellipse.set_angle(60)\n    assert ellipse.get_angle() == 60\n\n    # Test Polygon subclass\n    polygon = Polygon([[0, 0], [1, 1], [", "def test_patch_contains():\n    # Test the contains method of Patch class for different shapes\n    fig, ax = plt.subplots()\n    \n    rect = Rectangle((0, 0), 2, 2)\n    circ = mpatches.Circle((1, 1), radius=1)\n    poly = Polygon([[0, 0], [2, 0], [1, 2]], closed=True)\n\n    ax.add_patch(rect)\n    ax.add_patch(circ)\n    ax.add_patch(poly)\n\n    rect_contains, _ = rect.contains(SimpleNamespace(x=1, y=1))\n    circ_contains, _ = circ.contains(SimpleNamespace(x=1, y=1))\n    poly_contains, _ = poly.contains(SimpleNamespace(x=1, y=1))\n\n    assert rect_contains\n    assert circ_contains\n    assert poly_contains\n\n    rect_not_contains, _ = rect.contains(SimpleNamespace(x=3, y=3))\n    circ_not_contains, _ = circ.contains(SimpleNamespace(x=3, y=3))\n    poly_not_contains, _ = poly.contains(SimpleNamespace(x=3, y=3))\n\n    assert not rect_not_contains\n    assert not circ_not_contains\n    assert not poly_not_contains\n"], "sample_556": ["def test_add_axes_existing_axes():\n    fig = plt.figure()\n    ax1 = fig.add_axes([0, 0, 1, 1])\n    ax2 = fig.add_axes(ax1)\n    assert ax1 in fig.axes\n    assert ax2 in fig.axes\n    assert ax1 is not ax2\n    assert fig.axes == [ax1, ax2]\n\n    ax3 = fig.add_axes([0.5, 0.5, 0.4, 0.4])\n    assert ax3 in fig.axes\n    assert fig.axes == [ax1, ax2, ax3]\n\n    fig.delaxes(ax2)\n    assert ax2 not in fig.axes\n    assert fig.axes == [ax1, ax3]\n\n    fig.delaxes(ax1)\n    assert ax1 not in fig.axes\n    assert fig.axes == [ax3]\n\n    fig.clear()\n    assert fig.axes == []\n", "def test_add_artist_clip():\n    # Test the add_artist method with clipping enabled\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    rect = plt.Rectangle((0.1, 0.1), 0.5, 0.5, color='blue')\n    fig.add_artist(rect, clip=True)\n    \n    assert rect.get_clip_path() == fig.patch\n    assert rect.get_figure() == fig\n    assert rect in fig.artists\n\n    # Ensure that the artist is drawn\n    fig.canvas.draw()\n", "def test_subplotparams_update():\n    params = SubplotParams(left=0.1, bottom=0.2, right=0.9, top=0.9, wspace=0.3, hspace=0.4)\n    assert params.left == 0.1\n    assert params.bottom == 0.2\n    assert params.right == 0.9\n    assert params.top == 0.9\n    assert params.wspace == 0.3\n    assert params.hspace == 0.4\n\n    params.update(left=0.2, bottom=0.1, right=0.8, top=0.95, wspace=0.2, hspace=0.3)\n    assert params.left == 0.2\n    assert params.bottom == 0.1\n    assert params.right == 0.8\n    assert params.top == 0.95\n    assert params.wspace == 0.2\n    assert params.hspace == 0.3\n\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        params.update(left=1.0)\n\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        params.update(bottom=1.0)\n", "def test_add_artist_with_clip():\n    fig, ax = plt.subplots()\n    artist = plt.Line2D([0, 1], [0, 1], color='blue')\n    fig.add_artist(artist, clip=True)\n    assert artist.get_clip_path() == fig.patch\n    assert artist.get_figure() == fig\n    assert artist in fig.artists\n", "def test_subplot_mosaic_string_input():\n    fig = plt.figure()\n    mosaic = \"\"\"\n             ABC\n             DEF\n             GHI\n             \"\"\"\n    ax_dict = fig.subplot_mosaic(mosaic.strip())\n\n    expected_keys = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\n    assert list(ax_dict.keys()) == expected_keys\n\n    for key, ax in ax_dict.items():\n        assert ax.get_label() == key\n        ax.set_title(key)\n\n    fig.tight_layout()\n"], "sample_557": ["def test_figure_add_artist_clip():\n    fig, ax = plt.subplots()\n    artist = mpl.patches.Circle((0.5, 0.5), radius=0.2)\n    fig.add_artist(artist, clip=True)\n    \n    assert artist.get_clip_path() == fig.patch\n    assert artist.get_figure() == fig\n\n    artist2 = mpl.patches.Rectangle((0.2, 0.2), 0.6, 0.6)\n    fig.add_artist(artist2, clip=False)\n    \n    assert artist2.get_clip_path() is None\n    assert artist2.get_figure() == fig\n\n    assert artist in fig.artists\n    assert artist2 in fig.artists\n", "def test_update_subplot_params():\n    fig = plt.figure()\n    with pytest.raises(ValueError, match='left cannot be >= right'):\n        fig.subplotpars.update(left=0.9, right=0.8)\n    with pytest.raises(ValueError, match='bottom cannot be >= top'):\n        fig.subplotpars.update(bottom=0.9, top=0.8)\n\n    fig.subplotpars.update(left=0.1, bottom=0.1, right=0.9, top=0.9)\n    assert fig.subplotpars.left == 0.1\n    assert fig.subplotpars.bottom == 0.1\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.top == 0.9\n", "def test_subplot_params_update():\n    # Create SubplotParams instance with default values\n    params = SubplotParams()\n    assert params.left == mpl.rcParams[\"figure.subplot.left\"]\n    assert params.right == mpl.rcParams[\"figure.subplot.right\"]\n    assert params.bottom == mpl.rcParams[\"figure.subplot.bottom\"]\n    assert params.top == mpl.rcParams[\"figure.subplot.top\"]\n    assert params.wspace == mpl.rcParams[\"figure.subplot.wspace\"]\n    assert params.hspace == mpl.rcParams[\"figure.subplot.hspace\"]\n\n    # Update values\n    params.update(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.2, hspace=0.2)\n    assert params.left == 0.1\n    assert params.right == 0.9\n    assert params.bottom == 0.1\n    assert params.top == 0.9\n    assert params.wspace == 0.2\n    assert params.hspace == 0.2\n\n    # Test invalid updates\n    with pytest.raises(ValueError, match=\"left cannot be >= right\"):\n        params.update(left=1.0, right=0.5)\n    with pytest.raises(ValueError, match=\"bottom cannot be >= top\"):\n        params.update(bottom=1.0, top=0.5)\n", "def test_axes_stack_methods():\n    stack = Figure()._axstack\n\n    ax1 = plt.subplot(221)\n    ax2 = plt.subplot(222)\n    ax3 = plt.subplot(223)\n    ax4 = plt.subplot(224)\n\n    stack.add(ax1)\n    stack.add(ax2)\n    stack.add(ax3)\n    stack.add(ax4)\n    \n    assert stack.as_list() == [ax1, ax2, ax3, ax4]\n\n    stack.bubble(ax2)\n    assert stack.current() == ax2\n\n    stack.remove(ax2)\n    assert stack.as_list() == [ax1, ax3, ax4]\n    assert stack.current() == ax4\n\n    with pytest.raises(ValueError, match=\"Axes has not been added yet\"):\n        stack.bubble(ax2)\n\n    state = stack.__getstate__()\n    new_stack = Figure()._axstack\n    new_stack.__setstate__(state)\n    assert new_stack.as_list() == [ax1, ax3, ax4]\n    assert new_stack.current() == ax4\n", "def test_add_remove_axes():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    assert ax1 in fig.axes\n    fig.delaxes(ax1)\n    assert ax1 not in fig.axes\n    \n    ax2 = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    assert ax2 in fig.axes\n    fig.delaxes(ax2)\n    assert ax2 not in fig.axes\n\n    ax3 = fig.add_subplot(211)\n    ax4 = fig.add_subplot(212)\n    assert ax3 in fig.axes\n    assert ax4 in fig.axes\n    fig.delaxes(ax3)\n    assert ax3 not in fig.axes\n    assert ax4 in fig.axes\n    fig.delaxes(ax4)\n    assert ax4 not in fig.axes\n"], "sample_558": ["def test_grid_set_get_aspect():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2))\n    assert grid.get_aspect() == False  # default value\n    grid.set_aspect(True)\n    assert grid.get_aspect() == True\n", "def test_grid_set_get_axes_pad():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2))\n    \n    # Check initial padding\n    assert grid.get_axes_pad() == (0.02, 0.02)\n    \n    # Set new padding\n    new_pad = (0.1, 0.1)\n    grid.set_axes_pad(new_pad)\n    \n    # Check if new padding is set correctly\n    assert grid.get_axes_pad() == new_pad\n", "def test_grid_set_get_axes_pad():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), axes_pad=(0.1, 0.2))\n    assert grid.get_axes_pad() == (0.1, 0.2)\n    grid.set_axes_pad((0.3, 0.4))\n    assert grid.get_axes_pad() == (0.3, 0.4)\n", "def test_image_grid_cbar_pad():\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111, nrows_ncols=(1, 1), cbar_mode=\"single\", cbar_pad=0.2)\n    ax = grid[0]\n    im = ax.imshow([[1, 2]], norm=mpl.colors.LogNorm())\n    cb = ax.cax.colorbar(im)\n    fig.canvas.draw()\n    assert cb.ax.get_position().height == pytest.approx(0.2, rel=1e-2)\n", "def test_grid_set_get_aspect():\n    fig = plt.figure()\n    grid = Grid(fig, 111, nrows_ncols=(2, 2))\n    assert grid.get_aspect() is False  # Default aspect is False\n    grid.set_aspect(True)\n    assert grid.get_aspect() is True\n"], "sample_559": ["def test_indicate_inset_zoom():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), range(10))\n    inset_ax = ax.inset_axes([0.5, 0.5, 0.4, 0.4])\n    inset_ax.plot(range(10), range(10, 20))\n    patch, connectors = ax.indicate_inset_zoom(inset_ax)\n    assert isinstance(patch, mpatches.Rectangle)\n    assert len(connectors) == 4\n    assert all(isinstance(connector, mpatches.ConnectionPatch) for connector in connectors)\n", "def test_indicate_inset_zoom():\n    fig, ax = plt.subplots()\n    ax.plot(np.linspace(0, 10, 100), np.sin(np.linspace(0, 10, 100)))\n    axin = ax.inset_axes([5, -1, 5, 2], transform=ax.transData)\n    axin.plot(np.linspace(5, 10, 100), np.sin(np.linspace(5, 10, 100)))\n    patch, connectors = ax.indicate_inset_zoom(axin)\n    assert patch.get_xy()[0] == 5\n    assert patch.get_width() == 5\n    assert patch.get_height() == 2\n    assert all(isinstance(conn, mpatches.ConnectionPatch) for conn in connectors)\n    fig.canvas.draw()\n", "def test_fill_between():\n    x = np.linspace(0, 2 * np.pi, 100)\n    y1 = np.sin(x)\n    y2 = np.sin(x) + 0.2\n\n    fig, ax = plt.subplots()\n    ax.fill_between(x, y1, y2, color='skyblue', alpha=0.4)\n    ax.plot(x, y1, color='Slateblue', alpha=0.6, linewidth=2)\n    ax.plot(x, y2, color='Slateblue', alpha=0.6, linewidth=2)\n\n    # Ensure fill_between creates PolyCollection\n    poly_collections = [coll for coll in ax.collections if isinstance(coll, mpl.collections.PolyCollection)]\n    assert len(poly_collections) == 1\n\n    poly = poly_collections[0]\n    assert poly.get_facecolor().shape == (1, 4)  # Single color with RGBA\n    np.testing.assert_almost_equal(poly.get_alpha(), 0.4, decimal=2)\n    np.testing.assert_array_equal(poly.get_facecolor()[0, :3], mpl.colors.to_rgba('skyblue')[:3])\n", "def test_indicate_inset_zoom():\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    inset_ax = inset_axes(ax, width=\"30%\", height=\"30%\", loc='upper right')\n    inset_ax.plot(range(10))\n    \n    # Apply indicate_inset_zoom to highlight the inset area\n    rect, lines = ax.indicate_inset_zoom(inset_ax)\n    \n    # Check that the rectangle and lines are added to the parent ax\n    assert rect in ax.patches\n    assert all(line in ax.patches for line in lines)\n    \n    # Check the coordinates of the rectangle\n    xlim = inset_ax.get_xlim()\n    ylim = inset_ax.get_ylim()\n    assert rect.get_x() == xlim[0]\n    assert rect.get_y() == ylim[0]\n    assert rect.get_width() == xlim[1] - xlim[0]\n    assert rect.get_height() == ylim[1] - ylim[0]\n    \n    # Render the figure to make sure no exceptions are raised\n    fig.canvas.draw()\n", "def test_indicate_inset():\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    ax.inset_axes([0.1, 0.1, 0.4, 0.4])\n    inset_ax = ax.inset_axes([2, 3, 4, 4], transform=ax.transData)\n    rect, connects = ax.indicate_inset([1, 2, 4, 4], inset_ax)\n    assert isinstance(rect, mpatches.Rectangle)\n    assert len(connects) == 4\n    assert all(isinstance(conn, mpatches.ConnectionPatch) for conn in connects)\n"], "sample_560": ["def test_legend_invalid_shadow_type():\n    # Test that setting shadow to an invalid type raises an appropriate error\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    with pytest.raises(ValueError, match=\"Legend shadow must be a dict or bool, not\"):\n        ax.legend(shadow=\"invalid_type\")\n", "def test_legend_set_loc_outside_figure():\n    \"\"\"\n    Test that setting the legend location outside the figure raises an error\n    for an axes legend.\n    \"\"\"\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), label='test')\n    leg = ax.legend()\n\n    with pytest.raises(ValueError, match=\"only works for figure legends\"):\n        leg.set_loc(\"outside upper right\")\n    \n    with pytest.raises(ValueError, match=\"only works for figure legends\"):\n        leg.set_loc(\"outside lower left\")\n", "def test_legend_invalid_alignment():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    with pytest.raises(ValueError, match=\"alignment must be 'center', 'left', or 'right'\"):\n        ax.legend(alignment='invalid')\n", "def test_legend_handler_mapping():\n    \"\"\"Test that custom legend handler mapping works as expected.\"\"\"\n    class CustomHandler:\n            return mpatches.Circle((0.5, 0.5), 0.25)\n\n    fig, ax = plt.subplots()\n    lines = ax.plot(range(10), label='line')\n    ax.legend(handler_map={lines[0]: CustomHandler()})\n    leg = ax.get_legend()\n    legend_handles = leg.legend_handles\n    assert len(legend_handles) == 1\n    assert isinstance(legend_handles[0], mpatches.Circle)\n", "def test_legend_set_bbox_to_anchor_invalid():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend()\n\n    with pytest.raises(ValueError, match=\"Invalid bbox:\"):\n        leg.set_bbox_to_anchor(\"invalid_bbox\")\n\n    with pytest.raises(ValueError, match=\"Invalid bbox:\"):\n        leg.set_bbox_to_anchor([1, 2, 3, \"invalid\"])\n\n    with pytest.raises(ValueError, match=\"Invalid bbox:\"):\n        leg.set_bbox_to_anchor([1, 2, 3])\n\n    with pytest.raises(ValueError, match=\"Invalid bbox:\"):\n        leg.set_bbox_to_anchor((1, 2, \"invalid\"))\n\n    with pytest.raises(ValueError, match=\"Invalid bbox:\"):\n        leg.set_bbox_to_anchor((1,))\n"], "sample_561": ["def test_marker_filled_markers():\n    filled_markers = [\n        '.', 'o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd',\n        'P', 'X'\n    ]\n    for marker in filled_markers:\n        marker_style = markers.MarkerStyle(marker)\n        assert marker_style.is_filled(), f\"Marker {marker} should be filled\"\n\n    unfilled_markers = [\n        '+', 'x', '|', '_', markers.TICKLEFT, markers.TICKRIGHT,\n        markers.TICKUP, markers.TICKDOWN, markers.CARETLEFT, markers.CARETRIGHT,\n        markers.CARETUP, markers.CARETDOWN, markers.CARETLEFTBASE, markers.CARETRIGHTBASE,\n        markers.CARETUPBASE, markers.CARETDOWNBASE, \"None\", \"none\", ' ', ''\n    ]\n    for marker in unfilled_markers:\n        marker_style = markers.MarkerStyle(marker)\n        assert not marker_style.is_filled(), f\"Marker {marker} should not be filled\"\n", "def test_is_filled(marker, expected_filled):\n    marker_style = markers.MarkerStyle(marker)\n    assert marker_style.is_filled() == expected_filled\n", "def test_marker_no_marker():\n    marker_style = markers.MarkerStyle(marker=None)\n    assert marker_style.get_marker() is None\n    assert not marker_style.is_filled()\n    assert not marker_style.get_path().vertices.size\n", "def test_marker_get_path():\n    marker_style = markers.MarkerStyle(marker='o')\n    path = marker_style.get_path()\n    assert isinstance(path, Path)\n    assert path.vertices.shape == (8, 2)\n    assert path.codes is None\n", "def test_marker_custom_path():\n    custom_path = Path([[0, 0], [1, 0], [0.5, 1], [0, 0]], closed=True)\n    marker_style = markers.MarkerStyle(custom_path)\n    assert marker_style.get_path() == custom_path\n\n    custom_path_2 = Path([[0, 0], [1, 0], [0.5, 1], [0, 0], [0, 0.5]], closed=False)\n    marker_style_2 = markers.MarkerStyle(custom_path_2)\n    assert marker_style_2.get_path() == custom_path_2\n"], "sample_562": ["def test_set_dashes():\n    fig, ax = plt.subplots()\n    line, = ax.plot(range(10))\n    line.set_dashes([5, 2, 10, 5])\n    assert line.get_linestyle() == '--'\n    assert line._dash_pattern == (0, [5, 2, 10, 5])\n    with pytest.raises(ValueError, match='Unrecognized linestyle'):\n        line.set_dashes([None, None])\n", "def test_Line2D_str():\n    line_with_label = mlines.Line2D([0, 1, 2], [2, 3, 4], label=\"TestLine\")\n    assert str(line_with_label) == \"Line2D(TestLine)\"\n\n    line_with_no_label = mlines.Line2D([0, 1, 2], [2, 3, 4])\n    assert str(line_with_no_label) == \"Line2D((0,2),(1,3),...,(2,4))\"\n\n    line_with_few_points = mlines.Line2D([0, 1], [2, 3])\n    assert str(line_with_few_points) == \"Line2D((0,2),(1,3))\"\n\n    line_with_no_points = mlines.Line2D([], [])\n    assert str(line_with_no_points) == \"Line2D()\"\n", "def test_vertex_selector():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 10)\n    y = np.sin(x)\n    line, = ax.plot(x, y, 'bs-', picker=5)\n\n    class HighlightSelected(mlines.VertexSelector):\n            super().__init__(line)\n            self.markers, = self.axes.plot([], [], fmt, **kwargs)\n\n            self.markers.set_data(xs, ys)\n            self.canvas.draw()\n\n    selector = HighlightSelected(line)\n\n    # Simulate a pick event\n    fake_event = SimpleNamespace(artist=line, ind=np.array([1, 3, 5]))\n    selector.onpick(fake_event)\n\n    assert selector.ind == {1, 3, 5}\n    assert_array_equal(selector.line.get_data(), (x, y))\n    assert_array_equal(selector.markers.get_data(), (x[[1, 3, 5]], y[[1, 3, 5]]))\n", "def test_invalid_marker():\n    line = mlines.Line2D([], [])\n    with pytest.raises(ValueError):\n        line.set_marker(\"invalid_marker\")\n", "def test_get_dash_pattern():\n    \"\"\"Test the `_get_dash_pattern` function with various styles.\"\"\"\n    assert mlines._get_dash_pattern('solid') == (0, None)\n    assert mlines._get_dash_pattern('None') == (0, None)\n    assert mlines._get_dash_pattern('dashed') == (0, tuple(mpl.rcParams['lines.dashed_pattern']))\n    assert mlines._get_dash_pattern('dashdot') == (0, tuple(mpl.rcParams['lines.dashdot_pattern']))\n    assert mlines._get_dash_pattern('dotted') == (0, tuple(mpl.rcParams['lines.dotted_pattern']))\n    with pytest.raises(ValueError, match=\"Unrecognized linestyle\"):\n        mlines._get_dash_pattern((None, None))\n    with pytest.raises(ValueError, match=\"Unrecognized linestyle\"):\n        mlines._get_dash_pattern('invalid')\n    assert mlines._get_dash_pattern((5, (1, 2, 3, 4))) == (5, (1, 2, 3, 4))\n"], "sample_563": ["def test_offsetbox_set_offset():\n    fig, ax = plt.subplots()\n    ta = TextArea(\"Offset Test\")\n    ab = AnchoredOffsetbox('center', child=ta)\n    ax.add_artist(ab)\n    ab.set_offset((10, 20))\n    \n    fig.canvas.draw()\n    bbox = ab.get_window_extent(fig.canvas.get_renderer())\n    expected_offset = (10, 20)\n    actual_offset = ab.get_offset(bbox, fig.canvas.get_renderer())\n    assert actual_offset == expected_offset, f\"Expected offset {expected_offset}, but got {actual_offset}\"\n", "def test_offsetbox_contains():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    anchored_box = AnchoredOffsetbox(loc='center', child=da, pad=0., frameon=False, bbox_to_anchor=(.5, .5), bbox_transform=ax.transAxes, borderpad=0.)\n    ax.add_artist(anchored_box)\n\n    fig.canvas.draw()\n    \n    # Create a MouseEvent at the center of the DrawingArea\n    x, y = ax.transData.transform((0.5, 0.5))\n    event = MouseEvent(\"motion_notify_event\", fig.canvas, x, y, MouseButton.LEFT)\n    \n    contains, _ = anchored_box.contains(event)\n    assert contains, \"AnchoredOffsetbox should contain the mouse event at its center.\"\n\n    # Create a MouseEvent outside the DrawingArea\n    x, y = ax.transData.transform((0, 0))\n    event = MouseEvent(\"motion_notify_event\", fig.canvas, x, y, MouseButton.LEFT)\n    \n    contains, _ = anchored_box.contains(event)\n    assert not contains, \"AnchoredOffsetbox should not contain the mouse event outside its area.\"\n", "def test_offsetbox_set_get_offset():\n    # Test the set_offset and get_offset methods of OffsetBox\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    ob = OffsetBox()\n\n    # Set the offset as a tuple\n    ob.set_offset((10, 20))\n    assert ob.get_offset(None, None) == (10, 20)\n\n    # Set the offset using a callable\n        return (width + xdescent, height + ydescent)\n\n    ob.set_offset(offset_func)\n    bbox = Bbox.from_bounds(0, 0, 100, 100)\n    assert ob.get_offset(bbox, None) == (100, 100)\n", "def test_offsetbox_set_offset_callable():\n    # Test that set_offset works correctly with a callable\n    fig, ax = plt.subplots()\n        return (width / 2, height / 2)\n\n    ta = TextArea(\"test\")\n    ta.set_offset(offset_callable)\n    ab = AnchoredOffsetbox(loc='center', child=ta)\n    ax.add_artist(ab)\n\n    fig.canvas.draw()\n    bbox = ta.get_bbox(fig.canvas.get_renderer())\n    offset = ta.get_offset(bbox, fig.canvas.get_renderer())\n    assert offset == (bbox.width / 2, bbox.height / 2)\n", "def test_draggable_annotation():\n    fig, ax = plt.subplots()\n    ab = AnnotationBbox(DrawingArea(20, 20, 0, 0, clip=True), (0.5, 0.5),\n                        xybox=(-0.2, 0.4), xycoords='data',\n                        boxcoords='axes fraction', arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ab)\n    ab.draggable(True)\n\n    # Trigger pick event\n    calls = []\n    fig.canvas.mpl_connect('pick_event', lambda event: calls.append(event))\n    MouseEvent(\"button_press_event\", fig.canvas, *fig.transFigure.transform((0.5, 0.5)), MouseButton.LEFT)._process()\n    assert len(calls) == 1 and calls[0].artist == ab\n\n    # Check the offset\n    draggable = ab.get_draggable()\n    initial_offset = draggable.offsetbox.get_offset()\n    dx, dy = 10, 10\n    draggable.update_offset(dx, dy)\n    assert draggable.offsetbox.get_offset() == (initial_offset[0] + dx, initial_offset[1] + dy)\n    \n    # Trigger motion event to update position\n    motion_event = MouseEvent(\"motion_notify_event\", fig.canvas, *fig.transFigure.transform((0.6, 0.6)), MouseButton.LEFT)\n    draggable.on_motion(motion_event)\n    assert draggable.offsetbox.get_offset() == (initial_offset[0] + 0.1 * fig.dpi / 72, initial_offset[1] + 0.1 * fig.dpi / 72)\n\n    # Trigger release event to finalize the position\n    draggable.on_release(MouseEvent(\"button_release_event\", fig.canvas, 1, 1))\n    assert not draggable.got_artist\n"], "sample_564": ["def test_axes3d_set_aspect():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    plot_cuboid(ax, scale=[1, 1, 5])\n    ax.set_aspect('equal', adjustable='box')\n    ax.view_init(elev=20, azim=30)\n", "def test_set_box_aspect():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    \n    # default aspect ratio\n    plot_cuboid(ax, scale=[1, 1, 1])\n    ax.set_title('Default Aspect')\n    \n    # set different box aspect ratios\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.set_box_aspect((1, 2, 1))\n    plot_cuboid(ax, scale=[1, 1, 1])\n    ax.set_title('Aspect (1, 2, 1)')\n    \n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.set_box_aspect((2, 1, 1))\n    plot_cuboid(ax, scale=[1, 1, 1])\n    ax.set_title('Aspect (2, 1, 1)')\n    \n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.set_box_aspect((1, 1, 2))\n    plot_cuboid(ax, scale=[1, 1, 1])\n    ax.set_title('Aspect (1, 1, 2)')\n", "def test_get_zlabel():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.set_zlabel('Test Z label')\n    assert ax.get_zlabel() == 'Test Z label'\n", "def test_get_axis_position_custom_view():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    x = np.arange(10)\n    ax.plot(x, np.sin(x))\n\n    # Custom view to test different axis positions\n    ax.view_init(elev=45, azim=45)\n    fig.canvas.draw()\n    assert ax.get_axis_position() == (False, False, True)\n", "def test_wireframe3d_shaded():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    X, Y, Z = axes3d.get_test_data(0.05)\n    ax.plot_wireframe(X, Y, Z, rcount=13, ccount=13, shade=True)\n"], "sample_565": ["def test_bounding_boxes_and_connectors():\n    fig, ax = plt.subplots()\n    bbox1 = Bbox.from_bounds(0.1, 0.1, 0.3, 0.3)\n    bbox2 = Bbox.from_bounds(0.5, 0.5, 0.3, 0.3)\n\n    # Test BboxPatch\n    patch = BboxPatch(bbox1, fc='blue', alpha=0.5)\n    ax.add_patch(patch)\n    assert patch.get_path().vertices.shape == (5, 2)  # 4 corners + close\n\n    # Test BboxConnector\n    connector = BboxConnector(bbox1, bbox2, loc1=1, loc2=3, ec='red')\n    ax.add_patch(connector)\n    path = connector.get_path()\n    assert_array_almost_equal(path.vertices[0], [0.4, 0.4])  # loc1 of bbox1\n    assert_array_almost_equal(path.vertices[1], [0.5, 0.5])  # loc2 of bbox2\n\n    # Test BboxConnectorPatch\n    connector_patch = BboxConnectorPatch(\n        bbox1, bbox2, loc1a=1, loc2a=3, loc1b=2, loc2b=4, fc='yellow', alpha=0.3)\n    ax.add_patch(connector_patch)\n    path_patch = connector_patch.get_path()\n    assert len(path_patch.vertices) == 5  # 4 corners + close\n\n    fig.canvas.draw()\n", "def test_insetposition_initialization():\n    fig, ax = plt.subplots(figsize=(2, 2))\n    lbwh = [0.2, 0.25, 0.5, 0.4]\n    ip = InsetPosition(ax, lbwh)\n    assert ip.parent == ax\n    assert ip.lbwh == lbwh\n\n    ax_ins = plt.axes([0, 0, 1, 1])\n    ax_ins.set_axes_locator(ip)\n    fig.canvas.draw()\n    pos = ax_ins.get_position()\n    expected_pos = Bbox.from_bounds(*lbwh)\n    transformed_pos = BboxTransformTo(ax.get_position()).transform_bbox(expected_pos)\n    assert_array_almost_equal(pos.extents, transformed_pos.extents)\n", "def test_BboxPatch():\n    fig, ax = plt.subplots()\n    bbox = Bbox.from_bounds(0.2, 0.3, 0.4, 0.5)\n    patch = BboxPatch(bbox, facecolor='blue')\n    ax.add_patch(patch)\n    fig.canvas.draw()\n\n    path = patch.get_path()\n    assert_array_almost_equal(path.vertices, [[0.2, 0.3], [0.6, 0.3], [0.6, 0.8], [0.2, 0.8], [0.2, 0.3]])\n    assert patch.get_facecolor() == (0.0, 0.0, 1.0, 1.0)  # RGBA for blue\n", "def test_bboxes_connector():\n    fig, ax = plt.subplots()\n    bbox1 = Bbox.from_extents(0.1, 0.1, 0.4, 0.4)\n    bbox2 = Bbox.from_extents(0.5, 0.5, 0.8, 0.8)\n    \n    connector = BboxConnector(bbox1, bbox2, loc1=1, loc2=2, color=\"blue\")\n    ax.add_patch(connector)\n    \n    assert connector.get_path().vertices[0][0] == bbox1.extents[2]\n    assert connector.get_path().vertices[0][1] == bbox1.extents[3]\n    assert connector.get_path().vertices[1][0] == bbox2.extents[0]\n    assert connector.get_path().vertices[1][1] == bbox2.extents[3]\n    \n    connector_patch = BboxConnectorPatch(bbox1, bbox2, loc1a=1, loc2a=2, loc1b=3, loc2b=4, color=\"red\")\n    ax.add_patch(connector_patch)\n\n    assert connector_patch.get_path().vertices[0][0] == bbox1.extents[2]\n    assert connector_patch.get_path().vertices[0][1] == bbox1.extents[3]\n    assert connector_patch.get_path().vertices[1][0] == bbox2.extents[0]\n    assert connector_patch.get_path().vertices[1][1] == bbox2.extents[3]\n    assert connector_patch.get_path().vertices[2][0] == bbox2.extents[2]\n    assert connector_patch.get_path().vertices[2][1] == bbox2.extents[1]\n    assert connector_patch.get_path().vertices[3][0] == bbox1.extents[0]\n    assert connector_patch.get_path().vertices[3][1] == bbox1.extents[1]\n\n    plt.close(fig)\n", "def test_inset_position():\n    fig, ax = plt.subplots(figsize=(6, 5))\n    ax_ins = plt.axes([0, 0, 1, 1])\n    ip = InsetPosition(ax, [0.1, 0.1, 0.4, 0.3])\n    ax_ins.set_axes_locator(ip)\n    fig.canvas.draw()\n    bbox = ip(ax, fig.canvas.get_renderer())\n    assert_array_almost_equal(bbox.extents, \n                              [ax.bbox.x0 + 0.1 * ax.bbox.width, \n                               ax.bbox.y0 + 0.1 * ax.bbox.height,\n                               ax.bbox.x0 + 0.5 * ax.bbox.width,\n                               ax.bbox.y0 + 0.4 * ax.bbox.height])\n"], "sample_566": ["def test_suplabels():\n    fig = plt.figure()\n    ax = fig.add_subplot()\n    supxlabel_text = fig.suplabels(\"This is a supxlabel\", \n                                   info={'name': '_supxlabel', 'x0': 0.5, 'y0': 0.01, 'ha': 'center', 'va': 'bottom', 'rotation': 0, 'size': 'figure.labelsize', 'weight': 'figure.labelweight'})\n    supylabel_text = fig.suplabels(\"This is a supylabel\", \n                                   info={'name': '_supylabel', 'x0': 0.02, 'y0': 0.5, 'ha': 'left', 'va': 'center', 'rotation': 'vertical', 'rotation_mode': 'anchor', 'size': 'figure.labelsize', 'weight': 'figure.labelweight'})\n    suptitle_text = fig.suplabels(\"This is a suptitle\", \n                                  info={'name': '_suptitle', 'x0': 0.5, 'y0': 0.98, 'ha': 'center', 'va': 'top', 'rotation': 0, 'size': 'figure.titlesize', 'weight': 'figure.titleweight'})\n    \n    assert supxlabel_text.get_text() == \"This is a supxlabel\"\n    assert supylabel_text.get_text() == \"This is a supylabel\"\n    assert suptitle_text.get_text() == \"This is a suptitle\"\n", "def test_add_axes_order():\n    fig = plt.figure()\n    ax1 = fig.add_axes([0, 0, 1, 1])\n    ax2 = fig.add_axes([0, 0, 0.5, 0.5])\n    ax3 = fig.add_axes([0.5, 0.5, 0.5, 0.5])\n\n    assert fig.axes == [ax1, ax2, ax3], \"Axes are not in the order they were added.\"\n\n    fig.delaxes(ax2)\n    assert fig.axes == [ax1, ax3], \"Axes were not removed correctly.\"\n\n    ax2_new = fig.add_axes([0, 0, 0.5, 0.5])\n    assert fig.axes == [ax1, ax3, ax2_new], \"New Axes were not added to the end of the list.\"\n\n    fig.sca(ax1)\n    assert fig.gca() == ax1, \"Current Axes was not set correctly using sca().\"\n    fig.sca(ax3)\n    assert fig.gca() == ax3, \"Current Axes was not set correctly using sca().\"\n", "def test_add_artist_clip():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    rect = mpl.patches.Rectangle((0.5, 0.5), 0.2, 0.2, transform=fig.transFigure, facecolor=\"blue\")\n    \n    # Add artist without clipping\n    fig.add_artist(rect, clip=False)\n    assert rect.get_clip_path() is None\n    \n    # Add artist with clipping\n    rect2 = mpl.patches.Rectangle((0.2, 0.2), 0.2, 0.2, transform=fig.transFigure, facecolor=\"red\")\n    fig.add_artist(rect2, clip=True)\n    assert rect2.get_clip_path() == fig.patch\n", "def test_add_subplot_invalid_3digit():\n    fig = plt.figure()\n    with pytest.raises(ValueError, match='must be a three-digit integer'):\n        fig.add_subplot(42)\n    with pytest.raises(ValueError, match='must be a three-digit integer'):\n        fig.add_subplot(1000)\n", "def test_subfigure_subplot_add():\n    fig = plt.figure(layout='constrained')\n    subfig = fig.subfigures(1, 1)\n    ax1 = subfig.add_subplot(121)\n    ax2 = subfig.add_subplot(122)\n    assert ax1 in fig.axes\n    assert ax2 in fig.axes\n    assert ax1 in subfig.axes\n    assert ax2 in subfig.axes\n    assert len(fig.axes) == 2\n    assert len(subfig.axes) == 2\n"], "sample_567": ["def test_update_properties():\n    text = Text(0.5, 0.5, \"Initial Text\", color=\"blue\", fontsize=12)\n    assert text.get_text() == \"Initial Text\"\n    assert text.get_color() == \"blue\"\n    assert text.get_fontsize() == 12\n\n    text.update({\"text\": \"Updated Text\", \"color\": \"red\", \"fontsize\": 20})\n    assert text.get_text() == \"Updated Text\"\n    assert text.get_color() == \"red\"\n    assert text.get_fontsize() == 20\n", "def test_update_bbox_position_size():\n    fig, ax = plt.subplots()\n    text = Text(0.5, 0.5, \"Test BBox\")\n    ax.add_artist(text)\n    \n    # Mock the renderer\n    renderer = fig.canvas.get_renderer()\n    \n    # Initial bbox values\n    text.update_bbox_position_size(renderer)\n    bbox1 = text.get_bbox_patch().get_window_extent(renderer)\n\n    # Change the text and update bbox\n    text.set_text(\"Updated BBox\")\n    text.update_bbox_position_size(renderer)\n    bbox2 = text.get_bbox_patch().get_window_extent(renderer)\n    \n    # Check if the bbox updated correctly\n    assert bbox1 != bbox2\n", "def test_set_bbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Hello World', fontsize=12)\n    text.set_bbox(dict(facecolor='red', alpha=0.5))\n    assert text.get_bbox_patch() is not None\n    bbox_patch = text.get_bbox_patch()\n    assert bbox_patch.get_facecolor() == (1.0, 0.0, 0.0, 0.5)  # Red with 50% transparency\n    text.set_bbox(None)\n    assert text.get_bbox_patch() is None\n", "def test_text_set_horizontalalignment():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Aligned Text')\n    \n    text.set_horizontalalignment('center')\n    assert text.get_horizontalalignment() == 'center'\n    \n    text.set_horizontalalignment('right')\n    assert text.get_horizontalalignment() == 'right'\n    \n    text.set_horizontalalignment('left')\n    assert text.get_horizontalalignment() == 'left'\n    \n    with pytest.raises(ValueError, match=\"'align' must be one of 'center', 'right', 'left'\"):\n        text.set_horizontalalignment('invalid')\n", "def test_set_rotation_mode():\n    text = Text(0.5, 0.5, 'test', rotation_mode='anchor')\n    assert text.get_rotation_mode() == 'anchor'\n    text.set_rotation_mode('default')\n    assert text.get_rotation_mode() == 'default'\n    with pytest.raises(ValueError):\n        text.set_rotation_mode('invalid_mode')\n"], "sample_568": ["def test_rotate_axes():\n    xs = [1, 2, 3]\n    ys = [4, 5, 6]\n    zs = [7, 8, 9]\n    \n    # Test rotation with 'x'\n    rotated_x = art3d.rotate_axes(xs, ys, zs, 'x')\n    assert rotated_x == (ys, zs, xs)\n    \n    # Test rotation with '-y'\n    rotated_neg_y = art3d.rotate_axes(xs, ys, zs, '-y')\n    assert rotated_neg_y == (ys, zs, xs)\n    \n    # Test rotation with '-x'\n    rotated_neg_x = art3d.rotate_axes(xs, ys, zs, '-x')\n    assert rotated_neg_x == (zs, xs, ys)\n    \n    # Test rotation with 'y'\n    rotated_y = art3d.rotate_axes(xs, ys, zs, 'y')\n    assert rotated_y == (zs, xs, ys)\n    \n    # Test rotation with 'z' (default)\n    rotated_z = art3d.rotate_axes(xs, ys, zs, 'z')\n    assert rotated_z == (xs, ys, zs)\n", "def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array([1, 2, 3]))\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector('invalid')\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector([1, 2])\n", "def test_get_dir_vector():\n    # Test for standard direction inputs\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array((1, 0, 0)))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array((0, 1, 0)))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array((0, 0, 1)))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array((0, 0, 0)))\n\n    # Test for custom vector input\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array((1, 2, 3)))\n\n    # Test for invalid input\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector('invalid')\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        art3d.get_dir_vector((1, 2))  # vector not of length 3\n", "def test_text3d_invalid_zdir():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    with pytest.raises(ValueError, match=\"'x', 'y', 'z', None or vector of length 3 expected\"):\n        text = art3d.Text3D(0, 0, 0, 'Invalid zdir', zdir='invalid')\n        ax.add_artist(text)\n", "def test_get_dir_vector():\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector((1, 2, 3)), np.array([1, 2, 3]))\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector('a')\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector([1, 2])\n"], "sample_569": ["def test_establish_variables_with_logical_operations(self):\n    df = pd.DataFrame({\n        'a': [1, 2, 3, 4],\n        'b': [True, False, True, False],\n        'c': [0.5, 2.5, 1.5, 3.5],\n    })\n\n    p = lm._LinearPlotter()\n    p.establish_variables(df, x='b', y='c')\n    pdt.assert_series_equal(p.x, df.b)\n    pdt.assert_series_equal(p.y, df.c)\n    pdt.assert_frame_equal(p.data, df)\n", "def test_regplot_logx(self):\n\n    f, ax = plt.subplots()\n    lm.regplot(x=\"x\", y=\"y\", data=self.df, logx=True)\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 2\n\n    x_data = ax.lines[0].get_xdata()\n    assert np.all(x_data > 0)\n\n    expected_x = np.log(self.df.x[self.df.x > 0])\n    npt.assert_array_almost_equal(x_data, expected_x)\n", "def test_regressionplotter_scatter_and_lineplot():\n\n    f, ax = plt.subplots()\n\n    # Test scatterplot\n    p = lm._RegressionPlotter(x=\"x\", y=\"y\", data=TestRegressionPlotter.df, scatter=True, fit_reg=False)\n    p.plot(ax, scatter_kws={\"color\": \"blue\"}, line_kws={})\n    assert len(ax.collections) == 1  # One scatterplot collection\n    assert ax.collections[0].get_edgecolor()[0][:3] == mpl.colors.to_rgb(\"blue\")\n\n    ax.clear()\n\n    # Test lineplot\n    p = lm._RegressionPlotter(x=\"x\", y=\"y\", data=TestRegressionPlotter.df, scatter=False, fit_reg=True)\n    p.plot(ax, scatter_kws={}, line_kws={\"color\": \"red\"})\n    assert len(ax.lines) == 1  # One regression line\n    assert ax.lines[0].get_color() == \"red\"\n", "def test_establish_variables_from_numpy_scalar():\n    p = lm._LinearPlotter()\n    p.establish_variables(None, x=np.array(5), y=np.array(10))\n    npt.assert_array_equal(p.x, np.array([5]))\n    npt.assert_array_equal(p.y, np.array([10]))\n    assert p.data is None\n", "    def test_plot_estimate_data_with_ci(self):\n        p = lm._RegressionPlotter(self.df.d, self.df.y, x_estimator=np.mean, ci=95)\n        fig, ax = plt.subplots()\n        p.plot(ax, scatter_kws={}, line_kws={})\n\n        # Check scatter plot data\n        scatter_offsets = ax.collections[0].get_offsets()\n        npt.assert_array_equal(scatter_offsets[:, 0], p.scatter_data[0])\n        npt.assert_array_equal(scatter_offsets[:, 1], p.scatter_data[1])\n\n        # Check line plot data\n        if p.fit_reg:\n            line_data = ax.lines[0].get_xydata()\n            npt.assert_array_equal(line_data[:, 0], p.fit_regression()[0])\n            npt.assert_array_almost_equal(line_data[:, 1], p.fit_regression()[1])\n\n        # Check error bands\n        if p.fit_reg and p.ci:\n            error_band = ax.collections[-1].get_paths()[0].vertices\n            grid, _, err_bands = p.fit_regression()\n            npt.assert_array_almost_equal(error_band[:, 0], np.concatenate([grid, grid[::-1]]))\n            npt.assert_array_almost_equal(error_band[:, 1], np.concatenate([err_bands[0], err_bands[1][::-1]]))\n"], "sample_570": ["def test_clip_values(self, rng):\n    x = rng.normal(0, 3, 100)\n    clip = (-1, 1)\n    kde = KDE(clip=clip)\n    density, support = kde(x)\n    assert np.all(support >= clip[0])\n    assert np.all(support <= clip[1])\n", "def test_histogram_with_weights(self, x, weights):\n\n    h = Histogram()\n    heights, edges = h(x, weights=weights)\n    heights_mpl, edges_mpl = np.histogram(x, bins=\"auto\", weights=weights)\n\n    assert_array_equal(heights, heights_mpl)\n    assert_array_equal(edges, edges_mpl)\n", "def test_histogram_with_weights(self, x, weights):\n    h = Histogram()\n    heights, edges = h(x, weights=weights)\n    heights_manual, edges_manual = np.histogram(x, bins=\"auto\", weights=weights)\n\n    assert_array_equal(heights, heights_manual)\n    assert_array_equal(edges, edges_manual)\n", "def test_histogram_with_weights(self, x, weights):\n    h = Histogram()\n    heights, edges = h(x, weights=weights)\n    \n    heights_mpl, edges_mpl = np.histogram(x, bins=\"auto\", weights=weights)\n    \n    assert_array_equal(heights, heights_mpl)\n    assert_array_equal(edges, edges_mpl)\n", "def test_invalid_bivariate_kde(self, rng):\n    x = rng.normal(0, 3, 100)\n    with pytest.raises(ValueError, match=\"`x2` must be provided for bivariate KDE\"):\n        kde = KDE()\n        kde(x, None)\n"], "sample_571": ["def test_plot_with_multiple_confounds():\n\n    x = np.random.randn(100)\n    y = 2 * x + np.random.randn(100)\n    z = 3 * x + np.random.randn(100)\n    w = 4 * x + np.random.randn(100)\n\n    p = lm._RegressionPlotter(x, y, x_partial=z, y_partial=w)\n    assert p.x.shape == (100,)\n    assert p.y.shape == (100,)\n\n    confounded_x = p.regress_out(x, z)\n    confounded_y = p.regress_out(y, w)\n\n    npt.assert_array_equal(p.x, confounded_x)\n    npt.assert_array_equal(p.y, confounded_y)\n", "def test_establish_variables_from_dict(self):\n\n    p = lm._LinearPlotter()\n    p.establish_variables(None,\n                          x=dict(data=self.df, col=\"x\"),\n                          y=dict(data=self.df, col=\"y\"))\n    npt.assert_array_equal(p.x, self.df.x)\n    npt.assert_array_equal(p.y, self.df.y)\n    assert p.data is None\n", "def test_regplot_logx(self):\n\n    f, ax = plt.subplots()\n    x = np.arange(1, 100)\n    y = np.log(x) + np.random.normal(0, 0.1, size=x.shape)\n    ax = lm.regplot(x=x, y=y, logx=True)\n    \n    line = ax.lines[0]\n    assert np.all(np.log(line.get_xdata()) == line.get_xdata())\n    assert line.get_xdata()[0] > 0\n", "def test_lmplot_custom_palette(self):\n    custom_palette = {\"a\": \"red\", \"b\": \"blue\", \"c\": \"green\"}\n    g = lm.lmplot(x=\"x\", y=\"y\", hue=\"g\", data=self.df, palette=custom_palette)\n    ax = g.axes[0, 0]\n\n    expected_colors = [custom_palette[hue] for hue in sorted(self.df[\"g\"].unique())]\n    actual_colors = [line.get_color() for line in ax.lines]\n\n    assert expected_colors == actual_colors\n", "def test_plot_scatter_and_line(self):\n    f, ax = plt.subplots()\n    scatter_kws = {'color': 'blue', 's': 50, 'alpha': 0.6}\n    line_kws = {'color': 'red', 'lw': 2.5}\n    p = lm._RegressionPlotter(\"x\", \"y\", data=self.df)\n    p.plot(ax, scatter_kws=scatter_kws, line_kws=line_kws)\n    \n    # Check scatter plot properties\n    scatter = ax.collections[0]\n    npt.assert_array_equal(scatter.get_facecolors()[:, :3], mpl.colors.to_rgba('blue')[:3])\n    npt.assert_array_equal(scatter.get_facecolors()[:, 3], [0.6])\n    assert scatter.get_sizes() == [50]\n    \n    # Check line plot properties\n    line = ax.lines[0]\n    assert line.get_color() == 'red'\n    assert line.get_linewidth() == 2.5\n"], "sample_572": ["    def test_bivariate_clip(self, rng):\n\n        x, y = rng.normal(0, 3, (2, 100))\n        clip = (-1, 1), (-2, 2)\n        kde = KDE(clip=clip, gridsize=100)\n        _, (xx, yy) = kde(x, y)\n\n        assert xx.min() >= clip[0][0]\n        assert xx.max() <= clip[0][1]\n        assert yy.min() >= clip[1][0]\n        assert yy.max() <= clip[1][1]\n", "def test_bivariate_discrete_bins(self, rng):\n\n    x = rng.binomial(20, .5, 100)\n    y = rng.binomial(30, .3, 100)\n    h = Histogram(discrete=True)\n    bin_kws = h.define_bin_params(x, y)\n    assert bin_kws[\"bins\"][0].min() == x.min() - .5\n    assert bin_kws[\"bins\"][0].max() == x.max() + .5\n    assert bin_kws[\"bins\"][1].min() == y.min() - .5\n    assert bin_kws[\"bins\"][1].max() == y.max() + .5\n", "def test_ecdf_complementary(self, x):\n\n        ecdf = ECDF(complementary=True)\n        stat, vals = ecdf(x)\n        assert_array_equal(vals[1:], np.sort(x))\n        expected_stats = np.linspace(1, 0, len(x) + 1)[::-1][:-1]\n        assert_array_almost_equal(stat[1:], expected_stats)\n        assert stat[0] == expected_stats[-1]\n", "def test_histogram_invalid_bins(self, x):\n    with pytest.raises(TypeError):\n        Histogram(bins={\"invalid\": \"type\"}).define_bin_params(x)\n    with pytest.raises(ValueError):\n        Histogram(bins=\"invalid_bins_rule\").define_bin_params(x)\n", "    def test_invalid_weights_length(self, x):\n\n        ecdf = ECDF()\n\n        with pytest.raises(ValueError, match=\"Weights array must be the same length as the data array\"):\n            invalid_weights = np.ones(len(x) + 1)  # Invalid weights length\n            ecdf(x, weights=invalid_weights)\n"], "sample_573": ["def test_varying_orders(self, df):\n\n    groupby = GroupBy([\"group\"])\n    orders = [1, 2, 3]\n    for order in orders:\n        res = PolyFit(order=order, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        assert_array_equal(res.columns, [\"x\", \"y\"])\n\n        grid = np.linspace(df[\"x\"].min(), df[\"x\"].max(), 100)\n        assert_array_equal(res[\"x\"], grid)\n        # Check that the second derivative of the polynomial fit is close to zero\n        assert_array_almost_equal(\n            res[\"y\"].diff().diff().dropna(), np.zeros(grid.size - 2)\n        )\n", "def test_high_order_polynomial(self, df):\n    \n    groupby = GroupBy([\"group\"])\n    high_order = 10\n    res = PolyFit(order=high_order, gridsize=100)(df, groupby, \"x\", {})\n    \n    assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n    \n    ngroups = df[\"group\"].nunique()\n    assert_array_equal(res.index, np.arange(ngroups * 100))\n    \n    for _, part in res.groupby(\"group\"):\n        grid = np.linspace(part[\"x\"].min(), part[\"x\"].max(), 100)\n        assert_array_equal(part[\"x\"], grid)\n        assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n", "def test_order_parameter(self, df):\n\n    groupby = GroupBy([\"group\"])\n    res = PolyFit(order=3, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n    assert_array_equal(res.columns, [\"x\", \"y\"])\n\n    grid = np.linspace(df[\"x\"].min(), df[\"x\"].max(), 100)\n    assert_array_equal(res[\"x\"], grid)\n    assert_array_almost_equal(\n        res[\"y\"].diff().diff().diff().dropna(), np.zeros(grid.size - 3)\n    )\n", "def test_multiple_groupers(self, df):\n\n    groupby = GroupBy([\"group\", \"color\"])\n    gridsize = 30\n    res = PolyFit(gridsize=gridsize)(df, groupby, \"x\", {})\n\n    assert res.columns.to_list() == [\"x\", \"y\", \"group\", \"color\"]\n\n    unique_combinations = df[[\"group\", \"color\"]].drop_duplicates().shape[0]\n    assert_array_equal(res.index, np.arange(unique_combinations * gridsize))\n\n    for _, part in res.groupby([\"group\", \"color\"]):\n        grid = np.linspace(part[\"x\"].min(), part[\"x\"].max(), gridsize)\n        assert_array_equal(part[\"x\"], grid)\n        assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n", "def test_different_polynomial_orders(self, df):\n\n    groupby = GroupBy([\"group\"])\n\n    for order in [1, 2, 3, 4]:\n        polyfit = PolyFit(order=order, gridsize=100)\n        res = polyfit(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n\n        ngroups = df[\"group\"].nunique()\n        assert_array_equal(res.index, np.arange(ngroups * 100))\n\n        for _, part in res.groupby(\"group\"):\n            grid = np.linspace(part[\"x\"].min(), part[\"x\"].max(), 100)\n            assert_array_equal(part[\"x\"], grid)\n            assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n"], "sample_574": ["def test_label_concise_with_custom_locator(self, t, x):\n\n    ax = mpl.figure.Figure().subplots()\n    locator = mpl.dates.MonthLocator()\n    Temporal().tick(locator=locator).label(concise=True)._setup(t, Coordinate(), ax.xaxis)\n    formatter = ax.xaxis.get_major_formatter()\n    assert isinstance(formatter, mpl.dates.ConciseDateFormatter)\n    assert ax.xaxis.get_major_locator() is locator\n", "    def test_set_view_interval(self):\n        scale = Continuous()._identity()\n        axis = PseudoAxis(scale)\n        axis.set_view_interval(0, 10)\n        assert axis.get_view_interval() == (0, 10)\n", "def test_label_concise_formatting(self, t, x):\n    ax = mpl.figure.Figure().subplots()\n    Temporal().label(concise=True)._setup(t, Coordinate(), ax.xaxis)\n    formatter = ax.xaxis.get_major_formatter()\n    assert isinstance(formatter, mpl.dates.ConciseDateFormatter)\n    # Check that the formatted labels are concise\n    labels = [formatter(x) for x in ax.xaxis.get_majorticklocs()]\n    for label in labels:\n        assert re.match(r'^[\\d]{2}-\\w{3}$', label) or re.match(r'^\\w{3} \\d{4}$', label)\n", "    def x(self):\n        return pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"x\")\n", "    def test_tick_locator_input_check(self, t):\n\n        err = \"Tick locator must be an instance of .*?, not <class 'str'>.\"\n        with pytest.raises(TypeError, match=err):\n            Temporal().tick(locator=\"invalid_locator\")\n"], "sample_575": ["    def test_temporal_with_custom_order(self, t, x):\n        order = [pd.Timestamp(\"1975-06-24\"), pd.Timestamp(\"1980-12-14\"), pd.Timestamp(\"1972-09-27\")]\n        s = Temporal(order=order)._setup(t, Coordinate())\n        expected = [1, 0, 2]\n        assert_array_equal(s(t), expected)\n", "def test_nominal_tick_locator(self, x):\n    locator = mpl.ticker.FixedLocator([0.1, 0.3, 0.5])\n    s = Nominal().tick(locator=locator)._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    assert_array_equal(a.major.locator(), [0.1, 0.3, 0.5])\n", "    def test_continuous_logit_transform(self, x):\n        s = Continuous(trans=\"logit\")._setup(x, Coordinate())\n        expected = np.log(x / (1 - x))\n        assert_series_equal(s(x), expected)\n", "def test_identity_scale(self):\n    class MockProperty:\n            return lambda x: x\n\n    x = pd.Series([1, 2, 3], name=\"x\")\n    s = Scale._identity()._setup(x, MockProperty())\n    assert_array_equal(s(x), [1, 2, 3])\n", "    def test_nominal_tick_locator(self, x):\n        locs = [0, 2]\n        locator = mpl.ticker.FixedLocator(locs)\n        s = Nominal().tick(locator=locator)._setup(x, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        assert_array_equal(a.major.locator(), locs)\n"], "sample_576": ["    def test_clone_plot(self, long_df):\n        # Create an initial plot\n        p1 = Plot(long_df, x=\"x\", y=\"y\").add(MockMark()).facet(\"a\").pair(x=[\"x\", \"z\"]).scale(x=\"log\").label(title=\"Original\")\n        \n        # Clone the plot\n        p2 = p1._clone()\n        \n        # Ensure the cloned plot is not the same object\n        assert p1 is not p2\n        \n        # Ensure the cloned plot has the same data and properties\n        assert_frame_equal(p1._data.frame, p2._data.frame)\n        assert p1._layers == p2._layers\n        assert p1._facet_spec == p2._facet_spec\n        assert p1._pair_spec == p2._pair_spec\n        assert p1._scales == p2._scales\n        assert p1._labels == p2._labels\n        \n        # Modify the cloned plot and ensure it does not affect the original plot\n        p2.add(MockMark(), color=\"b\").label(title=\"Clone\")\n        assert len(p1._layers) != len(p2._layers)\n        assert p1._labels[\"title\"] == \"Original\"\n        assert p2._labels[\"title\"] == \"Clone\"\n", "    def test_clone_data_integrity(self, long_df):\n\n        p1 = Plot(long_df, x=\"a\", y=\"b\")\n        p2 = p1._clone()\n\n        assert p1._data.frame.equals(p2._data.frame)\n        assert p1._layers == p2._layers\n        assert p1._scales == p2._scales\n        assert p1._shares == p2._shares\n        assert p1._limits == p2._limits\n        assert p1._labels == p2._labels\n        assert p1._theme == p2._theme\n        assert p1._facet_spec == p2._facet_spec\n        assert p1._pair_spec == p2._pair_spec\n        assert p1._figure_spec == p2._figure_spec\n        assert p1._subplot_spec == p2._subplot_spec\n        assert p1._layout_spec == p2._layout_spec\n        assert p1._target == p2._target\n", "def test_theme_context_manager():\n\n    # Setup\n    test_params = {\"axes.facecolor\": \"red\", \"axes.edgecolor\": \"blue\"}\n    original_params = {k: mpl.rcParams[k] for k in test_params}\n\n    # Enter the context manager\n    with theme_context(test_params):\n        assert mpl.rcParams[\"axes.facecolor\"] == \"red\"\n        assert mpl.rcParams[\"axes.edgecolor\"] == \"blue\"\n\n    # After exiting the context manager, the rcParams should be restored to their original values\n    assert mpl.rcParams[\"axes.facecolor\"] == original_params[\"axes.facecolor\"]\n    assert mpl.rcParams[\"axes.edgecolor\"] == original_params[\"axes.edgecolor\"]\n", "    def test_clone_preserves_data(self, long_df):\n        p = Plot(long_df, x=\"x\", y=\"y\")\n        p_clone = p._clone()\n        assert_frame_equal(p._data.frame, p_clone._data.frame)\n        assert p._data.source_data is p_clone._data.source_data\n", "    def test_plot_signature_has_args_and_properties(self):\n        class MockPlot:\n            pass\n\n        plot_class = build_plot_signature(MockPlot)\n        sig = inspect.signature(plot_class)\n        param_names = [param.name for param in sig.parameters.values()]\n\n        assert \"args\" in param_names\n        assert \"data\" in param_names\n        for prop in PROPERTIES:\n            assert prop in param_names\n"], "sample_577": ["    def test_theme_context_changes(self):\n        initial_params = {k: mpl.rcParams[k] for k in ['axes.facecolor', 'lines.linewidth']}\n        new_params = {'axes.facecolor': 'red', 'lines.linewidth': 2.5}\n\n        with theme_context(new_params):\n            assert mpl.rcParams['axes.facecolor'] == 'red'\n            assert mpl.rcParams['lines.linewidth'] == 2.5\n\n        assert mpl.rcParams['axes.facecolor'] == initial_params['axes.facecolor']\n        assert mpl.rcParams['lines.linewidth'] == initial_params['lines.linewidth']\n", "    def test_theme_context_modifies_params(self):\n        original_params = mpl.rcParams['axes.facecolor']\n        custom_params = {'axes.facecolor': 'r'}\n        with theme_context(custom_params):\n            assert mpl.rcParams['axes.facecolor'] == 'r'\n        assert mpl.rcParams['axes.facecolor'] == original_params\n", "    def test_theme_context_with_valid_params(self):\n        params = {\"axes.facecolor\": \".888\"}\n        with theme_context(params):\n            assert mpl.rcParams[\"axes.facecolor\"] == \".888\"\n\n        assert mpl.rcParams[\"axes.facecolor\"] != \".888\"\n", "    def test_theme_context(self):\n        params = {\"axes.facecolor\": \"r\", \"axes.grid\": False}\n        orig_facecolor = mpl.rcParams[\"axes.facecolor\"]\n        orig_grid = mpl.rcParams[\"axes.grid\"]\n\n        with theme_context(params):\n            assert mpl.rcParams[\"axes.facecolor\"] == \"r\"\n            assert mpl.rcParams[\"axes.grid\"] is False\n\n        assert mpl.rcParams[\"axes.facecolor\"] == orig_facecolor\n        assert mpl.rcParams[\"axes.grid\"] == orig_grid\n", "    def test_add_mark_with_stat_and_move(self, long_df):\n        class CustomStat(Stat):\n                data[\"new_y\"] = data[\"y\"] * 2\n                return data\n\n        class CustomMove(Move):\n                data[\"new_y\"] = data[\"new_y\"] + 1\n                return data\n\n        m = MockMark()\n        s = CustomStat()\n        mv = CustomMove()\n        p = Plot(long_df, x=\"x\", y=\"y\").add(m, s, mv).plot()\n\n        expected = (long_df[\"y\"] * 2) + 1\n        assert_vector_equal(m.passed_data[0][\"new_y\"], expected)\n"], "sample_578": ["    def test_legend_artist(self):\n\n        mark = Bar(color=\"C0\", edgecolor=\"C1\", edgewidth=2, edgestyle=\"--\")\n        scales = {}\n        artist = mark._legend_artist(variables=[\"color\", \"edgecolor\"], value=None, scales=scales)\n\n        assert isinstance(artist, mpl.patches.Patch)\n        assert artist.get_facecolor() == to_rgba(\"C0\")\n        assert artist.get_edgecolor() == to_rgba(\"C1\")\n        assert artist.get_linewidth() == 2\n        assert artist.get_linestyle() == \"--\"\n", "    def test_edgealpha_effect(self):\n\n        x = [\"a\", \"b\", \"c\"]\n        y = [1, 2, 3]\n        \n        mark = Bar(\n            edgecolor=\"blue\",\n            edgealpha=.5,\n            edgewidth=2,\n        )\n\n        p = Plot(x, y).add(mark).plot()\n        ax = p._figure.axes[0]\n        for bar in ax.patches:\n            expected_edgecolor = to_rgba(\"blue\", mark.edgealpha)\n            assert bar.get_edgecolor() == expected_edgecolor\n            # Confirm edgealpha correctly affects edgecolor alpha channel\n            assert bar.get_edgecolor()[3] == pytest.approx(mark.edgealpha)\n            assert bar.get_linewidth() == mark.edgewidth * 2\n", "    def test_baseline(self):\n\n        x = [\"a\", \"b\", \"c\"]\n        y = [5, 7, 3]\n        baseline = 2\n\n        p = Plot(x, y).add(Bar(baseline=baseline)).plot()\n        ax = p._figure.axes[0]\n        for i, bar in enumerate(ax.patches):\n            self.check_bar(bar, i - 0.4, baseline, 0.8, y[i] - baseline)\n", "    def test_baseline_property(self):\n\n        x = [\"a\", \"b\", \"c\"]\n        y = [3, 5, 7]\n        baseline = 2\n\n        p = Plot(x, y).add(Bar(baseline=baseline)).plot()\n        ax = p._figure.axes[0]\n        for i, bar in enumerate(ax.patches):\n            assert bar.get_y() == pytest.approx(baseline if bar.get_height() > 0 else 0)\n            assert bar.get_height() == pytest.approx(y[i] - baseline)\n", "    def test_legend_artist(self):\n        \n        bar = Bar()\n        scales = {}\n        artist = bar._legend_artist([\"color\"], \"C0\", scales)\n        assert isinstance(artist, mpl.patches.Patch)\n        assert artist.get_facecolor() == to_rgba(\"C0\")\n        assert artist.get_edgecolor() == to_rgba(\"C0\")\n        assert artist.get_linewidth() == mpl.rcParams[\"patch.linewidth\"]\n        assert artist.get_linestyle() == (0, ())\n"], "sample_579": ["def test_heatmap_custom_annotation_data():\n    data = np.array([[1, 2], [3, 4]])\n    annot_data = np.array([['A', 'B'], ['C', 'D']])\n    ax = mat.heatmap(data, annot=annot_data, fmt=\"\")\n    \n    for val, text in zip(annot_data.flat, ax.texts):\n        assert text.get_text() == val\n", "def test_heatmap_square_aspect_ratio():\n    kws = self.default_kws.copy()\n    kws[\"square\"] = True\n    p = mat._HeatMapper(self.df_norm, **kws)\n\n    f, ax = plt.subplots()\n    p.plot(ax, cax=None, kws={})\n    assert ax.get_aspect() == \"equal\" or ax.get_aspect() == 1\n\n    plt.close(f)\n", "def test_heatmap_with_custom_colormap_list(self):\n    \"\"\"Test heatmap with a custom list of colors as colormap\"\"\"\n    custom_cmap = [\"#FF0000\", \"#00FF00\", \"#0000FF\"]\n    kws = self.default_kws.copy()\n    kws[\"cmap\"] = custom_cmap\n    p = mat._HeatMapper(self.df_norm, **kws)\n\n    assert isinstance(p.cmap, mpl.colors.ListedColormap)\n    assert p.cmap.colors == list(map(mpl.colors.to_rgb, custom_cmap))\n\n    ax = mat.heatmap(self.df_norm, cmap=custom_cmap)\n    fc = ax.collections[0].get_facecolors()\n    cvals = np.linspace(0, 1, len(custom_cmap))\n    npt.assert_array_almost_equal(fc, p.cmap(cvals), 2)\n", "def test_heatmap_custom_annotation_data():\n    annot_data = np.full(self.df_norm.shape, \"A\")\n    ax = mat.heatmap(self.df_norm, annot=annot_data, fmt=\"\")\n    for text in ax.texts:\n        assert text.get_text() == \"A\"\n\n    annot_data = pd.DataFrame(annot_data, index=self.df_norm.index, columns=self.df_norm.columns)\n    ax = mat.heatmap(self.df_norm, annot=annot_data, fmt=\"\")\n    for text in ax.texts:\n        assert text.get_text() == \"A\"\n", "def test_heatmap_custom_annot_data():\n    annot_data = np.array([[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"], \n                           [\"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\"], \n                           [\"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\"], \n                           [\"Y\", \"Z\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]])\n    ax = mat.heatmap(TestHeatmap.df_norm, annot=annot_data, fmt=\"s\")\n    for val, text in zip(annot_data.flat, ax.texts):\n        assert text.get_text() == f\"{val}\"\n"], "sample_580": ["def test_variable_type_with_all_na():\n\n    s = pd.Series([np.nan, np.nan, np.nan])\n    assert variable_type(s) == \"numeric\"\n\n    s = pd.Series([pd.NaT, pd.NaT])\n    assert variable_type(s) == \"numeric\"\n\n    s = pd.Series([None, None, None])\n    assert variable_type(s) == \"numeric\"\n\n    s = pd.Series([None, np.nan, pd.NaT])\n    assert variable_type(s) == \"numeric\"\n", "def test_variable_type_all_na():\n\n    s = pd.Series([np.nan, np.nan, np.nan])\n    assert variable_type(s) == \"numeric\"\n\n    s = pd.Series([pd.NA, pd.NA, pd.NA])\n    assert variable_type(s) == \"numeric\"\n\n    s = pd.Series([None, None, None])\n    assert variable_type(s) == \"numeric\"\n\n    s = pd.Series([float('nan'), float('nan'), float('nan')])\n    assert variable_type(s) == \"numeric\"\n", "def test_variable_type_with_mixed_types():\n\n    s = pd.Series([1, \"a\", pd.Timestamp(\"20230101\")])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, 2, \"a\", np.nan])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([pd.Timestamp(\"20230101\"), pd.Timestamp(\"20230102\"), \"a\"])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([pd.Timestamp(\"20230101\"), pd.Timestamp(\"20230102\"), np.nan])\n    assert variable_type(s) == \"datetime\"\n\n    s = pd.Series([1, 2, datetime(2023, 1, 1)])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([datetime(2023, 1, 1), datetime(2023, 1, 2), np.nan])\n    assert variable_type(s) == \"datetime\"\n", "def test_mixed_type_data():\n\n    s = pd.Series([1, \"a\", 3.0])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, datetime(2020, 1, 1), 3.0])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([datetime(2020, 1, 1), \"a\", datetime(2020, 1, 2)])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, 2, datetime(2020, 1, 1)])\n    assert variable_type(s) == \"categorical\"\n", "def test_variable_type_with_mixed_data():\n\n    s = pd.Series([1, \"a\", 3.5])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, \"a\", pd.Timestamp(3)])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, pd.Timestamp(2), pd.Timestamp(3)])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1.5, 2.5, pd.Timestamp(3)])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([True, 1, 0])\n    assert variable_type(s) == \"categorical\"\n"], "sample_581": ["def test_blueprint_add_url_rule(app, client):\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/existing\")\n        return \"existing\"\n\n    bp.add_url_rule(\"/new\", endpoint=\"new_endpoint\", view_func=lambda: \"new\")\n    \n    app.register_blueprint(bp)\n\n    assert client.get(\"/existing\").data == b\"existing\"\n    assert client.get(\"/new\").data == b\"new\"\n", "def test_blueprint_add_url_rule(app, client):\n    bp = flask.Blueprint(\"bp\", __name__, url_prefix=\"/bp\")\n\n    @bp.route(\"/hello\")\n        return \"Hello, World!\"\n\n    bp.add_url_rule(\"/goodbye\", endpoint=\"goodbye\", view_func=lambda: \"Goodbye, World!\")\n\n    app.register_blueprint(bp)\n\n    assert client.get(\"/bp/hello\").data == b\"Hello, World!\"\n    assert client.get(\"/bp/goodbye\").data == b\"Goodbye, World!\"\n", "def test_blueprint_static_route(app, client):\n    bp = flask.Blueprint(\"bp\", __name__, static_folder=\"static\", static_url_path=\"/static_bp\")\n\n    # Create a temporary static file to serve\n    static_file_content = b\"Test static content\"\n    static_file_path = os.path.join(bp.static_folder, \"test.txt\")\n    os.makedirs(bp.static_folder, exist_ok=True)\n    with open(static_file_path, \"wb\") as f:\n        f.write(static_file_content)\n\n    app.register_blueprint(bp)\n\n    rv = client.get(\"/static_bp/test.txt\")\n    assert rv.data == static_file_content\n    assert rv.status_code == 200\n\n    # Clean up the temporary file\n    os.remove(static_file_path)\n", "def test_blueprint_static_files(app, client):\n    bp = flask.Blueprint(\"bp\", __name__, static_folder=\"static\", static_url_path=\"/static_bp\")\n\n    app.register_blueprint(bp)\n\n    @app.route(\"/\")\n        return \"Main Page\"\n\n    rv = client.get(\"/static_bp/test.txt\")\n    assert rv.data.strip() == b\"Blueprint File\"\n\n    with app.test_request_context():\n        assert flask.url_for(\"bp.static\", filename=\"test.txt\") == \"/static_bp/test.txt\"\n", "def test_add_url_rule(app, client):\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/foo\")\n        return \"foo\"\n\n    bp.add_url_rule(\"/bar\", view_func=lambda: \"bar\")\n    \n    app.register_blueprint(bp, url_prefix=\"/py\")\n\n    assert client.get(\"/py/foo\").data == b\"foo\"\n    assert client.get(\"/py/bar\").data == b\"bar\"\n"], "sample_582": ["def test_find_app_by_string():\n    class Module:\n        app = Flask(\"appname\")\n        create_app = lambda: Flask(\"created_app\")\n        make_app = lambda: Flask(\"made_app\")\n        create_app_with_args = lambda x: Flask(f\"app_{x}\")\n\n    assert find_app_by_string(Module, \"app\").name == \"appname\"\n    assert find_app_by_string(Module, \"create_app\").name == \"created_app\"\n    assert find_app_by_string(Module, \"make_app\").name == \"made_app\"\n    assert find_app_by_string(Module, 'create_app_with_args(\"test\")').name == \"app_test\"\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"non_existent\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, 'create_app_with_args(\"test\", \"extra\")')\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"create_app_with_args(\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"create_app_with_invalid_args(x=1)\")\n", "def test_find_app_by_string():\n    from flask import Flask\n\n    class Module:\n        app = Flask(\"appname\")\n            return Flask(\"created_app\")\n            return Flask(\"made_app\")\n\n    # Test finding app by attribute name\n    assert find_app_by_string(Module, \"app\").name == \"appname\"\n\n    # Test finding app by function call\n    assert find_app_by_string(Module, \"create_app\").name == \"created_app\"\n    assert find_app_by_string(Module, \"make_app\").name == \"made_app\"\n    \n    # Test finding app by function call with arguments\n    class ModuleWithArgs:\n        @staticmethod\n            assert arg1 == \"foo\"\n            assert arg2 == \"bar\"\n            return Flask(\"created_app_with_args\")\n\n    assert find_app_by_string(ModuleWithArgs, 'create_app(\"foo\", \"bar\")').name == \"created_app_with_args\"\n\n    # Test finding app by invalid attribute name\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"invalid_app\")\n\n    # Test finding app by invalid function call\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"invalid_function()\")\n\n    # Test finding app by invalid syntax\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"create_app(\")\n", "def test_find_app_by_string():\n    class Module:\n        app = Flask(\"testapp\")\n        create_app = lambda: Flask(\"factoryapp\")\n        app_with_args = lambda x: Flask(f\"app{x}\")\n\n    assert find_app_by_string(Module, \"app\").name == \"testapp\"\n    assert find_app_by_string(Module, \"create_app\").name == \"factoryapp\"\n    assert find_app_by_string(Module, \"app_with_args(1)\").name == \"app1\"\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"nonexistent\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"app_with_invalid_args(1, 2)\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"invalid_syntax(\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"create_app_with_args(x=1)\")\n", "def test_find_app_by_string():\n    class Module:\n        myapp = Flask(\"testapp\")\n\n        @staticmethod\n            return Flask(\"created_app\")\n\n        @staticmethod\n            return Flask(f\"app_{foo}_{bar}\")\n\n    # Test finding app by attribute name\n    app = find_app_by_string(Module, \"myapp\")\n    assert isinstance(app, Flask)\n    assert app.name == \"testapp\"\n\n    # Test finding app by factory function without args\n    app = find_app_by_string(Module, \"create_app\")\n    assert isinstance(app, Flask)\n    assert app.name == \"created_app\"\n\n    # Test finding app by factory function with args\n    app = find_app_by_string(Module, 'create_app_with_args(\"foo\", \"bar\")')\n    assert isinstance(app, Flask)\n    assert app.name == \"app_foo_bar\"\n\n    # Test invalid attribute name\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"invalid_name\")\n\n    # Test invalid function call\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"create_app_with_args(foo, bar)\")\n", "def test_find_app_by_string_function_call():\n    from flask import Flask\n\n    class Module:\n        @staticmethod\n            return Flask(\"appname\")\n\n    app = find_app_by_string(Module, \"create_app()\")\n    assert isinstance(app, Flask)\n    assert app.name == \"appname\"\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"nonexistent_function()\")\n\n    with pytest.raises(NoAppException):\n        find_app_by_string(Module, \"create_app(invalid_arg)\")\n"], "sample_583": ["def test_expand_slice():\n    size = 10\n    slice_obj = slice(2, 8, 2)\n    expanded = indexing._expand_slice(slice_obj, size)\n    expected = np.array([2, 4, 6])\n    np.testing.assert_array_equal(expanded, expected)\n\n    slice_obj = slice(None, None, -1)\n    expanded = indexing._expand_slice(slice_obj, size)\n    expected = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n    np.testing.assert_array_equal(expanded, expected)\n\n    slice_obj = slice(8, 2, -2)\n    expanded = indexing._expand_slice(slice_obj, size)\n    expected = np.array([8, 6, 4])\n    np.testing.assert_array_equal(expanded, expected)\n", "def test_expanded_indexer_with_ellipsis():\n    x = np.random.randn(5, 6, 7)\n    I = ReturnItem()  # noqa\n    indexers = [I[Ellipsis, 1], I[1, Ellipsis], I[Ellipsis, 2, 3], I[1, Ellipsis, 3], I[Ellipsis]]\n    for i in indexers:\n        j = indexing.expanded_indexer(i, x.ndim)\n        assert_array_equal(x[i], x[j])\n        assert_array_equal(self.set_to_zero(x, i), self.set_to_zero(x, j))\n", "def test_posify_mask_indexer():\n    indexer = indexing.BasicIndexer((np.array([0, -1, 2]), slice(None)))\n    expected = indexing.BasicIndexer((np.array([0, 0, 2]), slice(None)))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert expected.tuple == actual.tuple\n\n    indexer = indexing.OuterIndexer((np.array([0, 1, -1]), np.array([-1, 2, 3])))\n    expected = indexing.OuterIndexer((np.array([0, 1, 1]), np.array([2, 2, 3])))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert expected.tuple == actual.tuple\n\n    indexer = indexing.VectorizedIndexer((np.array([0, -1, 2]), np.array([3, -1, 1])))\n    expected = indexing.VectorizedIndexer((np.array([0, 0, 2]), np.array([3, 3, 1])))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert expected.tuple == actual.tuple\n\n    # Test with empty indexer\n    indexer = indexing.BasicIndexer((slice(None),))\n    expected = indexing.BasicIndexer((slice(None),))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert expected.tuple == actual.tuple\n", "def test_expand_slice():\n    slice_ = slice(1, 5, 2)\n    size = 10\n    expanded = indexing._expand_slice(slice_, size)\n    expected = np.array([1, 3])\n    np.testing.assert_array_equal(expanded, expected)\n\n    slice_ = slice(None, 8, 3)\n    expanded = indexing._expand_slice(slice_, size)\n    expected = np.array([0, 3, 6])\n    np.testing.assert_array_equal(expanded, expected)\n\n    slice_ = slice(2, None, 4)\n    expanded = indexing._expand_slice(slice_, size)\n    expected = np.array([2, 6])\n    np.testing.assert_array_equal(expanded, expected)\n\n    slice_ = slice(None, None, -1)\n    expanded = indexing._expand_slice(slice_, size)\n    expected = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n    np.testing.assert_array_equal(expanded, expected)\n", "def test_expand_slice():\n    size = 10\n    slices = [\n        slice(None),\n        slice(2, 8),\n        slice(1, 9, 2),\n        slice(9, 1, -2),\n        slice(-9, -1),\n        slice(None, None, -1)\n    ]\n    for sl in slices:\n        expected = np.arange(*sl.indices(size))\n        actual = indexing._expand_slice(sl, size)\n        assert_array_equal(expected, actual)\n"], "sample_584": ["    def test_combine_single_element(self):\n        ds = create_test_data(0)\n        input = [ds]\n\n        expected = Dataset({'var1': ds.var1, 'var2': ds.var2})\n        actual = combine_nested(input, concat_dim='dim1')\n        assert_identical(expected, actual)\n", "    def test_concat_with_fill_value(self, create_combined_ids, concat_dim):\n        shape = (2,)\n        combined_ids = create_combined_ids(shape)\n        ds = create_test_data\n        fill_value = -999.0\n        result = _combine_all_along_first_dim(combined_ids, dim=concat_dim,\n                                              data_vars='all',\n                                              coords='different',\n                                              compat='no_conflicts',\n                                              fill_value=fill_value)\n\n        expected_ds = concat([ds(0), ds(1)], dim=concat_dim, fill_value=fill_value)\n        assert_combined_tile_ids_equal(result, {(): expected_ds})\n", "    def test_nested_3d(self):\n        ds = create_test_data\n        input = [[[ds(0)], [ds(1), ds(2)], [ds(3)]],\n                 [[ds(4)], [ds(5), ds(6)], [ds(7)]]]\n\n        expected = {(0, 0, 0): ds(0), (0, 1, 0): ds(1), (0, 1, 1): ds(2),\n                    (0, 2, 0): ds(3), (1, 0, 0): ds(4), (1, 1, 0): ds(5),\n                    (1, 1, 1): ds(6), (1, 2, 0): ds(7)}\n        actual = _infer_concat_order_from_positions(input)\n        assert_combined_tile_ids_equal(expected, actual)\n", "    def test_merge_only(self, create_combined_ids):\n        shape = (1, 2)\n        combined_ids = create_combined_ids(shape)\n        result = _combine_nd(combined_ids, concat_dims=[None, None])\n\n        ds = create_test_data\n        expected_ds = merge([ds(0), ds(1)])\n        expected = merge([expected_ds, merge([ds(2), ds(3)])])\n        assert_equal(result, expected)\n", "    def test_combine_along_first_dim(self):\n        ds0 = Dataset({'x': [0, 1], 'y': [10, 20]})\n        ds1 = Dataset({'x': [2, 3], 'y': [10, 20]})\n\n        combined_ids = OrderedDict({(0,): ds0, (1,): ds1})\n        result = _combine_all_along_first_dim(combined_ids, dim='x',\n                                              data_vars='all',\n                                              coords='different',\n                                              compat='no_conflicts')\n\n        expected = OrderedDict({(): concat([ds0, ds1], dim='x')})\n        assert_combined_tile_ids_equal(result, expected)\n"], "sample_585": ["def test_groupby_fillna():\n    # create test data\n    array = xr.DataArray([1, np.nan, 3, np.nan], [('x', [0, 0, 1, 1])])\n    expected = xr.DataArray([1, 1, 3, 3], [('x', [0, 0, 1, 1])])\n    actual = array.groupby('x').fillna(1)\n    assert_identical(expected, actual)\n\n    dataset = xr.Dataset({'foo': ('x', [1, np.nan, 3, np.nan])}, {'x': [0, 0, 1, 1]})\n    expected_ds = xr.Dataset({'foo': ('x', [1, 1, 3, 3])}, {'x': [0, 0, 1, 1]})\n    actual_ds = dataset.groupby('x').fillna(1)\n    assert_identical(expected_ds, actual_ds)\n", "def test_groupby_fillna():\n    # create a DataArray with missing values\n    array = xr.DataArray([1, np.nan, 3, np.nan, 5, 6],\n                         [('x', [1, 1, 1, 2, 2, 2])])\n    \n    # fill missing values within each group\n    expected = xr.DataArray([1, 1, 3, 5, 5, 6], [('x', [1, 1, 1, 2, 2, 2])])\n    actual = array.groupby('x').fillna(5)\n    assert_identical(expected, actual)\n    \n    # create a Dataset with missing values\n    dataset = xr.Dataset({'foo': ('x', [1, np.nan, 3, np.nan, 5, 6])},\n                         {'x': [1, 1, 1, 2, 2, 2]})\n    \n    # fill missing values within each group\n    expected = xr.Dataset({'foo': ('x', [1, 1, 3, 5, 5, 6])}, \n                          {'x': [1, 1, 1, 2, 2, 2]})\n    actual = dataset.groupby('x').fillna(5)\n    assert_identical(expected, actual)\n", "def test_groupby_fillna():\n    # create test data with NaN values\n    data = xr.DataArray([1, np.nan, 3, 4, np.nan, 6], dims='x')\n    # group by even and odd indices\n    groups = xr.DataArray([0, 1, 0, 1, 0, 1], dims='x', name='groups')\n    g = data.groupby(groups)\n    # fill NaNs within each group with the value 5\n    actual = g.fillna(5)\n    expected = xr.DataArray([1, 5, 3, 4, 5, 6], dims='x')\n    assert_identical(actual, expected)\n", "def test_groupby_fillna():\n\n    array = xr.DataArray([1, np.nan, 3, np.nan, 5, 6],\n                         [('x', [1, 1, 1, 2, 2, 2])])\n\n    # Fill with scalar value\n    expected = xr.DataArray([1, 0, 3, 0, 5, 6], [('x', [1, 2])])\n    actual = array.groupby('x').fillna(0)\n    assert_identical(expected, actual)\n\n    # Fill with DataArray\n    fill_value = xr.DataArray([0, 4], [('x', [1, 2])])\n    expected = xr.DataArray([1, 0, 3, 4, 5, 6], [('x', [1, 2])])\n    actual = array.groupby('x').fillna(fill_value)\n    assert_identical(expected, actual)\n", "def test_groupby_fillna():\n\n    array = xr.DataArray([1, np.nan, 3, np.nan, 5, np.nan],\n                         [('x', [1, 1, 1, 2, 2, 2])])\n\n    # Fill NaNs with scalar\n    expected = xr.DataArray([1, 0, 3, 0, 5, 0], [('x', [1, 2])])\n    actual = array.groupby('x').fillna(0)\n    assert_identical(expected, actual)\n\n    # Fill NaNs with group mean\n    expected = xr.DataArray([1, 2, 3, 5, 5, 5], [('x', [1, 2])])\n    actual = array.groupby('x').fillna(array.groupby('x').mean())\n    assert_identical(expected, actual)\n"], "sample_586": ["def test_concat_new_dimension():\n    # Test concatenation along a new dimension\n    ds1 = Dataset({\"a\": (\"x\", [1, 2, 3])})\n    ds2 = Dataset({\"a\": (\"x\", [4, 5, 6])})\n    ds3 = Dataset({\"a\": (\"x\", [7, 8, 9])})\n\n    expected = Dataset({\"a\": ((\"concat_dim\", \"x\"), [[1, 2, 3], [4, 5, 6], [7, 8, 9]])})\n    actual = concat([ds1, ds2, ds3], dim=\"concat_dim\")\n    assert_identical(expected, actual)\n\n    # Test concatenation along a new dimension with different shapes\n    ds1 = Dataset({\"a\": (\"x\", [1, 2])})\n    ds2 = Dataset({\"a\": (\"x\", [3, 4, 5])})\n    ds3 = Dataset({\"a\": (\"x\", [6, 7, 8, 9])})\n\n    expected = Dataset({\n        \"a\": ((\"concat_dim\", \"x\"), [\n            [1, 2, np.nan, np.nan],\n            [3, 4, 5, np.nan],\n            [6, 7, 8, 9],\n        ])\n    })\n    actual = concat([ds1, ds2, ds3], dim=\"concat_dim\")\n    assert_identical(expected, actual)\n", "def test_concat_positions():\n    data1 = Dataset({\"foo\": (\"x\", [1, 2, 3]), \"bar\": (\"x\", [4, 5, 6])}, coords={\"x\": [0, 1, 2]})\n    data2 = Dataset({\"foo\": (\"x\", [7, 8]), \"bar\": (\"x\", [9, 10])}, coords={\"x\": [3, 4]})\n    positions = [np.array([0, 2]), np.array([1, 3])]\n    actual = concat([data1, data2], dim=\"y\", positions=positions)\n    expected = Dataset(\n        {\n            \"foo\": ((\"y\", \"x\"), [[1, 2, 3], [7, 8, np.nan]]),\n            \"bar\": ((\"y\", \"x\"), [[4, 5, 6], [9, 10, np.nan]]),\n        },\n        coords={\"x\": [0, 1, 2, 3, 4]},\n    )\n    assert_identical(expected, actual)\n\n    with raises_regex(ValueError, \"not all arrays match the length of the corresponding dimension\"):\n        concat([data1, data2], dim=\"y\", positions=[np.array([0, 1]), np.array([2, 3, 4])])\n", "def test_concat_compat_override():\n    ds1 = Dataset(\n        {\n            \"var1\": (\"x\", [1, 2, 3]),\n            \"var2\": (\"y\", [4, 5, 6]),\n        },\n        coords={\"x\": [0, 1, 2], \"y\": [0, 1, 2]},\n        attrs={\"attr\": \"original\"}\n    )\n    ds2 = Dataset(\n        {\n            \"var1\": (\"x\", [7, 8, 9]),\n            \"var2\": (\"y\", [10, 11, 12]),\n        },\n        coords={\"x\": [0, 1, 2], \"y\": [0, 1, 2]},\n        attrs={\"attr\": \"new\"}\n    )\n\n    result = concat([ds1, ds2], dim=\"z\", compat=\"override\")\n    expected = Dataset(\n        {\n            \"var1\": ((\"z\", \"x\"), [[1, 2, 3], [7, 8, 9]]),\n            \"var2\": ((\"z\", \"y\"), [[4, 5, 6], [10, 11, 12]]),\n        },\n        coords={\"x\": [0, 1, 2], \"y\": [0, 1, 2]},\n        attrs={\"attr\": \"original\"}\n    )\n\n    assert_identical(result, expected)\n", "def test_concat_empty_objects():\n    # Test concatenating empty Datasets and DataArrays\n    ds1 = Dataset()\n    ds2 = Dataset()\n    result = concat([ds1, ds2], dim=\"x\")\n    expected = Dataset()\n    assert_identical(result, expected)\n\n    da1 = DataArray()\n    da2 = DataArray()\n    result = concat([da1, da2], dim=\"x\")\n    expected = DataArray()\n    assert_identical(result, expected)\n", "def test_concat_with_new_dimension():\n    data = create_test_data().drop_dims(\"dim3\")\n    split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n    \n    # Concatenate along a new dimension\n    actual = concat(split_data, dim=\"new_dim\")\n    \n    # Ensure the new dimension is added correctly\n    assert \"new_dim\" in actual.dims\n    assert actual.dims[\"new_dim\"] == 2  # because we concatenated two datasets\n    \n    # Ensure all the original dimensions and variables are present\n    assert set(actual.dims) == {\"new_dim\", \"dim1\", \"dim2\"}\n    assert set(actual.data_vars) == set(data.data_vars)\n    assert set(actual.coords) == set(data.coords)\n    \n    # Check if the data is as expected after concatenation\n    for var in data.variables:\n        expected_values = np.array([data[var].values, data[var].values])\n        assert_array_equal(actual[var].values, expected_values)\n\n    # Check if the attributes and encoding are preserved\n    assert actual.attrs == data.attrs\n    assert actual.encoding == data.encoding\n"], "sample_587": ["    def test_unique_variable(self):\n        var1 = xr.Variable(\"x\", [1, 2, 3])\n        var2 = xr.Variable(\"x\", [1, 2, 3])\n        var3 = xr.Variable(\"x\", [1, 2, 4])\n\n        # Test identical variables\n        result = merge.unique_variable(\"var\", [var1, var2], compat=\"identical\")\n        assert result.identical(var1)\n\n        # Test broadcast_equals compatibility\n        result = merge.unique_variable(\"var\", [var1, var2], compat=\"broadcast_equals\")\n        assert result.broadcast_equals(var1)\n\n        # Test minimal compatibility\n        result = merge.unique_variable(\"var\", [var1, var2], compat=\"minimal\")\n        assert result.broadcast_equals(var1)\n\n        # Test no_conflicts compatibility\n        var4 = xr.Variable(\"x\", [1, 2, np.nan])\n        result = merge.unique_variable(\"var\", [var1, var4], compat=\"no_conflicts\")\n        expected = xr.Variable(\"x\", [1, 2, 3])\n        assert result.identical(expected)\n\n        # Test override compatibility\n        result = merge.unique_variable(\"var\", [var1, var3], compat=\"override\")\n        assert result.identical(var1)\n\n        # Test merge error\n        with pytest.raises(merge.MergeError):\n            merge.unique_variable(\"var\", [var1, var3], compat=\"equals\")\n", "def test_merge_identical(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [4, 5, 6])})\n    ds2 = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [4, 5, 6]), \"c\": (\"x\", [7, 8, 9])})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [4, 5, 6]), \"c\": (\"x\", [7, 8, 9])})\n    assert expected.identical(ds1.merge(ds2, compat=\"identical\"))\n    \n    # Test with differing attributes\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [4, 5, 6], {\"attr\": \"value1\"})})\n    ds2 = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [4, 5, 6], {\"attr\": \"value2\"})})\n    with pytest.raises(xr.MergeError):\n        ds1.merge(ds2, compat=\"identical\")\n    \n    # Test with identical attributes\n    ds2 = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [4, 5, 6], {\"attr\": \"value1\"})})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [4, 5, 6], {\"attr\": \"value1\"})})\n    assert expected.identical(ds1.merge(ds2, compat=\"identical\"))\n", "def test_merge_variables(self):\n    data1 = {\"a\": xr.Variable(\"x\", [1, 2, 3]), \"b\": xr.Variable(\"y\", [4, 5, 6])}\n    data2 = {\"a\": xr.Variable(\"x\", [1, 2, 3]), \"b\": xr.Variable(\"y\", [4, 5, 6])}\n    result = merge.merge_variables([data1, data2], compat=\"equals\")\n    expected = OrderedDict(\n        {\"a\": xr.Variable(\"x\", [1, 2, 3]), \"b\": xr.Variable(\"y\", [4, 5, 6])}\n    )\n    assert expected == result\n\n    with pytest.raises(merge.MergeError):\n        data3 = {\"a\": xr.Variable(\"x\", [7, 8, 9])}\n        merge.merge_variables([data1, data3], compat=\"equals\")\n\n    data4 = {\"a\": xr.Variable(\"x\", [1, 2, 3]), \"b\": xr.Variable(\"x\", [7, 8, 9])}\n    result = merge.merge_variables([data1, data4], compat=\"no_conflicts\")\n    expected = OrderedDict(\n        {\n            \"a\": xr.Variable(\"x\", [1, 2, 3]),\n            \"b\": xr.Variable(\"y\", [4, 5, 6]),\n            \"b\": xr.Variable(\"x\", [7, 8, 9]),\n        }\n    )\n    assert expected == result\n", "def test_unique_variable(self):\n    var1 = xr.Variable(\"x\", [1, 2, 3])\n    var2 = xr.Variable(\"x\", [1, 2, 3])\n    var3 = xr.Variable(\"x\", [1, 2, 3])\n    var4 = xr.Variable(\"x\", [3, 2, 1])\n\n    # Test identical\n    assert merge.unique_variable(\"x\", [var1, var2], compat=\"identical\").identical(var1)\n\n    # Test equals\n    assert merge.unique_variable(\"x\", [var1, var2], compat=\"equals\").equals(var1)\n\n    # Test broadcast_equals\n    assert merge.unique_variable(\"x\", [var1, var2], compat=\"broadcast_equals\").broadcast_equals(var1)\n\n    # Test no_conflicts\n    var1_with_nan = xr.Variable(\"x\", [1, np.nan, 3])\n    var2_with_nan = xr.Variable(\"x\", [np.nan, 2, np.nan])\n    result = merge.unique_variable(\"x\", [var1_with_nan, var2_with_nan], compat=\"no_conflicts\")\n    expected = xr.Variable(\"x\", [1, 2, 3])\n    assert result.equals(expected)\n\n    # Test override\n    assert merge.unique_variable(\"x\", [var1, var4], compat=\"override\").identical(var1)\n\n    with pytest.raises(merge.MergeError):\n        merge.unique_variable(\"x\", [var1, var4], compat=\"equals\")\n\n    with pytest.raises(ValueError):\n        merge.unique_variable(\"x\", [var1, var4], compat=\"foobar\")\n", "    def test_merge_with_priority_vars(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"b\": (\"x\", [3, 4]), \"x\": [1, 2]})\n        ds3 = xr.Dataset({\"a\": (\"x\", [10, 20]), \"b\": (\"x\", [30, 40]), \"x\": [0, 1]})\n        \n        # `ds3` should take priority for variables `a` and `b`\n        expected = xr.Dataset({\"a\": (\"x\", [10, 20]), \"b\": (\"x\", [30, 40]), \"x\": [0, 1]})\n        actual = xr.merge([ds1, ds2, ds3], compat=\"no_conflicts\")\n        assert expected.identical(actual)\n"], "sample_588": ["    def test_combine_all_along_first_dim(self):\n        ds = create_test_data\n        combined_ids = {\n            (0, 0): ds(0),\n            (0, 1): ds(1),\n            (1, 0): ds(2),\n            (1, 1): ds(3),\n        }\n        result = _combine_all_along_first_dim(\n            combined_ids,\n            dim=\"new_dim\",\n            data_vars=\"all\",\n            coords=\"different\",\n            compat=\"no_conflicts\",\n        )\n\n        expected_ds1 = concat([ds(0), ds(1)], dim=\"new_dim\")\n        expected_ds2 = concat([ds(2), ds(3)], dim=\"new_dim\")\n        expected = {\n            (0,): expected_ds1,\n            (1,): expected_ds2,\n        }\n\n        assert_combined_tile_ids_equal(result, expected)\n", "    def test_combine_all_along_first_dim(self):\n        ds0 = Dataset({\"x\": [0, 1]})\n        ds1 = Dataset({\"x\": [2, 3]})\n        ds2 = Dataset({\"x\": [4, 5]})\n        combined_ids = OrderedDict({\n            (0,): ds0,\n            (1,): ds1,\n            (2,): ds2\n        })\n        result = _combine_all_along_first_dim(\n            combined_ids,\n            dim=\"x\",\n            data_vars=\"all\",\n            coords=\"different\",\n            compat=\"no_conflicts\",\n        )\n        expected_ds = concat([ds0, ds1, ds2], dim=\"x\")\n        assert_combined_tile_ids_equal(result, {(): expected_ds})\n", "    def test_auto_concat_with_non_standard_dims(self):\n        ds1 = Dataset({\"var\": ((\"x\", \"y\"), [[1, 2], [3, 4]])})\n        ds2 = Dataset({\"var\": ((\"x\", \"y\"), [[5, 6], [7, 8]])})\n\n        objs = [ds1, ds2]\n        result = auto_combine(objs, concat_dim='z')\n        expected = Dataset({\"var\": ((\"z\", \"x\", \"y\"), [[[1, 2], [3, 4]], [[5, 6], [7, 8]]])})\n\n        assert_identical(expected, result)\n", "    def test_string_coords_with_spaces(self):\n        ds0 = Dataset({\"city\": [\"New York\", \"Los Angeles\"]})\n        ds1 = Dataset({\"city\": [\"San Francisco\", \"Chicago\"]})\n\n        expected = {(0,): ds0, (1,): ds1}\n        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])\n        assert_combined_tile_ids_equal(expected, actual)\n        assert concat_dims == [\"city\"]\n", "    def test_dict_as_entry(self):\n        # Test with a dictionary instead of a list\n        ds = create_test_data(0)\n        input = {\"key1\": ds, \"key2\": [ds, ds]}\n        with raises_regex(TypeError, \"Input should be a list of lists\"):\n            actual = _infer_tile_ids_from_nested_list(input, ())\n            list(actual)  # force iteration\n"], "sample_589": ["def test_get_clean_interp_index():\n    # Create a DataArray with both uniform and non-uniform coordinates\n    uniform_da = xr.DataArray(\n        np.array([1, 2, np.nan, 4], dtype=np.float64), dims=\"x\", coords={\"x\": [0, 1, 2, 3]}\n    )\n    non_uniform_da = xr.DataArray(\n        np.array([1, 2, np.nan, 4], dtype=np.float64), dims=\"x\", coords={\"x\": [0, 2, 3, 7]}\n    )\n\n    # Test for uniform coordinates\n    index = get_clean_interp_index(uniform_da, dim=\"x\")\n    np.testing.assert_array_equal(index, np.array([0, 1, 2, 3]))\n\n    # Test for non-uniform coordinates\n    index = get_clean_interp_index(non_uniform_da, dim=\"x\")\n    np.testing.assert_array_equal(index, np.array([0.0, 2.0, 3.0, 7.0]))\n\n    # Test using coordinate flag\n    index = get_clean_interp_index(uniform_da, dim=\"x\", use_coordinate=False)\n    np.testing.assert_array_equal(index, np.array([0, 1, 2, 3]))\n\n    # Test for non-1D coordinates\n    non_1d_da = xr.DataArray(\n        np.array([[1, 2], [3, 4]], dtype=np.float64), dims=(\"x\", \"y\"), coords={\"x\": [0, 1], \"y\": [[0, 1], [2, 3]]}\n    )\n    with raises_regex(ValueError, \"Coordinates used for interpolation must be 1D\"):\n        get_clean_interp_index(non_1d_da, dim=\"x\", use_coordinate=\"y\")\n\n    # Test for non-monotonic index\n    non_monotonic_da = xr.DataArray(\n        np.array([1, 2, np.nan, 4], dtype=np.float64), dims=\"x\", coords={\"x\": [0, 3, 2, 1]}\n    )\n    with raises_regex(ValueError, \"Index 'x' must be monotonically increasing\"):\n        get_clean_interp_index(non_monotonic_da, dim=\"x\")\n\n    # Test for duplicate values\n    duplicate_values_da = xr.DataArray(\n        np.array([1, 2", "def test_spline_interpolator():\n    xi = np.linspace(0, 10, 10)\n    yi = np.sin(xi)\n    x = np.linspace(0, 10, 50)\n\n    # Initialize SplineInterpolator and interpolate\n    interpolator = SplineInterpolator(xi, yi, order=3)\n    interpolated_values = interpolator(x)\n\n    # Compare against scipy's UnivariateSpline directly\n    from scipy.interpolate import UnivariateSpline\n    spline = UnivariateSpline(xi, yi, k=3)\n    expected_values = spline(x)\n\n    np.testing.assert_allclose(interpolated_values, expected_values, atol=1e-6)\n\n    # Check error for invalid method\n    with pytest.raises(ValueError, match=\"only method `spline` is valid for the SplineInterpolator\"):\n        SplineInterpolator(xi, yi, method=\"linear\")\n", "def test_bfill():\n    da = xr.DataArray(np.array([4, np.nan, 5], dtype=np.float64), dims=\"x\")\n    expected = xr.DataArray(np.array([4, 5, 5], dtype=np.float64), dims=\"x\")\n    actual = da.bfill(\"x\")\n    assert_equal(actual, expected)\n", "def test_get_clean_interp_index():\n    arr = xr.DataArray(\n        np.array([1, 2, np.nan, 4, np.nan, 6], dtype=np.float64), \n        dims=\"time\",\n        coords={\"time\": pd.date_range(\"2000-01-01\", periods=6)}\n    )\n\n    # Test using coordinate\n    index = get_clean_interp_index(arr, dim=\"time\", use_coordinate=True)\n    expected = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=np.float64)\n    np.testing.assert_allclose(index, expected)\n\n    # Test not using coordinate\n    index = get_clean_interp_index(arr, dim=\"time\", use_coordinate=False)\n    expected = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=np.float64)\n    np.testing.assert_allclose(index, expected)\n", "def test_invalid_spline_interpolator():\n    with raises_regex(ValueError, \"only method `spline` is valid for the SplineInterpolator\"):\n        SplineInterpolator(np.array([1, 2, 3]), np.array([1, 2, 3]), method=\"linear\")\n    with raises_regex(ValueError, \"SplineInterpolator does not support fill_value\"):\n        SplineInterpolator(np.array([1, 2, 3]), np.array([1, 2, 3]), fill_value=0)\n"], "sample_590": ["def test_concat_empty_datasets():\n    # Test concatenating empty Datasets\n    ds1 = Dataset()\n    ds2 = Dataset()\n\n    result = concat([ds1, ds2], dim=\"x\")\n    expected = Dataset()\n    assert_identical(result, expected)\n\n    # Test concatenating an empty Dataset with a non-empty Dataset\n    ds3 = Dataset({\"foo\": (\"x\", [1, 2, 3])})\n    result = concat([ds1, ds3], dim=\"x\")\n    assert_identical(result, ds3)\n\n    result = concat([ds3, ds1], dim=\"x\")\n    assert_identical(result, ds3)\n\n    # Test concatenating multiple empty Datasets with a non-empty Dataset\n    result = concat([ds1, ds1, ds3, ds1], dim=\"x\")\n    assert_identical(result, ds3)\n", "def test_concat_with_positions():\n    ds1 = Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = Dataset({\"a\": (\"x\", [3, 4]), \"x\": [0, 1]})\n    ds3 = Dataset({\"a\": (\"x\", [5, 6]), \"x\": [0, 1]})\n    positions = [[0, 2], [1, 3]]\n\n    expected = Dataset({\"a\": (\"x\", [1, 3, 2, 4, 5, 6]), \"x\": [0, 0, 1, 1, 0, 1]})\n    actual = concat([ds1, ds2, ds3], dim=\"x\", positions=positions)\n    assert_identical(expected, actual)\n", "def test_concat_with_positions():\n    # create datasets with different lengths along the concatenation axis\n    ds1 = Dataset({\"var\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"var\": (\"x\", [4, 5])}, coords={\"x\": [3, 4]})\n    ds3 = Dataset({\"var\": (\"x\", [6, 7, 8])}, coords={\"x\": [5, 6, 7]})\n\n    # specify positions for concatenation\n    positions = [[0, 1, 2], [3, 4], [5, 6, 7]]\n    expected = Dataset({\"var\": (\"x\", [1, 2, 3, 4, 5, 6, 7, 8])}, coords={\"x\": [0, 1, 2, 3, 4, 5, 6, 7]})\n\n    actual = concat([ds1, ds2, ds3], dim=\"x\", positions=positions)\n    assert_identical(expected, actual)\n", "def test_concat_with_scalar_coords():\n    # Test concatenating scalar coordinates even if the concatenated dimension already exists\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2])}, coords={\"scalar_coord\": 10})\n    ds2 = Dataset({\"foo\": (\"x\", [3, 4])}, coords={\"scalar_coord\": 20})\n    expected = Dataset({\"foo\": (\"x\", [1, 2, 3, 4])}, coords={\"scalar_coord\": (\"x\", [10, 10, 20, 20])})\n    actual = concat([ds1, ds2], dim=\"x\", coords=\"all\")\n    assert_identical(expected, actual)\n", "def test_concat_unaligned():\n    # Test concatenation of datasets with unaligned coordinates\n    ds1 = Dataset(\n        {\"foo\": (\"x\", [1, 2])},\n        coords={\"x\": [0, 1], \"y\": (\"x\", [0.1, 0.2])}\n    )\n    ds2 = Dataset(\n        {\"foo\": (\"x\", [3, 4])},\n        coords={\"x\": [2, 3], \"y\": (\"x\", [0.3, 0.4])}\n    )\n    actual = concat([ds1, ds2], dim=\"x\")\n    expected = Dataset(\n        {\"foo\": (\"x\", [1, 2, 3, 4])},\n        coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [0.1, 0.2, 0.3, 0.4])}\n    )\n    assert_identical(expected, actual)\n"], "sample_591": ["def test_merge_with_missing_values():\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, np.nan, 3]), \"x\": [0, 1, 2]})\n    ds2 = xr.Dataset({\"b\": (\"x\", [np.nan, 4, 5]), \"x\": [1, 2, 3]})\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, np.nan, 3, np.nan]), \"b\": (\"x\", [np.nan, 4, 5, np.nan])},\n        {\"x\": [0, 1, 2, 3]},\n    )\n    assert expected.identical(ds1.merge(ds2))\n    assert expected.identical(ds2.merge(ds1))\n    assert expected.identical(xr.merge([ds1, ds2]))\n", "def test_merge_datasets_with_coords(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"y\", [5, 6])}, coords={\"x\": [0, 1], \"y\": [2, 3]})\n    ds2 = xr.Dataset({\"c\": (\"x\", [3, 4]), \"d\": (\"y\", [7, 8])}, coords={\"x\": [1, 2], \"y\": [3, 4]})\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2, np.nan]), \"b\": (\"y\", [5, 6, np.nan]), \"c\": (\"x\", [np.nan, 3, 4]), \"d\": (\"y\", [np.nan, 7, 8])},\n        coords={\"x\": [0, 1, 2], \"y\": [2, 3, 4]}\n    )\n    actual = xr.merge([ds1, ds2])\n    assert expected.identical(actual)\n\n    expected_left = xr.Dataset(\n        {\"a\": (\"x\", [1, 2]), \"b\": (\"y\", [5, 6]), \"c\": (\"x\", [np.nan, 3]), \"d\": (\"y\", [np.nan, 7])},\n        coords={\"x\": [0, 1], \"y\": [2, 3]}\n    )\n    actual_left = xr.merge([ds1, ds2], join=\"left\")\n    assert expected_left.identical(actual_left)\n\n    expected_right = xr.Dataset(\n        {\"a\": (\"x\", [2, np.nan]), \"b\": (\"y\", [6, np.nan]), \"c\": (\"x\", [3, 4]), \"d\": (\"y\", [7, 8])},\n        coords={\"x\": [1, 2], \"y\": [3, 4]}\n    )\n    actual_right = xr.merge([ds1, ds2], join=\"right\")\n    assert expected_right.identical(actual_right)\n\n    expected_inner = xr.Dataset(\n        {\"a\": (\"x\", [2]), \"b\": (\"y\", [6]), \"c\": (\"x\", [3]), \"d\": (\"y\", [7])},\n        coords={\"x\": [1], \"y\": [3]}\n    )\n    actual_inner = xr.merge([ds", "def test_merge_with_dataarray_coords():\n    # Test merging a Dataset with a DataArray that has coordinates\n    ds = xr.Dataset({\"a\": (\"x\", [1, 2])}, coords={\"x\": [0, 1], \"y\": (\"x\", [3, 4])})\n    da = xr.DataArray(data=[5, 6], dims=\"x\", coords={\"x\": [1, 2], \"z\": (\"x\", [7, 8])})\n\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2, np.nan]), \"b\": (\"x\", [np.nan, 5, 6])},\n        coords={\"x\": [0, 1, 2], \"y\": (\"x\", [3, 4, np.nan]), \"z\": (\"x\", [np.nan, 7, 8])},\n    )\n    actual = ds.merge(da.rename(\"b\"))\n    assert_identical(actual, expected)\n\n    # Test that coordinates from DataArray are preserved\n    da = xr.DataArray(data=[5, 6], dims=\"x\", coords={\"x\": [1, 2], \"y\": (\"x\", [7, 8])})\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2, np.nan]), \"b\": (\"x\", [np.nan, 5, 6])},\n        coords={\"x\": [0, 1, 2], \"y\": (\"x\", [3, 4, 8])},\n    )\n    actual = ds.merge(da.rename(\"b\"))\n    assert_identical(actual, expected)\n", "def test_merge_alignment_broadcast_error():\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"y\", [3, 4]), \"y\": [1, 2]})\n    with raises_regex(ValueError, \"cannot broadcast\"):\n        xr.merge([ds1, ds2], compat=\"broadcast_equals\")\n", "def test_merge_with_empty_dataset(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset()\n    expected = ds1.copy()\n    actual = ds1.merge(ds2)\n    assert expected.identical(actual)\n\n    actual = ds2.merge(ds1)\n    assert expected.identical(actual)\n"], "sample_592": ["def test_wrap_indent():\n    cases = [\n        (\"single line text\", \"prefix: \", 10, \"prefix: single line text\"),\n        (\"multi\\nline\\ntext\", \"prefix: \", 8, \"prefix: multi\\n        line\\n        text\"),\n        (\"single line text\", \"\", 0, \"single line text\"),\n        (\"\", \"prefix: \", 8, \"prefix: \"),\n        (\"text with\\nmultiple\\nlines\\nand spaces\", \">> \", 4, \">> text with\\n    multiple\\n    lines\\n    and spaces\"),\n    ]\n    for text, start, length, expected in cases:\n        actual = formatting.wrap_indent(text, start, length)\n        assert expected == actual\n", "def test_wrap_indent():\n    text = \"This is a test string\\nthat spans multiple lines\\nand should be indented.\"\n    start = \">> \"\n    length = 4\n    actual = formatting.wrap_indent(text, start, length)\n    expected = \">> This is a test string\\n    that spans multiple lines\\n    and should be indented.\"\n    assert actual == expected\n\n    actual = formatting.wrap_indent(text, start)\n    expected = \">> This is a test string\\n   that spans multiple lines\\n   and should be indented.\"\n    assert actual == expected\n\n    actual = formatting.wrap_indent(text)\n    expected = \"This is a test string\\nthat spans multiple lines\\nand should be indented.\"\n    assert actual == expected\n", "def test_summarize_variable():\n    var = xr.Variable((\"x\", \"y\"), np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\"))\n    name = \"test_var\"\n    col_width = 20\n    max_width = 80\n\n    expected = \"  test_var            (x, y) int64 1 2 3 4 5 6\"\n    actual = formatting.summarize_variable(name, var, col_width, marker=\" \", max_width=max_width)\n    assert actual == expected\n\n    var = xr.Variable(\"x\", np.array([1, 2, 3], dtype=\"int64\"))\n    expected = \"  test_var            (x) int64 1 2 3\"\n    actual = formatting.summarize_variable(name, var, col_width, marker=\" \", max_width=max_width)\n    assert actual == expected\n\n    var = xr.Variable((), np.array(1, dtype=\"int64\"))\n    expected = \"  test_var            int64 1\"\n    actual = formatting.summarize_variable(name, var, col_width, marker=\" \", max_width=max_width)\n    assert actual == expected\n", "def test_wrap_indent():\n    long_text = \"This is a long line of text that needs to be wrapped and indented.\"\n    start = \"Start: \"\n    length = len(start)\n    expected = \"Start: This is a long\\n       line of text\\n       that needs to\\n       be wrapped and\\n       indented.\"\n    actual = formatting.wrap_indent(long_text, start=start, length=length)\n    assert expected == actual\n\n    # Test without specifying the length\n    long_text = \"Another example of text\\nthat spans multiple lines.\"\n    start = \"Prefix: \"\n    expected = \"Prefix: Another example\\n        of text\\n        that spans\\n        multiple lines.\"\n    actual = formatting.wrap_indent(long_text, start=start)\n    assert expected == actual\n\n    # Test with no start and no length\n    long_text = \"Yet another example of text\\nwith different formatting.\"\n    expected = \"Yet another example\\nof text\\nwith different\\nformatting.\"\n    actual = formatting.wrap_indent(long_text)\n    assert expected == actual\n", "def test_wrap_indent():\n    text = \"This is a test string\\nwith multiple lines\\nand indentation\"\n    start = \">> \"\n    length = 5\n    expected = \">> This is a test string\\n     with multiple lines\\n     and indentation\"\n    actual = formatting.wrap_indent(text, start, length)\n    assert expected == actual\n\n    # Test with default length\n    length = None\n    expected = \">> This is a test string\\n   with multiple lines\\n   and indentation\"\n    actual = formatting.wrap_indent(text, start, length)\n    assert expected == actual\n"], "sample_593": ["def test_collapsible_section():\n    section = fh.collapsible_section(\n        \"Test Section\", inline_details=\"Some details\", details=\"Full details\", n_items=5\n    )\n    assert \"Test Section\" in section\n    assert \"Some details\" in section\n    assert \"Full details\" in section\n    assert \"title='Expand/collapse section'\" in section\n", "def test_summarize_coord():\n    name = \"x\"\n    var = xr.DataArray([1, 2, 3], dims=\"x\")\n    summarized = fh.summarize_coord(name, var)\n    assert name in summarized\n    assert \"xr-var-name\" in summarized[name]\n", "def test_summarize_variable_with_attrs_and_preview():\n    var = xr.Variable(\n        dims=[\"x\"],\n        data=[1, 2, 3],\n        attrs={\"foo\": \"bar\"},\n    )\n    name = \"test_var\"\n    is_index = False\n    dtype = \"int64\"\n    preview = \"custom_preview\"\n    formatted = fh.summarize_variable(name, var, is_index, dtype, preview)\n    assert \"test_var\" in formatted\n    assert \"(x)\" in formatted\n    assert \"int64\" in formatted\n    assert \"custom_preview\" in formatted\n    assert \"foo\" in formatted\n    assert \"bar\" in formatted\n", "def test_collapsible_section():\n    section_html = fh.collapsible_section(\n        name=\"Test Section\",\n        inline_details=\"Inline details\",\n        details=\"Detailed content\",\n        n_items=5,\n        enabled=True,\n        collapsed=False\n    )\n    assert \"Test Section: <span>(5)</span>\" in section_html\n    assert \"Inline details\" in section_html\n    assert \"Detailed content\" in section_html\n    assert \"type='checkbox'  checked>\" not in section_html\n", "def test_summarize_vars_with_multiple_variables(dataset):\n    variables = dataset.data_vars\n    summarized = fh.summarize_vars(variables)\n    assert \"class='xr-var-item'\" in summarized\n    assert \"tmin\" in summarized\n    assert \"tmax\" in summarized\n    assert \"class='xr-var-name'\" in summarized\n    assert \"class='xr-var-dims'\" in summarized\n    assert \"class='xr-var-dtype'\" in summarized\n    assert \"class='xr-var-preview'\" in summarized\n"], "sample_594": ["def test_wrap_indent():\n    text = \"This is a long text that needs to be wrapped with indentation.\"\n    start = \">> \"\n    length = 4\n\n    actual = formatting.wrap_indent(text, start, length)\n    expected = \">> This is a long text that needs to be wrapped with indentation.\"\n\n    assert actual == expected\n\n    text_multiline = \"This is a long text\\nthat spans multiple lines\\nand needs to be wrapped.\"\n    start_multiline = \">> \"\n    length_multiline = 3\n\n    actual_multiline = formatting.wrap_indent(text_multiline, start_multiline, length_multiline)\n    expected_multiline = \">> This is a long text\\n   that spans multiple lines\\n   and needs to be wrapped.\"\n\n    assert actual_multiline == expected_multiline\n", "def test_inline_dask_repr():\n    import dask.array as da\n\n    # Create a simple dask array with known chunks\n    dask_array = da.from_array(np.arange(100).reshape(10, 10), chunks=(5, 5))\n\n    # Test the inline_dask_repr function\n    expected_repr = \"dask.array<chunksize=(5, 5)>\"\n    actual_repr = formatting.inline_dask_repr(dask_array)\n    assert actual_repr == expected_repr\n", "def test_wrap_indent():\n    text = \"This is a test string.\\nThis should be on a new line.\"\n    start = \"Start: \"\n    length = len(start)\n    expected = \"Start: This is a test string.\\n        This should be on a new line.\"\n    actual = formatting.wrap_indent(text, start, length)\n    assert expected == actual\n\n    text = \"This is a single line string.\"\n    start = \"Prefix: \"\n    length = len(start)\n    expected = \"Prefix: This is a single line string.\"\n    actual = formatting.wrap_indent(text, start, length)\n    assert expected == actual\n\n    text = \"Line 1\\nLine 2\\nLine 3\"\n    start = \"\"\n    length = 4\n    expected = \"Line 1\\n    Line 2\\n    Line 3\"\n    actual = formatting.wrap_indent(text, start, length)\n    assert expected == actual\n", "def test_wrap_indent():\n    text = \"This is a test string\\nwith multiple lines\\nand indentation.\"\n    start = \"Start: \"\n    length = 10\n\n    result = formatting.wrap_indent(text, start=start, length=length)\n    expected = \"Start: This is a test string\\n          with multiple lines\\n          and indentation.\"\n    assert result == expected\n", "def test_wrap_indent():\n    text = \"This is a test string.\\nIt should be indented properly.\"\n    expected = \">>This is a test string.\\n>>It should be indented properly.\"\n    result = formatting.wrap_indent(text, start=\">>\", length=2)\n    assert result == expected\n\n    text = \"Single line text\"\n    expected = \"!!!Single line text\"\n    result = formatting.wrap_indent(text, start=\"!!!\", length=3)\n    assert result == expected\n\n    text = \"Multiple\\nLines\\nOf\\nText\"\n    expected = \"--Multiple\\n--Lines\\n--Of\\n--Text\"\n    result = formatting.wrap_indent(text, start=\"--\", length=2)\n    assert result == expected\n"], "sample_595": ["def test_slice_replace_edge_cases(dtype):\n    da = lambda x: xr.DataArray(x).astype(dtype)\n    values = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"edgecase\"])\n\n    # Test replacing from start to end\n    expected = da([\"\", \"\", \"\", \"\"])\n    result = values.str.slice_replace(0, None, \"\")\n    assert_equal(result, expected)\n\n    # Test replacing with empty string\n    expected = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"edgecase\"])\n    result = values.str.slice_replace(0, None, \"\")\n    assert_equal(result, expected)\n\n    # Test replacing with None replacement\n    expected = da([\"\", \"\", \"\", \"\"])\n    result = values.str.slice_replace(0, None, None)\n    assert_equal(result, expected)\n\n    # Test replacing with long replacement string\n    expected = da([\"longreplacementstring\", \"longreplacementstring\", \n                   \"longreplacementstring\", \"longreplacementstring\"])\n    result = values.str.slice_replace(0, None, \"longreplacementstring\")\n    assert_equal(result, expected)\n\n    # Test replacing with negative indices\n    expected = da([\"shor\", \"a bit longe\", \"evenlongerthant\", \"edgecas\"])\n    result = values.str.slice_replace(-1, None, \"\")\n    assert_equal(result, expected)\n", "def test_capitalize(dtype):\n    values = xr.DataArray([\"hello world\", \"python testing\", \"xarray str\"]).astype(dtype)\n    result = values.str.capitalize()\n    expected = xr.DataArray([\"Hello world\", \"Python testing\", \"Xarray str\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    # Check capitalization of empty strings\n    values = xr.DataArray([\"\", \"\", \"\"]).astype(dtype)\n    result = values.str.capitalize()\n    expected = xr.DataArray([\"\", \"\", \"\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    # Check capitalization of strings with mixed cases\n    values = xr.DataArray([\"HELLO world\", \"PYTHON Testing\", \"xARray StR\"]).astype(dtype)\n    result = values.str.capitalize()\n    expected = xr.DataArray([\"Hello world\", \"Python testing\", \"Xarray str\"]).astype(dtype)\n    assert_equal(result, expected)\n", "def test_capitalize(dtype):\n    values = xr.DataArray([\"hello world\", \"HELLO WORLD\", \"Hello World\"]).astype(dtype)\n    result = values.str.capitalize()\n    expected = xr.DataArray([\"Hello world\", \"Hello world\", \"Hello world\"]).astype(dtype)\n    assert_equal(result, expected)\n", "def test_slice_replace_edge_cases(dtype):\n    da = lambda x: xr.DataArray(x).astype(dtype)\n    values = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n\n    # Edge case: replacing with an empty string when start and stop are None\n    expected = da([\"\", \"\", \"\", \"\"])\n    result = values.str.slice_replace(None, None, \"\")\n    assert_equal(result, expected)\n\n    # Edge case: negative start and stop values\n    expected = da([\"sho\", \"a bit lon\", \"evenlongerth\", \"\"])\n    result = values.str.slice_replace(-3, -1, \"\")\n    assert_equal(result, expected)\n\n    # Edge case: start is greater than the string length\n    expected = da([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"])\n    result = values.str.slice_replace(100, 200, \"z\")\n    assert_equal(result, expected)\n\n    # Edge case: stop is greater than the string length\n    expected = da([\"shorz\", \"a bit longez\", \"evenlongerthanthaz\", \"z\"])\n    result = values.str.slice_replace(-1, 100, \"z\")\n    assert_equal(result, expected)\n\n    # Edge case: both start and stop are None but replacement is not empty\n    expected = da([\"z\", \"z\", \"z\", \"z\"])\n    result = values.str.slice_replace(None, None, \"z\")\n    assert_equal(result, expected)\n", "def test_slice_methods(dtype):\n    da = xr.DataArray([\"short\", \"a bit longer\", \"evenlongerthanthat\", \"\"]).astype(dtype)\n    \n    # Test slice\n    expected = xr.DataArray([\"or\", \"it\", \"en\", \"\"]).astype(dtype)\n    result = da.str.slice(1, 3)\n    assert_equal(result, expected)\n    \n    # Test slice with step\n    expected = xr.DataArray([\"srt\", \"a itlnr\", \"enoge hntt\", \"\"]).astype(dtype)\n    result = da.str.slice(0, None, 2)\n    assert_equal(result, expected)\n    \n    # Test slice with negative step\n    expected = xr.DataArray([\"rts\", \"tib a\", \"tahtnahtregnolneve\", \"\"]).astype(dtype)\n    result = da.str.slice(None, None, -1)\n    assert_equal(result, expected)\n    \n    # Test slice with start and step\n    expected = xr.DataArray([\"oh\", \" i \", \"ee\", \"\"]).astype(dtype)\n    result = da.str.slice(1, None, 2)\n    assert_equal(result, expected)\n    \n    # Test slice with stop and step\n    expected = xr.DataArray([\"so\", \"a \", \"e\", \"\"]).astype(dtype)\n    result = da.str.slice(None, 5, 2)\n    assert_equal(result, expected)\n    \n    # Test slice_replace with step\n    expected = xr.DataArray([\"sht\", \"a tlnr\", \"evnlne hntt\", \"z\"]).astype(dtype)\n    result = da.str.slice_replace(1, 3, \"z\")\n    assert_equal(result, expected)\n    \n    # Test slice_replace with negative step\n    expected = xr.DataArray([\"zrts\", \"ztib a\", \"ztahtnahtregnolneve\", \"z\"]).astype(dtype)\n    result = da.str.slice_replace(None, None, \"z\")\n    assert_equal(result, expected)\n    \n    # Test slice_replace with start and step\n    expected = xr.DataArray([\"shrt\", \"a zit lnger\", \"evznlongerthanthat\", \"z\"])\n    result = da.str.slice_replace(2, None, \"z\")\n    assert_equal(result, expected)\n    \n    # Test slice_replace with stop and step\n    expected = xr.DataArray([\"shrt\", \"a zit lnger\", \"evzn"], "sample_596": ["def test_concat_with_scalar_coords():\n    ds1 = Dataset(\n        {\"data\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2], \"scalar_coord\": 5}\n    )\n    ds2 = Dataset(\n        {\"data\": (\"x\", [4, 5, 6])}, coords={\"x\": [3, 4, 5], \"scalar_coord\": 5}\n    )\n\n    expected = Dataset(\n        {\"data\": (\"x\", [1, 2, 3, 4, 5, 6])}, coords={\"x\": [0, 1, 2, 3, 4, 5], \"scalar_coord\": 5}\n    )\n\n    actual = concat([ds1, ds2], dim=\"x\")\n    assert_identical(expected, actual)\n\n    ds3 = Dataset(\n        {\"data\": (\"x\", [7, 8, 9])}, coords={\"x\": [6, 7, 8], \"scalar_coord\": 10}\n    )\n\n    with raises_regex(ValueError, \"coordinate in some datasets but not others\"):\n        concat([ds1, ds2, ds3], dim=\"x\")\n", "def test_concat_with_positions():\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2]), \"bar\": (\"x\", [3, 4])}, coords={\"x\": [0, 1]})\n    ds2 = Dataset({\"foo\": (\"x\", [5, 6]), \"bar\": (\"x\", [7, 8])}, coords={\"x\": [2, 3]})\n    ds3 = Dataset({\"foo\": (\"x\", [9, 10]), \"bar\": (\"x\", [11, 12])}, coords={\"x\": [4, 5]})\n\n    positions = [0, 2, 4]\n    actual = concat([ds1, ds2, ds3], dim=\"x\", positions=positions)\n\n    expected = Dataset(\n        {\n            \"foo\": (\"x\", [1, 2, 5, 6, 9, 10]),\n            \"bar\": (\"x\", [3, 4, 7, 8, 11, 12]),\n        },\n        coords={\"x\": [0, 1, 2, 3, 4, 5]},\n    )\n\n    assert_identical(actual, expected)\n", "def test_concat_new_dimension():\n    ds1 = Dataset({\"var\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"var\": (\"x\", [4, 5, 6])}, coords={\"x\": [0, 1, 2]})\n    dim = DataArray([7, 8], dims=\"new_dim\")\n\n    expected = Dataset(\n        {\"var\": ((\"new_dim\", \"x\"), [[1, 2, 3], [4, 5, 6]])},\n        coords={\"x\": [0, 1, 2], \"new_dim\": [7, 8]},\n    )\n\n    actual = concat([ds1, ds2], dim=dim)\n    assert_identical(expected, actual)\n\n    with raises_regex(ValueError, \"concat dimension already exists\"):\n        concat([ds1, ds2], dim=\"x\")\n", "def test_concat_variable_dim_name_as_string():\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"foo\": (\"x\", [4, 5, 6])}, coords={\"x\": [3, 4, 5]})\n\n    actual = concat([ds1, ds2], dim=\"x\")\n    expected = Dataset({\"foo\": (\"x\", [1, 2, 3, 4, 5, 6])}, coords={\"x\": [0, 1, 2, 3, 4, 5]})\n    assert_identical(expected, actual)\n\n    # Test with a string dimension name that is not present in datasets\n    dim_name = \"new_dim\"\n    ds1 = Dataset({\"bar\": (\"y\", [7, 8, 9])}, coords={\"y\": [1, 2, 3]})\n    ds2 = Dataset({\"bar\": (\"y\", [10, 11, 12])}, coords={\"y\": [4, 5, 6]})\n\n    actual = concat([ds1, ds2], dim=dim_name)\n    expected = Dataset({\"bar\": (dim_name, [7, 8, 9, 10, 11, 12])})\n    assert_identical(expected, actual)\n", "def test_concat_different_dims():\n    # Test concatenation with different dimensions in the datasets\n    ds1 = Dataset({\"foo\": ((\"x\", \"y\"), [[1, 2], [3, 4]])}, coords={\"x\": [1, 2], \"y\": [1, 2]})\n    ds2 = Dataset({\"foo\": ((\"x\", \"z\"), [[5, 6], [7, 8]])}, coords={\"x\": [1, 2], \"z\": [3, 4]})\n\n    # The dimension \"z\" is in ds2 but not in ds1. The resulting dataset should have both \"y\" and \"z\" dimensions.\n    expected = Dataset(\n        {\n            \"foo\": ((\"x\", \"y\", \"z\"), [[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n        },\n        coords={\"x\": [1, 2], \"y\": [1, 2], \"z\": [3, 4]}\n    )\n    \n    actual = concat([ds1, ds2], dim=\"x\")\n    assert_identical(expected, actual)\n"], "sample_597": ["    def test_unique_variable_identical(self):\n        var1 = xr.Variable(\"x\", [1, 2, 3])\n        var2 = xr.Variable(\"x\", [1, 2, 3])\n        result = merge.unique_variable(\"var\", [var1, var2], compat=\"identical\")\n        assert result.equals(var1)\n", "def test_merge_with_priority_arg():\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [3, 4]), \"x\": [1, 2]})\n    ds3 = xr.Dataset({\"b\": (\"x\", [5, 6]), \"x\": [0, 1]})\n    \n    # Test with priority_arg\n    expected = xr.Dataset({\"a\": (\"x\", [3, 4]), \"b\": (\"x\", [5, 6]), \"x\": [1, 2]})\n    assert expected.identical(xr.merge([ds1, ds2, ds3], priority_arg=1))\n    \n    expected = xr.Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [5, 6]), \"x\": [0, 1]})\n    assert expected.identical(xr.merge([ds1, ds2, ds3], priority_arg=0))\n    \n    # Test with priority_arg and join=\"outer\"\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2, 4]), \"b\": (\"x\", [5, 6, np.nan]), \"x\": [0, 1, 2]}\n    )\n    assert expected.identical(xr.merge([ds1, ds2, ds3], priority_arg=0, join=\"outer\"))\n    \n    # Test with priority_arg and join=\"inner\"\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [5, 6]), \"x\": [0, 1]}\n    )\n    assert expected.identical(xr.merge([ds1, ds2, ds3], priority_arg=0, join=\"inner\"))\n    \n    # Test with priority_arg and compat=\"no_conflicts\"\n    ds4 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds5 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 3]), \"x\": [0, 1, 2]})\n    assert expected.identical(xr.merge([ds4", "    def test_merge_dataarray_with_coords(self):\n        ds = xr.Dataset({\"a\": 0}, coords={\"x\": [1, 2]})\n        da = xr.DataArray(data=[3, 4], coords={\"x\": [1, 2]}, dims=\"x\", name=\"b\")\n\n        expected = xr.Dataset({\"a\": 0, \"b\": (\"x\", [3, 4])}, coords={\"x\": [1, 2]})\n        actual = ds.merge(da)\n\n        assert_identical(expected, actual)\n", "def test_unique_variable(self):\n    var1 = xr.Variable((\"x\", \"y\"), [[1, 2], [3, 4]])\n    var2 = xr.Variable((\"x\", \"y\"), [[1, 2], [3, 4]])\n    var3 = xr.Variable((\"x\", \"y\"), [[5, 6], [7, 8]])\n\n    # Test when variables are identical\n    assert_identical(merge.unique_variable(\"var\", [var1, var2]), var1)\n\n    # Test when compat is \"override\"\n    assert_identical(merge.unique_variable(\"var\", [var1, var3], compat=\"override\"), var1)\n\n    # Test when variables have no conflicts with compat=\"no_conflicts\"\n    result = merge.unique_variable(\"var\", [var1, var3], compat=\"no_conflicts\")\n    expected = xr.Variable((\"x\", \"y\"), [[1, 2], [3, 4]])\n    assert_identical(result, expected)\n\n    # Test exception when variables are not equal\n    with pytest.raises(merge.MergeError):\n        merge.unique_variable(\"var\", [var1, var3])\n\n    # Test when compat is \"minimal\"\n    var4 = xr.Variable((\"x\", \"z\"), [[1, 2], [3, 4]])\n    result = merge.unique_variable(\"var\", [var1, var4], compat=\"minimal\")\n    assert_identical(result.dims, var1.dims)\n", "    def test_merge_coords(self):\n        data = create_test_data()\n        coords1 = {\"x\": (\"x\", [1, 2]), \"y\": (\"y\", [3, 4])}\n        coords2 = {\"x\": (\"x\", [2, 3]), \"y\": (\"y\", [4, 5])}\n        ds1 = xr.Dataset(coords=coords1)\n        ds2 = xr.Dataset(coords=coords2)\n\n        expected_coords = {\"x\": (\"x\", [1, 2, 3]), \"y\": (\"y\", [3, 4, 5])}\n        expected = xr.Dataset(coords=expected_coords)\n\n        actual = ds1.merge(ds2, compat=\"minimal\")\n        assert expected.identical(actual)\n\n        with pytest.raises(ValueError):\n            ds3 = xr.Dataset(coords={\"x\": (\"x\", [1, 2]), \"y\": (\"z\", [3, 4])})\n            ds1.merge(ds3, compat=\"minimal\")\n"], "sample_598": ["def test_format_timedelta():\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), None, \"10 days 01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), None, \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), None, \"0 days 03:00:00\"),\n        (pd.Timedelta(\"10 days\"), \"date\", \"10 days\"),\n        (pd.Timedelta(\"3 hours\"), \"time\", \"03:00:00\"),\n        (pd.Timedelta(\"1 days 03:00:00\"), \"date\", \"1 days\"),\n        (pd.Timedelta(\"1 days 03:00:00\"), \"time\", \"03:00:00\"),\n        (pd.Timedelta(\"NaT\"), None, \"NaT\"),\n    ]\n    for t, fmt, expected in cases:\n        actual = formatting.format_timedelta(t, fmt)\n        assert expected == actual\n", "def test_wrap_indent():\n    text = \"This is a test\\nfor wrap_indent\\nfunction.\"\n    start = \">> \"\n    length = 3\n    expected = \">> This is a test\\n   for wrap_indent\\n   function.\"\n    actual = formatting.wrap_indent(text, start, length)\n    assert expected == actual\n\n    # Test with default length\n    expected = \">> This is a test\\n   for wrap_indent\\n   function.\"\n    actual = formatting.wrap_indent(text, start)\n    assert expected == actual\n\n    # Test with single line text\n    text = \"Single line text\"\n    expected = \">> Single line text\"\n    actual = formatting.wrap_indent(text, start)\n    assert expected == actual\n\n    # Test with empty text\n    text = \"\"\n    expected = \">> \"\n    actual = formatting.wrap_indent(text, start)\n    assert expected == actual\n", "def test_wrap_indent():\n    text = \"This is a sample text\\nwith multiple lines\\nand some indentation.\"\n    start = \">>> \"\n    length = len(start)\n    expected = \">>> This is a sample text\\n    with multiple lines\\n    and some indentation.\"\n    \n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n\n    text = \"Single line text.\"\n    expected = \">>> Single line text.\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n\n    text = \"\"\n    expected = \">>> \"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n", "def test_wrap_indent():\n    text = \"This is a test string\\nthat spans multiple lines\\nfor testing.\"\n    start = \">>> \"\n    length = len(start)\n\n    actual = formatting.wrap_indent(text, start=start)\n    expected = \">>> This is a test string\\n>>> that spans multiple lines\\n>>> for testing.\"\n    assert actual == expected\n\n    actual = formatting.wrap_indent(text, start=start, length=5)\n    expected = \">>> This is a test string\\n     that spans multiple lines\\n     for testing.\"\n    assert actual == expected\n\n    actual = formatting.wrap_indent(\"Single line text\", start=start)\n    expected = \">>> Single line text\"\n    assert actual == expected\n\n    actual = formatting.wrap_indent(\"\")\n    expected = \"\"\n    assert actual == expected\n", "def test_wrap_indent():\n    cases = [\n        (\"hello world\", \"\", 4, \"hello world\"),\n        (\"hello\\nworld\", \"\", 4, \"hello\\n    world\"),\n        (\"hello\\nworld\", \">> \", None, \">> hello\\n   world\"),\n        (\"hello\\nworld\", \">> \", 4, \">> hello\\n    world\"),\n        (\"\", \">> \", 4, \">> \"),\n        (\"\\n\", \">> \", 4, \">> \\n    \"),\n    ]\n    for text, start, length, expected in cases:\n        actual = formatting.wrap_indent(text, start, length)\n        assert actual == expected\n"], "sample_599": ["def test_CFScaleOffsetCoder_encode_no_scaling_or_offset():\n    original = xr.Variable((\"x\",), np.arange(5), encoding={})\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert_identical(original, encoded)\n", "def test_UnsignedIntegerCoder_roundtrip():\n    original = xr.Variable((\"x\",), [0, 1, 2, 3], {\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n", "def test_CFMaskCoder_encode_no_fill_value():\n    original = xr.Variable((\"x\",), [0, np.nan, 1])\n    coder = variables.CFMaskCoder()\n    encoded = coder.encode(original)\n    assert_identical(original, encoded)\n", "def test_CFMaskCoder_encode_without_fill_values():\n    data = np.array([1, 2, 3, 4], dtype=np.float32)\n    original = xr.Variable((\"x\",), data)\n    coder = variables.CFMaskCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.float32\n    assert \"_FillValue\" not in encoded.attrs\n    assert \"missing_value\" not in encoded.attrs\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n", "def test_CFMaskCoder_conflicting_fill_values():\n    # Test the case where _FillValue and missing_value are both present and conflict\n    original = xr.Variable((\"x\",), [0, -1, 1], {\"_FillValue\": -1, \"missing_value\": -2})\n    coder = variables.CFMaskCoder()\n    \n    with pytest.raises(ValueError, match=\"conflicting _FillValue\"):\n        coder.encode(original)\n"], "sample_600": ["def test_CFScaleOffsetCoder_decode_with_offset():\n    data = np.arange(10, dtype=np.float32)\n    scale_factor = 2.0\n    add_offset = 1.0\n    original = xr.Variable((\"x\",), data, attrs={\"scale_factor\": scale_factor, \"add_offset\": add_offset})\n    coder = variables.CFScaleOffsetCoder()\n    decoded = coder.decode(original)\n    expected_data = data * scale_factor + add_offset\n    expected = xr.Variable((\"x\",), expected_data)\n    assert_identical(decoded, expected)\n", "def test_CFMaskCoder_conflicting_fill_values():\n    data = [0, -1, 1]\n    encoding = {\"_FillValue\": -1, \"missing_value\": -2}\n    variable = xr.Variable((\"x\",), data, encoding=encoding)\n    coder = variables.CFMaskCoder()\n    \n    with pytest.raises(ValueError, match=\"conflicting _FillValue\"):\n        coder.encode(variable)\n", "def test_UnsignedIntegerCoder_encode():\n    unsigned_dtype = np.dtype(\"u2\")\n    signed_dtype = np.dtype(\"i2\")\n    original_values = np.array([0, 1, 2, 3, 65535], dtype=unsigned_dtype)\n    original = xr.Variable((\"x\",), original_values, attrs={\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == signed_dtype\n    assert np.array_equal(\n        encoded.values, original_values.astype(signed_dtype)\n    )\n", "def test_CFMaskCoder_encode_multiple_fill_values_warning():\n    original = xr.Variable((\"x\",), [0, -1, -9999, 1], {\"_FillValue\": [-1, -9999]})\n    coder = variables.CFMaskCoder()\n    with pytest.warns(variables.SerializationWarning, match=\"multiple fill values\"):\n        encoded = coder.encode(original)\n    expected_encoding = {\"_FillValue\": [-1, -9999]}\n    assert encoded.encoding == expected_encoding\n    assert encoded.attrs == {}\n", "def test_CFMaskCoder_encode_with_no_fill_values():\n    original = xr.Variable((\"x\",), [0.0, 1.0, 2.0])\n    expected = xr.Variable((\"x\",), [0.0, 1.0, 2.0])\n    coder = variables.CFMaskCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n"], "sample_601": ["def test_dask_boolean_field_access(field):\n    import dask.array as da\n\n    expected = getattr(self.times_data.dt, field)\n\n    dask_times_arr = da.from_array(self.times_arr, chunks=(5, 5, 50))\n    dask_times_2d = xr.DataArray(\n        dask_times_arr, coords=self.data.coords, dims=self.data.dims, name=\"data\"\n    )\n\n    with raise_if_dask_computes():\n        actual = getattr(dask_times_2d.dt, field)\n\n    assert isinstance(actual.data, da.Array)\n    assert_chunks_equal(actual, dask_times_2d)\n    assert_equal(actual.compute(), expected.compute())\n", "def test_boolean_field_access(data, field):\n    result = getattr(data.time.dt, field)\n    expected = xr.DataArray(\n        getattr(xr.coding.cftimeindex.CFTimeIndex(data.time.values), field),\n        name=field,\n        coords=data.time.coords,\n        dims=data.time.dims,\n    )\n    assert_equal(result, expected)\n", "def test_cftime_strftime(cftime_rounding_dataarray, use_dask):\n    import dask.array as da\n\n    date_format = \"%Y-%m-%d %H:%M:%S\"\n    expected = xr.DataArray(\n        [\n            [\"0001-01-01 01:00:00\", \"0001-01-01 15:00:00\"],\n            [\"0001-01-01 23:00:00\", \"0001-01-02 01:00:00\"],\n        ],\n        name=\"strftime\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.strftime(date_format)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.strftime(date_format)\n\n    assert_identical(result, expected)\n", "def test_cftime_strftime_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    date_format = \"%Y-%m-%d %H:%M:%S\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 1).strftime(date_format), cftime_date_type(1, 1, 1, 15).strftime(date_format)],\n            [cftime_date_type(1, 1, 1, 23).strftime(date_format), cftime_date_type(1, 1, 2, 1).strftime(date_format)],\n        ],\n        name=\"strftime\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.strftime(date_format)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.strftime(date_format)\n\n    assert_identical(result, expected)\n", "def test_cftime_strftime_dask(cftime_rounding_dataarray, use_dask):\n    import dask.array as da\n\n    date_format = \"%Y-%m-%d %H:%M:%S\"\n    expected = xr.DataArray(\n        [\n            [\"0001-01-01 01:00:00\", \"0001-01-01 15:00:00\"],\n            [\"0001-01-01 23:00:00\", \"0001-01-02 01:00:00\"],\n        ],\n        name=\"strftime\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.strftime(date_format)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.strftime(date_format)\n\n    assert_identical(result, expected)\n"], "sample_602": ["def test_load_dataset():\n    data = xr.Dataset(\n        {\n            \"temperature\": (\"time\", [15.5, 17.2, 16.8]),\n            \"precipitation\": (\"time\", [0.5, 0.0, 0.2]),\n        },\n        coords={\"time\": pd.date_range(\"2023-01-01\", periods=3)},\n    )\n    \n    # Create a temporary netCDF file\n    with tempfile.NamedTemporaryFile(suffix=\".nc\", delete=False) as temp_file:\n        data.to_netcdf(temp_file.name)\n        temp_file_path = temp_file.name\n\n    try:\n        loaded_data = load_dataset(temp_file_path)\n        assert_identical(data, loaded_data)\n    finally:\n        os.remove(temp_file_path)\n", "def test_load_dataset():\n    ds = xr.Dataset({\"var\": (\"dim\", np.arange(10))})\n    with xr.backends.MemoryStore() as store:\n        ds.to_netcdf(store)\n        loaded_ds = load_dataset(store)\n        assert_identical(ds, loaded_ds)\n", "def test_open_dataset_invalid_engine():\n    try:\n        xr.open_dataset(\"fake_filename\", engine=\"invalid_engine\")\n    except ValueError as e:\n        assert str(e) == \"unrecognized engine for to_netcdf: 'invalid_engine'\"\n    else:\n        assert False, \"ValueError not raised\"\n", "def test__normalize_path():\n    from xarray.backends.api import _normalize_path\n    \n    # Test with a Path object\n    path_obj = Path(\"/example/path/to/file.nc\")\n    normalized_path = _normalize_path(path_obj)\n    assert normalized_path == str(path_obj)\n\n    # Test with a string\n    path_str = \"~/example/path/to/file.nc\"\n    normalized_path = _normalize_path(path_str)\n    assert normalized_path == os.path.abspath(os.path.expanduser(path_str))\n\n    # Test with a remote URI\n    remote_uri = \"http://example.org/test.nc\"\n    normalized_path = _normalize_path(remote_uri)\n    assert normalized_path == remote_uri\n", "def test__normalize_path():\n    relative_path = \"some/relative/path/file.nc\"\n    absolute_path = os.path.abspath(os.path.expanduser(relative_path))\n\n    assert _normalize_path(relative_path) == absolute_path\n    assert _normalize_path(Path(relative_path)) == absolute_path\n\n    remote_path = \"http://example.org/test.nc\"\n    assert _normalize_path(remote_path) == remote_path\n"], "sample_603": ["def test_collapsible_section():\n    section = fh.collapsible_section(\n        name=\"Test Section\",\n        inline_details=\"Some details\",\n        details=\"Detailed content\",\n        n_items=5,\n        enabled=True,\n        collapsed=True\n    )\n    assert \"Test Section\" in section\n    assert \"Some details\" in section\n    assert \"Detailed content\" in section\n    assert \"5\" in section\n    assert \"type='checkbox' enabled checked\" in section\n\n    section_disabled = fh.collapsible_section(\n        name=\"Test Section\",\n        inline_details=\"Some details\",\n        details=\"Detailed content\",\n        n_items=None,\n        enabled=False,\n        collapsed=False\n    )\n    assert \"disabled\" in section_disabled\n", "def test_collapsible_section():\n    section = fh.collapsible_section(\"Test Section\", inline_details=\"Inline details\", details=\"Full details\", n_items=5, enabled=True, collapsed=True)\n    assert \"Test Section\" in section\n    assert \"Inline details\" in section\n    assert \"Full details\" in section\n    assert \"5\" in section\n    assert \"type='checkbox' \" in section\n    assert \"checked\" not in section\n\n    section_collapsed = fh.collapsible_section(\"Test Section\", inline_details=\"Inline details\", details=\"Full details\", n_items=5, enabled=True, collapsed=False)\n    assert \"checked\" in section_collapsed\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Some details\"\n    details = \"More details\"\n    n_items = 3\n    enabled = True\n    collapsed = False\n    \n    section_html = fh.collapsible_section(\n        name, inline_details=inline_details, details=details, \n        n_items=n_items, enabled=enabled, collapsed=collapsed\n    )\n    \n    assert name in section_html\n    assert inline_details in section_html\n    assert details in section_html\n    assert f\"({n_items})\" in section_html\n    assert \"type='checkbox'\" in section_html\n    assert \"title='Expand/collapse section'\" in section_html\n", "def test_summarize_variable():\n    var = xr.Variable([\"dim1\", \"dim2\"], np.random.rand(2, 3), {\"attr1\": \"value1\"})\n    name = \"test_var\"\n    is_index = True\n    dtype = \"float64\"\n    preview = \"array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\"\n\n    formatted = fh.summarize_variable(name, var, is_index, dtype, preview)\n    assert \"class='xr-has-index'\" in formatted\n    assert \"test_var\" in formatted\n    assert \"(dim1, dim2)\" in formatted\n    assert \"float64\" in formatted\n    assert \"array\" in formatted\n    assert \"attr1\" in formatted\n    assert \"value1\" in formatted\n", "def test_summarize_variable():\n    var = xr.Variable([\"time\", \"x\"], [[1, 2, 3], [4, 5, 6]], {\"foo\": \"bar\"})\n    name = \"test_variable\"\n    formatted = fh.summarize_variable(name, var, is_index=True, dtype=\"int\", preview=\"preview\")\n    \n    assert \"test_variable\" in formatted\n    assert \"class='xr-has-index'\" in formatted\n    assert \"int\" in formatted\n    assert \"preview\" in formatted\n    assert \"foo\" in formatted\n    assert \"bar\" in formatted\n    assert \"attrs-\" in formatted\n    assert \"data-\" in formatted\n"], "sample_604": ["def test_wrap_indent():\n    text = \"This is a test string that will be wrapped.\"\n    start = \"-> \"\n    length = len(start)\n    expected = \"-> This is a test\\n   string that will\\n   be wrapped.\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert actual == expected\n\n    # Test with different length\n    length = 5\n    expected = \"-> This is a test\\n     string that will\\n     be wrapped.\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert actual == expected\n\n    # Test with default length\n    expected = \"-> This is a test\\n   string that will\\n   be wrapped.\"\n    actual = formatting.wrap_indent(text, start=start)\n    assert actual == expected\n\n    # Test with no start\n    expected = \"This is a test\\nstring that will\\nbe wrapped.\"\n    actual = formatting.wrap_indent(text)\n    assert actual == expected\n\n    # Test with empty string\n    expected = \"-> \"\n    actual = formatting.wrap_indent(\"\", start=start)\n    assert actual == expected\n", "def test_wrap_indent():\n    text = \"This is a test string that spans multiple lines\\nwith different indentations\\nand varying lengths\"\n    expected = \"  This is a test string that spans multiple lines\\n  with different indentations\\n  and varying lengths\"\n    actual = formatting.wrap_indent(text, start=\"  \", length=2)\n    assert expected == actual\n\n    text = \"Single line text\"\n    expected = \"-> Single line text\"\n    actual = formatting.wrap_indent(text, start=\"-> \", length=3)\n    assert expected == actual\n", "def test_wrap_indent():\n    cases = [\n        (\"This is a long string that should be wrapped\", \">>> \", None, \">>> This is a long string that should be wrapped\"),\n        (\"Another long string\\nwith a newline\", \">>> \", None, \">>> Another long string\\n    with a newline\"),\n        (\"Short\", \"\", 10, \"Short\"),\n        (\"Multiline\\nstring\\nwrap\", \">> \", 3, \">> Multiline\\n   string\\n   wrap\")\n    ]\n    for text, start, length, expected in cases:\n        actual = formatting.wrap_indent(text, start, length)\n        assert actual == expected\n", "def test_wrap_indent():\n    text = \"This is a sample text\\nwith multiple lines\\nand indentation.\"\n    start = \">> \"\n    length = 4\n    actual = formatting.wrap_indent(text, start, length)\n    expected = \">> This is a sample text\\n    with multiple lines\\n    and indentation.\"\n    assert actual == expected\n\n    # Test with default values\n    actual_default = formatting.wrap_indent(text)\n    expected_default = \"This is a sample text\\nwith multiple lines\\nand indentation.\"\n    assert actual_default == expected_default\n", "def test_format_timedelta():\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), None, \"10 days 01:00:00\"),\n        (pd.Timedelta(\"10 days 1 hour\"), \"date\", \"10 days\"),\n        (pd.Timedelta(\"10 days 1 hour\"), \"time\", \"01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), None, \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), None, \"0 days 03:00:00\"),\n        (pd.Timedelta(\"NaT\"), None, \"NaT\"),\n    ]\n    for timedelta, timedelta_format, expected in cases:\n        actual = formatting.format_timedelta(timedelta, timedelta_format)\n        assert expected == actual\n"], "sample_605": ["def test_groupby_first_last():\n    array = xr.DataArray([1, 2, 3, 4, 5, 6], coords={\"x\": [1, 1, 1, 2, 2, 2]}, dims=\"x\")\n\n    # Test 'first' method\n    expected_first = xr.DataArray([1, 4], coords={\"x\": [1, 2]})\n    actual_first = array.groupby(\"x\").first()\n    assert_identical(expected_first, actual_first)\n\n    # Test 'last' method\n    expected_last = xr.DataArray([3, 6], coords={\"x\": [1, 2]})\n    actual_last = array.groupby(\"x\").last()\n    assert_identical(expected_last, actual_last)\n\n    # Test 'first' method with skipna=False\n    array_with_nan = xr.DataArray([1, np.nan, 3, 4, 5, 6], coords={\"x\": [1, 1, 1, 2, 2, 2]}, dims=\"x\")\n    expected_first_nan = xr.DataArray([1, 4], coords={\"x\": [1, 2]})\n    actual_first_nan = array_with_nan.groupby(\"x\").first(skipna=False)\n    assert_identical(expected_first_nan, actual_first_nan)\n\n    # Test 'last' method with skipna=False\n    expected_last_nan = xr.DataArray([3, 6], coords={\"x\": [1, 2]})\n    actual_last_nan = array_with_nan.groupby(\"x\").last(skipna=False)\n    assert_identical(expected_last_nan, actual_last_nan)\n", "def test_groupby_fillna():\n    array = xr.DataArray(\n        [np.nan, 1, np.nan, 2, np.nan, 3, np.nan, 4, np.nan],\n        coords={\"x\": [0, 0, 0, 1, 1, 1, 2, 2, 2]},\n        dims=\"x\",\n    )\n    expected = xr.DataArray(\n        [0, 1, 0, 2, 2, 2, 4, 4, 4],\n        coords={\"x\": [0, 0, 0, 1, 1, 1, 2, 2, 2]},\n        dims=\"x\",\n    )\n    actual = array.groupby(\"x\").fillna(0)\n    assert_identical(expected, actual)\n\n    # Test fillna with different values for each group\n    fill_values = xr.DataArray([0, 5, 10], coords={\"x\": [0, 1, 2]}, dims=\"x\")\n    expected = xr.DataArray(\n        [0, 1, 0, 2, 5, 5, 10, 4, 10],\n        coords={\"x\": [0, 0, 0, 1, 1, 1, 2, 2, 2]},\n        dims=\"x\",\n    )\n    actual = array.groupby(\"x\").fillna(fill_values)\n    assert_identical(expected, actual)\n", "def test_groupby_first_last():\n    array = xr.DataArray(\n        data=[1, 2, 3, 4, 5, 6], coords={\"x\": [1, 1, 1, 2, 2, 2]}, dims=\"x\"\n    )\n\n    # Test first\n    expected_first = xr.DataArray(data=[1, 4], coords={\"x\": [1, 2]}, dims=\"x\")\n    actual_first = array.groupby(\"x\").first()\n    assert_identical(expected_first, actual_first)\n\n    # Test last\n    expected_last = xr.DataArray(data=[3, 6], coords={\"x\": [1, 2]}, dims=\"x\")\n    actual_last = array.groupby(\"x\").last()\n    assert_identical(expected_last, actual_last)\n\n    # Test first with skipna\n    array_with_nan = xr.DataArray(\n        data=[1, np.nan, 3, 4, np.nan, 6], coords={\"x\": [1, 1, 1, 2, 2, 2]}, dims=\"x\"\n    )\n    expected_first_skipna = xr.DataArray(data=[1, 4], coords={\"x\": [1, 2]}, dims=\"x\")\n    actual_first_skipna = array_with_nan.groupby(\"x\").first(skipna=True)\n    assert_identical(expected_first_skipna, actual_first_skipna)\n\n    # Test last with skipna\n    expected_last_skipna = xr.DataArray(data=[3, 6], coords={\"x\": [1, 2]}, dims=\"x\")\n    actual_last_skipna = array_with_nan.groupby(\"x\").last(skipna=True)\n    assert_identical(expected_last_skipna, actual_last_skipna)\n", "def test_groupby_fillna():\n\n    array = xr.DataArray(\n        data=[1, np.nan, 3, np.nan, 5, 6], coords={\"x\": [1, 1, 1, 2, 2, 2]}, dims=\"x\"\n    )\n\n    # Test fillna with a scalar value\n    expected = xr.DataArray(\n        data=[1, 0, 3, 0, 5, 6], coords={\"x\": [1, 1, 1, 2, 2, 2]}, dims=\"x\"\n    )\n    actual = array.groupby(\"x\").fillna(0)\n    assert_identical(expected, actual)\n\n    # Test fillna with another DataArray\n    fill_value = xr.DataArray(\n        data=[9, 8], coords={\"x\": [1, 2]}, dims=\"x\"\n    )\n    expected = xr.DataArray(\n        data=[1, 9, 3, 8, 5, 6], coords={\"x\": [1, 1, 1, 2, 2, 2]}, dims=\"x\"\n    )\n    actual = array.groupby(\"x\").fillna(fill_value)\n    assert_identical(expected, actual)\n\n    # Test fillna with a Dataset\n    dataset = xr.Dataset(\n        {\"foo\": ((\"x\",), [1, np.nan, 3, np.nan, 5, 6])},\n        {\"x\": [1, 1, 1, 2, 2, 2]},\n    )\n    fill_value_ds = xr.Dataset(\n        {\"foo\": (\"x\", [9, 8])},\n        {\"x\": [1, 2]},\n    )\n    expected = xr.Dataset(\n        {\"foo\": ((\"x\",), [1, 9, 3, 8, 5, 6])},\n        {\"x\": [1, 1, 1, 2, 2, 2]},\n    )\n    actual = dataset.groupby(\"x\").fillna(fill_value_ds)\n    assert_identical(expected, actual)\n", "def test_groupby_fillna():\n\n    array = xr.DataArray(\n        [1, np.nan, 3, np.nan, 5, 6], coords={\"x\": [1, 1, 1, 2, 2, 2]}, dims=\"x\"\n    )\n    expected = xr.DataArray(\n        [1, 1, 3, 5, 5, 6], coords={\"x\": [1, 1, 1, 2, 2, 2]}, dims=\"x\"\n    )\n    actual = array.groupby(\"x\").fillna(5)\n    assert_identical(expected, actual)\n\n    dataset = xr.Dataset(\n        {\"foo\": (\"x\", [1, np.nan, 3, np.nan, 5, 6])}, {\"x\": [1, 1, 1, 2, 2, 2]}\n    )\n    expected_ds = xr.Dataset(\n        {\"foo\": (\"x\", [1, 1, 3, 5, 5, 6])}, {\"x\": [1, 1, 1, 2, 2, 2]}\n    )\n    actual_ds = dataset.groupby(\"x\").fillna(5)\n    assert_identical(expected_ds, actual_ds)\n"], "sample_606": ["def test_apply_ufunc_with_kwargs() -> None:\n        return apply_ufunc(lambda x, y: x * y + c, a, b, kwargs={'c': c})\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    assert_identical(multiply_add(array, array, c=5), array * array + 5)\n    assert_identical(multiply_add(variable, variable, c=5), variable * variable + 5)\n    assert_identical(\n        multiply_add(data_array, data_array, c=5),\n        data_array * data_array + 5\n    )\n    assert_identical(\n        multiply_add(dataset, dataset, c=5),\n        dataset * dataset + 5\n    )\n", "def test_apply_ufunc_with_kwargs() -> None:\n        return a + b + offset\n\n    array1 = np.array([1, 2, 3])\n    array2 = np.array([4, 5, 6])\n    offset_value = 10\n\n    expected_result = array1 + array2 + offset_value\n\n    # Test with plain numpy arrays\n    actual_result = apply_ufunc(add_with_offset, array1, array2, kwargs={\"offset\": offset_value})\n    assert_identical(expected_result, actual_result)\n\n    # Test with xarray.Variable\n    variable1 = xr.Variable(\"x\", array1)\n    variable2 = xr.Variable(\"x\", array2)\n    expected_variable_result = xr.Variable(\"x\", expected_result)\n    actual_variable_result = apply_ufunc(add_with_offset, variable1, variable2, kwargs={\"offset\": offset_value})\n    assert_identical(expected_variable_result, actual_variable_result)\n\n    # Test with xarray.DataArray\n    data_array1 = xr.DataArray(variable1, [(\"x\", -array1)])\n    data_array2 = xr.DataArray(variable2, [(\"x\", -array2)])\n    expected_data_array_result = xr.DataArray(expected_variable_result, [(\"x\", -array1)])\n    actual_data_array_result = apply_ufunc(add_with_offset, data_array1, data_array2, kwargs={\"offset\": offset_value})\n    assert_identical(expected_data_array_result, actual_data_array_result)\n\n    # Test with xarray.Dataset\n    dataset1 = xr.Dataset({\"y\": variable1}, {\"x\": -array1})\n    dataset2 = xr.Dataset({\"y\": variable2}, {\"x\": -array2})\n    expected_dataset_result = xr.Dataset({\"y\": expected_variable_result}, {\"x\": -array1})\n    actual_dataset_result = apply_ufunc(add_with_offset, dataset1, dataset2, kwargs={\"offset\": offset_value})\n    assert_identical(expected_dataset_result, actual_dataset_result)\n\n    # Test with GroupBy objects\n    actual_groupby_result = apply_ufunc(add_with_offset, data_array1.groupby(\"x\"), data_array2.groupby(\"x\"), kwargs={\"offset\": offset_value})\n    assert_identical(expected_data_array_result, actual_groupby_result)\n", "def test_cross_invalid_inputs() -> None:\n    a = xr.DataArray([1, 2, 3])\n    b = xr.DataArray([4, 5])\n    with pytest.raises(ValueError, match=\"must be 1, 2, or 3\"):\n        xr.cross(a, b, dim=\"dim_0\")\n\n    a = xr.DataArray([1, 2])\n    b = xr.DataArray([4, 5, 6, 7])\n    with pytest.raises(ValueError, match=\"must be 1, 2, or 3\"):\n        xr.cross(a, b, dim=\"dim_0\")\n\n    a = xr.DataArray([[1, 2], [3, 4]])\n    b = xr.DataArray([[4, 5], [6, 7]])\n    with pytest.raises(ValueError, match=\"dimension .* not on a\"):\n        xr.cross(a, b, dim=\"dim_0\")\n\n    a = xr.DataArray([[1, 2], [3, 4]], dims=[\"x\", \"y\"])\n    b = xr.DataArray([[4, 5], [6, 7]], dims=[\"x\", \"y\"])\n    with pytest.raises(ValueError, match=\"dimension .* not on b\"):\n        xr.cross(a, b, dim=\"z\")\n", "def test_apply_ufunc_exclude_dims() -> None:\n        return x.sum(axis=-1)\n\n    # Test for excluding dimensions\n    array = np.array([[1, 2, 3], [4, 5, 6]])\n    data_array = xr.DataArray(array, dims=[\"x\", \"y\"])\n    expected = xr.DataArray(array.sum(axis=-1), dims=[\"x\"])\n\n    result = apply_ufunc(func, data_array, input_core_dims=[[\"y\"]], exclude_dims={\"y\"})\n    assert_identical(result, expected)\n\n    # Test for excluding dimensions with multiple inputs\n    array2 = np.array([[6, 5, 4], [3, 2, 1]])\n    data_array2 = xr.DataArray(array2, dims=[\"x\", \"y\"])\n    expected_mult = xr.DataArray((array + array2).sum(axis=-1), dims=[\"x\"])\n\n    result_mult = apply_ufunc(\n        lambda x, y: x + y,\n        data_array,\n        data_array2,\n        input_core_dims=[[\"y\"], [\"y\"]],\n        exclude_dims={\"y\"},\n    )\n    assert_identical(result_mult, expected_mult)\n", "def test_apply_groupby_mismatched_groups() -> None:\n    array = np.arange(5)\n    variable = xr.Variable(\"x\", array)\n    coords = {\"x\": -array, \"y\": (\"x\", [0, 0, 1, 1, 2])}\n    data_array = xr.DataArray(variable, coords, dims=\"x\")\n    dataset = xr.Dataset({\"z\": variable}, coords)\n\n    other_variable = xr.Variable(\"y\", [0, 10])\n    other_data_array = xr.DataArray(other_variable, dims=\"y\")\n\n    with pytest.raises(ValueError, match=\"grouped the same way\"):\n        apply_groupby_func(add, data_array.groupby(\"y\"), data_array[:4].groupby(\"y\"))\n\n    with pytest.raises(ValueError, match=\"grouped the same way\"):\n        apply_groupby_func(add, data_array.groupby(\"y\"), data_array[1:].groupby(\"y\"))\n\n    with pytest.raises(ValueError, match=\"grouped the same way\"):\n        apply_groupby_func(add, data_array.groupby(\"y\"), other_data_array.groupby(\"y\"))\n"], "sample_607": ["def test_detect_parameters():\n        pass\n\n        pass\n\n    # Test with a valid function\n    params = plugins.detect_parameters(valid_open_dataset)\n    assert params == (\"filename_or_obj\", \"decoder\")\n\n    # Test with an invalid function that uses *args\n    with pytest.raises(TypeError, match=r\"\\*args and \\*\\*kwargs is not supported\"):\n        plugins.detect_parameters(invalid_open_dataset)\n\n    # Test with a function with no self parameter\n        pass\n\n    params = plugins.detect_parameters(no_self_open_dataset)\n    assert params == (\"filename_or_obj\", \"decoder\")\n", "def test_detect_parameters():\n        pass\n\n        pass\n\n        pass\n\n    assert plugins.detect_parameters(open_dataset_1) == (\"filename_or_obj\", \"decoder\")\n    assert plugins.detect_parameters(open_dataset_2) == (\"filename_or_obj\", \"decoder\")\n\n    with pytest.raises(TypeError, match=r\"signature should be explicit\"):\n        plugins.detect_parameters(open_dataset_3)\n", "def test_detect_parameters():\n        pass\n\n        pass\n\n        pass\n\n        pass\n\n    parameters = plugins.detect_parameters(open_dataset_valid)\n    assert parameters == (\"filename\", \"decoder\", \"mode\")\n\n    with pytest.raises(TypeError, match=r\"\\*args and \\*\\*kwargs is not supported\"):\n        plugins.detect_parameters(open_dataset_varargs)\n\n    with pytest.raises(TypeError, match=r\"\\*args and \\*\\*kwargs is not supported\"):\n        plugins.detect_parameters(open_dataset_kwargs)\n\n    with pytest.raises(TypeError, match=r\"\\*args and \\*\\*kwargs is not supported\"):\n        plugins.detect_parameters(open_dataset_mixed)\n", "def test_detect_parameters():\n        pass\n\n    params = plugins.detect_parameters(open_dataset_valid)\n    assert params == (\"filename_or_obj\", \"decoder\")\n\n        pass\n\n    params = plugins.detect_parameters(open_dataset_no_params)\n    assert params == ()\n\n        pass\n\n    with pytest.raises(TypeError, match=r\"All the parameters in .* signature should be explicit\"):\n        plugins.detect_parameters(open_dataset_invalid)\n\n        pass\n\n    with pytest.raises(TypeError, match=r\"All the parameters in .* signature should be explicit\"):\n        plugins.detect_parameters(open_dataset_invalid_kwargs)\n", "def test_get_backend():\n    dummy_backend = DummyBackendEntrypoint1()\n    \n    with mock.patch(\"xarray.backends.plugins.list_engines\", return_value={\"dummy\": dummy_backend}):\n        backend = plugins.get_backend(\"dummy\")\n        assert backend is dummy_backend\n\n    with pytest.raises(ValueError, match=r\"unrecognized engine\"):\n        plugins.get_backend(\"non_existent_engine\")\n\n    with pytest.raises(TypeError, match=r\"engine must be a string or a subclass of\"):\n        plugins.get_backend(12345)\n\n    with pytest.raises(TypeError, match=r\"engine must be a string or a subclass of\"):\n        plugins.get_backend(object())\n"], "sample_608": ["def test_wrap_indent() -> None:\n    cases = [\n        (\"single line\", \"\", None, \"single line\"),\n        (\"two\\nlines\", \">>> \", None, \">>> two\\n>>> lines\"),\n        (\"multiple\\nlines\\nwith\\nindent\", \"--\", 4, \"--multiple\\n    lines\\n    with\\n    indent\"),\n    ]\n    for text, start, length, expected in cases:\n        actual = formatting.wrap_indent(text, start, length)\n        assert actual == expected\n", "def test_wrap_indent() -> None:\n    text = \"This is a test string\\nthat will be split\\ninto multiple lines.\"\n    start = \"Prefix: \"\n    expected = \"Prefix: This is a test string\\n        that will be split\\n        into multiple lines.\"\n    actual = formatting.wrap_indent(text, start)\n    assert actual == expected\n\n    text = \"Single line test\"\n    start = \"-> \"\n    expected = \"-> Single line test\"\n    actual = formatting.wrap_indent(text, start)\n    assert actual == expected\n\n    text = \"No prefix test\"\n    expected = \"No prefix test\"\n    actual = formatting.wrap_indent(text)\n    assert actual == expected\n", "def test_format_timedelta() -> None:\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), None, \"10 days 01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), None, \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), None, \"0 days 03:00:00\"),\n        (pd.Timedelta(\"10 days 1 hour\"), \"date\", \"10 days\"),\n        (pd.Timedelta(\"10 days 1 hour\"), \"time\", \"01:00:00\"),\n        (pd.Timedelta(\"NaT\"), None, \"NaT\"),\n    ]\n    for item, timedelta_format, expected in cases:\n        actual = formatting.format_timedelta(item, timedelta_format)\n        assert expected == actual\n", "def test_wrap_indent() -> None:\n    text = \"line1\\nline2\\nline3\"\n    start = \"> \"\n    length = 2\n    expected = \"> line1\\n  line2\\n  line3\"\n    actual = formatting.wrap_indent(text, start, length)\n    assert expected == actual\n\n    text = \"single line\"\n    expected = \"> single line\"\n    actual = formatting.wrap_indent(text, start, length)\n    assert expected == actual\n\n    text = \"line1\\nline2\\nline3\"\n    start = \"\"\n    length = 4\n    expected = \"line1\\n    line2\\n    line3\"\n    actual = formatting.wrap_indent(text, start, length)\n    assert expected == actual\n\n    text = \"line1\\nline2\\nline3\"\n    start = \"> \"\n    expected = \"> line1\\n  line2\\n  line3\"\n    actual = formatting.wrap_indent(text, start)\n    assert expected == actual\n", "def test_wrap_indent() -> None:\n    text = \"This is a test string\\nwith multiple lines\\nto test wrapping.\"\n    expected = \"Start This is a test string\\n      with multiple lines\\n      to test wrapping.\"\n    actual = formatting.wrap_indent(text, start=\"Start \", length=6)\n    assert expected == actual\n\n    text = \"Single line string\"\n    expected = \"Prefix Single line string\"\n    actual = formatting.wrap_indent(text, start=\"Prefix \")\n    assert expected == actual\n\n    text = \"Line 1\\nLine 2\\nLine 3\"\n    expected = \"-> Line 1\\n   Line 2\\n   Line 3\"\n    actual = formatting.wrap_indent(text, start=\"-> \", length=3)\n    assert expected == actual\n\n    text = \"One\\nTwo\\nThree\"\n    expected = \"One\\n  Two\\n  Three\"\n    actual = formatting.wrap_indent(text, start=\"\", length=2)\n    assert expected == actual\n"], "sample_609": ["def test_apply_variable_ufunc() -> None:\n        return x * 2\n\n    array = np.arange(5)\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    # Test for Variable\n    expected_variable = xr.Variable(\"x\", array * 2)\n    result_variable = apply_ufunc(multiply_by_two, variable)\n    assert_identical(expected_variable, result_variable)\n\n    # Test for DataArray\n    expected_data_array = xr.DataArray(expected_variable, [(\"x\", -array)])\n    result_data_array = apply_ufunc(multiply_by_two, data_array)\n    assert_identical(expected_data_array, result_data_array)\n\n    # Test for Dataset\n    expected_dataset = xr.Dataset({\"y\": expected_variable}, {\"x\": -array})\n    result_dataset = apply_ufunc(multiply_by_two, dataset)\n    assert_identical(expected_dataset, result_dataset)\n\n    # Test for GroupBy on DataArray\n    grouped_data_array = data_array.groupby(\"x\")\n    result_grouped_data_array = apply_ufunc(multiply_by_two, grouped_data_array)\n    assert_identical(expected_data_array, result_grouped_data_array)\n\n    # Test for GroupBy on Dataset\n    grouped_dataset = dataset.groupby(\"x\")\n    result_grouped_dataset = apply_ufunc(multiply_by_two, grouped_dataset)\n    assert_identical(expected_dataset, result_grouped_dataset)\n", "def test_apply_groupby_func() -> None:\n    array = np.arange(6).reshape(2, 3)\n    variable = xr.Variable([\"x\", \"y\"], array)\n    coords = {\"x\": [\"a\", \"b\"], \"y\": [0, 0, 1]}\n    data_array = xr.DataArray(variable, coords, dims=[\"x\", \"y\"])\n    groupby = data_array.groupby(\"y\")\n    \n        return x.sum(dim=\"x\")\n    \n    expected = xr.DataArray([1, 5], dims=[\"y\"], coords={\"y\": [0, 1]})\n    result = apply_groupby_func(sum_func, groupby)\n    assert_identical(expected, result)\n", "def test_apply_dataarray_vfunc() -> None:\n        return np.mean(args, axis=0)\n\n    array = np.array([1, 2, 3])\n    data_array = xr.DataArray(array, dims=\"x\")\n    data_array2 = xr.DataArray(array * 2, dims=\"x\")\n\n    result = apply_dataarray_vfunc(mean_func, data_array, data_array2, signature=_UFuncSignature([[\"x\"], [\"x\"]]))\n\n    expected = xr.DataArray([1.5, 3.0, 4.5], dims=\"x\")\n\n    assert_identical(result, expected)\n\n    # Test keep_attrs\n    data_array.attrs[\"test\"] = \"value\"\n    result = apply_dataarray_vfunc(mean_func, data_array, data_array2, signature=_UFuncSignature([[\"x\"], [\"x\"]]), keep_attrs=\"override\")\n    assert result.attrs[\"test\"] == \"value\"\n\n    result = apply_dataarray_vfunc(mean_func, data_array, data_array2, signature=_UFuncSignature([[\"x\"], [\"x\"]]), keep_attrs=\"drop\")\n    assert \"test\" not in result.attrs\n", "def test_apply_custom_function() -> None:\n        return a * b + 2\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    other_array = np.array([4, 5, 6])\n    other_variable = xr.Variable(\"x\", other_array)\n    other_data_array = xr.DataArray(other_variable, [(\"x\", -other_array)])\n    other_dataset = xr.Dataset({\"y\": other_variable}, {\"x\": -other_array})\n\n    expected_array = custom_func(array, other_array)\n    expected_variable = xr.Variable(\"x\", expected_array)\n    expected_data_array = xr.DataArray(expected_variable, [(\"x\", -array)])\n    expected_dataset = xr.Dataset({\"y\": expected_variable}, {\"x\": -array})\n\n    apply_custom_func = functools.partial(apply_ufunc, custom_func)\n\n    assert_identical(expected_array, apply_custom_func(array, other_array))\n    assert_identical(expected_variable, apply_custom_func(variable, other_variable))\n    assert_identical(expected_data_array, apply_custom_func(data_array, other_data_array))\n    assert_identical(expected_dataset, apply_custom_func(dataset, other_dataset))\n", "def test_apply_array_ufunc() -> None:\n    # Test apply_ufunc on plain arrays to ensure apply_array_ufunc is covered\n        return x * y\n\n    array1 = np.array([1, 2, 3])\n    array2 = np.array([4, 5, 6])\n    result = apply_ufunc(multiply, array1, array2)\n\n    expected = np.array([4, 10, 18])\n    assert_array_equal(result, expected)\n\n    # Test handling of dask array without xarray objects\n    if has_dask:\n        import dask.array as da\n\n        dask_array1 = da.from_array(array1, chunks=(3,))\n        dask_array2 = da.from_array(array2, chunks=(3,))\n        with pytest.raises(ValueError, match=\"at least one input is an xarray object\"):\n            apply_ufunc(multiply, dask_array1, dask_array2, dask=\"parallelized\")\n"], "sample_610": ["def test_shift_invalid_freq_string():\n    index = xr.cftime_range(\"2000-01-01\", periods=5, calendar=\"gregorian\")\n    with pytest.raises(ValueError, match=\"Invalid frequency string\"):\n        index.shift(1, \"invalid\")\n", "def test_cftimeindex_constructor_with_invalid_data():\n    import cftime\n\n    invalid_data = [pd.Timestamp('2000-01-01'), '2000-01-02', 123]\n    \n    with pytest.raises(TypeError, match=r\"CFTimeIndex requires cftime.datetime objects. Got object of .*\"):\n        CFTimeIndex(invalid_data)\n\n    valid_cftime_data = [cftime.DatetimeNoLeap(2000, 1, 1), cftime.DatetimeNoLeap(2000, 1, 2)]\n    mixed_data = valid_cftime_data + [pd.Timestamp('2000-01-03')]\n    \n    with pytest.raises(TypeError, match=r\"CFTimeIndex requires using datetime objects of all the same type. Got\\n.*\"):\n        CFTimeIndex(mixed_data)\n", "def test_format_times():\n    index = xr.cftime_range(\"2000-01-01\", periods=20, freq=\"D\")\n    result = format_times(index, max_width=80, offset=2)\n    assert len(result.split(\"\\n\")) == 3  # Should split into 3 lines\n    for line in result.split(\"\\n\"):\n        assert len(line) <= 80  # Each line should respect max_width\n", "def test_cftimeindex_repr_max_length():\n    \"\"\"Test CFTimeIndex repr length when max elements exceed ITEMS_IN_REPR_MAX_ELSE_ELLIPSIS.\"\"\"\n    index = xr.cftime_range(start=\"2000\", periods=ITEMS_IN_REPR_MAX_ELSE_ELLIPSIS + 1, freq=\"D\")\n    repr_str = repr(index)\n    assert \"...\" in repr_str  # Ensure ellipsis is present\n    assert repr_str.count(\", \") <= ITEMS_IN_REPR_MAX_ELSE_ELLIPSIS\n    assert \"length=101\" in repr_str  # Ensure the length is correctly displayed\n", "def test_cftimeindex_repr_truncated():\n    \"\"\"Test that cftimeindex.__repr__ truncates long indices correctly.\"\"\"\n    index = xr.cftime_range(start=\"2000\", periods=150, freq=\"D\")\n    repr_str = index.__repr__()\n    assert \"...\" in repr_str, \"Expected truncated repr with '...'\"\n    assert repr_str.count(\"\\n\") == 3, \"Expected repr to have 3 lines with truncation\"\n"], "sample_611": ["def test_to_offset_tick_frequencies(freq, expected):\n    assert to_offset(freq) == expected\n", "def test_apply_method(calendar, offset, initial_date_args, expected_date_args):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    expected = date_type(*expected_date_args)\n    result = offset.__apply__(initial)\n    assert result == expected\n", "def test_get_day_of_month(calendar, date_args, expected_day):\n    date_type = get_date_type(calendar)\n    date = date_type(*date_args)\n    assert _get_day_of_month(date, \"end\") == expected_day\n", "def test_add_quarter_custom_month(calendar, offset, initial_date_args, expected_date_args):\n    date_type = get_date_type(calendar)\n    initial = date_type(*initial_date_args)\n    result = initial + offset\n    expected = date_type(*expected_date_args)\n    assert result == expected\n", "def test_to_cftime_datetime_with_pandas_dates(calendar, start, expected):\n    result = to_cftime_datetime(pd.Timestamp(start), calendar)\n    assert result.year == expected.year\n    assert result.month == expected.month\n    assert result.day == expected.day\n"], "sample_612": ["def test_groupby_reduce():\n    array = xr.DataArray(\n        np.random.rand(3, 4),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": [\"a\", \"b\", \"c\"], \"y\": [1, 2, 3, 4]}\n    )\n    grouped = array.groupby(\"x\")\n    expected_sum = xr.DataArray(\n        [array.sel(x=\"a\").sum(), array.sel(x=\"b\").sum(), array.sel(x=\"c\").sum()],\n        dims=[\"x\"],\n        coords={\"x\": [\"a\", \"b\", \"c\"]}\n    )\n    expected_mean = xr.DataArray(\n        [array.sel(x=\"a\").mean(), array.sel(x=\"b\").mean(), array.sel(x=\"c\").mean()],\n        dims=[\"x\"],\n        coords={\"x\": [\"a\", \"b\", \"c\"]}\n    )\n\n    assert_identical(grouped.reduce(np.sum), expected_sum)\n    assert_identical(grouped.reduce(np.mean), expected_mean)\n", "def test_groupby_multidim_quantile() -> None:\n    array = DataArray(\n        [[[0, 1], [2, 3]], [[5, 10], [15, 20]]],\n        coords={\n            \"lon\": ([\"ny\", \"nx\"], [[30, 40], [40, 50]]),\n            \"lat\": ([\"ny\", \"nx\"], [[10, 10], [20, 20]]),\n        },\n        dims=[\"time\", \"ny\", \"nx\"],\n    )\n\n    # Quantile over 'lon'\n    expected_quantile_lon = DataArray(\n        [0.5, 7.5, 23.0],\n        coords={\"lon\": [30.0, 40.0, 50.0], \"quantile\": 0.5},\n        dims=[\"lon\"],\n    )\n    actual_quantile_lon = array.groupby(\"lon\").quantile(0.5)\n    assert_identical(expected_quantile_lon, actual_quantile_lon)\n\n    # Quantile over 'lat'\n    expected_quantile_lat = DataArray(\n        [7.5, 17.5],\n        coords={\"lat\": [10.0, 20.0], \"quantile\": 0.5},\n        dims=[\"lat\"],\n    )\n    actual_quantile_lat = array.groupby(\"lat\").quantile(0.5)\n    assert_identical(expected_quantile_lat, actual_quantile_lat)\n", "def test_groupby_repr_empty_group() -> None:\n    array = xr.DataArray([1, 2, 3, 4, 5], dims=\"x\", coords={\"x\": [0, 0, 1, 2, 2]})\n    grouped = array.groupby(\"x\")\n    expected = \"DataArrayGroupBy, grouped over 'x'\\n3 groups with labels 0, 1, 2.\"\n    assert repr(grouped) == expected\n\n    dataset = xr.Dataset({\"foo\": (\"x\", [1, 2, 3, 4, 5])}, coords={\"x\": [0, 0, 1, 2, 2]})\n    grouped = dataset.groupby(\"x\")\n    expected = \"DatasetGroupBy, grouped over 'x'\\n3 groups with labels 0, 1, 2.\"\n    assert repr(grouped) == expected\n", "def test_groupby_maybe_restore_empty_groups() -> None:\n    # Test for _maybe_restore_empty_groups\n    ds = xr.Dataset(\n        {\"foo\": (\"time\", [1, 2, 3, 4])},\n        {\"time\": pd.date_range(\"2000-01-01\", periods=4, freq=\"1D\")}\n    )\n    grouper = ds[\"time\"].dt.day\n    g = ds.groupby(grouper)\n\n    # Simulate dropping of groups\n    g._full_index = pd.Index([1, 2, 3, 4, 5], name=\"day\")\n    reduced = g.mean()\n    \n    # Add the dropped group back\n    actual = g._maybe_restore_empty_groups(reduced)\n    expected = ds.reindex({\"time\": pd.date_range(\"2000-01-01\", periods=5, freq=\"1D\")}).mean()\n    assert_identical(expected, actual)\n", "def test_inverse_permutation_indices() -> None:\n    # Testing with no positions\n    assert _inverse_permutation_indices([]) is None\n\n    # Testing with slices\n    assert np.array_equal(_inverse_permutation_indices([slice(0, 2), slice(2, 4)]), np.array([0, 1, 2, 3]))\n    assert np.array_equal(_inverse_permutation_indices([slice(0, 3), slice(3, 5)]), np.array([0, 1, 2, 3, 4]))\n    assert np.array_equal(_inverse_permutation_indices([slice(1, 4), slice(4, 7)]), np.array([1, 2, 3, 4, 5, 6]))\n\n    # Testing with ndarrays\n    assert np.array_equal(_inverse_permutation_indices([np.array([1, 2]), np.array([0, 3])]), np.array([2, 0, 1, 3]))\n    assert np.array_equal(_inverse_permutation_indices([np.array([2, 0]), np.array([3, 1])]), np.array([1, 3, 0, 2]))\n\n    # Testing with mixed valid slices and ndarrays\n    assert np.array_equal(_inverse_permutation_indices([slice(0, 2), np.array([2, 3])]), np.array([0, 1, 2, 3]))\n\n    # Ensure ValueError is raised for non-slice input\n    with pytest.raises(ValueError, match=r\"list element is not a slice: .*\"):\n        _inverse_permutation_indices([slice(0, 2), 3])\n"], "sample_613": ["def test_groupby_fillna_with_different_dtypes() -> None:\n    # Test fillna with different dtypes\n    ds = xr.Dataset(\n        {\"a\": (\"x\", [np.nan, 1.0, np.nan, 3.0]), \"b\": (\"x\", [np.nan, 1, np.nan, 3])},\n        {\"x\": [0, 1, 2, 3], \"c\": (\"x\", [0, 0, 1, 1])},\n    )\n\n    # Fill with float\n    expected_float = xr.Dataset({\"a\": (\"x\", [0.0, 1.0, 2.0, 3.0]), \"b\": (\"x\", [0, 1, 2, 3])})\n    for target in [ds, expected_float]:\n        target.coords[\"c\"] = (\"x\", [0, 0, 1, 1])\n    actual_float = ds.groupby(\"c\").fillna(xr.Dataset({\"a\": (\"c\", [0.0, 2.0]), \"b\": (\"c\", [0, 2])}))\n    assert_identical(expected_float, actual_float)\n\n    # Fill with integer\n    expected_int = xr.Dataset({\"a\": (\"x\", [0.0, 1.0, 2.0, 3.0]), \"b\": (\"x\", [0, 1, 2, 3])})\n    for target in [ds, expected_int]:\n        target.coords[\"c\"] = (\"x\", [0, 0, 1, 1])\n    actual_int = ds.groupby(\"c\").fillna(xr.Dataset({\"a\": (\"c\", [0, 2]), \"b\": (\"c\", [0, 2])}))\n    assert_identical(expected_int, actual_int)\n\n    # Fill with string\n    ds_str = xr.Dataset({\"a\": (\"x\", [np.nan, 1.0, np.nan, 3.0]), \"b\": (\"x\", [np.nan, 1, np.nan, 3]), \"d\": (\"x\", [\"a\", np.nan, \"b\", np.nan])}, {\"x\": [0, 1, 2, 3], \"c\": (\"x\", [0, 0, 1, 1])})\n    expected_str = xr.Dataset({\"a\": (\"", "def test_groupby_fillna_with_different_shapes() -> None:\n    ds = xr.Dataset(\n        {\"a\": (\"x\", [np.nan, 1, np.nan, 3]), \"b\": (\"x\", [4, np.nan, 6, np.nan])},\n        {\"x\": [0, 1, 2, 3]},\n    )\n\n    fill_value = xr.Dataset({\"a\": (\"x\", [0, 2]), \"b\": (\"x\", [5, 7])})\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [0, 1, 2, 3]), \"b\": (\"x\", [4, 5, 6, 7])},\n        {\"x\": [0, 1, 2, 3]},\n    )\n    actual = ds.groupby(\"x // 2\").fillna(fill_value)\n    assert_identical(expected, actual)\n\n    fill_value = xr.Dataset({\"a\": (\"x\", [0, 2]), \"b\": (\"x\", [5, np.nan])})\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [0, 1, 2, 3]), \"b\": (\"x\", [4, 5, 6, np.nan])},\n        {\"x\": [0, 1, 2, 3]},\n    )\n    actual = ds.groupby(\"x // 2\").fillna(fill_value)\n    assert_identical(expected, actual)\n", "def test_groupby_map_apply_transformations() -> None:\n    array = xr.DataArray([10, 20, 30, 40], dims=\"x\", coords={\"x\": [0, 1, 2, 2]})\n    grouped = array.groupby(\"x\")\n\n        return da * 2\n\n        return da - da.mean()\n\n    # Apply a series of transformations\n    expected = xr.DataArray([20, 40, 60, 60], dims=\"x\", coords={\"x\": [0, 1, 2, 2]})\n    actual = grouped.map(double_values)\n    assert_identical(expected, actual)\n\n    expected = xr.DataArray([0, 0, 0, 0], dims=\"x\", coords={\"x\": [0, 1, 2, 2]})\n    actual = grouped.map(subtract_mean)\n    assert_identical(expected, actual)\n\n    # Apply multiple transformations in sequence\n        return subtract_mean(double_values(da))\n\n    expected = xr.DataArray([0, 0, 0, 0], dims=\"x\", coords={\"x\": [0, 1, 2, 2]})\n    actual = grouped.map(double_and_subtract_mean)\n    assert_identical(expected, actual)\n", "def test_groupby_consolidate_slices() -> None:\n    # Test consolidation of slices with step sizes\n    slices = [slice(0, 3, 2), slice(3, 6, 2)]\n    expected = [slice(0, 6, 2)]\n    assert _consolidate_slices(slices) == expected\n\n    # Test consolidation with mixed step sizes\n    slices = [slice(0, 2, 1), slice(2, 4, 2)]\n    expected = [slice(0, 2, 1), slice(2, 4, 2)]\n    assert _consolidate_slices(slices) == expected\n\n    # Test consolidation with same start and stop but different step\n    slices = [slice(0, 4, 1), slice(4, 8, 2)]\n    expected = [slice(0, 4, 1), slice(4, 8, 2)]\n    assert _consolidate_slices(slices) == expected\n\n    # Test consolidation with overlapping slices\n    slices = [slice(0, 4, 1), slice(3, 8, 1)]\n    expected = [slice(0, 4, 1), slice(3, 8, 1)]\n    assert _consolidate_slices(slices) == expected\n\n    # Test with slice starting from negative index\n    slices = [slice(-3, 0, 1), slice(0, 3, 1)]\n    expected = [slice(-3, 3, 1)]\n    assert _consolidate_slices(slices) == expected\n", "def test_groupby_no_empty_groups() -> None:\n    # test to ensure no empty groups in the result\n    array = xr.DataArray([1, 2, 3, 4, 5, 6], coords={\"x\": [0, 0, 1, 1, 2, 2]}, dims=\"x\")\n    bins = [0, 1, 3]\n    bin_coords = pd.cut(array[\"x\"], bins).categories\n\n    grouped = array.groupby_bins(\"x\", bins)\n\n    # check that there are no empty groups\n    for group in grouped:\n        assert group[1].size > 0\n\n    # actual sum\n    actual = grouped.sum()\n    expected = xr.DataArray([3, 12], coords={\"x_bins\": bin_coords}, dims=\"x_bins\")\n    assert_identical(expected, actual)\n"], "sample_614": ["def test_wrap_indent() -> None:\n    cases = [\n        (\"hello\\nworld\", \"    \", \"    hello\\n    world\"),\n        (\"hello\\nworld\", \"\", \"hello\\nworld\"),\n        (\"hello\\nworld\", \">>\", \">>hello\\n>>world\"),\n        (\"hello world\", \"    \", \"    hello world\"),\n    ]\n    for text, start, expected in cases:\n        actual = formatting.wrap_indent(text, start)\n        assert expected == actual\n", "def test_wrap_indent() -> None:\n    text = \"This is a test string that should be wrapped and indented properly.\"\n    start = \">>> \"\n    length = len(start)\n    expected = \">>> This is a test string\\n    that should be wrapped\\n    and indented properly.\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert actual == expected\n\n    text = \"Single line text\"\n    expected = \">>> Single line text\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert actual == expected\n\n    text = \"Line1\\nLine2\\nLine3\"\n    expected = \">>> Line1\\n    Line2\\n    Line3\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert actual == expected\n", "def test_wrap_indent() -> None:\n    text = \"This is a test string that will be wrapped and indented.\"\n    expected = \"Start\\n     This\\n     is\\n     a\\n     test\\n     string\\n     that\\n     will\\n     be\\n     wrapped\\n     and\\n     indented.\"\n    actual = formatting.wrap_indent(text, start=\"Start\", length=5)\n    assert actual == expected\n\n    text = \"Another example with different indentation.\"\n    expected = \"Indent\\n        Another\\n        example\\n        with\\n        different\\n        indentation.\"\n    actual = formatting.wrap_indent(text, start=\"Indent\", length=8)\n    assert actual == expected\n\n    text = \"Single line\"\n    expected = \"Prefix\\n      Single line\"\n    actual = formatting.wrap_indent(text, start=\"Prefix\", length=6)\n    assert actual == expected\n", "def test_wrap_indent() -> None:\n    text = \"This is a long text that needs to be wrapped properly.\"\n    start = \"Prefix: \"\n    length = len(start)\n    expected = \"Prefix: This is a\\n        long text\\n        that needs\\n        to be wrapped\\n        properly.\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n\n    text = \"Short text\"\n    expected = \"Prefix: Short text\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n\n    text = \"Single\\nLine\\nText\"\n    expected = \"Prefix: Single\\n        Line\\n        Text\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n\n    text = \"\"\n    expected = \"Prefix: \"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n", "def test_format_timedelta() -> None:\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), None, \"10 days 01:00:00\"),\n        (pd.Timedelta(\"10 days 1 hour\"), \"date\", \"10 days\"),\n        (pd.Timedelta(\"10 days 1 hour\"), \"time\", \"01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), None, \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), None, \"0 days 03:00:00\"),\n        (pd.Timedelta(\"NaT\"), None, \"NaT\"),\n    ]\n    for timedelta, format, expected in cases:\n        actual = formatting.format_timedelta(timedelta, timedelta_format=format)\n        assert expected == actual\n"], "sample_615": ["def test_cross_invalid_dim() -> None:\n    a = xr.DataArray([1, 2, 3], dims=[\"dim_0\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"dim_0\"])\n    with pytest.raises(ValueError, match=\"Dimension 'invalid_dim' not on a\"):\n        xr.cross(a, b, dim=\"invalid_dim\")\n    with pytest.raises(ValueError, match=\"Dimension 'invalid_dim' not on b\"):\n        xr.cross(b, a, dim=\"invalid_dim\")\n", "def test_apply_variable_ufunc() -> None:\n    array = np.array([[1, 2, 3], [4, 5, 6]])\n    variable = xr.Variable([\"x\", \"y\"], array)\n    data_array = xr.DataArray(variable, {\"x\": [\"a\", \"b\"], \"y\": [-1, -2, -3]})\n\n        return x + 1\n\n    expected = xr.Variable([\"x\", \"y\"], array + 1)\n    result = apply_variable_ufunc(increment, variable, signature=_UFuncSignature([[\"x\", \"y\"]]))\n\n    assert_identical(result, expected)\n\n    expected_da = data_array + 1\n    result_da = apply_variable_ufunc(increment, data_array, signature=_UFuncSignature([[\"x\", \"y\"]]))\n\n    assert_identical(result_da, expected_da)\n", "def test_apply_array_ufunc_handling() -> None:\n        return x + y\n\n    array1 = np.array([1, 2, 3])\n    array2 = np.array([4, 5, 6])\n    result = apply_array_ufunc(custom_ufunc, array1, array2)\n    expected = np.array([5, 7, 9])\n    assert_identical(result, expected)\n\n    with pytest.raises(ValueError, match=\"handling for dask arrays has not been enabled\"):\n        apply_array_ufunc(custom_ufunc, array1, array2, dask=\"parallelized\")\n\n    with pytest.raises(ValueError, match=\"unknown setting for dask array handling\"):\n        apply_array_ufunc(custom_ufunc, array1, array2, dask=\"unknown\")\n", "def test_apply_ufunc_with_kwargs() -> None:\n    # This test checks that apply_ufunc correctly passes kwargs to the function\n\n        return x * scale\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    scale = 2\n\n    expected_variable = xr.Variable(\"x\", array * scale)\n    expected_data_array = xr.DataArray(expected_variable, [(\"x\", -array)])\n    expected_dataset = xr.Dataset({\"y\": expected_variable}, {\"x\": -array})\n\n    # Apply function with kwargs to variable\n    actual = apply_ufunc(func_with_kwargs, variable, kwargs={\"scale\": scale})\n    assert_identical(expected_variable, actual)\n\n    # Apply function with kwargs to data_array\n    actual = apply_ufunc(func_with_kwargs, data_array, kwargs={\"scale\": scale})\n    assert_identical(expected_data_array, actual)\n\n    # Apply function with kwargs to dataset\n    actual = apply_ufunc(func_with_kwargs, dataset, kwargs={\"scale\": scale})\n    assert_identical(expected_dataset, actual)\n\n    # Apply function with kwargs to groupby object\n    actual = apply_ufunc(func_with_kwargs, data_array.groupby(\"x\"), kwargs={\"scale\": scale})\n    assert_identical(expected_data_array, actual)\n\n    actual = apply_ufunc(func_with_kwargs, dataset.groupby(\"x\"), kwargs={\"scale\": scale})\n    assert_identical(expected_dataset, actual)\n", "def test_build_output_coords_and_indexes() -> None:\n    var1 = xr.Variable((\"x\", \"y\"), np.random.rand(4, 5))\n    var2 = xr.Variable((\"x\", \"y\"), np.random.rand(4, 5))\n    da1 = xr.DataArray(var1, coords={\"x\": range(4), \"y\": range(5)})\n    da2 = xr.DataArray(var2, coords={\"x\": range(4), \"y\": range(5)})\n    \n    signature = _UFuncSignature(input_core_dims=[[\"x\", \"y\"], [\"x\", \"y\"]], output_core_dims=[[\"x\", \"y\"]])\n    exclude_dims = frozenset([\"y\"])\n    coords, indexes = build_output_coords_and_indexes([da1, da2], signature, exclude_dims)\n    \n    expected_coords = [{\"x\": da1.coords[\"x\"], \"y\": da1.coords[\"y\"]}]\n    expected_indexes = [{}]\n    \n    assert len(coords) == 1\n    assert len(indexes) == 1\n    assert_identical(coords[0], expected_coords[0])\n    assert indexes[0] == expected_indexes[0]\n"], "sample_616": ["def test_unified_dim_sizes_exceptions() -> None:\n    # Test for duplicate dimensions\n    with pytest.raises(ValueError, match=\"duplicate dimensions\"):\n        unified_dim_sizes([xr.Variable((\"x\", \"x\"), [[1, 2], [3, 4]])])\n    \n    # Test for mismatched lengths\n    with pytest.raises(ValueError, match=\"operands cannot be broadcast together\"):\n        unified_dim_sizes([xr.Variable(\"x\", [1, 2]), xr.Variable(\"x\", [1, 2, 3])])\n    \n    # Test for multiple mismatches with exclude_dims\n    with pytest.raises(ValueError, match=\"operands cannot be broadcast together\"):\n        unified_dim_sizes(\n            [xr.Variable((\"x\", \"y\"), [[1, 2], [3, 4]]), xr.Variable((\"y\", \"z\"), [[1, 2], [3, 4]])],\n            exclude_dims={\"y\"}\n        )\n", "def test_cross_invalid_dimension() -> None:\n    # Test for invalid dimension\n    a = xr.DataArray([1, 2, 3], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"x\")\n\n    with pytest.raises(ValueError, match=\"Dimension 'invalid_dim' not on a\"):\n        xr.cross(a, b, dim=\"invalid_dim\")\n\n    # Test for dimension not present in one of the arrays\n    a = xr.DataArray([1, 2, 3], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"y\")\n\n    with pytest.raises(ValueError, match=\"Dimension 'x' not on b\"):\n        xr.cross(a, b, dim=\"x\")\n", "def test_cross_product_exceptions() -> None:\n    a = xr.DataArray([1, 2, 3], dims=\"dim_0\")\n    b = xr.DataArray([4, 5, 6], dims=\"dim_1\")\n\n    # Dimension not in array 'a'\n    with pytest.raises(ValueError, match=\"Dimension 'dim_1' not on a\"):\n        xr.cross(a, b, dim=\"dim_1\")\n\n    # Dimension not in array 'b'\n    with pytest.raises(ValueError, match=\"Dimension 'dim_0' not on b\"):\n        xr.cross(b, a, dim=\"dim_0\")\n\n    # Size of dimension in 'a' not in [1, 2, 3]\n    a_invalid = xr.DataArray([1, 2, 3, 4], dims=\"dim_0\")\n    with pytest.raises(ValueError, match=\"size of 'dim_0' on a must be 1, 2, or 3\"):\n        xr.cross(a_invalid, b, dim=\"dim_0\")\n\n    # Size of dimension in 'b' not in [1, 2, 3]\n    b_invalid = xr.DataArray([4, 5, 6, 7], dims=\"dim_1\")\n    with pytest.raises(ValueError, match=\"size of 'dim_1' on b must be 1, 2, or 3\"):\n        xr.cross(a, b_invalid, dim=\"dim_1\")\n\n    # Dimensions without coordinates must have length of 2 or 3\n    a_1d = xr.DataArray([1], dims=\"dim_0\")\n    with pytest.raises(ValueError, match=\"dimensions without coordinates must have have a length of 2 or 3\"):\n        xr.cross(a_1d, b, dim=\"dim_0\")\n", "def test_apply_dataset_vfunc() -> None:\n        return x * 2\n\n    ds = xr.Dataset(\n        {\"a\": (\"x\", np.arange(3)), \"b\": (\"y\", np.arange(4))},\n        coords={\"x\": [0, 1, 2], \"y\": [3, 4, 5, 6]},\n    )\n\n    result = apply_dataset_vfunc(multiply, ds, signature=_UFuncSignature([[], []]))\n    expected = xr.Dataset(\n        {\"a\": (\"x\", np.arange(3) * 2), \"b\": (\"y\", np.arange(4) * 2)},\n        coords={\"x\": [0, 1, 2], \"y\": [3, 4, 5, 6]},\n    )\n\n    assert_identical(result, expected)\n", "def test_apply_array_ufunc() -> None:\n        return x + 1\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    assert_identical(add_one(array), apply_ufunc(add_one, array))\n    assert_identical(xr.Variable(\"x\", add_one(array)), apply_ufunc(add_one, variable))\n    assert_identical(data_array + 1, apply_ufunc(add_one, data_array))\n    assert_identical(dataset + 1, apply_ufunc(add_one, dataset))\n\n    # Test with Dask arrays if Dask is available\n    if has_dask:\n        import dask.array as da\n\n        dask_array = da.from_array(array, chunks=2)\n        dask_variable = xr.Variable(\"x\", dask_array)\n        dask_data_array = xr.DataArray(dask_variable, [(\"x\", -array)])\n        dask_dataset = xr.Dataset({\"y\": dask_variable}, {\"x\": -array})\n\n        assert_identical(add_one(dask_array), apply_ufunc(add_one, dask_array, dask=\"allowed\"))\n        assert_identical(xr.Variable(\"x\", add_one(dask_array)), apply_ufunc(add_one, dask_variable, dask=\"allowed\"))\n        assert_identical(dask_data_array + 1, apply_ufunc(add_one, dask_data_array, dask=\"allowed\"))\n        assert_identical(dask_dataset + 1, apply_ufunc(add_one, dask_dataset, dask=\"allowed\"))\n"], "sample_617": ["def test_apply_array_ufunc() -> None:\n    # Test apply_ufunc on plain numpy arrays, which should just directly apply the function\n    array1 = np.array([1, 2, 3])\n    array2 = np.array([4, 5, 6])\n    expected = np.add(array1, array2)\n    result = apply_ufunc(np.add, array1, array2)\n    assert_array_equal(result, expected)\n\n    # Test with scalars\n    scalar1 = 3\n    scalar2 = 4\n    expected = np.add(scalar1, scalar2)\n    result = apply_ufunc(np.add, scalar1, scalar2)\n    assert result == expected\n\n    # Test with a mix of numpy arrays and scalars\n    expected = np.add(array1, scalar1)\n    result = apply_ufunc(np.add, array1, scalar1)\n    assert_array_equal(result, expected)\n\n    expected = np.add(scalar2, array2)\n    result = apply_ufunc(np.add, scalar2, array2)\n    assert_array_equal(result, expected)\n\n    # Test with incompatible shapes, should raise ValueError\n    array3 = np.array([[1, 2], [3, 4]])\n    with pytest.raises(ValueError, match=\"operands could not be broadcast together\"):\n        apply_ufunc(np.add, array1, array3)\n", "def test_apply_array_ufunc() -> None:\n        return a + b\n\n    array1 = np.array([1, 2, 3])\n    array2 = np.array([4, 5, 6])\n    array3 = np.array([7, 8, 9])\n\n    # Test with numpy arrays\n    result = apply_array_ufunc(add, array1, array2)\n    expected = np.array([5, 7, 9])\n    assert_array_equal(result, expected)\n\n    # Test with a single numpy array and a scalar\n    result = apply_array_ufunc(add, array1, 5)\n    expected = np.array([6, 7, 8])\n    assert_array_equal(result, expected)\n\n    # Test with multiple numpy arrays\n        return a + b + c\n\n    result = apply_array_ufunc(add_three, array1, array2, array3)\n    expected = np.array([12, 15, 18])\n    assert_array_equal(result, expected)\n    \n    # Test with nested ufunc calls\n    result = apply_array_ufunc(add, apply_array_ufunc(add, array1, array2), array3)\n    expected = np.array([12, 15, 18])\n    assert_array_equal(result, expected)\n\n    # Test invalid dask setting\n    with pytest.raises(ValueError, match=r\"unknown setting for dask array handling\"):\n        apply_array_ufunc(add, array1, array2, dask=\"unknown\")\n", "def test_apply_ufunc_output_dtypes() -> None:\n        return x + y, x * y\n\n    array = np.array([1, 2, 3], dtype=np.int32)\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    expected_sum = xr.DataArray([2, 4, 6], dims=[\"x\"])\n    expected_prod = xr.DataArray([1, 4, 9], dims=[\"x\"])\n\n    out_sum, out_prod = apply_ufunc(\n        func,\n        data_array,\n        data_array,\n        output_core_dims=[[], []],\n        output_dtypes=[np.int32, np.int32]\n    )\n\n    assert_identical(expected_sum, out_sum)\n    assert_identical(expected_prod, out_prod)\n    assert out_sum.dtype == np.int32\n    assert out_prod.dtype == np.int32\n\n    out_sum, out_prod = apply_ufunc(\n        func,\n        data_array,\n        data_array,\n        output_core_dims=[[], []],\n        output_dtypes=[np.float64, np.float64]\n    )\n\n    assert out_sum.dtype == np.float64\n    assert out_prod.dtype == np.float64\n", "def test_apply_variable_ufunc() -> None:\n        return arr + 1\n\n        return arr + 2\n\n    var = xr.Variable(\"x\", np.array([1, 2, 3]))\n    data_array = xr.DataArray(var, dims=\"x\")\n    dataset = xr.Dataset({\"data\": data_array})\n\n    expected_var = xr.Variable(\"x\", np.array([2, 3, 4]))\n    expected_data_array = xr.DataArray(expected_var, dims=\"x\")\n    expected_dataset = xr.Dataset({\"data\": expected_data_array})\n\n    result_var = apply_variable_ufunc(add_one, var, signature=_UFuncSignature([[\"x\"]]))\n    result_data_array = apply_variable_ufunc(add_one, data_array, signature=_UFuncSignature([[\"x\"]]))\n    result_dataset = apply_variable_ufunc(add_one, dataset[\"data\"], signature=_UFuncSignature([[\"x\"]]))\n\n    assert_identical(result_var, expected_var)\n    assert_identical(result_data_array, expected_data_array)\n    assert_identical(result_dataset, expected_data_array)\n\n    expected_var = xr.Variable(\"x\", np.array([3, 4, 5]))\n    expected_data_array = xr.DataArray(expected_var, dims=\"x\")\n    expected_dataset = xr.Dataset({\"data\": expected_data_array})\n\n    result_var = apply_variable_ufunc(add_two, var, signature=_UFuncSignature([[\"x\"]]))\n    result_data_array = apply_variable_ufunc(add_two, data_array, signature=_UFuncSignature([[\"x\"]]))\n    result_dataset = apply_variable_ufunc(add_two, dataset[\"data\"], signature=_UFuncSignature([[\"x\"]]))\n\n    assert_identical(result_var, expected_var)\n    assert_identical(result_data_array, expected_data_array)\n    assert_identical(result_dataset, expected_data_array)\n", "def test_broadcast_compat_data_core_dims_mismatch() -> None:\n    data = np.arange(12).reshape(3, 4)\n    var = xr.Variable([\"x\", \"y\"], data)\n\n    # Core dims mismatch scenario: core_dims contains a dim not in var.dims\n    with pytest.raises(ValueError, match=r\"operand to apply_ufunc has required core dimensions\"):\n        broadcast_compat_data(var, (\"x\",), (\"y\", \"z\"))\n        \n    # Another mismatch scenario: core_dims contains duplicate dimensions\n    with pytest.raises(ValueError, match=r\"duplicate dimensions on a variable\"):\n        broadcast_compat_data(var, (\"x\", \"y\"), (\"y\", \"y\"))\n"], "sample_618": ["def test_apply_func_with_kwargs() -> None:\n        return x * y + offset\n\n    array1 = np.array([1, 2, 3])\n    array2 = np.array([4, 5, 6])\n    expected = array1 * array2 + 10\n\n    variable1 = xr.Variable(\"x\", array1)\n    variable2 = xr.Variable(\"x\", array2)\n    expected_variable = xr.Variable(\"x\", expected)\n\n    data_array1 = xr.DataArray(variable1, [(\"x\", -array1)])\n    data_array2 = xr.DataArray(variable2, [(\"x\", -array2)])\n    expected_data_array = xr.DataArray(expected_variable, [(\"x\", -array1)])\n\n    dataset1 = xr.Dataset({\"y\": variable1}, {\"x\": -array1})\n    dataset2 = xr.Dataset({\"y\": variable2}, {\"x\": -array2})\n    expected_dataset = xr.Dataset({\"y\": expected_variable}, {\"x\": -array1})\n\n    apply_multiply_with_offset = functools.partial(apply_ufunc, multiply_with_offset, kwargs={\"offset\": 10})\n\n    assert_identical(expected, apply_multiply_with_offset(array1, array2))\n    assert_identical(expected_variable, apply_multiply_with_offset(variable1, variable2))\n    assert_identical(expected_data_array, apply_multiply_with_offset(data_array1, data_array2))\n    assert_identical(expected_dataset, apply_multiply_with_offset(dataset1, dataset2))\n", "def test_apply_ufunc_meta_argument() -> None:\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n\n        return x\n\n    # Test with meta argument\n    actual = apply_ufunc(identity_meta, variable, meta=np.array([], dtype=int))\n    assert_identical(variable, actual)\n\n    # Test with dask array and meta argument\n    if has_dask:\n        import dask.array as da\n\n        dask_array = da.from_array(array, chunks=2)\n        dask_variable = xr.Variable(\"x\", dask_array)\n        actual_dask = apply_ufunc(identity_meta, dask_variable, dask=\"allowed\", meta=np.array([], dtype=int))\n        assert isinstance(actual_dask.data, da.Array)\n        assert_identical(variable, actual_dask.compute())\n", "def test_apply_ufunc_with_keep_attrs_callable() -> None:\n        return apply_ufunc(operator.add, a, b, keep_attrs=lambda attrs, context: {\"sum_attrs\": True})\n\n    a = xr.DataArray([0, 1], [(\"x\", [0, 1])])\n    a.attrs[\"attr\"] = \"da\"\n    b = xr.DataArray([1, 2], [(\"x\", [0, 1])])\n\n    actual = add(a, b)\n    assert actual.attrs == {\"sum_attrs\": True}\n\n    actual = add(a.variable, b.variable)\n    assert actual.attrs == {\"sum_attrs\": True}\n\n    ds_a = xr.Dataset({\"x\": [0, 1]})\n    ds_a.attrs[\"attr\"] = \"ds\"\n    ds_b = xr.Dataset({\"x\": [0, 1]})\n\n    actual = add(ds_a, ds_b)\n    assert actual.attrs == {\"sum_attrs\": True}\n", "def test_apply_ufunc_with_kwargs() -> None:\n        return x * factor\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    factor = 10\n\n    expected_array = array * factor\n    expected_variable = xr.Variable(\"x\", expected_array)\n    expected_data_array = xr.DataArray(expected_variable, [(\"x\", -array)])\n    expected_dataset = xr.Dataset({\"y\": expected_variable}, {\"x\": -array})\n\n    apply_multiply_with_factor = functools.partial(apply_ufunc, multiply_with_factor, kwargs={\"factor\": factor})\n\n    assert_identical(expected_array, apply_multiply_with_factor(array))\n    assert_identical(expected_variable, apply_multiply_with_factor(variable))\n    assert_identical(expected_data_array, apply_multiply_with_factor(data_array))\n    assert_identical(expected_dataset, apply_multiply_with_factor(dataset))\n\n    assert_identical(expected_data_array, apply_multiply_with_factor(data_array.groupby(\"x\")))\n    assert_identical(expected_dataset, apply_multiply_with_factor(dataset.groupby(\"x\")))\n", "def test_apply_ufunc_with_core_dims_and_output_sizes() -> None:\n        return x + y, x * y\n\n    array1 = xr.DataArray(np.array([[1, 2], [3, 4]]), dims=[\"x\", \"y\"])\n    array2 = xr.DataArray(np.array([[10, 20], [30, 40]]), dims=[\"x\", \"y\"])\n\n    expected_sum = xr.DataArray(np.array([[11, 22], [33, 44]]), dims=[\"x\", \"y\"])\n    expected_product = xr.DataArray(np.array([[10, 40], [90, 160]]), dims=[\"x\", \"y\"])\n\n    sum_result, product_result = apply_ufunc(\n        func,\n        array1,\n        array2,\n        input_core_dims=[[\"y\"], [\"y\"]],\n        output_core_dims=[[\"y\"], [\"y\"]],\n        output_sizes={\"y\": 2}\n    )\n\n    assert_identical(sum_result, expected_sum)\n    assert_identical(product_result, expected_product)\n"], "sample_619": ["def test_convert_time_or_go_back() -> None:\n    # Test converting dates to a new date type, potentially going back a day or two if invalid.\n    import cftime\n\n    date = cftime.DatetimeNoLeap(2000, 2, 29)  # A leap day in a leap year\n    date_type = cftime.Datetime360Day\n\n    # Check conversion with valid date\n    converted_date = coding.times.convert_time_or_go_back(date, date_type)\n    assert converted_date == date_type(2000, 2, 29)\n\n    # Check conversion with invalid date (2000-02-29 doesn't exist in 360-day calendar)\n    date = cftime.DatetimeNoLeap(2001, 2, 29)\n    converted_date = coding.times.convert_time_or_go_back(date, date_type)\n    assert converted_date == date_type(2001, 2, 28)  # Goes back one day\n\n    # Check conversion with invalid date (2001-02-29 doesn't exist in the 360-day calendar, needs to go back two days)\n    date = cftime.DatetimeNoLeap(2001, 3, 1)\n    converted_date = coding.times.convert_time_or_go_back(date, date_type)\n    assert converted_date == date_type(2001, 2, 28)  # Goes back two days\n", "def test_format_cftime_datetime_invalid_date_type() -> None:\n    # Test to ensure format_cftime_datetime raises an error on invalid date type\n    invalid_date = \"2000-01-01\"\n    with pytest.raises(AttributeError, match=\"object has no attribute 'year'\"):\n        coding.times.format_cftime_datetime(invalid_date)\n", "def test_cleanup_netcdf_time_units(units, ref_date, expected) -> None:\n    input_units = f\"{units} since {ref_date}\"\n    assert coding.times._cleanup_netcdf_time_units(input_units) == expected\n", "def test_infer_calendar_name(calendar) -> None:\n    import cftime\n\n    date_type = _all_cftime_date_types()[calendar]\n    dates = [date_type(2000, 1, 1), date_type(2000, 1, 2)]\n    inferred_calendar = coding.times.infer_calendar_name(dates)\n    assert inferred_calendar == calendar\n\n    numpy_dates = pd.date_range(\"2000-01-01\", periods=2).to_numpy()\n    inferred_calendar = coding.times.infer_calendar_name(numpy_dates)\n    assert inferred_calendar == \"proleptic_gregorian\"\n", "def test_convert_times_with_ambiguous_date(calendar) -> None:\n    from cftime import DatetimeProlepticGregorian\n\n    # Test for conversion with an ambiguous date which falls back a day\n    times = [DatetimeProlepticGregorian(2001, 2, 28, 0, 0)]\n    target_calendar = \"noleap\"  # noleap doesn't have Feb 29\n    result = convert_times(times, target_calendar)\n    expected = [\n        cftime.DatetimeNoLeap(2001, 2, 28, 0, 0)\n    ]  # Should convert to noleap Feb 28\n    assert_array_equal(result, expected)\n\n    # Test for conversion with an ambiguous date which falls back two days\n    times = [DatetimeProlepticGregorian(2001, 3, 1, 0, 0)]\n    target_calendar = \"noleap\"\n    result = convert_times(times, target_calendar)\n    expected = [\n        cftime.DatetimeNoLeap(2001, 2, 28, 0, 0)\n    ]  # Should convert to noleap Feb 28\n    assert_array_equal(result, expected)\n"], "sample_620": ["def test_concat_different_compat_options() -> None:\n    ds1 = Dataset({\"var\": (\"x\", [1, 2, 3])})\n    ds2 = Dataset({\"var\": (\"x\", [4, 5, 6])})\n\n    # Test for 'identical' compat option\n    result = concat([ds1, ds2], dim=\"x\", compat=\"identical\")\n    expected = Dataset({\"var\": (\"x\", [1, 2, 3, 4, 5, 6])})\n    assert_identical(result, expected)\n\n    # Test for 'equals' compat option\n    ds3 = Dataset({\"var\": (\"x\", [4, 5, 6], {\"attr\": \"value\"})})\n    result = concat([ds1, ds3], dim=\"x\", compat=\"equals\")\n    expected = Dataset({\"var\": (\"x\", [1, 2, 3, 4, 5, 6], {\"attr\": \"value\"})})\n    assert_identical(result, expected)\n\n    # Test for 'broadcast_equals' compat option\n    ds4 = Dataset({\"var\": (\"x\", [4, 5, 6], {\"attr\": \"value\", \"extra\": \"info\"})})\n    result = concat([ds1, ds4], dim=\"x\", compat=\"broadcast_equals\")\n    expected = Dataset({\"var\": (\"x\", [1, 2, 3, 4, 5, 6], {\"attr\": \"value\", \"extra\": \"info\"})})\n    assert_identical(result, expected)\n\n    # Test for 'no_conflicts' compat option\n    ds5 = Dataset({\"var\": (\"x\", [4, 5, 6], {\"attr\": \"different\"})})\n    result = concat([ds1, ds5], dim=\"x\", compat=\"no_conflicts\")\n    expected = Dataset({\"var\": (\"x\", [1, 2, 3, 4, 5, 6], {\"attr\": \"different\"})})\n    assert_identical(result, expected)\n\n    # Test for 'override' compat option\n    ds6 = Dataset({\"var\": (\"x\", [4, 5, 6], {\"attr\": \"different\"})})\n    result = concat([ds1, ds6], dim=\"x\", compat=\"override\")\n    expected = Dataset({\"var\": (\"x\", [1, 2,", "def test_concat_empty_datasets() -> None:\n    # Test concatenation when input datasets are empty\n    ds1 = Dataset()\n    ds2 = Dataset()\n    \n    result = concat([ds1, ds2], dim=\"x\")\n    expected = Dataset()\n    assert_identical(result, expected)\n\n    # Test concatenation with one empty dataset and one non-empty dataset\n    ds3 = Dataset({\"foo\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]})\n    result = concat([ds1, ds3], dim=\"x\")\n    assert_identical(result, ds3)\n\n    result = concat([ds3, ds1], dim=\"x\")\n    assert_identical(result, ds3)\n\n    # Test concatenation with multiple dimensions in an empty dataset\n    ds4 = Dataset({\"bar\": ((\"x\", \"y\"), [[1, 2], [3, 4]])}, coords={\"x\": [0, 1], \"y\": [0, 1]})\n    ds_empty_multi_dim = Dataset(coords={\"x\": [], \"y\": []})\n    result = concat([ds_empty_multi_dim, ds4], dim=\"x\")\n    assert_identical(result, ds4)\n", "def test_concat_with_scalar_coords() -> None:\n    ds1 = Dataset({\"a\": (\"x\", [1, 2])}, coords={\"x\": [0, 1], \"scalar_coord\": 1})\n    ds2 = Dataset({\"a\": (\"x\", [3, 4])}, coords={\"x\": [2, 3]})\n\n    expected = Dataset(\n        {\"a\": (\"x\", [1, 2, 3, 4])},\n        coords={\"x\": [0, 1, 2, 3], \"scalar_coord\": 1}\n    )\n\n    actual = concat([ds1, ds2], dim=\"x\", coords=\"minimal\")\n    assert_identical(expected, actual)\n\n    with pytest.raises(ValueError, match=r\"coordinates in some datasets but not others\"):\n        concat([ds1, ds2], dim=\"x\", coords=\"all\")\n", "def test_concat_with_new_and_existing_dimension() -> None:\n    # Test concatenating along a new dimension\n    ds1 = Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4])}, coords={\"x\": [10, 20]})\n    ds2 = Dataset({\"a\": (\"x\", [5, 6]), \"b\": (\"x\", [7, 8])}, coords={\"x\": [30, 40]})\n\n    # Concatenating along a new dimension\n    expected_new_dim = Dataset(\n        {\"a\": ((\"new_dim\", \"x\"), [[1, 2], [5, 6]]), \"b\": ((\"new_dim\", \"x\"), [[3, 4], [7, 8]])},\n        coords={\"x\": [10, 20]}\n    )\n    actual_new_dim = concat([ds1, ds2], dim=\"new_dim\")\n    assert_identical(expected_new_dim, actual_new_dim)\n\n    # Concatenating along an existing dimension with a new coordinate\n    new_dim_coord = pd.Index([100, 200], name=\"new_dim\")\n    expected_new_dim_coord = Dataset(\n        {\"a\": ((\"new_dim\", \"x\"), [[1, 2], [5, 6]]), \"b\": ((\"new_dim\", \"x\"), [[3, 4], [7, 8]])},\n        coords={\"x\": [10, 20], \"new_dim\": [100, 200]}\n    )\n    actual_new_dim_coord = concat([ds1, ds2], dim=new_dim_coord)\n    assert_identical(expected_new_dim_coord, actual_new_dim_coord)\n", "def test_concat_with_scalar_coordinates() -> None:\n    # Test concatenation with scalar coordinates\n    ds1 = Dataset({\"foo\": ((\"x\", \"y\"), [[1, 2], [3, 4]])}, coords={\"scalar\": 1})\n    ds2 = Dataset({\"foo\": ((\"x\", \"y\"), [[5, 6], [7, 8]])}, coords={\"scalar\": 1})\n    ds3 = Dataset({\"foo\": ((\"x\", \"y\"), [[9, 10], [11, 12]])}, coords={\"scalar\": 2})\n\n    expected = Dataset(\n        {\"foo\": ((\"z\", \"x\", \"y\"), [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]])},\n        coords={\"scalar\": (\"z\", [1, 1, 2])}\n    )\n    \n    actual = concat([ds1, ds2, ds3], dim=\"z\", coords=\"all\")\n    assert_identical(actual, expected)\n"], "sample_621": ["def test_pandas_index_isel(self) -> None:\n    pd_idx = pd.Index([1, 2, 3], name=\"foo\")\n    index = PandasIndex(pd_idx, \"x\")\n\n    # Test slice indexer\n    result = index.isel({\"x\": slice(1, 3)})\n    expected = PandasIndex(pd.Index([2, 3], name=\"foo\"), \"x\")\n    assert result.equals(expected)\n\n    # Test int indexer\n    result = index.isel({\"x\": 1})\n    assert result is None\n\n    # Test array indexer\n    result = index.isel({\"x\": np.array([0, 2])})\n    expected = PandasIndex(pd.Index([1, 3], name=\"foo\"), \"x\")\n    assert result.equals(expected)\n\n    # Test Variable indexer\n    var_indexer = Variable(\"x\", np.array([0, 2]))\n    result = index.isel({\"x\": var_indexer})\n    expected = PandasIndex(pd.Index([1, 3], name=\"foo\"), \"x\")\n    assert result.equals(expected)\n", "def test_custom_index_inheritance(self) -> None:\n    class MyCustomIndex(Index):\n            self.data = data\n\n    index1 = MyCustomIndex([1, 2, 3])\n    index2 = MyCustomIndex([1, 2, 3])\n    index3 = MyCustomIndex([4, 5, 6])\n\n    assert isinstance(index1, Index)\n    assert index1.equals(index2) is False  # since .equals is not implemented\n    assert index1.equals(index3) is False  # since .equals is not implemented\n\n    with pytest.raises(NotImplementedError):\n        index1.from_variables({})\n    with pytest.raises(NotImplementedError):\n        index1.concat([], \"x\")\n    with pytest.raises(NotImplementedError):\n        index1.stack({}, \"x\")\n    with pytest.raises(NotImplementedError):\n        index1.sel({})\n    with pytest.raises(NotImplementedError):\n        index1.join(index3)\n    with pytest.raises(NotImplementedError):\n        index1.reindex_like(index3)\n    with pytest.raises(NotImplementedError):\n        index1.unstack()\n    with pytest.raises(NotImplementedError):\n        index1.__getitem__(0)\n", "def test_isel(self) -> None:\n    pd_idx = pd.Index([10, 20, 30], name=\"index\")\n    index = PandasIndex(pd_idx, \"dim\", coord_dtype=np.int32)\n\n    # Test slicing\n    actual = index.isel({\"dim\": slice(1, 3)})\n    expected = PandasIndex(pd.Index([20, 30], name=\"index\"), \"dim\", coord_dtype=np.int32)\n    assert actual.equals(expected)\n\n    # Test integer indexing\n    actual = index.isel({\"dim\": 1})\n    assert actual is None  # Should return None for scalar indexing\n\n    # Test array indexing\n    actual = index.isel({\"dim\": np.array([0, 2])})\n    expected = PandasIndex(pd.Index([10, 30], name=\"index\"), \"dim\", coord_dtype=np.int32)\n    assert actual.equals(expected)\n\n    # Test Variable indexing\n    var_indexer = Variable((\"dim\",), np.array([0, 2]))\n    actual = index.isel({\"dim\": var_indexer})\n    assert actual.equals(expected)\n", "def test_isel(self) -> None:\n    index = PandasIndex(pd.Index([1, 2, 3]), \"x\")\n    # Test with slice indexer\n    result = index.isel({\"x\": slice(1, 3)})\n    expected = PandasIndex(pd.Index([2, 3]), \"x\")\n    assert result.equals(expected)\n\n    # Test with integer indexer\n    result = index.isel({\"x\": 1})\n    assert result is None\n\n    # Test with ndarray indexer\n    result = index.isel({\"x\": np.array([0, 2])})\n    expected = PandasIndex(pd.Index([1, 3]), \"x\")\n    assert result.equals(expected)\n\n    # Test with variable indexer\n    var_indexer = Variable(\"x\", np.array([0, 2]))\n    result = index.isel({\"x\": var_indexer})\n    assert result.equals(expected)\n\n    # Test with variable indexer with new dimensions\n    var_indexer = Variable((\"y\", \"z\"), np.array([[0, 1], [1, 2]]))\n    result = index.isel({\"x\": var_indexer})\n    assert result is None\n", "def test_pandasindex_isel() -> None:\n    index = PandasIndex(pd.Index([1, 2, 3, 4]), \"x\")\n\n    # Test with a slice\n    actual = index.isel({\"x\": slice(1, 3)})\n    expected = PandasIndex(pd.Index([2, 3]), \"x\")\n    assert actual.equals(expected)\n\n    # Test with an integer index\n    actual = index.isel({\"x\": 2})\n    assert actual is None  # Integer index should drop the index\n\n    # Test with a numpy array index\n    actual = index.isel({\"x\": np.array([0, 3])})\n    expected = PandasIndex(pd.Index([1, 4]), \"x\")\n    assert actual.equals(expected)\n\n    # Test with a Variable index\n    var_index = Variable(\"x\", np.array([0, 2]))\n    actual = index.isel({\"x\": var_index})\n    expected = PandasIndex(pd.Index([1, 3]), \"x\")\n    assert actual.equals(expected)\n\n    # Test with a scalar Variable index\n    scalar_var_index = Variable(\"x\", np.array(2))\n    actual = index.isel({\"x\": scalar_var_index})\n    assert actual is None\n"], "sample_622": ["def test_ensure_not_multiindex():\n    multiindex = pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]], names=[\"first\", \"second\"])\n    index_var = IndexVariable(\"multiindex\", multiindex)\n    \n    with pytest.raises(NotImplementedError, match=\"MultiIndex\"):\n        conventions.ensure_not_multiindex(index_var, name=\"multiindex\")\n\n    single_index = pd.Index([1, 2, 3], name=\"singleindex\")\n    index_var = IndexVariable(\"singleindex\", single_index)\n    \n    # This should not raise an error\n    try:\n        conventions.ensure_not_multiindex(index_var, name=\"singleindex\")\n    except NotImplementedError:\n        pytest.fail(\"ensure_not_multiindex() raised NotImplementedError unexpectedly!\")\n", "def test_ensure_not_multiindex() -> None:\n    # Create a MultiIndex pandas IndexVariable\n    multi_index = pd.MultiIndex.from_tuples(\n        [(\"a\", 1), (\"b\", 2), (\"c\", 3)], names=[\"level_1\", \"level_2\"]\n    )\n    index_var = IndexVariable([\"x\"], multi_index)\n\n    # Ensure NotImplementedError is raised when trying to encode MultiIndex\n    with pytest.raises(NotImplementedError, match=\"variable 'multi' is a MultiIndex\"):\n        conventions.ensure_not_multiindex(index_var, name=\"multi\")\n\n    # Test with a normal IndexVariable\n    normal_index = pd.Index([\"a\", \"b\", \"c\"], name=\"level_1\")\n    index_var_normal = IndexVariable([\"x\"], normal_index)\n    try:\n        conventions.ensure_not_multiindex(index_var_normal, name=\"normal\")\n    except NotImplementedError:\n        pytest.fail(\"ensure_not_multiindex raised NotImplementedError unexpectedly!\")\n", "def test_encode_cf_variable_bool():\n    # Test encoding of boolean variables\n    original = Variable([\"x\"], np.array([True, False, True]))\n    encoded = conventions.encode_cf_variable(original)\n    expected_data = np.array([1, 0, 1], dtype=\"i1\")\n    assert_array_equal(encoded.data, expected_data)\n    assert encoded.attrs[\"dtype\"] == \"bool\"\n", "def test_ensure_not_multiindex() -> None:\n    # Test for ensure_not_multiindex function\n    # Create an IndexVariable with MultiIndex\n    index = pd.MultiIndex.from_tuples([('a', 1), ('b', 2)], names=['letters', 'numbers'])\n    var = IndexVariable('index', index)\n    \n    with pytest.raises(NotImplementedError, match=\"variable 'index' is a MultiIndex\"):\n        conventions.ensure_not_multiindex(var, name='index')\n\n    # Create an IndexVariable without MultiIndex\n    index = pd.Index([1, 2, 3], name='numbers')\n    var = IndexVariable('index', index)\n    \n    # This should not raise any error\n    conventions.ensure_not_multiindex(var, name='index')\n", "def test_ensure_not_multiindex():\n    index = pd.MultiIndex.from_arrays([np.arange(3), np.arange(3)])\n    var = IndexVariable(\"x\", index)\n    with pytest.raises(NotImplementedError, match=\"variable 'x' is a MultiIndex\"):\n        conventions.ensure_not_multiindex(var, name=\"x\")\n\n    single_index_var = Variable(\"x\", np.arange(3))\n    # This should not raise an error\n    conventions.ensure_not_multiindex(single_index_var, name=\"x\")\n"], "sample_623": ["def test__get_default_engine_missing_dependencies() -> None:\n    import sys\n    import importlib\n\n    original_modules = sys.modules.copy()\n\n    try:\n        sys.modules['netCDF4'] = None\n        sys.modules['scipy'] = None\n        sys.modules['pydap'] = None\n\n        with pytest.raises(ValueError, match=\"netCDF4 or pydap is required for accessing remote datasets via OPeNDAP\"):\n            _get_default_engine(\"http://example.org/test.nc\", allow_remote=True)\n\n        with pytest.raises(ValueError, match=\"scipy is required for accessing .gz files\"):\n            _get_default_engine(\"/example.gz\")\n\n        with pytest.raises(ValueError, match=\"cannot read or write netCDF files without netCDF4-python or scipy installed\"):\n            _get_default_engine(\"/example\")\n    finally:\n        sys.modules.clear()\n        sys.modules.update(original_modules)\n", "def test__validate_attrs() -> None:\n    from xarray.backends.api import _validate_attrs\n    import xarray as xr\n\n    # Valid dataset with different attribute types\n    ds = xr.Dataset()\n    ds.attrs['valid_str'] = 'attribute'\n    ds.attrs['valid_number'] = 42\n    ds.attrs['valid_ndarray'] = np.array([1, 2, 3])\n    ds.attrs['valid_list'] = [1, 2, 'three']\n    ds.attrs['valid_tuple'] = (1, 2, 'three')\n    ds.attrs['valid_bool'] = True\n\n    _validate_attrs(ds, invalid_netcdf=False)\n\n    # Invalid attribute key (non-string)\n    ds.attrs[123] = 'invalid'\n    with pytest.raises(TypeError):\n        _validate_attrs(ds, invalid_netcdf=False)\n    del ds.attrs[123]\n\n    # Invalid attribute value (unsupported type)\n    ds.attrs['invalid_attr'] = set([1, 2, 3])\n    with pytest.raises(TypeError):\n        _validate_attrs(ds, invalid_netcdf=False)\n    del ds.attrs['invalid_attr']\n\n    # Valid numpy.bool_ attribute for h5netcdf engine with invalid_netcdf=True\n    ds.attrs['valid_bool_'] = np.bool_(True)\n    _validate_attrs(ds, invalid_netcdf=True)\n\n    # Invalid numpy.bool_ attribute for other engines\n    with pytest.raises(TypeError):\n        _validate_attrs(ds, invalid_netcdf=False)\n", "def test__get_default_engine_no_remote_support() -> None:\n    with pytest.raises(ValueError, match=\"netCDF4 or pydap is required for accessing remote datasets via OPeNDAP\"):\n        _get_default_engine(\"http://example.org/test.nc\", allow_remote=True, engine=\"dummy_engine\")\n", "def test__validate_dataset_names() -> None:\n    valid_dataset = xr.Dataset({\"valid_name\": (\"dim\", [1, 2, 3])})\n    invalid_dataset_empty_string = xr.Dataset({\"\": (\"dim\", [1, 2, 3])})\n    invalid_dataset_non_string = xr.Dataset({123: (\"dim\", [1, 2, 3])})\n\n    # Should not raise any exceptions\n    _validate_dataset_names(valid_dataset)\n\n    # Should raise ValueError for empty string name\n    with pytest.raises(ValueError, match=\"Invalid name '' for DataArray or Dataset key\"):\n        _validate_dataset_names(invalid_dataset_empty_string)\n\n    # Should raise TypeError for non-string and non-None name\n    with pytest.raises(TypeError, match=\"Invalid name 123 for DataArray or Dataset key\"):\n        _validate_dataset_names(invalid_dataset_non_string)\n\n", "def test_open_dataset_with_invalid_engine() -> None:\n    with pytest.raises(ValueError, match=\"unrecognized engine for to_netcdf\"):\n        xr.open_dataset(\"fake_filename\", engine=\"invalid_engine\")\n"], "sample_624": ["def test_wrap_indent() -> None:\n    cases = [\n        (\"simple text\", \"prefix: \", 10, \"prefix: simple text\"),\n        (\"multi\\nline\\ntext\", \"start: \", None, \"start: multi\\n       line\\n       text\"),\n        (\"indented\\ntext\\nblock\", \"prefix\", 6, \"prefixindented\\n      text\\n      block\"),\n        (\"no new line\", \"\", None, \"no new line\"),\n    ]\n    for text, start, length, expected in cases:\n        assert formatting.wrap_indent(text, start, length) == expected\n", "def test_wrap_indent() -> None:\n    text = \"This is a sample text that needs to be wrapped.\"\n    start = \"Start: \"\n    length = len(start)\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    expected = \"Start: This is a sample text that needs to be wrapped.\"\n    assert actual == expected\n\n    text = \"This is a sample text\\nthat needs to be wrapped.\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    expected = \"Start: This is a sample text\\n       that needs to be wrapped.\"\n    assert actual == expected\n\n    text = \"This is a sample text\\nthat needs to\\nbe wrapped.\"\n    actual = formatting.wrap_indent(text, start=start)\n    expected = \"Start: This is a sample text\\n       that needs to\\n       be wrapped.\"\n    assert actual == expected\n", "def test_wrap_indent() -> None:\n    cases = [\n        (\"Hello\\nWorld\", \"\", 4, \"Hello\\n    World\"),\n        (\"Hello\\nWorld\", \">>\", 2, \">>Hello\\n>>  World\"),\n        (\"Hello\\nWorld\", \">>\", None, \">>Hello\\n  World\"),\n        (\"SingleLine\", \">>\", 2, \">>SingleLine\"),\n        (\"Line1\\nLine2\\nLine3\", \">>\", 2, \">>Line1\\n>>  Line2\\n>>  Line3\"),\n    ]\n    for text, start, length, expected in cases:\n        actual = formatting.wrap_indent(text, start, length)\n        assert actual == expected\n", "def test_wrap_indent() -> None:\n    cases = [\n        (\"Hello\\nWorld\", \"  \", 2, \"  Hello\\n  World\"),\n        (\"Hello World\", \">>>\", 3, \">>>Hello World\"),\n        (\"A\\nB\\nC\", \"\", 1, \"A\\n B\\n C\"),\n        (\"\", \">>>\", 3, \">>>\")\n    ]\n    for text, start, length, expected in cases:\n        actual = formatting.wrap_indent(text, start, length)\n        assert expected == actual\n", "def test_wrap_indent() -> None:\n    text = \"This is a test string that will be wrapped and indented.\"\n    start = \"Start: \"\n    length = len(start)\n    \n    expected = \"Start: This is a test string\\n       that will be wrapped and\\n       indented.\"\n    actual = formatting.wrap_indent(text, start=start, length=length)\n    assert expected == actual\n\n    # Test without providing length\n    expected = \"Start: This is a test string\\n       that will be wrapped and\\n       indented.\"\n    actual = formatting.wrap_indent(text, start=start)\n    assert expected == actual\n\n    # Test with different text and start\n    text = \"Another example of text that needs to be wrapped appropriately.\"\n    start = \"Begin: \"\n    \n    expected = \"Begin: Another example of text\\n       that needs to be wrapped\\n       appropriately.\"\n    actual = formatting.wrap_indent(text, start=start, length=len(start))\n    assert expected == actual\n\n    # Test with multiline text\n    text = \"First line\\nSecond line\\nThird line\"\n    start = \"> \"\n    \n    expected = \"> First line\\n  Second line\\n  Third line\"\n    actual = formatting.wrap_indent(text, start=start)\n    assert expected == actual\n"], "sample_625": ["def test_apply_ufunc_with_kwargs() -> None:\n        return a + b + offset\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    zero_array = np.zeros_like(array)\n    zero_variable = xr.Variable(\"x\", zero_array)\n    zero_data_array = xr.DataArray(zero_variable, [(\"x\", -array)])\n    zero_dataset = xr.Dataset({\"y\": zero_variable}, {\"x\": -array})\n\n    assert_identical(array + 5, apply_ufunc(add_with_offset, array, zero_array, kwargs={'offset': 5}))\n    assert_identical(variable + 5, apply_ufunc(add_with_offset, variable, zero_variable, kwargs={'offset': 5}))\n    assert_identical(data_array + 5, apply_ufunc(add_with_offset, data_array, zero_data_array, kwargs={'offset': 5}))\n    assert_identical(dataset + 5, apply_ufunc(add_with_offset, dataset, zero_dataset, kwargs={'offset': 5}))\n", "def test_apply_ufunc_with_kwargs() -> None:\n        return x + y + offset\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, [(\"x\", -array)])\n    dataset = xr.Dataset({\"y\": variable}, {\"x\": -array})\n\n    zero_array = np.zeros_like(array)\n    zero_variable = xr.Variable(\"x\", zero_array)\n    zero_data_array = xr.DataArray(zero_variable, [(\"x\", -array)])\n    zero_dataset = xr.Dataset({\"y\": zero_variable}, {\"x\": -array})\n\n    expected = array + 5\n    assert_identical(expected, apply_ufunc(add_with_offset, array, zero_array, kwargs={\"offset\": 5}))\n    assert_identical(expected, apply_ufunc(add_with_offset, zero_array, array, kwargs={\"offset\": 5}))\n\n    expected = variable + 5\n    assert_identical(expected, apply_ufunc(add_with_offset, variable, zero_variable, kwargs={\"offset\": 5}))\n    assert_identical(expected, apply_ufunc(add_with_offset, zero_variable, variable, kwargs={\"offset\": 5}))\n\n    expected = data_array + 5\n    assert_identical(expected, apply_ufunc(add_with_offset, data_array, zero_data_array, kwargs={\"offset\": 5}))\n    assert_identical(expected, apply_ufunc(add_with_offset, zero_data_array, data_array, kwargs={\"offset\": 5}))\n\n    expected = dataset + 5\n    assert_identical(expected, apply_ufunc(add_with_offset, dataset, zero_dataset, kwargs={\"offset\": 5}))\n    assert_identical(expected, apply_ufunc(add_with_offset, zero_dataset, dataset, kwargs={\"offset\": 5}))\n", "def test_apply_dataarray_vfunc() -> None:\n    # Test data\n    data = np.array([[1, 2], [3, 4]])\n    coords = {'x': ['a', 'b'], 'y': [-1, -2]}\n    data_array = xr.DataArray(data, dims=['x', 'y'], coords=coords)\n\n    # Function to be applied\n        return -x\n\n    # Expected result\n    expected_data = -data\n    expected_data_array = xr.DataArray(expected_data, dims=['x', 'y'], coords=coords)\n\n    # Apply the function using apply_dataarray_vfunc\n    result = apply_dataarray_vfunc(negate, data_array, signature=_UFuncSignature([['x', 'y']]))\n\n    # Assert the result is as expected\n    assert_identical(result, expected_data_array)\n", "def test_apply_dataarray_vfunc() -> None:\n    data_array = xr.DataArray(np.random.rand(3, 4), dims=[\"x\", \"y\"])\n    signature = _UFuncSignature([[\"x\", \"y\"]], [[\"x\", \"y\"]])\n\n        return arr * 2\n\n    result = apply_dataarray_vfunc(\n        func,\n        data_array,\n        signature=signature\n    )\n\n    expected = data_array * 2\n    assert_identical(result, expected)\n", "def test_apply_dataarray_vfunc() -> None:\n    data_array = xr.DataArray([1, 2, 3], dims=\"x\", name=\"data\")\n    zero_data_array = xr.DataArray([0, 0, 0], dims=\"x\", name=\"data\")\n\n        return apply_ufunc(operator.mul, a, b, keep_attrs=True)\n\n    expected = xr.DataArray([0, 0, 0], dims=\"x\", name=\"data\")\n    actual = multiply(data_array, zero_data_array)\n    assert_identical(expected, actual)\n\n    # Test with different name\n    zero_data_array.name = \"new_data\"\n    expected = xr.DataArray([0, 0, 0], dims=\"x\", name=None)\n    actual = multiply(data_array, zero_data_array)\n    assert_identical(expected, actual)\n"], "sample_626": ["def test_copy_on_write_array() -> None:\n    original_array = np.array([1, 2, 3])\n    cow_array = CopyOnWriteArray(original_array)\n\n    # Test accessing the data\n    np.testing.assert_array_equal(np.array(cow_array), original_array)\n\n    # Modify the copy and check that original is unchanged\n    cow_array[1] = 10\n    np.testing.assert_array_equal(np.array(cow_array), [1, 10, 3])\n    np.testing.assert_array_equal(original_array, [1, 2, 3])\n\n    # Ensure that deepcopy works as intended\n    copied = copy.deepcopy(cow_array)\n    assert isinstance(copied, CopyOnWriteArray)\n    np.testing.assert_array_equal(np.array(copied), np.array(cow_array))\n", "def test_merge_sel_results() -> None:\n    index_sel_result_1 = IndexSelResult(\n        dim_indexers={\"x\": [0, 1], \"y\": slice(None)},\n        indexes={\"x\": CustomIndex({\"x\": 2})},\n        variables={\"var1\": xr.Variable(\"x\", [1, 2])},\n        drop_coords=[\"coord1\"],\n        drop_indexes=[\"index1\"],\n        rename_dims={\"old_dim\": \"new_dim\"}\n    )\n    index_sel_result_2 = IndexSelResult(\n        dim_indexers={\"z\": [2, 3], \"y\": slice(1, 3)},\n        indexes={\"z\": CustomIndex({\"z\": 2})},\n        variables={\"var2\": xr.Variable(\"z\", [3, 4])},\n        drop_coords=[\"coord2\"],\n        drop_indexes=[\"index2\"],\n        rename_dims={\"another_old_dim\": \"another_new_dim\"}\n    )\n    \n    merged_result = merge_sel_results([index_sel_result_1, index_sel_result_2])\n    \n    expected_dim_indexers = {\n        \"x\": [0, 1],\n        \"y\": slice(1, 3),\n        \"z\": [2, 3]\n    }\n    expected_indexes = {\n        \"x\": CustomIndex({\"x\": 2}),\n        \"z\": CustomIndex({\"z\": 2})\n    }\n    expected_variables = {\n        \"var1\": xr.Variable(\"x\", [1, 2]),\n        \"var2\": xr.Variable(\"z\", [3, 4])\n    }\n    expected_drop_coords = [\"coord1\", \"coord2\"]\n    expected_drop_indexes = [\"index1\", \"index2\"]\n    expected_rename_dims = {\n        \"old_dim\": \"new_dim\",\n        \"another_old_dim\": \"another_new_dim\"\n    }\n    \n    assert merged_result.dim_indexers == expected_dim_indexers\n    assert merged_result.indexes.keys() == expected_indexes.keys()\n    assert all([expected_indexes[k].dims == v.dims for k, v in merged_result.indexes.items()])\n    assert merged_result.variables.keys() == expected_variables.keys()\n    assert all([expected_variables[k].dims == v.dims for k, v in merged_result.variables.items()])\n    assert merged_result.drop_coords == expected_drop_coords\n    assert merged_result.drop_indexes == expected_drop_indexes\n    assert merged_result.rename_dims == expected_rename_dims\n", "def test_explicit_indexing_adapter_basic() -> None:\n    array = np.arange(100).reshape(10, 10)\n    indexer = BasicIndexer((slice(1, 5), slice(2, 6)))\n\n        return array[key]\n\n    result = explicit_indexing_adapter(indexer, array.shape, IndexingSupport.BASIC, raw_indexing_method)\n    expected = array[1:5, 2:6]\n    np.testing.assert_array_equal(result, expected)\n", "def test_merge_sel_results():\n    # Create some IndexSelResult objects with different attributes\n    result1 = IndexSelResult(\n        dim_indexers={\"x\": [0, 1]},\n        indexes={\"x\": Index()},\n        variables={\"var1\": Variable(\"x\", [0, 1])},\n        drop_coords=[\"coord1\"],\n        drop_indexes=[\"index1\"],\n        rename_dims={\"old_dim1\": \"new_dim1\"},\n    )\n    result2 = IndexSelResult(\n        dim_indexers={\"y\": [2, 3]},\n        indexes={\"y\": Index()},\n        variables={\"var2\": Variable(\"y\", [2, 3])},\n        drop_coords=[\"coord2\"],\n        drop_indexes=[\"index2\"],\n        rename_dims={\"old_dim2\": \"new_dim2\"},\n    )\n\n    # Merge the results\n    merged_result = merge_sel_results([result1, result2])\n\n    # Check that the merged attributes are correct\n    assert merged_result.dim_indexers == {\"x\": [0, 1], \"y\": [2, 3]}\n    assert \"x\" in merged_result.indexes and \"y\" in merged_result.indexes\n    assert \"var1\" in merged_result.variables and \"var2\" in merged_result.variables\n    assert merged_result.drop_coords == [\"coord1\", \"coord2\"]\n    assert merged_result.drop_indexes == [\"index1\", \"index2\"]\n    assert merged_result.rename_dims == {\"old_dim1\": \"new_dim1\", \"old_dim2\": \"new_dim2\"}\n\n    # Test for ValueError with duplicate dimensions\n    result3 = IndexSelResult(\n        dim_indexers={\"x\": [4, 5]},\n        indexes={},\n        variables={},\n        drop_coords=[],\n        drop_indexes=[],\n        rename_dims={},\n    )\n    with pytest.raises(ValueError, match=\"Xarray does not support label-based selection with more than one index\"):\n        merge_sel_results([result1, result3])\n", "def test_merge_sel_results() -> None:\n    result1 = IndexSelResult(\n        dim_indexers={\"dim1\": [0, 1]},\n        indexes={\"index1\": \"idx1\"},\n        variables={\"var1\": \"value1\"},\n        drop_coords=[\"coord1\"],\n        drop_indexes=[\"index1\"],\n        rename_dims={\"old_dim1\": \"new_dim1\"},\n    )\n\n    result2 = IndexSelResult(\n        dim_indexers={\"dim2\": [2, 3]},\n        indexes={\"index2\": \"idx2\"},\n        variables={\"var2\": \"value2\"},\n        drop_coords=[\"coord2\"],\n        drop_indexes=[\"index2\"],\n        rename_dims={\"old_dim2\": \"new_dim2\"},\n    )\n\n    merged_result = merge_sel_results([result1, result2])\n\n    assert merged_result.dim_indexers == {\"dim1\": [0, 1], \"dim2\": [2, 3]}\n    assert merged_result.indexes == {\"index1\": \"idx1\", \"index2\": \"idx2\"}\n    assert merged_result.variables == {\"var1\": \"value1\", \"var2\": \"value2\"}\n    assert merged_result.drop_coords == [\"coord1\", \"coord2\"]\n    assert merged_result.drop_indexes == [\"index1\", \"index2\"]\n    assert merged_result.rename_dims == {\"old_dim1\": \"new_dim1\", \"old_dim2\": \"new_dim2\"}\n"], "sample_627": ["def test_concat_mixed_types_compat() -> None:\n    ds1 = Dataset(\n        {\n            \"temp\": (\"x\", [1.1, 2.2]),\n            \"time\": (\"x\", pd.date_range(\"2023-01-01\", periods=2)),\n        },\n        coords={\"x\": [0, 1]},\n    )\n    ds2 = Dataset(\n        {\n            \"temp\": (\"x\", [3.3, 4.4]),\n            \"time\": (\"x\", pd.date_range(\"2023-01-03\", periods=2)),\n        },\n        coords={\"x\": [2, 3]},\n    )\n    ds3 = Dataset(\n        {\n            \"temp\": (\"x\", [5.5, 6.6]),\n            \"time\": (\"x\", pd.date_range(\"2023-01-05\", periods=2)),\n        },\n        coords={\"x\": [4, 5]},\n    )\n\n    # Concatenate datasets along dimension 'x'\n    result = concat([ds1, ds2, ds3], dim=\"x\", compat=\"equals\")\n    expected = Dataset(\n        {\n            \"temp\": (\"x\", [1.1, 2.2, 3.3, 4.4, 5.5, 6.6]),\n            \"time\": (\n                \"x\",\n                pd.date_range(\"2023-01-01\", periods=6, freq=\"D\"),\n            ),\n        },\n        coords={\"x\": [0, 1, 2, 3, 4, 5]},\n    )\n\n    assert_identical(result, expected)\n\n    # Test with compat=\"identical\"\n    ds2_identical = ds2.copy(deep=True)\n    ds2_identical[\"temp\"] = (\"x\", [2.2, 3.3])\n    with pytest.raises(ValueError, match=r\"array names not identical\"):\n        concat([ds1, ds2_identical, ds3], dim=\"x\", compat=\"identical\")\n", "def test_concat_with_new_dimension() -> None:\n    ds1 = Dataset(\n        {\"var1\": ((\"x\", \"y\"), [[1, 2], [3, 4]])},\n        coords={\"x\": [0, 1], \"y\": [10, 20]},\n    )\n    ds2 = Dataset(\n        {\"var1\": ((\"x\", \"y\"), [[5, 6], [7, 8]])},\n        coords={\"x\": [2, 3], \"y\": [30, 40]},\n    )\n\n    expected = Dataset(\n        {\"var1\": ((\"new_dim\", \"x\", \"y\"), [[[1, 2], [3, 4]], [[5, 6], [7, 8]]])},\n        coords={\"x\": [0, 1, 2, 3], \"y\": [10, 20, 30, 40]},\n    )\n    \n    actual = concat([ds1, ds2], dim=\"new_dim\")\n\n    assert_identical(actual, expected)\n", "def test_concat_with_new_dimension_name() -> None:\n    ds1 = Dataset({\"a\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2]})\n    ds2 = Dataset({\"a\": (\"x\", [4, 5, 6])}, coords={\"x\": [3, 4, 5]})\n\n    actual = concat([ds1, ds2], dim=\"new_dim\")\n    expected = Dataset(\n        {\"a\": ((\"new_dim\", \"x\"), [[1, 2, 3], [4, 5, 6]])},\n        coords={\"x\": [0, 1, 2, 3, 4, 5]},\n    )\n\n    assert_identical(actual, expected)\n", "def test_concat_empty_dataarrays() -> None:\n    da1 = DataArray(name=\"da1\")\n    da2 = DataArray(name=\"da2\")\n\n    expected = DataArray(name=\"da1\")\n    actual = concat([da1, da2], dim=\"x\")\n\n    assert_identical(expected, actual)\n\n    ds1 = Dataset({\"da1\": da1})\n    ds2 = Dataset({\"da2\": da2})\n\n    with pytest.raises(ValueError, match=\"must supply at least one object to concatenate\"):\n        concat([ds1, ds2], dim=\"x\")\n", "def test_concat_dim_as_pandas_index() -> None:\n    ds1 = Dataset({\"a\": (\"x\", [1, 2])}, coords={\"x\": [1, 2]})\n    ds2 = Dataset({\"a\": (\"x\", [3, 4])}, coords={\"x\": [3, 4]})\n    index = pd.Index([1, 2, 3, 4], name=\"x\")\n    \n    expected = Dataset({\"a\": (\"x\", [1, 2, 3, 4])}, coords={\"x\": index})\n    actual = concat([ds1, ds2], dim=index)\n    \n    assert_identical(expected, actual)\n"], "sample_628": ["def test_skip_words_with_slash(self):\n    stmt = astroid.extract_node(\n        'class TestSlash(object):\\n   \"\"\"before/after with a bad comment\"\"\"\\n   pass'\n    )\n    with self.assertAddsMessages(\n        Message(\n            \"wrong-spelling-in-docstring\",\n            line=2,\n            args=(\n                \"comment\",\n                \"before/after with a bad comment\",\n                \"                       ^^^^^^^\",\n                self._get_msg_suggestions(\"comment\"),\n            ),\n        )\n    ):\n        self.checker.visit_classdef(stmt)\n", "    def test_skip_words_with_single_quotes(self):\n        stmt = astroid.extract_node(\n            '''def fff(param_name):\\n   \"\"\"test param'name\"\"\"\\n   pass'''\n        )\n        self.checker.visit_functiondef(stmt)\n        assert self.linter.release_messages() == []\n", "def test_skip_words_with_special_characters(self):\n    self.checker.process_tokens(_tokenize_str(\"# word-with-hyphen\"))\n    assert self.linter.release_messages() == []\n    self.checker.process_tokens(_tokenize_str(\"# word.with.dot\"))\n    assert self.linter.release_messages() == []\n    self.checker.process_tokens(_tokenize_str(\"# word_with_underscore\"))\n    assert self.linter.release_messages() == []\n    self.checker.process_tokens(_tokenize_str(\"# word+with+plus\"))\n    assert self.linter.release_messages() == []\n    self.checker.process_tokens(_tokenize_str(\"# word&with&ampersand\"))\n    assert self.linter.release_messages() == []\n", "    def test_skip_directives_in_comment_at_middle_of_line(self):\n        full_comment = \"# This is a comment fmt: off directive\"\n        with self.assertAddsMessages(\n            Message(\n                \"wrong-spelling-in-comment\",\n                line=1,\n                args=(\n                    \"fmt\",\n                    full_comment,\n                    \"                  ^^^\",\n                    self._get_msg_suggestions(\"fmt\"),\n                ),\n            )\n        ):\n            self.checker.process_tokens(_tokenize_str(full_comment))\n", "    def test_handle_unicode_literals(self):\n        stmt = astroid.extract_node(\n            'def fff():\\n   \"\"\"bad c\u00f6ment\"\"\"\\n   pass'\n        )\n        with self.assertAddsMessages(\n            Message(\n                \"wrong-spelling-in-docstring\",\n                line=2,\n                args=(\n                    \"c\u00f6ment\",\n                    \"bad c\u00f6ment\",\n                    \"    ^^^^^^\",\n                    self._get_msg_suggestions(\"c\u00f6ment\"),\n                ),\n            )\n        ):\n            self.checker.visit_functiondef(stmt)\n"], "sample_629": ["def test_get_python_path():\n    # Test with a file path that does not exist\n    filepath = \"/non/existent/path/to/file.py\"\n    result = get_python_path(filepath)\n    assert result == os.path.dirname(filepath)\n\n    # Test with a directory path that does not exist\n    dirpath = \"/non/existent/path/to/directory/\"\n    result = get_python_path(dirpath)\n    assert result == dirpath\n\n    # Test with a real directory containing __init__.py\n    with pytest.raises(OSError):\n        os.mkdir(\"test_package\")\n        with open(\"test_package/__init__.py\", \"w\") as f:\n            f.write(\"# init file\")\n        result = get_python_path(\"test_package\")\n        assert result == os.path.realpath(\"test_package\")\n\n    # Cleanup\n    if os.path.exists(\"test_package/__init__.py\"):\n        os.remove(\"test_package/__init__.py\")\n    if os.path.exists(\"test_package\"):\n        os.rmdir(\"test_package\")\n", "def test_expand_modules_with_ignore_list():\n    ignore_list = [\"ignore_this.py\"]\n    ignore_list_re = [re.compile(\".*ignoreme.*\")]\n    ignore_list_paths_re = [re.compile(\".*ignorepath.*\")]\n    files_or_modules = [\n        __file__,\n        str(TEST_DIRECTORY / \"ignore_this.py\"),\n        str(TEST_DIRECTORY / \"ignoreme_file.py\"),\n        str(TEST_DIRECTORY / \"ignorepath_file.py\"),\n    ]\n    expected = [this_file]\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "def test_get_python_path():\n    init_dir = os.path.join(TEST_DIRECTORY, \"lint\")\n    with pytest.raises(FileNotFoundError):\n        get_python_path(\"/path/that/does/not/exist\")\n    assert get_python_path(init_dir) == str(TEST_DIRECTORY)\n    non_package_dir = os.path.join(TEST_DIRECTORY, \"some_non_package_dir\")\n    os.makedirs(non_package_dir, exist_ok=True)\n    try:\n        assert get_python_path(non_package_dir) == non_package_dir\n    finally:\n        os.rmdir(non_package_dir)\n", "def test_get_python_path():\n    # Create a temporary directory structure\n    with pytest.MonkeyPatch.context() as m:\n        m.chdir(TEST_DIRECTORY)\n        temp_dir = TEST_DIRECTORY / \"temp_test_dir\"\n        temp_dir_init = temp_dir / \"__init__.py\"\n        temp_dir.mkdir(parents=True, exist_ok=True)\n        temp_dir_init.touch()\n\n        # Test get_python_path when there is an __init__.py file\n        result = get_python_path(str(temp_dir))\n        assert result == str(TEST_DIRECTORY), f\"Expected {str(TEST_DIRECTORY)}, got {result}\"\n\n        # Remove the __init__.py file and test again\n        temp_dir_init.unlink()\n        result = get_python_path(str(temp_dir))\n        assert result == str(temp_dir), f\"Expected {str(temp_dir)}, got {result}\"\n\n        # Cleanup\n        temp_dir.rmdir()\n", "def test_get_python_path():\n    current_dir = os.getcwd()\n    sub_dir = os.path.join(current_dir, \"subdir\")\n    file_path = os.path.join(sub_dir, \"somefile.py\")\n    init_file = os.path.join(sub_dir, \"__init__.py\")\n\n    os.makedirs(sub_dir, exist_ok=True)\n    try:\n        with open(init_file, \"w\") as f:\n            f.write(\"# init file\")\n\n        assert get_python_path(file_path) == sub_dir\n\n        os.remove(init_file)\n        assert get_python_path(file_path) == current_dir\n    finally:\n        os.rmdir(sub_dir)\n"], "sample_630": ["def test_dotwriter_get_values():\n    \"\"\"Test the get_values method of DotWriter for different class shapes and annotations.\"\"\"\n\n    class MockFunc:\n            self.name = name\n            self.returns = returns\n            self.args = args\n\n    class MockArgs:\n            self.args = args\n            self.annotations = annotations\n\n    class MockObj:\n            self.title = title\n            self.shape = shape\n            self.attrs = attrs\n            self.methods = methods\n            self.node = None\n\n    config = Config()\n    dot_writer = DotWriter(config)\n\n    obj = MockObj(\n        title=\"TestClass\",\n        shape=\"class\",\n        attrs=[\"attr1\", \"attr2\"],\n        methods=[\n            MockFunc(\n                name=\"method1\",\n                returns=None,\n                args=MockArgs(args=[\"self\", \"arg1\", \"arg2\"], annotations=[None, None, None]),\n            ),\n            MockFunc(\n                name=\"method2\",\n                returns=\"int\",\n                args=MockArgs(args=[\"self\"], annotations=[None]),\n            ),\n        ],\n    )\n\n    expected_label = r\"{TestClass|attr1\\l|attr2\\l|method1(arg1, arg2)\\lmethod2(): int\\l}\"\n    expected_values = {\"label\": expected_label, \"shape\": \"record\"}\n\n    values = dot_writer.get_values(obj)\n    assert values == expected_values, f\"got {values} instead of {expected_values}\"\n", "def test_vcgwriter_get_values_shapes(class_shape, expected_shape):\n    \"\"\"Test that VCGWriter returns correct shapes based on class type.\"\"\"\n    class MockClass:\n            self.title = title\n            self.shape = shape\n            self.attrs = attrs\n            self.methods = methods\n            self.node = node\n\n    mock_config = Config()\n    writer = VCGWriter(mock_config)\n    \n    if class_shape == \"exception\":\n        node = astroid.ClassDef(name=\"Exception\", lineno=1, col_offset=0)\n    else:\n        node = astroid.ClassDef(name=\"RegularClass\", lineno=1, col_offset=0)\n    \n    obj = MockClass(\"MockClass\", class_shape, [\"attr1\", \"attr2\"], [\"method1\"], node)\n    values = writer.get_values(obj)\n    assert values[\"shape\"] == expected_shape\n", "def test_dot_writer_initialization():\n    \"\"\"Test the initialization of the DotWriter class\"\"\"\n    config = Config()\n    writer = DotWriter(config)\n    assert writer.config == config\n    assert writer.pkg_edges == dict(arrowtail=\"none\", arrowhead=\"open\")\n    assert writer.inh_edges == dict(arrowtail=\"none\", arrowhead=\"empty\")\n    assert writer.imp_edges == dict(arrowtail=\"node\", arrowhead=\"empty\", style=\"dashed\")\n    assert writer.association_edges == dict(\n        fontcolor=\"green\", arrowtail=\"none\", arrowhead=\"diamond\", style=\"solid\"\n    )\n    assert writer.printer is None\n", "def test_dot_writer_initialization(config, styles):\n    writer = DotWriter(config)\n    assert writer.config == config\n    assert writer.pkg_edges == styles[0]\n    assert writer.inh_edges == styles[1]\n    assert writer.imp_edges == styles[2]\n    assert writer.association_edges == styles[3]\n    assert writer.printer is None\n", "def test_dotwriter_get_values():\n    \"\"\"Test DotWriter.get_values to ensure it returns correct label and shape for classes.\"\"\"\n    class MockFunc:\n            self.name = name\n            self.returns = returns\n            self.args = args\n\n    class MockArgs:\n            self.args = args\n            self.annotations = annotations\n\n    class MockObj:\n            self.title = title\n            self.shape = shape\n            self.attrs = attrs\n            self.methods = methods\n            self.node = node\n\n    config = Config()\n    writer = DotWriter(config)\n\n    obj = MockObj(\n        title=\"TestClass\",\n        shape=\"record\",\n        attrs=[\"attr1\", \"attr2\"],\n        methods=[\n            MockFunc(\n                name=\"method1\",\n                returns=None,\n                args=MockArgs(args=[], annotations=[])\n            ),\n            MockFunc(\n                name=\"method2\",\n                returns=\"str\",\n                args=MockArgs(args=[\"self\", \"param1\"], annotations=[None, \"int\"])\n            ),\n        ],\n        node=None\n    )\n\n    result = writer.get_values(obj)\n    expected_label = r\"{TestClass|attr1\\l|attr2\\l|method1()\\lmethod2(param1: int): str\\l}\"\n    assert result[\"label\"] == expected_label\n    assert result[\"shape\"] == \"record\"\n"], "sample_631": ["def test_unbalanced_tuple_unpacking(self):\n        node = astroid.extract_node(\n            \"\"\"\n            a, b = (1, 2, 3)\n            \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\n                \"unbalanced-tuple-unpacking\",\n                node=node,\n                args=(\"\", 2, 3),\n            )\n        ):\n            self.walk(node)\n", "def test_in_for_else_branch(self):\n    node = astroid.extract_node(\n        \"\"\"\n        for i in range(10):\n            pass\n        else:\n            x = 1  #@\n        \"\"\"\n    )\n    parent = node.parent\n    stmt = node\n    self.assertTrue(variables.in_for_else_branch(parent, stmt))\n", "def test_unbalanced_tuple_unpacking(self):\n    \"\"\"Test that unbalanced tuple unpacking is properly detected.\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        a, b = (1, 2, 3)  #@\n        \"\"\"\n    )\n    msg = Message(\"unbalanced-tuple-unpacking\", node=node, args=(\" defined at line 1\", 2, 3))\n    with self.assertAddsMessages(msg):\n        self.walk(node.root())\n", "    def test_self_cls_assignment(self):\n        \"\"\"Test that self/cls assignment detection works correctly.\"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n        class MyClass:\n                self = 42  #@\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"self-cls-assignment\", node=node, args=(\"self\",))\n        ):\n            self.walk(node)\n\n        node = astroid.extract_node(\n            \"\"\"\n        class MyClass:\n            @classmethod\n                cls = 42  #@\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"self-cls-assignment\", node=node, args=(\"cls\",))\n        ):\n            self.walk(node)\n\n        node = astroid.extract_node(\n            \"\"\"\n        class MyClass:\n            @staticmethod\n                cls = 42  # No message for staticmethod\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n", "    def test_unused_variables_in_function(self):\n        \"\"\"Test that unused variables in a function emit the correct message.\"\"\"\n        node = astroid.parse(\n            \"\"\"\n            a = 10\n            b = 20\n            return a\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"unused-variable\", node=node.body[0].body[1], args=\"b\")\n        ):\n            self.walk(node)\n"], "sample_632": ["def test_combine_mapreduce_data():\n    \"\"\"Tests the combination of mapreduce data.\"\"\"\n    # Create two SimilarChecker instances with different inputs\n    linter = PyLinter(reporter=Reporter())\n\n    source_streams = (\n        str(INPUT / \"similar_lines_a.py\"),\n        str(INPUT / \"similar_lines_b.py\"),\n    )\n\n    sim1 = similar.SimilarChecker(linter)\n    sim2 = similar.SimilarChecker(linter)\n\n    for source_fname in source_streams:\n        with open(source_fname) as stream:\n            sim1.append_stream(source_fname, stream)\n            sim2.append_stream(source_fname, stream)\n\n    # Get map data from both instances\n    map_data1 = sim1.get_map_data()\n    map_data2 = sim2.get_map_data()\n\n    # Combine data using the combine_mapreduce_data method\n    combined = similar.SimilarChecker(linter)\n    combined.combine_mapreduce_data([map_data1, map_data2])\n\n    # Verify that the combined instance has the correct number of linesets\n    assert len(combined.linesets) == len(map_data1) + len(map_data2)\n\n    # Verify that the combined linesets contain the expected lines\n    expected_linelists = map_data1 + map_data2\n    for expected_lineset, combined_lineset in zip(expected_linelists, combined.linesets):\n        assert expected_lineset.name == combined_lineset.name\n        assert expected_lineset._stripped_lines == combined_lineset._stripped_lines\n", "def test_combine_mapreduce_data():\n    \"\"\"Tests that a SimilarChecker can combine map/reduce data correctly.\"\"\"\n    linter = PyLinter(reporter=Reporter())\n\n    # Create some sample line sets\n    lineset1 = similar.LineSet(\n        name=\"file1.py\",\n        lines=[\n            \"def foo():\",\n            \"    pass\",\n            \"def bar():\",\n            \"    pass\",\n        ],\n    )\n\n    lineset2 = similar.LineSet(\n        name=\"file2.py\",\n        lines=[\n            \"def baz():\",\n            \"    pass\",\n            \"def qux():\",\n            \"    pass\",\n        ],\n    )\n\n    # Combine the line sets using the SimilarChecker combine_mapreduce_data method\n    checker = similar.SimilarChecker(linter)\n    checker.combine_mapreduce_data([[lineset1], [lineset2]])\n\n    # Verify the combined result\n    assert len(checker.linesets) == 2\n    assert checker.linesets[0].name == \"file1.py\"\n    assert checker.linesets[1].name == \"file2.py\"\n", "def test_append_stream_unicode_decode_error():\n    \"\"\"Test append_stream method to ensure it handles UnicodeDecodeError gracefully.\"\"\"\n    content = b\"\\xff\\xff\\xff\"\n    stream = StringIO(content.decode(\"utf-8\", errors=\"ignore\"))\n    \n    sim = similar.Similar(ignore_comments=True, ignore_docstrings=True)\n    try:\n        sim.append_stream(\"testfile\", stream)\n        assert len(sim.linesets) == 0  # Expecting no lineset to be appended due to decoding error\n    except UnicodeDecodeError:\n        pytest.fail(\"append_stream should handle UnicodeDecodeError gracefully.\")\n", "def test_similar_lines():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR1, SIMILAR2, SIMILAR3])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            \"\"\"", "def test_min_similarity_lines():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--duplicates\", \"3\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            \"\"\""], "sample_633": ["def test_multiple_files_similarity() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR1, SIMILAR2, SIMILAR3])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f\"\"\"", "def test_lineset_stripped_lines() -> None:\n    lines = [\n        \"import os\\n\",\n        \"def foo():\\n\",\n        '    \"\"\"Docstring\"\"\"\\n',\n        \"    pass\\n\",\n        \"# Comment\\n\",\n        \"    code_line = 1\\n\",\n    ]\n    expected_stripped_lines = [\n        similar.LineSpecifs(text=\"import os\", line_number=similar.LineNumber(0)),\n        similar.LineSpecifs(text=\"def foo():\", line_number=similar.LineNumber(1)),\n        similar.LineSpecifs(text=\"pass\", line_number=similar.LineNumber(3)),\n        similar.LineSpecifs(text=\"code_line = 1\", line_number=similar.LineNumber(5)),\n    ]\n\n    lineset = similar.LineSet(\n        name=\"test_file.py\",\n        lines=lines,\n        ignore_comments=True,\n        ignore_docstrings=True,\n        ignore_imports=False,\n        ignore_signatures=False,\n    )\n    assert lineset.stripped_lines == expected_stripped_lines\n", "def test_similar_different_files() -> None:\n    # This test ensures that the Similar class can handle files that are different and do not have similar lines\n    output = StringIO()\n    unique_file_1 = str(INPUT / \"unique_file_1.py\")\n    unique_file_2 = str(INPUT / \"unique_file_2.py\")\n    \n    with open(unique_file_1, \"w\") as f1, open(unique_file_2, \"w\") as f2:\n        f1.write(\"line1_unique1\\nline2_unique1\\nline3_unique1\\n\")\n        f2.write(\"line1_unique2\\nline2_unique2\\nline3_unique2\\n\")\n    \n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([unique_file_1, unique_file_2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"TOTAL lines=6 duplicates=0 percent=0.00\"\n    )\n", "def test_report_similarities() -> None:\n    sect = []\n    stats = {\"nb_duplicated_lines\": 10, \"percent_duplicated_lines\": 16.13}\n    old_stats = {\"nb_duplicated_lines\": 5, \"percent_duplicated_lines\": 8.06}\n    similar.report_similarities(sect, stats, old_stats)\n    assert sect == [\n        \"\",\n        \"now\",\n        \"previous\",\n        \"difference\",\n        [\"nb_duplicated_lines\", \"10\", \"5\", \"5\"],\n        [\"percent_duplicated_lines\", \"16.13\", \"8.06\", \"8.07\"],\n    ]\n", "def test_ignore_imports_with_code() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-imports\", HIDE_CODE_WITH_IMPORTS])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\""], "sample_634": ["def test_expand_modules_with_ignore_list(self, files_or_modules, ignore_list, expected):\n    \"\"\"Test expand_modules with files or modules in the ignore list\"\"\"\n    ignore_list_re = []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "def test__is_in_ignore_list_re_no_match() -> None:\n    patterns = [\n        re.compile(\".*enchilada.*\"),\n        re.compile(\"unittest_.*\"),\n        re.compile(\".*tests/.*\"),\n    ]\n    assert not _is_in_ignore_list_re(\"utils.py\", patterns)\n    assert not _is_in_ignore_list_re(\"cheese.xml\", patterns)\n    assert not _is_in_ignore_list_re(\"src/whatever.xml\", patterns)\n", "def test__is_in_ignore_list_re_no_match() -> None:\n    patterns = [\n        re.compile(\".*enchilada.*\"),\n        re.compile(\"unittest_.*\"),\n        re.compile(\".*tests/.*\"),\n    ]\n    assert not _is_in_ignore_list_re(\"utils.py\", patterns)\n    assert not _is_in_ignore_list_re(\"cheese_burrito.xml\", patterns)\n    assert not _is_in_ignore_list_re(\"src/whatever.xml\", patterns)\n", "def test_expand_modules_with_ignore_list(self, files_or_modules, ignore_list, expected):\n    \"\"\"Test expand_modules with specific files in the ignore list\"\"\"\n    ignore_list_re = []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "def test_expand_modules_with_nonexistent_files(self, files_or_modules, expected):\n    \"\"\"Test expand_modules with nonexistent files or modules\"\"\"\n    ignore_list, ignore_list_re = [], []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self.checker, \"ignore-paths\"),\n    )\n    assert modules == expected\n    assert errors == []\n"], "sample_635": ["def test_ignores_no_param_doc_for_property_google(self) -> None:\n    \"\"\"Example of a property that should not require parameter documentation\n    in a Google style docstring\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n    class Foo(object):\n        @property\n            '''int: docstring ...\n\n            Raises:\n                RuntimeError: Always\n            '''\n            raise RuntimeError()\n            return 10\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "    def test_raises_in_function_sphinx(self) -> None:\n        \"\"\"Example of a function with raises documentation in\n        a Sphinx style docstring\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''function foo ...\n\n            :raises ValueError: If something goes wrong\n            :raises TypeError: If something else goes wrong\n            '''\n            raise ValueError(\"Something went wrong\")\n            raise TypeError(\"Something else went wrong\")\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.checker.visit_functiondef(node)\n", "    def test_docstringify_sphinx_docstring(self) -> None:\n        \"\"\"Test docstringify function with a valid Sphinx style docstring.\"\"\"\n        docstring = \"\"\"\n        Function description.\n\n        :param x: Description of x.\n        :type x: int\n        :param y: Description of y.\n        :type y: str\n        :return: Description of return value.\n        :rtype: float\n        \"\"\"\n        doc_instance = docstringify(docstring, \"sphinx\")\n        assert isinstance(doc_instance, SphinxDocstring)\n        assert doc_instance.is_valid()\n        assert doc_instance.has_params()\n        assert doc_instance.has_returns()\n        assert doc_instance.has_rtype()\n", "def test_docstringify_with_different_formats(self) -> None:\n    \"\"\"Test the `docstringify` function with different docstring formats.\"\"\"\n    sphinx_doc = \"\"\"\n    :param x: Description of x\n    :type x: int\n    :param y: Description of y\n    :type y: str\n    :returns: Description of return value\n    :rtype: bool\n    \"\"\"\n    google_doc = \"\"\"\n    Args:\n        x (int): Description of x\n        y (str): Description of y\n\n    Returns:\n        bool: Description of return value\n    \"\"\"\n    numpy_doc = \"\"\"\n    Parameters\n    ----------\n    x : int\n        Description of x\n    y : str\n        Description of y\n\n    Returns\n    -------\n    bool\n        Description of return value\n    \"\"\"\n    epytext_doc = \"\"\"\n    @param x: Description of x\n    @type x: int\n    @param y: Description of y\n    @type y: str\n    @return: Description of return value\n    @rtype: bool\n    \"\"\"\n\n    assert isinstance(docstringify(sphinx_doc, \"sphinx\"), SphinxDocstring)\n    assert isinstance(docstringify(google_doc, \"google\"), GoogleDocstring)\n    assert isinstance(docstringify(numpy_doc, \"numpy\"), NumpyDocstring)\n    assert isinstance(docstringify(epytext_doc, \"epytext\"), EpytextDocstring)\n", "def test_missing_type_in_param_in_sphinx_docstring(self) -> None:\n    \"\"\"Example of a function with a parameter documented but missing type in Sphinx style docstring\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        '''docstring ...\n\n        :param x: bla\n        :type x: int\n\n        :param y: blah blah\n\n        :param z: bar\n        :type z: float\n        '''\n        pass\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"missing-type-doc\", node=node, args=(\"y\",))\n    ):\n        self.checker.visit_functiondef(node)\n"], "sample_636": ["    def test_duplicate_code_ignore_imports(self) -> None:\n        \"\"\"Tests that import statements are ignored in similarity computation.\"\"\"\n        path = join(DATA, \"ignore_imports\")\n        expected_output = \"TOTAL lines=30 duplicates=0 percent=0.00\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-imports=y\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_ignore_comments(self) -> None:\n        \"\"\"Tests that ignoring comments option works correctly.\"\"\"\n        path = join(DATA, \"raw_strings_with_comments\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-comments=y\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_with_imports_ignored(self) -> None:\n        \"\"\"Tests similarity detection with import statements ignored.\"\"\"\n        path = join(DATA, \"raw_strings_with_imports\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-imports=y\"],\n            expected_output=expected_output,\n        )\n", "    def test_ignore_comments(self) -> None:\n        \"\"\"Tests if ignoring comments works correctly.\"\"\"\n        path = join(DATA, \"raw_strings_with_comments\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-comments=y\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_ignore_comments(self) -> None:\n        \"\"\"Tests that comments are ignored in the similarity computation.\"\"\"\n        path = join(DATA, \"ignore_comments\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-comments=y\"],\n            expected_output=expected_output,\n        )\n"], "sample_637": ["    def test_valid_ascii_encoding(self) -> None:\n        code = \"\"\"# This is a valid ASCII encoded file\"\"\"\n        node = self._check_string(code, \"ascii\")\n        with self.assertNoMessages():\n            self.checker.process_module(node)\n", "    def test_non_ascii_without_encoding_declaration(self) -> None:\n        code = \"\"\"# This is a comment\n                a = \"\u00e9\"\n                \"\"\"\n        with self.assertNoMessages():\n            self.checker.process_module(self._module_node(code))\n", "    def test_message_enabled_by_id(self) -> None:\n        self.checker.linter._by_id_managed_msgs = [\n            (\"test_module\", \"I0011\", \"use-symbolic-message-instead\", 2, False)\n        ]\n        code = \"\"\"a = 1\n        # pylint: enable=I0011\n        \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"use-symbolic-message-instead\",\n                line=2,\n                args=\"'I0011' is cryptic: use '# pylint: enable=use-symbolic-message-instead' instead\",\n            )\n        ):\n            node = self.checker.linter._astroid_cache.get(\"test_module\")\n            self.checker.process_module(node)\n        self.assertEqual(self.checker.linter._by_id_managed_msgs, [])\n", "    def test_valid_ascii_encoding(self) -> None:\n        code = \"# This is a valid ASCII encoded file\\nprint('Hello, World!')\"\n        node = self.create_module_node(code)\n        with self.assertNoMessages():\n            self.checker.process_module(node)\n", "    def test_valid_encoding(self) -> None:\n        code = \"\"\"# coding: utf-8\n                a = 1\n                \"\"\"\n        with self.assertNoMessages():\n            self.checker.process_module(self.create_node(\"module\", code, file_encoding=\"utf-8\"))\n"], "sample_638": ["def test_run_no_arguments(capsys):\n    \"\"\"Test that running without arguments prints help and exits with code 1.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    stdout = capsys.readouterr().out\n    assert \"Usage: \" in stdout\n    assert \"create UML diagrams for classes and modules in\" in stdout\n    assert wrapped_sysexit.value.code == 1\n", "def test_directly_supported_format(mock_writer):\n    \"\"\"Test that directly supported formats do not use Graphviz.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([\"-o\", \"dot\", TEST_DATA_DIR])\n    # Check that no message about Graphviz is shown to the user\n    assert (\n        \"Pyreverse will try to generate it using Graphviz...\"\n        not in capsys.readouterr().out\n    )\n    # Check that pyreverse actually made the call to create the diagram and we exit cleanly\n    mock_writer.DiagramWriter().write.assert_called_once()\n    assert wrapped_sysexit.value.code == 0\n", "def test_directly_supported_format(mock_writer):\n    \"\"\"Test that directly supported formats are handled correctly.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"dot\", TEST_DATA_DIR])\n    # Check that no Graphviz-specific messages are shown\n    assert \"Pyreverse will try to generate it using Graphviz...\" not in capsys.readouterr().out\n    # Check that pyreverse actually made the call to create the diagram and we exit cleanly\n    mock_writer.DiagramWriter().write.assert_called_once()\n    assert wrapped_sysexit.value.code == 0\n", "def test_run_without_arguments(mock_writer, capsys):\n    \"\"\"Test that running without arguments displays help message and exits.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    # Check that the help message is shown to the user\n    stdout = capsys.readouterr().out\n    assert \"create UML diagrams for classes and modules in <packages>\" in stdout\n    # Check that no diagrams are created and we exit with the expected error code\n    mock_writer.DiagramWriter().write.assert_not_called()\n    assert wrapped_sysexit.value.code == 1\n", "def test_run_with_minimal_arguments(mock_writer, mock_diadefs_handler, mock_linker):\n    \"\"\"Test running the tool with minimal arguments.\"\"\"\n    mock_diadefs_handler().get_diadefs.return_value = \"mock_diadefs\"\n    mock_writer().write.return_value = None\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([TEST_DATA_DIR])\n    # Ensure the project creation and writing occurred\n    mock_linker.assert_called_once()\n    mock_diadefs_handler().get_diadefs.assert_called_once()\n    mock_writer().write.assert_called_once_with(\"mock_diadefs\")\n    # Check that we exited with the expected success code\n    assert wrapped_sysexit.value.code == 0\n"], "sample_639": ["def test_base_checker_consistency() -> None:\n    \"\"\"Test the consistency check of message IDs in a checker.\"\"\"\n    consistent_checker = OtherBasicChecker()\n    # This should not raise an exception\n    consistent_checker.check_consistency()\n\n    inconsistent_checker = DifferentBasicChecker()\n    inconsistent_checker.msgs = {\n        \"W0002\": (\n            \"Blah blah example.\",\n            \"blah-blah-example\",\n            \"I only exist to be different to OtherBasicChecker :(\",\n        ),\n        \"E0101\": (\n            \"Another inconsistent message.\",\n            \"inconsistent-message\",\n            \"This message ID is inconsistent.\",\n        ),\n    }\n    try:\n        inconsistent_checker.check_consistency()\n        assert False, \"Expected InvalidMessageError\"\n    except InvalidMessageError as e:\n        assert \"Inconsistent checker part in message id\" in str(e)\n", "def test_base_checker_consistency() -> None:\n    \"\"\"Test consistency check for checkers based on their msgid.\"\"\"\n    consistent_checker = OtherBasicChecker()\n    inconsistent_checker = DifferentBasicChecker()\n    inconsistent_checker.msgs = {\n        \"W0002\": (\n            \"Blah blah example.\",\n            \"blah-blah-example\",\n            \"I only exist to be different to OtherBasicChecker :(\",\n        ),\n        \"E0002\": (\n            \"Inconsistent example.\",\n            \"inconsistent-example\",\n            \"This message has an inconsistent checker id.\",\n        )\n    }\n\n    # This should not raise an error\n    consistent_checker.check_consistency()\n\n    try:\n        # This should raise an InvalidMessageError due to inconsistent msgid\n        inconsistent_checker.check_consistency()\n    except InvalidMessageError as e:\n        assert str(e) == (\n            \"Inconsistent checker part in message id 'E0002' \"\n            \"(expected 'x0002xx' because we already had ['W0002']).\"\n        )\n    else:\n        assert False, \"Expected InvalidMessageError not raised\"\n", "def test_base_checker_add_message() -> None:\n    \"\"\"Test the add_message method of BaseChecker.\"\"\"\n    class MockLinter:\n            self.messages = []\n\n            self,\n            msgid,\n            line,\n            node,\n            args,\n            confidence,\n            col_offset,\n            end_lineno,\n            end_col_offset,\n        ):\n            self.messages.append((msgid, line, node, args, confidence, col_offset, end_lineno, end_col_offset))\n\n    mock_linter = MockLinter()\n    checker = OtherBasicChecker(mock_linter)\n    checker.add_message(\"W0001\", line=10, args=\"test args\", confidence=Confidence.HIGH)\n    \n    expected_message = (\"W0001\", 10, None, \"test args\", Confidence.HIGH, None, None, None)\n    assert mock_linter.messages == [expected_message]\n", "def test_base_checker_add_message() -> None:\n    \"\"\"Test the add_message method of the BaseChecker class.\"\"\"\n    class MockLinter:\n            self.messages = []\n\n            self.messages.append((msgid, line, node, args, confidence, col_offset, end_lineno, end_col_offset))\n\n    mock_linter = MockLinter()\n    checker = OtherBasicChecker(mock_linter)\n    checker.add_message(\"W0001\", line=10, node=None, args=None, confidence=None, col_offset=None, end_lineno=None, end_col_offset=None)\n\n    assert len(mock_linter.messages) == 1\n    assert mock_linter.messages[0] == (\"W0001\", 10, None, None, None, None, None, None)\n", "def test_base_checker_consistency() -> None:\n    \"\"\"Test the check_consistency method of BaseChecker.\"\"\"\n    basic = OtherBasicChecker()\n    basic.check_consistency()  # Should not raise an error\n\n    inconsistent_checker = DifferentBasicChecker()\n    inconsistent_checker.msgs = {\n        \"W0002\": (\n            \"Blah blah example.\",\n            \"blah-blah-example\",\n            \"I only exist to be different to OtherBasicChecker :(\",\n        ),\n        \"E1234\": (\n            \"Inconsistent message id example.\",\n            \"inconsistent-example\",\n            \"This message id is inconsistent with others.\",\n        ),\n    }\n    try:\n        inconsistent_checker.check_consistency()\n    except InvalidMessageError as e:\n        assert str(e) == \"Inconsistent checker part in message id 'E1234' \" \\\n                         \"(expected 'xdixx' because we already had ['W0002']).\"\n    else:\n        assert False, \"InvalidMessageError not raised\"\n"], "sample_640": ["def test_is_super() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    super().__init__()  #@\n    not_super()  #@\n    super_call = super()  #@\n    super().method()  #@\n    \"\"\"\n    )\n    assert utils.is_super(code[0])\n    assert not utils.is_super(code[1])\n    assert utils.is_super(code[2])\n    assert utils.is_super(code[3])\n", "def test_is_defined_before() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    x = 1  #@\n        x = 2  #@\n        if True:\n            x = 3  #@\n        y = 4  #@\n        z = x + 1  #@\n    \"\"\")\n    assert utils.is_defined_before(code[5])  # check if x is defined before within the function\n    assert not utils.is_defined_before(code[3])  # check if y is defined before the assignment to y\n    assert utils.is_defined_before(code[2])  # check if x is defined before within the function\n    assert utils.is_defined_before(code[1])  # check if x is defined before within the function\n", "def test_is_func_decorator() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    @decorator\n        pass\n\n        @decorator\n            pass\n    \"\"\"\n    )\n    assert utils.is_func_decorator(code[0]) is True\n    assert utils.is_func_decorator(code[1].body[0]) is True\n", "def test_is_defined_before() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    x = 1\n    y = x + 1  #@\n    \n    z = 3\n        return z  #@\n    z = 4\n    \"\"\"\n    )\n    assert utils.is_defined_before(code[0])\n    assert not utils.is_defined_before(code[1])\n", "def test_is_error() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n        raise Exception()\n\n        return \"no exception\"\n\n        print(\"Hello\")\n        raise Exception()\n    \"\"\"\n    )\n    assert isinstance(code, list) and len(code) == 3\n\n    assert utils.is_error(code[0]) is True\n    assert utils.is_error(code[1]) is False\n    assert utils.is_error(code[2]) is False\n"], "sample_641": ["def test_save_results_invalid_path(linter_stats: LinterStats, invalid_path: str) -> None:\n    with pytest.raises(OSError):\n        save_results(linter_stats, invalid_path)\n", "def test_load_results_nonexistent_file(path: str) -> None:\n    loaded = load_results(path)\n    assert loaded is None\n", "def test_get_pdata_path_invalid_base_name(invalid_path) -> None:\n    with pytest.raises(TypeError):\n        _get_pdata_path(invalid_path, 1)\n", "def test_save_results_cannot_create_directory(path: str, monkeypatch) -> None:\n        raise OSError(\"Cannot create directory\")\n\n    monkeypatch.setattr(Path, \"mkdir\", mock_mkdir)\n    \n    with pytest.raises(OSError, match=\"Cannot create directory\"):\n        save_results(LinterStats(), path)\n", "def test_load_results_non_existent_file(path: str) -> None:\n    \"\"\"Test load_results with a path that does not exist.\"\"\"\n    loaded = load_results(path)\n    assert loaded is None\n"], "sample_642": ["def test_convert_option_to_argument(opt: str, optdict: dict[str, Any], expected_type: type) -> None:\n    \"\"\"Test the _convert_option_to_argument function with various input options.\"\"\"\n    result = _convert_option_to_argument(opt, optdict)\n    assert isinstance(result, expected_type)\n", "def test_preprocess_options() -> None:\n    \"\"\"Test the _preprocess_options function.\"\"\"\n    run = mock.Mock(spec=Run)\n    args = [\n        \"--init-hook=print('Hello, World!')\",\n        \"--rcfile=myrcfile\",\n        \"--output=myoutput\",\n        \"--load-plugins=plugin1,plugin2\",\n        \"--verbose\",\n        \"-v\",\n        \"--enable-all-extensions\",\n    ]\n    \n    expected_processed_args = []\n    \n    processed_args = _preprocess_options(run, args)\n    \n    assert processed_args == expected_processed_args\n    \n    run._rcfile == \"myrcfile\"\n    run._output == \"myoutput\"\n    run._plugins == [\"plugin1\", \"plugin2\"] + run._plugins\n    run.verbose is True\n    assert \"pylint.extensions.\" in run._plugins\n", "def test_convert_option_to_argument(opt, optdict, expected) -> None:\n    \"\"\"Test the _convert_option_to_argument function with various inputs.\"\"\"\n    result = _convert_option_to_argument(opt, optdict)\n    assert isinstance(result, expected)\n", "def test_preprocess_options() -> None:\n    \"\"\"Test preprocessing of command line options.\"\"\"\n    run_instance = mock.Mock(spec=Run)\n    \n    # Test setting rcfile\n    args = [\"--rcfile=myrcfile\"]\n    processed_args = config._preprocess_options(run_instance, args)\n    assert processed_args == []\n    run_instance._rcfile = \"myrcfile\"\n\n    # Test setting output\n    args = [\"--output=myoutput\"]\n    processed_args = config._preprocess_options(run_instance, args)\n    assert processed_args == []\n    run_instance._output = \"myoutput\"\n\n    # Test enabling verbose mode\n    args = [\"--verbose\"]\n    processed_args = config._preprocess_options(run_instance, args)\n    assert processed_args == []\n    run_instance.verbose = True\n\n    # Test enabling all extensions\n    args = [\"--enable-all-extensions\"]\n    processed_args = config._preprocess_options(run_instance, args)\n    assert processed_args == []\n    assert \"pylint.extensions.some_extension\" in run_instance._plugins\n\n    # Test invalid options\n    with pytest.raises(config.ArgumentPreprocessingError):\n        config._preprocess_options(run_instance, [\"--rcfile\"])\n\n    with pytest.raises(config.ArgumentPreprocessingError):\n        config._preprocess_options(run_instance, [\"--verbose=extra\"])\n", "def test_preprocess_options_errors(args, expected_error) -> None:\n    \"\"\"Test that preprocessing options raises appropriate errors.\"\"\"\n    run = mock.Mock(spec=Run)\n    with pytest.raises(ArgumentPreprocessingError, match=expected_error):\n        _preprocess_options(run, args)\n"], "sample_643": ["def test_colorize_ansi_with_valid_message_style():\n    \"\"\"Test the colorize_ansi function with a valid MessageStyle.\"\"\"\n    msg = \"Test message\"\n    msg_style = MessageStyle(\"red\", (\"bold\", \"underline\"))\n    result = colorize_ansi(msg, msg_style)\n    expected_escape_code = \"\\033[1;4;31m\"\n    assert result == f\"{expected_escape_code}Test message\\033[0m\"\n\n", "def test_colorize_ansi() -> None:\n    \"\"\"Test the colorize_ansi function.\"\"\"\n    msg = \"Test message\"\n    # Test with no style and no color\n    assert colorize_ansi(msg) == msg\n\n    # Test with only color\n    assert colorize_ansi(msg, \"red\") == \"\\033[31mTest message\\033[0m\"\n\n    # Test with only style\n    assert colorize_ansi(msg, style=\"bold\") == \"\\033[1mTest message\\033[0m\"\n\n    # Test with both color and style\n    assert colorize_ansi(msg, \"red\", style=\"bold\") == \"\\033[1;31mTest message\\033[0m\"\n\n    # Test with MessageStyle\n    msg_style = MessageStyle(color=\"green\", style=(\"underline\",))\n    assert colorize_ansi(msg, msg_style) == \"\\033[4;32mTest message\\033[0m\"\n\n    # Test with invalid color\n    assert colorize_ansi(msg, \"invalid_color\") == msg\n\n    # Test with invalid style\n    assert colorize_ansi(msg, style=\"invalid_style\") == msg\n", "def test_colorize_ansi():\n    \"\"\"Test the colorize_ansi function with various inputs.\"\"\"\n    # No style or color\n    msg = \"Hello, World!\"\n    result = colorize_ansi(msg)\n    assert result == msg\n\n    # Only color\n    msg_style = MessageStyle(\"red\")\n    result = colorize_ansi(msg, msg_style)\n    assert result == f\"{ANSI_PREFIX}31{ANSI_END}{msg}{ANSI_RESET}\"\n\n    # Only style\n    msg_style = MessageStyle(None, (\"bold\",))\n    result = colorize_ansi(msg, msg_style)\n    assert result == f\"{ANSI_PREFIX}1{ANSI_END}{msg}{ANSI_RESET}\"\n\n    # Both color and style\n    msg_style = MessageStyle(\"green\", (\"underline\",))\n    result = colorize_ansi(msg, msg_style)\n    assert result == f\"{ANSI_PREFIX}4;32{ANSI_END}{msg}{ANSI_RESET}\"\n\n    # Invalid color\n    msg_style = MessageStyle(\"invalid_color\", (\"bold\",))\n    result = colorize_ansi(msg, msg_style)\n    assert result == msg\n\n    # Backwards compatibility with deprecated parameters\n    result = colorize_ansi(msg, msg_style=\"blue\", style=\"italic\")\n    assert result == f\"{ANSI_PREFIX}3;34{ANSI_END}{msg}{ANSI_RESET}\"\n", "def test_colorize_ansi() -> None:\n    \"\"\"Test the colorize_ansi function.\"\"\"\n\n    # Test with no style or color\n    msg = \"Hello, World!\"\n    result = colorize_ansi(msg)\n    assert result == msg\n\n    # Test with only color\n    msg_style = MessageStyle(\"red\")\n    result = colorize_ansi(msg, msg_style)\n    assert result == f\"{ANSI_PREFIX}{ANSI_COLORS['red']}{ANSI_END}{msg}{ANSI_RESET}\"\n\n    # Test with color and style\n    msg_style = MessageStyle(\"green\", (\"bold\", \"underline\"))\n    result = colorize_ansi(msg, msg_style)\n    expected_ansi_code = (\n        f\"{ANSI_PREFIX}{ANSI_STYLES['bold']};{ANSI_STYLES['underline']};{ANSI_COLORS['green']}{ANSI_END}\"\n    )\n    assert result == f\"{expected_ansi_code}{msg}{ANSI_RESET}\"\n\n    # Test with deprecated parameters (color and style)\n    result = colorize_ansi(msg, \"blue\", \"italic\", color=\"yellow\")\n    expected_ansi_code = (\n        f\"{ANSI_PREFIX}{ANSI_STYLES['italic']};{ANSI_COLORS['yellow']}{ANSI_END}\"\n    )\n    assert result == f\"{expected_ansi_code}{msg}{ANSI_RESET}\"\n", "def test_colorize_ansi() -> None:\n    \"\"\"Test the colorize_ansi function.\"\"\"\n    msg = \"Test Message\"\n    msg_style = MessageStyle(color=\"red\", style=(\"bold\", \"underline\"))\n    colored_msg = colorize_ansi(msg, msg_style)\n\n    expected_colored_msg = f\"{ANSI_PREFIX}{ANSI_COLORS['red']};{ANSI_STYLES['bold']};{ANSI_STYLES['underline']}{ANSI_END}{msg}{ANSI_RESET}\"\n    assert colored_msg == expected_colored_msg\n\n    # Test with no style or color\n    msg_style = MessageStyle(color=None, style=())\n    plain_msg = colorize_ansi(msg, msg_style)\n    assert plain_msg == msg\n\n    # Test deprecated usage with color and style\n    with pytest.warns(DeprecationWarning):\n        deprecated_colored_msg = colorize_ansi(msg, msg_style=\"red\", style=\"bold\")\n        assert deprecated_colored_msg == f\"{ANSI_PREFIX}{ANSI_COLORS['red']};{ANSI_STYLES['bold']}{ANSI_END}{msg}{ANSI_RESET}\"\n"], "sample_644": ["def test_import_outside_toplevel(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"outside_toplevel\", REGR_DATA)\n    import_node = module.body[1].body[0]  # Assuming the import is within a function\n\n    msg = MessageTest(\n        msg_id=\"import-outside-toplevel\",\n        node=import_node,\n        args=\"os\",\n        confidence=UNDEFINED,\n        line=2,\n        col_offset=4,\n        end_line=2,\n        end_col_offset=6,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n", "def test_import_self() -> None:\n    \"\"\"Test that the 'import-self' warning is triggered when a module imports itself.\"\"\"\n    module_code = \"\"\"\n    import self\n    \"\"\"\n    module = astroid.parse(module_code)\n    import_node = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"import-self\",\n        node=import_node,\n        line=2,\n        col_offset=0,\n        end_line=2,\n        end_col_offset=10,\n    )\n\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n", "def test_import_self(self) -> None:\n    module_code = \"\"\"\n    import mymodule\n    from mymodule import submodule\n    \"\"\"\n    module = astroid.parse(module_code)\n    import_1, import_2 = module.body\n\n    msg_1 = MessageTest(\n        msg_id=\"import-self\",\n        node=import_1,\n        args=(),\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=17,\n    )\n    msg_2 = MessageTest(\n        msg_id=\"import-self\",\n        node=import_2,\n        args=(),\n        line=2,\n        col_offset=0,\n        end_line=2,\n        end_col_offset=29,\n    )\n\n    with self.assertAddsMessages(msg_1):\n        self.checker.visit_import(import_1)\n    with self.assertAddsMessages(msg_2):\n        self.checker.visit_importfrom(import_2)\n", "def test_import_outside_toplevel(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"import_outside_toplevel\", REGR_DATA)\n        function = module.body[1]\n        import_from = function.body[0]\n\n        msg = MessageTest(\n            msg_id=\"import-outside-toplevel\",\n            node=import_from,\n            args=\"os\",\n            line=3,\n            col_offset=4,\n            end_line=3,\n            end_col_offset=19,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_importfrom(import_from)\n", "def test_import_self() -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"import_self\", REGR_DATA)\n    import_node = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"import-self\",\n        node=import_node,\n        confidence=UNDEFINED,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=18,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n"], "sample_645": ["def test_percent_style_multiline_formatting(caplog):\n    caplog.set_level(logging.INFO)\n    multiline_message = \"This is line one\\nThis is line two\\nThis is line three\"\n    logger.info(multiline_message)\n\n    formatted_text = caplog.text\n    assert \"This is line one\" in formatted_text\n    assert \"This is line two\" in formatted_text\n    assert \"This is line three\" in formatted_text\n    assert \"\\n    \" in formatted_text  # Check if lines are indented\n", "def test_multiline_logging_format(caplog):\n    caplog.set_level(logging.INFO)\n    multiline_message = \"This is a multiline\\nmessage for testing\\nlog formatting.\"\n    logger.info(multiline_message)\n    \n    formatted_text = caplog.text\n    assert \"This is a multiline\" in formatted_text\n    assert \"message for testing\" in formatted_text\n    assert \"log formatting.\" in formatted_text\n", "def test_colored_level_formatter():\n    tw = TerminalWriter()\n    formatter = ColoredLevelFormatter(tw, fmt=\"%(levelname)s %(message)s\")\n    formatter.add_color_level(logging.INFO, \"green\", \"bold\")\n    \n    record = logging.LogRecord(name=\"test\", level=logging.INFO, pathname=\"\", lineno=0, msg=\"info message\", args=(), exc_info=None)\n    formatted_message = formatter.format(record)\n    \n    assert \"info message\" in formatted_message\n    assert \"\\x1b[\" in formatted_message  # ANSI escape sequence\n", "def test_colored_formatter(monkeypatch, caplog):\n    from _pytest.config import create_terminal_writer\n    from _pytest.logging import ColoredLevelFormatter\n\n    terminal_writer = create_terminal_writer(None)\n    formatter = ColoredLevelFormatter(terminal_writer, fmt=\"%(levelname)s: %(message)s\")\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    caplog.set_level(logging.DEBUG)\n\n    with monkeypatch.context() as m:\n        m.setattr(logger, 'handlers', [handler])\n        logger.debug(\"debug message\")\n        logger.info(\"info message\")\n        logger.warning(\"warning message\")\n        logger.error(\"error message\")\n        logger.critical(\"critical message\")\n\n    assert \"debug message\" in caplog.text\n    assert \"info message\" in caplog.text\n    assert \"warning message\" in caplog.text\n    assert \"error message\" in caplog.text\n    assert \"critical message\" in caplog.text\n", "def test_colored_level_formatter(pytester: Pytester) -> None:\n    \"\"\"Test that ColoredLevelFormatter applies colors to log levels correctly.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        from _pytest.logging import ColoredLevelFormatter, create_terminal_writer\n        from _pytest.config import Config\n\n            config = Config.fromdictargs({})\n            terminal_writer = create_terminal_writer(config)\n            log_format = \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\"\n            formatter = ColoredLevelFormatter(terminal_writer, log_format)\n            handler = logging.StreamHandler()\n            handler.setFormatter(formatter)\n            logger = logging.getLogger(\"test_logger\")\n            logger.addHandler(handler)\n            logger.setLevel(logging.DEBUG)\n\n            with caplog.at_level(logging.DEBUG):\n                logger.debug(\"This is a debug message\")\n                logger.info(\"This is an info message\")\n                logger.warning(\"This is a warning message\")\n                logger.error(\"This is an error message\")\n                logger.critical(\"This is a critical message\")\n            \n            assert \"DEBUG\" in caplog.text\n            assert \"INFO\" in caplog.text\n            assert \"WARNING\" in caplog.text\n            assert \"ERROR\" in caplog.text\n            assert \"CRITICAL\" in caplog.text\n\n            # Check for color codes in the formatted text.\n            assert \"\\x1b[\" in caplog.text\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_646": ["def test_collect_unittest_case(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                self.assertTrue(True)\n        \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(passed=1)\n", "def test_teardown_called_on_exception(pytester: Pytester) -> None:\n    \"\"\"Ensure tearDown is called when an exception is raised during the test.\"\"\"\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                self.teardown_called = False\n\n                self.teardown_called = True\n\n                raise Exception(\"This is a test exception\")\n\n                assert self.teardown_called is True\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(failed=1, passed=1)\n", "def test_unittest_teardown_called_on_failure(pytester: Pytester) -> None:\n    \"\"\"Ensure tearDown is called even if the test fails\"\"\"\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                self.torn_down = True\n\n                self.torn_down = False\n                self.assertFalse(True)\n            \n                assert self.torn_down\n\n        \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(passed=1, failed=1)\n", "def test_inject_setup_teardown_fixtures(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @classmethod\n                cls.class_var = \"setup\"\n\n                self.instance_var = \"setup\"\n\n                assert self.class_var == \"setup\"\n                assert self.instance_var == \"setup\"\n\n                self.instance_var = \"teardown\"\n\n            @classmethod\n                cls.class_var = \"teardown\"\n\n            assert MyTestCase.class_var == \"teardown\"\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(passed=2)\n", "def test_teardown_with_exceptions(pytester: Pytester) -> None:\n    \"\"\"\n    Ensures that tearDown method handles exceptions and that they are reported correctly.\n    \"\"\"\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                raise Exception(\"teardown exception\")\n                \n                assert True\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-s\", testpath)\n    result.stdout.fnmatch_lines(\n        [\n            \"*teardown exception*\",\n            \"*1 error*\",\n        ]\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(failed=1)\n"], "sample_647": ["def test_unformatted_warning_format():\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning,\n        template=\"This is a test warning with value: {value}\",\n    )\n    formatted_warning = warning.format(value=42)\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"This is a test warning with value: 42\"\n", "def test_unformatted_warning_format():\n    warning_template = \"This is a {adjective} warning message with {number} placeholders.\"\n    unformatted_warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning,\n        template=warning_template\n    )\n    formatted_warning = unformatted_warning.format(adjective=\"test\", number=2)\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"This is a test warning message with 2 placeholders.\"\n", "def test_unformatted_warning_format():\n    \"\"\"Test the format method of UnformattedWarning.\"\"\"\n    warning = warning_types.UnformattedWarning(\n        warning_types.PytestWarning, \"This is a {adjective} warning!\"\n    )\n    formatted_warning = warning.format(adjective=\"test\")\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"This is a test warning!\"\n", "def test_unformatted_warning_format() -> None:\n    \"\"\"Test the UnformattedWarning class to ensure the format method works correctly.\"\"\"\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestExperimentalApiWarning,\n        template=\"{name} is an experimental feature.\",\n    )\n    formatted_warning = warning.format(name=\"new_feature\")\n    assert isinstance(formatted_warning, warning_types.PytestExperimentalApiWarning)\n    assert str(formatted_warning) == \"new_feature is an experimental feature.\"\n", "compilation error"], "sample_648": ["def test_mark_decorator_with_class_and_function() -> None:\n    \"\"\"Test MarkDecorator with a combination of class and function calls.\"\"\"\n    class SomeClass:\n        pass\n\n        pass\n\n    mark = pytest.mark.custom\n    marked_class = mark(SomeClass)\n    marked_function = mark(some_function)\n    assert marked_class is SomeClass\n    assert marked_function is some_function\n    assert hasattr(SomeClass, 'pytestmark')\n    assert hasattr(some_function, 'pytestmark')\n    assert any(m.name == 'custom' for m in get_unpacked_marks(SomeClass))\n    assert any(m.name == 'custom' for m in get_unpacked_marks(some_function))\n", "def test_parametrize_empty_parameterset_mark(pytester: Pytester) -> None:\n    \"\"\"Test behavior when empty parameter set mark is used.\"\"\"\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        {}=skip\n        \"\"\".format(\n            EMPTY_PARAMETERSET_OPTION\n        )\n    )\n    p1 = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"empty\", [])\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(str(p1))\n    result.stdout.fnmatch_lines(\n        [\n            \"collected 1 item\",\n            \"*test.py::test SKIPPED*\",\n            \"*= 1 skipped in *\",\n        ]\n    )\n    assert result.ret == 0\n", "def test_mark_decorator_with_args_and_kwargs() -> None:\n    mark_decorator = pytest.mark.example.with_args(1, 2, key='value')\n    \n    assert mark_decorator.args == (1, 2)\n    assert mark_decorator.kwargs == {'key': 'value'}\n    assert mark_decorator.name == 'example'\n\n        return x\n\n    marked_function = mark_decorator(some_function)\n    assert hasattr(marked_function, 'pytestmark')\n    assert any(mark.name == 'example' for mark in marked_function.pytestmark)\n    assert any(mark.args == (1, 2) for mark in marked_function.pytestmark)\n    assert any(mark.kwargs == {'key': 'value'} for mark in marked_function.pytestmark)\n", "def test_combined_with_merging() -> None:\n    mark1 = pytest.mark.some_mark(arg1=1)\n    mark2 = pytest.mark.some_mark(arg2=2)\n    combined_mark = mark1.mark.combined_with(mark2.mark)\n    assert combined_mark.name == \"some_mark\"\n    assert combined_mark.kwargs == {\"arg1\": 1, \"arg2\": 2}\n    assert combined_mark.args == mark1.mark.args + mark2.mark.args\n", "def test_mark_decorator_with_class_method(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        class TestClass:\n            @pytest.mark.foo\n                pass\n        \"\"\"\n    )\n    reprec = pytester.inline_run()\n    passed, skipped, failed = reprec.listoutcomes()\n    assert len(passed) == 1\n    assert list(passed[0].iter_markers(name=\"foo\"))\n"], "sample_649": ["def test_percent_style_multiline_formatting(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            handler = logging.StreamHandler()\n            formatter = logging.Formatter('%(levelname)s: %(message)s')\n            formatter._style = logging.PercentStyle('%(levelname)s: %(message)s', auto_indent=True)\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            logger.setLevel(logging.DEBUG)\n            \n            logger.info('This is a single line message')\n            logger.info('This is a\\\\nmulti-line\\\\nmessage')\n            assert True\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-s\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*INFO: This is a single line message*\",\n            \"*INFO: This is a*\",\n            \"*      multi-line*\",\n            \"*      message*\",\n        ]\n    )\n", "def test_percent_style_multiline(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            handler = caplog.handler\n            handler.setFormatter(logging.Formatter('%(message)s'))\n            logger.addHandler(handler)\n            \n            logger.info('first line\\\\nsecond line')\n            \n            assert len(caplog.records) == 1\n            assert caplog.records[0].message == 'first line\\\\nsecond line'\n            assert caplog.text == 'first line\\\\nsecond line\\\\n'\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*first line*\",\n            \"*second line*\",\n        ]\n    )\n", "def test_log_capture_fixture_set_level_restoration(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger('testlogger')\n            initial_level = logger.level\n            caplog.set_level(logging.DEBUG, 'testlogger')\n            logger.debug('This is a debug message')\n            assert logger.level == logging.DEBUG\n            assert caplog.records[-1].message == 'This is a debug message'\n            assert caplog.records[-1].levelno == logging.DEBUG\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_percent_style_multiline_formatting() -> None:\n    log_record_single_line = logging.LogRecord(\n        name=\"test_logger\",\n        level=logging.INFO,\n        pathname=__file__,\n        lineno=10,\n        msg=\"Single line message\",\n        args=(),\n        exc_info=None,\n    )\n    log_record_multiline = logging.LogRecord(\n        name=\"test_logger\",\n        level=logging.INFO,\n        pathname=__file__,\n        lineno=20,\n        msg=\"First line\\nSecond line\",\n        args=(),\n        exc_info=None,\n    )\n\n    formatter = PercentStyleMultiline(fmt=\"%(levelname)s: %(message)s\", auto_indent=True)\n\n    single_line_output = formatter.format(log_record_single_line)\n    multiline_output = formatter.format(log_record_multiline)\n\n    assert single_line_output == \"INFO: Single line message\"\n    assert multiline_output == \"INFO: First line\\n      Second line\"\n", "def test_percent_style_multiline_formatting(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        class CustomHandler(logging.StreamHandler):\n                super().__init__()\n                self.records = []\n\n                self.records.append(self.format(record))\n\n        logger = logging.getLogger(\"test_multiline\")\n        handler = CustomHandler()\n        formatter = logging.Formatter(\"%(levelname)s: %(message)s\")\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(logging.DEBUG)\n\n            logger.info(\"First line\\\\nSecond line\")\n            assert handler.records[0] == \"INFO: First line\\\\nSecond line\"\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n"], "sample_650": ["def test_log_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Check that log_auto_indent affects output correctly.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('This is a warning\\\\nwith multiple lines')\n            logger.info('This is an info\\\\nwith multiple lines')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(levelname)s: %(message)s\n        log_auto_indent=4\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"WARNING: This is a warning\",\n            \"    with multiple lines\",\n            \"INFO: This is an info\",\n            \"    with multiple lines\",\n        ]\n    )\n", "def test_percentstylemultiline_auto_indent(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('line1\\\\nline2\\\\nline3', extra={\"auto_indent\": True})\n            assert caplog.records[0].message == 'line1\\\\nline2\\\\nline3'\n            assert caplog.text == 'INFO     test_percentstylemultiline_auto_indent.py:6 line1\\\\n         line2\\\\n         line3\\\\n'\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Test auto-indent functionality of multiline log messages.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('First line\\\\nSecond line\\\\nThird line')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_auto_indent=true\n        log_level=INFO\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO    *First line\",\n            \"*         Second line\",\n            \"*         Third line\",\n        ]\n    )\n", "def test_percentstylemultiline_auto_indent(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        class TestFormatter(logging.Formatter):\n                super().__init__('%(levelname)s: %(message)s')\n\n            logger = logging.getLogger('test')\n            handler = logging.StreamHandler()\n            formatter = TestFormatter()\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            logger.setLevel(logging.DEBUG)\n            logger.propagate = False\n\n            record = logger.makeRecord('test', logging.DEBUG, None, None, 'First line\\\\nSecond line', None, None)\n            assert isinstance(formatter._style, logging.PercentStyle)\n            formatter._style = PercentStyleMultiline('%(levelname)s: %(message)s', auto_indent=True)\n            formatted = formatter.format(record)\n            expected = 'DEBUG: First line\\\\n      Second line'\n            assert formatted == expected\n\n            logger = logging.getLogger('test')\n            handler = logging.StreamHandler()\n            formatter = TestFormatter()\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            logger.setLevel(logging.DEBUG)\n            logger.propagate = False\n\n            record = logger.makeRecord('test', logging.DEBUG, None, None, 'First line\\\\nSecond line', None, None)\n            formatter._style = PercentStyleMultiline('%(levelname)s: %(message)s', auto_indent=10)\n            formatted = formatter.format(record)\n            expected = 'DEBUG: First line\\\\n          Second line'\n            assert formatted == expected\n\n            logger = logging.getLogger('test')\n            handler = logging.StreamHandler()\n            formatter = TestFormatter()\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            logger.setLevel(logging.DEBUG)\n            logger.propagate = False\n\n            record = logger.makeRecord('test', logging.DEBUG, None, None, 'First line\\\\nSecond line', None, None)\n            formatter._style = PercentStyleMultiline('%(levelname)s: %(message)s', auto_indent=False)\n            formatted = formatter.format(record)\n            expected = 'DEBUG: First line\\\\nSecond line'\n            assert formatted == expected\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=3)\n", "def test_log_file_custom_handler(pytester: Pytester) -> None:\n    \"\"\"Test custom log file handler functionality.\"\"\"\n    log_file = str(pytester.path.joinpath(\"custom_log.log\"))\n\n    pytester.makeconftest(\n        \"\"\"\n        import logging\n        import os\n\n        class CustomFileHandler(logging.FileHandler):\n                record.msg = \"Custom: \" + record.msg\n                super().emit(record)\n\n            plugin = config.pluginmanager.get_plugin('logging-plugin')\n            handler = CustomFileHandler(os.path.join({}, 'custom_log.log'))\n            handler.setLevel(logging.INFO)\n            handler.setFormatter(plugin.formatter)\n            logging.getLogger().addHandler(handler)\n        \"\"\".format(\n            pytester.path\n        )\n    )\n\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('original message')\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"-s\")\n    result.assert_outcomes(passed=1)\n\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert \"Custom: original message\" in contents\n"], "sample_652": ["def test_call_with_optional_argument():\n        return arg\n\n        return \"no arg\"\n\n    assert _call_with_optional_argument(func_with_arg, \"test\") == \"test\"\n    assert _call_with_optional_argument(func_without_arg, \"test\") == \"no arg\"\n", "def test_invalid_fixture_scope():\n    import pytest\n\n    with pytest.raises(ValueError, match=\"Got an unexpected scope value 'invalid'\"):\n        @pytest.fixture(scope=\"invalid\")\n            pass\n", "def test_pytest_addoption():\n    parser = pytest.Parser()\n    pytest_addoption(parser)\n    group = parser.getgroup(\"general\")\n    option = group.options[0]\n\n    assert option.dest == \"showfixtures\"\n    assert option.default is False\n    assert option.help == \"show available fixtures, sorted by plugin appearance (fixtures with leading '_' are only shown with '-v')\"\n\n    option = group.options[1]\n    assert option.dest == \"show_fixtures_per_test\"\n    assert option.default is False\n    assert option.help == \"show fixtures per test\"\n\n    assert parser._inidict[\"usefixtures\"][\"default\"] == []\n    assert parser._inidict[\"python_files\"][\"default\"] == [\"test_*.py\", \"*_test.py\"]\n    assert parser._inidict[\"python_classes\"][\"default\"] == [\"Test\"]\n    assert parser._inidict[\"python_functions\"][\"default\"] == [\"test\"]\n    assert parser._inidict[\"disable_test_id_escaping_and_forfeit_all_rights_to_community_support\"][\"default\"] is False\n\n    option = group.options[2]\n    assert option.dest == \"importmode\"\n    assert option.default == \"prepend\"\n    assert option.help == \"prepend/append to sys.path when importing test modules, default is to prepend.\"\n", "def test_pyobj_property():\n    class MockParent:\n            self.obj = obj\n\n            return self\n\n    class TestNode:\n            self.parent = MockParent(obj)\n            self.getparent = pyobj_property(\"Test\")\n\n    node = TestNode(\"test_obj\")\n    assert node.parent.getparent(\"Test\") == \"test_obj\"\n", "def test_pyobj_property():\n    class MockParent:\n            self.obj = obj\n\n            return MockParent(obj=name)\n\n    class TestNode:\n            self.getparent = MockParent\n\n    node = TestNode()\n    func_prop = pyobj_property(\"Function\")\n    assert func_prop.__doc__ == \"python function object this node was collected from (can be None).\"\n    assert func_prop.__get__(node) == \"Function\"\n\n    class_prop = pyobj_property(\"Class\")\n    assert class_prop.__doc__ == \"python class object this node was collected from (can be None).\"\n    assert class_prop.__get__(node) == \"Class\"\n\n    instance_prop = pyobj_property(\"Instance\")\n    assert instance_prop.__doc__ == \"python instance object this node was collected from (can be None).\"\n    assert instance_prop.__get__(node) == \"Instance\"\n"], "sample_655": ["def test_capture_fixtures_interaction(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import sys\n        \n        @pytest.fixture\n            print(\"fixture using capsys\")\n            yield\n            out, err = capsys.readouterr()\n            assert \"fixture using capsys\" in out\n        \n        @pytest.fixture\n            print(\"fixture using capfd\")\n            yield\n            out, err = capfd.readouterr()\n            assert \"fixture using capfd\" in out\n        \n            print(\"test interaction\")\n            assert True\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_readouterr_resets_internal_buffer(capsys):\n    print(\"test output\")\n    captured = capsys.readouterr()\n    assert captured.out == \"test output\\n\"\n    assert captured.err == \"\"\n    \n    print(\"new test output\")\n    captured = capsys.readouterr()\n    assert captured.out == \"new test output\\n\"\n    assert captured.err == \"\"\n", "def test_capture_and_teardown_logging(testdir):\n    \"\"\"Ensure that both capturing and teardown logging work correctly together.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.warning(\"teardown log\")\n            print(\"teardown print\")\n            assert 0\n\n            print(\"in func1\")\n            logging.info(\"func1 log\")\n            out, err = capfd.readouterr()\n            assert out == \"in func1\\\\n\"\n            assert err == \"\"\n\n            print(\"in func2\")\n            logging.info(\"func2 log\")\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_func1*\",\n            \"*Captured stdout call*\",\n            \"in func1\",\n            \"*Captured stderr teardown*\",\n            \"teardown print\",\n            \"*teardown log*\",\n            \"*test_func2*\",\n            \"*Captured stdout call*\",\n            \"in func2\",\n            \"*Captured stderr teardown*\",\n            \"teardown print\",\n            \"*teardown log*\",\n            \"*2 failed*\",\n        ]\n    )\n", "def test_capture_fixture_disabled_in_context_manager(capsys):\n    print(\"Captured outside context\")\n    with capsys.disabled():\n        print(\"Captured inside context\")\n    captured = capsys.readouterr()\n    assert captured.out == \"Captured outside context\\n\"\n    assert captured.err == \"\"\n", "def test_global_capturing_management():\n    capman = CaptureManager(\"sys\")\n    assert not capman.is_capturing()\n    capman.start_global_capturing()\n    assert capman.is_capturing() == \"global\"\n    capman.suspend_global_capture()\n    assert not capman.is_capturing()\n    capman.resume_global_capture()\n    assert capman.is_capturing() == \"global\"\n    capman.stop_global_capturing()\n    assert not capman.is_capturing()\n"], "sample_656": ["def test_capture_global_and_fixture_disabled():\n    capsys = StdCapture()\n    with capsys:\n        print(\"captured before\")\n        with capsys.global_and_fixture_disabled():\n            print(\"while capture is disabled\")\n        print(\"captured after\")\n        out, err = capsys.readouterr()\n    assert out == \"captured before\\ncaptured after\\n\"\n    assert \"while capture is disabled\" not in out\n", "def test_syscapture_resume_from_suspended_state():\n    cap = capture.SysCapture(1)\n    cap.start()\n    sys.stdout.write(\"before suspend\")\n    assert cap.snap() == \"before suspend\"\n    cap.suspend()\n    sys.stdout.write(\"during suspend\")\n    assert cap.snap() == \"\"\n    cap.resume()\n    sys.stdout.write(\"after resume\")\n    assert cap.snap() == \"after resume\"\n    cap.done()\n", "def test_capture_manager_repr():\n    capman = CaptureManager(\"fd\")\n    assert repr(capman) == \"<CaptureManager _method='fd' _global_capturing=None _current_item=None>\"\n    capman.start_global_capturing()\n    assert \"_global_capturing=<MultiCapture\" in repr(capman)\n    capman.stop_global_capturing()\n    assert \"_global_capturing=None\" in repr(capman)\n", "def test_capture_readouterr_on_closed_capture():\n    cap = capture.MultiCapture(out=True, err=True, in_=False, Capture=capture.SysCapture)\n    cap.start_capturing()\n    print(\"test output\")\n    cap.stop_capturing()\n    with pytest.raises(ValueError, match=\"I/O operation on closed file.\"):\n        cap.readouterr()\n", "def test_global_and_fixture_disabled(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            print(\"captured before\")\n            with capsys.disabled():\n                print(\"no capture\")\n            print(\"captured after\")\n            out, err = capsys.readouterr()\n            assert out == \"captured before\\\\ncaptured after\\\\n\"\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert \"no capture\" in result.stdout.str()\n    assert \"captured before\" not in result.stdout.str()\n    assert \"captured after\" not in result.stdout.str()\n"], "sample_657": ["def test_mark_decorator_with_args():\n    mark = pytest.mark.foo.with_args(\"arg1\", \"arg2\", key=\"value\")\n    assert mark.args == (\"arg1\", \"arg2\")\n    assert mark.kwargs == {\"key\": \"value\"}\n    assert mark.name == \"foo\"\n    assert repr(mark) == \"<MarkDecorator Mark(name='foo', args=('arg1', 'arg2'), kwargs={'key': 'value'})>\"\n", "def test_mark_decorator_with_args():\n    @pytest.mark.test_mark(arg1=1, arg2='value')\n        pass\n\n    assert hasattr(test_func, \"pytestmark\")\n    mark = test_func.pytestmark[0]\n    assert mark.name == \"test_mark\"\n    assert mark.kwargs == {\"arg1\": 1, \"arg2\": 'value'}\n", "def test_parametrize_args_parsing():\n    argnames, force_tuple = ParameterSet._parse_parametrize_args(\"a, b, c\", None)\n    assert argnames == [\"a\", \"b\", \"c\"]\n    assert not force_tuple\n\n    argnames, force_tuple = ParameterSet._parse_parametrize_args(\"a\", None)\n    assert argnames == [\"a\"]\n    assert force_tuple\n\n    argnames, force_tuple = ParameterSet._parse_parametrize_args([\"a\", \"b\", \"c\"], None)\n    assert argnames == [\"a\", \"b\", \"c\"]\n    assert not force_tuple\n", "def test_mark_decorator_combined_with():\n    mark1 = pytest.mark.foo\n    mark2 = pytest.mark.foo(arg1=\"value1\", arg2=\"value2\")\n    combined_mark = mark1.combined_with(mark2)\n    assert combined_mark.name == \"foo\"\n    assert combined_mark.args == mark2.args\n    assert combined_mark.kwargs == mark2.kwargs\n", "def test_get_empty_parameterset_mark_skip(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        empty_parameter_set_mark = skip\n    \"\"\"\n    )\n    config = testdir.parseconfig()\n    from _pytest.mark import get_empty_parameterset_mark\n\n        pass\n\n    mark = get_empty_parameterset_mark(config, (\"a\", \"b\"), dummy_func)\n    assert mark.name == \"skip\"\n    assert mark.kwargs[\"reason\"].startswith(\"got empty parameter set \")\n"], "sample_658": ["    def test_doctest_continue_on_failure_flag(self, testdir):\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            doctest_continue_on_failure = True\n        \"\"\"\n        )\n        testdir.maketxtfile(\n            test_something=\"\"\"\n            >>> i = 5\n            >>> def foo():\n            ...     raise ValueError('error1')\n            >>> foo()\n            >>> i\n            5\n            >>> i + 2\n            7\n            >>> i + 1\n            6\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.assert_outcomes(passed=1, failed=1)\n        result.stdout.fnmatch_lines(\n            [\"*5: DocTestFailure*\", \"*8: DocTestFailure*\"]\n        )\n", "    def test_doctestmodule_with_mocked_object(self, testdir):\n        \"\"\"Test that doctests can be collected and run with mocked objects\n        without causing inspect.unwrap to fail.\n        \"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            from unittest.mock import Mock\n\n            class Example:\n                \"\"\"\n                >>> mock = Mock()\n                >>> mock.method()\n                <Mock name='mock.method()' id='...'>\n                \"\"\"\n                pass\n        \"\"\"\n        )\n        reprec = testdir.inline_run(\"--doctest-modules\")\n        reprec.assertoutcome(passed=1)\n", "    def test_collect_file_with_ignore_import_errors(self, testdir):\n        path = testdir.makepyfile(\n            \"\"\"\n            import non_existent_module\n\n                '''\n                >>> dummy_function()\n                '''\n                pass\n        \"\"\"\n        )\n        items, reprec = testdir.inline_genitems(path, \"--doctest-modules\", \"--doctest-ignore-import-errors\")\n        assert len(items) == 1\n        assert isinstance(items[0], DoctestItem)\n        assert isinstance(items[0].parent, DoctestModule)\n        reprec = testdir.inline_run(path, \"--doctest-modules\", \"--doctest-ignore-import-errors\")\n        reprec.assertoutcome(skipped=1)\n", "    def test_doctestmodule_with_mocked_object(self, testdir):\n        \"\"\"Test that doctests correctly handle mocked objects without causing recursion errors.\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            from unittest.mock import MagicMock\n\n                '''\n                >>> mocked_obj = MagicMock()\n                >>> mocked_obj.some_method()\n                '''\n                pass\n            \"\"\"\n        )\n        reprec = testdir.inline_run(\"--doctest-modules\")\n        reprec.assertoutcome(passed=1)\n", "    def test_doctestmodule_continue_on_failure_flag(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            '''\n            >>> x = 1\n            >>> y = 2\n            >>> x\n            2\n            >>> y\n            3\n            >>> x + y\n            4\n            '''\n        \"\"\"\n        )\n        reprec = testdir.inline_run(p, \"--doctest-modules\", \"--doctest-continue-on-failure\")\n        reprec.assertoutcome(failed=1)\n        out = reprec.stdout.str()\n        assert \"DocTestFailure\" in out\n        assert \">>> x\" in out\n        assert \">>> y\" in out\n        assert \">>> x + y\" not in out\n"], "sample_659": ["    def test_traceback_cut(self):\n                raise ValueError(\"foo\")\n            g()\n\n        try:\n            f()\n        except ValueError:\n            excinfo = pytest.ExceptionInfo.from_current()\n            tb = excinfo.traceback\n            assert len(tb) > 1\n            cut_tb = tb.cut(path=tb[-1].path, lineno=tb[-1].lineno)\n            assert len(cut_tb) == 1\n            assert cut_tb[-1].frame.code.name == \"g\"\n", "    def test_traceback_entry_getsource(self):\n        import _pytest._code\n\n        source_code = \"def foo():\\n    return 42\\nfoo()\"\n        code = compile(source_code, \"<string>\", \"exec\")\n        frame = None\n\n        try:\n            exec(code)\n        except:\n            tb = sys.exc_info()[2]\n            while tb.tb_next:\n                tb = tb.tb_next\n            frame = tb.tb_frame\n\n        assert frame is not None\n        entry = _pytest._code.TracebackEntry(tb)\n\n        source = entry.getsource()\n        assert source is not None\n        assert \"def foo():\" in str(source)\n        assert \"return 42\" in str(source)\n", "def test_Traceback_cut(self):\n    import traceback\n\n        try:\n            raise ValueError(\"test error\")\n        except ValueError:\n            tb = sys.exc_info()[2]\n            return Traceback(tb)\n\n    tb = func()\n    cut_tb = tb.cut(path=__file__, lineno=sys._getframe().f_lineno - 5)\n\n    assert len(cut_tb) == 1\n    assert cut_tb[0].frame.code.path == __file__\n    assert cut_tb[0].lineno == sys._getframe().f_lineno - 7\n", "    def test_code_getargs(self):\n            pass\n\n        code_obj = Code(sample_func)\n        assert code_obj.getargs() == (\"a\", \"b\")\n        assert code_obj.getargs(var=True) == (\"a\", \"b\", \"args\", \"kwargs\")\n", "    def test_tracebackentry_repr(self):\n        \"\"\"Test __repr__ method of TracebackEntry.\"\"\"\n        try:\n            1 / 0\n        except ZeroDivisionError:\n            excinfo = pytest.raises(ExceptionInfo)\n            entry = excinfo.traceback[-1]\n            assert repr(entry) == \"<TracebackEntry %s:%d>\" % (entry.frame.code.path, entry.lineno + 1)\n"], "sample_660": ["def test_record_multiple_properties(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            record_property(\"key1\", \"value1\")\n            record_property(\"key2\", \"value2\")\n            record_property(\"key3\", \"value3\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-rwv\")\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    psnode = tnode.find_first_by_tag(\"properties\")\n    pnodes = psnode.find_by_tag(\"property\")\n    pnodes[0].assert_attr(name=\"key1\", value=\"value1\")\n    pnodes[1].assert_attr(name=\"key2\", value=\"value2\")\n    pnodes[2].assert_attr(name=\"key3\", value=\"value3\")\n", "def test_bin_xml_escape():\n    from _pytest.junitxml import bin_xml_escape\n\n    assert bin_xml_escape(\"simple text\").uniobj == \"simple text\"\n    assert bin_xml_escape(\"text with <xml> tags\").uniobj == \"text with #x003Cxml#x003E tags\"\n    assert bin_xml_escape(\"null byte \\x00 in text\").uniobj == \"null byte #x00 in text\"\n", "def test_bin_xml_escape():\n    from _pytest.junitxml import bin_xml_escape\n\n    input_string = \"Invalid \\x01\\x02 characters & < > ' \\\" should be escaped\"\n    escaped_string = bin_xml_escape(input_string)\n    assert (\n        escaped_string.uniobj\n        == \"Invalid #x01#x02 characters &amp; &lt; &gt; &apos; &quot; should be escaped\"\n    )\n", "def test_bin_xml_escape():\n    from _pytest.junitxml import bin_xml_escape\n\n    escaped = bin_xml_escape(\"<test&>\")\n    assert str(escaped) == \"#x003Ctest#x0026#x003E\", f\"Unexpected escape sequence: {escaped}\"\n\n    escaped = bin_xml_escape(\"normaltext\")\n    assert str(escaped) == \"normaltext\", f\"Unexpected escape sequence: {escaped}\"\n", "def test_custom_properties_node(testdir):\n    path = testdir.tmpdir.join(\"test_custom_properties.xml\")\n    log = LogXML(str(path), None)\n\n    class Report(BaseReport):\n        sections = []\n        nodeid = \"test_node_id\"\n\n    log.pytest_sessionstart()\n    log.add_global_property(\"env\", \"staging\")\n    log.add_global_property(\"version\", \"1.0.0\")\n    log.pytest_sessionfinish()\n\n    dom = minidom.parse(str(path))\n\n    properties = dom.getElementsByTagName(\"properties\")\n\n    assert properties.length == 1, \"There must be one <properties> node\"\n\n    property_list = dom.getElementsByTagName(\"property\")\n\n    assert property_list.length == 2, \"There must be only 2 property nodes\"\n\n    expected = {\"env\": \"staging\", \"version\": \"1.0.0\"}\n    actual = {}\n\n    for p in property_list:\n        k = str(p.getAttribute(\"name\"))\n        v = str(p.getAttribute(\"value\"))\n        actual[k] = v\n\n    assert actual == expected\n"], "sample_661": ["def test_record_testsuite_property_with_special_chars(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"special_chars\", \"<>&'\\\"\")\n\n            record_testsuite_property(\"escaped_chars\", \"<>&'\\\"\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p2_node = properties_node.find_nth_by_tag(\"property\", 1)\n    p1_node.assert_attr(name=\"special_chars\", value=\"&lt;&gt;&amp;&apos;&quot;\")\n    p2_node.assert_attr(name=\"escaped_chars\", value=\"&lt;&gt;&amp;&apos;&quot;\")\n", "def test_record_testsuite_property_type_checking_value_error(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"stats\", {})\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--junitxml=tests.xml\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\"*TypeError: value parameter needs to be a string, but dict given\"]\n    )\n", "def test_bin_xml_escape():\n    from _pytest.junitxml import bin_xml_escape\n    \n    # Testing standard XML escape behavior\n    assert bin_xml_escape(\"This is a <test> & a test\").uniobj == \"This is a &lt;test&gt; &amp; a test\"\n    \n    # Testing for invalid XML characters\n    assert bin_xml_escape(\"Invalid char \\x00 in string\").uniobj == \"Invalid char #x00 in string\"\n    \n    # Testing for UTF-8 characters\n    assert bin_xml_escape(\"UTF-8 char: \\u263A\").uniobj == \"UTF-8 char: \u263a\"\n    \n    # Testing for high unicode characters\n    assert bin_xml_escape(\"High unicode char: \\U0001F600\").uniobj == \"High unicode char: \ud83d\ude00\"\n", "def test_bin_xml_escape():\n    from _pytest.junitxml import bin_xml_escape\n\n    test_string = \"Here is a string with illegal XML char \\x01 and \\x0B.\"\n    escaped_string = bin_xml_escape(test_string).uniobj\n    assert \"#x01\" in escaped_string\n    assert \"#x0B\" in escaped_string\n    assert \"Here is a string with illegal XML char \" in escaped_string\n", "def test_record_xml_attribute_with_special_characters(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            record_xml_attribute(\"foo\", \"<&>\")\n            record_xml_attribute(\"bar\", 'special \"chars\" & <test>');\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-rw\")\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    tnode.assert_attr(foo=\"<&>\")\n    tnode.assert_attr(bar='special \"chars\" & <test>')\n    result.stdout.fnmatch_lines(\n        [\"*test_record_xml_attribute_with_special_characters.py:6:*record_xml_attribute is an experimental feature\"]\n    )\n"], "sample_662": ["def test_collect_error_repr_serialization():\n    \"\"\"Test serialization and deserialization of CollectErrorRepr.\"\"\"\n    from _pytest.reports import CollectErrorRepr\n\n    error_msg = \"This is a collection error\"\n    collect_error_repr = CollectErrorRepr(error_msg)\n\n    class MockOut:\n            self.lines = []\n\n            self.lines.append(msg)\n\n    out = MockOut()\n    collect_error_repr.toterminal(out)\n    assert out.lines == [error_msg]\n\n    report_dict = {\"longrepr\": error_msg, \"$report_type\": \"CollectErrorRepr\"}\n    serialized = pytest_report_to_serializable(collect_error_repr)\n    assert serialized == report_dict\n\n    deserialized = pytest_report_from_serializable(report_dict)\n    assert isinstance(deserialized, CollectErrorRepr)\n    assert deserialized.longrepr == error_msg\n", "def test_getslaveinfoline_cache(testdir):\n    \"\"\"Test that `getslaveinfoline` caches the result in `_slaveinfocache`.\"\"\"\n    class Node:\n            self.slaveinfo = slaveinfo\n\n    node_info = {\n        \"id\": \"node1\",\n        \"sysplatform\": \"linux\",\n        \"version_info\": (3, 8, 5),\n        \"executable\": \"/usr/bin/python3\"\n    }\n    \n    node = Node(node_info)\n\n    # Call getslaveinfoline first time, should compute and cache the result\n    line1 = getslaveinfoline(node)\n    assert hasattr(node, '_slaveinfocache')\n    assert node._slaveinfocache == line1\n\n    # Call getslaveinfoline second time, should return cached result\n    line2 = getslaveinfoline(node)\n    assert line1 == line2\n", "compilation error", "    def test_longreprtext_property(self, testdir):\n        \"\"\"Test the longreprtext property of TestReport.\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n                assert False, 'Expected Message'\n                pass\n            \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n\n        failed_report = reports[1]\n        assert failed_report.when == \"call\"\n        assert failed_report.outcome == \"failed\"\n        assert \"Expected Message\" in failed_report.longreprtext\n\n        passed_report = reports[4]\n        assert passed_report.when == \"call\"\n        assert passed_report.outcome == \"passed\"\n        assert passed_report.longreprtext == \"\"\n", "def test_tostring_on_collecterrorrepr(self, testdir):\n    \"\"\"Test to ensure CollectErrorRepr's toterminal method works correctly.\"\"\"\n    reprec = testdir.inline_runsource(\n        \"\"\"\n            raise ValueError('test error')\n        \"\"\"\n    )\n    reports = reprec.getreports(\"pytest_collectreport\")\n    assert reports\n    for rep in reports:\n        error_repr = CollectErrorRepr(\"This is a test error message\")\n        out = py.io.TerminalWriter(stringio=True)\n        error_repr.toterminal(out)\n        output = out.stringio.getvalue().strip()\n        assert output == \"This is a test error message\"\n"], "sample_663": ["def test_pytest_ignore_collect_config_option(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n            return \"ignore\" in path.basename\n    \"\"\"\n    )\n    ignored_file = testdir.tmpdir.join(\"ignore_me.py\")\n    ignored_file.write(\"def test_ignore(): pass\")\n    \n    result = testdir.runpytest(\"--ignore-glob=ignore*\")\n    result.stdout.no_fnmatch_line(\"*ignore_me*\")\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n", "def test_pytest_addoption():\n    from _pytest.config import Config\n    from _pytest.config.argparsing import Parser\n\n    parser = Parser()\n    pytest_addoption(parser)\n    config = Config()\n    config._parser = parser\n\n    # Check default values\n    assert config.getini(\"norecursedirs\") == [\n        \".*\", \"build\", \"dist\", \"CVS\", \"_darcs\", \"{arch}\", \"*.egg\", \"venv\"\n    ]\n    assert config.getini(\"testpaths\") == []\n    assert config.getoption(\"maxfail\") == 0\n    assert config.getoption(\"strict_markers\") is False\n    assert config.getoption(\"inifilename\") is None\n    assert config.getoption(\"continue_on_collection_errors\") is False\n    assert config.getoption(\"rootdir\") is None\n    assert config.getoption(\"collectonly\") is False\n    assert config.getoption(\"pyargs\") is False\n    assert config.getoption(\"ignore\") is None\n    assert config.getoption(\"ignore_glob\") is None\n    assert config.getoption(\"deselect\") is None\n    assert config.getoption(\"confcutdir\") is None\n    assert config.getoption(\"noconftest\") is False\n    assert config.getoption(\"keepduplicates\") is False\n    assert config.getoption(\"collect_in_virtualenv\") is False\n    assert config.getoption(\"basetemp\") is None\n", "def test_exit_first_on_first_failure(testdir):\n    \"\"\"Verify that pytest exits immediately on the first test failure when using the --exitfirst option.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            pass\n        \n            assert 0\n        \n            assert True\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-x\")\n    result.stdout.fnmatch_lines([\n        \"*def test_fail():*\",\n        \"*assert 0*\",\n        \"*1 failed, 1 passed in*\"\n    ])\n    assert result.ret == ExitCode.TESTS_FAILED\n    result.stdout.no_fnmatch_line(\"*test_not_run*\")\n", "def test_exit_code_enum():\n    assert ExitCode.OK == 0\n    assert ExitCode.TESTS_FAILED == 1\n    assert ExitCode.INTERRUPTED == 2\n    assert ExitCode.INTERNAL_ERROR == 3\n    assert ExitCode.USAGE_ERROR == 4\n    assert ExitCode.NO_TESTS_COLLECTED == 5\n", "def test_wrap_session_exit_codes(testdir):\n    from _pytest.main import wrap_session, ExitCode, UsageError, Failed\n\n        return None\n\n    config = testdir.parseconfig()\n    \n    # Test OK exit code\n    exit_status = wrap_session(config, mock_doit)\n    assert exit_status == ExitCode.OK\n\n    # Test USAGE_ERROR exit code\n        raise UsageError(\"usage error\")\n    \n    exit_status = wrap_session(config, raise_usage_error)\n    assert exit_status == ExitCode.USAGE_ERROR\n\n    # Test TESTS_FAILED exit code\n        raise Failed(\"test failed\")\n    \n    exit_status = wrap_session(config, raise_failed_error)\n    assert exit_status == ExitCode.TESTS_FAILED\n\n    # Test INTERRUPTED exit code\n        raise KeyboardInterrupt\n    \n    exit_status = wrap_session(config, raise_keyboard_interrupt)\n    assert exit_status == ExitCode.INTERRUPTED\n\n    # Test INTERNAL_ERROR exit code\n        raise Exception(\"internal error\")\n    \n    exit_status = wrap_session(config, raise_internal_error)\n    assert exit_status == ExitCode.INTERNAL_ERROR\n"], "sample_664": ["def test_funcargnames_deprecation(testdir):\n    \"\"\"Check that using `funcargnames` attribute raises a deprecation warning\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from pytest import Item\n\n            item = Item(name='test_func', parent=None, config=None)\n            _ = item.funcargnames\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`,*\",\n            \"*since pytest 2.3 - use the newer attribute instead.*\",\n        ]\n    )\n", "def test_funcargnames_is_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`*\",\n            \"*since pytest 2.3 - use the newer attribute instead*\",\n        ]\n    )\n", "def test_fixture_positional_arguments_deprecation_warning(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(\"session\")\n            return \"example\"\n        \n            assert example_fixture == \"example\"\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\"\n        ]\n    )\n", "def test_funcargnames_deprecation_warning():\n    warning_msg = (\n        \"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for \"\n        \"`fixturenames`, since pytest 2.3 - use the newer attribute instead.*\"\n    )\n    with pytest.warns(pytest.PytestDeprecationWarning, match=warning_msg):\n        pytest.deprecated.FUNCARGNAMES  # Access the deprecated attribute to trigger the warning\n", "def test_fixture_positional_arguments_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(\"module\")\n            pass\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\",\n        ]\n    )\n"], "sample_665": ["def test_pytest_addoption_default_values():\n    from _pytest.config import Config\n\n    parser = Config()._getini\n    pytest_addoption(parser)\n\n    assert parser('python_files') == ['test_*.py', '*_test.py']\n    assert parser('python_classes') == ['Test']\n    assert parser('python_functions') == ['test']\n    assert parser('disable_test_id_escaping_and_forfeit_all_rights_to_community_support') == False\n", "def test_pytest_addoption():\n    from _pytest.config.argparsing import Parser\n    parser = Parser()\n    pytest_addoption(parser)\n    group = parser.getgroup(\"general\")\n    assert any(opt.dest == \"showfixtures\" for opt in group.options)\n    assert any(opt.dest == \"show_fixtures_per_test\" for opt in group.options)\n    assert any(opt.dest == \"importmode\" for opt in group.options)\n    assert parser.getini(\"python_files\") == [\"test_*.py\", \"*_test.py\"]\n    assert parser.getini(\"python_classes\") == [\"Test\"]\n    assert parser.getini(\"python_functions\") == [\"test\"]\n    assert parser.getini(\"disable_test_id_escaping_and_forfeit_all_rights_to_community_support\") is False\n", "def test_pytest_addoption_import_mode(testdir):\n    result = testdir.runpytest('--help')\n    result.stdout.fnmatch_lines([\n        \"*--import-mode*prepend/append*prepend*\",\n        \"*prepend/append to sys.path when importing test modules, default is to prepend.*\",\n    ])\n", "def test_pytest_addoption(testdir):\n    testdir.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", action=\"store_true\", help=\"foo option\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--foo\", \"--help\")\n    result.stdout.fnmatch_lines([\"--foo            foo option\"])\n", "def test_pytest_pyfunc_call_async_warn(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import asyncio\n\n        @pytest.mark.asyncio\n        async def test_async_function():\n            await asyncio.sleep(0.1)\n            assert True\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n    assert result.ret == 0\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.asyncio\n        async def test_async_function_no_plugin():\n            assert True\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*async def function and no async plugin installed (see warnings)*\"])\n    assert \"1 passed\" not in result.stdout.str()\n    assert \"1 skipped\" in result.stdout.str()\n    assert result.ret == 0\n"], "sample_666": ["def test_colored_level_formatter():\n    from _pytest.logging import ColoredLevelFormatter\n    from _pytest.logging import create_terminal_writer\n\n    class MockTerminalWriter:\n            return f\"[{','.join(kw.keys())}]{text}[/{','.join(kw.keys())}]\"\n\n    log_format = \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\"\n    terminal_writer = MockTerminalWriter()\n    formatter = ColoredLevelFormatter(terminal_writer, log_format)\n\n    record = logging.LogRecord(\n        name=\"test\",\n        level=logging.INFO,\n        pathname=__file__,\n        lineno=123,\n        msg=\"Test message\",\n        args=(),\n        exc_info=None,\n    )\n\n    formatted = formatter.format(record)\n    assert \"[green]\" in formatted\n    assert \"INFO\" in formatted\n", "def test_percentstylemultiline_formatting():\n    formatter = PercentStyleMultiline(\"%(message)s\", auto_indent=True)\n    record = logging.LogRecord(\n        name=\"test\",\n        level=logging.INFO,\n        pathname=\"test.py\",\n        lineno=10,\n        msg=\"line1\\nline2\\nline3\",\n        args=(),\n        exc_info=None,\n    )\n    formatted = formatter.format(record)\n    assert formatted == \"line1\\n       line2\\n       line3\"\n\n    formatter = PercentStyleMultiline(\"%(message)s\", auto_indent=False)\n    record = logging.LogRecord(\n        name=\"test\",\n        level=logging.INFO,\n        pathname=\"test.py\",\n        lineno=10,\n        msg=\"line1\\nline2\\nline3\",\n        args=(),\n        exc_info=None,\n    )\n    formatted = formatter.format(record)\n    assert formatted == \"line1\\nline2\\nline3\"\n\n    formatter = PercentStyleMultiline(\"%(message)s\", auto_indent=4)\n    record = logging.LogRecord(\n        name=\"test\",\n        level=logging.INFO,\n        pathname=\"test.py\",\n        lineno=10,\n        msg=\"line1\\nline2\\nline3\",\n        args=(),\n        exc_info=None,\n    )\n    formatted = formatter.format(record)\n    assert formatted == \"line1\\n    line2\\n    line3\"\n", "def test_colored_level_formatter():\n    from _pytest.config import create_terminal_writer\n    terminal_writer = create_terminal_writer(pytest.Config())\n    formatter = ColoredLevelFormatter(terminal_writer, \"%(levelname)s: %(message)s\")\n    \n    record = logging.LogRecord(\"test\", logging.WARNING, \"\", 0, \"This is a warning\", None, None)\n    formatted_message = formatter.format(record)\n    assert \"This is a warning\" in formatted_message\n    assert \"WARNING\" in formatted_message\n    assert \"\\x1b[\" in formatted_message  # Check for ANSI escape sequences\n\n    record = logging.LogRecord(\"test\", logging.ERROR, \"\", 0, \"This is an error\", None, None)\n    formatted_message = formatter.format(record)\n    assert \"This is an error\" in formatted_message\n    assert \"ERROR\" in formatted_message\n    assert \"\\x1b[\" in formatted_message  # Check for ANSI escape sequences\n", "def test_colored_level_formatter():\n    from _pytest.config import Config\n    from _pytest.config import create_terminal_writer\n\n    class MockTerminalWriter:\n            self.records = []\n\n            color = kwargs.get(\"green\", False)\n            if color:\n                return f\"\\033[32m{text}\\033[0m\"\n            return text\n\n    formatter = ColoredLevelFormatter(\n        terminalwriter=MockTerminalWriter(),\n        fmt=\"%(levelname)s: %(message)s\",\n        datefmt=None\n    )\n\n    record = logging.LogRecord(\n        name=\"test\",\n        level=logging.INFO,\n        pathname=__file__,\n        lineno=0,\n        msg=\"This is a test log\",\n        args=(),\n        exc_info=None\n    )\n\n    formatted = formatter.format(record)\n    expected = \"\\033[32mINFO\\033[0m: This is a test log\"\n    assert formatted == expected\n", "def test_colored_level_formatter():\n    \"\"\"Test if the ColoredLevelFormatter formats log levels correctly.\"\"\"\n    class MockTerminalWriter:\n            if color_kwargs.get(\"red\"):\n                return f\"\\033[91m{text}\\033[0m\"\n            elif color_kwargs.get(\"yellow\"):\n                return f\"\\033[93m{text}\\033[0m\"\n            elif color_kwargs.get(\"green\"):\n                return f\"\\033[92m{text}\\033[0m\"\n            elif color_kwargs.get(\"purple\"):\n                return f\"\\033[95m{text}\\033[0m\"\n            return text\n\n    log_format = \"%(levelname)-8s %(message)s\"\n    formatter = ColoredLevelFormatter(MockTerminalWriter(), log_format)\n\n    log_record = logging.LogRecord(name=\"test\", level=logging.ERROR, pathname=__file__, lineno=10, msg=\"An error occurred\", args=(), exc_info=None)\n    formatted_message = formatter.format(log_record)\n    \n    assert \"\\033[91mERROR\\033[0m\" in formatted_message or \"\\033[91m\\033[1mERROR\\033[0m\" in formatted_message\n    assert \"An error occurred\" in formatted_message\n"], "sample_667": ["def test_get_user_env_var(monkeypatch):\n    \"\"\"Test get_user() function when environment variables for username are present.\"\"\"\n    from _pytest.tmpdir import get_user\n\n    monkeypatch.setenv(\"USER\", \"testuser\")\n    assert get_user() == \"testuser\"\n\n    monkeypatch.delenv(\"USER\", raising=False)\n    monkeypatch.setenv(\"USERNAME\", \"testuser2\")\n    assert get_user() == \"testuser2\"\n", "def test_mktemp_with_invalid_basename(tmp_path_factory):\n    \"\"\"Test mktemp with invalid basename raises ValueError\"\"\"\n    with pytest.raises(ValueError, match=\"is not a normalized and relative path\"):\n        tmp_path_factory.mktemp(\"/invalidpath\")\n\n    with pytest.raises(ValueError, match=\"is not a normalized and relative path\"):\n        tmp_path_factory.mktemp(\"..\")\n\n    with pytest.raises(ValueError, match=\"is not a normalized and relative path\"):\n        tmp_path_factory.mktemp(\"../invalidpath\")\n", "def test_temp_path_factory_mktemp(tmp_path_factory):\n    tmp_dir = tmp_path_factory.mktemp(\"example\")\n    assert tmp_dir.exists()\n    assert tmp_dir.is_dir()\n    assert tmp_dir.name.startswith(\"example\")\n", "def test_mktemp_invalid_basename(testdir):\n    \"\"\"Test that mktemp raises a ValueError for an invalid basename.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            with pytest.raises(ValueError, match=\"is not a normalized and relative path\"):\n                tmp_path_factory.mktemp('/invalid_basename')\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.assert_outcomes(passed=1)\n", "def test_get_user_handles_import_error(monkeypatch):\n    \"\"\"Test that get_user() handles ImportError.\"\"\"\n    from _pytest.tmpdir import get_user\n\n        raise ImportError\n\n    monkeypatch.setattr(\"getpass.getuser\", raise_import_error)\n    assert get_user() is None\n"], "sample_668": ["def test_fixture_positional_arguments_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(\"module\")\n            pass\n\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\",\n            \"*pass them as a keyword argument instead.*\",\n        ]\n    )\n", "def test_fixture_positional_arguments_deprecation(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture('module')\n            return \"fixture\"\n\n            assert my_fixture == \"fixture\"\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them*\",\n            \"*as a keyword argument instead.*\",\n        ]\n    )\n", "def test_fixture_positional_arguments_warning(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        \n        @pytest.fixture('module')\n            return 42\n        \n            assert my_fixture == 42\n        \"\"\"\n    )\n    \n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\",\n        ]\n    )\n", "def test_fixture_positional_arguments_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture('module')\n            return 42\n\n            assert my_fixture == 42\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\",\n            \"*pass them as a keyword argument instead.*\",\n        ]\n    )\n", "def test_fixture_positional_arguments_warning(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture('module')\n            pass\n\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.*\"\n        ]\n    )\n"], "sample_669": ["def test_capture_sys_stdout_and_stderr():\n    cap = capture.SysCapture(1)\n    cap_err = capture.SysCapture(2)\n    cap.start()\n    cap_err.start()\n    try:\n        print(\"stdout output\")\n        sys.stderr.write(\"stderr output\\n\")\n        out = cap.snap()\n        err = cap_err.snap()\n    finally:\n        cap.done()\n        cap_err.done()\n    assert out == \"stdout output\\n\"\n    assert err == \"stderr output\\n\"\n", "def test_capturemanager_repr():\n    capman = CaptureManager(\"sys\")\n    repr_str = repr(capman)\n    expected_repr = \"<CaptureManager _method='sys' _global_capturing=None _capture_fixture=None>\"\n    assert repr_str == expected_repr\n", "def test_capture_sys_capture_binary():\n    cap = capture.SysCaptureBinary(1)\n    cap.start()\n    sys.stdout.buffer.write(b\"hello binary\")\n    captured = cap.snap()\n    cap.done()\n    assert captured == b\"hello binary\"\n", "def test_capturing_sys_and_fd_combination(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n        import os\n\n            print(\"stdout capsys\")\n            sys.stderr.write(\"stderr capsys\\\\n\")\n            out, err = capsys.readouterr()\n            assert out == \"stdout capsys\\\\n\"\n            assert err == \"stderr capsys\\\\n\"\n\n            os.write(1, b\"stdout capfd\\\\n\")\n            os.write(2, b\"stderr capfd\\\\n\")\n            out, err = capfd.readouterr()\n            assert out == \"stdout capfd\\\\n\"\n            assert err == \"stderr capfd\\\\n\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*2 passed*\"])\n", "def test_multicapture_repr():\n    mc = MultiCapture(out=True, err=True, in_=True, Capture=SysCapture)\n    expected_repr = \"<MultiCapture out=<SysCapture stdout _old=<UNSET> _state=None tmpfile=<UNSET>> err=<SysCapture stderr _old=<UNSET> _state=None tmpfile=<UNSET>> in_=<SysCapture stdin _old=<UNSET> _state=None tmpfile=<UNSET>> _state=None _in_suspended=False>\"\n    assert repr(mc) == expected_repr\n"], "sample_670": ["def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expression() -> None:\n    matcher = {\"A\": True, \"B\": False, \"C\": True, \"D\": False}.__getitem__\n    assert evaluate(\"A and (B or C) and not D or (A and B)\", matcher) is True\n", "def test_nested_expressions() -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True}.__getitem__\n    assert evaluate(\"((a and b) or c)\", matcher) is True\n    assert evaluate(\"((a or b) and (not b or c))\", matcher) is True\n    assert evaluate(\"(a and (b or c))\", matcher) is True\n    assert evaluate(\"((a or b) and not c)\", matcher) is False\n", "def test_combined_logical_expressions() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"(true or false) and not (false and true)\", matcher) is True\n    assert evaluate(\"not (false and (true or false)) or (true and (false or true))\", matcher) is True\n    assert evaluate(\"((true and false) or (false or true)) and (true or not false)\", matcher) is True\n", "def test_mixed_operators_and_parentheses() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"(true or false) and (not false and true)\", matcher) is True\n    assert evaluate(\"not (true and false) or (false and true)\", matcher) is False\n    assert evaluate(\"(true or (false and (true or false)))\", matcher) is True\n"], "sample_671": ["def test_xfail_marker_with_reason_and_run_false(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(reason=\"known issue\", run=False)\n            assert 0\n    \"\"\"\n    )\n    reports = runtestprotocol(item, log=False)\n    assert len(reports) == 3\n    callreport = reports[1]\n    assert callreport.skipped\n    assert callreport.wasxfail == \"known issue\"\n", "def test_xfail_with_raises_and_strict(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(raises=ZeroDivisionError, strict=True, reason=\"Expect ZeroDivisionError\")\n            1 / 0\n\n        @pytest.mark.xfail(raises=ZeroDivisionError, strict=True, reason=\"Expect ZeroDivisionError\")\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*XFAIL*test_raises_error*Expect ZeroDivisionError*\",\n            \"*XPASS(strict)*test_does_not_raise_error*Expect ZeroDivisionError*\",\n        ]\n    )\n    assert result.ret == 1\n", "    def test_xfail_with_raises(self, testdir):\n        item = testdir.getitem(\n            \"\"\"\n            import pytest\n            @pytest.mark.xfail(raises=ValueError, reason=\"expecting ValueError\")\n                raise ValueError(\"Expected error\")\n        \"\"\"\n        )\n        reports = runtestprotocol(item, log=False)\n        assert len(reports) == 3\n        callreport = reports[1]\n        assert callreport.skipped\n        assert callreport.wasxfail == \"expecting ValueError\"\n", "    def test_xfail_with_raises_exception(self, testdir):\n        item = testdir.getitem(\n            \"\"\"\n            import pytest\n            @pytest.mark.xfail(raises=ValueError, reason=\"expecting a ValueError\")\n                raise ValueError(\"this is a ValueError\")\n        \"\"\"\n        )\n        reports = runtestprotocol(item, log=False)\n        assert len(reports) == 3\n        callreport = reports[1]\n        assert callreport.skipped\n        assert callreport.wasxfail == \"expecting a ValueError\"\n", "def test_xfail_strict_config_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = true\n    \"\"\"\n    )\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(reason=\"this is a strict xfail from config\")\n            assert 1\n    \"\"\"\n    )\n    reports = runtestprotocol(item, log=False)\n    callreport = reports[1]\n    assert callreport.failed\n    assert callreport.longrepr == \"[XPASS(strict)] this is a strict xfail from config\"\n    assert not hasattr(callreport, \"wasxfail\")\n"], "sample_672": ["def test_custom_class_with_long_repr():\n    \"\"\"Test saferepr() for a custom class with a long __repr__ result.\"\"\"\n\n    class LongReprClass:\n            return \"x\" * 1000\n\n    obj = LongReprClass()\n    result = saferepr(obj, maxsize=50)\n    assert len(result) == 50\n    assert result.startswith(\"x\" * 23)\n    assert result.endswith(\"x\" * 23)\n    assert \"...\" in result\n", "def test_safeformat():\n    class CustomClass:\n            return \"CustomClass()\"\n\n    obj = {\"key1\": [1, 2, 3], \"key2\": CustomClass()}\n    formatted = safeformat(obj)\n    assert \"CustomClass()\" in formatted\n    assert \"'key1': [1, 2, 3]\" in formatted\n\n    class BrokenRepr:\n            raise ValueError(\"broken\")\n\n    obj = {\"key1\": [1, 2, 3], \"key2\": BrokenRepr()}\n    formatted = safeformat(obj)\n    assert \"ValueError\" in formatted\n    assert \"'key1': [1, 2, 3]\" in formatted\n", "def test_safeformat():\n    class BrokenRepr:\n            raise ValueError(\"broken\")\n\n    obj = {\"key\": BrokenRepr()}\n    formatted = safeformat(obj)\n    assert \"<[ValueError('broken') raised in repr()] BrokenRepr object at 0x\" in formatted\n", "def test_safeformat():\n    class CustomObj:\n            raise ValueError(\"Custom exception in repr\")\n\n    obj = CustomObj()\n    formatted = safeformat(obj)\n    assert formatted.startswith(\"<[ValueError('Custom exception in repr') raised in repr()] CustomObj object at 0x\")\n\n    complex_obj = {\"key\": [1, 2, 3], \"broken\": CustomObj()}\n    formatted_complex = safeformat(complex_obj)\n    assert \"Custom exception in repr\" in formatted_complex\n", "def test_safeformat():\n    class BrokenRepr:\n            raise ValueError(\"broken repr\")\n\n    class CustomObj:\n            self.attr = BrokenRepr()\n\n            return \"CustomObj()\"\n\n    obj = CustomObj()\n    formatted = safeformat(obj)\n    assert \"ValueError\" in formatted\n    assert \"broken repr\" in formatted\n"], "sample_673": ["def test_doctestmodule_with_mocked_object(self, testdir):\n    \"\"\"Test to ensure that doctests handle mocked objects correctly.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n        from unittest.mock import Mock\n\n        class Foo:\n                self.bar = Mock()\n            \n                '''\n                >>> foo = Foo()\n                >>> foo.bar.some_method()\n                'mocked value'\n                '''\n                return self.bar.some_method()\n\n        foo = Foo()\n        foo.bar.some_method.return_value = 'mocked value'\n        \"\"\"\n    )\n    reprec = testdir.inline_run(p, \"--doctest-modules\")\n    reprec.assertoutcome(passed=1)\n", "def test_doctest_pytest_warning_on_unwrap(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest.doctest import _patch_unwrap_mock_aware\n        import inspect\n\n        class Broken:\n                raise KeyError(\"This should be an AttributeError\")\n\n            bad_instance = Broken()\n            with _patch_unwrap_mock_aware():\n                with pytest.warns(pytest.PytestWarning, match=\"^Got KeyError.* when unwrapping\"):\n                    with pytest.raises(KeyError):\n                        inspect.unwrap(bad_instance)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"* 1 passed *\"])\n", "    def test_doctest_continue_on_failure_with_multiple_failures(self, testdir):\n        testdir.maketxtfile(\n            test_something=\"\"\"\n            >>> i = 5\n            >>> def foo():\n            ...     raise ValueError('error1')\n            >>> foo()\n            >>> i\n            6\n            >>> i + 2\n            8\n            >>> i + 1\n            6\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\", \"--doctest-continue-on-failure\")\n        result.assert_outcomes(passed=0, failed=1)\n        # The lines that contains the failure are 4, 5, 6, and 8.  The first one\n        # is a stack trace and the other three are mismatches.\n        result.stdout.fnmatch_lines(\n            [\"*4: UnexpectedException*\", \"*5: DocTestFailure*\", \"*6: DocTestFailure*\", \"*8: DocTestFailure*\"]\n        )\n", "def test_doctest_reporting_invalid_options(self, testdir):\n    \"\"\"\n    Test to ensure invalid doctest options in the ini file are properly handled.\n    \"\"\"\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        doctest_optionflags = INVALID_OPTION\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stderr.fnmatch_lines(\n        [\"*Error: Unknown optionflag 'INVALID_OPTION'*\"]\n    )\n", "    def test_doctestmodule_with_setup_py_import(self, testdir):\n        \"\"\"Test that doctests in a Python module that imports setup.py are collected and run.\"\"\"\n        testdir.makepyfile(setup=\"\"\"\n            import setuptools\n                '''\n                >>> sample_function()\n                'sample'\n                '''\n                return 'sample'\n        \"\"\")\n        reprec = testdir.inline_run(\"--doctest-modules\")\n        reprec.assertoutcome(passed=1)\n"], "sample_674": ["def test_node_add_marker():\n    node = nodes.Node(\"test_node\", config=pytest.Config.fromdictargs({}), session=object())\n    node.add_marker(\"test_marker\")\n    assert \"test_marker\" in node.keywords\n    assert any(marker.name == \"test_marker\" for marker in node.own_markers)\n", "def test_node_repr():\n    class TestNode(nodes.Node):\n            self.name = name\n            self.parent = None\n            self.config = None\n            self.session = None\n            self.fspath = None\n            self._nodeid = name\n\n    node = TestNode(\"test_node\")\n    assert repr(node) == \"<TestNode test_node>\"\n", "def test_node_add_marker():\n    node = nodes.Node(\"test_node\", config=pytest.Config(pytest.PluginManager()))\n    node.add_marker(\"example_marker\")\n    assert \"example_marker\" in node.keywords\n    assert any(mark.name == \"example_marker\" for mark in node.own_markers)\n\n    marker = pytest.mark.skip\n    node.add_marker(marker)\n    assert \"skip\" in node.keywords\n    assert any(mark.name == \"skip\" for mark in node.own_markers)\n", "def test_add_marker():\n    node = nodes.Node(\"test_node\", config=pytest.Config.fromdictargs({}))\n    marker = \"my_marker\"\n    node.add_marker(marker)\n    assert marker in node.keywords\n    assert any(mark.name == marker for mark in node.own_markers)\n", "def test_node_listchain():\n    root = nodes.Node(\"root\", config=pytest.Config(), session=pytest.Session())\n    child1 = nodes.Node.from_parent(root, name=\"child1\")\n    child2 = nodes.Node.from_parent(child1, name=\"child2\")\n\n    chain = child2.listchain()\n    assert chain == [root, child1, child2]\n"], "sample_675": ["def test_log_capture_handler_reset(testdir):\n    \"\"\"\n    Test that LogCaptureHandler resets the records and stream correctly.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger(\"test_logger\")\n            logger.addHandler(caplog.handler)\n            logger.setLevel(logging.INFO)\n            \n            logger.info(\"first message\")\n            assert len(caplog.records) == 1\n            assert \"first message\" in caplog.text\n\n            caplog.clear()\n            assert len(caplog.records) == 0\n            assert caplog.text == \"\"\n\n            logger.info(\"second message\")\n            assert len(caplog.records) == 1\n            assert \"second message\" in caplog.text\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_custom_formatter_with_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('This is a multiline\\\\nlog message\\\\nwith auto-indent')\n            assert True\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent=on\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured *log call -*\",\n            \"*This is a multiline\",\n            \" log message\",\n            \" with auto-indent*\",\n        ]\n    )\n", "def test_custom_formatter(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            handler = logging.StreamHandler()\n            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            logger.setLevel(logging.DEBUG)\n            logger.debug('debug message with custom formatter')\n            logger.info('info message with custom formatter')\n            logger.warning('warning message with custom formatter')\n            logger.error('error message with custom formatter')\n            logger.critical('critical message with custom formatter')\n            assert True\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\n        '*debug message with custom formatter*',\n        '*info message with custom formatter*',\n        '*warning message with custom formatter*',\n        '*error message with custom formatter*',\n        '*critical message with custom formatter*',\n    ])\n", "def test_logging_format_multiline_message(testdir):\n    \"\"\"\n    Test that multiline log messages are formatted with proper indentation.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            message = \"line1\\\\nline2\\\\nline3\"\n            logger.info(message)\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO*line1\",\n            \"        line2\",\n            \"        line3\",\n        ]\n    )\n", "def test_log_file_custom_format(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_file_format = %(asctime)s %(levelname)s %(message)s\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_handler.level == logging.INFO\n            logging.getLogger('catchlog').info(\"This log message should be formatted\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines([\"test_log_file_custom_format.py PASSED\"])\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert re.search(r\"\\d{2}:\\d{2}:\\d{2} INFO This log message should be formatted\", contents)\n"], "sample_676": ["def test_more_quiet_action():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--quiet', action=MoreQuietAction, dest='verbose', default=0)\n    args = parser.parse_args(['--quiet'])\n    assert args.verbose == -1\n    assert args.quiet == 1\n", "def test_more_quiet_action():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-q', '--quiet', action=MoreQuietAction, dest='verbose', default=0)\n    parser.add_argument('-v', '--verbose', action='count', default=0)\n\n    # Test decreasing verbosity with --quiet\n    namespace = parser.parse_args(['--quiet'])\n    assert namespace.verbose == -1\n\n    # Test combining --quiet and --verbose\n    namespace = parser.parse_args(['--quiet', '--verbose'])\n    assert namespace.verbose == 0\n\n    # Test multiple --quiet options\n    namespace = parser.parse_args(['--quiet', '--quiet'])\n    assert namespace.verbose == -2\n", "def test_more_quiet_action():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-q', '--quiet', action=MoreQuietAction, default=0, dest='verbose')\n\n    # Simulate passing the -q argument multiple times\n    args = parser.parse_args(['-q', '-q'])\n    \n    # The verbosity should have decreased twice, resulting in -2\n    assert args.verbose == -2\n    assert args.quiet == 2\n", "def test_more_quiet_action():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-q\", \"--quiet\", action=MoreQuietAction, default=0, dest=\"verbose\")\n\n    # Simulate command-line arguments\n    args = parser.parse_args([\"-q\"])\n    assert args.verbose == -1\n    assert args.quiet == 1\n\n    args = parser.parse_args([\"-q\", \"-q\"])\n    assert args.verbose == -2\n    assert args.quiet == 2\n", "def test_pytest_addoption():\n    parser = argparse.ArgumentParser()\n    pytest_addoption(parser)\n    options = parser.parse_args([])\n    assert options.verbose == 0\n    assert options.quiet == 0\n    assert options.showlocals is False\n    assert options.tbstyle == \"auto\"\n    assert options.showcapture == \"all\"\n    assert options.fulltrace is False\n    assert options.color == \"auto\"\n    assert options.disable_warnings is False\n"], "sample_677": ["def test_complex_expression() -> None:\n    expr = \"(true and false) or (false and (true or false)) or not (false or (false and true))\"\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is True\n", "def test_complex_expressions(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_nested_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expressions(expr: str, matcher_dict: dict, expected: bool) -> None:\n    matcher = matcher_dict.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_invalid_syntax_mismatch_parens() -> None:\n    with pytest.raises(ParseError) as excinfo:\n        evaluate(\"(true and (false)\", lambda ident: ident == \"true\")\n    assert excinfo.value.column == 19\n    assert excinfo.value.message == \"expected right parenthesis; got end of input\"\n\n    with pytest.raises(ParseError) as excinfo:\n        evaluate(\"((true) and false))\", lambda ident: ident == \"true\")\n    assert excinfo.value.column == 19\n    assert excinfo.value.message == \"expected end of input; got right parenthesis\"\n"], "sample_678": ["def test_create_cleanup_lock(tmp_path):\n    \"\"\"Ensure that create_cleanup_lock properly creates a lock file and handles exceptions.\"\"\"\n    path = tmp_path / \"temp\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n\n    # Check that the lock file is created successfully\n    created_lock_path = create_cleanup_lock(path)\n    assert created_lock_path.is_file()\n    assert created_lock_path == lock_path\n\n    # Check that the lock file cannot be created again\n    with pytest.raises(OSError, match=\"cannot create lockfile in\"):\n        create_cleanup_lock(path)\n", "def test_resolve_from_str():\n    \"\"\"Test resolve_from_str function.\"\"\"\n    root = \"/home/user\"\n    assert resolve_from_str(\"~\", root) == Path(expanduser(\"~\"))\n    assert resolve_from_str(\"$HOME\", root) == Path(expandvars(\"$HOME\"))\n    assert resolve_from_str(\"/absolute/path\", root) == Path(\"/absolute/path\")\n    assert resolve_from_str(\"relative/path\", root) == Path(root, \"relative/path\")\n", "def test_create_cleanup_lock(tmp_path):\n    \"\"\"Ensure that create_cleanup_lock creates a lock file and writes the PID correctly.\"\"\"\n    path = tmp_path / \"lock_test\"\n    path.mkdir()\n    \n    lock_path = create_cleanup_lock(path)\n    assert lock_path.is_file()\n    \n    with open(lock_path, \"r\") as f:\n        pid = int(f.read().strip())\n    \n    assert pid == os.getpid()\n", "def test_create_cleanup_lock(tmp_path):\n    \"\"\"Ensure that a cleanup lock can be created and contains the correct PID.\"\"\"\n    path = tmp_path / \"test\"\n    path.mkdir()\n    \n    lock_path = create_cleanup_lock(path)\n    assert lock_path.is_file()\n    \n    with lock_path.open(\"r\") as f:\n        content = f.read()\n    \n    assert content == str(os.getpid())\n", "def test_symlink_or_skip(tmp_path):\n    \"\"\"Ensure symlink_or_skip creates a symlink or skips if symlinks are not supported.\"\"\"\n    src = tmp_path / \"src\"\n    src.touch()\n    dst = tmp_path / \"dst\"\n\n    try:\n        symlink_or_skip(src, dst)\n        assert dst.is_symlink()\n        assert dst.resolve() == src\n    except pytest.skip.Exception:\n        assert not dst.exists()\n"], "sample_679": ["def test_mark_evaluator_istrue():\n    from _pytest.nodes import Node\n    from _pytest.mark import Mark\n\n    class DummyItem(Node):\n            super().__init__(name, parent=None, config=config)\n            self.config = config\n\n            markers = [Mark(name=\"testmark\", args=(), kwargs={\"condition\": \"2+2==4\"})]\n            if name is None:\n                return markers\n            return [m for m in markers if m.name == name]\n\n    config = mock.Mock()\n    item = DummyItem(config=config, name=\"testnode\")\n    evaluator = MarkEvaluator(item=item, name=\"testmark\")\n    assert evaluator.istrue() is True\n", "def test_MarkEvaluator_istrue_with_invalid_expression(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        class TestItem:\n            config = None\n                return [pytest.mark.skipif(\"invalid_syntax_here()\")]\n\n            item = TestItem()\n            evaluator = pytest.MarkEvaluator(item, \"skipif\")\n            assert not evaluator.istrue()\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(failed=1)\n", "def test_mark_evaluator_istrue(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest.mark import MarkEvaluator\n        from _pytest.nodes import Item\n\n        class DummyItem(Item):\n                self.config = None\n                self.obj = None\n                super().__init__(*args, **kwargs)\n\n                if name == \"custom\":\n                    return [pytest.mark.custom(condition=\"True\", reason=\"reason for custom mark\")]\n                return []\n\n            item = DummyItem(name=\"dummy\", parent=None)\n            evaluator = MarkEvaluator(item, \"custom\")\n            assert evaluator.istrue() is True\n            assert evaluator.wasvalid() is True\n            assert evaluator.getexplanation() == \"reason for custom mark\"\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_mark_evaluator_istrue():\n    from _pytest.nodes import Node\n    from _pytest.config import Config\n    from _pytest.mark.structures import Mark\n\n    class DummyItem(Node):\n            super().__init__(name, config)\n            self.obj = obj\n\n            if name == \"testmark\":\n                return [Mark(name, args=(True,), kwargs={})]\n            return []\n\n    config = Config()\n    item = DummyItem(name=\"dummy\", config=config)\n    evaluator = MarkEvaluator(item, \"testmark\")\n    assert evaluator.istrue() is True\n", "def test_mark_evaluator_istrue_with_string_condition(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest.mark import MarkEvaluator\n        from _pytest.nodes import Item\n\n        @pytest.mark.foo\n            pass\n\n            node = Item.from_parent(parent=None, name=\"dummy_test\")\n            mark = pytest.mark.foo.with_args(condition=\"1 == 1\")\n            node.add_marker(mark)\n            evaluator = MarkEvaluator(node, \"foo\")\n            assert evaluator.istrue()\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_680": ["def test_skipif_with_multiple_conditions(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"hasattr(os, 'sep')\", reason=\"test reason 1\")\n        @pytest.mark.skipif(\"not hasattr(sys, 'version')\", reason=\"test reason 2\")\n            pass\n    \"\"\"\n    )\n    skipped = evaluate_skip_marks(item)\n    assert skipped\n    assert skipped.reason == \"test reason 1\"\n", "def test_evaluate_condition_syntax_error(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_condition(item, item.own_markers[0], \"invalid syntax\")\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_xfail_imperative_runtime(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n            if True:\n                pytest.xfail(\"this test is expected to fail\")\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n    result = testdir.runpytest(\"-rx\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_func*\", \"*reason:*this test is expected to fail*\"])\n    result = testdir.runpytest(\"--runxfail\")\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "def test_evaluate_condition_with_syntax_error(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'skipif' condition\" in excinfo.value.msg\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_runxfail_option(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--runxfail\")\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n"], "sample_681": ["def test_caplog_at_level_context_manager(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger('test')\n            logger.setLevel(logging.DEBUG)\n\n            with caplog.at_level(logging.INFO):\n                logger.debug('This should not be captured')\n                logger.info('This should be captured')\n                logger.warning('This should also be captured')\n            \n            assert 'This should not be captured' not in caplog.text\n            assert 'This should be captured' in caplog.text\n            assert 'This should also be captured' in caplog.text\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=DEBUG\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_caplog_at_level*\",\n            \"* 1 passed in *\",\n        ]\n    )\n", "def test_percentstylemultiline_formatting(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            handler = logging.StreamHandler()\n            formatter = logging.Formatter('%(levelname)s: %(message)s')\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            logger.setLevel(logging.DEBUG)\n            logger.debug('debug\\\\nmessage')\n            logger.info('info\\\\nmessage')\n            logger.warning('warning\\\\nmessage')\n            logger.error('error\\\\nmessage')\n            logger.critical('critical\\\\nmessage')\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=DEBUG\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\n        \"*DEBUG: debug\",\n        \"       message*\",\n        \"*INFO: info\",\n        \"      message*\",\n        \"*WARNING: warning\",\n        \"         message*\",\n        \"*ERROR: error\",\n        \"       message*\",\n        \"*CRITICAL: critical\",\n        \"          message*\",\n    ])\n", "def test_percent_style_multiline():\n    from _pytest.logging import PercentStyleMultiline\n\n    record_dict = {\n        \"name\": \"test_logger\",\n        \"levelname\": \"INFO\",\n        \"message\": \"This is a test\\nwith multiple\\nlines\",\n    }\n    fmt = \"%(levelname)s: %(message)s\"\n    \n    # Test different auto_indent options\n    style = PercentStyleMultiline(fmt, auto_indent=True)\n    formatted_message = style.format(logging.LogRecord(**record_dict))\n    assert formatted_message == \"INFO: This is a test\\n      with multiple\\n      lines\"\n\n    style = PercentStyleMultiline(fmt, auto_indent=0)\n    formatted_message = style.format(logging.LogRecord(**record_dict))\n    assert formatted_message == \"INFO: This is a test\\nwith multiple\\nlines\"\n\n    style = PercentStyleMultiline(fmt, auto_indent=4)\n    formatted_message = style.format(logging.LogRecord(**record_dict))\n    assert formatted_message == \"INFO: This is a test\\n    with multiple\\n    lines\"\n\n    style = PercentStyleMultiline(fmt, auto_indent=\"off\")\n    formatted_message = style.format(logging.LogRecord(**record_dict))\n    assert formatted_message == \"INFO: This is a test\\nwith multiple\\nlines\"\n", "def test_custom_log_formatter(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        from _pytest.logging import PercentStyleMultiline\n\n        logger = logging.getLogger(__name__)\n\n            handler = logging.StreamHandler()\n            formatter = logging.Formatter('%(levelname)s: %(message)s')\n            formatter._style = PercentStyleMultiline('%(levelname)s: %(message)s', auto_indent=2)\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            logger.setLevel(logging.DEBUG)\n            \n            logger.info('info message\\\\nsecond line')\n            logger.warning('warning message\\\\nsecond line')\n            logger.error('error message\\\\nsecond line')\n\n            assert True\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*INFO: info message\",\n            \"  second line*\",\n            \"*WARNING: warning message\",\n            \"  second line*\",\n            \"*ERROR: error message\",\n            \"  second line*\",\n        ]\n    )\n", "def test_auto_indent_multiline_messages(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('First line\\\\nSecond line\\\\nThird line', extra={'auto_indent': True})\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured *log call -*\",\n            \"*First line\",\n            \"* Second line\",\n            \"* Third line*\",\n        ]\n    )\n"], "sample_682": ["def test_evaluate_condition_invalid_syntax(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax here\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_condition(item, item.get_closest_marker('skipif'), \"invalid syntax here\")\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_skip_reason_with_expression(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip(reason=\"Skipping because condition is met: \" + str(hasattr(os, 'sep')))\n            pass\n    \"\"\"\n    )\n    skipped = evaluate_skip_marks(item)\n    assert skipped\n    assert skipped.reason == \"Skipping because condition is met: True\"\n", "def test_evaluate_condition_invalid_syntax(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\", reason=\"Invalid syntax\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_condition(item, item.get_closest_marker(\"skipif\"), \"invalid syntax\")\n    assert excinfo.value.msg is not None\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_xfail_with_specific_exception(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(raises=ValueError, reason=\"Expected ValueError\")\n            raise ValueError(\"this is a ValueError\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rx\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*XFAIL*test_xfail_with_specific_exception.py::test_func*Expected ValueError*\",\n            \"*1 xfailed*\",\n        ]\n    )\n    assert result.ret == 0\n", "def test_evaluate_condition_syntax_error(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'skipif' condition\" in excinfo.value.msg\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n"], "sample_683": ["def test_capture_stdout_with_custom_stream():\n    class CustomStream(io.StringIO):\n            super().write(f\"custom: {s}\")\n            return len(s)\n\n    custom_stream = CustomStream()\n    original_stdout = sys.stdout\n    sys.stdout = custom_stream\n\n    capsys = StdCapture(out=True, err=False)\n    capsys.start_capturing()\n    try:\n        print(\"hello world\")\n        out, err = capsys.readouterr()\n        assert out == \"hello world\\n\"\n        assert custom_stream.getvalue() == \"custom: hello world\\n\"\n    finally:\n        capsys.stop_capturing()\n        sys.stdout = original_stdout\n", "def test_capture_fixture_with_subprocess(testdir):\n    \"\"\"Test capsys and capfd fixtures with subprocess to ensure they work correctly with subprocess output.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\\\n        import subprocess\n\n            subprocess.run(['echo', 'hello from subprocess'])\n            out, err = capsys.readouterr()\n            assert out.strip() == 'hello from subprocess'\n\n            subprocess.run(['echo', 'hello from subprocess'])\n            out, err = capfd.readouterr()\n            assert out.strip() == 'hello from subprocess'\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*2 passed*\"])\n", "def test_no_capture_method():\n    capman = CaptureManager(\"no\")\n    assert not capman.is_capturing()\n    capman.start_global_capturing()\n    assert capman.is_capturing() == \"global\"\n    capman.suspend_global_capture()\n    assert not capman.is_capturing()\n    capman.resume_global_capture()\n    assert capman.is_capturing() == \"global\"\n    capman.stop_global_capturing()\n    assert not capman.is_capturing()\n", "def test_fdcapture_suspend_resume_multiple_times():\n    cap = capture.FDCapture(1)\n    cap.start()\n    os.write(1, b\"start\")\n    cap.suspend()\n    os.write(1, b\" suspend1\")\n    cap.resume()\n    os.write(1, b\" resume1\")\n    cap.suspend()\n    os.write(1, b\" suspend2\")\n    cap.resume()\n    os.write(1, b\" resume2\")\n    result = cap.snap()\n    assert result == \"start resume1 resume2\"\n    cap.done()\n", "def test_multicapture_fd_sys_combination():\n    # Test combining fd and sys capturing methods\n    cap_fd = StdCaptureFD(out=True, err=True, in_=True)\n    cap_sys = StdCapture(out=True, err=True, in_=True)\n\n    try:\n        cap_fd.start_capturing()\n        cap_sys.start_capturing()\n\n        os.write(1, b\"fd stdout\")\n        os.write(2, b\"fd stderr\")\n        print(\"sys stdout\")\n        sys.stderr.write(\"sys stderr\\n\")\n\n        # Reading and asserting outputs from both capturing methods\n        fd_out, fd_err = cap_fd.readouterr()\n        sys_out, sys_err = cap_sys.readouterr()\n\n        assert fd_out == \"fd stdout\"\n        assert fd_err == \"fd stderr\"\n        assert sys_out == \"sys stdout\\n\"\n        assert sys_err == \"sys stderr\\n\"\n    finally:\n        cap_fd.stop_capturing()\n        cap_sys.stop_capturing()\n"], "sample_684": ["def test_tracebackentry_ishidden() -> None:\n        return True\n\n    try:\n        if False:\n            pass\n        else:\n            __tracebackhide__()\n            assert False\n    except AssertionError:\n        exci = ExceptionInfo.from_current()\n    entry = exci.traceback[0]\n    assert entry.ishidden() is True\n", "def test_traceback_filter_traceback() -> None:\n        assert False\n\n    try:\n        func()\n    except AssertionError:\n        excinfo = ExceptionInfo.from_current()\n\n    tb = excinfo.traceback\n    filtered_tb = tb.filter(lambda entry: \"func\" in entry.frame.code.name)\n\n    assert len(filtered_tb) == 1\n    assert filtered_tb[0].frame.code.name == \"func\"\n", "def test_frame_eval() -> None:\n        return sys._getframe(0)\n    \n    fr = Frame(f(2, 3))\n    result = fr.eval(\"x + y\")\n    assert result == 5\n\n    result = fr.eval(\"x * y\")\n    assert result == 6\n", "def test_traceback_cut() -> None:\n        func2()\n\n        func3()\n\n        raise ValueError(\"some error\")\n\n    try:\n        func1()\n    except ValueError:\n        excinfo = ExceptionInfo.from_current()\n\n    tb = excinfo.traceback\n    assert len(tb) == 3  # func1 -> func2 -> func3\n\n    cut_tb = tb.cut(path=func3.__code__.co_filename, lineno=func3.__code__.co_firstlineno)\n    assert len(cut_tb) == 1  # Only func3 should remain\n\n    cut_tb = tb.cut(path=func2.__code__.co_filename, lineno=func2.__code__.co_firstlineno)\n    assert len(cut_tb) == 2  # func2 -> func3\n\n    cut_tb = tb.cut(path=func1.__code__.co_filename, lineno=func1.__code__.co_firstlineno)\n    assert len(cut_tb) == 3  # func1 -> func2 -> func3\n", "def test_traceback_entry_hidden() -> None:\n        __tracebackhide__ = True\n        raise ValueError(\"test hidden\")\n\n    try:\n        func()\n    except ValueError:\n        exci = ExceptionInfo.from_current()\n\n    entry = exci.traceback[0]\n    assert entry.ishidden() is True\n"], "sample_685": ["def test_log_cli_output(testdir: Testdir):\n    \"\"\"Ensure that live log CLI output respects configured levels.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logger = logging.getLogger('cli_logger')\n            caplog.set_level(logging.DEBUG)\n            logger.debug(\"CLI DEBUG message should be visible\")\n            logger.info(\"CLI INFO message should be visible\")\n            logger.warning(\"CLI WARNING message should be visible\")\n            logger.error(\"CLI ERROR message should be visible\")\n            logger.critical(\"CLI CRITICAL message should be visible\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=DEBUG\", \"-s\")\n    result.stdout.fnmatch_lines([\n        \"*CLI DEBUG message should be visible*\",\n        \"*CLI INFO message should be visible*\",\n        \"*CLI WARNING message should be visible*\",\n        \"*CLI ERROR message should be visible*\",\n        \"*CLI CRITICAL message should be visible*\"\n    ])\n    assert result.ret == 0\n", "def test_log_capture_handler_emit():\n    handler = LogCaptureHandler()\n    handler.setLevel(logging.INFO)\n    logger = logging.getLogger(\"test_logger\")\n    logger.addHandler(handler)\n    \n    logger.info(\"Test message\")\n    \n    assert len(handler.records) == 1\n    assert handler.records[0].getMessage() == \"Test message\"\n    assert handler.stream.getvalue().strip() == \"Test message\"\n    \n    logger.removeHandler(handler)\n", "def test_colored_level_formatter(testdir: Testdir) -> None:\n    \"\"\"Ensure that ColoredLevelFormatter correctly formats log levels with colors.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        from _pytest.config import create_terminal_writer\n        from _pytest.logging import ColoredLevelFormatter\n\n            terminal_writer = create_terminal_writer()\n            formatter = ColoredLevelFormatter(\n                terminal_writer,\n                \"%(levelname)-8s %(message)s\"\n            )\n\n            log_record = logging.LogRecord(\n                \"test_logger\", logging.WARNING, \"\", 0, \"Test warning message\", None, None\n            )\n            formatted_message = formatter.format(log_record)\n            assert \"WARNING\" in formatted_message\n            assert \"Test warning message\" in formatted_message\n            assert \"\\x1b[\" in formatted_message  # Check for ANSI escape codes indicating color\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_capture_handler_reset():\n    handler = LogCaptureHandler()\n    logger.addHandler(handler)\n    logger.setLevel(logging.INFO)\n\n    logger.info(\"Initial log\")\n    assert len(handler.records) == 1\n    assert \"Initial log\" in handler.stream.getvalue()\n\n    handler.reset()\n    assert len(handler.records) == 0\n    assert handler.stream.getvalue() == \"\"\n\n    logger.info(\"Log after reset\")\n    assert len(handler.records) == 1\n    assert \"Log after reset\" in handler.stream.getvalue()\n\n    logger.removeHandler(handler)\n", "def test_colored_level_formatter():\n    log_format = \"%(levelname)s %(message)s\"\n    terminal_writer = create_terminal_writer(pytest.config._config)\n    formatter = ColoredLevelFormatter(terminal_writer, log_format)\n\n    logger = logging.getLogger(\"test_logger\")\n    logger.setLevel(logging.DEBUG)\n\n    # Create a handler with the custom formatter\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    with caplog.at_level(logging.DEBUG, logger.name):\n        logger.debug(\"debug message\")\n        logger.info(\"info message\")\n        logger.warning(\"warning message\")\n        logger.error(\"error message\")\n        logger.critical(\"critical message\")\n\n    # Verify that color codes are present in the formatted messages\n    assert \"\\x1b[\" in caplog.text\n"], "sample_686": ["def test_fixture_positional_arguments_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        \n        @pytest.fixture('module')\n            pass\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\"\n        ]\n    )\n", "def test_funcargnames_is_deprecated() -> None:\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"The `funcargnames` attribute was an alias for `fixturenames`\"):\n        warnings.warn(deprecated.FUNCARGNAMES)\n", "def test_fixture_positional_arguments_deprecation(testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture('session')\n            return \"fixture_value\"\n\n            assert my_fixture == \"fixture_value\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\",\n        ]\n    )\n", "compilation error", "compilation error"], "sample_687": ["def test_custom_formatter(caplog):\n    class CustomFormatter(logging.Formatter):\n            return f\"CUSTOM: {record.levelname} - {record.getMessage()}\"\n\n    custom_formatter = CustomFormatter()\n    caplog.handler.setFormatter(custom_formatter)\n    caplog.set_level(logging.INFO)\n    logger.info(\"custom format message\")\n    assert \"CUSTOM: INFO - custom format message\" in caplog.text\n", "def test_log_file_logging(testdir: Testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            plugin.set_log_path('test_log_file.log')\n            logger = logging.getLogger('filelogger')\n\n            logger.error('This is an error message')\n            logger.info('This is an info message')\n\n            caplog.set_level(logging.INFO, logger.name)\n            with caplog.at_level(logging.DEBUG, logger.name):\n                logger.debug('This is a debug message')\n\n            with open('test_log_file.log', 'r') as f:\n                log_content = f.read()\n\n            assert 'This is an error message' in log_content\n            assert 'This is an info message' in log_content\n            assert 'This is a debug message' in log_content\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file=test_log_file.log\n        log_file_level=DEBUG\n        log_file_format=%(levelname)s:%(message)s\n    \"\"\"\n    )\n    \n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_multiline_log_formatting(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"First line\\nSecond line\\nThird line\")\n\n    assert \"First line\" in caplog.text\n    assert \"Second line\" in caplog.text\n    assert \"Third line\" in caplog.text\n    assert caplog.text.count(\"\\n\") == 2  # Ensure multiline formatting is applied correctly\n", "def test_log_capture_handler_reset():\n    \"\"\"Test that resetting LogCaptureHandler works as expected.\"\"\"\n    handler = LogCaptureHandler()\n    logger.addHandler(handler)\n    logger.setLevel(logging.INFO)\n\n    logger.info(\"First log message\")\n    assert len(handler.records) == 1\n    assert \"First log message\" in handler.stream.getvalue()\n\n    handler.reset()\n    assert len(handler.records) == 0\n    assert handler.stream.getvalue() == \"\"\n\n    logger.info(\"Second log message\")\n    assert len(handler.records) == 1\n    assert \"Second log message\" in handler.stream.getvalue()\n\n    logger.removeHandler(handler)\n", "def test_set_level_multiple_loggers(caplog):\n    caplog.set_level(logging.INFO, logger=__name__)\n    caplog.set_level(logging.ERROR, logger=sublogger.name)\n    \n    logger.info(\"root INFO level\")\n    logger.error(\"root ERROR level\")\n    sublogger.info(\"sublogger INFO level\")\n    sublogger.error(\"sublogger ERROR level\")\n\n    assert \"root INFO level\" in caplog.text\n    assert \"root ERROR level\" in caplog.text\n    assert \"sublogger INFO level\" not in caplog.text\n    assert \"sublogger ERROR level\" in caplog.text\n"], "sample_688": ["def test_get_lock_path(tmp_path):\n    \"\"\"Test get_lock_path function to ensure it appends .lock to the provided path.\"\"\"\n    test_path = tmp_path / \"testdir\"\n    lock_path = get_lock_path(test_path)\n    assert lock_path.name == \".lock\"\n    assert lock_path.parent == test_path\n", "def test_fnmatch_ex():\n    \"\"\"Test the fnmatch_ex function with various patterns and paths.\"\"\"\n    assert fnmatch_ex(\"*.py\", \"test.py\")\n    assert fnmatch_ex(\"*.py\", Path(\"test.py\"))\n    assert not fnmatch_ex(\"*.txt\", \"test.py\")\n    assert fnmatch_ex(\"tests/**/test_*.py\", \"tests/foo/bar/doc/test_foo.py\")\n    assert fnmatch_ex(\"tests/**/doc/test*.py\", \"tests/foo/bar/doc/test_foo.py\")\n    assert not fnmatch_ex(\"tests/**/doc/test*.py\", \"tests/foo/bar/test_foo.py\")\n    assert fnmatch_ex(\"tests/*/bar/doc/test_*.py\", \"tests/foo/bar/doc/test_foo.py\")\n    assert fnmatch_ex(\"tests/*/bar/**/test_*.py\", \"tests/foo/bar/doc/test_foo.py\")\n", "def test_get_lock_path():\n    path = Path(\"/some/path\")\n    lock_path = get_lock_path(path)\n    assert lock_path == path.joinpath(\".lock\")\n", "def test_ensure_reset_dir(tmp_path):\n    \"\"\"Test the ensure_reset_dir function to ensure it properly resets the directory.\"\"\"\n    test_dir = tmp_path / \"test_dir\"\n    test_dir.mkdir()\n    (test_dir / \"test_file.txt\").write_text(\"Hello, world!\")\n\n    # Ensure the directory is reset\n    ensure_reset_dir(test_dir)\n    \n    assert test_dir.exists()\n    assert len(list(test_dir.iterdir())) == 0\n\n", "def test_ensure_extended_length_path_windows(mocked_fs, monkeypatch):\n    \"\"\"Test ensure_extended_length_path on Windows.\"\"\"\n    import sys\n\n    if not sys.platform.startswith(\"win32\"):\n        pytest.skip(\"This test is for Windows only\")\n\n        return Path(\"\\\\fake\\\\resolved\\\\path\")\n\n    monkeypatch.setattr(Path, \"resolve\", fake_resolve)\n\n    path = Path(\"C:\\\\some\\\\very\\\\long\\\\path\\\\that\\\\exceeds\\\\max\\\\path\\\\length\")\n    extended_path = ensure_extended_length_path(path)\n    expected = Path(\"\\\\\\\\?\\\\C:\\\\fake\\\\resolved\\\\path\")\n    assert extended_path == expected\n"], "sample_689": ["def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            import warnings\n            warnings.warn(\"This is a test warning\", DeprecationWarning)\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-p\", \"no:warnings\")\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: The pytest_warning_captured is deprecated and will be removed in a future release.*\"]\n    )\n", "def test_pytest_warning_captured_deprecated(pytester: Pytester) -> None:\n    \"\"\"Ensure pytest_warning_captured deprecation warning is raised.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated and will be removed in a future release.*\",\n            \"*Please use pytest_warning_recorded instead.*\",\n        ]\n    )\n", "def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated and will be removed in a future release.*\",\n            \"*Please use pytest_warning_recorded instead.*\",\n        ]\n    )\n", "def test_warning_captured_hook_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            \"The pytest_warning_captured is deprecated and will be removed in a future release.\\n\"\n            \"Please use pytest_warning_recorded instead.\"\n        ),\n    ):\n        warnings.warn(pytest.PytestDeprecationWarning(\"pytest_warning_captured\"))\n", "def test_pytest_collect_module_warning_format() -> None:\n    warning = deprecated.PYTEST_COLLECT_MODULE.format(name=\"Module\")\n    assert warning.message == \"pytest.collect.Module was moved to pytest.Module\\nPlease update to the new name.\"\n"], "sample_690": ["def test_xfail_with_raises_condition(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(raises=ValueError, reason=\"expected ValueError\")\n            raise ValueError(\"This is a ValueError\")\n    \"\"\"\n    )\n    result = pytester.runpytest(p, \"-rx\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*XFAIL*expected ValueError*\",\n            \"*1 xfailed*\"\n        ]\n    )\n", "def test_xfail_with_raises(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(raises=ZeroDivisionError, reason=\"Expect ZeroDivisionError\")\n            1 / 0\n\n        @pytest.mark.xfail(raises=ZeroDivisionError, reason=\"Expect ZeroDivisionError\")\n            assert 0\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-rxs\")\n    result.stdout.fnmatch_lines([\"*XFAIL*Expect ZeroDivisionError*\"])\n    result.assert_outcomes(xfailed=1, failed=1)\n", "    def test_marked_xfail_with_raises_and_strict(self, pytester: Pytester) -> None:\n        item = pytester.getitem(\n            \"\"\"\n            import pytest\n            @pytest.mark.xfail(raises=ZeroDivisionError, strict=True, reason=\"Expecting ZeroDivisionError\")\n                1 / 0\n            \"\"\"\n        )\n        reports = runtestprotocol(item, log=False)\n        assert len(reports) == 3\n        callreport = reports[1]\n        assert callreport.skipped\n        assert callreport.wasxfail == \"Expecting ZeroDivisionError\"\n", "def test_xfail_with_raises_and_reason(pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(raises=ValueError, reason=\"ValueError expected\")\n            raise ValueError(\"This is a ValueError\")\n        \"\"\"\n    )\n    reports = runtestprotocol(item, log=False)\n    assert len(reports) == 3\n    callreport = reports[1]\n    assert callreport.skipped\n    assert callreport.wasxfail == \"ValueError expected\"\n", "def test_xfail_mark_with_raises(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(raises=ValueError, reason=\"Expected ValueError\")\n            raise ValueError(\"This is a ValueError\")\n\n        @pytest.mark.xfail(raises=TypeError, reason=\"Expected TypeError\")\n            raise ValueError(\"This is a ValueError\")\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rx\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*XFAIL*test_func*Expected ValueError*\",\n            \"*FAIL*test_func_2*Expected TypeError*\",\n        ]\n    )\n    assert result.ret == 1\n"], "sample_691": ["def test_faulthandler_reenabled_on_unconfigure(pytester: Pytester) -> None:\n    \"\"\"Test that faulthandler is re-enabled on unconfigure with sys.stderr.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        pass\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n\n    import faulthandler\n    assert faulthandler.is_enabled()\n", "def test_pytest_configure_with_faulthandler_disabled(pytester: Pytester) -> None:\n    \"\"\"Test if FaultHandlerHooks is registered when faulthandler is not enabled initially.\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n            from _pytest.faulthandler import FaultHandlerHooks\n            assert config.pluginmanager.has_plugin(\"faulthandler-hooks\")\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n            assert True\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n", "def test_faulthandler_hooks_pytest_configure(pytester: Pytester) -> None:\n    \"\"\"Test FaultHandlerHooks' pytest_configure method to ensure faulthandler is enabled.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            import faulthandler\n            assert faulthandler.is_enabled()\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n", "def test_pytest_addoption(pytester: Pytester) -> None:\n    \"\"\"Test that the 'faulthandler_timeout' option is added to pytest configuration.\"\"\"\n    config = pytester.parseconfig()\n    assert config.getini(\"faulthandler_timeout\") == 0.0\n", "def test_faulthandler_enabled_during_test(pytester: Pytester) -> None:\n    \"\"\"Test that faulthandler gets enabled during the test if not already enabled.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import faulthandler\n        assert faulthandler.is_enabled(), \"faulthandler should be enabled during the test\"\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n"], "sample_692": ["def test_get_user_returns_username(monkeypatch):\n    \"\"\"Test that get_user() function returns the expected username when environment variables are set correctly.\"\"\"\n    monkeypatch.setenv(\"USER\", \"testuser\")\n    assert get_user() == \"testuser\"\n    monkeypatch.delenv(\"USER\", raising=False)\n    monkeypatch.setenv(\"USERNAME\", \"testuser2\")\n    assert get_user() == \"testuser2\"\n", "def test_getbasetemp_creates_directory(tmp_path_factory: TempPathFactory) -> None:\n    # Ensure that getbasetemp() creates the directory if it does not exist\n    basetemp = tmp_path_factory.getbasetemp()\n    assert basetemp.exists() and basetemp.is_dir()\n", "def test_temp_path_factory_mktemp_absolute_path(pytester: Pytester) -> None:\n    \"\"\"Test that TempPathFactory.mktemp raises a ValueError when given an absolute path.\"\"\"\n    mytemp = pytester.mkdir(\"mytemp\")\n    p = pytester.makepyfile(\n        \"\"\"\n            import pytest\n            with pytest.raises(ValueError, match=\"is not a normalized and relative path\"):\n                tmp_path_factory.mktemp('/absolute/path', numbered=False)\n        \"\"\"\n    )\n    result = pytester.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert result.ret == 0\n", "def test_mktemp_invalid_basename(tmp_path_factory: TempPathFactory) -> None:\n    with pytest.raises(ValueError):\n        tmp_path_factory.mktemp(\"../../invalid_path\", numbered=False)\n", "def test_get_user_handles_import_error(monkeypatch) -> None:\n    \"\"\"Test that get_user() function handles ImportError gracefully.\"\"\"\n    monkeypatch.setattr(\"getpass.getuser\", lambda: (_ for _ in ()).throw(ImportError(\"Mocked ImportError\")))\n    assert get_user() is None\n"], "sample_693": ["def test_incompatible_exception_representation(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        class MyTestCase(unittest.TestCase):\n                raise ValueError(\"Some error\")\n\n        class IncompatibleExceptionInfo:\n                self.exc_type = exc_type\n                self.exc_value = exc_value\n                self.tb = tb\n\n            @property\n                return (self.exc_type, self.exc_value, self.tb)\n\n            if isinstance(item, unittest.TestCase):\n                call.excinfo = IncompatibleExceptionInfo(ValueError, \"Some error\", None)\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*NOTE: Incompatible Exception Representation*\",\n            \"*Some error*\",\n        ]\n    )\n", "def test_unittest_skip_decorator_on_method(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @unittest.skip(\"skipping this test\")\n                pass\n\n                self.assertTrue(True)\n        \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(skipped=1, passed=1)\n", "def test_teardown_failure_with_cleanup(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                self.addCleanup(lambda: self.values.append(1))\n                assert False, \"teardown failed\"\n                pass\n            assert MyTestCase.values == [1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(failed=1, passed=1)\n", "def test_unittest_teardown_with_exception(pytester: Pytester) -> None:\n    \"\"\"Test that an exception in tearDownClass is handled and reported properly.\"\"\"\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @classmethod\n                cls.resource = \"something\"\n\n            @classmethod\n                if hasattr(cls, 'resource'):\n                    del cls.resource\n                raise Exception(\"tearDownClass exception\")\n\n                self.assertEqual(1, 1)\n\n            assert hasattr(MyTestCase, 'resource') is False\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(passed=2, failed=1)\n    assert \"Exception: tearDownClass exception\" in str(reprec.getreports(\"call\")[1].longrepr)\n", "def test_pycollect_makeitem(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                pass\n\n        class NotATestCase:\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*collected 1 item*\"])\n"], "sample_694": ["def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--myopt\", action=\"store\", default=\"default\", help=\"some help\", type=\"%default\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"*\",\n        ]\n    )\n", "def test_argument_type_str_choice_deprecated():\n    with pytest.warns(\n        pytest.PytestRemovedIn8Warning,\n        match=re.escape(\"`type` argument to addoption() is the string 'str'. For choices this is optional and can be omitted, but when supplied should be a type (for example `str` or `int`). (options: ['--example'])\"),\n    ):\n        warnings.warn(\n            deprecated.ARGUMENT_TYPE_STR_CHOICE.format(\n                typ='str', names=\"['--example']\"\n            )\n        )\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", type=\"choice\", choices=[\"a\", \"b\", \"c\"])\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'choice'.*\",\n            \"*For choices this is optional and can be omitted, but when supplied should be a type (for example `str` or `int`).*\",\n            \"*options: --foo*\",\n        ]\n    )\n", "def test_argument_percent_default_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", action=\"store\", default=\"bar\", help=\"foo option\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"\",\n        ]\n    )\n", "def test_argument_type_str_choice_deprecation(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\n                '--foo', action='store', type='str', choices=['a', 'b', 'c'], help='foo option'\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'str'.*\",\n            \"*For choices this is optional and can be omitted,*\",\n            \"*but when supplied should be a type (for example `str` or `int`).*\"\n        ]\n    )\n"], "sample_695": ["def test_node_repr() -> None:\n    node = nodes.Node(\"test_node\", parent=None, config=cast(pytest.Config, {}))\n    assert repr(node) == \"<Node test_node>\"\n", "def test_node_add_marker() -> None:\n    node = nodes.Node(name=\"test_node\", parent=None, config=pytest.Config.fromdictargs({}, []), session=None)\n    marker_name = \"test_marker\"\n    node.add_marker(marker_name)\n    assert marker_name in node.keywords\n", "def test_imply_path_with_mismatched_paths() -> None:\n    path = Path(\"/some/path\")\n    fspath = legacy_path(\"/some/other/path\")\n    with pytest.raises(ValueError, match=\"if both path and fspath are given they need to be equal\"):\n        nodes._imply_path(path, fspath)\n", "compilation error", "def test_node_repr() -> None:\n    \"\"\"Test the string representation of a Node instance.\"\"\"\n    node = nodes.Node(name=\"test_node\", parent=None, config=cast(pytest.Config, {}), session=cast(pytest.Session, {}))\n    assert repr(node) == \"<Node test_node>\"\n"], "sample_696": ["def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'\n        ),\n    ):\n        warn(deprecated.ARGUMENT_PERCENT_DEFAULT)\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", default=\"bar\", help=\"foo option with default\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"*\"\n        ]\n    )\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape('pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'),\n    ):\n        warnings.warn('pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"', pytest.PytestDeprecationWarning)\n", "compilation error", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape('pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'),\n    ):\n        warn(deprecated.ARGUMENT_PERCENT_DEFAULT)\n"], "sample_697": ["def test_get_user_with_valid_user(monkeypatch):\n    \"\"\"Test get_user() function with valid user environment variables set.\"\"\"\n    monkeypatch.setenv(\"USER\", \"testuser\")\n    assert get_user() == \"testuser\"\n\n    monkeypatch.delenv(\"USER\", raising=False)\n    monkeypatch.setenv(\"USERNAME\", \"testuser\")\n    assert get_user() == \"testuser\"\n", "def test_tmp_path_factory_handles_none_user(tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test that tmp_path_factory handles None user gracefully.\"\"\"\n    monkeypatch.setattr(\"getpass.getuser\", lambda: None)\n    # _basetemp / _given_basetemp are cached / set in parallel runs, patch them\n    monkeypatch.setattr(tmp_path_factory, \"_basetemp\", None)\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", None)\n    p = tmp_path_factory.getbasetemp()\n    assert \"pytest-of-unknown\" in str(p)\n", "def test_get_user_when_env_vars_present(monkeypatch):\n    \"\"\"Ensure get_user returns the correct value when USERNAME environment variable is set\"\"\"\n    monkeypatch.setenv(\"USER\", \"testuser1\")\n    assert get_user() == \"testuser1\"\n\n    monkeypatch.delenv(\"USER\", raising=False)\n    monkeypatch.setenv(\"USERNAME\", \"testuser2\")\n    assert get_user() == \"testuser2\"\n\n    monkeypatch.delenv(\"USERNAME\", raising=False)\n    monkeypatch.setenv(\"LOGNAME\", \"testuser3\")\n    assert get_user() == \"testuser3\"\n", "def test_get_user_handles_special_characters(monkeypatch):\n    \"\"\"Test that get_user handles usernames with special characters correctly.\"\"\"\n    special_username = \"user!@#$%^&*()_+\"\n    monkeypatch.setattr(\"getpass.getuser\", lambda: special_username)\n    assert get_user() == special_username\n", "def test_temp_path_factory_with_no_given_basetemp(tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test that TempPathFactory handles the case where no given_basetemp is provided.\"\"\"\n    monkeypatch.setattr(tmp_path_factory, \"_basetemp\", None)\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", None)\n    \n    # Simulate the environment with no PYTEST_DEBUG_TEMPROOT\n    monkeypatch.delenv(\"PYTEST_DEBUG_TEMPROOT\", raising=False)\n    \n    # Since no _basetemp or _given_basetemp is set, it should use the default temp directory\n    basetemp = tmp_path_factory.getbasetemp()\n    \n    assert basetemp.is_dir()\n    assert \"pytest-of-\" in str(basetemp)\n"], "sample_698": ["def test_logcapturehandler_emit() -> None:\n    handler = LogCaptureHandler()\n    logger = logging.getLogger(\"test_logger\")\n    logger.addHandler(handler)\n\n    logger.info(\"This is a test log message\")\n\n    assert len(handler.records) == 1\n    assert handler.records[0].getMessage() == \"This is a test log message\"\n\n    handler.reset()\n    assert len(handler.records) == 0\n    assert handler.stream.getvalue() == \"\"\n", "def test_catching_logs_context_manager() -> None:\n    logfmt = \"%(levelname)-8s %(message)s\"\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    handler = logging.StreamHandler(StringIO())\n    formatter = logging.Formatter(logfmt)\n    handler.setFormatter(formatter)\n\n    with catching_logs(handler, level=logging.INFO) as log_handler:\n        logger = logging.getLogger()\n        logger.addHandler(log_handler)\n        logger.setLevel(logging.INFO)\n        logger.info(\"Test Message\")\n        output = log_handler.stream.getvalue().strip()\n        assert output == \"INFO     Test Message\"\n        logger.removeHandler(log_handler)\n", "def test_logcapturehandler_emit_and_reset() -> None:\n    handler = LogCaptureHandler()\n    logger = logging.getLogger(\"testlogger\")\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n\n    logger.info(\"Test log message\")\n    \n    assert len(handler.records) == 1\n    assert handler.records[0].getMessage() == \"Test log message\"\n    \n    handler.reset()\n    \n    assert len(handler.records) == 0\n    assert handler.stream.getvalue() == \"\"\n    \n    logger.removeHandler(handler)\n", "def test_logcapturehandler_emit_and_reset() -> None:\n    handler = LogCaptureHandler()\n    \n    record = logging.LogRecord(\n        name=\"test\",\n        level=logging.DEBUG,\n        pathname=\"testpath\",\n        lineno=1,\n        msg=\"Debug Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    handler.emit(record)\n    assert len(handler.records) == 1\n    assert handler.records[0].msg == \"Debug Message\"\n    \n    handler.reset()\n    assert len(handler.records) == 0\n    assert handler.stream.getvalue() == \"\"\n", "def test_logcapturehandler_emit() -> None:\n    handler = LogCaptureHandler()\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n    handler.emit(record)\n    \n    assert len(handler.records) == 1\n    assert handler.records[0].message == \"Test Message\"\n    assert handler.stream.getvalue().strip() == \"Test Message\"\n"], "sample_699": ["def test_pytest_collect_file_for_py_modules(self, pytester: Pytester):\n    path = pytester.makepyfile(\n        whatever=\"\"\"\n        '''\n        >>> def add(a, b):\n        ...     return a + b\n        >>> add(1, 2)\n        3\n        '''\n    \"\"\"\n    )\n    collector = DoctestModule.from_parent(pytester.path, path=path)\n    assert collector is not None\n    assert isinstance(collector, DoctestModule)\n    items, reprec = pytester.inline_genitems(path, \"--doctest-modules\")\n    assert len(items) == 1\n    assert isinstance(items[0], DoctestItem)\n", "def test_doctestmodule_with_import_error_handling(self, pytester: Pytester):\n    \"\"\"Test that doctest module with import error handling works correctly.\"\"\"\n    pytester.makepyfile(\n        module_with_import_error=\"\"\"\n        try:\n            import nonexistentmodule\n        except ImportError:\n            nonexistentmodule = None\n\n            '''\n            >>> func()\n            'fallback'\n            '''\n            if nonexistentmodule is None:\n                return 'fallback'\n            return nonexistentmodule.some_function()\n    \"\"\"\n    )\n    reprec = pytester.inline_run(\"--doctest-modules\")\n    reprec.assertoutcome(passed=1)\n", "def test_doctest_module_with_mocked_function(self, pytester: Pytester):\n    \"\"\"Test that doctests work correctly with mocked functions.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        from unittest.mock import patch\n\n            '''\n            >>> from unittest.mock import patch\n            >>> with patch('builtins.print') as mock_print:\n            ...     print('Hello, world!')\n            ...     mock_print.assert_called_with('Hello, world!')\n            '''\n            pass\n    \"\"\"\n    )\n    reprec = pytester.inline_run(\"--doctest-modules\")\n    reprec.assertoutcome(passed=1)\n", "    def test_doctest_continue_on_failure_option(self, pytester: Pytester):\n        pytester.makeini(\n            \"\"\"\n            [pytest]\n            doctest_continue_on_failure = True\n        \"\"\"\n        )\n        p = pytester.maketxtfile(\n            test_continue_failure=\"\"\"\n            >>> i = 1\n            >>> i\n            2\n            >>> i + 1\n            2\n        \"\"\"\n        )\n        reprec = pytester.inline_run(p)\n        reprec.assertoutcome(failed=1)\n", "def test_doctestmodule_unexpected_exception_traceback(self, pytester: Pytester):\n    \"\"\"Test that the full traceback is shown for unexpected exceptions within doctests.\"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n            '''\n            >>> def broken():\n            ...     raise ValueError(\"unexpected error\")\n            >>> broken()\n            '''\n        \"\"\"\n    )\n    result = pytester.runpytest(p, \"--doctest-modules\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*= FAILURES =*\",\n            \"*_ [[]doctest[]] test_doctestmodule_unexpected_exception_traceback.func _*\",\n            \"UNEXPECTED EXCEPTION: ValueError('unexpected error')\",\n            \"Traceback (most recent call last):\",\n            '  File \"*/doctest.py\", line *, in __run',\n            \"    *\",\n            '  File \"<doctest test_doctestmodule_unexpected_exception_traceback.func[2]>\", line 1, in <module>',\n            '    broken()',\n            \"  File \\\"<doctest test_doctestmodule_unexpected_exception_traceback.func[1]>\\\", line 2, in broken\",\n            '    raise ValueError(\"unexpected error\")',\n            \"ValueError: unexpected error\",\n            \"*/test_doctestmodule_unexpected_exception_traceback.py:None: UnexpectedException\",\n            \"*= 1 failed in *\",\n        ],\n        consecutive=True,\n    )\n"], "sample_700": ["    def test_pytest_addoption(self, pytester: Pytester) -> None:\n        pytester.makeconftest(\n            \"\"\"\n                parser.addoption(\"--myopt\", action=\"store_true\", help=\"my custom option\")\n            \"\"\"\n        )\n        result = pytester.runpytest(\"--help\")\n        result.stdout.fnmatch_lines([\"*--myopt*my custom option*\"])\n", "    def test_pytest_addoption(self, pytester: Pytester) -> None:\n        pytester.makeconftest(\n            \"\"\"\n                group = parser.getgroup(\"custom\")\n                group.addoption(\n                    \"--custom-option\",\n                    action=\"store_true\",\n                    dest=\"custom_option\",\n                    default=False,\n                    help=\"custom option for testing\",\n                )\n            \"\"\"\n        )\n        result = pytester.runpytest(\"--custom-option\")\n        assert result.ret == 0\n        assert result.parseoutcomes()[\"errors\"] == 0\n", "def test_pytest_addoption_showfixtures(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--fixtures\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*show available fixtures*\",\n            \"*fixtures with leading '_' are only shown with '-v'*\",\n        ]\n    )\n", "def test_pytest_addoption_defaults(pytester: Pytester) -> None:\n    config = pytester.parseconfig()\n    assert not config.option.showfixtures\n    assert not config.option.show_fixtures_per_test\n    assert config.getini(\"python_files\") == [\"test_*.py\", \"*_test.py\"]\n    assert config.getini(\"python_classes\") == [\"Test\"]\n    assert config.getini(\"python_functions\") == [\"test\"]\n    assert not config.getini(\"disable_test_id_escaping_and_forfeit_all_rights_to_community_support\")\n", "def test_pytest_addoption() -> None:\n    parser = pytest.Parser()\n    pytest_addoption(parser)\n    option = parser.parse([\"--fixtures\"])\n    assert option.showfixtures\n    option = parser.parse([\"--fixtures-per-test\"])\n    assert option.show_fixtures_per_test\n    assert parser.getini(\"python_files\") == [\"test_*.py\", \"*_test.py\"]\n    assert parser.getini(\"python_classes\") == [\"Test\"]\n    assert parser.getini(\"python_functions\") == [\"test\"]\n    assert not parser.getini(\"disable_test_id_escaping_and_forfeit_all_rights_to_community_support\")\n"], "sample_701": ["def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", help=\"foo option\", default=\"bar\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--foo=bar\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"*%default*\\\" should be changed to \\\"*%(default)s*\\\"\",\n        ]\n    )\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(PytestDeprecationWarning, match=re.escape('pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"')):\n        warnings.warn('pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"', PytestDeprecationWarning)\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'\n        ),\n    ):\n        warnings.warn(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"',\n            PytestDeprecationWarning,\n        )\n", "def test_argument_percent_default_deprecated() -> None:\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape('pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'),\n    ):\n        warnings.warn('pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"', PytestDeprecationWarning)\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", action=\"store\", default=\"bar\", help=\"some help\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"*\",\n        ]\n    )\n"], "sample_702": ["def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    # Mock subprocess.run to return predefined output\n    mock_output = \"f1\\0/path/to/file1\\nf2\\0/path/to/file2\\nf3\\0/path/to/file3\"\n    monkeypatch.setattr(subprocess, \"run\", lambda *args, **kwargs: subprocess.CompletedProcess(args, 0, mock_output))\n\n    checker = pytester.getitem(\"def test_func(): pass\").config.pluginmanager.getplugin(\"LsofFdLeakChecker\")\n    assert isinstance(checker, LsofFdLeakChecker)\n\n    # Check if get_open_files method works correctly\n    open_files = checker.get_open_files()\n    assert open_files == [(\"1\", \"/path/to/file1\"), (\"2\", \"/path/to/file2\"), (\"3\", \"/path/to/file3\")]\n\n    # Check if matching_platform method works correctly\n    assert checker.matching_platform() == True\n\n    # Test the pytest_runtest_protocol hook implementation\n    item = pytester.getitem(\"def test_func(): pass\")\n    recorder = pytester.make_hook_recorder(item.config.pluginmanager)\n    with recorder.hook.pytest_runtest_protocol(item=item):\n        pass\n    assert not recorder.getfailures()\n", "def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    # Create a mock for subprocess.run to simulate lsof output\n        class MockCompletedProcess:\n                self.stdout = stdout\n\n        if \"-v\" in args[0]:\n            return MockCompletedProcess(stdout=\"lsof version 4.93.2\")\n        return MockCompletedProcess(stdout=\"f1\\0/path/to/file1\\0f2\\0/path/to/file2\")\n\n    monkeypatch.setattr(subprocess, \"run\", mock_subprocess_run)\n\n    checker = LsofFdLeakChecker()\n\n    # Check if matching_platform works correctly\n    assert checker.matching_platform() is True\n\n    # Check if get_open_files works correctly\n    open_files = checker.get_open_files()\n    assert open_files == [(\"1\", \"/path/to/file1\"), (\"2\", \"/path/to/file2\")]\n\n    # Integrate with pytest hook to ensure it registers correctly\n    pytester.makepyfile(\n        \"\"\"\n            assert True\n        \"\"\"\n    )\n    config = pytester.parseconfig()\n    config.pluginmanager.register(checker)\n\n    item = pytester.getitem(\"def test_func(): pass\")\n    next(pytester.inline_run(\"--lsof\", str(item.fspath)).hook.pytest_runtest_protocol(item=item))\n    \n    # Verify no FD leakage detected\n    assert not item.own_markers\n", "def test_get_public_names() -> None:\n    assert get_public_names([\"_private\", \"public\", \"__hidden\", \"visible\"]) == [\"public\", \"visible\"]\n", "def test_run_with_lsof_option(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test running pytest with the --lsof option.\"\"\"\n    # Create a dummy test file\n    testfile = pytester.makepyfile(\"def test_dummy(): assert True\")\n\n    # Mock the subprocess.run method to avoid actual lsof execution\n        class MockCompletedProcess:\n                self.stdout = \"f123\\0/home/user/file.txt\\0\"\n        return MockCompletedProcess()\n\n    monkeypatch.setattr(subprocess, \"run\", mock_subprocess_run)\n\n    # Run pytest with the --lsof option\n    result = pytester.runpytest(\"--lsof\", testfile)\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_lsof_fd_leak_checker_platform_match(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    lsof_checker = pytester._request.config.pluginmanager.get_plugin(\"LsofFdLeakChecker\")\n    monkeypatch.setattr(subprocess, \"run\", lambda *args, **kwargs: subprocess.CompletedProcess(args, 0))\n\n    assert lsof_checker.matching_platform() is True\n\n    monkeypatch.setattr(subprocess, \"run\", lambda *args, **kwargs: (_ for _ in ()).throw(OSError))\n    assert lsof_checker.matching_platform() is False\n"], "sample_703": ["def test_complex_expression() -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True, \"d\": False}.__getitem__\n    expr = \"a and (b or c) and not (d or (a and b))\"\n    assert evaluate(expr, matcher) is True\n", "def test_complex_expressions() -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True, \"d\": False}.__getitem__\n    assert evaluate(\"(a or b) and (c or d)\", matcher) is True\n    assert evaluate(\"(a and b) or (c and d)\", matcher) is False\n    assert evaluate(\"(a and not b) or (not c and d)\", matcher) is True\n    assert evaluate(\"((a or b) and (c or d)) or not (a and b)\", matcher) is True\n", "def test_multiple_identifiers() -> None:\n    matcher = {\"foo\": True, \"bar\": False, \"baz\": True}.__getitem__\n    assert evaluate(\"foo and bar or baz\", matcher) is True\n    assert evaluate(\"foo and (bar or baz)\", matcher) is True\n    assert evaluate(\"foo and (not bar or baz)\", matcher) is True\n    assert evaluate(\"foo and not bar and baz\", matcher) is True\n    assert evaluate(\"(foo or bar) and baz\", matcher) is True\n    assert evaluate(\"foo or bar and baz\", matcher) is True\n    assert evaluate(\"foo and (bar and baz)\", matcher) is False\n    assert evaluate(\"(foo and bar) or (baz and bar)\", matcher) is False\n", "def test_combined_expressions() -> None:\n    matcher = {\"true\": True, \"false\": False, \"maybe\": True, \"never\": False}.__getitem__\n    assert evaluate(\"(true or false) and maybe\", matcher) is True\n    assert evaluate(\"(true and false) or never\", matcher) is False\n    assert evaluate(\"not (maybe and never) or true\", matcher) is True\n    assert evaluate(\"not (true or false) and never\", matcher) is False\n    assert evaluate(\"(not maybe or true) and (false or never)\", matcher) is False\n", "def test_complex_nested_expression() -> None:\n    expr = \"((true and not false) or (false or not false)) and (true or (not false and (true or false)))\"\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is True\n"], "sample_704": ["def test_node_repr() -> None:\n    class TestNode(nodes.Node):\n            super().__init__(name, parent=parent, config=config, session=session, nodeid=nodeid)\n\n    parent_node = TestNode(name=\"parent\")\n    node = TestNode(name=\"child\", parent=parent_node)\n\n    assert repr(node) == \"<TestNode child>\"\n    assert repr(parent_node) == \"<TestNode parent>\"\n", "def test_iter_markers_with_node() -> None:\n    class DummyParent(nodes.Node):\n            super().__init__(name, parent=parent, **kw)\n            self.own_markers = []\n\n    parent = DummyParent(name=\"parent\", nodeid=\"parent\")\n    child = nodes.Node(name=\"child\", parent=parent, nodeid=\"parent::child\")\n\n    parent.add_marker(pytest.mark.skip)\n    child.add_marker(pytest.mark.slow)\n\n    markers = list(child.iter_markers_with_node())\n    assert len(markers) == 2\n    assert markers[0][1].name == \"slow\"\n    assert markers[1][1].name == \"skip\"\n", "def test_add_marker_to_node() -> None:\n    node = nodes.Node.from_parent(None, name=\"test_node\", config=pytest.Config())\n    marker_name = \"custom_marker\"\n    node.add_marker(marker_name)\n    assert marker_name in node.keywords\n    assert any(marker.name == marker_name for marker in node.own_markers)\n", "def test_node_keywords_inheritance() -> None:\n    parent_node = nodes.Node.from_parent(None, name=\"parent\", config=pytest.Config())\n    parent_node.keywords[\"parent_marker\"] = \"parent_value\"\n\n    child_node = nodes.Node.from_parent(parent_node, name=\"child\")\n\n    assert \"parent_marker\" in child_node.keywords\n    assert child_node.keywords[\"parent_marker\"] == \"parent_value\"\n", "def test_node_repr() -> None:\n    parent_node = nodes.Node.from_parent(name=\"parent\", parent=None, nodeid=\"parent\")\n    child_node = nodes.Node.from_parent(name=\"child\", parent=parent_node, nodeid=\"parent::child\")\n    assert repr(child_node) == \"<Node child>\"\n"], "sample_705": ["def test_get_public_names() -> None:\n    assert get_public_names([\"_private\", \"public\", \"__very_private\"]) == [\"public\"]\n    assert get_public_names([\"a\", \"b\", \"c\"]) == [\"a\", \"b\", \"c\"]\n    assert get_public_names([]) == []\n    assert get_public_names([\"_hidden\"]) == []\n", "def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n        class MockCompletedProcess:\n            stdout = \"f1\\0/home/user/file1\\nf2\\0/home/user/file2\\n\"\n        return MockCompletedProcess()\n\n    monkeypatch.setattr(subprocess, \"run\", mock_run)\n    checker = LsofFdLeakChecker()\n    assert checker.matching_platform()\n    open_files = checker.get_open_files()\n    assert open_files == [(\"1\", \"/home/user/file1\"), (\"2\", \"/home/user/file2\")]\n\n    # Ensure the hook works correctly\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--lsof\")\n    assert result.ret == 0\n    assert \"FD leakage detected\" not in \"\\n\".join(result.outlines)\n", "def test_hookrecorder_clear_functionality(pytester: Pytester) -> None:\n    pm = PytestPluginManager()\n    recorder = pytester.make_hook_recorder(pm)\n\n    class rep:\n        excinfo = None\n        passed = False\n        failed = True\n        skipped = False\n        when = \"call\"\n\n    recorder.hook.pytest_runtest_logreport(report=rep)  # type: ignore[attr-defined]\n    failures = recorder.getfailures()\n    assert len(failures) == 1  # Ensure there is one failure recorded\n\n    recorder.clear()  # Clear the recorder\n    failures = recorder.getfailures()\n    assert len(failures) == 0  # Ensure there are no failures after clearing\n", "def test_get_public_names() -> None:\n    names = [\"_private1\", \"public1\", \"__private2\", \"public2\"]\n    result = pytester_mod.get_public_names(names)\n    assert result == [\"public1\", \"public2\"]\n", "def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    class MockSubprocess:\n            self.returncode = returncode\n            self.stdout = stdout\n\n            if \"check\" in kwargs and kwargs[\"check\"] and self.returncode != 0:\n                raise subprocess.CalledProcessError(self.returncode, args)\n            return self\n\n    # Mocking subprocess.run to simulate lsof output\n    mock_output = \"f123\\0n/tmp/testfile\\0f124\\0n/var/lib/sss/mc/passwd\\0\"\n    monkeypatch.setattr(subprocess, \"run\", MockSubprocess(stdout=mock_output).run)\n\n    checker = LsofFdLeakChecker()\n    open_files = checker.get_open_files()\n    \n    assert open_files == [(\"123\", \"/tmp/testfile\")]  # Should filter out ignored path\n\n    # Test platform matching\n    monkeypatch.setattr(subprocess, \"run\", MockSubprocess(returncode=0).run)\n    assert checker.matching_platform()\n\n    monkeypatch.setattr(subprocess, \"run\", MockSubprocess(returncode=1).run)\n    assert not checker.matching_platform()\n\n    # Testing pytest_runtest_protocol hook implementation\n    class MockItem:\n        location = (\"filename\", 1, \"testname\")\n            self.warning = warning\n    \n    item = MockItem()\n    monkeypatch.setattr(checker, \"get_open_files\", lambda: [(\"123\", \"/tmp/testfile\")])\n    generator = checker.pytest_runtest_protocol(item)\n    next(generator)\n    monkeypatch.setattr(checker, \"get_open_files\", lambda: [(\"123\", \"/tmp/testfile\"), (\"124\", \"/tmp/testfile2\")])\n    with pytest.raises(StopIteration):\n        next(generator)\n\n    assert hasattr(item, 'warning')\n    assert \"FD leakage detected\" in str(item.warning)\n"], "sample_706": ["def test_complex_expressions() -> None:\n    matcher = {\"true\": True, \"false\": False, \"maybe\": False, \"sometimes\": True, \"often\": True, \"rarely\": False}.__getitem__\n    assert evaluate(\"true and (false or true and not false)\", matcher)\n    assert not evaluate(\"false or (false and true)\", matcher)\n    assert evaluate(\"true or (false and true) and (true or false)\", matcher)\n    assert not evaluate(\"false and (true or false) and (true or false)\", matcher)\n    assert evaluate(\"(true and (false or true)) or (sometimes and not rarely)\", matcher)\n", "def test_complex_expressions() -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True, \"d\": False}.__getitem__\n    assert evaluate(\"a and b or c and not d\", matcher) is True\n    assert evaluate(\"a and (b or c) and not d\", matcher) is False\n    assert evaluate(\"(a or b) and (c or d)\", matcher) is True\n    assert evaluate(\"not (a and b) or (c and not d)\", matcher) is True\n    assert evaluate(\"(not a or not b) and (not c or not d)\", matcher) is False\n", "def test_combined_and_or_not() -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True, \"d\": False}.__getitem__\n    assert evaluate(\"a and not b or c and not d\", matcher) is True\n    assert evaluate(\"a and b or c and d\", matcher) is False\n    assert evaluate(\"not a or b and c or not d\", matcher) is True\n    assert evaluate(\"(a and b) or (c and d)\", matcher) is False\n", "def test_complex_expressions() -> None:\n    matcher = {\n        \"a\": True,\n        \"b\": False,\n        \"c\": True,\n        \"d\": False,\n        \"e\": True,\n        \"f\": False,\n    }.__getitem__\n    \n    assert evaluate(\"a and b or c and (d or e)\", matcher) is True\n    assert evaluate(\"not (a and b) or (c and d) and e\", matcher) is True\n    assert evaluate(\"((a or b) and (c or d)) or ((e and f) or (a and not b))\", matcher) is True\n    assert evaluate(\"((a and b) or (c and d)) and ((e or f) and (not a or b))\", matcher) is False\n    assert evaluate(\"(a and (b or c) and (d or (e and f)))\", matcher) is False\n", "def test_complex_nested_expression() -> None:\n    matcher = {\n        \"a\": True,\n        \"b\": False,\n        \"c\": True,\n        \"d\": False,\n        \"e\": True,\n        \"f\": False,\n    }.__getitem__\n    expr = \"(a and (b or (c and (d or (e and f))))) or (not (a or (not b and (c or not d))))\"\n    assert evaluate(expr, matcher) is False\n"], "sample_707": ["def test_fscollector_initialization_with_node_and_path(tmp_path: Path) -> None:\n    \"\"\"Test FSCollector initialization with both a parent node and a path.\"\"\"\n    config = cast(pytest.Config, None)\n    session = cast(pytest.Session, None)\n\n    # Create a parent FSCollector node.\n    parent_node = nodes.FSCollector.from_parent(\n        parent=None,\n        path=tmp_path,\n        config=config,\n        session=session,\n    )\n\n    # Initialize FSCollector with the parent node and a sub path.\n    sub_path = tmp_path / \"subdir\"\n    sub_path.mkdir()\n    collector_node = nodes.FSCollector.from_parent(\n        parent=parent_node,\n        path=sub_path,\n        config=config,\n        session=session,\n    )\n\n    assert collector_node.parent == parent_node\n    assert collector_node.path == sub_path\n    assert collector_node.name == \"subdir\"\n    assert collector_node.session == parent_node.session\n", "def test_node_add_marker() -> None:\n    parent = nodes.Node.from_parent(None, name=\"parent\", session=pytest.Session(), config=pytest.Config())\n    node = nodes.Node.from_parent(parent, name=\"child\")\n\n    marker_name = \"my_marker\"\n    node.add_marker(marker_name)\n\n    assert marker_name in node.keywords\n    assert any(marker.name == marker_name for marker in node.own_markers)\n", "def test_fscollector_initialization(tmp_path: Path) -> None:\n    \"\"\"Test the initialization of FSCollector.\"\"\"\n    class FakeSession:\n        _initialpaths = frozenset({tmp_path})\n        config = cast(pytest.Config, type(\"Config\", (object,), {\"rootpath\": tmp_path, \"invocation_params\": type(\"Params\", (object,), {\"dir\": tmp_path})()}))\n        \n            return None\n\n            return False\n\n    session = cast(pytest.Session, FakeSession)\n    path = tmp_path / \"some\" / \"path\"\n    path.mkdir(parents=True)\n    node = nodes.FSCollector.from_parent(parent=session, path=path)\n\n    assert node.name == \"path\"\n    assert node.path == path\n    assert node.session == session\n    assert node.config == session.config\n    assert node.nodeid == \"some/path\"\n", "def test_node_add_marker() -> None:\n    class DummySession:\n            self._setupstate = None\n\n    session = DummySession()\n    node = nodes.Node.from_parent(None, name=\"dummy\", session=session, config=None)\n    node.add_marker(\"my_marker\")\n    assert \"my_marker\" in node.keywords\n    assert any(marker.name == \"my_marker\" for marker in node.own_markers)\n\n    node.add_marker(pytest.mark.skip)\n    assert \"skip\" in node.keywords\n    assert any(marker.name == \"skip\" for marker in node.own_markers)\n", "def test_node_keywords() -> None:\n    \"\"\"\n    Ensure that NodeKeywords works correctly, allowing for setting and getting keywords.\n    \"\"\"\n    class FakeNode(nodes.Node):\n            self.name = name\n            self.keywords = nodes.NodeKeywords(self)\n            super().__init__(name, parent=None, config=None, session=None)\n\n    node = FakeNode(name=\"test_node\")\n    node.keywords[\"keyword1\"] = \"value1\"\n    assert node.keywords[\"keyword1\"] == \"value1\"\n    assert \"keyword1\" in node.keywords\n    assert node.keywords.keys() == {\"keyword1\"}\n    assert node.keywords.values() == {\"value1\"}\n    assert node.keywords.items() == {(\"keyword1\", \"value1\")}\n"], "sample_708": ["def test_source_from_multiline_string() -> None:\n    source = Source(\n        \"\"\"\\\n            return (\n                \"line1\"\n                \"line2\"\n            )\n    \"\"\"\n    )\n    expected_lines = [\n        \"def foo():\",\n        '    return (',\n        '        \"line1\"',\n        '        \"line2\"',\n        '    )'\n    ]\n    assert source.lines == expected_lines\n", "def test_source_from_object_with_getrawcode() -> None:\n    class TestClass:\n            pass\n\n    source = Source(TestClass().method)\n    assert \"def method(self)\" in str(source)\n", "def test_source_deindent() -> None:\n    source = Source(\n        \"\"\"\\\n            if True:\n                return 42\n    \"\"\"\n    )\n    deindented = source.deindent()\n    assert deindented.lines == [\n        \"def foo():\",\n        \"    if True:\",\n        \"        return 42\",\n    ]\n", "def test_source_indent() -> None:\n    source = Source(\"def test():\\n    pass\")\n    indented_source = source.indent(\"  \")\n    assert str(indented_source) == \"  def test():\\n      pass\"\n", "def test_source_from_multiline_string() -> None:\n    multiline_string = \"\"\"This is a multiline\n    string with various\n    indentation levels and spaces.\"\"\"\n    \n    source = Source(multiline_string)\n    assert source.lines == [\n        \"This is a multiline\",\n        \"string with various\",\n        \"indentation levels and spaces.\"\n    ]\n"], "sample_709": ["def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test the LsofFdLeakChecker functionality.\"\"\"\n\n    class MockSubprocessRun:\n            self.returncode = returncode\n            self.stdout = stdout\n\n            class Result:\n                    self.returncode = returncode\n                    self.stdout = stdout\n\n            return Result(self.returncode, self.stdout)\n\n    mock_subprocess = MockSubprocessRun(\n        stdout=\"f1\\0/path/to/file1\\nf2\\0/path/to/file2\\n\"\n    )\n    monkeypatch.setattr(subprocess, \"run\", mock_subprocess.run)\n\n    checker = LsofFdLeakChecker()\n\n    assert checker.matching_platform()\n\n    open_files = checker.get_open_files()\n    assert open_files == [(\"1\", \"/path/to/file1\"), (\"2\", \"/path/to/file2\")]\n\n    mock_subprocess = MockSubprocessRun(\n        stdout=\"f1\\0/path/to/file1\\nf2\\0/path/to/file2\\nf3\\0/path/to/file3\\n\"\n    )\n    monkeypatch.setattr(subprocess, \"run\", mock_subprocess.run)\n\n    item = pytester.getitem(\"def test_func(): pass\")\n    item.warn = lambda warning: None  # Mock the warn method\n\n        lines1 = checker.get_open_files()\n        if hasattr(sys, \"pypy_version_info\"):\n            gc.collect()\n        lines2 = checker.get_open_files()\n\n        new_fds = {t[0] for t in lines2} - {t[0] for t in lines1}\n        leaked_files = [t for t in lines2 if t[0] in new_fds]\n        if leaked_files:\n            error = [\n                \"***** %s FD leakage detected\" % len(leaked_files),\n                *(str(f) for f in leaked_files),\n                \"*** Before:\",\n                *(str(f) for f in lines1),\n                \"*** After:\",\n                *(str(f) for f in lines2),\n                \"***** %s FD leakage detected\" % len(leaked_files),\n                \"*** function %s:%s: %s \" % item.location,\n                \"See issue #2366\",\n            ]\n            item.warn(Py", "def test_pytester_inline_run_with_plugin(pytester: Pytester) -> None:\n    \"\"\"Test inline_run method with a plugin.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            assert 1\n        \"\"\"\n    )\n    plugin = type(\"MyPlugin\", (object,), {\"pytest_addoption\": lambda self, parser: None})()\n    result = pytester.inline_run(\"--strict-markers\", plugins=[plugin])\n    result.assertoutcome(passed=1)\n", "def test_linecomp_assert_contains_lines() -> None:\n    comp = pytester_mod.LineComp()\n    comp.stringio.write(\"line1\\nline2\\nline3\\n\")\n    comp.assert_contains_lines([\"line2\"])\n    with pytest.raises(pytest.fail.Exception, match=\"line 'line4' not found in output\"):\n        comp.assert_contains_lines([\"line4\"])\n", "def test_lsof_fd_leak_checker_get_open_files(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n        class MockCompletedProcess:\n                self.stdout = stdout\n\n        assert args[0] == (\"lsof\", \"-Ffn0\", \"-p\", str(os.getpid()))\n        return MockCompletedProcess(\n            \"fcwd\\nfmem\\nftxt\\nf1\\0/path/to/file1\\nf2\\0/path/to/file2\\ndel\\n\"\n            \"f3\\0/var/lib/sss/mc/passwd\\n\"\n        )\n\n    monkeypatch.setattr(subprocess, \"run\", mock_run)\n    checker = LsofFdLeakChecker()\n    open_files = checker.get_open_files()\n    assert open_files == [(\"1\", \"/path/to/file1\"), (\"2\", \"/path/to/file2\")]\n", "def test_lsof_fd_leak_checker(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n        class MockCompletedProcess:\n                self.stdout = \"f123\\0/tmp/testfile\\nf456\\0/var/lib/sss/mc/passwd\\n\"\n        return MockCompletedProcess()\n\n    monkeypatch.setattr(subprocess, \"run\", mock_subprocess_run)\n    lsof_checker = pytester.LsofFdLeakChecker()\n\n    open_files = lsof_checker.get_open_files()\n    assert open_files == [(\"123\", \"/tmp/testfile\")]\n\n    assert lsof_checker.matching_platform() is True\n\n    class MockItem:\n        location = (\"mockfile.py\", 1, \"mocktest\")\n\n    item = MockItem()\n    with pytest.raises(pytest.PytestWarning) as excinfo:\n        with lsof_checker.pytest_runtest_protocol(item):\n            pass  # simulate test run\n\n    assert \"1 FD leakage detected\" in str(excinfo.value)\n"], "sample_710": ["def test_custom_exception_handling(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        class MyCustomException(Exception):\n            pass\n\n        class MyTestCase(unittest.TestCase):\n                raise MyCustomException(\"This is a custom exception\")\n\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    assert reprec.matchreport(\"test_custom_exception\").failed\n    assert \"MyCustomException: This is a custom exception\" in str(reprec.matchreport(\"test_custom_exception\").longrepr)\n", "def test_unittest_setup_teardown_classes(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestSetupTeardown(unittest.TestCase):\n            setup_called = False\n            teardown_called = False\n\n            @classmethod\n                cls.setup_called = True\n\n            @classmethod\n                cls.teardown_called = True\n\n                self.assertTrue(self.__class__.setup_called)\n                self.assertFalse(self.__class__.teardown_called)\n\n        class TestAnotherClass(unittest.TestCase):\n                self.assertFalse(TestSetupTeardown.teardown_called)\n\n            assert TestSetupTeardown.teardown_called\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(passed=3)\n", "def test_makeitem_not_unittest_subclass(pytester: Pytester) -> None:\n    \"\"\"Test that pytest_pycollect_makeitem returns None for non-unittest subclass.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        class MyNonTestCase:\n            pass\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=0, failed=0, skipped=0)\n", "def test_teardown_with_multiple_cleanups(pytester: Pytester) -> None:\n    \"\"\"Ensure multiple cleanup functions are called correctly in tearDown\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        cleanups = []\n\n        class TestMultipleCleanups(unittest.TestCase):\n\n                self.addCleanup(cleanups.append, \"cleanup1\")\n                self.addCleanup(cleanups.append, \"cleanup2\")\n\n                pass\n\n            assert cleanups == [\"cleanup1\", \"cleanup2\"]\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*::test_func PASSED*\",\n            \"*::test_check_cleanups PASSED*\",\n        ]\n    )\n    result.assert_outcomes(passed=2)\n", "def test_unittest_skip_class_attribute(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            __unittest_skip__ = True\n            __unittest_skip_why__ = 'skipping due to class attribute'\n\n                self.fail(\"This should not run\")\n\n        class AnotherTestCase(unittest.TestCase):\n                self.assertTrue(True)\n        \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(skipped=1, passed=1)\n"], "sample_711": ["def test_get_closest_marker() -> None:\n    \"\"\"Test the get_closest_marker method to ensure it finds the closest marker.\"\"\"\n\n    class TestNode(nodes.Node):\n            super().__init__(*args, **kwargs)\n            self.own_markers = [\n                nodes.Mark(name=\"marker1\", args=(), kwargs={}),\n                nodes.Mark(name=\"marker2\", args=(), kwargs={}),\n            ]\n\n    parent = TestNode(name=\"parent\")\n    child = TestNode(name=\"child\", parent=parent)\n\n    # Case where the marker exists in the child node\n    marker = child.get_closest_marker(\"marker1\")\n    assert marker is not None\n    assert marker.name == \"marker1\"\n\n    # Case where the marker does not exist\n    marker = child.get_closest_marker(\"nonexistent\")\n    assert marker is None\n\n    # Case where the marker exists in the parent node\n    marker = parent.get_closest_marker(\"marker2\")\n    assert marker is not None\n    assert marker.name == \"marker2\"\n", "def test_node_repr() -> None:\n    class TestNode(nodes.Node):\n            super().__init__(name, parent)\n\n    parent_node = TestNode(\"parent\")\n    child_node = TestNode(\"child\", parent=parent_node)\n\n    assert repr(parent_node) == \"<TestNode parent>\"\n    assert repr(child_node) == \"<TestNode child>\"\n", "def test_node_add_marker() -> None:\n    node = nodes.Node.from_parent(parent=None, name=\"test_node\", session=pytest.Session())\n    node.add_marker(\"test_marker\")\n    assert \"test_marker\" in node.keywords\n\n    # Test that adding a MarkDecorator works as expected.\n    mark_decorator = pytest.mark.example\n    node.add_marker(mark_decorator)\n    assert \"example\" in node.keywords\n", "compilation error", "def test_iter_markers_with_node() -> None:\n    class CustomNode(nodes.Node):\n            super().__init__(*args, **kwargs)\n            self.own_markers = [pytest.mark.custom_marker]\n\n    parent_node = CustomNode(name=\"parent\")\n    child_node = CustomNode.from_parent(parent=parent_node, name=\"child\")\n\n    markers = list(child_node.iter_markers_with_node(name=\"custom_marker\"))\n    assert len(markers) == 1\n    assert markers[0][1].name == \"custom_marker\"\n    assert markers[0][0] == parent_node\n"], "sample_712": ["def test_ordinal_encoder_handle_unknown():\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2]])\n    X2 = np.array([['Male', 4], ['Unknown', 3]])\n\n    # Test that ordinal encoder raises error for unknown features present during transform.\n    oe = OrdinalEncoder()\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n\n    # Test the ignore option, ignores unknown features (giving all 0's)\n    oe = OrdinalEncoder(handle_unknown='ignore')\n    oe.fit(X)\n    X2_passed = X2.copy()\n    assert_array_equal(\n        oe.transform(X2_passed),\n        np.array([[1, 3], [0, 2]])\n    )\n    # ensure transformed data was not modified in place\n    assert_allclose(X2, X2_passed)\n\n    # Raise error if handle_unknown is neither ignore nor error.\n    oe = OrdinalEncoder(handle_unknown='invalid_option')\n    assert_raises(ValueError, oe.fit, X)\n", "def test_ordinal_encoder_handle_unknown():\n    X = [['cat', 1], ['dog', 2], ['cat', 3]]\n    X2 = [['cat', 2], ['lion', 1]]\n\n    # Test that OrdinalEncoder raises error for unknown categories during transform\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    assert_raises(ValueError, enc.transform, X2)\n\n    # Test the ignore option, ignores unknown categories (giving -1)\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    X2_transformed = enc.transform(X2)\n    assert_array_equal(X2_transformed, [[0., 1.], [-1., 0.]])\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['a', 1], ['b', 2], ['c', 3]])\n    X_unknown = np.array([['d', 4]])\n\n    # Test that ordinal encoder raises error for unknown categories present during transform.\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    assert_raises(ValueError, enc.transform, X_unknown)\n\n    # Test the inverse_transform with unknown values\n    X_trans = enc.transform(X)\n    X_unknown_trans = np.array([[3, 4]])  # unknown categories\n    assert_raises(ValueError, enc.inverse_transform, X_unknown_trans)\n", "def test_ordinal_encoder_specified_categories():\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    categories = [['Female', 'Male'], [1, 2, 3]]\n    enc = OrdinalEncoder(categories=categories)\n    X_trans = enc.fit_transform(X)\n    exp = np.array([[1, 0], [0, 2], [0, 1]], dtype=np.float64)\n    assert_array_equal(X_trans, exp)\n    assert_array_equal(enc.categories_, categories)\n\n    # Test with unsorted categories\n    categories_unsorted = [['Male', 'Female'], [3, 1, 2]]\n    enc_unsorted = OrdinalEncoder(categories=categories_unsorted)\n    X_trans_unsorted = enc_unsorted.fit_transform(X)\n    exp_unsorted = np.array([[0, 1], [1, 0], [1, 2]], dtype=np.float64)\n    assert_array_equal(X_trans_unsorted, exp_unsorted)\n    assert_array_equal(enc_unsorted.categories_, [['Female', 'Male'], [1, 2, 3]])\n", "def test_ordinal_encoder_specified_categories():\n    X = [['cat1', 1], ['cat2', 2], ['cat1', 3]]\n    categories = [['cat1', 'cat2'], [1, 2, 3]]\n    enc = OrdinalEncoder(categories=categories)\n    X_trans = enc.fit_transform(X)\n    expected_trans = np.array([[0, 0], [1, 1], [0, 2]], dtype=np.float64)\n    assert_array_equal(X_trans, expected_trans)\n\n    # Test with unknown categories\n    X_unknown = [['cat3', 1], ['cat1', 4]]\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_unknown)\n\n    # Test with handle_unknown='ignore'\n    enc_ignore = OrdinalEncoder(categories=categories, handle_unknown='ignore')\n    X_trans_ignore = enc_ignore.fit_transform(X)\n    assert_array_equal(X_trans_ignore, expected_trans)\n\n    X_unknown_ignore = [['cat3', 1], ['cat1', 4]]\n    X_trans_unknown_ignore = enc_ignore.transform(X_unknown_ignore)\n    expected_unknown_ignore_trans = np.array([[np.nan, 0], [0, np.nan]], dtype=np.float64)\n    assert_array_equal(X_trans_unknown_ignore, expected_unknown_ignore_trans, equal_nan=True)\n"], "sample_713": ["def test_ridge_regression_return_n_iter():\n    # Test if ridge_regression correctly returns the number of iterations.\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    alpha = 1.0\n\n    # Test for solver 'lsqr' which returns number of iterations\n    coef, n_iter = ridge_regression(X, y, alpha, solver='lsqr', return_n_iter=True)\n    assert_true(isinstance(n_iter, np.ndarray))\n    assert_equal(len(n_iter), 1)  # single target\n\n    # Test for solver 'sag' which returns number of iterations\n    coef, n_iter = ridge_regression(X, y, alpha, solver='sag', return_n_iter=True, max_iter=100)\n    assert_true(isinstance(n_iter, np.ndarray))\n    assert_equal(len(n_iter), 1)  # single target\n\n    # Multi-target case\n    y_multi = np.vstack([y, y]).T\n    coef, n_iter = ridge_regression(X, y_multi, alpha, solver='lsqr', return_n_iter=True)\n    assert_true(isinstance(n_iter, np.ndarray))\n    assert_equal(len(n_iter), 2)  # two targets\n\n    coef, n_iter = ridge_regression(X, y_multi, alpha, solver='sag', return_n_iter=True, max_iter=100)\n    assert_true(isinstance(n_iter, np.ndarray))\n    assert_equal(len(n_iter), 2)  # two targets\n", "def test_ridge_regression_multiple_targets():\n    # Test ridge regression with multiple target variables\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_targets = 10, 5, 3\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    alpha = np.array([1.0, 0.5, 0.1])\n\n    solvers = [\"svd\", \"sparse_cg\", \"cholesky\", \"lsqr\", \"sag\", \"saga\"]\n    for solver in solvers:\n        ridge = Ridge(alpha=alpha, solver=solver)\n        ridge.fit(X, y)\n        coef = ridge.coef_\n        assert_equal(coef.shape, (n_targets, n_features))\n        assert_greater(ridge.score(X, y), 0.5)\n\n    # Check different alpha values per target\n    ridge = Ridge(alpha=alpha, solver=\"cholesky\")\n    ridge.fit(X, y)\n    coef = ridge.coef_\n    assert_equal(coef.shape, (n_targets, n_features))\n    assert_greater(ridge.score(X, y), 0.5)\n", "def test_ridge_regression_return_n_iter():\n    # Test that ridge_regression returns n_iter when return_n_iter is True\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 5)\n    y = rng.randn(100)\n    alpha = 1.0\n\n    for solver in [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]:\n        coef, n_iter = ridge_regression(X, y, alpha, solver=solver, return_n_iter=True)\n        assert_true(n_iter is not None)\n        assert_true(isinstance(n_iter, int) or isinstance(n_iter, np.ndarray))\n\n    # Test that n_iter is None when return_n_iter is False\n    for solver in [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]:\n        coef = ridge_regression(X, y, alpha, solver=solver, return_n_iter=False)\n        assert_true(isinstance(coef, np.ndarray))\n", "def test_ridge_regression_varied_alpha():\n    # Test ridge regression with different alpha values\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 100, 20\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    alphas = [0.1, 1.0, 10.0, 100.0]\n    \n    for alpha in alphas:\n        for solver in ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']:\n            ridge = Ridge(alpha=alpha, solver=solver)\n            ridge.fit(X, y)\n            assert_greater(ridge.score(X, y), 0.5, \n                           msg=\"Failed with alpha={}, solver={}\".format(alpha, solver))\n\n            # Check that the coefficients have the correct shape and dtype\n            assert_equal(ridge.coef_.shape, (n_features,))\n            assert_equal(ridge.coef_.dtype, X.dtype)\n            \n            # Ensure that the prediction is consistent with the shape of y\n            assert_equal(ridge.predict(X).shape, y.shape)\n", "def test_ridge_intercept_sparse():\n    # Test intercept handling for sparse input with different solvers\n    X, y = make_regression(n_samples=100, n_features=2, n_informative=2,\n                           bias=10.0, random_state=42)\n    X_csr = sp.csr_matrix(X)\n\n    for solver in ['saga', 'sag', 'sparse_cg', 'lsqr']:\n        ridge_dense = Ridge(alpha=1.0, solver=solver, fit_intercept=True)\n        ridge_sparse = Ridge(alpha=1.0, solver=solver, fit_intercept=True)\n\n        ridge_dense.fit(X, y)\n        ridge_sparse.fit(X_csr, y)\n\n        assert_almost_equal(ridge_dense.intercept_, ridge_sparse.intercept_)\n        assert_array_almost_equal(ridge_dense.coef_, ridge_sparse.coef_)\n\n        if solver == 'lsqr':\n            # Ensure solver switch warning for lsqr\n            ridge_sparse = Ridge(alpha=1.0, solver='lsqr', fit_intercept=True)\n            assert_warns(UserWarning, ridge_sparse.fit, X_csr, y)\n            assert_almost_equal(ridge_dense.intercept_, ridge_sparse.intercept_)\n            assert_array_almost_equal(ridge_dense.coef_, ridge_sparse.coef_)\n"], "sample_714": ["def test_f1_score_with_multilabel_indicator():\n    # Test f1_score with multilabel-indicator format\n    y_true = np.array([[1, 0, 0], [0, 1, 1], [1, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 0, 1], [1, 1, 1]])\n\n    # Check per class\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average=None)\n    assert_array_almost_equal(p, [1.0, 1.0, 0.66], 2)\n    assert_array_almost_equal(r, [1.0, 0.5, 1.0], 2)\n    assert_array_almost_equal(f, [1.0, 0.66, 0.8], 2)\n    assert_array_almost_equal(s, [2, 2, 2], 2)\n\n    # Check macro average\n    assert_almost_equal(f1_score(y_true, y_pred, average='macro'), 0.82, 2)\n\n    # Check micro average\n    assert_almost_equal(f1_score(y_true, y_pred, average='micro'), 0.83, 2)\n\n    # Check weighted average\n    assert_almost_equal(f1_score(y_true, y_pred, average='weighted'), 0.82, 2)\n\n    # Check samples average\n    assert_almost_equal(f1_score(y_true, y_pred, average='samples'), 0.88, 2)\n", "def test_balanced_accuracy_score():\n    # Test balanced_accuracy_score\n    # This function is particularly useful for imbalanced dataset\n    y_true = [0, 1, 0, 0, 1, 0]\n    y_pred = [0, 1, 0, 0, 0, 1]\n    \n    # True negatives: 3, False positives: 1\n    # True positives: 1, False negatives: 1\n    # Sensitivity = Recall = TP / (TP + FN) = 1 / (1 + 1) = 0.5\n    # Specificity = TN / (TN + FP) = 3 / (3 + 1) = 0.75\n    # Balanced Accuracy = (Sensitivity + Specificity) / 2 = (0.5 + 0.75) / 2 = 0.625\n\n    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(balanced_accuracy, 0.625)\n\n    # Check that the balanced accuracy score matches the recall_score with average='macro'\n    recall_macro = recall_score(y_true, y_pred, average='macro')\n    assert_almost_equal(balanced_accuracy, recall_macro)\n\n    # Check that the balanced accuracy score raises a ValueError for non-binary classification\n    assert_raises(ValueError, balanced_accuracy_score, [0, 1, 2], [0, 1, 2])\n", "def test_balanced_accuracy_score():\n    # Test balanced_accuracy_score in the binary case\n\n    # Perfect accuracy\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 1, 0, 1]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 1.0)\n\n    # Completely wrong predictions\n    y_pred = [1, 0, 1, 0]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.0)\n\n    # Half right predictions\n    y_pred = [0, 0, 1, 1]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.5)\n\n    # Imbalanced dataset\n    y_true = [0, 0, 0, 1]\n    y_pred = [0, 0, 1, 1]\n    # Sensitivity = 1/1 = 1.0, Specificity = 2/3 = 0.6667\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), (1.0 + 0.6667) / 2)\n\n    # Check with sample weights\n    sample_weight = [1, 1, 1, 2]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight), (1.0 + 0.6667) / 2)\n\n    # Check ValueError is raised for multiclass targets\n    y_true = [0, 1, 2, 2]\n    y_pred = [0, 2, 1, 2]\n    assert_raises(ValueError, balanced_accuracy_score, y_true, y_pred)\n", "def test_balanced_accuracy_score():\n    # Test balanced_accuracy_score for binary classification\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Calculate balanced accuracy manually\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    sensitivity = tp / (tp + fn)\n    specificity = tn / (tn + fp)\n    balanced_accuracy_manual = (sensitivity + specificity) / 2\n\n    # Calculate using balanced_accuracy_score function\n    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(balanced_accuracy, balanced_accuracy_manual, decimal=2)\n\n    # Test for perfect score\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 1, 0, 1]\n    assert_equal(balanced_accuracy_score(y_true, y_pred), 1.0)\n\n    # Test for worst score\n    y_true = [0, 1, 0, 1]\n    y_pred = [1, 0, 1, 0]\n    assert_equal(balanced_accuracy_score(y_true, y_pred), 0.0)\n\n    # Test invalid y_type\n    y_true = [0, 1, 2, 1]\n    y_pred = [0, 2, 1, 1]\n    assert_raises(ValueError, balanced_accuracy_score, y_true, y_pred)\n", "def test_balanced_accuracy_score():\n    # Test balanced accuracy score for binary classification\n    y_true = [0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1]\n    y_pred = [1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1]\n    \n    # Calculate sensitivity and specificity\n    sensitivity = recall_score(y_true, y_pred)\n    specificity = recall_score(y_true, y_pred, pos_label=0)\n    \n    # Balanced accuracy is the average of sensitivity and specificity\n    balanced_accuracy = (sensitivity + specificity) / 2\n    \n    # Use the balanced_accuracy_score function to compute the score\n    computed_balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n    \n    assert_almost_equal(computed_balanced_accuracy, balanced_accuracy, decimal=2)\n\n    # Test that it raises ValueError for non-binary classification problems\n    y_true_mc = [0, 1, 2, 0, 1, 2]\n    y_pred_mc = [0, 2, 1, 0, 0, 1]\n    assert_raises(ValueError, balanced_accuracy_score, y_true_mc, y_pred_mc)\n"], "sample_715": ["def test_cross_val_predict_with_groups():\n    # Test that cross_val_predict correctly handles the 'groups' parameter\n    X, y = make_classification(n_samples=30, random_state=0)\n    groups = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5,\n                       6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14])\n    clf = SVC(kernel=\"linear\")\n    \n    # Use LeaveOneGroupOut cross-validation strategy\n    cv = LeaveOneGroupOut()\n    \n    # Generate cross-validated predictions for each input data point\n    predictions = cross_val_predict(clf, X, y, cv=cv, groups=groups)\n    \n    # Ensure the length of predictions is the same as the length of y\n    assert_equal(len(predictions), len(y))\n    \n    # Check that predictions are not all the same (ensuring model was fit)\n    assert_greater(np.var(predictions), 0)\n    \n    # Test with method='predict_proba'\n    predictions_proba = cross_val_predict(clf, X, y, cv=cv, groups=groups, method='predict_proba')\n    assert_equal(predictions_proba.shape, (len(y), len(np.unique(y))))\n    \n    # Test with method='decision_function'\n    predictions_decision_function = cross_val_predict(clf, X, y, cv=cv, groups=groups, method='decision_function')\n    assert_equal(predictions_decision_function.shape[0], len(y))\n", "def test_cross_validate_with_groups():\n    # Ensure that cross_validate correctly handles the groups parameter\n    X, y = make_classification(n_samples=50, n_features=5, random_state=42)\n    groups = np.tile([1, 2, 3, 4, 5], 10)\n    estimator = LogisticRegression(random_state=42)\n    cv = GroupKFold(n_splits=5)\n    \n    # Use cross_validate with groups\n    scores = cross_validate(estimator, X, y, groups=groups, cv=cv, return_train_score=True)\n    \n    # Check if the keys are correct\n    expected_keys = {'fit_time', 'score_time', 'test_score', 'train_score'}\n    assert_true(expected_keys.issubset(set(scores.keys())))\n    \n    # Ensure that the correct number of scores are returned\n    assert_equal(len(scores['test_score']), 5)\n    assert_equal(len(scores['train_score']), 5)\n    \n    # Ensure that the scores are between 0 and 1\n    assert_true(np.all(scores['test_score'] >= 0) and np.all(scores['test_score'] <= 1))\n    assert_true(np.all(scores['train_score'] >= 0) and np.all(scores['train_score'] <= 1))\n", "def test_cross_val_predict_sparse_input():\n    # Check cross_val_predict with sparse input data\n    X, y = make_classification(n_samples=50, n_features=10, random_state=42)\n    X_sparse = csr_matrix(X)\n    \n    clf = SVC(kernel=\"linear\")\n    \n    # Ensure it works with default settings\n    preds_sparse = cross_val_predict(clf, X_sparse, y, cv=5)\n    clf.fit(X, y)\n    preds_dense = clf.predict(X)\n    assert_equal(preds_sparse.shape, y.shape)\n    assert_array_almost_equal(preds_sparse[:10], preds_dense[:10])\n\n    # Ensure it works with method='decision_function'\n    preds_sparse_decision = cross_val_predict(clf, X_sparse, y, cv=5, method='decision_function')\n    preds_dense_decision = clf.decision_function(X)\n    assert_equal(preds_sparse_decision.shape, preds_dense_decision.shape)\n    assert_array_almost_equal(preds_sparse_decision[:10], preds_dense_decision[:10])\n\n    # Ensure it works with method='predict_proba'\n    clf_proba = LogisticRegression(random_state=42)\n    preds_sparse_proba = cross_val_predict(clf_proba, X_sparse, y, cv=5, method='predict_proba')\n    clf_proba.fit(X, y)\n    preds_dense_proba = clf_proba.predict_proba(X)\n    assert_equal(preds_sparse_proba.shape, preds_dense_proba.shape)\n    assert_array_almost_equal(preds_sparse_proba[:10], preds_dense_proba[:10])\n\n    # Ensure it works with method='predict_log_proba'\n    preds_sparse_log_proba = cross_val_predict(clf_proba, X_sparse, y, cv=5, method='predict_log_proba')\n    preds_dense_log_proba = clf_proba.predict_log_proba(X)\n    assert_equal(preds_sparse_log_proba.shape, preds_dense_log_proba.shape)\n    assert_array_almost_equal(preds_sparse_log_proba[:10], preds_dense_log_proba[:10])\n", "def test_cross_validate_with_fit_params():\n    # Test cross_validate with fit_params\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = MockClassifier()\n    n_samples = X.shape[0]\n    n_classes = len(np.unique(y))\n\n    fit_params = {'sample_weight': np.ones(n_samples),\n                  'class_prior': np.ones(n_classes) / n_classes}\n\n    cv_results = cross_validate(clf, X, y, cv=3, fit_params=fit_params)\n    \n    assert 'fit_time' in cv_results\n    assert 'score_time' in cv_results\n    assert 'test_score' in cv_results\n    assert 'train_score' in cv_results  # Since return_train_score is True by default\n\n    # Ensure sample_weight is passed to fit method\n    assert_almost_equal(cv_results['test_score'], clf.score(X, y))\n    assert_almost_equal(cv_results['train_score'], clf.score(X, y))\n\n    # Ensure fit_params are passed correctly\n    clf_ = clone(clf)\n    clf_.fit(X, y, **fit_params)\n    assert_array_equal(clf_.sample_weight, fit_params['sample_weight'])\n    assert_array_equal(clf_.class_prior, fit_params['class_prior'])\n", "def test_cross_val_predict_with_groups():\n    # Ensure groups are correctly handled in cross_val_predict\n    X, y = make_classification(n_samples=20, n_classes=2, random_state=0)\n    groups = np.array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4])\n    \n    clf = SVC(kernel=\"linear\")\n    group_cvs = [LeaveOneGroupOut(), GroupKFold(), GroupShuffleSplit()]\n    \n    for cv in group_cvs:\n        preds = cross_val_predict(clf, X, y, groups=groups, cv=cv)\n        assert_equal(len(preds), len(y))\n        # Ensure that the cross_val_predict raises an error when groups is None\n        assert_raises(ValueError, cross_val_predict, clf, X, y, cv=cv)\n"], "sample_716": ["def test_ridge_regression_solver_auto():\n    # Test the 'auto' solver selection in ridge_regression\n\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n    alpha = 1.0\n\n    # Ensure the 'auto' solver picks 'cholesky' for dense data\n    coef_auto = ridge_regression(X, y, alpha=alpha, solver='auto')\n    coef_cholesky = ridge_regression(X, y, alpha=alpha, solver='cholesky')\n    assert_array_almost_equal(coef_auto, coef_cholesky)\n\n    # Ensure the 'auto' solver picks 'sparse_cg' for sparse data\n    X_sparse = sp.csr_matrix(X)\n    coef_auto_sparse = ridge_regression(X_sparse, y, alpha=alpha, solver='auto')\n    coef_sparse_cg = ridge_regression(X_sparse, y, alpha=alpha, solver='sparse_cg')\n    assert_array_almost_equal(coef_auto_sparse, coef_sparse_cg)\n", "def test_ridge_regression_solver_auto():\n    # Test that the 'auto' solver chooses the correct solver based on input\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    y = rng.randn(n_samples)\n    X_dense = rng.randn(n_samples, n_features)\n    X_sparse = sp.csr_matrix(X_dense)\n\n    # Test with dense data\n    ridge_dense = Ridge(alpha=1.0, solver='auto')\n    ridge_dense.fit(X_dense, y)\n    assert ridge_dense.solver == 'cholesky'\n\n    # Test with sparse data\n    ridge_sparse = Ridge(alpha=1.0, solver='auto')\n    ridge_sparse.fit(X_sparse, y)\n    assert ridge_sparse.solver == 'sparse_cg'\n\n    # Test with sample weights\n    sample_weight = np.ones(n_samples)\n    ridge_dense_sw = Ridge(alpha=1.0, solver='auto')\n    ridge_dense_sw.fit(X_dense, y, sample_weight=sample_weight)\n    assert ridge_dense_sw.solver == 'cholesky'\n\n    ridge_sparse_sw = Ridge(alpha=1.0, solver='auto')\n    ridge_sparse_sw.fit(X_sparse, y, sample_weight=sample_weight)\n    assert ridge_sparse_sw.solver == 'sparse_cg'\n", "def test_ridge_regression_intercept_handling():\n    # Test that ridge regression correctly handles intercept fitting\n\n    # Create a dataset with a non-zero intercept\n    X = np.array([[1], [2], [3], [4], [5]])\n    y = 3 * X.squeeze() + 5\n\n    # Test with fit_intercept=True\n    ridge = Ridge(alpha=0.1, fit_intercept=True, solver='auto')\n    ridge.fit(X, y)\n    assert_almost_equal(ridge.intercept_, 5.0, decimal=1)\n    assert_almost_equal(ridge.coef_, [3.0], decimal=1)\n\n    # Test with fit_intercept=False\n    ridge = Ridge(alpha=0.1, fit_intercept=False, solver='auto')\n    ridge.fit(X, y)\n    assert_almost_equal(ridge.intercept_, 0.0)\n    assert_almost_equal(ridge.coef_, [3.0], decimal=1)\n\n    # Test with sparse input\n    X_sparse = sp.csr_matrix(X)\n    ridge = Ridge(alpha=0.1, fit_intercept=True, solver='auto')\n    ridge.fit(X_sparse, y)\n    assert_almost_equal(ridge.intercept_, 5.0, decimal=1)\n    assert_almost_equal(ridge.coef_, [3.0], decimal=1)\n\n    # Test with sparse input and fit_intercept=False\n    ridge = Ridge(alpha=0.1, fit_intercept=False, solver='auto')\n    ridge.fit(X_sparse, y)\n    assert_almost_equal(ridge.intercept_, 0.0)\n    assert_almost_equal(ridge.coef_, [3.0], decimal=1)\n", "def test_ridge_solver_saga():\n    # Test the SAGA solver in Ridge with both dense and sparse data\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    for fit_intercept in [True, False]:\n        # With more samples than features\n        n_samples, n_features = 6, 5\n        y = rng.randn(n_samples)\n        X = rng.randn(n_samples, n_features)\n\n        ridge = Ridge(alpha=alpha, solver='saga', fit_intercept=fit_intercept)\n        ridge.fit(X, y)\n        assert_equal(ridge.coef_.shape, (X.shape[1],))\n        assert_greater(ridge.score(X, y), 0.47)\n\n        if fit_intercept:\n            ridge_sparse = Ridge(alpha=alpha, solver='saga', fit_intercept=fit_intercept)\n            ridge_sparse.fit(sp.csr_matrix(X), y)\n            assert_almost_equal(ridge.intercept_, ridge_sparse.intercept_)\n            assert_array_almost_equal(ridge.coef_, ridge_sparse.coef_)\n        \n        # With more features than samples\n        n_samples, n_features = 5, 10\n        y = rng.randn(n_samples)\n        X = rng.randn(n_samples, n_features)\n        ridge = Ridge(alpha=alpha, solver='saga', fit_intercept=fit_intercept)\n        ridge.fit(X, y)\n        assert_greater(ridge.score(X, y), .9)\n\n        if fit_intercept:\n            ridge_sparse = Ridge(alpha=alpha, solver='saga', fit_intercept=fit_intercept)\n            ridge_sparse.fit(sp.csr_matrix(X), y)\n            assert_almost_equal(ridge.intercept_, ridge_sparse.intercept_)\n            assert_array_almost_equal(ridge.coef_, ridge_sparse.coef_)\n", "def test_ridge_solver_switch_warning():\n    # Test whether a warning is raised when switching solvers automatically\n\n    X, y = make_regression(n_samples=100, n_features=20, random_state=42)\n    X_sparse = sp.csr_matrix(X)\n\n    dense_ridge = Ridge(alpha=1., tol=1.e-15, solver='sag', fit_intercept=False)\n    sparse_ridge = Ridge(alpha=1., tol=1.e-15, solver='auto', fit_intercept=True)\n\n    # Fit with dense data and check no warnings are raised\n    with warnings.catch_warnings(record=True) as w:\n        dense_ridge.fit(X, y)\n        assert len(w) == 0\n\n    # Fit with sparse data and check warning for solver switch\n    with warnings.catch_warnings(record=True) as w:\n        sparse_ridge.fit(X_sparse, y)\n        assert len(w) == 1\n        assert issubclass(w[0].category, UserWarning)\n        assert \"Solver has been automatically changed to 'sag'\" in str(w[0].message)\n"], "sample_717": ["def test_fetch_lfw_people_default_params():\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,\n                                  download_if_missing=False)\n    \n    # Check the default images shape with default slice and resize\n    assert_equal(lfw_people.images.shape, (17, 62, 47))\n    assert_equal(lfw_people.data.shape, (17, 2914))\n    \n    # Check if the target is correctly formed\n    assert_array_equal(lfw_people.target,\n                       [0, 0, 1, 6, 5, 6, 3, 6, 0, 3, 6, 1, 2, 4, 5, 1, 2])\n    \n    # Check if the target names are correct\n    expected_classes = ['Abdelatif Smith', 'Abhati Kepler', 'Camara Alvaro',\n                        'Chen Dupont', 'John Lee', 'Lin Bauman', 'Onur Lopez']\n    assert_array_equal(lfw_people.target_names, expected_classes)\n    \n    # Check if DESCR is non-empty\n    assert lfw_people.DESCR\n", "def test_fetch_lfw_people_resize_and_slice():\n    # Test fetch_lfw_people with different resize and slice parameters\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,\n                                  resize=0.3,\n                                  slice_=(slice(50, 200), slice(60, 180)),\n                                  download_if_missing=False)\n\n    # Ensure the shape of the images and data matches the expected resized and sliced dimensions\n    assert_equal(lfw_people.images.shape, (10, 45, 36))\n    assert_equal(lfw_people.data.shape, (10, 1620))\n\n    # The target and target names should remain the same\n    assert_array_equal(lfw_people.target, [2, 0, 1, 0, 2, 0, 2, 1, 1, 2])\n    expected_classes = ['Abdelatif Smith', 'Abhati Kepler', 'Onur Lopez']\n    assert_array_equal(lfw_people.target_names, expected_classes)\n", "def test_load_lfw_pairs_10_folds():\n    lfw_pairs_10_folds = fetch_lfw_pairs(subset='10_folds', data_home=SCIKIT_LEARN_DATA,\n                                         download_if_missing=False)\n\n    # The data is cropped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs_10_folds.pairs.shape, (10, 2, 62, 47))\n\n    # the target is whether the person is the same or not\n    assert_array_equal(lfw_pairs_10_folds.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # names of the persons can be found using the target_names array\n    expected_classes = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs_10_folds.target_names, expected_classes)\n\n    # It is possible to ask for the original data without any cropping or color\n    # conversion\n    lfw_pairs_10_folds = fetch_lfw_pairs(subset='10_folds', data_home=SCIKIT_LEARN_DATA, resize=None,\n                                         slice_=None, color=True, download_if_missing=False)\n    assert_equal(lfw_pairs_10_folds.pairs.shape, (10, 2, 250, 250, 3))\n\n    # the ids and class names are the same as previously\n    assert_array_equal(lfw_pairs_10_folds.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n    assert_array_equal(lfw_pairs_10_folds.target_names, expected_classes)\n", "def test_fetch_lfw_people_without_funneled():\n    # Test fetching LFW people data without using the funneled version\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, funneled=False,\n                                  min_faces_per_person=3, download_if_missing=False)\n\n    # Verify the shape of the images and data arrays\n    assert_equal(lfw_people.images.shape, (10, 62, 47))\n    assert_equal(lfw_people.data.shape, (10, 2914))\n\n    # Verify the target labels and target names\n    assert_array_equal(lfw_people.target, [2, 0, 1, 0, 2, 0, 2, 1, 1, 2])\n    expected_classes = ['Abdelatif Smith', 'Abhati Kepler', 'Onur Lopez']\n    assert_array_equal(lfw_people.target_names, expected_classes)\n", "def test_scale_face():\n    # Test with a known face array\n    face = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=np.float32)\n    scaled_face = scale_face(face)\n    \n    # The minimum value in the array should now be 0.0\n    assert_equal(scaled_face.min(), 0.0)\n    \n    # The maximum value in the array should now be 1.0\n    assert_equal(scaled_face.max(), 1.0)\n    \n    # Ensure that the scaled values are correct\n    expected_scaled_face = np.array([[0.0, 1.0/8.0, 2.0/8.0], [3.0/8.0, 4.0/8.0, 5.0/8.0], [6.0/8.0, 7.0/8.0, 1.0]], dtype=np.float32)\n    assert_array_equal(scaled_face, expected_scaled_face)\n"], "sample_718": ["def test_check_classifiers_classes():\n    class SingleClassClassifier(BaseBadClassifier):\n            X, y = check_X_y(X, y)\n            self.classes_ = np.unique(y)\n            return self\n\n    # Check that classifier doesn't throw error on single class during fit\n    clf = SingleClassClassifier()\n    X = np.array([[0], [1], [2], [3], [4]])\n    y_single_class = np.array([1, 1, 1, 1, 1])\n    try:\n        clf.fit(X, y_single_class)\n    except ValueError as e:\n        assert False, f\"SingleClassClassifier raised an error: {e}\"\n\n    # Check that classifier correctly identifies single class\n    assert clf.classes_ == [1], f\"Expected classes to be [1], got {clf.classes_}\"\n\n    # Check that classifier throws an error when fitted with multiple classes\n    y_multi_class = np.array([1, 2, 1, 2, 1])\n    with assert_raises_regex(ValueError, \"Expected single class\"):\n        clf.fit(X, y_multi_class)\n\n    check_classifiers_classes(\"SingleClassClassifier\", SingleClassClassifier())\n", "def test_check_estimator_sparse_data():\n    # check that the estimator correctly handles sparse data\n    class SparseEstimator(BaseBadClassifier):\n            X, y = check_X_y(X, y, accept_sparse=['csr', 'csc'])\n            return self\n\n    check_estimator(SparseEstimator)\n\n    # test that fitting a sparse matrix raises the appropriate errors if not supported\n    class NoSparseHandlingEstimator(BaseBadClassifier):\n            X, y = check_X_y(X, y, accept_sparse=False)\n            return self\n\n    msg = \"Estimator NoSparseHandlingEstimator doesn't seem to fail gracefully on sparse data\"\n    old_stdout = sys.stdout\n    string_buffer = StringIO()\n    sys.stdout = string_buffer\n    try:\n        check_estimator(NoSparseHandlingEstimator)\n    except:\n        pass\n    finally:\n        sys.stdout = old_stdout\n    assert_true(msg in string_buffer.getvalue())\n", "def test_check_estimator_sparse_data():\n    class SparseEstimator(BaseEstimator, ClassifierMixin):\n            X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'])\n            self.coef_ = np.ones(X.shape[1])\n            return self\n\n            X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n            return np.ones(X.shape[0])\n\n    est = SparseEstimator()\n    check_estimator(est)\n", "def test_check_transformer_general():\n    # Check that check_transformer_general works as expected for transformers\n    from sklearn.preprocessing import StandardScaler, PowerTransformer\n\n    # StandardScaler is a typical transformer\n    check_transformer_general(StandardScaler.__name__, StandardScaler())\n\n    # PowerTransformer has additional constraints on the input\n    check_transformer_general(PowerTransformer.__name__, PowerTransformer())\n", "def test_check_estimators_dtypes():\n    class DummyEstimator(BaseEstimator):\n            assert_true(X.dtype in [np.float32, np.float64, np.int32, np.int64],\n                        \"X should have a valid dtype. Got %s instead.\" % X.dtype)\n            return self\n\n            assert_true(X.dtype in [np.float32, np.float64, np.int32, np.int64],\n                        \"X should have a valid dtype. Got %s instead.\" % X.dtype)\n            return np.ones(X.shape[0])\n\n    check_estimators_dtypes(\"DummyEstimator\", DummyEstimator())\n"], "sample_719": ["def test_vectorizer_empty_documents():\n    # Test vectorizers with empty documents\n    empty_docs = [\"\", \"\", \"\"]\n    for Vectorizer in (CountVectorizer, TfidfVectorizer, HashingVectorizer):\n        vect = Vectorizer()\n        X = vect.fit_transform(empty_docs)\n        assert_equal(X.shape[0], len(empty_docs))\n        assert_equal(X.shape[1], 0)\n        assert_equal(X.nnz, 0)\n\n        # ensure inverse_transform works with empty docs\n        inversed_docs = vect.inverse_transform(X)\n        assert_equal(len(inversed_docs), len(empty_docs))\n        for doc in inversed_docs:\n            assert_equal(doc, [])\n", "def test_vectorizer_empty_string_input():\n    # Test how vectorizers handle empty string input\n    empty_docs = [\"\", \" \", \"   \"]\n\n    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        X = vec.fit_transform(empty_docs)\n        assert_equal(X.shape[0], len(empty_docs))\n        assert_equal(X.nnz, 0)  # No non-zero entries should be in the matrix\n", "def test_vectorizer_strip_accents():\n    # Test if strip_accents parameter works for different options\n    text = \"\u00c0\u00c1\u00c2\u00c3\u00c4\u00c5\u00e7\u00c7\u00e8\u00e9\u00ea\u00eb\u00cc\u00cd\u00ce\u00cf\u00d1\u00f2\u00f3\u00f4\u00f5\u00f6\u00d9\u00da\u00db\u00dc\u00fd\"\n    expected_ascii = \"AAAAAAcCeeeeIIIINooooUUUUy\"\n    expected_unicode = \"AAAAAA\u00e7\u00c7eeee\u00cc\u00cd\u00ce\u00cf\u00d1oooo\u00d9\u00da\u00db\u00dc\u00fd\"\n    \n    # Test with strip_accents='ascii'\n    vect = CountVectorizer(strip_accents='ascii', analyzer='char')\n    X = vect.fit_transform([text])\n    assert_equal(''.join(vect.get_feature_names()), expected_ascii)\n    \n    # Test with strip_accents='unicode'\n    vect = CountVectorizer(strip_accents='unicode', analyzer='char')\n    X = vect.fit_transform([text])\n    assert_equal(''.join(vect.get_feature_names()), expected_unicode)\n    \n    # Test with strip_accents=None\n    vect = CountVectorizer(strip_accents=None, analyzer='char')\n    X = vect.fit_transform([text])\n    assert_equal(''.join(vect.get_feature_names()), text)\n", "def test_strip_tags():\n    html_string = \"<div>This is <b>bold</b> and this is <i>italic</i></div>\"\n    expected = \"This is bold and this is italic\"\n    assert_equal(strip_tags(html_string), expected)\n\n    html_string = \"<p>Paragraph with <a href='http://example.com'>link</a>.</p>\"\n    expected = \"Paragraph with link .\"\n    assert_equal(strip_tags(html_string), expected)\n\n    html_string = \"<html><body><h1>Title</h1><p>Content here</p></body></html>\"\n    expected = \"Title Content here\"\n    assert_equal(strip_tags(html_string), expected)\n\n    # Test with nested tags\n    html_string = \"<div><p><span>Nested <b>bold</b> text</span></p></div>\"\n    expected = \"Nested bold text\"\n    assert_equal(strip_tags(html_string), expected)\n", "def test_strip_tags():\n    # HTML tags stripping\n    html = \"<b>Bold</b> text and <a href='http://example.com'>link</a>\"\n    expected = \"Bold text and link\"\n    assert_equal(strip_tags(html), expected)\n\n    # XML tags stripping\n    xml = \"<root><child>Content</child></root>\"\n    expected = \" Content \"\n    assert_equal(strip_tags(xml), expected)\n\n    # mixed content\n    mixed = \"Text with <b>bold</b> and <i>italic</i> tags.\"\n    expected = \"Text with bold and italic tags.\"\n    assert_equal(strip_tags(mixed), expected)\n"], "sample_720": ["def test_handle_zeros_in_scale_scalar():\n    # Test _handle_zeros_in_scale with scalar input\n    scaler = 0.0\n    scaler_out = _handle_zeros_in_scale(scaler, copy=False)\n    assert_equal(scaler_out, 1.0)\n\n    scaler = 5.0\n    scaler_out = _handle_zeros_in_scale(scaler, copy=False)\n    assert_equal(scaler_out, 5.0)\n", "def test_handle_zeros_in_scale_1d_array():\n    # Test _handle_zeros_in_scale with 1D array\n    scale = np.array([0., 1., 2., 0.])\n    scale_corrected = _handle_zeros_in_scale(scale)\n    expected = np.array([1., 1., 2., 1.])\n    assert_array_equal(scale_corrected, expected)\n\n    # Test _handle_zeros_in_scale with 1D array and copy=False\n    scale = np.array([0., 1., 2., 0.])\n    scale_corrected = _handle_zeros_in_scale(scale, copy=False)\n    expected = np.array([1., 1., 2., 1.])\n    assert_array_equal(scale_corrected, expected)\n", "def test_handle_zeros_in_scale_scalar():\n    # Test the _handle_zeros_in_scale function with scalar input\n    scale = 0.0\n    result = _handle_zeros_in_scale(scale)\n    assert_equal(result, 1.0)\n", "def test_inverse_transform_no_scaling():\n    X = np.abs(X_2d)[:, 0:1]\n\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    X_trans = pt.fit_transform(X)\n\n    # Manually compute the inverse transformation\n    lmbda = pt.lambdas_[0]\n    X_manual_inverse = np.exp(X_trans) if lmbda == 0 else (X_trans * lmbda + 1) ** (1 / lmbda)\n\n    assert_array_almost_equal(pt.inverse_transform(X_trans), X_manual_inverse)\n    assert_array_almost_equal(pt.inverse_transform(X_trans), X)\n", "def test_handle_zeros_in_scale_with_scalar():\n    # Test _handle_zeros_in_scale with scalar input\n    scalar_scale = 0.0\n    result = _handle_zeros_in_scale(scalar_scale)\n    assert_equal(result, 1.0, \"Expected scalar scale to be converted to 1.0\")\n\n    scalar_scale = 2.0\n    result = _handle_zeros_in_scale(scalar_scale)\n    assert_equal(result, 2.0, \"Expected scalar scale to remain 2.0\")\n"], "sample_721": ["def test_check_is_fitted_custom_msg():\n    # Test check_is_fitted with custom messages\n    class EstimatorWithCustomMsg:\n            self.coef_ = None\n\n            self.coef_ = np.zeros(X.shape[1])\n\n    est = EstimatorWithCustomMsg()\n\n    # Custom message without %(name)s placeholder\n    custom_msg = \"Custom error message.\"\n    try:\n        check_is_fitted(est, \"coef_\", msg=custom_msg)\n    except NotFittedError as e:\n        assert_equal(str(e), custom_msg)\n\n    # Custom message with %(name)s placeholder\n    custom_msg_with_name = \"Custom error message for %(name)s.\"\n    try:\n        check_is_fitted(est, \"coef_\", msg=custom_msg_with_name)\n    except NotFittedError as e:\n        assert_equal(str(e), \"Custom error message for EstimatorWithCustomMsg.\")\n    \n    # Ensure no error is raised when attribute is present\n    est.fit(np.random.rand(10, 2), np.random.rand(10))\n    assert_equal(None, check_is_fitted(est, \"coef_\", msg=custom_msg_with_name))\n", "def test_check_is_fitted_custom_message():\n    # Test custom error message in check_is_fitted\n    class MockEstimator:\n            self.attr = None\n\n    estimator = MockEstimator()\n    message = \"Custom error message for %(name)s\"\n    with pytest.raises(NotFittedError, match=message % {'name': 'MockEstimator'}):\n        check_is_fitted(estimator, \"attr\", msg=message)\n\n    # Test with multiple attributes and custom message\n    estimator = MockEstimator()\n    message = \"Multiple attributes check for %(name)s\"\n    with pytest.raises(NotFittedError, match=message % {'name': 'MockEstimator'}):\n        check_is_fitted(estimator, [\"attr\", \"another_attr\"], msg=message)\n\n    # Test with all_or_any parameter set to any\n    estimator.attr = \"value\"\n    with pytest.raises(NotFittedError, match=message % {'name': 'MockEstimator'}):\n        check_is_fitted(estimator, [\"attr\", \"another_attr\"], msg=message, all_or_any=any)\n", "def test_check_random_state():\n    # Test with None\n    random_state = check_random_state(None)\n    assert_true(isinstance(random_state, np.random.RandomState))\n\n    # Test with an integer\n    random_state = check_random_state(42)\n    assert_true(isinstance(random_state, np.random.RandomState))\n    assert_equal(random_state.randint(100), np.random.RandomState(42).randint(100))\n\n    # Test with an existing RandomState\n    rng = np.random.RandomState(42)\n    random_state = check_random_state(rng)\n    assert_true(random_state is rng)\n\n    # Test with an invalid input\n    assert_raises(ValueError, check_random_state, \"invalid_seed\")\n", "def test_check_non_negative():\n    X_positive = np.array([[1, 2], [3, 4]])\n    X_negative = np.array([[1, -2], [3, 4]])\n    X_mixed_sparse = sp.csr_matrix([[1, 2], [-3, 4]])\n    \n    # Check that no exception is raised for non-negative arrays\n    check_non_negative(X_positive, \"test\")\n    check_non_negative(sp.csr_matrix(X_positive), \"test\")\n    \n    # Check that ValueError is raised for arrays with negative values\n    assert_raises(ValueError, check_non_negative, X_negative, \"test\")\n    assert_raises(ValueError, check_non_negative, X_mixed_sparse, \"test\")\n", "def test_check_no_negative():\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert_no_warnings(check_non_negative, X, \"test\")\n\n    X_with_negative = np.array([[1, -2, 3], [4, 5, -6], [7, 8, 9]])\n    assert_raises_regex(ValueError, \"Negative values in data passed to test\",\n                        check_non_negative, X_with_negative, \"test\")\n\n    X_sparse = sp.csr_matrix([[1, 0, 3], [0, 5, 6], [7, 8, 0]])\n    assert_no_warnings(check_non_negative, X_sparse, \"test\")\n\n    X_sparse_with_negative = sp.csr_matrix([[1, -2, 0], [4, 0, 6], [0, 8, 9]])\n    assert_raises_regex(ValueError, \"Negative values in data passed to test\",\n                        check_non_negative, X_sparse_with_negative, \"test\")\n"], "sample_722": ["def test_k_means_custom_callable_init():\n        return X[:n_clusters]\n\n    km = KMeans(init=custom_init, n_clusters=n_clusters, random_state=42)\n    km.fit(X)\n    _check_fitted_model(km)\n\n    mb_k_means = MiniBatchKMeans(init=custom_init, n_clusters=n_clusters,\n                                 random_state=42)\n    mb_k_means.fit(X)\n    _check_fitted_model(mb_k_means)\n", "def test_k_means_max_iter():\n    # Test that KMeans stops after max_iter iterations even if not converged\n    max_iter = 2\n    km = KMeans(n_clusters=n_clusters, max_iter=max_iter, random_state=42)\n    km.fit(X)\n    assert_equal(km.n_iter_, max_iter)\n\n    # Similar test for MiniBatchKMeans\n    mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, max_iter=max_iter, random_state=42)\n    mb_k_means.fit(X)\n    assert_equal(mb_k_means.n_iter_, max_iter * int(np.ceil(float(n_samples) / mb_k_means.batch_size)))\n", "def test_k_means_invalid_n_clusters():\n    # Check that an error is raised if n_clusters is not a positive integer\n    invalid_n_clusters = [-1, 0, 3.5, \"three\", None]\n    for n_clusters in invalid_n_clusters:\n        km = KMeans(n_clusters=n_clusters, random_state=42)\n        assert_raises(ValueError, km.fit, X)\n\n    # Check MiniBatchKMeans with invalid n_clusters\n    for n_clusters in invalid_n_clusters:\n        mb_km = MiniBatchKMeans(n_clusters=n_clusters, random_state=42)\n        assert_raises(ValueError, mb_km.fit, X)\n", "def test_predict_before_fit():\n    km = KMeans(n_clusters=n_clusters, random_state=42)\n    assert_raises(NotFittedError, km.predict, X)\n", "def test_kmeans_convergence_warning():\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])  # simple dataset with distinct points\n    n_clusters = 5  # more clusters than points\n    km = KMeans(n_clusters=n_clusters, random_state=42)\n\n    # fitting should trigger a ConvergenceWarning\n    msg = \"Number of distinct clusters (4) found smaller than n_clusters (5).\"\n    assert_warns_message(ConvergenceWarning, msg, km.fit, X)\n"], "sample_723": ["def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy.\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9]\n    ])\n    \n    invalid_strategy = \"invalid_strategy\"\n    \n    with assert_raises(ValueError, match=\"Can only use these strategies:\"):\n        SimpleImputer(strategy=invalid_strategy).fit(X)\n\n", "def test_imputation_with_nan_values():\n    # Test imputation when missing_values is np.nan.\n    X = np.array([\n        [np.nan, 1, 2, np.nan],\n        [3, np.nan, np.nan, 0],\n        [1, 2, 3, 4],\n        [np.nan, np.nan, 5, 6]\n    ])\n\n    # Expected results\n    X_imputed_mean = np.array([\n        [2, 1, 2, 3],\n        [3, 1.5, 3.33333333, 0],\n        [1, 2, 3, 4],\n        [2, 1.5, 5, 6],\n    ])\n    statistics_mean = [2, 1.5, 3.33333333, 3]\n\n    X_imputed_median = np.array([\n        [2, 1, 2, 4],\n        [3, 2, 3, 0],\n        [1, 2, 3, 4],\n        [2, 2, 5, 6],\n    ])\n    statistics_median = [2, 2, 3, 4]\n\n    X_imputed_most_frequent = np.array([\n        [1, 1, 2, 4],\n        [3, 1, 2, 0],\n        [1, 2, 3, 4],\n        [1, 1, 5, 6],\n    ])\n    statistics_most_frequent = [1, 1, 2, 4]\n\n    _check_statistics(X, X_imputed_mean, \"mean\", statistics_mean, np.nan)\n    _check_statistics(X, X_imputed_median, \"median\", statistics_median, np.nan)\n    _check_statistics(X, X_imputed_most_frequent, \"most_frequent\", statistics_most_frequent, np.nan)\n", "def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9],\n    ])\n\n    invalid_strategy = 'invalid_strategy'\n    imputer = SimpleImputer(strategy=invalid_strategy)\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_invalid_strategy():\n    # Test that an invalid strategy raises a ValueError.\n    X = np.random.randn(10, 2)\n    X[::2] = np.nan\n\n    invalid_strategies = ['meanmode', 'average', 'invalid']\n\n    for strategy in invalid_strategies:\n        imputer = SimpleImputer(strategy=strategy)\n        assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_invalid_strategy():\n    # Test invalid strategy raises a ValueError\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9]\n    ])\n    \n    invalid_strategy = \"invalid_strategy\"\n    imputer = SimpleImputer(strategy=invalid_strategy)\n    assert_raises(ValueError, imputer.fit, X)\n"], "sample_724": ["def test_imputation_invalid_parameters():\n    # Test imputation with invalid parameters to check error handling\n    X = np.array([[1, 2], [3, 4]])\n\n    # Test invalid strategy\n    assert_raises(ValueError, Imputer, strategy=\"invalid_strategy\")\n\n    # Test invalid axis\n    assert_raises(ValueError, Imputer, axis=2)\n\n    # Test invalid missing_values\n    imputer = Imputer(missing_values=\"invalid_missing_values\")\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Test missing_values that can't be found in the data\n    imputer = Imputer(missing_values=999, strategy=\"mean\")\n    X_trans = imputer.fit_transform(X)\n    assert_array_equal(X, X_trans)\n\n    # Test non-numeric data type\n    X_non_numeric = np.array([['a', 'b'], ['c', 'd']])\n    imputer = Imputer(strategy=\"mean\")\n    assert_raises(ValueError, imputer.fit, X_non_numeric)\n", "def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy\n    X = np.array([[1, 2], [3, 4]])\n\n    for strategy in [\"invalid\", 123, None]:\n        imputer = Imputer(strategy=strategy)\n        assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy\n    X = np.array([\n        [np.nan, 1, 2],\n        [3, np.nan, 5],\n        [6, 7, np.nan]\n    ])\n    \n    invalid_strategy = \"invalid_strategy\"\n    \n    # Expect ValueError to be raised with invalid strategy\n    imputer = Imputer(strategy=invalid_strategy)\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy\n    X = np.array([\n        [np.nan, 1, 2],\n        [3, np.nan, 4],\n        [5, 6, np.nan]\n    ])\n\n    invalid_strategies = [\"invalid\", \"average\", \"mode\"]\n    for strategy in invalid_strategies:\n        imputer = Imputer(strategy=strategy)\n        assert_raises(ValueError, imputer.fit, X)\n\n", "def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy.\n    X = np.array([\n        [np.nan, 1, 2],\n        [3, np.nan, 0],\n        [5, 6, np.nan]\n    ])\n\n    invalid_strategy = \"invalid_strategy\"\n    imputer = Imputer(strategy=invalid_strategy)\n    assert_raises(ValueError, imputer.fit, X)\n"], "sample_725": ["def test_check_non_negative():\n    # Test check_non_negative with dense array\n    X = np.array([1, 2, 3])\n    assert_no_warnings(check_non_negative, X, \"test\")\n\n    # Test check_non_negative with sparse matrix\n    X_sparse = sp.csr_matrix([1, 2, 3])\n    assert_no_warnings(check_non_negative, X_sparse, \"test\")\n\n    # Test check_non_negative with array containing negative values\n    X_neg = np.array([1, -2, 3])\n    assert_raises(ValueError, check_non_negative, X_neg, \"test\")\n\n    # Test check_non_negative with sparse matrix containing negative values\n    X_sparse_neg = sp.csr_matrix([1, -2, 3])\n    assert_raises(ValueError, check_non_negative, X_sparse_neg, \"test\")\n", "def test_check_random_state():\n    # Test check_random_state function\n\n    # Test with None, should return the global RandomState instance\n    rs = check_random_state(None)\n    assert_true(isinstance(rs, np.random.RandomState))\n    \n    # Test with an integer, should return a new RandomState instance\n    rs = check_random_state(42)\n    assert_true(isinstance(rs, np.random.RandomState))\n    assert_equal(rs.randint(100), np.random.RandomState(42).randint(100))\n    \n    # Test with a RandomState instance, should return the same instance\n    rng = np.random.RandomState(42)\n    rs = check_random_state(rng)\n    assert_true(rs is rng)\n    \n    # Test with an invalid input, should raise ValueError\n    assert_raises(ValueError, check_random_state, \"invalid_seed\")\n", "def test_column_or_1d():\n    # Test function for column_or_1d\n    # 1D array should stay the same\n    y = np.array([1, 2, 3, 4])\n    assert_array_equal(column_or_1d(y), y)\n\n    # Column vector should be raveled\n    y = np.array([[1], [2], [3], [4]])\n    assert_array_equal(column_or_1d(y), y.ravel())\n\n    # Test warning when column vector is passed\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        y = np.array([[1], [2], [3], [4]])\n        result = column_or_1d(y, warn=True)\n        assert_array_equal(result, y.ravel())\n        assert_equal(len(w), 1)\n        assert_true(issubclass(w[0].category, DataConversionWarning))\n\n    # Test ValueError when bad input shape is passed\n    y = np.array([[1, 2], [3, 4]])\n    assert_raises(ValueError, column_or_1d, y)\n\n    # Test ValueError when 3D array is passed\n    y = np.array([[[1, 2]], [[3, 4]]])\n    assert_raises(ValueError, column_or_1d, y)\n", "def test_check_symmetric_force_conversion():\n    arr_asym = np.array([[0, 2], [0, 2]])\n    arr_sym = np.array([[0, 1], [1, 2]])\n    arr_asym_csr = sp.csr_matrix(arr_asym)\n\n    # Check that asymmetric arrays are symmetrized correctly without warning\n    with warnings.catch_warnings(record=True) as w:\n        output = check_symmetric(arr_asym, raise_warning=False)\n        assert_equal(len(w), 0)\n        assert_array_equal(output, arr_sym)\n\n        output = check_symmetric(arr_asym_csr, raise_warning=False)\n        assert_equal(len(w), 0)\n        assert_array_equal(output.toarray(), arr_sym)\n\n    # Check that forcing symmetrization raises warning and corrects array\n    assert_warns(UserWarning, check_symmetric, arr_asym)\n    assert_warns(UserWarning, check_symmetric, arr_asym_csr)\n\n    # Check that forcing symmetrization raises exception\n    assert_raises(ValueError, check_symmetric, arr_asym, raise_exception=True)\n    assert_raises(ValueError, check_symmetric, arr_asym_csr, raise_exception=True)\n", "def test_check_array_force_all_finite_invalid_values():\n    # Check that check_array raises ValueError for invalid force_all_finite values\n    X = np.array([[1, 2], [3, 4]])\n\n    # Invalid force_all_finite value: 'allow-inf'\n    with pytest.raises(ValueError, match='force_all_finite should be a bool or \"allow-nan\"'):\n        check_array(X, force_all_finite='allow-inf')\n\n    # Invalid force_all_finite value: 1 (should be bool or \"allow-nan\")\n    with pytest.raises(ValueError, match='force_all_finite should be a bool or \"allow-nan\"'):\n        check_array(X, force_all_finite=1)\n\n    # Invalid force_all_finite value: None (should be bool or \"allow-nan\")\n    with pytest.raises(ValueError, match='force_all_finite should be a bool or \"allow-nan\"'):\n        check_array(X, force_all_finite=None)\n"], "sample_726": ["def test_label_binarizer_with_float_labels():\n    # Test LabelBinarizer with float labels\n    lb = LabelBinarizer()\n    float_labels = [1.1, 2.2, 3.3, 1.1, 2.2]\n    expected = np.array([[1, 0, 0],\n                         [0, 1, 0],\n                         [0, 0, 1],\n                         [1, 0, 0],\n                         [0, 1, 0]])\n    got = lb.fit_transform(float_labels)\n    assert_array_equal(lb.classes_, [1.1, 2.2, 3.3])\n    assert_array_equal(expected, got)\n    assert_array_equal(lb.inverse_transform(got), float_labels)\n\n    # Test inverse_transform with float labels\n    transformed = np.array([[1, 0, 0],\n                            [0, 1, 0],\n                            [0, 0, 1]])\n    assert_array_equal(lb.inverse_transform(transformed), [1.1, 2.2, 3.3])\n\n    # Test with unseen labels\n    assert_raises(ValueError, lb.transform, [4.4])\n", "def test_label_encoder_non_standard_labels():\n    # Test LabelEncoder with non-standard labels (e.g., tuples, mixed types)\n    le = LabelEncoder()\n    non_standard_labels = [(1, 'a'), (2, 'b'), (1, 'a'), (3, 'c')]\n    le.fit(non_standard_labels)\n    assert_array_equal(le.classes_, [(1, 'a'), (2, 'b'), (3, 'c')])\n    transformed = le.transform([(2, 'b'), (1, 'a'), (3, 'c')])\n    assert_array_equal(transformed, [1, 0, 2])\n    inverse_transformed = le.inverse_transform([1, 0, 2])\n    assert_array_equal(inverse_transformed, [(2, 'b'), (1, 'a'), (3, 'c')])\n    assert_raises(ValueError, le.transform, [(4, 'd')])  # unseen label\n", "def test_label_encoder_non_numerical():\n    # Test LabelEncoder with non-numerical labels\n    le = LabelEncoder()\n    le.fit([\"dog\", \"cat\", \"fish\", \"dog\", \"cat\"])\n    assert_array_equal(le.classes_, [\"cat\", \"dog\", \"fish\"])\n    assert_array_equal(le.transform([\"dog\", \"cat\", \"fish\", \"dog\"]),\n                       [1, 0, 2, 1])\n    assert_array_equal(le.inverse_transform([1, 0, 2, 1]),\n                       [\"dog\", \"cat\", \"fish\", \"dog\"])\n    assert_raises(ValueError, le.transform, [\"bird\"])\n    assert_raises(ValueError, le.inverse_transform, [3])\n", "def test_label_encoder_non_hashable():\n    # Test if non-hashable labels raise appropriate error\n    le = LabelEncoder()\n    msg = \"unhashable type\"\n    assert_raise_message(TypeError, msg, le.fit_transform, [[1], [2]])\n    assert_raise_message(TypeError, msg, le.fit, [[1], [2]])\n    assert_raise_message(TypeError, msg, le.transform, [[1], [2]])\n    assert_raise_message(TypeError, msg, le.inverse_transform, [[1], [2]])\n", "def test_label_encoder_with_strings():\n    # Test LabelEncoder with string labels\n    le = LabelEncoder()\n    le.fit([\"cat\", \"dog\", \"fish\", \"cat\", \"dog\"])\n    assert_array_equal(le.classes_, [\"cat\", \"dog\", \"fish\"])\n    transformed = le.transform([\"dog\", \"fish\", \"cat\"])\n    assert_array_equal(transformed, [1, 2, 0])\n    inverse_transformed = le.inverse_transform(transformed)\n    assert_array_equal(inverse_transformed, [\"dog\", \"fish\", \"cat\"])\n\n    # Test unseen labels\n    assert_raises(ValueError, le.transform, [\"bird\"])\n\n    # Test fit_transform\n    ret = le.fit_transform([\"apple\", \"banana\", \"apple\", \"banana\", \"cherry\"])\n    assert_array_equal(ret, [0, 1, 0, 1, 2])\n    assert_array_equal(le.classes_, [\"apple\", \"banana\", \"cherry\"])\n"], "sample_727": ["def test_imputation_invalid_strategy():\n    # Test if ValueError is raised for an invalid strategy.\n    X = np.array([\n        [1, 2, np.nan],\n        [3, np.nan, 5],\n        [np.nan, 4, 6]\n    ])\n    \n    imputer = Imputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9]\n    ])\n\n    invalid_strategy = \"invalid_strategy\"\n    imputer = Imputer(strategy=invalid_strategy)\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputer_invalid_strategy():\n    # Test that providing an invalid strategy raises a ValueError\n    X = np.array([[1, 2], [3, np.nan], [np.nan, 4]])\n    \n    invalid_strategy = 'invalid'\n    imputer = Imputer(strategy=invalid_strategy)\n    assert_raises(ValueError, imputer.fit, X)\n\n", "def test_imputation_invalid_strategy():\n    # Test Imputer with an invalid strategy\n    X = np.array([\n        [1, 2],\n        [3, 4],\n        [np.nan, 6]\n    ])\n    imputer = Imputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy\n    X = np.array([\n        [np.nan, 1, 2],\n        [3, np.nan, 0],\n        [1, 2, np.nan]\n    ])\n    \n    imputer = Imputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n"], "sample_728": ["def test_make_gaussian_quantiles():\n    mean = [5, 5]\n    cov = 1.0\n    n_samples = 100\n    n_features = 2\n    n_classes = 3\n    X, y = make_gaussian_quantiles(mean=mean, cov=cov, n_samples=n_samples,\n                                   n_features=n_features, n_classes=n_classes,\n                                   random_state=0)\n\n    assert_equal(X.shape, (n_samples, n_features), \"X shape mismatch\")\n    assert_equal(y.shape, (n_samples,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (n_classes,), \"Unexpected number of classes\")\n\n    for cls in range(n_classes):\n        assert_true(np.sum(y == cls) > 0, f\"No samples found for class {cls}\")\n\n    assert_array_almost_equal(np.mean(X, axis=0), mean, decimal=0, err_msg=\"Mean mismatch\")\n    assert_array_almost_equal(np.cov(X, rowvar=False), cov * np.identity(n_features), decimal=0, err_msg=\"Covariance matrix mismatch\")\n\n    # Check shuffling\n    X_shuffled, y_shuffled = make_gaussian_quantiles(mean=mean, cov=cov, n_samples=n_samples,\n                                                     n_features=n_features, n_classes=n_classes,\n                                                     shuffle=True, random_state=0)\n    assert_true((X != X_shuffled).any() or (y != y_shuffled).any(), \"Data was not shuffled\")\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=[0, 0], cov=1.0, n_samples=100, \n                                   n_features=2, n_classes=3, shuffle=True, \n                                   random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n\n    # Verify that the samples are approximately normally distributed\n    assert_almost_equal(np.mean(X, axis=0), 0, decimal=1, err_msg=\"Mean mismatch\")\n    assert_almost_equal(np.std(X, axis=0), 1, decimal=1, err_msg=\"Standard deviation mismatch\")\n\n    # Test for n_samples < n_classes\n    with pytest.raises(ValueError):\n        make_gaussian_quantiles(n_samples=2, n_classes=3)\n\n    # Check for consistent output with same random state\n    X2, y2 = make_gaussian_quantiles(mean=[0, 0], cov=1.0, n_samples=100,\n                                     n_features=2, n_classes=3, shuffle=True,\n                                     random_state=0)\n    assert_array_almost_equal(X, X2)\n    assert_array_equal(y, y2)\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=[0, 0], cov=2.0, n_samples=100,\n                                   n_features=2, n_classes=3, random_state=0)\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n\n    for cls in range(3):\n        assert_true(np.any(y == cls), \"Class {} missing from y\".format(cls))\n\n    # Check if shuffling is working\n    X_shuffled, y_shuffled = make_gaussian_quantiles(mean=[0, 0], cov=2.0,\n                                                     n_samples=100,\n                                                     n_features=2, \n                                                     n_classes=3, \n                                                     shuffle=True,\n                                                     random_state=0)\n    assert not np.array_equal(X, X_shuffled), \"Shuffling did not work\"\n    assert not np.array_equal(y, y_shuffled), \"Shuffling did not work\"\n\n    # Ensure raise ValueError for incorrect n_samples < n_classes\n    assert_raises(ValueError, make_gaussian_quantiles, \n                  mean=[0, 0], cov=2.0, n_samples=2, \n                  n_features=2, n_classes=3, random_state=0)\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3,\n                                   random_state=0)\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n\n    # Check that the samples are normally distributed\n    assert_almost_equal(np.mean(X, axis=0), [0.0, 0.0], decimal=1,\n                        err_msg=\"Samples are not centered around the mean\")\n    assert_almost_equal(np.std(X, axis=0), [1.0, 1.0], decimal=1,\n                        err_msg=\"Samples do not have unit variance\")\n\n    # Check class balance\n    class_counts = np.bincount(y)\n    assert_almost_equal(class_counts / float(len(y)), [1/3.0] * 3, decimal=1,\n                        err_msg=\"Class distribution is not approximately uniform\")\n\n    # Test with different mean and covariance\n    mean = [1.0, 2.0]\n    cov = 2.0\n    X, y = make_gaussian_quantiles(mean=mean, cov=cov, n_samples=100, n_features=2, n_classes=2, random_state=0)\n    assert_almost_equal(np.mean(X, axis=0), mean, decimal=1,\n                        err_msg=\"Samples are not centered around the specified mean\")\n    assert_almost_equal(np.std(X, axis=0), [np.sqrt(cov)] * 2, decimal=1,\n                        err_msg=\"Samples do not have the specified variance\")\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3, random_state=0)\n    \n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n    assert_equal(np.bincount(y).sum(), 100, \"Unexpected total number of samples\")\n\n    X, y = make_gaussian_quantiles(mean=[3, 3], cov=2., n_samples=50, n_features=2, n_classes=2, random_state=0)\n    \n    assert_equal(X.shape, (50, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (50,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (2,), \"Unexpected number of classes\")\n    assert_equal(np.bincount(y).sum(), 50, \"Unexpected total number of samples\")\n    assert_array_almost_equal(X.mean(axis=0), [3, 3], decimal=1, err_msg=\"Mean mismatch\")\n\n    assert_raises(ValueError, make_gaussian_quantiles, n_samples=2, n_classes=3)\n"], "sample_729": ["def test_alpha_grid_invalid_l1_ratio():\n    # Test that _alpha_grid raises an error for l1_ratio=0\n    X = np.array([[1, 2, 3], [4, 5, 6]]).T\n    y = np.array([1, 2, 3])\n    with pytest.raises(ValueError, match=\"Automatic alpha grid generation is not supported for l1_ratio=0\"):\n        _alpha_grid(X, y, l1_ratio=0)\n", "def test_elasticnet_params():\n    # Test ElasticNet initialization with different parameters\n    X, y, X_test, y_test = build_dataset()\n\n    enet = ElasticNet(alpha=0.1, l1_ratio=0.7, fit_intercept=False, normalize=True, precompute=True,\n                      max_iter=200, tol=1e-5, warm_start=True, positive=True, random_state=42, selection='random')\n    enet.fit(X, y)\n    assert_almost_equal(enet.alpha, 0.1)\n    assert_almost_equal(enet.l1_ratio, 0.7)\n    assert_false(enet.fit_intercept)\n    assert_true(enet.normalize)\n    assert_true(enet.precompute)\n    assert_equal(enet.max_iter, 200)\n    assert_almost_equal(enet.tol, 1e-5)\n    assert_true(enet.warm_start)\n    assert_true(enet.positive)\n    assert_equal(enet.random_state, 42)\n    assert_equal(enet.selection, 'random')\n\n    # Check that coefficients are positive due to positive constraint\n    assert_true(np.all(enet.coef_ >= 0))\n    assert_greater(enet.score(X_test, y_test), 0.95)\n", "def test_elasticnet_l1_ratio_zero_error():\n    # Test that ElasticNet raises an error when l1_ratio is set to 0 and alphas are not provided\n    X, y, _, _ = build_dataset()\n\n    enet = ElasticNet(l1_ratio=0)\n    with pytest.raises(ValueError, match=\"Automatic alpha grid generation is not supported for l1_ratio=0\"):\n        enet.fit(X, y)\n", "def test_alpha_grid_value_error():\n    X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    y = [1, 2, 3]\n\n    with pytest.raises(ValueError, match=\"Automatic alpha grid generation is not supported for l1_ratio=0.\"):\n        _alpha_grid(X, y, l1_ratio=0)\n", "def test_lasso_path_shapes():\n    # Test to ensure the shapes of the output arrays from lasso_path are correct\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n    alphas, coefs, gaps = lasso_path(X, y, n_alphas=10)\n\n    # Check the shape of the output\n    assert_equal(alphas.shape, (10,))\n    assert_equal(coefs.shape, (X.shape[1], 10))\n    assert_equal(gaps.shape, (10,))\n\n    # Check the consistency of the output with respect to coefficients\n    for i in range(10):\n        assert_equal(coefs[:, i].shape, (X.shape[1],))\n"], "sample_730": ["def test_enet_non_float_y():\n    X = [[0, 0], [1, 1], [-1, -1]]\n    y = [0, 1, 2]\n    y_int = [0, 1, 2]\n\n    for model in [ElasticNet]:\n        clf = model(fit_intercept=False)\n        clf.fit(X, y)\n        clf_int = model(fit_intercept=False)\n        clf_int.fit(X, y_int)\n        assert_array_equal(clf.coef_, clf_int.coef_)\n", "def test_elastic_net_zero_data():\n    # Check that the ElasticNet can handle zero data without crashing\n    X = np.zeros((3, 3))\n    y = np.zeros(3)\n    clf = ElasticNet(alpha=0.1).fit(X, y)\n    pred = clf.predict([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert_array_almost_equal(clf.coef_, [0, 0, 0])\n    assert_array_almost_equal(pred, [0, 0, 0])\n    assert_almost_equal(clf.dual_gap_, 0)\n", "def test_enet_path_verbose():\n    # Test the verbose parameter in enet_path to ensure it doesn't break functionality\n    X, y, _, _ = build_dataset(n_samples=200, n_features=100,\n                               n_informative_features=100)\n    max_iter = 150\n\n    # Verbose mode should not affect the path results\n    clf_verbose = ElasticNetCV(alphas=[0.01, 0.05, 0.1], eps=2e-3,\n                               l1_ratio=[0.5, 0.7], cv=3,\n                               max_iter=max_iter, verbose=True)\n    ignore_warnings(clf_verbose.fit)(X, y)\n    \n    clf_non_verbose = ElasticNetCV(alphas=[0.01, 0.05, 0.1], eps=2e-3,\n                                   l1_ratio=[0.5, 0.7], cv=3,\n                                   max_iter=max_iter, verbose=False)\n    ignore_warnings(clf_non_verbose.fit)(X, y)\n\n    assert_array_almost_equal(clf_verbose.coef_, clf_non_verbose.coef_)\n    assert_almost_equal(clf_verbose.alpha_, clf_non_verbose.alpha_)\n    assert_almost_equal(clf_verbose.l1_ratio_, clf_non_verbose.l1_ratio_)\n", "def test_elasticnet_cv_pipeline():\n    # Test ElasticNetCV with pipeline and StandardScaler\n    from sklearn.pipeline import make_pipeline\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.model_selection import KFold\n    from sklearn import datasets\n\n    diabetes = datasets.load_diabetes()\n    X = diabetes.data\n    y = diabetes.target\n\n    pipe = make_pipeline(\n        StandardScaler(),\n        ElasticNetCV(cv=KFold(n_splits=5))\n    )\n    pipe.fit(X, y)\n\n    assert isinstance(pipe.named_steps['elasticnetcv'].alpha_, float)\n    assert isinstance(pipe.named_steps['elasticnetcv'].l1_ratio_, float)\n    assert pipe.named_steps['elasticnetcv'].coef_.shape[0] == X.shape[1]\n    assert pipe.named_steps['elasticnetcv'].n_iter_ > 0\n", "def test_elasticnet_path_return_n_iter():\n    # Test that enet_path returns the number of iterations correctly.\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n    alphas, coefs, dual_gaps, n_iters = enet_path(X, y, return_n_iter=True, max_iter=5000, tol=1e-8)\n    \n    assert len(n_iters) == len(alphas)\n    assert np.all(np.array(n_iters) > 0)\n    assert np.all(np.array(dual_gaps) <= 1e-8)\n"], "sample_731": ["def test_feature_names():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    expected_feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                              \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert(data.feature_names == expected_feature_names)\n", "def test_feature_names():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset cannot be loaded.\")\n    expected_feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                              \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert expected_feature_names == data.feature_names\n", "def test_fetch_feature_names():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    expected_feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                              \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert expected_feature_names == data.feature_names\n", "def test_feature_names():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    \n    expected_feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                              \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert data.feature_names == expected_feature_names\n", "def test_fetch_custom_data_home():\n    from tempfile import TemporaryDirectory\n\n    with TemporaryDirectory() as temp_dir:\n        try:\n            data = fetch(data_home=temp_dir)\n        except IOError:\n            raise SkipTest(\"California housing dataset can not be loaded.\")\n        assert((20640, 8) == data.data.shape)\n        assert((20640, ) == data.target.shape)\n\n    # Check that data is loaded correctly from the custom data home\n    with TemporaryDirectory() as temp_dir:\n        try:\n            data = fetch(data_home=temp_dir)\n        except IOError:\n            raise SkipTest(\"California housing dataset can not be loaded.\")\n        assert((20640, 8) == data.data.shape)\n        assert((20640, ) == data.target.shape)\n"], "sample_732": ["def test_full_dataset():\n    try:\n        data = fetch_kddcup99(percent10=False, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"Full kddcup99 dataset cannot be loaded due to its size or absence.\")\n\n    assert_equal(data.data.shape, (4898431, 41))\n    assert_equal(data.target.shape, (4898431,))\n\n    data_shuffled = fetch_kddcup99(percent10=False, shuffle=True, random_state=0)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n\n    fetch_func = partial(fetch_kddcup99, percent10=False)\n    check_return_X_y(data, fetch_func)\n", "def test_fetch_full_dataset():\n    try:\n        data = fetch_kddcup99(percent10=False, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"Full kddcup99 dataset can not be loaded.\")\n\n    assert_equal(data.data.shape, (4898431, 41))\n    assert_equal(data.target.shape, (4898431,))\n\n    data_shuffled = fetch_kddcup99(percent10=False, shuffle=True, random_state=0)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n\n    fetch_func = partial(fetch_kddcup99, percent10=False)\n    check_return_X_y(data, fetch_func)\n", "def test_no_shuffle():\n    try:\n        dataset = fetch_kddcup99(random_state=0, subset='SA', shuffle=False,\n                                 percent10=True, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    # Check that the dataset is not shuffled by comparing first few elements\n    assert_equal(dataset.target[:5].tolist(), [b'normal.', b'normal.', b'normal.', b'normal.', b'normal.'])\n\n    # Ensure same data shape as the shuffled dataset\n    dataset_shuffled = fetch_kddcup99(random_state=0, subset='SA', shuffle=True,\n                                      percent10=True, download_if_missing=False)\n    assert_equal(dataset.data.shape, dataset_shuffled.data.shape)\n    assert_equal(dataset.target.shape, dataset_shuffled.target.shape)\n", "def test_load_full_dataset():\n    try:\n        data = fetch_kddcup99(percent10=False, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"Full kddcup99 dataset can not be loaded due to size constraints.\")\n\n    assert_equal(data.data.shape, (4898431, 41))\n    assert_equal(data.target.shape, (4898431,))\n\n    data_shuffled = fetch_kddcup99(percent10=False, shuffle=True, random_state=42)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n", "def test_fetch_full_data():\n    try:\n        data = fetch_kddcup99(percent10=False, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"Full kddcup99 dataset can not be loaded.\")\n\n    assert_equal(data.data.shape, (4898431, 41))\n    assert_equal(data.target.shape, (4898431,))\n\n    data_shuffled = fetch_kddcup99(percent10=False, shuffle=True, random_state=42)\n    assert_equal(data.data.shape, data_shuffled.data.shape)\n    assert_equal(data.target.shape, data_shuffled.target.shape)\n\n    fetch_func = partial(fetch_kddcup99, percent10=False)\n    check_return_X_y(data, fetch_func)\n"], "sample_733": ["def test_hashingvectorizer_with_filenames():\n    # Create temporary text files\n    import tempfile\n    doc1 = tempfile.NamedTemporaryFile(delete=False)\n    doc2 = tempfile.NamedTemporaryFile(delete=False)\n    \n    doc1.write(b\"the pizza beer\")\n    doc1.flush()\n    doc2.write(b\"the burger beer\")\n    doc2.flush()\n    \n    filenames = [doc1.name, doc2.name]\n    \n    try:\n        # Test HashingVectorizer with 'filename' input\n        hv = HashingVectorizer(input='filename')\n        X = hv.fit_transform(filenames)\n        \n        assert_equal(X.shape[0], 2)\n        assert_greater(X.nnz, 0)  # Ensure some features are extracted\n    finally:\n        doc1.close()\n        doc2.close()\n        os.unlink(doc1.name)\n        os.unlink(doc2.name)\n", "def test_countvectorizer_char_ngrams():\n    # Test for character n-grams with CountVectorizer\n    cv = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n    X = cv.fit_transform([\"abcd\", \"efgh\"])\n    feature_names = cv.get_feature_names()\n    expected_features = ['ab', 'bc', 'cd', 'ef', 'fg', 'gh']\n    assert_array_equal(feature_names, expected_features)\n\n    # Test for character n-grams with a different range\n    cv = CountVectorizer(analyzer='char', ngram_range=(2, 3))\n    X = cv.fit_transform([\"abcd\", \"efgh\"])\n    feature_names = cv.get_feature_names()\n    expected_features = ['ab', 'abc', 'bc', 'bcd', 'cd', 'ef', 'efg', 'fg', 'fgh', 'gh']\n    assert_array_equal(feature_names, expected_features)\n\n    # Test character n-grams with preprocessor\n        return re.sub(r'[aeiou]', '', s)\n\n    cv = CountVectorizer(analyzer='char', ngram_range=(2, 2), preprocessor=remove_vowels)\n    X = cv.fit_transform([\"abcdefgh\"])\n    feature_names = cv.get_feature_names()\n    expected_features = ['bc', 'cd', 'df', 'fg', 'gh']\n    assert_array_equal(feature_names, expected_features)\n", "def test_strip_tags():\n    # Basic test for strip_tags\n    html = \"<html><body><h1>Title</h1><p>This is a <b>test</b>.</p></body></html>\"\n    expected = \"  Title  This is a  test  .   \"\n    assert_equal(strip_tags(html), expected)\n\n    # Test with nested tags\n    html = \"<div><span>Text with <em>nested</em> tags</span></div>\"\n    expected = \" Text with  nested  tags \"\n    assert_equal(strip_tags(html), expected)\n\n    # Test with self-closing tags\n    html = \"Self-closing tags like <br/> or <img src='image.jpg'/> should be handled.\"\n    expected = \" Self-closing tags like   or  should be handled. \"\n    assert_equal(strip_tags(html), expected)\n\n    # Test with multiple lines of tags\n    html = \"<div>\\n<p>Line 1</p>\\n<p>Line 2</p>\\n</div>\"\n    expected = \"  Line 1  Line 2  \"\n    assert_equal(strip_tags(html), expected)\n", "def test_strip_tags():\n    # Test basic HTML tag stripping\n    assert_equal(strip_tags(\"<b>Bold</b> and <i>italic</i>\"), \"Bold  and  italic \")\n\n    # Test nested tags\n    assert_equal(strip_tags(\"<div><p>Nested <b>tags</b></p></div>\"), \"Nested  tags \")\n\n    # Test tags with attributes\n    assert_equal(strip_tags(\"<a href='https://example.com'>Link</a>\"), \"Link \")\n\n    # Test self-closing tags\n    assert_equal(strip_tags(\"Self-closing <br/> tag\"), \"Self-closing  tag\")\n\n    # Test non-html content\n    assert_equal(strip_tags(\"No tags here\"), \"No tags here\")\n", "def test_hashingvectorizer_non_negative_alternate_sign():\n    # Check that non_negative and alternate_sign parameters work correctly\n    hv = HashingVectorizer(non_negative=True, alternate_sign=True)\n    X = hv.transform(ALL_FOOD_DOCS)\n    assert_true((X.data >= 0).all(), \"All values should be non-negative\")\n\n    hv = HashingVectorizer(non_negative=False, alternate_sign=True)\n    X = hv.transform(ALL_FOOD_DOCS)\n    assert_true((X.data > 0).any() and (X.data < 0).any(),\n                \"Values should be both positive and negative with alternate_sign\")\n\n    hv = HashingVectorizer(non_negative=False, alternate_sign=False)\n    X = hv.transform(ALL_FOOD_DOCS)\n    assert_true((X.data >= 0).all(), \"All values should be non-negative without alternate sign\")\n\n    hv = HashingVectorizer(non_negative=True, alternate_sign=False)\n    X = hv.transform(ALL_FOOD_DOCS)\n    assert_true((X.data >= 0).all(), \"All values should be non-negative without alternate sign\")\n"], "sample_734": ["def test_empty_labels():\n    # Test that all scoring functions handle empty input gracefully\n    for score_func in score_funcs:\n        assert_equal(score_func([], []), 1.0)\n\n    labels_true = []\n    labels_pred = []\n    assert_equal(fowlkes_mallows_score(labels_true, labels_pred), 0.0)\n    assert_equal(mutual_info_score(labels_true, labels_pred), 0.0)\n    assert_equal(adjusted_mutual_info_score(labels_true, labels_pred), 1.0)\n    assert_equal(normalized_mutual_info_score(labels_true, labels_pred), 1.0)\n    assert_equal(contingency_matrix(labels_true, labels_pred).size, 0)\n    assert_equal(entropy(labels_true), 1.0)\n", "def test_overlapping_labels():\n    # Tests where labels_true and labels_pred overlap partially\n    labels_true = [0, 0, 1, 1, 2, 2, 3, 3]\n    labels_pred = [0, 1, 1, 2, 2, 3, 3, 0]\n    \n    h, c, v = homogeneity_completeness_v_measure(labels_true, labels_pred)\n    assert_almost_equal(h, 0.57143, 5)\n    assert_almost_equal(c, 0.57143, 5)\n    assert_almost_equal(v, 0.57143, 5)\n    \n    ari = adjusted_rand_score(labels_true, labels_pred)\n    assert_almost_equal(ari, 0.23077, 5)\n    \n    ami = adjusted_mutual_info_score(labels_true, labels_pred)\n    assert_almost_equal(ami, 0.24398, 5)\n    \n    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n    assert_almost_equal(nmi, 0.57143, 5)\n    \n    fmi = fowlkes_mallows_score(labels_true, labels_pred)\n    assert_almost_equal(fmi, 0.5, 5)\n    \n    mi = mutual_info_score(labels_true, labels_pred)\n    assert_almost_equal(mi, 0.63651, 5)\n", "def test_v_measure_score_symmetry():\n    # Test the symmetry property of v_measure_score\n    labels_a = np.array([0, 0, 1, 1, 2, 2])\n    labels_b = np.array([2, 2, 1, 1, 0, 0])\n    \n    v_ab = v_measure_score(labels_a, labels_b)\n    v_ba = v_measure_score(labels_b, labels_a)\n    \n    assert_almost_equal(v_ab, v_ba)\n\n    labels_c = np.array([1, 1, 0, 0, 2, 2])\n    \n    v_ac = v_measure_score(labels_a, labels_c)\n    v_ca = v_measure_score(labels_c, labels_a)\n    \n    assert_almost_equal(v_ac, v_ca)\n\n    v_bc = v_measure_score(labels_b, labels_c)\n    v_cb = v_measure_score(labels_c, labels_b)\n    \n    assert_almost_equal(v_bc, v_cb)\n", "def test_fowlkes_mallows_score_with_non_integer_labels():\n    # Labels that are not integers\n    score = fowlkes_mallows_score(['a', 'a', 'a', 'b', 'b', 'b'],\n                                  ['x', 'x', 'y', 'y', 'z', 'z'])\n    assert_almost_equal(score, 4. / np.sqrt(12. * 6.))\n\n    # Perfect match but where the label names changed\n    perfect_score = fowlkes_mallows_score(['a', 'a', 'a', 'b', 'b', 'b'],\n                                          ['y', 'y', 'y', 'x', 'x', 'x'])\n    assert_almost_equal(perfect_score, 1.)\n\n    # Worst case\n    worst_score = fowlkes_mallows_score(['a', 'a', 'a', 'a', 'a', 'a'],\n                                        ['x', 'y', 'z', 'w', 'v', 'u'])\n    assert_almost_equal(worst_score, 0.)\n\n    # General case with mixed type labels\n    score_mixed = fowlkes_mallows_score([0, 'a', 'a', 1, 1, 'b'],\n                                        ['x', 'x', 'y', 'y', 'z', 'z'])\n    assert_almost_equal(score_mixed, 4. / np.sqrt(12. * 6.))\n", "def test_fowlkes_mallows_score_with_empty_labels():\n    # Testing Fowlkes-Mallows score with empty labels\n    assert_equal(fowlkes_mallows_score([], []), 0.0)\n    assert_equal(fowlkes_mallows_score([], [0, 1, 2]), 0.0)\n    assert_equal(fowlkes_mallows_score([0, 1, 2], []), 0.0)\n"], "sample_735": ["def test_gaussian_mixture_covariance_matrices():\n    # Test the GaussianMixture's covariance matrices are correctly estimated\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=5)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components, covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n        \n        if covar_type == 'full':\n            covariances = _estimate_gaussian_covariances_full(gmm.predict_proba(X), X, gmm.weights_ * X.shape[0], gmm.means_, gmm.reg_covar)\n            assert_array_almost_equal(covariances, gmm.covariances_)\n        elif covar_type == 'tied':\n            covariance = _estimate_gaussian_covariances_tied(gmm.predict_proba(X), X, gmm.weights_ * X.shape[0], gmm.means_, gmm.reg_covar)\n            assert_array_almost_equal(covariance, gmm.covariances_)\n        elif covar_type == 'diag':\n            covariances = _estimate_gaussian_covariances_diag(gmm.predict_proba(X), X, gmm.weights_ * X.shape[0], gmm.means_, gmm.reg_covar)\n            assert_array_almost_equal(covariances, gmm.covariances_)\n        elif covar_type == 'spherical':\n            covariances = _estimate_gaussian_covariances_spherical(gmm.predict_proba(X), X, gmm.weights_ * X.shape[0], gmm.means_, gmm.reg_covar)\n            assert_array_almost_equal(covariances, gmm.covariances_)\n", "def test_gaussian_mixture_init_params():\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 2)\n\n    # Check that init_params can be correctly set to 'kmeans' or 'random'\n    for init_param in ['kmeans', 'random']:\n        gmm = GaussianMixture(init_params=init_param, random_state=rng)\n        assert_equal(gmm.init_params, init_param)\n        gmm.fit(X)\n\n    # Check invalid init_params raises ValueError\n    invalid_init_param = 'invalid_method'\n    gmm = GaussianMixture(init_params=invalid_init_param, random_state=rng)\n    assert_raise_message(ValueError,\n                         \"Unimplemented initialization method '%s'\"\n                         % invalid_init_param,\n                         gmm.fit, X)\n", "def test_gaussian_mixture_initialization_methods():\n    # Test the two initialization methods 'kmeans' and 'random'\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n\n        # Test kmeans initialization\n        gmm_kmeans = GaussianMixture(n_components=n_components,\n                                     covariance_type=covar_type,\n                                     init_params='kmeans', random_state=rng)\n        gmm_kmeans.fit(X)\n        assert gmm_kmeans.converged_\n\n        # Test random initialization\n        gmm_random = GaussianMixture(n_components=n_components,\n                                     covariance_type=covar_type,\n                                     init_params='random', random_state=rng)\n        gmm_random.fit(X)\n        assert gmm_random.converged_\n\n        # Check that the means are not all the same\n        assert not np.allclose(gmm_kmeans.means_, gmm_random.means_)\n\n        # Check that the covariances are not all the same\n        assert not np.allclose(gmm_kmeans.covariances_, gmm_random.covariances_)\n", "def test_predict_proba_sum_to_one():\n    \"\"\"Test that the probabilities returned by predict_proba sum to one.\"\"\"\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=5)\n    n_samples = rand_data.n_samples\n    n_features = rand_data.n_features\n    n_components = rand_data.n_components\n\n    X = rng.rand(n_samples, n_features)\n    for covar_type in COVARIANCE_TYPE:\n        weights = rand_data.weights\n        means = rand_data.means\n        precisions = rand_data.precisions[covar_type]\n        g = GaussianMixture(n_components=n_components, random_state=rng,\n                            weights_init=weights, means_init=means,\n                            precisions_init=precisions,\n                            covariance_type=covar_type)\n        g.fit(X)\n        proba = g.predict_proba(X)\n        assert_array_almost_equal(proba.sum(axis=1), np.ones(n_samples))\n", "def test_check_precision_positivity():\n    rng = np.random.RandomState(0)\n    n_features = 2\n\n    # Check precision positivity for different covariance types\n    precisions = {\n        'full': rng.rand(n_features, n_features),\n        'tied': rng.rand(n_features, n_features),\n        'diag': rng.rand(n_features),\n        'spherical': rng.rand(1)\n    }\n\n    for covar_type, precision in precisions.items():\n        if covar_type in ['diag', 'spherical']:\n            _check_precision_positivity(precision, covar_type)\n        else:\n            assert_raise_message(ValueError, \"'%s precision' should be positive\" % covar_type,\n                                 _check_precision_positivity, -precision, covar_type)\n\n    # Check for ValueError when precision is not positive\n    for covar_type, precision in precisions.items():\n        if covar_type in ['diag', 'spherical']:\n            assert_raise_message(ValueError, \"'%s precision' should be positive\" % covar_type,\n                                 _check_precision_positivity, -precision, covar_type)\n"], "sample_736": ["def test_logistic_regression_path_large_C_values():\n    # Test logistic_regression_path with very large C values\n    rng = np.random.RandomState(0)\n    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))\n    y = [1] * 100 + [-1] * 100\n\n    # Large C values indicate lower regularization\n    Cs = [1e5, 1e6, 1e7]\n\n    # Testing for all solvers\n    for solver in ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga']:\n        coefs, Cs, _ = logistic_regression_path(\n            X, y, Cs=Cs, fit_intercept=True, tol=1e-6, solver=solver,\n            intercept_scaling=10000., random_state=0\n        )\n\n        for i, C in enumerate(Cs):\n            lr = LogisticRegression(C=C, fit_intercept=True, tol=1e-4,\n                                    solver=solver, intercept_scaling=10000.,\n                                    random_state=0)\n            lr.fit(X, y)\n            lr_coef = np.concatenate([lr.coef_.ravel(), lr.intercept_])\n            assert_array_almost_equal(lr_coef, coefs[i], decimal=4,\n                                      err_msg=\"with solver = %s\" % solver)\n", "def test_logistic_loss_and_grad_regularization():\n    # Test the effect of regularization in logistic loss and gradient\n    X_ref, y = make_classification(n_samples=20, random_state=0)\n    n_features = X_ref.shape[1]\n\n    X_sp = X_ref.copy()\n    X_sp[X_sp < .1] = 0\n    X_sp = sp.csr_matrix(X_sp)\n    \n    for X in (X_ref, X_sp):\n        w = np.zeros(n_features)\n\n        # Test the effect of different values of alpha on the logistic loss and gradient\n        for alpha in [0.01, 0.1, 1, 10]:\n            loss, grad = _logistic_loss_and_grad(w, X, y, alpha=alpha)\n            approx_grad = optimize.approx_fprime(\n                w, lambda w: _logistic_loss_and_grad(w, X, y, alpha=alpha)[0], 1e-3\n            )\n            assert_array_almost_equal(grad, approx_grad, decimal=2)\n\n            # Assert that higher regularization (higher alpha) leads to a higher loss\n            if alpha > 0.01:\n                prev_loss, _ = _logistic_loss_and_grad(w, X, y, alpha=alpha/10)\n                assert_greater(loss, prev_loss)\n", "def test_logistic_loss():\n    # Test logistic loss function for both dense and sparse data\n    X_ref, y = make_classification(n_samples=20, random_state=0)\n    n_features = X_ref.shape[1]\n\n    X_sp = X_ref.copy()\n    X_sp[X_sp < .1] = 0\n    X_sp = sp.csr_matrix(X_sp)\n\n    for X in (X_ref, X_sp):\n        w = np.zeros(n_features)\n\n        # Check the logistic loss without intercept\n        loss = _logistic_loss(w, X, y, alpha=1.)\n        expected_loss = log_loss(y, expit(X.dot(w))) + 0.5 * np.dot(w, w)\n        assert_almost_equal(loss, expected_loss)\n\n        # Check the logistic loss with intercept\n        w = np.zeros(n_features + 1)\n        loss_interp = _logistic_loss(w, X, y, alpha=1.)\n        expected_loss_interp = log_loss(y, expit(X.dot(w[:-1]) + w[-1])) + 0.5 * np.dot(w[:-1], w[:-1])\n        assert_almost_equal(loss_interp, expected_loss_interp)\n", "def test_logistic_loss():\n    # Test _logistic_loss function\n    X, y = make_classification(n_samples=20, random_state=0)\n    n_features = X.shape[1]\n    w = np.zeros(n_features)\n\n    # First check that the loss is consistent with _logistic_loss_and_grad\n    loss1, _ = _logistic_loss_and_grad(w, X, y, alpha=1.)\n    loss2 = _logistic_loss(w, X, y, alpha=1.)\n    assert_almost_equal(loss1, loss2)\n\n    # Check that the intercept handling is consistent\n    w = np.zeros(n_features + 1)\n    loss1, _ = _logistic_loss_and_grad(w, X, y, alpha=1.)\n    loss2 = _logistic_loss(w, X, y, alpha=1.)\n    assert_almost_equal(loss1, loss2)\n\n    # Verify loss with sample weights\n    sample_weight = np.ones(X.shape[0])\n    loss1, _ = _logistic_loss_and_grad(w, X, y, alpha=1., sample_weight=sample_weight)\n    loss2 = _logistic_loss(w, X, y, alpha=1., sample_weight=sample_weight)\n    assert_almost_equal(loss1, loss2)\n\n    sample_weight = np.random.rand(X.shape[0])\n    loss1, _ = _logistic_loss_and_grad(w, X, y, alpha=1., sample_weight=sample_weight)\n    loss2 = _logistic_loss(w, X, y, alpha=1., sample_weight=sample_weight)\n    assert_almost_equal(loss1, loss2)\n", "def test_logistic_regression_path_warm_start():\n    # Test that logistic_regression_path works with warm_start\n\n    # Generate a binary classification dataset\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n\n    # Use the logistic_regression_path function with warm_start enabled\n    coefs, Cs, n_iter = logistic_regression_path(\n        X, y, Cs=[0.1, 1, 10], solver='lbfgs', warm_start=True, max_iter=100, random_state=42)\n\n    # Fit logistic regression models for each value of C and compare the coefficients\n    for i, C in enumerate(Cs):\n        lr = LogisticRegression(C=C, solver='lbfgs', max_iter=100, random_state=42)\n        lr.fit(X, y)\n        assert_array_almost_equal(coefs[i], lr.coef_.ravel())\n\n    # Check if the warm_start has been effective by comparing the number of iterations\n    for i in range(1, len(n_iter)):\n        assert_greater(n_iter[i - 1], n_iter[i])\n"], "sample_737": ["def test_strip_tags():\n    html = \"<div>Text with <a href='#'>link</a> and <b>bold</b> tags</div>\"\n    expected = \"Text with  link  and  bold  tags\"\n    assert_equal(strip_tags(html), expected)\n\n    html = \"<p>Paragraph with <br> line <br/> breaks.</p>\"\n    expected = \"Paragraph with   line   breaks.\"\n    assert_equal(strip_tags(html), expected)\n\n    html = \"<p>Unclosed tag\"\n    expected = \"Unclosed tag\"\n    assert_equal(strip_tags(html), expected)\n\n    html = \"<!DOCTYPE html><html><head><title>Title</title></head><body>Body</body></html>\"\n    expected = \"  Title   Body \"\n    assert_equal(strip_tags(html), expected)\n", "def test_hashingvectorizer_alternate_sign():\n    # Check that the alternate_sign parameter works as expected\n    hv = HashingVectorizer(alternate_sign=True)\n    X = hv.fit_transform(['hello world', 'hello hello'])\n    assert_true(np.any(X.data < 0))  # Ensure that there are negative values\n\n    hv = HashingVectorizer(alternate_sign=False)\n    X = hv.fit_transform(['hello world', 'hello hello'])\n    assert_true(np.all(X.data >= 0))  # Ensure no negative values\n\n    # Ensure that non_negative parameter overrides alternate_sign\n    hv = HashingVectorizer(alternate_sign=True, non_negative=True)\n    X = hv.fit_transform(['hello world', 'hello hello'])\n    assert_true(np.all(X.data >= 0))  # Ensure no negative values\n", "def test_strip_accents_unicode_non_latin():\n    # Check some non-Latin characters\n    a = '\u0103\u0219\u021b\u00ee\u00e2'\n    expected = 'asti\u00e2'  # \u0103 -> a, \u0219 -> s, \u021b -> t, \u00ee -> i, \u00e2 remains as it is\n    assert_equal(strip_accents_unicode(a), expected)\n\n    a = '\u010d\u0107\u0111\u0161\u017e'\n    expected = 'ccdss'  # \u010d -> c, \u0107 -> c, \u0111 -> d, \u0161 -> s, \u017e -> z\n    assert_equal(strip_accents_unicode(a), expected)\n\n    a = '\u011f\u00fc\u015f\u0131\u00f6\u00e7'\n    expected = 'gusioc'  # \u011f -> g, \u00fc -> u, \u015f -> s, \u0131 -> i, \u00f6 -> o, \u00e7 -> c\n    assert_equal(strip_accents_unicode(a), expected)\n", "def test_strip_tags():\n    html_text = \"<html><head><title>Test</title></head><body><p>Test paragraph.</p></body></html>\"\n    expected = \" Test Test  Test paragraph. \"\n    assert_equal(strip_tags(html_text), expected)\n\n    # Test with nested tags\n    html_text = \"<div><p><span>Nested</span> tags</p> within <a href='#'>HTML</a></div>\"\n    expected = \" Nested tags  within  HTML \"\n    assert_equal(strip_tags(html_text), expected)\n\n    # Test with malformed HTML\n    html_text = \"<p>Unclosed tag\"\n    expected = \" Unclosed tag\"\n    assert_equal(strip_tags(html_text), expected)\n\n    # Test with no tags\n    html_text = \"No tags here!\"\n    expected = \"No tags here!\"\n    assert_equal(strip_tags(html_text), expected)\n", "def test_vectorizer_strip_accents_callable():\n    # Test case to cover strip_accents when it's a callable\n        return s.replace('\u00e1', 'a').replace('\u00e9', 'e')\n\n    v = CountVectorizer(strip_accents=custom_strip_accents)\n    data = [\"h\u00e9llo w\u00f3rld\", \"h\u00f3w \u00e1re y\u00f3u\"]\n    X = v.fit_transform(data)\n    feature_names = v.get_feature_names()\n    assert_equal(feature_names, ['are', 'hello', 'how', 'world', 'you'])\n    assert_array_equal(X.toarray(), [[1, 1, 0, 1, 0], [0, 0, 1, 0, 1]])\n"], "sample_738": ["def test_hashingvectorizer_char_ngram_range():\n    # Test HashingVectorizer with character n-grams of varying lengths\n    hv = HashingVectorizer(analyzer='char', ngram_range=(2, 4), norm=None)\n    X = hv.fit_transform(['hello world', 'hello'])\n    assert_equal(X.shape[1], hv.n_features)\n\n    # Check that the feature values are correct\n    # Since it uses hashing, we can't check for specific values\n    # but we can check the presence of non-zero values\n    assert_true(np.any(X[0].data))\n    assert_true(np.any(X[1].data))\n\n    # Check that the n-grams of specified lengths are generated\n    cnga = CountVectorizer(analyzer='char', ngram_range=(2, 4)).build_analyzer()\n    expected_ngrams = cnga('hello world')\n    actual_ngrams = hv.build_analyzer()('hello world')\n\n    # We can't assert equal directly due to hashing, but we can check lengths\n    assert_equal(len(expected_ngrams), len(actual_ngrams))\n\n    # Ensure that the transformation is consistent\n    X2 = hv.transform(['hello world', 'hello'])\n    assert_array_equal(X.toarray(), X2.toarray())\n", "def test_vectorizer_empty_docs():\n    # Test vectorizers with empty documents\n    empty_docs = [\"\", \"\", \"\"]\n    non_empty_docs = [\"This is a test.\", \"Another test.\"]\n\n    for vectorizer in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        # Testing empty documents\n        X_empty = vectorizer.fit_transform(empty_docs)\n        assert_equal(X_empty.shape[0], len(empty_docs))\n        assert_equal(X_empty.shape[1], 0 if vectorizer != HashingVectorizer() else vectorizer.n_features)\n        assert_equal(X_empty.nnz, 0)\n\n        # Testing mixed empty and non-empty documents\n        X_mixed = vectorizer.fit_transform(empty_docs + non_empty_docs)\n        assert_equal(X_mixed.shape[0], len(empty_docs) + len(non_empty_docs))\n        if vectorizer != HashingVectorizer():\n            assert_greater(X_mixed.shape[1], 0)\n        assert_greater(X_mixed.nnz, 0)\n", "def test_strip_tags():\n    html_snippet = \"<div>Some <b>bold</b> text</div>\"\n    expected_output = \"Some bold text\"\n    assert_equal(strip_tags(html_snippet), expected_output)\n\n    html_snippet = \"<p>This is a <a href='#'>link</a> inside a paragraph.</p>\"\n    expected_output = \"This is a link inside a paragraph.\"\n    assert_equal(strip_tags(html_snippet), expected_output)\n\n    html_snippet = \"<html><body><h1>Title</h1><p>Paragraph</p></body></html>\"\n    expected_output = \"Title Paragraph\"\n    assert_equal(strip_tags(html_snippet), expected_output)\n\n    html_snippet = \"<span>Text with <script>script</script> tags</span>\"\n    expected_output = \"Text with script tags\"\n    assert_equal(strip_tags(html_snippet), expected_output)\n\n    html_snippet = \"<!-- Comment --><p>Text</p>\"\n    expected_output = \"Text\"\n    assert_equal(strip_tags(html_snippet), expected_output)\n", "def test_strip_tags():\n    text = \"<div>Some <b>bold</b> text</div>\"\n    expected = \"Some bold text\"\n    assert_equal(strip_tags(text), expected)\n\n    text = \"<p>This is a <a href='link'>link</a>.</p>\"\n    expected = \"This is a link.\"\n    assert_equal(strip_tags(text), expected)\n\n    text = \"No tags here!\"\n    expected = \"No tags here!\"\n    assert_equal(strip_tags(text), expected)\n\n    text = \"<html><body>Text with <i>italic</i> and <u>underline</u>.</body></html>\"\n    expected = \"Text with italic and underline.\"\n    assert_equal(strip_tags(text), expected)\n\n    text = \"<p>Mixed <b>bold</b> and <i>italic</i> <u>text</u>.</p>\"\n    expected = \"Mixed bold and italic text.\"\n    assert_equal(strip_tags(text), expected)\n", "def test_vectorizer_char_wb_ngrams():\n    # test char_wb analyzer with different ngram ranges\n    for Vectorizer in (CountVectorizer, HashingVectorizer):\n        for ngram_range in [(2, 2), (2, 3), (3, 3)]:\n            v = Vectorizer(analyzer='char_wb', ngram_range=ngram_range)\n            analyze = v.build_analyzer()\n\n            if ngram_range == (2, 2):\n                expected = [' t', 'th', 'hi', 'is', 's ', ' i', 'is', 's ']\n            elif ngram_range == (2, 3):\n                expected = [' t', ' th', 'thi', 'his', 'is ', 's i', ' is', 'is ', 's ']\n            elif ngram_range == (3, 3):\n                expected = [' th', 'thi', 'his', ' is', ' is', ' is']\n\n            assert_equal(analyze(\"this is\"), expected)\n"], "sample_739": ["def test_label_binarizer_binary_decision_function_output():\n    # Test the LabelBinarizer with decision function output\n    lb = LabelBinarizer()\n    lb.fit([0, 1])\n\n    # binary classification decision function output\n    decision_function_output = np.array([-2.5, 0.3, 1.2, -1.0])\n    expected = np.array([[0], [1], [1], [0]])\n    got = lb.inverse_transform(decision_function_output, threshold=0)\n    assert_array_equal(got, [0, 1, 1, 0])\n\n    # binary classification predict_proba output\n    predict_proba_output = np.array([0.1, 0.6, 0.8, 0.3])\n    got = lb.inverse_transform(predict_proba_output, threshold=0.5)\n    assert_array_equal(got, [0, 1, 1, 0])\n", "def test_multilabel_binarizer_with_empty_classes():\n    # Test MultiLabelBinarizer with empty classes parameter\n    mlb = MultiLabelBinarizer(classes=[])\n    inp = [(1, 2), (3,), (2,)]\n    expected = np.zeros((3, 0), dtype=int)\n    \n    got = mlb.fit_transform(inp)\n    assert_array_equal(got, expected)\n    assert_array_equal(mlb.classes_, [])\n    \n    inverse = mlb.inverse_transform(got)\n    assert_equal(len(inverse), 3)\n    assert_equal(inverse[0], ())\n    assert_equal(inverse[1], ())\n    assert_equal(inverse[2], ())\n", "def test_label_encoder_non_numeric_labels():\n    # Test non-numeric labels\n    le = LabelEncoder()\n    le.fit([\"dog\", \"cat\", \"fish\"])\n    assert_array_equal(le.classes_, [\"cat\", \"dog\", \"fish\"])\n    assert_array_equal(le.transform([\"cat\", \"dog\", \"dog\", \"fish\"]),\n                       [0, 1, 1, 2])\n    assert_array_equal(le.inverse_transform([0, 1, 1, 2]),\n                       [\"cat\", \"dog\", \"dog\", \"fish\"])\n    \n    # Test unseen non-numeric label\n    assert_raises(ValueError, le.transform, [\"cat\", \"bird\"])\n", "def test_multilabel_binarizer_empty_classes():\n    mlb = MultiLabelBinarizer(classes=[])\n    y = [[1, 2], [1], []]\n    Y = np.array([[0, 0],\n                  [0, 0],\n                  [0, 0]])\n    assert_array_equal(mlb.fit_transform(y), Y)\n\n    mlb = MultiLabelBinarizer(classes=[])\n    assert_raises(ValueError, mlb.fit_transform, [[1, 2], [1], [3]])\n\n    mlb = MultiLabelBinarizer(classes=[])\n    y_sparse = csr_matrix([[1, 0], [0, 1], [0, 0]])\n    assert_raises(ValueError, mlb.fit_transform, y_sparse)\n", "def test_label_encoder_non_numeric():\n    # Test LabelEncoder with non-numeric labels\n    le = LabelEncoder()\n    non_numeric_labels = [\"cat\", \"dog\", \"fish\"]\n    le.fit(non_numeric_labels)\n    assert_array_equal(le.classes_, [\"cat\", \"dog\", \"fish\"])\n    transformed = le.transform([\"cat\", \"fish\", \"dog\", \"dog\"])\n    assert_array_equal(transformed, [0, 2, 1, 1])\n    inverse_transformed = le.inverse_transform([0, 2, 1, 1])\n    assert_array_equal(inverse_transformed, [\"cat\", \"fish\", \"dog\", \"dog\"])\n\n    # Test unseen labels\n    assert_raises(ValueError, le.transform, [\"bird\"])\n\n    # Test empty array\n    transformed_empty = le.transform([])\n    assert_array_equal(transformed_empty, np.array([]))\n    inverse_transformed_empty = le.inverse_transform([])\n    assert_array_equal(inverse_transformed_empty, np.array([]))\n"], "sample_740": ["def test_check_non_negative():\n    # Test the check_non_negative function with various inputs\n    X_positive = np.array([[1, 2], [3, 4]])\n    X_negative = np.array([[1, -2], [3, 4]])\n    X_mixed = np.array([[1, -2], [3, -4]])\n    X_sparse_positive = sp.csr_matrix([[0, 1], [1, 0]])\n    X_sparse_negative = sp.csr_matrix([[0, -1], [1, 0]])\n\n    # These should pass without errors\n    assert_no_warnings(check_non_negative, X_positive, \"X_positive\")\n    assert_no_warnings(check_non_negative, X_sparse_positive, \"X_sparse_positive\")\n\n    # These should raise ValueError\n    assert_raises(ValueError, check_non_negative, X_negative, \"X_negative\")\n    assert_raises(ValueError, check_non_negative, X_mixed, \"X_mixed\")\n    assert_raises(ValueError, check_non_negative, X_sparse_negative, \"X_sparse_negative\")\n", "def test_check_X_y():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # Test with sparse matrix\n    X_sparse = sp.csr_matrix(X)\n    X_checked, y_checked = check_X_y(X_sparse, y, accept_sparse=True)\n    assert_array_equal(X_checked.toarray(), X)\n    assert_array_equal(y_checked, y)\n\n    # Test with inconsistent length\n    y_inconsistent = np.array([1, 2])\n    assert_raises(ValueError, check_X_y, X, y_inconsistent)\n\n    # Test with multi-output\n    y_multi = np.array([[1, 2], [3, 4], [5, 6]])\n    X_checked, y_checked = check_X_y(X, y_multi, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y_multi)\n\n    # Test y with NaN\n    y_nan = np.array([1, np.nan, 3])\n    assert_raises(ValueError, check_X_y, X, y_nan)\n\n    # Test y with inf\n    y_inf = np.array([1, np.inf, 3])\n    assert_raises(ValueError, check_X_y, X, y_inf)\n\n    # Test y with numeric conversion\n    y_str = np.array(['1', '2', '3'])\n    X_checked, y_checked = check_X_y(X, y_str, y_numeric=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, np.array([1, 2, 3], dtype=np.float64))\n", "def test_check_is_fitted_with_custom_msg():\n    # Test check_is_fitted with custom message\n    class MyEstimator:\n            self.is_fitted_ = False\n\n            self.is_fitted_ = True\n\n    est = MyEstimator()\n    msg = \"My custom error message for %(name)s\"\n\n    # Before fitting\n    with pytest.raises(NotFittedError, match=msg % {'name': 'MyEstimator'}):\n        check_is_fitted(est, 'is_fitted_', msg=msg)\n    \n    # After fitting\n    est.fit(np.array([[1], [2], [3]]), np.array([1, 2, 3]))\n    assert check_is_fitted(est, 'is_fitted_', msg=msg) is None\n", "def test_check_random_state():\n    # Test that check_random_state returns a RandomState instance\n    random_state = check_random_state(None)\n    assert_true(isinstance(random_state, np.random.RandomState))\n    \n    random_state = check_random_state(42)\n    assert_true(isinstance(random_state, np.random.RandomState))\n    assert_equal(random_state.randint(1000), 7270)\n    \n    random_state_instance = np.random.RandomState(42)\n    random_state = check_random_state(random_state_instance)\n    assert_true(isinstance(random_state, np.random.RandomState))\n    assert_equal(random_state.randint(1000), 7270)\n    \n    # Test that invalid inputs raise ValueError\n    assert_raises(ValueError, check_random_state, \"invalid_input\")\n", "def test_check_is_fitted_custom_message():\n    # Test custom message in check_is_fitted\n    class CustomEstimator:\n            self.is_fitted_ = True\n    \n    est = CustomEstimator()\n    \n    # Test before fitting\n    custom_message = \"CustomEstimator instance is not fitted yet. Please call 'fit'.\"\n    with pytest.raises(NotFittedError, match=custom_message):\n        check_is_fitted(est, \"is_fitted_\", msg=custom_message)\n    \n    # Test after fitting\n    est.fit(np.array([[1, 2], [3, 4]]), np.array([1, 0]))\n    assert check_is_fitted(est, \"is_fitted_\", msg=custom_message) is None\n"], "sample_741": ["def test_parameter_sampler_distribution():\n    # Test that ParameterSampler samples from distributions correctly\n    param_distributions = {\"param1\": expon(scale=5), \"param2\": uniform(0, 1)}\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=100, random_state=0)\n    samples = [x for x in sampler]\n    assert_equal(len(samples), 100)\n    for sample in samples:\n        assert_true(0 <= sample[\"param2\"] <= 1)\n        assert_true(sample[\"param1\"] >= 0)\n\n    # Check that the distribution of samples matches the expected distribution\n    param1_samples = [sample[\"param1\"] for sample in samples]\n    param2_samples = [sample[\"param2\"] for sample in samples]\n\n    from scipy.stats import kstest\n\n    # Perform Kolmogorov-Smirnov test for the distributions\n    d_statistic, p_value = kstest(param1_samples, 'expon', args=(0, 5))\n    assert_true(p_value > 0.05)\n\n    d_statistic, p_value = kstest(param2_samples, 'uniform', args=(0, 1))\n    assert_true(p_value > 0.05)\n", "def test_randomized_search_with_list_distributions():\n    # Test RandomizedSearchCV with parameter distributions given as lists\n    X, y = make_classification(n_samples=100, n_features=4, random_state=42)\n\n    param_distributions = {\n        'C': [0.1, 1, 10],\n        'kernel': ['linear', 'rbf'],\n        'degree': [2, 3, 4]\n    }\n\n    search = RandomizedSearchCV(SVC(gamma='scale'), param_distributions=param_distributions, n_iter=5, random_state=0)\n    search.fit(X, y)\n    \n    # Check if all sampled parameters are from the given lists\n    for params in search.cv_results_['params']:\n        assert_true(params['C'] in param_distributions['C'])\n        assert_true(params['kernel'] in param_distributions['kernel'])\n        assert_true(params['degree'] in param_distributions['degree'])\n\n    # Check if the results have the correct shape\n    assert_equal(search.cv_results_['mean_test_score'].shape[0], 5)\n    assert_equal(search.cv_results_['std_test_score'].shape[0], 5)\n\n    # Check if best_params_ is one of the sampled parameters\n    assert_true(search.best_params_ in search.cv_results_['params'])\n", "def test_grid_search_with_multiple_scorers():\n    # Test GridSearchCV with multiple scorers\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n    scoring = {'accuracy': make_scorer(accuracy_score), 'f1': make_scorer(f1_score)}\n    clf = SVC(gamma='scale')\n    param_grid = {'C': [0.1, 1, 10]}\n\n    grid_search = GridSearchCV(clf, param_grid, scoring=scoring, refit='f1')\n    grid_search.fit(X, y)\n\n    # Ensure that the best_estimator_ is set correctly\n    best_estimator = grid_search.best_estimator_\n    assert_true(hasattr(best_estimator, 'predict'))\n\n    # Ensure that the best score and best params are recorded correctly\n    assert_true(grid_search.best_score_ is not None)\n    assert_true(grid_search.best_params_['C'] in [0.1, 1, 10])\n\n    # Ensure that the cv_results_ keys contain the appropriate keys for multiple scorers\n    assert_true('mean_test_accuracy' in grid_search.cv_results_)\n    assert_true('std_test_accuracy' in grid_search.cv_results_)\n    assert_true('mean_test_f1' in grid_search.cv_results_)\n    assert_true('std_test_f1' in grid_search.cv_results_)\n\n    # Check that the scorer used for refit is the one specified\n    assert_equal(grid_search.refit, 'f1')\n    assert_almost_equal(grid_search.best_score_, max(grid_search.cv_results_['mean_test_f1']))\n", "def test_grid_search_with_duplicate_param_grid():\n    # Test that GridSearchCV handles duplicate parameter values in param_grid\n    X, y = make_classification(n_samples=100, n_features=4, random_state=42)\n\n    param_grid = {'C': [1, 1, 1]}\n    clf = SVC()\n\n    grid_search = GridSearchCV(clf, param_grid)\n    grid_search.fit(X, y)\n\n    # Check that the duplicate parameter values are handled correctly\n    assert_equal(grid_search.best_params_['C'], 1)\n    assert_equal(len(grid_search.cv_results_['params']), 1)\n    assert_almost_equal(grid_search.cv_results_['mean_test_score'][0], grid_search.best_score_)\n", "def test_parameter_sampler_with_distributions():\n    # Test that ParameterSampler works with distributions\n    param_distributions = {\"a\": [1, 2, 3], \"b\": expon(), \"c\": uniform(0, 1)}\n    sampler = ParameterSampler(param_distributions=param_distributions, n_iter=5, random_state=0)\n\n    samples = list(sampler)\n    assert_equal(len(samples), 5)\n    \n    for sample in samples:\n        assert_true(sample[\"a\"] in [1, 2, 3])\n        assert_true(0 <= sample[\"c\"] <= 1)\n        assert_true(isinstance(sample[\"b\"], float))\n\n    # Ensure that repeated calls yield identical parameters with same random_state\n    sampler1 = ParameterSampler(param_distributions=param_distributions, n_iter=5, random_state=0)\n    sampler2 = ParameterSampler(param_distributions=param_distributions, n_iter=5, random_state=0)\n    assert_equal(list(sampler1), list(sampler2))\n"], "sample_742": ["def test_logistic_regression_path_parameters():\n    # Test logistic_regression_path with different parameter configurations\n    X, y = make_classification(n_samples=20, n_features=5, random_state=0)\n    \n    # Test with different solvers\n    for solver in ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga']:\n        coefs, Cs, n_iter = logistic_regression_path(\n            X, y, Cs=5, fit_intercept=True, max_iter=100, tol=1e-4,\n            solver=solver, verbose=0, class_weight=None, dual=False,\n            penalty='l2', intercept_scaling=1., multi_class='ovr',\n            random_state=42, check_input=True, sample_weight=None)\n        assert_equal(len(coefs), 5)\n        assert_equal(len(Cs), 5)\n        assert_equal(len(n_iter), 5)\n\n    # Test with class_weight='balanced'\n    coefs, Cs, n_iter = logistic_regression_path(\n        X, y, Cs=5, fit_intercept=True, max_iter=100, tol=1e-4,\n        solver='lbfgs', verbose=0, class_weight='balanced', dual=False,\n        penalty='l2', intercept_scaling=1., multi_class='ovr',\n        random_state=42, check_input=True, sample_weight=None)\n    assert_equal(len(coefs), 5)\n    assert_equal(len(Cs), 5)\n    assert_equal(len(n_iter), 5)\n\n    # Test with sample weights\n    sample_weight = np.ones(len(y))\n    coefs, Cs, n_iter = logistic_regression_path(\n        X, y, Cs=5, fit_intercept=True, max_iter=100, tol=1e-4,\n        solver='lbfgs', verbose=0, class_weight=None, dual=False,\n        penalty='l2', intercept_scaling=1., multi_class='ovr',\n        random_state=42, check_input=True, sample_weight=sample_weight)\n    assert_equal(len(coefs), 5)\n    assert_equal(len(Cs), 5)\n    assert_equal(len(n_iter), 5)\n\n    # Test with multi_class='multinomial' and solver='saga'\n    coefs, Cs, n_iter = logistic_regression_path(\n        X, y, Cs=5, fit_intercept=True, max", "def test_logistic_regression_path_multinomial_convergence_fail():\n    # Test that logistic_regression_path raises a warning if it fails to converge\n    rng = np.random.RandomState(0)\n    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))\n    y = np.array([1] * 100 + [-1] * 100)\n    Cs = [1e3]\n\n    for solver in ['lbfgs', 'newton-cg']:\n        assert_warns_message(ConvergenceWarning, \"failed to converge\",\n                             logistic_regression_path, X, y, Cs=Cs, tol=0.,\n                             max_iter=1, solver=solver, multi_class='multinomial')\n", "def test_logistic_regression_path_multinomial():\n    # Test logistic_regression_path with multinomial loss\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3, random_state=0)\n    Cs = np.logspace(-4, 4, 10)\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=Cs, solver='lbfgs', multi_class='multinomial', tol=1e-5, max_iter=200)\n    \n    assert_equal(len(coefs), len(Cs))\n    assert_equal(len(n_iter), len(Cs))\n\n    for coef in coefs:\n        assert_equal(coef.shape, (3, X.shape[1] + 1))  # 3 classes, 21 features including intercept\n\n    for i, C in enumerate(Cs):\n        lr = LogisticRegression(C=C, solver='lbfgs', multi_class='multinomial', tol=1e-5, max_iter=200)\n        lr.fit(X, y)\n        assert_array_almost_equal(lr.coef_, coefs[i][:, :-1], decimal=4)\n        assert_array_almost_equal(lr.intercept_, coefs[i][:, -1], decimal=4)\n", "def test_logistic_regression_path_class_weight():\n    # Test logistic_regression_path with class weights\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3, \n                               n_clusters_per_class=1, weights=[0.1, 0.2, 0.7], \n                               random_state=42)\n    class_weights = {0: 1, 1: 2, 2: 3}\n    \n    coefs, Cs, n_iter = logistic_regression_path(X, y, class_weight=class_weights,\n                                                 solver='lbfgs', fit_intercept=True)\n    \n    lr = LogisticRegression(class_weight=class_weights, solver='lbfgs', fit_intercept=True)\n    lr.fit(X, y)\n    \n    assert_array_almost_equal(lr.coef_, coefs[-1][:, :-1], decimal=4)\n    assert_array_almost_equal(lr.intercept_, coefs[-1][:, -1], decimal=4)\n\n    # Test that class weights are correctly applied to the sample weights\n    sample_weight = np.ones(y.shape[0])\n    sample_weight[y == 1] *= 2\n    sample_weight[y == 2] *= 3\n    \n    coefs_sw, Cs_sw, n_iter_sw = logistic_regression_path(X, y, sample_weight=sample_weight, \n                                                          solver='lbfgs', fit_intercept=True)\n    \n    assert_array_almost_equal(coefs[-1], coefs_sw[-1], decimal=4)\n", "def test_logistic_loss():\n    # Test that logistic loss implementation is correct\n    X_ref, y = make_classification(n_samples=20, random_state=0)\n    n_features = X_ref.shape[1]\n\n    X_sp = X_ref.copy()\n    X_sp[X_sp < .1] = 0\n    X_sp = sp.csr_matrix(X_sp)\n\n    w = np.zeros(n_features)\n    for X in (X_ref, X_sp):\n        # Compute the loss using the helper function\n        loss = _logistic_loss(w, X, y, alpha=1.)\n\n        # Manually compute the logistic loss for comparison\n        yz = y * safe_sparse_dot(X, w)\n        sample_weight = np.ones(y.shape[0])\n        expected_loss = -np.sum(sample_weight * log_logistic(yz)) + 0.5 * np.dot(w, w)\n\n        assert_almost_equal(loss, expected_loss)\n\n        # Test that the loss is consistent with the one including intercept\n        w_intercept = np.zeros(n_features + 1)\n        loss_intercept = _logistic_loss(w_intercept, X, y, alpha=1.)\n        assert_almost_equal(loss, loss_intercept)\n\n        # Ensure that the loss is positive\n        assert_greater(loss, 0)\n"], "sample_743": ["def test_neighbors_base_check_algorithm_metric():\n    # Test NeighborsBase._check_algorithm_metric for various scenarios\n    class TestNeighborsBase(NeighborsBase):\n            super().__init__(**kwargs)\n\n            return self\n\n    # Valid algorithm-metric combinations\n    valid_combinations = [\n        ('ball_tree', 'minkowski'),\n        ('kd_tree', 'minkowski'),\n        ('brute', 'cosine'),\n        ('auto', 'euclidean'),\n        ('auto', 'precomputed'),\n        ('auto', lambda x, y: np.sqrt(np.sum((x - y) ** 2)))  # callable metric\n    ]\n\n    for algorithm, metric in valid_combinations:\n        model = TestNeighborsBase(algorithm=algorithm, metric=metric)\n        model._check_algorithm_metric()\n\n    # Invalid algorithm-metric combinations\n    invalid_combinations = [\n        ('kd_tree', 'precomputed'),\n        ('kd_tree', lambda x, y: np.sqrt(np.sum((x - y) ** 2))),  # callable metric\n        ('ball_tree', 'invalid_metric'),\n        ('brute', 'invalid_metric'),\n        ('auto', 'invalid_metric'),\n        ('invalid_algorithm', 'minkowski')\n    ]\n\n    for algorithm, metric in invalid_combinations:\n        model = TestNeighborsBase(algorithm=algorithm, metric=metric)\n        assert_raises(ValueError, model._check_algorithm_metric)\n\n    # Test warning for metric_params containing 'p' while initializing with 'p'\n    with assert_warns(SyntaxWarning):\n        model = TestNeighborsBase(algorithm='ball_tree', metric='minkowski', p=2, metric_params={'p': 3})\n        model._check_algorithm_metric()\n\n    # Test for minkowski metric with invalid p value\n    assert_raises(ValueError, TestNeighborsBase, algorithm='ball_tree', metric='minkowski', p=0.5)\n", "def test_check_weights():\n    # Test the _check_weights function for different scenarios\n    assert_equal(_check_weights(None), None)\n    assert_equal(_check_weights('uniform'), 'uniform')\n    assert_equal(_check_weights('distance'), 'distance')\n    \n        return dist\n    \n    assert_equal(_check_weights(custom_weights), custom_weights)\n    \n    with assert_raises(ValueError):\n        _check_weights('invalid')\n", "def test_check_weights():\n    # Test the _check_weights function for valid and invalid inputs\n    valid_weights = [None, 'uniform', 'distance', lambda x: x]\n    for weight in valid_weights:\n        assert_equal(_check_weights(weight), weight)\n\n    invalid_weights = ['invalid', 123, {}, []]\n    for weight in invalid_weights:\n        assert_raises(ValueError, _check_weights, weight)\n", "def test_invalid_weights():\n    # Test if an invalid weight string raises the appropriate error\n    X = np.array([[0], [1], [2]])\n    y = np.array([0, 1, 2])\n    \n    knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n    \n    assert_raises(ValueError, knn.set_params, weights=\"invalid_weight\")\n    \n    with assert_raises(ValueError):\n        knn.set_params(weights=\"invalid_weight\").fit(X, y)\n", "def test_neighborsbase_init():\n    # Test initialization of NeighborsBase class\n    class DummyNeighbors(NeighborsBase):\n                     algorithm='auto', leaf_size=30, metric='minkowski',\n                     p=2, metric_params=None, n_jobs=1):\n            super().__init__(n_neighbors, radius, algorithm, leaf_size, metric,\n                             p, metric_params, n_jobs)\n\n            return self._fit(X)\n\n    dummy = DummyNeighbors(n_neighbors=3, radius=1.5, algorithm='ball_tree',\n                           leaf_size=50, metric='euclidean', p=2, n_jobs=2)\n    \n    assert_equal(dummy.n_neighbors, 3)\n    assert_equal(dummy.radius, 1.5)\n    assert_equal(dummy.algorithm, 'ball_tree')\n    assert_equal(dummy.leaf_size, 50)\n    assert_equal(dummy.metric, 'euclidean')\n    assert_equal(dummy.p, 2)\n    assert_equal(dummy.n_jobs, 2)\n    assert_true(dummy.effective_metric_ is None)\n    assert_true(dummy.effective_metric_params_ is None)\n\n    # Test valid metric parameters warning\n    metric_params = {'p': 3}\n    dummy = DummyNeighbors(metric='minkowski', metric_params=metric_params)\n    with assert_warns(SyntaxWarning):\n        dummy._check_algorithm_metric()\n\n    # Test invalid algorithm\n    with assert_raises(ValueError):\n        DummyNeighbors(algorithm='invalid')._check_algorithm_metric()\n\n    # Test invalid metric\n    with assert_raises(ValueError):\n        DummyNeighbors(metric='invalid')._check_algorithm_metric()\n\n    # Test invalid p for minkowski metric\n    with assert_raises(ValueError):\n        DummyNeighbors(metric='minkowski', p=0.5)._check_algorithm_metric()\n"], "sample_744": ["def test_minmax_scale_1d_all_zeros():\n    # Test minmax_scale on 1d array that contains all zeros\n    X = np.zeros(5)\n    X_scaled = minmax_scale(X)\n    assert_array_almost_equal(X_scaled, np.zeros(5))\n    \n    # Test minmax_scale with feature_range=(1, 2)\n    X_scaled = minmax_scale(X, feature_range=(1, 2))\n    assert_array_almost_equal(X_scaled, np.ones(5))\n    \n    # Test minmax_scale with feature_range=(-1, 1)\n    X_scaled = minmax_scale(X, feature_range=(-1, 1))\n    assert_array_almost_equal(X_scaled, -np.ones(5))\n", "def test_minmax_scale_function_interface():\n    # Test the function interface of minmax_scale\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n    # default feature_range=(0, 1)\n    X_scaled = minmax_scale(X)\n    expected = np.array([[0, 0, 0], [0.5, 0.5, 0.5], [1, 1, 1]])\n    assert_array_almost_equal(X_scaled, expected)\n\n    # feature_range=(1, 2)\n    X_scaled = minmax_scale(X, feature_range=(1, 2))\n    expected = np.array([[1, 1, 1], [1.5, 1.5, 1.5], [2, 2, 2]])\n    assert_array_almost_equal(X_scaled, expected)\n\n    # feature_range=(-1, 1)\n    X_scaled = minmax_scale(X, feature_range=(-1, 1))\n    expected = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n    assert_array_almost_equal(X_scaled, expected)\n\n    # Test with axis=1\n    X_scaled = minmax_scale(X, axis=1)\n    expected = np.array([[0, 0.5, 1], [0, 0.5, 1], [0, 0.5, 1]])\n    assert_array_almost_equal(X_scaled, expected)\n\n    # Test with copy=False\n    X_copy = X.copy()\n    X_scaled = minmax_scale(X_copy, copy=False)\n    assert_true(X_scaled is X_copy)\n    assert_array_almost_equal(X_copy, expected)\n\n    # Test input validation\n    X_invalid = np.array([[np.nan, 2, 3], [4, 5, np.inf]])\n    with pytest.raises(ValueError):\n        minmax_scale(X_invalid)\n", "def test_standard_scaler_sparse_std():\n    # Test that StandardScaler correctly handles sparse input with std scaling\n    rng = np.random.RandomState(42)\n    n_features = 5\n    n_samples = 10\n    X_dense = rng.randn(n_samples, n_features)\n    X_csr = sparse.csr_matrix(X_dense)\n    X_csc = sparse.csc_matrix(X_dense)\n    \n    scaler = StandardScaler(with_mean=False, with_std=True)\n\n    # Check that the scaler works correctly with CSR matrix\n    X_csr_scaled = scaler.fit_transform(X_csr)\n    assert isinstance(X_csr_scaled, sparse.csr_matrix)\n    assert_array_almost_equal(X_csr_scaled.mean(axis=0).A1, np.zeros(n_features))\n    assert_array_almost_equal(X_csr_scaled.std(axis=0).A1, np.ones(n_features))\n\n    # Check that the scaler works correctly with CSC matrix\n    X_csc_scaled = scaler.fit_transform(X_csc)\n    assert isinstance(X_csc_scaled, sparse.csc_matrix)\n    assert_array_almost_equal(X_csc_scaled.mean(axis=0).A1, np.zeros(n_features))\n    assert_array_almost_equal(X_csc_scaled.std(axis=0).A1, np.ones(n_features))\n\n    # Check inverse transform\n    X_csr_scaled_back = scaler.inverse_transform(X_csr_scaled)\n    X_csc_scaled_back = scaler.inverse_transform(X_csc_scaled)\n    assert_array_almost_equal(X_csr_scaled_back.toarray(), X_dense)\n    assert_array_almost_equal(X_csc_scaled_back.toarray(), X_dense)\n", "def test_minmax_scaler_with_nan():\n    # Test MinMaxScaler with NaN values\n    X = np.array([[np.nan, 1], [1, 2], [2, np.nan], [0, 0]])\n    scaler = MinMaxScaler()\n\n    # Fit transform with NaNs should not raise an error\n    X_trans = scaler.fit_transform(X)\n    \n    # Ensure NaNs are preserved in transformation\n    assert np.isnan(X_trans[0, 0])\n    assert np.isnan(X_trans[2, 1])\n\n    # Ensure the non-NaN values are properly scaled\n    assert_array_almost_equal(X_trans[1:, 0], [0.5, 1.0, 0.0], decimal=2)\n    assert_array_almost_equal(X_trans[1:, 1], [0.5, np.nan, 0.0], decimal=2)\n\n    # Test inverse transformation\n    X_inv = scaler.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv[1:, 0], [1, 2, 0], decimal=2)\n    assert_array_almost_equal(X_inv[1:, 1], [2, np.nan, 0], decimal=2)\n    assert np.isnan(X_inv[0, 0])\n    assert np.isnan(X_inv[2, 1])\n\n    # Test partial_fit with NaNs\n    scaler = MinMaxScaler()\n    for batch in gen_batches(X.shape[0], 2):\n        scaler.partial_fit(X[batch])\n    \n    X_partial_trans = scaler.transform(X)\n    assert np.isnan(X_partial_trans[0, 0])\n    assert np.isnan(X_partial_trans[2, 1])\n    assert_array_almost_equal(X_partial_trans[1:, 0], [0.5, 1.0, 0.0], decimal=2)\n    assert_array_almost_equal(X_partial_trans[1:, 1], [0.5, np.nan, 0.0], decimal=2)\n\n    X_partial_inv = scaler.inverse_transform(X_partial_trans)\n    assert_array_almost_equal(X_partial_inv[1:, 0], [1, 2, 0], decimal=2)\n    assert_array_almost_equal(X_partial_inv[1:, 1], [2, np.nan, 0], decimal=2)\n    assert np.isnan(X_partial_inv[0, 0])\n    assert np", "def test_scale_sparse_input():\n    rng = np.random.RandomState(42)\n    X_dense = rng.randn(4, 5)\n    X_dense[:, 0] = 0.0  # First feature is always zero\n    X_sparse_csr = sparse.csr_matrix(X_dense)\n    X_sparse_csc = sparse.csc_matrix(X_dense)\n\n    # Test scale with sparse input (csr and csc)\n    for X_sparse in [X_sparse_csr, X_sparse_csc]:\n        X_scaled = scale(X_sparse, with_mean=False)\n        assert_false(np.any(np.isnan(X_scaled.data)))\n        assert_array_almost_equal(X_scaled.mean(axis=0).A.flatten(), [0., -0.01, 2.24, -0.35, -0.78], 2)\n        assert_array_almost_equal(X_scaled.std(axis=0).A.flatten(), [0., 1., 1., 1., 1.])\n\n        # Check that scaling without centering doesn't alter sparsity\n        assert_true(sparse.isspmatrix_csr(X_scaled) or sparse.isspmatrix_csc(X_scaled))\n\n    # Check that centering a sparse matrix raises an error\n    assert_raises(ValueError, scale, X_sparse_csr, with_mean=True)\n    assert_raises(ValueError, scale, X_sparse_csc, with_mean=True)\n"], "sample_745": ["def test_no_validation():\n    X = np.random.randn(5, 2)\n    transformer = FunctionTransformer(validate=False)\n\n    # Test that transform works without validation\n    assert_array_equal(transformer.transform(X), X)\n\n    # Test that inverse_transform works without validation\n    assert_array_equal(transformer.inverse_transform(X), X)\n\n    # Ensure accept_sparse has no effect when validate is False\n    sparse_X = sparse.csr_matrix(X)\n    transformer_sparse = FunctionTransformer(validate=False, accept_sparse=True)\n    assert_array_equal(transformer_sparse.transform(sparse_X).toarray(), X)\n    assert_array_equal(transformer_sparse.inverse_transform(sparse_X).toarray(), X)\n", "def test_accept_sparse():\n    X_dense = np.array([[1, 2], [3, 4]])\n    X_sparse = sparse.csr_matrix(X_dense)\n    \n    # Test that sparse matrix is handled correctly\n    transformer = FunctionTransformer(func=np.sqrt, accept_sparse=True)\n    transformed_sparse = transformer.transform(X_sparse)\n    assert_allclose_dense_sparse(transformed_sparse, np.sqrt(X_sparse))\n    \n    # Test that sparse matrix causes error if accept_sparse is False\n    transformer = FunctionTransformer(func=np.sqrt, accept_sparse=False)\n    with pytest.raises(TypeError):\n        transformer.transform(X_sparse)\n", "def test_transform_with_custom_function():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    \n        return X * 2\n    \n    transformer = FunctionTransformer(func=custom_func)\n    X_transformed = transformer.transform(X)\n    \n    assert_array_equal(\n        X_transformed,\n        X * 2,\n        err_msg='transform should have doubled the input values'\n    )\n", "def test_pass_y_deprecation_warning():\n    X = np.arange(10).reshape((5, 2))\n    y = np.arange(5)\n    transformer = FunctionTransformer(func=lambda X, y: X + y[:, None], pass_y=True)\n    \n    with pytest.warns(DeprecationWarning, match=\"pass_y is deprecated\"):\n        transformed = transformer.transform(X, y)\n    \n    expected_transformed = X + y[:, None]\n    assert_array_equal(transformed, expected_transformed)\n", "def test_custom_func():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n\n    # Define a custom function that scales the input\n        return X * 2\n\n    transformer = FunctionTransformer(func=custom_func)\n    transformed_X = transformer.transform(X)\n\n    # Check if the custom function correctly transformed the input\n    assert_array_equal(transformed_X, X * 2, \n                       err_msg='Custom function should have scaled the input by 2')\n\n    # Check if the inverse function works correctly\n    transformer = FunctionTransformer(func=custom_func, inverse_func=custom_func)\n    inverse_transformed_X = transformer.inverse_transform(transformed_X)\n    assert_array_equal(inverse_transformed_X, X,\n                       err_msg='Inverse transformation should have returned the original input')\n"], "sample_746": ["def test_balanced_accuracy_score():\n    # Test balanced accuracy score for binary classification\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 0, 0, 1]\n    \n    # True negatives: 1, False negatives: 1, True positives: 1, False positives: 1\n    # Sensitivity (Recall): 1 / (1 + 1) = 0.5\n    # Specificity: 1 / (1 + 1) = 0.5\n    # Balanced accuracy: (Sensitivity + Specificity) / 2 = 0.5\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.5)\n\n    y_true = [0, 1, 0, 1, 0, 1]\n    y_pred = [0, 1, 0, 1, 0, 0]\n\n    # True negatives: 3, False negatives: 1, True positives: 2, False positives: 0\n    # Sensitivity (Recall): 2 / (2 + 1) = 0.6667\n    # Specificity: 3 / (3 + 0) = 1.0\n    # Balanced accuracy: (Sensitivity + Specificity) / 2 = 0.8333\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.8333, decimal=4)\n\n    # Test sample_weight parameter\n    sample_weight = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n\n    # With sample weights, the balanced accuracy should remain the same\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight), 0.8333, decimal=4)\n\n    # Test exception for non-binary targets\n    y_true_multiclass = [0, 1, 2, 1]\n    y_pred_multiclass = [0, 2, 1, 1]\n    assert_raises(ValueError, balanced_accuracy_score, y_true_multiclass, y_pred_multiclass)\n", "def test_balanced_accuracy_score():\n    # Test balanced accuracy score for binary classification\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Compute balanced accuracy manually\n    cm = confusion_matrix(y_true, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    sensitivity = tp / (tp + fn)\n    specificity = tn / (tn + fp)\n    true_balanced_accuracy = (sensitivity + specificity) / 2\n\n    # Compare with balanced_accuracy_score function\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), true_balanced_accuracy)\n\n    # Test with perfect predictions\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 1, 0, 1]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 1.0)\n\n    # Test with completely wrong predictions\n    y_true = [0, 1, 0, 1]\n    y_pred = [1, 0, 1, 0]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.0)\n\n    # Test with imbalanced dataset\n    y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n    y_pred = [0, 0, 0, 0, 1, 1, 1, 1]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.875)\n", "def test_confusion_matrix_binary_with_sample_weight():\n    # Test confusion matrix - binary classification case with sample weights\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Assign weights to the samples\n    sample_weight = np.ones_like(y_true, dtype=np.float64)\n    sample_weight[0] = 0.5\n    sample_weight[1] = 0.2\n\n    cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    assert_array_almost_equal(cm, [[21.8, 2.8], [7.8, 17.4]], decimal=1)\n\n    # Ensure the confusion matrix is consistent with manual calculation\n    tp = np.dot((y_true == 1) & (y_pred == 1), sample_weight)\n    tn = np.dot((y_true == 0) & (y_pred == 0), sample_weight)\n    fp = np.dot((y_true == 0) & (y_pred == 1), sample_weight)\n    fn = np.dot((y_true == 1) & (y_pred == 0), sample_weight)\n\n    manual_cm = np.array([[tn, fp], [fn, tp]])\n    assert_array_almost_equal(cm, manual_cm, decimal=1)\n", "def test_confusion_matrix_with_custom_labels():\n    # Test confusion matrix with custom label ordering and label subset\n    y_true = [1, 2, 1, 3, 3, 2]\n    y_pred = [1, 3, 1, 3, 2, 2]\n\n    # Confusion matrix with labels in custom order\n    cm = confusion_matrix(y_true, y_pred, labels=[3, 2, 1])\n    expected_cm = [[1, 1, 0],\n                   [0, 2, 0],\n                   [0, 0, 2]]\n    assert_array_equal(cm, expected_cm)\n\n    # Confusion matrix with a subset of labels\n    cm = confusion_matrix(y_true, y_pred, labels=[1, 3])\n    expected_cm = [[2, 0],\n                   [1, 1]]\n    assert_array_equal(cm, expected_cm)\n\n    # Confusion matrix with labels that are not in y_true or y_pred\n    cm = confusion_matrix(y_true, y_pred, labels=[0, 4])\n    expected_cm = [[0, 0],\n                   [0, 0]]\n    assert_array_equal(cm, expected_cm)\n\n    # Edge case: labels contain extra label not in y_true/y_pred but affects matrix size\n    cm = confusion_matrix(y_true, y_pred, labels=[1, 2, 3, 4])\n    expected_cm = [[2, 0, 0, 0],\n                   [0, 2, 0, 0],\n                   [0, 1, 1, 0],\n                   [0, 0, 0, 0]]\n    assert_array_equal(cm, expected_cm)\n", "def test_balanced_accuracy_score():\n    # Test balanced accuracy score\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.78, 2)\n\n    # Test with balanced dataset\n    y_true = np.array([0, 1, 0, 1])\n    y_pred = np.array([0, 1, 1, 0])\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.5, 2)\n\n    # Test with imbalanced dataset\n    y_true = np.array([0, 0, 0, 1])\n    y_pred = np.array([0, 0, 0, 0])\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.75, 2)\n\n    # Test that balanced accuracy score is 1.0 for perfect predictions\n    assert_almost_equal(balanced_accuracy_score(y_true, y_true), 1.0, 2)\n\n    # Test that an error is raised for non-binary targets\n    y_true = np.array([0, 1, 2, 1])\n    y_pred = np.array([0, 1, 1, 2])\n    assert_raises(ValueError, balanced_accuracy_score, y_true, y_pred)\n"], "sample_747": ["def test_min_max_scaler_on_constant_feature():\n    # Test MinMaxScaler on data with one constant feature\n    X = [[1, 2, 3], [1, 5, 6], [1, 8, 9]]\n    \n    # Test default feature_range (0, 1)\n    scaler = MinMaxScaler()\n    X_trans = scaler.fit_transform(X)\n    X_expected = [[0.0, 0.0, 0.0],\n                  [0.0, 0.5, 0.5],\n                  [0.0, 1.0, 1.0]]\n    assert_array_almost_equal(X_trans, X_expected)\n    \n    # Test inverse transform\n    X_inv_trans = scaler.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_inv_trans)\n    \n    # Test custom feature_range (1, 2)\n    scaler = MinMaxScaler(feature_range=(1, 2))\n    X_trans = scaler.fit_transform(X)\n    X_expected = [[1.0, 1.0, 1.0],\n                  [1.0, 1.5, 1.5],\n                  [1.0, 2.0, 2.0]]\n    assert_array_almost_equal(X_trans, X_expected)\n    \n    # Test inverse transform\n    X_inv_trans = scaler.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_inv_trans)\n", "def test_min_max_scaler_partial_fit_with_nan():\n    # Test if MinMaxScaler works with NaN values during partial_fit\n    X = np.array([[np.nan, 1, 2], [3, 4, np.nan], [5, 6, 7]])\n    chunk_size = 1\n\n    scaler_batch = MinMaxScaler().fit(X)\n    scaler_incr = MinMaxScaler()\n    for batch in gen_batches(X.shape[0], chunk_size):\n        scaler_incr = scaler_incr.partial_fit(X[batch])\n\n    assert_array_almost_equal(scaler_batch.data_min_,\n                              scaler_incr.data_min_)\n    assert_array_almost_equal(scaler_batch.data_max_,\n                              scaler_incr.data_max_)\n    assert_equal(scaler_batch.n_samples_seen_, scaler_incr.n_samples_seen_)\n    assert_array_almost_equal(scaler_batch.data_range_,\n                              scaler_incr.data_range_)\n    assert_array_almost_equal(scaler_batch.scale_, scaler_incr.scale_)\n    assert_array_almost_equal(scaler_batch.min_, scaler_incr.min_)\n", "def test_min_max_scaler_fit_on_empty():\n    # Test MinMaxScaler fitting on an empty array\n    X_empty = np.array([]).reshape(0, n_features)\n    scaler = MinMaxScaler()\n    assert_raises(ValueError, scaler.fit, X_empty)\n", "def test_quantile_transformer_nans():\n    # Check if the transformer correctly handles NaN values\n    X = np.array([[np.nan, 1, 2], [3, 4, np.nan], [5, np.nan, 6], [7, 8, 9]])\n\n    transformer = QuantileTransformer(n_quantiles=10, random_state=0)\n    X_trans = transformer.fit_transform(X)\n    \n    # Ensure that the shape remains the same\n    assert_equal(X.shape, X_trans.shape)\n    \n    # Ensure that NaNs are preserved\n    assert np.isnan(X_trans[0, 0])\n    assert np.isnan(X_trans[1, 2])\n    assert np.isnan(X_trans[2, 1])\n    \n    # Ensure that non-NaN values are transformed correctly\n    X_non_nan = np.array([[1, 2], [4, np.nan], [np.nan, 6], [8, 9]])\n    X_non_nan_trans = transformer.transform(X_non_nan)\n    assert not np.isnan(X_non_nan_trans[0, 0])\n    assert not np.isnan(X_non_nan_trans[0, 1])\n    assert np.isnan(X_non_nan_trans[1, 1])\n    assert np.isnan(X_non_nan_trans[2, 0])\n    assert not np.isnan(X_non_nan_trans[3, 0])\n    assert not np.isnan(X_non_nan_trans[3, 1])\n\n    # Ensure that inverse_transform correctly handles NaNs\n    X_trans_inv = transformer.inverse_transform(X_trans)\n    assert_array_almost_equal(X[np.isnan(X) == False], X_trans_inv[np.isnan(X_trans_inv) == False])\n    assert np.isnan(X_trans_inv[0, 0])\n    assert np.isnan(X_trans_inv[1, 2])\n    assert np.isnan(X_trans_inv[2, 1])\n", "def test_power_transformer_with_pipeline():\n    # Test PowerTransformer within a pipeline\n    from sklearn.linear_model import LinearRegression\n    from sklearn.pipeline import make_pipeline\n\n    X = np.abs(X_2d)\n    y = np.dot(X, np.array([1.5, -2.0, 1.0, 0.5, -0.5, 1.0, -1.0, 0.5, 1.5, -2.0,\n                            1.0, 0.5, -0.5, 1.0, -1.0, 0.5, 1.5, -2.0, 1.0, 0.5,\n                            -0.5, 1.0, -1.0, 0.5, 1.5, -2.0, 1.0, 0.5, -0.5, 1.0]))\n\n    # Create a pipeline with PowerTransformer and LinearRegression\n    pipeline = make_pipeline(PowerTransformer(method='box-cox'), LinearRegression())\n    pipeline.fit(X, y)\n\n    # Ensure the pipeline has fitted correctly and can make predictions\n    y_pred = pipeline.predict(X)\n    assert y_pred.shape == y.shape\n    assert pipeline.score(X, y) > 0.9\n\n    # Test inverse transformation\n    X_trans = pipeline.named_steps['powertransformer'].transform(X)\n    X_inv = pipeline.named_steps['powertransformer'].inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_inv)\n"], "sample_748": ["def test_parameter_sampler_distribution():\n    # Test if the ParameterSampler correctly samples from given distributions\n    param_distributions = {\n        'a': [1, 2, 3],\n        'b': expon(scale=10),\n        'c': ['foo', 'bar'],\n        'd': uniform(0, 1)\n    }\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=5, random_state=42)\n    samples = list(sampler)\n    assert_equal(len(samples), 5)\n    for sample in samples:\n        assert_true(sample['a'] in [1, 2, 3])\n        assert_true(0 <= sample['d'] <= 1)\n        assert_true(sample['c'] in ['foo', 'bar'])\n        assert_true(sample['b'] >= 0)\n\n    # Check that repeated calls with the same random_state yield the same samples\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=5, random_state=42)\n    samples2 = list(sampler)\n    assert_equal(samples, samples2)\n", "def test_parameter_sampler_iterable_check():\n    # Test that all parameter values are iterables\n    param_distributions = {\"param_a\": [1, 2, 3], \"param_b\": [\"a\", \"b\", \"c\"]}\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=5, random_state=0)\n    for sample in sampler:\n        assert_true(isinstance(sample[\"param_a\"], int))\n        assert_true(isinstance(sample[\"param_b\"], str))\n\n    # Test for non-iterable values in param_distributions\n    param_distributions_invalid = {\"param_a\": 1, \"param_b\": [\"a\", \"b\", \"c\"]}\n    assert_raises(TypeError, ParameterSampler, param_distributions_invalid, 5)\n\n    param_distributions_invalid = {\"param_a\": [1, 2, 3], \"param_b\": \"abc\"}\n    assert_raises(TypeError, ParameterSampler, param_distributions_invalid, 5)\n", "def test_parameter_sampler_with_distributions():\n    # Test that the ParameterSampler correctly samples from distributions\n    param_distributions = {\n        'C': expon(scale=10),\n        'kernel': ['linear', 'rbf'],\n        'degree': randint(2, 4)\n    }\n    n_iter = 20\n    sampler = ParameterSampler(param_distributions, n_iter=n_iter, random_state=0)\n    samples = list(sampler)\n    \n    assert_equal(len(samples), n_iter)\n    for sample in samples:\n        assert_true(0 <= sample['C'] <= 10 * 5)  # 5 is a rough upper bound for exp(10)\n        assert_true(sample['kernel'] in ['linear', 'rbf'])\n        assert_true(2 <= sample['degree'] < 4)\n\n    # Check that the sampling is reproducible with the same random_state\n    sampler = ParameterSampler(param_distributions, n_iter=n_iter, random_state=0)\n    samples2 = list(sampler)\n    assert_equal(samples, samples2)\n\n    # Check that different random_state gives different samples\n    sampler = ParameterSampler(param_distributions, n_iter=n_iter, random_state=1)\n    samples3 = list(sampler)\n    assert_not_equal(samples, samples3)\n", "def test_parameter_sampler_invalid_distributions():\n    # Test that invalid parameter distributions raise appropriate errors\n    param_distributions = {\"a\": \"not_a_distribution\"}\n    assert_raises(TypeError, ParameterSampler, param_distributions, n_iter=10)\n\n    param_distributions = {\"a\": 123}\n    assert_raises(TypeError, ParameterSampler, param_distributions, n_iter=10)\n\n    # Ensure that non-iterable objects raise TypeError\n    with pytest.raises(TypeError, match=\"Parameter value is not iterable\"):\n        ParameterSampler({\"a\": 123}, n_iter=10)\n", "def test_parameter_sampler_with_distributions():\n    # Test ParameterSampler with actual distributions\n\n    param_distributions = {\n        'a': expon(scale=10),\n        'b': uniform(0, 1),\n        'c': [0, 1, 2]\n    }\n    \n    sampler = ParameterSampler(param_distributions=param_distributions, n_iter=5, random_state=0)\n    \n    samples = list(sampler)\n    assert_equal(len(samples), 5)\n    \n    for sample in samples:\n        assert_true(0 <= sample['b'] <= 1)\n        assert_true(sample['c'] in [0, 1, 2])\n    \n    # Check that distribution sampling with random state is consistent\n    sampler = ParameterSampler(param_distributions=param_distributions, n_iter=5, random_state=0)\n    samples_1 = list(sampler)\n    \n    sampler = ParameterSampler(param_distributions=param_distributions, n_iter=5, random_state=0)\n    samples_2 = list(sampler)\n    \n    assert_equal(samples_1, samples_2)\n\n    # Check if we can properly handle non-numeric distributions\n    param_distributions = {'a': ['foo', 'bar', 'baz']}\n    sampler = ParameterSampler(param_distributions=param_distributions, n_iter=3, random_state=0)\n    samples = list(sampler)\n    \n    assert_equal(len(samples), 3)\n    for sample in samples:\n        assert_true(sample['a'] in ['foo', 'bar', 'baz'])\n"], "sample_749": ["def test_column_transformer_with_mixed_types():\n    pd = pytest.importorskip('pandas')\n\n    X_df = pd.DataFrame({\n        'col1': ['A', 'B', 'C'],\n        'col2': [1, 2, 3],\n        'col3': [0.1, 0.2, 0.3]\n    })\n\n    from sklearn.preprocessing import OneHotEncoder\n\n    ct = ColumnTransformer(\n        [('cat', OneHotEncoder(), 'col1'),\n         ('num', StandardScaler(), ['col2', 'col3'])],\n        remainder='drop'\n    )\n\n    X_trans = ct.fit_transform(X_df)\n\n    expected_output = np.array([\n        [1.0, 0.0, 0.0, -1.22474487, -1.22474487],\n        [0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 1.22474487, 1.22474487]\n    ])\n\n    assert_array_almost_equal(X_trans, expected_output, decimal=5)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n", "def test_column_transformer_mixed_transformer_types():\n    X_array = np.array([[0, 1, 2, 3], [2, 4, 6, 8]]).T\n\n    # Defining a case where different types of transformers are used\n    ct = ColumnTransformer([\n        ('scaler', StandardScaler(), [0, 1]),\n        ('double', DoubleTrans(), [2]),\n        ('dropper', 'drop', [3]),\n    ], remainder='passthrough')\n\n    X_trans = ct.fit_transform(X_array)\n\n    # Expected result\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_array[:, [0, 1]])\n    X_doubled = 2 * X_array[:, 2].reshape(-1, 1)\n    X_expected = np.hstack([X_scaled, X_doubled])\n\n    assert_array_equal(X_trans, X_expected)\n    assert len(ct.transformers_) == 4  # 3 transformers + 1 remainder\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'passthrough'\n    assert_array_equal(ct.transformers_[-1][2], [3])\n", "def test_column_transformer_combination_different_transformers():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [1, 3, 5]]).T\n\n    # Use a combination of StandardScaler and Normalizer\n    ct = ColumnTransformer([\n        ('scaler', StandardScaler(), [0]),\n        ('norm', Normalizer(), [1])\n    ], remainder='passthrough')\n\n    X_trans = ct.fit_transform(X_array)\n    scaler = StandardScaler().fit(X_array[:, 0].reshape(-1, 1))\n    norm = Normalizer().fit(X_array[:, 1].reshape(-1, 1))\n\n    expected_transformed_first_col = scaler.transform(X_array[:, 0].reshape(-1, 1))\n    expected_transformed_second_col = norm.transform(X_array[:, 1].reshape(-1, 1))\n    expected_transformed_data = np.hstack([\n        expected_transformed_first_col,\n        expected_transformed_second_col,\n        X_array[:, 2].reshape(-1, 1)\n    ])\n\n    assert_allclose_dense_sparse(X_trans, expected_transformed_data)\n    assert len(ct.transformers_) == 3\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'passthrough'\n    assert_array_equal(ct.transformers_[-1][2], [2])\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans', DictVectorizer(), 0)],\n                           remainder=DictVectorizer())\n    # fit the transformer\n    ct.fit(X_array)\n    \n    # check if get_feature_names raises NotImplementedError when using passthrough\n    assert_raise_message(\n        NotImplementedError, 'get_feature_names is not yet supported',\n        ct.get_feature_names)\n\n    # check if get_feature_names works with valid transformers\n    X = np.array([[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}],\n                  [{'c': 5}, {'c': 6}]], dtype=object).T\n    ct = ColumnTransformer(\n        [('col0', DictVectorizer(), 0), ('col1', DictVectorizer(), 1)],\n        remainder=DictVectorizer())\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['col0__a', 'col0__b', 'col1__c'])\n", "def test_column_transformer_mixed_data_types():\n    pd = pytest.importorskip('pandas')\n\n    X_array = np.array([[0, 'a', 2.0], [2, 'b', 6.0]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second', 'third'])\n\n    # Transform numerical and string columns separately\n    ct = ColumnTransformer([('num', StandardScaler(), [0, 2]),\n                            ('cat', DictVectorizer(), [1])], remainder='drop')\n\n    X_res_num = np.array([[0, 2.0], [2, 6.0]]).T\n    X_res_num_scaled = StandardScaler().fit_transform(X_res_num)\n\n    # Transform categorical column with DictVectorizer\n    X_res_cat = np.array([{'second': 'a'}, {'second': 'b'}])\n    X_res_cat_trans = DictVectorizer().fit_transform(X_res_cat).toarray()\n\n    # Expected result after concatenation\n    X_res = np.hstack((X_res_num_scaled, X_res_cat_trans))\n\n    assert_array_equal(ct.fit_transform(X_df), X_res)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n"], "sample_750": ["def test_omp_with_intercept():\n    # Test OMP with fit_intercept=True and fit_intercept=False\n    rng = check_random_state(42)\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    omp_no_intercept = OrthogonalMatchingPursuit(n_nonzero_coefs=5, fit_intercept=False)\n    omp_with_intercept = OrthogonalMatchingPursuit(n_nonzero_coefs=5, fit_intercept=True)\n\n    omp_no_intercept.fit(X, y)\n    omp_with_intercept.fit(X, y)\n\n    assert_equal(omp_no_intercept.intercept_, 0)\n    assert np.isclose(np.mean(y - omp_with_intercept.predict(X)), 0, atol=1e-10)\n", "def test_orthogonal_matching_pursuit_init():\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=5, tol=0.1, fit_intercept=False, normalize=False, precompute=True)\n    assert_equal(omp.n_nonzero_coefs, 5)\n    assert_equal(omp.tol, 0.1)\n    assert_equal(omp.fit_intercept, False)\n    assert_equal(omp.normalize, False)\n    assert_equal(omp.precompute, True)\n\n    omp_default = OrthogonalMatchingPursuit()\n    assert_equal(omp_default.n_nonzero_coefs, None)\n    assert_equal(omp_default.tol, None)\n    assert_equal(omp_default.fit_intercept, True)\n    assert_equal(omp_default.normalize, True)\n    assert_equal(omp_default.precompute, 'auto')\n", "def test_omp_with_intercept():\n    rng = check_random_state(0)\n    n_samples, n_features = 50, 20\n    X = rng.randn(n_samples, n_features)\n    coef = np.zeros(n_features)\n    coef[:5] = rng.randn(5)\n    y = np.dot(X, coef) + 3.0  # Adding a constant intercept term\n\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=5, fit_intercept=True)\n    omp.fit(X, y)\n    assert_true(np.count_nonzero(omp.coef_) <= 5)\n    assert_array_almost_equal(omp.intercept_, 3.0, decimal=1)\n", "def test_omp_different_dtypes():\n    # Test orthogonal_mp with different dtypes\n    y_ = y[:, 0].astype(np.float32)\n    X_ = X.astype(np.float32)\n    assert_true(orthogonal_mp(X_, y_, n_nonzero_coefs=5).dtype == np.float32)\n\n    y_ = y[:, 0].astype(np.float64)\n    X_ = X.astype(np.float64)\n    assert_true(orthogonal_mp(X_, y_, n_nonzero_coefs=5).dtype == np.float64)\n\n    y_ = y[:, 0].astype(np.float32)\n    X_ = X.astype(np.float64)\n    assert_true(orthogonal_mp(X_, y_, n_nonzero_coefs=5).dtype == np.float64)\n\n    y_ = y[:, 0].astype(np.float64)\n    X_ = X.astype(np.float32)\n    assert_true(orthogonal_mp(X_, y_, n_nonzero_coefs=5).dtype == np.float64)\n", "def test_omp_with_multiple_targets():\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\n    omp.fit(X, y)\n    assert_equal(omp.coef_.shape, (n_targets, n_features))\n    assert_equal(omp.intercept_.shape, (n_targets,))\n    for target in range(n_targets):\n        assert_true(np.count_nonzero(omp.coef_[target]) <= n_nonzero_coefs)\n        \n    omp_with_tol = OrthogonalMatchingPursuit(tol=1.0)\n    omp_with_tol.fit(X, y)\n    assert_equal(omp_with_tol.coef_.shape, (n_targets, n_features))\n    assert_equal(omp_with_tol.intercept_.shape, (n_targets,))\n    for target in range(n_targets):\n        assert_true(np.sum((y[:, target] - np.dot(X, omp_with_tol.coef_[target])) ** 2) <= 1.0)\n"], "sample_751": ["def test_random_forest_classifier():\n    # Test RandomForestClassifier on a toy dataset\n    X, y = datasets.make_classification(n_samples=100, n_features=4,\n                                        n_informative=2, n_redundant=0,\n                                        random_state=0, shuffle=False)\n    clf = RandomForestClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n\n    assert_equal(len(np.unique(y_pred)), len(np.unique(y)))\n    assert_greater(clf.score(X, y), 0.9)\n    assert_equal(clf.n_features_, X.shape[1])\n    assert_equal(len(clf.estimators_), clf.n_estimators)\n\n    # Test feature importances\n    feature_importances = clf.feature_importances_\n    assert_equal(len(feature_importances), X.shape[1])\n    assert_greater(feature_importances[0], 0)\n    assert_greater(feature_importances[1], 0)\n", "def test_random_forest_classifier_toy():\n    # Check RandomForestClassifier on a toy dataset.\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n    assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)\n    assert_equal(clf.predict_proba(T).shape, (len(T), 2))\n    assert_equal(clf.score(X, y_class), 1.0)\n", "def test_random_forest_classifier_toy():\n    # Check RandomForestClassifier on a toy dataset.\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n    assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)\n    assert_equal(clf.predict_proba(T).shape, (len(T), 2))\n    assert_equal(clf.predict_log_proba(T).shape, (len(T), 2))\n\n    # Check feature importances\n    importances = clf.feature_importances_\n    assert_equal(importances.shape[0], 2)\n    assert_true(np.all(importances >= 0))\n    assert_true(np.sum(importances) <= 1)\n", "def test_random_forest_classifier_toy():\n    # Check RandomForestClassifier on a toy dataset.\n    clf = RandomForestClassifier(n_estimators=5, random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n    assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)\n    assert_equal(clf.predict_proba(T).shape, (len(T), 2))\n    assert_equal(clf.decision_function(T).shape, (len(T),))\n", "def test_random_forest_classifier():\n    # Check RandomForestClassifier on a toy dataset.\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n    assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)\n    assert_equal(clf.predict_proba(T).shape, (len(T), 2))\n    assert_equal(clf.decision_function(T).shape, (len(T),))\n\n    # Check RandomForestClassifier on the iris dataset\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(iris.data, iris.target)\n    assert_array_equal(np.unique(iris.target), clf.classes_)\n    proba = clf.predict_proba(iris.data)\n    assert_equal(proba.shape[1], len(np.unique(iris.target)))\n    assert_equal(clf.decision_function(iris.data).shape[1], len(np.unique(iris.target)))\n    score = clf.score(iris.data, iris.target)\n    assert score > 0.9, \"Failed with score = %f\" % score\n\n    # Check RandomForestClassifier on the boston dataset\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(boston.data, boston.target)\n    score = clf.score(boston.data, boston.target)\n    assert score > 0.85\n"], "sample_752": ["def test_iforest_contamination():\n    \"\"\"Check the behavior with different contamination levels.\"\"\"\n    X = iris.data\n    y = np.ones(X.shape[0])\n    y[-10:] = -1  # Introduce some outliers\n\n    # Test different contamination levels\n    for contamination in [0.05, 0.1, 0.2, 'auto']:\n        clf = IsolationForest(contamination=contamination, random_state=rng)\n        clf.fit(X)\n        pred = clf.predict(X)\n        decision_func = clf.decision_function(X)\n        \n        # Check if the contamination level is respected in predictions\n        if contamination == 'auto':\n            expected_outliers = 0.1\n        else:\n            expected_outliers = contamination\n        \n        outlier_ratio = np.mean(pred == -1)\n        assert_almost_equal(outlier_ratio, expected_outliers, decimal=1,\n                            err_msg=f\"Failed for contamination level {contamination}\")\n        \n        # Check if decision function is consistent with predictions\n        assert_array_equal(pred, np.where(decision_func < 0, -1, 1))\n", "def test_iforest_bootstrap():\n    \"\"\"Check if bootstrap parameter is working correctly.\"\"\"\n    rng = check_random_state(0)\n    X = iris.data\n    clf = IsolationForest(bootstrap=True, random_state=rng).fit(X)\n    predictions = clf.predict(X)\n    \n    # Check if predictions are made correctly\n    assert len(predictions) == len(X)\n    assert all(pred in [1, -1] for pred in predictions)\n\n    clf_no_bootstrap = IsolationForest(bootstrap=False, random_state=rng).fit(X)\n    predictions_no_bootstrap = clf_no_bootstrap.predict(X)\n\n    # Check if predictions without bootstrap are made correctly\n    assert len(predictions_no_bootstrap) == len(X)\n    assert all(pred in [1, -1] for pred in predictions_no_bootstrap)\n\n    # Ensure predictions with and without bootstrap are consistent\n    assert_array_equal(predictions, predictions_no_bootstrap)\n", "def test_iforest_with_different_contaminations():\n    \"\"\"Test Isolation Forest with different contamination values.\"\"\"\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]\n\n    for contamination in [0.0, 0.05, 0.15, 0.3, 0.5]:\n        clf = IsolationForest(random_state=rng, contamination=contamination)\n        clf.fit(X)\n        pred = clf.predict(X)\n        # Check that contamination parameter is respected\n        n_outliers = sum(pred == -1)\n        expected_outliers = int(len(X) * contamination)\n        assert_equal(n_outliers, expected_outliers, \n                     f\"Expected {expected_outliers} outliers, got {n_outliers}\")\n", "def test_iforest_fit_with_sample_weight():\n    \"\"\"Check if Isolation Forest properly handles sample weights.\"\"\"\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n    sample_weight = np.array([1, 2, 3, 4, 5])\n    \n    # Fit without sample weight\n    clf1 = IsolationForest(random_state=rng)\n    clf1.fit(X)\n\n    # Fit with sample weight\n    clf2 = IsolationForest(random_state=rng)\n    clf2.fit(X, sample_weight=sample_weight)\n    \n    # Check if the model handles sample weight correctly\n    assert clf1.estimators_ != clf2.estimators_, \"Models should differ due to sample weight\"\n    assert clf1.score_samples(X) != clf2.score_samples(X), \"Score samples should differ due to sample weight\"\n", "def test_iforest_fit_predict():\n    # Check fit and predict methods of IsolationForest on a toy dataset\n    X = np.array([[1, 1], [2, 2], [2, 1], [3, 3], [4, 4], [0, 0]])\n    clf = IsolationForest(random_state=rng, contamination=0.25)\n    clf.fit(X)\n    predictions = clf.predict(X)\n    expected_predictions = np.array([1, 1, 1, 1, -1, 1])  # Last sample is expected to be an outlier\n\n    assert_array_equal(predictions, expected_predictions)\n\n    # Check if the decision function is consistent with the predict method\n    decision_scores = clf.decision_function(X)\n    pred_from_decision = np.where(decision_scores < 0, -1, 1)\n    assert_array_equal(predictions, pred_from_decision)\n"], "sample_753": ["def test_logistic_regression_path():\n    # Test logistic_regression_path on synthetic dataset\n    X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=0)\n    Cs = [1e-4, 1e-2, 1, 100]\n    \n    coefs, Cs_, n_iter = logistic_regression_path(X, y, Cs=Cs, fit_intercept=True, max_iter=100, tol=1e-4, solver='lbfgs')\n    assert_array_equal(Cs, Cs_)\n    assert_equal(len(coefs), len(Cs))\n    assert_equal(len(n_iter), len(Cs))\n    \n    for coef in coefs:\n        assert_equal(coef.shape, (X.shape[1] + 1,))\n        assert_true(np.all(np.isfinite(coef)))\n\n    # Check if the loss decreases as C increases (less regularization)\n    losses = []\n    for coef in coefs:\n        intercept = coef[-1]\n        coef = coef[:-1]\n        y_pred = X.dot(coef) + intercept\n        loss = log_loss(y, expit(y_pred))\n        losses.append(loss)\n    assert_true(all(x <= y for x, y in zip(losses, losses[1:])))\n", "def test_logistic_regression_path_check_input():\n    # Test the logistic_regression_path function with check_input=False\n    rng = np.random.RandomState(0)\n    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))\n    y = np.array([1] * 100 + [-1] * 100)\n    Cs = [1e-3, 1, 1e3]\n\n    # Call logistic_regression_path with check_input=False\n    coefs, Cs_res, n_iter = logistic_regression_path(\n        X, y, Cs=Cs, fit_intercept=True, check_input=False, solver='lbfgs'\n    )\n\n    # Check returned coefficients\n    for i, C in enumerate(Cs_res):\n        lr = LogisticRegression(C=C, fit_intercept=True, solver='lbfgs')\n        lr.fit(X, y)\n        lr_coef = np.concatenate([lr.coef_.ravel(), lr.intercept_])\n        assert_array_almost_equal(lr_coef, coefs[i], decimal=4)\n\n    # Check if number of iterations match\n    for iter_count in n_iter:\n        assert iter_count > 0\n", "def test_logistic_regression_path_multinomial():\n    # Test logistic_regression_path with multinomial loss for different solvers\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=3, random_state=0)\n    Cs = [1e-2, 1e-1, 1, 10]\n    \n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        coefs, Cs_res, n_iter = logistic_regression_path(\n            X, y, Cs=Cs, fit_intercept=True, tol=1e-5, solver=solver,\n            max_iter=1000, multi_class='multinomial', random_state=0)\n\n        # Check that the coefficients have the correct shape\n        assert_equal(coefs[0].shape, (3, X.shape[1] + 1))\n\n        # Check that the number of iterations match the length of Cs\n        assert_equal(len(n_iter), len(Cs))\n        \n        # Check that the coefficients and Cs_res are not empty\n        assert len(coefs) > 0\n        assert len(Cs_res) > 0\n", "def test_logistic_regression_path_multiclass():\n    # Test logistic_regression_path for multiclass classification\n    X, y = make_classification(n_samples=100, n_features=5, n_classes=3,\n                               n_informative=3, n_redundant=0, random_state=42)\n    Cs = np.logspace(-4, 4, 10)\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=Cs, solver='lbfgs',\n                                                 multi_class='multinomial',\n                                                 max_iter=200, tol=1e-4, random_state=42)\n\n    # Check shapes of output arrays\n    n_classes = len(np.unique(y))\n    n_features = X.shape[1]\n    assert_equal(len(coefs), len(Cs))\n    assert_equal(coefs[0].shape, (n_classes, n_features + 1))\n    assert_equal(n_iter.shape, (len(Cs),))\n\n    # Check that coefficients are not all zeros\n    for coef in coefs:\n        assert_true(np.any(coef))\n\n    # Check the convergence\n    assert_true(np.all(n_iter < 200))\n", "def test_logistic_loss():\n    # Test the logistic loss calculation\n    X, y = make_classification(n_samples=20, random_state=0)\n    n_features = X.shape[1]\n\n    w = np.zeros(n_features)\n    alpha = 1.\n\n    # First check loss without intercept\n    loss = _logistic_loss(w, X, y, alpha)\n    assert loss > 0\n\n    # Check loss with intercept\n    w = np.zeros(n_features + 1)\n    loss_with_intercept = _logistic_loss(w, X, y, alpha)\n    assert loss_with_intercept > 0\n\n    # Check that loss with intercept is greater than or equal to loss without intercept\n    assert loss_with_intercept >= loss\n"], "sample_754": ["compilation error", "def test_sparse_pca_convergence(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 50)\n    spca = SparsePCA(n_components=10, max_iter=500, tol=1e-4, random_state=rng, normalize_components=norm_comp)\n    spca.fit(X)\n    assert spca.n_iter_ <= spca.max_iter, \"SparsePCA did not converge within the given iterations.\"\n    assert_array_almost_equal(spca.components_.dot(spca.components_.T), np.eye(10), decimal=1)\n", "def test_sparsepca_method_parameter(method, norm_comp):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    spca = SparsePCA(n_components=3, method=method, random_state=0, \n                     alpha=1, normalize_components=norm_comp)\n    spca.fit(Y)\n    assert spca.method == method\n", "def test_sparse_pca_tolerance(spca_cls):\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 8)\n    spca = spca_cls(n_components=3, tol=1e-4, random_state=rng)\n    spca.fit(X)\n    initial_error = spca.error_[-1]\n    \n    spca_tol_strict = spca_cls(n_components=3, tol=1e-8, random_state=rng)\n    spca_tol_strict.fit(X)\n    strict_error = spca_tol_strict.error_[-1]\n    \n    # Ensure that with stricter tolerance the error is smaller or equal\n    assert_true(strict_error <= initial_error)\n", "def test_sparse_pca_default_params():\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 8)\n    spca = SparsePCA(random_state=rng)\n    spca.fit(X)\n    assert_equal(spca.n_components, 8)\n    assert_equal(spca.alpha, 1)\n    assert_equal(spca.ridge_alpha, 0.01)\n    assert_equal(spca.max_iter, 1000)\n    assert_equal(spca.tol, 1e-8)\n    assert_equal(spca.method, 'lars')\n    assert_equal(spca.n_jobs, 1)\n    assert_equal(spca.verbose, False)\n    assert_equal(spca.random_state, rng)\n    assert_equal(spca.normalize_components, False)\n"], "sample_755": ["def test_davies_bouldin_score_zero_intra_distances():\n    # Test case when intra cluster distances are zero leading to high DB score\n    X = [[1, 1], [1, 1], [5, 5], [5, 5], [9, 9], [9, 9]]\n    labels = [0, 0, 1, 1, 2, 2]\n\n    # Intra-cluster distances are all zero, inter-cluster distances are large.\n    # Expected score is high as clusters are well separated.\n    score = davies_bouldin_score(X, labels)\n    assert score == pytest.approx(0.0)\n", "def test_davies_bouldin_score_identical_clusters():\n    # Test Davies-Bouldin score with identical clusters\n    X = [[1, 2], [1, 2], [3, 4], [3, 4]]\n    labels = [0, 0, 1, 1]\n\n    # When clusters are identical, the intra-cluster distances should be zero,\n    # leading to a Davies-Bouldin score of zero\n    assert davies_bouldin_score(X, labels) == pytest.approx(0.0)\n\n    # Test with more clusters with identical points\n    X = [[1, 2], [1, 2], [3, 4], [3, 4], [5, 6], [5, 6]]\n    labels = [0, 0, 1, 1, 2, 2]\n    assert davies_bouldin_score(X, labels) == pytest.approx(0.0)\n", "def test_silhouette_with_callable_metric():\n    # Test silhouette score with a custom callable distance metric\n        return np.sum(np.abs(x - y))\n\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = dataset.target\n\n    score_callable = silhouette_score(X, y, metric=custom_metric)\n    score_euclidean = silhouette_score(X, y, metric='euclidean')\n\n    # The scores will not be exactly the same since the distance metrics are different,\n    # but ensure that the score with the custom metric does not result in an error\n    assert score_callable > -1\n    assert score_callable < 1\n    assert score_callable != score_euclidean\n\n    samples_callable = silhouette_samples(X, y, metric=custom_metric)\n    samples_euclidean = silhouette_samples(X, y, metric='euclidean')\n\n    # Again, the samples' silhouette scores will differ, but they should be within a reasonable range\n    assert np.all(samples_callable > -1)\n    assert np.all(samples_callable < 1)\n    assert not np.all(samples_callable == samples_euclidean)\n", "def test_silhouette_samples_sparse_input():\n    # Test the silhouette_samples function with sparse input\n    dataset = datasets.load_iris()\n    X_dense = dataset.data\n    y = dataset.target\n\n    X_csr = csr_matrix(X_dense)\n    X_dok = sp.dok_matrix(X_dense)\n    X_lil = sp.lil_matrix(X_dense)\n\n    for X in [X_csr, X_dok, X_lil]:\n        D = pairwise_distances(X, metric='euclidean')\n        # Given that the actual labels are used, we can assume that S would be positive.\n        samples_precomputed = silhouette_samples(D, y, metric='precomputed')\n        assert np.all(samples_precomputed > 0)\n        # Test without calculating D\n        samples_euclidean = silhouette_samples(X, y, metric='euclidean')\n        assert np.all(samples_euclidean > 0)\n\n        if X is X_dense:\n            samples_dense = samples_precomputed\n        else:\n            assert np.allclose(samples_euclidean, samples_dense)\n\n        # Test with sampling\n        samples_precomputed = silhouette_samples(D, y, metric='precomputed',\n                                                 sample_size=int(X.shape[0] / 2),\n                                                 random_state=0)\n        samples_euclidean = silhouette_samples(X, y, metric='euclidean',\n                                               sample_size=int(X.shape[0] / 2),\n                                               random_state=0)\n        assert np.all(samples_precomputed > 0)\n        assert np.all(samples_euclidean > 0)\n        assert np.allclose(samples_euclidean, samples_precomputed)\n", "def test_silhouette_score_with_callable_metric():\n    # Test silhouette_score with a callable metric\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = dataset.target\n\n        return np.sqrt(np.sum((a - b) ** 2))\n\n    score_callable = silhouette_score(X, y, metric=custom_metric)\n    assert score_callable > 0\n\n    # Test with sampling\n    score_callable_sampled = silhouette_score(X, y, metric=custom_metric,\n                                              sample_size=int(X.shape[0] / 2),\n                                              random_state=0)\n    assert score_callable_sampled > 0\n    pytest.approx(score_callable_sampled, score_callable)\n"], "sample_756": ["def test_core_sample_indices_dtype():\n    # Test that core_sample_indices_ has the correct data type\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    clust = OPTICS(min_samples=10).fit(X)\n\n    assert clust.core_sample_indices_.dtype.kind == 'i'\n", "def test_optics_parameters():\n    # Test various parameter combinations to ensure the algorithm handles them correctly\n\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.4, random_state=0)\n\n    # Test with different metrics\n    for metric in ['cityblock', 'cosine', 'euclidean']:\n        clust = OPTICS(metric=metric, min_samples=5, max_eps=0.5)\n        clust.fit(X)\n        assert clust.labels_.shape == (len(X),)\n        assert clust.reachability_.shape == (len(X),)\n        assert clust.ordering_.shape == (len(X),)\n    \n    # Test with various leaf sizes\n    for leaf_size in [10, 30, 50]:\n        clust = OPTICS(leaf_size=leaf_size, min_samples=5, max_eps=0.5)\n        clust.fit(X)\n        assert clust.leaf_size == leaf_size\n        assert clust.labels_.shape == (len(X),)\n        assert clust.reachability_.shape == (len(X),)\n        assert clust.ordering_.shape == (len(X),)\n    \n    # Test with different algorithms\n    for algorithm in ['auto', 'ball_tree', 'kd_tree', 'brute']:\n        clust = OPTICS(algorithm=algorithm, min_samples=5, max_eps=0.5)\n        clust.fit(X)\n        assert clust.algorithm == algorithm\n        assert clust.labels_.shape == (len(X),)\n        assert clust.reachability_.shape == (len(X),)\n        assert clust.ordering_.shape == (len(X),)\n\n    # Test with different p values for Minkowski metric\n    for p in [1, 2, 3]:\n        clust = OPTICS(metric='minkowski', p=p, min_samples=5, max_eps=0.5)\n        clust.fit(X)\n        assert clust.p == p\n        assert clust.labels_.shape == (len(X),)\n        assert clust.reachability_.shape == (len(X),)\n        assert clust.ordering_.shape == (len(X),)\n", "def test_core_distances_computation():\n    # Test _compute_core_distances_ method directly\n    \n    X_small = np.array([[0, 0], [1, 1], [2, 2], [10, 10]])\n    \n    # Expected core distances (nearest neighbor distances)\n    expected_core_distances = np.array([np.sqrt(2), np.sqrt(2), np.sqrt(2), np.inf])\n    \n    clust = OPTICS(min_samples=2, metric='euclidean', algorithm='ball_tree')\n    clust.fit(X_small)\n    \n    neighbors = NearestNeighbors(n_neighbors=clust.min_samples, metric=clust.metric, algorithm=clust.algorithm)\n    neighbors.fit(X_small)\n    \n    core_distances = clust._compute_core_distances_(X_small, neighbors)\n    \n    assert_allclose(core_distances, expected_core_distances)\n", "def test_optics_with_custom_metric():\n    # Test OPTICS with a custom metric function\n\n        return np.sqrt(np.sum((a - b) ** 2))\n\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    clust = OPTICS(min_samples=10, metric=custom_metric)\n    clust.fit(X)\n\n    # Check that the labels make sense given the custom metric\n    assert clust.labels_.shape == (750,)\n    assert clust.labels_.dtype.kind == 'i'\n    assert set(clust.labels_) != {-1}  # Ensure that not all points are noise\n\n    # Ensure that reachability and core distances have been computed\n    assert clust.reachability_.shape == (750,)\n    assert clust.reachability_.dtype.kind == 'f'\n    assert clust.core_distances_.shape == (750,)\n    assert clust.core_distances_.dtype.kind == 'f'\n\n    # Ensure that ordering has been computed\n    assert clust.ordering_.shape == (750,)\n    assert clust.ordering_.dtype.kind == 'i'\n    assert set(clust.ordering_) == set(range(750))\n", "def test_invalid_metric_params():\n    # Test invalid metric parameters are correctly handled\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    # Compute OPTICS with invalid metric_params\n    clust = OPTICS(min_samples=10, metric_params={'invalid_param': 1})\n    msg = \"Some metric parameters are not compatible with the selected metric.\"\n    assert_raise_message(ValueError, msg, clust.fit, X)\n"], "sample_757": ["def test_ordinal_encoder_handle_unknown():\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])\n    X2 = np.array([[4, 1, 1]])\n\n    # Test that ordinal encoder raises error for unknown features present during transform.\n    oe = OrdinalEncoder(categories='auto')\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n    \n    # Test the ignore option, ignores unknown features (giving all 0's)\n    oe = OrdinalEncoder(categories=[range(5), range(3), range(4)], dtype=np.int, handle_unknown='ignore')\n    oe.fit(X)\n    X2_passed = X2.copy()\n    assert_array_equal(oe.transform(X2_passed), np.array([[4, 1, 1]]))\n    # ensure transformed data was not modified in place\n    assert_allclose(X2, X2_passed)\n    \n    # Raise error if handle_unknown is neither ignore or error.\n    oe = OrdinalEncoder(handle_unknown='42')\n    assert_raises(ValueError, oe.fit, X)\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2]])\n    X_unknown = np.array([['Female', 4]])\n\n    # Test that ordinal encoder raises error for unknown features\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_unknown)\n\n    # Test ignore option for unknown categories\n    enc = OrdinalEncoder(categories=[['Female', 'Male'], [1, 2, 3]])\n    X_trans = enc.fit_transform(X)\n    assert_array_equal(X_trans, np.array([[1, 0], [0, 2], [0, 1]], dtype=np.float64))\n    enc = OrdinalEncoder(categories=[['Female', 'Male'], [1, 2, 3]], handle_unknown='ignore')\n    X_trans_ignore = enc.fit_transform(X_unknown)\n    assert_array_equal(X_trans_ignore, np.array([[0, 0]], dtype=np.float64))\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([[0, 2], [1, 0], [1, 2]])\n    X2 = np.array([[2, 1]])\n\n    # Test that ordinal encoder raises error for unknown features\n    # present during transform.\n    oe = OrdinalEncoder()\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n\n    # Test the ignore option, ignores unknown features (giving all -1's)\n    oe = OrdinalEncoder(categories=[[0, 1], [0, 2]])\n    oe.fit(X)\n    X2_transformed = oe.transform(X2)\n    assert_array_equal(X2_transformed, [[-1, 1]])\n\n    # Test with mixed data types\n    X = np.array([['a', 1], ['b', 2], ['b', 1]], dtype=object)\n    X2 = np.array([['c', 1]], dtype=object)\n    oe = OrdinalEncoder()\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n\n    # Test ignore option for mixed data types\n    oe = OrdinalEncoder(categories=[['a', 'b'], [1, 2]])\n    oe.fit(X)\n    X2_transformed = oe.transform(X2)\n    assert_array_equal(X2_transformed, [[-1, 0]])\n", "def test_one_hot_encoder_categories_auto():\n    # Test OneHotEncoder with categories='auto' for various types of data\n    X = [['a', 1], ['b', 2], ['c', 1]]\n    enc = OneHotEncoder(categories='auto')\n    X_trans = enc.fit_transform(X).toarray()\n    \n    expected_output = np.array([\n        [1., 0., 0., 1., 0.],\n        [0., 1., 0., 0., 1.],\n        [0., 0., 1., 1., 0.]\n    ])\n    assert_array_equal(X_trans, expected_output)\n    assert_array_equal(enc.categories_[0], np.array(['a', 'b', 'c'], dtype=object))\n    assert_array_equal(enc.categories_[1], np.array([1, 2], dtype=np.int64))\n\n    # Test with numeric data\n    X_num = [[1, 2], [3, 4]]\n    enc_num = OneHotEncoder(categories='auto')\n    X_trans_num = enc_num.fit_transform(X_num).toarray()\n    \n    expected_output_num = np.array([\n        [1., 0., 1., 0.],\n        [0., 1., 0., 1.]\n    ])\n    assert_array_equal(X_trans_num, expected_output_num)\n    assert_array_equal(enc_num.categories_[0], np.array([1, 3], dtype=np.int64))\n    assert_array_equal(enc_num.categories_[1], np.array([2, 4], dtype=np.int64))\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])\n    X2 = np.array([[4, 1, 1]])\n\n    # Test that ordinal encoder raises error for unknown features\n    # present during transform.\n    oe = OrdinalEncoder()\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n\n    # Test the ignore option for OrdinalEncoder doesn't exist, should raise error\n    with pytest.raises(TypeError):\n        OrdinalEncoder(handle_unknown='ignore')\n"], "sample_758": ["def test_assert_all_finite():\n    # Test that assert_all_finite raises ValueError on NaN and inf\n    X = np.array([0, 1, 2, np.nan, 4])\n    assert_raises(ValueError, assert_all_finite, X)\n\n    X = np.array([0, 1, 2, np.inf, 4])\n    assert_raises(ValueError, assert_all_finite, X)\n\n    # Test that assert_all_finite allows NaN if allow_nan=True\n    assert_no_warnings(assert_all_finite, X, allow_nan=True)\n\n    # Test that assert_all_finite allows finite values\n    X = np.array([0, 1, 2, 3, 4])\n    assert_no_warnings(assert_all_finite, X)\n    \n    # Test that assert_all_finite handles sparse matrices correctly\n    X_sparse = sp.csr_matrix(X)\n    assert_no_warnings(assert_all_finite, X_sparse)\n    \n    X_sparse_with_nan = sp.csr_matrix([0, 1, np.nan, 3, 4])\n    assert_raises(ValueError, assert_all_finite, X_sparse_with_nan)\n    \n    # Test that assert_all_finite handles non-numeric types\n    X_non_numeric = ['a', 'b', 'c']\n    assert_raises(ValueError, assert_all_finite, X_non_numeric)\n", "def test_check_random_state():\n    # Test check_random_state with different types of inputs\n    rs = np.random.RandomState(42)\n    \n    # Case when seed is None\n    assert_true(check_random_state(None) is np.random.mtrand._rand)\n    \n    # Case when seed is an integer\n    assert_true(isinstance(check_random_state(42), np.random.RandomState))\n    assert_array_equal(check_random_state(42).randint(0, 10, 5), rs.randint(0, 10, 5))\n    \n    # Case when seed is already a RandomState instance\n    assert_true(check_random_state(rs) is rs)\n    \n    # Case when seed is np.random\n    assert_true(check_random_state(np.random) is np.random.mtrand._rand)\n    \n    # Invalid seed\n    assert_raises(ValueError, check_random_state, \"invalid_seed\")\n", "def test_as_float_array_with_sparse_matrix():\n    # Test function for as_float_array with a sparse matrix input\n    X = sp.csr_matrix([[1, 0], [0, 1]], dtype=np.int32)\n    X2 = as_float_array(X, copy=False)\n    assert_equal(X2.dtype, np.float32)\n    assert_true(sp.issparse(X2))\n    \n    X = sp.csr_matrix([[1, 0], [0, 1]], dtype=np.int64)\n    X2 = as_float_array(X, copy=True)\n    assert_equal(X2.dtype, np.float64)\n    assert_true(sp.issparse(X2))\n    # Check that X was not modified\n    assert_false(np.allclose(X.data, X2.data))\n", "def test_check_symmetric_no_warning_or_exception():\n    arr_sym = np.array([[1, 2], [2, 3]])\n    arr_sym_sp = sp.csr_matrix(arr_sym)\n\n    # Check symmetric array does not raise warning or exception\n    assert_no_warnings(check_symmetric, arr_sym)\n    assert_no_warnings(check_symmetric, arr_sym_sp)\n    \n    # Explicitly check that the returned array is the same\n    assert_array_equal(check_symmetric(arr_sym, raise_warning=False), arr_sym)\n    assert_array_equal(check_symmetric(arr_sym_sp, raise_warning=False).toarray(), arr_sym)\n", "def test_assert_all_finite():\n    # Test with finite values\n    X = np.array([1.0, 2.0, 3.0])\n    assert_no_warnings(assert_all_finite, X)\n\n    # Test with NaN values\n    X_nan = np.array([1.0, np.nan, 3.0])\n    assert_raises(ValueError, assert_all_finite, X_nan)\n\n    # Test with infinity values\n    X_inf = np.array([1.0, np.inf, 3.0])\n    assert_raises(ValueError, assert_all_finite, X_inf)\n\n    # Test with NaN values and allow_nan=True\n    assert_no_warnings(assert_all_finite, X_nan, allow_nan=True)\n\n    # Test with infinity values and allow_nan=True\n    assert_raises(ValueError, assert_all_finite, X_inf, allow_nan=True)\n\n    # Test with sparse matrix containing NaN\n    X_sparse_nan = sp.csr_matrix([[1.0, 2.0], [np.nan, 3.0]])\n    assert_raises(ValueError, assert_all_finite, X_sparse_nan)\n\n    # Test with sparse matrix containing infinity\n    X_sparse_inf = sp.csr_matrix([[1.0, 2.0], [np.inf, 3.0]])\n    assert_raises(ValueError, assert_all_finite, X_sparse_inf)\n"], "sample_759": ["def test_one_hot_encoder_legacy_mode_deprecation():\n    X = [[1, 2], [3, 4]]\n    \n    # Test deprecated n_values parameter\n    with pytest.warns(DeprecationWarning, match=\"Passing 'n_values' is deprecated\"):\n        enc = OneHotEncoder(n_values=5)\n        enc.fit(X)\n    \n    # Test deprecated categorical_features parameter\n    with pytest.warns(DeprecationWarning, match=\"The 'categorical_features' keyword is deprecated\"):\n        enc = OneHotEncoder(categorical_features=[0, 1])\n        enc.fit(X)\n\n    # Test deprecated feature_indices_ attribute\n    enc = OneHotEncoder(categories='auto')\n    enc.fit(X)\n    with pytest.warns(DeprecationWarning, match=\"The ``feature_indices_`` attribute was deprecated\"):\n        _ = enc.feature_indices_\n    \n    # Test deprecated active_features_ attribute\n    with pytest.warns(DeprecationWarning, match=\"The ``active_features_`` attribute was deprecated\"):\n        _ = enc.active_features_\n\n    # Test deprecated n_values_ attribute\n    with pytest.warns(DeprecationWarning, match=\"The ``n_values_`` attribute was deprecated\"):\n        _ = enc.n_values_\n", "def test_ordinal_encoder_unseen_categories():\n    # Test that OrdinalEncoder raises an error for unseen categories\n    X_train = [['cat1', 'dog1'], ['cat2', 'dog2']]\n    X_test = [['cat1', 'dog3'], ['cat3', 'dog1']]\n\n    enc = OrdinalEncoder()\n    enc.fit(X_train)\n\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_test)\n\n    # Test that setting handle_unknown='ignore' leads to the correct behavior\n    enc = OrdinalEncoder()\n    enc.fit(X_train)\n    enc.categories_ = [np.array(['cat1', 'cat2']), np.array(['dog1', 'dog2', 'dog3'])]  # Simulate unknown category\n\n    X_trans = enc.transform([['cat1', 'dog3']])\n    expected = np.array([[0.0, 2.0]])\n    assert_array_equal(X_trans, expected)\n", "def test_one_hot_encoder_missing_values():\n    # Test OneHotEncoder with missing values\n    X = np.array([[np.nan, 2], [1, 3], [1, np.nan]], dtype=object)\n    \n    # Test that an error is raised when there are missing values in input\n    enc = OneHotEncoder()\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        enc.fit(X)\n    \n    # Test that no error is raised when handle_unknown='ignore'\n    enc = OneHotEncoder(handle_unknown='ignore')\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        enc.fit(X)\n", "def test_one_hot_encoder_fit_transform_with_handle_unknown_ignore():\n    X = [['cat', 1], ['dog', 2], ['cat', 3]]\n    X_test = [['cat', 1], ['dog', 4], ['bird', 2]]\n    \n    enc = OneHotEncoder(handle_unknown='ignore')\n    X_trans = enc.fit_transform(X)\n    \n    expected_categories = [np.array(['cat', 'dog'], dtype=object), np.array([1, 2, 3], dtype=object)]\n    assert_array_equal(enc.categories_, expected_categories)\n    \n    X_test_trans = enc.transform(X_test).toarray()\n    expected_transform = np.array([\n        [1., 0., 1., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 0., 1., 0.]\n    ])\n    \n    assert_array_equal(X_test_trans, expected_transform)\n    \n    X_inv = enc.inverse_transform(X_test_trans)\n    expected_inverse = np.array([\n        ['cat', 1],\n        ['dog', None],\n        [None, 2]\n    ], dtype=object)\n    \n    assert_array_equal(X_inv, expected_inverse)\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['a', 'b'], ['c', 'd']])\n    X2 = np.array([['e', 'b'], ['c', 'd']])\n\n    # Test that ordinal encoder raises error for unknown features present during transform.\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X2)\n\n    # Test the inverse transform with unknown categories encountered during transform.\n    enc = OrdinalEncoder()\n    X_transformed = enc.fit_transform(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.inverse_transform(np.array([[2, 1], [1, 0]]))\n"], "sample_760": ["def test_make_scorer_kwargs():\n    # Test that make_scorer passes additional kwargs correctly.\n        return multiplier * np.mean(y_true == y_pred)\n\n    # Create scorer with additional kwargs\n    scorer = make_scorer(custom_score, multiplier=2)\n    \n    # Create a simple classifier and dataset\n    X, y = make_classification(random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_train, y_train)\n\n    # Test that additional kwargs are passed correctly\n    score = scorer(clf, X_test, y_test)\n    expected_score = custom_score(y_test, clf.predict(X_test), multiplier=2)\n    assert_almost_equal(score, expected_score)\n\n    # Test that repr includes additional kwargs\n    repr_str = repr(scorer)\n    assert \"multiplier=2\" in repr_str\n", "def test_scorer_with_custom_kwargs():\n    # Test that scorers accept custom kwargs\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    \n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_train, y_train)\n    \n    # Custom scorer with kwargs\n    custom_scorer = make_scorer(f1_score, average='micro')\n    score1 = custom_scorer(clf, X_test, y_test)\n    score2 = f1_score(y_test, clf.predict(X_test), average='micro')\n    assert_almost_equal(score1, score2)\n    \n    # Custom scorer with another set of kwargs\n    custom_scorer = make_scorer(f1_score, average='macro')\n    score1 = custom_scorer(clf, X_test, y_test)\n    score2 = f1_score(y_test, clf.predict(X_test), average='macro')\n    assert_almost_equal(score1, score2)\n\n    # test that custom scorer with kwargs can be pickled\n    unpickled_scorer = pickle.loads(pickle.dumps(custom_scorer))\n    score3 = unpickled_scorer(clf, X_test, y_test)\n    assert_almost_equal(score1, score3)\n", "def test_custom_scorer_with_kwargs():\n    # Test custom scorer with additional kwargs\n    X, y = make_classification(n_samples=30, n_features=5, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_train, y_train)\n\n    # Define a custom scoring function\n        y_pred_thresholded = (y_pred > threshold).astype(int)\n        return accuracy_score(y_true, y_pred_thresholded)\n\n    # Create scorer with additional kwargs\n    scorer = make_scorer(custom_scorer, threshold=0.7)\n\n    # Calculate score using the custom scorer\n    score = scorer(clf, X_test, y_test)\n    y_pred = clf.predict_proba(X_test)[:, 1]\n    expected_score = custom_scorer(y_test, y_pred, threshold=0.7)\n    assert_almost_equal(score, expected_score)\n\n    # Pickle and unpickle the scorer to ensure it works\n    unpickled_scorer = pickle.loads(pickle.dumps(scorer))\n    score_unpickled = unpickled_scorer(clf, X_test, y_test)\n    assert_almost_equal(score, score_unpickled)\n", "def test_make_scorer_with_kwargs():\n    # Test make_scorer with additional keyword arguments\n    scorer = make_scorer(fbeta_score, beta=2)\n    estimator = DecisionTreeClassifier()\n    X, y = make_classification(n_samples=30, n_features=5, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    \n    estimator.fit(X_train, y_train)\n    score = scorer(estimator, X_test, y_test)\n    expected_score = fbeta_score(y_test, estimator.predict(X_test), beta=2)\n    assert_almost_equal(score, expected_score)\n", "def test_make_scorer_with_needs_proba():\n    # Test the make_scorer function with needs_proba=True\n    X, y = make_blobs(random_state=0, centers=2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_train, y_train)\n\n    proba_scorer = make_scorer(roc_auc_score, needs_proba=True)\n    score1 = proba_scorer(clf, X_test, y_test)\n    score2 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n    assert_almost_equal(score1, score2)\n\n    # Test that an error is raised if the estimator does not have predict_proba\n    clf = LinearSVC(random_state=0)\n    clf.fit(X_train, y_train)\n    with pytest.raises(AttributeError, match=\"predict_proba\"):\n        proba_scorer(clf, X_test, y_test)\n"], "sample_761": ["def test_simple_imputer_invalid_fill_value():\n    # Test SimpleImputer with invalid fill_value type\n    X = np.array([\n        [np.nan, 1, 5],\n        [2, np.nan, 1],\n        [6, 3, np.nan],\n        [1, 2, 9]\n    ])\n\n    with pytest.raises(ValueError, match=\"'fill_value' must be a string or numerical value. Got None instead.\"):\n        imputer = SimpleImputer(strategy='constant', fill_value=None)\n        imputer.fit_transform(X)\n", "def test_simple_imputer_fit_transform():\n    # Test SimpleImputer's fit_transform method for consistency\n    X = np.array([\n        [np.nan, 1, 5],\n        [2, np.nan, 1],\n        [6, 3, np.nan],\n        [1, 2, 9]\n    ])\n    \n    imputer = SimpleImputer(strategy=\"mean\")\n    X_trans_fit = imputer.fit(X).transform(X)\n    X_trans_fit_transform = imputer.fit_transform(X)\n\n    assert_array_equal(X_trans_fit, X_trans_fit_transform)\n", "def test_simple_imputer_invalid_fill_value():\n    # Test SimpleImputer with invalid fill_value for different strategies\n    X = np.array([\n        [np.nan, 1, 5],\n        [2, np.nan, 1],\n        [6, 3, np.nan],\n        [1, 2, 9]\n    ])\n\n    imputer_mean = SimpleImputer(strategy=\"mean\", fill_value=\"invalid\")\n    with pytest.raises(ValueError, match=\"'fill_value'='invalid' is invalid\"):\n        imputer_mean.fit_transform(X)\n\n    imputer_median = SimpleImputer(strategy=\"median\", fill_value=\"invalid\")\n    with pytest.raises(ValueError, match=\"'fill_value'='invalid' is invalid\"):\n        imputer_median.fit_transform(X)\n\n    imputer_most_frequent = SimpleImputer(strategy=\"most_frequent\", fill_value=\"invalid\")\n    with pytest.raises(ValueError, match=\"'fill_value'='invalid' is invalid\"):\n        imputer_most_frequent.fit_transform(X)\n\n    imputer_constant = SimpleImputer(strategy=\"constant\", fill_value={\"a\": 1})\n    with pytest.raises(ValueError, match=\"'fill_value'={'a': 1} is invalid\"):\n        imputer_constant.fit_transform(X)\n", "def test_imputation_error_on_invalid_fill_value():\n    # Check that ValueError is raised when fill_value is invalid for numeric and object data\n    X_numeric = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [np.nan, 5, 9]\n    ])\n    \n    X_object = np.array([\n        [\"a\", \"b\", None],\n        [\"d\", None, \"f\"],\n        [None, \"e\", \"g\"]\n    ], dtype=object)\n    \n    invalid_fill_value = {\"invalid_key\": \"invalid_value\"}\n    \n    # Test for numeric data\n    with pytest.raises(ValueError, match=\"'fill_value'={} is invalid.\".format(invalid_fill_value)):\n        imputer = SimpleImputer(strategy=\"constant\", fill_value=invalid_fill_value)\n        imputer.fit_transform(X_numeric)\n    \n    # Test for object data\n    with pytest.raises(ValueError, match=\"'fill_value'={} is invalid.\".format(invalid_fill_value)):\n        imputer = SimpleImputer(strategy=\"constant\", fill_value=invalid_fill_value)\n        imputer.fit_transform(X_object)\n", "def test_imputer_mixed_types():\n    # Test imputer's behavior with mixed types (numerical and string)\n    X = np.array([\n        [np.nan, \"a\", 1],\n        [2, np.nan, 2],\n        [3, \"b\", np.nan],\n        [np.nan, np.nan, 3]\n    ], dtype=object)\n    \n    imputer = SimpleImputer(strategy=\"most_frequent\")\n    X_trans = imputer.fit_transform(X)\n    X_true = np.array([\n        [2, \"a\", 1],\n        [2, \"a\", 2],\n        [3, \"b\", 3],\n        [2, \"a\", 3]\n    ], dtype=object)\n    \n    assert_array_equal(X_trans, X_true)\n    \n    imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")\n    X_trans = imputer.fit_transform(X)\n    X_true = np.array([\n        [\"missing\", \"a\", 1],\n        [2, \"missing\", 2],\n        [3, \"b\", \"missing\"],\n        [\"missing\", \"missing\", 3]\n    ], dtype=object)\n    \n    assert_array_equal(X_trans, X_true)\n"], "sample_762": ["def test_first_and_last_element():\n    # Test _first_and_last_element with numpy arrays\n    arr = np.array([1, 2, 3, 4, 5])\n    first, last = _first_and_last_element(arr)\n    assert_equal(first, 1)\n    assert_equal(last, 5)\n\n    # Test _first_and_last_element with sparse matrices\n    sparse_arr = sp.csr_matrix(np.array([1, 2, 3, 4, 5]))\n    first, last = _first_and_last_element(sparse_arr)\n    assert_equal(first, 1)\n    assert_equal(last, 5)\n\n    # Test _first_and_last_element with dok_matrix\n    dok_arr = sp.dok_matrix((5, 5))\n    dok_arr[0, 0] = 1\n    dok_arr[4, 4] = 5\n    first, last = _first_and_last_element(dok_arr)\n    assert_equal(first, 1)\n    assert_equal(last, 5)\n\n    # Test with an empty numpy array\n    empty_arr = np.array([])\n    assert_raises(IndexError, _first_and_last_element, empty_arr)\n\n    # Test with an empty sparse matrix\n    empty_sparse_arr = sp.csr_matrix((5, 5))\n    assert_raises(IndexError, _first_and_last_element, empty_sparse_arr)\n", "def test_first_and_last_element():\n    # Test for _first_and_last_element function with numpy array\n    arr = np.array([1, 2, 3, 4, 5])\n    assert_equal(_first_and_last_element(arr), (1, 5))\n\n    # Test for _first_and_last_element function with sparse matrix\n    arr = sp.csr_matrix([1, 2, 3, 4, 5])\n    assert_equal(_first_and_last_element(arr), (1, 5))\n\n    # Test for _first_and_last_element function with sparse matrix without .data attribute\n    arr = sp.dok_matrix((5, 5))\n    arr[0, 0] = 1\n    arr[4, 4] = 5\n    assert_equal(_first_and_last_element(arr), (1, 5))\n\n    # Test for empty array\n    arr = np.array([])\n    assert_raises(IndexError, _first_and_last_element, arr)\n\n    # Test for empty sparse matrix\n    arr = sp.csr_matrix((0, 0))\n    assert_raises(IndexError, _first_and_last_element, arr)\n", "def test_base_estimator_get_param_names():\n    # Test if _get_param_names returns correct parameter names\n    class MockEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n    est = MockEstimator()\n    assert_equal(est._get_param_names(), ['param1', 'param2'])\n\n    # Check with no params\n    class MockEstimatorNoParams(BaseEstimator):\n            pass\n\n    est_no_params = MockEstimatorNoParams()\n    assert_equal(est_no_params._get_param_names(), [])\n", "def test_is_regressor():\n    dtr = DecisionTreeRegressor()\n    assert is_regressor(dtr)\n    assert is_regressor(GridSearchCV(dtr, {'max_depth': [3, 5]}))\n    assert is_regressor(Pipeline([('dtr', dtr)]))\n    assert_true(is_regressor(Pipeline(\n        [('dtr_cv', GridSearchCV(dtr, {'max_depth': [3, 5]}))])))\n", "def test_get_indices():\n    # Test get_indices method of BiclusterMixin\n    class BiclusterExample(BaseEstimator, BiclusterMixin):\n            self.rows_ = rows\n            self.columns_ = columns\n\n    rows = np.array([[True, False], [False, True]])\n    columns = np.array([[True, True, False], [False, True, True]])\n\n    bicluster = BiclusterExample(rows, columns)\n\n    row_ind, col_ind = bicluster.get_indices(0)\n    assert_array_equal(row_ind, np.array([0]))\n    assert_array_equal(col_ind, np.array([0, 1]))\n\n    row_ind, col_ind = bicluster.get_indices(1)\n    assert_array_equal(row_ind, np.array([1]))\n    assert_array_equal(col_ind, np.array([1, 2]))\n"], "sample_763": ["def test_check_array_force_all_finite_string_value():\n    # Test check_array with force_all_finite='allow-nan' for string array\n    X = np.array([['a', 'b'], ['c', 'nan']], dtype=object)\n    with pytest.raises(ValueError, match='Cannot convert non-numeric data to float'):\n        check_array(X, dtype='numeric', force_all_finite='allow-nan')\n\n    X = np.array([['a', 'b'], ['c', np.nan]], dtype=object)\n    with pytest.raises(ValueError, match='Cannot convert non-numeric data to float'):\n        check_array(X, dtype='numeric', force_all_finite='allow-nan')\n", "def test_assert_all_finite():\n    # Test that ValueError is raised for arrays with NaN or inf\n    X = np.array([0, np.nan, np.inf])\n    with pytest.raises(ValueError, match=\"Input contains NaN, infinity\"):\n        assert_all_finite(X)\n\n    # Test that no error is raised for finite arrays\n    X = np.array([0, 1, 2])\n    assert_all_finite(X)\n\n    # Test that no error is raised when assume_finite is True\n    sklearn.set_config(assume_finite=True)\n    assert_all_finite(X)\n    sklearn.set_config(assume_finite=False)\n\n    # Test sparse matrix support\n    X = sp.csr_matrix([0, 1, np.inf])\n    with pytest.raises(ValueError, match=\"Input contains infinity\"):\n        assert_all_finite(X)\n\n    # Test sparse matrix with NaN\n    X = sp.csr_matrix([0, 1, np.nan])\n    with pytest.raises(ValueError, match=\"Input contains NaN, infinity\"):\n        assert_all_finite(X)\n\n    # Test sparse matrix with finite values\n    X = sp.csr_matrix([0, 1, 2])\n    assert_all_finite(X)\n", "def test_check_no_complex_data():\n    # Ensure check_array raises an error with complex data for various input types\n    data_types = [\n        np.array([[1 + 2j, 3 + 4j], [5 + 6j, 7 + 8j]]),\n        [[1 + 2j, 3 + 4j], [5 + 6j, 7 + 8j]],\n        ((1 + 2j, 3 + 4j), (5 + 6j, 7 + 8j)),\n        [np.array([1 + 2j, 3 + 4j]), np.array([5 + 6j, 7 + 8j])],\n        (np.array([1 + 2j, 3 + 4j]), np.array([5 + 6j, 7 + 8j])),\n        MockDataFrame(np.array([[1 + 2j, 3 + 4j], [5 + 6j, 7 + 8j]])),\n        sp.coo_matrix([[1 + 2j, 0], [0, 3 + 4j]])\n    ]\n    for data in data_types:\n        assert_raises_regex(ValueError, \"Complex data not supported\", check_array, data)\n", "def test_check_X_y_multi_output():\n    # Test that check_X_y handles multi-output y correctly\n    X = np.arange(6).reshape(3, 2)\n    y_multi = np.arange(6).reshape(3, 2)\n    \n    X_checked, y_checked = check_X_y(X, y_multi, multi_output=True)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y_multi, y_checked)\n    \n    # Test that error is raised if y contains NaN with multi_output=True\n    y_multi[1, 1] = np.nan\n    assert_raises(ValueError, check_X_y, X, y_multi, multi_output=True)\n    \n    # Test that error is raised if y is not 2D when multi_output=True\n    y_single = np.array([1, 2, 3])\n    assert_raises(ValueError, check_X_y, X, y_single, multi_output=True)\n\n    # Test with sparse matrix\n    y_sparse = sp.csr_matrix(y_multi)\n    X_checked, y_checked = check_X_y(X, y_sparse, multi_output=True)\n    assert_array_equal(X, X_checked)\n    assert_allclose_dense_sparse(y_sparse, y_checked)\n", "def test_check_symmetric_non_square():\n    # Test that check_symmetric raises ValueError for non-square matrices\n    non_square_arr = np.array([[1, 2, 3], [4, 5, 6]])\n    non_square_csr = sp.csr_matrix(non_square_arr)\n\n    msg = \"array must be 2-dimensional and square\"\n    assert_raise_message(ValueError, msg, check_symmetric, non_square_arr)\n    assert_raise_message(ValueError, msg, check_symmetric, non_square_csr)\n"], "sample_764": ["def test_column_transformer_non_fitted_transformers():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # Transformer that does not implement fit\n    class NoFitTrans(BaseEstimator):\n            return X\n\n    ct = ColumnTransformer([('trans', NoFitTrans(), [0])])\n    assert_raise_message(TypeError, \"All estimators should implement fit\",\n                         ct.fit, X_array)\n    assert_raise_message(TypeError, \"All estimators should implement fit\",\n                         ct.fit_transform, X_array)\n    \n    # Transformer that does not implement transform\n    class NoTransformTrans(BaseEstimator):\n            return self\n\n    ct = ColumnTransformer([('trans', NoTransformTrans(), [0])])\n    assert_raise_message(TypeError, \"All estimators should implement transform\",\n                         ct.fit, X_array)\n    assert_raise_message(TypeError, \"All estimators should implement transform\",\n                         ct.fit_transform, X_array)\n", "def test_column_transformer_custom_transformer():\n    class CustomTransformer(BaseEstimator, TransformerMixin):\n            self.mean_ = np.mean(X, axis=0)\n            return self\n\n            return X - self.mean_\n\n    X_array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    X_res = X_array - np.mean(X_array, axis=0)\n\n    ct = ColumnTransformer([('custom', CustomTransformer(), [0, 1, 2])])\n    assert_array_equal(ct.fit_transform(X_array), X_res)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res)\n    assert hasattr(ct.named_transformers_['custom'], 'mean_')\n    assert_array_equal(ct.named_transformers_['custom'].mean_, np.mean(X_array, axis=0))\n\n    # Check with remainder\n    X_res_remainder = np.hstack((X_res, X_array[:, [2]]))\n    ct = ColumnTransformer([('custom', CustomTransformer(), [0, 1])], remainder='passthrough')\n    assert_array_equal(ct.fit_transform(X_array), X_res_remainder)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_remainder)\n    assert hasattr(ct.named_transformers_['custom'], 'mean_')\n    assert_array_equal(ct.named_transformers_['custom'].mean_, np.mean(X_array[:, [0, 1]], axis=0))\n", "def test_column_transformer_invalid_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # Check for invalid remainder values\n    invalid_remainders = [123, \"invalid_estimator\", {\"not\": \"valid\"}, [1, 2, 3]]\n    for remainder in invalid_remainders:\n        ct = ColumnTransformer([('trans', Trans(), [0])], remainder=remainder)\n        assert_raise_message(ValueError, \n                             \"The remainder keyword needs to be one of 'drop', 'passthrough', or estimator.\",\n                             ct.fit, X_array)\n        assert_raise_message(ValueError, \n                             \"The remainder keyword needs to be one of 'drop', 'passthrough', or estimator.\",\n                             ct.fit_transform, X_array)\n\n    # Check for valid estimator as remainder\n    remainder = StandardScaler()\n    ct = ColumnTransformer([('trans1', Trans(), [0])], remainder=remainder)\n    assert isinstance(ct.remainder, StandardScaler)\n\n    # Check if the remainder estimator is applied correctly\n    X_res_both = np.hstack([X_array[:, [0]], remainder.fit_transform(X_array[:, [1]])])\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n", "def test_column_transformer_with_multiple_transformers():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    # Test with multiple transformers and mixed types\n    transformers = [\n        ('trans1', StandardScaler(), [0]),\n        ('trans2', OneHotEncoder(), [1]),\n        ('trans3', Normalizer(), [2])\n    ]\n\n    ct = ColumnTransformer(transformers, sparse_threshold=0.8)\n    X_trans = ct.fit_transform(X_array)\n    \n    expected_trans1 = StandardScaler().fit_transform(X_array[:, [0]])\n    expected_trans2 = OneHotEncoder().fit_transform(X_array[:, [1]])\n    expected_trans3 = Normalizer().fit_transform(X_array[:, [2]])\n    \n    if sparse.issparse(expected_trans2):\n        expected_trans2 = expected_trans2.toarray()\n    \n    expected_result = np.hstack([expected_trans1, expected_trans2, expected_trans3])\n    \n    assert_array_equal(X_trans, expected_result)\n    assert len(ct.transformers_) == 3\n    assert ct.transformers_[-1][0] != 'remainder'\n", "def test_column_transformer_unknown_transformer():\n    # Test case where an unknown transformer is used\n\n    class UnknownTransformer(BaseEstimator):\n            return self\n\n            return X\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # Unknown transformer\n    ct = ColumnTransformer([('unknown', UnknownTransformer(), [0])])\n    assert_raise_message(TypeError, \"All estimators should implement fit and transform\",\n                         ct.fit, X_array)\n    assert_raise_message(TypeError, \"All estimators should implement fit and transform\",\n                         ct.fit_transform, X_array)\n\n    # Correctly implemented transformer\n    ct = ColumnTransformer([('known', Trans(), [0])])\n    ct.fit(X_array)\n    assert_array_equal(ct.transform(X_array), np.array([[0], [1], [2]]))\n"], "sample_765": ["def test_jaccard_similarity_score():\n    # Test Jaccard similarity score for different cases\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Dense binary array case\n    expected_score = accuracy_score(y_true, y_pred)\n    assert_almost_equal(jaccard_similarity_score(y_true, y_pred), expected_score)\n\n    # Multiclass case\n    y_true_mc, y_pred_mc, _ = make_prediction(binary=False)\n    expected_score_mc = accuracy_score(y_true_mc, y_pred_mc)\n    assert_almost_equal(jaccard_similarity_score(y_true_mc, y_pred_mc), expected_score_mc)\n\n    # Multilabel case\n    y_true_ml = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred_ml = np.array([[0, 0, 1], [1, 0, 1]])\n    expected_score_ml = 0.75\n    assert_almost_equal(jaccard_similarity_score(y_true_ml, y_pred_ml), expected_score_ml)\n\n    # Test with normalize=False\n    expected_score_nonorm = accuracy_score(y_true_ml, y_pred_ml, normalize=False)\n    assert_almost_equal(jaccard_similarity_score(y_true_ml, y_pred_ml, normalize=False), expected_score_nonorm)\n", "def test_f1_score_warning_on_empty_labels():\n    # Test that f1_score raises a warning when there are empty labels\n    y_true = [0, 0, 0, 1]\n    y_pred = [0, 0, 0, 0]\n\n    f1 = assert_warns(UndefinedMetricWarning, f1_score, y_true, y_pred, average='macro')\n    assert_almost_equal(f1, 0.0)\n\n    f1 = assert_warns(UndefinedMetricWarning, f1_score, y_true, y_pred, average='weighted')\n    assert_almost_equal(f1, 0.0)\n\n    f1 = assert_warns(UndefinedMetricWarning, f1_score, y_true, y_pred, average='micro')\n    assert_almost_equal(f1, 0.75)\n\n    f1 = assert_warns(UndefinedMetricWarning, f1_score, y_true, y_pred, average=None)\n    assert_array_almost_equal(f1, [1.0, 0.0])\n", "def test_classification_report_multilabel_dict_output():\n    # Test classification report with dictionary output for multilabel case\n    n_classes = 3\n    n_samples = 50\n\n    _, y_true = make_multilabel_classification(n_features=1,\n                                               n_samples=n_samples,\n                                               n_classes=n_classes,\n                                               random_state=0)\n\n    _, y_pred = make_multilabel_classification(n_features=1,\n                                               n_samples=n_samples,\n                                               n_classes=n_classes,\n                                               random_state=1)\n\n    expected_report = {'0': {'precision': 0.5, 'recall': 0.67, 'f1-score': 0.57, 'support': 24},\n                       '1': {'precision': 0.51, 'recall': 0.74, 'f1-score': 0.61, 'support': 27},\n                       '2': {'precision': 0.29, 'recall': 0.08, 'f1-score': 0.12, 'support': 26},\n                       'micro avg': {'precision': 0.50, 'recall': 0.51, 'f1-score': 0.50, 'support': 104},\n                       'macro avg': {'precision': 0.45, 'recall': 0.51, 'f1-score': 0.46, 'support': 104},\n                       'weighted avg': {'precision': 0.45, 'recall': 0.51, 'f1-score': 0.46, 'support': 104},\n                       'samples avg': {'precision': 0.46, 'recall': 0.42, 'f1-score': 0.40, 'support': 104}}\n\n    report = classification_report(y_true, y_pred, output_dict=True)\n\n    # assert the two dicts are equal.\n    assert(report.keys() == expected_report.keys())\n    for key in expected_report:\n        if key == 'accuracy':\n            assert isinstance(report[key], float)\n            assert report[key] == expected_report[key]\n        else:\n            assert report[key].keys() == expected_report[key].keys()\n            for metric in expected_report[key]:\n                assert_almost_equal(expected_report[key][metric],\n                                    report[key][metric])\n\n    assert type(expected_report['0']['precision']) == float\n    assert type(expected_report['macro avg']['precision']) == float\n   ", "def test_cohen_kappa_binary():\n    # Test cohen_kappa_score function for binary classification task\n    y_true = [0, 1, 0, 1, 1, 0]\n    y_pred = [0, 0, 1, 1, 0, 0]\n\n    # Compute confusion matrix manually\n    cm = confusion_matrix(y_true, y_pred)\n    sum0 = np.sum(cm, axis=0)\n    sum1 = np.sum(cm, axis=1)\n    expected = np.outer(sum0, sum1) / np.sum(sum0)\n\n    # Compute kappa manually\n    w_mat = np.ones((2, 2), dtype=np.int)\n    w_mat.flat[::3] = 0  # zero diagonal\n    k = np.sum(w_mat * cm) / np.sum(w_mat * expected)\n    true_kappa = 1 - k\n\n    # Compute kappa using the function\n    kappa = cohen_kappa_score(y_true, y_pred)\n    assert_almost_equal(kappa, true_kappa, decimal=4)\n", "def test_f1_score_multiclass_with_specified_labels():\n    # Test F1 score for multiclass classification with specified labels\n    y_true = [0, 1, 2, 2, 1]\n    y_pred = [0, 2, 1, 2, 0]\n    labels = [0, 1, 2]\n\n    # expected f1 scores for each class\n    expected_f1_scores = [1.0, 0.0, 0.8]\n\n    f1_scores = f1_score(y_true, y_pred, labels=labels, average=None)\n    \n    assert_array_almost_equal(f1_scores, expected_f1_scores, decimal=2)\n"], "sample_766": ["def test_update_dict_function():\n    # Test the _update_dict function for different scenarios\n    rng = np.random.RandomState(42)\n    \n    n_features, n_components, n_samples = 8, 5, 10\n    dictionary = rng.randn(n_features, n_components)\n    Y = rng.randn(n_features, n_samples)\n    code = rng.randn(n_components, n_samples)\n    \n    # Ensure dictionary update works without positivity constraint\n    updated_dict = _update_dict(dictionary.copy(), Y, code)\n    assert updated_dict.shape == dictionary.shape\n    \n    # Ensure dictionary update works with positivity constraint\n    updated_dict_pos = _update_dict(dictionary.copy(), Y, code, positive=True)\n    assert updated_dict_pos.shape == dictionary.shape\n    assert (updated_dict_pos >= 0).all()\n\n    # Check return_r2 option\n    updated_dict, residuals = _update_dict(dictionary.copy(), Y, code, return_r2=True)\n    assert updated_dict.shape == dictionary.shape\n    assert isinstance(residuals, float)\n    assert residuals >= 0\n", "def test_update_dict():\n    # Testing the _update_dict function\n    n_components = 5\n    n_features = 10\n    n_samples = 8\n\n    rng = np.random.RandomState(0)\n    dictionary = rng.randn(n_features, n_components)\n    Y = rng.randn(n_features, n_samples)\n    code = rng.randn(n_components, n_samples)\n\n    # Update dictionary without positivity constraint\n    updated_dict = _update_dict(dictionary.copy(), Y, code, positive=False)\n    assert updated_dict.shape == (n_features, n_components)\n\n    # Update dictionary with positivity constraint\n    updated_dict_positive = _update_dict(dictionary.copy(), Y, code, positive=True)\n    assert updated_dict_positive.shape == (n_features, n_components)\n    assert (updated_dict_positive >= 0).all()\n\n    # Check if the dictionary is significantly updated\n    assert not np.allclose(updated_dict, dictionary)\n    assert not np.allclose(updated_dict_positive, dictionary)\n\n    # Check dictionary with return_r2=True\n    updated_dict, residuals = _update_dict(dictionary.copy(), Y, code, positive=False, return_r2=True)\n    assert isinstance(residuals, float)\n", "def test_sparse_coder_split_sign():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    \n    sc = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',\n                     transform_alpha=0.001, split_sign=True)\n    code = sc.transform(X)\n    \n    assert code.shape == (X.shape[0], 2 * n_components)\n    assert_array_almost_equal(code[:, :n_components] - code[:, n_components:], \n                              sparse_encode(X, V, algorithm='lasso_lars', alpha=0.001))\n", "def test_dict_learning_online_partial_fit_with_positive_code_dict():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n\n    # With positive code and dictionary constraints\n    dict1 = MiniBatchDictionaryLearning(n_components, n_iter=10 * len(X),\n                                        batch_size=1, alpha=1, shuffle=False,\n                                        dict_init=V, random_state=0,\n                                        positive_code=True, positive_dict=True).fit(X)\n    dict2 = MiniBatchDictionaryLearning(n_components, alpha=1, n_iter=1,\n                                        dict_init=V, random_state=0, positive_code=True,\n                                        positive_dict=True)\n    for i in range(10):\n        for sample in X:\n            dict2.partial_fit(sample[np.newaxis, :])\n\n    assert not np.all(sparse_encode(X, dict1.components_, alpha=1, positive=True) == 0)\n    assert_array_almost_equal(dict1.components_, dict2.components_, decimal=2)\n    assert (dict1.components_ >= 0).all()\n    assert (dict2.components_ >= 0).all()\n\n    code1 = dict1.transform(X)\n    code2 = dict2.transform(X)\n    assert (code1 >= 0).all()\n    assert (code2 >= 0).all()\n", "def test_dict_learning_partial_fit_random():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    X_random = rng.randn(100, 20)\n    V = rng.randn(n_components, 20)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    dict1 = MiniBatchDictionaryLearning(n_components, n_iter=10 * len(X_random),\n                                        batch_size=1,\n                                        alpha=1, shuffle=True, dict_init=V,\n                                        random_state=0).fit(X_random)\n    dict2 = MiniBatchDictionaryLearning(n_components, alpha=1,\n                                        n_iter=1, dict_init=V,\n                                        random_state=0)\n    for i in range(10):\n        for sample in X_random:\n            dict2.partial_fit(sample[np.newaxis, :])\n\n    assert not np.all(sparse_encode(X_random, dict1.components_, alpha=1) == 0)\n    assert_array_almost_equal(dict1.components_, dict2.components_,\n                              decimal=2)\n"], "sample_767": ["def test_column_transformer_with_mixed_dtypes():\n    # Test ColumnTransformer with mixed dtypes (numeric and strings)\n    X = np.array([[1, 2, 'a'],\n                  [3, 4, 'b'],\n                  [5, 6, 'a']])\n\n    ct = ColumnTransformer(\n        [('scaler', StandardScaler(), [0, 1]),\n         ('encoder', OneHotEncoder(), [2])],\n        remainder='drop'\n    )\n\n    X_trans = ct.fit_transform(X)\n    expected_result = np.array([[-1.22474487, -1.22474487, 1., 0.],\n                                [0., 0., 0., 1.],\n                                [1.22474487, 1.22474487, 1., 0.]])\n    assert_allclose_dense_sparse(X_trans, expected_result)\n\n    # Ensure `get_feature_names` works with mixed dtypes\n    ct.fit(X)\n    assert ct.get_feature_names() == ['scaler__x0', 'scaler__x1', 'encoder__x0_a', 'encoder__x0_b']\n", "def test_column_transformer_invalid_remainder_estimator():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    class InvalidEstimator:\n            return self\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])], remainder=InvalidEstimator())\n    assert_raise_message(ValueError, \"The remainder keyword needs to be one of 'drop', 'passthrough', or estimator\",\n                         ct.fit, X_array)\n    assert_raise_message(ValueError, \"The remainder keyword needs to be one of 'drop', 'passthrough', or estimator\",\n                         ct.fit_transform, X_array)\n", "def test_column_transformer_mixed_types():\n    # Test a ColumnTransformer with mixed types of transformers\n    X = [\n        [1, \"cat\", 0.1],\n        [2, \"dog\", 0.2],\n        [3, \"cat\", 0.3]\n    ]\n    expected_result = np.array([\n        [-1.22474487, 0., 1., 0., 0.1],\n        [0., 1., 0., 1., 0.2],\n        [1.22474487, 0., 1., 0., 0.3]\n    ])\n\n    ct = ColumnTransformer([\n        ('scaler', StandardScaler(), [0]),\n        ('onehot', OneHotEncoder(), [1]),\n        ('passthrough', 'passthrough', [2])\n    ])\n\n    result = ct.fit_transform(X)\n    assert_allclose_dense_sparse(result, expected_result)\n    assert_allclose_dense_sparse(ct.fit(X).transform(X), expected_result)\n", "def test_column_transformer_mixed_types():\n    X_array = np.array([[0, 'a', 1.0], [1, 'b', 0.5], [2, 'c', 2.5]], dtype=object)\n\n    ct = ColumnTransformer(\n        [\n            ('num', StandardScaler(), [0, 2]),\n            ('cat', OneHotEncoder(), [1])\n        ]\n    )\n\n    X_trans = ct.fit_transform(X_array)\n    expected = np.array([\n        [-1.22474487, 0.26726124, 1.0, 0.0, 0.0],\n        [0.0, -1.33630621, 0.0, 1.0, 0.0],\n        [1.22474487, 1.06904497, 0.0, 0.0, 1.0]\n    ])\n\n    assert X_trans.shape == (3, 5)\n    assert_allclose_dense_sparse(X_trans, expected)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n", "def test_column_transformer_mixed_data_types():\n    # Test ColumnTransformer with mixed data types (numerical and categorical)\n    X_mixed = np.array([\n        [1, 'a'],\n        [2, 'b'],\n        [3, 'a'],\n        [4, 'b']\n    ])\n\n    ct = ColumnTransformer([\n        ('num', StandardScaler(), [0]),\n        ('cat', OneHotEncoder(), [1])\n    ])\n\n    expected_result = np.array([\n        [-1.34164079,  1., 0.],\n        [-0.4472136,  0., 1.],\n        [0.4472136,  1., 0.],\n        [1.34164079, 0., 1.],\n    ])\n\n    assert_allclose_dense_sparse(ct.fit_transform(X_mixed), expected_result)\n\n    # Ensure that it can also transform after fitting\n    assert_allclose_dense_sparse(ct.fit(X_mixed).transform(X_mixed), expected_result)\n\n    # Test with a DataFrame\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(X_mixed, columns=['numeric', 'categorical'])\n\n    expected_result_df = np.array([\n        [-1.34164079,  1., 0.],\n        [-0.4472136,  0., 1.],\n        [0.4472136,  1., 0.],\n        [1.34164079, 0., 1.],\n    ])\n\n    assert_allclose_dense_sparse(ct.fit_transform(X_df), expected_result_df)\n    assert_allclose_dense_sparse(ct.fit(X_df).transform(X_df), expected_result_df)\n"], "sample_768": ["def test_get_n_splits_with_none_y_and_groups():\n    # Check get_n_splits when y and groups are None\n    assert_equal(KFold(3).get_n_splits(), 3)\n    assert_equal(StratifiedKFold(3).get_n_splits(), 3)\n    assert_equal(GroupKFold(3).get_n_splits(), 3)\n    assert_equal(ShuffleSplit(3).get_n_splits(), 3)\n    assert_equal(StratifiedShuffleSplit(3).get_n_splits(), 3)\n    assert_equal(TimeSeriesSplit(3).get_n_splits(), 3)\n    assert_equal(LeaveOneOut().get_n_splits(), 10)  # 10 samples in X\n    assert_equal(LeavePOut(2).get_n_splits(), comb(10, 2, exact=True))  # X has 10 samples\n    assert_equal(LeaveOneGroupOut().get_n_splits(groups=np.arange(10)), 10)\n    assert_equal(LeavePGroupsOut(2).get_n_splits(groups=np.arange(10)), comb(10, 2, exact=True))\n    assert_equal(RepeatedKFold(n_splits=2, n_repeats=2).get_n_splits(), 4)\n    assert_equal(RepeatedStratifiedKFold(n_splits=2, n_repeats=2).get_n_splits(), 4)\n    assert_equal(PredefinedSplit(test_fold=[0, 1, -1, 1, 0, -1, 1, 0, 1, 1]).get_n_splits(), 2)\n", "def test_leave_one_out():\n    # Test LeaveOneOut cross-validator\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n\n    expected_splits = [\n        (np.array([1, 2, 3]), np.array([0])),\n        (np.array([0, 2, 3]), np.array([1])),\n        (np.array([0, 1, 3]), np.array([2])),\n        (np.array([0, 1, 2]), np.array([3]))\n    ]\n\n    assert_equal(loo.get_n_splits(X), len(X))\n\n    splits = list(loo.split(X))\n    for expected, result in zip(expected_splits, splits):\n        assert_array_equal(result[0], expected[0])\n        assert_array_equal(result[1], expected[1])\n\n    # Test the string representation\n    assert_equal(repr(loo), \"LeaveOneOut()\")\n\n    # Test the case when X is None\n    with assert_raises(ValueError):\n        loo.get_n_splits(None, y, groups=None)\n", "def test_leave_one_out():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    \n    loo = LeaveOneOut()\n    splits = list(loo.split(X, y))\n    \n    # Check number of splits\n    assert_equal(len(splits), len(X))\n    \n    # Check if all splits are valid and unique\n    for i, (train_index, test_index) in enumerate(splits):\n        assert_equal(len(train_index), len(X) - 1)\n        assert_equal(len(test_index), 1)\n        assert_array_equal(X[test_index], np.array([X[i]]))\n        assert not np.any(np.in1d(train_index, test_index))\n        \n    # Check get_n_splits method\n    assert_equal(loo.get_n_splits(X, y), len(X))\n    \n    # Check if the representation is correct\n    loo_repr = \"LeaveOneOut()\"\n    assert_equal(repr(loo), loo_repr)\n", "def test_leave_p_out():\n    # Test LeavePOut with p=2\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    lpo = LeavePOut(p=2)\n    splits = list(lpo.split(X, y))\n    expected_splits = [\n        (np.array([2, 3]), np.array([0, 1])),\n        (np.array([1, 3]), np.array([0, 2])),\n        (np.array([1, 2]), np.array([0, 3])),\n        (np.array([0, 3]), np.array([1, 2])),\n        (np.array([0, 2]), np.array([1, 3])),\n        (np.array([0, 1]), np.array([2, 3])),\n    ]\n    for split, expected_split in zip(splits, expected_splits):\n        train, test = split\n        expected_train, expected_test = expected_split\n        assert_array_equal(train, expected_train)\n        assert_array_equal(test, expected_test)\n    \n    # Test get_n_splits\n    assert_equal(lpo.get_n_splits(X, y), 6)\n    \n    # Test with single sample input to raise ValueError\n    X_single_sample = np.array([[1, 2]])\n    y_single_sample = np.array([1])\n    lpo = LeavePOut(p=1)\n    assert_raise_message(ValueError, \"The 'X' parameter should not be None.\", lpo.get_n_splits, None, y_single_sample)\n", "def test_leave_one_group_out_split():\n    # Test LeaveOneGroupOut split method for expected behavior\n    X = np.arange(10).reshape((5, 2))\n    y = np.array([0, 1, 0, 1, 2])\n    groups = np.array([1, 1, 2, 2, 3])\n\n    logo = LeaveOneGroupOut()\n    splits = list(logo.split(X, y, groups))\n\n    expected_splits = [\n        (np.array([2, 3, 4]), np.array([0, 1])),\n        (np.array([0, 1, 4]), np.array([2, 3])),\n        (np.array([0, 1, 2, 3]), np.array([4]))\n    ]\n\n    for (train, test), (exp_train, exp_test) in zip(splits, expected_splits):\n        assert_array_equal(train, exp_train)\n        assert_array_equal(test, exp_test)\n"], "sample_769": ["def test_multilabel_confusion_matrix_sparse():\n    # Test multilabel confusion matrix with sparse input\n    from scipy.sparse import csr_matrix\n\n    y_true = csr_matrix(np.array([[0, 1, 1], [1, 0, 1], [0, 0, 1]]))\n    y_pred = csr_matrix(np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]]))\n\n    expected_cm = np.array([[[1, 1], [0, 1]],\n                            [[1, 1], [0, 1]],\n                            [[0, 1], [0, 2]]])\n\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, expected_cm)\n\n    # Test with sample weights\n    sample_weight = np.array([1, 2, 3])\n    expected_cm_weighted = np.array([[[1, 1], [0, 3]],\n                                     [[3, 2], [0, 2]],\n                                     [[0, 3], [0, 3]]])\n\n    cm_weighted = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    assert_array_equal(cm_weighted, expected_cm_weighted)\n", "def test_zero_one_loss_multiclass():\n    # Test zero_one_loss with multiclass classification task\n    y_true, y_pred, _ = make_prediction(binary=False)\n\n    # compute zero-one loss with default settings\n    loss = zero_one_loss(y_true, y_pred)\n    assert_almost_equal(loss, 1 - accuracy_score(y_true, y_pred))\n\n    # compute zero-one loss with normalize=False\n    loss = zero_one_loss(y_true, y_pred, normalize=False)\n    assert_equal(loss, np.sum(y_true != y_pred))\n\n    # compute zero-one loss with sample weights\n    sample_weight = np.ones_like(y_true)\n    loss = zero_one_loss(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(loss, 1 - accuracy_score(y_true, y_pred))\n\n    # compute zero-one loss with sample weights and normalize=False\n    sample_weight = np.ones_like(y_true)\n    loss = zero_one_loss(y_true, y_pred, sample_weight=sample_weight, normalize=False)\n    assert_equal(loss, np.sum(y_true != y_pred))\n\n    # Check for perfect prediction\n    loss = zero_one_loss(y_true, y_true)\n    assert_equal(loss, 0)\n\n    # Check for completely wrong prediction\n    wrong_pred = np.zeros_like(y_true)\n    loss = zero_one_loss(y_true, wrong_pred)\n    assert_equal(loss, 1)\n", "def test_accuracy_score_multiclass():\n    # Test accuracy score for multiclass classification\n    y_true = [0, 1, 2, 2, 1, 0, 1, 2]\n    y_pred = [0, 2, 2, 2, 0, 0, 1, 2]\n\n    # Calculate accuracy score with normalization\n    accuracy = accuracy_score(y_true, y_pred)\n    assert_almost_equal(accuracy, 0.625)\n\n    # Calculate accuracy score without normalization\n    accuracy = accuracy_score(y_true, y_pred, normalize=False)\n    assert_equal(accuracy, 5)\n\n    # Check with sample_weight\n    sample_weight = [1, 2, 3, 4, 5, 6, 7, 8]\n    accuracy = accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    expected_accuracy = np.average([1, 0, 1, 1, 0, 1, 1, 1], weights=sample_weight)\n    assert_almost_equal(accuracy, expected_accuracy)\n", "def test_confusion_matrix_multilabel():\n    # Test confusion matrix in a multilabel setting\n    y_true = [[1, 0, 1], [0, 1, 0], [1, 1, 0]]\n    y_pred = [[1, 0, 0], [0, 1, 1], [1, 0, 0]]\n\n    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n    \n    # Expected confusion matrix:\n    # True class 0:\n    # [2 (True Negatives), 0 (False Positives)]\n    # [1 (False Negatives), 0 (True Positives)]\n    # \n    # True class 1:\n    # [1 (True Negatives), 1 (False Positives)]\n    # [1 (False Negatives), 1 (True Positives)]\n    \n    expected_cm = [\n        [2, 0],\n        [1, 0],\n        [1, 1],\n        [1, 1]\n    ]\n    \n    assert_array_equal(cm, expected_cm)\n", "def test_confusion_matrix_multiclass_with_sample_weight():\n    # Test confusion matrix with sample weights\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n    sample_weight = [1, 2, 3, 4, 5, 6]\n\n    # Compute confusion matrix with sample weights\n    cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    expected_cm = np.array([[5, 0, 0],\n                            [0, 0, 2],\n                            [0, 6, 0]])\n\n    assert_array_equal(cm, expected_cm)\n\n    # Test with different sample weights\n    sample_weight = [2, 3, 4, 5, 6, 7]\n    cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    expected_cm = np.array([[7, 0, 0],\n                            [0, 0, 3],\n                            [0, 7, 0]])\n\n    assert_array_equal(cm, expected_cm)\n\n    # Test with uniform sample weights (should behave like no weights)\n    sample_weight = [1, 1, 1, 1, 1, 1]\n    cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    expected_cm = np.array([[2, 0, 0],\n                            [0, 0, 1],\n                            [0, 2, 1]])\n\n    assert_array_equal(cm, expected_cm)\n"], "sample_770": ["def test_silhouette_samples_sparse():\n    # Test silhouette_samples with sparse matrices\n    dataset = datasets.load_iris()\n    X_dense = dataset.data\n    y = dataset.target\n\n    for sparse_format in [csr_matrix, sp.csc_matrix, sp.coo_matrix, sp.dok_matrix, sp.lil_matrix]:\n        X_sparse = sparse_format(X_dense)\n        sil_samples_dense = silhouette_samples(X_dense, y)\n        sil_samples_sparse = silhouette_samples(X_sparse, y)\n        assert_array_equal(sil_samples_dense, sil_samples_sparse)\n", "def test_davies_bouldin_score_with_single_sample_clusters():\n    # Test Davies-Bouldin score with clusters having a single sample\n    X = [[0.], [1.], [2.], [3.]]\n    labels = np.array([0, 1, 2, 3])\n\n    # Each cluster has one sample, expect the score to be 0.0\n    score = davies_bouldin_score(X, labels)\n    assert score == pytest.approx(0.0)\n\n    # Test with one cluster having multiple samples and others having single samples\n    X = [[0.], [0.5], [1.], [2.], [3.]]\n    labels = np.array([0, 0, 0, 1, 2])\n\n    score = davies_bouldin_score(X, labels)\n    assert score > 0.0  # Expect a positive score as there is some intra-cluster distance\n\n    # Ensure divide by zero warning is not raised\n    with pytest.warns(None) as record:\n        davies_bouldin_score(X, labels)\n    div_zero_warnings = [\n        warning for warning in record\n        if \"divide by zero encountered\" in warning.message.args[0]\n    ]\n    assert len(div_zero_warnings) == 0\n", "def test_invalid_metric_silhouette_score():\n    # Test silhouette_score with an invalid metric\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = dataset.target\n\n    with pytest.raises(ValueError, match=\".*is not recognized. Valid options are.*\"):\n        silhouette_score(X, y, metric='invalid_metric')\n", "def test_silhouette_score_with_callable_metric():\n    # Test silhouette_score with a custom metric (callable)\n        return np.abs(x - y).sum()\n\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = dataset.target\n\n    score_custom_metric = silhouette_score(X, y, metric=custom_metric)\n    assert_greater(score_custom_metric, 0)\n\n    # Confirm consistency when using the custom metric directly\n    D = pairwise_distances(X, metric=custom_metric)\n    score_precomputed = silhouette_score(D, y, metric='precomputed')\n    assert_greater(score_precomputed, 0)\n    pytest.approx(score_custom_metric, score_precomputed)\n", "def test_davies_bouldin_with_identical_samples():\n    # Test Davies-Bouldin score with clusters containing identical samples\n    X = np.array([[1.0, 2.0], [1.0, 2.0], [1.0, 2.0], [3.0, 4.0], [3.0, 4.0]])\n    labels = np.array([0, 0, 0, 1, 1])\n\n    # Cluster 0 has identical samples, intra-cluster distance should be 0\n    # Cluster 1 has identical samples, intra-cluster distance should be 0\n    # The inter-cluster distance will be non-zero\n    score = davies_bouldin_score(X, labels)\n    assert score == pytest.approx(0.0)\n\n    # Test with single sample per cluster, should return 0.0\n    X_single = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n    labels_single = np.array([0, 1, 2])\n    score_single = davies_bouldin_score(X_single, labels_single)\n    assert score_single == pytest.approx(0.0)\n"], "sample_771": ["def test_handle_zeros_in_scale_scalar_input():\n    # Test _handle_zeros_in_scale with scalar input\n    assert_almost_equal(_handle_zeros_in_scale(0.0), 1.0)\n    assert_almost_equal(_handle_zeros_in_scale(5.0), 5.0)\n    assert_almost_equal(_handle_zeros_in_scale(-3.0), -3.0)\n", "def test_handle_zeros_in_scale_with_scalar():\n    # Check _handle_zeros_in_scale with scalar input\n    assert _handle_zeros_in_scale(0) == 1.0\n    assert _handle_zeros_in_scale(1) == 1.0\n    assert _handle_zeros_in_scale(-1) == -1.0\n    assert _handle_zeros_in_scale(0.0) == 1.0\n    assert _handle_zeros_in_scale(1.0) == 1.0\n    assert _handle_zeros_in_scale(-1.0) == -1.0\n", "def test_add_dummy_feature_with_value():\n    # Test adding dummy feature with a specific value\n    X = [[1, 0], [0, 1], [0, 1]]\n    value = 3.5\n    X_with_dummy = add_dummy_feature(X, value=value)\n    expected_output = [[value, 1, 0], [value, 0, 1], [value, 0, 1]]\n    assert_array_equal(X_with_dummy, expected_output)\n\n    # Test with sparse matrix input\n    X_sparse = sparse.csr_matrix([[1, 0], [0, 1], [0, 1]])\n    X_with_dummy_sparse = add_dummy_feature(X_sparse, value=value)\n    assert sparse.isspmatrix_csr(X_with_dummy_sparse)\n    assert_array_equal(X_with_dummy_sparse.toarray(), expected_output)\n", "def test_handle_zeros_in_scale_scalar():\n    # Test _handle_zeros_in_scale with scalar input\n    assert _handle_zeros_in_scale(0.0) == 1.0\n    assert _handle_zeros_in_scale(5.0) == 5.0\n    assert _handle_zeros_in_scale(-3.0) == -3.0\n\n    # Test _handle_zeros_in_scale with array input\n    scale = np.array([0.0, 1.0, 2.0, -3.0])\n    expected = np.array([1.0, 1.0, 2.0, -3.0])\n    assert_array_equal(_handle_zeros_in_scale(scale), expected)\n", "def test_min_max_scaler_sparse_input():\n    # Test MinMaxScaler on sparse input with zero variance features\n    X = sparse.csr_matrix([[0., 1., 0.5],\n                           [0., 1., -0.3],\n                           [0., 1., 1.5]])\n\n    scaler = MinMaxScaler()\n    X_trans = scaler.fit_transform(X)\n    X_expected = sparse.csr_matrix([[0., 0., 0.5],\n                                    [0., 0., 0.],\n                                    [0., 0., 1.]])\n    assert_array_almost_equal(X_trans.toarray(), X_expected.toarray())\n    X_trans_inv = scaler.inverse_transform(X_trans)\n    assert_array_almost_equal(X.toarray(), X_trans_inv.toarray())\n\n    # Check that inverse_transform works correctly on new data\n    X_new = sparse.csr_matrix([[0., 2., 0.5],\n                               [-1., 1., 0.0],\n                               [0., 1., 1.5]])\n    X_trans_new = scaler.transform(X_new)\n    X_expected_new = sparse.csr_matrix([[0., 2., 0.5],\n                                        [-1., 1., 0.],\n                                        [0., 1., 1.5]])\n    assert_array_almost_equal(X_trans_new.toarray(), X_expected_new.toarray())\n"], "sample_772": ["def check_feature_importances(name, X, y):\n    \"\"\"Check that the feature importances are consistent and non-negative.\"\"\"\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(random_state=0, bootstrap=False)\n    est.fit(X, y)\n    \n    importances = est.feature_importances_\n    assert importances.shape[0] == X.shape[1], \"Feature importances shape mismatch\"\n    assert np.all(importances >= 0), \"Feature importances must be non-negative\"\n    assert np.isclose(np.sum(importances), 1.0), \"Feature importances must sum to 1\"\n", "def check_generate_sample_indices():\n    # Check the functionality of _generate_sample_indices\n    random_state = 42\n    n_samples = 10\n    indices = _generate_sample_indices(random_state, n_samples)\n    \n    assert_equal(len(indices), n_samples, \"Generated indices length mismatch\")\n    assert all(isinstance(i, int) for i in indices), \"Indices should be integers\"\n    assert max(indices) < n_samples, \"Indices exceed number of samples\"\n    assert min(indices) >= 0, \"Indices should be non-negative\"\n", "def test_feature_importances():\n    # Check if feature_importances_ attribute is correctly computed\n    for name, ForestEstimator in FOREST_ESTIMATORS.items():\n        est = ForestEstimator(n_estimators=10, random_state=0)\n        est.fit(X_large, y_large)\n        importances = est.feature_importances_\n        \n        # Ensure the sum of feature importances is 1.0\n        assert_almost_equal(importances.sum(), 1.0, decimal=5,\n                            err_msg=\"Feature importances do not sum to 1.0 for {0}\".format(name))\n        \n        # Ensure all importances are non-negative\n        assert np.all(importances >= 0.0), \"Feature importances contain negative values for {0}\".format(name)\n\n        # Ensure importances are correctly computed\n        if hasattr(est, 'estimators_'):\n            aggregated_importances = np.mean([tree.feature_importances_ for tree in est.estimators_], axis=0)\n            assert_array_almost_equal(importances, aggregated_importances, decimal=5,\n                                      err_msg=\"Aggregated feature importances do not match computed importances for {0}\".format(name))\n", "def check_fit_transform(name):\n    # Test if fit_transform works correctly for RandomTreesEmbedding\n    if name in FOREST_TRANSFORMERS:\n        X, y = datasets.make_classification(n_samples=100, n_features=4,\n                                            random_state=0)\n        transformer = FOREST_TRANSFORMERS[name](n_estimators=10,\n                                                random_state=0)\n        X_transformed = transformer.fit_transform(X, y)\n        assert X_transformed.shape[0] == X.shape[0]\n        assert isinstance(X_transformed, (np.ndarray, csr_matrix))\n", "def check_bootstrap_consistency(name):\n    # Test consistency of bootstrap sampling\n    ForestEstimator = FOREST_ESTIMATORS[name]\n\n    X, y = datasets.make_classification(n_samples=100, n_features=4,\n                                        n_informative=2, n_redundant=0,\n                                        random_state=42)\n\n    n_estimators = 50\n    est = ForestEstimator(n_estimators=n_estimators, bootstrap=True, random_state=42)\n    est.fit(X, y)\n    \n    # Check if each estimator was trained on a bootstrap sample\n    for estimator in est.estimators_:\n        bootstrap_sample = _generate_sample_indices(estimator.random_state, len(X))\n        unique_samples = np.unique(bootstrap_sample)\n        assert len(unique_samples) < len(X), \"Bootstrap sample should not contain all unique samples.\"\n"], "sample_773": ["def test_multinomial_loss_grad():\n    # Check the consistency of multinomial loss and gradient functions\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=3, random_state=0)\n    Y = LabelBinarizer().fit_transform(y)\n    w = np.zeros(X.shape[1] * 3 + 3)  # Shape (n_classes * (n_features + 1),)\n    sample_weight = np.ones(X.shape[0])\n\n    # Compute loss and gradient using _multinomial_loss_grad\n    loss, grad, p = _multinomial_loss_grad(w, X, Y, alpha=1., sample_weight=sample_weight)\n\n    # Approximate gradient using finite differences\n    approx_grad = optimize.approx_fprime(\n        w, lambda w: _multinomial_loss_grad(w, X, Y, alpha=1., sample_weight=sample_weight)[0], 1e-5\n    )\n\n    assert_array_almost_equal(grad, approx_grad, decimal=3, err_msg=\"Gradient check for multinomial loss failed.\")\n", "def test_logistic_regression_with_sample_weights_and_class_weights():\n    # Test sample_weights and class_weights combined to see their effect.\n    X, y = make_classification(n_samples=100, n_features=4, random_state=0)\n    sample_weight = np.random.RandomState(0).rand(y.shape[0])\n    \n    class_weight = {0: 1, 1: 2}\n    clf = LogisticRegression(solver='liblinear', class_weight=class_weight, random_state=0)\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    clf_no_weights = LogisticRegression(solver='liblinear', random_state=0)\n    clf_no_weights.fit(X, y)\n\n    assert clf.coef_.shape == clf_no_weights.coef_.shape\n    assert not np.allclose(clf.coef_, clf_no_weights.coef_)\n", "def test_logistic_regression_solver_penalty_combinations(solver, penalty):\n    # Test that all valid solver and penalty combinations work as expected\n    if solver in ['newton-cg', 'sag', 'lbfgs'] and penalty == 'l1':\n        with pytest.raises(ValueError, match=f\"Solver {solver} supports only 'l2' or 'none' penalties, got {penalty} penalty.\"):\n            LogisticRegression(solver=solver, penalty=penalty).fit(X, Y1)\n    elif solver != 'saga' and penalty == 'elasticnet':\n        with pytest.raises(ValueError, match=f\"Only 'saga' solver supports elasticnet penalty, got solver={solver}.\"):\n            LogisticRegression(solver=solver, penalty=penalty).fit(X, Y1)\n    else:\n        clf = LogisticRegression(solver=solver, penalty=penalty)\n        clf.fit(X, Y1)\n        assert clf.coef_.shape == (1, len(X[0]))\n        assert clf.intercept_.shape == (1,)\n        assert clf.predict(X).shape == (len(X),)\n", "def test_logistic_regression_path_multiple_classes(solver, multi_class):\n    # Test logistic_regression_path function with multiple classes and ensure\n    # proper shape and consistency of coefficients.\n    X, y = make_classification(n_samples=200, n_classes=3, n_informative=2,\n                               n_redundant=0, random_state=0, n_features=2)\n    Cs = np.logspace(-4, 4, 5)\n    coefs, Cs, n_iter = _logistic_regression_path(X, y, Cs=Cs, solver=solver,\n                                                  multi_class=multi_class,\n                                                  max_iter=100)\n\n    assert coefs.shape[0] == len(Cs)\n    if multi_class == 'multinomial':\n        assert coefs.shape[1] == 3\n        assert coefs.shape[2] == X.shape[1] + 1\n    else:\n        assert coefs.shape[1] == X.shape[1] + 1\n\n    for coef in coefs:\n        if multi_class == 'multinomial':\n            for c in coef:\n                assert len(c) == X.shape[1] + 1\n        else:\n            assert len(coef) == X.shape[1] + 1\n\n    assert len(Cs) == coefs.shape[0]\n    assert len(n_iter) == coefs.shape[0]\n", "def test_logistic_regression_path_penalty_solver_combinations(solver, penalty):\n    # Test different combinations of solvers and penalties in logistic_regression_path\n    # to ensure that the function handles all valid configurations correctly.\n    if solver in ['newton-cg', 'lbfgs', 'sag'] and penalty == 'l1':\n        with pytest.raises(ValueError, match=f\"Solver {solver} supports only 'l2' or 'none' penalties\"):\n            _logistic_regression_path(X, Y1, penalty=penalty, solver=solver)\n    else:\n        Cs = np.logspace(-4, 4, 3)\n        coefs, Cs, _ = _logistic_regression_path(X, Y1, penalty=penalty, Cs=Cs, solver=solver)\n        assert coefs is not None\n        assert Cs is not None\n"], "sample_774": ["def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_partial_fit_transform():\n    # Test OneHotEncoder's fit, transform and inverse_transform separately.\n    X = [['Male', 1], ['Female', 3]]\n    enc = OneHotEncoder(categories='auto')\n    enc.fit(X)\n    X_trans = enc.transform(X).toarray()\n    expected_trans = np.array([[0., 1., 1., 0., 0.],\n                               [1., 0., 0., 0., 1.]])\n    assert_array_equal(X_trans, expected_trans)\n    \n    # Ensure inverse_transform correctly recovers the original data\n    X_inv = enc.inverse_transform(X_trans)\n    expected_inv = np.array([['Male', 1], ['Female', 3]], dtype=object)\n    assert_array_equal(X_inv, expected_inv)\n\n    # Test with dropping the first category\n    enc = OneHotEncoder(categories='auto', drop='first')\n    enc.fit(X)\n    X_trans = enc.transform(X).toarray()\n    expected_trans_drop = np.array([[1., 1., 0.],\n                                    [0., 0., 1.]])\n    assert_array_equal(X_trans, expected_trans_drop)\n\n    # Ensure inverse_transform correctly recovers the original data\n    X_inv = enc.inverse_transform(X_trans)\n    expected_inv_drop = np.array([['Male', 1], ['Female', 3]], dtype=object)\n    assert_array_equal(X_inv, expected_inv_drop)\n\n    # Test with sparse output\n    enc = OneHotEncoder(categories='auto', sparse=True)\n    enc.fit(X)\n    X_trans = enc.transform(X)\n    expected_trans_sparse = sparse.csr_matrix(expected_trans)\n    assert_array_equal(X_trans.toarray(), expected_trans_sparse.toarray())\n\n    # Ensure inverse_transform correctly recovers the original data\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X_inv, expected_inv)\n", "def test_one_hot_encoder_drop_single_category():\n    # Test OneHotEncoder's behavior when a feature has only one category\n    X = [['Male', 1], ['Male', 1]]\n    enc = OneHotEncoder(drop='first')\n    X_trans = enc.fit_transform(X).toarray()\n    # Since 'Male' and 1 are the only categories and are dropped, the output should be empty\n    assert_equal(X_trans.shape, (2, 0))\n    assert_array_equal(X_trans, np.empty((2, 0)))\n\n    # Test inverse transformation for the same\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X, X_inv)\n\n    # Test handling unknown categories with drop\n    enc = OneHotEncoder(handle_unknown='ignore', drop='first')\n    X_trans = enc.fit_transform(X).toarray()\n    assert_equal(X_trans.shape, (2, 0))\n    assert_array_equal(X_trans, np.empty((2, 0)))\n\n    X_test = [['Female', 2]]\n    X_trans_test = enc.transform(X_test).toarray()\n    assert_equal(X_trans_test.shape, (1, 0))\n    assert_array_equal(X_trans_test, np.empty((1, 0)))\n\n    X_inv_test = enc.inverse_transform(X_trans_test)\n    expected_inv = [[None, None]]\n    assert_array_equal(X_inv_test, expected_inv)\n", "def test_ordinal_encoder_unseen_categories():\n    # Test OrdinalEncoder raises error when unseen categories are encountered\n    X_train = [['Male', 1], ['Female', 3], ['Female', 2]]\n    X_test = [['Male', 4], ['Female', 1]]\n\n    enc = OrdinalEncoder(categories='auto')\n    enc.fit(X_train)\n    \n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_test)\n    \n    # Test ignore unknown categories during transform\n    enc = OrdinalEncoder(categories='auto', dtype=np.float64)\n    enc.fit(X_train)\n    X_transformed = enc.transform([['Male', 4], ['Female', 1]])\n    expected_transformed = np.array([[1., np.nan], [0., 0.]])\n    \n    assert_array_equal(X_transformed, expected_transformed)\n", "def test_ordinal_encoder_unseen_categories():\n    # Test OrdinalEncoder with unseen categories\n    X_train = [['dog'], ['cat']]\n    X_test = [['dog'], ['mouse']]\n\n    enc = OrdinalEncoder()\n    enc.fit(X_train)\n\n    # Check the encoding of the training data\n    X_train_transformed = enc.transform(X_train)\n    assert_array_equal(X_train_transformed, [[1], [0]])\n\n    # Check that unseen category raises an error during transform\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_test)\n"], "sample_775": ["def test_custom_dict_pretty_print():\n    # Test that dictionaries are pretty-printed correctly with the compact=True parameter\n    custom_dict = {\n        'a': 1,\n        'b': 2,\n        'c': 3,\n        'd': 4,\n        'e': 5,\n        'f': 6,\n        'g': 7,\n        'h': 8,\n        'i': 9,\n        'j': 10,\n        'k': 11,\n        'l': 12,\n        'm': 13,\n        'n': 14,\n        'o': 15,\n        'p': 16,\n        'q': 17,\n        'r': 18,\n        's': 19,\n        't': 20,\n        'u': 21,\n        'v': 22,\n        'w': 23,\n        'x': 24,\n        'y': 25,\n        'z': 26\n    }\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=10)\n    expected = r\"\"\"", "def test_indent_at_name():\n    # Ensure that the indent_at_name parameter correctly affects indentation\n    set_config(print_changed_only=False)\n    \n    pp = _EstimatorPrettyPrinter(compact=True, indent=2, indent_at_name=True)\n    lr = LogisticRegression(C=99, class_weight=.4, fit_intercept=False, tol=1234, verbose=True)\n    expected = \"\"\"", "def test_estimator_with_custom_init():\n    # Test an estimator with a custom __init__ method\n    class CustomEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n            return {'param1': self.param1, 'param2': self.param2}\n\n    est = CustomEstimator(param1=42, param2='custom')\n    expected = \"CustomEstimator(param1=42, param2='custom')\"\n    assert est.__repr__() == expected\n\n    est = CustomEstimator()\n    expected = \"CustomEstimator(param1=1, param2='default')\"\n    assert est.__repr__() == expected\n", "def test_nested_pipeline_gridsearch():\n    # Render a nested pipeline inside a gridsearch\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    \n    pipeline = Pipeline([\n        ('reduce_dim', PCA()),\n        ('feature_selection', SelectKBest(chi2)),\n        ('classify', SVC())\n    ])\n    \n    N_FEATURES_OPTIONS = [2, 4, 8]\n    C_OPTIONS = [1, 10, 100, 1000]\n    K_OPTIONS = [1, 2, 3]\n\n    param_grid = [\n        {\n            'reduce_dim__n_components': N_FEATURES_OPTIONS,\n            'feature_selection__k': K_OPTIONS,\n            'classify__C': C_OPTIONS,\n            'classify__kernel': ['linear', 'rbf']\n        }\n    ]\n    gspipeline = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid)\n    expected = \"\"\"", "def test_custom_key_val_tuple():\n    # Test the custom KeyValTuple and KeyValTupleParam classes\n\n    # Test KeyValTuple\n    key_val = KeyValTuple(('key', 'value'))\n    expected = \"('key', 'value')\"\n    assert repr(key_val) == expected\n\n    # Test KeyValTupleParam\n    key_val_param = KeyValTupleParam(('param', 'value'))\n    expected = \"('param', 'value')\"\n    assert repr(key_val_param) == expected\n"], "sample_776": ["def test_lars_path_max_iter():\n    # Test that the max_iter parameter works correctly in lars_path\n    max_iter = 5\n    alphas_, active, coef_path_ = linear_model.lars_path(\n        diabetes.data, diabetes.target, method=\"lar\", max_iter=max_iter)\n    \n    # Ensure that the number of iterations does not exceed max_iter\n    assert len(alphas_) <= max_iter + 1\n    assert len(active) <= max_iter\n    assert coef_path_.shape[1] <= max_iter + 1\n", "def test_lars_path_dtype():\n    # Test that the lars_path function works correctly with different dtypes\n\n    X_float32 = X.astype(np.float32)\n    y_float32 = y.astype(np.float32)\n\n    alphas_float32, active_float32, coef_path_float32 = linear_model.lars_path(X_float32, y_float32, method=\"lar\")\n    alphas, active, coef_path = linear_model.lars_path(X, y, method=\"lar\")\n\n    assert_array_almost_equal(alphas_float32, alphas, decimal=6)\n    assert_array_almost_equal(coef_path_float32, coef_path, decimal=6)\n\n    X_float64 = X.astype(np.float64)\n    y_float64 = y.astype(np.float64)\n\n    alphas_float64, active_float64, coef_path_float64 = linear_model.lars_path(X_float64, y_float64, method=\"lar\")\n\n    assert_array_almost_equal(alphas_float64, alphas)\n    assert_array_almost_equal(coef_path_float64, coef_path)\n", "def test_lars_path_positive_constraint():\n    # Test the positive constraint in lars_path function\n\n    # Create a simple dataset\n    X = np.array([[1, 1], [2, 2], [3, 3]])\n    y = np.array([1, 2, 3])\n\n    # Ensure coefficients are non-negative when positive=True\n    alphas, active, coefs = linear_model.lars_path(X, y, method='lasso', positive=True)\n    assert (coefs >= 0).all()\n\n    # Ensure coefficients can be negative when positive=False\n    alphas, active, coefs = linear_model.lars_path(X, y, method='lasso', positive=False)\n    assert (coefs < 0).any()\n", "def test_lars_path_positive_option():\n    # Test that lars_path correctly handles the positive option for both 'lar' and 'lasso' methods\n\n    # Generate a simple dataset\n    X = np.array([[0.5, 1], [1, 2], [1.5, 3], [2, 4], [2.5, 5]])\n    y = np.array([1, 2, 3, 4, 5])\n\n    # Test for method='lar', which should raise a warning\n    with pytest.warns(DeprecationWarning, match=\"positive option is broken for Least\"):\n        alphas, active, coefs = linear_model.lars_path(X, y, method='lar', positive=True)\n    \n    # Ensure that the coefficients are not all non-negative\n    assert np.any(coefs < 0)\n\n    # Test for method='lasso' with positive=True\n    alphas, active, coefs = linear_model.lars_path(X, y, method='lasso', positive=True)\n\n    # Ensure that all coefficients are non-negative\n    assert np.all(coefs >= 0)\n\n    # Test for method='lasso' with positive=False\n    alphas, active, coefs = linear_model.lars_path(X, y, method='lasso', positive=False)\n\n    # Ensure that there are some negative coefficients\n    assert np.any(coefs < 0)\n", "def test_lars_path_warning_on_convergence():\n    # Check that a warning is raised when the system becomes too ill-conditioned\n    X, y = datasets.make_regression(n_samples=10, n_features=20, noise=0.1)\n    with pytest.warns(ConvergenceWarning, match=\"Regressors in active set degenerate\"):\n        linear_model.lars_path(X, y, method=\"lar\", max_iter=5)\n"], "sample_777": ["def test_zero_estimator():\n    # Test ZeroEstimator's fit and predict methods\n\n    # Create a dataset\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y_class = np.array([0, 1, 0])\n    y_regress = np.array([1.5, 2.0, 1.0])\n\n    # Classification\n    clf = ZeroEstimator()\n    clf.fit(X, y_class)\n    assert hasattr(clf, 'n_classes')\n    assert_array_equal(clf.predict(X), np.zeros((3, 1)))\n    assert_array_equal(clf.predict_proba(X), np.zeros((3, 1)))\n\n    # Regression\n    reg = ZeroEstimator()\n    reg.fit(X, y_regress)\n    assert hasattr(reg, 'n_classes')\n    assert_array_equal(reg.predict(X), np.zeros((3, 1)))\n", "def test_custom_init_estimator():\n    # Check if a custom init estimator can be used with GradientBoostingClassifier and GradientBoostingRegressor\n    class CustomInitEstimator(BaseEstimator):\n            return self\n\n            return np.zeros(X.shape[0])\n\n            return np.tile([0.5, 0.5], (X.shape[0], 1))\n\n    # GradientBoostingClassifier\n    X, y = datasets.make_classification(n_samples=100, n_features=4, random_state=42)\n    clf = GradientBoostingClassifier(init=CustomInitEstimator(), n_estimators=10, random_state=42)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert y_pred is not None\n\n    # GradientBoostingRegressor\n    X, y = datasets.make_regression(n_samples=100, n_features=4, random_state=42)\n    reg = GradientBoostingRegressor(init=CustomInitEstimator(), n_estimators=10, random_state=42)\n    reg.fit(X, y)\n    y_pred = reg.predict(X)\n    assert y_pred is not None\n", "def test_mean_estimator():\n    # Test MeanEstimator for correct functionality\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    mean_est = MeanEstimator()\n    mean_est.fit(X, y)\n    y_pred = mean_est.predict(X)\n    assert_almost_equal(y_pred.mean(), 2.0, decimal=1)\n    assert_array_almost_equal(y_pred, np.full(X.shape[0], 2.0), decimal=1)\n", "def test_zero_weight_samples():\n    # Check if zero weight samples are handled correctly.\n    X = [[1, 0],\n         [1, 0],\n         [0, 1],\n         [0, 1]]\n    y_clf = [0, 1, 0, 1]\n    y_reg = [0.0, 1.0, 0.0, 1.0]\n    sample_weight = [1, 0, 0, 1]  # ignore the middle two samples\n\n    clf = GradientBoostingClassifier(n_estimators=10, random_state=1)\n    clf.fit(X, y_clf, sample_weight=sample_weight)\n    assert_array_equal(clf.predict([[1, 0], [0, 1]]), [0, 1])\n\n    reg = GradientBoostingRegressor(n_estimators=10, random_state=1)\n    reg.fit(X, y_reg, sample_weight=sample_weight)\n    assert_array_almost_equal(reg.predict([[1, 0], [0, 1]]), [0, 1], decimal=2)\n", "def test_gradient_boosting_zero_init():\n    # Check that ZeroEstimator initialization works correctly\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n\n    # GradientBoostingClassifier with init='zero'\n    gbc = GradientBoostingClassifier(n_estimators=10, init='zero', random_state=42)\n    gbc.fit(X, y)\n    assert_greater(gbc.score(X, y), 0.85)\n\n    # GradientBoostingRegressor with init='zero'\n    X_reg, y_reg = make_regression(n_samples=100, n_features=20, random_state=42)\n    gbr = GradientBoostingRegressor(n_estimators=10, init='zero', random_state=42)\n    gbr.fit(X_reg, y_reg)\n    assert_less(mean_squared_error(y_reg, gbr.predict(X_reg)), 150)\n\n    # Check that invalid init values raise errors\n    gbc_invalid = GradientBoostingClassifier(n_estimators=10, init='invalid', random_state=42)\n    gbr_invalid = GradientBoostingRegressor(n_estimators=10, init='invalid', random_state=42)\n    assert_raises(ValueError, gbc_invalid.fit, X, y)\n    assert_raises(ValueError, gbr_invalid.fit, X_reg, y_reg)\n"], "sample_778": ["def test_invalid_init_method():\n    # Test that an error is raised for invalid initialization methods\n    A = np.ones((2, 2))\n    invalid_init_methods = ['invalid', 123, None]\n\n    for method in invalid_init_methods:\n        msg = f\"Invalid init parameter: got '{method}' instead of one of\"\n        assert_raise_message(ValueError, msg, NMF(init=method).fit, A)\n", "def test_check_init_function():\n    # Test _check_init function for various scenarios\n    rng = np.random.mtrand.RandomState(42)\n    \n    # Valid case\n    A = np.abs(rng.randn(10, 10))\n    try:\n        nmf._check_init(A, (10, 10), \"Test\")\n    except Exception as e:\n        pytest.fail(f\"Unexpected exception {e}\")\n    \n    # Invalid shape\n    B = np.abs(rng.randn(5, 5))\n    with pytest.raises(ValueError) as excinfo:\n        nmf._check_init(B, (10, 10), \"Test\")\n    assert \"Array with wrong shape passed to Test\" in str(excinfo.value)\n    \n    # Negative values\n    C = -1 * np.abs(rng.randn(10, 10))\n    with pytest.raises(ValueError) as excinfo:\n        nmf._check_init(C, (10, 10), \"Test\")\n    assert \"Negative values in data passed to Test\" in str(excinfo.value)\n    \n    # All zeros\n    D = np.zeros((10, 10))\n    with pytest.raises(ValueError) as excinfo:\n        nmf._check_init(D, (10, 10), \"Test\")\n    assert \"Array passed to Test is full of zeros\" in str(excinfo.value)\n", "def test_non_negative_factorization_custom_init():\n    # Test non_negative_factorization with custom initialization\n    random_state = np.random.RandomState(0)\n    A = np.abs(random_state.randn(6, 5))\n    n_components = 4\n    avg = np.sqrt(A.mean() / n_components)\n    H_init = np.abs(avg * random_state.randn(n_components, 5))\n    W_init = np.abs(avg * random_state.randn(6, n_components))\n\n    W, H, n_iter = non_negative_factorization(A, W=W_init, H=H_init, \n                                              n_components=n_components, \n                                              init='custom', solver='cd', \n                                              random_state=0)\n\n    # Verify the output matrices are non-negative\n    assert not ((W < 0).any() or (H < 0).any())\n\n    # Verify the matrices have the correct shapes\n    assert W.shape == (6, 4)\n    assert H.shape == (4, 5)\n\n    # Verify the reconstruction error is reasonable\n    reconstruction_err = np.linalg.norm(A - np.dot(W, H), 'fro')\n    assert_less(reconstruction_err, 1.0)\n", "def test_nmf_fit_transform_equivalence():\n    # Test that fit_transform and fit + transform produce the same results\n    rng = np.random.mtrand.RandomState(42)\n    A = np.abs(rng.randn(6, 5))\n    \n    for solver in ('cd', 'mu'):\n        model = NMF(n_components=3, solver=solver, init='random', random_state=0)\n        \n        W_fit_transform = model.fit_transform(A)\n        model.fit(A)\n        W_fit_then_transform = model.transform(A)\n        \n        assert_array_almost_equal(W_fit_transform, W_fit_then_transform, decimal=6)\n", "def test_check_string_param():\n    # Test various invalid string parameters for _check_string_param\n    msg = \"Invalid solver parameter: got 'invalid_solver' instead of one of\"\n    with pytest.raises(ValueError, match=msg):\n        nmf._check_string_param('invalid_solver', 'both', 'frobenius', 'random')\n\n    msg = \"Invalid regularization parameter: got 'invalid_reg' instead of one of\"\n    with pytest.raises(ValueError, match=msg):\n        nmf._check_string_param('cd', 'invalid_reg', 'frobenius', 'random')\n\n    msg = \"Invalid beta_loss parameter: solver 'cd' does not handle beta_loss = 'kullback-leibler'\"\n    with pytest.raises(ValueError, match=msg):\n        nmf._check_string_param('cd', 'both', 'kullback-leibler', 'random')\n\n    msg = \"Invalid beta_loss parameter: solver 'cd' does not handle beta_loss = 1\"\n    with pytest.raises(ValueError, match=msg):\n        nmf._check_string_param('cd', 'both', 1, 'random')\n\n    msg = \"The multiplicative update ('mu') solver cannot update zeros present in the initialization\"\n    with pytest.warns(UserWarning, match=msg):\n        nmf._check_string_param('mu', 'both', 'frobenius', 'nndsvd')\n"], "sample_779": ["def test_check_sparse_transformer():\n    # Test that sparse transformer correctly handles sparse input and output\n\n    class SparseTransformerCheck(BaseEstimator):\n            self.X_shape_ = check_array(X).shape\n            return self\n\n            return self.fit(X, y).transform(X)\n\n            X = check_array(X, accept_sparse=True)\n            if X.shape[1] != self.X_shape_[1]:\n                raise ValueError('Bad number of features')\n            return sp.csr_matrix(X)\n\n    check_estimator(SparseTransformerCheck)\n", "def test_check_estimators_dtypes():\n    # Check that estimators handle different dtypes correctly\n\n    class EstimatorWithDtypeCheck(BaseEstimator):\n            X = check_array(X, dtype=[np.float64, np.float32, np.int64, np.int32])\n            if y is not None:\n                y = check_array(y, ensure_2d=False)\n            self.fitted_ = True\n            return self\n\n            check_is_fitted(self, 'fitted_')\n            X = check_array(X, dtype=[np.float64, np.float32, np.int64, np.int32])\n            return np.ones(X.shape[0])\n\n    # Check for dtype handling\n    check_estimator(EstimatorWithDtypeCheck())\n", "def test_check_sample_weights_invariance():\n    class SampleWeightInvarianceClassifier(BaseEstimator, ClassifierMixin):\n            X, y = check_X_y(X, y, multi_output=True)\n            self.coef_ = np.ones(X.shape[1])\n            return self\n\n            check_is_fitted(self, 'coef_')\n            X = check_array(X)\n            return np.ones(X.shape[0])\n\n    check_sample_weights_invariance(\"SampleWeightInvarianceClassifier\", SampleWeightInvarianceClassifier())\n", "def test_check_estimators_pickle():\n    # check if we can pickle and unpickle estimators without issues\n    from sklearn.linear_model import LogisticRegression\n\n    # Create an instance of the estimator\n    estimator = LogisticRegression()\n\n    # Set random state and fit the estimator\n    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],\n                      random_state=0, n_features=2, cluster_std=0.1)\n    X -= X.min()\n    set_random_state(estimator)\n    estimator.fit(X, y)\n\n    # Check if pickling and unpickling works correctly\n    check_estimators_pickle(estimator.__class__.__name__, estimator)\n", "def test_check_transformer_general():\n    # Tests that check_transformer_general works with a simple transformer\n\n    class SimpleTransformer(BaseEstimator):\n            self.mean_ = np.mean(X, axis=0)\n            return self\n\n            return X - self.mean_\n\n    check_transformer_general(\"SimpleTransformer\", SimpleTransformer())\n"], "sample_780": ["def test_lda_doc_topic_prior():\n    # Test the effect of different doc_topic_prior values\n    rng = np.random.RandomState(0)\n    n_components, X = _build_sparse_mtx()\n    \n    # Small prior value\n    lda_small_prior = LatentDirichletAllocation(n_components=n_components,\n                                                doc_topic_prior=0.1,\n                                                random_state=rng)\n    lda_small_prior.fit(X)\n    doc_topic_distr_small_prior = lda_small_prior.transform(X)\n    \n    # Large prior value\n    lda_large_prior = LatentDirichletAllocation(n_components=n_components,\n                                                doc_topic_prior=10.0,\n                                                random_state=rng)\n    lda_large_prior.fit(X)\n    doc_topic_distr_large_prior = lda_large_prior.transform(X)\n    \n    # Check that the document-topic distributions are different for different priors\n    assert np.any(doc_topic_distr_small_prior != doc_topic_distr_large_prior)\n\n    # Check that the distributions are normalized\n    assert_array_almost_equal(doc_topic_distr_small_prior.sum(axis=1),\n                              np.ones(doc_topic_distr_small_prior.shape[0]))\n    assert_array_almost_equal(doc_topic_distr_large_prior.sum(axis=1),\n                              np.ones(doc_topic_distr_large_prior.shape[0]))\n", "def test_lda_doc_topic_prior():\n    # Test if doc_topic_prior is set correctly\n    rng = np.random.RandomState(0)\n    n_components, X = _build_sparse_mtx()\n    doc_topic_prior = 0.5\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    doc_topic_prior=doc_topic_prior,\n                                    random_state=rng)\n    lda.fit(X)\n    assert_almost_equal(lda.doc_topic_prior_, doc_topic_prior)\n\n    # Test if default doc_topic_prior is set correctly\n    lda_default = LatentDirichletAllocation(n_components=n_components,\n                                            random_state=rng)\n    lda_default.fit(X)\n    expected_prior = 1. / n_components\n    assert_almost_equal(lda_default.doc_topic_prior_, expected_prior)\n", "def test_lda_doc_topic_prior():\n    # Test LDA with custom doc_topic_prior value\n    rng = np.random.RandomState(0)\n    n_components, X = _build_sparse_mtx()\n    doc_topic_prior = 0.5\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    doc_topic_prior=doc_topic_prior,\n                                    random_state=rng)\n    lda.fit(X)\n    \n    # Verify that doc_topic_prior_ attribute matches the provided value\n    assert_almost_equal(lda.doc_topic_prior_, doc_topic_prior)\n    \n    # Check if the document-topic distribution is properly normalized\n    doc_topic_distr = lda.transform(X)\n    assert (doc_topic_distr > 0.0).any()\n    assert_array_almost_equal(np.sum(doc_topic_distr, axis=1),\n                              np.ones(doc_topic_distr.shape[0]))\n", "def test_lda_max_doc_update_iter():\n    # Test the effect of max_doc_update_iter parameter\n    rng = np.random.RandomState(0)\n    n_components, X = _build_sparse_mtx()\n    \n    # Test with max_doc_update_iter = 1\n    lda_1 = LatentDirichletAllocation(n_components=n_components,\n                                      max_doc_update_iter=1,\n                                      random_state=rng)\n    lda_1.fit(X)\n    \n    # Test with max_doc_update_iter = 100 (default)\n    lda_2 = LatentDirichletAllocation(n_components=n_components,\n                                      max_doc_update_iter=100,\n                                      random_state=rng)\n    lda_2.fit(X)\n\n    # Ensure that the components differ due to different max_doc_update_iter\n    with pytest.raises(AssertionError):\n        assert_array_almost_equal(lda_1.components_, lda_2.components_)\n", "def test_lda_init_latent_vars():\n    # Test initialization of latent variables in LDA\n    rng = np.random.RandomState(0)\n    n_components = 5\n    n_features = 10\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    random_state=rng)\n    lda._init_latent_vars(n_features)\n    \n    assert_equal(lda.components_.shape, (n_components, n_features))\n    assert np.all(lda.components_ > 0)\n    assert_almost_equal(lda.doc_topic_prior_, 1. / n_components)\n    assert_almost_equal(lda.topic_word_prior_, 1. / n_components)\n    \n    # Check exp_dirichlet_component_\n    exp_dirichlet_component = np.exp(_dirichlet_expectation_2d(lda.components_))\n    assert_allclose(lda.exp_dirichlet_component_, exp_dirichlet_component)\n"], "sample_781": ["def test_warm_start_feature_importances():\n    # Test warm start and feature importances consistency with new estimators.\n    X, y = make_classification(n_samples=100, n_features=4, random_state=1)\n    clf = RandomForestClassifier(n_estimators=5, warm_start=True, random_state=1)\n    clf.fit(X, y)\n    initial_importances = clf.feature_importances_\n\n    # Add more estimators\n    clf.set_params(n_estimators=10)\n    clf.fit(X, y)\n    new_importances = clf.feature_importances_\n\n    assert initial_importances.shape == new_importances.shape\n    assert not np.array_equal(initial_importances, new_importances)\n", "def test_feature_importances_zero_sum(name):\n    # Test that feature_importances_ returns zero array when trees are single node trees.\n    X = np.zeros((10, 5))\n    y = np.ones((10,))\n    forest = FOREST_ESTIMATORS[name](n_estimators=10, random_state=42)\n    forest.fit(X, y)\n    importances = forest.feature_importances_\n    assert_array_equal(importances, np.zeros(5, dtype=np.float64))\n", "def test_random_trees_embedding_feature_importances():\n    # Check that RandomTreesEmbedding doesn't have feature_importances_ attribute\n    hasher = RandomTreesEmbedding(n_estimators=10, random_state=0)\n    X, y = datasets.make_classification(n_samples=100, n_features=4, random_state=0)\n    hasher.fit(X, y)\n    with pytest.raises(AttributeError):\n        _ = hasher.feature_importances_\n", "def test_oob_decision_function(name):\n    # Test out-of-bag decision function\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n    clf = ForestClassifier(n_estimators=50, bootstrap=True, oob_score=True, random_state=42)\n    clf.fit(iris.data, iris.target)\n    \n    assert hasattr(clf, \"oob_decision_function_\")\n    assert clf.oob_decision_function_.shape == (iris.data.shape[0], len(clf.classes_))\n    assert_array_almost_equal(np.sum(clf.oob_decision_function_, axis=1), np.ones(iris.data.shape[0]))\n\n    # Check oob_decision_function for multi-output\n    y_multi = np.vstack((iris.target, iris.target[::-1])).T\n    clf.fit(iris.data, y_multi)\n    assert hasattr(clf, \"oob_decision_function_\")\n    assert len(clf.oob_decision_function_) == 2\n    assert clf.oob_decision_function_[0].shape == (iris.data.shape[0], len(clf.classes_[0]))\n    assert clf.oob_decision_function_[1].shape == (iris.data.shape[0], len(clf.classes_[1]))\n", "def test_oob_score_with_class_weight_balanced(name):\n    # Test that oob score is computed correctly with class_weight='balanced'\n    X, y = make_classification(n_samples=100, n_features=4, n_classes=3,\n                               n_informative=3, n_redundant=0, random_state=0)\n    \n    clf = FOREST_CLASSIFIERS[name](oob_score=True, class_weight='balanced', \n                                   random_state=0, n_estimators=20, bootstrap=True)\n    clf.fit(X, y)\n    assert clf.oob_score_ > 0.5, f\"Failed for {name} with oob_score={clf.oob_score_}\"\n"], "sample_782": ["def test_column_transformer_mixed_dtypes():\n    pd = pytest.importorskip('pandas')\n\n    # Mixed types data\n    X_mixed = pd.DataFrame({\n        'num': [1, 2, 3],\n        'cat': ['a', 'b', 'c'],\n        'text': ['apple', 'banana', 'cherry']\n    })\n\n    # Expected transformed result\n    expected_result = np.array([\n        [-1.22474487, 0., 1.22474487, 1., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 1., 0.],\n        [1.22474487, 0., -1.22474487, 0., 0., 1., 0., 0., 0., 1.]\n    ])\n\n    ct = ColumnTransformer([\n        ('num', StandardScaler(), ['num']),\n        ('cat', OneHotEncoder(), ['cat']),\n        ('text', DictVectorizer(), 'text')\n    ], remainder='passthrough')\n\n    # Fit and transform\n    result = ct.fit_transform(X_mixed)\n\n    # Verify the result\n    assert result.shape == (3, 10)\n    assert_allclose_dense_sparse(result, expected_result)\n\n    # Verify get_feature_names\n    feature_names = ct.get_feature_names()\n    expected_feature_names = ['num__num', 'cat__cat=a', 'cat__cat=b', 'cat__cat=c', 'text__apple', 'text__banana', 'text__cherry']\n    assert feature_names[:len(expected_feature_names)] == expected_feature_names\n", "def test_column_transformer_mixed_types_and_remainder():\n    # Test different data types and the impact of the remainder parameter\n    X = np.array([\n        [1, 2.5, 'a', True],\n        [3, 4.5, 'b', False]\n    ], dtype=object)\n\n    # Expected output after applying transformers\n    expected_output_with_remainder = np.array([\n        [1, 2.5, 1, 0, 'a', True],\n        [3, 4.5, 0, 1, 'b', False]\n    ], dtype=object)\n    \n    expected_output_without_remainder = np.array([\n        [1, 2.5, 1, 0],\n        [3, 4.5, 0, 1]\n    ], dtype=object)\n\n    # Column transformer with different types of transformers\n    ct_with_remainder = ColumnTransformer([\n        ('num', StandardScaler(), [0, 1]),\n        ('cat', OneHotEncoder(sparse=False), [2])\n    ], remainder='passthrough')\n\n    ct_without_remainder = ColumnTransformer([\n        ('num', StandardScaler(), [0, 1]),\n        ('cat', OneHotEncoder(sparse=False), [2])\n    ], remainder='drop')\n\n    # Test fit_transform with remainder passthrough\n    output_with_remainder = ct_with_remainder.fit_transform(X)\n    assert_array_equal(output_with_remainder, expected_output_with_remainder)\n\n    # Test fit_transform with remainder drop\n    output_without_remainder = ct_without_remainder.fit_transform(X)\n    assert_array_equal(output_without_remainder, expected_output_without_remainder)\n\n    # Ensure the remainder parameter behaves correctly after fitting\n    assert ct_with_remainder.transformers_[-1][0] == 'remainder'\n    assert ct_with_remainder.transformers_[-1][1] == 'passthrough'\n    assert ct_without_remainder.transformers_[-1][0] == 'remainder'\n    assert ct_without_remainder.transformers_[-1][1] == 'drop'\n", "def test_column_transformer_mixed_types():\n    # Create a mixed type array\n    X_mixed = np.array([\n        [1, 'a', 3.0],\n        [4, 'b', 5.0],\n        [6, 'c', 7.0]\n    ])\n\n    # Define transformers for each column type\n    ct = ColumnTransformer([\n        ('num', StandardScaler(), [0]),\n        ('cat', OneHotEncoder(), [1]),\n        ('float', Normalizer(), [2])\n    ])\n\n    # Fit and transform the data\n    X_transformed = ct.fit_transform(X_mixed)\n\n    # Expected transformation\n    expected = np.hstack([\n        StandardScaler().fit_transform(np.array([[1], [4], [6]])),\n        OneHotEncoder().fit_transform(np.array([['a'], ['b'], ['c']])).toarray(),\n        Normalizer().fit_transform(np.array([[3.0], [5.0], [7.0]]))\n    ])\n\n    # Check the transformed data\n    assert_allclose_dense_sparse(X_transformed, expected)\n", "def test_column_transformer_n_jobs():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])], n_jobs=2)\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert ct.n_jobs == 2\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])], n_jobs=-1)\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert ct.n_jobs == -1\n\n    # Check if 'n_jobs' can be updated correctly with set_params\n    ct.set_params(n_jobs=3)\n    assert ct.n_jobs == 3\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n\n    # Check if 'n_jobs' is correctly passed to Parallel in fit_transform\n    X = np.random.randn(100, 3)\n    ct = ColumnTransformer([('scaler', StandardScaler(), [0, 1, 2])], n_jobs=2)\n    assert_array_equal(ct.fit_transform(X), StandardScaler().fit_transform(X))\n", "def test_column_transformer_with_different_transformers():\n    X_array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).T\n\n    ct = ColumnTransformer([\n        ('scaler', StandardScaler(), [0]),\n        ('normalizer', Normalizer(), [1]),\n        ('onehot', OneHotEncoder(categories='auto'), [2])\n    ])\n    \n    X_trans = ct.fit_transform(X_array)\n    \n    scaler = StandardScaler().fit(X_array[:, [0]])\n    normalizer = Normalizer().fit(X_array[:, [1]])\n    onehot = OneHotEncoder(categories='auto').fit(X_array[:, [2]])\n    \n    expected_result = np.hstack([\n        scaler.transform(X_array[:, [0]]),\n        normalizer.transform(X_array[:, [1]]),\n        onehot.transform(X_array[:, [2]]).toarray()\n    ])\n    \n    assert_allclose_dense_sparse(X_trans, expected_result)\n    \n    assert_array_equal(ct.fit(X_array).transform(X_array), expected_result)\n    \n    # Ensure that transformers_ attribute is correctly populated\n    assert len(ct.transformers_) == 3\n    assert isinstance(ct.transformers_[0][1], StandardScaler)\n    assert isinstance(ct.transformers_[1][1], Normalizer)\n    assert isinstance(ct.transformers_[2][1], OneHotEncoder)\n"], "sample_783": ["def test_imputation_dense_input_with_different_dtypes():\n    # Test imputation with dense input arrays having different dtypes\n    X_float = np.array([\n        [np.nan, 1.1, 0, np.nan],\n        [1.2, np.nan, 1.3, np.nan],\n        [0, 0, np.nan, np.nan],\n        [1.4, 1.5, 0, np.nan]\n    ], dtype=np.float64)\n\n    X_int = np.array([\n        [-1, 2, 3, -1],\n        [4, -1, 5, -1],\n        [6, 7, -1, -1],\n        [8, 9, 0, -1]\n    ], dtype=np.int32)\n\n    X_object = np.array([\n        [None, \"a\", \"b\", None],\n        [\"c\", None, \"d\", None],\n        [\"e\", \"f\", None, None],\n        [\"g\", \"h\", \"i\", None]\n    ], dtype=object)\n\n    # Impute using constant strategy\n    imputer_float = SimpleImputer(strategy=\"constant\", fill_value=-1.0)\n    X_float_imputed = imputer_float.fit_transform(X_float)\n    X_float_expected = np.array([\n        [-1.0, 1.1, 0, -1.0],\n        [1.2, -1.0, 1.3, -1.0],\n        [0, 0, -1.0, -1.0],\n        [1.4, 1.5, 0, -1.0]\n    ], dtype=np.float64)\n    assert_allclose(X_float_imputed, X_float_expected)\n\n    imputer_int = SimpleImputer(strategy=\"constant\", fill_value=0)\n    X_int_imputed = imputer_int.fit_transform(X_int)\n    X_int_expected = np.array([\n        [0, 2, 3, 0],\n        [4, 0, 5, 0],\n        [6, 7, 0, 0],\n        [8, 9, 0, 0]\n    ], dtype=np.int32)\n    assert_array_equal(X_int_imputed, X_int_expected)\n\n    imputer_object = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")\n    X_object_imputed = imputer", "def test_simple_imputer_fill_value():\n    # Test that SimpleImputer correctly uses fill_value with the \"constant\" strategy\n    X = np.array([\n        [np.nan, 1, np.nan, 3],\n        [4, np.nan, 6, np.nan],\n        [7, 8, 9, 10]\n    ])\n\n    # Expected result after imputation\n    X_true = np.array([\n        [-1, 1, -1, 3],\n        [4, -1, 6, -1],\n        [7, 8, 9, 10]\n    ])\n\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=-1)\n    X_trans = imputer.fit_transform(X)\n\n    assert_array_equal(X_trans, X_true)\n\n    # Check for string fill_value\n    X_str = np.array([\n        [np.nan, 'a', np.nan, 'c'],\n        ['d', np.nan, 'f', np.nan],\n        ['g', 'h', 'i', 'j']\n    ], dtype=object)\n\n    # Expected result after imputation\n    X_true_str = np.array([\n        ['missing', 'a', 'missing', 'c'],\n        ['d', 'missing', 'f', 'missing'],\n        ['g', 'h', 'i', 'j']\n    ], dtype=object)\n\n    imputer_str = SimpleImputer(strategy=\"constant\", fill_value='missing')\n    X_trans_str = imputer_str.fit_transform(X_str)\n\n    assert_array_equal(X_trans_str, X_true_str)\n", "def test_imputation_mixed_dtypes():\n    # Test imputation with mixed data types (numerical and categorical)\n    X = np.array([\n        [1.0, 'a', np.nan],\n        [2.0, np.nan, 'b'],\n        [np.nan, 'c', 'c'],\n        [4.0, 'd', np.nan]\n    ], dtype=object)\n    \n    X_true_mean = np.array([\n        [1.0, 'a', 'c'],\n        [2.0, 'missing_value', 'b'],\n        [2.3333333333333335, 'c', 'c'],\n        [4.0, 'd', 'missing_value']\n    ], dtype=object)\n\n    X_true_most_frequent = np.array([\n        [1.0, 'a', 'c'],\n        [2.0, 'c', 'b'],\n        [2.0, 'c', 'c'],\n        [4.0, 'd', 'c']\n    ], dtype=object)\n\n    imputer_mean = SimpleImputer(strategy=\"constant\", fill_value=\"missing_value\")\n    X_trans_mean = imputer_mean.fit_transform(X)\n    assert_array_equal(X_trans_mean, X_true_mean)\n\n    imputer_most_frequent = SimpleImputer(strategy=\"most_frequent\")\n    X_trans_most_frequent = imputer_most_frequent.fit_transform(X)\n    assert_array_equal(X_trans_most_frequent, X_true_most_frequent)\n", "def test_imputation_most_frequent_multiple_modes():\n    # Test imputation with the most frequent strategy when multiple modes exist\n    X = np.array([\n        [np.nan, 1, 2, 2],\n        [np.nan, 2, 2, 3],\n        [np.nan, 2, 3, 3],\n        [np.nan, 3, 3, 4]\n    ])\n    \n    # Expected output: the mode for the first column is 2 (smallest value among multiple modes)\n    X_true = np.array([\n        [2, 1, 2, 2],\n        [2, 2, 2, 3],\n        [2, 2, 3, 3],\n        [2, 3, 3, 4]\n    ])\n    \n    imputer = SimpleImputer(strategy=\"most_frequent\")\n    X_trans = imputer.fit_transform(X)\n    \n    assert_array_equal(X_trans, X_true)\n    assert_array_equal(imputer.statistics_, [2, 2, 2, 2])\n", "def test_imputation_empty_column():\n    # Test imputation on a column that is completely empty (all values missing)\n    X = np.array([\n        [np.nan, 1, 2],\n        [np.nan, 4, 5],\n        [np.nan, 7, 8]\n    ])\n\n    # Check that using 'mean' strategy raises a warning and discards the column\n    with pytest.warns(UserWarning, match=\"Deleting\"):\n        imputer = SimpleImputer(strategy=\"mean\", verbose=True)\n        X_trans = imputer.fit_transform(X)\n        assert X_trans.shape[1] == 2  # Only 2 columns should remain\n\n    # Check that using 'constant' strategy does not discard the column\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=-1)\n    X_trans = imputer.fit_transform(X)\n    assert X_trans.shape[1] == 3  # All 3 columns should remain\n    assert_array_equal(X_trans[:, 0], np.array([-1, -1, -1]))\n"], "sample_784": ["def test_calibration_edge_cases():\n    \"\"\"Test calibration with edge cases such as single sample and same class\"\"\"\n    # Single sample\n    X_single = np.array([[1, 2, 3, 4, 5]])\n    y_single = np.array([0])\n    clf_single = LinearSVC()\n    clf_single.fit(X_single, y_single)\n    cal_clf_single = CalibratedClassifierCV(clf_single, method='sigmoid', cv='prefit')\n    \n    # Fit should not raise an error\n    cal_clf_single.fit(X_single, y_single)\n    \n    # Predict probabilities should be consistent with single sample\n    prob_single = cal_clf_single.predict_proba(X_single)\n    assert_array_almost_equal(prob_single, np.array([[1.0, 0.0]]))\n    \n    # All samples with the same class\n    X_same_class = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    y_same_class = np.array([1, 1, 1])\n    clf_same_class = LinearSVC()\n    clf_same_class.fit(X_same_class, y_same_class)\n    cal_clf_same_class = CalibratedClassifierCV(clf_same_class, method='sigmoid', cv='prefit')\n    \n    # Fit should not raise an error\n    cal_clf_same_class.fit(X_same_class, y_same_class)\n    \n    # Predict probabilities should be consistent with the same class\n    prob_same_class = cal_clf_same_class.predict_proba(X_same_class)\n    assert_array_almost_equal(prob_same_class, np.array([[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]))\n", "def test_calibration_curve_custom_bins():\n    \"\"\"Check calibration_curve function with custom bins\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1, 0, 0, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1., 0.3, 0.4, 0.7, 0.6])\n    \n    # Define custom bins\n    custom_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=5)\n    \n    # Manually calculate true and predicted probabilities for custom bins\n    bin_0_0_2 = (y_pred < 0.2).nonzero()[0]\n    bin_0_2_0_4 = ((y_pred >= 0.2) & (y_pred < 0.4)).nonzero()[0]\n    bin_0_4_0_6 = ((y_pred >= 0.4) & (y_pred < 0.6)).nonzero()[0]\n    bin_0_6_0_8 = ((y_pred >= 0.6) & (y_pred < 0.8)).nonzero()[0]\n    bin_0_8_1_0 = (y_pred >= 0.8).nonzero()[0]\n    \n        return y_true[bin].mean(), y_pred[bin].mean()\n    \n    custom_prob_true = []\n    custom_prob_pred = []\n    \n    for bin in [bin_0_0_2, bin_0_2_0_4, bin_0_4_0_6, bin_0_6_0_8, bin_0_8_1_0]:\n        if len(bin) > 0:\n            true, pred = get_bin_stats(bin)\n            custom_prob_true.append(true)\n            custom_prob_pred.append(pred)\n    \n    assert_equal(len(prob_true), len(prob_pred))\n    assert_almost_equal(prob_true, custom_prob_true)\n    assert_almost_equal(prob_pred, custom_prob_pred)\n\n    # Check that an exception is raised when n_bins does not match custom bins length", "def test_calibration_invalid_method():\n    \"\"\"Test that invalid calibration method raises an error\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6,\n                               random_state=42)\n    clf = MultinomialNB().fit(X[:n_samples], y[:n_samples])\n\n    # Invalid method 'foo' should raise a ValueError\n    with pytest.raises(ValueError, match=\"method should be \\\"sigmoid\\\" or \\\"isotonic\\\". Got foo.\"):\n        CalibratedClassifierCV(clf, method='foo').fit(X, y)\n", "def test_calibration_with_sparse_matrix():\n    \"\"\"Test calibration with sparse matrix input\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6, random_state=42)\n\n    # Convert X to sparse matrix\n    X_sparse = sparse.csr_matrix(X)\n\n    X_train, y_train = X_sparse[:n_samples], y[:n_samples]\n    X_test, y_test = X_sparse[n_samples:], y[n_samples:]\n\n    clf = LinearSVC(random_state=42)\n    clf.fit(X_train, y_train)\n\n    for method in ['sigmoid', 'isotonic']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        cal_clf.fit(X_train, y_train)\n        probas = cal_clf.predict_proba(X_test)\n        \n        # Check that sum of probabilities is 1\n        assert_array_almost_equal(np.sum(probas, axis=1), np.ones(len(X_test)))\n\n        # Check that log-loss of calibrated classifier is smaller than\n        # log-loss of uncalibrated classifier\n        uncalibrated_log_loss = log_loss(y_test, clf.decision_function(X_test))\n        calibrated_log_loss = log_loss(y_test, probas)\n        assert_greater_equal(uncalibrated_log_loss, calibrated_log_loss)\n", "def test_calibration_isotonic_regression():\n    \"\"\"Test that IsotonicRegression is used and works correctly\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=n_samples, n_features=2,\n                               random_state=42)\n    \n    X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]\n    X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]\n\n    # Use Naive Bayes as the base estimator for calibration\n    clf = MultinomialNB()\n    clf.fit(X_train, y_train)\n    \n    # Calibrate the classifier with isotonic regression\n    cal_clf = CalibratedClassifierCV(clf, method='isotonic', cv=3)\n    cal_clf.fit(X_train, y_train)\n    \n    # Check that isotonic regression calibration improves Brier score\n    prob_pos_clf = clf.predict_proba(X_test)[:, 1]\n    prob_pos_cal_clf = cal_clf.predict_proba(X_test)[:, 1]\n\n    assert_greater(brier_score_loss(y_test, prob_pos_clf),\n                   brier_score_loss(y_test, prob_pos_cal_clf))\n\n    # Ensure isotonic regression is used\n    for cal in cal_clf.calibrated_classifiers_:\n        for calibrator in cal.calibrators_:\n            assert isinstance(calibrator, IsotonicRegression)\n"], "sample_785": ["def test_time_series_split_with_varying_gaps():\n    # Check that TimeSeriesSplit handles various gaps correctly\n    X = np.arange(20).reshape(10, 2)\n    tscv = TimeSeriesSplit(n_splits=3, max_train_size=None)\n\n    # Manually check that Time Series CV preserves the data ordering with gaps\n    splits = tscv.split(X)\n    train, test = next(splits)\n    assert_array_equal(train, [0, 1, 2])\n    assert_array_equal(test, [3, 4])\n\n    train, test = next(splits)\n    assert_array_equal(train, [0, 1, 2, 3, 4])\n    assert_array_equal(test, [5, 6])\n\n    train, test = next(splits)\n    assert_array_equal(train, [0, 1, 2, 3, 4, 5, 6])\n    assert_array_equal(test, [7, 8])\n\n    # Check that TimeSeriesSplit handles gaps correctly when max_train_size is specified\n    tscv = TimeSeriesSplit(n_splits=3, max_train_size=4)\n    splits = tscv.split(X)\n    train, test = next(splits)\n    assert_array_equal(train, [0, 1, 2])\n    assert_array_equal(test, [3, 4])\n\n    train, test = next(splits)\n    assert_array_equal(train, [1, 2, 3, 4])\n    assert_array_equal(test, [5, 6])\n\n    train, test = next(splits)\n    assert_array_equal(train, [3, 4, 5, 6])\n    assert_array_equal(test, [7, 8])\n\n    # Check that get_n_splits returns the correct number of splits with gaps\n    splits = tscv.split(X)\n    n_splits_actual = len(list(splits))\n    assert_equal(n_splits_actual, tscv.get_n_splits())\n    assert_equal(n_splits_actual, 3)\n", "def test_leave_one_out():\n    # Test LeaveOneOut split\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    loo = LeaveOneOut()\n\n    expected_splits = [\n        (np.array([1, 2, 3]), np.array([0])),\n        (np.array([0, 2, 3]), np.array([1])),\n        (np.array([0, 1, 3]), np.array([2])),\n        (np.array([0, 1, 2]), np.array([3])),\n    ]\n\n    splits = list(loo.split(X, y))\n    assert_equal(len(splits), len(expected_splits))\n\n    for i, (train_index, test_index) in enumerate(splits):\n        expected_train_index, expected_test_index = expected_splits[i]\n        assert_array_equal(train_index, expected_train_index)\n        assert_array_equal(test_index, expected_test_index)\n\n    # Check that get_n_splits returns the correct number of splits\n    assert_equal(loo.get_n_splits(X, y), 4)\n", "def test_leave_one_out():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n\n    # Check that the number of splits is equal to the number of samples\n    assert_equal(loo.get_n_splits(X, y), len(X))\n\n    # Check that each sample is used once as a test set\n    splits = list(loo.split(X, y))\n    assert_equal(len(splits), len(X))\n\n    for i, (train_index, test_index) in enumerate(splits):\n        assert_equal(len(test_index), 1)\n        assert_equal(test_index[0], i)\n        assert_equal(len(train_index), len(X) - 1)\n        assert not np.any(np.isin(train_index, test_index))\n\n    # Check that the split method works with 1-dimensional y\n    splits_1d = list(loo.split(X, y))\n    splits_2d = list(loo.split(X, y[:, np.newaxis]))\n    assert_equal(splits_1d, splits_2d)\n\n    # Check that the split method raises error when X is None\n    with pytest.raises(ValueError, match=\"The 'X' parameter should not be None.\"):\n        loo.get_n_splits(None, y)\n\n    # Check the string representation of the LeaveOneOut object\n    assert_equal(repr(loo), \"LeaveOneOut()\")\n", "def test_leave_p_out():\n    # Test LeavePOut cross-validator\n    X = np.ones(5)\n    y = np.array([0, 0, 1, 1, 2])\n    lpo = LeavePOut(p=2)\n    \n    # Expected splits\n    expected_splits = [\n        (np.array([2, 3, 4]), np.array([0, 1])),\n        (np.array([1, 3, 4]), np.array([0, 2])),\n        (np.array([1, 2, 4]), np.array([0, 3])),\n        (np.array([1, 2, 3]), np.array([0, 4])),\n        (np.array([0, 3, 4]), np.array([1, 2])),\n        (np.array([0, 2, 4]), np.array([1, 3])),\n        (np.array([0, 2, 3]), np.array([1, 4])),\n        (np.array([0, 1, 4]), np.array([2, 3])),\n        (np.array([0, 1, 3]), np.array([2, 4])),\n        (np.array([0, 1, 2]), np.array([3, 4]))\n    ]\n\n    # Validate the splits\n    splits = list(lpo.split(X, y))\n    for i, (train_idx, test_idx) in enumerate(splits):\n        expected_train_idx, expected_test_idx = expected_splits[i]\n        assert_array_equal(train_idx, expected_train_idx)\n        assert_array_equal(test_idx, expected_test_idx)\n\n    # Check the number of splits\n    assert_equal(lpo.get_n_splits(X, y), 10)\n\n    # Check that ValueError is raised when X is None\n    with pytest.raises(ValueError):\n        lpo.get_n_splits(None, y)\n\n    # Check that __repr__ works without errors\n    assert_equal(repr(lpo), \"LeavePOut(p=2)\")\n", "def test_leaveoneout_split():\n    # Test if LeaveOneOut split correctly generates the expected train/test indices\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 2])\n    loo = LeaveOneOut()\n    splits = list(loo.split(X, y))\n    \n    expected_splits = [\n        (np.array([1, 2]), np.array([0])),\n        (np.array([0, 2]), np.array([1])),\n        (np.array([0, 1]), np.array([2]))\n    ]\n    \n    for (train, test), (expected_train, expected_test) in zip(splits, expected_splits):\n        assert_array_equal(train, expected_train)\n        assert_array_equal(test, expected_test)\n    \n    # Ensure that get_n_splits returns the correct number of splits\n    assert_equal(loo.get_n_splits(X), len(X))\n"], "sample_786": ["def test_invalid_transform_shape():\n    est = KBinsDiscretizer(n_bins=3).fit(X)\n    incorrect_shape_X = np.arange(10).reshape(2, 5)\n    assert_raise_message(ValueError,\n                         \"Incorrect number of features. Expecting 4, \"\n                         \"received 5\", est.transform, incorrect_shape_X)\n", "def test_constant_features():\n    # Test that constant features are replaced with 0 and proper warning is raised\n    X_constant = np.array([[5, 2, 2], [5, 2, 2], [5, 2, 2]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        est.fit(X_constant)\n        assert len(w) == 2  # Two warnings for constant features\n        assert \"Feature 0 is constant and will be replaced with 0.\" in str(w[0].message)\n        assert \"Feature 1 is constant and will be replaced with 0.\" in str(w[1].message)\n\n    Xt = est.transform(X_constant)\n    assert_array_equal(Xt[:, 0], np.zeros(X_constant.shape[0]))\n    assert_array_equal(Xt[:, 1], np.zeros(X_constant.shape[0]))\n    assert_array_equal(Xt[:, 2], np.zeros(X_constant.shape[0]))\n", "def test_transform_after_fit_with_one_feature():\n    X_single_feature = np.array([[0], [1], [2], [3], [4]]).reshape(-1, 1)\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    est.fit(X_single_feature)\n    Xt = est.transform(X_single_feature)\n    assert_array_equal(Xt, np.array([[0], [0], [1], [2], [2]]))\n\n    # Verify inverse transformation works properly\n    Xinv = est.inverse_transform(Xt)\n    bin_centers = (est.bin_edges_[0][1:] + est.bin_edges_[0][:-1]) * 0.5\n    expected_Xinv = bin_centers[np.int_(Xt).ravel()]\n    assert_array_equal(Xinv.ravel(), expected_Xinv)\n", "def test_fit_with_non_numeric_data():\n    X_non_numeric = [['a', 'b'], ['c', 'd']]\n    est = KBinsDiscretizer(n_bins=2)\n    assert_raise_message(ValueError, \"could not convert string to float: 'a'\", est.fit, X_non_numeric)\n", "def test_fit_transform_with_nan():\n    X_with_nan = np.array([[np.nan, 2.5, -3, -0.5],\n                           [0, 3.5, -2, 0.5],\n                           [1, 4.5, -1, 2]])\n\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        est.fit_transform(X_with_nan)\n"], "sample_787": ["def test_cohen_kappa_score_with_different_labels():\n    # Test the cohen_kappa_score with different combinations of labels and weights\n    y1 = [0, 0, 1, 1, 2, 2, 2]\n    y2 = [0, 1, 1, 1, 2, 2, 0]\n\n    # Test without labels and weights\n    kappa = cohen_kappa_score(y1, y2)\n    assert_almost_equal(kappa, 0.42857, decimal=5)\n\n    # Test with labels\n    kappa = cohen_kappa_score(y1, y2, labels=[0, 1, 2])\n    assert_almost_equal(kappa, 0.42857, decimal=5)\n\n    # Test with linear weights\n    kappa = cohen_kappa_score(y1, y2, weights=\"linear\")\n    assert_almost_equal(kappa, 0.42857, decimal=5)\n\n    # Test with quadratic weights\n    kappa = cohen_kappa_score(y1, y2, weights=\"quadratic\")\n    assert_almost_equal(kappa, 0.42857, decimal=5)\n", "def test_multilabel_confusion_matrix_with_single_label():\n    # Test multilabel confusion matrix when there is only one unique label in y_true\n    y_true = np.array([[1, 0, 0], [1, 0, 0], [1, 0, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    expected_cm = np.array([[[0, 0], [0, 3]],\n                            [[2, 1], [0, 0]],\n                            [[2, 1], [0, 0]]])\n    \n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, expected_cm)\n    \n    # Check with sparse matrix inputs\n    from scipy.sparse import csr_matrix\n    y_true_sparse = csr_matrix(y_true)\n    y_pred_sparse = csr_matrix(y_pred)\n    \n    cm_sparse = multilabel_confusion_matrix(y_true_sparse, y_pred_sparse)\n    assert_array_equal(cm_sparse, expected_cm)\n", "def test_confusion_matrix_multiclass_unordered_labels():\n    # Test confusion matrix - multi-class case with unordered labels\n    y_true, y_pred, _ = make_prediction(binary=False)\n    \n    # compute confusion matrix with unordered labels\n    labels = [2, 0, 1]\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    assert_array_equal(cm, [[18, 2, 0],\n                            [5, 19, 4],\n                            [28, 3, 0]])\n\n    # check for exception when none of the specified labels are in y_true\n    extra_label = np.max(y_true) + 1\n    assert_raises(ValueError, confusion_matrix, y_true, y_pred,\n                  labels=[extra_label, extra_label + 1])\n", "def test_multilabel_confusion_matrix_sample_weight():\n    # Test multilabel confusion matrix with sample weights\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]])\n    sample_weight = [2, 1, 3]\n    \n    cm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    expected_cm = np.array([[[2, 0], [1, 3]],\n                            [[2, 0], [1, 1]],\n                            [[0, 3], [1, 0]]])\n    \n    assert_array_equal(cm, expected_cm)\n    \n    # Test with different sample weights\n    sample_weight = [1, 1, 1]\n    cm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    expected_cm = np.array([[[2, 0], [1, 1]],\n                            [[1, 0], [1, 1]],\n                            [[0, 2], [1, 0]]])\n    \n    assert_array_equal(cm, expected_cm)\n", "def test_fbeta_score_for_different_beta_values():\n    # Test fbeta_score for different beta values\n    y_true = [0, 1, 1, 0, 1, 1]\n    y_pred = [0, 1, 0, 1, 1, 1]\n\n    # Beta < 1: more weight on precision\n    f2 = fbeta_score(y_true, y_pred, beta=0.5, average='binary')\n    assert_almost_equal(f2, (1 + 0.5**2) * 0.8 * 0.75 / (0.5**2 * 0.8 + 0.75), 2)\n\n    # Beta == 1: equal weight on precision and recall (F1 score)\n    f1 = fbeta_score(y_true, y_pred, beta=1, average='binary')\n    assert_almost_equal(f1, (1 + 1**2) * 0.8 * 0.75 / (1**2 * 0.8 + 0.75), 2)\n\n    # Beta > 1: more weight on recall\n    f3 = fbeta_score(y_true, y_pred, beta=2, average='binary')\n    assert_almost_equal(f3, (1 + 2**2) * 0.8 * 0.75 / (2**2 * 0.8 + 0.75), 2)\n\n    # Check for multiclass with 'macro' average\n    y_true_mc = [0, 1, 2, 2, 1, 0]\n    y_pred_mc = [0, 2, 1, 2, 0, 1]\n\n    f2_mc = fbeta_score(y_true_mc, y_pred_mc, beta=0.5, average='macro')\n    assert_almost_equal(f2_mc, 0.22222222, 2)\n\n    f1_mc = fbeta_score(y_true_mc, y_pred_mc, beta=1, average='macro')\n    assert_almost_equal(f1_mc, 0.26666667, 2)\n\n    f3_mc = fbeta_score(y_true_mc, y_pred_mc, beta=2, average='macro')\n    assert_almost_equal(f3_mc, 0.21739130, 2)\n"], "sample_788": ["def test_transform_with_different_dtypes():\n    X = np.array([[0, 1], [2, 3], [4, 5]])\n    for dtype in [np.float32, np.float64, np.int32, np.int64]:\n        X_typed = X.astype(dtype)\n        est = KBinsDiscretizer(n_bins=2, encode='ordinal', strategy='uniform')\n        Xt = est.fit_transform(X_typed)\n        assert Xt.dtype == np.float64\n        assert_array_equal(Xt, est.transform(X_typed))\n", "def test_invalid_transform_input():\n    est = KBinsDiscretizer(n_bins=3).fit(X)\n    \n    # Test with non-numeric input\n    bad_X = [['a', 'b', 'c', 'd']]\n    assert_raises(ValueError, est.transform, bad_X)\n    \n    # Test with 3D array input\n    bad_X = np.arange(24).reshape(2, 3, 4)\n    assert_raises(ValueError, est.transform, bad_X)\n    \n    # Test with empty array input\n    bad_X = np.empty((0, 4))\n    assert_raises(ValueError, est.transform, bad_X)\n\n    # Test with None input\n    assert_raises(ValueError, est.transform, None)\n", "def test_all_bins_empty():\n    X = [[0, 0], [0, 0], [0, 0], [0, 0]]\n    est = KBinsDiscretizer(n_bins=3, strategy='uniform', encode='ordinal')\n    msg = (\"Bins whose width are too small (i.e., <= 1e-8) in feature 0 \"\n           \"are removed. Consider decreasing the number of bins.\")\n    assert_warns_message(UserWarning, msg, est.fit, X)\n    assert_array_equal(est.n_bins_, [1, 1])\n    assert_array_equal(est.transform(X), np.zeros_like(X))\n", "def test_transform_sparse():\n    X_sparse = sp.csr_matrix(X)\n    est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform')\n    est.fit(X_sparse)\n    Xt_sparse = est.transform(X_sparse)\n    assert sp.issparse(Xt_sparse)\n    assert_array_equal(Xt_sparse.toarray(), est.transform(X))\n", "def test_invalid_data_types():\n    # Test with string data\n    X_str = [['a', 'b'], ['c', 'd']]\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    assert_raises(ValueError, est.fit, X_str)\n\n    # Test with mixed data types\n    X_mixed = [[1, 'a'], [2, 'b']]\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    assert_raises(ValueError, est.fit, X_mixed)\n\n    # Test with boolean data\n    X_bool = [[True, False], [False, True]]\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    assert_raises(ValueError, est.fit, X_bool)\n\n    # Test with object data\n    X_object = np.array([['a', 1], ['b', 2]], dtype=object)\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    assert_raises(ValueError, est.fit, X_object)\n"], "sample_789": ["def test_adaboost_regressor_loss():\n    # Check that AdaBoostRegressor respects the 'loss' parameter\n    X, y = datasets.make_regression(n_samples=100, n_features=4, random_state=42)\n\n    for loss in ['linear', 'square', 'exponential']:\n        reg = AdaBoostRegressor(loss=loss, random_state=0)\n        reg.fit(X, y)\n        predictions = reg.predict(X)\n        assert_equal(len(predictions), len(y))\n\n    # Check that an invalid loss parameter raises a ValueError\n    with pytest.raises(ValueError, match=\"loss must be 'linear', 'square', or 'exponential'\"):\n        reg = AdaBoostRegressor(loss='invalid', random_state=0)\n        reg.fit(X, y)\n", "def test_feature_importances():\n    # Check feature importances for AdaBoostClassifier and AdaBoostRegressor\n    X, y = datasets.make_classification(n_samples=100, n_features=20,\n                                        n_informative=5, n_redundant=5,\n                                        random_state=42)\n\n    clf = AdaBoostClassifier(n_estimators=30, random_state=0)\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n    assert_equal(importances.shape[0], 20)\n    assert_greater(importances.sum(), 0)\n    \n    X_reg, y_reg = datasets.make_regression(n_samples=100, n_features=20,\n                                            n_informative=5, random_state=42)\n    \n    reg = AdaBoostRegressor(n_estimators=30, random_state=0)\n    reg.fit(X_reg, y_reg)\n    importances_reg = reg.feature_importances_\n    assert_equal(importances_reg.shape[0], 20)\n    assert_greater(importances_reg.sum(), 0)\n", "def test_adaboost_classifier_with_sample_weight():\n    # Check AdaBoostClassifier with custom sample weights\n    sample_weight = np.array([1, 1, 1, 0.5, 0.5, 0.5])\n    clf = AdaBoostClassifier(algorithm='SAMME', random_state=0)\n    clf.fit(X, y_class, sample_weight=sample_weight)\n    \n    # Ensure predictions are as expected\n    predictions = clf.predict(T)\n    assert_array_equal(predictions, y_t_class)\n\n    # Ensure predicted probabilities sum to 1\n    proba = clf.predict_proba(T)\n    assert_array_almost_equal(proba.sum(axis=1), np.ones(proba.shape[0]))\n\n    # Ensure decision function shape is correct\n    decision = clf.decision_function(T)\n    assert_equal(decision.shape, (len(T),))\n\n    # Check the importance of each feature\n    importances = clf.feature_importances_\n    assert_equal(importances.shape[0], 2)\n    assert_greater(importances.sum(), 0)\n", "def test_staged_predict_log_proba():\n    # Check staged predictions for predict_log_proba.\n    rng = np.random.RandomState(0)\n    iris_weights = rng.randint(10, size=iris.target.shape)\n\n    # AdaBoost classification\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg, n_estimators=10)\n        clf.fit(iris.data, iris.target, sample_weight=iris_weights)\n\n        log_proba = clf.predict_log_proba(iris.data)\n        staged_log_probas = [p for p in clf.staged_predict_log_proba(iris.data)]\n\n        assert_equal(len(staged_log_probas), 10)\n        assert_array_almost_equal(log_proba, staged_log_probas[-1])\n", "def test_adaboost_regressor_loss_functions():\n    \"\"\"\n    Test the different loss functions for AdaBoostRegressor.\n    \"\"\"\n    X, y = datasets.make_regression(n_samples=100, n_features=20, noise=0.1, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    \n    for loss in ['linear', 'square', 'exponential']:\n        reg = AdaBoostRegressor(loss=loss, random_state=0)\n        reg.fit(X_train, y_train)\n        y_pred = reg.predict(X_test)\n        assert_greater(reg.score(X_test, y_test), 0.5, f\"Failed with loss={loss}\")\n        assert len(reg.estimators_) > 1, f\"Failed to use multiple estimators with loss={loss}\"\n        assert_equal(len(reg.estimator_weights_), len(reg.estimator_errors_), f\"Mismatch in weights and errors with loss={loss}\")\n"], "sample_790": ["def test_kernel_pca_inverse_transform_error():\n    # Test that inverse_transform raises an error if fit_inverse_transform is False\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    kpca = KernelPCA(n_components=4, fit_inverse_transform=False)\n    kpca.fit(X_fit)\n    X_transformed = kpca.transform(X_fit)\n    with pytest.raises(NotFittedError):\n        kpca.inverse_transform(X_transformed)\n", "def test_kernel_pca_inverse_transform_not_fitted():\n    # Test inverse_transform raises error if fit_inverse_transform is False\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    kpca = KernelPCA(n_components=2, fit_inverse_transform=False)\n    kpca.fit(X)\n\n    with pytest.raises(NotFittedError):\n        kpca.inverse_transform(X)\n", "def test_kernel_pca_inverse_transform_error():\n    # Test if NotFittedError is raised when trying to inverse transform without fit_inverse_transform\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n\n    kpca = KernelPCA(fit_inverse_transform=False)\n    kpca.fit(X_fit)\n\n    with pytest.raises(NotFittedError):\n        kpca.inverse_transform(X_fit)\n", "def test_kernel_pca_invalid_eigen_solver():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    invalid_eigen_solver = \"invalid_solver\"\n    kpca = KernelPCA(n_components=2, eigen_solver=invalid_eigen_solver)\n    with pytest.raises(ValueError, match=\"Unrecognized eigen_solver\"):\n        kpca.fit(X_fit)\n", "def test_kernel_pca_inverse_transform_not_fitted_error():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((2, 4))\n    \n    kpca = KernelPCA(n_components=2, fit_inverse_transform=False)\n    kpca.fit(X)\n\n    with assert_raises(NotFittedError):\n        kpca.inverse_transform(X)\n"], "sample_791": ["def test_ordinal_encoder_specified_categories_mixed_columns():\n    # multiple columns with mixed types\n    X = np.array([['a', 0], ['b', 2]], dtype=object).T\n    enc = OrdinalEncoder(categories=[['a', 'b', 'c'], [0, 1, 2]])\n    exp = np.array([[0., 0.],\n                    [1., 2.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['a', 'b', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n    assert enc.categories_[1].tolist() == [0, 1, 2]\n    assert np.issubdtype(enc.categories_[1].dtype, np.integer)\n\n    # when specifying categories manually, unknown categories should raise\n    X2 = np.array([['a', 3]], dtype=object).T\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit(X2)\n", "def test_ordinal_encoder_handle_unknown_ignore():\n    # Test OrdinalEncoder with handle_unknown set to 'ignore'\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    \n    # Transform new data with unknown categories\n    X_new = [['Female', 3], ['Male', 4]]\n    X_trans = enc.transform(X_new)\n    \n    # Expect the unknown category '4' to be handled and the rest correctly transformed\n    expected_output = np.array([[0., 2.], [1., -1.]])\n    assert_array_equal(X_trans, expected_output)\n    \n    # Test inverse transform to ensure unknown category handling\n    X_inv = enc.inverse_transform(X_trans)\n    expected_inv_output = np.array([['Female', 3], ['Male', None]], dtype=object)\n    assert_array_equal(X_inv, expected_inv_output)\n", "def test_ordinal_encoder_mixed_string_and_numeric():\n    # Test OrdinalEncoder with mixed string and numeric data\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2]])\n    enc = OrdinalEncoder()\n\n    # Fit and transform the data\n    X_trans = enc.fit_transform(X)\n    assert X_trans.shape == (3, 2)\n    assert_array_equal(X_trans, np.array([[1, 0], [0, 2], [0, 1]], dtype=float))\n\n    # Test inverse transform\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X_inv, X)\n\n    # Check the categories\n    assert enc.categories_ == [np.array(['Female', 'Male'], dtype=object), np.array([1, 2, 3])]\n\n    # Test with handle_unknown='ignore'\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X2 = np.array([['Female', 3], ['Male', 4]])\n    X2_trans = enc.transform(X2)\n    assert_array_equal(X2_trans, np.array([[0, 2], [1, -1]], dtype=float))\n", "def test_ordinal_encoder_inverse_with_unknowns():\n    # Ensure inverse_transform correctly handles unknown categories as None\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OrdinalEncoder()\n    X_tr = enc.fit_transform(X)\n\n    # introduce unknown category\n    X_tr[1, 0] = 999\n    expected_output = np.array([['abc', 2, 55], [None, 1, 55]], dtype=object)\n\n    assert_array_equal(enc.inverse_transform(X_tr), expected_output)\n", "def test_ordinal_encoder_non_string_labels():\n    # Test OrdinalEncoder with numerical labels\n    X = [[1, 2, 3], [2, 3, 1], [3, 1, 2]]\n    enc = OrdinalEncoder()\n    X_trans = enc.fit_transform(X)\n    assert_array_equal(X_trans, np.array([[0., 1., 2.],\n                                          [1., 2., 0.],\n                                          [2., 0., 1.]]))\n\n    # Test inverse_transform\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X_inv, np.array([[1, 2, 3],\n                                        [2, 3, 1],\n                                        [3, 1, 2]]))\n\n    # Test with mixed types\n    X = [[1, 'a', 3], [2, 'b', 1], [3, 'c', 2]]\n    enc = OrdinalEncoder()\n    X_trans = enc.fit_transform(X)\n    assert_array_equal(X_trans, np.array([[0., 0., 2.],\n                                          [1., 1., 0.],\n                                          [2., 2., 1.]]))\n\n    # Test inverse_transform with mixed types\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X_inv, np.array([[1, 'a', 3],\n                                        [2, 'b', 1],\n                                        [3, 'c', 2]], dtype=object))\n\n    # Test with missing values\n    X = [[1, 'a', 3], [2, 'b', None], [None, 'c', 2]]\n    enc = OrdinalEncoder()\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        enc.fit_transform(X)\n"], "sample_792": ["def test_gnb_var_smoothing():\n    # Test that var_smoothing parameter is properly applied.\n    X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])\n    y = np.array([1, 1, 1, 2, 2, 2])\n    \n    var_smoothing_values = [1e-2, 1e-5, 1e-9]\n    for var_smoothing in var_smoothing_values:\n        clf = GaussianNB(var_smoothing=var_smoothing)\n        clf.fit(X, y)\n        assert clf.var_smoothing == var_smoothing\n        \n        # Check that the added variance is correct\n        max_variance = np.var(X, axis=0).max()\n        epsilon_expected = var_smoothing * max_variance\n        assert_almost_equal(clf.epsilon_, epsilon_expected)\n        \n        # Check predictions remain consistent\n        y_pred = clf.predict(X)\n        assert_array_equal(y_pred, y)\n", "def test_complementnb():\n    # Test Complement Naive Bayes classification.\n    # This checks that ComplementNB implements fit and predict and returns\n    # correct values for a simple toy dataset.\n\n    X_comp = np.array([[1, 0], [0, 1], [1, 1], [1, 0], [0, 1], [1, 1]])\n    y_comp = np.array([0, 1, 2, 0, 1, 2])\n    \n    # Check the ability to predict the learning set.\n    clf = ComplementNB()\n    y_pred = clf.fit(X_comp, y_comp).predict(X_comp)\n    assert_array_equal(y_pred, y_comp)\n\n    # Verify that np.log(clf.predict_proba(X)) gives the same results as\n    # clf.predict_log_proba(X)\n    y_pred_proba = clf.predict_proba(X_comp)\n    y_pred_log_proba = clf.predict_log_proba(X_comp)\n    assert_array_almost_equal(np.log(y_pred_proba), y_pred_log_proba, 8)\n\n    # Check that incremental fitting yields the same results\n    clf2 = ComplementNB()\n    clf2.partial_fit(X_comp[:2], y_comp[:2], classes=np.unique(y_comp))\n    clf2.partial_fit(X_comp[2:4], y_comp[2:4])\n    clf2.partial_fit(X_comp[4:], y_comp[4:])\n\n    y_pred2 = clf2.predict(X_comp)\n    assert_array_equal(y_pred2, y_comp)\n\n    y_pred_proba2 = clf2.predict_proba(X_comp)\n    y_pred_log_proba2 = clf2.predict_log_proba(X_comp)\n    assert_array_almost_equal(np.log(y_pred_proba2), y_pred_log_proba2, 8)\n    assert_array_almost_equal(y_pred_proba2, y_pred_proba)\n    assert_array_almost_equal(y_pred_log_proba2, y_pred_log_proba)\n\n    # Partial fit on the whole data at once should be the same as fit too\n    clf3 = ComplementNB()\n    clf3.partial_fit(X_comp, y_comp, classes=np.unique(y_comp))\n\n    y_pred3 = clf3.predict(X_comp)\n    assert_array_equal(y_pred3, y_comp)\n    y_pred_proba3 = clf3.predict_proba(X_comp)\n    y_pred_log_proba3 = clf3.predict_log_proba", "def test_gnb_multiclass_partial_fit():\n    # Test GaussianNB with multiple partial_fit calls on multiclass data\n\n    # Create a dataset with three classes\n    X_multiclass = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [6, 7], [7, 8]])\n    y_multiclass = np.array([0, 1, 2, 0, 1, 2])\n\n    clf = GaussianNB()\n\n    # First partial fit with a subset of the data\n    clf.partial_fit(X_multiclass[:3], y_multiclass[:3], classes=np.unique(y_multiclass))\n    assert_array_almost_equal(clf.class_prior_, np.array([1/3, 1/3, 1/3]))\n\n    # Second partial fit with the remaining data\n    clf.partial_fit(X_multiclass[3:], y_multiclass[3:])\n    assert_array_almost_equal(clf.class_prior_, np.array([1/3, 1/3, 1/3]))\n\n    # Ensure predictions are consistent with the training data\n    y_pred = clf.predict(X_multiclass)\n    assert_array_equal(y_pred, y_multiclass)\n\n    # Check that the learned parameters (mean and variance) are consistent\n    expected_theta = np.array([[3, 4], [4, 5], [5, 6]])\n    expected_sigma = np.array([[4, 4], [4, 4], [4, 4]])\n    assert_array_almost_equal(clf.theta_, expected_theta)\n    assert_array_almost_equal(clf.sigma_, expected_sigma)\n", "def test_gaussiannb_refit():\n    \"\"\"Test if refitting GaussianNB works as expected.\"\"\"\n    clf = GaussianNB()\n    # Fit the classifier\n    clf.fit(X, y)\n    initial_theta = clf.theta_.copy()\n    initial_sigma = clf.sigma_.copy()\n    initial_class_prior = clf.class_prior_.copy()\n\n    # Refit the classifier on new data\n    X_new = np.array([[4, 5], [5, 4], [6, 5]])\n    y_new = np.array([1, 1, 1])\n    clf.fit(X_new, y_new)\n\n    # Check if the parameters are updated correctly\n    assert not np.array_equal(clf.theta_, initial_theta)\n    assert not np.array_equal(clf.sigma_, initial_sigma)\n    assert not np.array_equal(clf.class_prior_, initial_class_prior)\n\n    # Refit the classifier back on the original data\n    clf.fit(X, y)\n\n    # Check if the parameters are reset correctly\n    assert_array_almost_equal(clf.theta_, initial_theta)\n    assert_array_almost_equal(clf.sigma_, initial_sigma)\n    assert_array_almost_equal(clf.class_prior_, initial_class_prior)\n", "def test_multinomial_nb_partial_fit_incremental():\n    # Test incremental updates with MultinomialNB\n\n    # Create toy training data\n    X_train = np.array([[2, 1], [3, 4], [1, 6], [4, 5]])\n    y_train = np.array([0, 1, 1, 0])\n\n    # Create chunks for incremental learning\n    X_chunk1 = X_train[:2]\n    y_chunk1 = y_train[:2]\n    X_chunk2 = X_train[2:]\n    y_chunk2 = y_train[2:]\n\n    # Incremental fit with partial_fit\n    clf = MultinomialNB()\n    clf.partial_fit(X_chunk1, y_chunk1, classes=np.unique(y_train))\n    clf.partial_fit(X_chunk2, y_chunk2)\n\n    # Fit with all data at once\n    clf_full = MultinomialNB()\n    clf_full.fit(X_train, y_train)\n\n    # Check if incremental fit matches fit with all data\n    assert_array_almost_equal(clf.feature_log_prob_, clf_full.feature_log_prob_)\n    assert_array_almost_equal(clf.class_log_prior_, clf_full.class_log_prior_)\n    assert_array_almost_equal(clf.predict_proba(X_train), clf_full.predict_proba(X_train))\n    assert_array_almost_equal(clf.predict_log_proba(X_train), clf_full.predict_log_proba(X_train))\n    assert_array_equal(clf.predict(X_train), clf_full.predict(X_train))\n"], "sample_793": ["def test_iforest_predict():\n    \"\"\"Test the predict method of Isolation Forest.\"\"\"\n    rng = check_random_state(42)\n    X_train = rng.randn(100, 2)\n    X_test = rng.randn(20, 2)\n\n    clf = IsolationForest(n_estimators=10, random_state=rng).fit(X_train)\n    predictions = clf.predict(X_test)\n\n    # Check the output is -1 or 1\n    assert np.all(np.isin(predictions, [-1, 1]))\n\n    # Check the proportion of outliers is reasonable\n    outlier_proportion = np.mean(predictions == -1)\n    assert 0 <= outlier_proportion <= 0.5\n", "def test_iforest_with_sample_weights():\n    \"\"\"Check IsolationForest with sample weights.\"\"\"\n    rng = check_random_state(42)\n    X = rng.normal(size=(100, 2))\n    sample_weight = rng.rand(100)\n    \n    clf = IsolationForest(random_state=rng)\n    clf.fit(X, sample_weight=sample_weight)\n    \n    # Ensure predictions do not raise any error\n    predictions = clf.predict(X)\n    assert predictions.shape[0] == X.shape[0]\n    \n    # Ensure that score_samples do not raise any error\n    scores = clf.score_samples(X)\n    assert scores.shape[0] == X.shape[0]\n", "def test_iforest_fit_with_sample_weight():\n    \"\"\"Check Isolation Forest fit method with sample weights.\"\"\"\n    X = iris.data\n    sample_weight = np.ones(X.shape[0])\n    sample_weight[:10] = 10  # Increase weight for the first 10 samples\n\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X, sample_weight=sample_weight)\n\n    # Check that estimators are fitted and sample weights are used\n    assert hasattr(clf, \"estimators_\")\n    assert hasattr(clf, \"estimators_samples_\")\n    assert_greater(len(clf.estimators_), 0)\n    assert_greater(len(clf.estimators_samples_), 0)\n\n    # Check if sample weights are used by comparing predictions with and without weights\n    clf_no_weights = IsolationForest(random_state=rng)\n    clf_no_weights.fit(X)\n    pred_with_weights = clf.predict(X)\n    pred_no_weights = clf_no_weights.predict(X)\n    assert np.any(pred_with_weights != pred_no_weights), \"Sample weights did not affect the fit\"\n", "def test_iforest_with_sample_weight():\n    \"\"\"Test Isolation Forest with sample weights.\"\"\"\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]\n    sample_weight = [1, 2, 1, 1, 1, 1, 1, 1]\n\n    # Train Isolation Forest with sample weights\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X, sample_weight=sample_weight)\n    \n    # Predict\n    pred = clf.predict(X)\n    \n    # Check that outliers are correctly predicted\n    decision_func = -clf.decision_function(X)\n    assert_greater(np.min(decision_func[-2:]), np.max(decision_func[:-2]))\n    assert_array_equal(pred, 6 * [1] + 2 * [-1])\n", "def test_fit_predict():\n    \"\"\"Test fit and predict methods together for isolation forest.\"\"\"\n    X_train = [[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]]\n    X_test = [[5, 5], [6, 6], [7, 7]]\n\n    clf = IsolationForest(contamination=0.2, random_state=42)\n    clf.fit(X_train)\n    predictions = clf.predict(X_test)\n\n    # Since contamination is set to 0.2, we expect 1 outlier in the training data\n    # Therefore, all test data points should be considered outliers (-1) because they\n    # are far from the training data points.\n    expected_predictions = [-1, -1, -1]\n    assert_array_equal(predictions, expected_predictions)\n"], "sample_794": ["def test_ridge_regression_intercept():\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 10, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    # Test ridge regression with intercept\n    alpha = 1.0\n    ridge = Ridge(alpha=alpha, fit_intercept=True)\n    ridge.fit(X, y)\n    coef_with_intercept = ridge.coef_\n    intercept = ridge.intercept_\n\n    # Test ridge regression without intercept\n    ridge_no_intercept = Ridge(alpha=alpha, fit_intercept=False)\n    ridge_no_intercept.fit(X, y)\n    coef_no_intercept = ridge_no_intercept.coef_\n\n    # Ensure intercept is not zero when fit_intercept=True\n    assert intercept != 0.0\n\n    # Ensure coefficients differ when fit_intercept is used\n    assert not np.allclose(coef_with_intercept, coef_no_intercept)\n", "def test_ridge_regression_invalid_alpha():\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 5)\n    y = rng.randn(10)\n\n    invalid_alphas = [-1.0, -0.1, 0]\n\n    for solver in [\"svd\", \"sparse_cg\", \"cholesky\", \"lsqr\", \"sag\", \"saga\"]:\n        for alpha in invalid_alphas:\n            assert_raises(ValueError, ridge_regression, X, y, alpha=alpha, solver=solver)\n", "def test_ridge_regression_check_input_argument():\n    # Test if ridge_regression handles check_input argument correctly\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 10, 5\n    X = rng.rand(n_samples, n_features)\n    y = np.dot(X, rng.rand(n_features)) + rng.rand(n_samples)\n    alpha = 1.0\n\n    # Check input is True by default\n    coef = ridge_regression(X, y, alpha=alpha, solver=\"cholesky\")\n    assert coef is not None\n\n    # Invalid data type should raise an error when check_input is True\n    X_invalid = X.astype('int')\n    with pytest.raises(ValueError):\n        ridge_regression(X_invalid, y, alpha=alpha, solver=\"cholesky\")\n\n    # Invalid data type should not raise an error when check_input is False\n    coef = ridge_regression(X_invalid, y, alpha=alpha, solver=\"cholesky\", check_input=False)\n    assert coef is not None\n", "def test_ridge_regression_with_different_tol_values():\n    # Test the effect of varying tolerance values on the Ridge regression coefficients\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n\n    tol_values = [1e-1, 1e-3, 1e-5, 1e-7, 1e-10]\n    coefs_list = []\n\n    for tol in tol_values:\n        ridge = Ridge(alpha=1.0, solver='auto', tol=tol)\n        ridge.fit(X, y)\n        coefs_list.append(ridge.coef_)\n\n    for i in range(len(coefs_list) - 1):\n        assert not np.allclose(coefs_list[i], coefs_list[i + 1]), \\\n            f\"Coefficients for tol={tol_values[i]} and tol={tol_values[i+1]} should not be the same\"\n", "def test_ridge_regression_multitarget():\n    # Test Ridge regression with multiple targets\n\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_targets = 6, 5, 3\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n\n    ridge = Ridge(alpha=1.0)\n    ridge.fit(X, y)\n    coefs = ridge.coef_\n    assert_equal(coefs.shape, (n_targets, n_features))\n\n    # Test for different solvers\n    for solver in ['svd', 'sparse_cg', 'cholesky', 'lsqr', 'sag', 'saga']:\n        ridge = Ridge(alpha=1.0, solver=solver)\n        ridge.fit(X, y)\n        coefs = ridge.coef_\n        assert_equal(coefs.shape, (n_targets, n_features))\n\n    # Test that coefficients are the same for different solvers\n    coefs_reference = Ridge(alpha=1.0, solver='cholesky').fit(X, y).coef_\n    for solver in ['svd', 'sparse_cg', 'lsqr', 'sag', 'saga']:\n        ridge = Ridge(alpha=1.0, solver=solver)\n        ridge.fit(X, y)\n        assert_array_almost_equal(ridge.coef_, coefs_reference, decimal=5)\n"], "sample_795": ["def test_check_estimators_dtypes():\n    # Test that check_estimators_dtypes works as expected\n    from sklearn.linear_model import LogisticRegression\n\n    # Create a simple classifier\n    clf = LogisticRegression()\n\n    # The function should run without raising an error\n    check_estimators_dtypes(clf.__class__.__name__, clf)\n\n    # Create a new instance of the classifier with some invalid dtype handling\n    class BadDtypeClassifier(LogisticRegression):\n            if X.dtype not in [np.float32, np.float64]:\n                raise ValueError(\"Invalid dtype for input data\")\n            return super().fit(X, y)\n\n    # This should raise an error since the dtype check should fail\n    msg = \"Estimator BadDtypeClassifier should not raise error on float32, float64, int32, or int64 data.\"\n    assert_raises_regex(ValueError, \"Invalid dtype for input data\",\n                        check_estimator, BadDtypeClassifier())\n", "def test_check_estimators_overwrite_params():\n    # Check that check_estimators_overwrite_params detects changes correctly\n\n    class EstimatorWithParams(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n            self.param1 = 3  # This should trigger an error in the test\n            return self\n\n    class EstimatorWithoutParamsChange(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n            return self\n\n    msg = (\"Estimator EstimatorWithParams should not change or mutate \"\n           \" the parameter param1 from 1 to 3 during fit.\")\n    assert_raises_regex(AssertionError, msg, check_estimator,\n                        EstimatorWithParams())\n\n    # This should pass without issues\n    check_estimator(EstimatorWithoutParamsChange)\n", "def test_check_sample_weights_invariance():\n    # check that the estimators yield same results for unit weights and no weights\n    class SampleWeightInvariantEstimator(BaseEstimator):\n            self.coef_ = np.mean(X * y[:, np.newaxis], axis=0)\n            return self\n\n            return np.dot(X, self.coef_)\n\n    check_sample_weights_invariance(\"SampleWeightInvariantEstimator\", SampleWeightInvariantEstimator())\n", "def test_check_supervised_y_no_nan():\n    # check that estimators raise error when targets contain NaN or infinity\n    class EstimatorWithNoValidation(BaseBadClassifier):\n            if not np.isfinite(y).all():\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\")\n            return self\n\n    msg = \"Estimator EstimatorWithNoValidation should have raised an error on fitting array y with NaN value.\"\n    assert_raises_regex(ValueError, msg, check_supervised_y_no_nan, \"EstimatorWithNoValidation\", EstimatorWithNoValidation())\n", "def test_check_dont_overwrite_parameters():\n    # checks that estimator's fit method only changes or sets private attributes\n    \n    class EstimatorWithPublicAttr(BaseEstimator):\n            self.param = param\n            \n            self.public_attr = \"I should not be here\"\n            return self\n    \n    class EstimatorWithPrivateAttr(BaseEstimator):\n            self.param = param\n            \n            self._private_attr = \"I am allowed\"\n            return self\n    \n    # Check that EstimatorWithPublicAttr raises an error\n    msg = (\"Estimator EstimatorWithPublicAttr should not change or mutate \"\n           \"the parameter public_attr during fit.\")\n    assert_raises_regex(AssertionError, msg,\n                        check_dont_overwrite_parameters,\n                        \"EstimatorWithPublicAttr\", EstimatorWithPublicAttr())\n    \n    # Check that EstimatorWithPrivateAttr does not raise an error\n    check_dont_overwrite_parameters(\"EstimatorWithPrivateAttr\", EstimatorWithPrivateAttr())\n"], "sample_796": ["def test_huber_invalid_epsilon():\n    # Test that HuberRegressor raises a ValueError for invalid epsilon values\n    X, y = make_regression_with_outliers()\n    with pytest.raises(ValueError, match=\"epsilon should be greater than or equal to 1.0\"):\n        HuberRegressor(epsilon=0.5).fit(X, y)\n", "def test_huber_negative_alpha():\n    # Test that HuberRegressor raises ValueError for negative alpha\n    X, y = make_regression_with_outliers()\n    with pytest.raises(ValueError, match=\"alpha must be non-negative\"):\n        HuberRegressor(alpha=-0.1).fit(X, y)\n", "def test_huber_epsilon_validation():\n    # Test that HuberRegressor raises ValueError when epsilon < 1.0\n    X, y = make_regression_with_outliers()\n    with pytest.raises(ValueError, match=\"epsilon should be greater than or equal to 1.0\"):\n        huber = HuberRegressor(fit_intercept=True, epsilon=0.5)\n        huber.fit(X, y)\n", "def test_huber_invalid_epsilon():\n    # Test that HuberRegressor raises ValueError for invalid epsilon values\n    X, y = make_regression_with_outliers()\n    invalid_epsilons = [0.5, -1.0, 0.99]\n    \n    for epsilon in invalid_epsilons:\n        with pytest.raises(ValueError, match=\"epsilon should be greater than or equal to 1.0\"):\n            HuberRegressor(epsilon=epsilon).fit(X, y)\n", "def test_huber_intercept():\n    # Test that the intercept is correctly set when fit_intercept is True/False\n    X, y = make_regression_with_outliers()\n\n    # Case when fit_intercept is True\n    huber_with_intercept = HuberRegressor(fit_intercept=True)\n    huber_with_intercept.fit(X, y)\n    assert huber_with_intercept.intercept_ != 0, \"Intercept should be non-zero when fit_intercept=True\"\n\n    # Case when fit_intercept is False\n    huber_without_intercept = HuberRegressor(fit_intercept=False)\n    huber_without_intercept.fit(X, y)\n    assert huber_without_intercept.intercept_ == 0, \"Intercept should be zero when fit_intercept=False\"\n"], "sample_797": ["def test_minmax_scaler_with_nan_values():\n    # Test MinMaxScaler with NaN values in the dataset\n    X = np.array([[np.nan, 1, 2],\n                  [3, np.nan, 5],\n                  [6, 7, np.nan]])\n\n    scaler = MinMaxScaler()\n    X_trans = scaler.fit_transform(X)\n    X_expected = np.array([[np.nan, 0, 0],\n                           [0, np.nan, 1],\n                           [1, 1, np.nan]])\n\n    assert_array_almost_equal(X_trans, X_expected, decimal=2)\n    \n    # Ensure inverse_transform maintains NaN values and correctly restores original values\n    X_trans_inv = scaler.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_trans_inv, decimal=2)\n", "def test_min_max_scaler_with_nan():\n    # Test MinMaxScaler with NaN values\n    X = np.array([[np.nan, 1, 2], [3, 4, np.nan], [1, 3, 2], [3, 2, 1]])\n    scaler = MinMaxScaler()\n    X_trans = scaler.fit_transform(X)\n    X_expected = np.array([[np.nan, 0, 1], [1, 1, np.nan], [0, 0.5, 1], [1, 0, 0]])\n    assert_array_almost_equal(X_trans, X_expected, decimal=1)\n\n    # Check inverse_transform\n    X_trans_inv = scaler.inverse_transform(X_trans)\n    X_expected_inv = np.array([[np.nan, 1, 2], [3, 4, np.nan], [1, 3, 2], [3, 2, 1]])\n    assert_array_almost_equal(X_trans_inv, X_expected_inv, decimal=1)\n\n    # Check with different feature_range\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    X_trans = scaler.fit_transform(X)\n    X_expected = np.array([[np.nan, -1, 1], [1, 1, np.nan], [-1, 0, 1], [1, -1, -1]])\n    assert_array_almost_equal(X_trans, X_expected, decimal=1)\n\n    X_trans_inv = scaler.inverse_transform(X_trans)\n    X_expected_inv = np.array([[np.nan, 1, 2], [3, 4, np.nan], [1, 3, 2], [3, 2, 1]])\n    assert_array_almost_equal(X_trans_inv, X_expected_inv, decimal=1)\n", "def test_handle_zeros_in_scale_with_scalars():\n    # Test _handle_zeros_in_scale with scalar inputs\n    scale = 0.0\n    handled_scale = _handle_zeros_in_scale(scale)\n    assert_equal(handled_scale, 1.0)\n\n    scale = 5.0\n    handled_scale = _handle_zeros_in_scale(scale)\n    assert_equal(handled_scale, 5.0)\n", "def test_handle_zeros_in_scale_scalar():\n    # Test _handle_zeros_in_scale with scalar input\n    scale = 0.0\n    handled_scale = _handle_zeros_in_scale(scale)\n    assert handled_scale == 1.0\n\n    scale = 5.0\n    handled_scale = _handle_zeros_in_scale(scale)\n    assert handled_scale == 5.0\n\n    scale = -2.0\n    handled_scale = _handle_zeros_in_scale(scale)\n    assert handled_scale == -2.0\n", "def test_handle_zeros_in_scale_scalar():\n    # Test _handle_zeros_in_scale when input is a scalar\n    scale = 0.0\n    updated_scale = _handle_zeros_in_scale(scale)\n    assert_equal(updated_scale, 1.0)\n\n    scale = 2.0\n    updated_scale = _handle_zeros_in_scale(scale)\n    assert_equal(updated_scale, 2.0)\n"], "sample_798": ["def test_ridge_regression_solver_auto():\n    # Test that 'auto' solver correctly selects different solvers\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features = 6, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    # Case where n_samples > n_features, should select 'cholesky'\n    ridge = Ridge(alpha=alpha, solver='auto')\n    ridge.fit(X, y)\n    assert_almost_equal(ridge.predict(X), Ridge(alpha=alpha, solver='cholesky').fit(X, y).predict(X))\n\n    # Case where n_features > n_samples, should select 'cholesky'\n    n_samples, n_features = 5, 10\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    ridge = Ridge(alpha=alpha, solver='auto')\n    ridge.fit(X, y)\n    assert_almost_equal(ridge.predict(X), Ridge(alpha=alpha, solver='cholesky').fit(X, y).predict(X))\n\n    # Case with sparse input, should select 'sparse_cg'\n    X_sparse = sp.csr_matrix(X)\n    ridge = Ridge(alpha=alpha, solver='auto')\n    ridge.fit(X_sparse, y)\n    assert_almost_equal(ridge.predict(X_sparse), Ridge(alpha=alpha, solver='sparse_cg').fit(X_sparse, y).predict(X_sparse))\n\n    # Case with intercept and sparse input, should select 'sag'\n    ridge = Ridge(alpha=alpha, solver='auto', fit_intercept=True)\n    ridge.fit(X_sparse, y)\n    assert_almost_equal(ridge.predict(X_sparse), Ridge(alpha=alpha, solver='sag', fit_intercept=True).fit(X_sparse, y).predict(X_sparse))\n", "def test_ridge_regression_different_solvers():\n    # Test ridge_regression with different solvers\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 10, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    alpha = 1.0\n\n    solvers = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n    for solver in solvers:\n        coef = ridge_regression(X, y, alpha=alpha, solver=solver)\n        assert coef.shape == (n_features,)\n        assert np.isfinite(coef).all()\n\n    # Test with sample weights\n    sample_weight = np.ones(n_samples)\n    for solver in solvers:\n        if solver in ['sag', 'saga', 'sparse_cg', 'auto']:\n            coef = ridge_regression(X, y, alpha=alpha, solver=solver, sample_weight=sample_weight)\n            assert coef.shape == (n_features,)\n            assert np.isfinite(coef).all()\n", "def test_ridge_regression_multiple_targets():\n    # Test ridge regression with multiple targets\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_targets = 10, 5, 3\n    X = rng.randn(n_samples, n_features)\n    Y = rng.randn(n_samples, n_targets)\n    alphas = [1.0, 0.1, 0.01]\n\n    for solver in ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']:\n        ridge = Ridge(alpha=alphas, solver=solver)\n        ridge.fit(X, Y)\n        assert_equal(ridge.coef_.shape, (n_targets, n_features))\n        y_pred = ridge.predict(X)\n        assert_equal(y_pred.shape, (n_samples, n_targets))\n\n        # Check the consistency of the predictions\n        for i, alpha in enumerate(alphas):\n            ridge_single = Ridge(alpha=alpha, solver=solver)\n            ridge_single.fit(X, Y[:, i])\n            assert_almost_equal(ridge.coef_[i], ridge_single.coef_)\n            assert_almost_equal(y_pred[:, i], ridge_single.predict(X))\n", "def test_ridge_solver_auto():\n    # Test the 'auto' solver option for Ridge regression\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    # With more samples than features\n    n_samples, n_features = 6, 5\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n\n    ridge = Ridge(alpha=alpha, solver='auto')\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (X.shape[1],))\n    assert_greater(ridge.score(X, y), 0.47)\n\n    # With more features than samples\n    n_samples, n_features = 5, 10\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n    ridge = Ridge(alpha=alpha, solver='auto')\n    ridge.fit(X, y)\n    assert_greater(ridge.score(X, y), .9)\n\n    # Test 'auto' solver with sparse data\n    X_sparse = sp.csr_matrix(X)\n    ridge.fit(X_sparse, y)\n    assert_greater(ridge.score(X_sparse, y), .9)\n", "def test_ridge_regression_return_n_iter():\n    # Test that ridge_regression returns n_iter when return_n_iter is True\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 10, 8\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    alpha = 1.0\n\n    coefs, n_iter = ridge_regression(X, y, alpha, solver='lsqr', return_n_iter=True)\n    assert isinstance(n_iter, int)\n    assert n_iter > 0\n\n    # Test with sparse input\n    X_sparse = sp.csr_matrix(X)\n    coefs_sparse, n_iter_sparse = ridge_regression(X_sparse, y, alpha, solver='lsqr', return_n_iter=True)\n    assert isinstance(n_iter_sparse, int)\n    assert n_iter_sparse > 0\n    assert_array_almost_equal(coefs, coefs_sparse)\n"], "sample_799": ["def test_cross_val_predict_no_predict_method():\n    # Test cross_val_predict with an estimator that doesn't have a predict method\n    class NoPredictEstimator(BaseEstimator):\n            return self\n    \n    X, y = make_classification(n_samples=30, n_features=2, random_state=0)\n    est = NoPredictEstimator()\n    \n    assert_raises(AttributeError, cross_val_predict, est, X, y)\n", "def test_cross_val_score_with_callable_scorer():\n    # Ensure that cross_val_score works with a callable scorer\n        return np.mean(estimator.predict(X) == y)\n    \n    # Create a simple dataset\n    X, y = make_classification(n_samples=100, n_features=4, random_state=0)\n    clf = MockClassifier()\n    \n    # Use cross_val_score with the custom scorer\n    scores = cross_val_score(clf, X, y, scoring=custom_scorer, cv=5)\n    \n    # Verify that scores are computed correctly\n    expected_scores = np.ones(5)  # Since MockClassifier always predicts perfectly\n    assert_array_equal(scores, expected_scores)\n", "def test_cross_val_predict_with_groups():\n    # Test that cross_val_predict correctly handles groups parameter.\n    X, y = make_classification(n_samples=100, n_classes=2, random_state=42)\n    groups = np.array([i // 10 for i in range(100)])\n    \n    clf = SVC(kernel=\"linear\")\n    group_cvs = [LeaveOneGroupOut(), GroupKFold(n_splits=5), GroupShuffleSplit(n_splits=5)]\n    \n    for cv in group_cvs:\n        # Check that the 'groups' parameter works for cross_val_predict\n        predictions = cross_val_predict(clf, X, y, cv=cv, groups=groups)\n        assert_equal(len(predictions), len(y))\n        \n        # Ensure that predictions are consistent with manual loop\n        manual_predictions = np.zeros_like(y)\n        for train_idx, test_idx in cv.split(X, y, groups):\n            clf.fit(X[train_idx], y[train_idx])\n            manual_predictions[test_idx] = clf.predict(X[test_idx])\n        \n        assert_array_equal(predictions, manual_predictions)\n", "def test_cross_validate_with_error_score():\n    # Test cross_validate with various error_score values\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = FailingClassifier(parameter=FailingClassifier.FAILING_PARAMETER)\n    \n    # error_score='raise'\n    with pytest.raises(ValueError, match=\"Failing classifier failed as required\"):\n        cross_validate(clf, X, y, cv=3, error_score='raise')\n    \n    # error_score='raise-deprecating'\n    with pytest.warns(FutureWarning, match=\"From version 0.22, errors during fit will result in a cross validation score of NaN by default\"):\n        with pytest.raises(ValueError, match=\"Failing classifier failed as required\"):\n            cross_validate(clf, X, y, cv=3, error_score='raise-deprecating')\n    \n    # error_score=np.nan\n    scores = cross_validate(clf, X, y, cv=3, error_score=np.nan)\n    assert np.isnan(scores['test_score']).all()\n    \n    # error_score=0\n    scores = cross_validate(clf, X, y, cv=3, error_score=0)\n    assert (scores['test_score'] == 0).all()\n", "def test_cross_val_predict_group_kfold():\n    # Test cross_val_predict with GroupKFold\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_clusters_per_class=1, random_state=0)\n    groups = np.array([0, 1, 1, 2, 2, 2, 0, 0, 1, 1, 0, 2, 1, 2, 0, 0, 1, 2, 2, 1, 0, 1, 1, 2, 1, 0, 2, 2, 0, 1])\n    clf = LogisticRegression()\n    cv = GroupKFold(n_splits=3)\n    \n    # Naive loop (should be same as cross_val_predict):\n    preds2 = np.zeros_like(y)\n    for train, test in cv.split(X, y, groups):\n        clf.fit(X[train], y[train])\n        preds2[test] = clf.predict(X[test])\n\n    preds = cross_val_predict(clf, X, y, cv=cv, groups=groups)\n    assert_array_almost_equal(preds, preds2)\n\n    # Verify length of predictions\n    assert_equal(len(preds), len(y))\n\n    # Ensure predictions are same for all group splits\n    group_kfold = GroupKFold(n_splits=3)\n    predictions = cross_val_predict(clf, X, y, cv=group_kfold, groups=groups)\n    assert_equal(len(predictions), len(y))\n"], "sample_800": ["def test_check_supervised_y_no_nan():\n    # Check that the estimator raises an error when y contains NaN or inf\n    class EstimatorWithYCheck(BaseEstimator):\n            if np.any(np.isnan(y)) or np.any(np.isinf(y)):\n                raise ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\")\n            return self\n\n    estimator = EstimatorWithYCheck()\n    X = np.array([[1], [2], [3]])\n    y_with_nan = np.array([1, 2, np.nan])\n    y_with_inf = np.array([1, 2, np.inf])\n\n    check_supervised_y_no_nan(\"EstimatorWithYCheck\", estimator)\n\n    with assert_raises_regex(ValueError, \"Input contains NaN, infinity or a value too large for dtype('float64').\"):\n        estimator.fit(X, y_with_nan)\n\n    with assert_raises_regex(ValueError, \"Input contains NaN, infinity or a value too large for dtype('float64').\"):\n        estimator.fit(X, y_with_inf)\n", "def test_check_transformer_n_iter():\n    # Tests that check_transformer_n_iter correctly identifies transformers\n    # with a max_iter parameter and checks that n_iter_ is at least 1.\n\n    class TransformerWithNIter(BaseEstimator):\n            self.max_iter = max_iter\n\n            self.n_iter_ = self.max_iter\n            return self\n\n            return X\n\n    # Test a transformer that correctly sets n_iter_\n    transformer = TransformerWithNIter(max_iter=10)\n    check_transformer_n_iter(\"TransformerWithNIter\", transformer)\n\n    # Test a transformer that incorrectly sets n_iter_ to 0\n    class TransformerWithZeroNIter(BaseEstimator):\n            self.max_iter = max_iter\n\n            self.n_iter_ = 0\n            return self\n\n            return X\n\n    transformer_with_zero_n_iter = TransformerWithZeroNIter(max_iter=10)\n    msg = \"0 is not >= 1\"\n    assert_raises_regex(AssertionError, msg, check_transformer_n_iter, \"TransformerWithZeroNIter\", transformer_with_zero_n_iter)\n", "def test_check_classifiers_one_label():\n    # Check that classifiers raise an error when only one class is present in y\n    assert_raises_regex(ValueError, \"Classifier can't train when only one class is present.\",\n                        check_classifiers_one_label, 'BaseBadClassifier', BaseBadClassifier())\n    assert_raises_regex(ValueError, \"Classifier can't train when only one class is present.\",\n                        check_classifiers_one_label, 'NoSparseClassifier', NoSparseClassifier())\n    assert_raises_regex(ValueError, \"Classifier can't train when only one class is present.\",\n                        check_classifiers_one_label, 'CorrectNotFittedErrorClassifier', CorrectNotFittedErrorClassifier())\n", "def test_check_estimators_nan_inf():\n    # check that estimators raise error on NaN and inf in input\n    class EstimatorWithNanInfCheck(BaseBadClassifier):\n            X, y = check_X_y(X, y)\n            if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n                raise ValueError(\"Input contains NaN or infinity values\")\n            return self\n\n            X = check_array(X)\n            if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n                raise ValueError(\"Input contains NaN or infinity values\")\n            return np.ones(X.shape[0])\n\n    check_estimator(EstimatorWithNanInfCheck)\n", "def test_check_estimators_partial_fit_n_features():\n    # check if number of features changes between calls to partial_fit\n    class EstimatorWithPartialFit(BaseEstimator):\n            self.is_fitted_ = False\n\n            if hasattr(self, 'n_features_') and self.n_features_ != X.shape[1]:\n                raise ValueError(\"Number of features is inconsistent.\")\n            self.n_features_ = X.shape[1]\n            self.is_fitted_ = True\n            return self\n\n            self.is_fitted_ = True\n            self.n_features_ = X.shape[1]\n            return self\n\n            check_is_fitted(self, 'is_fitted_')\n            if X.shape[1] != self.n_features_:\n                raise ValueError(\"Number of features is inconsistent.\")\n            return np.ones(X.shape[0])\n    \n    check_estimator(EstimatorWithPartialFit)\n"], "sample_801": ["def test_clone():\n    # Test the clone function for BaseEstimator\n    lr = LogisticRegression(C=2, fit_intercept=False)\n    lr_clone = clone(lr)\n    \n    assert lr.get_params() == lr_clone.get_params()\n    assert lr is not lr_clone\n\n    # Test with a list of BaseEstimators\n    estimators = [LogisticRegression(C=2), StandardScaler()]\n    estimators_clone = clone(estimators)\n\n    for est, est_clone in zip(estimators, estimators_clone):\n        assert est.get_params() == est_clone.get_params()\n        assert est is not est_clone\n\n    # Test with non-BaseEstimator object and safe=False\n    custom_obj = {'key': 'value'}\n    custom_obj_clone = clone(custom_obj, safe=False)\n    assert custom_obj == custom_obj_clone\n    assert custom_obj is not custom_obj_clone\n\n    # Test with non-BaseEstimator object and safe=True, expecting TypeError\n    try:\n        clone(custom_obj, safe=True)\n    except TypeError as e:\n        assert str(e) == \"Cannot clone object '{'key': 'value'}' (type <class 'dict'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.\"\n    else:\n        assert False, \"Expected TypeError not raised\"\n", "def test_clone():\n    # Test the clone functionality\n    lr = LogisticRegression(C=2)\n    lr_clone = clone(lr)\n    assert lr.get_params() == lr_clone.get_params()\n    assert lr is not lr_clone  # Ensure a new instance is created\n\n    # Test with nested estimators\n    pipeline = make_pipeline(StandardScaler(), lr)\n    pipeline_clone = clone(pipeline)\n    assert pipeline.get_params() == pipeline_clone.get_params()\n    assert pipeline is not pipeline_clone\n\n    # Test safe=False functionality\n    custom_object = {\"key\": \"value\"}\n    cloned_object = clone(custom_object, safe=False)\n    assert custom_object == cloned_object\n    assert custom_object is not cloned_object\n\n    # Test clone raises TypeError for non-estimator objects when safe=True\n    try:\n        clone(custom_object)\n    except TypeError as e:\n        assert \"Cannot clone object\" in str(e)\n    else:\n        raise AssertionError(\"Expected TypeError was not raised\")\n", "def test_clone_estimator():\n    # Test the clone function for a custom estimator\n    class CustomEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n    est = CustomEstimator(param1=3, param2=4)\n    cloned_est = clone(est)\n\n    assert cloned_est.param1 == 3\n    assert cloned_est.param2 == 4\n    assert est is not cloned_est\n\n    # Test cloning with nested estimators\n    class NestedEstimator(BaseEstimator):\n            self.base_estimator = base_estimator\n\n    nested_est = NestedEstimator(base_estimator=est)\n    cloned_nested_est = clone(nested_est)\n\n    assert cloned_nested_est.base_estimator.param1 == 3\n    assert cloned_nested_est.base_estimator.param2 == 4\n    assert nested_est is not cloned_nested_est\n    assert nested_est.base_estimator is not cloned_nested_est.base_estimator\n\n    # Test cloning with a list of estimators\n    est_list = [CustomEstimator(param1=i) for i in range(5)]\n    cloned_est_list = clone(est_list)\n\n    for i, cloned_est in enumerate(cloned_est_list):\n        assert cloned_est.param1 == i\n        assert est_list[i] is not cloned_est\n", "def test_clone():\n    # Test the clone function to ensure it correctly clones an estimator\n\n    class MockEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n    estimator = MockEstimator(param1=10, param2=20)\n    cloned_estimator = clone(estimator)\n\n    assert estimator.param1 == cloned_estimator.param1\n    assert estimator.param2 == cloned_estimator.param2\n    assert estimator is not cloned_estimator  # Ensure it's a different object\n\n    # Test cloning a list of estimators\n    estimators = [MockEstimator(param1=30, param2=40), MockEstimator(param1=50, param2=60)]\n    cloned_estimators = clone(estimators)\n\n    for orig, cloned in zip(estimators, cloned_estimators):\n        assert orig.param1 == cloned.param1\n        assert orig.param2 == cloned.param2\n        assert orig is not cloned\n\n    # Test cloning a non-estimator object with safe=False\n    obj = [1, 2, 3]\n    cloned_obj = clone(obj, safe=False)\n    assert obj == cloned_obj\n    assert obj is not cloned_obj\n\n    # Test cloning a non-estimator object with safe=True should raise TypeError\n    try:\n        clone(obj, safe=True)\n    except TypeError as e:\n        assert \"Cannot clone object\" in str(e)\n\n", "def test_clone_estimator():\n    # Test the clone function to ensure it correctly clones an estimator\n    lr = LogisticRegression(C=2)\n    lr_clone = clone(lr)\n    \n    # Check if the parameters are the same\n    assert lr.get_params() == lr_clone.get_params()\n    \n    # Check if the cloned object is a different instance\n    assert lr is not lr_clone\n    \n    # Check if a deep copy was not done for attached data (as there is no data attached initially, this will pass)\n    lr.fit([], [])\n    assert lr is not lr_clone\n\n    # Check if an exception is raised for non-estimator objects when safe=True\n    try:\n        clone(object(), safe=True)\n    except TypeError:\n        pass\n    else:\n        assert False, \"TypeError not raised for non-estimator object when safe=True\"\n    \n    # Check if deep copy fallback works for non-estimator objects when safe=False\n    obj = object()\n    cloned_obj = clone(obj, safe=False)\n    assert obj is not cloned_obj\n    assert obj == cloned_obj\n\n    # Check if the clone function correctly clones nested estimators\n    pipeline = Pipeline([('scaler', StandardScaler()), ('classifier', lr)])\n    pipeline_clone = clone(pipeline)\n    assert pipeline.get_params() == pipeline_clone.get_params()\n    assert pipeline is not pipeline_clone\n    assert pipeline.steps[0][1] is not pipeline_clone.steps[0][1]\n    assert pipeline.steps[1][1] is not pipeline_clone.steps[1][1]\n"], "sample_802": ["def test_pipeline_len():\n    # Test __len__ method of Pipeline\n    pipe = Pipeline([('transf1', Transf()), ('transf2', Transf()), ('clf', FitParamT())])\n    assert len(pipe) == 3\n\n    pipe2 = Pipeline([('clf', FitParamT())])\n    assert len(pipe2) == 1\n", "def test_pipeline_get_params():\n    # Test that get_params works correctly and returns expected results.\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n    params = pipe.get_params()\n    expected_keys = [\n        'memory', 'steps', 'transf', 'clf', \n        'transf__a', 'transf__b', 'clf__successful'\n    ]\n    assert_equal(sorted(params.keys()), sorted(expected_keys))\n    assert params['transf'] == pipe.named_steps['transf']\n    assert params['clf'] == pipe.named_steps['clf']\n\n    # Test with nested pipeline\n    nested_pipe = Pipeline([\n        ('pca', PCA(n_components=2)), \n        ('clf_pipe', Pipeline([('svc', SVC())]))\n    ])\n    nested_params = nested_pipe.get_params()\n    expected_nested_keys = [\n        'memory', 'steps', 'pca', 'clf_pipe',\n        'pca__copy', 'pca__iterated_power', 'pca__n_components', 'pca__random_state',\n        'pca__svd_solver', 'pca__tol', 'pca__whiten',\n        'clf_pipe__memory', 'clf_pipe__steps', 'clf_pipe__svc',\n        'clf_pipe__svc__C', 'clf_pipe__svc__break_ties', 'clf_pipe__svc__cache_size',\n        'clf_pipe__svc__class_weight', 'clf_pipe__svc__coef0', 'clf_pipe__svc__decision_function_shape',\n        'clf_pipe__svc__degree', 'clf_pipe__svc__gamma', 'clf_pipe__svc__kernel', \n        'clf_pipe__svc__max_iter', 'clf_pipe__svc__probability', 'clf_pipe__svc__random_state', \n        'clf_pipe__svc__shrinking', 'clf_pipe__svc__tol', 'clf_pipe__svc__verbose'\n    ]\n    assert_equal(sorted(nested_params.keys()), sorted(expected_nested_keys))\n    assert nested_params['pca'] == nested_pipe.named_steps['pca']\n    assert nested_params['clf_pipe'] == nested_pipe.named_steps['clf_pipe']\n", "def test_pipeline_len():\n    # Test the __len__ method of the Pipeline\n    clf = SVC()\n    filter1 = SelectKBest(f_classif)\n    pipe = Pipeline([('anova', filter1), ('svc', clf)])\n    assert_equal(len(pipe), 2)\n\n    pipe = Pipeline([('svc', clf)])\n    assert_equal(len(pipe), 1)\n\n    pipe = Pipeline([])\n    assert_equal(len(pipe), 0)\n", "def test_pipeline_with_invalid_step():\n    # Test that a pipeline with an invalid step raises an appropriate error\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    class InvalidStep:\n            pass\n\n            return X\n\n    invalid_step = InvalidStep()\n    pipe = Pipeline([('invalid', invalid_step), ('clf', SVC())])\n\n    # Invalid step should raise an error during fit\n    assert_raises_regex(TypeError,\n                        \"All intermediate steps should be transformers and implement fit and transform\",\n                        pipe.fit, X, y)\n\n    # Invalid step should raise an error during transform\n    pipe = Pipeline([('invalid', invalid_step)])\n    assert_raises_regex(TypeError,\n                        \"All intermediate steps should be transformers and implement fit and transform\",\n                        pipe.transform, X)\n\n    # Invalid step should raise an error during fit_transform\n    assert_raises_regex(TypeError,\n                        \"All intermediate steps should be transformers and implement fit and transform\",\n                        pipe.fit_transform, X, y)\n", "def test_pipeline_transform_with_passthrough():\n    # Test pipeline with 'passthrough' in the middle of the steps\n    iris = load_iris()\n    X = iris.data\n    pca = PCA(n_components=2, svd_solver='full')\n    transf = Transf()\n    pipeline = Pipeline([('pca', pca), ('pass', 'passthrough'), ('transf', transf)])\n\n    # Test fit_transform\n    X_trans = pipeline.fit_transform(X)\n    X_pca = pca.fit_transform(X)\n    assert_array_almost_equal(X_trans, transf.fit_transform(X_pca))\n\n    # Test transform after fitting\n    X_trans2 = pipeline.fit(X).transform(X)\n    assert_array_almost_equal(X_trans2, transf.transform(X_pca))\n"], "sample_803": ["def test_coverage_error_no_positive_labels():\n    # Test coverage error when there are no positive labels in y_true\n    y_true = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n    y_score = np.array([[0.1, 0.4, 0.35], [0.2, 0.5, 0.1], [0.3, 0.3, 0.3]])\n    assert_almost_equal(coverage_error(y_true, y_score), 0)\n\n    # Test coverage error when there is a mix of positive and negative labels\n    y_true = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n    y_score = np.array([[0.1, 0.4, 0.35], [0.2, 0.5, 0.1], [0.3, 0.3, 0.3]])\n    assert_almost_equal(coverage_error(y_true, y_score), 2)\n", "def test_average_precision_score_multiclass():\n    # Test that average_precision_score returns correct value for multiclass\n    y_true = np.array([0, 1, 2, 2])\n    y_score = np.array([[0.9, 0.05, 0.05], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8], [0.1, 0.2, 0.7]])\n    score = average_precision_score(y_true, y_score)\n    expected_score = (1.0 + 1.0 + 1.0) / 3  # Perfect prediction for each class\n    assert_almost_equal(score, expected_score)\n\n    # Test with sample weights\n    sample_weight = np.array([1, 2, 3, 4])\n    weighted_score = average_precision_score(y_true, y_score, sample_weight=sample_weight)\n    expected_weighted_score = (1.0 * 1 + 1.0 * 2 + 1.0 * 3 + 1.0 * 4) / sum(sample_weight)\n    assert_almost_equal(weighted_score, expected_weighted_score)\n", "def test_average_precision_score_edge_cases():\n    # Test average_precision_score for edge cases\n\n    # Only one positive label\n    y_true = [1]\n    y_score = [0.5]\n    assert_almost_equal(average_precision_score(y_true, y_score), 1.0)\n\n    y_true = [0, 0, 1]\n    y_score = [0.5, 0.5, 0.5]\n    assert_almost_equal(average_precision_score(y_true, y_score), 1.0)\n\n    y_true = [0, 0, 1, 0, 0]\n    y_score = [0.5, 0.5, 0.5, 0.5, 0.5]\n    assert_almost_equal(average_precision_score(y_true, y_score), 1.0)\n", "def test_coverage_error_toy_data():\n    # Test coverage_error with toy data\n    y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    y_score = np.array([[0.2, 0.1, 0.4], [0.3, 0.6, 0.2], [0.5, 0.4, 0.8]])\n\n    # Test with no sample weights\n    assert_almost_equal(coverage_error(y_true, y_score), 1.6666667, decimal=7)\n\n    # Test with sample weights\n    sample_weight = np.array([1, 1, 1])\n    assert_almost_equal(coverage_error(y_true, y_score, sample_weight=sample_weight), 1.6666667, decimal=7)\n\n    # Test with different sample weights\n    sample_weight = np.array([2, 1, 3])\n    assert_almost_equal(coverage_error(y_true, y_score, sample_weight=sample_weight), 1.8333333, decimal=7)\n", "def test_average_precision_score_multilabel():\n    # Test average precision score for multilabel-indicator format\n    y_true = np.array([[0, 1], [1, 0], [1, 1], [0, 0]])\n    y_scores = np.array([[0.5, 0.7], [0.3, 0.9], [0.8, 0.6], [0.2, 0.4]])\n    average_precision = average_precision_score(y_true, y_scores, average='macro')\n    assert_almost_equal(average_precision, 0.8333333333333333)\n\n    average_precision = average_precision_score(y_true, y_scores, average='micro')\n    assert_almost_equal(average_precision, 0.875)\n\n    average_precision = average_precision_score(y_true, y_scores, average='samples')\n    assert_almost_equal(average_precision, 0.875)\n\n    average_precision = average_precision_score(y_true, y_scores, average='weighted')\n    assert_almost_equal(average_precision, 0.8333333333333333)\n"], "sample_804": ["def test_ordinal_encoder_with_mixed_types():\n    # Test OrdinalEncoder with mixed types\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', '2'], ['Male', 2]]\n    enc.fit(X)\n    expected_categories = [\n        np.array(['Female', 'Male'], dtype=object),\n        np.array([1, 2, 3, '2'], dtype=object)\n    ]\n    for cats, expected_cats in zip(enc.categories_, expected_categories):\n        assert_array_equal(cats, expected_cats)\n\n    # Transform the mixed type data\n    X_trans = enc.transform(X)\n    expected_trans = np.array([\n        [1, 0],\n        [0, 2],\n        [0, 3],\n        [1, 1]\n    ], dtype=np.float64)\n    assert_array_equal(X_trans, expected_trans)\n\n    # Inverse transform the data\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X_inv, np.array(X, dtype=object))\n", "def test_ordinal_encoder_transform_error():\n    # Test that an error is raised if the number of features during transform\n    # is different from the number of features during fit.\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 2]]\n    enc.fit(X)\n    \n    X_diff = [['Male'], ['Female']]\n    msg = (\"Shape of the passed X data is not correct. Expected 2 columns, got 1.\")\n    with pytest.raises(ValueError, match=msg):\n        enc.transform(X_diff)\n\n    X_diff = [['Male', 1, 0], ['Female', 2, 1]]\n    msg = (\"Shape of the passed X data is not correct. Expected 2 columns, got 3.\")\n    with pytest.raises(ValueError, match=msg):\n        enc.transform(X_diff)\n", "def test_ordinal_encoder_handle_unknown():\n    # Test that ordinal encoder raises error for unknown categories during transform\n    X = np.array([['cat', 'dog'], ['bird', 'fish']])\n    X_new = np.array([['cat', 'elephant']])\n\n    enc = OrdinalEncoder(handle_unknown='error')\n    enc.fit(X)\n    assert_raises(ValueError, enc.transform, X_new)\n    \n    # Test the ignore option, ignores unknown categories\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X_new_transformed = enc.transform(X_new)\n    expected_transformed = np.array([[0., np.nan]])\n    assert_array_equal(X_new_transformed, expected_transformed)\n\n    # Test the unknown categories result in NaN after inverse_transform\n    X_transformed = enc.transform(X)\n    X_inverse_transformed = enc.inverse_transform(X_transformed)\n    assert_array_equal(X, X_inverse_transformed)\n", "def test_ordinal_encoder_inverse_with_nan():\n    X = [['a', 2, 55], ['b', np.nan, 55], ['a', 3, np.nan]]\n    enc = OrdinalEncoder()\n    X_tr = enc.fit_transform(X)\n\n    X_inv = enc.inverse_transform(X_tr)\n    assert_allclose(X_inv[:, 0], ['a', 'b', 'a'])\n    assert np.isnan(X_inv[1, 1])\n    assert np.isnan(X_inv[2, 2])\n\n    # Test with pandas DataFrame input\n    pd = pytest.importorskip(\"pandas\")\n    X_df = pd.DataFrame(X, columns=[\"col1\", \"col2\", \"col3\"])\n    enc = OrdinalEncoder()\n    X_tr = enc.fit_transform(X_df)\n\n    X_inv = enc.inverse_transform(X_tr)\n    assert_allclose(X_inv[:, 0], ['a', 'b', 'a'])\n    assert np.isnan(X_inv[1, 1])\n    assert np.isnan(X_inv[2, 2])\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['cat', 1, 'red'], ['dog', 3, 'green']])\n    X2 = np.array([['cat', 2, 'blue'], ['mouse', 1, 'red']])\n\n    # Test that ordinal encoder raises error for unknown features\n    # present during transform.\n    oe = OrdinalEncoder(handle_unknown='error')\n    oe.fit(X)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        oe.transform(X2)\n\n    # Test the ignore option, ignores unknown features\n    oe = OrdinalEncoder(handle_unknown='ignore')\n    oe.fit(X)\n    X2_transformed = oe.transform(X2)\n    expected_transformed = np.array([[0., np.nan, np.nan], [np.nan, 0., 0.]])\n    assert np.allclose(X2_transformed, expected_transformed, equal_nan=True)\n"], "sample_805": ["def test_max_error():\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2, 8]\n    assert_almost_equal(max_error(y_true, y_pred), 1.0)\n\n    y_true = [1, 1, 1, 1]\n    y_pred = [1, 1, 1, 1]\n    assert_almost_equal(max_error(y_true, y_pred), 0.0)\n\n    y_true = [0, 0, 0, 0]\n    y_pred = [1, 1, 1, 1]\n    assert_almost_equal(max_error(y_true, y_pred), 1.0)\n\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2, 8]\n    with pytest.raises(ValueError, match=\"Multioutput not supported in max_error\"):\n        max_error([[3, 1], [0, -0.5], [2, 1], [7, 1]], [[2.5, 1], [0.0, 0], [2, 1], [8, 1]])\n\n    y_true = np.array([1, 2, 3])\n    y_pred = np.array([1, 2, 3])\n    assert_almost_equal(max_error(y_true, y_pred), 0.0)\n", "def test_max_error():\n    # Test the max_error function\n    y_true = [3, 2, 7, 1]\n    y_pred = [4, 2, 7, 1]\n    assert_almost_equal(max_error(y_true, y_pred), 1)\n\n    # Test with negative values\n    y_true = [-3, -2, -7, -1]\n    y_pred = [-4, -2, -7, -1]\n    assert_almost_equal(max_error(y_true, y_pred), 1)\n\n    # Test with a larger dataset\n    y_true = np.arange(1000)\n    y_pred = y_true + np.random.RandomState(0).randn(1000)\n    assert_almost_equal(max_error(y_true, y_pred), np.max(np.abs(y_true - y_pred)))\n\n    # Test multioutput case (should raise ValueError)\n    y_true = [[1, 2], [3, 4], [5, 6]]\n    y_pred = [[1, 2], [3, 5], [5, 6]]\n    with pytest.raises(ValueError, match=\"Multioutput not supported in max_error\"):\n        max_error(y_true, y_pred)\n", "def test_max_error():\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2, 8]\n    assert_almost_equal(max_error(y_true, y_pred), 1.0)\n\n    y_true = [1, 2, 3, 4]\n    y_pred = [2, 2, 3, 4]\n    assert_almost_equal(max_error(y_true, y_pred), 1.0)\n\n    # Test multioutput case raises an error\n    y_true = np.array([[1, 2], [2, 3]])\n    y_pred = np.array([[2, 2], [2, 3]])\n    with pytest.raises(ValueError, match=\"Multioutput not supported in max_error\"):\n        max_error(y_true, y_pred)\n", "def test_max_error():\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2, 8]\n    assert_almost_equal(max_error(y_true, y_pred), 1.0)\n\n    y_true = [1, 2, 3, 4, 5]\n    y_pred = [1, 2, 3, 4, 5]\n    assert_almost_equal(max_error(y_true, y_pred), 0.0)\n\n    y_true = [1, 2, 3, 4, 5]\n    y_pred = [1, 3, 3, 2, 5]\n    assert_almost_equal(max_error(y_true, y_pred), 2.0)\n\n    # Check for exception when multioutput is passed\n    y_true = [[1, 2], [3, 4], [5, 6]]\n    y_pred = [[1, 2], [3, 4], [5, 6]]\n    assert_raises(ValueError, max_error, y_true, y_pred)\n", "def test_max_error_exceptions():\n    y_true = np.array([[1, 2], [3, 4]])\n    y_pred = np.array([[1, 2], [3, 4]])\n    \n    # Test for continuous-multioutput not supported in max_error\n    with pytest.raises(ValueError, match=\"Multioutput not supported in max_error\"):\n        max_error(y_true, y_pred)\n\n    y_true = np.array([1, 2, 3])\n    y_pred = np.array([1, 2])\n    \n    # Test for inconsistent length error\n    with pytest.raises(ValueError, match=\"Found input variables with inconsistent numbers of samples\"):\n        max_error(y_true, y_pred)\n\n    y_true = np.array([1, 2, 3])\n    y_pred = np.array([1, 2, 3])\n    \n    # Test for valid max error calculation\n    assert_almost_equal(max_error(y_true, y_pred), 0.0)\n"], "sample_806": ["def test_estimator_inheritance():\n    # Check if GradientBoostingClassifier and GradientBoostingRegressor\n    # inherit from correct base classes.\n\n    assert issubclass(GradientBoostingClassifier, BaseGradientBoosting)\n    assert issubclass(GradientBoostingClassifier, ClassifierMixin)\n\n    assert issubclass(GradientBoostingRegressor, BaseGradientBoosting)\n    assert issubclass(GradientBoostingRegressor, RegressorMixin)\n", "def test_gradient_boosting_classifier_staged_predict():\n    # Test staged_predict on GradientBoostingClassifier.\n    X, y = datasets.make_classification(n_samples=1200, random_state=1)\n    X_train, y_train = X[:200], y[:200]\n    X_test, y_test = X[200:], y[200:]\n\n    clf = GradientBoostingClassifier(n_estimators=50, random_state=1)\n    clf.fit(X_train, y_train)\n    staged_predictions = list(clf.staged_predict(X_test))\n\n    # Check that the final staged prediction equals the normal prediction\n    final_staged_prediction = staged_predictions[-1]\n    normal_prediction = clf.predict(X_test)\n    assert_array_equal(final_staged_prediction, normal_prediction)\n\n    # Check that the predictions are consistent and improve over iterations\n    accuracy = [np.mean(pred == y_test) for pred in staged_predictions]\n    assert np.all(np.diff(accuracy) >= 0)\n\n    # Check staged predict_proba\n    staged_proba = list(clf.staged_predict_proba(X_test))\n    final_staged_proba = staged_proba[-1]\n    normal_proba = clf.predict_proba(X_test)\n    assert_array_almost_equal(final_staged_proba, normal_proba)\n", "def test_custom_init_estimator():\n    # Test if custom init estimators work correctly with GradientBoosting\n    class CustomInitEstimator(BaseEstimator):\n            self.constant_ = np.mean(y)\n            return self\n\n            return np.full(X.shape[0], self.constant_)\n\n            proba = np.zeros((X.shape[0], 2))\n            proba[:, 1] = 1.0\n            return proba\n\n    X, y = datasets.make_classification(n_samples=100, random_state=1)\n    init_estimator = CustomInitEstimator()\n\n    gb_clf = GradientBoostingClassifier(init=init_estimator, n_estimators=10)\n    gb_clf.fit(X, y)\n    assert_greater(gb_clf.score(X, y), 0.9)\n\n    gb_reg = GradientBoostingRegressor(init=init_estimator, n_estimators=10)\n    gb_reg.fit(X, y)\n    assert_less(mean_squared_error(y, gb_reg.predict(X)), 0.2)\n", "def test_huber_loss_function():\n    # Test Huber loss function with various alpha values\n    X, y = datasets.make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n    alphas = [0.5, 0.75, 0.9, 0.95]\n\n    for alpha in alphas:\n        gbr = GradientBoostingRegressor(loss='huber', alpha=alpha, n_estimators=50, random_state=42)\n        gbr.fit(X, y)\n        y_pred = gbr.predict(X)\n        mse = mean_squared_error(y, y_pred)\n        assert mse < 0.1, f\"Failed for alpha={alpha} with MSE={mse}\"\n", "def test_invalid_alpha_quantile_estimator():\n    # Test if QuantileEstimator raises ValueError with invalid alpha.\n    with pytest.raises(ValueError, match=\"`alpha` must be in \\(0, 1.0\\)\"):\n        QuantileEstimator(alpha=-0.1)\n        \n    with pytest.raises(ValueError, match=\"`alpha` must be in \\(0, 1.0\\)\"):\n        QuantileEstimator(alpha=1.1)\n        \n    with pytest.raises(ValueError, match=\"`alpha` must be in \\(0, 1.0\\)\"):\n        QuantileEstimator(alpha=0)\n        \n    with pytest.raises(ValueError, match=\"`alpha` must be in \\(0, 1.0\\)\"):\n        QuantileEstimator(alpha=1)\n"], "sample_807": ["def test_calibration_invalid_cv():\n    \"\"\"Test that an invalid cv parameter raises an error\"\"\"\n    X, y = make_classification(n_samples=100, n_features=6, random_state=42)\n    \n    clf = LinearSVC()\n    invalid_cv = \"invalid_cv\"\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=invalid_cv)\n    \n    with pytest.raises(ValueError, match=r\".* cross-validation splitting strategy .*\"):\n        cal_clf.fit(X, y)\n", "def test_calibration_with_different_base_estimators():\n    \"\"\"Test calibration with different base estimators\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6, random_state=42)\n    X_train, y_train = X[:n_samples], y[:n_samples]\n    X_test, y_test = X[n_samples:], y[n_samples:]\n\n    base_estimators = [\n        RandomForestClassifier(n_estimators=10, random_state=42),\n        LinearSVC(random_state=42),\n        MultinomialNB()\n    ]\n\n    for base_estimator in base_estimators:\n        for method in ['isotonic', 'sigmoid']:\n            calibrated_clf = CalibratedClassifierCV(base_estimator, method=method, cv=3)\n            calibrated_clf.fit(X_train, y_train)\n            probas = calibrated_clf.predict_proba(X_test)\n            assert_array_almost_equal(np.sum(probas, axis=1), np.ones(len(X_test)))\n\n            # Check that log-loss of calibrated classifier is smaller than\n            # log-loss of uncalibrated classifier\n            if hasattr(base_estimator, \"predict_proba\"):\n                uncalibrated_probas = base_estimator.fit(X_train, y_train).predict_proba(X_test)\n                uncalibrated_loss = log_loss(y_test, uncalibrated_probas)\n            else:\n                uncalibrated_decisions = base_estimator.fit(X_train, y_train).decision_function(X_test)\n                uncalibrated_probas = expit(uncalibrated_decisions)\n                uncalibrated_loss = log_loss(y_test, uncalibrated_probas)\n\n            calibrated_loss = log_loss(y_test, probas)\n            assert_greater(uncalibrated_loss, calibrated_loss)\n", "def test_calibrated_classifier_cv_default_base_estimator():\n    \"\"\"Test CalibratedClassifierCV with default base estimator (LinearSVC)\"\"\"\n    X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n    X_train, y_train = X[:80], y[:80]\n    X_test, y_test = X[80:], y[80:]\n\n    for method in ['sigmoid', 'isotonic']:\n        cal_clf = CalibratedClassifierCV(method=method, cv=3)\n        cal_clf.fit(X_train, y_train)\n        probas = cal_clf.predict_proba(X_test)\n        \n        # Check the shape of the output probabilities\n        assert probas.shape == (20, 2)\n        \n        # Check that the probabilities sum to 1\n        assert_array_almost_equal(probas.sum(axis=1), np.ones(probas.shape[0]))\n        \n        # Check that the predicted classes match the highest probability\n        y_pred = cal_clf.predict(X_test)\n        assert_array_equal(y_pred, np.argmax(probas, axis=1))\n", "def test_calibrated_classifier_cv_clone():\n    \"\"\"Test that CalibratedClassifierCV is cloneable\"\"\"\n    X, y = make_classification(n_samples=100, n_features=6, random_state=42)\n    clf = LinearSVC(random_state=42)\n\n    # Create CalibratedClassifierCV with sigmoid method and 2-fold CV\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=2)\n    cal_clf.fit(X, y)\n\n    from sklearn.base import clone\n\n    # Clone the CalibratedClassifierCV\n    cloned_clf = clone(cal_clf)\n\n    # Test that the clone has the same parameters but is not fitted yet\n    assert cloned_clf.method == cal_clf.method\n    assert cloned_clf.cv == cal_clf.cv\n    assert not hasattr(cloned_clf, \"calibrated_classifiers_\")\n\n    # Fit the cloned classifier and check it works as expected\n    cloned_clf.fit(X, y)\n    assert_array_almost_equal(cloned_clf.predict_proba(X), cal_clf.predict_proba(X))\n", "def test_predict_proba_shape():\n    \"\"\"Test that predict_proba returns array of correct shape\"\"\"\n    X, y = make_classification(n_samples=100, n_features=6, random_state=42)\n    X_train, y_train = X[:50], y[:50]\n    X_test = X[50:]\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train, y_train)\n\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n        cal_clf.fit(X_train, y_train)\n        probas = cal_clf.predict_proba(X_test)\n        assert_equal(probas.shape, (50, len(np.unique(y))))\n"], "sample_808": ["def test_iforest_predict():\n    \"\"\"Test the predict method of IsolationForest\"\"\"\n    rng = check_random_state(0)\n\n    # Generate train/test data\n    X = 0.3 * rng.randn(100, 2)\n    X_train = np.r_[X + 2, X - 2]\n\n    # Generate some abnormal novel observations\n    X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n    X_test = np.r_[X[80:], X_outliers]\n\n    # fit the model\n    clf = IsolationForest(max_samples=100, random_state=rng).fit(X_train)\n\n    # predict if a particular sample is an outlier or not\n    y_pred = clf.predict(X_test)\n\n    # check that inliers are labeled as 1 and outliers as -1\n    assert_array_equal(y_pred[:20], np.ones(20))\n    assert_array_equal(y_pred[20:], -np.ones(20))\n", "def test_iforest_fit_with_sample_weight():\n    \"\"\"Check Isolation Forest fitting with sample weights.\"\"\"\n    rng = check_random_state(42)\n    X = rng.randn(100, 2)\n    sample_weight = np.ones(100)\n    sample_weight[::2] = 2.0  # double the weight for even samples\n\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X, sample_weight=sample_weight)\n\n    # Ensure that the estimator was fitted correctly\n    assert hasattr(clf, \"estimators_\")\n    assert len(clf.estimators_) == clf.n_estimators\n\n    # Ensure that the decision function does not raise any errors\n    decision_function = clf.decision_function(X)\n    assert len(decision_function) == len(X)\n\n    # Check that the predict method works as expected\n    predict = clf.predict(X)\n    assert len(predict) == len(X)\n\n    # Check that the anomaly score reflects the sample weights\n    y_pred_no_weight = IsolationForest(random_state=rng).fit(X).decision_function(X)\n    assert np.mean(decision_function != y_pred_no_weight) > 0, \"Sample weights should affect the decision function\"\n", "def test_iforest_predict():\n    \"\"\"Test predict method of IsolationForest\"\"\"\n    rng = check_random_state(42)\n    \n    # Generate training data\n    X_train = 0.3 * rng.randn(100, 2)\n    X_train = np.r_[X_train + 2, X_train - 2]\n    \n    # Generate test data (normal data + outliers)\n    X_test = rng.uniform(low=-4, high=4, size=(20, 2))\n    X_test = np.r_[X_train[:20], X_test]\n    \n    # Train the model\n    clf = IsolationForest(max_samples=50, random_state=rng).fit(X_train)\n    \n    # Perform predictions\n    y_pred = clf.predict(X_test)\n    \n    # Check that the first 20 samples are inliers (+1) and the last 20 are outliers (-1)\n    assert_array_equal(y_pred[:20], np.ones(20))\n    assert_array_equal(y_pred[20:], -np.ones(20))\n", "def test_iforest_random_state():\n    \"\"\"Check that different random_state give different models and same random_state gives same model.\"\"\"\n    X_train = iris.data\n\n    clf1 = IsolationForest(random_state=0).fit(X_train)\n    clf2 = IsolationForest(random_state=42).fit(X_train)\n    clf3 = IsolationForest(random_state=0).fit(X_train)\n\n    # Different random states should lead to different predictions\n    pred1 = clf1.predict(X_train)\n    pred2 = clf2.predict(X_train)\n    assert not np.array_equal(pred1, pred2)\n\n    # Same random states should lead to same predictions\n    pred3 = clf3.predict(X_train)\n    assert_array_equal(pred1, pred3)\n", "def test_iforest_invalid_contamination():\n    \"\"\"Test Invalid contamination parameter.\"\"\"\n    X = iris.data\n\n    # Test invalid contamination values\n    for contamination in [-0.1, 0.6, 1.0, \"invalid\"]:\n        assert_raises(ValueError, IsolationForest(contamination=contamination).fit, X)\n\n    # Test valid contamination value\n    clf = IsolationForest(contamination=0.3).fit(X)\n    assert clf.contamination == 0.3\n"], "sample_809": ["def test_mutual_info_regression_sparse():\n    # Test mutual_info_regression with sparse input\n    rng = check_random_state(42)\n    X_dense = rng.rand(100, 5)\n    y = rng.rand(100)\n    X_sparse = csr_matrix(X_dense)\n\n    mi_dense = mutual_info_regression(X_dense, y, random_state=0)\n    mi_sparse = mutual_info_regression(X_sparse, y, random_state=0)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_estimate_mi_continuous_target():\n    # Test _estimate_mi directly with continuous target and continuous features.\n    rng = check_random_state(0)\n    X = rng.rand(100, 3)\n    y = X[:, 0] + 0.5 * X[:, 1] + rng.rand(100) * 0.1  # Continuous target\n\n    mi = _estimate_mi(X, y, discrete_features=False, discrete_target=False, n_neighbors=3, random_state=0)\n    assert_array_equal(np.argsort(-mi), [0, 1, 2])\n\n    for n_neighbors in [5, 7, 9]:\n        mi_nn = _estimate_mi(X, y, discrete_features=False, discrete_target=False, n_neighbors=n_neighbors, random_state=0)\n        # Check that mutual information increases or stays same with more neighbors\n        assert_greater(mi_nn[0], mi[0])\n        assert_greater(mi_nn[1], mi[1])\n        assert_greater(mi_nn[2], mi[2])\n", "def test_compute_mi_cc_negative_mi():\n    # Test that computed mutual information is non-negative for continuous variables.\n    rng = check_random_state(0)\n    x = rng.normal(size=100)\n    y = rng.normal(size=100)\n\n    for n_neighbors in [3, 5, 7]:\n        I_computed = _compute_mi(x, y, False, False, n_neighbors)\n        assert_greater_equal(I_computed, 0)\n", "def test_iterate_columns():\n    # Test _iterate_columns with dense array\n    X_dense = np.array([[1, 2, 3],\n                        [4, 5, 6],\n                        [7, 8, 9]])\n\n    columns = list(_iterate_columns(X_dense))\n    assert_equal(len(columns), 3)\n    assert_array_equal(columns[0], np.array([1, 4, 7]))\n    assert_array_equal(columns[1], np.array([2, 5, 8]))\n    assert_array_equal(columns[2], np.array([3, 6, 9]))\n\n    # Test _iterate_columns with sparse matrix\n    X_sparse = csr_matrix([[1, 0, 3],\n                           [0, 5, 0],\n                           [7, 0, 9]])\n\n    columns = list(_iterate_columns(X_sparse))\n    assert_equal(len(columns), 3)\n    assert_array_equal(columns[0], np.array([1, 0, 7]))\n    assert_array_equal(columns[1], np.array([0, 5, 0]))\n    assert_array_equal(columns[2], np.array([3, 0, 9]))\n\n    # Test _iterate_columns with specified columns\n    columns = list(_iterate_columns(X_dense, columns=[0, 2]))\n    assert_equal(len(columns), 2)\n    assert_array_equal(columns[0], np.array([1, 4, 7]))\n    assert_array_equal(columns[1], np.array([3, 6, 9]))\n", "def test_iterate_columns():\n    # Test dense matrix iteration\n    X_dense = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    dense_columns = list(_iterate_columns(X_dense))\n    assert_array_equal(dense_columns[0], [1, 4, 7])\n    assert_array_equal(dense_columns[1], [2, 5, 8])\n    assert_array_equal(dense_columns[2], [3, 6, 9])\n\n    # Test sparse matrix iteration\n    X_sparse = csr_matrix(X_dense)\n    sparse_columns = list(_iterate_columns(X_sparse))\n    assert_array_equal(sparse_columns[0], [1, 4, 7])\n    assert_array_equal(sparse_columns[1], [2, 5, 8])\n    assert_array_equal(sparse_columns[2], [3, 6, 9])\n\n    # Test specific columns iteration\n    specific_columns = list(_iterate_columns(X_dense, columns=[0, 2]))\n    assert_array_equal(specific_columns[0], [1, 4, 7])\n    assert_array_equal(specific_columns[1], [3, 6, 9])\n"], "sample_810": ["def test_pipeline_passthrough():\n    # Test that 'passthrough' works correctly in pipelines\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Pipeline with a passthrough step\n    pipe = Pipeline([\n        ('scaler', 'passthrough'),\n        ('svc', SVC(gamma='scale', random_state=0))\n    ])\n\n    # Fit and predict\n    pipe.fit(X, y)\n    assert pipe.score(X, y) > 0.9\n\n    # Set params to use an actual transformer\n    pipe.set_params(scaler=StandardScaler())\n    pipe.fit(X, y)\n    assert pipe.score(X, y) > 0.9\n\n    # Set params back to passthrough\n    pipe.set_params(scaler='passthrough')\n    pipe.fit(X, y)\n    assert pipe.score(X, y) > 0.9\n", "def test_pipeline_with_none_step():\n    # Test pipeline with None step works correctly.\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Create a pipeline with a None step in the middle\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('none', None),\n        ('svc', SVC(gamma='scale', probability=True, random_state=0))\n    ])\n\n    # Fit the pipeline\n    pipe.fit(X, y)\n\n    # Check the output shapes of various prediction functions\n    predict = pipe.predict(X)\n    assert_equal(predict.shape, (X.shape[0],))\n\n    proba = pipe.predict_proba(X)\n    assert_equal(proba.shape, (X.shape[0], len(np.unique(y))))\n\n    log_proba = pipe.predict_log_proba(X)\n    assert_equal(log_proba.shape, (X.shape[0], len(np.unique(y))))\n\n    decision_function = pipe.decision_function(X)\n    assert_equal(decision_function.shape, (X.shape[0], len(np.unique(y))))\n\n    score = pipe.score(X, y)\n    assert isinstance(score, float)\n\n    # Check that setting the None step works as expected\n    pipe.set_params(none='passthrough')\n    pipe.fit(X, y)\n    predict = pipe.predict(X)\n    assert_equal(predict.shape, (X.shape[0],))\n", "def test_pipeline_cached_transformer_reuse():\n    # Test that the pipeline reuses the cached transformer when memory is enabled\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            memory = Memory(cachedir=cachedir, verbose=0)\n        else:\n            memory = Memory(location=cachedir, verbose=0)\n\n        iris = load_iris()\n        X, y = iris.data, iris.target\n\n        # Create a DummyTransf with a cache to check for reuse\n        transf = DummyTransf()\n        pipeline = Pipeline([('transf', transf), ('svc', SVC())], memory=memory)\n\n        # Initial fit\n        pipeline.fit(X, y)\n        timestamp_initial_fit = pipeline.named_steps['transf'].timestamp_\n\n        # Fit again to check if the transformer reuses the cache\n        pipeline.fit(X, y)\n        timestamp_after_second_fit = pipeline.named_steps['transf'].timestamp_\n\n        assert timestamp_initial_fit == timestamp_after_second_fit, \\\n            \"The transformer was not reused from the cache.\"\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_fit_transform_intermediate_steps():\n    # Test that fit_transform works correctly with intermediate steps\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Create a pipeline with intermediate steps\n    transf1 = Transf()\n    transf2 = Transf()\n    clf = FitParamT()\n    pipe = Pipeline([('transf1', transf1), ('transf2', transf2), ('clf', clf)])\n\n    # Test fit_transform\n    X_trans = pipe.fit_transform(X, y)\n    X_trans_expected = transf2.fit_transform(transf1.fit_transform(X, y), y)\n    assert_array_almost_equal(X_trans, X_trans_expected)\n\n    # Test that intermediate steps are accessible and correct\n    assert pipe.named_steps['transf1'] is transf1\n    assert pipe.named_steps['transf2'] is transf2\n    assert pipe.named_steps['clf'] is clf\n\n    # Test fit, predict, and score\n    pipe.fit(X, y)\n    pred = pipe.predict(X)\n    score = pipe.score(X, y)\n    assert isinstance(pred, np.ndarray)\n    assert isinstance(score, (float, np.float64))\n", "def test_pipeline_memory_with_feature_union():\n    # Test Pipeline with FeatureUnion and memory caching\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n\n        # Construct a pipeline with a FeatureUnion\n        pca = PCA(n_components=2, svd_solver='randomized', random_state=0)\n        select = SelectKBest(k=1)\n        union = FeatureUnion([(\"pca\", pca), (\"select\", select)], memory=memory)\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        pipe = Pipeline([('union', union), ('svc', clf)], memory=memory)\n\n        # Memoize the transformer at the first fit\n        pipe.fit(X, y)\n        ts_pca = pipe.named_steps['union'].transformer_list[0][1].timestamp_\n        ts_select = pipe.named_steps['union'].transformer_list[1][1].timestamp_\n\n        # Check that we are reading the cache while fitting a second time\n        pipe.fit(X, y)\n        assert_equal(ts_pca, pipe.named_steps['union'].transformer_list[0][1].timestamp_)\n        assert_equal(ts_select, pipe.named_steps['union'].transformer_list[1][1].timestamp_)\n\n        # Ensure results are consistent with and without caching\n        pipe_no_cache = Pipeline([('union', FeatureUnion([(\"pca\", PCA(n_components=2, svd_solver='randomized', random_state=0)),\n                                                           (\"select\", SelectKBest(k=1))])), \n                                  ('svc', SVC(gamma='scale', probability=True, random_state=0))])\n        pipe_no_cache.fit(X, y)\n\n        assert_array_equal(pipe.predict(X), pipe_no_cache.predict(X))\n        assert_array_equal(pipe.predict_proba(X), pipe_no_cache.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X), pipe_no_cache.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), pipe_no_cache.score(X, y))\n    finally:\n        shutil.rmtree(cachedir)\n"], "sample_811": ["def test_pairwise_distances_chunked_different_metrics():\n    # Test pairwise distances chunked with different metrics for increased coverage\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((100, 4))\n    Y = rng.random_sample((80, 4))\n\n    metrics = ['cityblock', 'cosine', 'haversine', 'manhattan', 'euclidean']\n    for metric in metrics:\n        check_pairwise_distances_chunked(X, Y, working_memory=1, metric=metric)\n    \n    # Testing with a custom metric function\n        return np.sum((x - y) ** 2)\n\n    check_pairwise_distances_chunked(X, Y, working_memory=1, metric=custom_metric)\n\n    # Testing with minkowski metric and additional parameters\n    check_pairwise_distances_chunked(X, Y, working_memory=1, metric='minkowski', p=3)\n", "def test_pairwise_distances_precomputed():\n    # Test the precomputed metric with pairwise_distances\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    D = euclidean_distances(X)\n    \n    # Check that passing a precomputed distance matrix returns the same matrix\n    D_precomputed = pairwise_distances(D, metric='precomputed')\n    assert_array_almost_equal(D, D_precomputed)\n\n    # Check error when passing a non-square precomputed distance matrix\n    D_non_square = np.zeros((5, 3))\n    with pytest.raises(ValueError, match=\"shape\"):\n        pairwise_distances(D_non_square, metric='precomputed')\n\n    # Check error when passing a non-square precomputed distance matrix with Y\n    D_non_square_Y = np.zeros((5, 4))\n    with pytest.raises(ValueError, match=\"shape\"):\n        pairwise_distances(D_non_square_Y, np.zeros((4, 4)), metric='precomputed')\n\n    # Check error when precomputed distance matrix has negative values\n    D_negative = np.full((5, 5), -1)\n    with pytest.raises(ValueError, match=\"non-negative values\"):\n        pairwise_distances(D_negative, metric='precomputed')\n", "def test_cosine_similarity_sparse_output():\n    # Test the cosine_similarity function with sparse input\n    rng = np.random.RandomState(0)\n    X_dense = rng.random_sample((5, 4))\n    Y_dense = rng.random_sample((3, 4))\n    X_sparse = csr_matrix(X_dense)\n    Y_sparse = csr_matrix(Y_dense)\n\n    # Calculate cosine similarity with sparse input\n    K_sparse = cosine_similarity(X_sparse, Y_sparse)\n    K_dense = cosine_similarity(X_dense, Y_dense)\n\n    # The results should be the same as when the input is dense\n    assert_array_almost_equal(K_sparse, K_dense)\n\n    # Ensure that the output type is consistent\n    assert type(K_sparse) == type(K_dense)\n\n    # Test the diagonal elements to be 1 for self-similarity\n    K_sparse_self = cosine_similarity(X_sparse)\n    assert_array_almost_equal(np.diag(K_sparse_self.toarray()), np.ones(X_sparse.shape[0]))\n\n    # Ensure the similarity is symmetric\n    assert_array_almost_equal(K_sparse_self.toarray(), K_sparse_self.toarray().T)\n", "def test_rbf_kernel_gamma():\n    # Test the RBF kernel with different gamma values\n\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((2, 4))\n\n    # Default gamma = 1.0 / n_features\n    K1 = rbf_kernel(X, Y)\n    gamma_default = 1.0 / X.shape[1]\n    K2 = np.exp(-gamma_default * euclidean_distances(X, Y, squared=True))\n    assert_array_almost_equal(K1, K2)\n\n    # Specific gamma value\n    gamma = 0.5\n    K1 = rbf_kernel(X, Y, gamma=gamma)\n    K2 = np.exp(-gamma * euclidean_distances(X, Y, squared=True))\n    assert_array_almost_equal(K1, K2)\n\n    # Test with X = Y\n    K1 = rbf_kernel(X, gamma=gamma)\n    K2 = np.exp(-gamma * euclidean_distances(X, squared=True))\n    assert_array_almost_equal(K1, K2)\n", "def test_invalid_precomputed_distance_shape():\n    # Test that precomputed metric raises an error for invalid shape\n    X = np.random.random((5, 4))\n    Y = np.random.random((4, 4))  # Invalid shape for precomputed distances\n    with pytest.raises(ValueError, match=\"shape\"):\n        pairwise_distances(X, Y, metric=\"precomputed\")\n\n    Y = np.random.random((5, 5))  # Invalid shape for precomputed distances\n    with pytest.raises(ValueError, match=\"shape\"):\n        pairwise_distances(X, Y, metric=\"precomputed\")\n\n    Y = np.random.random((4, 5))  # Valid shape for precomputed distances\n    pairwise_distances(X, Y, metric=\"precomputed\")\n"], "sample_812": ["def test_custom_estimator_with_non_default_params():\n    # Test custom estimator with non-default parameter values\n    class CustomEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n            self.param3 = param3\n\n    estimator = CustomEstimator(param1=10, param2=\"non-default\")\n    expected = \"\"\"CustomEstimator(param1=10, param2='non-default', param3=None)\"\"\"\n    assert estimator.__repr__() == expected\n\n    # Test with changed_only=True\n    set_config(print_changed_only=True)\n    estimator = CustomEstimator(param1=10)\n    expected = \"\"\"CustomEstimator(param1=10)\"\"\"\n    assert estimator.__repr__() == expected\n\n    estimator = CustomEstimator(param2=\"non-default\")\n    expected = \"\"\"CustomEstimator(param2='non-default')\"\"\"\n    assert estimator.__repr__() == expected\n\n    estimator = CustomEstimator(param3=\"not None\")\n    expected = \"\"\"CustomEstimator(param3='not None')\"\"\"\n    assert estimator.__repr__() == expected\n    set_config(print_changed_only=False)\n", "def test_custom_key_val_tuples():\n    # Testing custom KeyValTuple and KeyValTupleParam classes\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n\n    # Test with KeyValTuple\n    custom_dict = KeyValTuple((('key1', 'value1'), ('key2', 'value2')))\n    expected = \"\"\"", "def test_compact_dict():\n    # Test that dict items are correctly compacted with compact=True\n    class CustomEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n            self.param3 = param3\n            self.param4 = param4\n            self.param5 = param5\n\n    estimator = CustomEstimator()\n    pp = _EstimatorPrettyPrinter(compact=True)\n    expected = \"\"\"", "def test_custom_estimator():\n    # Custom estimator to test pretty-printing\n    class CustomEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n            self.param3 = param3\n\n    set_config(print_changed_only=True)\n\n    estimator = CustomEstimator(param1='changed_value')\n    expected = \"CustomEstimator(param1='changed_value')\"\n    assert estimator.__repr__() == expected\n\n    set_config(print_changed_only=False)\n\n    expected = \"\"\"", "def test_custom_key_val_tuple():\n    # Test custom KeyValTuple and KeyValTupleParam handling\n    vectorizer = CountVectorizer(vocabulary={i: i for i in range(5)}, tokenizer=lambda x: x.split())\n    expected = r\"\"\""], "sample_813": ["def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with and without fit_intercept parameter\n    X = np.array([[0], [1], [2]])\n    y = np.array([1, 2, 3])\n\n    # Case with fit_intercept=True\n    clf = BayesianRidge(fit_intercept=True)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_almost_equal(clf.intercept_, 1.0)\n    assert_array_almost_equal(y_pred, y, decimal=6)\n\n    # Case with fit_intercept=False\n    clf = BayesianRidge(fit_intercept=False)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_almost_equal(clf.intercept_, 0.0)\n    assert_array_almost_equal(y_pred, np.array([1, 2, 3]), decimal=6)\n", "def test_ard_regression_threshold_lambda():\n    # Test pruning of weights with high precision in ARDRegression\n    X = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 0, 1]])\n    y = np.array([1, 2, 3, 4, 5])\n\n    clf = ARDRegression(threshold_lambda=1.e+2)\n    clf.fit(X, y)\n\n    # Ensure that weights with high precision are pruned\n    assert np.count_nonzero(clf.coef_) < X.shape[1]\n    # Ensure that the pruned weights are set to zero\n    assert_array_almost_equal(clf.coef_[clf.lambda_ >= clf.threshold_lambda], 0)\n", "def test_ard_threshold_lambda():\n    # Test ARDRegression with varying threshold_lambda values\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n\n    # Test with a very high threshold, effectively no pruning\n    clf_high = ARDRegression(threshold_lambda=1e+10, compute_score=True)\n    clf_high.fit(X, y)\n    high_coef = clf_high.coef_.copy()\n\n    # Test with a very low threshold, effective pruning\n    clf_low = ARDRegression(threshold_lambda=1e-10, compute_score=True)\n    clf_low.fit(X, y)\n    low_coef = clf_low.coef_.copy()\n\n    # Coefficients with high threshold should be non-zero\n    assert np.any(high_coef != 0), \"Coefficients should not be zero with high threshold_lambda\"\n\n    # Coefficients with low threshold should be pruned to zero\n    assert np.all(low_coef == 0), \"Coefficients should be zero with low threshold_lambda\"\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with fit_intercept=False\n    X = np.array([[1], [2], [6], [8], [10]])\n    y = np.array([1, 2, 6, 8, 10])\n    \n    # Without intercept\n    clf_no_intercept = BayesianRidge(fit_intercept=False)\n    clf_no_intercept.fit(X, y)\n    assert_almost_equal(clf_no_intercept.intercept_, 0)\n    \n    # With intercept\n    clf_with_intercept = BayesianRidge(fit_intercept=True)\n    clf_with_intercept.fit(X, y)\n    assert clf_with_intercept.intercept_ != 0\n\n    # Check that the predictions are the same when the data is centered\n    X_centered = X - X.mean(axis=0)\n    y_centered = y - y.mean()\n    clf_centered = BayesianRidge(fit_intercept=False)\n    clf_centered.fit(X_centered, y_centered)\n    assert_array_almost_equal(clf_with_intercept.predict(X), clf_centered.predict(X_centered))\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with and without intercept\n    X = np.array([[1], [2], [6], [8], [10]])\n    y = np.array([1, 2, 6, 8, 10])\n\n    # Test with intercept\n    clf_with_intercept = BayesianRidge(fit_intercept=True)\n    clf_with_intercept.fit(X, y)\n    y_pred_with_intercept = clf_with_intercept.predict(X)\n    \n    # Test without intercept\n    clf_without_intercept = BayesianRidge(fit_intercept=False)\n    clf_without_intercept.fit(X, y)\n    y_pred_without_intercept = clf_without_intercept.predict(X)\n\n    # Predictions should be the same if the data is centered\n    X_centered = X - np.mean(X, axis=0)\n    y_centered = y - np.mean(y)\n    clf_without_intercept_centered = BayesianRidge(fit_intercept=False)\n    clf_without_intercept_centered.fit(X_centered, y_centered)\n    y_pred_without_intercept_centered = clf_without_intercept_centered.predict(X_centered)\n    \n    assert_array_almost_equal(y_pred_without_intercept, y_pred_without_intercept_centered)\n"], "sample_814": ["def test_mean_estimator_predict():\n    # Test the MeanEstimator's predict method.\n\n    est = MeanEstimator()\n    X_train = np.array([[1], [2], [3]])\n    y_train = np.array([10, 20, 30])\n    est.fit(X_train, y_train)\n\n    X_test = np.array([[4], [5]])\n    y_pred = est.predict(X_test)\n\n    # Since the mean of y_train is 20, it should predict 20 for all inputs\n    expected_pred = np.array([20, 20], dtype=np.float64)\n    assert_array_almost_equal(y_pred.ravel(), expected_pred)\n", "def test_loss_functions_deprecated_warning():\n    # Check that deprecated loss functions raise a warning\n    deprecated_losses = [\n        ('ls', 'MeanEstimator'),\n        ('lad', 'QuantileEstimator'),\n        ('huber', 'QuantileEstimator'),\n        ('quantile', 'QuantileEstimator'),\n        ('deviance', 'LogOddsEstimator'),\n        ('exponential', 'ScaledLogOddsEstimator')\n    ]\n    \n    for loss, estimator in deprecated_losses:\n        if loss in ['ls', 'lad', 'huber', 'quantile']:\n            clf = GradientBoostingRegressor(loss=loss)\n        else:\n            clf = GradientBoostingClassifier(loss=loss)\n        \n        with pytest.warns(DeprecationWarning, match=f\"{estimator} is deprecated\"):\n            clf.fit(X, y)\n", "def test_mean_estimator():\n    # Test MeanEstimator\n    X = np.array([[1], [2], [3]])\n    y = np.array([4, 5, 6])\n    mean_est = MeanEstimator()\n    mean_est.fit(X, y)\n    predictions = mean_est.predict(X)\n    assert_array_almost_equal(predictions, np.full(X.shape, 5.0))\n", "def test_quantile_estimator():\n    # Test QuantileEstimator\n    X = np.array([[1], [2], [3], [4], [5]])\n    y = np.array([1, 2, 3, 4, 5])\n    quantile_estimator = QuantileEstimator(alpha=0.5)\n    quantile_estimator.fit(X, y)\n    y_pred = quantile_estimator.predict(X)\n    assert_array_equal(y_pred, np.array([3.0, 3.0, 3.0, 3.0, 3.0]))\n\n    # Test with sample weights\n    sample_weight = np.array([1, 1, 1, 1, 1])\n    quantile_estimator.fit(X, y, sample_weight=sample_weight)\n    y_pred = quantile_estimator.predict(X)\n    assert_array_equal(y_pred, np.array([3.0, 3.0, 3.0, 3.0, 3.0]))\n\n    # Test invalid alpha\n    with pytest.raises(ValueError):\n        quantile_estimator = QuantileEstimator(alpha=1.5)\n", "def test_quantile_estimator():\n    # Test the deprecated QuantileEstimator\n    X = np.array([[1], [2], [3], [4], [5]])\n    y = np.array([1, 2, 3, 4, 5])\n    est = QuantileEstimator(alpha=0.5)\n    est.fit(X, y)\n    preds = est.predict(X)\n    assert_array_almost_equal(preds, np.full(X.shape, 3.0))\n    assert_raises(ValueError, QuantileEstimator, alpha=-0.5)\n    assert_raises(ValueError, QuantileEstimator, alpha=1.5)\n"], "sample_815": ["def test_weighted_sum_normalization():\n    # Test _weighted_sum with normalization\n    sample_scores = np.array([0.8, 0.6, 0.7])\n    sample_weights = np.array([1, 2, 1])\n    normalized_sum = _weighted_sum(sample_scores, sample_weights, normalize=True)\n    expected_normalized_sum = np.average(sample_scores, weights=sample_weights)\n    assert_almost_equal(normalized_sum, expected_normalized_sum)\n\n    # Test _weighted_sum without normalization\n    non_normalized_sum = _weighted_sum(sample_scores, sample_weights, normalize=False)\n    expected_non_normalized_sum = np.dot(sample_scores, sample_weights)\n    assert_almost_equal(non_normalized_sum, expected_non_normalized_sum)\n\n    # Test _weighted_sum with no weights\n    sample_scores_no_weights = np.array([0.8, 0.6, 0.7])\n    sum_no_weights = _weighted_sum(sample_scores_no_weights, None, normalize=False)\n    expected_sum_no_weights = sample_scores_no_weights.sum()\n    assert_almost_equal(sum_no_weights, expected_sum_no_weights)\n", "def test_multilabel_confusion_matrix_sample_weight():\n    # Test multilabel confusion matrix with sample weights\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]])\n    sample_weight = np.array([2, 3, 1])\n\n    # Compute confusion matrix without sample weights\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    expected_cm = np.array([[[1, 0], [1, 1]], [[1, 0], [1, 1]], [[0, 2], [1, 0]]])\n    assert_array_equal(cm, expected_cm)\n\n    # Compute confusion matrix with sample weights\n    cm_weighted = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    expected_cm_weighted = np.array([[[2, 0], [1, 3]], [[3, 0], [1, 2]], [[0, 5], [1, 0]]])\n    assert_array_equal(cm_weighted, expected_cm_weighted)\n\n    # Test with samplewise=True and sample weights\n    cm_samplewise = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight, samplewise=True)\n    expected_cm_samplewise = np.array([[[2, 0], [1, 1]], [[3, 0], [0, 1]], [[0, 3], [1, 0]]])\n    assert_array_equal(cm_samplewise, expected_cm_samplewise)\n\n    # Test with specified labels and sample weights\n    cm_labels = multilabel_confusion_matrix(y_true, y_pred, labels=[2, 0], sample_weight=sample_weight)\n    expected_cm_labels = np.array([[[0, 5], [1, 0]], [[2, 0], [1, 3]]])\n    assert_array_equal(cm_labels, expected_cm_labels)\n", "def test_multilabel_confusion_matrix_with_sample_weight():\n    # Test multilabel confusion matrix with sample weights\n    from scipy.sparse import csc_matrix, csr_matrix\n\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]])\n    sample_weight = np.array([2, 1, 3])\n\n    real_cm = [[[2, 0], [3, 1]],\n               [[1, 0], [1, 1]],\n               [[0, 2], [1, 0]]]\n\n    cm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    assert_array_equal(cm, real_cm)\n\n    # test with different sparse matrix formats\n    y_true_csr = csr_matrix(y_true)\n    y_pred_csr = csr_matrix(y_pred)\n    y_true_csc = csc_matrix(y_true)\n    y_pred_csc = csc_matrix(y_pred)\n\n    cm = multilabel_confusion_matrix(y_true_csr, y_pred_csr, sample_weight=sample_weight)\n    assert_array_equal(cm, real_cm)\n\n    cm = multilabel_confusion_matrix(y_true_csc, y_pred_csc, sample_weight=sample_weight)\n    assert_array_equal(cm, real_cm)\n\n    # test support for labels with sample_weight\n    cm = multilabel_confusion_matrix(y_true, y_pred, labels=[2, 0], sample_weight=sample_weight)\n    assert_array_equal(cm, [[[0, 2], [1, 0]],\n                            [[2, 0], [3, 1]]])\n\n    # test support for sample_weight with samplewise\n    cm = multilabel_confusion_matrix(y_true, y_pred,\n                                     sample_weight=sample_weight,\n                                     samplewise=True)\n    assert_array_equal(cm, [[[2, 0], [3, 1]],\n                            [[1, 1], [0, 1]],\n                            [[0, 3], [6, 0]]])\n", "def test_zero_one_loss_multiclass():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n\n    # Test without normalization (absolute number of misclassifications)\n    loss = zero_one_loss(y_true, y_pred, normalize=False)\n    assert_equal(loss, 4)\n\n    # Test with normalization (fraction of misclassifications)\n    loss = zero_one_loss(y_true, y_pred, normalize=True)\n    assert_almost_equal(loss, 4 / 6)\n\n    # Test with sample weights\n    sample_weight = [1, 1, 1, 1, 1, 1]\n    loss = zero_one_loss(y_true, y_pred, normalize=True, sample_weight=sample_weight)\n    assert_almost_equal(loss, 4 / 6)\n\n    # Test with different sample weights\n    sample_weight = [1, 2, 1, 1, 2, 1]\n    loss = zero_one_loss(y_true, y_pred, normalize=True, sample_weight=sample_weight)\n    assert_almost_equal(loss, 6 / 8)\n", "def test_multiclass_log_loss_with_missing_labels():\n    # Test log_loss function with multiclass classification and missing labels\n    y_true = [1, 1, 2, 2]\n    y_pred = np.array([[0.2, 0.7, 0.1], [0.1, 0.6, 0.3], [0.5, 0.3, 0.2], [0.3, 0.3, 0.4]])\n    \n    # Without specifying labels, it should raise a ValueError\n    assert_raises(ValueError, log_loss, y_true, y_pred)\n    \n    # Specify the labels to avoid the error\n    labels = [0, 1, 2]\n    calculated_log_loss = log_loss(y_true, y_pred, labels=labels)\n    \n    # Calculate the expected log loss manually\n    y_true_bin = label_binarize(y_true, classes=labels)\n    y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    log_loss_per_sample = -np.sum(y_true_bin * np.log(y_pred_clipped), axis=1)\n    expected_log_loss = np.mean(log_loss_per_sample)\n    \n    assert_almost_equal(calculated_log_loss, expected_log_loss, decimal=6)\n"], "sample_816": ["def test_vectorizer_with_non_ascii_stop_words():\n    # Test CountVectorizer with non-ASCII stop words\n    stop_words = ['\u00fcber', 'fa\u00e7ade', 'ni\u00f1o']\n    vect = CountVectorizer(stop_words=stop_words)\n    data = [\"\u00dcber the fa\u00e7ade of the building, a ni\u00f1o was playing\"]\n    X = vect.fit_transform(data)\n    feature_names = vect.get_feature_names_out()\n    # Check that non-ASCII stop words are removed\n    assert '\u00fcber' not in feature_names\n    assert 'fa\u00e7ade' not in feature_names\n    assert 'ni\u00f1o' not in feature_names\n    # Check that other words are present\n    assert 'building' in feature_names\n    assert 'playing' in feature_names\n\n    # Test TfidfVectorizer with non-ASCII stop words\n    vect = TfidfVectorizer(stop_words=stop_words)\n    X = vect.fit_transform(data)\n    feature_names = vect.get_feature_names_out()\n    # Check that non-ASCII stop words are removed\n    assert '\u00fcber' not in feature_names\n    assert 'fa\u00e7ade' not in feature_names\n    assert 'ni\u00f1o' not in feature_names\n    # Check that other words are present\n    assert 'building' in feature_names\n    assert 'playing' in feature_names\n", "def test_vectorizer_with_none_preprocessor_and_tokenizer():\n    # Test that the vectorizer works correctly when preprocessor and tokenizer are None\n    vectorizer = CountVectorizer(preprocessor=None, tokenizer=None)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    feature_names = vectorizer.get_feature_names()\n    \n    expected_feature_names = ['beer', 'burger', 'coke', 'copyright', 'pizza']\n    assert_equal(set(feature_names), set(expected_feature_names))\n\n    # Check the transformed data\n    X_expected = np.array([\n        [1, 0, 0, 1, 2],\n        [1, 1, 0, 1, 1],\n        [2, 0, 0, 1, 1],\n        [1, 1, 0, 1, 0],\n        [0, 1, 2, 1, 0],\n        [0, 2, 1, 0, 0]\n    ])\n    assert_array_equal(X.toarray(), X_expected)\n", "def test_tfidf_transformer_fit_transform():\n    # Ensure TfidfTransformer.fit_transform works correctly\n    X = np.array([[3, 0, 1], [2, 0, 0], [3, 0, 0], [4, 0, 0], [3, 2, 0], [3, 0, 2]])\n    transformer = TfidfTransformer()\n    X_tfidf = transformer.fit_transform(X).toarray()\n\n    expected_tfidf = [\n        [0.70710678, 0., 0.70710678],\n        [1., 0., 0.],\n        [1., 0., 0.],\n        [1., 0., 0.],\n        [0.83205029, 0.5547002, 0.],\n        [0.70710678, 0., 0.70710678]\n    ]\n    assert_array_almost_equal(X_tfidf, expected_tfidf)\n", "def test_vectorizer_empty_input():\n    # Test behavior on empty input\n    vectorizers = [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]\n\n    for vec in vectorizers:\n        # Empty list\n        empty_list = []\n        X = vec.fit_transform(empty_list)\n        assert_equal(X.shape[0], 0)\n        \n        # Empty string\n        empty_string = [\"\"]\n        X = vec.fit_transform(empty_string)\n        assert_equal(X.shape[0], 1)\n\n        # Ensure the vocabulary is empty or contains no meaningful entries\n        if hasattr(vec, 'vocabulary_'):\n            assert_equal(len(vec.vocabulary_), 0)\n", "def test_strip_tags():\n    # HTML tags\n    html = \"<html><body><h1>Title</h1><p>This is a <b>bold</b> paragraph.</p></body></html>\"\n    expected = \"Title This is a bold paragraph.\"\n    assert_equal(strip_tags(html), expected)\n\n    # Nested HTML tags\n    html = \"<div><p>Some <span>nested <em>tags</em></span> in a paragraph.</p></div>\"\n    expected = \"Some nested tags in a paragraph.\"\n    assert_equal(strip_tags(html), expected)\n\n    # HTML tags with attributes\n    html = '<a href=\"https://example.com\">link</a>'\n    expected = \"link\"\n    assert_equal(strip_tags(html), expected)\n\n    # Self-closing HTML tags\n    html = \"Line break here:<br/>And here\"\n    expected = \"Line break here: And here\"\n    assert_equal(strip_tags(html), expected)\n\n    # XML tags\n    xml = \"<note><to>User</to><from>Admin</from><message>Hello</message></note>\"\n    expected = \"User Admin Hello\"\n    assert_equal(strip_tags(xml), expected)\n"], "sample_817": ["def test_all_features_removed():\n    # Test that VarianceThreshold raises an error when all features are removed.\n    data = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]\n    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n        msg = \"No feature in X meets the variance threshold 0.00000\"\n        with pytest.raises(ValueError, match=msg):\n            VarianceThreshold().fit(X)\n\n    data = [[1, 1], [1, 1], [1, 1], [1, 2]]\n    for X in [data, csr_matrix(data)]:\n        sel = VarianceThreshold(threshold=0.1)\n        X_transformed = sel.fit_transform(X)\n        assert_equal(X_transformed.shape[1], 1)\n", "def test_variance_threshold_edge_case():\n    # Test VarianceThreshold on edge cases, such as an empty dataset or a dataset with a single sample.\n    \n    # Empty dataset\n    X_empty = np.empty((0, 0))\n    with pytest.raises(ValueError, match=\"No feature in X meets the variance threshold 0.00000\"):\n        VarianceThreshold().fit(X_empty)\n    \n    # Single sample dataset\n    X_single_sample = np.array([[1, 2, 3, 4, 5]])\n    with pytest.raises(ValueError, match=\"No feature in X meets the variance threshold 0.00000 \\(X contains only one sample\\)\"):\n        VarianceThreshold().fit(X_single_sample)\n", "def test_negative_threshold():\n    # Test VarianceThreshold with negative threshold.\n    sel = VarianceThreshold(threshold=-1.0).fit(data)\n    assert_array_equal([0, 1, 2, 3, 4], sel.get_support(indices=True))\n\n    sparse_data = csr_matrix(data)\n    sel_sparse = VarianceThreshold(threshold=-1.0).fit(sparse_data)\n    assert_array_equal([0, 1, 2, 3, 4], sel_sparse.get_support(indices=True))\n", "def test_fit_transform():\n    # Test the fit_transform method directly\n    X = np.array(data)\n    sel = VarianceThreshold()\n    X_transformed = sel.fit_transform(X)\n    expected_result = np.array([[1, 3, 4], [2, 3, 5], [1, 4, 0]])\n    assert_array_equal(X_transformed, expected_result)\n", "def test_high_variance_threshold():\n    # Test VarianceThreshold with a high variance threshold that removes all features.\n    high_variance_data = [[0, 1, 2, 3, 4],\n                          [0, 2, 2, 3, 5],\n                          [1, 1, 2, 4, 0]]\n\n    threshold = 10.0  # Set a very high threshold to remove all features\n\n    for X in [high_variance_data, csr_matrix(high_variance_data), csc_matrix(high_variance_data), bsr_matrix(high_variance_data)]:\n        sel = VarianceThreshold(threshold=threshold)\n        with pytest.raises(ValueError, match=f\"No feature in X meets the variance threshold {threshold:.5f}\"):\n            sel.fit(X)\n"], "sample_818": ["def test_spectral_clustering_with_precomputed_affinity():\n    # Test spectral clustering with precomputed affinity matrix\n    X, y = make_blobs(n_samples=100, centers=3, cluster_std=0.5, random_state=42)\n    affinity_matrix = rbf_kernel(X, gamma=1.0)\n    \n    model = SpectralClustering(n_clusters=3, affinity='precomputed', random_state=42)\n    labels = model.fit(affinity_matrix).labels_\n    \n    assert adjusted_rand_score(y, labels) > 0.8\n", "def test_spectral_clustering_with_precomputed_affinity():\n    # Test spectral clustering with precomputed affinity matrix\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    \n    # Use RBF kernel to create a precomputed affinity matrix\n    affinity_matrix = rbf_kernel(X, gamma=1)\n    \n    # Fit the model with precomputed affinity matrix\n    sp = SpectralClustering(n_clusters=2, affinity='precomputed', random_state=0)\n    labels = sp.fit(affinity_matrix).labels_\n    \n    assert adjusted_rand_score(y, labels) == 1\n\n    # Ensure the affinity matrix inside the model is the same as the precomputed one\n    assert_array_equal(sp.affinity_matrix_, affinity_matrix)\n\n    # Test that fitting the model raises a warning if affinity matrix is not fully connected\n    affinity_matrix[0, 1] = 0  # Make the affinity matrix not fully connected\n    sp = SpectralClustering(n_clusters=2, affinity='precomputed', random_state=0)\n    assert_warns_message(UserWarning, 'not fully connected', sp.fit, affinity_matrix)\n", "def test_spectral_clustering_precomputed_affinity():\n    # Test spectral clustering with precomputed affinity matrix\n    X, y = make_blobs(n_samples=30, centers=[[2, 2], [-2, -2], [2, -2]], cluster_std=0.5, random_state=42)\n    affinity_matrix = rbf_kernel(X, gamma=0.1)\n    \n    model = SpectralClustering(n_clusters=3, affinity='precomputed', random_state=42).fit(affinity_matrix)\n    labels = model.labels_\n    assert len(np.unique(labels)) == 3\n    assert adjusted_rand_score(y, labels) > 0.8  # Verify clustering quality\n", "def test_spectral_clustering_precomputed_affinity():\n    # Test spectral clustering with a precomputed affinity matrix\n    X, y = make_blobs(n_samples=20, random_state=0, centers=[[1, 1], [-1, -1]], cluster_std=0.1)\n    S = rbf_kernel(X, gamma=1)\n    \n    model = SpectralClustering(n_clusters=2, affinity='precomputed', random_state=0).fit(S)\n    labels = model.labels_\n    \n    # Ensure the labels are correctly assigned\n    assert adjusted_rand_score(y, labels) == 1\n\n    # Test if the same labels are produced with a sparse affinity matrix\n    S_sparse = sparse.csr_matrix(S)\n    model_sparse = SpectralClustering(n_clusters=2, affinity='precomputed', random_state=0).fit(S_sparse)\n    labels_sparse = model_sparse.labels_\n    \n    # Ensure the sparse matrix produces the same labels\n    assert_array_equal(labels, labels_sparse)\n", "def test_spectral_clustering_with_precomputed_affinity():\n    # Test spectral clustering with a precomputed affinity matrix\n    X, y = make_blobs(n_samples=50, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    \n    # Compute the RBF (Gaussian) affinity matrix\n    affinity_matrix = rbf_kernel(X, gamma=1.0)\n    \n    # Use the precomputed affinity matrix for spectral clustering\n    model = SpectralClustering(n_clusters=2, affinity='precomputed', random_state=0)\n    labels = model.fit(affinity_matrix).labels_\n    \n    # Assert that the adjusted rand index is 1, indicating perfect clustering\n    assert adjusted_rand_score(y, labels) == 1\n\n    # Test that the affinity matrix stored in the model is the same as the input\n    assert_array_equal(model.affinity_matrix_, affinity_matrix)\n"], "sample_819": ["def test_voting_regressor_transform():\n    \"\"\"Check transform method of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16]])\n    y = np.array([2, 6, 12, 20])\n\n    ereg1 = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)]).fit(X, y)\n    ereg2 = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)], weights=[0.5, 0.5]).fit(X, y)\n\n    transform_res1 = ereg1.transform(X)\n    transform_res2 = ereg2.transform(X)\n\n    assert transform_res1.shape == (4, 2)\n    assert transform_res2.shape == (4, 2)\n\n    avg_predictions_ereg1 = np.average(transform_res1, axis=1)\n    avg_predictions_ereg2 = np.average(transform_res2, axis=1)\n\n    assert_array_almost_equal(ereg1.predict(X), avg_predictions_ereg1, decimal=2)\n    assert_array_almost_equal(ereg2.predict(X), avg_predictions_ereg2, decimal=2)\n", "def test_voting_regressor_transform():\n    \"\"\"Check transform method of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    X = np.array([[1, 2], [2, 4], [3, 9], [4, 16], [5, 25]])\n    y = np.array([3, 5, 7, 9, 11])\n\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2)])\n    ereg.fit(X, y)\n\n    # Testing transform method on the regressor\n    transformed_predictions = ereg.transform(X)\n    assert transformed_predictions.shape == (5, 2)\n\n    reg1_predictions = reg1.fit(X, y).predict(X)\n    reg2_predictions = reg2.fit(X, y).predict(X)\n\n    assert_array_almost_equal(transformed_predictions[:, 0], reg1_predictions)\n    assert_array_almost_equal(transformed_predictions[:, 1], reg2_predictions)\n", "def test_voting_regressor_predict():\n    \"\"\"Test VotingRegressor's predict method on a toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    reg3 = DummyRegressor(strategy='median')\n\n    X = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n    y = np.array([2, 3, 5, 7])\n\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dr', reg3)],\n        weights=[1, 2, 1])\n    ereg.fit(X, y)\n    y_pred = ereg.predict(X)\n\n    reg1_pred = reg1.fit(X, y).predict(X)\n    reg2_pred = reg2.fit(X, y).predict(X)\n    reg3_pred = reg3.fit(X, y).predict(X)\n\n    avg_pred = np.average(np.vstack([reg1_pred, reg2_pred, reg3_pred]), axis=0, weights=[1, 2, 1])\n    assert_array_almost_equal(y_pred, avg_pred, decimal=2)\n", "def test_voting_regressor_single_estimator():\n    \"\"\"Check VotingRegressor with only one estimator\"\"\"\n    reg1 = LinearRegression()\n    ereg = VotingRegressor([('lr', reg1)])\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n    ereg.fit(X, y)\n    assert_array_almost_equal(ereg.predict(X), reg1.fit(X, y).predict(X))\n\n    # Check weights\n    ereg = VotingRegressor([('lr', reg1)], weights=[1])\n    assert_array_almost_equal(ereg.fit(X, y).predict(X), reg1.predict(X))\n", "def test_voting_regressor_predict():\n    \"\"\"Check prediction of VotingRegressor on toy dataset.\"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([1, 3, 5, 7, 9])\n    \n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123, n_estimators=5)\n    reg3 = DummyRegressor(strategy='mean')\n\n    ereg1 = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dummy', reg3)], weights=[1, 1, 1])\n    \n    ereg1.fit(X, y)\n    pred1 = ereg1.predict(X)\n\n    # Manually compute expected prediction\n    pred_lr = reg1.fit(X, y).predict(X)\n    pred_rf = reg2.fit(X, y).predict(X)\n    pred_dummy = reg3.fit(X, y).predict(X)\n\n    expected_pred = np.mean([pred_lr, pred_rf, pred_dummy], axis=0)\n\n    assert_array_almost_equal(pred1, expected_pred, decimal=2)\n\n    # Test with different weights\n    ereg2 = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dummy', reg3)], weights=[2, 1, 1])\n    \n    ereg2.fit(X, y)\n    pred2 = ereg2.predict(X)\n\n    expected_pred_weighted = np.average([pred_lr, pred_rf, pred_dummy], axis=0, weights=[2, 1, 1])\n    \n    assert_array_almost_equal(pred2, expected_pred_weighted, decimal=2)\n"], "sample_820": ["def test_voting_regressor_predict():\n    \"\"\"Check the predictions of the VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    reg3 = DummyRegressor(strategy='mean')\n    \n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([3, 7, 11, 15])\n    \n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dummy', reg3)],\n        weights=[2, 1, 0.5])\n    \n    ereg.fit(X, y)\n    \n    # Check if the VotingRegressor makes correct predictions\n    predictions = ereg.predict(X)\n    expected_predictions = np.average(\n        np.array([reg1.fit(X, y).predict(X), \n                  reg2.fit(X, y).predict(X),\n                  reg3.fit(X, y).predict(X)]),\n        axis=0, weights=[2, 1, 0.5])\n    \n    assert_array_almost_equal(predictions, expected_predictions, decimal=1)\n", "def test_voting_regressor_predict():\n    \"\"\"Test VotingRegressor prediction and averaging mechanism.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    reg3 = DummyRegressor(strategy=\"mean\")\n\n    X_r = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y_r = np.array([2, 6, 12, 20, 30, 42])\n\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dummy', reg3)], weights=[1, 2, 0.5])\n\n    ereg.fit(X_r, y_r)\n    y_pred = ereg.predict(X_r)\n\n    # Check if the prediction is a weighted average of individual predictions\n    reg1_pred = reg1.predict(X_r)\n    reg2_pred = reg2.predict(X_r)\n    reg3_pred = reg3.predict(X_r)\n    expected_pred = np.average([reg1_pred, reg2_pred, reg3_pred], axis=0, weights=[1, 2, 0.5])\n\n    assert_almost_equal(y_pred, expected_pred, decimal=2)\n", "def test_voting_regressor_predict():\n    \"\"\"Check predict method of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    X_toy = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n    y_toy = np.array([2, 3, 4, 5, 6, 7])\n\n    ereg1 = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)], weights=[1, 1])\n    ereg1.fit(X_toy, y_toy)\n    y_pred1 = ereg1.predict(X_toy)\n\n    # Manually compute the expected prediction\n    reg1_pred = reg1.fit(X_toy, y_toy).predict(X_toy)\n    reg2_pred = reg2.fit(X_toy, y_toy).predict(X_toy)\n    expected_pred = np.average(np.array([reg1_pred, reg2_pred]), axis=0, weights=[1, 1])\n\n    assert_array_almost_equal(y_pred1, expected_pred, decimal=2)\n\n    # Test with different weights\n    ereg2 = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)], weights=[0.2, 0.8])\n    ereg2.fit(X_toy, y_toy)\n    y_pred2 = ereg2.predict(X_toy)\n\n    expected_pred_weighted = np.average(np.array([reg1_pred, reg2_pred]), axis=0, weights=[0.2, 0.8])\n    assert_array_almost_equal(y_pred2, expected_pred_weighted, decimal=2)\n", "def test_voting_regressor_average():\n    \"\"\"Check VotingRegressor average prediction on a toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n\n    reg1_pred = reg1.fit(X, y).predict(X)\n    reg2_pred = reg2.fit(X, y).predict(X)\n    \n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2)])\n    ereg_pred = ereg.fit(X, y).predict(X)\n\n    avg = np.mean(np.vstack((reg1_pred, reg2_pred)), axis=0)\n    assert_almost_equal(ereg_pred, avg, decimal=2)\n\n    # Test with weights\n    weights = [0.7, 0.3]\n    ereg_weights = VotingRegressor([('lr', reg1), ('rf', reg2)], weights=weights)\n    ereg_weights_pred = ereg_weights.fit(X, y).predict(X)\n\n    weighted_avg = np.average(np.vstack((reg1_pred, reg2_pred)), axis=0, weights=weights)\n    assert_almost_equal(ereg_weights_pred, weighted_avg, decimal=2)\n", "def test_get_params():\n    \"\"\"Test get_params method of VotingClassifier and VotingRegressor\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n\n    # Check that the parameters of the VotingClassifier are returned correctly\n    params = eclf.get_params()\n    assert 'lr__C' in params\n    assert 'rf__n_estimators' in params\n    assert 'gnb__priors' in params\n    assert 'voting' in params\n\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    ereg = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)])\n\n    # Check that the parameters of the VotingRegressor are returned correctly\n    params = ereg.get_params()\n    assert 'lr__fit_intercept' in params\n    assert 'rf__n_estimators' in params\n    assert 'n_jobs' in params\n    assert 'weights' in params\n"], "sample_821": ["def test_affinity_propagation_damping():\n    # Test AffinityPropagation with different damping factors\n    for damping in [0.5, 0.6, 0.7, 0.8, 0.9]:\n        af = AffinityPropagation(damping=damping, preference=np.median(-euclidean_distances(X, squared=True)))\n        labels = af.fit_predict(X)\n        assert len(np.unique(labels)) <= n_clusters  # Should not create more clusters than actual\n        assert af.damping == damping  # Ensure the damping value is correctly set\n        assert af.n_iter_ <= af.max_iter  # Ensure it does not exceed max iterations\n", "def test_affinity_propagation_single_sample():\n    # Test AffinityPropagation with a single sample\n    X = np.array([[1, 2]])\n    S = -euclidean_distances(X, squared=True)\n\n    # Compute Affinity Propagation\n    cluster_centers_indices, labels = affinity_propagation(S)\n\n    assert_equal(len(cluster_centers_indices), 1)\n    assert_array_equal(cluster_centers_indices, [0])\n    assert_array_equal(labels, [0])\n\n    af = AffinityPropagation()\n    labels = af.fit(X).labels_\n\n    assert_equal(len(af.cluster_centers_indices_), 1)\n    assert_array_equal(af.cluster_centers_indices_, [0])\n    assert_array_equal(labels, [0])\n\n    # Test input validation for single sample\n    assert_raises(ValueError, affinity_propagation, np.array([[1, 2]]))\n", "def test_affinity_propagation_damping_bounds():\n    # Test that damping factor out of bounds raises ValueError\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S)\n    \n    # Damping factor too low\n    with pytest.raises(ValueError, match='damping must be >= 0.5 and < 1'):\n        affinity_propagation(S, preference=preference, damping=0.4)\n    \n    # Damping factor too high\n    with pytest.raises(ValueError, match='damping must be >= 0.5 and < 1'):\n        affinity_propagation(S, preference=preference, damping=1)\n", "def test_affinity_propagation_damping():\n    # Test different damping values\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n\n    # Test valid damping values\n    for damping in [0.5, 0.6, 0.7, 0.8, 0.9]:\n        cluster_centers_indices, labels = affinity_propagation(\n            S, preference=preference, damping=damping)\n        n_clusters_ = len(cluster_centers_indices)\n        assert_equal(n_clusters, n_clusters_)\n\n    # Test invalid damping values\n    for damping in [0.4, 1.0]:\n        assert_raises(ValueError, affinity_propagation, S, preference=preference, damping=damping)\n", "def test_affinity_propagation_damping_values():\n    # Test AffinityPropagation with different damping values\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n\n    # Test invalid damping values\n    assert_raises(ValueError, affinity_propagation, S, preference=preference, damping=0.4)\n    assert_raises(ValueError, affinity_propagation, S, preference=preference, damping=1.0)\n\n    # Test valid damping values\n    valid_damping_values = [0.5, 0.6, 0.7, 0.8, 0.9]\n    for damping in valid_damping_values:\n        cluster_centers_indices, labels = affinity_propagation(S, preference=preference, damping=damping)\n        n_clusters_ = len(cluster_centers_indices)\n        assert_equal(n_clusters, n_clusters_)\n"], "sample_822": ["def test_check_pairwise_arrays_invalid_inputs():\n    # Test that check_pairwise_arrays raises appropriate errors for invalid inputs\n    rng = np.random.RandomState(0)\n\n    # Test with 1-dimensional arrays\n    XA = rng.random_sample(5)\n    XB = rng.random_sample(5)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n\n    # Test with arrays of different lengths\n    XA = rng.random_sample((5, 4))\n    XB = rng.random_sample((4, 4))\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n\n    # Test with arrays of different second dimension\n    XB = rng.random_sample((5, 3))\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n\n    # Test with non-numeric data types\n    XA = [['a', 'b'], ['c', 'd']]\n    XB = [['e', 'f'], ['g', 'h']]\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n\n    # Test with unsupported sparse matrix formats\n    XA_sparse = dok_matrix(rng.random_sample((5, 4)))\n    XB_sparse = csr_matrix(rng.random_sample((5, 4)))\n    assert_raises(TypeError, check_pairwise_arrays, XA_sparse, XB_sparse)\n", "def test_pairwise_distances_argmin_min_edge_cases():\n    # Test edge cases for pairwise_distances_argmin_min\n    X = np.array([[0, 1], [1, 1], [2, 2]])\n    Y = np.array([[1, 0], [1, 1], [3, 3]])\n\n    # Case where closest distance is 0\n    idx, vals = pairwise_distances_argmin_min(X, X, metric=\"euclidean\")\n    assert_array_equal(idx, [0, 1, 2])\n    assert_array_equal(vals, [0.0, 0.0, 0.0])\n\n    # Case where closest distance is non-zero\n    idx, vals = pairwise_distances_argmin_min(X, Y, metric=\"euclidean\")\n    assert_array_equal(idx, [0, 1, 2])\n    assert_array_almost_equal(vals, [1.0, 0.0, np.sqrt(2)])\n\n    # Case with empty arrays\n    X_empty = np.empty((0, 2))\n    Y_empty = np.empty((0, 2))\n    idx, vals = pairwise_distances_argmin_min(X_empty, Y, metric=\"euclidean\")\n    assert_array_equal(idx, np.empty((0,), dtype=int))\n    assert_array_equal(vals, np.empty((0,)))\n\n    idx, vals = pairwise_distances_argmin_min(X, Y_empty, metric=\"euclidean\")\n    assert_array_equal(idx, np.empty((0,), dtype=int))\n    assert_array_equal(vals, np.empty((0,)))\n\n    # Case with one element arrays\n    X_one = np.array([[0, 0]])\n    Y_one = np.array([[1, 1]])\n    idx, vals = pairwise_distances_argmin_min(X_one, Y_one, metric=\"euclidean\")\n    assert_array_equal(idx, [0])\n    assert_array_almost_equal(vals, [np.sqrt(2)])\n", "def test_pairwise_distances_chunked_callable_metric():\n    # Test pairwise_distances_chunked with a callable metric\n        return np.sum(np.abs(x - y))\n\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n    Y = rng.random_sample((200, 4))\n\n    # Compute expected result using pairwise_distances\n    expected_result = pairwise_distances(X, Y, metric=custom_metric)\n\n    # Compute result using pairwise_distances_chunked\n    chunked_result = list(pairwise_distances_chunked(X, Y, metric=custom_metric, working_memory=1))\n    chunked_result = np.vstack(chunked_result)\n\n    assert_array_almost_equal(chunked_result, expected_result)\n\n    # Test with X = Y\n    expected_result = pairwise_distances(X, metric=custom_metric)\n    chunked_result = list(pairwise_distances_chunked(X, metric=custom_metric, working_memory=1))\n    chunked_result = np.vstack(chunked_result)\n    \n    assert_array_almost_equal(chunked_result, expected_result)\n", "def test_check_pairwise_arrays_invalid_input():\n    # Ensure proper exceptions are raised for invalid input in check_pairwise_arrays\n\n    # Invalid input type\n    XA = \"invalid_input\"\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(TypeError, check_pairwise_arrays, XA, XB)\n\n    # Different number of features\n    XA = np.resize(np.arange(40), (5, 9))\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n\n    # Invalid input dimensionality\n    XA = np.arange(45)  # 1D array\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n", "def test_euclidean_distances_with_chunks():\n    # Test euclidean_distances with chunked processing on dense and sparse arrays\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((500, 4))\n    Y = rng.random_sample((300, 4))\n\n    # Expected results using default processing\n    expected = euclidean_distances(X, Y)\n\n    # Test with different chunk sizes\n    for chunk_size in [2**10, 2**16, 2**20]:\n        result = np.vstack(list(pairwise_distances_chunked(X, Y, working_memory=chunk_size, metric='euclidean')))\n        assert_allclose(result, expected, atol=1e-7)\n\n    # Repeat tests with sparse input\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    expected_sparse = euclidean_distances(X_sparse, Y_sparse)\n\n    for chunk_size in [2**10, 2**16, 2**20]:\n        result_sparse = np.vstack(list(pairwise_distances_chunked(X_sparse, Y_sparse, working_memory=chunk_size, metric='euclidean')))\n        assert_allclose(result_sparse, expected_sparse, atol=1e-7)\n"], "sample_823": ["def test_additive_chi2_kernel():\n    # Test the additive_chi2_kernel function.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((10, 4))\n\n    # Check that the kernel is computed correctly\n    K = additive_chi2_kernel(X, Y)\n    for i, x in enumerate(X):\n        for j, y in enumerate(Y):\n            chi2 = -np.sum((x - y) ** 2 / (x + y))\n            assert_almost_equal(K[i, j], chi2)\n\n    # Check diagonal elements are zero for identical inputs\n    K_self = additive_chi2_kernel(X)\n    assert_array_equal(np.diag(K_self), [0.] * X.shape[0])\n\n    # Check the kernel with zero vectors\n    X_zero = np.zeros((2, 4))\n    Y_zero = np.zeros((3, 4))\n    K_zero = additive_chi2_kernel(X_zero, Y_zero)\n    assert_array_equal(K_zero, np.zeros((2, 3)))\n\n    # Check that negative values raise a ValueError\n    X_negative = rng.random_sample((5, 4)) - 0.5\n    assert_raises(ValueError, additive_chi2_kernel, X_negative, Y)\n    assert_raises(ValueError, additive_chi2_kernel, X, X_negative)\n    assert_raises(ValueError, additive_chi2_kernel, X_negative, X_negative)\n    \n    # Check that different n_features in X and Y raise a ValueError\n    Y_diff_features = rng.random_sample((10, 5))\n    assert_raises(ValueError, additive_chi2_kernel, X, Y_diff_features)\n    \n    # Check sparse inputs raise a ValueError\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    assert_raises(ValueError, additive_chi2_kernel, X_sparse, Y_sparse)\n", "def test_check_pairwise_arrays_dtype():\n    # Check that the dtype argument in check_pairwise_arrays works correctly\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4)).astype(np.float32)\n    Y = rng.random_sample((5, 4)).astype(np.float32)\n\n    # Test with default dtype\n    X_checked, Y_checked = check_pairwise_arrays(X, Y)\n    assert X_checked.dtype == np.float32\n    assert Y_checked.dtype == np.float32\n\n    # Test with explicitly provided dtype\n    X_checked, Y_checked = check_pairwise_arrays(X, Y, dtype=np.float64)\n    assert X_checked.dtype == np.float64\n    assert Y_checked.dtype == np.float64\n\n    # Test with mismatched dtypes\n    Y = rng.random_sample((5, 4)).astype(np.float64)\n    X_checked, Y_checked = check_pairwise_arrays(X, Y)\n    assert X_checked.dtype == np.float64\n    assert Y_checked.dtype == np.float64\n\n    # Test with integer dtype\n    X_int = rng.randint(0, 10, size=(5, 4)).astype(np.int64)\n    Y_int = rng.randint(0, 10, size=(5, 4)).astype(np.int64)\n    X_checked, Y_checked = check_pairwise_arrays(X_int, Y_int, dtype=np.float)\n    assert X_checked.dtype == np.float\n    assert Y_checked.dtype == np.float\n", "def test_additive_chi2_kernel():\n    # Check additive chi2 kernel computation\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((10, 4))\n    K = additive_chi2_kernel(X, Y)\n    \n        return -np.sum((x - y) ** 2 / (x + y))\n    \n    # Validate each element in the computed kernel matrix\n    for i in range(X.shape[0]):\n        for j in range(Y.shape[0]):\n            assert_almost_equal(K[i, j], slow_additive_chi2_kernel(X[i], Y[j]))\n\n    # Check that kernel of similar things is greater than dissimilar ones\n    X = [[0.3, 0.7], [1.0, 0]]\n    Y = [[0, 1], [0.9, 0.1]]\n    K = additive_chi2_kernel(X, Y)\n    assert_greater(K[0, 0], K[0, 1])\n    assert_greater(K[1, 1], K[1, 0])\n\n    # Test invalid input with negative values\n    assert_raises(ValueError, additive_chi2_kernel, [[0, -1]])\n    assert_raises(ValueError, additive_chi2_kernel, [[0, -1]], [[-1, -1]])\n    assert_raises(ValueError, additive_chi2_kernel, [[0, 1]], [[-1, -1]])\n\n    # Test different dimensions in X and Y\n    assert_raises(ValueError, additive_chi2_kernel, [[0, 1]], [[0.2, 0.2, 0.6]])\n    \n    # Test with sparse matrices\n    assert_raises(ValueError, additive_chi2_kernel, csr_matrix(X), csr_matrix(Y))\n", "def test_paired_cosine_distances():\n    # Check the paired cosine distances computation\n    X = np.array([[1, 0, 0], [0, 1, 0], [1, 1, 1]])\n    Y = np.array([[0, 1, 0], [1, 0, 0], [1, 1, 0]])\n    \n    # Manually compute expected cosine distances\n    expected_distances = np.array([\n        1 - np.dot(X[0], Y[0]) / (np.linalg.norm(X[0]) * np.linalg.norm(Y[0])),\n        1 - np.dot(X[1], Y[1]) / (np.linalg.norm(X[1]) * np.linalg.norm(Y[1])),\n        1 - np.dot(X[2], Y[2]) / (np.linalg.norm(X[2]) * np.linalg.norm(Y[2]))\n    ])\n    \n    D = paired_cosine_distances(X, Y)\n    assert_array_almost_equal(D, expected_distances)\n", "def test_haversine_distances_zero_distance():\n    # Check haversine distance returns zero for identical points\n    identical_points = np.array([[0, 0], [0, 0]])\n    D = haversine_distances(identical_points)\n    assert_array_almost_equal(D, np.zeros((2, 2)))\n    \n    # Also check for multiple identical points\n    multiple_identical_points = np.array([[0, 0], [0, 0], [0, 0]])\n    D = haversine_distances(multiple_identical_points)\n    assert_array_almost_equal(D, np.zeros((3, 3)))\n"], "sample_824": ["def test_cosine_distances_with_precomputed():\n    # Ensure that precomputed cosine distances return the same values\n    # and handle non-negative check correctly\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n\n    # Calculate cosine distances using the function\n    S = cosine_distances(X, Y)\n    \n    # Create precomputed distances\n    precomputed_S = pairwise_distances(X, Y, metric=\"cosine\")\n\n    # Ensure that the precomputed distances and function-based distances are the same\n    assert_array_almost_equal(S, precomputed_S)\n\n    # Ensure that pairwise_distances with metric=\"precomputed\" does not raise an error\n    assert_raises(ValueError, pairwise_distances, precomputed_S * -1, metric=\"precomputed\")\n\n    # Ensure that precomputed distances pass non-negative check\n    S_precomputed = pairwise_distances(precomputed_S, metric=\"precomputed\")\n    assert_array_almost_equal(S_precomputed, precomputed_S)\n", "def test_return_float_dtype():\n    # Test the _return_float_dtype utility function\n    X = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\n    Y = np.array([[5.0, 6.0], [7.0, 8.0]], dtype=np.float32)\n    X_out, Y_out, dtype_out = _return_float_dtype(X, Y)\n    assert X_out.dtype == np.float32\n    assert Y_out.dtype == np.float32\n    assert dtype_out == np.float32\n\n    X = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float64)\n    Y = np.array([[5.0, 6.0], [7.0, 8.0]], dtype=np.float64)\n    X_out, Y_out, dtype_out = _return_float_dtype(X, Y)\n    assert X_out.dtype == np.float64\n    assert Y_out.dtype == np.float64\n    assert dtype_out == np.float64\n\n    X = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\n    Y = np.array([[5.0, 6.0], [7.0, 8.0]], dtype=np.float64)\n    X_out, Y_out, dtype_out = _return_float_dtype(X, Y)\n    assert X_out.dtype == np.float32\n    assert Y_out.dtype == np.float64\n    assert dtype_out == np.float\n\n    X = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float64)\n    X_out, Y_out, dtype_out = _return_float_dtype(X, None)\n    assert X_out.dtype == np.float64\n    assert Y_out.dtype == np.float64\n    assert dtype_out == np.float64\n\n    X = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\n    X_out, Y_out, dtype_out = _return_float_dtype(X, None)\n    assert X_out.dtype == np.float32\n    assert Y_out.dtype == np.float32\n    assert dtype_out == np.float32\n", "def test_pairwise_distances_invalid_precomputed():\n    # Test that precomputed distance matrix must be square when Y is None\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n\n    # Creating an invalid precomputed distance matrix (non-square)\n    D_invalid = np.zeros((5, 3))\n\n    # Should raise ValueError because the precomputed distance matrix is not square\n    with pytest.raises(ValueError, match=r\".* shape .*\"):\n        pairwise_distances(D_invalid, metric='precomputed')\n\n    # Creating another invalid precomputed distance matrix (non-square) with Y\n    D_invalid_with_Y = np.zeros((5, 4))\n    Y_invalid = rng.random_sample((4, 4))\n\n    # Should raise ValueError because the precomputed distance matrix is not square\n    with pytest.raises(ValueError, match=r\".* shape .*\"):\n        pairwise_distances(D_invalid_with_Y, Y_invalid, metric='precomputed')\n", "def test_sigmoid_kernel():\n    # Check the sigmoid kernel computation\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((10, 4))\n\n    # Test with default gamma and coef0\n    K = sigmoid_kernel(X, Y)\n    expected_K = np.tanh(np.dot(X, Y.T) + 1)\n    assert_array_almost_equal(K, expected_K)\n\n    # Test with custom gamma and coef0\n    gamma = 0.5\n    coef0 = 0.1\n    K = sigmoid_kernel(X, Y, gamma=gamma, coef0=coef0)\n    expected_K = np.tanh(gamma * np.dot(X, Y.T) + coef0)\n    assert_array_almost_equal(K, expected_K)\n\n    # Test with gamma and coef0 as integers\n    gamma = 2\n    coef0 = -1\n    K = sigmoid_kernel(X, Y, gamma=gamma, coef0=coef0)\n    expected_K = np.tanh(gamma * np.dot(X, Y.T) + coef0)\n    assert_array_almost_equal(K, expected_K)\n\n    # Test with sparse matrices\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    K_sparse = sigmoid_kernel(X_sparse, Y_sparse, gamma=gamma, coef0=coef0)\n    assert_array_almost_equal(K_sparse, expected_K)\n\n    # Ensure diagonal elements are as expected for X with itself\n    K_self = sigmoid_kernel(X, X, gamma=gamma, coef0=coef0)\n    expected_K_self = np.tanh(gamma * np.dot(X, X.T) + coef0)\n    assert_array_almost_equal(np.diag(K_self), np.diag(expected_K_self))\n", "def test_paired_cosine_distances():\n    # Check the paired cosine distances computation\n    X = [[0, 1], [1, 1]]\n    Y = [[1, 1], [2, 2]]\n    D = paired_cosine_distances(X, Y)\n    assert_array_almost_equal(D, [0.29289322, 0.29289322])\n\n    # Test with tuples as X and Y\n    X_tuples = tuple([tuple(row) for row in X])\n    Y_tuples = tuple([tuple(row) for row in Y])\n    D2 = paired_cosine_distances(X_tuples, Y_tuples)\n    assert_array_almost_equal(D, D2)\n\n    # Test with sparse X and Y\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    D3 = paired_cosine_distances(X_sparse, Y_sparse)\n    assert_array_almost_equal(D, D3)\n\n    # Test that a value error is raised when the lengths of X and Y differ\n    Y_diff = [[1, 1]]\n    assert_raises(ValueError, paired_cosine_distances, X, Y_diff)\n"], "sample_825": ["def test_pls_zero_variance_feature():\n    # Test PLS with a feature having zero variance\n    # This will ensure that the implementation can handle features with zero variance\n\n    X = np.array([[0., 0., 1.],\n                  [1., 0., 0.],\n                  [2., 0., 2.],\n                  [2., 0., 4.]])\n    Y = np.array([[0.1, -0.2],\n                  [0.9, 1.1],\n                  [6.2, 5.9],\n                  [11.9, 12.3]])\n    \n    # Test for PLSRegression\n    pls_reg = pls_.PLSRegression(n_components=2)\n    pls_reg.fit(X, Y)\n    X_transformed = pls_reg.transform(X)\n    Y_pred = pls_reg.predict(X)\n    \n    assert X_transformed.shape[1] == 2\n    assert Y_pred.shape == Y.shape\n    \n    # Test for PLSCanonical\n    pls_can = pls_.PLSCanonical(n_components=2)\n    pls_can.fit(X, Y)\n    X_transformed, Y_transformed = pls_can.transform(X, Y)\n    \n    assert X_transformed.shape[1] == 2\n    assert Y_transformed.shape[1] == 2\n    \n    # Test for PLSSVD\n    pls_svd = pls_.PLSSVD(n_components=2)\n    pls_svd.fit(X, Y)\n    X_transformed_svd, Y_transformed_svd = pls_svd.transform(X, Y)\n    \n    assert X_transformed_svd.shape[1] == 2\n    assert Y_transformed_svd.shape[1] == 2\n\n    # Ensure the implementation is stable and doesn't raise any exceptions\n    assert True\n", "def test_pls_svd_transform():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    pls_svd = pls_.PLSSVD(n_components=2, scale=True)\n    pls_svd.fit(X, Y)\n\n    X_transformed = pls_svd.transform(X)\n    assert X_transformed.shape == (X.shape[0], 2), \"Transformed X has incorrect shape\"\n\n    X_transformed, Y_transformed = pls_svd.transform(X, Y)\n    assert X_transformed.shape == (X.shape[0], 2), \"Transformed X has incorrect shape\"\n    assert Y_transformed.shape == (Y.shape[0], 2), \"Transformed Y has incorrect shape\"\n\n    # Ensure the transform method doesn't alter the original data\n    X_copy = X.copy()\n    Y_copy = Y.copy()\n    pls_svd.transform(X_copy, Y_copy)\n    assert_array_equal(X, X_copy)\n    assert_array_equal(Y, Y_copy)\n", "def test_pls_transform_consistency():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls = pls_.PLSCanonical(n_components=2)\n    pls.fit(X, Y)\n    X_transformed_1 = pls.transform(X)\n    X_transformed_2, Y_transformed_2 = pls.transform(X, Y)\n    \n    # Check if transform produces consistent results\n    assert_array_almost_equal(X_transformed_1, pls.x_scores_,\n                              err_msg=\"Inconsistent transformation for X\")\n    assert_array_almost_equal(X_transformed_2, pls.x_scores_,\n                              err_msg=\"Inconsistent transformation for X with Y\")\n    assert_array_almost_equal(Y_transformed_2, pls.y_scores_,\n                              err_msg=\"Inconsistent transformation for Y\")\n", "def test_pls_transform_return_shapes():\n    # Test that transform returns arrays of the correct shape\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    n_components = 2\n\n    for clf in [pls_.PLSCanonical, pls_.PLSRegression, pls_.PLSSVD]:\n        pls = clf(n_components=n_components)\n        pls.fit(X, Y)\n\n        X_transformed = pls.transform(X)\n        assert_equal(X_transformed.shape, (X.shape[0], n_components))\n\n        if hasattr(pls, 'transform') and 'Y' in pls.transform.__code__.co_varnames:\n            X_transformed, Y_transformed = pls.transform(X, Y)\n            assert_equal(X_transformed.shape, (X.shape[0], n_components))\n            assert_equal(Y_transformed.shape, (Y.shape[0], n_components))\n", "def test_pls_transform():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls = pls_.PLSRegression(n_components=2)\n    pls.fit(X, Y)\n    \n    X_trans = pls.transform(X)\n    assert_equal(X_trans.shape[1], 2, \"Transformed X should have 2 components\")\n\n    X_trans, Y_trans = pls.transform(X, Y)\n    assert_equal(X_trans.shape[1], 2, \"Transformed X should have 2 components\")\n    assert_equal(Y_trans.shape[1], 2, \"Transformed Y should have 2 components\")\n    \n    assert_array_almost_equal(X_trans, pls.x_scores_, err_msg=\"Transformed X does not match x_scores_\")\n    assert_array_almost_equal(Y_trans, pls.y_scores_, err_msg=\"Transformed Y does not match y_scores_\")\n"], "sample_826": ["def test_ordinal_encoder_dtype():\n    X = [['a', 2, 55], ['b', 1, 55], ['a', 3, 55]]\n    enc = OrdinalEncoder()\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == np.float64\n\n    enc = OrdinalEncoder(dtype=np.int32)\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == np.int32\n\n    enc = OrdinalEncoder(dtype=np.float64)\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == np.float64\n", "def test_one_hot_encoder_inverse_with_unknown():\n    # Test inverse transform with unknown categories and handle_unknown='ignore'\n    X_train = [['cat', 1], ['dog', 2]]\n    X_test = [['fish', 1], ['cat', 3]]\n\n    enc = OneHotEncoder(handle_unknown='ignore')\n    enc.fit(X_train)\n    X_train_transformed = enc.transform(X_train).toarray()\n    X_test_transformed = enc.transform(X_test).toarray()\n\n    # Check that inverse transform correctly handles unknown categories\n    X_train_inverse = enc.inverse_transform(X_train_transformed)\n    X_test_inverse = enc.inverse_transform(X_test_transformed)\n\n    assert_array_equal(X_train_inverse, np.array(X_train, dtype=object))\n\n    expected_test_inverse = np.array([['None', 1], ['cat', 'None']], dtype=object)\n    assert_array_equal(X_test_inverse, expected_test_inverse)\n", "def test_one_hot_encoder_large_categories():\n    # Test OneHotEncoder with a large number of categories\n    X = np.array([['cat{}'.format(i)] for i in range(1000)])\n    enc = OneHotEncoder(sparse=False)\n    X_trans = enc.fit_transform(X)\n    assert_equal(X_trans.shape, (1000, 1000))\n    assert_array_equal(X_trans.sum(axis=0), np.ones(1000))\n    assert_array_equal(X_trans.sum(axis=1), np.ones(1000))\n    feature_names = enc.get_feature_names(['feature'])\n    assert_array_equal(feature_names, ['feature_cat{}'.format(i) for i in range(1000)])\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([[0, 1, 2], [1, 0, 2], [1, 0, 1]])\n    X2 = np.array([[1, 0, 3], [0, 2, 1]])\n\n    # Test that ordinal encoder raises error for unknown features\n    # present during transform.\n    oe = OrdinalEncoder(handle_unknown='error')\n    oe.fit(X)\n    assert_raises(ValueError, oe.transform, X2)\n\n    # Test the ignore option, ignores unknown features (giving all 0's)\n    oe = OrdinalEncoder(handle_unknown='ignore')\n    oe.fit(X)\n    X2_trans = oe.transform(X2)\n    expected_result = np.array([[1, 0, -1], [0, -1, 0]])\n    assert_array_equal(X2_trans, expected_result)\n\n    # Ensure transformed data was not modified in place\n    assert_allclose(X2, np.array([[1, 0, 3], [0, 2, 1]]))\n\n    # Raise error if handle_unknown is neither ignore nor error.\n    oe = OrdinalEncoder(handle_unknown='unknown')\n    assert_raises(ValueError, oe.fit, X)\n", "def test_ordinal_encoder_specified_categories_unsorted():\n    X = np.array([['a', 'b']], dtype=object).T\n    X2 = np.array([['a', 'd']], dtype=object).T\n\n    # unsorted categories for a string feature\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted categories should raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n\n    # specifying categories with mixed types should raise an error\n    X = np.array([['a', 1], ['b', 2]], dtype=object)\n    enc = OrdinalEncoder(categories=[['a', 'b'], [1, '2']])\n    with pytest.raises(ValueError, match='should not mix strings and numeric values'):\n        enc.fit(X)\n"], "sample_827": ["def test_inplace_swap_row_csc():\n    X = np.array([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9]], dtype=np.float64)\n    X_csc = sp.csc_matrix(X)\n\n    # Swap rows 0 and 2\n    inplace_swap_row_csc(X_csc, 0, 2)\n    expected = np.array([[7, 8, 9],\n                         [4, 5, 6],\n                         [1, 2, 3]], dtype=np.float64)\n    assert_array_equal(X_csc.toarray(), expected)\n\n    # Swap rows 1 and 1 (no-op)\n    inplace_swap_row_csc(X_csc, 1, 1)\n    assert_array_equal(X_csc.toarray(), expected)\n\n    # Swap rows -1 and 0 (equivalent to swapping rows 2 and 0)\n    inplace_swap_row_csc(X_csc, -1, 0)\n    expected = np.array([[1, 2, 3],\n                         [4, 5, 6],\n                         [7, 8, 9]], dtype=np.float64)\n    assert_array_equal(X_csc.toarray(), expected)\n\n    # Test invalid row indices\n    assert_raises(ValueError, inplace_swap_row_csc, X_csc, 0, 3)\n    assert_raises(ValueError, inplace_swap_row_csc, X_csc, -4, 0)\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc, np.array([0]), 1)\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc, 0, np.array([1]))\n", "def test_inplace_swap_row_csc():\n    X = np.array([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9],\n                  [10, 11, 12]], dtype=np.float64)\n    X_csc = sp.csc_matrix(X)\n\n    # Swap first and last row\n    inplace_swap_row_csc(X_csc, 0, -1)\n    expected = np.array([[10, 11, 12],\n                         [4, 5, 6],\n                         [7, 8, 9],\n                         [1, 2, 3]])\n    assert_array_equal(X_csc.toarray(), expected)\n\n    # Swap second and third row\n    inplace_swap_row_csc(X_csc, 1, 2)\n    expected = np.array([[10, 11, 12],\n                         [7, 8, 9],\n                         [4, 5, 6],\n                         [1, 2, 3]])\n    assert_array_equal(X_csc.toarray(), expected)\n\n    # Test with negative indices\n    inplace_swap_row_csc(X_csc, -3, -2)\n    expected = np.array([[10, 11, 12],\n                         [4, 5, 6],\n                         [7, 8, 9],\n                         [1, 2, 3]])\n    assert_array_equal(X_csc.toarray(), expected)\n\n    # Check for TypeError with non-integer indices\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc, 1, np.array([2]))\n\n    # Check for TypeError with non-CSC matrix\n    X_csr = sp.csr_matrix(X)\n    assert_raises(TypeError, inplace_swap_row_csc, X_csr, 0, 1)\n", "def test_inplace_swap_row_csc():\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csc = sp.csc_matrix(X)\n\n    swap = linalg.get_blas_funcs(('swap',), (X,))\n    swap = swap[0]\n    X[0], X[-1] = swap(X[0], X[-1])\n    inplace_swap_row_csc(X_csc, 0, -1)\n    assert_array_equal(X, X_csc.toarray())\n\n    X[2], X[3] = swap(X[2], X[3])\n    inplace_swap_row_csc(X_csc, 2, 3)\n    assert_array_equal(X, X_csc.toarray())\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc.toarray(), 0, 1)\n\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float32)\n    X_csc = sp.csc_matrix(X)\n    swap = linalg.get_blas_funcs(('swap',), (X,))\n    swap = swap[0]\n    X[0], X[-1] = swap(X[0], X[-1])\n    inplace_swap_row_csc(X_csc, 0, -1)\n    assert_array_equal(X, X_csc.toarray())\n    X[2], X[3] = swap(X[2], X[3])\n    inplace_swap_row_csc(X_csc, 2, 3)\n    assert_array_equal(X, X_csc.toarray())\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc.toarray(), 0, 1)\n", "def test_inplace_swap_row_csc():\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csc = sp.csc_matrix(X)\n\n    swap = linalg.get_blas_funcs(('swap',), (X,))\n    swap = swap[0]\n    X[[0, 4], :] = X[[4, 0], :]\n    inplace_swap_row_csc(X_csc, 0, 4)\n    assert_array_equal(X, X_csc.toarray())\n\n    X[[2, 3], :] = X[[3, 2], :]\n    inplace_swap_row_csc(X_csc, 2, 3)\n    assert_array_equal(X, X_csc.toarray())\n\n    # Check with negative indices\n    X[[0, -1], :] = X[[-1, 0], :]\n    inplace_swap_row_csc(X_csc, 0, -1)\n    assert_array_equal(X, X_csc.toarray())\n\n    # Check for TypeError with invalid indices\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc, np.array([0]), 2)\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc, 0, np.array([2]))\n", "def test_inplace_swap_row_csc():\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csc = sp.csc_matrix(X)\n\n    # Swap rows 0 and 4\n    inplace_swap_row_csc(X_csc, 0, 4)\n    X[[0, 4]] = X[[4, 0]]\n    assert_array_equal(X, X_csc.toarray())\n\n    # Swap rows 1 and 3\n    inplace_swap_row_csc(X_csc, 1, 3)\n    X[[1, 3]] = X[[3, 1]]\n    assert_array_equal(X, X_csc.toarray())\n\n    # Swap rows with negative indices\n    inplace_swap_row_csc(X_csc, -1, -3)\n    X[[-1, -3]] = X[[-3, -1]]\n    assert_array_equal(X, X_csc.toarray())\n\n    # Ensure TypeError is raised for invalid input\n    assert_raises(TypeError, inplace_swap_row_csc, X_csc.toarray(), 0, 1)\n"], "sample_828": ["def test_cosine_distances_sparse():\n    # Check the pairwise Cosine distances computation with sparse matrices\n    rng = np.random.RandomState(1337)\n    x = np.abs(rng.rand(910))\n    XA = csr_matrix(np.vstack([x, x]))\n    D = cosine_distances(XA)\n    assert_array_almost_equal(D.toarray(), [[0., 0.], [0., 0.]])\n    # check that all elements are in [0, 2]\n    assert np.all(D.toarray() >= 0.)\n    assert np.all(D.toarray() <= 2.)\n    # check that diagonal elements are equal to 0\n    assert_array_almost_equal(D.diagonal(), [0., 0.])\n\n    XB = csr_matrix(np.vstack([x, -x]))\n    D2 = cosine_distances(XB)\n    # check that all elements are in [0, 2]\n    assert np.all(D2.toarray() >= 0.)\n    assert np.all(D2.toarray() <= 2.)\n    # check that diagonal elements are equal to 0 and non diagonal to 2\n    assert_array_almost_equal(D2.toarray(), [[0., 2.], [2., 0.]])\n\n    # check large random matrix\n    X = csr_matrix(np.abs(rng.rand(1000, 5000)))\n    D = cosine_distances(X)\n    # check that diagonal elements are equal to 0\n    assert_array_almost_equal(D.diagonal(), [0.] * D.shape[0])\n    assert np.all(D.toarray() >= 0.)\n    assert np.all(D.toarray() <= 2.)\n", "def test_pairwise_distances_with_different_dtypes():\n    # Check pairwise_distances with different input dtypes\n    rng = np.random.RandomState(0)\n\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((2, 4))\n    \n    # Convert to different dtypes\n    X_int = X.astype(np.int)\n    Y_int = Y.astype(np.int)\n    X_float32 = X.astype(np.float32)\n    Y_float32 = Y.astype(np.float32)\n\n    # Euclidean distance\n    S = pairwise_distances(X, Y, metric=\"euclidean\")\n    S_int = pairwise_distances(X_int, Y_int, metric=\"euclidean\")\n    S_float32 = pairwise_distances(X_float32, Y_float32, metric=\"euclidean\")\n    \n    assert_array_almost_equal(S, S_int)\n    assert_array_almost_equal(S, S_float32)\n\n    # Manhattan distance\n    S = pairwise_distances(X, Y, metric=\"manhattan\")\n    S_int = pairwise_distances(X_int, Y_int, metric=\"manhattan\")\n    S_float32 = pairwise_distances(X_float32, Y_float32, metric=\"manhattan\")\n    \n    assert_array_almost_equal(S, S_int)\n    assert_array_almost_equal(S, S_float32)\n\n    # Cosine distance\n    S = pairwise_distances(X, Y, metric=\"cosine\")\n    S_int = pairwise_distances(X_int, Y_int, metric=\"cosine\")\n    S_float32 = pairwise_distances(X_float32, Y_float32, metric=\"cosine\")\n    \n    assert_array_almost_equal(S, S_int)\n    assert_array_almost_equal(S, S_float32)\n", "def test_pairwise_distances_argmin_min_chunked():\n    # Check pairwise minimum distances computation with chunked distances\n    rng = np.random.RandomState(0)\n    X = rng.randn(97, 149)\n    Y = rng.randn(111, 149)\n\n    dist = pairwise_distances(X, Y, metric=\"manhattan\")\n    dist_orig_ind = dist.argmin(axis=0)\n    dist_orig_val = dist[dist_orig_ind, range(len(dist_orig_ind))]\n\n    dist_chunked_ind, dist_chunked_val = pairwise_distances_argmin_min(\n        X, Y, axis=0, metric=\"manhattan\")\n    assert_array_almost_equal(dist_orig_ind, dist_chunked_ind)\n    assert_array_almost_equal(dist_orig_val, dist_chunked_val)\n\n    # Test with different metric\n    dist = pairwise_distances(X, Y, metric=\"cosine\")\n    dist_orig_ind = dist.argmin(axis=0)\n    dist_orig_val = dist[dist_orig_ind, range(len(dist_orig_ind))]\n\n    dist_chunked_ind, dist_chunked_val = pairwise_distances_argmin_min(\n        X, Y, axis=0, metric=\"cosine\")\n    assert_array_almost_equal(dist_orig_ind, dist_chunked_ind)\n    assert_array_almost_equal(dist_orig_val, dist_chunked_val)\n", "def test_sigmoid_kernel():\n    # Test the sigmoid kernel function.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n    \n    # Test with Y=None\n    K1 = sigmoid_kernel(X)\n    K2 = np.tanh(np.dot(X, X.T) + 1)\n    assert_array_almost_equal(K1, K2)\n    \n    # Test with Y=Y\n    K1 = sigmoid_kernel(X, Y)\n    K2 = np.tanh(np.dot(X, Y.T) + 1)\n    assert_array_almost_equal(K1, K2)\n    \n    # Test with custom gamma and coef0\n    gamma = 0.5\n    coef0 = 2\n    K1 = sigmoid_kernel(X, Y, gamma=gamma, coef0=coef0)\n    K2 = np.tanh(gamma * np.dot(X, Y.T) + coef0)\n    assert_array_almost_equal(K1, K2)\n    \n    # Test with tuples as X and Y\n    X_tuples = tuple([tuple([v for v in row]) for row in X])\n    Y_tuples = tuple([tuple([v for v in row]) for row in Y])\n    K2 = sigmoid_kernel(X_tuples, Y_tuples, gamma=gamma, coef0=coef0)\n    assert_array_almost_equal(K1, K2)\n    \n    # Test with sparse matrices\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    K1 = sigmoid_kernel(X_sparse, Y_sparse, gamma=gamma, coef0=coef0)\n    assert_array_almost_equal(K1.toarray(), K2)\n", "def test_haversine_distances_pairwise():\n    # Check that pairwise_distances works with haversine distances\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 2))\n    X[:, 0] = (X[:, 0] - 0.5) * 2 * np.pi/2\n    X[:, 1] = (X[:, 1] - 0.5) * 2 * np.pi\n    Y = rng.random_sample((3, 2))\n    Y[:, 0] = (Y[:, 0] - 0.5) * 2 * np.pi/2\n    Y[:, 1] = (Y[:, 1] - 0.5) * 2 * np.pi\n\n    # Test haversine distance between X and X\n    S = pairwise_distances(X, metric=\"haversine\")\n    S_expected = haversine_distances(X)\n    assert_array_almost_equal(S, S_expected)\n\n    # Test haversine distance between X and Y\n    S = pairwise_distances(X, Y, metric=\"haversine\")\n    S_expected = haversine_distances(X, Y)\n    assert_array_almost_equal(S, S_expected)\n\n    # Test haversine distance with tuple inputs\n    X_tuples = tuple([tuple(row) for row in X])\n    Y_tuples = tuple([tuple(row) for row in Y])\n    S = pairwise_distances(X_tuples, Y_tuples, metric=\"haversine\")\n    assert_array_almost_equal(S, S_expected)\n"], "sample_829": ["def test_fit_transform_with_different_batch_sizes():\n    # Test IncrementalPCA.fit_transform with different batch sizes\n    X, _ = datasets.make_classification(n_samples=200, n_features=10, random_state=0)\n    batch_sizes = [10, 20, 50, 100]\n    for batch_size in batch_sizes:\n        ipca = IncrementalPCA(n_components=5, batch_size=batch_size)\n        X_transformed = ipca.fit_transform(X)\n        assert X_transformed.shape == (200, 5)\n        assert ipca.components_.shape == (5, 10)\n        assert ipca.singular_values_.shape == (5,)\n        assert ipca.explained_variance_.shape == (5,)\n        assert ipca.explained_variance_ratio_.shape == (5,)\n", "def test_incremental_pca_whiten_attribute():\n    # Test the whiten attribute to ensure transformed components have unit variance\n    rng = np.random.RandomState(1999)\n    X = datasets.make_low_rank_matrix(200, 20, tail_strength=0., effective_rank=5, random_state=1999)\n\n    # Check when whiten is set to True\n    ipca_whiten_true = IncrementalPCA(n_components=5, batch_size=20, whiten=True).fit(X)\n    X_transformed_whiten_true = ipca_whiten_true.transform(X)\n    components_variance_whiten_true = np.var(X_transformed_whiten_true, axis=0)\n\n    # Assert that the variance of each component is approximately 1\n    np.testing.assert_allclose(components_variance_whiten_true, np.ones(5), rtol=1e-2)\n\n    # Check when whiten is set to False\n    ipca_whiten_false = IncrementalPCA(n_components=5, batch_size=20, whiten=False).fit(X)\n    X_transformed_whiten_false = ipca_whiten_false.transform(X)\n    components_variance_whiten_false = np.var(X_transformed_whiten_false, axis=0)\n\n    # Assert that the variance of each component is not necessarily 1\n    assert not np.allclose(components_variance_whiten_false, np.ones(5), rtol=1e-2)\n", "def test_incremental_pca_copy_param():\n    # Test that the copy parameter affects the input data as expected.\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 5)\n\n    # When copy=True, input data should remain unchanged\n    X_copy = X.copy()\n    ipca = IncrementalPCA(n_components=3, copy=True)\n    ipca.fit(X)\n    assert_array_almost_equal(X, X_copy)\n\n    # When copy=False, input data can be modified\n    ipca = IncrementalPCA(n_components=3, copy=False)\n    ipca.fit(X)\n    assert not np.allclose(X, X_copy)\n\n", "def test_partial_fit_check_mean_var():\n    # Test that partial_fit updates mean_ and var_ correctly.\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 100, 5\n    X = rng.randn(n_samples, n_features)\n\n    batch_size = 10\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\n    \n    # Performing partial_fit in batches\n    batch_itr = np.arange(0, n_samples + 1, batch_size)\n    for i, j in zip(batch_itr[:-1], batch_itr[1:]):\n        ipca.partial_fit(X[i:j, :])\n    \n    # Checking mean and variance of the entire data\n    global_mean = np.mean(X, axis=0)\n    global_var = np.var(X, axis=0)\n\n    assert_almost_equal(ipca.mean_, global_mean)\n    assert_almost_equal(ipca.var_, global_var)\n", "def test_incremental_pca_whiten_parameter():\n    # Test that the whiten parameter works as expected.\n    rng = np.random.RandomState(0)\n    n_samples = 200\n    n_features = 5\n    X = rng.randn(n_samples, n_features)\n\n    # Test with whiten=True\n    ipca_whiten = IncrementalPCA(n_components=3, whiten=True, batch_size=50).fit(X)\n    transformed_whiten = ipca_whiten.transform(X)\n    assert np.allclose(np.var(transformed_whiten, axis=0), 1, rtol=1e-3)\n\n    # Test with whiten=False\n    ipca_no_whiten = IncrementalPCA(n_components=3, whiten=False, batch_size=50).fit(X)\n    transformed_no_whiten = ipca_no_whiten.transform(X)\n    assert not np.allclose(np.var(transformed_no_whiten, axis=0), 1, rtol=1e-3)\n"], "sample_830": ["def test_get_blas_info():\n    from sklearn.utils._show_versions import _get_blas_info\n\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_show_versions_without_blas(monkeypatch, capsys):\n    # Monkeypatch the _get_blas_info function to simulate an ImportError\n    from sklearn.utils import _show_versions\n        raise ImportError(\"scikit-learn not installed\")\n    monkeypatch.setattr(_show_versions, \"_get_blas_info\", mock_get_blas_info)\n\n    show_versions()\n    out, err = capsys.readouterr()\n    assert 'python' in out\n    assert 'numpy' in out\n    assert 'BLAS' not in out\n", "def test_get_blas_info(monkeypatch):\n    from sklearn.utils._show_versions import _get_blas_info\n    \n        return ['libcblas'], {\n            'define_macros': [('HAVE_CBLAS', 1)],\n            'library_dirs': ['/usr/local/lib']\n        }\n\n    monkeypatch.setattr('sklearn._build_utils.get_blas_info', mock_get_blas_info)\n    \n    blas_info = _get_blas_info()\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n    assert 'HAVE_CBLAS=1' in blas_info['macros']\n    assert '/usr/local/lib' in blas_info['lib_dirs']\n    assert 'libcblas' in blas_info['cblas_libs']\n", "def test_get_blas_info(mocker):\n    # Mock the get_blas_info to control the output\n    from sklearn.utils._show_versions import _get_blas_info\n    mock_get_blas_info = mocker.patch('sklearn.utils._show_versions.get_blas_info')\n    \n    # Define the mock return values\n    mock_get_blas_info.return_value = (['libcblas'], {'define_macros': [('HAVE_CBLAS', 1)], 'library_dirs': ['/usr/local/lib']})\n    \n    blas_info = _get_blas_info()\n    \n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n    assert blas_info['macros'] == 'HAVE_CBLAS=1'\n    assert blas_info['lib_dirs'] == '/usr/local/lib'\n    assert blas_info['cblas_libs'] == 'libcblas'\n", "def test_get_blas_info():\n    from sklearn.utils._show_versions import _get_blas_info\n\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n"], "sample_831": ["def test_plot_tree_regression(pyplot):\n    # Test plot_tree for a regression tree\n    reg = DecisionTreeRegressor(max_depth=3,\n                                min_samples_split=2,\n                                criterion=\"mse\",\n                                random_state=2)\n    reg.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(reg, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\nmse = 1.0\\n\"\n                                   \"samples = 6\\nvalue = 0.0\")\n    assert nodes[1].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = -1.0\"\n    assert nodes[2].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = 1.0\"\n", "def test_plot_tree_proportion():\n    # Check the proportion display in plot_tree\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    nodes = plot_tree(clf, proportion=True)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"X[0] <= 0.0\\ngini = 0.5\\n\"\n                                   \"samples = 100.0%\\nvalue = [0.5, 0.5]\")\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 50.0%\\nvalue = [1.0, 0.0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 50.0%\\nvalue = [0.0, 1.0]\"\n\n", "def test_plot_tree_proportion(pyplot):\n    # Test plot_tree with proportion=True\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names, proportion=True)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\ngini = 0.5\\n\"\n                                   \"samples = 100.0%\\nvalue = [0.5, 0.5]\")\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 50.0%\\nvalue = [1.0, 0.0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 50.0%\\nvalue = [0.0, 1.0]\"\n", "def test_color_brew():\n    # Test the _color_brew function with various values of n\n    for n in range(1, 10):\n        colors = _color_brew(n)\n        assert_equal(len(colors), n)\n        for color in colors:\n            assert_equal(len(color), 3)\n            assert all(0 <= c <= 255 for c in color)\n", "def test_color_brew():\n    # Test the _color_brew function for generating n colors\n    n = 5\n    colors = _color_brew(n)\n    assert_equal(len(colors), n)\n    assert all(isinstance(color, list) and len(color) == 3 for color in colors), \\\n        \"Each color should be a list of 3 integers\"\n    # Ensure the values are within the RGB range\n    assert all(0 <= component <= 255 for color in colors for component in color), \\\n        \"Color components should be within the range [0, 255]\"\n\n    n = 10\n    colors = _color_brew(n)\n    assert_equal(len(colors), n)\n    assert all(isinstance(color, list) and len(color) == 3 for color in colors), \\\n        \"Each color should be a list of 3 integers\"\n    # Ensure the values are within the RGB range\n    assert all(0 <= component <= 255 for color in colors for component in color), \\\n        \"Color components should be within the range [0, 255]\"\n"], "sample_832": ["def test_ard_regression_initial_params():\n    # Test ARDRegression with initial values (alpha_init, lambda_init)\n    X = np.vander(np.linspace(0, 4, 5), 4)\n    y = np.array([0., 1., 0., -1., 0.])    # y = (x^3 - 6x^2 + 8x) / 3\n\n    # In this case, starting from the default initial values will increase\n    # the bias of the fitted curve. So, lambda_init should be small.\n    reg = ARDRegression(n_iter=1000)\n    reg.fit(X, y)\n    initial_alpha = reg.alpha_\n    initial_lambda = reg.lambda_.copy()\n    \n    # Ensure ARDRegression fits the data well by checking the R2 score\n    r2 = reg.score(X, y)\n    assert r2 > 0.9, f\"Expected R2 > 0.9 but got {r2}\"\n\n    # Refit with initial values to check consistency\n    reg2 = ARDRegression(alpha_1=initial_alpha, lambda_1=initial_lambda, n_iter=1000)\n    reg2.fit(X, y)\n    r2_with_initial_params = reg2.score(X, y)\n    assert r2_with_initial_params > 0.9, f\"Expected R2 > 0.9 but got {r2_with_initial_params}\"\n    assert_almost_equal(initial_alpha, reg2.alpha_, decimal=3)\n    assert_array_almost_equal(initial_lambda, reg2.lambda_, decimal=3)\n", "def test_ard_sample_weights():\n    # Test correctness of the sample_weights method in ARDRegression\n    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n    y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n    w = np.array([4, 3, 3, 1, 1, 2, 3]).T\n\n    # A Ridge regression model using an alpha value equal to the ratio of\n    # lambda_ and alpha_ from the ARDRegression model must be identical\n    ard_model = ARDRegression(compute_score=True).fit(X, y, sample_weight=w)\n    rr_model = Ridge(alpha=ard_model.lambda_[ard_model.lambda_ < ard_model.threshold_lambda].mean() / ard_model.alpha_).fit(\n        X, y, sample_weight=w)\n    assert_array_almost_equal(rr_model.coef_, ard_model.coef_[ard_model.lambda_ < ard_model.threshold_lambda])\n    assert_almost_equal(rr_model.intercept_, ard_model.intercept_)\n", "def test_bayesian_ridge_fit_intercept():\n    # Test BayesianRidge with fit_intercept=False\n    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n    y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n\n    br_model = BayesianRidge(fit_intercept=False).fit(X, y)\n    rr_model = Ridge(alpha=br_model.lambda_ / br_model.alpha_, fit_intercept=False).fit(X, y)\n    assert_array_almost_equal(rr_model.coef_, br_model.coef_)\n    assert_almost_equal(rr_model.intercept_, br_model.intercept_, decimal=9)\n", "def test_bayesian_ridge_intercept_and_normalize():\n    # Test BayesianRidge with fit_intercept and normalize parameters\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n    y = np.array([1, 2, 3, 4, 5])\n\n    # Test with fit_intercept=True and normalize=True\n    clf = BayesianRidge(fit_intercept=True, normalize=True)\n    clf.fit(X, y)\n    assert clf.fit_intercept is True\n    assert clf.normalize is True\n    assert_almost_equal(clf.intercept_, 0, decimal=1)\n\n    # Test with fit_intercept=False and normalize=True\n    clf = BayesianRidge(fit_intercept=False, normalize=True)\n    clf.fit(X, y)\n    assert clf.fit_intercept is False\n    assert clf.normalize is True\n    assert_almost_equal(clf.intercept_, 0, decimal=1)\n\n    # Test with fit_intercept=True and normalize=False\n    clf = BayesianRidge(fit_intercept=True, normalize=False)\n    clf.fit(X, y)\n    assert clf.fit_intercept is True\n    assert clf.normalize is False\n    assert_almost_equal(clf.intercept_, 0, decimal=1)\n\n    # Test with fit_intercept=False and normalize=False\n    clf = BayesianRidge(fit_intercept=False, normalize=False)\n    clf.fit(X, y)\n    assert clf.fit_intercept is False\n    assert clf.normalize is False\n    assert_almost_equal(clf.intercept_, 0, decimal=1)\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with fit_intercept set to True and False\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([1, 2, 3, 4, 5])\n\n    # Test with fit_intercept=True\n    clf_intercept = BayesianRidge(fit_intercept=True)\n    clf_intercept.fit(X, y)\n    assert clf_intercept.intercept_ != 0.0\n\n    # Test with fit_intercept=False\n    clf_no_intercept = BayesianRidge(fit_intercept=False)\n    clf_no_intercept.fit(X, y)\n    assert clf_no_intercept.intercept_ == 0.0\n"], "sample_833": ["def test_logistic_regression_path_edge_cases():\n    # Test logistic_regression_path with edge case parameters\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    Cs = [0.1, 1, 10]\n    \n    # Case with fit_intercept=False and very high max_iter\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=Cs, fit_intercept=False, max_iter=10000, tol=1e-4)\n    assert len(coefs) == len(Cs)\n    assert all(iter_count <= 10000 for iter_count in n_iter)\n\n    # Case with fit_intercept=True and very low max_iter\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=Cs, fit_intercept=True, max_iter=1, tol=1e-4)\n    assert len(coefs) == len(Cs)\n    assert all(iter_count == 1 for iter_count in n_iter)\n    \n    # Check if it raises a ValueError for an empty Cs list\n    with pytest.raises(ValueError):\n        logistic_regression_path(X, y, Cs=[], fit_intercept=True)\n", "def test_multinomial_loss():\n    # Ensure that the multinomial loss function returns expected values\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_classes = 100, 5, 3\n    X = rng.randn(n_samples, n_features)\n    w = rng.rand(n_classes, n_features)\n    Y = np.zeros((n_samples, n_classes))\n    ind = np.argmax(np.dot(X, w.T), axis=1)\n    Y[range(0, n_samples), ind] = 1\n    w = w.ravel()\n    alpha = 1.0\n    sample_weights = np.ones(X.shape[0])\n\n    # Compute the loss and probabilities\n    loss, p, _ = _multinomial_loss(w, X, Y, alpha, sample_weights)\n\n    # Manually compute the expected loss\n    expected_p = np.dot(X, w.reshape(n_classes, n_features).T)\n    expected_p -= logsumexp(expected_p, axis=1)[:, np.newaxis]\n    expected_loss = -(sample_weights[:, np.newaxis] * Y * expected_p).sum()\n    expected_loss += 0.5 * alpha * squared_norm(w.reshape(n_classes, n_features))\n\n    assert_almost_equal(loss, expected_loss)\n    assert_array_almost_equal(p, np.exp(expected_p))\n", "def test_logistic_regression_path_l1_ratio():\n    # Test that logistic_regression_path correctly uses l1_ratio parameter for elasticnet penalty\n    X, y = make_classification(n_samples=100, n_features=10, n_classes=2, random_state=0)\n    \n    # Define a range of C values\n    Cs = [0.01, 0.1, 1, 10]\n\n    # Define a l1_ratio value\n    l1_ratio = 0.5\n\n    # Compute logistic regression path with elasticnet penalty\n    coefs, _, _ = _logistic_regression_path(X, y, Cs=Cs, penalty='elasticnet', l1_ratio=l1_ratio, solver='saga', random_state=0)\n\n    # Ensure that the coefficients are not all zeros\n    for coef in coefs:\n        assert not np.allclose(coef, 0), \"Coefficients should not be all zeros when using elasticnet penalty.\"\n\n    # Compare with l1 and l2 penalties for reference\n    coefs_l1, _, _ = _logistic_regression_path(X, y, Cs=Cs, penalty='l1', solver='saga', random_state=0)\n    coefs_l2, _, _ = _logistic_regression_path(X, y, Cs=Cs, penalty='l2', solver='saga', random_state=0)\n\n    # Check that the coefficients with elasticnet penalty are different from those with pure l1 or l2 penalties\n    for coef, coef_l1, coef_l2 in zip(coefs, coefs_l1, coefs_l2):\n        assert not np.allclose(coef, coef_l1), \"Elasticnet coefficients should differ from L1 coefficients.\"\n        assert not np.allclose(coef, coef_l2), \"Elasticnet coefficients should differ from L2 coefficients.\"\n", "def test_logistic_regression_path_multinomial_solver():\n    # Test logistic_regression_path with multinomial solver\n    X, y = make_classification(n_samples=100, n_classes=3, n_informative=5,\n                               random_state=0)\n    Cs = [0.1, 1, 10]\n    \n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        coefs, Cs, n_iter = _logistic_regression_path(X, y, Cs=Cs,\n                                                      fit_intercept=True,\n                                                      solver=solver,\n                                                      multi_class='multinomial',\n                                                      tol=1e-5,\n                                                      random_state=0,\n                                                      max_iter=100)\n        assert len(coefs) == len(Cs)\n        assert len(n_iter) == len(Cs)\n        \n        for coef in coefs:\n            assert coef.shape == (3, X.shape[1] + 1)\n", "def test_logistic_regression_intercept_dot():\n    # Test _intercept_dot function for correct results with and without intercept\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, -1, 1])\n    w = np.array([0.5, -0.5, 1.0])  # last element is the intercept\n    w_no_intercept = np.array([0.5, -0.5])\n    \n    # With intercept\n    coef_, intercept_, yz_ = _intercept_dot(w, X, y)\n    assert_array_almost_equal(coef_, np.array([0.5, -0.5]))\n    assert_almost_equal(intercept_, 1.0)\n    assert_array_almost_equal(yz_, y * (X.dot(coef_) + intercept_))\n\n    # Without intercept\n    coef_, intercept_, yz_ = _intercept_dot(w_no_intercept, X, y)\n    assert_array_almost_equal(coef_, np.array([0.5, -0.5]))\n    assert_almost_equal(intercept_, 0.0)\n    assert_array_almost_equal(yz_, y * (X.dot(coef_)))\n"], "sample_834": ["def test_transform_before_fit():\n    \"\"\"Test that calling transform before fit raises NotFittedError.\"\"\"\n    X = iris_data\n    nca = NeighborhoodComponentsAnalysis()\n    with pytest.raises(NotFittedError, match=\"This NeighborhoodComponentsAnalysis instance is not fitted yet.\"):\n        nca.transform(X)\n", "def test_nca_transform_unfitted():\n    \"\"\"Test that transform raises an error if called before fit.\"\"\"\n    X = iris_data\n    nca = NeighborhoodComponentsAnalysis()\n    with pytest.raises(ValueError, match=\"This NeighborhoodComponentsAnalysis instance is not fitted yet\"):\n        nca.transform(X)\n", "def test_random_state_consistency():\n    \"\"\"Test that random_state produces consistent results\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n    \n    nca1 = NeighborhoodComponentsAnalysis(init='random', random_state=0)\n    nca1.fit(X, y)\n    transformation1 = nca1.components_\n\n    nca2 = NeighborhoodComponentsAnalysis(init='random', random_state=0)\n    nca2.fit(X, y)\n    transformation2 = nca2.components_\n\n    assert_array_almost_equal(transformation1, transformation2, decimal=6)\n\n    # Check different random_state gives different results\n    nca3 = NeighborhoodComponentsAnalysis(init='random', random_state=1)\n    nca3.fit(X, y)\n    transformation3 = nca3.components_\n\n    with pytest.raises(AssertionError):\n        assert_array_almost_equal(transformation1, transformation3, decimal=6)\n", "def test_transform_shape():\n    \"\"\"Test that the transform function returns the correct shape based on the number of components.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3, random_state=42)\n    \n    # Test with n_components less than n_features\n    n_components = 5\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components)\n    nca.fit(X, y)\n    X_transformed = nca.transform(X)\n    assert X_transformed.shape == (X.shape[0], n_components)\n    \n    # Test with n_components equal to n_features\n    n_components = X.shape[1]\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components)\n    nca.fit(X, y)\n    X_transformed = nca.transform(X)\n    assert X_transformed.shape == (X.shape[0], n_components)\n    \n    # Test with n_components set to None (default)\n    nca = NeighborhoodComponentsAnalysis(n_components=None)\n    nca.fit(X, y)\n    X_transformed = nca.transform(X)\n    assert X_transformed.shape == (X.shape[0], X.shape[1])\n", "def test_transform_before_fit():\n    \"\"\"Test that transform raises an error if called before fit.\"\"\"\n    X = iris_data\n    nca = NeighborhoodComponentsAnalysis()\n    with pytest.raises(ValueError, match=\"This NeighborhoodComponentsAnalysis instance is not fitted yet.\"):\n        nca.transform(X)\n\n"], "sample_835": ["def test_adaboost_classifier_weights_update():\n    # Test the weights update mechanism of AdaBoostClassifier.\n    X_weights = np.array([[0, 0], [1, 1], [1, -1], [2, 2], [3, 3]])\n    y_weights = np.array([0, 1, 1, 0, 0])\n    \n    # Set a weak classifier with a depth of 1\n    weak_clf = DecisionTreeClassifier(max_depth=1)\n    \n    clf = AdaBoostClassifier(base_estimator=weak_clf, n_estimators=3, algorithm='SAMME', random_state=42)\n    clf.fit(X_weights, y_weights)\n    \n    # Check if the weights sum to 1\n    for est_weight in clf.estimator_weights_:\n        assert est_weight > 0\n    \n    # Check if the weights are updated correctly\n    assert len(clf.estimator_weights_) == 3\n    assert len(clf.estimator_errors_) == 3\n    assert len(clf.estimators_) == 3\n\n    # Ensure that the classifier produces consistent predictions\n    predictions = clf.predict(X_weights)\n    assert len(predictions) == len(y_weights)\n", "def test_adaboost_classifier_with_different_base_estimators():\n    \"\"\"\n    Test AdaBoostClassifier with different types of base estimators.\n    \"\"\"\n\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.naive_bayes import GaussianNB\n\n    base_estimators = [\n        DecisionTreeClassifier(max_depth=2),\n        SVC(probability=True, kernel='linear'),\n        LogisticRegression(),\n        GaussianNB()\n    ]\n\n    for base_estimator in base_estimators:\n        clf = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=10, random_state=0)\n        clf.fit(iris.data, iris.target)\n        predictions = clf.predict(iris.data)\n        score = clf.score(iris.data, iris.target)\n\n        assert score > 0.8, f\"Failed with base estimator {base_estimator.__class__.__name__} and score = {score}\"\n        assert_array_equal(np.unique(predictions), np.unique(iris.target))\n        assert len(clf.estimators_) == 10\n", "def test_adaboost_classifier_with_different_base_estimators():\n    # Test AdaBoostClassifier with different base estimators\n    base_estimators = [\n        DecisionTreeClassifier(max_depth=2),\n        SVC(kernel='linear', probability=True),\n        SVC(kernel='rbf', probability=True),\n        DecisionTreeClassifier(max_depth=5),\n    ]\n    for base_estimator in base_estimators:\n        clf = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=10, random_state=0)\n        clf.fit(X, y_class)\n        predictions = clf.predict(T)\n        assert len(predictions) == len(y_t_class)\n        proba = clf.predict_proba(T)\n        assert proba.shape == (len(T), len(clf.classes_))\n        decision = clf.decision_function(T)\n        assert decision.shape == (len(T), len(clf.classes_))\n        assert_array_equal(np.unique(predictions), clf.classes_)\n", "def test_fit_with_different_learning_rates():\n    # Check if AdaBoostClassifier and AdaBoostRegressor handle different learning rates correctly\n    for learning_rate in [0.1, 0.5, 1.0, 1.5, 2.0]:\n        clf = AdaBoostClassifier(learning_rate=learning_rate, random_state=0)\n        clf.fit(X, y_class)\n        assert clf.learning_rate == learning_rate\n        assert len(clf.estimators_) > 0  # Check that estimators were created\n\n        reg = AdaBoostRegressor(learning_rate=learning_rate, random_state=0)\n        reg.fit(X, y_regr)\n        assert reg.learning_rate == learning_rate\n        assert len(reg.estimators_) > 0  # Check that estimators were created\n", "def test_estimator_weights_errors():\n    # Test the length and values of estimator_weights_ and estimator_errors_.\n    # This will also check edge cases when the fit should terminate early.\n\n    # Case 1: Perfect fit early termination\n    X = [[1], [2], [3], [4]]\n    y = [0, 1, 0, 1]\n    clf = AdaBoostClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y)\n    assert len(clf.estimator_weights_) <= 10\n    assert len(clf.estimator_errors_) <= 10\n    assert clf.estimator_weights_[-1] == 1.0\n    assert clf.estimator_errors_[-1] == 0.0\n\n    # Case 2: Non-positive weighted sum of samples\n    sample_weight = [0.5, 0.5, 0, 0]\n    clf = AdaBoostClassifier(n_estimators=10, random_state=0)\n    with pytest.raises(ValueError, match=\"Attempting to fit with a non-positive\"):\n        clf.fit(X, y, sample_weight=sample_weight)\n\n    # Case 3: Early termination for AdaBoostRegressor\n    X = [[1], [2], [3], [4]]\n    y = [1, 2, 3, 4]\n    reg = AdaBoostRegressor(n_estimators=10, random_state=0)\n    reg.fit(X, y)\n    assert len(reg.estimator_weights_) <= 10\n    assert len(reg.estimator_errors_) <= 10\n    assert reg.estimator_weights_[-1] == 1.0\n    assert reg.estimator_errors_[-1] == 0.0\n"], "sample_836": ["def test_check_partial_fit_first_call():\n    class MockClassifier:\n            self.classes_ = None\n\n    clf = MockClassifier()\n\n    # Initial call with classes\n    assert _check_partial_fit_first_call(clf, classes=[0, 1, 2])\n    assert_array_equal(clf.classes_, [0, 1, 2])\n\n    # Subsequent call with the same classes\n    assert not _check_partial_fit_first_call(clf, classes=[0, 1, 2])\n\n    # Subsequent call with different classes should raise an error\n    with pytest.raises(ValueError, match=\"`classes=\\\\[0, 1, 3\\\\]` is not the same as on last call to partial_fit, was: \\\\[0, 1, 2\\\\]\"):\n        _check_partial_fit_first_call(clf, classes=[0, 1, 3])\n\n    # Subsequent call without classes\n    assert not _check_partial_fit_first_call(clf)\n", "def test_check_classification_targets_multilabel_sequences():\n    for example in MULTILABEL_SEQUENCES:\n        msg = 'Unknown label type: '\n        assert_raises_regex(ValueError, msg, check_classification_targets, example)\n", "def test_is_multilabel_with_sparse_inputs():\n    # Test is_multilabel with various sparse input formats\n    y_dense = np.array([[1, 0, 0], [0, 1, 1], [1, 1, 0]])\n    y_csr = csr_matrix(y_dense)\n    y_csc = csc_matrix(y_dense)\n    y_coo = coo_matrix(y_dense)\n    y_dok = dok_matrix(y_dense)\n    y_lil = lil_matrix(y_dense)\n\n    sparse_formats = [y_csr, y_csc, y_coo, y_dok, y_lil]\n\n    for y_sparse in sparse_formats:\n        assert is_multilabel(y_sparse), (\n            'is_multilabel(%r) should be True' % y_sparse)\n\n    # Test with sparse formats that should not be multilabel\n    y_dense_invalid = np.array([[1], [0], [1]])\n    y_csr_invalid = csr_matrix(y_dense_invalid)\n    y_csc_invalid = csc_matrix(y_dense_invalid)\n    y_coo_invalid = coo_matrix(y_dense_invalid)\n    y_dok_invalid = dok_matrix(y_dense_invalid)\n    y_lil_invalid = lil_matrix(y_dense_invalid)\n\n    sparse_formats_invalid = [y_csr_invalid, y_csc_invalid, y_coo_invalid, y_dok_invalid, y_lil_invalid]\n\n    for y_sparse_invalid in sparse_formats_invalid:\n        assert not is_multilabel(y_sparse_invalid), (\n            'is_multilabel(%r) should be False' % y_sparse_invalid)\n", "def test_is_integral_float():\n    # Check that _is_integral_float works as expected\n\n    # Integer values but in float type\n    assert _is_integral_float(np.array([1.0, 2.0, 3.0]))\n    assert _is_integral_float(np.array([1.0, 0.0, -1.0]))\n\n    # Not all integers in float type\n    assert not _is_integral_float(np.array([1.1, 2.0, 3.0]))\n    assert not _is_integral_float(np.array([1.0, 2.5, 3.0]))\n\n    # Integer values in integer type should not be considered integral floats\n    assert not _is_integral_float(np.array([1, 2, 3]))\n\n    # Edge cases with empty array and mixed types\n    assert _is_integral_float(np.array([]))\n    assert not _is_integral_float(np.array([1.0, \"2.0\"]))\n", "def test_check_classification_targets_invalid():\n    # Test check_classification_targets with invalid target types\n    invalid_targets = EXAMPLES[\"unknown\"] + EXAMPLES[\"continuous\"] + EXAMPLES[\"continuous-multioutput\"]\n    for target in invalid_targets:\n        with pytest.raises(ValueError, match=\"Unknown label type:\"):\n            check_classification_targets(target)\n"], "sample_837": ["def test_get_blas_info():\n    from sklearn.utils._show_versions import _get_blas_info\n\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info(monkeypatch):\n    from sklearn.utils._show_versions import _get_blas_info\n    from sklearn._build_utils import get_blas_info\n\n        return ['mock_cblas_lib'], {'define_macros': [('HAVE_CBLAS', '1')], 'library_dirs': ['/mock/path']}\n\n    monkeypatch.setattr(get_blas_info, \"get_blas_info\", mock_get_blas_info)\n    \n    blas_info = _get_blas_info()\n    \n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n    assert blas_info['macros'] == 'HAVE_CBLAS=1'\n    assert blas_info['lib_dirs'] == '/mock/path'\n    assert blas_info['cblas_libs'] == 'mock_cblas_lib'\n", "def test_get_blas_info(mocker):\n    from sklearn.utils._show_versions import _get_blas_info\n    mock_get_blas_info = mocker.patch('sklearn.utils._show_versions.get_blas_info', return_value=(['libcblas'], {'define_macros': [('HAVE_CBLAS', '1')], 'library_dirs': ['/usr/local/lib']}))\n\n    blas_info = _get_blas_info()\n    \n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n    assert blas_info['macros'] == 'HAVE_CBLAS=1'\n    assert blas_info['lib_dirs'] == '/usr/local/lib'\n    assert blas_info['cblas_libs'] == 'libcblas'\n", "def test_get_blas_info(mocker):\n    mocker.patch(\"sklearn.utils._show_versions.get_blas_info\", return_value=(['libcblas'], {'define_macros': [('HAVE_CBLAS', '1')], 'library_dirs': ['/usr/local/lib']}))\n    from sklearn.utils._show_versions import _get_blas_info\n    \n    blas_info = _get_blas_info()\n    \n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n    assert blas_info['macros'] == 'HAVE_CBLAS=1'\n    assert blas_info['lib_dirs'] == '/usr/local/lib'\n    assert blas_info['cblas_libs'] == 'libcblas'\n", "def test_get_blas_info(mocker):\n    from sklearn.utils._show_versions import _get_blas_info\n    from sklearn._build_utils import get_blas_info\n\n    mock_blas_info = (\n        ['libcblas.so', 'libatlas.so'], \n        {'define_macros': [('HAVE_CBLAS', '1')], 'library_dirs': ['/usr/local/lib']}\n    )\n\n    mocker.patch('sklearn.utils._show_versions.get_blas_info', return_value=mock_blas_info)\n\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n    assert 'HAVE_CBLAS=1' in blas_info['macros']\n    assert '/usr/local/lib' in blas_info['lib_dirs']\n    assert 'libcblas.so' in blas_info['cblas_libs']\n"], "sample_838": ["def test_column_transformer_with_drop_and_passthrough():\n    X_array = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]).T\n\n    # Case with drop\n    ct_drop = ColumnTransformer([\n        ('trans1', Trans(), [0]),\n        ('trans2', 'drop', [1])\n    ], remainder='passthrough')\n    \n    result = ct_drop.fit_transform(X_array)\n    expected = np.array([[0, 2],\n                         [3, 5],\n                         [6, 8]])\n    \n    assert_array_equal(result, expected)\n    assert len(ct_drop.transformers_) == 3  # including remainder\n    assert ct_drop.transformers_[-1][0] == 'remainder'\n    assert ct_drop.transformers_[-1][1] == 'passthrough'\n    assert_array_equal(ct_drop.transformers_[-1][2], [2])\n\n    # Case with passthrough\n    ct_passthrough = ColumnTransformer([\n        ('trans1', Trans(), [0]),\n        ('trans2', 'passthrough', [1])\n    ], remainder='passthrough')\n\n    result = ct_passthrough.fit_transform(X_array)\n    expected = np.array([[0, 1, 2],\n                         [3, 4, 5],\n                         [6, 7, 8]])\n\n    assert_array_equal(result, expected)\n    assert len(ct_passthrough.transformers_) == 2\n    assert ct_passthrough.transformers_[-1][0] != 'remainder'\n", "def test_column_transformer_callable_column_return():\n    # Test case where the callable returns columns in different formats\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n        return [0, 1]\n\n        return slice(0, 2)\n\n        return np.array([True, True, False])\n\n        return np.array([0, 1])\n\n    for func in [return_list, return_slice, return_bool_mask, return_numpy_array]:\n        ct = ColumnTransformer([('trans', Trans(), func)], remainder='drop')\n        X_res = X_array[:, :2]\n        assert_array_equal(ct.fit_transform(X_array), X_res)\n        assert_array_equal(ct.fit(X_array).transform(X_array), X_res)\n        assert callable(ct.transformers[0][2])\n        assert not callable(ct.transformers_[0][2])\n", "def test_column_transformer_with_callable_remainder():\n    pd = pytest.importorskip('pandas')\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n\n        return X * 2\n\n    class RemainderTransformer(BaseEstimator):\n            return self\n\n            return remainder_func(X)\n\n    # Test with callable remainder that doubles the remaining columns\n    ct = ColumnTransformer([('trans', Trans(), ['first'])],\n                           remainder=RemainderTransformer())\n    \n    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_remainder = np.array([4, 8, 12]).reshape(-1, 1)\n    X_expected = np.hstack((X_res_first, X_res_remainder))\n\n    assert_array_equal(ct.fit_transform(X_df), X_expected)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_expected)\n    assert len(ct.transformers_) == 2  # including remainder\n    assert isinstance(ct.transformers_[-1][1], RemainderTransformer)\n", "def test_column_transformer_invalid_remainder_estimator():\n    # Ensure invalid estimators in the remainder raise an error\n    class NoTransform(BaseEstimator):\n            return self\n\n            return X\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # Invalid remainder\n    ct = ColumnTransformer([('trans', StandardScaler(), [0])],\n                           remainder=NoTransform())\n    assert_raise_message(ValueError, \"The remainder keyword needs to be one of 'drop', 'passthrough', or estimator.\",\n                         ct.fit, X_array)\n    assert_raise_message(ValueError, \"The remainder keyword needs to be one of 'drop', 'passthrough', or estimator.\",\n                         ct.fit_transform, X_array)\n", "def test_column_transformer_get_feature_names_remainder():\n    X = np.array([[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}],\n                  [{'c': 5}, {'c': 6}]], dtype=object).T\n    \n    # Test when remainder is 'drop'\n    ct = ColumnTransformer(\n        [('col0', DictVectorizer(), 0), ('col1', DictVectorizer(), 1)],\n        remainder='drop'\n    )\n    ct.fit(X)\n    assert ct.get_feature_names() == ['col0__a', 'col0__b', 'col1__c']\n\n    # Test when remainder is a transformer\n    ct = ColumnTransformer(\n        [('col0', DictVectorizer(), 0)], remainder=DictVectorizer()\n    )\n    ct.fit(X)\n    assert ct.get_feature_names() == ['col0__a', 'col0__b', 'remainder__c']\n\n    # Test error when remainder is 'passthrough'\n    ct = ColumnTransformer(\n        [('col0', DictVectorizer(), 0)], remainder='passthrough'\n    )\n    ct.fit(X)\n    assert_raise_message(\n        NotImplementedError, 'get_feature_names is not yet supported',\n        ct.get_feature_names\n    )\n"], "sample_839": ["def test_strip_tags():\n    # Test basic HTML tag stripping\n    text = \"<b>bold</b> and <i>italic</i> text\"\n    expected = \" bold  and  italic  text\"\n    assert strip_tags(text) == expected\n\n    # Test nested tags\n    text = \"<div><p>Paragraph with <a href='#'>link</a></p></div>\"\n    expected = \" Paragraph with  link \"\n    assert strip_tags(text) == expected\n\n    # Test unclosed tags\n    text = \"Unclosed <b>bold tag\"\n    expected = \"Unclosed  bold tag\"\n    assert strip_tags(text) == expected\n\n    # Test tag with attributes\n    text = \"<img src='image.jpg' alt='image'>\"\n    expected = \" \"\n    assert strip_tags(text) == expected\n\n    # Test empty string\n    text = \"\"\n    expected = \"\"\n    assert strip_tags(text) == expected\n\n    # Test no tags\n    text = \"No tags here\"\n    expected = \"No tags here\"\n    assert strip_tags(text) == expected\n", "def test_vectorizer_empty_docs():\n    # Ensure that vectorizers handle empty documents correctly\n\n    empty_docs = [\"\", \"   \", \"\\n\", \"\\t\"]\n\n    for Vectorizer in [CountVectorizer, TfidfVectorizer, HashingVectorizer]:\n        vec = Vectorizer()\n\n        # fit_transform should handle empty docs gracefully\n        X = vec.fit_transform(empty_docs)\n        assert X.shape[0] == len(empty_docs)\n\n        if hasattr(vec, 'vocabulary_'):\n            assert len(vec.vocabulary_) == 0\n\n        # transform on empty docs should return the correct shape\n        X_transformed = vec.transform(empty_docs)\n        assert X_transformed.shape[0] == len(empty_docs)\n        assert X_transformed.shape[1] == X.shape[1]\n", "def test_tfidf_vectorizer_with_custom_tokenizer():\n    # Test TfidfVectorizer with a custom tokenizer function\n        return re.findall(r'\\b\\w\\w+\\b', text.lower())\n\n    vect = TfidfVectorizer(tokenizer=custom_tokenizer, norm=None)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n    feature_names = vect.get_feature_names_out()\n    \n    assert 'pizza' in feature_names\n    assert 'beer' in feature_names\n    assert 'celeri' in feature_names\n\n    # Check that the custom tokenizer tokenizes correctly\n    expected_features = [\n        'beer', 'burger', 'celeri', 'coke', 'copyright', 'document', \n        'first', 'is', 'one', 'pizza', 'salad', 'second', 'sparkling', \n        'the', 'third', 'this', 'tomato', 'water'\n    ]\n    assert sorted(feature_names) == sorted(expected_features)\n\n    # Check tf-idf values for a specific document\n    doc_tfidf = X[0].toarray()[0]\n    assert doc_tfidf[vect.vocabulary_['pizza']] > 0\n    assert doc_tfidf[vect.vocabulary_['beer']] > 0\n    assert doc_tfidf[vect.vocabulary_['copyright']] > 0\n    assert doc_tfidf[vect.vocabulary_['salad']] == 0\n", "def test_hashingvectorizer_different_ngram_range():\n    # Test HashingVectorizer with different ngram ranges\n    data = [\"the quick brown fox\", \"jumps over the lazy dog\"]\n    \n    # Unigram\n    vectorizer = HashingVectorizer(ngram_range=(1, 1), alternate_sign=False, norm=None)\n    X_unigram = vectorizer.fit_transform(data).toarray()\n    assert X_unigram.shape[1] == vectorizer.n_features\n    assert np.sum(X_unigram) == 9  # 9 unique words in the data\n    \n    # Bigram\n    vectorizer = HashingVectorizer(ngram_range=(2, 2), alternate_sign=False, norm=None)\n    X_bigram = vectorizer.fit_transform(data).toarray()\n    assert X_bigram.shape[1] == vectorizer.n_features\n    assert np.sum(X_bigram) == 7  # 7 unique bigrams in the data\n    \n    # Trigram\n    vectorizer = HashingVectorizer(ngram_range=(3, 3), alternate_sign=False, norm=None)\n    X_trigram = vectorizer.fit_transform(data).toarray()\n    assert X_trigram.shape[1] == vectorizer.n_features\n    assert np.sum(X_trigram) == 5  # 5 unique trigrams in the data\n", "def test_vectorizer_ngram_range():\n    # Test the ngram_range parameter for different vectorizers\n    corpus = [\"This is a test\", \"Another test\"]\n\n    for Vectorizer in [CountVectorizer, TfidfVectorizer]:\n        # Unigrams\n        vectorizer = Vectorizer(ngram_range=(1, 1))\n        X = vectorizer.fit_transform(corpus)\n        feature_names = vectorizer.get_feature_names_out()\n        assert_array_equal(feature_names, ['another', 'is', 'test', 'this'])\n\n        # Bigrams\n        vectorizer = Vectorizer(ngram_range=(1, 2))\n        X = vectorizer.fit_transform(corpus)\n        feature_names = vectorizer.get_feature_names_out()\n        expected_features = [\n            'another', 'another test', 'is', 'is test', 'test', 'this', 'this is'\n        ]\n        assert_array_equal(feature_names, expected_features)\n\n        # Trigrams\n        vectorizer = Vectorizer(ngram_range=(1, 3))\n        X = vectorizer.fit_transform(corpus)\n        feature_names = vectorizer.get_feature_names_out()\n        expected_features = [\n            'another', 'another test', 'is', 'is test', 'test', 'this', 'this is', 'this is test'\n        ]\n        assert_array_equal(feature_names, expected_features)\n"], "sample_840": ["def test_pls_custom_params():\n    # Test PLS with custom parameters\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    # Initialize PLSCanonical with custom parameters\n    plsca = pls_.PLSCanonical(n_components=2, scale=False, algorithm=\"nipals\", max_iter=300, tol=1e-4, copy=False)\n    plsca.fit(X, Y)\n    X_scores, Y_scores = plsca.transform(X, Y)\n\n    # Check that the number of components is as specified\n    assert plsca.n_components == 2\n\n    # Ensure that the algorithm used is \"nipals\"\n    assert plsca.algorithm == \"nipals\"\n\n    # Check that the tolerance and max_iter are set correctly\n    assert plsca.tol == 1e-4\n    assert plsca.max_iter == 300\n\n    # Check that scaling is disabled\n    assert not plsca.scale\n\n    # Check that x_scores_ and y_scores_ have the correct shapes\n    assert X_scores.shape == (X.shape[0], 2)\n    assert Y_scores.shape == (Y.shape[0], 2)\n\n    # Ensure results are consistent when fitting again with the same parameters\n    plsca.fit(X, Y)\n    X_scores_re, Y_scores_re = plsca.transform(X, Y)\n    assert_array_almost_equal(X_scores, X_scores_re)\n    assert_array_almost_equal(Y_scores, Y_scores_re)\n", "def test_pls_predict():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    # Test PLSRegression\n    pls = pls_.PLSRegression(n_components=2)\n    pls.fit(X, Y)\n    Y_pred = pls.predict(X)\n    assert Y_pred.shape == Y.shape, \"Prediction shape mismatch for PLSRegression\"\n\n    # Test PLSCanonical\n    pls_ca = pls_.PLSCanonical(n_components=2)\n    pls_ca.fit(X, Y)\n    Y_pred_ca = pls_ca.predict(X)\n    assert Y_pred_ca.shape == Y.shape, \"Prediction shape mismatch for PLSCanonical\"\n    \n    # Test PLSSVD\n    pls_svd = pls_.PLSSVD(n_components=2)\n    pls_svd.fit(X, Y)\n    Y_pred_svd = pls_svd.transform(X)\n    assert Y_pred_svd.shape == pls_svd.x_scores_.shape, \"Prediction shape mismatch for PLSSVD\"\n", "def test_center_scale_xy():\n    # Test the _center_scale_xy function separately\n    X = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])\n    Y = np.array([[1., 2.], [3., 4.], [5., 6.]])\n\n    X_c, Y_c, x_mean, y_mean, x_std, y_std = pls_._center_scale_xy(X, Y, scale=True)\n    \n    assert_array_almost_equal(X_c.mean(axis=0), 0, err_msg=\"X not centered correctly\")\n    assert_array_almost_equal(Y_c.mean(axis=0), 0, err_msg=\"Y not centered correctly\")\n    assert_array_almost_equal(X_c.std(axis=0, ddof=1), 1, err_msg=\"X not scaled correctly\")\n    assert_array_almost_equal(Y_c.std(axis=0, ddof=1), 1, err_msg=\"Y not scaled correctly\")\n\n    X_c, Y_c, x_mean, y_mean, x_std, y_std = pls_._center_scale_xy(X, Y, scale=False)\n    \n    assert_array_almost_equal(X_c.mean(axis=0), 0, err_msg=\"X not centered correctly\")\n    assert_array_almost_equal(Y_c.mean(axis=0), 0, err_msg=\"Y not centered correctly\")\n    assert_array_almost_equal(X_c.std(axis=0, ddof=1), X.std(axis=0, ddof=1), err_msg=\"X should not be scaled\")\n    assert_array_almost_equal(Y_c.std(axis=0, ddof=1), Y.std(axis=0, ddof=1), err_msg=\"Y should not be scaled\")\n", "def test_pls_transform():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls = pls_.PLSRegression(n_components=2)\n    pls.fit(X, Y)\n    X_transformed = pls.transform(X)\n    Y_transformed = pls.transform(X, Y)\n\n    assert X_transformed.shape == (X.shape[0], 2), \"X transformed shape mismatch\"\n    assert Y_transformed[0].shape == (X.shape[0], 2), \"X transformed shape mismatch when Y is provided\"\n    assert Y_transformed[1].shape == (Y.shape[0], 2), \"Y transformed shape mismatch\"\n", "def test_pls_transform_no_y():\n    # Test the transform method with X only, without providing Y\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    pls = pls_.PLSCanonical(n_components=2)\n    pls.fit(X, Y)\n    X_transformed = pls.transform(X)\n    \n    # Check the shape of the transformed X\n    assert X_transformed.shape == (X.shape[0], 2)\n    \n    # Check that the transformed X has the same values as the x_scores_\n    assert_array_almost_equal(X_transformed, pls.x_scores_)\n"], "sample_841": ["def test_ridge_regression_with_different_parameters(solver, alpha, fit_intercept, normalize):\n    # This test checks Ridge regression with different combinations of parameters\n    X, y = make_regression(n_samples=100, n_features=20, noise=0.1, random_state=42)\n    ridge = Ridge(solver=solver, alpha=alpha, fit_intercept=fit_intercept, normalize=normalize)\n    ridge.fit(X, y)\n    assert ridge.coef_.shape == (X.shape[1],)\n    if fit_intercept:\n        assert isinstance(ridge.intercept_, np.float64)\n    else:\n        assert ridge.intercept_ == 0.0\n    assert ridge.score(X, y) > 0.8  # Check that the model fits reasonably well\n", "def test_ridge_regression_solver_return_n_iter(solver, X_type, return_n_iter):\n    # Test if ridge_regression returns n_iter correctly for different solvers and input types\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    X = X_type(X)\n    \n    alpha = 1.0\n    tol = 1e-6\n\n    result = ridge_regression(X, y, alpha=alpha, solver=solver, return_n_iter=return_n_iter, tol=tol)\n    \n    if return_n_iter:\n        coef, n_iter = result\n        assert n_iter is not None\n        assert isinstance(n_iter, int)\n    else:\n        coef = result\n\n    assert coef is not None\n    assert coef.shape == (n_features,)\n", "def test_ridge_regression_svd_solver():\n    # Test ridge regression with SVD solver and both dense and sparse matrices\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features = 10, 8\n    y = rng.randn(n_samples)\n    X_dense = rng.randn(n_samples, n_features)\n    X_sparse = sp.csr_matrix(X_dense)\n\n    # Test with dense matrix\n    ridge_dense = Ridge(alpha=alpha, solver='svd')\n    ridge_dense.fit(X_dense, y)\n    assert ridge_dense.coef_.shape == (X_dense.shape[1],)\n    assert ridge_dense.score(X_dense, y) > 0.5\n\n    # Test with sparse matrix - should raise a TypeError\n    ridge_sparse = Ridge(alpha=alpha, solver='svd')\n    with pytest.raises(TypeError, match='SVD solver does not support sparse'):\n        ridge_sparse.fit(X_sparse, y)\n", "def test_ridge_intercept_with_sparse_input():\n    # Test Ridge regression with intercept and sparse input\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 10, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    X_sparse = sp.csr_matrix(X)\n\n    ridge_dense = Ridge(fit_intercept=True, solver='sparse_cg')\n    ridge_sparse = Ridge(fit_intercept=True, solver='sparse_cg')\n\n    ridge_dense.fit(X, y)\n    ridge_sparse.fit(X_sparse, y)\n\n    assert np.allclose(ridge_dense.coef_, ridge_sparse.coef_, rtol=1e-4)\n    assert np.allclose(ridge_dense.intercept_, ridge_sparse.intercept_, rtol=1e-4)\n\n    # Test error with non-sparse_cg solver\n    ridge_error = Ridge(fit_intercept=True, solver='cholesky')\n    with pytest.raises(ValueError, match=\"solver='cholesky' does not support fitting the intercept on sparse data\"):\n        ridge_error.fit(X_sparse, y)\n", "def test_ridge_regression_invalid_input():\n    # Test invalid input scenarios for ridge_regression function\n\n    # 1. Negative alpha\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    alpha = -1\n    with pytest.raises(ValueError, match=\"Alpha must be a non-negative float\"):\n        ridge_regression(X, y, alpha=alpha)\n\n    # 2. Mismatched sample sizes between X and y\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2])\n    alpha = 1\n    with pytest.raises(ValueError, match=\"Number of samples in X and y does not correspond\"):\n        ridge_regression(X, y, alpha=alpha)\n\n    # 3. Invalid solver\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    alpha = 1\n    solver = 'invalid_solver'\n    with pytest.raises(ValueError, match=\"Known solvers are\"):\n        ridge_regression(X, y, alpha=alpha, solver=solver)\n\n    # 4. Invalid input type for X\n    X = \"invalid_input\"\n    y = np.array([1, 2])\n    alpha = 1\n    with pytest.raises(TypeError, match=\"Expected array-like (array or non-string sequence), got <class 'str'>\"):\n        ridge_regression(X, y, alpha=alpha)\n\n    # 5. Invalid input type for y\n    X = np.array([[1, 2], [3, 4]])\n    y = \"invalid_input\"\n    alpha = 1\n    with pytest.raises(TypeError, match=\"Expected array-like (array or non-string sequence), got <class 'str'>\"):\n        ridge_regression(X, y, alpha=alpha)\n"], "sample_842": ["def test_estimator_clone(estimator_class):\n    # Test that cloning works for all estimator base and mixin classes.\n    if hasattr(estimator_class, '__init__'):\n        estimator = estimator_class()\n        cloned_estimator = clone(estimator)\n        assert cloned_estimator.__class__ == estimator.__class__\n        assert cloned_estimator.get_params() == estimator.get_params()\n", "def test_base_estimator_get_params():\n    class MyEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n    est = MyEstimator(param1=5, param2=10)\n    params = est.get_params()\n    assert params['param1'] == 5\n    assert params['param2'] == 10\n", "def test_kernel_set_params_invalid(kernel):\n    # Test that set_params raises ValueError on invalid parameter names.\n    invalid_params = {'invalid_param': 0.1}\n    with pytest.raises(ValueError, match=\"Invalid parameter\"):\n        kernel.set_params(**invalid_params)\n\n    # Test that set_params raises ValueError on invalid parameter values.\n    valid_params = kernel.get_params()\n    for param in valid_params:\n        invalid_params = {param: 'invalid_value'}\n        with pytest.raises(TypeError):\n            kernel.set_params(**invalid_params)\n", "def test_estimator_get_params(EstimatorClass):\n    # Test that get_params works correctly for different estimators\n    est = EstimatorClass()\n    params = est.get_params()\n    assert isinstance(params, dict)\n    if hasattr(est, '_get_param_names'):\n        param_names = est._get_param_names()\n        assert set(params.keys()) == set(param_names)\n", "def test_get_params_estimator_classes(estimator_class):\n    # Test that get_params works for base estimator classes.\n    estimator = estimator_class()\n    params = estimator.get_params()\n    assert isinstance(params, dict)\n"], "sample_843": ["def test_kernel_bounds(kernel):\n    # Check that kernel bounds are correctly log-transformed and set\n    for hyperparameter in kernel.hyperparameters:\n        if isinstance(hyperparameter.bounds, str):\n            if hyperparameter.bounds == \"fixed\":\n                continue\n        bounds = kernel.bounds\n        index = 0\n        for hp in kernel.hyperparameters:\n            if isinstance(hp.bounds, str) and hp.bounds == \"fixed\":\n                continue\n            size = hp.n_elements\n            if size > 1:  # anisotropic kernels\n                assert_almost_equal(bounds[index:index + size],\n                                    np.log(hp.bounds))\n                index += size\n            else:\n                assert_almost_equal(bounds[index], np.log(hp.bounds[0]))\n                assert_almost_equal(bounds[index + 1], np.log(hp.bounds[1]))\n                index += 1\n\n        # Set bounds to a new value and check if it is reflected correctly\n        new_bounds = (1e-3, 1e3)\n        params = kernel.get_params()\n        params[hyperparameter.name + \"_bounds\"] = new_bounds\n        kernel_class = kernel.__class__\n        new_kernel = kernel_class(**params)\n        assert np.all(new_kernel.bounds == np.log(new_bounds))\n", "def test_bounds_property(kernel):\n    # Test the bounds property of kernels to ensure it returns correct log-transformed bounds.\n\n    bounds = kernel.bounds\n    assert bounds.shape[0] == kernel.theta.shape[0]\n    assert bounds.shape[1] == 2\n\n    for i, hyperparameter in enumerate(kernel.hyperparameters):\n        if not hyperparameter.fixed:\n            assert_almost_equal(bounds[i, :], np.log(hyperparameter.bounds))\n", "def test_kernel_addition(kernel):\n    # Test addition of kernels.\n    kernel_sum = kernel + kernel\n    K = kernel(X)\n    K_sum = kernel_sum(X)\n    assert_almost_equal(2 * K, K_sum)\n\n    # Test addition with a constant\n    kernel_sum_const = kernel + 1.0\n    K_sum_const = kernel_sum_const(X)\n    assert_almost_equal(K + 1.0, K_sum_const)\n", "def test_operator_overloading(kernel):\n    # Test operator overloading for kernel combinations\n    kernel_sum = kernel + ConstantKernel(2.0)\n    kernel_product = kernel * ConstantKernel(2.0)\n    kernel_power = kernel ** 2\n\n    # Ensure the resulting objects are of the correct type\n    assert isinstance(kernel_sum, Sum)\n    assert isinstance(kernel_product, Product)\n    assert isinstance(kernel_power, Exponentiation)\n\n    # Evaluate kernels to ensure they work as expected\n    K_sum = kernel_sum(X)\n    K_product = kernel_product(X)\n    K_power = kernel_power(X)\n\n    # Check shapes to ensure correct computation\n    assert K_sum.shape == (X.shape[0], X.shape[0])\n    assert K_product.shape == (X.shape[0], X.shape[0])\n    assert K_power.shape == (X.shape[0], X.shape[0])\n\n    # Check that the values are sensible (non-negative, not infinite)\n    assert np.all(np.isfinite(K_sum))\n    assert np.all(np.isfinite(K_product))\n    assert np.all(np.isfinite(K_power))\n", "def test_kernel_bounds(kernel):\n    # Check that bounds property of kernels returns correct log-transformed bounds.\n    bounds = kernel.bounds\n    for i, hyperparameter in enumerate(kernel.hyperparameters):\n        if not hyperparameter.fixed:\n            expected_bounds = np.log(hyperparameter.bounds)\n            assert_array_almost_equal(bounds[i], expected_bounds)\n"], "sample_844": ["def test_optics_with_different_metrics():\n    # Test OPTICS with different metrics to ensure it works with each supported metric\n    rng = np.random.RandomState(42)\n    n_points_per_cluster = 10\n    C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)\n    C3 = [1, -2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C4 = [-2, 3] + .3 * rng.randn(n_points_per_cluster, 2)\n    C5 = [3, -2] + 1.6 * rng.randn(n_points_per_cluster, 2)\n    C6 = [5, 6] + 2 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2, C3, C4, C5, C6))\n    \n    metrics = ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']\n    \n    for metric in metrics:\n        clust = OPTICS(min_samples=3, max_eps=5.0, metric=metric).fit(X)\n        # Checking that labels have been computed\n        assert clust.labels_ is not None\n        # Checking that reachability distances have been computed\n        assert clust.reachability_ is not None\n        # Checking that core distances have been computed\n        assert clust.core_distances_ is not None\n        # Checking that ordering has been computed\n        assert clust.ordering_ is not None\n", "def test_incorrect_min_samples_type():\n    # Test that an incorrect type for min_samples raises a ValueError\n    msg = \"min_samples must be an integer greater than 1 or a float between 0 and 1.\"\n\n    X = [[1, 1], [2, 2], [3, 3]]\n    clust = OPTICS(max_eps=5.0, min_samples='ten')\n    \n    assert_raise_message(ValueError, msg, clust.fit, X)\n\n    clust = OPTICS(max_eps=5.0, min_samples=-5)\n    \n    assert_raise_message(ValueError, msg, clust.fit, X)\n\n    clust = OPTICS(max_eps=5.0, min_samples=1.5)\n    \n    assert_raise_message(ValueError, msg, clust.fit, X)\n", "def test_optics_hierarchy():\n    # Test to check if the hierarchy of clusters is properly formed\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 100\n\n    C1 = [0, 0] + 0.1 * rng.randn(n_points_per_cluster, 2)\n    C2 = [5, 5] + 0.1 * rng.randn(n_points_per_cluster, 2)\n    C3 = [10, 10] + 0.1 * rng.randn(n_points_per_cluster, 2)\n    C4 = [15, 15] + 0.1 * rng.randn(n_points_per_cluster, 2)\n\n    X = np.vstack((C1, C2, C3, C4))\n\n    clust = OPTICS(min_samples=5, max_eps=np.inf, cluster_method='xi', xi=0.1).fit(X)\n\n    # Expecting four clusters\n    assert len(clust.cluster_hierarchy_) == 4\n\n    # Check if the clusters are in hierarchical order\n    cluster_sizes = [end - start + 1 for start, end in clust.cluster_hierarchy_]\n    assert all(cluster_sizes[i] <= cluster_sizes[i+1] for i in range(len(cluster_sizes) - 1))\n\n    # Check if the labels reflect the hierarchy\n    labels = clust.labels_\n    unique_labels = np.unique(labels)\n    assert len(unique_labels[unique_labels != -1]) == 4\n", "def test_cluster_hierarchy_complex():\n    # More complex hierarchy test with nested clusters\n    rng = np.random.RandomState(42)\n    n_points_per_cluster = 50\n    C1 = [0, 0] + 0.5 * rng.randn(n_points_per_cluster, 2)\n    C2 = [5, 5] + 0.5 * rng.randn(n_points_per_cluster, 2)\n    C3 = [10, 10] + 0.5 * rng.randn(n_points_per_cluster, 2)\n    C4 = [5, 5] + 2.0 * rng.randn(n_points_per_cluster, 2)  # Larger, nested cluster\n    X = np.vstack((C1, C2, C3, C4))\n    X = shuffle(X, random_state=0)\n\n    clusters = OPTICS(min_samples=10, xi=.1).fit(X).cluster_hierarchy_\n    assert clusters.shape[0] > 2  # Should identify more than 2 clusters due to nesting\n    for start, end in clusters:\n        assert start <= end\n        assert start >= 0\n        assert end < len(X)\n", "def test_metric_callable():\n    # Custom distance metric\n        return np.sqrt(np.sum((x - y) ** 2))\n\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 20\n    C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)\n    C3 = [1, -2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C4 = [-2, 3] + .3 * rng.randn(n_points_per_cluster, 2)\n    C5 = [3, -2] + 1.6 * rng.randn(n_points_per_cluster, 2)\n    C6 = [5, 6] + 2 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2, C3, C4, C5, C6))\n\n    clust = OPTICS(min_samples=5, max_eps=np.inf, metric=custom_metric).fit(X)\n    assert clust.labels_.shape == (len(X),)\n    assert clust.reachability_.shape == (len(X),)\n    assert clust.core_distances_.shape == (len(X),)\n    assert clust.ordering_.shape == (len(X),)\n    assert set(clust.ordering_) == set(range(len(X)))\n"], "sample_845": ["def test_strip_tags():\n    # Ensure that HTML/XML tags are correctly stripped from the input string\n    html_str = \"<b>Bold</b> <i>Italic</i> <u>Underline</u>\"\n    expected_str = \" Bold  Italic  Underline \"\n    assert strip_tags(html_str) == expected_str\n\n    # Test with nested tags\n    html_str_nested = \"<div><p>Paragraph with <a href='#'>link</a></p></div>\"\n    expected_str_nested = \" Paragraph with  link  \"\n    assert strip_tags(html_str_nested) == expected_str_nested\n\n    # Test with no tags\n    no_tags_str = \"Just a plain text string.\"\n    assert strip_tags(no_tags_str) == no_tags_str\n\n    # Test with empty string\n    empty_str = \"\"\n    assert strip_tags(empty_str) == empty_str\n", "def test_strip_tags():\n    text = \"<html><body><p>This is a <b>test</b>.</p></body></html>\"\n    expected = \" This is a  test . \"\n    assert strip_tags(text) == expected\n\n    text = \"No tags in this text.\"\n    expected = \"No tags in this text.\"\n    assert strip_tags(text) == expected\n\n    text = \"<div>Multiple <span>nested</span> tags</div>\"\n    expected = \"Multiple  nested  tags \"\n    assert strip_tags(text) == expected\n", "def test_hashingvectorizer_alternate_sign():\n    # Test the effect of the alternate_sign parameter on the HashingVectorizer\n    data = [\"HashingVectorizer with alternate sign\", \n            \"Another text with alternate sign\", \n            \"One more text without alternate sign\"]\n\n    # Vectorizer with alternate_sign=True (default)\n    hv_true = HashingVectorizer(alternate_sign=True, norm=None)\n    X_true = hv_true.fit_transform(data)\n    assert X_true.shape == (len(data), hv_true.n_features)\n    assert np.min(X_true.data) < 0\n    assert np.max(X_true.data) > 0\n\n    # Vectorizer with alternate_sign=False\n    hv_false = HashingVectorizer(alternate_sign=False, norm=None)\n    X_false = hv_false.fit_transform(data)\n    assert X_false.shape == (len(data), hv_false.n_features)\n    assert np.min(X_false.data) >= 0\n    assert np.max(X_false.data) > 0\n\n    # Ensure that the data values differ\n    assert not np.array_equal(X_true.data, X_false.data)\n", "def test_vectorizer_with_custom_preprocessor_and_tokenizer():\n    # Test the combination of a custom preprocessor and tokenizer\n        return text.replace(\"!\", \"\")\n\n        return text.split()\n\n    vectorizer = CountVectorizer(preprocessor=custom_preprocessor, tokenizer=custom_tokenizer)\n    corpus = [\"Hello world!\", \"This is a test!\", \"Preprocessing and tokenizing!\"]\n    X = vectorizer.fit_transform(corpus)\n    assert X.shape == (3, 5)\n    assert set(vectorizer.get_feature_names()) == {\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\", \"preprocessing\", \"and\", \"tokenizing\"}\n\n    # Ensure the custom preprocessor and tokenizer work with TfidfVectorizer\n    tfidf_vectorizer = TfidfVectorizer(preprocessor=custom_preprocessor, tokenizer=custom_tokenizer)\n    X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n    assert X_tfidf.shape == (3, 5)\n    assert set(tfidf_vectorizer.get_feature_names()) == {\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\", \"preprocessing\", \"and\", \"tokenizing\"}\n", "def test_vectorizer_with_custom_decoder():\n    custom_decoder = lambda x: x.replace(\"\u00e0\", \"a\").replace(\"\u00e9\", \"e\").replace(\"\u00e7\", \"c\")\n    \n    # Test with CountVectorizer\n    vect = CountVectorizer(input='content', decoder=custom_decoder)\n    data = [\"J'ai mang\u00e9 une pomme avec du sucre.\"]\n    X = vect.fit_transform(data)\n    assert \"mange\" in vect.vocabulary_\n    assert \"pomme\" in vect.vocabulary_\n    assert \"sucre\" in vect.vocabulary_\n    \n    # Test with TfidfVectorizer\n    vect = TfidfVectorizer(input='content', decoder=custom_decoder)\n    X = vect.fit_transform(data)\n    assert \"mange\" in vect.vocabulary_\n    assert \"pomme\" in vect.vocabulary_\n    assert \"sucre\" in vect.vocabulary_\n    \n    # Test with HashingVectorizer\n    vect = HashingVectorizer(input='content', decoder=custom_decoder)\n    X = vect.transform(data)\n    assert X.shape[0] == 1  # Ensure transformation was applied\n"], "sample_846": ["def test_column_transformer_callable_column_returns_invalid():\n    # Test case where the callable returns an invalid column specifier\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n        return 'invalid_column'\n\n    ct = ColumnTransformer([('trans', Trans(), invalid_column_callable)],\n                           remainder='drop')\n    assert_raise_message(ValueError, \"Specifying the columns\",\n                         ct.fit_transform, X_array)\n    assert_raise_message(ValueError, \"Specifying the columns\",\n                         ct.fit, X_array)\n", "def test_column_transformer_single_column_dataframe():\n    \"\"\"Test ColumnTransformer with a single column in a pandas DataFrame.\"\"\"\n    pd = pytest.importorskip('pandas')\n\n    X_df = pd.DataFrame({'first': [0, 1, 2]})\n\n    X_res_first = np.array([[0], [1], [2]])\n\n    # Test single column selection with string key\n    ct = ColumnTransformer([('trans', Trans(), 'first')])\n    assert_array_equal(ct.fit_transform(X_df), X_res_first)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert len(ct.transformers_) == 1\n\n    # Test single column selection with positional integer key\n    ct = ColumnTransformer([('trans', Trans(), 0)])\n    assert_array_equal(ct.fit_transform(X_df), X_res_first)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert len(ct.transformers_) == 1\n\n    # Test single column selection with list of string key\n    ct = ColumnTransformer([('trans', Trans(), ['first'])])\n    assert_array_equal(ct.fit_transform(X_df), X_res_first)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert len(ct.transformers_) == 1\n\n    # Test single column selection with list of positional integer key\n    ct = ColumnTransformer([('trans', Trans(), [0])])\n    assert_array_equal(ct.fit_transform(X_df), X_res_first)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert len(ct.transformers_) == 1\n", "def test_column_transformer_transformers_unchanged():\n    # Test that the original transformers are not changed after fitting\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    orig_transformers = [('trans1', StandardScaler(), [0]),\n                         ('trans2', OneHotEncoder(), [1])]\n    ct = ColumnTransformer(orig_transformers)\n    ct.fit(X_array)\n    \n    # Check that the original transformers are unchanged\n    assert orig_transformers == [('trans1', StandardScaler(), [0]),\n                                 ('trans2', OneHotEncoder(), [1])]\n    \n    # Check that the transformers in the ColumnTransformer object are fitted\n    assert hasattr(ct.transformers_[0][1], 'mean_')\n    assert hasattr(ct.transformers_[1][1], 'categories_')\n", "def test_column_transformer_invalid_remainder_transformer():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    \n    # Transformer without fit or transform methods should raise an error\n    class InvalidTransformer(BaseEstimator):\n        pass\n\n    invalid_remainder = InvalidTransformer()\n    \n    ct = ColumnTransformer([('trans', Trans(), [0])], remainder=invalid_remainder)\n    assert_raise_message(TypeError,\n                         \"The remainder keyword needs to be one of 'drop', 'passthrough', or estimator. 'InvalidTransformer' was passed instead\",\n                         ct.fit, X_array)\n    assert_raise_message(TypeError,\n                         \"The remainder keyword needs to be one of 'drop', 'passthrough', or estimator. 'InvalidTransformer' was passed instead\",\n                         ct.fit_transform, X_array)\n", "def test_column_transformer_multiple_transformers():\n    # Test case to check ColumnTransformer with multiple transformers applied\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n\n    # Applying StandardScaler and Normalizer on different columns\n    ct = ColumnTransformer([\n        ('scaler', StandardScaler(), [0]),\n        ('norm', Normalizer(), [1])\n    ], remainder='passthrough')\n\n    # Expected result after transformation\n    scaler = StandardScaler().fit(X_array[:, [0]])\n    norm = Normalizer().fit(X_array[:, [1]])\n    X_scaled = scaler.transform(X_array[:, [0]])\n    X_normalized = norm.transform(X_array[:, [1]])\n    expected_result = np.hstack([X_scaled, X_normalized, X_array[:, [2]]])\n\n    # Check fit_transform and transform\n    assert_array_equal(ct.fit_transform(X_array), expected_result)\n    assert_array_equal(ct.fit(X_array).transform(X_array), expected_result)\n\n    # Check attributes\n    assert len(ct.transformers_) == 3  # 2 transformers + 1 remainder\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'passthrough'\n    assert_array_equal(ct.transformers_[-1][2], [2])\n"], "sample_847": ["def test_lasso_path():\n    # Test lasso_path function on a simple dataset\n    X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    y = np.array([1, 2, 3.1])\n    alphas = [5., 1., .5]\n\n    # Get lasso path coefficients\n    _, coefs, _ = lasso_path(X, y, alphas=alphas, return_n_iter=False)\n    \n    # Expected coefficients calculated using an alternate method\n    expected_coefs = np.array([\n        [0.0, 0.0, 0.46874778],\n        [0.2159048, 0.4425765, 0.23689075]\n    ])\n    \n    assert_array_almost_equal(coefs, expected_coefs)\n", "def test_elasticnet_path():\n    # Test ElasticNet path with a small example\n\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    y = np.array([1, 2, 3])\n    alphas = [0.1, 0.01, 0.001]\n\n    enet_alphas, enet_coefs, enet_dual_gaps = enet_path(X, y, alphas=alphas)\n\n    # Ensure that the alphas are correctly returned\n    assert_array_almost_equal(enet_alphas, alphas)\n\n    # Check that the coefficients have the correct shape\n    assert enet_coefs.shape == (3, len(alphas))\n\n    # Check that dual gaps are correctly computed\n    assert enet_dual_gaps.shape == (len(alphas),)\n    assert np.all(enet_dual_gaps >= 0)\n\n    # Test with positive constraint\n    enet_alphas_pos, enet_coefs_pos, enet_dual_gaps_pos = enet_path(X, y, alphas=alphas, positive=True)\n    assert np.all(enet_coefs_pos >= 0)\n\n    # Check that the results with and without Gram matrix are the same\n    Gram = np.dot(X.T, X)\n    enet_alphas_gram, enet_coefs_gram, enet_dual_gaps_gram = enet_path(X, y, alphas=alphas, precompute=Gram)\n    assert_array_almost_equal(enet_coefs, enet_coefs_gram)\n    assert_array_almost_equal(enet_dual_gaps, enet_dual_gaps_gram)\n", "def test_elasticnet_zero_alpha():\n    # Check that the ElasticNet model behaves as expected with zero alpha\n    X, y, _, _ = build_dataset()\n    clf = ElasticNet(alpha=0.0)\n    with pytest.raises(ValueError, match=\"ElasticNet with alpha=0 is not supported\"):\n        clf.fit(X, y)\n", "def test_enet_path_same_results_with_normalize_and_fit_intercept():\n    # Test enet_path returns the same results with normalize and fit_intercept\n    X, y, _, _ = build_dataset(n_samples=50, n_features=50, n_targets=1)\n    X = X.copy(order='F')\n\n    # Without normalization and intercept fitting\n    alphas, coefs, _ = enet_path(X, y, fit_intercept=False, normalize=False)\n\n    # With normalization and intercept fitting\n    alphas_norm, coefs_norm, _ = enet_path(X, y, fit_intercept=True, normalize=True)\n\n    assert_array_almost_equal(coefs, coefs_norm, decimal=4)\n    assert_array_almost_equal(alphas, alphas_norm, decimal=4)\n", "def test_lasso_path_precompute_array():\n    # Test that lasso_path works with precomputed Gram matrix\n    X, y, _, _ = build_dataset()\n    Gram = np.dot(X.T, X)\n\n    alphas, coefs, _ = lasso_path(X, y, precompute=Gram)\n    assert len(alphas) > 0\n    assert coefs.shape[1] == len(alphas)\n\n    # Check that it raises an error with a bad Gram matrix\n    Gram_bad = Gram[:-1, :-1]\n    assert_raises(ValueError, lasso_path, X, y, precompute=Gram_bad)\n"], "sample_848": ["def test_multi_output_regressor_tags():\n    estimator = MultiOutputRegressor(GradientBoostingRegressor())\n    tags = estimator._more_tags()\n    assert 'multioutput_only' in tags\n    assert tags['multioutput_only'] == True\n", "def test_multi_output_regressor_no_fit_method():\n    # Test that an error is raised if the base estimator does not have a fit method\n    class NoFitEstimator:\n            return X\n\n    X, y = datasets.make_regression(n_targets=3)\n    mor = MultiOutputRegressor(NoFitEstimator())\n    with pytest.raises(ValueError, match=\"The base estimator should implement a fit method\"):\n        mor.fit(X, y)\n", "def test_multi_output_regressor_score():\n    # Test score method of MultiOutputRegressor\n    X, y = datasets.make_regression(n_targets=3)\n    X_train, y_train = X[:50], y[:50]\n    X_test, y_test = X[50:], y[50:]\n\n    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    rgr.fit(X_train, y_train)\n    score = rgr.score(X_test, y_test)\n\n    expected_score = np.mean([\n        GradientBoostingRegressor(random_state=0).fit(X_train, y_train[:, i]).score(X_test, y_test[:, i])\n        for i in range(y.shape[1])\n    ])\n\n    assert_almost_equal(score, expected_score)\n", "def test_multi_output_regressor_predict_unfitted():\n    # Test MultiOutputRegressor predict without fitting raises NotFittedError\n    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    X, _ = datasets.make_regression(n_targets=3)\n    assert_raises(NotFittedError, rgr.predict, X)\n", "def test_multi_output_classifier_partial_fit_one_target():\n    # Test partial_fit method with only one target variable\n    X, y = datasets.make_classification(n_classes=3)\n    y = y.reshape(-1, 1)  # Single target\n\n    clf = SGDClassifier(loss='log', random_state=1, max_iter=5)\n    multi_target_clf = MultiOutputClassifier(clf)\n\n    # Check for ValueError when only one target is provided\n    with pytest.raises(ValueError, match=\"y must have at least two dimensions\"):\n        multi_target_clf.partial_fit(X, y, classes=[np.unique(y)])\n"], "sample_849": ["def test_repeated_cv_with_empty_trainset():\n    X = np.array([[1], [2], [3]])  # 3 samples\n    y = np.array([0, 1, 2])\n    with pytest.raises(\n            ValueError,\n            match='With n_samples=3, test_size=0.67 and train_size=None, '\n                  'the resulting train set will be empty'):\n        next(RepeatedKFold(n_splits=3, n_repeats=2, random_state=42).split(X, y))\n    with pytest.raises(\n            ValueError,\n            match='With n_samples=3, test_size=0.67 and train_size=None, '\n                  'the resulting train set will be empty'):\n        next(RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=42).split(X, y))\n", "def test_leave_one_out_split():\n    # Test LeaveOneOut split function for different input sizes\n    X_small = np.array([[1, 2]])\n    X_normal = np.array([[1, 2], [3, 4], [5, 6]])\n    X_large = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n\n    # Expect error for input size 1\n    loo = LeaveOneOut()\n    with pytest.raises(ValueError, match='Cannot perform LeaveOneOut with n_samples=1'):\n        next(loo.split(X_small))\n\n    # Normal and large inputs should work correctly\n    splits_normal = list(loo.split(X_normal))\n    splits_large = list(loo.split(X_large))\n\n    assert len(splits_normal) == 3\n    assert len(splits_large) == 5\n\n    for train_idx, test_idx in splits_normal:\n        assert len(train_idx) == 2\n        assert len(test_idx) == 1\n        assert set(train_idx).union(test_idx) == set(range(3))\n\n    for train_idx, test_idx in splits_large:\n        assert len(train_idx) == 4\n        assert len(test_idx) == 1\n        assert set(train_idx).union(test_idx) == set(range(5))\n", "def test_leave_one_out_split_indices():\n    # Test that LeaveOneOut properly generates the split indices\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    loo = LeaveOneOut()\n    splits = list(loo.split(X, y))\n    expected_splits = [\n        (np.array([1, 2, 3]), np.array([0])),\n        (np.array([0, 2, 3]), np.array([1])),\n        (np.array([0, 1, 3]), np.array([2])),\n        (np.array([0, 1, 2]), np.array([3]))\n    ]\n    for split, expected_split in zip(splits, expected_splits):\n        assert_array_equal(split[0], expected_split[0])\n        assert_array_equal(split[1], expected_split[1])\n", "def test_leave_p_out_coverage():\n    # Test LeavePOut for more coverage\n\n    # Test with p=1 to ensure it behaves like LeaveOneOut\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    lpo = LeavePOut(p=1)\n    expected_splits = [\n        (np.array([1, 2]), np.array([0])),\n        (np.array([0, 2]), np.array([1])),\n        (np.array([0, 1]), np.array([2]))\n    ]\n    splits = list(lpo.split(X, y))\n    assert len(splits) == len(expected_splits)\n    for split, expected in zip(splits, expected_splits):\n        assert_array_equal(split[0], expected[0])\n        assert_array_equal(split[1], expected[1])\n\n    # Test with p=2\n    lpo = LeavePOut(p=2)\n    expected_splits = [\n        (np.array([2]), np.array([0, 1])),\n        (np.array([1]), np.array([0, 2])),\n        (np.array([0]), np.array([1, 2]))\n    ]\n    splits = list(lpo.split(X, y))\n    assert len(splits) == len(expected_splits)\n    for split, expected in zip(splits, expected_splits):\n        assert_array_equal(split[0], expected[0])\n        assert_array_equal(split[1], expected[1])\n\n    # Test for ValueError if p is greater than or equal to the number of samples\n    with pytest.raises(ValueError, match=\"p=3 must be strictly less than the number of samples=3\"):\n        lpo = LeavePOut(p=3)\n        list(lpo.split(X, y))\n\n    with pytest.raises(ValueError, match=\"p=4 must be strictly less than the number of samples=3\"):\n        lpo = LeavePOut(p=4)\n        list(lpo.split(X, y))\n", "def test_leave_one_out_split():\n    # Test LeaveOneOut split\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    loo = LeaveOneOut()\n    splits = list(loo.split(X, y))\n\n    assert len(splits) == len(X)  # there should be as many splits as samples\n    for i, (train_index, test_index) in enumerate(splits):\n        # In each split, the test set should contain exactly one sample\n        assert len(test_index) == 1\n        assert test_index[0] == i\n        # The train set should contain all other samples\n        assert len(train_index) == len(X) - 1\n        assert_array_equal(np.sort(np.concatenate((train_index, test_index))),\n                           np.arange(len(X)))\n"], "sample_850": ["def test_rbf_sampler_sparse_input():\n    # Test RBFSampler with sparse input\n    gamma = 15.\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n\n    # Compute exact kernel\n    kernel = rbf_kernel(X_sparse.toarray(), Y_sparse.toarray(), gamma=gamma)\n\n    # Approximate kernel mapping\n    rbf_transform = RBFSampler(gamma=gamma, n_components=500, random_state=42)\n    X_trans = rbf_transform.fit_transform(X_sparse)\n    Y_trans = rbf_transform.transform(Y_sparse)\n    kernel_approx = np.dot(X_trans, Y_trans.T)\n\n    error = kernel - kernel_approx\n    assert np.abs(np.mean(error)) <= 0.01  # close to unbiased\n    np.abs(error, out=error)\n    assert np.max(error) <= 0.1  # nothing too far off\n    assert np.mean(error) <= 0.05  # mean is fairly close\n", "def test_nystroem_sparse_input():\n    # Test Nystroem on sparse input data\n    rng = np.random.RandomState(0)\n    X_sparse = csr_matrix(rng.random_sample(size=(300, 50)))\n\n    # With n_components = n_samples this is exact\n    X_transformed = Nystroem(n_components=X_sparse.shape[0]).fit_transform(X_sparse)\n    K = rbf_kernel(X_sparse.toarray())\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\n\n    trans = Nystroem(n_components=2, random_state=rng)\n    X_transformed = trans.fit(X_sparse).transform(X_sparse)\n    assert X_transformed.shape == (X_sparse.shape[0], 2)\n\n    # test callable kernel on sparse data\n        return np.dot(X, Y.T)\n    trans = Nystroem(n_components=2, kernel=linear_kernel, random_state=rng)\n    X_transformed = trans.fit(X_sparse).transform(X_sparse)\n    assert X_transformed.shape == (X_sparse.shape[0], 2)\n\n    # test that available kernels fit and transform on sparse data\n    kernels_available = kernel_metrics()\n    for kern in kernels_available:\n        trans = Nystroem(n_components=2, kernel=kern, random_state=rng)\n        X_transformed = trans.fit(X_sparse).transform(X_sparse)\n        assert X_transformed.shape == (X_sparse.shape[0], 2)\n", "def test_rbfsampler_fitted():\n    # Test if RBFSampler raises a NotFittedError when transform is called before fit\n    rbf_sampler = RBFSampler(gamma=1.0, n_components=100, random_state=42)\n    X_test = np.random.rand(10, 5)\n\n    with pytest.raises(ValueError, match=\"This RBFSampler instance is not fitted yet.\"):\n        rbf_sampler.transform(X_test)\n", "def test_rbf_sampler_different_random_states():\n    # test that RBFSampler produces different approximations with different random states\n    gamma = 10.\n    rbf_transform_1 = RBFSampler(gamma=gamma, n_components=100, random_state=42)\n    X_trans_1 = rbf_transform_1.fit_transform(X)\n    Y_trans_1 = rbf_transform_1.transform(Y)\n\n    rbf_transform_2 = RBFSampler(gamma=gamma, n_components=100, random_state=43)\n    X_trans_2 = rbf_transform_2.fit_transform(X)\n    Y_trans_2 = rbf_transform_2.transform(Y)\n\n    # Ensure the transformations are different\n    assert np.any(np.not_equal(X_trans_1, X_trans_2)), \"Transformations with different random states should not be identical\"\n    assert np.any(np.not_equal(Y_trans_1, Y_trans_2)), \"Transformations with different random states should not be identical\"\n", "def test_rbf_sampler_different_params():\n    # Test RBFSampler with different parameters\n\n    # Test with different values of gamma\n    for gamma in [0.1, 1, 10]:\n        kernel = rbf_kernel(X, Y, gamma=gamma)\n        rbf_transform = RBFSampler(gamma=gamma, n_components=1000, random_state=42)\n        X_trans = rbf_transform.fit_transform(X)\n        Y_trans = rbf_transform.transform(Y)\n        kernel_approx = np.dot(X_trans, Y_trans.T)\n        error = kernel - kernel_approx\n        assert np.abs(np.mean(error)) <= 0.01  # close to unbiased\n        np.abs(error, out=error)\n        assert np.max(error) <= 0.1  # nothing too far off\n        assert np.mean(error) <= 0.05  # mean is fairly close\n\n    # Test with different numbers of components\n    for n_components in [500, 1000, 1500]:\n        rbf_transform = RBFSampler(gamma=1, n_components=n_components, random_state=42)\n        X_trans = rbf_transform.fit_transform(X)\n        Y_trans = rbf_transform.transform(Y)\n        kernel_approx = np.dot(X_trans, Y_trans.T)\n        error = kernel - kernel_approx\n        assert np.abs(np.mean(error)) <= 0.01  # close to unbiased\n        np.abs(error, out=error)\n        assert np.max(error) <= 0.1  # nothing too far off\n        assert np.mean(error) <= 0.05  # mean is fairly close\n"], "sample_851": ["def test_max_error():\n    # Test max_error with a simple example\n    y_true = [3, 2, 7, 1]\n    y_pred = [4, 2, 7, 1]\n    assert_almost_equal(max_error(y_true, y_pred), 1)\n\n    # Test max_error with zero error\n    y_true = [1, 2, 3, 4, 5]\n    y_pred = [1, 2, 3, 4, 5]\n    assert_almost_equal(max_error(y_true, y_pred), 0)\n\n    # Test max_error with negative differences\n    y_true = [5, 6, 7, 8, 9]\n    y_pred = [4, 5, 6, 7, 8]\n    assert_almost_equal(max_error(y_true, y_pred), 1)\n\n    # Test max_error with larger differences\n    y_true = [0, 0, 0, 0, 0]\n    y_pred = [10, 20, 30, 40, 50]\n    assert_almost_equal(max_error(y_true, y_pred), 50)\n\n    # Test max_error raises ValueError for multioutput data\n    y_true = np.array([[1, 2], [3, 4]])\n    y_pred = np.array([[1, 2], [3, 5]])\n    with pytest.raises(ValueError, match=\"Multioutput not supported in max_error\"):\n        max_error(y_true, y_pred)\n", "def test_max_error():\n    # Test case where the max error is non-zero\n    y_true = [1, 2, 3, 4]\n    y_pred = [1, 3, 2, 6]\n    assert_almost_equal(max_error(y_true, y_pred), 2.0)\n\n    # Test case where the max error is zero\n    y_true = [1, 2, 3, 4]\n    y_pred = [1, 2, 3, 4]\n    assert_almost_equal(max_error(y_true, y_pred), 0.0)\n\n    # Test case with negative values\n    y_true = [-1, -2, -3, -4]\n    y_pred = [-1, -3, -2, -6]\n    assert_almost_equal(max_error(y_true, y_pred), 2.0)\n\n    # Test case with mixed positive and negative values\n    y_true = [1, -2, 3, -4]\n    y_pred = [1, -3, 2, -6]\n    assert_almost_equal(max_error(y_true, y_pred), 2.0)\n\n    # Edge case with single element\n    y_true = [2]\n    y_pred = [5]\n    assert_almost_equal(max_error(y_true, y_pred), 3.0)\n\n    # Edge case with zero elements\n    y_true = []\n    y_pred = []\n    with pytest.raises(ValueError, match=\"Found input variables with inconsistent numbers of samples\"):\n        max_error(y_true, y_pred)\n", "def test_max_error():\n    # Test max_error with basic inputs\n    y_true = [3, 2, 7, 1]\n    y_pred = [4, 2, 7, 1]\n    assert_almost_equal(max_error(y_true, y_pred), 1)\n\n    # Test max_error with zero error\n    y_true = [1, 2, 3, 4]\n    y_pred = [1, 2, 3, 4]\n    assert_almost_equal(max_error(y_true, y_pred), 0)\n\n    # Test max_error with larger errors\n    y_true = [3, -2, 7, -1]\n    y_pred = [4, 0, 10, -3]\n    assert_almost_equal(max_error(y_true, y_pred), 3)\n\n    # Test max_error with float inputs\n    y_true = [1.5, 2.5, 3.5]\n    y_pred = [1.0, 2.0, 3.0]\n    assert_almost_equal(max_error(y_true, y_pred), 0.5)\n\n    # Test max_error with multi-output (should raise ValueError)\n    y_true = [[1, 2], [3, 4]]\n    y_pred = [[1, 2], [3, 5]]\n    with pytest.raises(ValueError, match=\"Multioutput not supported in max_error\"):\n        max_error(y_true, y_pred)\n", "def test_mean_gamma_deviance():\n    y_true = np.array([2, 0.5, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    # Gamma deviance is equivalent to Tweedie deviance with power=2\n    gamma_deviance = mean_gamma_deviance(y_true, y_pred)\n    tweedie_deviance = mean_tweedie_deviance(y_true, y_pred, power=2)\n\n    assert_almost_equal(gamma_deviance, tweedie_deviance, decimal=5)\n    assert_almost_equal(gamma_deviance, 1.0568, decimal=4)\n", "def test_regression_metrics_with_sample_weights():\n    y_true = np.array([3, -0.5, 2, 7])\n    y_pred = np.array([2.5, 0.0, 2, 8])\n    sample_weight = np.array([1, 2, 1, 1])\n\n    assert_almost_equal(mean_squared_error(y_true, y_pred, sample_weight=sample_weight), 0.416, decimal=3)\n    assert_almost_equal(mean_absolute_error(y_true, y_pred, sample_weight=sample_weight), 0.625, decimal=3)\n    assert_almost_equal(median_absolute_error(y_true, y_pred), 0.5, decimal=3)\n    assert_almost_equal(max_error(y_true, y_pred), 1.0, decimal=1)\n    assert_almost_equal(r2_score(y_true, y_pred, sample_weight=sample_weight), 0.948, decimal=3)\n    assert_almost_equal(explained_variance_score(y_true, y_pred, sample_weight=sample_weight), 0.957, decimal=3)\n\n    # Testing tweedie deviance with sample weights for power=2\n    y_true = np.array([2, 0.5, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n    sample_weight = np.array([1, 1, 1, 1])\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, sample_weight=sample_weight, power=2), 1.056, decimal=3)\n"], "sample_852": ["def test_make_gaussian_quantiles():\n    n_samples = 100\n    n_features = 2\n    n_classes = 3\n    mean = [1, 2]\n    cov = 1.0\n    shuffle = False\n    X, y = make_gaussian_quantiles(mean=mean, cov=cov, n_samples=n_samples,\n                                   n_features=n_features, n_classes=n_classes,\n                                   shuffle=shuffle, random_state=0)\n\n    assert X.shape == (n_samples, n_features), \"X shape mismatch\"\n    assert y.shape == (n_samples,), \"y shape mismatch\"\n    assert np.unique(y).shape == (n_classes,), \"Unexpected number of classes\"\n\n    # Check if the mean of X is close to the provided mean\n    assert_array_almost_equal(X.mean(axis=0), mean, decimal=1,\n                              err_msg=\"The mean of X is not as expected\")\n\n    # Check if the covariance of X is close to the provided covariance\n    assert_almost_equal(np.cov(X.T)[0, 0], cov, decimal=1,\n                        err_msg=\"The covariance of X is not as expected\")\n\n    # Check if points are labeled correctly by quantiles\n    steps = n_samples // n_classes\n    for i in range(n_classes):\n        assert (y[steps * i: steps * (i + 1)] == i).all(), \"Points not labeled correctly by quantiles\"\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3, random_state=0)\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert len(np.unique(y)) == 3, \"Unexpected number of classes\"\n\n    X, y = make_gaussian_quantiles(mean=[5, 5], cov=2, n_samples=50, n_features=2, n_classes=2, random_state=0)\n    assert X.shape == (50, 2), \"X shape mismatch\"\n    assert y.shape == (50,), \"y shape mismatch\"\n    assert len(np.unique(y)) == 2, \"Unexpected number of classes\"\n    assert_almost_equal(np.mean(X, axis=0), [5, 5], decimal=1, err_msg=\"Unexpected mean of generated samples\")\n\n    with pytest.raises(ValueError, match=\"n_samples must be at least n_classes\"):\n        make_gaussian_quantiles(n_samples=1, n_features=2, n_classes=3, random_state=0)\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3, random_state=0)\n    \n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n\n    # Test with mean and covariance\n    mean = [1, 2]\n    cov = 2.0\n    X, y = make_gaussian_quantiles(mean=mean, cov=cov, n_samples=100, n_features=2, n_classes=3, random_state=0)\n    \n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n    assert_array_almost_equal(np.mean(X, axis=0), mean, decimal=1, err_msg=\"Mean of generated samples mismatch\")\n    assert_almost_equal(np.cov(X, rowvar=False)[0, 0], cov, decimal=1, err_msg=\"Covariance of generated samples mismatch\")\n\n    # Test with shuffle=False\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3, shuffle=False, random_state=0)\n    \n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n\n    # Test with different number of classes\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=2, random_state=0)\n    \n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (2,), \"Unexpected number of classes\"\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=[3, 3], cov=2.0, n_samples=100,\n                                   n_features=2, n_classes=3, shuffle=True,\n                                   random_state=0)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n\n    # Check the class balance\n    counts = np.bincount(y)\n    assert len(counts) == 3, \"Unexpected number of classes\"\n    assert counts.min() > 0, \"Some classes have no samples\"\n    assert np.all(np.abs(counts - np.mean(counts)) < 10), \"Class imbalance too large\"\n\n    # Check that data is approximately centered around the mean\n    mean_X = np.mean(X, axis=0)\n    assert_almost_equal(mean_X, [3, 3], decimal=1, err_msg=\"Data mean mismatch\")\n\n    # Check that covariance is approximately equal to the specified covariance\n    cov_X = np.cov(X, rowvar=False)\n    assert_almost_equal(cov_X[0, 0], 2.0, decimal=1, err_msg=\"Covariance mismatch\")\n    assert_almost_equal(cov_X[1, 1], 2.0, decimal=1, err_msg=\"Covariance mismatch\")\n    assert_almost_equal(cov_X[0, 1], 0.0, decimal=1, err_msg=\"Covariance mismatch\")\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3,\n                                   shuffle=True, random_state=0)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n\n    # Test that the mean and variance of the generated features are consistent\n    assert_almost_equal(X.mean(axis=0), np.zeros(2), decimal=1,\n                        err_msg=\"Mean of features is not zero\")\n    assert_almost_equal(X.var(axis=0), np.ones(2), decimal=1,\n                        err_msg=\"Variance of features is not one\")\n\n    # Test that the samples are correctly shuffled\n    X2, y2 = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3,\n                                     shuffle=False, random_state=0)\n    assert not np.array_equal(X, X2), \"Samples are not shuffled\"\n    assert not np.array_equal(y, y2), \"Labels are not shuffled\"\n\n    # Test that ValueError is raised for n_samples < n_classes\n    with pytest.raises(ValueError, match=\"n_samples must be at least n_classes\"):\n        make_gaussian_quantiles(n_samples=2, n_features=2, n_classes=3)\n"], "sample_853": ["def test_transform_target_regressor_default_identity_transformer():\n    # Check that the default transformer is the identity transformer\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression())\n    y_pred = regr.fit(X, y).predict(X)\n    assert y.shape == y_pred.shape\n    assert_allclose(y, y_pred, rtol=1e-5)\n    \n    # Ensure that the default transformer is indeed the identity transformer\n    y_tran = regr.transformer_.transform(y.reshape(-1, 1)).squeeze()\n    assert_allclose(y, y_tran)\n    assert_allclose(y, regr.transformer_.inverse_transform(y_tran.reshape(-1, 1)).squeeze())\n", "def test_transform_target_regressor_default_regressor():\n    X, y = friedman\n    # Test default regressor (LinearRegression) when regressor is None\n    regr = TransformedTargetRegressor(transformer=StandardScaler())\n    y_pred = regr.fit(X, y).predict(X)\n    assert y.shape == y_pred.shape\n\n    # Verify the regressor is LinearRegression\n    assert isinstance(regr.regressor_, LinearRegression)\n\n    # Check the transformer output\n    y_tran = regr.transformer_.transform(y.reshape(-1, 1)).squeeze()\n    _check_standard_scaled(y, y_tran)\n    assert_allclose(y, regr.transformer_.inverse_transform(\n        y_tran.reshape(-1, 1)).squeeze())\n\n    # Check the regressor output\n    lr = LinearRegression().fit(X, regr.transformer_.transform(y.reshape(-1, 1)).squeeze())\n    assert_allclose(regr.regressor_.coef_.ravel(), lr.coef_.ravel())\n", "def test_transform_target_regressor_no_transformer_no_func():\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression())\n    y_pred = regr.fit(X, y).predict(X)\n    # check that the transformer is an identity transformer\n    assert isinstance(regr.transformer_, FunctionTransformer)\n    assert regr.transformer_.func is None\n    assert regr.transformer_.inverse_func is None\n    assert_allclose(y, y_pred)\n    # check the regressor output\n    lr = LinearRegression().fit(X, y)\n    assert_allclose(regr.regressor_.coef_.ravel(), lr.coef_.ravel())\n", "def test_transform_target_regressor_default_transformer():\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression())\n    y_pred = regr.fit(X, y).predict(X)\n    # since no transformer or functions are provided, it should be the identity transformation\n    assert_allclose(y, y_pred)\n    assert y.shape == y_pred.shape\n    # check that the transformer is an identity function transformer\n    assert isinstance(regr.transformer_, FunctionTransformer)\n    assert regr.transformer_.func is None\n    assert regr.transformer_.inverse_func is None\n", "def test_transform_target_regressor_default_transformer():\n    # Test the default transformer (identity) when both func and transformer are None\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression())\n    y_pred = regr.fit(X, y).predict(X)\n    assert y.shape == y_pred.shape\n    assert_allclose(y_pred, regr.regressor_.predict(X))\n\n    # Check that the transformer has not altered the data\n    assert_allclose(y, regr.transformer_.transform(y))\n    assert_allclose(y, regr.transformer_.inverse_transform(y))\n"], "sample_854": ["def test_linearsvc_multiclass_decision_function():\n    # Test that the decision_function of LinearSVC works correctly for multi-class classification\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3, n_informative=10, random_state=42)\n    clf = svm.LinearSVC(random_state=0)\n    clf.fit(X, y)\n    decision = clf.decision_function(X)\n    assert decision.shape == (100, 3)\n    predictions = np.argmax(decision, axis=1)\n    assert_array_equal(predictions, clf.predict(X))\n\n    # Test the decision function for Crammer-Singer formulation\n    clf_cs = svm.LinearSVC(random_state=0, multi_class='crammer_singer')\n    clf_cs.fit(X, y)\n    decision_cs = clf_cs.decision_function(X)\n    assert decision_cs.shape == (100, 3)\n    predictions_cs = np.argmax(decision_cs, axis=1)\n    assert_array_equal(predictions_cs, clf_cs.predict(X))\n", "def test_base_libsvm_fit_invalid_gamma():\n    # Test invalid gamma parameter in BaseLibSVM.fit\n    class DummyLibSVM(BaseLibSVM):\n        _impl = 'c_svc'\n\n                     coef0=0.0, tol=1e-3, C=1.0, nu=0.5, epsilon=0.1,\n                     shrinking=True, probability=False, cache_size=200,\n                     class_weight=None, verbose=False, max_iter=-1,\n                     random_state=None):\n            super().__init__(kernel, degree, gamma, coef0, tol, C, nu, epsilon,\n                             shrinking, probability, cache_size, class_weight,\n                             verbose, max_iter, random_state)\n\n                       random_seed):\n            pass\n\n                        random_seed):\n            pass\n\n            return np.zeros(X.shape[0])\n\n            return np.zeros((1, X.shape[1]))\n\n    X, y = make_classification(n_samples=10, n_features=5)\n    clf = DummyLibSVM(gamma='invalid_gamma')\n\n    with pytest.raises(ValueError, match=\"When 'gamma' is a string, it should be either 'scale' or 'auto'\"):\n        clf.fit(X, y)\n", "def test_one_vs_one_coef():\n    # This test checks the _one_vs_one_coef function.\n    dual_coef = np.array([[1, -1, 0.5], [-0.5, 1, -1]])\n    n_support = np.array([2, 3, 2])\n    support_vectors = np.array([\n        [0, 1],\n        [1, 1],\n        [2, 2],\n        [3, 3],\n        [4, 4],\n        [5, 5],\n        [6, 6]\n    ])\n\n    coef = _one_vs_one_coef(dual_coef, n_support, support_vectors)\n    expected_coef = [\n        np.array([-0.5, -0.5]),\n        np.array([1.5, 1.5]),\n        np.array([-2.5, -2.5])\n    ]\n    assert len(coef) == len(expected_coef)\n    for c, ec in zip(coef, expected_coef):\n        assert_array_almost_equal(c, ec, decimal=1)\n", "def test_custom_kernel_function():\n    # Test SVC with a custom kernel function (polynomial kernel in this case)\n        return (np.dot(X, Y.T) + 1) ** 3\n\n    clf = svm.SVC(kernel=custom_poly_kernel)\n    clf.fit(X, Y)\n    pred = clf.predict(T)\n\n    # Manually compute the polynomial kernel for comparison\n    K_train = custom_poly_kernel(np.array(X), np.array(X))\n    K_test = custom_poly_kernel(np.array(T), np.array(X))\n\n    clf_manual = svm.SVC(kernel='precomputed')\n    clf_manual.fit(K_train, Y)\n    pred_manual = clf_manual.predict(K_test)\n\n    assert_array_equal(pred, true_result)\n    assert_array_equal(pred, pred_manual)\n", "def test_libsvm_scale_gamma():\n    # Test gamma='scale' functionality in SVMs using libsvm\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n    \n    # Test SVC with gamma='scale'\n    clf = svm.SVC(gamma='scale')\n    clf.fit(X, y)\n    X_var = np.var(X, ddof=0)\n    expected_gamma = 1.0 / (X.shape[1] * X_var)\n    assert_almost_equal(clf._gamma, expected_gamma, decimal=5)\n    assert clf.score(X, y) > 0.9\n    \n    # Test NuSVC with gamma='scale'\n    clf = svm.NuSVC(gamma='scale')\n    clf.fit(X, y)\n    assert_almost_equal(clf._gamma, expected_gamma, decimal=5)\n    assert clf.score(X, y) > 0.9\n    \n    # Test SVR with gamma='scale'\n    y = y.astype(float)\n    clf = svm.SVR(gamma='scale')\n    clf.fit(X, y)\n    assert_almost_equal(clf._gamma, expected_gamma, decimal=5)\n    assert clf.score(X, y) > 0.1\n    \n    # Test NuSVR with gamma='scale'\n    clf = svm.NuSVR(gamma='scale')\n    clf.fit(X, y)\n    assert_almost_equal(clf._gamma, expected_gamma, decimal=5)\n    assert clf.score(X, y) > 0.1\n    \n    # Test OneClassSVM with gamma='scale'\n    clf = svm.OneClassSVM(gamma='scale')\n    clf.fit(X)\n    assert_almost_equal(clf._gamma, expected_gamma, decimal=5)\n    assert np.mean(clf.predict(X) == 1) > 0.9\n"], "sample_855": ["def test_predict_before_fit_raises_error():\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    with pytest.raises(NotFittedError):\n        clf.predict([[0]])\n        \n    reg = DummyRegressor(strategy=\"mean\")\n    with pytest.raises(NotFittedError):\n        reg.predict([[0]])\n", "def test_dummy_classifier_uniform_strategy_sparse_target():\n    X = [[0]] * 5  # ignored\n    y = sp.csc_matrix(np.array([[1, 0],\n                                [0, 3],\n                                [4, 0],\n                                [0, 1],\n                                [1, 0]]))\n\n    clf = DummyClassifier(strategy=\"uniform\", random_state=0)\n    clf.fit(X, y)\n\n    X = [[0]] * 500\n    y_pred = clf.predict(X)\n    assert sp.issparse(y_pred)\n    y_pred = y_pred.toarray()\n\n    for k in range(y.shape[1]):\n        p = np.bincount(y_pred[:, k]) / float(len(X))\n        assert_almost_equal(p[0], 2. / 5, decimal=1)\n        assert_almost_equal(p[1], 2. / 5, decimal=1)\n        assert_almost_equal(p[4], 1. / 5, decimal=1)\n", "def test_dummy_classifier_prior_with_sample_weight():\n    X = [[0], [0], [0], [0], [0], [0]]\n    y = [0, 1, 1, 1, 2, 2]\n    sample_weight = [0.1, 0.2, 0.2, 0.2, 0.1, 0.2]\n\n    clf = DummyClassifier(strategy=\"prior\", random_state=0)\n    clf.fit(X, y, sample_weight=sample_weight)\n    assert_array_almost_equal(clf.class_prior_, [0.1/1.0, 0.6/1.0, 0.3/1.0])\n    assert_array_equal(clf.predict(X), [1, 1, 1, 1, 1, 1])\n    _check_predict_proba(clf, X, y)\n", "def test_dummy_classifier_sparse_target_with_sample_weight():\n    X = [[0]] * 5  # ignored\n    y = sp.csc_matrix(np.array([[1, 0],\n                                [1, 3],\n                                [1, 0],\n                                [1, 1],\n                                [1, 0]]))\n    sample_weight = np.array([0.1, 0.5, 0.2, 0.1, 0.1])\n    clf = DummyClassifier(strategy=\"most_frequent\", random_state=0)\n    clf.fit(X, y, sample_weight)\n    \n    y_pred = clf.predict(X)\n    assert sp.issparse(y_pred)\n    assert_array_equal(y_pred.toarray(), np.hstack([np.ones((len(X), 1)), np.zeros((len(X), 1))]))\n", "def test_dummy_classifier_invalid_constant_in_fit():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = [2, 1, 2, 2]\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0, constant=3)  # constant not in y\n    with pytest.raises(ValueError, match=\"The constant target value must be present in the training data\"):\n        clf.fit(X, y)\n\n    y_multiclass = np.array([[2, 1],\n                             [1, 2],\n                             [2, 1],\n                             [2, 1]])\n    clf = DummyClassifier(strategy=\"constant\", random_state=0, constant=[2, 3])  # second constant not in y\n    with pytest.raises(ValueError, match=\"The constant target value must be present in the training data\"):\n        clf.fit(X, y_multiclass)\n"], "sample_856": ["def test_group_shuffle_split_with_fewer_groups():\n    # Test GroupShuffleSplit with fewer unique groups than n_splits\n    X = np.ones(8)\n    y = np.ones(8)\n    groups = [1, 1, 2, 2, 2, 3, 3, 3]\n\n    gss = GroupShuffleSplit(n_splits=5, random_state=0)\n    # Since there are fewer unique groups than n_splits, it should raise an error\n    with pytest.raises(ValueError, match=\"Cannot have number of splits n_splits=5 greater than the number of groups: 3.\"):\n        next(gss.split(X, y, groups))\n", "def test_leave_one_group_out_split():\n    # Test LeaveOneGroupOut split on a simple dataset\n    X = np.arange(10).reshape((5, 2))\n    y = np.array([1, 2, 1, 2, 1])\n    groups = np.array([1, 1, 2, 2, 3])\n    \n    logo = LeaveOneGroupOut()\n    splits = list(logo.split(X, y, groups))\n    \n    expected_splits = [\n        (np.array([2, 3, 4]), np.array([0, 1])),\n        (np.array([0, 1, 4]), np.array([2, 3])),\n        (np.array([0, 1, 2, 3]), np.array([4]))\n    ]\n    \n    for (train, test), (exp_train, exp_test) in zip(splits, expected_splits):\n        assert_array_equal(train, exp_train)\n        assert_array_equal(test, exp_test)\n", "def test_predefined_split_invalid_test_fold():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 1, 2, 2])\n    \n    # Test case when the length of test_fold does not match the number of samples in X\n    test_fold = [0, 1, 2]  # Incorrect length\n    with pytest.raises(ValueError, match=\"Found array with 3 sample\\(s\\)\"):\n        ps = PredefinedSplit(test_fold)\n        list(ps.split(X, y))\n    \n    # Test case when test_fold contains invalid values (not -1 or positive integers)\n    test_fold = [0, 1, -2, 2]  # Invalid value -2\n    with pytest.raises(ValueError, match=\"invalid value in test_fold\"):\n        ps = PredefinedSplit(test_fold)\n        list(ps.split(X, y))\n    \n    # Test case with valid test_fold\n    test_fold = [0, 1, -1, 2]\n    ps = PredefinedSplit(test_fold)\n    assert ps.get_n_splits() == 3  # Unique folds are 0, 1, and 2\n    splits = list(ps.split(X, y))\n    assert len(splits) == 3\n    for train, test in splits:\n        assert set(test).isdisjoint(set(train))  # Train and test sets should be disjoint\n", "def test_check_cv_iterable():\n    # Test check_cv with iterable generating train-test splits\n    X = np.ones(9)\n    y_binary = np.array([0, 1, 0, 1, 0, 0, 1, 1, 1])\n\n    # Simple iterable\n    iterable = [(np.array([0, 1, 2, 3, 4, 5, 6]), np.array([7, 8]))]\n    cv = check_cv(iterable)\n    assert hasattr(cv, 'split')\n    np.testing.assert_equal(list(cv.split(X, y_binary)), iterable)\n\n    # Iterable generating train-test splits\n        yield np.array([0, 1, 2, 3, 4, 5, 6]), np.array([7, 8])\n\n    cv = check_cv(iterable_generator())\n    assert hasattr(cv, 'split')\n    np.testing.assert_equal(list(cv.split(X, y_binary)), iterable)\n", "def test_time_series_split_with_max_train_size():\n    # Test TimeSeriesSplit with max_train_size on a small dataset\n    X = np.array([[1], [2], [3], [4], [5], [6]])\n    tscv = TimeSeriesSplit(n_splits=3, max_train_size=2)\n\n    expected_splits = [\n        (np.array([0, 1]), np.array([2, 3])),\n        (np.array([2, 3]), np.array([4, 5])),\n        (np.array([4, 5]), np.array([]))  # No more data for test after 4 splits\n    ]\n\n    for (train, test), (expected_train, expected_test) in zip(tscv.split(X), expected_splits):\n        assert_array_equal(train, expected_train)\n        assert_array_equal(test, expected_test)\n\n    # Test max_train_size larger than the number of training samples\n    tscv = TimeSeriesSplit(n_splits=3, max_train_size=5)\n    for train, test in tscv.split(X):\n        assert len(train) <= 5\n"], "sample_857": ["def test_get_depth_and_n_leaves():\n    # Check if get_depth and get_n_leaves methods work correctly.\n\n    # Create a dataset and fit a simple tree\n    X = [[0, 0], [1, 1], [2, 2], [3, 3]]\n    y_class = [0, 0, 1, 1]\n    y_reg = [0.5, 1.5, 2.5, 3.5]\n\n    clf = DecisionTreeClassifier()\n    clf.fit(X, y_class)\n    assert clf.get_depth() == clf.tree_.max_depth, \"get_depth() did not match tree's max_depth\"\n    assert clf.get_n_leaves() == clf.tree_.n_leaves, \"get_n_leaves() did not match tree's n_leaves\"\n\n    reg = DecisionTreeRegressor()\n    reg.fit(X, y_reg)\n    assert reg.get_depth() == reg.tree_.max_depth, \"get_depth() did not match tree's max_depth\"\n    assert reg.get_n_leaves() == reg.tree_.n_leaves, \"get_n_leaves() did not match tree's n_leaves\"\n\n    # Test on multi-output\n    y_multi_class = [[0, 0], [1, 1], [2, 2], [3, 3]]\n    clf_multi = DecisionTreeClassifier()\n    clf_multi.fit(X, y_multi_class)\n    assert clf_multi.get_depth() == clf_multi.tree_.max_depth, \"get_depth() did not match tree's max_depth\"\n    assert clf_multi.get_n_leaves() == clf_multi.tree_.n_leaves, \"get_n_leaves() did not match tree's n_leaves\"\n\n    y_multi_reg = [[0.5, 1.5], [2.5, 3.5], [4.5, 5.5], [6.5, 7.5]]\n    reg_multi = DecisionTreeRegressor()\n    reg_multi.fit(X, y_multi_reg)\n    assert reg_multi.get_depth() == reg_multi.tree_.max_depth, \"get_depth() did not match tree's max_depth\"\n    assert reg_multi.get_n_leaves() == reg_multi.tree_.n_leaves, \"get_n_leaves() did not match tree's n_leaves\"\n", "def test_min_samples_split_float():\n    # Test float min_samples_split parameter for both classifiers and regressors\n    for name, Tree in ALL_TREES.items():\n        est = Tree(min_samples_split=0.1, random_state=0)\n        est.fit(X_small, y_small if \"Classifier\" in name else y_small_reg)\n        assert est.min_samples_split == 0.1\n        assert est.min_samples_split * len(y_small) >= 2\n\n        # Ensure the splits respect the min_samples_split parameter\n        node_samples = est.tree_.n_node_samples[est.tree_.children_left != -1]\n        assert np.min(node_samples) >= est.min_samples_split * len(y_small)\n", "def test_tree_pruning_with_large_ccp_alpha():\n    # Test tree pruning with a very large ccp_alpha\n    X = [[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]]\n    y = [0, 1, 0, 1, 0]\n    \n    # Create a classifier with a large ccp_alpha to prune almost all nodes\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=10)\n    clf.fit(X, y)\n    \n    # The tree should be pruned to a single node\n    assert clf.tree_.node_count == 1\n    assert clf.tree_.max_depth == 0\n    assert_array_equal(clf.predict(X), [0, 0, 0, 0, 0])\n    \n    # Create a regressor with a large ccp_alpha to prune almost all nodes\n    reg = DecisionTreeRegressor(random_state=0, ccp_alpha=10)\n    reg.fit(X, y)\n    \n    # The tree should be pruned to a single node\n    assert reg.tree_.node_count == 1\n    assert reg.tree_.max_depth == 0\n    assert_array_almost_equal(reg.predict(X), [0.4, 0.4, 0.4, 0.4, 0.4])\n", "def test_min_impurity_decrease_on_small_dataset():\n    # Test if min_impurity_decrease works on a small dataset\n    X = np.array([[0], [1], [2], [3], [4], [5]])\n    y = np.array([0, 1, 0, 1, 0, 1])\n\n    for name, TreeEstimator in ALL_TREES.items():\n        est = TreeEstimator(random_state=0, min_impurity_decrease=0.1)\n        est.fit(X, y)\n        assert est.tree_.max_depth <= 2, (\n            \"Failed with {0}, max_depth is {1}\".format(name, est.tree_.max_depth))\n\n        # Ensure min_impurity_decrease has an effect on split\n        est_no_split = TreeEstimator(random_state=0, min_impurity_decrease=0.5)\n        est_no_split.fit(X, y)\n        assert est_no_split.tree_.max_depth == 0, (\n            \"Failed with {0}, expected no split\".format(name))\n", "def test_empty_fit():\n    # Test fitting on an empty dataset raises ValueError\n    X_empty = np.empty((0, 4))\n    y_empty = np.empty((0,))\n    \n    for name, TreeEstimator in ALL_TREES.items():\n        est = TreeEstimator()\n        with pytest.raises(ValueError):\n            est.fit(X_empty, y_empty)\n            \n        est_sparse = TreeEstimator()\n        with pytest.raises(ValueError):\n            est_sparse.fit(csc_matrix(X_empty), y_empty)\n"], "sample_858": ["def test_predict_with_dropped_estimator():\n    \"\"\"Test VotingClassifier prediction with a dropped estimator.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    \n    # Fit VotingClassifier with all estimators\n    eclf1 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard').fit(X, y)\n    \n    # Fit VotingClassifier with one dropped estimator\n    eclf2 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', 'drop'), ('gnb', clf3)],\n        voting='hard').fit(X, y)\n    \n    # Predictions should still be consistent\n    assert_array_equal(eclf1.predict(X), eclf2.predict(X))\n\n    # Check that the dropped estimator is no longer in the fitted estimators_\n    assert 'rf' not in eclf2.named_estimators_\n    assert len(eclf2.estimators_) == 2\n    assert len(eclf2.named_estimators_) == 2\n", "def test_voting_regressor_predict():\n    \"\"\"Check prediction of VotingRegressor on a toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = DecisionTreeRegressor(random_state=1)\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16]])\n    y = np.array([2, 6, 12, 20])\n\n    reg1_pred = reg1.fit(X, y).predict(X)\n    reg2_pred = reg2.fit(X, y).predict(X)\n    \n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('dt', reg2)],\n        weights=[1, 1]).fit(X, y)\n    ereg_pred = ereg.predict(X)\n\n    avg = np.average(np.vstack([reg1_pred, reg2_pred]), axis=0,\n                     weights=[1, 1])\n    assert_array_almost_equal(ereg_pred, avg, decimal=2)\n", "def test_votingregressor_example():\n    \"\"\"Test VotingRegressor example provided in the docstring.\"\"\"\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n    er = VotingRegressor([('lr', r1), ('rf', r2)])\n    er.fit(X, y)\n    predictions = er.predict(X)\n    expected_predictions = np.array([3.3, 5.7, 11.8, 19.7, 28.0, 40.3])\n    assert_array_almost_equal(predictions, expected_predictions, decimal=1)\n", "def test_voting_regressor_weighted_predictions():\n    \"\"\"Check weighted average predictions on toy dataset for VotingRegressor.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=42)\n    reg3 = DecisionTreeRegressor()\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([3, 5, 7, 9])\n\n    # Individual predictions\n    reg1.fit(X, y)\n    reg2.fit(X, y)\n    reg3.fit(X, y)\n\n    reg1_pred = reg1.predict(X)\n    reg2_pred = reg2.predict(X)\n    reg3_pred = reg3.predict(X)\n\n    # VotingRegressor with weights\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dt', reg3)], weights=[1, 2, 3])\n    ereg.fit(X, y)\n    ereg_pred = ereg.predict(X)\n\n    avg = np.average(np.array([reg1_pred, reg2_pred, reg3_pred]), axis=0,\n                     weights=[1, 2, 3])\n    assert_almost_equal(ereg_pred, avg, decimal=2)\n\n    # VotingRegressor without weights (should use uniform weights)\n    ereg_no_weights = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dt', reg3)])\n    ereg_no_weights.fit(X, y)\n    ereg_no_weights_pred = ereg_no_weights.predict(X)\n\n    avg_no_weights = np.mean(np.array([reg1_pred, reg2_pred, reg3_pred]), axis=0)\n    assert_almost_equal(ereg_no_weights_pred, avg_no_weights, decimal=2)\n", "def test_voting_regressor_toy_example():\n    \"\"\"Manually check predicted regression values for a toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=123)\n    reg3 = DecisionTreeRegressor()\n\n    X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n    y = np.array([1, 2, 3, 4, 5])\n\n    assert_array_almost_equal(reg1.fit(X, y).predict(X), y)\n    assert_array_almost_equal(reg2.fit(X, y).predict(X), y, decimal=0)\n    assert_array_almost_equal(reg3.fit(X, y).predict(X), y)\n\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dt', reg3)],\n        weights=[1, 1, 1])\n    \n    assert_array_almost_equal(ereg.fit(X, y).predict(X), y, decimal=0)\n\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2), ('dt', reg3)],\n        weights=[2, 1, 1])\n    \n    assert_array_almost_equal(ereg.fit(X, y).predict(X), y, decimal=0)\n"], "sample_859": ["def test_enet_path_custom_alpha_grid():\n    # Test enet_path with a custom grid of alphas\n\n    # Generate dataset\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    \n    # Custom alphas\n    custom_alphas = np.logspace(-4, 1, 30)\n    \n    # ElasticNet with custom alphas\n    clf = ElasticNetCV(alphas=custom_alphas, max_iter=100)\n    clf.fit(X, y)\n    \n    # Ensure that the alphas used are the custom alphas\n    assert_array_almost_equal(clf.alphas_, custom_alphas)\n    \n    # Check that the model performs well\n    assert clf.score(X, y) > 0.7\n\n    # Also test Lasso with custom alphas\n    clf = LassoCV(alphas=custom_alphas, max_iter=100)\n    clf.fit(X, y)\n    \n    # Ensure that the alphas used are the custom alphas\n    assert_array_almost_equal(clf.alphas_, custom_alphas)\n    \n    # Check that the model performs well\n    assert clf.score(X, y) > 0.7\n", "def test_enet_path_precompute_array():\n    # Test that using an array as the precompute parameter works correctly\n\n    X, y, _, _ = build_dataset()\n    Gram = X.T.dot(X)\n    clf = ElasticNet(alpha=0.5, max_iter=100, precompute=Gram)\n    clf.fit(X, y)\n    pred = clf.predict(X)\n    \n    # Check coefficients and intercepts are calculated correctly\n    assert clf.coef_.shape == (X.shape[1],)\n    assert np.isscalar(clf.intercept_)\n    # Make sure predictions are in the right shape\n    assert pred.shape == (X.shape[0],)\n    # Check that the coefficients are consistent with the Gram matrix\n    assert_array_almost_equal(clf.coef_, ElasticNet(alpha=0.5, max_iter=100, precompute=False).fit(X, y).coef_, decimal=4)\n", "def test_enet_path_manual_alphas():\n    # Test enet_path with manually provided alphas\n    X, y, _, _ = build_dataset(n_samples=50, n_features=50, n_targets=1)\n    alphas = np.logspace(-3, 0, 10)\n    l1_ratio = 0.5\n\n    # Test for default ElasticNet path\n    enet_alphas, enet_coefs, enet_gaps = enet_path(X, y, alphas=alphas, l1_ratio=l1_ratio)\n    assert len(enet_alphas) == len(alphas)\n    assert enet_coefs.shape == (X.shape[1], len(alphas))\n    assert len(enet_gaps) == len(alphas)\n    \n    # Test for Lasso path\n    lasso_alphas, lasso_coefs, lasso_gaps = lasso_path(X, y, alphas=alphas)\n    assert len(lasso_alphas) == len(alphas)\n    assert lasso_coefs.shape == (X.shape[1], len(alphas))\n    assert len(lasso_gaps) == len(alphas)\n    \n    assert_array_almost_equal(enet_alphas, lasso_alphas)\n    assert_array_almost_equal(enet_coefs, lasso_coefs)\n    assert_array_almost_equal(enet_gaps, lasso_gaps)\n", "def test_elasticnet_warm_start_with_different_l1_ratio():\n    X, y, _, _ = build_dataset()\n    max_iter = 50\n    tol = 1e-3\n\n    # Fit with a low l1_ratio and warm start\n    clf_low_l1 = ElasticNet(alpha=0.1, l1_ratio=0.1, max_iter=max_iter, tol=tol, warm_start=True)\n    clf_low_l1.fit(X, y)\n    coef_low_l1 = clf_low_l1.coef_\n\n    # Increase the l1_ratio and continue fitting\n    clf_low_l1.set_params(l1_ratio=0.9)\n    clf_low_l1.fit(X, y)\n    coef_high_l1 = clf_low_l1.coef_\n\n    # Fit with the higher l1_ratio directly\n    clf_high_l1 = ElasticNet(alpha=0.1, l1_ratio=0.9, max_iter=max_iter, tol=tol)\n    clf_high_l1.fit(X, y)\n    coef_direct_high_l1 = clf_high_l1.coef_\n\n    # Check that the coefficients are the same when using warm start and when fitting directly\n    assert_array_almost_equal(coef_high_l1, coef_direct_high_l1)\n", "def test_enet_path_with_precomputed_gram():\n    # Test that enet_path works with precomputed Gram matrix\n\n    X, y, _, _ = build_dataset(n_samples=50, n_features=10)\n    Gram = np.dot(X.T, X)\n    Xy = np.dot(X.T, y)\n\n    alphas, coefs, gaps = enet_path(X, y, precompute=Gram, Xy=Xy, eps=1e-3,\n                                    n_alphas=50, fit_intercept=False,\n                                    l1_ratio=0.5)\n\n    # Ensure that coefficients are correctly computed\n    assert coefs.shape == (X.shape[1], len(alphas))\n    assert np.all(gaps >= 0)\n    assert np.all(np.isfinite(coefs))\n\n    # Compare with directly computed path without precomputing\n    alphas_direct, coefs_direct, gaps_direct = enet_path(X, y, eps=1e-3,\n                                                         n_alphas=50,\n                                                         fit_intercept=False,\n                                                         l1_ratio=0.5)\n\n    assert_array_almost_equal(alphas, alphas_direct, decimal=5)\n    assert_array_almost_equal(coefs, coefs_direct, decimal=5)\n    assert_array_almost_equal(gaps, gaps_direct, decimal=5)\n"], "sample_860": ["def test_check_symmetric_sparse():\n    # check if sparse matrix remains sparse after symmetrization\n    X = sp.csr_matrix([[1, 2], [3, 4]])\n    X_sym = check_symmetric(X)\n    assert sp.issparse(X_sym)\n    assert isinstance(X_sym, sp.csr_matrix)\n    assert np.allclose(X_sym.toarray(), np.array([[1, 2.5], [2.5, 4]]))\n\n    # check if error is raised for non-square sparse matrix\n    X_non_square = sp.csr_matrix([[1, 2, 3], [4, 5, 6]])\n    with pytest.raises(ValueError, match=\"array must be 2-dimensional and square\"):\n        check_symmetric(X_non_square)\n", "def test_check_array_accept_sparse_invalid_dtype():\n    # Test that an appropriate error is raised when the dtype is not valid\n    X_csr = sp.csr_matrix([[1, 2], [3, 4]])\n    invalid_dtype = \"invalid-dtype\"\n    msg = \"Data type 'invalid-dtype' not understood\"\n    assert_raise_message(TypeError, msg, check_array, X_csr, dtype=invalid_dtype)\n", "def test_check_is_fitted_with_attributes():\n    # Test check_is_fitted with estimator having specific fitted attributes\n    class EstimatorWithAttributes:\n            self.coef_ = None\n            self.intercept_ = None\n\n            self.coef_ = np.array([1, 2, 3])\n            self.intercept_ = np.array([4, 5, 6])\n    \n    estimator = EstimatorWithAttributes()\n    assert_raises(NotFittedError, check_is_fitted, estimator)\n\n    estimator.fit(np.array([[1, 2], [3, 4]]), np.array([1, 2]))\n    assert check_is_fitted(estimator) is None\n    \n    # Check with attributes parameter (deprecated, for backward compatibility)\n    with pytest.warns(DeprecationWarning, match=\"Passing attributes to check_is_fitted is deprecated\"):\n        check_is_fitted(estimator, attributes=['coef_', 'intercept_'])\n", "def test_check_X_y_invalid_multi_output():\n    # check that multi-output=True raises an error with invalid y dimensions\n    X = np.ones((2, 2))\n    y = np.array([[1, 2, 3], [4, 5, 6]])\n    msg = \"Found input variables with inconsistent numbers of samples\"\n    with pytest.raises(ValueError, match=msg):\n        check_X_y(X, y, multi_output=True)\n\n    # check that multi-output=False raises an error with 2D y\n    y = np.array([[1, 2], [3, 4]])\n    msg = \"y should be a 1d array, got an array of shape \\(2, 2\\) instead\"\n    with pytest.raises(ValueError, match=msg):\n        check_X_y(X, y, multi_output=False)\n", "def test_check_symmetric_sparse_formats():\n    arr_sym = np.array([[0, 1], [1, 2]])\n    arr_asym = np.array([[0, 2], [0, 2]])\n    sparse_formats = ['csr', 'csc', 'coo', 'dok', 'lil', 'bsr', 'dia']\n\n    for fmt in sparse_formats:\n        sparse_matrix = getattr(sp, fmt + '_matrix')(arr_asym)\n        symmetrized_matrix = check_symmetric(sparse_matrix, raise_warning=False)\n        assert sp.issparse(symmetrized_matrix)\n        assert_array_equal(symmetrized_matrix.toarray(), arr_sym)\n\n    with pytest.raises(ValueError):\n        check_symmetric(sp.csr_matrix(arr_asym), raise_exception=True)\n"], "sample_861": ["def test_parameter_sampler_with_distributions():\n    # Test ParameterSampler with distributions\n    param_distributions = {'a': [1, 2, 3], 'b': expon(scale=1.0)}\n    n_iter = 10\n    sampler = ParameterSampler(param_distributions, n_iter=n_iter, random_state=42)\n\n    samples = list(sampler)\n    assert len(samples) == n_iter\n    for sample in samples:\n        assert sample['a'] in [1, 2, 3]\n        assert sample['b'] >= 0  # Exponential distribution is always non-negative\n\n    # Check that repeated calls with same random_state yield identical samples\n    sampler1 = ParameterSampler(param_distributions, n_iter=n_iter, random_state=42)\n    sampler2 = ParameterSampler(param_distributions, n_iter=n_iter, random_state=42)\n    assert [x for x in sampler1] == [x for x in sampler2]\n\n    # Ensure that samples are different for different random states\n    sampler3 = ParameterSampler(param_distributions, n_iter=n_iter, random_state=43)\n    assert [x for x in sampler1] != [x for x in sampler3]\n", "def test_parameter_sampler_with_distributions():\n    # Test ParameterSampler with various distributions including scipy.stats distributions\n    param_distributions = {\n        'a': [1, 2, 3],\n        'b': expon(scale=2),\n        'c': [True, False],\n        'd': uniform(loc=0, scale=5)\n    }\n    n_iter = 20\n    sampler = ParameterSampler(param_distributions=param_distributions, n_iter=n_iter, random_state=42)\n    samples = list(sampler)\n    assert len(samples) == n_iter\n\n    for sample in samples:\n        assert sample['a'] in [1, 2, 3]\n        assert isinstance(sample['b'], float)\n        assert sample['b'] >= 0\n        assert sample['c'] in [True, False]\n        assert isinstance(sample['d'], float)\n        assert 0 <= sample['d'] <= 5\n", "def test_random_search_with_scorer_callable():\n        predictions = estimator.predict(X)\n        return np.mean(predictions == y)\n\n    X, y = make_classification(n_samples=50, n_features=4, random_state=0)\n    param_distributions = {'C': expon(scale=100), 'gamma': expon(scale=.1)}\n    clf = RandomizedSearchCV(SVC(), param_distributions, n_iter=10, \n                             scoring=custom_scorer, random_state=0)\n    clf.fit(X, y)\n    \n    assert 'mean_test_score' in clf.cv_results_\n    assert 'std_test_score' in clf.cv_results_\n    assert clf.best_score_ > 0\n", "def test_grid_search_with_callable_parameter():\n    # Test GridSearchCV with a callable parameter\n    X, y = make_classification(n_samples=100, n_features=4, random_state=42)\n    param_grid = {'C': [0.1, 1, 10], 'kernel': lambda: ['linear', 'rbf']}\n    clf = SVC()\n\n    grid_search = GridSearchCV(clf, param_grid, cv=3)\n    grid_search.fit(X, y)\n\n    # Ensure that the callable parameter was evaluated and used correctly\n    param_keys = grid_search.cv_results_['param_kernel'].data\n    assert len(param_keys) == 6\n    assert all(k in ['linear', 'rbf'] for k in param_keys)\n", "def test_custom_scorer():\n    # Test GridSearchCV with a custom scorer\n        predictions = estimator.predict(X)\n        return np.sum(predictions == y) / len(y)  # simple accuracy\n\n    X, y = make_classification(n_samples=100, n_features=4, random_state=42)\n    param_grid = {'C': [0.1, 1.0, 10.0]}\n    svc = SVC()\n\n    grid_search = GridSearchCV(svc, param_grid, scoring=custom_scorer, cv=5)\n    grid_search.fit(X, y)\n\n    assert grid_search.best_estimator_ is not None\n    assert grid_search.best_params_['C'] in param_grid['C']\n    assert isinstance(grid_search.best_score_, float)\n    assert 'mean_test_score' in grid_search.cv_results_\n\n    # Check if the custom scorer was used\n    mean_test_scores = grid_search.cv_results_['mean_test_score']\n    assert all(0 <= score <= 1 for score in mean_test_scores)\n"], "sample_862": ["def test_vectorizer_empty_input():\n    # Test for empty input data\n    empty_data = []\n\n    for Vectorizer in [CountVectorizer, TfidfVectorizer, HashingVectorizer]:\n        vect = Vectorizer()\n        X = vect.fit_transform(empty_data)\n        assert X.shape[0] == 0\n        assert X.shape[1] == 0\n\n    for Transformer in [TfidfTransformer]:\n        transf = Transformer()\n        X_trans = transf.fit_transform(sparse.csr_matrix((0, 0)))\n        assert X_trans.shape == (0, 0)\n", "def test_custom_preprocessor_in_vectorizer():\n    # Test custom preprocessor in CountVectorizer\n    preprocessor = lambda x: x.replace(\"junk\", \"healthy\")\n    vectorizer = CountVectorizer(preprocessor=preprocessor)\n    corpus = [\"I love junk food\", \"junk food is bad\", \"healthy food is good\"]\n    X = vectorizer.fit_transform(corpus).toarray()\n    feature_names = vectorizer.get_feature_names_out()\n    expected_features = [\"bad\", \"food\", \"good\", \"healthy\", \"is\", \"love\"]\n    assert_array_equal(feature_names, expected_features)\n    expected_result = [[0, 1, 0, 1, 0, 1],\n                       [1, 1, 0, 1, 1, 0],\n                       [0, 1, 1, 1, 1, 0]]\n    assert_array_equal(X, expected_result)\n\n    # Test custom preprocessor in TfidfVectorizer\n    vectorizer = TfidfVectorizer(preprocessor=preprocessor)\n    X = vectorizer.fit_transform(corpus).toarray()\n    feature_names = vectorizer.get_feature_names_out()\n    assert_array_equal(feature_names, expected_features)\n    expected_result = [[0., 0.5, 0., 0.5, 0., 0.5],\n                       [0.4472136, 0.4472136, 0., 0.4472136, 0.4472136, 0.],\n                       [0., 0.4472136, 0.63245553, 0.4472136, 0.4472136, 0.]]\n    assert_array_almost_equal(X, expected_result, decimal=5)\n", "def test_strip_tags():\n    # Test to ensure that HTML/XML tags are stripped correctly\n    a = \"<p>This is a <b>test</b> string with <a href='#'>HTML</a> tags.</p>\"\n    expected = \" This is a  test  string with  HTML  tags. \"\n    assert strip_tags(a) == expected\n\n    a = \"<div>Another <div>test</div> string with nested <span>tags</span>.</div>\"\n    expected = \" Another  test  string with nested  tags. \"\n    assert strip_tags(a) == expected\n\n    a = \"String without tags\"\n    expected = \"String without tags\"\n    assert strip_tags(a) == expected\n\n    a = \"<div><p>Multiple tags</p> in <span>one string</span>.</div>\"\n    expected = \" Multiple tags  in  one string. \"\n    assert strip_tags(a) == expected\n\n    # Test with different languages and symbols\n    a = \"<p>\u041f\u0440\u0438\u0432\u0435\u0442, \u043c\u0438\u0440! <a href='#'>\u0422\u0435\u0441\u0442</a></p>\"\n    expected = \" \u041f\u0440\u0438\u0432\u0435\u0442, \u043c\u0438\u0440!  \u0422\u0435\u0441\u0442 \"\n    assert strip_tags(a) == expected\n\n    a = \"<p>\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\uff01 <a href='#'>\u30c6\u30b9\u30c8</a></p>\"\n    expected = \" \u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\uff01  \u30c6\u30b9\u30c8 \"\n    assert strip_tags(a) == expected\n", "def test_strip_tags():\n    html = \"<html><body><h1>Title</h1><p>This is a <b>test</b> string.</p></body></html>\"\n    expected = \"  Title   This is a  test  string.   \"\n    assert strip_tags(html) == expected\n\n    html = \"<div><a href='url'>Link</a></div>\"\n    expected = \"  Link  \"\n    assert strip_tags(html) == expected\n\n    html = \"<div><p>Paragraph with <a href='url'>link</a>.</p></div>\"\n    expected = \"  Paragraph with  link . \"\n    assert strip_tags(html) == expected\n\n    html = \"No tags here!\"\n    expected = \"No tags here!\"\n    assert strip_tags(html) == expected\n", "def test_tfidf_vectorizer_sublinear_tf():\n    corpus = [\n        \"This is the first document.\",\n        \"This document is the second document.\",\n        \"And this is the third one.\",\n        \"Is this the first document?\",\n    ]\n    vectorizer = TfidfVectorizer(sublinear_tf=True)\n    tfidf_matrix = vectorizer.fit_transform(corpus)\n    assert tfidf_matrix.shape == (4, 9)\n    # Check that sublinear_tf has been applied correctly\n    tfidf_data = tfidf_matrix.toarray()\n    assert np.all(tfidf_data >= 0)  # Ensure no negative values\n    assert np.any(tfidf_data < 1)  # There should be values less than 1 due to log scaling\n"], "sample_863": ["def test_pipeline_with_none_step():\n    # Test pipeline with an intermediate step set to None\n    X = iris.data\n    y = iris.target\n    \n    clf = LogisticRegression()\n    filter1 = SelectKBest(f_classif, k=2)\n    \n    pipe = Pipeline([('anova', filter1), ('none_step', None), ('logistic', clf)])\n    pipe.fit(X, y)\n    pipe.predict(X)\n    \n    # Ensure pipeline still works as expected\n    assert pipe.score(X, y) > 0.5\n\n    # Ensure the pipeline correctly handles setting a step to None\n    pipe.set_params(none_step=SelectKBest(f_classif, k=3))\n    pipe.fit(X, y)\n    assert pipe.score(X, y) > 0.5\n\n    # Ensure the pipeline can set the step back to None\n    pipe.set_params(none_step=None)\n    pipe.fit(X, y)\n    assert pipe.score(X, y) > 0.5\n", "def test_pipeline_transform_intermediate_passthrough():\n    # Test that intermediate 'passthrough' steps are correctly handled in transform\n    X = iris.data\n    y = iris.target\n    \n    transf1 = Transf()\n    transf2 = Transf()\n    mult = Mult(mult=2)\n    \n    # Create a pipeline with an intermediate 'passthrough' step\n    pipe = Pipeline([('transf1', transf1), ('passthrough', 'passthrough'), ('transf2', transf2), ('mult', mult)])\n    \n    # Test fit and transform\n    X_trans = pipe.fit_transform(X, y)\n    X_trans_expected = mult.fit_transform(transf2.fit_transform(transf1.fit_transform(X)))\n    assert_array_almost_equal(X_trans, X_trans_expected)\n    \n    # Test inverse_transform\n    X_inv = pipe.inverse_transform(X_trans)\n    X_inv_expected = transf1.inverse_transform(transf2.inverse_transform(mult.inverse_transform(X_trans)))\n    assert_array_almost_equal(X_inv, X_inv_expected)\n", "def test_pipeline_transform_with_feature_union():\n    # Test if Pipeline works well with FeatureUnion as one of its steps\n    X, y = iris.data, iris.target\n\n    pca = PCA(n_components=2)\n    select = SelectKBest(k=1)\n    feature_union = FeatureUnion([(\"pca\", pca), (\"select\", select)])\n\n    clf = LogisticRegression()\n    pipe = Pipeline([(\"features\", feature_union), (\"clf\", clf)])\n    pipe.fit(X, y)\n\n    # Test predict\n    predictions = pipe.predict(X)\n    assert predictions.shape == (X.shape[0],)\n\n    # Test predict_proba\n    if hasattr(clf, \"predict_proba\"):\n        proba = pipe.predict_proba(X)\n        assert proba.shape == (X.shape[0], len(np.unique(y)))\n\n    # Test predict_log_proba\n    if hasattr(clf, \"predict_log_proba\"):\n        log_proba = pipe.predict_log_proba(X)\n        assert log_proba.shape == (X.shape[0], len(np.unique(y)))\n\n    # Test decision_function\n    if hasattr(clf, \"decision_function\"):\n        decision_function = pipe.decision_function(X)\n        if len(np.unique(y)) == 2:\n            assert decision_function.shape == (X.shape[0],)\n        else:\n            assert decision_function.shape == (X.shape[0], len(np.unique(y)))\n", "def test_pipeline_len():\n    # Test __len__ method of Pipeline\n    clf = SVC()\n    filter1 = SelectKBest(f_classif, k=2)\n    pipe = Pipeline([('anova', filter1), ('svc', clf)])\n    assert len(pipe) == 2\n\n    pipe.set_params(svc='passthrough')\n    assert len(pipe) == 2\n\n    pipe = Pipeline([('anova', filter1)])\n    assert len(pipe) == 1\n", "def test_pipeline_with_drop_step():\n    # Test pipeline with 'drop' step\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    y = np.array([1, 2])\n\n    mult2 = Mult(mult=2)\n    mult3 = Mult(mult=3)\n\n    pipe = Pipeline([\n        ('m2', mult2),\n        ('drop_step', 'drop'),\n        ('m3', mult3)\n    ])\n\n    pipe.fit(X, y)\n    X_transformed = pipe.transform(X)\n    # Only m2 and m3 should be applied, drop_step should be ignored\n    expected = mult3.transform(mult2.transform(X))\n    assert_array_equal(X_transformed, expected)\n\n    # Test fit_transform\n    X_fit_transformed = pipe.fit_transform(X, y)\n    assert_array_equal(X_fit_transformed, expected)\n\n    # Ensure predict works as expected\n    predictions = pipe.predict(X)\n    expected_predictions = mult3.predict(mult2.transform(X))\n    assert_array_equal(predictions, expected_predictions)\n"], "sample_864": ["compilation error", "def test_invalid_bandwidth_type():\n    # Test MeanShift with invalid bandwidth type (string)\n    bandwidth = \"invalid\"\n    ms = MeanShift(bandwidth=bandwidth)\n    msg = (r\"bandwidth needs to be greater than zero or None,\"\n           r\" got %s\" % bandwidth)\n    with pytest.raises(ValueError, match=msg):\n        ms.fit(X)\n", "def test_zero_bandwidth():\n    # Test MeanShift with zero bandwidth\n    ms = MeanShift(bandwidth=0)\n    msg = \"bandwidth needs to be greater than zero or None, got 0.000000\"\n    with pytest.raises(ValueError, match=msg):\n        ms.fit(X)\n", "def test_seeds_argument():\n    # Test that providing seeds works as expected\n    seeds = np.array([[1, 1], [-1, -1], [1, -1]]) + 10\n    ms = MeanShift(bandwidth=1.2, seeds=seeds)\n    ms.fit(X)\n    assert_array_equal(ms.cluster_centers_, seeds)\n\n    # Test with empty seeds array\n    seeds = np.empty((0, 2))\n    ms = MeanShift(bandwidth=1.2, seeds=seeds)\n    ms.fit(X)\n    assert len(ms.cluster_centers_) > 0  # Should fall back to default seeding\n\n    # Test with fewer seeds than clusters\n    seeds = np.array([[1, 1]]) + 10\n    ms = MeanShift(bandwidth=1.2, seeds=seeds)\n    ms.fit(X)\n    assert len(ms.cluster_centers_) > 1  # Should find more clusters than seeds\n", "def test_meanshift_invalid_seeds():\n    # Test MeanShift with invalid seeds\n    invalid_seeds = np.array([[1, 1, 1], [2, 2, 2]])  # Incorrect shape\n    ms = MeanShift(bandwidth=1.2, seeds=invalid_seeds)\n    msg = \"seeds should have the same number of features as the input data\"\n    with pytest.raises(ValueError, match=msg):\n        ms.fit(X)\n"], "sample_865": ["def test_criterion_init_invalid():\n    # Test that invalid criterion raises proper errors during initialization.\n    # Invalid criterion for DecisionTreeClassifier\n    for criterion in [\"mean_squared_error\", \"mae\", \"friedman_mse\"]:\n        with pytest.raises(ValueError, match=f\"Unknown criterion: {criterion}\"):\n            DecisionTreeClassifier(criterion=criterion)\n\n    # Invalid criterion for DecisionTreeRegressor\n    for criterion in [\"gini\", \"entropy\"]:\n        with pytest.raises(ValueError, match=f\"Unknown criterion: {criterion}\"):\n            DecisionTreeRegressor(criterion=criterion)\n\n    # Invalid criterion for ExtraTreeClassifier\n    for criterion in [\"mean_squared_error\", \"mae\", \"friedman_mse\"]:\n        with pytest.raises(ValueError, match=f\"Unknown criterion: {criterion}\"):\n            ExtraTreeClassifier(criterion=criterion)\n\n    # Invalid criterion for ExtraTreeRegressor\n    for criterion in [\"gini\", \"entropy\"]:\n        with pytest.raises(ValueError, match=f\"Unknown criterion: {criterion}\"):\n            ExtraTreeRegressor(criterion=criterion)\n", "def test_max_features_auto_for_regressors():\n    # Check if \"auto\" for max_features behaves correctly in regressors.\n    for name, TreeRegressor in REG_TREES.items():\n        reg = TreeRegressor(max_features=\"auto\")\n        reg.fit(boston.data, boston.target)\n        assert reg.max_features_ == boston.data.shape[1], (\n            \"Failed with {0}, max_features=auto\".format(name))\n", "def test_feature_importances():\n    # Check feature importances on a small toy dataset.\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n    y = np.array([0, 1, 0, 1])\n\n    for name, TreeEstimator in ALL_TREES.items():\n        est = TreeEstimator(random_state=0)\n        est.fit(X, y)\n        importances = est.feature_importances_\n        assert importances.shape[0] == X.shape[1], (\n            \"Failed with {0}, expected importances shape to be ({1},), got {2}\"\n            .format(name, X.shape[1], importances.shape))\n        assert np.sum(importances) == pytest.approx(1.0), (\n            \"Failed with {0}, expected sum of importances to be 1, got {1}\"\n            .format(name, np.sum(importances)))\n", "def test_predict_proba_unfitted():\n    # Test predict_proba and predict_log_proba on an unfitted model to ensure\n    # it raises NotFittedError\n    for name, TreeClassifier in CLF_TREES.items():\n        clf = TreeClassifier()\n        with pytest.raises(NotFittedError):\n            clf.predict_proba(X)\n        with pytest.raises(NotFittedError):\n            clf.predict_log_proba(X)\n", "def test_tree_pruning_effectiveness():\n    # Test pruning effectiveness on a simple dataset\n    X = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]\n    y = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n\n    # Train a tree without pruning\n    clf_no_prune = DecisionTreeClassifier(random_state=0)\n    clf_no_prune.fit(X, y)\n\n    # Train a tree with pruning\n    clf_prune = DecisionTreeClassifier(random_state=0, ccp_alpha=0.01)\n    clf_prune.fit(X, y)\n\n    # Ensure the pruned tree has fewer or equal nodes\n    assert clf_prune.tree_.node_count <= clf_no_prune.tree_.node_count\n\n    # Ensure the pruned tree has a lower or equal max depth\n    assert clf_prune.tree_.max_depth <= clf_no_prune.tree_.max_depth\n\n    # Check if the pruned tree maintains reasonable accuracy\n    accuracy_no_prune = accuracy_score(y, clf_no_prune.predict(X))\n    accuracy_prune = accuracy_score(y, clf_prune.predict(X))\n\n    # The accuracy of the pruned tree should be close to that of the unpruned tree\n    assert abs(accuracy_no_prune - accuracy_prune) <= 0.1\n"], "sample_866": ["def test_affinity_propagation_damping():\n    # Test different damping factors for AffinityPropagation\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n\n    for damping in [0.5, 0.7, 0.9]:\n        cluster_centers_indices, labels = affinity_propagation(\n            S, preference=preference, damping=damping)\n\n        n_clusters_ = len(cluster_centers_indices)\n        assert n_clusters == n_clusters_\n\n        af = AffinityPropagation(preference=preference, affinity=\"precomputed\", damping=damping)\n        labels_precomputed = af.fit(S).labels_\n\n        af = AffinityPropagation(preference=preference, verbose=True, damping=damping)\n        labels = af.fit(X).labels_\n\n        assert_array_equal(labels, labels_precomputed)\n\n        cluster_centers_indices = af.cluster_centers_indices_\n\n        n_clusters_ = len(cluster_centers_indices)\n        assert np.unique(labels).size == n_clusters_\n        assert n_clusters == n_clusters_\n", "def test_affinity_propagation_damping_factor():\n    # Test that ValueError is raised when damping factor is out of bounds\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n\n    with pytest.raises(ValueError, match=\"damping must be >= 0.5 and < 1\"):\n        affinity_propagation(S, preference=preference, damping=0.4)\n\n    with pytest.raises(ValueError, match=\"damping must be >= 0.5 and < 1\"):\n        affinity_propagation(S, preference=preference, damping=1.0)\n\n    af = AffinityPropagation(damping=0.4)\n    with pytest.raises(ValueError, match=\"damping must be >= 0.5 and < 1\"):\n        af.fit(X)\n\n    af = AffinityPropagation(damping=1.0)\n    with pytest.raises(ValueError, match=\"damping must be >= 0.5 and < 1\"):\n        af.fit(X)\n", "def test_affinity_propagation_no_samples():\n    # Test AffinityPropagation with no samples\n    X = np.empty((0, 2))\n\n    af = AffinityPropagation()\n\n    with pytest.raises(ValueError):\n        af.fit(X)\n", "def test_affinity_propagation_damping_values():\n    # Test AffinityPropagation with various damping values\n    damping_values = [0.5, 0.6, 0.7, 0.8, 0.9]\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n\n    for damping in damping_values:\n        af = AffinityPropagation(preference=preference, affinity=\"precomputed\", damping=damping)\n        labels = af.fit_predict(S)\n        n_clusters_ = len(np.unique(labels))\n        assert n_clusters_ > 0  # Ensure that at least one cluster is formed\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input data\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels = af.fit_predict(X_sparse)\n    \n    # Convert sparse to dense and compare results to ensure consistency\n    af_dense = AffinityPropagation(affinity=\"euclidean\")\n    labels_dense = af_dense.fit_predict(X.toarray())\n    \n    assert_array_equal(labels, labels_dense)\n"], "sample_867": ["def test_parameter_sampler_validate_input():\n    # Test validation of input for ParameterSampler\n\n    # Non-dict or non-iterable input\n    assert_raises(TypeError, ParameterSampler, param_distributions=0, n_iter=10)\n    assert_raises(TypeError, ParameterSampler, param_distributions=\"not a dict\", n_iter=10)\n    \n    # Non-dict entries within an iterable\n    assert_raises(TypeError, ParameterSampler, param_distributions=[{'a': [1, 2]}, 0], n_iter=10)\n\n    # Values that are neither iterable nor distributions\n    assert_raises(TypeError, ParameterSampler, param_distributions={'a': 1}, n_iter=10)\n\n    # Valid input should not raise error\n    try:\n        sampler = ParameterSampler(param_distributions={'a': [1, 2]}, n_iter=2)\n    except TypeError as e:\n        pytest.fail(f\"Unexpected TypeError: {e}\")\n", "def test_randomized_search_cv_results_multimetric_with_callable_refit():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n    n_splits = 3\n    n_search_iter = 30\n    scoring = ('accuracy', 'recall')\n\n    params = dict(C=np.logspace(-4, 1, 3),\n                  gamma=np.logspace(-5, 0, 3, base=0.1))\n\n        \"\"\"\n        Return the index of a model that has the least\n        `mean_test_recall`.\n        \"\"\"\n        return cv_results['mean_test_recall'].argmin()\n\n    random_search = RandomizedSearchCV(SVC(probability=True, random_state=42),\n                                       n_iter=n_search_iter, cv=n_splits,\n                                       param_distributions=params,\n                                       scoring=scoring, refit=refit_callable,\n                                       random_state=0)\n    random_search.fit(X, y)\n    \n    # Ensure the best index is set by the callable\n    assert random_search.best_index_ == refit_callable(random_search.cv_results_)\n    \n    # Ensure the best score is not available with refit callable\n    assert not hasattr(random_search, 'best_score_')\n\n    # Check if the results keys are correct\n    cv_results = random_search.cv_results_\n    param_keys = ('param_C', 'param_degree', 'param_gamma', 'param_kernel')\n    score_keys = ('mean_test_accuracy', 'mean_test_recall', 'rank_test_accuracy', 'rank_test_recall', \n                  'split0_test_accuracy', 'split1_test_accuracy', 'split2_test_accuracy', \n                  'split0_test_recall', 'split1_test_recall', 'split2_test_recall', \n                  'std_test_accuracy', 'std_test_recall', 'mean_fit_time', 'std_fit_time', \n                  'mean_score_time', 'std_score_time')\n    n_cand = n_search_iter\n\n    check_cv_results_array_types(random_search, param_keys, score_keys)\n    check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)\n", "def test_parameter_sampler_with_distributions():\n    # Test ParameterSampler with different types of distributions\n    param_distributions = {\n        'param_float': expon(scale=10),\n        'param_int': randint(0, 10),\n        'param_cat': ['a', 'b', 'c']\n    }\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=10, random_state=0)\n    samples = list(sampler)\n    assert len(samples) == 10\n    for sample in samples:\n        assert isinstance(sample['param_float'], float)\n        assert 0 <= sample['param_int'] < 10\n        assert sample['param_cat'] in ['a', 'b', 'c']\n\n    # Test that repeated calls yield identical parameters\n    sampler = ParameterSampler(param_distributions=param_distributions,\n                               n_iter=3, random_state=0)\n    assert [x for x in sampler] == [x for x in sampler]\n\n    # Ensure sampling without replacement when possible\n    param_distributions_no_dist = {'param_int': list(range(10))}\n    sampler = ParameterSampler(param_distributions=param_distributions_no_dist,\n                               n_iter=9, random_state=0)\n    samples = list(sampler)\n    assert len(samples) == 9\n    assert len(set([sample['param_int'] for sample in samples])) == 9\n", "def test_parameter_sampler_with_distributions():\n    # Test ParameterSampler with actual distributions\n    param_distributions = {\n        \"a\": [1, 2, 3],\n        \"b\": expon(scale=1.0),\n        \"c\": [True, False],\n        \"d\": uniform(0, 1)\n    }\n    sampler = ParameterSampler(param_distributions, n_iter=10, random_state=42)\n    \n    samples = list(sampler)\n    assert len(samples) == 10\n    for sample in samples:\n        assert sample[\"a\"] in [1, 2, 3]\n        assert sample[\"b\"] >= 0  # Exponential distribution should be non-negative\n        assert sample[\"c\"] in [True, False]\n        assert 0 <= sample[\"d\"] <= 1\n\n    # Test that repeated calls yield identical parameters\n    sampler = ParameterSampler(param_distributions, n_iter=10, random_state=42)\n    samples1 = list(sampler)\n    sampler = ParameterSampler(param_distributions, n_iter=10, random_state=42)\n    samples2 = list(sampler)\n    assert samples1 == samples2\n", "def test_randomized_search():\n    # Test RandomizedSearchCV for basic functionality\n    clf = MockClassifier()\n    random_search = RandomizedSearchCV(clf, {'foo_param': [1, 2, 3]}, n_iter=3, cv=3, verbose=3)\n    \n    random_search.fit(X, y)\n    assert random_search.best_estimator_.foo_param in [1, 2, 3]\n    assert random_search.n_iter == 3\n\n    assert_array_equal(random_search.cv_results_[\"param_foo_param\"].data, [1, 2, 3])\n    \n    # Smoke test the score etc:\n    random_search.score(X, y)\n    random_search.predict_proba(X)\n    random_search.decision_function(X)\n    random_search.transform(X)\n\n    # Test exception handling on scoring\n    random_search.scoring = 'sklearn'\n    assert_raises(ValueError, random_search.fit, X, y)\n\n    # Test with a different set of parameters to ensure variety\n    random_search = RandomizedSearchCV(clf, {'foo_param': [10, 20, 30]}, n_iter=3, cv=3, verbose=3)\n    random_search.fit(X, y)\n    assert random_search.best_estimator_.foo_param in [10, 20, 30]\n    assert random_search.n_iter == 3\n\n    assert_array_equal(random_search.cv_results_[\"param_foo_param\"].data, [10, 20, 30])\n"], "sample_868": ["def test_zero_labels(metric_name):\n    # Test case with all zero labels\n    metric = SUPERVISED_METRICS[metric_name]\n    labels_true = [0, 0, 0, 0, 0]\n    labels_pred = [0, 0, 0, 0, 0]\n    assert metric(labels_true, labels_pred) == pytest.approx(1.0)\n\n    # Test case with no matching labels\n    labels_pred = [1, 1, 1, 1, 1]\n    assert metric(labels_true, labels_pred) == pytest.approx(0.0)\n", "def test_empty_labels(metric):\n    # Test empty inputs for supervised metrics\n    assert metric([], []) == 1.0\n", "def test_adjusted_rand_score(labels_true, labels_pred, expected_score):\n    assert_allclose(adjusted_rand_score(labels_true, labels_pred), expected_score)\n", "def test_empty_labels(metric_func):\n    # Test that all supervised metrics handle empty label arrays correctly\n    assert metric_func([], []) == 1.0\n", "def test_empty_labels(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == pytest.approx(1.0)\n"], "sample_869": ["def test_multilabel_confusion_matrix_with_sample_weight():\n    # Test multilabel confusion matrix - multilabel-indicator case with sample weight\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]])\n    sample_weight = np.array([2, 1, 3])\n\n    cm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    expected_cm = np.array([[[2, 0], [1, 2]], [[1, 0], [1, 2]], [[0, 3], [1, 0]]])\n    assert_array_equal(cm, expected_cm)\n\n    # Test with labels argument\n    cm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight, labels=[2, 0])\n    expected_cm = np.array([[[0, 3], [1, 0]], [[2, 0], [1, 2]]])\n    assert_array_equal(cm, expected_cm)\n\n    # Test with samplewise=True\n    cm = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight, samplewise=True)\n    expected_cm = np.array([[[2, 0], [1, 2]], [[1, 1], [0, 1]], [[0, 1], [2, 0]]])\n    assert_array_equal(cm, expected_cm)\n", "def test_confusion_matrix_multiclass_unordered_labels():\n    # Test confusion matrix with unordered labels in multiclass setting\n    y_true = [0, 1, 2, 2, 1, 0]\n    y_pred = [0, 2, 1, 2, 0, 1]\n    labels = [2, 0, 1]\n    \n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    # The confusion matrix will be reordered according to the labels argument\n    expected_cm = np.array([\n        [2, 0, 0],\n        [1, 0, 1],\n        [0, 1, 1]\n    ])\n    assert_array_equal(cm, expected_cm)\n", "def test_multilabel_accuracy_score_sparse():\n    # Test multilabel accuracy score with sparse label indicator matrix format\n    from scipy.sparse import csr_matrix\n\n    y_true_dense = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred_dense = np.array([[0, 0, 1], [1, 0, 1]])\n\n    y_true_sparse = csr_matrix(y_true_dense)\n    y_pred_sparse = csr_matrix(y_pred_dense)\n\n    assert accuracy_score(y_true_sparse, y_pred_sparse) == 0.5\n    assert accuracy_score(y_true_sparse, y_true_sparse) == 1\n    assert accuracy_score(y_pred_sparse, y_pred_sparse) == 1\n    assert accuracy_score(y_pred_sparse, csr_matrix(np.logical_not(y_pred_dense))) == 0\n    assert accuracy_score(y_true_sparse, csr_matrix(np.logical_not(y_true_dense))) == 0\n    assert accuracy_score(y_true_sparse, csr_matrix(np.zeros(y_true_dense.shape))) == 0\n    assert accuracy_score(y_pred_sparse, csr_matrix(np.zeros(y_true_dense.shape))) == 0\n", "def test_f1_score_with_zero_division():\n    # Test F1 score with zero_division parameter handling\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n\n    # zero_division = \"warn\" (default)\n    with pytest.warns(UndefinedMetricWarning, match=\"F-score is ill-defined\"):\n        assert_almost_equal(f1_score(y_true, y_pred, zero_division=\"warn\"), 0.0)\n\n    # zero_division = 0\n    assert_almost_equal(f1_score(y_true, y_pred, zero_division=0), 0.0)\n\n    # zero_division = 1\n    assert_almost_equal(f1_score(y_true, y_pred, zero_division=1), 1.0)\n", "def test_cohen_kappa_edge_cases():\n    # Test Cohen's kappa with edge cases\n    y1 = np.array([1, 1, 1, 1])\n    y2 = np.array([1, 1, 1, 1])\n    kappa = cohen_kappa_score(y1, y2)\n    assert_almost_equal(kappa, 1.0, decimal=3)\n\n    y1 = np.array([0, 0, 0, 0])\n    y2 = np.array([1, 1, 1, 1])\n    kappa = cohen_kappa_score(y1, y2)\n    assert_almost_equal(kappa, 0.0, decimal=3)\n\n    y1 = np.array([0, 1, 0, 1])\n    y2 = np.array([1, 0, 1, 0])\n    kappa = cohen_kappa_score(y1, y2)\n    assert_almost_equal(kappa, -1.0, decimal=3)\n\n    # Test Kappa with non-integer labels\n    y1 = np.array([\"cat\", \"cat\", \"dog\", \"dog\"])\n    y2 = np.array([\"cat\", \"dog\", \"cat\", \"dog\"])\n    kappa = cohen_kappa_score(y1, y2)\n    assert_almost_equal(kappa, -0.333, decimal=3)\n\n    # Test Kappa with weights\n    y1 = np.array([0, 1, 2, 2])\n    y2 = np.array([0, 2, 2, 1])\n    kappa_linear = cohen_kappa_score(y1, y2, weights=\"linear\")\n    kappa_quadratic = cohen_kappa_score(y1, y2, weights=\"quadratic\")\n    assert_almost_equal(kappa_linear, 0.428, decimal=3)\n    assert_almost_equal(kappa_quadratic, 0.555, decimal=3)\n"], "sample_870": ["def test_optimizer_unfitted_model(kernel):\n    \"\"\"Test that the optimizer works correctly even for an unfitted model.\"\"\"\n        # Dummy optimizer which returns initial_theta as the optimal theta\n        return initial_theta, obj_func(initial_theta, eval_gradient=False)\n\n    gpr = GaussianProcessRegressor(kernel=kernel, optimizer=optimizer)\n    \n    # Since the model is not fitted, it should predict based on GP prior\n    y_mean, y_std = gpr.predict(X2, return_std=True)\n    \n    assert y_mean.shape == (X2.shape[0],)\n    assert y_std.shape == (X2.shape[0],)\n", "def test_log_marginal_likelihood_gradient(n_targets):\n    \"\"\"Test the gradient of the log-marginal likelihood for different number of targets.\"\"\"\n    rng = np.random.RandomState(42)\n    X = rng.randn(10, 2)\n    y = rng.randn(10, n_targets if n_targets is not None else 1)\n\n    kernel = RBF(length_scale=1.0)\n    gpr = GaussianProcessRegressor(kernel=kernel, n_targets=n_targets)\n    gpr.fit(X, y)\n\n    lml, lml_gradient = gpr.log_marginal_likelihood(kernel.theta, eval_gradient=True)\n    lml_gradient_approx = approx_fprime(\n        kernel.theta, lambda theta: gpr.log_marginal_likelihood(theta, clone_kernel=False), 1e-10\n    )\n\n    assert_almost_equal(lml_gradient, lml_gradient_approx, decimal=5)\n", "def test_invalid_kernel_type():\n    \"\"\"Test that the GaussianProcessRegressor raises an error when an invalid kernel type is passed.\"\"\"\n    invalid_kernel = \"invalid_kernel_type\"\n    with pytest.raises(TypeError, match=\".*kernel.*\"):\n        GaussianProcessRegressor(kernel=invalid_kernel).fit(X, y)\n", "def test_predict_before_fit(kernel):\n    # Test that predictions before fitting the model raise an appropriate error.\n    gpr = GaussianProcessRegressor(kernel=kernel)\n    with pytest.raises(AttributeError, match=\"has no attribute 'X_train_'\"):\n        gpr.predict(X2)\n", "def test_gpr_optimizer_callable():\n    \"\"\"Test that using a custom optimizer as a callable works as expected.\"\"\"\n\n        \"\"\"A simple custom optimizer that tries 10 random samples.\"\"\"\n        best_theta = initial_theta\n        best_func_value = obj_func(initial_theta)[0]\n        rng = np.random.RandomState(42)\n        \n        for _ in range(10):\n            theta = rng.uniform(bounds[:, 0], bounds[:, 1])\n            func_value = obj_func(theta)[0]\n            if func_value < best_func_value:\n                best_func_value = func_value\n                best_theta = theta\n        \n        return best_theta, best_func_value\n\n    kernel = RBF(length_scale=1.0)\n    gpr = GaussianProcessRegressor(kernel=kernel, optimizer=custom_optimizer)\n    gpr.fit(X, y)\n\n    # Test that the optimizer actually runs and improves the log-marginal likelihood\n    initial_lml = gpr.log_marginal_likelihood(kernel.theta)\n    optimized_lml = gpr.log_marginal_likelihood(gpr.kernel_.theta)\n    assert optimized_lml > initial_lml\n\n    # Ensure the optimizer modified the kernel parameters\n    assert not np.array_equal(kernel.theta, gpr.kernel_.theta)\n"], "sample_871": ["def test_silhouette_score_with_callable_metric():\n    \"\"\"Check that silhouette_score works with a callable metric.\"\"\"\n\n        return np.sqrt(np.sum((x - y) ** 2))\n\n    dataset = datasets.load_iris()\n    X = dataset.data\n    y = dataset.target\n\n    # Test silhouette score with the custom metric\n    score_callable = silhouette_score(X, y, metric=custom_metric)\n    assert score_callable > 0\n\n    # Ensure consistency with the built-in 'euclidean' metric\n    score_euclidean = silhouette_score(X, y, metric='euclidean')\n    assert_allclose(score_callable, score_euclidean, rtol=1e-5)\n", "def test_silhouette_samples_sparse_format_conversion():\n    \"\"\"Check that silhouette_samples works correctly after sparse format conversion.\"\"\"\n    X_dense = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    \n    # Convert dense array to different sparse formats\n    X_csr = csr_matrix(X_dense)\n    X_csc = csc_matrix(X_dense)\n    X_dok = dok_matrix(X_dense)\n    X_lil = lil_matrix(X_dense)\n    \n    for X_sparse in [X_csr, X_csc, X_dok, X_lil]:\n        # Convert sparse format back to CSR for computation\n        if not issparse(X_sparse):\n            X_sparse = csr_matrix(X_sparse)\n        \n        pdist_dense = pairwise_distances(X_dense)\n        pdist_sparse = pairwise_distances(X_sparse)\n        \n        # Check that the results are the same for dense and sparse\n        output_with_dense_input = silhouette_samples(pdist_dense, y, metric=\"precomputed\")\n        output_with_sparse_input = silhouette_samples(pdist_sparse, y, metric=\"precomputed\")\n        \n        assert_allclose(output_with_sparse_input, output_with_dense_input)\n", "def test_davies_bouldin_score_identical_clusters():\n    # Test case where two clusters are identical\n    X = np.array([[0.0, 0.0], [0.1, 0.1], [1.0, 1.0], [1.1, 1.1]])\n    labels = np.array([0, 0, 1, 1])\n    \n    # Both clusters should be very similar, leading to a high Davies-Bouldin score\n    score = davies_bouldin_score(X, labels)\n    assert score > 0\n\n    # Change the labels to have identical clusters\n    labels_identical = np.array([0, 0, 0, 0])\n    with pytest.raises(ValueError, match=\"Number of labels is\"):\n        davies_bouldin_score(X, labels_identical)\n", "def test_silhouette_score_sparse_matrix_diagonal():\n    # Check that silhouette_score works for sparse matrices with zero diagonal.\n    # Non-regression test for issue where silhouette_score fails with sparse matrices with zero diagonal.\n    \n    X = np.array([[0.0, 0.1, 0.1, 0.2, 0.1], [0.1, 0.0, 0.1, 0.2, 0.1], [0.1, 0.1, 0.0, 0.2, 0.1], [0.2, 0.2, 0.2, 0.0, 0.1], [0.1, 0.1, 0.1, 0.1, 0.0]])\n    X_sparse = csr_matrix(X)\n    y = [0, 0, 1, 1, 1]\n\n    # Ensure that silhouette score can handle sparse matrices with zero diagonal\n    result_sparse = silhouette_score(X_sparse, y, metric=\"precomputed\")\n    result_dense = silhouette_score(X, y, metric=\"precomputed\")\n    assert result_sparse == pytest.approx(result_dense)\n", "def test_silhouette_sparse_with_sampling():\n    \"\"\"Check that silhouette_score works correctly for sparse matrices with sampling.\"\"\"\n    dataset = datasets.load_iris()\n    X_dense = dataset.data\n    X_csr = csr_matrix(X_dense)\n    X_csc = csc_matrix(X_dense)\n    X_dok = dok_matrix(X_dense)\n    X_lil = lil_matrix(X_dense)\n    y = dataset.target\n\n    for X in [X_csr, X_csc, X_dok, X_lil]:\n        D = pairwise_distances(X, metric=\"euclidean\")\n        score_precomputed = silhouette_score(\n            D, y, metric=\"precomputed\", sample_size=int(X.shape[0] / 2), random_state=0\n        )\n        score_euclidean = silhouette_score(\n            X, y, metric=\"euclidean\", sample_size=int(X.shape[0] / 2), random_state=0\n        )\n        assert score_precomputed > 0\n        assert score_euclidean > 0\n        pytest.approx(score_euclidean, score_precomputed)\n"], "sample_872": ["def test_roc_auc_score_multiclass_raise():\n    # Test that roc_auc_score raises an error when multi_class='raise' \n    # and y_true has more than 2 classes.\n    rng = check_random_state(404)\n    y_true = rng.randint(0, 3, size=20)\n    y_score = rng.rand(20, 3)\n\n    with pytest.raises(ValueError, match=\"multi_class must be in ('ovo', 'ovr')\"):\n        roc_auc_score(y_true, y_score, multi_class=\"raise\")\n", "def test_auc_memmap():\n    # Test if auc works correctly with np.memmap\n    x = np.array([0, 1, 2, 3])\n    y = np.array([0, 1, 4, 9])\n\n    # Create a temporary file\n    filename = 'temp_memmap.dat'\n    with open(filename, 'wb') as f:\n        f.write(b'\\x00' * (x.nbytes + y.nbytes))\n\n    # Create memmap arrays\n    x_memmap = np.memmap(filename, dtype=x.dtype, mode='r+', shape=x.shape, offset=0)\n    y_memmap = np.memmap(filename, dtype=y.dtype, mode='r+', shape=y.shape, offset=x.nbytes)\n\n    # Copy data to memmap arrays\n    x_memmap[:] = x\n    y_memmap[:] = y\n\n    # Test auc\n    assert_array_almost_equal(auc(x_memmap, y_memmap), 10.0)\n\n    # Clean up\n    import os\n    os.remove(filename)\n", "def test_auc_memmap():\n    # Test that auc function handles np.memmap inputs correctly\n    x = np.array([0, 0.5, 1])\n    y = np.array([0, 0.5, 1])\n    \n    # Create a memmap from the arrays\n    x_memmap = np.memmap('x_memmap.dat', dtype='float64', mode='w+', shape=x.shape)\n    y_memmap = np.memmap('y_memmap.dat', dtype='float64', mode='w+', shape=y.shape)\n    \n    x_memmap[:] = x[:]\n    y_memmap[:] = y[:]\n    \n    # Compute the auc with memmap inputs and compare to the expected result\n    assert_array_almost_equal(auc(x_memmap, y_memmap), 0.5)\n\n    # Clean up the memmap files\n    del x_memmap\n    del y_memmap\n", "def test_roc_auc_score_multilabel():\n    # Test the roc_auc_score function for multilabel classification.\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_score = np.array([[0.9, 0.1, 0.8], [0.2, 0.7, 0.3]])\n\n    # 'macro' average\n    macro_auc = roc_auc_score(y_true, y_score, average=\"macro\")\n    assert_almost_equal(macro_auc, (1 + 1) / 2)\n\n    # 'weighted' average\n    weighted_auc = roc_auc_score(y_true, y_score, average=\"weighted\")\n    assert_almost_equal(weighted_auc, (1 * 2 + 1 * 1) / 3)\n\n    # 'samples' average\n    samples_auc = roc_auc_score(y_true, y_score, average=\"samples\")\n    assert_almost_equal(samples_auc, (1 + 1) / 2)\n\n    # 'micro' average\n    micro_auc = roc_auc_score(y_true, y_score, average=\"micro\")\n    assert_almost_equal(micro_auc, 1.0)\n\n    # No average\n    no_average_auc = roc_auc_score(y_true, y_score, average=None)\n    assert_array_almost_equal(no_average_auc, [1, 1])\n\n    # Test binary pos_label consistency\n    y_true = np.array([[0, 1], [1, 0], [1, 1], [0, 0]])\n    y_score = np.array([[0.1, 0.9], [0.8, 0.2], [0.6, 0.4], [0.4, 0.6]])\n    auc_default = roc_auc_score(y_true, y_score)\n    auc_1 = roc_auc_score(y_true, y_score, pos_label=1)\n    auc_0 = roc_auc_score(y_true, y_score, pos_label=0)\n    assert_almost_equal(auc_default, auc_1)\n    assert_almost_equal(auc_0, 1 - auc_default)\n", "def test_auc_memmap():\n    # Test that auc works with numpy memmap\n    x = np.array([0, 0.5, 1])\n    y = np.array([0, 0.5, 1])\n    with tempfile.NamedTemporaryFile() as tmp:\n        memmap_x = np.memmap(tmp.name, dtype='float64', mode='w+', shape=x.shape)\n        memmap_y = np.memmap(tmp.name, dtype='float64', mode='w+', shape=y.shape)\n        memmap_x[:] = x\n        memmap_y[:] = y\n        assert_array_almost_equal(auc(memmap_x, memmap_y), 0.5)\n"], "sample_873": ["def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n\n    # Default input_features=None\n    feature_names_out = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    # Explicit input_features\n    feature_names_out_explicit = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_explicit)\n\n    # Check error when input_features do not match feature_names_in_\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=[\"W\", \"X\", \"Y\", \"Z\"])\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    \n    # Check feature names for transformed data\n    transformed_feature_names = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, transformed_feature_names)\n    \n    # Check feature names for transformed data with default input features\n    sel.feature_names_in_ = np.array(feature_names)\n    transformed_feature_names_default = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, transformed_feature_names_default)\n    \n    # Check feature names for transformed data without feature_names_in_\n    sel.feature_names_in_ = None\n    generated_feature_names = np.array([\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\"])\n    transformed_generated_feature_names = sel.get_feature_names_out()\n    assert_array_equal(generated_feature_names[::2], transformed_generated_feature_names)\n\n    # Check wrong shape raises error\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(np.array([\"a\", \"b\", \"c\"]))\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    # Check with default input features\n    sel.fit(X, y)\n    feature_names_out_default = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out_default)\n\n    # Check error when input_features doesn't match feature_names_in_\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=[\"wrong\", \"names\"])\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    # Check with default feature names\n    sel_no_features = StepSelector()\n    sel_no_features.fit(X)\n    default_feature_names_out = sel_no_features.get_feature_names_out()\n    assert_array_equal([f\"x{i}\" for i in support_inds], default_feature_names_out)\n\n    # Check with invalid input feature names raises error\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out([\"invalid_name\"])\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    \n    # Check feature names when input_features is provided\n    feature_names_out = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n    \n    # Check feature names when input_features is not provided\n    sel.feature_names_in_ = np.array(feature_names)\n    feature_names_out = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out)\n    \n    # Check feature names when input_features is None and feature_names_in_ is not defined\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out = sel.get_feature_names_out()\n    default_feature_names = np.array([\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\"])\n    assert_array_equal(default_feature_names[::2], feature_names_out)\n\n    # Check wrong input_features raises error\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out([\"a\", \"b\"])\n"], "sample_874": ["def test_set_params():\n    sel = StepSelector(step=3)\n    assert sel.get_params() == {'step': 3}\n\n    sel.set_params(step=5)\n    assert sel.get_params() == {'step': 5}\n\n    with pytest.raises(ValueError, match=\"Invalid parameter 'invalid' for estimator StepSelector\"):\n        sel.set_params(invalid=10)\n", "def test_clone_estimator():\n    \"\"\"Test that cloning an estimator creates a new, unfitted estimator with the same parameters.\"\"\"\n    sel = StepSelector(step=3)\n    sel_clone = clone(sel)\n\n    # Check that the clone is a different object\n    assert sel is not sel_clone\n\n    # Check that the parameters are the same\n    assert sel.get_params() == sel_clone.get_params()\n\n    # Check that the cloned estimator is unfitted\n    with pytest.raises(AttributeError, match=\"has no attribute 'n_features_in_'\"):\n        getattr(sel_clone, \"n_features_in_\")\n    \n    # Fit the original estimator and ensure the clone remains unfitted\n    sel.fit(X, y)\n    with pytest.raises(AttributeError, match=\"has no attribute 'n_features_in_'\"):\n        getattr(sel_clone, \"n_features_in_\")\n", "def test_clone():\n    \"\"\"Test the clone function.\"\"\"\n    sel = StepSelector(step=3)\n    sel_clone = clone(sel)\n    assert sel.get_params() == sel_clone.get_params()\n    assert sel is not sel_clone\n\n    # Test cloning when step is modified\n    sel.set_params(step=4)\n    sel_clone = clone(sel)\n    assert sel.get_params() == sel_clone.get_params()\n    assert sel is not sel_clone\n\n    # Test cloning a list of selectors\n    selectors = [StepSelector(step=2), StepSelector(step=3)]\n    clones = clone(selectors)\n    for orig, cl in zip(selectors, clones):\n        assert orig.get_params() == cl.get_params()\n        assert orig is not cl\n", "def test_estimator_clone():\n    \"\"\"Test cloning functionality of an estimator.\"\"\"\n    sel = StepSelector(step=3)\n    sel_clone = sel.__sklearn_clone__()\n\n    # Ensure the cloned object is a different instance\n    assert sel is not sel_clone\n\n    # Ensure the parameters are the same in the cloned object\n    assert sel.get_params() == sel_clone.get_params()\n\n    # Fit the original estimator and check that the clone is unfitted\n    sel.fit(X, y)\n    assert hasattr(sel, 'n_features_in_')\n    assert not hasattr(sel_clone, 'n_features_in_')\n", "def test_clone():\n    \"\"\"Test the clone function with StepSelector.\"\"\"\n    sel = StepSelector(step=3)\n    sel_cloned = clone(sel)\n    assert sel.get_params() == sel_cloned.get_params()\n    assert sel is not sel_cloned  # Ensure they are not the same object\n\n    # Check if the cloned selector works as expected\n    Xt_actual = sel_cloned.fit(X, y).transform(X)\n    assert_array_equal(Xt[:, :2], Xt_actual[:, :2])\n"], "sample_875": ["def test_multilabel_confusion_matrix_with_all_zeros():\n    # Test multilabel confusion matrix with all zeros input\n    y_true = np.array([[0, 0, 0], [0, 0, 0]])\n    y_pred = np.array([[0, 0, 0], [0, 0, 0]])\n    \n    expected_cm = np.array([[[2, 0], [0, 0]], [[2, 0], [0, 0]], [[2, 0], [0, 0]]])\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, expected_cm)\n\n    # Test with samplewise=True\n    expected_cm_samplewise = np.array([[[2, 0], [0, 0]], [[2, 0], [0, 0]]])\n    cm_samplewise = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    assert_array_equal(cm_samplewise, expected_cm_samplewise)\n\n    # Test with sample_weight\n    sample_weight = np.array([2, 1])\n    expected_cm_weighted = np.array([[[3, 0], [0, 0]], [[3, 0], [0, 0]], [[3, 0], [0, 0]]])\n    cm_weighted = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    assert_array_equal(cm_weighted, expected_cm_weighted)\n\n    # Test with labels parameter\n    expected_cm_labels = np.array([[[2, 0], [0, 0]], [[2, 0], [0, 0]]])\n    cm_labels = multilabel_confusion_matrix(y_true, y_pred, labels=[0, 1])\n    assert_array_equal(cm_labels, expected_cm_labels)\n", "def test_multilabel_hamming_loss_weighted():\n    # Test hamming_loss with sample weights in multilabel-indicator case\n    y1 = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]])\n    y2 = np.array([[0, 0, 1], [1, 0, 1], [1, 0, 0]])\n    sample_weight = np.array([2, 1, 3])\n\n    # Calculate hamming loss\n    loss = hamming_loss(y1, y2, sample_weight=sample_weight)\n    # Manually calculate expected loss\n    expected_loss = (\n        (2 * (1 / 3 + 0 / 3 + 1 / 3) + 1 * (0 / 3 + 0 / 3 + 0 / 3) + 3 * (0 / 3 + 1 / 3 + 0 / 3))\n        / (2 + 1 + 3)\n    )\n    assert_almost_equal(loss, expected_loss, decimal=6)\n", "def test_classification_report_multilabel_indicator():\n    # Test classification report with multilabel indicator format\n    y_true = np.array([[0, 1, 1], [1, 0, 0], [0, 0, 1], [1, 1, 0]])\n    y_pred = np.array([[0, 0, 1], [1, 0, 1], [0, 0, 1], [1, 1, 1]])\n\n    expected_report = \"\"\"\\\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         2\n           1       0.67      0.50      0.57         2\n           2       0.67      1.00      0.80         2\n\n   micro avg       0.75      0.83      0.79         6\n   macro avg       0.78      0.83      0.79         6", "def test_log_loss_with_empty_input():\n    # Test log_loss with empty input\n    y_true = np.array([])\n    y_pred = np.array([]).reshape(0, 2)\n    with pytest.raises(ValueError, match=\"y_true must not be empty\"):\n        log_loss(y_true, y_pred)\n\n    y_true = np.array([0, 1])\n    y_pred = np.array([]).reshape(0, 2)\n    with pytest.raises(ValueError, match=\"y_pred must not be empty\"):\n        log_loss(y_true, y_pred)\n\n    y_true = np.array([])\n    y_pred = np.array([[0.5, 0.5], [0.1, 0.9]])\n    with pytest.raises(ValueError, match=\"y_true must not be empty\"):\n        log_loss(y_true, y_pred)\n", "def test_classification_report_zero_division():\n    y_true = [1, 0, 1, 1]\n    y_pred = [0, 0, 0, 0]\n    with warnings.catch_warnings(record=True) as record:\n        report = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n        assert report[\"accuracy\"] == 0.0\n        assert report[\"macro avg\"][\"precision\"] == 0.0\n        assert report[\"macro avg\"][\"recall\"] == 0.0\n        assert report[\"macro avg\"][\"f1-score\"] == 0.0\n        assert report[\"weighted avg\"][\"precision\"] == 0.0\n        assert report[\"weighted avg\"][\"recall\"] == 0.0\n        assert report[\"weighted avg\"][\"f1-score\"] == 0.0\n        assert len(record) == 0  # No warnings expected\n\n        report = classification_report(y_true, y_pred, zero_division=1, output_dict=True)\n        assert report[\"accuracy\"] == 0.0\n        assert report[\"macro avg\"][\"precision\"] == 0.5\n        assert report[\"macro avg\"][\"recall\"] == 0.5\n        assert report[\"macro avg\"][\"f1-score\"] == 0.5\n        assert report[\"weighted avg\"][\"precision\"] == 0.5\n        assert report[\"weighted avg\"][\"recall\"] == 0.5\n        assert report[\"weighted avg\"][\"f1-score\"] == 0.5\n        assert len(record) == 0  # No warnings expected\n"], "sample_876": ["def test_mlp_regressor_custom_loss():\n    # Test MLPRegressor with a custom loss function\n        return np.mean(np.abs(y_true - y_pred))\n\n    class CustomMLPRegressor(MLPRegressor):\n            super()._initialize(y, layer_units, dtype)\n            self.loss = custom_loss\n\n    X = np.random.rand(100, 5)\n    y = np.random.rand(100)\n    reg = CustomMLPRegressor(hidden_layer_sizes=(10,), max_iter=200, random_state=0)\n    reg.fit(X, y)\n    assert reg.score(X, y) > 0.5  # Ensure the custom loss does not break the fit process\n", "def test_forward_pass():\n    # Test that the forward pass through the network produces the expected values\n    X = np.array([[0.6, 0.8, 0.7]])\n    mlp = MLPClassifier(\n        solver=\"sgd\",\n        activation=\"logistic\",\n        random_state=1,\n        max_iter=1,\n        hidden_layer_sizes=2,\n        momentum=0,\n    )\n    mlp.coefs_ = [\n        np.array([[0.1, 0.2], [0.3, 0.1], [0.5, 0]]),\n        np.array([[0.1], [0.2]])\n    ]\n    mlp.intercepts_ = [\n        np.array([0.1, 0.1]),\n        np.array([1.0])\n    ]\n    mlp.n_layers_ = 3\n    mlp.out_activation_ = \"logistic\"\n    mlp._random_state = np.random.RandomState(1)\n\n    activations = mlp._forward_pass([X, None, None])\n\n    assert_almost_equal(activations[1], np.array([[0.679178699175393, 0.574442516811659]]), decimal=5)\n    assert_almost_equal(activations[2], np.array([[0.7654329236196236]]), decimal=5)\n", "def test_mlp_classifier_max_fun():\n    # Test that the max_fun parameter in MLPClassifier limits the number of function calls\n    X = X_digits_multi[:50]\n    y = y_digits_multi[:50]\n\n    max_fun = 10\n    mlp = MLPClassifier(\n        solver=\"lbfgs\",\n        hidden_layer_sizes=5,\n        max_iter=200,\n        max_fun=max_fun,\n        random_state=1,\n    )\n\n    with pytest.warns(ConvergenceWarning):\n        mlp.fit(X, y)\n        assert mlp.n_iter_ <= max_fun\n", "def test_activation_functions():\n    # Test that all activation functions work correctly in MLPClassifier and MLPRegressor.\n    X, y = X_digits_binary[:50], y_digits_binary[:50]\n    activations = [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n    \n    for activation in activations:\n        clf = MLPClassifier(hidden_layer_sizes=5, activation=activation, random_state=1, max_iter=50)\n        reg = MLPRegressor(hidden_layer_sizes=5, activation=activation, random_state=1, max_iter=50)\n        \n        with ignore_warnings(category=ConvergenceWarning):\n            clf.fit(X, y)\n            reg.fit(X, y)\n        \n        clf_predict = clf.predict(X)\n        reg_predict = reg.predict(X)\n        \n        assert len(clf_predict) == len(y)\n        assert len(reg_predict) == len(y)\n        \n        # Check if predict_proba and predict_log_proba are implemented and work correctly for classifiers\n        if activation != \"identity\":\n            clf_proba = clf.predict_proba(X)\n            clf_log_proba = clf.predict_log_proba(X)\n            \n            assert clf_proba.shape == (X.shape[0], 2)\n            assert clf_log_proba.shape == (X.shape[0], 2)\n            assert_allclose(np.log(clf_proba), clf_log_proba)\n", "def test_mlp_regressor_output_activation_identity():\n    # Test that MLPRegressor correctly uses 'identity' as the output activation.\n    X, y = make_regression(n_samples=200, n_features=10, bias=20.0, noise=100.0, random_state=7)\n    y = scale(y)\n    \n    mlp = MLPRegressor(\n        solver=\"adam\",\n        hidden_layer_sizes=(50,),\n        activation=\"relu\",\n        max_iter=150,\n        random_state=1,\n        learning_rate_init=0.01\n    )\n    \n    mlp.fit(X, y)\n    y_pred = mlp.predict(X)\n    \n    # Check if the output activation function is identity\n    assert mlp.out_activation_ == \"identity\"\n    \n    # Check if predictions are close to the actual values (R2 score should be high)\n    assert r2_score(y, y_pred) > 0.8\n"], "sample_877": ["def test_isotonic_regression_with_negative_weights():\n    # Test isotonic regression with negative weights to ensure proper handling\n    x = [1, 2, 3, 4, 5, 6, 7]\n    y = [1, 2, 3, 4, 5, 6, 7]\n    sample_weight = [1, -1, 1, -1, 1, -1, 1]\n\n    # Ensure that a ValueError is raised when negative weights are used\n    ir = IsotonicRegression()\n    with pytest.raises(ValueError, match=\"Sample weights must be non-negative\"):\n        ir.fit(x, y, sample_weight=sample_weight)\n", "def test_check_increasing_zero_spearman_correlation():\n    x = [0, 1, 2, 3, 4, 5]\n    y = [10, 1, 10, 1, 10, 1]\n\n    # Check that we got a warning about zero Spearman correlation interval\n    msg = \"Confidence interval of the Spearman correlation coefficient spans zero\"\n    with pytest.warns(UserWarning, match=msg):\n        is_increasing = check_increasing(x, y)\n\n    # Since correlation is zero, increasing could be either True or False\n    assert isinstance(is_increasing, bool)\n", "def test_isotonic_regression_empty_input():\n    # Ensure IsotonicRegression raises error if input is empty\n    X = np.array([])\n    y = np.array([])\n\n    ir = IsotonicRegression()\n\n    msg = \"Found array with 0 sample(s)\"\n    with pytest.raises(ValueError, match=msg):\n        ir.fit(X, y)\n\n    with pytest.raises(ValueError, match=msg):\n        isotonic_regression(y)\n", "def test_isotonic_regression_with_nan_inputs():\n    # Check that IsotonicRegression raises an error if input contains NaNs\n    X = np.array([1, 2, 3, 4, np.nan, 6, 7])\n    y = np.array([1, 2, 3, 1, 2, 5, 4])\n    ir = IsotonicRegression()\n\n    msg = \"Input contains NaN\"\n    with pytest.raises(ValueError, match=msg):\n        ir.fit(X, y)\n        \n    X = np.array([1, 2, 3, 4, 5, 6, 7])\n    y = np.array([1, 2, 3, np.nan, 2, 5, 4])\n\n    with pytest.raises(ValueError, match=msg):\n        ir.fit(X, y)\n", "def test_isotonic_regression_with_nan_values():\n    # Test how isotonic regression handles NaN values in input\n    x = np.array([1, 2, 3, np.nan, 5, 6])\n    y = np.array([1, 2, 3, 4, 5, 6])\n\n    ir = IsotonicRegression()\n    msg = \"Input contains NaN, infinity or a value too large for dtype\"\n    with pytest.raises(ValueError, match=msg):\n        ir.fit(x, y)\n\n    x = np.array([1, 2, 3, 4, 5, 6])\n    y = np.array([1, 2, 3, np.nan, 5, 6])\n    with pytest.raises(ValueError, match=msg):\n        ir.fit(x, y)\n"], "sample_878": ["def test_column_transformer_set_params_remainder():\n    # Test setting parameters of remainder\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    ct = ColumnTransformer([(\"trans1\", StandardScaler(), [0])], remainder=StandardScaler())\n\n    # Check initial parameters\n    params = ct.get_params()\n    assert params[\"remainder__with_mean\"]\n    assert params[\"remainder__with_std\"]\n\n    # Change parameters\n    ct.set_params(remainder__with_mean=False, remainder__with_std=False)\n    params = ct.get_params()\n    assert not params[\"remainder__with_mean\"]\n    assert not params[\"remainder__with_std\"]\n\n    # Fit and transform with new parameters\n    X_trans = ct.fit_transform(X_array)\n    assert np.allclose(X_trans[:, 1:], (X_array[:, 1:] - X_array[:, 1:].mean(axis=0)))\n\n    # Change parameters to passthrough\n    ct.set_params(remainder=\"passthrough\")\n    params = ct.get_params()\n    assert params[\"remainder\"] == \"passthrough\"\n\n    # Fit and transform with passthrough\n    X_trans = ct.fit_transform(X_array)\n    assert_array_equal(X_trans, X_array)\n\n    # Change parameters to drop\n    ct.set_params(remainder=\"drop\")\n    params = ct.get_params()\n    assert params[\"remainder\"] == \"drop\"\n\n    # Fit and transform with drop\n    X_trans = ct.fit_transform(X_array)\n    assert_array_equal(X_trans, X_array[:, :1])\n", "def test_column_transformer_set_output_on_cloned_transformer():\n    \"\"\"Check that set_output is respected even after cloning the transformer.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"col1\": [1, 2, 3],\n            \"col2\": [4, 5, 6],\n            \"col3\": [7, 8, 9],\n        }\n    )\n\n    ct = ColumnTransformer(\n        [(\"trans1\", StandardScaler(), [\"col1\"]), (\"trans2\", StandardScaler(), [\"col2\"])],\n        remainder=\"passthrough\",\n        verbose_feature_names_out=False\n    ).set_output(transform=\"pandas\")\n\n    # Fit the transformer and clone it\n    ct.fit(df)\n    ct_cloned = clone(ct)\n\n    # Transform the data with the cloned transformer\n    df_trans = ct_cloned.transform(df)\n\n    # Ensure the output is a DataFrame\n    assert isinstance(df_trans, pd.DataFrame)\n    assert_array_equal(df_trans.columns, [\"col1\", \"col2\", \"col3\"])\n\n    # Check that the data is correctly transformed\n    df_trans_expected = pd.DataFrame(\n        {\n            \"col1\": [-1.22474487, 0.0, 1.22474487],\n            \"col2\": [-1.22474487, 0.0, 1.22474487],\n            \"col3\": [7, 8, 9],\n        }\n    )\n    assert_allclose(df_trans, df_trans_expected)\n", "def test_column_transformer_with_mixed_dtypes():\n    \"\"\"Test ColumnTransformer with mixed dtypes and FunctionTransformer\"\"\"\n\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame({\n        \"int_col\": [1, 2, 3],\n        \"float_col\": [0.1, 0.2, 0.3],\n        \"str_col\": [\"a\", \"b\", \"c\"]\n    })\n\n        return X * 2\n\n    double_transformer = FunctionTransformer(double_values)\n    ohe_transformer = OneHotEncoder()\n\n    ct = ColumnTransformer(\n        [\n            (\"double_int\", double_transformer, \"int_col\"),\n            (\"double_float\", double_transformer, \"float_col\"),\n            (\"onehot_str\", ohe_transformer, \"str_col\"),\n        ],\n        remainder=\"passthrough\",\n    )\n\n    X_trans = ct.fit_transform(X_df)\n\n    expected_output = np.hstack((\n        (X_df[\"int_col\"] * 2).values.reshape(-1, 1),\n        (X_df[\"float_col\"] * 2).values.reshape(-1, 1),\n        ohe_transformer.fit_transform(X_df[[\"str_col\"]]).toarray()\n    ))\n\n    assert_array_equal(X_trans, expected_output)\n", "def test_column_transformer_set_params_invalid_key():\n    # Test setting parameters with invalid key\n    ct = ColumnTransformer(\n        [(\"trans1\", StandardScaler(), [0]), (\"trans2\", StandardScaler(), [1])]\n    )\n    with pytest.raises(ValueError, match=\"Invalid parameter trans3 for estimator\"):\n        ct.set_params(trans3__with_mean=False)\n\n    # Test setting parameters with invalid parameter name for a valid transformer\n    with pytest.raises(ValueError, match=\"Invalid parameter with_mean_wrong for estimator\"):\n        ct.set_params(trans1__with_mean_wrong=False)\n", "def test_column_transformer_callable_column_with_fit_params():\n    \"\"\"Test callable column specifier with additional fit parameters.\"\"\"\n    X = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n    y = np.array([1, 0, 1])\n\n        assert X.shape[0] == len(y)\n        return [0, 2]\n\n    ct = ColumnTransformer(\n        [(\"trans1\", StandardScaler(), column_selector)],\n        remainder=\"passthrough\"\n    )\n\n    X_trans = ct.fit_transform(X, y=y)\n    expected_X_trans = StandardScaler().fit_transform(X[:, [0, 2]])\n    expected_X_trans = np.hstack([expected_X_trans, X[:, 1].reshape(-1, 1)])\n\n    assert_allclose(X_trans, expected_X_trans)\n"], "sample_879": ["def test_ohe_handle_unknown_infrequent_if_exist():\n    \"\"\"Check that the OneHotEncoder handles infrequent categories correctly with 'infrequent_if_exist'.\"\"\"\n\n    X = np.array([[\"cat\"], [\"dog\"], [\"cat\"], [\"bird\"], [\"dog\"], [\"dog\"], [\"fish\"], [\"fish\"], [\"fish\"]])\n    ohe = OneHotEncoder(handle_unknown=\"infrequent_if_exist\", min_frequency=2, sparse_output=False)\n    ohe.fit(X)\n\n    assert_array_equal(ohe.infrequent_categories_, [[\"bird\"]])\n\n    X_test = np.array([[\"cat\"], [\"bird\"], [\"dog\"], [\"fish\"], [\"rabbit\"]])\n    X_trans = ohe.transform(X_test)\n\n    expected = np.array([\n        [1, 0, 0],\n        [0, 0, 1],\n        [0, 1, 0],\n        [0, 0, 0],\n        [0, 0, 1]\n    ])\n    \n    assert_allclose(X_trans, expected)\n\n    expected_inv = [[\"cat\"], [\"infrequent_sklearn\"], [\"dog\"], [\"fish\"], [\"infrequent_sklearn\"]]\n    X_inv = ohe.inverse_transform(X_trans)\n    assert_array_equal(expected_inv, X_inv)\n\n    feature_names = ohe.get_feature_names_out()\n    assert_array_equal([\"x0_cat\", \"x0_dog\", \"x0_infrequent_sklearn\"], feature_names)\n", "def test_one_hot_encoder_sparse_output_false():\n    # check that sparse_output=False returns a dense array\n    X = np.array([[0, 1], [1, 0], [0, 1]])\n    enc = OneHotEncoder(sparse_output=False)\n    X_trans = enc.fit_transform(X)\n    \n    assert isinstance(X_trans, np.ndarray)\n    assert not sparse.issparse(X_trans)\n\n    expected_output = np.array([\n        [1., 0., 1., 0.],\n        [0., 1., 0., 1.],\n        [1., 0., 1., 0.]\n    ])\n\n    assert_array_equal(X_trans, expected_output)\n", "def test_one_hot_encoder_handle_unknown_infrequent_if_exist():\n    # Test that 'infrequent_if_exist' properly handles unknown categories\n    X = [[\"apple\", 1], [\"banana\", 2], [\"apple\", 3], [\"cherry\", 1], [\"cherry\", 2]]\n    X_test = [[\"banana\", 1], [\"grape\", 2], [\"cherry\", 4]]\n\n    ohe = OneHotEncoder(handle_unknown=\"infrequent_if_exist\", min_frequency=2, sparse_output=False)\n    ohe.fit(X)\n    \n    assert_array_equal(ohe.infrequent_categories_, [[\"apple\"], None])\n    \n    # Check transformed output\n    X_trans = ohe.transform(X_test)\n    expected_trans = np.array([\n        [1, 0, 1, 0],  # Known category \"banana\" and known integer 1\n        [0, 0, 0, 0],  # Unknown category \"grape\" treated as infrequent\n        [0, 1, 0, 0],  # Known category \"cherry\" and unknown integer 4 treated as infrequent\n    ])\n    assert_allclose(X_trans, expected_trans)\n\n    # Check inverse transformation\n    X_inv = ohe.inverse_transform(X_trans)\n    expected_inv = np.array([\n        [\"banana\", 1],\n        [\"infrequent_sklearn\", 2],\n        [\"cherry\", \"infrequent_sklearn\"],\n    ], dtype=object)\n    assert_array_equal(X_inv, expected_inv)\n\n    # Check feature names\n    expected_feature_names = [\"x0_banana\", \"x0_cherry\", \"x1_1\", \"x1_2\"]\n    assert_array_equal(ohe.get_feature_names_out(), expected_feature_names)\n", "def test_one_hot_encoder_non_string_categories():\n    \"\"\"Test OneHotEncoder with non-string categories including integers and floats.\"\"\"\n    X = np.array([[1, 2.5, 'cat'], [2, 3.5, 'dog'], [3, 2.5, 'cat'], [1, 4.5, 'mouse']])\n    enc = OneHotEncoder()\n    X_trans = enc.fit_transform(X)\n\n    # Expected categories and transformation\n    expected_categories = [np.array([1, 2, 3]), np.array([2.5, 3.5, 4.5]), np.array(['cat', 'dog', 'mouse'])]\n    expected_transformed = np.array([\n        [1., 0., 0., 1., 0., 0., 1., 0., 0.],\n        [0., 1., 0., 0., 1., 0., 0., 1., 0.],\n        [0., 0., 1., 1., 0., 0., 1., 0., 0.],\n        [1., 0., 0., 0., 0., 1., 0., 0., 1.]\n    ])\n\n    for i, categories in enumerate(enc.categories_):\n        assert_array_equal(categories, expected_categories[i])\n\n    assert_array_equal(X_trans.toarray(), expected_transformed)\n", "def test_ordinal_encoder_custom_unknown_value():\n    # Check if custom unknown_value is properly handled\n    X = np.array([[\"dog\"], [\"cat\"], [\"mouse\"], [\"horse\"], [\"cat\"]], dtype=object)\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-999)\n    enc.fit(X)\n\n    X_test = np.array([[\"dog\"], [\"lion\"], [\"mouse\"]], dtype=object)\n    X_trans = enc.transform(X_test)\n\n    # Check if the unknown category 'lion' is encoded as -999\n    expected = np.array([[1], [-999], [2]])\n    assert_array_equal(X_trans, expected)\n\n    # Check inverse transform\n    X_inv = enc.inverse_transform(X_trans)\n    expected_inv = np.array([[\"dog\"], [None], [\"mouse\"]], dtype=object)\n    assert_array_equal(X_inv, expected_inv)\n"], "sample_880": ["def test_unique_labels_mixed_formats():\n    # Test for mixed formats with unique_labels function\n\n    # Mix of lists and numpy arrays\n    assert_array_equal(unique_labels([3, 5, 7], np.array([1, 2, 3])), np.array([1, 2, 3, 5, 7]))\n    assert_array_equal(unique_labels([1, 2], np.array([[0, 1], [1, 0]])), np.array([0, 1, 2]))\n\n    # Mix of sparse matrix and numpy arrays\n    sparse_matrix = csr_matrix(np.array([[0, 1], [1, 0]]))\n    assert_array_equal(unique_labels(np.array([2, 3]), sparse_matrix), np.array([0, 1, 2, 3]))\n\n    # Test for mixed data types\n    with pytest.raises(ValueError, match=\"Mix of label input types\"):\n        unique_labels([1, 2, 'a'], np.array([3, 'b']))\n\n    # Test for mix of multilabel indicator and multiclass\n    with pytest.raises(ValueError, match=\"Mix type of y not allowed\"):\n        unique_labels([[1, 2]], [3, 4, 5])\n", "def test_ovr_decision_function_tiebreaker():\n    # Test tiebreaking logic in _ovr_decision_function\n\n    # Predictions and confidences leading to ties with slight differences in confidence\n    predictions = np.array([[0, 1, 1], [1, 0, 1], [1, 0, 1], [1, 0, 1]])\n\n    confidences = np.array(\n        [[0.1, 0.1, 0.1], [0.1, 0.2, -0.1], [0.2, 0.1, 0.1], [0.3, 0.1, 0.1]]\n    )\n\n    n_classes = 3\n\n    dec_values = _ovr_decision_function(predictions, confidences, n_classes)\n\n    # Check that the decision values are within 0.5 range of the votes\n    votes = np.array([[1, 0, 2], [1, 1, 1], [1, 1, 1], [1, 1, 1]])\n\n    assert_allclose(votes, dec_values, atol=0.5)\n\n    # Check that the predictions are what we expect\n    expected_prediction = np.array([2, 1, 1, 1])\n    assert_array_equal(np.argmax(dec_values, axis=1), expected_prediction)\n\n    # Assert that higher confidence breaks the tie as expected\n    assert dec_values[1, 1] > dec_values[1, 0]\n    assert dec_values[2, 1] > dec_values[2, 0]\n    assert dec_values[3, 1] > dec_values[3, 0]\n", "def test_check_partial_fit_first_call():\n    class MockClassifier:\n            self.classes_ = None\n\n    clf = MockClassifier()\n\n    # Test first call with classes provided\n    assert _check_partial_fit_first_call(clf, classes=[0, 1])\n    assert_array_equal(clf.classes_, [0, 1])\n\n    # Test subsequent call with same classes\n    assert not _check_partial_fit_first_call(clf, classes=[0, 1])\n    assert_array_equal(clf.classes_, [0, 1])\n\n    # Test subsequent call with different classes\n    with pytest.raises(ValueError, match=\"`classes=.* is not the same as on last call\"):\n        _check_partial_fit_first_call(clf, classes=[0, 1, 2])\n\n    # Test first call without classes provided\n    clf = MockClassifier()\n    with pytest.raises(ValueError, match=\"classes must be passed on the first call\"):\n        _check_partial_fit_first_call(clf)\n", "def test_check_partial_fit_first_call():\n    class DummyClassifier:\n            self.classes_ = None\n\n    clf = DummyClassifier()\n\n    # Case 1: First call with classes provided\n    assert _check_partial_fit_first_call(clf, classes=[0, 1]) is True\n    assert_array_equal(clf.classes_, [0, 1])\n\n    # Case 2: Subsequent call with the same classes\n    assert _check_partial_fit_first_call(clf, classes=[0, 1]) is False\n\n    # Case 3: Subsequent call with different classes\n    with pytest.raises(ValueError, match=\"`classes=\\[0, 2\\]` is not the same as on last call to partial_fit, was: \\[0, 1\\]\"):\n        _check_partial_fit_first_call(clf, classes=[0, 2])\n\n    # Case 4: First call with no classes provided\n    clf = DummyClassifier()\n    with pytest.raises(ValueError, match=\"classes must be passed on the first call to partial_fit.\"):\n        _check_partial_fit_first_call(clf)\n", "def test_is_multilabel_with_sparse_matrix():\n    # Test is_multilabel with different types of sparse matrices\n\n    sparse_matrices = [\n        csr_matrix([[0, 1], [1, 0]]),\n        csc_matrix([[0, 1], [1, 0]]),\n        coo_matrix([[0, 1], [1, 0]]),\n        dok_matrix([[0, 1], [1, 0]]),\n        lil_matrix([[0, 1], [1, 0]]),\n    ]\n\n    for sparse_matrix in sparse_matrices:\n        assert is_multilabel(sparse_matrix)\n        assert not is_multilabel(sparse_matrix[:1, :1])\n\n    # Test with empty sparse matrices\n    empty_matrices = [\n        csr_matrix((2, 2)),\n        csc_matrix((2, 2)),\n        coo_matrix((2, 2)),\n        dok_matrix((2, 2)),\n        lil_matrix((2, 2)),\n    ]\n\n    for empty_matrix in empty_matrices:\n        assert not is_multilabel(empty_matrix)\n\n    # Test with sparse matrices having one column\n    one_col_matrices = [\n        csr_matrix([[0], [1], [0]]),\n        csc_matrix([[0], [1], [0]]),\n        coo_matrix([[0], [1], [0]]),\n        dok_matrix([[0], [1], [0]]),\n        lil_matrix([[0], [1], [0]]),\n    ]\n\n    for one_col_matrix in one_col_matrices:\n        assert not is_multilabel(one_col_matrix)\n"], "sample_881": ["def test_auc_numpy_memmap():\n    # Test AUC computation with numpy.memmap\n    filename = 'test_memmap.dat'\n    x = np.memmap(filename, dtype='float64', mode='w+', shape=(100,))\n    y = np.memmap(filename, dtype='float64', mode='w+', shape=(100,))\n    \n    x[:] = np.linspace(0, 1, 100)\n    y[:] = np.linspace(0, 1, 100)\n    \n    assert_array_almost_equal(auc(x, y), 0.5)\n    \n    x[:] = np.linspace(1, 0, 100)\n    y[:] = np.linspace(0, 1, 100)\n    \n    assert_array_almost_equal(auc(x, y), 0.5)\n    \n    x[:] = np.linspace(1, 0, 100)\n    y[:] = np.ones(100)\n    \n    assert_array_almost_equal(auc(x, y), 1.0)\n    \n    del x, y\n    import os\n    os.remove(filename)\n", "def test_roc_auc_score_multilabel_errors():\n    # Test that roc_auc_score function raises an error for inappropriate multilabel\n    y_true = np.array([[0, 1], [1, 0]])\n    y_score = np.array([[0.7, 0.2], [0.3, 0.6]])\n\n    # Test for unsupported average method in multilabel\n    with pytest.raises(ValueError, match=r\"average must be one of \\('macro', 'weighted', None\\) for multilabel-indicator targets\"):\n        roc_auc_score(y_true, y_score, average=\"samples\")\n\n    # Test for unsupported multi_class method in multilabel\n    with pytest.raises(ValueError, match=r\"multi_class must be one of \\('ovr'\\) for multilabel-indicator targets\"):\n        roc_auc_score(y_true, y_score, multi_class=\"ovo\")\n\n    # Test for inconsistent number of labels in y_true and y_score\n    y_score_inconsistent = np.array([[0.7, 0.2, 0.1], [0.3, 0.6, 0.1]])\n    with pytest.raises(ValueError, match=r\"Number of classes in y_true not equal to the number of columns in 'y_score'\"):\n        roc_auc_score(y_true, y_score_inconsistent)\n", "def test_roc_auc_score_multiclass_macro_average():\n    # Test roc_auc_score for multiclass with macro average\n    y_true = np.array([0, 1, 2, 2])\n    y_scores = np.array(\n        [[0.1, 0.8, 0.1], [0.3, 0.4, 0.3], [0.35, 0.5, 0.15], [0, 0.2, 0.8]]\n    )\n    # Expected outputs computed manually\n    expected_macro_roc_auc = 0.75\n\n    # roc_auc_score with 'macro' average\n    assert_almost_equal(\n        roc_auc_score(y_true, y_scores, average=\"macro\", multi_class=\"ovr\"),\n        expected_macro_roc_auc\n    )\n", "def test_roc_auc_score_multilabel():\n    # Test roc_auc_score for multilabel-indicator format\n    y_true = np.array([[0, 1, 0], [1, 0, 1], [1, 1, 0], [0, 0, 1]])\n    y_score = np.array([[0.1, 0.9, 0.2], [0.8, 0.1, 0.7], [0.9, 0.8, 0.3], [0.2, 0.4, 0.9]])\n    expected_score_macro = (0.6666667 + 1.0 + 0.5) / 3.0  # average of individual roc_auc_scores for each label\n    expected_score_micro = 0.7272727\n\n    # Check macro average\n    assert_almost_equal(\n        roc_auc_score(y_true, y_score, average=\"macro\"), expected_score_macro, decimal=6\n    )\n\n    # Check micro average\n    assert_almost_equal(\n        roc_auc_score(y_true, y_score, average=\"micro\"), expected_score_micro, decimal=6\n    )\n\n    # Check None average\n    scores = roc_auc_score(y_true, y_score, average=None)\n    assert len(scores) == y_true.shape[1]\n    for s in scores:\n        assert 0 <= s <= 1\n", "def test_auc_memmap():\n    # Test that auc function works with numpy memmap\n    x = np.array([0, 1, 2, 3, 4, 5])\n    y = np.array([0, 1, 4, 9, 16, 25])\n    with pytest.raises(ValueError, match=\"At least 2 points are needed\"):\n        auc(np.memmap(x, dtype=x.dtype, mode='w+', shape=x.shape), np.memmap(y[:1], dtype=y.dtype, mode='w+', shape=y[:1].shape))\n\n    assert_array_almost_equal(auc(np.memmap(x, dtype=x.dtype, mode='w+', shape=x.shape), np.memmap(y, dtype=y.dtype, mode='w+', shape=y.shape)), 75.0)\n"], "sample_882": ["def test_predict_proba_edge_cases():\n    # Test edge cases for predict_proba method in MLPClassifier\n    X = [[0, 0], [1, 1], [10, 10], [100, 100]]\n    y = [0, 1, 1, 0]\n    clf = MLPClassifier(random_state=0, hidden_layer_sizes=(10,), solver=\"lbfgs\")\n    clf.fit(X, y)\n    \n    # Check probabilities for extreme values in X\n    y_proba = clf.predict_proba([[0, 0], [1000, 1000], [-1000, -1000]])\n    \n    assert y_proba.shape == (3, 2)\n    \n    # Probabilities should sum to 1\n    assert_allclose(y_proba.sum(axis=1), np.ones(3))\n    \n    # Check for valid probability values\n    assert np.all(y_proba >= 0) and np.all(y_proba <= 1)\n\n    # Check log probabilities\n    y_log_proba = clf.predict_log_proba([[0, 0], [1000, 1000], [-1000, -1000]])\n    assert_allclose(y_log_proba, np.log(y_proba), rtol=1e-5)\n", "def test_activation_functions():\n    # Test all activation functions in MLPClassifier and MLPRegressor\n    X, y = X_digits_binary[:50], y_digits_binary[:50]\n    X_reg, y_reg = make_regression(n_samples=50, n_features=10, noise=0.1, random_state=1)\n\n    for activation in ACTIVATION_TYPES:\n        # Test MLPClassifier with different activations\n        clf = MLPClassifier(hidden_layer_sizes=(10,), activation=activation, random_state=1)\n        with ignore_warnings(category=ConvergenceWarning):\n            clf.fit(X, y)\n        assert clf.score(X, y) > 0.8\n\n        # Test MLPRegressor with different activations\n        reg = MLPRegressor(hidden_layer_sizes=(10,), activation=activation, random_state=1)\n        with ignore_warnings(category=ConvergenceWarning):\n            reg.fit(X_reg, y_reg)\n        assert reg.score(X_reg, y_reg) > 0.5\n", "def test_mlp_classifier_max_iter():\n    # Test that max_iter parameter controls the number of iterations correctly.\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n\n    # Test with max_iter=1\n    clf1 = MLPClassifier(hidden_layer_sizes=10, max_iter=1, random_state=1)\n    with pytest.warns(ConvergenceWarning):\n        clf1.fit(X, y)\n    assert clf1.n_iter_ == 1\n\n    # Test with max_iter=10\n    clf2 = MLPClassifier(hidden_layer_sizes=10, max_iter=10, random_state=1)\n    with pytest.warns(ConvergenceWarning):\n        clf2.fit(X, y)\n    assert clf2.n_iter_ == 10\n\n    # Test with max_iter=50\n    clf3 = MLPClassifier(hidden_layer_sizes=10, max_iter=50, random_state=1)\n    with pytest.warns(ConvergenceWarning):\n        clf3.fit(X, y)\n    assert clf3.n_iter_ == 50\n\n    # Test with max_iter=200 (default)\n    clf4 = MLPClassifier(hidden_layer_sizes=10, max_iter=200, random_state=1)\n    with pytest.warns(ConvergenceWarning):\n        clf4.fit(X, y)\n    assert clf4.n_iter_ == 200\n", "def test_pack_unpack():\n    # Test that packing and unpacking parameters does not change them\n    X, y = make_regression(n_samples=50, n_features=10, random_state=0)\n    mlp = MLPRegressor(hidden_layer_sizes=(5,), random_state=0)\n    mlp.fit(X, y)\n    \n    packed_params = _pack(mlp.coefs_, mlp.intercepts_)\n    mlp._unpack(packed_params)\n    \n    for coef, coef_packed in zip(mlp.coefs_, mlp._coef_indptr):\n        start, end, shape = coef_packed\n        assert_array_equal(coef, np.reshape(packed_params[start:end], shape))\n    \n    for intercept, intercept_packed in zip(mlp.intercepts_, mlp._intercept_indptr):\n        start, end = intercept_packed\n        assert_array_equal(intercept, packed_params[start:end])\n", "def test_mlp_solver_parameter_constraints():\n    # Test that invalid solver parameters raise appropriate errors.\n    X = X_digits_binary[:100]\n    y = y_digits_binary[:100]\n\n    # Test invalid solver name\n    with pytest.raises(ValueError, match=\"The 'solver' parameter of MLPClassifier must be a str among {'lbfgs', 'sgd', 'adam'}. Got 'invalid_solver' instead.\"):\n        MLPClassifier(solver='invalid_solver').fit(X, y)\n\n    # Test invalid activation function\n    with pytest.raises(ValueError, match=\"The 'activation' parameter of MLPClassifier must be a str among {'identity', 'logistic', 'tanh', 'relu'}. Got 'invalid_activation' instead.\"):\n        MLPClassifier(activation='invalid_activation').fit(X, y)\n\n    # Test invalid learning rate\n    with pytest.raises(ValueError, match=\"The 'learning_rate' parameter of MLPClassifier must be a str among {'constant', 'invscaling', 'adaptive'}. Got 'invalid_learning_rate' instead.\"):\n        MLPClassifier(learning_rate='invalid_learning_rate').fit(X, y)\n"], "sample_883": ["def test_bayesian_ridge_predict_return_type():\n    # Test that the predict method returns arrays of the correct type and shape\n    X = np.array([[1, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n    y = np.array([1, 2, 3, 4, 5])\n    model = BayesianRidge()\n    model.fit(X, y)\n    \n    # Check return type and shape without standard deviation\n    predictions = model.predict(X)\n    assert isinstance(predictions, np.ndarray)\n    assert predictions.shape == (X.shape[0],)\n    \n    # Check return type and shape with standard deviation\n    predictions, std_dev = model.predict(X, return_std=True)\n    assert isinstance(predictions, np.ndarray)\n    assert predictions.shape == (X.shape[0],)\n    assert isinstance(std_dev, np.ndarray)\n    assert std_dev.shape == (X.shape[0],)\n", "def test_bayesian_ridge_alpha_lambda_shapes():\n    \"\"\"Test BayesianRidge to check shapes of alpha_ and lambda_ attributes\n    for different input dimensions.\"\"\"\n    X = np.random.randn(10, 5)\n    y = np.random.randn(10)\n    model = BayesianRidge()\n    model.fit(X, y)\n    assert model.alpha_.shape == (), \"alpha_ should be a scalar.\"\n    assert model.lambda_.shape == (), \"lambda_ should be a scalar.\"\n\n    X = np.random.randn(20, 10)\n    y = np.random.randn(20)\n    model.fit(X, y)\n    assert model.alpha_.shape == (), \"alpha_ should be a scalar.\"\n    assert model.lambda_.shape == (), \"lambda_ should be a scalar.\"\n", "def test_bayesian_ridge_parameter_constraints():\n    \"\"\"Test that BayesianRidge parameter constraints are enforced.\"\"\"\n    with pytest.raises(ValueError, match=\"Expected max_iter >= 1\"):\n        BayesianRidge(max_iter=0)\n    with pytest.raises(ValueError, match=\"Expected tol > 0\"):\n        BayesianRidge(tol=0)\n    with pytest.raises(ValueError, match=\"Expected alpha_1 > 0\"):\n        BayesianRidge(alpha_1=0)\n    with pytest.raises(ValueError, match=\"Expected alpha_2 > 0\"):\n        BayesianRidge(alpha_2=0)\n    with pytest.raises(ValueError, match=\"Expected lambda_1 > 0\"):\n        BayesianRidge(lambda_1=0)\n    with pytest.raises(ValueError, match=\"Expected lambda_2 > 0\"):\n        BayesianRidge(lambda_2=0)\n    with pytest.raises(ValueError, match=\"Expected alpha_init > 0\"):\n        BayesianRidge(alpha_init=0)\n    with pytest.raises(ValueError, match=\"Expected lambda_init > 0\"):\n        BayesianRidge(lambda_init=0)\n    with pytest.raises(ValueError, match=\"Unknown label type: 'unknown'\"):\n        BayesianRidge(verbose=\"unknown\")\n", "def test_bayesian_ridge_default_params():\n    # Test BayesianRidge with default parameters\n    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n    y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n\n    clf = BayesianRidge()\n    clf.fit(X, y)\n\n    # Check if the fitted model has the expected default parameters\n    assert clf.max_iter == 300  # default value\n    assert clf.tol == 1e-3\n    assert clf.alpha_1 == 1e-6\n    assert clf.alpha_2 == 1e-6\n    assert clf.lambda_1 == 1e-6\n    assert clf.lambda_2 == 1e-6\n    assert clf.alpha_init is None\n    assert clf.lambda_init is None\n    assert clf.compute_score is False\n    assert clf.fit_intercept is True\n    assert clf.copy_X is True\n    assert clf.verbose is False\n    assert clf.n_iter == \"deprecated\"  # default value\n", "def test_bayesian_ridge_predict_with_std():\n    # Test that BayesianRidge's predict method returns correct mean and standard deviation\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([2.1, 4.3, 6.1, 8.2])\n    \n    model = BayesianRidge()\n    model.fit(X, y)\n    \n    X_test = np.array([[2, 3], [4, 5], [6, 7]])\n    y_mean, y_std = model.predict(X_test, return_std=True)\n    \n    # Check that the mean prediction is close to the expected values\n    expected_means = np.array([3.2, 5.2, 7.2])\n    assert_array_almost_equal(y_mean, expected_means, decimal=1)\n    \n    # Check that the standard deviation is non-negative\n    assert np.all(y_std >= 0)\n"], "sample_884": ["def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        assert MockClass2().n_features_ == 10\n", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        val = MockClass2().n_features_\n    assert val == 10\n", "def test_deprecated_property():\n    mock_instance = MockClass2()\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        assert mock_instance.n_features_ == 10\n", "def test_property_deprecation():\n    obj = MockClass2()\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        n_features = obj.n_features_\n    assert n_features == 10\n", "def test_deprecated_property():\n    mock_instance = MockClass2()\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        assert mock_instance.n_features_ == 10\n"], "sample_885": ["def test_generate_invalid_param_val_for_non_interval():\n    \"\"\"Check that generate_invalid_param_val raises NotImplementedError for non-interval constraints when appropriate.\"\"\"\n    with pytest.raises(NotImplementedError):\n        generate_invalid_param_val(_InstancesOf(int))\n    with pytest.raises(NotImplementedError):\n        generate_invalid_param_val(_NoneConstraint())\n", "def test_arraylikes_constraint():\n    \"\"\"Check that the _ArrayLikes constraint accepts various array-like structures.\"\"\"\n    constraint = _ArrayLikes()\n    assert constraint.is_satisfied_by([1, 2, 3])\n    assert constraint.is_satisfied_by((1, 2, 3))\n    assert constraint.is_satisfied_by(np.array([1, 2, 3]))\n    assert constraint.is_satisfied_by({1, 2, 3})\n    assert not constraint.is_satisfied_by(42)\n    assert not constraint.is_satisfied_by(\"string\")\n    assert not constraint.is_satisfied_by(None)\n", "def test_generate_invalid_param_val_unknown_constraint():\n    \"\"\"Check that NotImplementedError is raised for unknown constraints in generate_invalid_param_val.\"\"\"\n    class UnknownConstraint(_Constraint):\n            return False\n\n            return \"unknown constraint\"\n\n    constraint = UnknownConstraint()\n    with pytest.raises(NotImplementedError):\n        generate_invalid_param_val(constraint)\n", "def test_validate_parameter_constraints_error_messages(param, param_constraints, expected_msg):\n    \"\"\"Check that validate_parameter_constraints raises informative error messages.\"\"\"\n    with pytest.raises(InvalidParameterError, match=expected_msg):\n        validate_parameter_constraints({\"param\": param_constraints}, {\"param\": param}, \"test_func\")\n", "def test_hidden_constraint_with_method():\n    \"\"\"Check that internal constraints are not exposed in the error message for methods.\"\"\"\n\n    class SomeClass:\n        @validate_params({\"param\": [Hidden(list), dict]})\n            pass\n\n    instance = SomeClass()\n\n    # list and dict are valid params\n    instance.method({\"a\": 1, \"b\": 2})\n    instance.method([1, 2, 3])\n\n    with pytest.raises(\n        InvalidParameterError, match=\"The 'param' parameter\"\n    ) as exc_info:\n        instance.method(param=\"bad\")\n\n    # the list option is not exposed in the error message\n    err_msg = str(exc_info.value)\n    assert \"an instance of 'dict'\" in err_msg\n    assert \"an instance of 'list'\" not in err_msg\n"], "sample_886": ["def test_wrap_data_with_container():\n    \"\"\"Test _wrap_data_with_container for various configurations.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 2, 3], [4, 5, 6]])\n    X_df = pd.DataFrame(X, columns=[\"a\", \"b\", \"c\"])\n    \n    est = EstimatorWithSetOutput().fit(X)\n\n    # Test with default config\n    est.set_output(transform=\"default\")\n    wrapped_data = _wrap_data_with_container(\"transform\", X, X_df, est)\n    assert isinstance(wrapped_data, np.ndarray)\n\n    # Test with pandas config\n    est.set_output(transform=\"pandas\")\n    wrapped_data = _wrap_data_with_container(\"transform\", X, X_df, est)\n    assert isinstance(wrapped_data, pd.DataFrame)\n    assert_array_equal(wrapped_data.columns, est.get_feature_names_out())\n\n    # Test with invalid estimator configuration\n    est.set_output(transform=\"invalid\")\n    with pytest.raises(ValueError, match=\"output config must be 'default' or 'pandas'\"):\n        _wrap_data_with_container(\"transform\", X, X_df, est)\n", "def test_wrap_data_with_container():\n    \"\"\"Check _wrap_data_with_container for default and pandas outputs.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    class MockEstimator:\n            return np.array([\"feat1\", \"feat2\", \"feat3\"], dtype=object)\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = MockEstimator()\n\n    # Test with default configuration\n    with config_context(transform_output=\"default\"):\n        output = _wrap_data_with_container(\"transform\", X, X, est)\n        assert isinstance(output, np.ndarray)\n\n    # Test with pandas configuration\n    with config_context(transform_output=\"pandas\"):\n        output = _wrap_data_with_container(\"transform\", X, X, est)\n        assert isinstance(output, pd.DataFrame)\n        assert_array_equal(output.columns, [\"feat1\", \"feat2\", \"feat3\"])\n        assert_array_equal(output.index, range(len(X)))\n\n    # Test with estimator specific configuration\n    est._sklearn_output_config = {\"transform\": \"pandas\"}\n    output = _wrap_data_with_container(\"transform\", X, X, est)\n    assert isinstance(output, pd.DataFrame)\n    assert_array_equal(output.columns, [\"feat1\", \"feat2\", \"feat3\"])\n    assert_array_equal(output.index, range(len(X)))\n", "def test_wrap_method_output():\n    \"\"\"Check that _wrap_method_output correctly wraps the transform method.\"\"\"\n    class DummyEstimator(_SetOutputMixin):\n            return X + 1\n        \n            return np.asarray([f\"X{i}\" for i in range(X.shape[1])], dtype=object)\n        \n    est = DummyEstimator()\n    wrapped_transform = _wrap_method_output(est.transform, \"transform\")\n    \n    # Check that the wrapped method returns expected output\n    X = np.asarray([[1, 2, 3], [4, 5, 6]])\n    X_trans = wrapped_transform(est, X)\n    assert_array_equal(X_trans, X + 1)\n    \n    # Check that the wrapped method preserves the docstring and name\n    assert wrapped_transform.__doc__ == est.transform.__doc__\n    assert wrapped_transform.__name__ == est.transform.__name__\n", "def test_wrap_method_output_tuple():\n    \"\"\"Test the _wrap_method_output when the wrapped method returns a tuple.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    class EstimatorWithTupleOutput(_SetOutputMixin):\n            return (X, X)\n\n            return np.asarray([f\"X{i}\" for i in range(X.shape[1])], dtype=object)\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithTupleOutput().fit(X)\n\n    # Check that both elements of the tuple are returned correctly without wrapping\n    est.set_output(transform=\"default\")\n    output = est.transform(X)\n    assert isinstance(output, tuple)\n    assert isinstance(output[0], np.ndarray)\n    assert isinstance(output[1], np.ndarray)\n\n    # Check that the first element of the tuple is wrapped in a DataFrame when configured for pandas\n    est.set_output(transform=\"pandas\")\n    output = est.transform(X)\n    assert isinstance(output, tuple)\n    assert isinstance(output[0], pd.DataFrame)\n    assert isinstance(output[1], np.ndarray)\n", "def test_set_output_method_without_fit():\n    \"\"\"Check that set_output works correctly without calling fit.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput()\n\n    # set_output should still configure without calling fit\n    est.set_output(transform=\"pandas\")\n\n    # Check if transform works without calling fit\n    X_trans = est.transform(X)\n    assert isinstance(X_trans, pd.DataFrame)\n    assert_array_equal(X_trans.columns, [\"X0\", \"X1\", \"X2\"])\n"], "sample_887": ["def test_calibration_classifier_unfitted_predict_proba():\n    \"\"\"Check that calling `predict_proba` before fitting raises a NotFittedError.\"\"\"\n    clf = CalibratedClassifierCV(estimator=LogisticRegression())\n    with pytest.raises(NotFittedError):\n        clf.predict_proba(np.array([[1, 2], [3, 4]]))\n", "def test_calibration_with_pipeline_and_sample_weight():\n    \"\"\"Test that calibration works when the estimator is part of a pipeline and \n    sample weights are provided.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n    sample_weight = np.random.RandomState(42).uniform(size=y.size)\n\n    pipeline = make_pipeline(\n        StandardScaler(), \n        LogisticRegression(random_state=42)\n    )\n\n    cal_clf = CalibratedClassifierCV(pipeline, method=\"sigmoid\", cv=3)\n    cal_clf.fit(X, y, sample_weight=sample_weight)\n    prob_pos_cal_clf = cal_clf.predict_proba(X)[:, 1]\n\n    # Check that the calibration was applied\n    assert prob_pos_cal_clf is not None\n    assert len(prob_pos_cal_clf) == len(y)\n\n    # Fit without sample weights and check if the predictions differ\n    cal_clf.fit(X, y)\n    prob_pos_cal_clf_no_sw = cal_clf.predict_proba(X)[:, 1]\n\n    assert not np.allclose(prob_pos_cal_clf, prob_pos_cal_clf_no_sw)\n", "def test_calibrated_classifier_cv_prefit_multiple_classes():\n    \"\"\"Test CalibratedClassifierCV with prefitted estimator handling multiple classes.\"\"\"\n    X, y = make_classification(n_samples=300, n_features=5, n_informative=3, n_classes=3, random_state=42)\n    X_train, X_calib, y_train, y_calib = train_test_split(X, y, test_size=0.5, random_state=42)\n\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X_train, y_train)\n\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=\"prefit\")\n    cal_clf.fit(X_calib, y_calib)\n    \n    probas = cal_clf.predict_proba(X_calib)\n    assert probas.shape == (len(X_calib), 3)\n\n    predictions = cal_clf.predict(X_calib)\n    assert set(predictions) <= set(y)\n", "def test_calibration_with_different_base_estimators(data, method):\n    # Test calibration with different base estimators\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.tree import DecisionTreeClassifier\n\n    X, y = data\n\n    base_estimators = [\n        LogisticRegression(),\n        KNeighborsClassifier(),\n        DecisionTreeClassifier(),\n    ]\n\n    for estimator in base_estimators:\n        cal_clf = CalibratedClassifierCV(estimator, method=method, cv=3)\n        cal_clf.fit(X, y)\n        prob_pos = cal_clf.predict_proba(X)[:, 1]\n        \n        # Check that probabilities are within valid range [0, 1]\n        assert np.all(prob_pos >= 0) and np.all(prob_pos <= 1)\n        \n        # Check that classifier has been calibrated\n        assert hasattr(cal_clf, \"calibrated_classifiers_\")\n        assert len(cal_clf.calibrated_classifiers_) == 3\n", "def test_calibrated_classifier_sigmoid_calibration_values():\n    \"\"\"Check the exact calibration values of the sigmoid method with a fixed dataset.\"\"\"\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n    y = np.array([0, 0, 1, 1, 1])\n    estimator = LogisticRegression().fit(X, y)\n\n    cal_clf = CalibratedClassifierCV(estimator, method=\"sigmoid\", cv=\"prefit\")\n    cal_clf.fit(X, y)\n\n    # These expected values are derived from running the calibration on the dataset\n    expected_probs = np.array([\n        [0.197, 0.803],\n        [0.295, 0.705],\n        [0.417, 0.583],\n        [0.544, 0.456],\n        [0.666, 0.334]\n    ])\n    predicted_probs = cal_clf.predict_proba(X)\n    assert_allclose(predicted_probs, expected_probs, atol=0.01)\n"], "sample_888": ["def test_iforest_fit_predict():\n    \"\"\"Test IsolationForest fit and predict methods\"\"\"\n    X = [[-1.1], [0.3], [0.5], [100]]\n    clf = IsolationForest(random_state=0).fit(X)\n    predictions = clf.predict([[0.1], [0], [90]])\n    expected_predictions = [1, 1, -1]\n    assert_array_equal(predictions, expected_predictions)\n", "def test_iforest_feature_subsampling():\n    \"\"\"Test Isolation Forest with feature subsampling.\"\"\"\n\n    rng = check_random_state(0)\n    X = rng.randn(100, 5)\n\n    # Fit IsolationForest with max_features < n_features\n    clf = IsolationForest(max_features=3, random_state=rng)\n    clf.fit(X)\n\n    # Check that the number of features used per estimator is correct\n    for features in clf.estimators_features_:\n        assert len(features) == 3\n\n    # Check that the prediction works correctly\n    prediction = clf.predict(X)\n    assert prediction.shape == (100,)\n    assert np.all(np.isin(prediction, [1, -1]))\n", "def test_iforest_parameter_validation():\n    \"\"\"Test parameter validation for IsolationForest.\"\"\"\n    X = iris.data\n\n    # Test invalid n_estimators\n    with pytest.raises(ValueError, match=\"n_estimators must be an integer\"):\n        IsolationForest(n_estimators=\"invalid\").fit(X)\n\n    # Test invalid max_samples\n    with pytest.raises(ValueError, match=\"max_samples must be 'auto', an integer or a float\"):\n        IsolationForest(max_samples=\"invalid\").fit(X)\n    with pytest.raises(ValueError, match=\"max_samples must be in the range\"):\n        IsolationForest(max_samples=-1).fit(X)\n    with pytest.raises(ValueError, match=\"max_samples must be in the range\"):\n        IsolationForest(max_samples=1.5).fit(X)\n\n    # Test invalid contamination\n    with pytest.raises(ValueError, match=\"contamination must be 'auto' or a float\"):\n        IsolationForest(contamination=\"invalid\").fit(X)\n    with pytest.raises(ValueError, match=\"contamination must be in the range\"):\n        IsolationForest(contamination=-0.1).fit(X)\n    with pytest.raises(ValueError, match=\"contamination must be in the range\"):\n        IsolationForest(contamination=0.6).fit(X)\n\n    # Test invalid max_features\n    with pytest.raises(ValueError, match=\"max_features must be an integer or a float\"):\n        IsolationForest(max_features=\"invalid\").fit(X)\n    with pytest.raises(ValueError, match=\"max_features must be in the range\"):\n        IsolationForest(max_features=-0.1).fit(X)\n    with pytest.raises(ValueError, match=\"max_features must be in the range\"):\n        IsolationForest(max_features=1.5).fit(X)\n\n    # Test invalid bootstrap\n    with pytest.raises(ValueError, match=\"bootstrap must be a boolean\"):\n        IsolationForest(bootstrap=\"invalid\").fit(X)\n\n    # Test invalid n_jobs\n    with pytest.raises(ValueError, match=\"n_jobs must be an integer or None\"):\n        IsolationForest(n_jobs=\"invalid\").fit(X)\n\n    # Test invalid random_state\n    with pytest.raises(ValueError, match=\"random_state must be an integer, RandomState instance or None\"):\n        IsolationForest(random_state=\"invalid\").fit(X)\n\n    # Test invalid verbose\n    with pytest.raises(ValueError, match=\"verbose must be an integer\"):\n        IsolationForest(verbose=\"invalid\").fit(X)\n\n    # Test invalid warm_start\n    with pytest.raises(ValueError, match=\"warm_start must", "def test_iforest_with_various_dtypes():\n    \"\"\"Test IsolationForest with different data types (int, float, double).\"\"\"\n\n    X_int = np.array([[1, 2], [2, 3], [3, 4], [4, 5]], dtype=int)\n    X_float = np.array([[1.1, 2.2], [2.2, 3.3], [3.3, 4.4], [4.4, 5.5]], dtype=float)\n    X_double = np.array([[1.11, 2.22], [2.22, 3.33], [3.33, 4.44], [4.44, 5.55]], dtype=np.float64)\n\n    clf = IsolationForest(random_state=0)\n    \n    # Fit and predict with int data\n    clf.fit(X_int)\n    pred_int = clf.predict(X_int)\n    assert len(pred_int) == len(X_int)\n\n    # Fit and predict with float data\n    clf.fit(X_float)\n    pred_float = clf.predict(X_float)\n    assert len(pred_float) == len(X_float)\n\n    # Fit and predict with double data\n    clf.fit(X_double)\n    pred_double = clf.predict(X_double)\n    assert len(pred_double) == len(X_double)\n\n    # Ensure predictions are consistent\n    assert_array_equal(pred_int, pred_float)\n    assert_array_equal(pred_float, pred_double)\n", "def test_iforest_with_different_dtypes():\n    \"\"\"Test IsolationForest with different data types for input array.\"\"\"\n    X_train = [[0, 1], [1, 2], [2, 1]]\n    \n    # Test with integer data\n    clf_int = IsolationForest().fit(X_train)\n    pred_int = clf_int.predict(X_train)\n    assert all(pred_int == 1)\n\n    # Test with float data\n    X_train_float = np.array(X_train, dtype=np.float32)\n    clf_float = IsolationForest().fit(X_train_float)\n    pred_float = clf_float.predict(X_train_float)\n    assert all(pred_float == 1)\n\n    # Test with mixed integer and float data\n    X_train_mixed = [[0, 1.0], [1, 2], [2, 1.0]]\n    clf_mixed = IsolationForest().fit(X_train_mixed)\n    pred_mixed = clf_mixed.predict(X_train_mixed)\n    assert all(pred_mixed == 1)\n"], "sample_889": ["def test_calibrated_classifier_cv_invalid_method(data):\n    # Test CalibratedClassifierCV with an invalid method\n    X, y = data\n    clf = LogisticRegression()\n    invalid_method = \"invalid_method\"\n\n    with pytest.raises(ValueError, match=\"Invalid method 'invalid_method'. Valid options are 'sigmoid' and 'isotonic'.\"):\n        cal_clf = CalibratedClassifierCV(clf, method=invalid_method, cv=3)\n        cal_clf.fit(X, y)\n", "def test_calibrated_classifier_cv_invalid_method(data):\n    \"\"\"Check that an error is raised when an invalid method is provided.\"\"\"\n    X, y = data\n    invalid_method = \"invalid_method\"\n    clf = LinearSVC(random_state=42)\n\n    with pytest.raises(ValueError, match=\"Invalid 'method' parameter\"):\n        CalibratedClassifierCV(clf, method=invalid_method).fit(X, y)\n", "def test_calibration_multiclass_with_relabeling(data, method, ensemble):\n    # Test calibration for multiclass with relabeling of classes\n    n_samples = N_SAMPLES // 2\n    X, y = data\n    X_train, y_train = X[:n_samples], y[:n_samples]\n    X_test, y_test = X[n_samples:], y[n_samples:]\n\n    estimator = LinearSVC(random_state=42)\n    calibrated_clf = CalibratedClassifierCV(estimator, method=method, ensemble=ensemble, cv=3)\n\n    # Fit with original labels\n    calibrated_clf.fit(X_train, y_train)\n    probas_original = calibrated_clf.predict_proba(X_test)\n\n    # Fit with relabeled classes [0, 1, 2] -> [3, 4, 5]\n    y_train_relabel = y_train + 3\n    calibrated_clf.fit(X_train, y_train_relabel)\n    probas_relabel = calibrated_clf.predict_proba(X_test)\n\n    # Check that the probabilities are the same for the original and relabeled classes\n    assert_allclose(probas_original, probas_relabel)\n\n    # Fit with relabeled classes [0, 1, 2] -> [-3, -2, -1]\n    y_train_relabel = y_train - 3\n    calibrated_clf.fit(X_train, y_train_relabel)\n    probas_relabel = calibrated_clf.predict_proba(X_test)\n\n    # Check that the probabilities are the same for the original and relabeled classes\n    assert_allclose(probas_original, probas_relabel)\n", "def test_calibration_display_custom_ref_line(pyplot):\n    \"\"\"Test that a custom reference line can be plotted.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=2, random_state=42)\n    clf = LogisticRegression()\n    clf.fit(X, y)\n    y_prob = clf.predict_proba(X)[:, 1]\n\n    # Define a custom reference line\n        ax.plot([0, 1], [0, 0.5], \"r--\", label=\"Custom ref line\")\n\n    # Use the custom reference line in the plot\n    viz = CalibrationDisplay.from_predictions(y, y_prob, ref_line=False)\n    viz.plot(ref_line=custom_ref_line)\n\n    labels = viz.ax_.get_legend_handles_labels()[1]\n    assert \"Custom ref line\" in labels\n", "def test_calibrated_classifier_cv_custom_cv():\n    \"\"\"Check that CalibratedClassifierCV works with a custom cross-validation splitter.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=6, random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=y.size)\n    \n    # Define a custom cross-validation splitter\n    class CustomCV:\n            self.n_splits = n_splits\n\n            n_samples = len(X)\n            indices = np.arange(n_samples)\n            for i in range(self.n_splits):\n                test_start = i * (n_samples // self.n_splits)\n                test_end = (i + 1) * (n_samples // self.n_splits)\n                test_indices = indices[test_start:test_end]\n                train_indices = np.concatenate([indices[:test_start], indices[test_end:]])\n                yield train_indices, test_indices\n\n            return self.n_splits\n\n    custom_cv = CustomCV(n_splits=3)\n    clf = LogisticRegression()\n\n    cal_clf = CalibratedClassifierCV(clf, cv=custom_cv)\n    cal_clf.fit(X, y, sample_weight=sample_weight)\n\n    assert len(cal_clf.calibrated_classifiers_) == custom_cv.get_n_splits()\n    assert hasattr(cal_clf, \"classes_\")\n    assert hasattr(cal_clf, \"n_features_in_\")\n    assert hasattr(cal_clf, \"feature_names_in_\") or not hasattr(clf, \"feature_names_in_\")\n"], "sample_890": ["def test_estimator_has_no_fit_method():\n    \"\"\"Check that an error is raised when the estimator does not have a fit method\"\"\"\n    class NoFitEstimator:\n        pass\n\n    X, y = make_regression(n_features=5, random_state=0)\n    sfs = SequentialFeatureSelector(NoFitEstimator(), n_features_to_select=3)\n\n    with pytest.raises(ValueError, match=\"The provided estimator does not have the required 'fit' method\"):\n        sfs.fit(X, y)\n", "def test_different_scoring_methods(scoring):\n    \"\"\"Test SequentialFeatureSelector with different scoring methods.\"\"\"\n    X, y = make_regression(n_features=10, random_state=0)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=5,\n        scoring=scoring,\n        cv=2,\n    )\n    sfs.fit(X, y)\n    assert sfs.transform(X).shape[1] == 5\n", "def test_pipeline_with_feature_selection():\n    \"\"\"Check that a pipeline with feature selection and final estimator works as expected.\"\"\"\n    from sklearn.pipeline import Pipeline\n    from sklearn.svm import SVC\n    from sklearn.preprocessing import StandardScaler\n\n    X, y = make_classification(n_features=10, n_informative=5, random_state=0)\n\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('sfs', SequentialFeatureSelector(SVC(kernel=\"linear\"), n_features_to_select=5)),\n        ('svc', SVC(kernel=\"linear\"))\n    ])\n    \n    pipe.fit(X, y)\n    assert pipe.score(X, y) > 0.8  # Expected to have a reasonably high score\n    transformed_shape = pipe.named_steps['sfs'].transform(X).shape[1]\n    assert transformed_shape == 5  # Ensure that the number of selected features is as expected\n", "def test_invalid_tol_value():\n    \"\"\"Check that an error is raised when tol is not a float or None\"\"\"\n    X, y = make_regression(n_features=10, random_state=0)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        tol=\"invalid\",\n    )\n\n    with pytest.raises(ValueError, match=\"The 'tol' parameter of SequentialFeatureSelector must be a float or None.\"):\n        sfs.fit(X, y)\n", "def test_direction_type_validation():\n    \"\"\"Check that SequentialFeatureSelector raises an error for invalid direction type\"\"\"\n    X, y = make_regression(n_features=10, random_state=0)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        direction=\"sideways\",  # Invalid direction type\n    )\n\n    with pytest.raises(ValueError, match=\"direction must be either 'forward' or 'backward'\"):\n        sfs.fit(X, y)\n"], "sample_891": ["def test_auc_with_memmap():\n    # Test the auc function with numpy memmap\n    x = np.array([0, 1, 2, 3], dtype=np.float64)\n    y = np.array([0, 1, 4, 9], dtype=np.float64)\n\n    with pytest.raises(ValueError, match=\"At least 2 points are needed to compute area under curve\"):\n        auc(x[:1], y[:1])\n    \n    with pytest.raises(ValueError, match=\"x is neither increasing nor decreasing\"):\n        auc(x[::-1], y)\n\n    temp_dir = tempfile.mkdtemp()\n    try:\n        x_memmap = np.memmap(os.path.join(temp_dir, 'x.dat'), dtype=np.float64, mode='w+', shape=x.shape)\n        y_memmap = np.memmap(os.path.join(temp_dir, 'y.dat'), dtype=np.float64, mode='w+', shape=y.shape)\n        x_memmap[:] = x[:]\n        y_memmap[:] = y[:]\n        \n        assert auc(x_memmap, y_memmap) == auc(x, y)\n    finally:\n        shutil.rmtree(temp_dir)\n", "def test_average_precision_score_toydata():\n    # Binary classification\n    y_true = [0, 1, 1, 0, 1]\n    y_score = [0.1, 0.4, 0.35, 0.8, 0.65]\n    precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n    expected_precision = [0.75, 0.66666667, 0.66666667, 0.5, 1.0]\n    expected_recall = [1.0, 1.0, 0.66666667, 0.66666667, 0.0]\n    expected_thresholds = [0.1, 0.35, 0.4, 0.65]\n    average_precision = average_precision_score(y_true, y_score)\n    assert_array_almost_equal(precision, expected_precision, decimal=2)\n    assert_array_almost_equal(recall, expected_recall, decimal=2)\n    assert_array_almost_equal(thresholds, expected_thresholds, decimal=2)\n    assert_almost_equal(average_precision, 0.833, decimal=2)\n    \n    # Multilabel classification\n    y_true = np.array([[0, 1], [0, 1], [1, 0], [0, 0]])\n    y_score = np.array([[0.5, 0.75], [0.4, 0.65], [0.8, 0.3], [0.2, 0.1]])\n    average_precision = average_precision_score(y_true, y_score, average=\"macro\")\n    expected_average_precision = 0.8333333333333333\n    assert_almost_equal(average_precision, expected_average_precision, decimal=2)\n    \n    # With sample weights\n    y_true = [0, 0, 1, 1]\n    y_score = [0.1, 0.4, 0.35, 0.8]\n    sample_weight = [1, 2, 2, 1]\n    average_precision = average_precision_score(y_true, y_score, sample_weight=sample_weight)\n    expected_average_precision = 0.7\n    assert_almost_equal(average_precision, expected_average_precision, decimal=2)\n", "def test_det_curve_with_various_thresholds(y_true, y_score, expected_fpr, expected_fnr, expected_thresholds):\n    fpr, fnr, thresholds = det_curve(y_true, y_score)\n    assert_allclose(fpr, expected_fpr, atol=1e-2)\n    assert_allclose(fnr, expected_fnr, atol=1e-2)\n    assert_allclose(thresholds, expected_thresholds,", "def test_auc_with_large_input():\n    # Test Area Under Curve (AUC) computation with large inputs\n    rng = np.random.RandomState(42)\n    x = rng.rand(100000)\n    y = rng.rand(100000)\n    assert_array_almost_equal(auc(x, y), np.trapz(y, x))\n", "def test_auc_memmap_handling():\n    # Test that auc handles np.memmap correctly\n    x = np.array([0, 0.5, 1])\n    y = np.array([0, 0.5, 1])\n\n    with tempfile.NamedTemporaryFile(delete=False) as tmp:\n        filepath = tmp.name\n        memmap_x = np.memmap(filepath, dtype=np.float64, mode='w+', shape=x.shape)\n        memmap_x[:] = x[:]\n        memmap_y = np.memmap(filepath, dtype=np.float64, mode='w+', shape=y.shape)\n        memmap_y[:] = y[:]\n\n        assert_array_almost_equal(auc(memmap_x, memmap_y), 0.5)\n\n        memmap_x._mmap.close()\n        memmap_y._mmap.close()\n        del memmap_x\n        del memmap_y\n        os.remove(filepath)\n"], "sample_892": ["def test_adaboost_classifier_early_stopping():\n    # Test early stopping of AdaBoostClassifier when estimator error is zero.\n    X, y = datasets.make_classification(n_samples=100, n_features=20, random_state=42)\n    \n    # Create a classifier with a high number of estimators\n    clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n    clf.fit(X, y)\n    \n    # Check if early stopping occurred\n    assert len(clf.estimators_) < 100, \"Early stopping did not occur as expected.\"\n    \n    # Check if the training score is perfect (since stopping criterion is zero error)\n    assert clf.score(X, y) == 1.0\n", "def test_adaboost_classifier_with_custom_estimator():\n    \"\"\"Test AdaBoostClassifier with a custom estimator that has fit and predict methods.\"\"\"\n\n    class CustomEstimator(BaseEstimator):\n            self.classes_ = np.unique(y)\n            return self\n\n            return np.zeros(X.shape[0], dtype=int)\n\n            return np.tile([0.5, 0.5], (X.shape[0], 1))\n\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n\n    # Initialize AdaBoostClassifier with the custom estimator\n    clf = AdaBoostClassifier(estimator=CustomEstimator(), n_estimators=10, random_state=0)\n    clf.fit(X, y)\n    \n    # Check that fit works and attributes are set\n    assert hasattr(clf, 'estimators_')\n    assert hasattr(clf, 'estimator_weights_')\n    assert hasattr(clf, 'estimator_errors_')\n\n    # Check predict\n    predictions = clf.predict(X)\n    assert_array_equal(predictions, np.zeros_like(y))\n\n    # Check predict_proba\n    proba = clf.predict_proba(X)\n    assert proba.shape == (X.shape[0], 2)\n    assert_array_almost_equal(proba, np.tile([0.5, 0.5], (X.shape[0], 1)))\n\n    # Check decision_function\n    decision = clf.decision_function(X)\n    assert decision.shape == (X.shape[0],)\n    assert_array_almost_equal(decision, np.zeros(X.shape[0]))\n\n    # Check staged_predict\n    staged_predictions = list(clf.staged_predict(X))\n    assert len(staged_predictions) == 10\n    for pred in staged_predictions:\n        assert_array_equal(pred, np.zeros_like(y))\n    \n    # Check staged_predict_proba\n    staged_probas = list(clf.staged_predict_proba(X))\n    assert len(staged_probas) == 10\n    for prob in staged_probas:\n        assert_array_almost_equal(prob, np.tile([0.5, 0.5], (X.shape[0], 1)))\n\n    # Check staged_score\n    staged_scores = list(clf.st", "def test_adaboost_classifier_staged_predict_consistency():\n    # Ensure that staged_predict and final predict are consistent for AdaBoostClassifier\n    clf = AdaBoostClassifier(n_estimators=10, algorithm=\"SAMME.R\", random_state=42)\n    clf.fit(iris.data, iris.target)\n    \n    final_predictions = clf.predict(iris.data)\n    staged_predictions = list(clf.staged_predict(iris.data))\n    \n    # The last staged prediction should be the same as the final prediction\n    assert_array_equal(final_predictions, staged_predictions[-1])\n", "def test_adaboostregressor_no_bootstrap():\n    \"\"\"Test AdaBoostRegressor without bootstrap sampling\"\"\"\n    rng = np.random.RandomState(42)\n    X, y = datasets.make_regression(n_samples=100, n_features=10, noise=0.1, random_state=rng)\n    \n    reg = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=3), random_state=rng, n_estimators=5)\n    reg.fit(X, y)\n    predictions = reg.predict(X)\n    \n    # Check that the predictions are reasonably close to the true values\n    assert np.abs(np.corrcoef(predictions, y)[0, 1]) > 0.9\n\n    # Ensure that the model used multiple estimators\n    assert len(reg.estimators_) > 1\n\n    # Check staged predictions\n    staged_predictions = [p for p in reg.staged_predict(X)]\n    assert len(staged_predictions) == 5\n    assert_array_almost_equal(predictions, staged_predictions[-1])\n", "def test_adaboostclassifier_with_custom_estimator():\n    # Test AdaBoostClassifier with a custom estimator that does not support predict_proba\n    class CustomEstimator(BaseEstimator):\n            self.classes_ = np.unique(y)\n            return self\n\n            return np.zeros(X.shape[0], dtype=int)\n\n    clf = AdaBoostClassifier(estimator=CustomEstimator(), algorithm=\"SAMME\")\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), [0, 0, 0])\n\n    clf = AdaBoostClassifier(estimator=CustomEstimator(), algorithm=\"SAMME.R\")\n    with pytest.raises(TypeError, match=\"requires that the weak learner supports the calculation of class probabilities\"):\n        clf.fit(X, y_class)\n"], "sample_893": ["def test_plot_tree_max_depth(pyplot):\n    # Test plot_tree with max_depth parameter\n    clf = DecisionTreeClassifier(\n        max_depth=3, min_samples_split=2, criterion=\"gini\", random_state=2\n    )\n    clf.fit(X, y)\n\n    # Test max_depth = 1\n    nodes = plot_tree(clf, max_depth=1, feature_names=[\"first feat\", \"sepal_width\"])\n    assert len(nodes) == 1\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n    )\n\n    # Test max_depth = 0\n    nodes = plot_tree(clf, max_depth=0, feature_names=[\"first feat\", \"sepal_width\"])\n    assert len(nodes) == 0\n", "def test_plot_tree_filled(pyplot):\n    # Check correctness of plot_tree with filled=True\n    clf = DecisionTreeClassifier(max_depth=3, random_state=2)\n    clf.fit(X, y)\n\n    # Test plot_tree with filled=True\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(clf, feature_names=feature_names, filled=True)\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n    )\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n    # Check if nodes have fill color\n    assert nodes[0].get_bbox_patch().get_facecolor() != nodes[0].get_axes().get_facecolor()\n    assert nodes[1].get_bbox_patch().get_facecolor() != nodes[1].get_axes().get_facecolor()\n    assert nodes[2].get_bbox_patch().get_facecolor() != nodes[2].get_axes().get_facecolor()\n", "def test_plot_tree_filled_and_rounded(pyplot):\n    # Test the plot_tree function with filled and rounded parameters\n    clf = DecisionTreeClassifier(\n        max_depth=3, min_samples_split=2, criterion=\"gini\", random_state=2\n    )\n    clf.fit(X, y)\n\n    # Test with filled=True and rounded=True\n    nodes = plot_tree(clf, filled=True, rounded=True, feature_names=[\"feat1\", \"feat2\"])\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"feat1 <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n    )\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n\n    # Check that nodes are filled with color\n    assert nodes[0].get_bbox_patch().get_facecolor() is not None\n    assert nodes[1].get_bbox_patch().get_facecolor() is not None\n    assert nodes[2].get_bbox_patch().get_facecolor() is not None\n\n    # Check that nodes have rounded corners\n    assert nodes[0].get_bbox_patch().get_boxstyle().split(\",\")[0] == \"round\"\n    assert nodes[1].get_bbox_patch().get_boxstyle().split(\",\")[0] == \"round\"\n    assert nodes[2].get_bbox_patch().get_boxstyle().split(\",\")[0] == \"round\"\n", "def test_plot_tree_max_depth(pyplot):\n    # Check if plot_tree respects the max_depth parameter\n    clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n    clf.fit(X, y)\n    \n    # Test max_depth parameter\n    nodes = plot_tree(clf, max_depth=1)\n    assert len(nodes) == 1\n    assert nodes[0].get_text() == \"feature_1 <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n\n    nodes = plot_tree(clf, max_depth=2)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == \"feature_1 <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n", "def test_color_brew():\n    # Test _color_brew function for generating colors\n    colors = _color_brew(5)\n    assert len(colors) == 5, \"Should generate 5 colors\"\n    for color in colors:\n        assert len(color) == 3, \"Each color should have 3 components\"\n        assert all(0 <= c <= 255 for c in color), \"Color components should be within 0-255 range\"\n"], "sample_894": ["def test_random_trees_embedding_dense_output():\n    \"\"\"Test RandomTreesEmbedding's dense output functionality.\"\"\"\n    X, y = datasets.make_classification(n_samples=100, n_features=4, random_state=0)\n    embedding = RandomTreesEmbedding(n_estimators=10, sparse_output=False, random_state=0)\n    transformed_X = embedding.fit_transform(X)\n    \n    assert isinstance(transformed_X, np.ndarray), \"Transformed output should be a dense numpy array.\"\n    assert transformed_X.shape[0] == X.shape[0], \"Number of samples should remain the same.\"\n    assert transformed_X.shape[1] == embedding.n_estimators * (2 ** embedding.max_depth), \"Transformed feature count should be equal to n_estimators * 2^max_depth.\"\n    \n    # Check that the dense output retains the same transformation as the sparse output when converted to dense\n    embedding_sparse = RandomTreesEmbedding(n_estimators=10, sparse_output=True, random_state=0)\n    transformed_X_sparse = embedding_sparse.fit_transform(X)\n    \n    assert_array_equal(transformed_X, transformed_X_sparse.toarray(), \"Dense output should match the dense conversion of sparse output.\")\n", "def test_random_trees_embedding_fit_transform_sparse_output():\n    \"\"\"Test RandomTreesEmbedding with sparse_output=True.\"\"\"\n    X, _ = datasets.make_classification(n_samples=100, n_features=10, random_state=0)\n    hasher = RandomTreesEmbedding(n_estimators=10, sparse_output=True, random_state=0)\n    X_transformed = hasher.fit_transform(X)\n    \n    # Check the type of the output\n    assert isinstance(X_transformed, csr_matrix), \"Output should be a sparse CSR matrix\"\n    \n    # Check the shape of the output\n    assert X_transformed.shape[0] == X.shape[0], \"Output should have the same number of samples as input\"\n    \n    # Check the number of active features\n    assert X_transformed.sum(axis=1).max() == hasher.n_estimators, \"Each sample should be represented by n_estimators\"\n", "def test_random_trees_embedding_fit_transform(name, X_type):\n    \"\"\"Check fit_transform method of RandomTreesEmbedding.\"\"\"\n    X, _ = datasets.make_classification(n_samples=100, n_features=10, random_state=0)\n    X = _convert_container(X, constructor_name=X_type)\n\n    transformer = FOREST_TRANSFORMERS[name](\n        n_estimators=10, random_state=0, max_depth=3, sparse_output=True\n    )\n    Xt = transformer.fit_transform(X)\n    \n    assert Xt.shape[0] == X.shape[0]\n    assert Xt.nnz == 10 * X.shape[0]  # Each sample should be assigned to one leaf node per tree\n    assert isinstance(Xt, csr_matrix)\n", "def test_feature_importances_normalized(name):\n    \"\"\"Test that feature importances are normalized and sum up to 1.\"\"\"\n    X, y = datasets.make_classification(\n        n_samples=100, n_features=10, n_informative=5, random_state=0\n    )\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(n_estimators=10, random_state=0)\n    est.fit(X, y)\n    importances = est.feature_importances_\n    assert np.isclose(np.sum(importances), 1.0), \"Feature importances do not sum to 1\"\n    assert np.all(importances >= 0), \"Feature importances contain negative values\"\n\n    # Check if feature importances change with different random states\n    est_random = ForestEstimator(n_estimators=10, random_state=42)\n    est_random.fit(X, y)\n    importances_random = est_random.feature_importances_\n    assert np.isclose(np.sum(importances_random), 1.0), \"Feature importances do not sum to 1\"\n    assert np.all(importances_random >= 0), \"Feature importances contain negative values\"\n    assert not np.array_equal(importances, importances_random), \"Feature importances are identical for different random states\"\n", "def test_fit_with_sample_weight(ForestClass):\n    \"\"\"Test that fitting with sample weights works as expected.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=4, random_state=0)\n    sample_weight = np.random.RandomState(42).randint(1, 10, X.shape[0])\n    \n    # Fit without sample weights\n    clf1 = ForestClass(random_state=0, bootstrap=True, max_samples=0.5)\n    clf1.fit(X, y)\n    \n    # Fit with sample weights\n    clf2 = ForestClass(random_state=0, bootstrap=True, max_samples=0.5)\n    clf2.fit(X, y, sample_weight=sample_weight)\n    \n    # Check that predictions with and without sample weights are similar\n    pred1 = clf1.predict(X)\n    pred2 = clf2.predict(X)\n    \n    assert_array_almost_equal(pred1, pred2, decimal=1)\n\n    # Check that feature importances with and without sample weights are similar\n    fi1 = clf1.feature_importances_\n    fi2 = clf2.feature_importances_\n    \n    assert_array_almost_equal(fi1, fi2, decimal=1)\n"], "sample_895": ["def test_column_transformer_with_callable_remainder():\n    \"\"\"Test ColumnTransformer with a callable remainder.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    X_df = pd.DataFrame(X_array, columns=[\"first\", \"second\"])\n\n        return X[:, 1:] * 2\n\n    # Test with callable remainder\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"first\"])], remainder=remainder_callable\n    )\n    X_trans = ct.fit_transform(X_df)\n    expected_result = np.hstack([X_df[[\"first\"]].values, X_df[[\"second\"]].values * 2])\n    assert_array_equal(X_trans, expected_result)\n\n    # Test if callable remainder still works after transformation\n    X_trans = ct.transform(X_df)\n    assert_array_equal(X_trans, expected_result)\n\n    # Test with callable remainder and verbose_feature_names_out=False\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"first\"])],\n        remainder=remainder_callable,\n        verbose_feature_names_out=False,\n    )\n    X_trans = ct.fit_transform(X_df)\n    assert_array_equal(X_trans, expected_result)\n\n    X_trans = ct.transform(X_df)\n    assert_array_equal(X_trans, expected_result)\n", "def test_column_transformer_mixed_types():\n    # Test ColumnTransformer with mixed types of transformers including 'drop' and 'passthrough'\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\n        'numeric': [1.1, 2.2, 3.3],\n        'categorical': ['cat', 'dog', 'cat'],\n        'text': ['hello', 'world', 'foo'],\n        'bool': [True, False, True]\n    })\n\n    ct = ColumnTransformer(\n        transformers=[\n            ('num', StandardScaler(), ['numeric']),\n            ('cat', OneHotEncoder(), ['categorical']),\n            ('text', 'drop', ['text']),\n            ('bool', 'passthrough', ['bool'])\n        ]\n    )\n\n    transformed = ct.fit_transform(df)\n    assert transformed.shape == (3, 4)  # 1 numeric, 2 categorical (one-hot), 1 bool\n    assert isinstance(transformed, np.ndarray)\n\n    # Check that the transformation results are as expected\n    expected = np.array([\n        [-1.22474487,  1.        ,  0.        ,  1.        ],\n        [ 0.        ,  0.        ,  1.        ,  0.        ],\n        [ 1.22474487,  1.        ,  0.        ,  1.        ]\n    ])\n    assert_allclose(transformed, expected)\n", "def test_column_transformer_set_params_remainder():\n    # Test that setting parameters for the remainder transformer works correctly\n    X = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n\n    ct = ColumnTransformer(\n        [(\"trans1\", StandardScaler(), [0])],\n        remainder=StandardScaler()\n    )\n    ct.set_params(remainder__with_mean=False, remainder__with_std=False)\n    ct.fit(X)\n    \n    assert not ct.named_transformers_.remainder.with_mean\n    assert not ct.named_transformers_.remainder.with_std\n\n    X_trans = ct.transform(X)\n    expected_trans = StandardScaler(with_mean=False, with_std=False).fit_transform(X[:, [1, 2]])\n    assert_allclose(X_trans[:, 1:], expected_trans)\n", "def test_column_transformer_get_feature_names_out_remainder():\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\n        \"a\": [1, 2, 3],\n        \"b\": [4, 5, 6],\n        \"c\": [7, 8, 9],\n    })\n\n    ct = ColumnTransformer([\n        (\"scaler\", StandardScaler(), [\"a\"]),\n    ], remainder=Normalizer())\n\n    ct.fit(df)\n    \n    # Test that get_feature_names_out includes remainder columns properly\n    feature_names_out = ct.get_feature_names_out()\n    assert_array_equal(feature_names_out, [\"scaler__a\", \"remainder__b\", \"remainder__c\"])\n\n    # Set verbose_feature_names_out to False and test again\n    ct.set_params(verbose_feature_names_out=False)\n    feature_names_out = ct.get_feature_names_out()\n    assert_array_equal(feature_names_out, [\"a\", \"b\", \"c\"])\n", "def test_column_transformer_with_callable_remainder():\n    \"\"\"Test ColumnTransformer with a callable remainder.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame(\n        {\n            \"col1\": [0, 1, 2],\n            \"col2\": [2, 4, 6],\n            \"col3\": [\"a\", \"b\", \"c\"],\n        }\n    )\n\n        if hasattr(X, \"iloc\"):\n            # pandas DataFrame\n            return X.select_dtypes(include=[np.number]) * 2\n        return X * 2\n\n    ct = ColumnTransformer(\n        [(\"trans\", Trans(), [\"col1\"])],\n        remainder=remainder_transformer,\n        verbose=True\n    )\n\n    X_trans = ct.fit_transform(X_df)\n    assert isinstance(X_trans, np.ndarray)\n    expected_result = np.array([[0, 4, 0], [1, 8, 2], [2, 12, 4]])\n    assert_array_equal(X_trans, expected_result)\n"], "sample_896": ["def test_beta_loss_to_float_conversion():\n    # Test the _beta_loss_to_float conversion function for correct conversion of strings to floats\n    assert nmf._beta_loss_to_float(\"frobenius\") == 2\n    assert nmf._beta_loss_to_float(\"kullback-leibler\") == 1\n    assert nmf._beta_loss_to_float(\"itakura-saito\") == 0\n\n    # Test for unsupported string values\n    with pytest.raises(KeyError):\n        nmf._beta_loss_to_float(\"unsupported\")\n\n    # Test for float values being passed directly\n    assert nmf._beta_loss_to_float(0.5) == 0.5\n    assert nmf._beta_loss_to_float(1.5) == 1.5\n", "def test_non_negative_factorization_beta_loss_string():\n    # Test that beta_loss as string options work correctly in non_negative_factorization\n    rng = np.random.RandomState(0)\n    X = np.abs(rng.randn(10, 10))\n    n_components = 5\n\n    for beta_loss in [\"frobenius\", \"kullback-leibler\", \"itakura-saito\"]:\n        W, H, _ = non_negative_factorization(\n            X,\n            n_components=n_components,\n            beta_loss=beta_loss,\n            init=\"random\",\n            random_state=0,\n            tol=1e-2,\n        )\n        assert W.shape == (X.shape[0], n_components)\n        assert H.shape == (n_components, X.shape[1])\n        assert not np.any(np.isnan(W))\n        assert not np.any(np.isnan(H))\n", "def test_special_sparse_dot_handle_empty_matrix():\n    # Test the _special_sparse_dot function with an empty sparse matrix\n    rng = np.random.mtrand.RandomState(42)\n    n_samples = 10\n    n_features = 5\n    n_components = 3\n\n    X_empty = sp.csr_matrix((n_samples, n_features))  # Create an empty sparse matrix\n    W = np.abs(rng.randn(n_samples, n_components))\n    H = np.abs(rng.randn(n_components, n_features))\n\n    WH_safe = nmf._special_sparse_dot(W, H, X_empty)\n    \n    # Since X_empty is empty, WH_safe should also be empty\n    assert WH_safe.nnz == 0  # Check that the number of non-zero elements is zero\n    assert WH_safe.shape == X_empty.shape  # Check that the shape matches the input empty matrix\n", "def test_initialize_nmf_default(init):\n    # Test that _initialize_nmf raises error with invalid initialization method\n    rng = np.random.mtrand.RandomState(42)\n    data = np.abs(rng.randn(10, 10))\n    with pytest.raises(ValueError, match=\"Invalid init parameter\"):\n        nmf._initialize_nmf(data, 10, init=\"invalid_init\", random_state=0)\n\n    # Test that the default initialization method is set to 'nndsvda' when\n    # n_components <= min(n_samples, n_features)\n    W, H = nmf._initialize_nmf(data, 5, init=None, random_state=0)\n    assert W.shape == (10, 5)\n    assert H.shape == (5, 10)\n    assert not ((W < 0).any() or (H < 0).any())\n\n    # Test that the default initialization method is set to 'random' when\n    # n_components > min(n_samples, n_features)\n    W, H = nmf._initialize_nmf(data, 15, init=None, random_state=0)\n    assert W.shape == (10, 15)\n    assert H.shape == (15, 10)\n    assert not ((W < 0).any() or (H < 0).any())\n", "def test_non_negative_factorization_custom_init_types():\n    # Test that non_negative_factorization raises a TypeError if custom init is not array-like\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n\n    invalid_inits = [123, \"invalid_init\", {\"not\": \"array-like\"}, (1, 2, 3)]\n    for invalid_init in invalid_inits:\n        with pytest.raises(TypeError, match=\"Expected sequence or array-like\"):\n            non_negative_factorization(X, W=invalid_init, H=invalid_init, init=\"custom\")\n\n"], "sample_897": ["def test_partial_dependence_various_combinations(\n    pyplot, clf_diabetes, diabetes, features, kind, subsample, grid_resolution", "def test_partial_dependence_display_categorical_feature_names(pyplot):\n    \"\"\"Test that categorical feature names are properly handled and displayed.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = pd.DataFrame({\n        \"Feature_A\": [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"],\n        \"Feature_B\": [1, 2, 3, 4, 5, 6],\n        \"Feature_C\": [\"X\", \"Y\", \"Z\", \"X\", \"Y\", \"Z\"]\n    })\n    y = np.array([1, 2, 3, 4, 5, 6])\n\n    categorical_features = [\"Feature_A\", \"Feature_C\"]\n    preprocessor = make_column_transformer((OneHotEncoder(), categorical_features))\n    model = make_pipeline(preprocessor, GradientBoostingRegressor(n_estimators=10, random_state=1))\n    model.fit(X, y)\n\n    disp = PartialDependenceDisplay.from_estimator(\n        model,\n        X,\n        features=[\"Feature_A\", \"Feature_C\", (\"Feature_A\", \"Feature_C\")],\n        feature_names=[\"Feature_A\", \"Feature_B\", \"Feature_C\"],\n        categorical_features=categorical_features\n    )\n\n    assert disp.figure_ is pyplot.gcf()\n    assert disp.bars_.shape == (1, 3)\n    assert disp.bars_[0, 0] is not None\n    assert disp.bars_[0, 1] is not None\n    assert disp.bars_[0, 2] is None\n    assert disp.lines_.shape == (1, 3)\n    assert disp.lines_[0, 0] is None\n    assert disp.lines_[0, 1] is None\n    assert disp.lines_[0, 2] is None\n    assert disp.contours_.shape == (1, 3)\n    assert disp.contours_[0, 0] is None\n    assert disp.contours_[0, 1] is None\n    assert disp.contours_[0, 2] is None\n    assert disp.deciles_vlines_.shape == (1, 3)\n    assert disp.deciles_vlines_[0, 0] is None\n    assert disp.deciles_vlines_[0, 1] is None\n    assert disp.deciles_vlines_[0, 2] is None", "def test_plot_partial_dependence_multiple_estimators(pyplot, clf_diabetes, diabetes):\n    # Test that multiple estimators' partial dependence plots can be displayed on the same axes\n    grid_resolution = 25\n    fig, axes = pyplot.subplots(1, 2)\n    \n    # First estimator\n    disp1 = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, 2],\n        grid_resolution=grid_resolution,\n        feature_names=diabetes.feature_names,\n        ax=axes\n    )\n\n    # Second estimator (linear regression)\n    lr = LinearRegression()\n    lr.fit(diabetes.data, diabetes.target)\n    disp2 = PartialDependenceDisplay.from_estimator(\n        lr,\n        diabetes.data,\n        [0, 2],\n        grid_resolution=grid_resolution,\n        feature_names=diabetes.feature_names,\n        ax=axes\n    )\n\n    # Assert the axes are the same\n    assert disp1.figure_ is fig\n    assert disp2.figure_ is fig\n    assert np.all(disp1.axes_ == disp2.axes_)\n    assert len(disp1.axes_) == 2\n    \n    # Check if both lines are plotted on the same axes\n    for ax in axes:\n        lines = ax.get_lines()\n        assert len(lines) == 2  # One line for each estimator\n", "def test_plot_partial_dependence_subsampling(pyplot, clf_diabetes, diabetes, subsample):\n    # Test partial dependence plot function with subsampling.\n    feature_names = diabetes.feature_names\n    disp = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, 2],\n        grid_resolution=20,\n        feature_names=feature_names,\n        kind=\"both\",\n        subsample=subsample,\n        random_state=42,\n    )\n    fig = pyplot.gcf()\n    axs = fig.get_axes()\n    assert disp.figure_ is fig\n    assert len(axs) == 3\n\n    assert disp.bounding_ax_ is not None\n    assert disp.axes_.shape == (1, 2)\n    assert disp.lines_.shape == (1, 2, 21 if subsample is None else 11 if subsample == 10 else 11)\n    assert disp.contours_.shape == (1, 2)\n    assert disp.deciles_vlines_.shape == (1, 2)\n    assert disp.deciles_hlines_.shape == (1, 2)\n\n    for i in range(2):\n        assert disp.deciles_vlines_[0, i] is not None\n        assert disp.deciles_hlines_[0, i] is None\n        assert disp.contours_[0, i] is None\n\n    assert disp.features == [(0,), (2,)]\n    assert np.all(disp.feature_names == feature_names)\n    assert len(disp.deciles) == 2\n    for i in [0, 2]:\n        assert_allclose(\n            disp.deciles[i],\n            mquantiles(diabetes.data[:, i], prob=np.arange(0.1, 1.0, 0.1)),\n        )\n\n    for i in range(2):\n        ax = disp.axes_[0, i]\n        assert ax.get_ylabel() == \"Partial dependence\" if i == 0 else \"\"\n        assert ax.get_xlabel() == diabetes.feature_names[i]\n\n        line = disp.lines_[0, i, -1]\n\n        avg_preds = disp.pd_results[i]\n        assert avg_preds.average.shape == (1, 20)\n        target_idx = disp.target_idx\n\n        line_data = line.get_data()\n        assert_allclose(line_data[", "def test_partial_dependence_display_with_non_constant_sample_weight(pyplot, clf_diabetes, diabetes):\n    \"\"\"Check that the utilization of a non-constant sample weight changes the\n    behavior correctly.\n    \"\"\"\n    sample_weight = np.linspace(1, 2, len(diabetes.target))\n    disp = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, 1],\n        sample_weight=sample_weight,\n        kind=\"average\",\n        method=\"brute\",\n    )\n\n    assert disp.pd_results is not None\n    assert len(disp.pd_results) == 2  # We provided two features for plotting\n    for pd_result in disp.pd_results:\n        assert \"average\" in pd_result\n        assert pd_result[\"average\"].shape == (1, 100)  # Default grid_resolution is 100\n\n    fig = pyplot.gcf()\n    assert disp.figure_ is fig\n    assert disp.axes_.shape == (1, 2)\n    assert disp.lines_.shape == (1, 2)\n    assert disp.contours_.shape == (1, 2)\n    assert disp.deciles_vlines_.shape == (1, 2)\n    assert disp.deciles_hlines_.shape == (1, 2)\n"], "sample_898": ["def test_auc():\n    # Test the `auc` function with various inputs\n    x = np.array([0, 0.5, 1])\n    y = np.array([0, 0.5, 1])\n    assert_almost_equal(auc(x, y), 0.5, err_msg=\"AUC failed for linear increase\")\n\n    x = np.array([1, 0.5, 0])\n    y = np.array([0, 0.5, 1])\n    assert_almost_equal(auc(x, y), 0.5, err_msg=\"AUC failed for linear decrease\")\n\n    x = np.array([0, 1])\n    y = np.array([0, 1])\n    assert_almost_equal(auc(x, y), 0.5, err_msg=\"AUC failed for perfect classifier\")\n\n    x = np.array([0, 1, 2])\n    y = np.array([0, 1, 2])\n    assert_raises(ValueError, auc, x, y, reorder=True)\n\n    x = np.array([0, 1, 2])\n    y = np.array([2, 1, 0])\n    assert_raises(ValueError, auc, x, y, reorder=True)\n\n    x = np.array([0, 1, 2, 2])\n    y = np.array([0, 1, 2, 2])\n    assert_raises(ValueError, auc, x, y)\n\n    x = np.array([0.5, 0.5, 1])\n    y = np.array([0.5, 0.5, 1])\n    assert_raises(ValueError, auc, x, y)\n\n    x = np.array([0])\n    y = np.array([0])\n    assert_raises(ValueError, auc, x, y)\n\n    x = np.array([0, 1])\n    y = np.array([0, 1])\n    assert_raises(ValueError, auc, x, y, reorder=True)\n", "def test_auc_score():\n    # Test the auc function with different scenarios\n    x1 = np.array([0, 1])\n    y1 = np.array([0, 1])\n    assert_almost_equal(auc(x1, y1), 0.5, err_msg=\"AUC failed for perfect separation\")\n\n    x2 = np.array([0, 0.5, 1])\n    y2 = np.array([0, 0.5, 1])\n    assert_almost_equal(auc(x2, y2), 0.5, err_msg=\"AUC failed for linear separation\")\n\n    x3 = np.array([1, 0.5, 0])\n    y3 = np.array([1, 0.5, 0])\n    assert_almost_equal(auc(x3, y3), 0.5, err_msg=\"AUC failed for reverse separation\")\n\n    x4 = np.array([0, 1, 2])\n    y4 = np.array([0, 1, 4])\n    assert_almost_equal(auc(x4, y4), 4.0, err_msg=\"AUC failed for quadratic separation\")\n\n    x5 = np.array([0, 1, 2])\n    y5 = np.array([0, 0.5, 1])\n    assert_raises(ValueError, auc, x5[:1], y5[:1], err_msg=\"AUC did not raise error for insufficient points\")\n\n    x6 = np.array([0, 2, 1])\n    y6 = np.array([0, 1, 2])\n    assert_raises(ValueError, auc, x6, y6, err_msg=\"AUC did not raise error for unsorted x\")\n\n    x7 = np.array([0, 1, 2])\n    y7 = np.array([0, 1, 2])\n    assert_raises(ValueError, auc, x7, y7, reorder=True, err_msg=\"AUC did not raise error for deprecated reorder parameter\")\n", "def test_auc_function():\n    # Test the auc function with simple increasing and decreasing sequences\n    x_inc = np.array([1, 2, 3])\n    y_inc = np.array([1, 2, 3])\n    x_dec = np.array([3, 2, 1])\n    y_dec = np.array([1, 2, 3])\n\n    # Simple increasing sequences should return positive area\n    assert_almost_equal(auc(x_inc, y_inc), 4.0, err_msg=\"Increasing sequence AUC failed\")\n\n    # Simple decreasing sequences should return negative area\n    assert_almost_equal(auc(x_dec, y_dec), -4.0, err_msg=\"Decreasing sequence AUC failed\")\n\n    # Test for error when x has fewer than 2 points\n    assert_raises(ValueError, auc, np.array([1]), np.array([1]))\n\n    # Test for error when x is neither increasing nor decreasing\n    assert_raises(ValueError, auc, np.array([1, 3, 2]), np.array([1, 2, 3]))\n\n    # Test with floating point values\n    x_float = np.array([0.1, 0.4, 0.35, 0.8])\n    y_float = np.array([0.1, 0.2, 0.25, 0.7])\n    assert_almost_equal(auc(x_float, y_float), 0.165, decimal=3, err_msg=\"Floating point AUC failed\")\n", "def test_auc_function():\n    # Test the auc function with monotonic increasing x\n    x = np.array([0, 1, 2, 3])\n    y = np.array([0, 1, 2, 3])\n    assert_almost_equal(auc(x, y), 4.5, err_msg=\"AUC computation failed for monotonic increasing x\")\n\n    # Test the auc function with monotonic decreasing x\n    x = np.array([3, 2, 1, 0])\n    y = np.array([3, 2, 1, 0])\n    assert_almost_equal(auc(x, y), -4.5, err_msg=\"AUC computation failed for monotonic decreasing x\")\n\n    # Test the auc function with x not ordered\n    x = np.array([0, 3, 2, 1])\n    y = np.array([0, 3, 2, 1])\n    assert_raises(ValueError, auc, x, y)\n\n    # Test the auc function with less than 2 points\n    x = np.array([0])\n    y = np.array([0])\n    assert_raises(ValueError, auc, x, y)\n\n    # Test the auc function with reorder=True (deprecated)\n    x = np.array([0, 2, 1, 3])\n    y = np.array([0, 1, 2, 3])\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        auc(x, y, reorder=True)\n        assert len(w) == 1\n        assert issubclass(w[0].category, DeprecationWarning)\n", "def test_auc():\n    # Test for auc function\n    y_true = np.array([1, 1, 2, 2])\n    y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=2)\n    assert_almost_equal(auc(fpr, tpr), 0.75)\n\n    # Test for auc with monotonic increasing x\n    x = np.array([0, 1, 2])\n    y = np.array([0, 0.5, 1])\n    assert_almost_equal(auc(x, y), 1.0)\n\n    # Test for auc with monotonic decreasing x\n    x = np.array([2, 1, 0])\n    y = np.array([1, 0.5, 0])\n    assert_almost_equal(auc(x, y), 1.0)\n\n    # Test for auc with non-monotonic x\n    x = np.array([0, 2, 1])\n    y = np.array([0, 1, 0.5])\n    assert_raises(ValueError, auc, x, y)\n\n    # Test for auc with reorder=True\n    x = np.array([0, 2, 1])\n    y = np.array([0, 1, 0.5])\n    assert_almost_equal(auc(x, y, reorder=True), 1.0)\n"], "sample_899": ["def test_check_estimators_dtypes():\n    # check that estimators handle different data types appropriately\n    class EstimatorWithDtypes(BaseEstimator):\n            X = check_array(X, dtype='float')\n            if y is not None:\n                y = check_array(y, ensure_2d=False)\n            return self\n\n            X = check_array(X, dtype='float')\n            return np.ones(X.shape[0])\n\n    check_estimators_dtypes('EstimatorWithDtypes', EstimatorWithDtypes())\n", "def test_check_transformers_unfitted():\n    class UnfittedTransformer(BaseEstimator):\n            return self\n\n            if not hasattr(self, 'is_fitted_'):\n                raise ValueError(\"This transformer instance is not fitted yet.\")\n            return X\n\n    transformer = UnfittedTransformer()\n    check_transformers_unfitted(UnfittedTransformer.__name__, transformer)\n", "def test_check_estimators_dtypes():\n    # check that the estimator handles different dtypes correctly\n    from sklearn.linear_model import Ridge\n\n    class EstimatorWithDifferentDtypes(BaseEstimator):\n            self.dtype_ = X.dtype\n            return self\n\n            if X.dtype != self.dtype_:\n                raise ValueError(\"Dtype mismatch. Expected {}, got {}\".format(\n                    self.dtype_, X.dtype))\n            return np.ones(X.shape[0])\n\n    check_estimator(EstimatorWithDifferentDtypes)\n    check_estimators_dtypes(\"Ridge\", Ridge())\n", "def test_check_estimators_sparse_data():\n    # check that check_estimator properly handles sparse data\n    # and raises the appropriate errors or warnings.\n    \n    # A valid estimator that should handle sparse data\n    class SparseCapableClassifier(BaseEstimator, ClassifierMixin):\n            X, y = check_X_y(X, y, accept_sparse=['csr', 'csc'])\n            self.classes_ = np.unique(y)\n            return self\n        \n            X = check_array(X, accept_sparse=['csr', 'csc'])\n            return np.full(X.shape[0], self.classes_[0])\n\n    # An estimator that does not handle sparse data correctly\n    class SparseIncapableClassifier(BaseBadClassifier):\n            if sp.issparse(X):\n                raise TypeError(\"Sparse data not supported\")\n            return self\n\n    # check that the correct estimator passes the sparse data check\n    check_estimator(SparseCapableClassifier)\n    \n    # check that the incorrect estimator fails the sparse data check\n    old_stdout = sys.stdout\n    string_buffer = StringIO()\n    sys.stdout = string_buffer\n    try:\n        check_estimator(SparseIncapableClassifier)\n    except:\n        pass\n    finally:\n        sys.stdout = old_stdout\n    assert_true(\"Sparse data not supported\" in string_buffer.getvalue())\n", "def test_check_classifiers_classes():\n    # Test that classifiers correctly handle different class labels\n    # and their predictions match the expected classes.\n    from sklearn.datasets import make_classification\n    X, y = make_classification(n_samples=100, n_features=4, n_classes=3, random_state=0)\n    \n    class DummyClassifier(BaseBadClassifier):\n            self.classes_ = np.unique(y)\n            return self\n\n    check_classifiers_classes(\"DummyClassifier\", DummyClassifier())\n"], "sample_900": ["def test_invalid_hidden_layer_sizes():\n    # Test that invalid values for hidden_layer_sizes raise value error\n    X = [[3, 2], [1, 6]]\n    y = [1, 0]\n    \n    invalid_hidden_layer_sizes = [\n        (-1,),  # negative size\n        (0,),   # zero size\n        (100, -1),  # valid and invalid size combination\n        \"invalid\",  # non-iterable type\n    ]\n    \n    for hidden_layer_size in invalid_hidden_layer_sizes:\n        clf = MLPClassifier(hidden_layer_sizes=hidden_layer_size)\n        assert_raises(ValueError, clf.fit, X, y)\n", "def test_invalid_batch_size():\n    # Test that invalid batch_size raises a value error.\n    X = [[3, 2], [1, 6]]\n    y = [1, 0]\n\n    clf = MLPClassifier(solver='sgd', batch_size=-1)\n    assert_raises(ValueError, clf.fit, X, y)\n\n    clf = MLPClassifier(solver='sgd', batch_size=0)\n    assert_raises(ValueError, clf.fit, X, y)\n\n    clf = MLPClassifier(solver='sgd', batch_size=3)\n    assert_raises(ValueError, clf.fit, X[:2], y[:2])\n\n    clf = MLPRegressor(solver='sgd', batch_size=-1)\n    assert_raises(ValueError, clf.fit, X, y)\n\n    clf = MLPRegressor(solver='sgd', batch_size=0)\n    assert_raises(ValueError, clf.fit, X, y)\n\n    clf = MLPRegressor(solver='sgd', batch_size=3)\n    assert_raises(ValueError, clf.fit, X[:2], y[:2])\n", "def test_forward_pass():\n    # Test the forward pass computation for different activation functions.\n    X = np.array([[0.6, 0.8, 0.7]])\n    y = np.array([0])\n    \n    for activation in ACTIVATION_TYPES:\n        mlp = MLPClassifier(hidden_layer_sizes=(2,), activation=activation, solver='lbfgs', random_state=1)\n        mlp.coefs_ = [np.array([[0.1, 0.2], [0.3, 0.1], [0.5, 0]])]\n        mlp.intercepts_ = [np.array([0.1, 0.1])]\n        mlp.out_activation_ = activation\n        mlp.n_layers_ = 3\n        mlp.n_outputs_ = 1\n\n        activations = mlp._forward_pass([X, np.empty((X.shape[0], 2)), np.empty((X.shape[0], 1))])\n\n        assert activations[0].shape == (1, 3)\n        assert activations[1].shape == (1, 2)\n        assert activations[2].shape == (1, 1)\n\n        if activation == 'logistic':\n            expected_output = 1 / (1 + np.exp(-np.dot(X, mlp.coefs_[0]) - mlp.intercepts_[0]))\n            expected_output = 1 / (1 + np.exp(-np.dot(expected_output, np.array([[0.1], [0.2]])) - 1.0))\n        elif activation == 'tanh':\n            expected_output = np.tanh(np.dot(X, mlp.coefs_[0]) + mlp.intercepts_[0])\n            expected_output = np.tanh(np.dot(expected_output, np.array([[0.1], [0.2]])) + 1.0)\n        elif activation == 'relu':\n            expected_output = np.maximum(0, np.dot(X, mlp.coefs_[0]) + mlp.intercepts_[0])\n            expected_output = np.maximum(0, np.dot(expected_output, np.array([[0.1], [0.2]])) + 1.0)\n        elif activation == 'identity':\n            expected_output = np.dot(X, mlp.coefs_[0]) + mlp.intercepts_[0]\n            expected_output = np.dot(expected_output, np.array([[0.1], [0.", "def test_unseen_activation():\n    # Test that an unseen activation function raises an error.\n    X = [[3, 2], [1, 6]]\n    y = [1, 0]\n    clf = MLPClassifier(activation='unknown_activation')\n    assert_raises(ValueError, clf.fit, X, y)\n", "def test_validation_fraction_errors():\n    # Test for validation_fraction parameter when early_stopping is True.\n    X = [[3, 2], [1, 6]]\n    y = [1, 0]\n\n    clf = MLPClassifier(early_stopping=True, validation_fraction=1.1)\n    assert_raises(ValueError, clf.fit, X, y)\n\n    clf = MLPClassifier(early_stopping=True, validation_fraction=-0.1)\n    assert_raises(ValueError, clf.fit, X, y)\n\n    clf = MLPClassifier(early_stopping=True, validation_fraction=0)\n    assert_raises(ValueError, clf.fit, X, y)\n\n    clf = MLPClassifier(early_stopping=True, validation_fraction=0.5)\n    # Should not raise an error\n    clf.fit(X, y)\n"], "sample_901": ["def test_k_means_invalid_algorithm():\n    # Check that KMeans raises a ValueError for an invalid algorithm parameter\n    invalid_algo = \"invalid\"\n    km = KMeans(n_clusters=n_clusters, algorithm=invalid_algo)\n    msg = \"Algorithm must be 'auto', 'full' or 'elkan', got %s\" % invalid_algo\n    assert_raises_regex(ValueError, msg, km.fit, X)\n", "def test_minibatch_kmeans_max_iter():\n    # Check that max_iter parameter is respected in MiniBatchKMeans\n    max_iters = [1, 5, 10]\n    for max_iter in max_iters:\n        mb_kmeans = MiniBatchKMeans(n_clusters=n_clusters, max_iter=max_iter,\n                                    random_state=42, verbose=True)\n        old_stdout = sys.stdout\n        sys.stdout = StringIO()\n        try:\n            mb_kmeans.fit(X)\n        finally:\n            sys.stdout = old_stdout\n        assert mb_kmeans.n_iter_ <= max_iter\n\n        # Check that fitting MiniBatchKMeans with max_iter=1 returns sensible results\n        if max_iter == 1:\n            assert mb_kmeans.n_iter_ == 1\n            assert mb_kmeans.inertia_ > 0\n            assert hasattr(mb_kmeans, 'cluster_centers_')\n", "def test_k_means_with_zero_or_negative_tolerance():\n    # Test that a zero or negative tolerance is handled correctly\n    km_zero_tol = KMeans(tol=0, n_clusters=n_clusters, random_state=42)\n    km_neg_tol = KMeans(tol=-1e-4, n_clusters=n_clusters, random_state=42)\n\n    # Fit with zero tolerance\n    km_zero_tol.fit(X)\n    _check_fitted_model(km_zero_tol)\n    \n    # Fit with negative tolerance\n    km_neg_tol.fit(X)\n    _check_fitted_model(km_neg_tol)\n\n    assert km_zero_tol.inertia_ > 0.0\n    assert km_neg_tol.inertia_ > 0.0\n", "def test_k_means_plus_plus_initialization():\n    # Test k-means++ initialization to ensure centroids are initialized correctly\n    rng = np.random.RandomState(42)\n    X = rng.rand(100, 5)\n\n    # Use k-means++ to initialize\n    initial_centers = _k_init(X, n_clusters=3, x_squared_norms=row_norms(X, squared=True), random_state=rng)\n\n    # Check that the initial centers are unique\n    assert len(np.unique(initial_centers, axis=0)) == 3\n\n    # Check that the initial centers are from the input data\n    assert np.all(np.isin(initial_centers, X))\n\n    # Check that the initial centers are float dtype\n    assert initial_centers.dtype == np.float64\n\n    # Check that the initial centers are of correct shape\n    assert initial_centers.shape == (3, 5)\n", "def test_k_means_single_init():\n    # Check that using single init produces consistent results\n    init_centers = np.array([[0, 0], [1, 1]])\n    kmeans = KMeans(n_clusters=2, n_init=1, init=init_centers, random_state=42)\n    \n    # Use a fixed dataset to test\n    X_test = np.array([[0, 0], [0.5, 0], [0.5, 1], [1, 1]])\n    expected_centers = np.array([[0, 0], [1, 1]])\n    \n    kmeans.fit(X_test)\n    \n    assert_array_almost_equal(kmeans.cluster_centers_, expected_centers)\n    assert kmeans.n_iter_ == 1\n"], "sample_902": ["def test_pipeline_with_none_transformer():\n    # Test that a pipeline works when intermediate transformer is set to None\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Define a pipeline with None in the middle\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('none_transformer', None),\n        ('svc', SVC(probability=True, random_state=0))\n    ])\n\n    # Fit and predict\n    pipe.fit(X, y)\n    predictions = pipe.predict(X)\n    assert_equal(predictions.shape, (X.shape[0],))\n\n    # Check that StandardScaler has transformed the data\n    scaled_X = StandardScaler().fit_transform(X)\n    svc = SVC(probability=True, random_state=0).fit(scaled_X, y)\n    expected_predictions = svc.predict(scaled_X)\n    assert_array_equal(predictions, expected_predictions)\n\n    # Check that predict_proba works\n    probabilities = pipe.predict_proba(X)\n    expected_probabilities = svc.predict_proba(scaled_X)\n    assert_array_almost_equal(probabilities, expected_probabilities)\n\n    # Check that predict_log_proba works\n    log_probabilities = pipe.predict_log_proba(X)\n    expected_log_probabilities = svc.predict_log_proba(scaled_X)\n    assert_array_almost_equal(log_probabilities, expected_log_probabilities)\n\n    # Check score method\n    score = pipe.score(X, y)\n    expected_score = svc.score(scaled_X, y)\n    assert_almost_equal(score, expected_score)\n", "def test_pipeline_memory_with_different_transformers():\n    # Test that memory works correctly with different transformers\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(cachedir=cachedir, verbose=10)\n        # Define two different transformers\n        transf1 = DummyTransf()\n        transf2 = StandardScaler()\n        clf = SVC(probability=True, random_state=0)\n\n        # Create pipelines with different transformers\n        pipe1 = Pipeline([('transf', transf1), ('svc', clf)], memory=memory)\n        pipe2 = Pipeline([('transf', transf2), ('svc', clf)], memory=memory)\n\n        # Fit the pipelines\n        pipe1.fit(X, y)\n        pipe2.fit(X, y)\n\n        # Check that the results are different due to different transformers\n        assert_false(np.array_equal(pipe1.predict(X), pipe2.predict(X)))\n        assert_false(np.array_equal(pipe1.predict_proba(X), pipe2.predict_proba(X)))\n        assert_false(np.array_equal(pipe1.predict_log_proba(X), pipe2.predict_log_proba(X)))\n        assert_false(pipe1.score(X, y) == pipe2.score(X, y))\n\n        # Check that the cache is used\n        ts1 = pipe1.named_steps['transf'].timestamp_\n        pipe1.fit(X, y)\n        assert_equal(ts1, pipe1.named_steps['transf'].timestamp_)\n\n        # Ensure StandardScaler does not have timestamp_\n        assert_false(hasattr(pipe2.named_steps['transf'], 'timestamp_'))\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_with_none_intermediate_step():\n    # Test pipeline with None as intermediate step\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    pca = PCA(n_components=2, svd_solver='full')\n    svc = SVC(probability=True, random_state=0)\n    pipeline = Pipeline([('pca', pca), ('none', None), ('svc', svc)])\n\n    # Fit and predict with the pipeline\n    pipeline.fit(X, y)\n    predictions = pipeline.predict(X)\n\n    # Ensure predictions work as expected\n    assert_equal(predictions.shape, (X.shape[0],))\n    assert_array_equal(predictions, svc.fit(pca.fit_transform(X), y).predict(pca.transform(X)))\n\n    # Ensure score works as expected\n    score = pipeline.score(X, y)\n    assert_equal(score, svc.score(pca.transform(X), y))\n", "def test_pipeline_with_none_step():\n    # Test that a pipeline works correctly when one of its steps is set to None\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    clf = SVC(probability=True, random_state=0)\n    pipe = Pipeline([('transformer', None), ('svc', clf)])\n    \n    # Ensure the pipeline can be fitted and used for predictions\n    pipe.fit(X, y)\n    predictions = pipe.predict(X)\n    assert_equal(predictions.shape, (X.shape[0],))\n    \n    # Ensure other methods work as expected\n    proba = pipe.predict_proba(X)\n    assert_equal(proba.shape, (X.shape[0], len(np.unique(y))))\n    \n    decision_func = pipe.decision_function(X)\n    assert_equal(decision_func.shape, (X.shape[0], len(np.unique(y))))\n    \n    log_proba = pipe.predict_log_proba(X)\n    assert_equal(log_proba.shape, (X.shape[0], len(np.unique(y))))\n    \n    score = pipe.score(X, y)\n    assert_true(isinstance(score, float))\n", "def test_pipeline_memory_transform_only():\n    # Test caching with transformer only pipeline\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(cachedir=cachedir, verbose=10)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', transf)], memory=memory)\n\n        # Fit and cache the transformer\n        pipe.fit(X, y)\n        ts = pipe.named_steps['transf'].timestamp_\n\n        # Fit again and ensure the timestamp has not changed (cache hit)\n        pipe.fit(X, y)\n        assert_equal(ts, pipe.named_steps['transf'].timestamp_)\n\n        # Check the transform method yields consistent results\n        X_transformed = pipe.transform(X)\n        X_transformed_again = pipe.transform(X)\n        assert_array_equal(X_transformed, X_transformed_again)\n\n    finally:\n        shutil.rmtree(cachedir)\n"], "sample_903": ["def test_gradient_descent_with_different_learning_rates():\n    # Test gradient descent with different learning rates.\n    class ObjectiveSimple:\n            self.it = -1\n\n            self.it += 1\n            return (10 - self.it) / 10.0, np.array([1e-5])\n\n    learning_rates = [10, 100, 1000]\n    for learning_rate in learning_rates:\n        _, error, it = _gradient_descent(\n            ObjectiveSimple(), np.zeros(1), 0, n_iter=100,\n            n_iter_without_progress=100, momentum=0.0, learning_rate=learning_rate,\n            min_gain=0.0, min_grad_norm=1e-5, verbose=0)\n        assert_equal(error, 0.9)\n        assert_equal(it, 1)\n", "def test_trustworthiness_with_different_metrics():\n    # Test trustworthiness with various metrics.\n    random_state = check_random_state(0)\n    X = random_state.randn(100, 2)\n    metrics = ['euclidean', 'manhattan', 'cosine']\n    for metric in metrics:\n        if metric == 'euclidean':\n            D = pairwise_distances(X, metric=metric)\n        elif metric == 'manhattan':\n            D = manhattan_distances(X)\n        elif metric == 'cosine':\n            D = cosine_distances(X)\n        \n        tsne = TSNE(n_components=2, perplexity=10, learning_rate=100.0,\n                    metric=metric, random_state=0)\n        X_embedded = tsne.fit_transform(X)\n        t = trustworthiness(X, X_embedded, n_neighbors=5, metric=metric)\n        assert_greater(t, 0.8, msg='Trustworthiness={:0.3f} < 0.8 for metric={}'\n                      .format(t, metric))\n", "def test_kl_divergence_bh_gradient():\n    # Test the gradient of the KL divergence using the Barnes-Hut approximation.\n    random_state = check_random_state(0)\n\n    n_samples = 100\n    n_features = 5\n    n_components = 2\n    alpha = 1.0\n    angle = 0.5\n\n    distances = random_state.randn(n_samples, n_features).astype(np.float32)\n    distances = np.abs(distances.dot(distances.T))\n    np.fill_diagonal(distances, 0.0)\n    X_embedded = random_state.randn(n_samples, n_components).astype(np.float32)\n\n    k = min(n_samples - 1, int(3. * 30.0 + 1))\n    knn = NearestNeighbors(algorithm='auto', n_neighbors=k, metric=\"euclidean\")\n    knn.fit(distances)\n    distances_nn, neighbors_nn = knn.kneighbors(None, n_neighbors=k)\n\n    P = _joint_probabilities_nn(distances_nn, neighbors_nn, desired_perplexity=30.0, verbose=0)\n\n        return _kl_divergence_bh(params, P, alpha, n_samples, n_components, angle=angle, verbose=0)[0]\n\n        return _kl_divergence_bh(params, P, alpha, n_samples, n_components, angle=angle, verbose=0)[1]\n\n    assert_almost_equal(check_grad(fun, grad, X_embedded.ravel()), 0.0, decimal=5)\n", "def test_kl_divergence_bh():\n    # Test gradient computation using Barnes-Hut method.\n    random_state = check_random_state(0)\n\n    n_samples = 50\n    n_features = 2\n    n_components = 2\n    alpha = 1.0\n    angle = 0.5\n\n    distances = random_state.randn(n_samples, n_features).astype(np.float32)\n    distances = np.abs(distances.dot(distances.T))\n    np.fill_diagonal(distances, 0.0)\n    X_embedded = random_state.randn(n_samples, n_components).astype(np.float32)\n\n    k = min(n_samples - 1, int(3. * 30.0 + 1))\n    knn = NearestNeighbors(algorithm='auto', n_neighbors=k, metric='euclidean')\n    knn.fit(distances)\n    distances_nn, neighbors_nn = knn.kneighbors(None, n_neighbors=k)\n    distances_nn **= 2  # convert to squared distances\n\n    P = _joint_probabilities_nn(distances_nn, neighbors_nn, 30.0, 0)\n\n        return _kl_divergence_bh(params, P, alpha, n_samples, n_components,\n                                 angle=angle, skip_num_points=0, verbose=0)[0]\n\n        return _kl_divergence_bh(params, P, alpha, n_samples, n_components,\n                                 angle=angle, skip_num_points=0, verbose=0)[1]\n\n    assert_almost_equal(check_grad(fun, grad, X_embedded.ravel()), 0.0,\n                        decimal=4)\n", "def test_trustworthiness_with_various_metrics():\n    # Test trustworthiness with a variety of metrics\n    random_state = check_random_state(0)\n    X = random_state.randn(100, 2)\n    metrics = ['euclidean', 'manhattan', 'chebyshev', 'cosine']\n\n    for metric in metrics:\n        trust = trustworthiness(X, X, metric=metric)\n        precomputed_trust = trustworthiness(pairwise_distances(X, metric=metric), X, metric='precomputed')\n        assert_almost_equal(trust, precomputed_trust, decimal=5)\n"], "sample_904": ["def test_generic_object_add_target_and_index():\n    env = mock.Mock(domaindata={})\n    env.app.registry.enumerable_nodes = {}\n    env.new_serialno.return_value = 1\n    env.get_domain.return_value = StandardDomain(env)\n\n    # Mock document\n    document = mock.Mock()\n    document.note_explicit_target = mock.Mock()\n    document.ids = {}\n\n    # Mock signature node\n    signode = addnodes.desc_signature('test', '')\n    signode['ids'] = []\n\n    # Initialize GenericObject and test add_target_and_index\n    obj = GenericObject(env)\n    obj.env = env\n    obj.state = mock.Mock()\n    obj.state.document = document\n    obj.objtype = 'testobj'\n    obj.indexnode = addnodes.index()\n    obj.indexnode['entries'] = []\n\n    obj.add_target_and_index('testname', 'testsig', signode)\n\n    assert 'testobj-testname' in signode['ids']\n    assert ('single', 'testname', 'testobj-testname', '', None) in obj.indexnode['entries']\n    document.note_explicit_target.assert_called_with(signode)\n", "def test_envvar_directive(app):\n    text = (\".. envvar:: VAR_NAME\\n\"\n            \"   :synopsis: A sample environment variable\\n\"\n            \"\\n\"\n            \"   The description of the environment variable.\\n\")\n    domain = app.env.get_domain('std')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"VAR_NAME\"],\n                                                    [desc_addname, \"\"])],\n                                  [desc_content, nodes.paragraph, \"The description of the environment variable.\"])]))\n    assert_node(doctree[0], addnodes.index,\n                entries=[('single', 'environment variable; VAR_NAME', 'envvar-VAR_NAME', '', None)])\n    assert ('envvar', 'VAR_NAME') in domain.objects\n    assert domain.objects[('envvar', 'VAR_NAME')] == ('index', 'envvar-VAR_NAME')\n", "def test_envvar():\n    text = \".. envvar:: MY_VARIABLE\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (index,\n                          [desc, ([desc_signature, ([desc_name, 'MY_VARIABLE'])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[0], addnodes.index,\n                entries=[('single', 'environment variable; MY_VARIABLE', 'envvar-MY_VARIABLE', '', None)])\n\n    objects = list(app.env.get_domain(\"std\").get_objects())\n    assert ('MY_VARIABLE', 'MY_VARIABLE', 'envvar', 'index', 'envvar-MY_VARIABLE', 1) in objects\n", "def test_envvar_role(app):\n    text = \"This is an :envvar:`example` of environment variable.\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, ([nodes.paragraph, (\"This is an \", [pending_xref, nodes.inline, \"example\"], \" of environment variable.\")],))\n\n    domain = app.env.get_domain(\"std\")\n    refnode = domain.resolve_xref(app.env, 'index', app.builder, 'envvar', 'example', pending_xref(), nodes.paragraph())\n    assert_node(refnode, nodes.reference, reftitle=\"example\", refid=\"index-0\")\n", "def test_make_old_id_generic_object():\n    env = mock.Mock(domaindata={})\n    env.app.registry.enumerable_nodes = {}\n    domain = StandardDomain(env)\n    \n    generic_object = GenericObject()\n    generic_object.env = env\n    generic_object.state = mock.Mock()\n    generic_object.state.document = mock.Mock()\n    \n    old_id = generic_object.make_old_id('testname')\n    assert old_id == 'genericobject-testname'\n"], "sample_905": ["def test_ispartial():\n        return a + b\n\n    p = functools.partial(func, 10)\n    pm = functools.partialmethod(func, 10)\n\n    assert inspect.ispartial(p) is True\n    assert inspect.ispartial(pm) is True\n    assert inspect.ispartial(func) is False\n    assert inspect.ispartial(10) is False\n", "def test_ispartial():\n        pass\n\n    p = functools.partial(func, 1)\n    assert inspect.ispartial(p) is True\n\n    pm = functools.partialmethod(func, 1)\n    assert inspect.ispartial(pm) is True\n\n    assert inspect.ispartial(func) is False\n\n    class Foo:\n            pass\n\n    f = Foo()\n    assert inspect.ispartial(f.method) is False\n", "def test_getall():\n    class NoAll:\n        pass\n\n    class WithValidAll:\n        __all__ = [\"a\", \"b\", \"c\"]\n\n    class WithInvalidAll:\n        __all__ = [\"a\", 1, None]\n\n    # Test module without __all__\n    assert inspect.getall(NoAll) is None\n\n    # Test module with valid __all__\n    assert inspect.getall(WithValidAll) == [\"a\", \"b\", \"c\"]\n\n    # Test module with invalid __all__\n    with pytest.raises(ValueError):\n        inspect.getall(WithInvalidAll)\n", "def test_getall():\n    class Foo:\n        __all__ = ['a', 'b', 'c']\n\n    class Bar:\n        __all__ = 'invalid'\n\n    class Baz:\n        pass\n\n    assert inspect.getall(Foo) == ['a', 'b', 'c']\n    with pytest.raises(ValueError):\n        inspect.getall(Bar)\n    assert inspect.getall(Baz) is None\n", "def test_isNewType():\n    if sys.version_info >= (3, 10):\n        NewType = typing.NewType('NewType', int)\n        assert inspect.isNewType(NewType) is True\n        assert inspect.isNewType(int) is False\n    else:\n        NewType = typing.NewType('NewType', int)\n        assert inspect.isNewType(NewType) is True\n        assert inspect.isNewType(int) is False\n"], "sample_906": ["def test_domain_c_ast_identifier():\n    # Test ASTIdentifier class functionality\n\n    ident1 = ASTIdentifier(\"example\")\n    ident2 = ASTIdentifier(\"example\")\n    ident3 = ASTIdentifier(\"different\")\n\n    # Test equality\n    assert ident1 == ident2\n    assert ident1 != ident3\n\n    # Test string representation\n    assert str(ident1) == \"example\"\n    assert ident1.get_display_string() == \"example\"\n\n    # Test anonymous identifier\n    anon_ident = ASTIdentifier(\"@anon\")\n    assert anon_ident.is_anon()\n    assert anon_ident.get_display_string() == \"[anonymous]\"\n", "def test_domain_c_ast_literals():\n    # Test boolean literals\n    boolean_true = \"true\"\n    boolean_false = \"false\"\n    assert str(ASTBooleanLiteral(True)) == boolean_true\n    assert str(ASTBooleanLiteral(False)) == boolean_false\n\n    # Test number literals\n    number_literal = \"42\"\n    assert str(ASTNumberLiteral(number_literal)) == number_literal\n\n    # Test char literals\n    char_literal = ASTCharLiteral(None, 'a')\n    assert str(char_literal) == \"'a'\"\n    char_literal_with_prefix = ASTCharLiteral('u8', 'a')\n    assert str(char_literal_with_prefix) == \"u8'a'\"\n\n    # Test string literals\n    string_literal = '\"hello\"'\n    assert str(ASTStringLiteral(string_literal)) == string_literal\n", "def test_domain_c_function_literals():\n    check('function', 'void f(int a, char b, float c, double d)', {1: 'f__i.c.f.d', 2: '1fi1c1f1d'})\n    check('function', 'void f(bool a, bool b)', {1: 'f__b.b', 2: '1fb1b'})\n    check('function', 'void f(char a, wchar_t b)', {1: 'f__c.wchar_t', 2: '1fc1w'})\n    check('function', 'void f(short a, long b)', {1: 'f__s.l', 2: '1fs1l'})\n    check('function', 'void f(unsigned a, signed b)', {1: 'f__u.s', 2: '1fu1s'})\n    check('function', 'void f(int64_t a, uint64_t b)', {1: 'f__i64.u64', 2: '1fi64u64'})\n    check('function', 'void f(_Bool a)', {1: 'f__b', 2: '1fb'})\n    check('function', 'void f(_Complex a, _Imaginary b)', {1: 'f__complex.imaginary', 2: '1fcomplex1imaginary'})\n    check('function', 'void f(void *a)', {1: 'f__voidP', 2: '1fPv'})\n    check('function', 'void f(const char *a)', {1: 'f__cCP', 2: '1fPKc'})\n    check('function', 'void f(char *restrict a)', {1: 'f__cPr', 2: '1fPrKc'})\n    check('function', 'void f(int a = 42)', {1: 'f__i', 2: '1fi'})\n    check('function', 'void f(const int a = 42)', {1: 'f__iC', 2: '1fKi'})\n    check('function', 'void f(unsigned int a = 42)', {1: 'f__u', 2: '1fu'})\n    check('function', 'void f(int a, char b = \\'c\\')', {1: 'f__i.c', 2: '1fi1c'})\n    check('", "def test_domain_c_basic_parsing():\n    # Test basic parsing for C constructs\n        class Config:\n            c_id_attributes = [\"id_attr\"]\n            c_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(string, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_declaration(name, name)\n        parser.assert_end()\n        return ast\n\n        ast = parse_c(name, input)\n        assert str(ast) == expected_ast_str\n\n    # Test simple function\n    check_c('function', 'void foo(int a)', 'void foo(int a)')\n\n    # Test simple variable\n    check_c('member', 'int x', 'int x')\n\n    # Test function with pointer argument\n    check_c('function', 'void bar(int *ptr)', 'void bar(int *ptr)')\n\n    # Test struct\n    check_c('struct', 'struct Point', 'struct Point')\n\n    # Test union\n    check_c('union', 'union Data', 'union Data')\n\n    # Test enum\n    check_c('enum', 'enum Color', 'enum Color')\n\n    # Test macro\n    check_c('macro', '#define MAX 100', '#define MAX 100')\n\n    # Test typedef\n    check_c('type', 'typedef int integer', 'typedef int integer')\n\n    # Test declarator with initializer\n    check_c('member', 'int y = 10', 'int y = 10')\n\n    # Test function with multiple arguments\n    check_c('function', 'void baz(int a, char b)', 'void baz(int a, char b)')\n", "def test_domain_c_ast_expression_parsing():\n        class Config:\n            c_id_attributes = [\"id_attr\"]\n            c_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(expr, location=None, config=Config())\n        ast = parser.parse_expression()\n        assert str(ast) == expected_str\n\n    exprCheck('5 + 3', '5 + 3')\n    exprCheck('10 - 2', '10 - 2')\n    exprCheck('a * b', 'a * b')\n    exprCheck('x / y', 'x / y')\n    exprCheck('p % q', 'p % q')\n    exprCheck('!true', '!true')\n    exprCheck('~0xff', '~0xff')\n    exprCheck('++i', '++i')\n    exprCheck('--j', '--j')\n    exprCheck('(a + b) * c', '(a + b) * c')\n    exprCheck('sizeof(int)', 'sizeof(int)')\n    exprCheck('alignof(double)', 'alignof(double)')\n    exprCheck('x < y', 'x < y')\n    exprCheck('x <= y', 'x <= y')\n    exprCheck('x > y', 'x > y')\n    exprCheck('x >= y', 'x >= y')\n    exprCheck('x == y', 'x == y')\n    exprCheck('x != y', 'x != y')\n    exprCheck('x && y', 'x && y')\n    exprCheck('x || y', 'x || y')\n    exprCheck('x = 5', 'x = 5')\n    exprCheck('x += 5', 'x += 5')\n    exprCheck('x -= 5', 'x -= 5')\n    exprCheck('x *= 5', 'x *= 5')\n    exprCheck('x /= 5', 'x /= 5')\n    exprCheck('x %= 5', 'x %= 5')\n    exprCheck('a & b', 'a & b')\n    exprCheck('a | b', 'a | b')\n    exprCheck('a ^ b', 'a ^ b')\n    exprCheck('a << 1', 'a << 1')\n    exprCheck('a >> 1', 'a >> 1')\n    exprCheck('x ? y : z', 'x ? y : z')\n"], "sample_907": ["def test_domain_cpp_ast_multiple_template_params():\n    check('class', 'template<typename T1, typename T2, typename T3> {key}A', {2: 'I000E1A'})\n    check('class', 'template<typename T1, template<typename> class T2, int T3> {key}A',\n          {2: 'I0I0E_iE1A'})\n    check('function', 'template<typename T1, typename T2, typename T3> void f()',\n          {2: 'I000E1fv', 4: 'I000E1fvv'})\n    check('member', 'template<typename T1, typename T2, typename T3> A a',\n          {2: 'I000E1a'})\n    check('type', 'template<typename T1, typename T2, typename T3> {key}a = A',\n          {2: 'I000E1a'}, key='using')\n", "def test_domain_cpp_ast_braced_init_lists():\n    # braced-init-lists as function parameters\n    check('function', 'void f(std::initializer_list<int> l)', {1: 'f__std::initializer_list:i:', 2: '1fNSt17initializer_listIiEE'})\n    check('function', 'void f(std::initializer_list<int>{1, 2, 3})', {1: 'f', 2: '1fv'})\n    check('function', 'void f(std::vector<int>{1, 2, 3})', {1: 'f', 2: '1fv'})\n    check('function', 'void f(std::map<int, std::string>{{1, \"one\"}, {2, \"two\"}})', {1: 'f', 2: '1fv'})\n\n    # braced-init-lists in member declarations\n    check('member', 'int arr[5] = {1, 2, 3, 4, 5}', {1: 'arr__A5_i', 2: '3arr'})\n    check('member', 'std::vector<int> vec = {1, 2, 3, 4, 5}', {1: 'vec__std::vector:i:', 2: '3vec'})\n    check('member', 'std::map<int, std::string> m = {{1, \"one\"}, {2, \"two\"}}', {1: 'm__std::map:i.std::string:', 2: '1m'})\n    \n    # braced-init-lists in template arguments\n    check('class', 'template<int... Is> {key}T<Is...>', {2: 'I_DpiE1TIJXIsEEE'})\n    check('class', 'template<int... Is> {key}T<Is...> = {{Is...}}', {2: 'I_DpiE1TIJXIsEEE'})\n    \n    # braced-init-lists in type alias\n    check('type', 'using IntList = std::initializer_list<int>', {1: 'IntList', 2: '7IntList'}, key='using')\n    check('type', 'using Vec = std::vector<int>', {1: 'Vec', 2: '3Vec'}, key='using')\n    check('type', 'using Map = std::map<int, std::", "def test_domain_cpp_parse_operator_overload(app):\n    # Test to ensure that operator overloads are parsed correctly and do not result in duplicate warnings\n    text = (\".. cpp:function:: bool operator==(const A& lhs, const A& rhs)\\n\"\n            \".. cpp:function:: bool operator==(const B& lhs, const B& rhs)\\n\"\n            \".. cpp:function:: bool operator!=(const A& lhs, const A& rhs)\\n\"\n            \".. cpp:function:: bool operator!=(const B& lhs, const B& rhs)\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, [desc, desc, desc, desc])\n    assert_node(doctree[0], desc, domain=\"cpp\", objtype=\"function\",\n                children=[addnodes.desc_signature, addnodes.desc_content])\n    assert_node(doctree[1], desc, domain=\"cpp\", objtype=\"function\",\n                children=[addnodes.desc_signature, addnodes.desc_content])\n    assert_node(doctree[2], desc, domain=\"cpp\", objtype=\"function\",\n                children=[addnodes.desc_signature, addnodes.desc_content])\n    assert_node(doctree[3], desc, domain=\"cpp\", objtype=\"function\",\n                children=[addnodes.desc_signature, addnodes.desc_content])\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 0\n", "def test_domain_cpp_ast_explicit_cast():\n    # Check explicit cast operators\n    check('function', 'void f() { int x = static_cast<int>(42); }',\n          {2: '1fv'})\n    check('function', 'void f() { int x = dynamic_cast<int&>(a); }',\n          {2: '1fv'})\n    check('function', 'void f() { int x = const_cast<int&>(a); }',\n          {2: '1fv'})\n    check('function', 'void f() { int x = reinterpret_cast<int&>(a); }',\n          {2: '1fv'})\n\n    # Check nested explicit cast operators\n    check('function', 'void f() { int x = static_cast<int>(const_cast<double&>(42.0)); }',\n          {2: '1fv'})\n    check('function', 'void f() { int x = dynamic_cast<int&>(reinterpret_cast<double&>(a)); }',\n          {2: '1fv'})\n    check('function', 'void f() { double x = const_cast<double&>(static_cast<int>(a)); }',\n          {2: '1fv'})\n\n    # Check with different combinations of casts\n    check('function', 'void f() { int x = const_cast<int>(dynamic_cast<double&>(a)); }',\n          {2: '1fv'})\n    check('function', 'void f() { double x = reinterpret_cast<double>(const_cast<int&>(42)); }',\n          {2: '1fv'})\n", "def test_domain_cpp_ast_template_specializations():\n    # Test template class specialization\n    check('class', 'template<> {key}A<int>', {2: 'IE1AIiE'}, key='class')\n    check('class', 'template<> {key}A<int*>', {2: 'IE1AIPiE'}, key='class')\n    check('class', 'template<> {key}A<const int>', {2: 'IE1AICKiE'}, key='class')\n\n    # Test template function specialization\n    check('function', 'template<> void f<int>(int)', {2: 'IE1fi', 4: 'IE1fii'})\n\n    # Test template member specialization\n    check('member', 'template<> int A<int>::x', {2: 'IEN1AIiE1xE'})\n\n    # Test nested template specialization\n    check('class', 'template<> template<> {key}A<int>::B<int>', {2: 'IEIEN1AIiE1BIiE'}, key='class')\n    check('function', 'template<> template<> void A<int>::B<int>::f()', {2: 'IEIEN1AIiE1BIiE1fv', 4: 'IEIEN1AIiE1BIiE1fvv'})\n\n    # Test partial template specialization\n    check('class', 'template<typename T> {key}A<T*>', {2: 'I0E1AI1TE'}, key='class')\n    check('function', 'template<typename T> void f(T*)', {2: 'I0E1fP1T', 4: 'I0E1fP1Tv'})\n"], "sample_908": ["def test_unparse_arguments():\n    source = \"def foo(a, b=2, *args, c, d=4, **kwargs): pass\"\n    expected = \"a, b=2, *args, c, d=4, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a, b=2, *args, c, d=4, **kwargs): pass\"\n    expected = \"a, b=2, *args, c, d=4, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a, b=2, *args, c, d=4, **kwargs): pass\"\n    expected = \"a, b=2, *args, c, d=4, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_functiondef(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a, b=1, *args, c, d=2, **kwargs): pass\"\n    expected = \"a, b=1, *args, c, d=2, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n"], "sample_909": ["    def test_consume_indented_block(self):\n        config = Config()\n        docstring = \"\"\"\n        Parameters:\n            param1 : int\n                Description of param1\n            param2 : str\n                Description of param2 with an indented block\n                that spans multiple lines\n        \"\"\"\n        parsed_docstring = GoogleDocstring(docstring, config=config)\n        expected = [\n            '',\n            ':Parameters:',\n            '    * **param1** (*int*) -- Description of param1',\n            '    * **param2** (*str*) -- Description of param2 with an indented block',\n            '      that spans multiple lines',\n            ''\n        ]\n        actual = parsed_docstring.lines()\n        self.assertEqual(expected, actual)\n", "    def test_empty_docstring(self):\n        config = Config()\n        docstring = \"\"\n        actual = str(GoogleDocstring(docstring, config))\n        expected = \"\"\n        self.assertEqual(expected, actual)\n", "    def test_custom_sections(self):\n        docstring = \"\"\"\\", "    def test_no_docstring(self):\n        config = Config()\n        actual = str(GoogleDocstring(\"\", config=config, app=None, what='module', name='module_name', obj=None))\n        expected = \"\"\n        self.assertEqual(expected, actual)\n", "    def test_custom_section_header_formatting(self):\n        docstring = \"\"\"\\"], "sample_910": ["def test_memory_handler(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n    \n    memhandler = logging.MemoryHandler()\n    memhandler.setLevel(logging.DEBUG)\n    logger.addHandler(memhandler)\n\n    # Log some messages\n    logger.debug('debug message')\n    logger.info('info message')\n    logger.warning('warning message')\n\n    # Check that the messages are buffered\n    assert len(memhandler.buffer) == 3\n\n    # Flush the messages to a new logger and check if the messages are handled\n    new_logger = logging.getLogger('new_logger')\n    memhandler.flushTo(new_logger)\n    assert len(memhandler.buffer) == 0\n\n    new_status = io.StringIO()\n    new_warning = io.StringIO()\n    logging.setup(app, new_status, new_warning)\n\n    # The new logger should handle the messages\n    assert 'debug message' in new_status.getvalue()\n    assert 'info message' in new_status.getvalue()\n    assert 'warning message' in new_warning.getvalue()\n", "def test_memory_handler(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    memhandler = logging.MemoryHandler()\n    memhandler.setLevel(logging.WARNING)\n    logger.addHandler(memhandler)\n\n    logger.warning('message1')\n    logger.warning('message2')\n\n    assert len(memhandler.buffer) == 2\n    assert memhandler.buffer[0].getMessage() == 'message1'\n    assert memhandler.buffer[1].getMessage() == 'message2'\n\n    logger.removeHandler(memhandler)\n\n    # flushing logs to another logger\n    logger2 = logging.getLogger('new_logger')\n    memhandler.flushTo(logger2)\n\n    assert len(memhandler.buffer) == 0  # buffer should be cleared\n", "def test_logcollector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n    log_collector = logging.LogCollector()\n\n    with log_collector.collect():\n        logger.info('message1')\n        logger.warning('message2')\n        logger.error('message3')\n\n    collected_logs = log_collector.logs\n    assert len(collected_logs) == 3\n    assert collected_logs[0].getMessage() == 'message1'\n    assert collected_logs[1].getMessage() == 'WARNING: message2'\n    assert collected_logs[2].getMessage() == 'WARNING: message3'\n", "def test_memory_handler(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n    memhandler = logging.MemoryHandler()\n\n    logger.addHandler(memhandler)\n    logger.warning('message1')\n    logger.error('message2')\n\n    assert len(memhandler.buffer) == 2\n    assert memhandler.buffer[0].msg == 'message1'\n    assert memhandler.buffer[1].msg == 'message2'\n\n    memhandler.clear()\n    assert len(memhandler.buffer) == 0\n", "def test_memory_handler(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n    memhandler = logging.MemoryHandler()\n\n    # log messages to the memory handler\n    logger.addHandler(memhandler)\n    logger.info('message1')\n    logger.warning('message2')\n\n    # check that messages are buffered but not flushed yet\n    assert len(memhandler.buffer) == 2\n    assert memhandler.buffer[0].getMessage() == 'message1'\n    assert memhandler.buffer[1].getMessage() == 'message2'\n\n    # flush messages to another logger and check they are no longer in buffer\n    target_logger = logging.getLogger('target')\n    memhandler.flushTo(target_logger)\n    assert len(memhandler.buffer) == 0\n    assert 'message1' in status.getvalue()\n    assert 'WARNING: message2' in warning.getvalue()\n\n    # clear memory handler and check buffer is empty\n    memhandler.clear()\n    assert len(memhandler.buffer) == 0\n"], "sample_911": ["def test_decl_specs():\n    # Test various combinations of declaration specifiers\n\n    check('type', 'static int a', {1: 'a', 2: '1a'}, 'int a')\n    check('type', 'extern int a', {1: 'a', 2: '1a'}, 'int a')\n    check('type', 'mutable int a', {1: 'a', 2: '1a'}, 'int a')\n    check('type', 'register int a', {1: 'a', 2: '1a'}, 'int a')\n    check('type', 'thread_local int a', {1: 'a', 2: '1a'}, 'int a')\n\n    check('function', 'inline void f()', {1: 'f', 2: '1fv'})\n    check('function', 'virtual void f()', {1: 'f', 2: '1fv'})\n    check('function', 'explicit void f()', {1: 'f', 2: '1fv'}, 'void f()')\n    check('function', 'constexpr void f()', {1: 'fCE', 2: '9fvv'}, 'constexpr void f()')\n    check('function', 'virtual constexpr void f()', {1: 'fCE', 2: '9fvv'}, 'constexpr void f()')\n    check('function', 'friend void f()', {1: 'f', 2: '1fv'})\n\n    check('type', 'constexpr int a', {1: 'a', 2: '1a'}, 'int a')\n    check('type', 'volatile int a', {1: 'a', 2: '1a'}, 'int a')\n    check('type', 'const int a', {1: 'a', 2: '1a'}, 'int a')\n    check('type', 'volatile const int a', {1: 'a', 2: '1a'}, 'int a')\n", "def test_nested_name_expressions():\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser._parse_nested_name()\n        res = str(ast)\n        if res != target:\n            print(\"\")\n            print(\"Input:    \", target)\n            print(\"Result:   \", res)\n            raise DefinitionError(\"\")\n\n    check_expr(\"A::B::C\")\n    check_expr(\"A<B>::C\")\n    check_expr(\"A<B, C>::D\")\n    check_expr(\"A<B<C>>::D\")\n    check_expr(\"A<B, C<D>>::E\")\n    check_expr(\"A::B::C<D>::E::F\")\n    check_expr(\"A::B::C<D>::template E::F\")\n    check_expr(\"A::B::template C<D>::E::F\")\n    check_expr(\"A::template B<C<D>>::E::F\")\n    check_expr(\"A<B<C<D, E<F>>>>::G\")\n    check_expr(\"A<B<C<D<E<F>>>>>::G\")\n    check_expr(\"A<B<C<D<E<F>::G>>>>\")\n    check_expr(\"A<B<C<D<E<F>::G>>>::H>\")\n    check_expr(\"A::B::C::D\")\n    check_expr(\"A::B<C>::D\")\n    check_expr(\"A<B::C>::D\")\n    check_expr(\"A<B::C>::D::E\")\n", "def test_symbol_lookup():\n    class Config:\n        cpp_id_attributes = []\n        cpp_paren_attributes = []\n\n    parser = DefinitionParser('template<typename T> class A;', location=None, config=Config())\n    ast = parser.parse_declaration('class', 'class')\n    parser.assert_end()\n\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n\n    parser = DefinitionParser('A<int>', location=None, config=Config())\n    ast = parser.parse_xref_object()[0]\n    parser.assert_end()\n\n    refSymbol = rootSymbol._find_first_named_symbol(ast.nestedName.names[0].identOrOp, None, ast.nestedName.names[0].templateArgs, False, False, True, True)\n    assert refSymbol is not None\n    assert refSymbol.declaration is not None\n    assert refSymbol.declaration.name.names[0].identOrOp.identifier == 'A'\n    assert refSymbol.declaration.templatePrefix.templates[0].params[0].get_identifier().identifier == 'T'\n", "def test_namespace_definitions():\n    # Test basic namespace parsing\n    check('namespace', 'N', {2: \"1N\"})\n    check('namespace', 'A::B::C', {2: \"N1A1B1CE\"})\n\n    # Test nested namespace with template\n    check('namespace', 'template<> A::B::C<int>', {2: \"IEN1A1B1CIiEE\"})\n\n    # Test namespace with multiple parameters\n    check('namespace', 'template<typename T, int I> A::B::C<T, I>', {2: \"I0_iEN1A1B1CI1TEI1IEE\"})\n\n    # Test namespace with nested template introduction\n    check('namespace', 'template<> Concept{U} A<int>::B::C', {2: 'IEI0EX7ConceptI1UEEN1AIiE1B1CE'})\n\n    # Test namespace with complex nested template introduction\n    check('namespace', 'abc::ns::foo{id_0, id_1, id_2} xyz::bar<id_0, id_1, id_2>', {2: 'I000EXN3abc2ns3fooEI4id_04id_14id_2EEN3xyz3barI4id_04id_14id_2EE'})\n\n    # Test namespace aliasing\n    alias = 'namespace Alias = A::B::C;'\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser(alias, location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    with pytest.raises(DefinitionError):\n        parser.parse_declaration('namespace', 'namespace')\n", "def test_definition_parser():\n    # Tests for the DefinitionParser class and its methods.\n\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(defn, location=None, config=Config())\n        return parser.parse_declaration(\"function\", \"function\")\n\n        parser = DefinitionParser(defn, location=None, config=Config())\n        return parser.parse_expression()\n\n    # Test parsing simple function definitions.\n    ast = parse_definition(\"void f(int x)\")\n    assert str(ast) == \"void f(int x)\"\n    ast = parse_definition(\"int g(double y, char z)\")\n    assert str(ast) == \"int g(double y, char z)\"\n\n    # Test parsing expressions.\n    expr = parse_expression(\"5 + 3\")\n    assert str(expr) == \"5 + 3\"\n    expr = parse_expression(\"a * (b + c)\")\n    assert str(expr) == \"a * (b + c)\"\n\n    # Test parsing type definitions.\n    ast = parse_definition(\"bool h()\")\n    assert str(ast) == \"bool h()\"\n    ast = parse_definition(\"std::vector<int> v()\")\n    assert str(ast) == \"std::vector<int> v()\"\n\n    # Test parsing of template declarations.\n    ast = parse_definition(\"template<typename T> void f(T t)\")\n    assert str(ast) == \"template<typename T> void f(T t)\"\n    ast = parse_definition(\"template<int N> class A\")\n    assert str(ast) == \"template<int N> class A\"\n"], "sample_912": ["def test_parse_annotation_with_typing():\n    \"\"\"Test _parse_annotation with complex typing annotations.\"\"\"\n    annotations = [\n        (\"Optional[int]\", [pending_xref, \"Optional\", desc_sig_punctuation, \"[\", pending_xref, \"int\", desc_sig_punctuation, \"]\"]),\n        (\"Union[int, str]\", [pending_xref, \"Union\", desc_sig_punctuation, \"[\", pending_xref, \"int\", desc_sig_punctuation, \", \", pending_xref, \"str\", desc_sig_punctuation, \"]\"]),\n        (\"Dict[str, List[int]]\", [pending_xref, \"Dict\", desc_sig_punctuation, \"[\", pending_xref, \"str\", desc_sig_punctuation, \", \", pending_xref, \"List\", desc_sig_punctuation, \"[\", pending_xref, \"int\", desc_sig_punctuation, \"]\", desc_sig_punctuation, \"]\"]),\n        (\"Tuple[int, ...]\", [pending_xref, \"Tuple\", desc_sig_punctuation, \"[\", pending_xref, \"int\", desc_sig_punctuation, \", \", nodes.Text(\"...\"), desc_sig_punctuation, \"]\"]),\n    ]\n\n    for annotation, expected in annotations:\n        doctree = _parse_annotation(annotation)\n        assert_node(doctree, expected)\n", "def test_parse_arglist():\n    \"\"\"Test the _parse_arglist function.\"\"\"\n    from sphinx.domains.python import _parse_arglist\n\n        signode = addnodes.desc_signature()\n        params = _parse_arglist(arglist)\n        signode += params\n        return signode.astext()\n\n    rv = parse_arglist('a, b, c')\n    assert rv == 'a, b, c'\n\n    rv = parse_arglist('a: int, b: str, c: bool = True')\n    assert rv == 'a: int, b: str, c: bool = True'\n\n    rv = parse_arglist('a: int = 10, *args, **kwargs')\n    assert rv == 'a: int = 10, *args, **kwargs'\n\n    rv = parse_arglist('a: int, b: str = \"default\", *args, **kwargs: dict')\n    assert rv == 'a: int, b: str = \"default\", *args, **kwargs: dict'\n\n    rv = parse_arglist('*args, **kwargs')\n    assert rv == '*args, **kwargs'\n", "def test_parse_annotation_with_nested_subscripts():\n    # Nested subscript annotations, e.g., Dict[str, List[int]]\n    doctree = _parse_annotation(\"Dict[str, List[int]]\")\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    # More complex nested subscript annotations, e.g., Dict[str, Tuple[int, Union[str, float]]]\n    doctree = _parse_annotation(\"Dict[str, Tuple[int, Union[str, float]]]\")\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"float\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    # Edge case: Empty subscript, e.g., List[]\n    doctree = _parse_annotation(\"List[]\")\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_handle_signature_pyfunction():\n    env = Mock(ref_context={'py:module': 'example', 'py:class': 'MyClass'})\n    domain = PythonDomain(env)\n    directive = PyFunction('py:function', [], {}, Mock(), Mock(), Mock(), Mock())\n    directive.env = env\n\n    signode = addnodes.desc_signature('func', '')\n\n    # Test normal function signature\n    name, prefix = directive.handle_signature('example_func(arg1, arg2)', signode)\n    assert name == 'example_func'\n    assert prefix == ''\n    assert signode.astext() == 'example_func(arg1, arg2)'\n\n    # Test function signature with annotations\n    signode = addnodes.desc_signature('func', '')\n    name, prefix = directive.handle_signature('annotated_func(arg: int) -> str', signode)\n    assert name == 'annotated_func'\n    assert prefix == ''\n    assert_node(signode, ([desc_name, \"annotated_func\"],\n                          [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"arg\"],\n                                                                 [desc_sig_punctuation, \":\"],\n                                                                 \" \",\n                                                                 [desc_sig_name, pending_xref, \"int\"])])],\n                          [desc_returns, ([pending_xref, \"str\"])]))\n", "def test_parse_annotation_with_fallback():\n    doctree = _parse_annotation(\"custom.UnknownType\")\n    assert_node(doctree, ([pending_xref, \"custom.UnknownType\"],))\n\n    doctree = _parse_annotation(\"unknownmodule.Class\")\n    assert_node(doctree, ([pending_xref, \"unknownmodule.Class\"],))\n\n    doctree = _parse_annotation(\"Invalid[Type\")\n    assert_node(doctree, ([pending_xref, \"Invalid[Type\"],))\n\n    doctree = _parse_annotation(\"Dict[int, unknownmodule.UnknownType]\")\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"unknownmodule.UnknownType\"],\n                          [desc_sig_punctuation, \"]\"]))\n"], "sample_913": ["def test_pyclass_signature(app):\n    text = (\".. py:class:: MyClass\\n\"\n            \"   :module: mymodule\\n\"\n            \"   :annotation: Custom annotation\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_addname, \"mymodule.\"],\n                                                    [desc_name, \"MyClass\"],\n                                                    [desc_annotation, \" Custom annotation\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"class\",\n                domain=\"py\", objtype=\"class\", noindex=False)\n\n    assert 'mymodule.MyClass' in domain.objects\n    assert domain.objects['mymodule.MyClass'] == ('index', 'mymodule.MyClass', 'class')\n", "def test_pymodule_directive(app):\n    text = (\".. py:module:: example.module\\n\"\n            \"   :synopsis: This is a test module.\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, ([addnodes.index],\n                          [nodes.target],\n                          [addnodes.index]))\n\n    assert 'example.module' in domain.modules\n    assert domain.modules['example.module'][1] == 'module-example.module'\n    assert domain.modules['example.module'][2] == 'This is a test module.'\n    assert domain.modules['example.module'][3] == 'Unix'\n    assert domain.modules['example.module'][4] is True\n", "def test_pymodule_signature(app):\n    text = (\".. py:module:: testmodule\\n\"\n            \"   :synopsis: This is a test module\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [nodes.target, {\"ids\": [\"module-testmodule\"], \"ismod\": True}],\n                          [addnodes.index, ([addnodes.indexentry, ([nodes.Text, \"module\"], [nodes.Text, \"testmodule\"])])]))\n    assert \"testmodule\" in domain.modules\n    assert domain.modules[\"testmodule\"] == ('index', 'module-testmodule', 'This is a test module', 'Unix', True)\n", "def test_pyclass_signature(app):\n    text = (\".. py:class:: TestClass\\n\"\n            \"   :module: testmodule\\n\"\n            \"\\n\"\n            \"   .. py:method:: method1(param1: int, param2: str) -> bool\\n\"\n            \"   .. py:attribute:: attribute1\\n\"\n            \"      :type: str\\n\"\n            \"      :value: 'default'\\n\"\n            \"   .. py:classmethod:: classmethod1(cls, param: int) -> None\\n\"\n            \"   .. py:staticmethod:: staticmethod1(param: str) -> int\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    \n    # Test class node\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"TestClass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n    assert 'testmodule.TestClass' in domain.objects\n    assert domain.objects['testmodule.TestClass'] == ('index', 'testmodule.TestClass', 'class')\n\n    # Test method node\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'method1() (testmodule.TestClass method)', 'testmodule.TestClass.method1', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"method1\"],\n                                                     [desc_parameterlist,\n                                                      ([desc_parameter, ([desc_sig_name, \"param1\"],\n                                                                         [desc_sig_punctuation, \":\"],\n                                                                         \" \",\n                                                                         [nodes.inline, pending_xref, \"int\"])],\n                                                       [desc_sig_punctuation, \", \"],\n                                                       [desc_parameter, ([desc_sig_name, \"param2\"],\n                                                                         [desc_sig_punctuation, \":\"],\n                                                                         \" \",\n                                                                         [nodes.inline, pending_xref, \"str\"])])],\n                                                     [desc_returns, ([pending_xref, \"bool\"])])],\n                                   [desc_content, ()]))\n    assert 'testmodule.TestClass.method1' in domain.objects\n    assert domain.objects['testmodule.TestClass.method1'] == ('index', 'testmodule.TestClass.method1', 'method')\n\n    # Test attribute node", "def test_parse_arglist():\n    arglist = \"a, b, c=1, d=None, *args, **kwargs\"\n    params = _parse_arglist(arglist)\n    assert_node(params, [desc_parameterlist,\n                         ([desc_parameter, ([desc_sig_name, \"a\"])],\n                          [desc_parameter, ([desc_sig_name, \"b\"])],\n                          [desc_parameter, ([desc_sig_name, \"c\"],\n                                            [desc_sig_operator, \"=\"],\n                                            [nodes.inline, \"1\"])],\n                          [desc_parameter, ([desc_sig_name, \"d\"],\n                                            [desc_sig_operator, \"=\"],\n                                            [nodes.inline, \"None\"])],\n                          [desc_parameter, ([desc_sig_operator, \"*\"],\n                                            [desc_sig_name, \"args\"])],\n                          [desc_parameter, ([desc_sig_operator, \"**\"],\n                                            [desc_sig_name, \"kwargs\"])])])\n"], "sample_914": ["def test_unparse_arguments():\n    source = \"def func(a: int, b: str = 'default', *args, c: float, **kwargs): pass\"\n    expected = \"a: int, b: str = 'default', *args, c: float, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "compilation error", "def test_unparse_function_def():\n    source = \"def foo(a, b=2, *args, c=3, **kwargs): pass\"\n    expected = \"foo(a, b=2, *args, c=3, **kwargs)\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_function_definitions(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_function_with_annotations():\n    source = \"def func(a: int, b: str = 'default') -> bool: pass\"\n    expected = \"a: int, b: str = 'default'\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n"], "sample_915": ["def test_isenumclass():\n    class SampleEnum(enum.Enum):\n        VAL1 = 1\n        VAL2 = 2\n\n    class NonEnumClass:\n        pass\n\n    assert inspect.isenumclass(SampleEnum) is True\n    assert inspect.isenumclass(NonEnumClass) is False\n    assert inspect.isenumclass(SampleEnum.VAL1) is False\n\n", "def test_isabstractmethod():\n    class AbstractClass:\n        @property\n            pass\n        \n        @property\n            return \"normal\"\n\n        @staticmethod\n            pass\n\n    class ConcreteClass(AbstractClass):\n        @property\n            pass\n\n    assert inspect.isabstractmethod(AbstractClass.prop) is True\n    assert inspect.isabstractmethod(AbstractClass.normal_prop) is False\n    assert inspect.isabstractmethod(AbstractClass.static_method) is False\n    assert inspect.isabstractmethod(ConcreteClass.prop) is True\n", "def test_isenumclass():\n    class MyEnum(enum.Enum):\n        ONE = 1\n        TWO = 2\n\n    class NotEnum:\n        pass\n\n    assert inspect.isenumclass(MyEnum) is True\n    assert inspect.isenumclass(NotEnum) is False\n    assert inspect.isenumclass(MyEnum.ONE) is False\n    assert inspect.isenumclass(enum.Enum) is True\n\n", "def test_isenumclass():\n    class MyEnum(enum.Enum):\n        ONE = 1\n        TWO = 2\n\n    class NotEnum:\n        pass\n\n    assert inspect.isenumclass(MyEnum) is True       # MyEnum is an enum class\n    assert inspect.isenumclass(NotEnum) is False     # NotEnum is not an enum class\n    assert inspect.isenumclass(MyEnum.ONE) is False  # MyEnum.ONE is an enum attribute, not a class\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return \"default\"\n\n    @func.register(int)\n        return \"int\"\n\n    @func.register(str)\n        return \"str\"\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func.dispatch(int)) is False\n    assert inspect.is_singledispatch_function(func.dispatch(str)) is False\n"], "sample_916": ["def test_c_domain_basic():\n    class Config:\n        c_id_attributes = [\"id_attr\"]\n        c_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(code, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = False\n        return parser.parse_declaration('function', 'function')\n\n        ast = parse_c(code)\n        assert str(ast) == expected_str, f\"Expected {expected_str}, but got {str(ast)}\"\n\n        ast = parse_c(code)\n        assert ast.get_id(version) == expected_id, f\"Expected {expected_id}, but got {ast.get_id(version)}\"\n\n    # Basic parsing and string conversion\n    check_ast(\"void f(int a)\", \"void f(int a)\")\n    check_ast(\"const char* g()\", \"const char *g()\")\n    \n    # ID generation\n    check_id(\"void f(int a)\", \"1f1i\")\n    check_id(\"const char* g()\", \"1gPKc\")\n\n    # Ensure AST describe_signature works\n    ast = parse_c(\"void f(int a)\")\n    signode = addnodes.desc_signature()\n    ast.describe_signature(signode, 'lastIsName', {}, {})\n    assert signode.astext() == \"void f(int a)\", f\"Expected 'void f(int a)', but got {signode.astext()}\"\n\n    # Test function pointer parsing and ID generation\n    check_ast(\"void (*fptr)(int a)\", \"void (*fptr)(int a)\")\n    check_id(\"void (*fptr)(int a)\", \"1fptrFivE\")\n", "def test_parse_expressions():\n    # Tests for expressions parsing in C domain\n\n        class Config:\n            c_id_attributes = []\n            c_paren_attributes = []\n\n        parser = DefinitionParser(expr, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_expression()\n        assert str(ast) == expected_str\n        assert isinstance(ast, expected_type)\n\n    check_expression(\"true\", \"true\", ASTBooleanLiteral)\n    check_expression(\"false\", \"false\", ASTBooleanLiteral)\n    check_expression(\"42\", \"42\", ASTNumberLiteral)\n    check_expression(\"'a'\", \"'a'\", ASTCharLiteral)\n    check_expression('\"string\"', '\"string\"', ASTStringLiteral)\n    check_expression(\"5 + 3\", \"5 + 3\", ASTBinOpExpr)\n    check_expression(\"(5 * 3)\", \"(5 * 3)\", ASTParenExpr)\n    check_expression(\"a && b\", \"a && b\", ASTBinOpExpr)\n    check_expression(\"sizeof(int)\", \"sizeof(int)\", ASTSizeofType)\n    check_expression(\"sizeof x\", \"sizeof x\", ASTSizeofExpr)\n    check_expression(\"alignof(int)\", \"alignof(int)\", ASTAlignofExpr)\n    check_expression(\"(a + b) * c\", \"(a + b) * c\", ASTBinOpExpr)\n\n    with pytest.raises(DefinitionError):\n        check_expression(\"invalid_expr\", \"invalid_expr\", ASTFallbackExpr)\n", "def test_ASTIdentifier():\n    identifier = ASTIdentifier(\"testIdent\")\n    assert identifier.identifier == \"testIdent\"\n    assert identifier.__str__() == \"testIdent\"\n    assert not identifier.is_anon()\n\n    anon_identifier = ASTIdentifier(\"@anonIdent\")\n    assert anon_identifier.is_anon()\n    assert anon_identifier.get_display_string() == \"[anonymous]\"\n    assert anon_identifier.__str__() == \"@anonIdent\"\n    assert identifier.get_display_string() == \"testIdent\"\n\n    signode = addnodes.desc_signature()\n    identifier.describe_signature(signode, 'markType', None, \"\", None)\n    assert len(signode) == 1\n    assert isinstance(signode[0], addnodes.pending_xref)\n    assert signode[0][0] == \"testIdent\"\n", "def test_c_domain_basic_structs():\n    from sphinx.domains.c import DefinitionParser, ASTNestedName, ASTIdentifier\n\n        parser = DefinitionParser(input_string, location=None, config=None)\n        ast = parser.parse_declaration(\"struct\", \"struct\")\n        name = ast.symbol.get_full_nested_name()\n        assert str(name) == expected_name_str, f\"Expected: {expected_name_str}, Got: {str(name)}\"\n        id_ = ast.get_id(version=2)\n        assert id_ == expected_id, f\"Expected: {expected_id}, Got: {id_}\"\n\n        parser = DefinitionParser(input_string, location=None, config=None)\n        ast = parser.parse_declaration(\"enum\", \"enum\")\n        name = ast.symbol.get_full_nested_name()\n        assert str(name) == expected_name_str, f\"Expected: {expected_name_str}, Got: {str(name)}\"\n        id_ = ast.get_id(version=2)\n        assert id_ == expected_id, f\"Expected: {expected_id}, Got: {id_}\"\n\n    # Test basic struct parsing\n    check_struct(\"struct MyStruct\", \"MyStruct\", \"N8MyStructE\")\n    check_struct(\"struct Namespace::MyStruct\", \"Namespace::MyStruct\", \"N9Namespace8MyStructE\")\n\n    # Test basic enum parsing\n    check_enum(\"enum MyEnum\", \"MyEnum\", \"N6MyEnumE\")\n    check_enum(\"enum Namespace::MyEnum\", \"Namespace::MyEnum\", \"N9Namespace6MyEnumE\")\n\n    # Check nested structs\n    check_struct(\"struct Outer { struct Inner { int x; }; };\", \"Outer::Inner\", \"N5Outer5InnerE\")\n\n    # Check nested enums\n    check_enum(\"enum Outer { enum Inner { VALUE }; };\", \"Outer::Inner\", \"N5Outer5InnerE\")\n\n    # Check anonymous struct\n    parser = DefinitionParser(\"struct { int x; }\", location=None, config=None)\n    ast = parser.parse_declaration(\"struct\", \"struct\")\n    assert ast.symbol.ident.is_anon(), \"Expected anonymous struct\"\n\n    # Check anonymous enum\n    parser = DefinitionParser(\"enum { VALUE }\", location=None, config=None)\n    ast = parser.parse_declaration(\"enum\", \"enum\")\n    assert ast.symbol.ident", "def test_c_domain_macros():\n        parser = DefinitionParser(signature, location=None, config=None)\n        ast = parser.parse_macro()\n        assert str(ast) == expected_display\n        assert ast.get_id(_max_id) == expected_id\n\n    check_macro(\"MY_MACRO\", \"c.MY_MACRO\", \"MY_MACRO\")\n    check_macro(\"MY_MACRO()\", \"c.MY_MACRO\", \"MY_MACRO()\")\n    check_macro(\"MY_MACRO(int a, float b)\", \"c.MY_MACRO\", \"MY_MACRO(int a, float b)\")\n\n        env = {\"docname\": \"testdoc\"}\n        parser = DefinitionParser(signature, location=None, config=None)\n        ast = parser.parse_macro()\n        root_symbol = Symbol(None, None, None, None)\n        parent_symbol = root_symbol.add_declaration(ast, docname=\"testdoc\")\n        parent_node = addnodes.desc()\n        signode = addnodes.desc_signature(signature, '')\n        parent_node += signode\n        ast.describe_signature(signode, 'lastIsName', env, options={})\n        c_domain = CDomain(env)\n        c_domain.add_target_and_index(ast, signature, signode)\n        assert signode['ids'][0] == expected_id\n        assert signode.astext() == expected_display\n\n    check_macro_target_and_index(\"MY_MACRO\", \"c.MY_MACRO\", \"MY_MACRO\")\n    check_macro_target_and_index(\"MY_MACRO()\", \"c.MY_MACRO\", \"MY_MACRO()\")\n    check_macro_target_and_index(\"MY_MACRO(int a, float b)\", \"c.MY_MACRO\", \"MY_MACRO(int a, float b)\")\n"], "sample_917": ["def test_nested_classes():\n    check('class', 'Outer::Inner', {2: 'N5Outer5InnerE'}, output='Outer::Inner')\n    check('class', 'Outer::Inner::MostInner', {2: 'N5Outer5Inner9MostInnerE'}, output='Outer::Inner::MostInner')\n    check('class', 'template<typename T> Outer::Inner', {2: 'I0EN5Outer5InnerE'}, output='template<typename T> Outer::Inner')\n    check('class', 'template<typename T> Outer::Inner::MostInner', {2: 'I0EN5Outer5Inner9MostInnerE'}, output='template<typename T> Outer::Inner::MostInner')\n\n    check('function', 'void Outer::Inner::func()', {2: 'N5Outer5Inner4funcEv', 4: 'N5Outer5Inner4funcEvv'}, output='void Outer::Inner::func()')\n    check('function', 'template<typename T> void Outer::Inner::func()', {2: 'I0EvN5Outer5Inner4funcE', 4: 'I0EvN5Outer5Inner4funcEvv'}, output='template<typename T> void Outer::Inner::func()')\n    \n    check('member', 'int Outer::Inner::var', {2: 'N5Outer5Inner3varE'}, output='int Outer::Inner::var')\n    check('member', 'template<typename T> int Outer::Inner::var', {2: 'I0EN5Outer5Inner3varE'}, output='template<typename T> int Outer::Inner::var')\n", "def test_declaration_with_nested_namespace():\n    check('class', 'namespace A { namespace B { class C; } }',\n          {2: 'N1A1B1C'}, output='A::B::C')\n    check('function', 'namespace A { namespace B { void f(); } }',\n          {2: 'N1A1B1fv', 4: 'N1A1B1fvv'}, output='A::B::f()')\n    check('member', 'namespace A { namespace B { int x; } }',\n          {2: 'N1A1B1x'}, output='A::B::x')\n    check('type', 'namespace A { namespace B { using T = int; } }',\n          {2: 'N1A1B1T'}, output='A::B::T = int')\n", "def test_parse_failures():\n    with pytest.raises(DefinitionError):\n        parse('function', 'int foo(B b=x(a)')  # unbalanced parenthesis\n\n    with pytest.raises(DefinitionError):\n        parse('function', 'int foo)C c=x(a))')  # misplaced parenthesis\n\n    with pytest.raises(DefinitionError):\n        parse('function', 'int foo(D d=x(a')  # missing closing parenthesis\n\n    with pytest.raises(DefinitionError):\n        parse('member', 'int invalid::')  # invalid member declaration\n\n    with pytest.raises(DefinitionError):\n        parse('class', 'template<>')  # incomplete template class\n\n    with pytest.raises(DefinitionError):\n        parse('function', 'operator bool(')  # incomplete operator function\n\n    with pytest.raises(DefinitionError):\n        parse('type', 'typedef int')  # incomplete typedef\n\n    with pytest.raises(DefinitionError):\n        parse('member', 'int * const volatile * i')  # invalid member declaration\n\n    with pytest.raises(DefinitionError):\n        parse('concept', 'template<int> Concept')  # missing concept name\n\n    with pytest.raises(DefinitionError):\n        parse('enum', 'enum A : std::underlying_type<B>')  # incomplete enum declaration\n", "def test_cpp_expr_with_special_chars():\n    special_chars_expr = [\n        ('5 + 3', 'plL5EL3E'),\n        ('a > b', 'gt1a1b'),\n        ('x && y', 'aa1x1y'),\n        ('x or_eq y', 'oR1x1y'),\n        ('a->b->c()', 'ptpt1a1bclE')\n    ]\n\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n\n    for expr, expected_id in special_chars_expr:\n        parser = DefinitionParser(expr, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_expression()\n        res = str(ast)\n        assert res == expr, f\"Expected {expr}, but got {res}\"\n        \n        rootSymbol = Symbol(None, None, None, None, None, None)\n        symbol = rootSymbol.add_declaration(\n            parse(\"class\", f\"template<> C<a[{expr}]>\"), docname=\"TestDoc\"\n        )\n        parentNode = addnodes.desc()\n        signode = addnodes.desc_signature(expr, '')\n        parentNode += signode\n        ast.describe_signature(signode, 'lastIsName', symbol, options={})\n\n        ids = 'IE1CIA{}1aE'.format(expr)\n        idDict = {2: ids, 3: ids.replace(expr, expected_id)}\n\n        idActual = [None]\n        for i in range(1, _max_id + 1):\n            try:\n                id = ast.get_id(version=i)\n                assert id is not None\n                idActual.append(id[len(_id_prefix[i]):])\n            except NoOldIdError:\n                idActual.append(None)\n\n        for i in range(2, _max_id + 1):\n            assert idDict.get(i) == idActual[i], f\"Failed for id version {i}, expected {idDict.get(i)}, but got {idActual[i]}\"\n", "def test_class_with_template_introduction():\n    check('class', 'template<typename T> Concept{U} AnotherClass',\n          {2: 'I0EI0EX7ConceptI1UEE11AnotherClass'})\n    check('class', 'template<int I> RequiresConcept{T} MyClass',\n          {2: 'I_iE0EX15RequiresConceptI1TEE7MyClass'})\n"], "sample_918": ["def test_pyfunction_with_complex_signature(app):\n    text = (\".. py:function:: complex_func(a: int, b: List[str], c: Tuple[int, ...], \"\n            \"*args: int, **kwargs: str) -> Dict[str, Any]\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"complex_func\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"Dict\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"a\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"int\"])],\n                                      [desc_parameter, ([desc_sig_name, \"b\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"List\"],\n                                                        [desc_sig_punctuation, \"[\"],\n                                                        [desc_sig_name, pending_xref, \"str\"],\n                                                        [desc_sig_punctuation, \"]\"])],\n                                      [desc_parameter, ([desc_sig_name, \"c\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"Tuple\"],\n                                                        [desc_sig_punctuation, \"[\"],\n                                                        [desc_sig_name, pending_xref, \"int\"],\n                                                        [desc_sig_punctuation, \", \"],\n                                                        [desc_sig_operator, \"...\"],\n                                                        [desc_sig_punctuation, \"]\"])],\n                                      [desc_parameter, ([desc_sig_operator, \"*\"],\n                                                        [desc_sig_name, \"args\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"int\"])],\n                                      [desc_parameter, ([desc_sig_operator, \"**\"],\n                                                        [desc_sig_name, \"kwargs\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"str\"])])])\n", "def test_type_to_xref():\n    from sphinx.domains.python import type_to_xref\n\n    xref = type_to_xref('int')\n    assert isinstance(xref, pending_xref)\n    assert xref['reftype'] == 'class'\n    assert xref['reftarget'] == 'int'\n\n    xref = type_to_xref('None')\n    assert isinstance(xref, pending_xref)\n    assert xref['reftype'] == 'obj'\n    assert xref['reftarget'] == 'None'\n\n    xref = type_to_xref('List')\n    assert isinstance(xref, pending_xref)\n    assert xref['reftype'] == 'class'\n    assert xref['reftarget'] == 'List'\n", "def test_pycurrentmodule(app):\n    text = (\".. py:currentmodule:: testmodule\\n\"\n            \"\\n\"\n            \"   .. py:function:: test_func1\\n\"\n            \"   .. py:class:: TestClass\\n\"\n            \"       .. py:method:: test_method1\\n\"\n            \"\\n\"\n            \".. py:currentmodule:: anothermodule\\n\"\n            \"\\n\"\n            \"   .. py:function:: test_func2\\n\"\n            \"   .. py:class:: AnotherClass\\n\"\n            \"       .. py:method:: test_method2\\n\"\n            \"\\n\"\n            \".. py:currentmodule:: None\\n\"\n            \"\\n\"\n            \"   .. py:function:: test_func3\\n\"\n            \"   .. py:class:: ThirdClass\\n\"\n            \"       .. py:method:: test_method3\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"test_func1\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"TestClass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"test_func2\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"AnotherClass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"test_func3\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"ThirdClass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n\n    assert domain.objects['testmodule.test_func1'] == ('index', 'testmodule.test_func1', 'function')\n    assert domain.objects['test", "def test_type_to_xref():\n    from sphinx.domains.python import type_to_xref\n\n    node = type_to_xref(\"None\")\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n\n    node = type_to_xref(\"int\")\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n\n    node = type_to_xref(\"str\")\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n\n    node = type_to_xref(\"List[int]\")\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"List[int]\")\n\n    node = type_to_xref(\"typing.List[int]\")\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"typing.List[int]\")\n", "def test_parse_arglist():\n    # Test parsing of argument list with various combinations\n    arglist = 'a, b=1, *args, c=2, **kwargs'\n    params = _parse_arglist(arglist)\n    assert_node(params, ([desc_parameter, ([desc_sig_name, \"a\"])],\n                         [desc_parameter, ([desc_sig_name, \"b\"],\n                                           [desc_sig_operator, \"=\"],\n                                           [nodes.inline, \"1\"])],\n                         [desc_parameter, ([desc_sig_operator, \"*\"],\n                                           [desc_sig_name, \"args\"])],\n                         [desc_parameter, ([desc_sig_name, \"c\"],\n                                           [desc_sig_operator, \"=\"],\n                                           [nodes.inline, \"2\"])],\n                         [desc_parameter, ([desc_sig_operator, \"**\"],\n                                           [desc_sig_name, \"kwargs\"])]))\n\n    # Test PEP-570 positional-only arguments\n    arglist = 'a, /, b, *, c'\n    params = _parse_arglist(arglist)\n    assert_node(params, ([desc_parameter, ([desc_sig_name, \"a\"])],\n                         [desc_parameter, ([desc_sig_operator, \"/\"])],\n                         [desc_parameter, ([desc_sig_name, \"b\"])],\n                         [desc_parameter, ([desc_sig_operator, \"*\"])],\n                         [desc_parameter, ([desc_sig_name, \"c\"])]))\n\n    # Test with type annotations\n    arglist = 'a: int, b: str = \"default\", *args: float, **kwargs: dict'\n    params = _parse_arglist(arglist)\n    assert_node(params, ([desc_parameter, ([desc_sig_name, \"a\"],\n                                           [desc_sig_punctuation, \":\"],\n                                           \" \",\n                                           [pending_xref, \"int\"])],\n                         [desc_parameter, ([desc_sig_name, \"b\"],\n                                           [desc_sig_punctuation, \":\"],\n                                           \" \",\n                                           [pending_xref, \"str\"],\n                                           \" \",\n                                           [desc_sig_operator, \"=\"],\n                                           \" \",\n                                           [nodes.inline, '\"default\"'])],\n                         [desc_parameter, ([desc_sig_operator, \"*\"],\n                                           [desc_sig_name, \"args\"],\n                                           [desc_sig_punctuation, \":\"],\n                                           \" \",\n                                           [pending_xref, \"float\"])],\n                         [desc_parameter, ([desc_sig_operator, \"**\"],\n                                           [desc_sig_name, \"kwargs\"],\n                                           [desc_sig_punctuation, \":\"],\n                                           \" \",\n                                           [pending_xref, \"dict\"])]))\n"], "sample_919": ["def test_namespace_definitions():\n    check('namespace', '{key}MyNamespace', {1: \"MyNamespace\", 2: \"10MyNamespace\"}, key='namespace')\n    check('namespace', '{key}MyNamespace::SubNamespace', {1: \"MyNamespace::SubNamespace\", 2: \"N10MyNamespace12SubNamespaceE\"}, key='namespace')\n    check('namespace', '{key}::GlobalNamespace', {1: \"GlobalNamespace\", 2: \"13GlobalNamespace\"}, key='namespace')\n    check('namespace', '{key}::GlobalNamespace::NestedNamespace', {1: \"GlobalNamespace::NestedNamespace\", 2: \"N13GlobalNamespace15NestedNamespaceE\"}, key='namespace')\n    check('namespace', 'private {key}PrivateNamespace', {1: \"PrivateNamespace\", 2: \"14PrivateNamespace\"}, key='namespace')\n", "def test_id_generation():\n    # Test for different versions of ID generation\n        if t == 'decltype(auto)':\n            return None\n        id = t.replace(\" \", \"-\").replace(\"long\", \"l\").replace(\"int\", \"i\")\n        id = id.replace(\"bool\", \"b\").replace(\"char\", \"c\")\n        id = id.replace(\"wc_t\", \"wchar_t\").replace(\"c16_t\", \"char16_t\")\n        id = id.replace(\"c32_t\", \"char32_t\")\n        return \"f__%s\" % id\n\n        id = id_v2\n        if t == \"std::nullptr_t\":\n            id = \"NSt9nullptr_tE\"\n        return \"1f%s\" % id\n\n    # Testing functions\n    for t, id_v2 in cppDomain._id_fundamental_v2.items():\n        check(\"function\", \"void f(%s arg)\" % t, {1: makeIdV1(t), 2: makeIdV2(t, id_v2)})\n\n    # Testing classes\n    check(\"class\", \"template<typename T> {key}MyClass\", {2: \"I0E7MyClass\"})\n    check(\"class\", \"template<typename T1, typename T2> {key}MyClass\", {2: \"I00E7MyClass\"})\n    \n    # Testing members\n    check(\"member\", \"template<typename T> T myMember\", {2: \"I0E7myMember\"})\n    check(\"member\", \"template<typename T1, typename T2> T1 myMember\", {2: \"I00E7myMember\"})\n    \n    # Testing type aliases\n    check(\"type\", \"template<typename T> {key}MyAlias = T\", {2: \"I0E7MyAlias\"}, key='using')\n    check(\"type\", \"template<typename T1, typename T2> {key}MyAlias = T1\", {2: \"I00E7MyAlias\"}, key='using')\n\n    # Testing functions with noexcept specifiers\n    check(\"function\", \"int get_value() const noexcept\", {1: \"get_valueC\", 2: \"NK9get_valueEv\"})\n    check(\"function\", \"int get_value() const noexcept(std::is_nothrow_move_constructible<T>::value", "def test_parse_literal_errors():\n    # Check for expected errors when parsing invalid literals\n\n    # Unsupported char literals\n    with pytest.raises(DefinitionError):\n        parse('member', \"char c = '\\\\x'\")  # incomplete hex escape\n    with pytest.raises(DefinitionError):\n        parse('member', \"char c = '\\\\u'\")  # incomplete Unicode escape\n    with pytest.raises(DefinitionError):\n        parse('member', \"char c = '\\\\U'\")  # incomplete Unicode escape\n    with pytest.raises(DefinitionError):\n        parse('member', \"char c = '\\\\u0G'\")  # invalid Unicode escape\n    with pytest.raises(DefinitionError):\n        parse('member', \"char c = '\\\\U0001g34c'\")  # invalid Unicode escape\n\n    # Invalid number literals\n    with pytest.raises(DefinitionError):\n        parse('member', \"int i = 0xG\")  # invalid hex literal\n    with pytest.raises(DefinitionError):\n        parse('member', \"int i = 0b2\")  # invalid binary literal\n    with pytest.raises(DefinitionError):\n        parse('member', \"int i = 0758\")  # invalid octal literal\n\n    # Invalid string literals\n    with pytest.raises(DefinitionError):\n        parse('member', 'std::string s = \"unterminated string')  # unterminated string\n    with pytest.raises(DefinitionError):\n        parse('member', 'std::string s = \"invalid\\\\escape\"')  # invalid escape sequence\n\n    # Invalid boolean literals\n    with pytest.raises(DefinitionError):\n        parse('member', \"bool b = 'true'\")  # invalid boolean literal\n    with pytest.raises(DefinitionError):\n        parse('member', \"bool b = 'false'\")  # invalid boolean literal\n\n    # Invalid user-defined literals\n    with pytest.raises(DefinitionError):\n        parse('member', 'int i = 123_invalid')  # invalid user-defined literal\n    with pytest.raises(DefinitionError):\n        parse('member', 'double d = 1.23_invalid')  # invalid user-defined literal\n", "def test_ast_char_literal():\n        try:\n            literal = ASTCharLiteral(prefix, char)\n        except Exception as e:\n            literal = None\n            print(f\"Failed to create ASTCharLiteral with prefix '{prefix}' and char '{char}': {e}\")\n        assert literal is not None\n        assert literal.get_id(version=2) == expected_id\n        assert str(literal) == f\"{prefix}'{char}'\" if prefix else f\"'{char}'\"\n\n    char_literals = [\n        ('', 'a', 'c97'),\n        ('u8', 'a', 'c97'),\n        ('u', '\\\\u0061', 'Ds97'),\n        ('U', '\\\\U00000061', 'Di97'),\n        ('L', 'a', 'w97'),\n        ('', '\\\\n', 'c10'),\n        ('u8', '\\\\n', 'c10'),\n        ('u', '\\\\u000A', 'Ds10'),\n        ('U', '\\\\U0000000A', 'Di10'),\n        ('L', '\\\\n', 'w10'),\n    ]\n\n    for prefix, char, expected_id in char_literals:\n        check_char_literal(prefix, char, expected_id)\n", "def test_unary_and_binary_operators():\n    # unary operators\n    check('function', 'void operator+()', {1: \"add-operator\", 2: \"plv\"})\n    check('function', 'void operator-()', {1: \"sub-operator\", 2: \"miv\"})\n    check('function', 'void operator*()', {1: \"mul-operator\", 2: \"mlv\"})\n    check('function', 'void operator&()', {1: \"and-operator\", 2: \"anv\"})\n    check('function', 'void operator~()', {1: \"inv-operator\", 2: \"cov\"})\n    check('function', 'void operator++()', {1: \"inc-operator\", 2: \"ppv\"})\n    check('function', 'void operator--()', {1: \"dec-operator\", 2: \"mmv\"})\n    check('function', 'void operator!()', {1: \"not-operator\", 2: \"ntv\"})\n\n    # binary operators\n    check('function', 'void operator==()', {1: \"eq-operator\", 2: \"eqv\"})\n    check('function', 'void operator!=()', {1: \"neq-operator\", 2: \"nev\"})\n    check('function', 'void operator<()', {1: \"lt-operator\", 2: \"ltv\"})\n    check('function', 'void operator>()', {1: \"gt-operator\", 2: \"gtv\"})\n    check('function', 'void operator<=()', {1: \"lte-operator\", 2: \"lev\"})\n    check('function', 'void operator>=()', {1: \"gte-operator\", 2: \"gev\"})\n    check('function', 'void operator+()', {1: \"add-operator\", 2: \"plv\"})\n    check('function', 'void operator-()', {1: \"sub-operator\", 2: \"miv\"})\n    check('function', 'void operator*()', {1: \"mul-operator\", 2: \"mlv\"})\n    check('function', 'void operator/()', {1: \"div-operator\", 2: \"dvv\"})\n    check('function', 'void operator%()', {1: \"mod-operator\", 2: \"rmv\"})\n    check('function', 'void operator&()', {1: \"and-operator\","], "sample_920": ["    def test_napoleon_custom_sections_as_dict(self):\n        docstring = \"\"\"\\", "    def test_returns_section_with_colon(self):\n        docstring = \"\"\"\\\n        Single line summary\n\n        Returns:\n            int: Description with a colon: and some more text.\n        \"\"\"\n        expected = \"\"\"\\\n        Single line summary\n\n        :returns: *int* -- Description with a colon: and some more text.\n        \"\"\"\n        config = Config(napoleon_use_rtype=False)\n        actual = str(GoogleDocstring(docstring, config))\n        self.assertEqual(dedent(expected), actual)\n", "    def test_empty_docstring(self):\n        config = Config()\n        docstring = \"\"\n        actual = str(GoogleDocstring(docstring, config=config, app=None,\n                     what='function', name='empty_function', obj=None))\n        expected = \"\"\n        self.assertEqual(expected, actual)\n", "    def test_empty_numpy_section(self):\n        docstring = \"\"\"\\\n        Single line summary\n\n        Parameters\n        ----------\n        \n        \"\"\"\n        expected = \"\"\"\\\n        Single line summary\n\n        :Parameters:\n        \"\"\"\n        config = Config()\n        actual = str(NumpyDocstring(docstring, config))\n        self.assertEqual(dedent(expected), dedent(actual))\n", "    def test_custom_sections_with_admonition(self):\n        docstrings = [(\"\"\"\\"], "sample_921": ["def test_isenumclass():\n    class MyEnum(enum.Enum):\n        VALUE1 = 1\n        VALUE2 = 2\n\n    class NotEnum:\n        pass\n\n    assert inspect.isenumclass(MyEnum) is True\n    assert inspect.isenumclass(enum.Enum) is True\n    assert inspect.isenumclass(NotEnum) is False\n    assert inspect.isenumclass(MyEnum.VALUE1) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return \"default\"\n\n    @func.register(int)\n        return \"int\"\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func.dispatch(int)) is False\n    assert inspect.is_singledispatch_function(42) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return \"default\"\n\n    @foo.register(int)\n        return \"int\"\n\n    assert inspect.is_singledispatch_function(foo) is True\n    assert inspect.is_singledispatch_function(foo.dispatch) is False\n    assert inspect.is_singledispatch_function(foo.register) is False\n\n        return \"not singledispatch\"\n\n    assert inspect.is_singledispatch_function(bar) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return \"default\"\n\n    @func.register(int)\n        return \"int\"\n\n    @func.register(str)\n        return \"str\"\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func.dispatch) is False\n    assert inspect.is_singledispatch_function(func.register) is False\n    assert inspect.is_singledispatch_function(lambda x: x) is False\n", "def test_isenumclass():\n    class TestEnum(enum.Enum):\n        VALUE_1 = 1\n        VALUE_2 = 2\n\n    class NonEnumClass:\n        pass\n\n    assert inspect.isenumclass(TestEnum) is True\n    assert inspect.isenumclass(NonEnumClass) is False\n    assert inspect.isenumclass(1) is False\n    assert inspect.isenumclass(\"string\") is False\n\n"], "sample_922": ["def test_type_to_xref():\n    from sphinx.domains.python import type_to_xref\n    \n    # Test for basic type\n    xref_node = type_to_xref(\"int\")\n    assert isinstance(xref_node, pending_xref)\n    assert_node(xref_node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    assert xref_node.astext() == \"int\"\n    \n    # Test for 'None' type\n    xref_node = type_to_xref(\"None\")\n    assert isinstance(xref_node, pending_xref)\n    assert_node(xref_node, pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n    assert xref_node.astext() == \"None\"\n    \n    # Test for complex type\n    xref_node = type_to_xref(\"List[int]\")\n    assert isinstance(xref_node, pending_xref)\n    assert_node(xref_node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"List[int]\")\n    assert xref_node.astext() == \"List[int]\"\n", "def test_pymodule():\n    text = (\".. py:module:: example_module\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :synopsis: This is an example module\\n\"\n            \"   :noindex:\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    \n    # Verify the index and description of the module\n    assert_node(doctree, (addnodes.index,\n                          [nodes.target, ([desc_annotation, \"module example_module\"],\n                                          [desc_content, ()])]))\n    \n    assert 'example_module' not in domain.objects\n    assert 'example_module' in domain.modules\n    assert domain.modules['example_module'].synopsis == 'This is an example module'\n    assert domain.modules['example_module'].platform == 'Unix'\n    assert domain.modules['example_module'].deprecated is True\n", "def test_type_to_xref():\n    # Test case for type 'None'\n    xref_node = type_to_xref('None')\n    assert_node(xref_node, pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n    assert xref_node.astext() == 'None'\n\n    # Test case for a regular type\n    xref_node = type_to_xref('int')\n    assert_node(xref_node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    assert xref_node.astext() == 'int'\n\n    # Test case for a user-defined type\n    xref_node = type_to_xref('MyClass')\n    assert_node(xref_node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"MyClass\")\n    assert xref_node.astext() == 'MyClass'\n\n    # Test case for a dotted type\n    xref_node = type_to_xref('module.MyClass')\n    assert_node(xref_node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"module.MyClass\")\n    assert xref_node.astext() == 'module.MyClass'\n", "def test_type_to_xref():\n    node = type_to_xref('int')\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    assert node[0].astext() == 'int'\n\n    node = type_to_xref('None')\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n    assert node[0].astext() == 'None'\n\n    node = type_to_xref('List[str]')\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"List\")\n    assert node[0].astext() == 'List'\n\n    node = type_to_xref('Tuple[int, str]')\n    assert_node(node, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"Tuple\")\n    assert node[0].astext() == 'Tuple'\n", "def test_type_to_xref():\n    xref = type_to_xref('int')\n    assert isinstance(xref, pending_xref)\n    assert xref.get('refdomain') == 'py'\n    assert xref.get('reftype') == 'class'\n    assert xref.get('reftarget') == 'int'\n    assert xref.astext() == 'int'\n\n    xref = type_to_xref('None')\n    assert isinstance(xref, pending_xref)\n    assert xref.get('refdomain') == 'py'\n    assert xref.get('reftype') == 'obj'\n    assert xref.get('reftarget') == 'None'\n    assert xref.astext() == 'None'\n"], "sample_923": ["def test_cast_expressions():\n        ids = 'cviL%sE'\n        idDict = {2: ids % expr, 3: ids % id}\n        if id4 is not None:\n            idDict[4] = ids % id4\n        check('class', 'template<> {key}C<a[%s]>' % expr, idDict)\n\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(expr, location=None,\n                                  config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_expression()\n        res = str(ast)\n        if res != expr:\n            print(\"\")\n            print(\"Input:    \", expr)\n            print(\"Result:   \", res)\n            raise DefinitionError(\"\")\n        displayString = ast.get_display_string()\n        if res != displayString:\n            print(\"\")\n            print(\"Input:    \", expr)\n            print(\"Result:   \", res)\n            print(\"Display:  \", displayString)\n            raise DefinitionError(\"\")\n\n    # cast expressions\n    castCheck('(int)2', 'L2E')\n    castCheck('(const int*)2', 'PKiL2E')\n    castCheck('(volatile int&)2', 'RVL2E')\n    castCheck('(const volatile int&&)2', 'RKVL2E')\n    castCheck('(double)4.2', 'dL4.2E')\n    castCheck('(char)\\'c\\'', 'cL\\'c\\'E')\n", "def test_c_language_domain():\n    from sphinx.domains.c import CDomain, CObject, CFunctionObject, CMemberObject, CMacroObject\n    from sphinx.domains.c import CStructObject, CUnionObject, CEnumObject, CEnumeratorObject, CTypeObject\n    from sphinx.domains.c import CNamespaceObject, CNamespacePushObject, CNamespacePopObject, CXRefRole, CExprRole\n    from sphinx.application import Sphinx\n    from sphinx.util.docutils import SphinxDirective\n    from docutils.parsers.rst import directives\n    from docutils.parsers.rst.roles import register_canonical_role\n\n    # Verify domain registration\n    app = Sphinx(srcdir='.', confdir='.', outdir='.', doctreedir='.', buildername='html')\n    domain = CDomain(env=app.env)\n    assert domain.name == 'c'\n    assert domain.label == 'C'\n\n    # Verify object types registration\n    assert isinstance(domain.object_types['function'], CObject)\n    assert isinstance(domain.object_types['member'], CObject)\n    assert isinstance(domain.object_types['macro'], CObject)\n    assert isinstance(domain.object_types['type'], CObject)\n    assert isinstance(domain.object_types['var'], CObject)\n\n    # Verify directives registration\n    assert isinstance(directives.directive('c:function'), CFunctionObject)\n    assert isinstance(directives.directive('c:member'), CMemberObject)\n    assert isinstance(directives.directive('c:var'), CMemberObject)\n    assert isinstance(directives.directive('c:macro'), CMacroObject)\n    assert isinstance(directives.directive('c:struct'), CStructObject)\n    assert isinstance(directives.directive('c:union'), CUnionObject)\n    assert isinstance(directives.directive('c:enum'), CEnumObject)\n    assert isinstance(directives.directive('c:enumerator'), CEnumeratorObject)\n    assert isinstance(directives.directive('c:type'), CTypeObject)\n    assert isinstance(directives.directive('c:namespace'), CNamespaceObject)\n    assert isinstance(directives.directive('c:namespace-push'), CNamespacePushObject)\n    assert isinstance(directives.directive('c:namespace-pop'), CNamespacePopObject)\n\n    # Verify roles registration\n    assert isinstance(register_canonical_role('c:func'), CXRefRole)\n    assert isinstance(register_canonical_role('c:member'), CXRefRole)\n    assert isinstance(register_canonical_role('c:data", "def test_c_domain_namespace_directives():\n    class MockEnv:\n            self.domaindata = {'c': {'root_symbol': Symbol(None, None, None, None)}}\n            self.temp_data = {}\n            self.ref_context = {}\n            self.config = {}\n            self.docname = 'test_doc'\n            \n    env = MockEnv()\n    directive = CNamespaceObject('namespace', [':namespace:', 'MyNamespace'], {}, '', 0, 0, None, env)\n    directive.run()\n    assert 'c:parent_symbol' in env.temp_data\n    assert env.temp_data['c:parent_symbol'].ident.identifier == 'MyNamespace'\n\n    directive = CNamespacePushObject('namespace-push', ['MyNamespace::InnerNamespace'], {}, '', 0, 0, None, env)\n    directive.run()\n    assert len(env.temp_data['c:namespace_stack']) == 2\n    assert env.temp_data['c:namespace_stack'][-1].ident.identifier == 'InnerNamespace'\n\n    directive = CNamespacePopObject('namespace-pop', [], {}, '', 0, 0, None, env)\n    directive.run()\n    assert len(env.temp_data['c:namespace_stack']) == 1\n    assert env.temp_data['c:parent_symbol'].ident.identifier == 'MyNamespace'\n", "def test_c_literals():\n    # Test boolean literals\n    assert str(ASTBooleanLiteral(True)) == 'true'\n    assert str(ASTBooleanLiteral(False)) == 'false'\n    \n    # Test number literals\n    assert str(ASTNumberLiteral(\"42\")) == '42'\n    assert str(ASTNumberLiteral(\"3.14\")) == '3.14'\n    \n    # Test character literals\n    assert str(ASTCharLiteral(None, \"a\")) == \"'a'\"\n    assert str(ASTCharLiteral(\"L\", \"a\")) == \"L'a'\"\n    \n    # Test string literals\n    assert str(ASTStringLiteral('\"hello\"')) == '\"hello\"'\n    assert str(ASTStringLiteral('u8\"hello\"')) == 'u8\"hello\"'\n    \n    # Test identifier expressions\n    identifier = ASTIdentifier(\"myVar\")\n    assert str(identifier) == \"myVar\"\n    assert identifier.get_display_string() == \"myVar\"\n    \n    # Test nested names\n    nested_name = ASTNestedName([ASTIdentifier(\"myNamespace\"), ASTIdentifier(\"myVar\")], rooted=False)\n    assert str(nested_name) == \"myNamespace.myVar\"\n", "def test_ast_identifier():\n    ident = ASTIdentifier(\"test\")\n    assert str(ident) == \"test\"\n    assert ident.get_display_string() == \"test\"\n    assert ident.is_anon() is False\n\n    anon_ident = ASTIdentifier(\"@anon\")\n    assert str(anon_ident) == \"@anon\"\n    assert anon_ident.get_display_string() == \"[anonymous]\"\n    assert anon_ident.is_anon() is True\n"], "sample_924": ["def test_namespace_definitions():\n    check('namespace', 'namespace ns', {2: \"ns\"}, output='{key}namespace ns')\n    check('namespace', 'namespace my::nested::namespace', {2: \"N2my6nested9namespaceE\"},\n          output='{key}namespace my::nested::namespace')\n    check('namespace', 'namespace ::global::namespace', {2: \"N6global9namespaceE\"},\n          output='{key}namespace ::global::namespace')\n    check('namespace', 'namespace std::placeholders::_1', {2: \"N3std12placeholders2_1E\"},\n          output='{key}namespace std::placeholders::_1')\n    check('namespace', 'namespace std::literals::string_literals::operator\"\"s',\n          {2: \"N3std8literals15string_literals7li4_sEE\"},\n          output='{key}namespace std::literals::string_literals::operator\"\"s')\n", "def test_template_introductions():\n    check('function', 'template<typename T> requires Concept<T> void foo()', \n          {4: 'I0EIQ1E3fooEv'})\n    check('function', 'template<typename T> requires (sizeof...(T) > 0) void foo()', \n          {4: 'I0EIQplszDpT0_E3fooEv'})\n    check('function', 'template<typename T, typename U> requires (sizeof(T) == sizeof(U)) void foo()', \n          {4: 'I00EIQeqsz1Tsz1U3fooEv'})\n    check('function', 'template<typename T> requires requires (T t) { t.f(); } void foo()',\n          {4: 'I0EIQ1E3fooEv'})\n", "def test_alias_parsing_and_resolution():\n    app = None  # mock environment, extend as needed\n    config = None  # mock configuration, extend as needed\n\n        return AliasNode(sig, env=env, parentKey=parentKey)\n\n    # Test alias parsing for valid signatures\n    valid_signatures = [\n        \"int myAlias = 42\",\n        \"using MyType = std::vector<int>\",\n        \"MyNamespace::MyAlias\",\n        \"template<typename T> using MyTemplateAlias = std::vector<T>\"\n    ]\n    for sig in valid_signatures:\n        node = create_alias_node(sig, env=app)\n        assert node.sig == sig\n        assert isinstance(node, AliasNode)\n\n    # Test alias parsing for invalid signatures\n    invalid_signatures = [\n        \"invalid alias syntax\",\n        \"using = std::vector<int>\",\n        \"template<typename T> using = std::vector<T>\"\n    ]\n    for sig in invalid_signatures:\n        try:\n            create_alias_node(sig, env=app)\n        except DefinitionError:\n            pass  # expected error\n        else:\n            raise AssertionError(\"Expected DefinitionError for invalid signature: \" + sig)\n\n    # Test resolution of alias in the mock environment\n        node = create_alias_node(target, env=app)\n        parser = DefinitionParser(target, location=node, config=config)\n        try:\n            ast, isShorthand = parser.parse_xref_object()\n            parser.assert_end()\n        except DefinitionError as e:\n            assert False, f\"Failed to parse alias target: {e}\"\n\n        assert ast.get_id(version=_max_id).endswith(expected_id)\n\n    test_alias_resolution(\"myAlias\", \"myAlias\")\n    test_alias_resolution(\"MyType\", \"MyType\")\n    test_alias_resolution(\"MyNamespace::MyAlias\", \"MyNamespace::MyAlias\")\n    test_alias_resolution(\"MyTemplateAlias<int>\", \"MyTemplateAliasI1EE\")\n\n", "def test_invalid_definitions():\n    with pytest.raises(DefinitionError):\n        parse('class', 'template template<typename T> class A')  # invalid template template parameter\n    with pytest.raises(DefinitionError):\n        parse('function', 'int f(char) {')  # invalid function definition with body\n    with pytest.raises(DefinitionError):\n        parse('function', 'int f(int, )')  # invalid function parameter list\n    with pytest.raises(DefinitionError):\n        parse('type', 'A = = B')  # invalid type alias definition\n    with pytest.raises(DefinitionError):\n        parse('member', 'int a :')  # invalid bit-field without size\n", "def test_pointer_to_member_functions():\n    check('function', 'int (A::*f)() const', {2: '1fM1AKFivE'})\n    check('function', 'int (A::*f)() volatile', {2: '1fM1AVFivE'})\n    check('function', 'int (A::*f)() const volatile', {2: '1fM1AVKFivE'})\n    check('function', 'int (A::*f)() &', {2: '1fM1ARFivE'})\n    check('function', 'int (A::*f)() &&', {2: '1fM1AOFivE'})\n    check('function', 'int (A::*f)() const &', {2: '1fM1AKRFivE'})\n    check('function', 'int (A::*f)() const &&', {2: '1fM1AKOFivE'})\n    check('function', 'int (A::*f)(int) const', {2: '1fM1AKFiE'})\n    check('function', 'int (A::*f)(int, double) const', {2: '1fM1AKFidE'})\n    check('function', 'void (A::*f)(int) const volatile', {2: '1fM1AVKFiE'})\n    check('function', 'void (A::*f)(int) const &&', {2: '1fM1AKOFiiE'})\n    check('function', 'void (A::*f)(int) const & noexcept', {2: '1fM1AKRFiiE'})\n    check('function', 'void (A::*f)(int) const & noexcept(true)', {2: '1fM1AKRFiiE'})\n    check('function', 'void (A::*f)(int) const & noexcept(noexcept(f(1)))', {2: '1fM1AKRFiiE'})\n"], "sample_925": ["def test_mock_with_multiple_modules():\n    modname1 = 'sphinx.unknown1'\n    modname2 = 'sphinx.unknown2'\n    submodule1 = modname1 + '.submodule'\n    submodule2 = modname2 + '.submodule'\n    assert modname1 not in sys.modules\n    assert modname2 not in sys.modules\n    with pytest.raises(ImportError):\n        import_module(modname1)\n    with pytest.raises(ImportError):\n        import_module(modname2)\n\n    with mock([modname1, modname2]):\n        import_module(modname1)\n        import_module(modname2)\n        assert modname1 in sys.modules\n        assert modname2 in sys.modules\n        assert isinstance(sys.modules[modname1], _MockModule)\n        assert isinstance(sys.modules[modname2], _MockModule)\n\n        # submodules are also mocked\n        import_module(submodule1)\n        import_module(submodule2)\n        assert submodule1 in sys.modules\n        assert submodule2 in sys.modules\n        assert isinstance(sys.modules[submodule1], _MockModule)\n        assert isinstance(sys.modules[submodule2], _MockModule)\n\n    assert modname1 not in sys.modules\n    assert modname2 not in sys.modules\n    with pytest.raises(ImportError):\n        import_module(modname1)\n    with pytest.raises(ImportError):\n        import_module(modname2)\n", "def test_MockFinder():\n    modname = 'sphinx.mocked'\n    finder = MockFinder([modname])\n    \n    # Test find_spec when the module is targeted for mocking\n    spec = finder.find_spec(modname, [])\n    assert spec is not None\n    assert isinstance(spec.loader, MockLoader)\n\n    # Test find_spec when the module is not targeted for mocking\n    spec = finder.find_spec('sphinx.not_mocked', [])\n    assert spec is None\n\n    # Test invalidate_caches\n    finder.mocked_modules.append(modname)\n    assert modname in sys.modules\n    finder.invalidate_caches()\n    assert modname not in sys.modules\n", "def test_mock_object_special_methods():\n    mock = _MockObject()\n\n    # Test __len__\n    assert len(mock) == 0\n\n    # Test __contains__\n    assert 'key' not in mock\n\n    # Test __iter__\n    for item in mock:\n        pytest.fail(\"Iterator should be empty\")\n\n    # Test __getitem__\n    item = mock['key']\n    assert isinstance(item, _MockObject)\n    assert repr(item) == '_MockObject.key'\n\n    # Test __call__\n    result = mock()\n    assert result is mock\n\n        return mock(func)\n    \n    @decorator\n        return \"decorated\"\n\n    assert decorated_function() == \"decorated\"\n\n    # Test __mro_entries__\n    class Base:\n        pass\n\n    class Derived(Base, mock):\n        pass\n\n    assert isinstance(Derived(), Derived)\n", "def test_mock_with_multiple_modules():\n    modnames = ['sphinx.unknown1', 'sphinx.unknown2']\n    submodule1 = modnames[0] + '.submodule'\n    submodule2 = modnames[1] + '.submodule'\n    for modname in modnames:\n        assert modname not in sys.modules\n        with pytest.raises(ImportError):\n            import_module(modname)\n\n    with mock(modnames):\n        for modname in modnames:\n            import_module(modname)\n            assert modname in sys.modules\n            assert isinstance(sys.modules[modname], _MockModule)\n\n        # submodules are also mocked\n        import_module(submodule1)\n        assert submodule1 in sys.modules\n        assert isinstance(sys.modules[submodule1], _MockModule)\n        \n        import_module(submodule2)\n        assert submodule2 in sys.modules\n        assert isinstance(sys.modules[submodule2], _MockModule)\n\n    for modname in modnames:\n        assert modname not in sys.modules\n        with pytest.raises(ImportError):\n            import_module(modname)\n", "def test_mock_with_multiple_modules():\n    modnames = ['sphinx.unknown1', 'sphinx.unknown2', 'sphinx.unknown3.submodule']\n    for modname in modnames:\n        assert modname not in sys.modules\n        with pytest.raises(ImportError):\n            import_module(modname)\n\n    with mock(modnames):\n        for modname in modnames:\n            import_module(modname)\n            assert modname in sys.modules\n            assert isinstance(sys.modules[modname], _MockModule)\n\n    for modname in modnames:\n        assert modname not in sys.modules\n        with pytest.raises(ImportError):\n            import_module(modname)\n"], "sample_926": ["def test_cdomain_simple():\n    from sphinx.domains.c import CDomain\n    from sphinx.application import Sphinx\n    from sphinx.testing.util import SphinxTestApp\n\n    # Create a Sphinx application object\n    app = SphinxTestApp()\n\n    # Initialize the C domain\n    domain = CDomain(app.env)\n\n    # Test some simple cases for adding and retrieving objects\n    domain.objects['test_function'] = ('docname', 'node_id', 'function')\n    domain.objects['test_macro'] = ('docname', 'node_id', 'macro')\n    domain.objects['test_type'] = ('docname', 'node_id', 'type')\n\n    # Check that the objects are correctly stored\n    assert domain.objects['test_function'] == ('docname', 'node_id', 'function')\n    assert domain.objects['test_macro'] == ('docname', 'node_id', 'macro')\n    assert domain.objects['test_type'] == ('docname', 'node_id', 'type')\n\n    # Test the resolve_xref method\n    from docutils.nodes import TextElement, pending_xref\n    from sphinx.builders.html import StandaloneHTMLBuilder\n    from sphinx.util.nodes import make_refnode\n\n    refnode = pending_xref('', refdomain='c', reftype='func', refexplicit=False, reftarget='test_function')\n    contnode = TextElement()\n\n    result_node = domain.resolve_xref(app.env, 'docname', StandaloneHTMLBuilder, 'function', 'test_function', refnode, contnode)\n    assert isinstance(result_node, make_refnode)\n\n    refnode['reftype'] = 'macro'\n    result_node = domain.resolve_xref(app.env, 'docname', StandaloneHTMLBuilder, 'macro', 'test_macro', refnode, contnode)\n    assert isinstance(result_node, make_refnode)\n\n    refnode['reftype'] = 'type'\n    result_node = domain.resolve_xref(app.env, 'docname', StandaloneHTMLBuilder, 'type', 'test_type', refnode, contnode)\n    assert isinstance(result_node, make_refnode)\n\n    # Close the Sphinx application\n    app.cleanup()\n", "def test_c_namespace_object():\n    text = \"\"\"\n    .. c:namespace:: MyNamespace\n\n    .. c:function:: void my_function()\n\n    \"\"\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, \n                [addnodes.desc, addnodes.desc_signature, \n                 addnodes.desc_content])\n    \n    # Ensure the namespace directive is correctly handled\n    assert 'MyNamespace' in app.env.temp_data['c:namespace_stack'][-1].ident.get_display_string()\n    assert_node(doctree[0],\n                addnodes.desc_signature, \n                ids=['c.MyNamespace.my_function'])\n", "def test_c_symbols():\n    # Test adding declarations and lookup of symbols\n    rootSymbol = Symbol(None, None, None, None)\n    assert rootSymbol is not None\n    \n    # Add a simple function declaration\n    decl1 = parse('function', 'void my_function(int a)')\n    symbol1 = rootSymbol.add_declaration(decl1, 'TestDoc1')\n    assert symbol1 is not None\n    assert symbol1.declaration == decl1\n\n    # Add a variable declaration\n    decl2 = parse('member', 'int my_var')\n    symbol2 = rootSymbol.add_declaration(decl2, 'TestDoc2')\n    assert symbol2 is not None\n    assert symbol2.declaration == decl2\n\n    # Check if we can find the added symbols\n    foundSymbol1 = rootSymbol.find_declaration(decl1.name, 'function', matchSelf=True, recurseInAnon=False)\n    assert foundSymbol1 == symbol1\n\n    foundSymbol2 = rootSymbol.find_declaration(decl2.name, 'member', matchSelf=True, recurseInAnon=False)\n    assert foundSymbol2 == symbol2\n\n    # Check if we can merge symbols\n    otherRootSymbol = Symbol(None, None, None, None)\n    otherDecl = parse('function', 'void other_function(double b)')\n    otherSymbol = otherRootSymbol.add_declaration(otherDecl, 'TestDoc3')\n    assert otherSymbol is not None\n    assert otherSymbol.declaration == otherDecl\n\n    rootSymbol.merge_with(otherRootSymbol, ['TestDoc3'], None)\n    foundOtherSymbol = rootSymbol.find_declaration(otherDecl.name, 'function', matchSelf=True, recurseInAnon=False)\n    assert foundOtherSymbol == otherSymbol\n\n    # Test symbol removal\n    symbol2.remove()\n    assert symbol2.parent is None\n    assert symbol2 not in rootSymbol._children\n\n    # Clear document and check if symbol declarations are removed\n    rootSymbol.clear_doc('TestDoc1')\n    assert symbol1.declaration is None\n    assert symbol1.docname is None\n", "def test_symbol_lookup():\n    # Setup symbols\n    root = Symbol(None, None, None, None)\n    nested_name_1 = ASTNestedName([ASTIdentifier('Test1')], rooted=False)\n    nested_name_2 = ASTNestedName([ASTIdentifier('Test2')], rooted=False)\n    declaration_1 = ASTDeclaration('type', 'type', ASTType(ASTDeclSpecs('type', None, None, None), ASTDeclaratorNameParam(nested_name_1, [], None)), False)\n    declaration_2 = ASTDeclaration('type', 'type', ASTType(ASTDeclSpecs('type', None, None, None), ASTDeclaratorNameParam(nested_name_2, [], None)), False)\n    symbol_1 = root.add_declaration(declaration_1, \"TestDoc1\")\n    symbol_2 = root.add_declaration(declaration_2, \"TestDoc2\")\n\n    # Perform lookups\n    result_1 = root.find_identifier(ASTIdentifier('Test1'), matchSelf=True, recurseInAnon=False, searchInSiblings=False)\n    result_2 = root.find_identifier(ASTIdentifier('Test2'), matchSelf=True, recurseInAnon=False, searchInSiblings=False)\n    \n    assert result_1 is symbol_1\n    assert result_2 is symbol_2\n\n    # Test lookup with nested names\n    nested_name_3 = ASTNestedName([ASTIdentifier('Test1'), ASTIdentifier('Inner')], rooted=False)\n    declaration_3 = ASTDeclaration('type', 'type', ASTType(ASTDeclSpecs('type', None, None, None), ASTDeclaratorNameParam(nested_name_3, [], None)), False)\n    symbol_3 = symbol_1.add_declaration(declaration_3, \"TestDoc3\")\n\n    result_3 = root.find_identifier(ASTIdentifier('Inner'), matchSelf=True, recurseInAnon=True, searchInSiblings=False)\n    assert result_3 is symbol_3\n", "def test_c_domain_expressions():\n    # Test for ASTBooleanLiteral\n    expr = 'true'\n    expected_id = 'L1E'\n    check('class', f'template<> {key}C<a[{expr}]>', {2: f'IE1CIA{expected_id}_1aE'})\n\n    expr = 'false'\n    expected_id = 'L0E'\n    check('class', f'template<> {key}C<a[{expr}]>', {2: f'IE1CIA{expected_id}_1aE'})\n\n    # Test for ASTNumberLiteral\n    expr = '42'\n    expected_id = 'L42E'\n    check('class', f'template<> {key}C<a[{expr}]>', {2: f'IE1CIA{expected_id}_1aE'})\n\n    # Test for ASTCharLiteral\n    expr = \"'a'\"\n    expected_id = 'c97E'\n    check('class', f'template<> {key}C<a[{expr}]>', {2: f'IE1CIA{expected_id}_1aE'})\n\n    # Test for ASTStringLiteral\n    expr = '\"hello\"'\n    expected_id = 'LA5_KcE'\n    check('class', f'template<> {key}C<a[{expr}]>', {2: f'IE1CIA{expected_id}_1aE'})\n\n    # Test for ASTIdExpression\n    expr = 'var'\n    expected_id = '1var'\n    check('class', f'template<> {key}C<a[{expr}]>', {2: f'IE1CIA{expected_id}_1aE'})\n\n    # Test for ASTParenExpr\n    expr = '(42)'\n    expected_id = 'L42E'\n    check('class', f'template<> {key}C<a[{expr}]>', {2: f'IE1CIA{expected_id}_1aE'})\n\n    # Test for ASTPostfixExpr with call expression\n    expr = 'func(42)'\n    expected_id = 'cl1funcL42EE'\n    check('class', f'template<> {key}C<a[{expr}]>', {2: f'IE1CIA{expected_id}_1aE'})\n\n    # Test for ASTUnaryOpExpr\n    expr = '-42'\n    expected_id = 'ngL42E'\n    check('class', f'template<> {"], "sample_927": ["def test_ast_identifier():\n    identifier = ASTIdentifier(\"test\")\n    assert identifier.identifier == \"test\"\n    assert identifier.is_anon() is False\n    assert identifier.get_id(1) == \"test\"\n    assert identifier.get_id(2) == \"4test\"\n    assert str(identifier) == \"test\"\n    assert identifier.get_display_string() == \"test\"\n\n    anon_identifier = ASTIdentifier(\"@anon\")\n    assert anon_identifier.is_anon() is True\n    assert anon_identifier.get_display_string() == \"[anonymous]\"\n    with pytest.raises(NoOldIdError):\n        anon_identifier.get_id(1)\n\n    signode = addnodes.desc_signature()\n    env = BuildEnvironment()\n    prefix = \"\"\n    templateArgs = \"\"\n    symbol = Symbol(None, identifier, None, None, None, \"testdoc\")\n    identifier.describe_signature(signode, 'markType', env, prefix, templateArgs, symbol)\n    assert_node(signode, addnodes.desc_signature, [addnodes.pending_xref, nodes.Text])\n    assert_node(signode[0], addnodes.pending_xref, refdomain='cpp', reftype='identifier', reftarget='test', \n                [nodes.Text])\n", "def test_alias_directive():\n    class MockBuildEnvironment:\n        temp_data = {}\n        ref_context = {}\n        domains = {'cpp': CPPDomain(MockBuildEnvironment())}\n        config = type('MockConfig', (object,), {'cpp_index_common_prefix': [],\n                                                'cpp_id_attributes': [],\n                                                'cpp_paren_attributes': []})\n\n    alias_node = AliasNode(\"int a\")\n    alias_node_copy = alias_node.copy()\n    assert alias_node_copy.sig == alias_node.sig\n    assert alias_node_copy.parentKey == alias_node.parentKey\n\n    alias_node = AliasNode(\"template<int T> class A\")\n    alias_node_copy = alias_node.copy()\n    assert alias_node_copy.sig == alias_node.sig\n    assert alias_node_copy.parentKey == alias_node.parentKey\n\n    transform = AliasTransform(MockBuildEnvironment())\n    transform.document = nodes.document('', '')\n    transform.document += alias_node\n    transform.apply()\n    # Since the AliasNode parsing was successful, it should have replaced the node\n    assert isinstance(transform.document[0], addnodes.desc_signature)\n", "def test_pointer_to_member_functions():\n    check('function', 'void (A::*memFnPtr)(int)', {2: 'M1AFviE'})\n    check('function', 'void (A::*memFnPtr)(int, double)', {2: 'M1AFvidE'})\n    check('function', 'void (A::*memFnPtr)() const', {2: 'KRM1AFvvE'})\n    check('function', 'void (A::*memFnPtr)(int, double) const', {2: 'KRM1AFvidE'})\n    check('function', 'void (A::*memFnPtr)(int, double) volatile', {2: 'VKM1AFvidE'})\n    check('function', 'void (A::*memFnPtr)(int, double) const volatile', {2: 'VRKM1AFvidE'})\n    check('function', 'void (A::*memFnPtr)(int, double) &', {2: 'RM1AFvidE'})\n    check('function', 'void (A::*memFnPtr)(int, double) &&', {2: 'OM1AFvidE'})\n", "def test_namespace_definitions():\n    check('namespace', 'namespace A', {2: '1A'})\n    check('namespace', 'namespace A::B', {2: 'N1A1BE'})\n    check('namespace', 'namespace A::B::C', {2: 'N1A1B1CE'})\n    check('namespace', 'namespace ::A', {2: '1A'})\n    check('namespace', 'namespace ::A::B', {2: 'N1A1BE'})\n    check('namespace', 'template<> namespace A<int>', {2: 'IE1AIiEE'}, output='template<> namespace A<int>')\n", "def test_alias_handling():\n    class MockEnv:\n        temp_data = {}\n        config = None\n        docname = \"test_doc\"\n\n    env = MockEnv()\n    parentKey = env.temp_data.get('cpp:parent_key', None)\n    alias_node = AliasNode(\"std::vector<int>\", env, parentKey)\n    \n    assert alias_node.sig == \"std::vector<int>\"\n    assert alias_node.parentKey == env.temp_data['cpp:parent_symbol'].get_lookup_key()\n    \n    alias_node_copy = alias_node.copy()\n    assert alias_node_copy.sig == alias_node.sig\n    assert alias_node_copy.parentKey == alias_node.parentKey\n    \n    transform = AliasTransform(MockEnv())\n    transform.apply()\n\n    # Ensure node is transformed correctly\n    assert isinstance(alias_node, AliasNode)\n"], "sample_928": ["def test_default_role():\n    from docutils.parsers.rst import roles\n    from sphinx.util.docutils import is_registered_role\n\n    docname = 'dummy_doc'\n    role_name = 'emphasis'\n    \n    # Ensure the role is not registered initially\n    assert not is_registered_role('')\n\n    with default_role(docname, role_name):\n        # Ensure the role is registered within the context\n        assert is_registered_role('')\n\n    # Ensure the role is unregistered after the context\n    assert not is_registered_role('')\n", "def test_default_role(app):\n    from docutils.parsers.rst import roles\n    \n    with default_role('dummy_doc', 'math'):\n        assert roles._roles[''].__name__ == 'math_role'\n    \n    assert '' not in roles._roles  # Ensure the role is unregistered after the context manager\n", "def test_default_role():\n    from sphinx.util.rst import default_role\n    from docutils.parsers.rst import roles\n\n    # Test case where role is found\n    with default_role('dummy_docname', 'title-reference'):\n        assert roles._roles[''].__name__ == 'generic_role'\n    \n    # Test case where role is not found\n    with default_role('dummy_docname', 'nonexistent-role'):\n        assert '' not in roles._roles  # Ensure no role was registered\n", "def test_default_role():\n    from sphinx.util.rst import default_role\n    from docutils.parsers.rst import roles\n    from sphinx.util import docutils\n\n    # Backup existing roles\n    original_role = roles._roles.get('')\n    \n    try:\n        # Register a new dummy role for testing\n        roles.register_local_role('dummy', lambda name, rawtext, text, lineno, inliner, options={}, content=[]: ([], []))\n        docname = 'test_doc'\n        \n        # Test default_role when the role exists\n        with default_role(docname, 'dummy'):\n            assert roles._roles.get('') is not None\n        \n        # Test default_role when the role does not exist\n        with default_role(docname, 'nonexistent'):\n            assert roles._roles.get('') is None\n    \n    finally:\n        # Restore the original role\n        if original_role:\n            roles.register_local_role('', original_role)\n        else:\n            roles.unregister_role('')\n", "def test_default_role():\n    from sphinx.util.docutils import roles as sphinx_roles\n\n    docname = 'dummy_doc'\n    role_name = 'emphasis'\n    \n    with default_role(docname, role_name):\n        assert '' in sphinx_roles._roles\n        assert sphinx_roles._roles[''] == roles.role(role_name, english, 0, Reporter('', 4, 4))[0]\n    \n    assert '' not in sphinx_roles._roles\n"], "sample_929": ["def test_parse_annotation_with_env_is_none():\n    with pytest.warns(RemovedInSphinx50Warning, match=\"The env parameter for _parse_annotation becomes required now.\"):\n        doctree = _parse_annotation(\"int\", None)\n    assert_node(doctree, ([pending_xref, \"int\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    \n    with pytest.warns(RemovedInSphinx50Warning, match=\"The env parameter for _parse_annotation becomes required now.\"):\n        doctree = _parse_annotation(\"List[int]\", None)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_pyfunction_async(app):\n    text = (\".. py:function:: async_func\\n\"\n            \"   :async:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"async \"],\n                                                    [desc_name, \"async_func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert 'async_func' in domain.objects\n    assert domain.objects['async_func'] == ('index', 'async_func', 'function')\n", "def test_pymodule():\n    text = (\".. py:module:: testmodule\\n\"\n            \"   :synopsis: This is a test module\\n\"\n            \"   :platform: any\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    \n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          [addnodes.index, [desc, ([desc_signature, ([desc_name, \"testmodule\"])],\n                                                desc_content)]]))\n\n    assert 'testmodule' in domain.modules\n    module_entry = domain.modules['testmodule']\n    assert module_entry.docname == 'index'\n    assert module_entry.synopsis == \"This is a test module\"\n    assert module_entry.platform == \"any\"\n    assert module_entry.deprecated == True\n    \n    assert 'testmodule' in domain.objects\n    assert domain.objects['testmodule'] == ('index', 'testmodule', 'module')\n", "def test_parse_signature_with_prefix():\n    \"\"\"Test parsing signatures with various prefixes\"\"\"\n    domain = PythonDomain(Mock())\n    \n    # Testing PyFunction with async prefix\n    text = \".. py:function:: async func(a: int) -> None\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"async \"],\n                                                    [desc_name, \"func\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"None\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    # Testing PyClasslike with final prefix\n    text = \".. py:class:: final class MyClass\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"final class \"],\n                                                    [desc_name, \"MyClass\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"class\",\n                domain=\"py\", objtype=\"class\", noindex=False)\n\n    # Testing PyMethod with property prefix\n    text = \".. py:method:: property meth\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"property \"],\n                                                    [desc_name, \"meth\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"method\",\n                domain=\"py\", objtype=\"method\", noindex=False)\n", "def test_parse_arglist():\n    # Test case where arglist is empty\n    arglist = \"\"\n    parsed = _parse_arglist(arglist)\n    assert_node(parsed, addnodes.desc_parameterlist, [])\n\n    # Test case with only positional arguments\n    arglist = \"a, b, c\"\n    parsed = _parse_arglist(arglist)\n    assert_node(parsed, addnodes.desc_parameterlist,\n                ([addnodes.desc_parameter, ([addnodes.desc_sig_name, \"a\"])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_name, \"b\"])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_name, \"c\"])]))\n\n    # Test case with keyword arguments\n    arglist = \"a=1, b=None\"\n    parsed = _parse_arglist(arglist)\n    assert_node(parsed, addnodes.desc_parameterlist,\n                ([addnodes.desc_parameter, ([addnodes.desc_sig_name, \"a\"],\n                                            [addnodes.desc_sig_operator, \"=\"],\n                                            [nodes.inline, \"1\"])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_name, \"b\"],\n                                            [addnodes.desc_sig_operator, \"=\"],\n                                            [nodes.inline, \"None\"])]))\n\n    # Test case with mixed positional and keyword arguments\n    arglist = \"a, b=2, *args, c=3, **kwargs\"\n    parsed = _parse_arglist(arglist)\n    assert_node(parsed, addnodes.desc_parameterlist,\n                ([addnodes.desc_parameter, ([addnodes.desc_sig_name, \"a\"])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_name, \"b\"],\n                                            [addnodes.desc_sig_operator, \"=\"],\n                                            [nodes.inline, \"2\"])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_operator, \"*\"],\n                                            [addnodes.desc_sig_name, \"args\"])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_name, \"c\"],\n                                            [addnodes.desc_sig_operator, \"=\"],\n                                            [nodes.inline, \"3\"])],\n                 [addnodes.desc_parameter, ([addnodes.desc_sig_operator, \"**\"],\n                                            [addnodes.desc_sig_name, \"kwargs\"])]))\n\n    # Test case with type annotations\n    arglist = \"a: int, b: str = 'default'\"\n    parsed = _parse_arglist(arglist)\n    assert_node(parsed, addnodes.desc_parameterlist,\n                ([addnodes.desc_parameter, ([addnodes.desc"], "sample_930": ["def test_create_mixed_index(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: triple: foo; bar; baz\\n\"\n            \".. index:: see: Sphinx; documentation tool\\n\"\n            \".. index:: seealso: reST; reStructuredText\\n\"\n            \".. index:: !main_entry\\n\"\n            \".. index:: sub_entry; sub\\n\"\n            \".. index:: mixed; see also Python\\n\"\n            \".. index:: mixed; see reST\\n\"\n            \".. index:: mixed; foo\\n\"\n            \".. index:: foo; bar\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 6\n    assert index[0] == ('B', [('bar', [[], [('baz, foo', [('', '#index-2')]),\n                                             ('foo', [('', '#index-10')])], None])])\n    assert index[1] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[2] == ('F', [('foo', [[], [('bar baz', [('', '#index-2')]),\n                                            ('bar', [('', '#index-10')]),\n                                            ('see also Python', []),\n                                            ('see reST', [])], None])])\n    assert index[3] == ('I', [('interpreter', [[], [('Python', [('', '#index-1')])], None])])\n    assert index[4] == ('M', [('main_entry', [[('main', '#index-5')], [], None]),\n                              ('mixed', [[], [('foo', [('', '#index-9')]),\n                                              ('see also Python', [('', '#index-8')]),\n                                              ('see reST', [('', '#index-7')])], None])])\n    assert index[5] == ('R', [('reST', [[], [('see also reStructuredText', [])], None])])\n", "def test_create_index_with_subitems(app):\n    text = (\".. index:: single: foo (module bar)\\n\"\n            \".. index:: single: foo (module baz)\\n\"\n            \".. index:: single: foo (module qux)\\n\"\n            \".. index:: single: bar (module foo)\\n\"\n            \".. index:: single: bar (module baz)\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 2\n    assert index[0] == ('B', [('bar', [[], [('(in module foo)', [('', '#index-3')]),\n                                            ('(in module baz)', [('', '#index-4')])], None])])\n    assert index[1] == ('F', [('foo', [[], [('(in module bar)', [('', '#index-0')]),\n                                            ('(in module baz)', [('', '#index-1')]),\n                                            ('(in module qux)', [('', '#index-2')])], None])])\n", "def test_create_index_with_invalid_entry(app):\n    text = (\".. index:: invalid: entry\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    \n    # The invalid entry type should trigger a warning and be ignored\n    assert len(index) == 0\n", "def test_create_mixed_entry_types_index(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: triple: Sphinx; documentation; tool\\n\"\n            \".. index:: see: HTML; HyperText Markup Language\\n\"\n            \".. index:: seealso: XML; Extensible Markup Language\\n\"\n            \".. index:: !mainentry\\n\"\n            \".. index:: mainentry\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 4\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('H', [('HTML', [[], [('see HyperText Markup Language', [])], None])])\n    assert index[2] == ('M', [('mainentry', [[('main', '#index-5'),\n                                             ('', '#index-6')], [], None])])\n    assert index[3] == ('P', [('Python', [[], [('interpreter', [('', '#index-1')])], None]),\n                              ('Sphinx', [[], [('documentation tool', [('', '#index-2')])], None])])\n    assert index[4] == ('X', [('XML', [[], [('see also Extensible Markup Language', [])], None])])\n", "def test_create_index_with_special_characters(app):\n    text = (\".. index:: single: docutils!\\n\"\n            \".. index:: single: Pyth@n\\n\"\n            \".. index:: single: #Sphinx\\n\"\n            \".. index:: single: !@#$%^&*()\\n\"\n            \".. index:: single: 12345\\n\"\n            \".. index:: single: \u3053\u3093\u306b\u3061\u306f\\n\"\n            \".. index:: single: \u4f60\u597d\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 6\n    assert index[0] == ('Symbols', [('!@#$%^&*()', [[('', '#index-3')], [], None]),\n                                    ('#Sphinx', [[('', '#index-2')], [], None])])\n    assert index[1] == ('1', [('12345', [[('', '#index-4')], [], None])])\n    assert index[2] == ('D', [('docutils!', [[('', '#index-0')], [], None])])\n    assert index[3] == ('P', [('Pyth@n', [[('', '#index-1')], [], None])])\n    assert index[4] == ('\u3053', [('\u3053\u3093\u306b\u3061\u306f', [[('', '#index-5')], [], None])])\n    assert index[5] == ('\u4f60', [('\u4f60\u597d', [[('', '#index-6')], [], None])])\n"], "sample_931": ["def test_type_to_xref():\n    env = Mock(ref_context={'py:module': 'module', 'py:class': 'Class'})\n\n    # Test with 'None' type\n    node = type_to_xref('None', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n    assert_node(node[0], nodes.Text, 'None')\n\n    # Test with 'str' type\n    node = type_to_xref('str', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='str')\n    assert_node(node[0], nodes.Text, 'str')\n\n    # Test without environment\n    node = type_to_xref('int')\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='int')\n    assert_node(node[0], nodes.Text, 'int')\n", "def test_pymodule_noindex(app):\n    text = \".. py:module:: example\\n   :noindex:\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, [])\n    assert 'example' not in domain.objects\n    assert 'example' not in domain.modules\n", "def test_handle_signature():\n    domain = PythonDomain(Mock())\n    signode = desc_signature()\n\n    # Testing a method signature handling\n    full, prefix = domain.directives['method']().handle_signature('MyClass.my_method(a, b=1)', signode)\n    assert full == 'MyClass.my_method'\n    assert prefix == 'MyClass.'\n    assert_node(signode, ([desc_addname, \"MyClass.\"],\n                          [desc_name, \"my_method\"],\n                          [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"a\"])],\n                                                [desc_parameter, ([desc_sig_name, \"b\"],\n                                                                  [desc_sig_operator, \"=\"],\n                                                                  [nodes.inline, \"1\"])])]))\n\n    # Testing a function signature handling with return annotation\n    signode = desc_signature()\n    full, prefix = domain.directives['function']().handle_signature('my_function(x: int) -> str', signode)\n    assert full == 'my_function'\n    assert prefix == ''\n    assert_node(signode, ([desc_name, \"my_function\"],\n                          [desc_parameterlist, [desc_parameter, ([desc_sig_name, \"x\"],\n                                                                 [desc_sig_punctuation, \":\"],\n                                                                 \" \",\n                                                                 [desc_sig_name, pending_xref, \"int\"])]],\n                          [desc_returns, ([pending_xref, \"str\"])],\n                          ))\n\n    # Testing class signature with module context\n    signode = desc_signature()\n    env = Mock(ref_context={'py:module': 'my_module'})\n    domain = PythonDomain(env)\n    full, prefix = domain.directives['class']().handle_signature('MyClass', signode)\n    assert full == 'my_module.MyClass'\n    assert prefix == ''\n    assert_node(signode, ([desc_name, \"MyClass\"]))\n\n    # Testing a decorator signature handling\n    signode = desc_signature()\n    full, prefix = domain.directives['decorator']().handle_signature('my_decorator', signode)\n    assert full == 'my_decorator'\n    assert prefix == ''\n    assert_node(signode, ([desc_addname, \"@\"],\n                          [desc_name, \"my_decorator\"]))\n\n    # Testing a variable signature handling with type and value\n    signode = desc_signature()\n    full, prefix = domain.directives['data']().handle_signature('my_var', signode)\n    domain.directives['data']().options = {'type': 'int', 'value': '42'}\n", "def test_type_to_xref():\n    from sphinx.environment import BuildEnvironment\n\n    env = Mock(BuildEnvironment)\n\n    # Test with `None`\n    xref_node = type_to_xref('None', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n\n    # Test with a standard type\n    xref_node = type_to_xref('int', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='class', reftarget='int')\n\n    # Test without env\n    xref_node = type_to_xref('str')\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='class', reftarget='str')\n", "def test_pymodule_options(app):\n    text = (\".. py:module:: testmodule\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :synopsis: This is a test module.\\n\"\n            \"   :noindex:\\n\"\n            \".. py:module:: testmodule.indexed\\n\"\n            \"   :platform: Windows\\n\"\n            \"   :synopsis: This is an indexed test module.\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, ([nodes.target, addnodes.index], \n                          [nodes.target, addnodes.index]))\n    \n    # Check the first module with noindex flag\n    assert 'testmodule' not in domain.objects\n    assert 'testmodule' not in domain.modules\n\n    # Check the second module\n    assert 'testmodule.indexed' in domain.objects\n    assert 'testmodule.indexed' in domain.modules\n    module_entry = domain.modules['testmodule.indexed']\n    assert module_entry.synopsis == \"This is an indexed test module.\"\n    assert module_entry.platform == \"Windows\"\n    assert module_entry.deprecated is True\n"], "sample_932": ["def test_parsing_template_classes():\n    check('class', 'template<int I> {key}MyClass<int, I>', {2: 'I_iE7MyClassIiiEE'})\n    check('class', 'template<typename T> {key}Outer<T>::Inner', {2: 'I0EN5OuterI1TEE5InnerE'})\n    check('class', 'template<template<typename> class T> {key}Container<T>', {2: 'II0E0E9ContainerI1TE'})\n    check('class', 'template<typename... Ts> {key}VariadicClass<Ts...>', {2: 'IDpE14VariadicClassIDpTsE'})\n    check('class', 'template<typename T, int N> {key}ArrayWrapper<T, N>', {2: 'I0_iE12ArrayWrapperI1TiE'})\n    check('class', 'template<typename T, template<typename> class U> {key}ComplexTemplate', {2: 'I0I0E13ComplexTemplateI1TI1UE'})\n    check('class', 'template<typename T> {key}MyClass<T>::Nested<T>', {2: 'I0EN7MyClassI1TEE6NestedI1TE'})\n    check('class', 'template<int I, int J> {key}DualParamClass<I, J>', {2: 'IiiE16DualParamClassIiiEE'})\n    check('class', 'template<typename T, typename U> {key}Pair<T, U>', {2: 'I00E4PairI1T1UE'})\n    check('class', 'template<typename T = int, typename U = double> {key}DefaultParams', {2: 'I0N1T1UE13DefaultParams'})\n", "def test_alias_handling():\n    # Test basic alias functionality\n    text = (\".. cpp:alias:: int_alias\\n\"\n            \"   :cpp:any:`int`\\n\"\n            \".. cpp:alias:: func_alias\\n\"\n            \"   :cpp:any:`void f(int)`\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (AliasNode, AliasNode))\n    \n    # Check that alias nodes are properly parsed\n    alias1, alias2 = doctree\n    assert isinstance(alias1, AliasNode)\n    assert alias1.sig == 'int_alias'\n    assert isinstance(alias2, AliasNode)\n    assert alias2.sig == 'func_alias'\n\n    # Check generated IDs for alias nodes\n    alias1_id = alias1.parentKey.data[0][0].identOrOp.get_id(2)\n    alias2_id = alias2.parentKey.data[0][0].identOrOp.get_id(2)\n    assert alias1_id == 'int'\n    assert alias2_id == '1f1iE'\n\n    # Test that alias nodes can be resolved correctly\n    env = app.env\n    builder = app.builder\n    fromdocname = 'index'\n    \n    title, target = 'int_alias', 'int'\n    node = pending_xref(refdomain='cpp', reftype='any', refexplicit=True, reftarget=target)\n    contnode = nodes.Text(title)\n    result = env.domains['cpp'].resolve_xref(env, fromdocname, builder, 'any', target, node, contnode)\n    assert result is not None\n    assert_node(result, nodes.reference, ids=['int'], refuri='#int')\n\n    title, target = 'func_alias', 'f(int)'\n    node = pending_xref(refdomain='cpp', reftype='any', refexplicit=True, reftarget=target)\n    contnode = nodes.Text(title)\n    result = env.domains['cpp'].resolve_xref(env, fromdocname, builder, 'any', target, node, contnode)\n    assert result is not None\n    assert_node(result, nodes.reference, ids=['_CPPv41f1iE'], refuri='#_CPPv41f1iE')\n", "def test_cpp_directives():\n    directives = [\n        ('class', 'class A', '1A'),\n        ('struct', 'struct A', '1A'),\n        ('union', 'union A', '1A'),\n        ('function', 'void f()', '1fv'),\n        ('member', 'int a', '1a'),\n        ('var', 'int a', '1a'),\n        ('type', 'using A = int', '1A'),\n        ('concept', 'template<typename T> concept C = true', 'I0E1C'),\n        ('enum', 'enum A', '1A'),\n        ('enum-struct', 'enum struct A', '1A'),\n        ('enum-class', 'enum class A', '1A'),\n        ('enumerator', 'A = 1', '1A'),\n    ]\n\n    for directive, declaration, expected_id in directives:\n        ast = parse(directive, declaration)\n        assert ast.get_id(version=2) == expected_id\n", "def test_operators_references():\n    check('function', 'void operator^()', {1: \"xor-operator\", 2: \"eov\"})\n    check('function', 'void operator^=()', {1: \"xor-assign-operator\", 2: \"eOv\"})\n    check('function', 'void operator<<()', {1: \"lshift-operator\", 2: \"lsv\"})\n    check('function', 'void operator>>()', {1: \"rshift-operator\", 2: \"rsv\"})\n    check('function', 'void operator<<=()', {1: \"lshift-assign-operator\", 2: \"lSv\"})\n    check('function', 'void operator>>=()', {1: \"rshift-assign-operator\", 2: \"rSv\"})\n    check('function', 'void operator==()', {1: \"eq-operator\", 2: \"eqv\"})\n    check('function', 'void operator!=()', {1: \"neq-operator\", 2: \"nev\"})\n    check('function', 'void operator<=()', {1: \"lte-operator\", 2: \"lev\"})\n    check('function', 'void operator>=()', {1: \"gte-operator\", 2: \"gev\"})\n    check('function', 'void operator!()', {1: \"not-operator\", 2: \"ntv\"})\n    check('function', 'void operator&&()', {1: \"sand-operator\", 2: \"aav\"})\n    check('function', 'void operator||()', {1: \"sor-operator\", 2: \"oov\"})\n", "def test_nested_template_args():\n    check('class', 'template<typename T> class A::B<T>::C', {2: 'I0EN1A1BI1T1CE'})\n    check('class', 'template<typename T, typename U> class A::B<T>::C<U>', {2: 'I00EN1A1BI1TEI1UE1CE'})\n    check('function', 'template<typename T> void f(A::B<T>::C &c)', {2: 'I0E1fRN1A1BI1TE1CE'})\n    check('type', 'template<typename T> using D = A::B<T>::C', {2: 'I0E1D'}, key='using')\n"], "sample_933": ["def test_gettext_with_uuids(app):\n    # Test gettext builder with UUIDs enabled\n    app.builder.build_all()\n\n    assert (app.outdir / 'sphinx.pot').isfile()\n\n    result = (app.outdir / 'sphinx.pot').read_text()\n    assert \"msgid \\\"Welcome\\\"\" in result\n    assert \"msgid \\\"Sphinx %(version)s\\\"\" in result\n\n    # Ensure that UUIDs are present in the output\n    uuid_pattern = re.compile(r'#: .* \\d+ [0-9a-fA-F]{32}')\n    assert uuid_pattern.search(result) is not None\n\n    # Ensure the catalog contains UUIDs correctly\n    catalog = app.builder.catalogs['sphinx']\n    for message in catalog:\n        assert all(len(uuid) == 32 for uuid in message.uuids)\n", "def test_message_catalog_builder_should_write_new_file(app, tmpdir):\n    content = \"\"\"msgid \"\"", "def test_should_write(app, tmpdir):\n    # Create a temporary file with old content\n    old_content = (\n        'msgid \"\"\\nmsgstr \"\"\\n'\n        '\"Project-Id-Version: Test 1.0\\\\n\"\\n'\n        '\"POT-Creation-Date: 2021-01-01 00:00+0000\\\\n\"\\n'\n        '\"PO-Revision-Date: 2021-01-01 00:00+0000\\\\n\"\\n'\n        '\"Last-Translator: FULL NAME <EMAIL@ADDRESS>\\\\n\"\\n'\n        '\"Language-Team: LANGUAGE <LL@li.org>\\\\n\"\\n'\n    )\n    old_file = tmpdir.join('old.pot')\n    old_file.write(old_content)\n    \n    # New content with different POT-Creation-Date but same body\n    new_content = (\n        'msgid \"\"\\nmsgstr \"\"\\n'\n        '\"Project-Id-Version: Test 1.0\\\\n\"\\n'\n        '\"POT-Creation-Date: 2022-01-01 00:00+0000\\\\n\"\\n'\n        '\"PO-Revision-Date: 2021-01-01 00:00+0000\\\\n\"\\n'\n        '\"Last-Translator: FULL NAME <EMAIL@ADDRESS>\\\\n\"\\n'\n        '\"Language-Team: LANGUAGE <LL@li.org>\\\\n\"\\n'\n    )\n    \n    # Check should_write function\n    assert should_write(str(old_file), new_content) == True\n\n    # Modify new content to have the same POT-Creation-Date\n    new_content = (\n        'msgid \"\"\\nmsgstr \"\"\\n'\n        '\"Project-Id-Version: Test 1.0\\\\n\"\\n'\n        '\"POT-Creation-Date: 2021-01-01 00:00+0000\\\\n\"\\n'\n        '\"PO-Revision-Date: 2021-01-01 00:00+0000\\\\n\"\\n'\n        '\"Last-Translator: FULL NAME <EMAIL@ADDRESS>\\\\n\"\\n'\n        '\"Language-Team: LANGUAGE <LL@li.org>\\\\n\"\\n'\n    )\n    \n    # Check should_write function again\n    assert should_write(str(old_file), new_content) == False\n\n    # Modify", "def test_should_write_function(app):\n    app.builder.build_all()\n    \n    # Create a mock content for new content\n    new_content = (\n        'msgid \"\"\\n'\n        'msgstr \"\"\\n'\n        '\"POT-Creation-Date: 2023-10-10 00:00+0000\\\\n\"\\n'\n        '\"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\\\n\"\\n'\n        '\"Content-Type: text/plain; charset=UTF-8\\\\n\"\\n'\n    )\n\n    # Test should_write when file does not exist\n    filepath = str(app.outdir / 'test_should_write.pot')\n    assert should_write(filepath, new_content)\n\n    # Test should_write when file exists and content is different\n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write('msgid \"\"\\nmsgstr \"\"\\n\"POT-Creation-Date: 2022-10-10 00:00+0000\\\\n\"\\n\"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\\\n\"\\n\"Content-Type: text/plain; charset=UTF-8\\\\n\"\\n')\n    assert should_write(filepath, new_content)\n\n    # Test should_write when file exists and content is the same\n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write(new_content)\n    assert not should_write(filepath, new_content)\n", "def test_should_write_new_file(app, tmp_path):\n    \"\"\"\n    Test the should_write function to ensure it correctly identifies when\n    a new POT file should be written.\n    \"\"\"\n    filename = tmp_path / \"test.pot\"\n    \n    # Create the initial file with some content\n    initial_content = (\n        '\"POT-Creation-Date: 2023-01-01 12:00+0000\\\\n\"\\n'\n        '\"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\\\n\"\\n'\n        '\"Content\"\\n'\n    )\n    filename.write_text(initial_content, encoding='utf-8')\n    \n    # Content with a different header that should trigger a rewrite\n    new_content = (\n        '\"POT-Creation-Date: 2023-01-02 12:00+0000\\\\n\"\\n'\n        '\"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\\\n\"\\n'\n        '\"Content\"\\n'\n    )\n    assert should_write(str(filename), new_content) is True\n\n    # Content with a different body that should trigger a rewrite\n    new_content_body_diff = (\n        '\"POT-Creation-Date: 2023-01-01 12:00+0000\\\\n\"\\n'\n        '\"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\\\n\"\\n'\n        '\"New Content\"\\n'\n    )\n    assert should_write(str(filename), new_content_body_diff) is True\n\n    # Content with same header and body that should not trigger a rewrite\n    same_content = initial_content\n    assert should_write(str(filename), same_content) is False\n"], "sample_934": ["def test_c_basic_types():\n        parser = DefinitionParser(type_string, location=None, config=None)\n        ast = parser._parse_trailing_type_spec()\n        assert str(ast) == type_string, f\"Expected {type_string}, got {str(ast)}\"\n        if expected_display:\n            assert ast.get_display_string() == expected_display, \\\n                f\"Expected display string {expected_display}, got {ast.get_display_string()}\"\n\n    # Fundamental types\n    for c_type in ['void', '_Bool', 'bool', 'char', 'int', 'float', 'double', '__int64']:\n        check_c_type(c_type, c_type)\n    \n    # Custom type with prefix\n    check_c_type('struct MyStruct', 'struct MyStruct')\n    check_c_type('enum MyEnum', 'enum MyEnum')\n    check_c_type('union MyUnion', 'union MyUnion')\n\n    # Check signed and unsigned types\n    check_c_type('unsigned int', 'unsigned int')\n    check_c_type('signed char', 'signed char')\n    check_c_type('long long', 'long long')\n    check_c_type('unsigned long long', 'unsigned long long')\n", "def test_symbol_lookup_and_merging():\n    # Create a root symbol\n    root = Symbol(None, None, None, None)\n\n    # Add a declaration to the root symbol\n    parser = DefinitionParser(\"int a;\", location=None, config=None)\n    ast = parser.parse_declaration(\"member\", \"member\")\n    root.add_declaration(ast, docname=\"TestDoc1\")\n\n    # Add a nested declaration\n    nested_name = ASTNestedName([ASTIdentifier(\"Nested\")], rooted=False)\n    nested_symbol = root.add_name(nested_name)\n    nested_parser = DefinitionParser(\"float b;\", location=None, config=None)\n    nested_ast = nested_parser.parse_declaration(\"member\", \"member\")\n    nested_symbol.add_declaration(nested_ast, docname=\"TestDoc2\")\n\n    # Test lookup of the nested declaration\n    found_symbol = root.find_declaration(nested_name, \"member\", matchSelf=True, recurseInAnon=True)\n    assert found_symbol is not None\n    assert found_symbol.declaration.declaration.decl.name.names[0].identifier == \"b\"\n\n    # Test merging another symbol tree\n    other_root = Symbol(None, None, None, None)\n    other_parser = DefinitionParser(\"char c;\", location=None, config=None)\n    other_ast = other_parser.parse_declaration(\"member\", \"member\")\n    other_root.add_declaration(other_ast, docname=\"TestDoc3\")\n\n    root.merge_with(other_root, [\"TestDoc3\"], env=None)\n\n    # Ensure the merged symbol is present\n    merged_symbol = root.find_declaration(ASTNestedName([ASTIdentifier(\"c\")], rooted=False), \"member\", matchSelf=True, recurseInAnon=True)\n    assert merged_symbol is not None\n    assert merged_symbol.declaration.declaration.decl.name.names[0].identifier == \"c\"\n", "def test_ast_identifier():\n    # Test ASTIdentifier creation and methods\n    ident = ASTIdentifier(\"test_identifier\")\n    assert str(ident) == \"test_identifier\"\n    assert ident.get_display_string() == \"test_identifier\"\n    assert not ident.is_anon()\n\n    # Test anonymous identifier\n    anon_ident = ASTIdentifier(\"@anon\")\n    assert str(anon_ident) == \"@anon\"\n    assert anon_ident.get_display_string() == \"[anonymous]\"\n    assert anon_ident.is_anon()\n\n    # Test equality\n    ident2 = ASTIdentifier(\"test_identifier\")\n    assert ident == ident2\n    assert ident != anon_ident\n", "def test_c_declaration_parsing():\n        class Config:\n            c_id_attributes = [\"id_attr\"]\n            c_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(input, location=None, config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_declaration(\"function\", \"function\")\n        parser.assert_end()\n        result = str(ast)\n        assert result == expected_output, f\"Failed parsing {input}. Expected: {expected_output}, but got: {result}\"\n\n    check(\"int f()\", \"int f()\")\n    check(\"void f(int a, double b)\", \"void f(int a, double b)\")\n    check(\"static inline int f()\", \"static inline int f()\")\n    check(\"extern void f()\", \"extern void f()\")\n    check(\"void *f()\", \"void *f()\")\n    check(\"const char *f()\", \"const char *f()\")\n    check(\"int (*f())(int, double)\", \"int (*f())(int, double)\")\n    check(\"int f(int arr[10])\", \"int f(int arr[10])\")\n    check(\"int f(int *p)\", \"int f(int *p)\")\n    check(\"int f(int (*p)[10])\", \"int f(int (*p)[10])\")\n", "def test_c_macro_definitions(app, status, warning):\n    # Test single macro definition\n    check('macro', '#define FOO 1', {2: '1FOO'}, key='macro')\n    \n    # Test macro with parameters\n    check('macro', '#define ADD(a, b) ((a) + (b))', {2: '3ADD'}, key='macro')\n    \n    # Test variadic macro\n    check('macro', '#define LOG(fmt, ...) fprintf(stderr, fmt, __VA_ARGS__)', {2: '3LOG'}, key='macro')\n    \n    # Test macro with string concatenation\n    check('macro', '#define STR(x) #x', {2: '3STR'}, key='macro')\n    \n    # Test macro with token pasting\n    check('macro', '#define CONCAT(a, b) a ## b', {2: '6CONCAT'}, key='macro')\n    \n    # Test macro with nested macros\n    check('macro', '#define DOUBLE(x) (x + x)', {2: '6DOUBLE'}, key='macro')\n    check('macro', '#define DOUBLE_ADD(a, b) DOUBLE(a) + DOUBLE(b)', {2: '10DOUBLE_ADD'}, key='macro')\n    \n    app.builder.build_all()\n    ws = filter_warnings(warning, \"macros\")\n    assert len(ws) == 0\n"], "sample_935": ["def test_alias_parsing():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n\n        node = AliasNode(sig, config=Config())\n        parser = DefinitionParser(sig, location=None, config=Config())\n        try:\n            ast, isShorthand = parser.parse_xref_object()\n            parser.assert_end()\n            if isShorthand:\n                assert isinstance(ast, ASTNamespace)\n            else:\n                assert isinstance(ast, ASTDeclaration)\n        except DefinitionError as e:\n            raise AssertionError(f\"Error parsing alias '{sig}': {e}\")\n        assert str(ast) == expected, f\"Expected '{expected}', got '{str(ast)}'\"\n\n    check_alias('MyClass::MyAlias', 'MyClass::MyAlias')\n    check_alias('namespace::Alias', 'namespace::Alias')\n    check_alias('std::vector<int>', 'std::vector<int>')\n    check_alias('decltype(auto) Alias', 'decltype(auto) Alias')\n", "def test_user_defined_literals():\n    # Testing user-defined literal operators\n    check('function', 'char operator\"\"_char_udl(const char* c)', {2: 'li8_char_udlPKc'})\n    check('function', 'int operator\"\"_int_udl(unsigned long long n)', {2: 'li7_int_udlmy'})\n    check('function', 'double operator\"\"_dbl_udl(long double d)', {2: 'li7_dbl_udlLd'})\n    check('function', 'std::string operator\"\"_str_udl(const char* str, size_t len)',\n          {2: 'li7_str_udlPKcm'})\n\n    # Testing user-defined literals with different types\n    check('function', 'constexpr int operator\"\"_my_int(char)', {2: 'li7_my_intc'})\n    check('function', 'constexpr int operator\"\"_my_int(unsigned long long)', {2: 'li7_my_intmy'})\n    check('function', 'constexpr long double operator\"\"_my_ld(long double)', {2: 'li6_my_ldLd'})\n    check('function', 'std::string operator\"\"_my_str(const char*, std::size_t)',\n          {2: 'li6_my_strPKcm'})\n\n    # Testing complex user-defined literals\n    check('function', 'std::string operator\"\"_complex(const char* str, std::size_t len, int radix)',\n          {2: 'li8_complexPKcmi'})\n", "def test_numerical_literals():\n    # Test integer literals\n    check('member', 'int a = 0', {1: 'a__i', 2: '1a'})\n    check('member', 'int b = 123', {1: 'b__i', 2: '1b'})\n    check('member', 'unsigned int c = 0x1A', {1: 'c__Ui', 2: '1c'})\n    check('member', 'long d = 0123', {1: 'd__l', 2: '1d'})\n    \n    # Test floating-point literals\n    check('member', 'float e = 1.0', {1: 'e__f', 2: '1e'})\n    check('member', 'double f = 1e10', {1: 'f__d', 2: '1f'})\n    check('member', 'long double g = 1.23e-4', {1: 'g__e', 2: '1g'})\n    \n    # Test complex literals\n    check('member', 'std::complex<double> h = {1.0, 2.0}', {1: 'h__std::complex:d:', 2: '1h'})\n    check('member', 'std::complex<float> i = {3.14f, 1.59f}', {1: 'i__std::complex:f:', 2: '1i'})\n    \n    # Test hexadecimal floating-point literals\n    check('member', 'double j = 0x1.2p3', {1: 'j__d', 2: '1j'})\n    check('member', 'float k = 0x0.1p-2', {1: 'k__f', 2: '1k'})\n", "def test_alias_transform():\n    class MockEnv:\n        temp_data = {}\n        domaindata = {'cpp': {'root_symbol': Symbol(None, None, None, None, None, None)}}\n\n    class MockBuilder:\n        pass\n\n    class MockNode:\n            self.sig = sig\n            self.parentKey = parentKey\n            self.children = []\n\n            self.children = new_nodes\n\n    env = MockEnv()\n    builder = MockBuilder()\n    parentKey = env.domaindata['cpp']['root_symbol'].get_lookup_key()\n    node = MockNode(\"int x\", parentKey)\n\n    alias_transform = AliasTransform(MockBuilder())\n    alias_transform.document = None  # Mock attribute as it is not used in apply\n    alias_transform.env = env\n    alias_transform.apply()\n\n    assert node.children, \"Alias transformation failed to replace the node with new nodes\"\n", "def test_operator_expressions():\n        ids = 'IE1CIA%s_1aE'\n        idDict = {2: ids % expr.format(), 3: ids % id}\n        if id4 is not None:\n            idDict[4] = ids % id4\n        check('class', 'template<> {key}C<a[%s]>' % expr, idDict)\n\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(expr, location=None,\n                                  config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_expression()\n        res = str(ast)\n        if res != expr:\n            print(\"\")\n            print(\"Input:    \", expr)\n            print(\"Result:   \", res)\n            raise DefinitionError(\"\")\n        displayString = ast.get_display_string()\n        if res != displayString:\n            print(\"\")\n            print(\"Input:    \", expr)\n            print(\"Result:   \", res)\n            print(\"Display:  \", displayString)\n            raise DefinitionError(\"\")\n\n    # binary operators with non-trivial expressions\n    exprCheck('a + b * c', 'pl1apl1b1cE')\n    exprCheck('a * b + c', 'plml1a1b1cE')\n    exprCheck('(a + b) * c', 'mlpl1a1b1cE')\n    exprCheck('a + (b * c)', 'pl1amll1b1cE')\n    exprCheck('(a + b) * (c + d)', 'mlpl1a1bl1c1dE')\n\n    # binary operators with user-defined literals\n    exprCheck('a + 1.0_foo', 'clpl1aL1.0_fooEE')\n    exprCheck('1.0_foo + a', 'clplL1.0_foo1aEE')\n    exprCheck('1.0_foo + 2.0_bar', 'clplL1.0_fooL2.0_barEE')\n\n    # complex nested expressions\n    exprCheck('(a + b) * (c + d) - (e / f)', 'mimlpl1a1bl1c1dl1e1fE')\n    exprCheck('a * b + c / d -"], "sample_936": ["def test_stringify_forward_references():\n    if sys.version_info >= (3, 7):\n        ForwardRef = typing.ForwardRef\n    else:\n        from sphinx.util.typing import ForwardRef\n\n    assert stringify(ForwardRef('int')) == \"int\"\n    assert stringify(ForwardRef('List[int]')) == \"List[int]\"\n    assert stringify(ForwardRef('MyClass1')) == \"test_util_typing.MyClass1\"\n", "def test_stringify_forward_references():\n    if sys.version_info >= (3, 7):\n        ForwardRef = typing.ForwardRef\n    else:\n        from sphinx.util.typing import ForwardRef\n\n    assert stringify(ForwardRef(\"MyClass1\")) == \"MyClass1\"\n    assert stringify(ForwardRef(\"List[MyClass1]\")) == \"List[MyClass1]\"\n", "def test_stringify_forward_refs():\n    ForwardRefType = ForwardRef(\"MyClass1\")\n    assert stringify(ForwardRefType) == \"MyClass1\"\n\n    if sys.version_info < (3, 7):\n        ForwardRefTypeLegacy = ForwardRef(\"MyClass1\", is_argument=False)\n        assert stringify(ForwardRefTypeLegacy) == \"MyClass1\"\n", "def test_stringify_forward_ref():\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef\n    else:\n        from sphinx.util.typing import ForwardRef\n\n    ref = ForwardRef('MyClass1')\n    assert stringify(ref) == 'MyClass1'\n\n    ref = ForwardRef('List[int]')\n    assert stringify(ref) == 'List[int]'\n\n    ref = ForwardRef('Union[int, str]')\n    assert stringify(ref) == 'Union[int, str]'\n", "def test_stringify_forward_ref():\n    if sys.version_info >= (3, 7):\n        assert stringify(ForwardRef('MyClass1')) == \"MyClass1\"\n    else:\n        forward_ref = ForwardRef('MyClass1')\n        assert stringify(forward_ref._evaluate({'MyClass1': MyClass1}, {})) == \"test_util_typing.MyClass1\"\n"], "sample_937": ["def test_unparse_with_arguments():\n    source = \"def func(a, b, c=3, *args, d, e=5, **kwargs): pass\"\n    expected = \"a, b, c=3, *args, d, e=5, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_function_with_annotations():\n    source = \"def func(x: int, y: str = 'default') -> None: pass\"\n    expected = \"def func(x: int, y: str='default') -> None: ...\"\n    module = ast.parse(source)\n    func_def = module.body[0]\n    func_str = f\"def {func_def.name}({ast.unparse(func_def.args)}) -> {ast.unparse(func_def.returns)}: ...\"\n    assert func_str == expected\n", "def test_generic_visit():\n    class CustomNode(ast.AST):\n        _fields = ('name',)\n\n            self.name = name\n\n    node = CustomNode(name=\"custom\")\n    visitor = ast._UnparseVisitor()\n    with pytest.raises(NotImplementedError):\n        visitor.visit(node)\n", "def test_unparse_arguments():\n    source = \"def func(a: int, b: str = 'default', *args, c: float, **kwargs): pass\"\n    expected = \"a: int, b = 'default', *args, c: float, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_parse_with_invalid_syntax():\n    with pytest.raises(SyntaxError):\n        ast.parse(\"a +\")\n"], "sample_938": ["def test_warning_no_man_pages_config(app, status, warning):\n    # Remove man_pages configuration to trigger the warning\n    app.config.man_pages = []\n    app.builder.build_all()\n\n    assert 'no \"man_pages\" config value found; no manual pages will be written' in warning.getvalue()\n", "def test_man_show_urls(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n\n    # Check that URLs are shown in the content\n    assert 'http://example.com' in content\n    assert r'\\[1\\] http://example.com' in content\n\n    # Ensure the URLs are in the correct format\n    assert '.URL \"http://example.com\"\\n' in content\n", "def test_supported_image_types(app, status, warning):\n    app.builder.supported_image_types = ['image/png', 'image/jpeg']\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    \n    # Check if the image types are properly handled in the output\n    assert '.image' in content\n", "def test_man_show_urls(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # Check that URLs are included in the output\n    assert 'http://example.com' in content\n", "def test_inline_all_toctrees(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert '.SH TOC Title\\n' in content\n    assert '.IP 1.\\n.B Subsection1\\n' in content\n    assert '.IP 2.\\n.B Subsection2\\n' in content\n"], "sample_939": ["def test_unparse_function_def(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_unparse_additional_cases(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_comprehensions_and_expressions(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "compilation error", "def test_unparse_complex_statements(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n"], "sample_940": ["def test_isenumclass():\n    class MyEnum(enum.Enum):\n        A = 1\n        B = 2\n\n    class MyClass:\n        pass\n\n    assert inspect.isenumclass(MyEnum) is True\n    assert inspect.isenumclass(MyEnum.A) is False\n    assert inspect.isenumclass(MyClass) is False\n    assert inspect.isenumclass(1) is False\n    assert inspect.isenumclass(None) is False\n", "def test_unwrap_all():\n            return f(*args, **kwargs)\n        return wrapper\n\n    @decorator\n        return a + b\n\n    assert inspect.unwrap_all(foo) is foo\n\n    p = functools.partial(foo, 1)\n    assert inspect.unwrap_all(p) is foo\n\n    class MyClass:\n        @decorator\n            return x\n\n    instance = MyClass()\n    assert inspect.unwrap_all(instance.method) is instance.method.__func__\n", "def test_safe_getmembers():\n    class Foo:\n        a = 1\n        b = 2\n        c = 3\n\n            return self.a + self.b + self.c\n\n    obj = Foo()\n    members = inspect.safe_getmembers(obj)\n    member_names = [name for name, value in members]\n\n    assert 'a' in member_names\n    assert 'b' in member_names\n    assert 'c' in member_names\n    assert 'meth' in member_names\n", "def test_ispartial():\n        pass\n\n    partial_func = functools.partial(normal_func, a=1)\n\n    assert inspect.ispartial(partial_func) is True\n    assert inspect.ispartial(normal_func) is False\n\n    class Example:\n        @functools.partialmethod\n            pass\n\n    assert inspect.ispartial(Example.method) is True\n    assert inspect.ispartial(Example().method) is True\n    assert inspect.ispartial(Example) is False\n", "def test_isenumclass():\n    class MyEnum(enum.Enum):\n        A = 1\n        B = 2\n\n    class NotEnum:\n        pass\n\n    assert inspect.isenumclass(MyEnum) is True\n    assert inspect.isenumclass(NotEnum) is False\n    assert inspect.isenumclass(MyEnum.A) is False\n\n"], "sample_941": ["def test_restify_forward_reference():\n    class MyForwardRef:\n            self.ref = ForwardRef('MyForwardRef')\n    \n    forward_ref_instance = MyForwardRef()\n    assert restify(forward_ref_instance.ref) == \":class:`MyForwardRef`\"\n", "def test_get_type_hints():\n        pass\n\n    class MyClass:\n        attr: int\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(MyClass) == {'attr': int}\n    assert get_type_hints(None) == {}\n", "def test_get_type_hints():\n    from sphinx.util.typing import get_type_hints\n\n        return True\n\n    # Test normal function with annotations\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n\n    class MyClass:\n            pass\n\n    # Test method inside class with annotations\n    assert get_type_hints(MyClass.method) == {'x': int, 'y': str, 'return': None}\n\n    # Test with empty annotations\n        return True\n    assert get_type_hints(func_no_annotations) == {}\n\n    # Test with ForwardRef\n    if sys.version_info >= (3, 7):\n        from __future__ import annotations\n            return MyClass()\n        assert get_type_hints(forward_ref_func, globals(), locals()) == {'a': MyClass, 'return': MyClass}\n", "def test_restify_NoneType():\n    assert restify(NoneType) == \":obj:`None`\"\n", "def test_stringify_forward_ref():\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef  # type: ignore\n        assert stringify(ForwardRef(\"MyClass1\")) == \"MyClass1\"\n    else:\n        from typing import _ForwardRef  # type: ignore\n        ref = _ForwardRef(\"MyClass1\")\n        assert stringify(ref._eval_type(globals(), locals())) == \"MyClass1\"\n"], "sample_942": ["def test_pycurrentmodule_directive(app):\n    text = (\".. py:currentmodule:: test_module\\n\"\n            \".. py:function:: test_func()\\n\"\n            \".. py:currentmodule:: None\\n\"\n            \".. py:function:: no_module_func()\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    \n    # Check that the current module is set and reset properly\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"test_module.\"],\n                                                    [desc_name, \"test_func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])],\n                          nodes.target,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"no_module_func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    \n    # Verify that the functions are correctly indexed under the appropriate module\n    assert 'test_module.test_func' in domain.objects\n    assert domain.objects['test_module.test_func'] == ('index', 'test_module.test_func', 'function', False)\n    \n    assert 'no_module_func' in domain.objects\n    assert domain.objects['no_module_func'] == ('index', 'no_module_func', 'function', False)\n", "def test_pymodule(app):\n    text = \".. py:module:: example\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target, addnodes.index))\n    assert_node(doctree[1], addnodes.index, entries=[('pair', 'module; example', 'module-example', '', None)])\n    assert 'example' in domain.modules\n    assert domain.modules['example'] == ('index', 'module-example', '', '', False)\n", "def test_pyclass_signature(app):\n    text = \".. py:class:: MyClass\\n   :canonical: mypackage.MyClass\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"MyClass\"])],\n                                  desc_content)]))\n    assert 'MyClass' in domain.objects\n    assert domain.objects['MyClass'] == ('index', 'MyClass', 'class', False)\n    assert 'mypackage.MyClass' in domain.objects\n    assert domain.objects['mypackage.MyClass'] == ('index', 'MyClass', 'class', True)\n", "def test_pymodule_directive(app):\n    text = (\".. py:module:: test_module\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :synopsis: A test module\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"module \"],\n                                                    [desc_name, \"test_module\"])],\n                                  [desc_content, ()])]))\n    \n    assert 'test_module' in domain.modules\n    module_entry = domain.modules['test_module']\n    assert module_entry.docname == 'index'\n    assert module_entry.synopsis == 'A test module'\n    assert module_entry.platform == 'Unix'\n    assert module_entry.deprecated is True\n", "def test_type_to_xref():\n    env = Mock(ref_context={})\n    xref = type_to_xref('int', env)\n    assert isinstance(xref, pending_xref)\n    assert xref['reftarget'] == 'int'\n    assert xref['reftype'] == 'class'\n    assert xref['refdomain'] == 'py'\n\n    env.ref_context = {'py:module': 'example'}\n    xref = type_to_xref('str', env)\n    assert isinstance(xref, pending_xref)\n    assert xref['reftarget'] == 'str'\n    assert xref['reftype'] == 'class'\n    assert xref['refdomain'] == 'py'\n    assert xref['py:module'] == 'example'\n\n    xref = type_to_xref('None', env)\n    assert isinstance(xref, pending_xref)\n    assert xref['reftarget'] == 'None'\n    assert xref['reftype'] == 'obj'\n    assert xref['refdomain'] == 'py'\n    assert xref['py:module'] == 'example'\n"], "sample_943": ["def test_dry_run_option(tempdir, capsys):\n    \"\"\"Test the --dry-run option to ensure no files are created and correct messages are printed.\"\"\"\n    outdir = tempdir / 'out'\n    (tempdir / 'module').makedirs()\n    (tempdir / 'module' / 'example.py').write_text('')\n\n    apidoc_main(['-o', outdir, '--dry-run', tempdir / 'module'])\n\n    # Check that no files have been created\n    assert not (outdir / 'module.rst').exists()\n    assert not (outdir / 'modules.rst').exists()\n\n    # Check the output messages\n    captured = capsys.readouterr()\n    assert \"Would create file\" in captured.out\n", "def test_follow_links(make_app, apidoc, tmpdir):\n    # Create a structure with symlinks\n    srcdir = tmpdir / 'src'\n    srcdir.ensure(dir=True)\n    (srcdir / 'module.py').write_text('')\n\n    destdir = tmpdir / 'dest'\n    destdir.ensure(dir=True)\n    (destdir / 'link_to_module.py').mksymlinkto(srcdir / 'module.py')\n\n    apidoc_main(['-o', destdir, srcdir, '--follow-links'])\n    \n    # Verify that the symlinked file is processed\n    assert (destdir / 'module.rst').exists()\n    content = (destdir / 'module.rst').read_text()\n    assert \"module module\" in content\n\n    # Build the Sphinx documentation\n    app = make_app('text', srcdir=destdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n", "def test_follow_links(make_app, apidoc):\n    \"\"\"Ensure the --follow-links option is handled correctly.\"\"\"\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'index.rst').isfile()\n\n    # Validate that the symbolic links are followed and processed\n    symlink_dir = apidoc.coderoot / 'symlinked_dir'\n    symlink_target = apidoc.coderoot / 'actual_dir'\n    os.makedirs(symlink_target)\n    (symlink_target / 'example.py').write_text('')\n\n    if not symlink_dir.exists():\n        os.symlink(symlink_target, symlink_dir)\n\n    apidoc_main(['-o', outdir, '-F', apidoc.coderoot, '--follow-links'])\n    assert (outdir / 'symlinked_dir.example.rst').isfile()\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n", "def test_dry_run(apidoc, capsys):\n    \"\"\"Test the --dry-run option to ensure no files are created.\"\"\"\n    outdir = apidoc.outdir\n    assert not (outdir / 'conf.py').exists()\n    assert not (outdir / 'index.rst').exists()\n\n    captured = capsys.readouterr()\n    assert 'Would create file' in captured.out\n", "def test_custom_suffix(make_app, apidoc):\n    \"\"\"Test custom file suffix generation.\"\"\"\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'index.md').isfile()\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n\n    builddir = outdir / '_build' / 'text'\n    assert (builddir / 'index.txt').isfile()\n"], "sample_944": ["def test_get_type_hints():\n        return True\n\n    class SampleClass:\n            pass\n\n    expected_function_hints = {'x': int, 'y': str, 'return': bool}\n    expected_method_hints = {'a': List[int], 'b': Dict[str, int], 'return': None}\n\n    assert get_type_hints(sample_function) == expected_function_hints\n    assert get_type_hints(SampleClass.method) == expected_method_hints\n\n    # Test with global and local namespace\n    globalns = {'List': List, 'Dict': Dict, 'int': int, 'str': str, 'bool': bool, 'None': None}\n    localns = {}\n    assert get_type_hints(sample_function, globalns, localns) == expected_function_hints\n    assert get_type_hints(SampleClass.method, globalns, localns) == expected_method_hints\n", "def test_get_type_hints():\n        return True\n\n    class SampleClass:\n            pass\n\n    assert get_type_hints(sample_function) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(SampleClass.method) == {'x': List[int], 'return': None}\n", "def test_get_type_hints():\n    from sphinx.util.inspect import safe_getattr\n\n        return y\n\n        return y\n\n    class ClassWithTypeHints:\n        attr: int\n\n            return b\n\n    assert get_type_hints(func_with_type_hints) == {'x': int, 'y': str, 'return': str}\n    assert get_type_hints(func_without_type_hints) == {}\n    assert get_type_hints(ClassWithTypeHints) == {'attr': int}\n    assert get_type_hints(ClassWithTypeHints.method_with_type_hints) == {'a': int, 'b': str, 'return': str}\n\n    # Test ForwardRef evaluation\n    if sys.version_info >= (3, 7):\n        class ForwardRefClass:\n                return x\n\n        expected_hints = {'x': ForwardRef('ForwardRefClass'), 'return': ForwardRef('ForwardRefClass')}\n        assert get_type_hints(ForwardRefClass.method_with_forward_ref) == expected_hints\n\n    # Test invalid object fallback to __annotations__\n    class InvalidClass:\n        __annotations__ = {'x': 'int'}\n\n    assert get_type_hints(InvalidClass) == {'x': 'int'}\n", "def test_get_type_hints():\n        return True\n\n    class MyClass:\n        a: int\n        b: str\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(MyClass) == {'a': int, 'b': str}\n\n    # Handle function without annotations\n        return True\n\n    assert get_type_hints(func_no_annotations) == {}\n\n    # Handle class without annotations\n    class MyClassNoAnnotations:\n        a = 1\n        b = \"string\"\n\n    assert get_type_hints(MyClassNoAnnotations) == {}\n\n    # Handle invalid object\n    assert get_type_hints(12345) == {}\n", "def test_get_type_hints():\n        return True\n\n    class TestClass:\n        attr: List[int]\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(TestClass) == {'attr': List[int]}\n    \n    # Test with globals and locals\n    globalns = {'List': List, 'int': int}\n    localns = {'str': str}\n    assert get_type_hints(func, globalns, localns) == {'a': int, 'b': str, 'return': bool}\n\n    # Test with forward references\n    class ForwardRefTest:\n        attr: 'ForwardRefTest'\n\n    assert get_type_hints(ForwardRefTest) == {'attr': ForwardRefTest}\n\n    # Test with invalid type\n    class InvalidClass:\n        __annotations__ = 'invalid'\n\n    assert get_type_hints(InvalidClass) == {}\n"], "sample_945": ["def test_add_target_and_index():\n    env = Mock()\n    state = Mock()\n    signode = desc_signature('test_signature', '')\n    env.config.add_module_names = True\n    env.ref_context.get = Mock(side_effect=lambda key: None if key == 'py:class' else 'test_module')\n    state.document = Mock()\n    state.document.note_explicit_target = Mock()\n    directive = PyFunction(env, state, None, None)\n\n    name_cls = ('test_function', '')\n    sig = 'test_function()'\n    directive.add_target_and_index(name_cls, sig, signode)\n\n    assert 'test_function' in directive.env.domaindata['py']['objects']\n    assert signode['ids'] == ['test_signature']\n    assert 'test_module.test_function' in directive.env.domaindata['py']['objects']\n    assert signode['ids'] == ['test_signature']\n    state.document.note_explicit_target.assert_called_once_with(signode)\n", "def test_type_to_xref(app, status, warning):\n    env = app.env\n    env.config.python_use_unqualified_type_names = True\n    xref = type_to_xref(\"List[int]\", env)\n    assert_node(xref, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"List\")\n    assert_node(xref[0], pending_xref_condition, condition=\"resolved\", children=[nodes.Text(\"List\")])\n    assert_node(xref[1], pending_xref_condition, condition=\"*\", children=[nodes.Text(\"List[int]\")])\n\n    env.config.python_use_unqualified_type_names = False\n    xref = type_to_xref(\"List[int]\", env)\n    assert_node(xref, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"List[int]\")\n    assert_node(xref[0], nodes.Text, \"List[int]\")\n", "def test_parse_annotation_with_complex_types(app):\n    \"\"\"Test _parse_annotation with complex type annotations.\"\"\"\n    doctree = _parse_annotation(\"Dict[str, Union[int, None]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"None\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"Optional[Callable[[Any], bool]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Optional\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Callable\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Any\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"bool\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"List[Tuple[int, str, List[float]]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"float\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_pyfunction_with_type_annotation(app):\n    text = \".. py:function:: hello(name: str, age: int) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    [desc_parameterlist,\n                                                     ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                                        [desc_sig_punctuation, \":\"],\n                                                                        \" \",\n                                                                        [nodes.inline, pending_xref, \"str\"])],\n                                                      [desc_parameter, ([desc_sig_name, \"age\"],\n                                                                        [desc_sig_punctuation, \":\"],\n                                                                        \" \",\n                                                                        [nodes.inline, pending_xref, \"int\"])])],\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1][0][0][1], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n    assert_node(doctree[1][0][1][0][1][1], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    assert_node(doctree[1][0][2], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n", "def test_type_to_xref():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    env.ref_context = {'py:module': 'example', 'py:class': 'ExampleClass'}\n\n    xref = type_to_xref('example.Class', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='example.Class')\n    assert_node(xref[0], nodes.Text, text='example.Class')\n\n    xref = type_to_xref('None', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n    assert_node(xref[0], nodes.Text, text='None')\n\n    env.config.python_use_unqualified_type_names = True\n    xref = type_to_xref('example.Class', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='example.Class')\n    assert_node(xref[0], pending_xref_condition, condition='resolved')\n    assert_node(xref[0][0], nodes.Text, text='Class')\n    assert_node(xref[1], pending_xref_condition, condition='*')\n    assert_node(xref[1][0], nodes.Text, text='example.Class')\n"], "sample_946": ["def test_pymodule_signature(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:module:: example.submodule\\n\"\n            \".. py:module:: othermodule\\n\"\n            \"   :noindex:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"module \"],\n                                                    [desc_name, \"example\"])],\n                                  desc_content)],\n                          nodes.target,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"module \"],\n                                                    [desc_addname, \"example.\"],\n                                                    [desc_name, \"submodule\"])],\n                                  desc_content)],\n                          nodes.target,\n                          desc))\n    assert 'example' in domain.modules\n    assert domain.modules['example'] == ('index', 'example', '', '', False)\n    assert 'example.submodule' in domain.modules\n    assert domain.modules['example.submodule'] == ('index', 'example.submodule', '', '', False)\n    assert 'othermodule' not in domain.modules\n", "def test_parse_annotation_edge_cases(app):\n    # Test for empty annotation\n    doctree = _parse_annotation(\"\", app.env)\n    assert_node(doctree, [])\n\n    # Test for invalid syntax in annotation\n    doctree = _parse_annotation(\"invalid syntax]\", app.env)\n    assert_node(doctree, ([pending_xref, \"invalid syntax]\"],))\n\n    # Test for annotation with only punctuation\n    doctree = _parse_annotation(\"[[[]]]\", app.env)\n    assert_node(doctree, ([desc_sig_punctuation, \"[\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_pyclass_with_nested_classes(app):\n    text = (\".. py:class:: OuterClass\\n\"\n            \"\\n\"\n            \"   .. py:class:: InnerClass\\n\"\n            \"   .. py:method:: inner_method()\\n\"\n            \"   .. py:attribute:: inner_attr\\n\"\n            \"      :type: str\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"OuterClass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n\n    # InnerClass\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'InnerClass (OuterClass class)', 'OuterClass.InnerClass', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, \"class \"],\n                                                     [desc_addname, \"OuterClass.\"],\n                                                     [desc_name, \"InnerClass\"])],\n                                   [desc_content, ()]))\n    assert 'OuterClass.InnerClass' in domain.objects\n    assert domain.objects['OuterClass.InnerClass'] == ('index', 'OuterClass.InnerClass', 'class', False)\n\n    # inner_method\n    assert_node(doctree[1][1][2], addnodes.index,\n                entries=[('single', 'inner_method() (OuterClass.InnerClass method)', 'OuterClass.InnerClass.inner_method', '', None)])\n    assert_node(doctree[1][1][3], ([desc_signature, ([desc_addname, \"OuterClass.InnerClass.\"],\n                                                     [desc_name, \"inner_method\"],\n                                                     [desc_parameterlist, ()])],\n                                   [desc_content, ()]))\n    assert 'OuterClass.InnerClass.inner_method' in domain.objects\n    assert domain.objects['OuterClass.InnerClass.inner_method'] == ('index', 'OuterClass.InnerClass.inner_method', 'method', False)\n\n    # inner_attr\n    assert_node(doctree[1][1][4], addnodes.index,\n                entries=[('single', 'inner_attr (OuterClass.InnerClass attribute)', 'OuterClass.InnerClass.inner_attr', '', None)])\n    assert_node(doctree[1][1][5], ([desc_signature", "def test_pyobject_handle_signature():\n    class TestPyObject(PyObject):\n            return 'testprefix '\n\n    obj = TestPyObject()\n    signode = addnodes.desc_signature('test', '')\n\n    # Testing with module context\n    obj.env = Mock()\n    obj.env.ref_context = {'py:module': 'mymodule'}\n    obj.handle_signature('MyClass.my_method', signode)\n\n    assert_node(signode, ([desc_annotation, 'testprefix '],\n                          [desc_addname, 'MyClass.'],\n                          [desc_name, 'my_method']))\n\n    # Testing with class context\n    obj.env.ref_context = {'py:class': 'MyClass'}\n    signode = addnodes.desc_signature('test', '')\n    obj.handle_signature('my_method', signode)\n\n    assert_node(signode, ([desc_annotation, 'testprefix '],\n                          [desc_name, 'my_method']))\n\n    # Testing with no context\n    obj.env.ref_context = {}\n    signode = addnodes.desc_signature('test', '')\n    obj.handle_signature('my_method', signode)\n\n    assert_node(signode, ([desc_annotation, 'testprefix '],\n                          [desc_name, 'my_method']))\n\n    # Testing with full signature\n    obj.env.ref_context = {}\n    signode = addnodes.desc_signature('test', '')\n    obj.handle_signature('mymodule.MyClass.my_method', signode)\n\n    assert_node(signode, ([desc_annotation, 'testprefix '],\n                          [desc_addname, 'mymodule.MyClass.'],\n                          [desc_name, 'my_method']))\n", "def test_pyvariable_signature(app):\n    text = (\".. py:variable:: my_var\\n\"\n            \"   :type: List[int]\\n\"\n            \"   :value: [1, 2, 3]\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"my_var\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"List\"],\n                                                                       [desc_sig_punctuation, \"[\"],\n                                                                       [pending_xref, \"int\"],\n                                                                       [desc_sig_punctuation, \"]\"])],\n                                                    [desc_annotation, \" = [1, 2, 3]\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"variable\",\n                domain=\"py\", objtype=\"variable\", noindex=False)\n    assert 'my_var' in domain.objects\n    assert domain.objects['my_var'] == ('index', 'my_var', 'variable', False)\n"], "sample_947": ["def test_trailing_type_spec():\n        class Config:\n            c_id_attributes = [\"id_attr\"]\n            c_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(string, location=None, config=Config())\n        return parser._parse_trailing_type_spec()\n\n    # Test fundamental types\n    assert str(parse_trailing_type_spec('void')) == 'void'\n    assert str(parse_trailing_type_spec('_Bool')) == '_Bool'\n    assert str(parse_trailing_type_spec('bool')) == 'bool'\n    assert str(parse_trailing_type_spec('char')) == 'char'\n    assert str(parse_trailing_type_spec('int')) == 'int'\n    assert str(parse_trailing_type_spec('float')) == 'float'\n    assert str(parse_trailing_type_spec('double')) == 'double'\n    assert str(parse_trailing_type_spec('__int64')) == '__int64'\n\n    # Test prefixed types\n    assert str(parse_trailing_type_spec('struct Foo')) == 'struct Foo'\n    assert str(parse_trailing_type_spec('union Bar')) == 'union Bar'\n    assert str(parse_trailing_type_spec('enum Baz')) == 'enum Baz'\n\n    # Test signed and unsigned types\n    assert str(parse_trailing_type_spec('signed char')) == 'signed char'\n    assert str(parse_trailing_type_spec('unsigned int')) == 'unsigned int'\n    assert str(parse_trailing_type_spec('long long')) == 'long long'\n    assert str(parse_trailing_type_spec('unsigned long long')) == 'unsigned long long'\n    assert str(parse_trailing_type_spec('signed short')) == 'signed short'\n    assert str(parse_trailing_type_spec('unsigned short')) == 'unsigned short'\n\n    # Test nested name\n    assert str(parse_trailing_type_spec('Foo::Bar')) == 'Foo::Bar'\n", "def test_cnamespace_object():\n    text = (\".. c:namespace:: MyNamespace\\n\"\n            \".. c:struct:: MyNamespaceStruct\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1], addnodes.desc, desctype=\"struct\",\n                domain=\"c\", objtype=\"struct\", noindex=False)\n\n    entry = _get_obj(app, 'MyNamespaceStruct')\n    assert entry == ('index', 'c.MyNamespaceStruct', 'struct')\n\n", "def test_trailing_type_spec():\n        class Config:\n            c_id_attributes = [\"id_attr\"]\n            c_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(spec_str, location=None, config=Config())\n        ast = parser._parse_trailing_type_spec()\n        assert str(ast) == expected_str, f\"Expected {expected_str}, but got {str(ast)}\"\n\n    # Test fundamental types\n    type_spec_check('void', 'void')\n    type_spec_check('_Bool', '_Bool')\n    type_spec_check('bool', 'bool')\n    type_spec_check('char', 'char')\n    type_spec_check('int', 'int')\n    type_spec_check('float', 'float')\n    type_spec_check('double', 'double')\n    type_spec_check('__int64', '__int64')\n\n    # Test prefixed keys\n    type_spec_check('struct MyStruct', 'struct MyStruct')\n    type_spec_check('enum MyEnum', 'enum MyEnum')\n    type_spec_check('union MyUnion', 'union MyUnion')\n\n    # Test with modifiers\n    type_spec_check('signed int', 'signed int')\n    type_spec_check('unsigned int', 'unsigned int')\n    type_spec_check('short int', 'short int')\n    type_spec_check('long int', 'long int')\n    type_spec_check('unsigned long int', 'unsigned long int')\n", "def test_alias_object():\n    text = \"\"\"", "def test_symbol_lookup_errors():\n    # Test for Symbol lookup edge cases and errors\n\n    class Config:\n        c_id_attributes = []\n        c_paren_attributes = []\n\n        root = Symbol(None, None, None, None, None)\n        struct_decl = parse('struct', 'struct MyStruct')\n        func_decl = parse('function', 'void myFunc(int a, float b)')\n        var_decl = parse('member', 'int myVar')\n        enum_decl = parse('enum', 'enum MyEnum')\n        enum_decl_2 = parse('enum', 'enum MyEnum2')\n        root.add_declaration(struct_decl, 'TestDoc', 1)\n        root.add_declaration(func_decl, 'TestDoc', 2)\n        root.add_declaration(var_decl, 'TestDoc', 3)\n        root.add_declaration(enum_decl, 'TestDoc', 4)\n        root.add_declaration(enum_decl_2, 'TestDoc', 5)\n        return root\n\n    root = create_symbol_tree()\n\n    # Test finding non-existent symbol\n    non_existent_symbol = root.find_declaration(ASTNestedName([ASTIdentifier('NonExistent')], False), 'struct', True, False)\n    assert non_existent_symbol is None\n\n    # Test finding symbol by wrong type\n    wrong_type_symbol = root.find_declaration(ASTNestedName([ASTIdentifier('MyStruct')], False), 'function', True, False)\n    assert wrong_type_symbol is None\n\n    # Test finding symbol with correct type\n    correct_symbol = root.find_declaration(ASTNestedName([ASTIdentifier('MyStruct')], False), 'struct', True, False)\n    assert correct_symbol is not None\n    assert correct_symbol.declaration.name.names[0].identifier == 'MyStruct'\n\n    # Test finding symbol in nested scopes\n    nested_scope = Symbol(root, ASTIdentifier('NestedScope'), None, 'TestDoc', 6)\n    nested_func_decl = parse('function', 'void nestedFunc(int x)')\n    nested_scope.add_declaration(nested_func_decl, 'TestDoc', 7)\n    root._children.append(nested_scope)\n\n    nested_symbol = nested_scope.find_declaration(ASTNestedName([ASTIdentifier('nestedFunc')], False), 'function', True, False)\n    assert nested_symbol is not None\n    assert nested_symbol.declaration.name.names[0].identifier == 'nestedFunc'\n\n    # Test finding symbol in wrong nested scope\n"], "sample_948": ["def test_template_member_function():\n    check('function', 'template<typename T> void A<T>::f()', \n          {2: 'I0EN1AI1TE1fEv'})\n    check('function', 'template<typename T> void A::f(T)', \n          {2: 'I0E1f1T'})\n    check('function', 'template<typename T> void A<T>::B<T>::f()', \n          {2: 'I0EN1AI1TE1BI1TE1fEv'})\n    check('function', 'template<typename T, typename U> void A<T>::f(U)', \n          {2: 'I00EN1AI1TE1f1U'})\n    check('function', 'template<typename T, typename U> void A::f(U)', \n          {2: 'I00E1f1U'})\n", "def test_cast_operators():\n    check('function', 'operator int() const', {1: 'castto-int-operatorC', 2: 'NKcviv'})\n    check('function', 'operator double() volatile', {1: 'castto-double-operatorV', 2: 'NVcvdv'})\n    check('function', 'operator float() const &', {1: 'castto-float-operatorCR', 2: 'NKcvfR'})\n    check('function', 'operator char() volatile &&', {1: 'castto-char-operatorVO', 2: 'NVKcvcvO'})\n    check('function', 'operator long() const &&', {1: 'castto-long-operatorCO', 2: 'NKcvlo'})\n    check('function', 'operator bool() &&', {1: 'castto-bool-operatorO', 2: 'NOcvbv'})\n    check('function', 'operator short() &', {1: 'castto-short-operatorR', 2: 'NRcvs'})\n    check('function', 'operator unsigned int() const &', {1: 'castto-unsigned-int-operatorCR', 2: 'NKcvjR'})\n", "def test_incomplete_template_syntax():\n    with pytest.raises(DefinitionError):\n        parse('class', \"template< {key}A\")\n    with pytest.raises(DefinitionError):\n        parse('class', \"template<> {key}A<\")\n    with pytest.raises(DefinitionError):\n        parse('class', \"template<typename T {key}A\")\n    with pytest.raises(DefinitionError):\n        parse('class', \"template<typename T> {key}A<\")\n    with pytest.raises(DefinitionError):\n        parse('class', \"template<typename T {key}A<T\")\n    with pytest.raises(DefinitionError):\n        parse('class', \"template<typename T> {key}A<T\")\n    with pytest.raises(DefinitionError):\n        parse('class', \"template<typename T> {key}A<T::\")\n    with pytest.raises(DefinitionError):\n        parse('class', \"template<typename T> {key}A<T::B\")\n    with pytest.raises(DefinitionError):\n        parse('class', \"template<typename T> {key}A<T::B<\")\n    with pytest.raises(DefinitionError):\n        parse('class', \"template<typename T> {key}A<T::B<>\")\n    with pytest.raises(DefinitionError):\n        parse('class', \"template<typename T> {key}A<T::B<T\")\n    with pytest.raises(DefinitionError):\n        parse('class', \"template<typename T> {key}A<T::B<T>\")\n", "def test_enum_scope():\n    # Test for scoped and unscoped enums\n    # Unscoped enum\n    check('enum', 'enum Colors { RED, GREEN, BLUE }', {2: '6Colors'})\n\n    # Scoped enum (enum class)\n    check('enum', 'enum class ScopedColors { RED, GREEN, BLUE }', {2: '11ScopedColors'})\n\n    # Scoped enum (enum struct)\n    check('enum', 'enum struct ScopedColorsStruct { RED, GREEN, BLUE }', {2: '16ScopedColorsStruct'})\n\n    # Unscoped enum with enumerators\n    check('enum', 'enum Colors { RED = 1, GREEN, BLUE }', {2: '6Colors'})\n    check('enumerator', 'RED = 1', {2: '3RED'})\n    check('enumerator', 'GREEN', {2: '5GREEN'})\n    check('enumerator', 'BLUE', {2: '4BLUE'})\n\n    # Scoped enum with enumerators\n    check('enum', 'enum class ScopedColors { RED = 1, GREEN, BLUE }', {2: '11ScopedColors'})\n    check('enumerator', 'ScopedColors::RED = 1', {2: 'N11ScopedColors3REDE'})\n    check('enumerator', 'ScopedColors::GREEN', {2: 'N11ScopedColors5GREENE'})\n    check('enumerator', 'ScopedColors::BLUE', {2: 'N11ScopedColors4BLUEE'})\n\n    # Scoped enum struct with enumerators\n    check('enum', 'enum struct ScopedColorsStruct { RED = 1, GREEN, BLUE }', {2: '16ScopedColorsStruct'})\n    check('enumerator', 'ScopedColorsStruct::RED = 1', {2: 'N16ScopedColorsStruct3REDE'})\n    check('enumerator', 'ScopedColorsStruct::GREEN', {2: 'N16ScopedColorsStruct5GREENE'})\n    check('enumerator', 'ScopedColorsStruct::BLUE', {2: 'N16ScopedColorsStruct4BLUEE'})\n", "def test_xrefrole_parsing():\n        role = cppDomain.CPPXRefRole()\n        config = {\n            'cpp_id_attributes': [],\n            'cpp_paren_attributes': [],\n        }\n        refnode = addnodes.pending_xref()\n        env = pytest.config._tmpdir_env  # Access the temporary Sphinx environment\n        title, target = role.process_link(env, refnode, False, target, target)\n        assert title == expected\n        assert target == target\n\n    check_xrefrole('class::method', 'class::method')\n    check_xrefrole('namespace::class', 'namespace::class')\n    check_xrefrole('void f()', 'void f')\n    check_xrefrole('operator<<', 'operator<<')\n    check_xrefrole('std::vector<int>::iterator', 'std::vector<int>::iterator')\n"], "sample_949": ["def test_warning_for_unknown_document(app, status, warning):\n    app.config.man_pages = [('unknown_doc', 'unknown', 'Unknown Doc', 'Author', 1)]\n    app.build()\n    assert '\"man_pages\" config value references unknown document unknown_doc' in warning.getvalue()\n", "def test_man_show_urls(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n\n    # Check that URLs are shown in the output when man_show_urls is enabled\n    assert 'http://example.com' in content\n", "def test_man_show_urls(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert r'URLs' in content\n    assert r'http://example.com' in content\n", "def test_man_show_urls(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert r'URL: http://sphinx-doc.org/' in content\n", "def test_no_man_pages_config_value(app, status, warning):\n    app.config.man_pages = []\n    with pytest.warns(UserWarning, match=r'no \"man_pages\" config value found; no manual pages will be written'):\n        app.builder.init()\n"], "sample_950": ["def test_parse_annotation_edge_cases(app):\n    # Invalid annotation\n    doctree = _parse_annotation(\"InvalidAnnotation\", app.env)\n    assert_node(doctree, ([pending_xref, \"InvalidAnnotation\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"InvalidAnnotation\")\n\n    # Nested types with space\n    doctree = _parse_annotation(\"Dict[ str , List[ Tuple[int , str] ] ]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_type_to_xref():\n    app = Mock()\n    app.env.ref_context = {'py:module': 'testmodule', 'py:class': 'TestClass'}\n    app.env.config.python_use_unqualified_type_names = True\n\n    # Test with standard type\n    xref = type_to_xref('int', app.env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='int')\n    assert len(xref) == 2\n    assert_node(xref[0], pending_xref_condition, condition='resolved', text='int')\n    assert_node(xref[1], pending_xref_condition, condition='*', text='int')\n\n    # Test with qualified type\n    xref = type_to_xref('testmodule.TestClass', app.env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='testmodule.TestClass')\n    assert len(xref) == 2\n    assert_node(xref[0], pending_xref_condition, condition='resolved', text='TestClass')\n    assert_node(xref[1], pending_xref_condition, condition='*', text='testmodule.TestClass')\n\n    # Test with None type\n    xref = type_to_xref('None', app.env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n    assert len(xref) == 2\n    assert_node(xref[0], pending_xref_condition, condition='resolved', text='None')\n    assert_node(xref[1], pending_xref_condition, condition='*', text='None')\n\n    # Test without environment\n    xref = type_to_xref('int')\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='int')\n    assert len(xref) == 1\n    assert_node(xref[0], nodes.Text, text='int')\n\n    # Test with python_use_unqualified_type_names set to False\n    app.env.config.python_use_unqualified_type_names = False\n    xref = type_to_xref('testmodule.TestClass', app.env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reft", "def test_pyobject_class_signature(app):\n    text = \".. py:class:: MyClass\\n   :module: mymodule\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_addname, \"mymodule.\"],\n                                                    [desc_name, \"MyClass\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"class\",\n                domain=\"py\", objtype=\"class\", noindex=False)\n    assert 'mymodule.MyClass' in app.env.domains['py'].data['objects']\n    assert app.env.domains['py'].data['objects']['mymodule.MyClass'] == \\\n           ('index', 'mymodule.MyClass', 'class', False)\n", "def test_parse_annotation_with_special_cases(app):\n    doctree = _parse_annotation(\"List[Tuple[int, Union[str, None]]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"None\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"Optional[Dict[str, Any]]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Optional\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"Any\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"Callable[..., int]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Callable\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [desc_sig_punctuation, \"...\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    doctree = _parse_annotation(\"Union[int, str, None]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Union\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \", \"],\n                          [pending_xref, \"None\"],\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_type_to_xref():\n    env = Mock()\n    env.ref_context = {'py:module': 'test_module', 'py:class': 'TestClass'}\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    # Case 1: Default use\n    xref = type_to_xref('int', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='int')\n    assert_node(xref[0], nodes.Text, text='int')\n\n    # Case 2: None type should use 'obj' reftype\n    xref = type_to_xref('None', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n    assert_node(xref[0], nodes.Text, text='None')\n\n    # Case 3: With python_use_unqualified_type_names set to True\n    env.config.python_use_unqualified_type_names = True\n    xref = type_to_xref('test_module.TestClass', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='test_module.TestClass')\n    assert_node(xref[0], pending_xref_condition, condition='resolved', text='TestClass')\n    assert_node(xref[1], pending_xref_condition, condition='*', text='test_module.TestClass')\n"], "sample_951": ["def test_ispartial():\n        pass\n\n    p = functools.partial(func, 1)\n    assert inspect.ispartial(p) is True\n\n    pm = functools.partialmethod(func, 1)\n    assert inspect.ispartial(pm) is True\n\n    assert inspect.ispartial(func) is False\n    assert inspect.ispartial(1) is False\n    assert inspect.ispartial(None) is False\n", "def test_isenumclass():\n    class MyEnum(enum.Enum):\n        VALUE1 = 1\n        VALUE2 = 2\n\n    class MyInt(int):\n        pass\n\n    assert inspect.isenumclass(MyEnum) is True\n    assert inspect.isenumclass(MyInt) is False\n    assert inspect.isenumclass(enum.Enum) is True\n    assert inspect.isenumclass(1) is False\n    assert inspect.isenumclass(\"string\") is False\n", "def test_getall():\n    class TestModule:\n        __all__ = ['foo', 'bar']\n\n    class InvalidAllModule:\n        __all__ = ['foo', 1]\n\n    class NoAllModule:\n        pass\n\n    assert inspect.getall(TestModule) == ['foo', 'bar']\n    assert inspect.getall(NoAllModule) is None\n\n    with pytest.raises(ValueError):\n        inspect.getall(InvalidAllModule)\n", "def test_ispartial():\n        pass\n\n    partial_func = functools.partial(normal_func, 1)\n    partialmethod_func = functools.partialmethod(normal_func, 1)\n\n    assert inspect.ispartial(partial_func) is True\n    assert inspect.ispartial(partialmethod_func) is True\n    assert inspect.ispartial(normal_func) is False\n", "def test_getall():\n    module_with_all = types.ModuleType('module_with_all')\n    module_with_all.__all__ = ['a', 'b', 'c']\n    assert inspect.getall(module_with_all) == ['a', 'b', 'c']\n\n    module_without_all = types.ModuleType('module_without_all')\n    assert inspect.getall(module_without_all) is None\n\n    module_with_invalid_all = types.ModuleType('module_with_invalid_all')\n    module_with_invalid_all.__all__ = ['a', 1, 'b']\n    with pytest.raises(ValueError):\n        inspect.getall(module_with_invalid_all)\n\n    not_a_module = object()\n    assert inspect.getall(not_a_module) is None\n"], "sample_952": ["def test_getall():\n    class Foo:\n        __all__ = ['a', 'b', 'c']\n\n    class Bar:\n        pass\n\n    class Baz:\n        __all__ = ['a', 1, {}]\n\n    assert inspect.getall(Foo) == ['a', 'b', 'c']\n    assert inspect.getall(Bar) is None\n\n    with pytest.raises(ValueError):\n        inspect.getall(Baz)\n", "def test_getargspec():\n        \"\"\"Sample function for testing getargspec\"\"\"\n        pass\n\n    argspec = inspect.getargspec(sample_func)\n    assert argspec.args == ['a', 'b', 'c', 'd']\n    assert argspec.varargs == 'args'\n    assert argspec.varkw == 'kwargs'\n    assert argspec.defaults == (1, 2)\n    assert argspec.kwonlyargs == ['c']\n    assert argspec.kwdefaults == {'d': 2}\n    assert argspec.annotations == {}\n", "def test_unwrap_all():\n        pass\n\n    partial_func = functools.partial(func)\n    wrapped_func = functools.wraps(func)(partial_func)\n\n    class Foo:\n        @staticmethod\n            pass\n\n        @classmethod\n            pass\n\n    assert inspect.unwrap_all(wrapped_func) is func\n    assert inspect.unwrap_all(Foo.static_meth) is Foo.static_meth.__func__\n    assert inspect.unwrap_all(Foo.class_meth) is Foo.class_meth.__func__\n", "def test_getall():\n    class ModuleWithAll:\n        __all__ = [\"foo\", \"bar\"]\n\n    class ModuleWithoutAll:\n        pass\n\n    class ModuleWithInvalidAll:\n        __all__ = [\"foo\", 42]\n\n    assert inspect.getall(ModuleWithAll) == [\"foo\", \"bar\"]\n    assert inspect.getall(ModuleWithoutAll) is None\n\n    with pytest.raises(ValueError):\n        inspect.getall(ModuleWithInvalidAll)\n\n", "def test_getannotations():\n    class Foo:\n        __annotations__ = {'x': int, 'y': str}\n\n    class Bar:\n        pass\n\n    class Baz:\n        __annotations__ = \"invalid\"\n\n    assert inspect.getannotations(Foo) == {'x': int, 'y': str}\n    assert inspect.getannotations(Bar) == {}\n    assert inspect.getannotations(Baz) == {}\n"], "sample_953": ["def test_quickstart_with_invalid_path(tempdir):\n    answers = {\n        'Root path': '/invalid/path',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    \n    with pytest.raises(SystemExit):\n        qs.ask_user(d)\n", "def test_is_path():\n    valid_path = tempdir / 'valid_dir'\n    invalid_path = tempdir / 'invalid_dir'\n    valid_path.mkdir()\n\n    # Valid path should not raise ValidationError\n    assert qs.is_path(valid_path)\n\n    # Invalid path should raise ValidationError\n    with pytest.raises(qs.ValidationError):\n        qs.is_path(invalid_path)\n", "def test_valid_dir():\n    # Test valid directory\n    valid_path = tempdir / 'valid_dir'\n    valid_path.mkdir()\n    d = {\n        'path': valid_path,\n        'sep': False,\n        'dot': '_',\n        'suffix': '.rst',\n        'master': 'index',\n    }\n    assert qs.valid_dir(d) is True\n\n    # Test invalid directory: file instead of dir\n    invalid_path = tempdir / 'invalid_dir'\n    invalid_path.write_text('This is a file')\n    d['path'] = invalid_path\n    assert qs.valid_dir(d) is False\n\n    # Test invalid directory: existing reserved files\n    reserved_path = tempdir / 'reserved_dir'\n    reserved_path.mkdir()\n    (reserved_path / 'conf.py').write_text('conf content')\n    d['path'] = reserved_path\n    assert qs.valid_dir(d) is False\n\n    # Test invalid directory: existing Makefile and make.bat\n    makefile_path = tempdir / 'makefile_dir'\n    makefile_path.mkdir()\n    (makefile_path / 'Makefile').write_text('makefile content')\n    (makefile_path / 'make.bat').write_text('batchfile content')\n    d['path'] = makefile_path\n    assert qs.valid_dir(d) is False\n", "def test_valid_dir():\n    # Test cases to check valid_dir function\n    test_cases = [\n        ({\"path\": \"non_existent_directory\", \"sep\": False, \"dot\": \"_\", \"master\": \"index\", \"suffix\": \".rst\"}, True),\n        ({\"path\": \"invalid_file\", \"sep\": False, \"dot\": \"_\", \"master\": \"index\", \"suffix\": \".rst\"}, False),\n        ({\"path\": \"existing_directory_with_files\", \"sep\": False, \"dot\": \"_\", \"master\": \"index\", \"suffix\": \".rst\"}, False),\n        ({\"path\": \"existing_directory_with_files\", \"sep\": True, \"dot\": \"_\", \"master\": \"index\", \"suffix\": \".rst\"}, False),\n        ({\"path\": \"empty_directory\", \"sep\": False, \"dot\": \"_\", \"master\": \"index\", \"suffix\": \".rst\"}, True),\n    ]\n\n    for case, expected in test_cases:\n        assert qs.valid_dir(case) == expected\n", "def test_valid_dir():\n    d = {\n        'path': '/non/existent/dir',\n        'sep': False,\n        'dot': '_',\n        'suffix': '.rst',\n        'master': 'index'\n    }\n    assert qs.valid_dir(d) is True\n\n    d = {\n        'path': tempdir,\n        'sep': False,\n        'dot': '_',\n        'suffix': '.rst',\n        'master': 'index'\n    }\n    qs.term_input = mock_input({'Root path': tempdir})\n    qs.ask_user(d)\n    (tempdir / 'conf.py').write_text('')\n    assert qs.valid_dir(d) is False\n\n    d = {\n        'path': tempdir,\n        'sep': True,\n        'dot': '_',\n        'suffix': '.rst',\n        'master': 'index'\n    }\n    (tempdir / 'source').mkdir()\n    assert qs.valid_dir(d) is True\n    (tempdir / 'source' / 'conf.py').write_text('')\n    assert qs.valid_dir(d) is False\n"], "sample_954": ["def test_nested_inline_transform(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'nested.1').read_text()\n\n    # Check that nested inline nodes are flattened correctly\n    assert '<strong>foo=</strong><emphasis>1</emphasis>' in content\n    assert '<strong>&bar=</strong><emphasis>2</emphasis>' in content\n", "def test_nested_inline_transform(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # Test the NestedInlineTransform by checking if the nested elements are flattened\n    assert '<strong>foo=</strong><emphasis>1</emphasis><strong>&bar=</strong><emphasis>2</emphasis>' in content\n", "def test_nested_inline_transform(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # Check that nested inline nodes are flattened correctly\n    assert '\\n\\\\fBfoo=\\\\fP\\\\fI1\\\\fP' in content\n    assert '\\n\\\\fB&bar=\\\\fP\\\\fI2\\\\fP' in content\n", "def test_admonition_labels(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    \n    for label in ['note', 'warning', 'caution', 'attention', 'danger', 'error', 'hint', 'important', 'tip']:\n        translated_label = app.config.language.labels.get(label, label).upper()\n        assert f'.SH {translated_label}\\n' in content\n\n", "def test_nested_inline(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # Ensure that nested inline nodes are flattened correctly\n    assert '.B foo=' in content\n    assert '\\\\fI1\\\\fP' in content\n    assert '.B &bar=' in content\n    assert '\\\\fI2\\\\fP' in content\n"], "sample_955": ["def test_unparse_function_definitions(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_unparse_function_and_class_defs(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_unparse_comprehensions_and_ifexpr(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_additional(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_unparse_additional(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n"], "sample_956": ["def test_read_from_url(monkeypatch):\n    \"\"\"Test _read_from_url to ensure it fetches data correctly.\"\"\"\n    class MockResponse:\n            self.url = url\n            self.raw = self\n            self.content = content\n            self.status_code = 200\n\n            if self.status_code != 200:\n                raise Exception(\"HTTP Error\", self.status_code)\n\n            if decode_content:\n                return self.content.decode()\n            return self.content\n\n        return MockResponse(url, b'Test data content')\n\n    monkeypatch.setattr('sphinx.util.requests.get', mock_requests_get)\n\n    config = mock.Mock()\n    config.intersphinx_timeout = 10\n\n    url = 'https://example.com/objects.inv'\n    result = _read_from_url(url, config)\n    \n    assert result.read() == 'Test data content'\n    assert result.url == url\n", "def test_fetch_inventory_local_file(tempdir, app, status, warning):\n    \"\"\"Test fetch_inventory function with a local file\"\"\"\n    inv_file = tempdir / 'local_inventory'\n    inv_file.write_bytes(inventory_v2)\n\n    intersphinx_setup(app)\n\n    # fetch inventory from a local file\n    invdata = fetch_inventory(app, '', str(inv_file))\n    assert invdata is not None\n    assert 'py:module' in invdata\n    assert 'module2' in invdata['py:module']\n    assert invdata['py:module']['module2'] == ('foo', '2.0', 'foo.html#module-module2', '-')\n", "def test_fetch_inventory_exception_handling(app, status, warning):\n    \"\"\"Test fetch_inventory handles exceptions gracefully.\"\"\"\n    intersphinx_setup(app)\n\n    # Test handling of a general exception during fetch\n    with mock.patch('sphinx.ext.intersphinx._read_from_url', side_effect=Exception(\"Test exception\")):\n        with pytest.raises(Exception) as excinfo:\n            fetch_inventory(app, 'http://hostname/', 'http://hostname/' + INVENTORY_FILENAME)\n        assert \"Test exception\" in str(excinfo.value)\n\n    # Test handling of non-existent local file\n    with pytest.raises(OSError):\n        fetch_inventory(app, 'http://hostname/', '/nonexistent/path/' + INVENTORY_FILENAME)\n\n    # Test handling of unsupported inventory version\n    with mock.patch('sphinx.ext.intersphinx._read_from_url') as _read_from_url:\n        _read_from_url().readline.return_value = b'# Unsupported inventory version'\n        with pytest.raises(ValueError, match=\"unknown or unsupported inventory version\"):\n            fetch_inventory(app, 'http://hostname/', 'http://hostname/' + INVENTORY_FILENAME)\n", "def test_fetch_inventory_file_not_found(app, status, warning, tempdir):\n    \"\"\"Test fetch_inventory with a non-existent local file.\"\"\"\n    intersphinx_setup(app)\n\n    non_existent_file = tempdir / 'non_existent_inventory.inv'\n    with pytest.raises(Exception, match=\"intersphinx inventory '.*' not fetchable due to FileNotFoundError\"):\n        fetch_inventory(app, 'http://hostname/', non_existent_file)\n", "def test_fetch_inventory_local_file(tempdir, app, status, warning):\n    \"\"\"Test fetch_inventory with a local file path.\"\"\"\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n\n    fetch_inventory(app, tempdir, str(inv_file))\n    assert 'intersphinx inventory has moved' not in status.getvalue()\n    assert InventoryFile.load.call_args[0][1] == tempdir\n"], "sample_957": ["def test_get_type_hints():\n        pass\n\n    assert get_type_hints(sample_function) == {'a': int, 'b': str, 'return': None}\n    \n    class SampleClass:\n            pass\n    \n    assert get_type_hints(SampleClass.method) == {'x': int, 'return': str}\n\n    globalns = {\"MyInt\": MyInt}\n    localns = {}\n    assert get_type_hints(\"MyInt\", globalns, localns) == {'MyInt': MyInt}\n", "def test_get_type_hints():\n    from sphinx.util.typing import get_type_hints\n\n    # Test with regular class\n    class TestClass:\n        attr: int\n\n    assert get_type_hints(TestClass) == {'attr': int}\n\n    # Test with function\n        return 42\n\n    assert get_type_hints(test_func) == {'param': str, 'return': int}\n\n    # Test with method\n    class AnotherClass:\n            return 42\n\n    assert get_type_hints(AnotherClass.method) == {'param': str, 'return': int}\n\n    # Test with global and local namespaces\n    globalns = {'int': float}\n    localns = {'str': bool}\n\n        return True\n\n    assert get_type_hints(another_func, globalns, localns) == {'param': float, 'return': bool}\n\n    # Test with a broken class\n    class BrokenClass:\n        __annotations__ = 'invalid'\n\n    assert get_type_hints(BrokenClass) == {}\n", "def test_restify_nested_type_hints():\n    NestedType = List[Dict[str, Tuple[int, Optional[float]]]]\n    assert restify(NestedType) == (\":class:`~typing.List`\\\\ \"\n                                   \"[:class:`~typing.Dict`\\\\ \"\n                                   \"[:class:`str`, :class:`~typing.Tuple`\\\\ \"\n                                   \"[:class:`int`, :obj:`~typing.Optional`\\\\ [:class:`float`]]]]\")\n", "def test_get_type_hints():\n    from sphinx.util.typing import get_type_hints\n\n        pass\n\n    class MyClass:\n            pass\n\n    globalns = globals()\n    localns = locals()\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': None}\n    assert get_type_hints(MyClass.method, globalns, localns) == {'a': int, 'b': str, 'return': None}\n    assert get_type_hints(MyClass) == {}\n", "def test_restify_invalid_builtin_classes():\n    assert restify(Struct) == \":class:`struct.Struct`\"\n    assert restify(TracebackType) == \":class:`types.TracebackType`\"\n"], "sample_958": ["def test_domain_c_ast_basic_parsing():\n    # Test basic parsing of C declarations and ensure correct AST construction.\n    \n    # Test function parsing\n    func_decl = \"int my_function(int a, float b)\"\n    func_id_dict = {2: \"1my_function\"}\n    check(\"function\", func_decl, func_id_dict)\n    \n    # Test struct parsing\n    struct_decl = \"struct MyStruct { int a; float b; }\"\n    struct_id_dict = {2: \"1MyStruct\"}\n    check(\"struct\", struct_decl, struct_id_dict)\n    \n    # Test union parsing\n    union_decl = \"union MyUnion { int a; float b; }\"\n    union_id_dict = {2: \"1MyUnion\"}\n    check(\"union\", union_decl, union_id_dict)\n    \n    # Test enum parsing\n    enum_decl = \"enum MyEnum { A, B, C }\"\n    enum_id_dict = {2: \"1MyEnum\"}\n    check(\"enum\", enum_decl, enum_id_dict)\n    \n    # Test macro parsing\n    macro_decl = \"#define MY_MACRO(x) (x * x)\"\n    macro_id_dict = {2: \"1MY_MACRO\"}\n    check(\"macro\", macro_decl, macro_id_dict)\n    \n    # Test typedef parsing\n    typedef_decl = \"typedef int MyInt\"\n    typedef_id_dict = {2: \"1MyInt\"}\n    check(\"type\", typedef_decl, typedef_id_dict, key='typedef')\n    \n    # Test nested struct parsing\n    nested_struct_decl = \"struct Outer { struct Inner { int a; }; int b; }\"\n    nested_struct_id_dict = {2: \"1Outer\"}\n    check(\"struct\", nested_struct_decl, nested_struct_id_dict)\n", "def test_domain_c_ast_identifier():\n    identifier = ASTIdentifier(\"test_identifier\")\n    assert identifier.identifier == \"test_identifier\"\n    assert identifier == ASTIdentifier(\"test_identifier\")\n    assert identifier != ASTIdentifier(\"other_identifier\")\n    assert not identifier.is_anon()\n    assert str(identifier) == \"test_identifier\"\n    assert identifier.get_display_string() == \"test_identifier\"\n\n    anon_identifier = ASTIdentifier(\"@anon_identifier\")\n    assert anon_identifier.identifier == \"@anon_identifier\"\n    assert anon_identifier.is_anon()\n    assert str(anon_identifier) == \"@anon_identifier\"\n    assert anon_identifier.get_display_string() == \"[anonymous]\"\n", "def test_domain_c_ast_expressions():\n        class Config:\n            c_id_attributes = [\"id_attr\"]\n            c_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(expr, location=None, config=Config())\n        ast = parser._parse_expression()\n        res = str(ast)\n        assert res == expected, f\"Expected: {expected}, but got: {res}\"\n\n    # Primary expressions\n    exprCheck(\"true\", \"true\")\n    exprCheck(\"false\", \"false\")\n    exprCheck(\"42\", \"42\")\n    exprCheck(\"3.14\", \"3.14\")\n    exprCheck(\"'a'\", \"'a'\")\n    exprCheck('\"hello\"', '\"hello\"')\n\n    # Unary expressions\n    exprCheck(\"+42\", \"+42\")\n    exprCheck(\"-42\", \"-42\")\n    exprCheck(\"!true\", \"!true\")\n    exprCheck(\"~0xFF\", \"~0xFF\")\n\n    # Binary expressions\n    exprCheck(\"42 + 58\", \"42 + 58\")\n    exprCheck(\"42 - 18\", \"42 - 18\")\n    exprCheck(\"42 * 2\", \"42 * 2\")\n    exprCheck(\"42 / 2\", \"42 / 2\")\n    exprCheck(\"42 % 10\", \"42 % 10\")\n    exprCheck(\"42 & 0xF\", \"42 & 0xF\")\n    exprCheck(\"42 | 0xF\", \"42 | 0xF\")\n    exprCheck(\"42 ^ 0xF\", \"42 ^ 0xF\")\n    exprCheck(\"42 && 1\", \"42 && 1\")\n    exprCheck(\"42 || 0\", \"42 || 0\")\n    exprCheck(\"42 == 42\", \"42 == 42\")\n    exprCheck(\"42 != 0\", \"42 != 0\")\n    exprCheck(\"42 < 50\", \"42 < 50\")\n    exprCheck(\"42 > 10\", \"42 > 10\")\n    exprCheck(\"42 <= 42\", \"42 <= 42\")\n    exprCheck(\"42 >= 42\", \"42 >= 42\")\n\n    # Conditional expressions\n    exprCheck(\"a ? b : c\", \"a ? b : c\")\n", "def test_c_ast_literal_parsing():\n        class Config:\n            c_id_attributes = []\n            c_paren_attributes = []\n        parser = DefinitionParser(input, location=None, config=Config())\n        literal = parser._parse_literal()\n        assert str(literal) == expected_output, f\"Failed to parse {input}\"\n\n    # Test boolean literals\n    check_literal(\"true\", \"true\")\n    check_literal(\"false\", \"false\")\n    \n    # Test integer literals\n    check_literal(\"42\", \"42\")\n    check_literal(\"0x2A\", \"0x2A\")\n    check_literal(\"075\", \"075\")\n\n    # Test character literals\n    check_literal(\"'a'\", \"'a'\")\n    check_literal(\"L'\\\\n'\", \"L'\\\\n'\")\n    check_literal(\"u'\\\\u0061'\", \"u'\\\\u0061'\")\n\n    # Test floating-point literals\n    check_literal(\"3.14\", \"3.14\")\n    check_literal(\"2.71e-1\", \"2.71e-1\")\n    check_literal(\"0x1.91eb851eb851fp+1\", \"0x1.91eb851eb851fp+1\")\n\n    # Test string literals\n    check_literal('\"hello\"', '\"hello\"')\n    check_literal('L\"wide string\"', 'L\"wide string\"')\n", "def test_c_ast_identifier():\n    id1 = ASTIdentifier(\"myIdentifier\")\n    id2 = ASTIdentifier(\"myIdentifier\")\n    id3 = ASTIdentifier(\"otherIdentifier\")\n\n    assert id1 == id2\n    assert id1 != id3\n    assert str(id1) == \"myIdentifier\"\n    assert id1.get_display_string() == \"myIdentifier\"\n\n    anon_id = ASTIdentifier(\"@anonymous\")\n    assert anon_id.is_anon()\n    assert anon_id.get_display_string() == \"[anonymous]\"\n\n    try:\n        ASTIdentifier(\"\")\n        assert False, \"Expected AssertionError for empty identifier\"\n    except AssertionError:\n        pass\n"], "sample_959": ["def test_domain_cpp_ast_decltype():\n    check('function', 'decltype(auto) f()', {1: 'f', 2: '1fv'})\n    check('function', 'decltype(int) f()', {1: 'f', 2: '1fv'})\n    check('function', 'auto f() -> decltype(auto)', {1: 'f', 2: '1fv'})\n    check('function', 'auto f() -> decltype(int)', {1: 'f', 2: '1fv'})\n    check('type', 'decltype(auto) v', {1: 'v', 2: '1v'}, key='typedef')\n    check('type', 'decltype(int) v', {1: 'v', 2: '1v'}, key='typedef')\n    check('type', 'decltype(auto) v = 42', {1: 'v', 2: '1v'}, key='typedef')\n    check('type', 'decltype(int) v = 42', {1: 'v', 2: '1v'}, key='typedef')\n", "def test_domain_cpp_ast_function_with_attributes():\n    check('function', '[[nodiscard]] int f()', {1: 'f', 2: '1fv'}, output='[[nodiscard]] int f()')\n    check('function', '[[nodiscard(\"reason\")]] int f()', {1: 'f', 2: '1fv'},\n          output='[[nodiscard(\"reason\")]] int f()')\n    check('function', '[[deprecated]] void g()', {1: 'g', 2: '1gv'}, output='[[deprecated]] void g()')\n    check('function', '[[deprecated(\"use new function\")]] void g()', {1: 'g', 2: '1gv'},\n          output='[[deprecated(\"use new function\")]] void g()')\n    check('function', '[[maybe_unused]] void h()', {1: 'h', 2: '1hv'}, output='[[maybe_unused]] void h()')\n", "def test_domain_cpp_ast_functions_with_template_parameters():\n    check('function', 'template<typename T> void f(T)', {2: 'I0E1f1T'})\n    check('function', 'template<int N> void f()', {2: 'I_iE1fv'})\n    check('function', 'template<typename T, int N> void f(T)', {2: 'I0_iE1f1T'})\n    check('function', 'template<template<typename> typename T> void f()', {2: 'II0E0E1fv'})\n    check('function', 'template<template<typename> typename T, int N> void f()', {2: 'II0E0_iE1fv'})\n    check('function', 'template<int N = 42> void f()', {2: 'I_iE1fv'})\n    check('function', 'template<int... Ns> void f()', {2: 'I_DpiE1fv'})\n    check('function', 'template<int N = 42, typename T = double> void f()', {2: 'I_i0E1fv'})\n    check('function', 'template<int... Ns, typename... Ts> void f()', {2: 'I_DpiDpE1fv'})\n    check('function', 'template<typename T, int N = 42> void f(T)', {2: 'I0_iE1f1T'})\n    check('function', 'template<typename... Ts, int... Ns> void f()', {2: 'IDp_DpiE1fv'})\n", "def test_domain_cpp_ast_template_parameters_and_qualifiers():\n    check('function', 'template<typename T> void f(T a)', {1: 'f__T', 2: '1f1T'})\n    check('function', 'template<typename T> void f(T a) noexcept', {1: 'f__T', 2: '1f1TC'})\n    check('function', 'template<typename T> void f(T a) noexcept(true)', {1: 'f__T', 2: '1f1TC'})\n    check('function', 'template<typename T> void f(T a) const', {1: 'f__T', 2: '1f1TC'})\n    check('function', 'template<typename T> void f(T a) const noexcept', {1: 'f__T', 2: '1f1TC'})\n    check('function', 'template<typename T> void f(T a) &', {1: 'f__T', 2: '1f1TR'})\n    check('function', 'template<typename T> void f(T a) &&', {1: 'f__T', 2: '1f1TO'})\n    check('function', 'template<typename T> void f(T a) const &', {1: 'f__T', 2: '1f1TCR'})\n    check('function', 'template<typename T> void f(T a) const &&', {1: 'f__T', 2: '1f1TCO'})\n    check('function', 'template<typename T> void f(T a) volatile', {1: 'f__T', 2: '1f1TV'})\n    check('function', 'template<typename T> void f(T a) volatile &', {1: 'f__T', 2: '1f1TVR'})\n    check('function', 'template<typename T> void f(T a) volatile &&', {1: 'f__T', 2: '1f1TVO'})\n    check('function', 'template<typename T> void f(T a) const volatile', {1: 'f__T', 2: '1f1TVC'})\n    check('function', 'template<typename T> void f(T a) const volatile &', {1: 'f__T', 2: '1f1TVCR'})\n    check('function', 'template<typename", "def test_domain_cpp_ast_template_params():\n    # Test unconstrained template parameter\n    check('type', 'template<typename T> {key}A', {2: 'I0E1A'}, key='using')\n    check('function', 'template<typename T> void f()', {2: 'I0E1fv', 4: 'I0E1fvv'})\n\n    # Test template parameter with default value\n    check('type', 'template<typename T = int> {key}A', {2: 'I0E1A'}, key='using')\n    check('function', 'template<typename T = int> void f()', {2: 'I0E1fv', 4: 'I0E1fvv'})\n\n    # Test variadic template parameter\n    check('type', 'template<typename... Ts> {key}A', {2: 'IDpE1A'}, key='using')\n    check('function', 'template<typename... Ts> void f()', {2: 'IDpE1fv', 4: 'IDpE1fvv'})\n\n    # Test nested template parameter\n    check('type', 'template<template<typename> class T> {key}A', {2: 'II0E0E1A'}, key='using')\n    check('function', 'template<template<typename> class T> void f()', {2: 'II0E0E1fv', 4: 'II0E0E1fvv'})\n\n    # Test non-type template parameter\n    check('type', 'template<int N> {key}A', {2: 'I_iE1A'}, key='using')\n    check('function', 'template<int N> void f()', {2: 'I_iE1fv', 4: 'I_iE1fvv'})\n\n    # Test non-type template parameter with default value\n    check('type', 'template<int N = 5> {key}A', {2: 'I_iE1A'}, key='using')\n    check('function', 'template<int N = 5> void f()', {2: 'I_iE1fv', 4: 'I_iE1fvv'})\n\n    # Test template template parameter with default value\n    check('type', 'template<template<typename> class T = Default> {key}A', {2: 'II0E0E"], "sample_960": ["def test_pymodule_signature(app):\n    text = (\".. py:module:: example\\n\"\n            \"   :synopsis: This is an example module.\\n\"\n            \"   :platform: Any\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index))\n    assert 'example' in domain.modules\n    module = domain.modules['example']\n    assert module.docname == 'index'\n    assert module.synopsis == 'This is an example module.'\n    assert module.platform == 'Any'\n    assert module.deprecated\n", "def test_pyfunction_with_custom_prefix(app):\n    text = (\".. py:module:: custom_module\\n\"\n            \".. py:function:: custom_prefix_func\\n\"\n            \"   :module: different_module\\n\"\n            \"   :async:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"async \"],\n                                                    [desc_addname, \"different_module.\"],\n                                                    [desc_name, \"custom_prefix_func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[2], addnodes.index,\n                entries=[('single', 'custom_prefix_func() (in module different_module)', \n                          'different_module.custom_prefix_func', '', None)])\n\n    assert 'different_module.custom_prefix_func' in domain.objects\n    assert domain.objects['different_module.custom_prefix_func'] == ('index', \n                                                                     'different_module.custom_prefix_func', \n                                                                     'function', False)\n", "def test_parse_arglist():\n    arglist = 'a, b=1, *args, c, d=2, **kwargs'\n    doctree = _parse_arglist(arglist)\n    assert_node(doctree, addnodes.desc_parameterlist)\n    assert_node(doctree[0], addnodes.desc_parameter, [addnodes.desc_sig_name, \"a\"])\n    assert_node(doctree[1], addnodes.desc_parameter, ([addnodes.desc_sig_name, \"b\"],\n                                                      [addnodes.desc_sig_operator, \"=\"],\n                                                      [nodes.inline, \"1\"]))\n    assert_node(doctree[2], addnodes.desc_parameter, ([addnodes.desc_sig_operator, \"*\"],\n                                                      [addnodes.desc_sig_name, \"args\"]))\n    assert_node(doctree[3], addnodes.desc_parameter, [addnodes.desc_sig_name, \"c\"])\n    assert_node(doctree[4], addnodes.desc_parameter, ([addnodes.desc_sig_name, \"d\"],\n                                                      [addnodes.desc_sig_operator, \"=\"],\n                                                      [nodes.inline, \"2\"]))\n    assert_node(doctree[5], addnodes.desc_parameter, ([addnodes.desc_sig_operator, \"**\"],\n                                                      [addnodes.desc_sig_name, \"kwargs\"]))\n", "def test_type_to_xref():\n    env = Mock()\n    env.ref_context = {'py:module': 'module1', 'py:class': 'Class1'}\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    # Test type conversion with fully qualified name\n    xref_node = type_to_xref('module1.Class1.Type', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='class', reftarget='module1.Class1.Type')\n    assert_node(xref_node[0], nodes.Text, 'module1.Class1.Type')\n\n    # Test type conversion with unqualified name\n    env.config.python_use_unqualified_type_names = True\n    xref_node = type_to_xref('module1.Class1.Type', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='class', reftarget='module1.Class1.Type')\n    assert_node(xref_node[0], pending_xref_condition, condition='resolved')\n    assert_node(xref_node[0][0], nodes.Text, 'Type')\n    assert_node(xref_node[1], pending_xref_condition, condition='*')\n    assert_node(xref_node[1][0], nodes.Text, 'module1.Class1.Type')\n\n    # Test 'None' type conversion\n    xref_node = type_to_xref('None', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n    assert_node(xref_node[0], nodes.Text, 'None')\n\n    # Test without environment\n    xref_node = type_to_xref('module1.Class1.Type')\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='class', reftarget='module1.Class1.Type')\n    assert_node(xref_node[0], nodes.Text, 'module1.Class1.Type')\n", "def test_pyfunction_with_kwargs(app):\n    text = \".. py:function:: hello(name: str, **kwargs: Any) -> None\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"None\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [nodes.inline, pending_xref, \"str\"])],\n                                      [desc_parameter, ([desc_sig_operator, \"**\"],\n                                                        [desc_sig_name, \"kwargs\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"Any\"])])])\n"], "sample_961": ["def test_type_to_xref():\n    env = Mock()\n    env.ref_context = {'py:module': 'module1', 'py:class': 'Class'}\n    env.config.python_use_unqualified_type_names = False\n\n    xref = type_to_xref('int', env)\n    assert isinstance(xref, pending_xref)\n    assert xref['reftarget'] == 'int'\n    assert xref['reftype'] == 'class'\n    assert_node(xref[0], nodes.Text, text='int')\n\n    xref = type_to_xref('module1.Class', env)\n    assert isinstance(xref, pending_xref)\n    assert xref['reftarget'] == 'module1.Class'\n    assert xref['reftype'] == 'class'\n    assert_node(xref[0], nodes.Text, text='module1.Class')\n\n    env.config.python_use_unqualified_type_names = True\n    xref = type_to_xref('module1.Class', env)\n    assert isinstance(xref, pending_xref)\n    assert xref['reftarget'] == 'module1.Class'\n    assert xref['reftype'] == 'class'\n    assert len(xref) == 2\n    assert_node(xref[0], pending_xref_condition, condition='resolved')\n    assert_node(xref[0][0], nodes.Text, text='Class')\n    assert_node(xref[1], pending_xref_condition, condition='*')\n    assert_node(xref[1][0], nodes.Text, text='module1.Class')\n", "def test_pyxrefmixin_make_xrefs(app):\n    env = Mock(ref_context={'py:module': 'module', 'py:class': 'Class'}, config=Mock(python_use_unqualified_type_names=True))\n    inliner = Mock()\n    location = Mock()\n    pyxrefmixin = PyXrefMixin()\n\n    # Test case: simple class reference\n    target = \"ClassName\"\n    result = pyxrefmixin.make_xrefs('class', 'py', target, nodes.emphasis, None, env, inliner, location)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftype'] == 'class'\n    assert result[0]['reftarget'] == 'ClassName'\n    assert result[0]['refspecific'] is True\n\n    # Test case: type reference with union operator\n    target = \"int | str\"\n    result = pyxrefmixin.make_xrefs('class', 'py', target, nodes.emphasis, None, env, inliner, location)\n    assert len(result) == 3\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == 'int'\n    assert result[1].astext() == ' | '\n    assert isinstance(result[2], pending_xref)\n    assert result[2]['reftarget'] == 'str'\n\n    # Test case: nested class reference with unqualified names\n    target = \"module.Class.SubClass\"\n    env.config.python_use_unqualified_type_names = True\n    result = pyxrefmixin.make_xrefs('class', 'py', target, nodes.emphasis, None, env, inliner, location)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == 'module.Class.SubClass'\n    assert result[0]['refspecific'] is True\n\n    # Test case: split content node\n    target = \"List[int]\"\n    contnode = nodes.Text(target)\n    result = pyxrefmixin.make_xrefs('class', 'py', target, nodes.emphasis, contnode, env, inliner, location)\n    assert len(result) == 4\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == 'List'\n    assert result[1].as", "def test_parse_annotation_with_literals(app):\n    doctree = _parse_annotation(\"Literal['a', 'b', 'c']\", app.env)\n    assert_node(doctree, ([pending_xref, \"Literal\"],\n                          [desc_sig_punctuation, \"[\"],\n                          \"'a'\",\n                          [desc_sig_punctuation, \", \"],\n                          \"'b'\",\n                          [desc_sig_punctuation, \", \"],\n                          \"'c'\",\n                          [desc_sig_punctuation, \"]\"]))\n    \n    doctree = _parse_annotation(\"Literal[3.14, 2.71, 1.0]\", app.env)\n    assert_node(doctree, ([pending_xref, \"Literal\"],\n                          [desc_sig_punctuation, \"[\"],\n                          \"3.14\",\n                          [desc_sig_punctuation, \", \"],\n                          \"2.71\",\n                          [desc_sig_punctuation, \", \"],\n                          \"1.0\",\n                          [desc_sig_punctuation, \"]\"]))\n", "def test_type_to_xref():\n    env = Mock(spec=BuildEnvironment)\n    env.ref_context = {'py:module': 'mymodule', 'py:class': 'MyClass'}\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = True\n\n    # Case: type is 'None', should use 'obj' role\n    xref_node = type_to_xref('None', env)\n    assert_node(xref_node, [pending_xref_condition, \"None\"])\n    assert xref_node['reftype'] == 'obj'\n    assert xref_node['reftarget'] == 'None'\n\n    # Case: regular class, should use 'class' role\n    xref_node = type_to_xref('MyClass', env)\n    assert_node(xref_node, [pending_xref_condition, \"MyClass\"])\n    assert xref_node['reftype'] == 'class'\n    assert xref_node['reftarget'] == 'MyClass'\n\n    # Case: full module path with unqualified type names config\n    xref_node = type_to_xref('mymodule.MyClass', env)\n    assert_node(xref_node, [pending_xref_condition, \"MyClass\"])\n    assert xref_node['reftype'] == 'class'\n    assert xref_node['reftarget'] == 'mymodule.MyClass'\n\n    # Case: without BuildEnvironment\n    xref_node = type_to_xref('AnotherClass')\n    assert_node(xref_node, [nodes.Text, \"AnotherClass\"])\n    assert xref_node['reftype'] == 'class'\n    assert xref_node['reftarget'] == 'AnotherClass'\n", "def test_type_to_xref():\n    env = Mock()\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = True\n    env.ref_context = {'py:module': 'example', 'py:class': 'ExampleClass'}\n\n    # Test for 'None' type\n    xref_node = type_to_xref('None', env)\n    assert_node(xref_node, pending_xref, reftype='obj', reftarget='None')\n    assert_node(xref_node[0], pending_xref_condition, condition='resolved', ([nodes.Text, \"None\"],))\n    assert_node(xref_node[1], pending_xref_condition, condition='*', ([nodes.Text, \"None\"],))\n\n    # Test for qualified type\n    xref_node = type_to_xref('example.QualifiedType', env)\n    assert_node(xref_node, pending_xref, reftype='class', reftarget='example.QualifiedType')\n    assert_node(xref_node[0], pending_xref_condition, condition='resolved', ([nodes.Text, \"QualifiedType\"],))\n    assert_node(xref_node[1], pending_xref_condition, condition='*', ([nodes.Text, \"example.QualifiedType\"],))\n\n    # Test for unqualified type names disabled\n    env.config.python_use_unqualified_type_names = False\n    xref_node = type_to_xref('example.AnotherType', env)\n    assert_node(xref_node, pending_xref, reftype='class', reftarget='example.AnotherType')\n    assert_node(xref_node[0], nodes.Text, \"example.AnotherType\")\n"], "sample_962": ["def test_mock_object_methods():\n    obj = _MockObject()\n    assert repr(obj) == '_MockObject'\n    assert len(obj) == 0\n    assert 'key' not in obj\n    assert list(iter(obj)) == []\n    assert obj['key'].__display_name__ == '_MockObject.key'\n    assert obj.attr.__display_name__ == '_MockObject.attr'\n    assert obj() is not None\n    assert obj.__mro_entries__(()) == (_MockObject,)\n", "def test_mock_object_behaviors():\n    mock_obj = _MockObject()\n\n    # Test __len__ method\n    assert len(mock_obj) == 0\n\n    # Test __contains__ method\n    assert \"key\" not in mock_obj\n\n    # Test __iter__ method\n    assert list(iter(mock_obj)) == []\n\n    # Test __mro_entries__ method\n    assert mock_obj.__mro_entries__(()) == (_MockObject,)\n\n    # Test __getitem__ method\n    item = mock_obj[\"key\"]\n    assert isinstance(item, _MockObject)\n    assert item.__name__ == \"key\"\n\n    # Test __getattr__ method\n    attr = mock_obj.some_attribute\n    assert isinstance(attr, _MockObject)\n    assert attr.__name__ == \"some_attribute\"\n\n    # Test __call__ method\n    called_obj = mock_obj(1, 2, 3)\n    assert isinstance(called_obj, _MockObject)\n    assert called_obj.__sphinx_decorator_args__ == (1, 2, 3)\n\n    # Test __repr__ method\n    assert repr(mock_obj) == \"_MockObject\"\n\n    # Test subclassing\n    class SubMock(_MockObject):\n        pass\n\n    sub_mock = SubMock()\n    assert isinstance(sub_mock, SubMock)\n    assert sub_mock.__class__.__name__ == \"SubMock\"\n", "def test_mock_object():\n    obj = _MockObject()\n    assert repr(obj) == '_MockObject'\n    assert len(obj) == 0\n    assert 'test' not in obj\n    assert list(iter(obj)) == []\n    assert obj.non_existent_attr.__class__.__name__ == '_MockObject'\n    assert obj['key'].__class__.__name__ == '_MockObject'\n    assert callable(obj)\n", "def test_mockobject_methods():\n    obj = _MockObject()\n\n    # Test __len__\n    assert len(obj) == 0\n\n    # Test __contains__\n    assert 'key' not in obj\n\n    # Test __iter__\n    assert list(iter(obj)) == []\n\n    # Test __getitem__\n    item = obj['key']\n    assert isinstance(item, _MockObject)\n    assert item.__name__ == 'key'\n\n    # Test __getattr__\n    attr = obj.some_attr\n    assert isinstance(attr, _MockObject)\n    assert attr.__name__ == 'some_attr'\n\n    # Test __call__\n    call = obj('arg1', 'arg2')\n    assert isinstance(call, _MockObject)\n    assert call.__sphinx_decorator_args__ == ('arg1', 'arg2')\n\n    # Test __repr__\n    assert repr(obj) == '_MockObject'\n", "def test_ismock():\n    with mock(['unknown']):\n        import unknown\n        mocked_obj = unknown.secret.Class()\n        assert ismock(mocked_obj) is True\n        assert ismock(unknown) is True\n        assert ismock(unknown.secret) is True\n        assert ismock(str) is False\n        assert ismock(int) is False\n"], "sample_963": ["def test_restify_forward_ref_py36():\n    if sys.version_info < (3, 7):\n        ForwardRef = typing.ForwardRef\n        assert restify(ForwardRef(\"MyClass1\")) == \":py:class:`MyClass1`\"\n", "def test_restify_invalid_builtin_classes():\n    assert restify(Struct) == \":py:class:`struct.Struct`\"\n    assert restify(TracebackType) == \":py:class:`types.TracebackType`\"\n", "def test_get_type_hints():\n        return True\n\n        return True\n\n    class ClassWithHints:\n        x: int\n        y: str\n\n            return True\n\n    class ClassWithoutHints:\n            return True\n\n    assert get_type_hints(func_with_hints) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(func_without_hints) == {}\n    assert get_type_hints(ClassWithHints) == {'x': int, 'y': str}\n    assert get_type_hints(ClassWithHints.method_with_hints) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(ClassWithoutHints) == {}\n    assert get_type_hints(ClassWithoutHints.method_without_hints) == {}\n", "def test_get_type_hints():\n    from sphinx.util.typing import get_type_hints\n\n    class ExampleClass:\n            pass\n\n        return b\n\n    assert get_type_hints(ExampleClass.method) == {'x': int, 'y': str}\n    assert get_type_hints(example_function) == {'a': int, 'b': str}\n", "def test_get_type_hints():\n        return True\n\n    class MyClass:\n        a: int\n        b: str\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(MyClass) == {'a': int, 'b': str}\n\n    class LocalClass:\n            return self\n\n    assert get_type_hints(LocalClass.method, {'LocalClass': LocalClass}) == {'x': LocalClass, 'return': LocalClass}\n\n    # Broken class with invalid annotations\n    class BrokenClass:\n        __annotations__ = {'x': 'invalid'}\n\n    assert get_type_hints(BrokenClass) == {'x': 'invalid'}\n"], "sample_964": ["def test_pymodule():\n    text = (\".. py:module:: example.module\\n\"\n            \"   :platform: Linux\\n\"\n            \"   :synopsis: Example module\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index))\n    assert 'example.module' in domain.modules\n    assert domain.modules['example.module'] == ('index', 'example.module', 'Example module', 'Linux', True)\n", "def test_type_to_xref():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    env.ref_context = {'py:module': 'test_module', 'py:class': 'TestClass'}\n    \n    # Test with class reference\n    xref_node = type_to_xref('int', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='class', reftarget='int', **{'py:module': 'test_module', 'py:class': 'TestClass'})\n    assert_node(xref_node[0], nodes.Text, \"int\")\n\n    # Test with None reference\n    xref_node = type_to_xref('None', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='obj', reftarget='None', **{'py:module': 'test_module', 'py:class': 'TestClass'})\n    assert_node(xref_node[0], nodes.Text, \"None\")\n\n    # Test with unqualified type names\n    env.config.python_use_unqualified_type_names = True\n    xref_node = type_to_xref('test_module.TestClass.inner_class', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='class', reftarget='test_module.TestClass.inner_class', **{'py:module': 'test_module', 'py:class': 'TestClass'})\n    assert_node(xref_node[0], pending_xref_condition, condition='resolved', *[\n        nodes.Text('inner_class')\n    ])\n    assert_node(xref_node[1], pending_xref_condition, condition='*', *[\n        nodes.Text('test_module.TestClass.inner_class')\n    ])\n", "def test_pyfunction_with_default_arguments(app):\n    text = (\".. py:function:: hello(name: str='world', age: int=25)\\n\"\n            \".. py:function:: greet(greeting: str='Hello', times: int=1) -> None\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    [desc_parameterlist, \n                                                     ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                                        [desc_sig_punctuation, \":\"],\n                                                                        desc_sig_space,\n                                                                        [pending_xref, \"str\"],\n                                                                        desc_sig_space,\n                                                                        [desc_sig_operator, \"=\"],\n                                                                        desc_sig_space,\n                                                                        [nodes.inline, \"'world'\"])],\n                                                      [desc_parameter, ([desc_sig_name, \"age\"],\n                                                                        [desc_sig_punctuation, \":\"],\n                                                                        desc_sig_space,\n                                                                        [pending_xref, \"int\"],\n                                                                        desc_sig_space,\n                                                                        [desc_sig_operator, \"=\"],\n                                                                        desc_sig_space,\n                                                                        [nodes.inline, \"25\"])])])],\n                                  desc_content)],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"greet\"],\n                                                    [desc_parameterlist, \n                                                     ([desc_parameter, ([desc_sig_name, \"greeting\"],\n                                                                        [desc_sig_punctuation, \":\"],\n                                                                        desc_sig_space,\n                                                                        [pending_xref, \"str\"],\n                                                                        desc_sig_space,\n                                                                        [desc_sig_operator, \"=\"],\n                                                                        desc_sig_space,\n                                                                        [nodes.inline, \"'Hello'\"])],\n                                                      [desc_parameter, ([desc_sig_name, \"times\"],\n                                                                        [desc_sig_punctuation, \":\"],\n                                                                        desc_sig_space,\n                                                                        [pending_xref, \"int\"],\n                                                                        desc_sig_space,\n                                                                        [desc_sig_operator, \"=\"],\n                                                                        desc_sig_space,\n                                                                        [nodes.inline, \"1\"])])],\n                                                    [desc_returns, pending_xref, \"None\"])],\n                                  desc_content)]))\n    assert_node(doctree[0], addnodes.index, entries=[('pair', 'built-in function; hello()', 'hello', '', None)])\n    assert_node(doctree[3], addnodes.index, entries=[('pair', 'built-in function; greet()', 'greet', '', None)])\n", "def test_type_to_xref():\n    env = Mock()\n    env.ref_context = {'py:module': 'test_module', 'py:class': 'TestClass'}\n    env.config = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    xref_node = type_to_xref('str', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='class', reftarget='str', **{'py:module': 'test_module', 'py:class': 'TestClass'})\n    assert_node(xref_node[0], nodes.Text, 'str')\n\n    env.config.python_use_unqualified_type_names = True\n    xref_node = type_to_xref('test_module.TestClass', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='class', reftarget='test_module.TestClass', **{'py:module': 'test_module', 'py:class': 'TestClass'})\n    assert_node(xref_node[0], pending_xref_condition, condition='resolved')\n    assert_node(xref_node[1], pending_xref_condition, condition='*')\n    assert_node(xref_node[0][0], nodes.Text, 'TestClass')\n    assert_node(xref_node[1][0], nodes.Text, 'test_module.TestClass')\n\n    xref_node = type_to_xref('None', env)\n    assert_node(xref_node, pending_xref, refdomain='py', reftype='obj', reftarget='None', **{'py:module': 'test_module', 'py:class': 'TestClass'})\n    assert_node(xref_node[0], nodes.Text, 'None')\n", "def test_pyfunction_signature_with_various_annotations(app):\n    text = (\".. py:function:: func1(a: int, b: str = 'default', *args: Any, \"\n            \"**kwargs: Union[int, str]) -> None\\n\"\n            \".. py:function:: func2(x: List[int], y: Dict[str, Any]) -> Optional[str]\\n\")\n    doctree = restructuredtext.parse(app, text)\n    \n    # func1 signature with various annotations\n    assert_node(doctree[2], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[2][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"a\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        desc_sig_space,\n                                                        [pending_xref, \"int\"])],\n                                      [desc_parameter, ([desc_sig_name, \"b\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        desc_sig_space,\n                                                        [pending_xref, \"str\"],\n                                                        desc_sig_space,\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"'default'\"])],\n                                      [desc_parameter, ([desc_sig_operator, \"*\"],\n                                                        [desc_sig_name, \"args\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        desc_sig_space,\n                                                        [pending_xref, \"Any\"])],\n                                      [desc_parameter, ([desc_sig_operator, \"**\"],\n                                                        [desc_sig_name, \"kwargs\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        desc_sig_space,\n                                                        [pending_xref, \"Union\"],\n                                                        [desc_sig_punctuation, \"[\"],\n                                                        [pending_xref, \"int\"],\n                                                        [desc_sig_punctuation, \",\"],\n                                                        desc_sig_space,\n                                                        [pending_xref, \"str\"],\n                                                        [desc_sig_punctuation, \"]\"])])])\n    assert_node(doctree[2][0][1][-1], ([desc_returns, pending_xref, \"None\"]))\n\n    # func2 signature with various annotations\n    assert_node(doctree[5], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[5][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"x\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        desc_sig_space,\n                                                        [pending_xref, \"List\"],\n                                                        ["], "sample_965": ["def test_getargspec():\n        pass\n\n    spec = inspect.getargspec(func)\n    assert spec.args == ['a', 'b', 'c']\n    assert spec.varargs == 'args'\n    assert spec.varkw == 'kwargs'\n    assert spec.defaults == (1,)\n    assert spec.kwonlyargs == []\n    assert spec.kwonlydefaults is None\n    assert spec.annotations == {}\n\n        pass\n\n    spec = inspect.getargspec(func_with_annotations)\n    assert spec.args == ['a', 'b', 'c']\n    assert spec.varargs == 'args'\n    assert spec.varkw == 'kwargs'\n    assert spec.defaults == (1,)\n    assert spec.kwonlyargs == []\n    assert spec.kwonlydefaults is None\n    assert spec.annotations == {'a': int, 'b': str, 'args': list, 'kwargs': dict, 'return': None}\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return \"default\"\n\n    @foo.register(int)\n        return \"int\"\n\n    @foo.register(str)\n        return \"str\"\n\n    assert inspect.is_singledispatch_function(foo) is True\n    assert inspect.is_singledispatch_function(foo.dispatch(int)) is False\n    assert inspect.is_singledispatch_function(foo.dispatch(str)) is False\n    assert inspect.is_singledispatch_function(\"not_a_function\") is False\n", "def test_getargspec():\n        return a + b\n\n    spec = inspect.getargspec(sample_func)\n    assert spec.args == ['a', 'b']\n    assert spec.varargs == 'args'\n    assert spec.varkw == 'kwargs'\n    assert spec.defaults == (2,)\n    assert spec.kwonlyargs == []\n    assert spec.kwonlydefaults is None\n    assert spec.annotations == {}\n    \n        pass\n\n    spec = inspect.getargspec(annotated_func)\n    assert spec.args == ['x', 'y']\n    assert spec.defaults == (\"default\",)\n    assert spec.annotations == {'x': int, 'y': str, 'return': None}\n", "def test_getargspec():\n        pass\n\n    spec = inspect.getargspec(func)\n    assert spec.args == ['a', 'b', 'c', 'd']\n    assert spec.varargs == 'e'\n    assert spec.varkw == 'f'\n    assert spec.defaults == (1, 2)\n    assert spec.kwonlyargs == []\n    assert spec.kwdefaults is None\n    assert spec.annotations == {}\n", "def test_getall():\n    class TestModule:\n        __all__ = ['foo', 'bar']\n\n    class InvalidModule:\n        __all__ = ['foo', 123]\n\n    assert inspect.getall(TestModule) == ['foo', 'bar']\n    assert inspect.getall(object()) is None\n\n    with pytest.raises(ValueError):\n        inspect.getall(InvalidModule)\n"], "sample_966": ["def test_pyvariable_signature(app):\n    text = (\".. py:variable:: var\\n\"\n            \"   :type: int\\n\"\n            \"   :value: 42\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"var\"],\n                                                    [desc_annotation, ([desc_sig_punctuation, ':'],\n                                                                       desc_sig_space,\n                                                                       [pending_xref, \"int\"])],\n                                                    [desc_annotation, (desc_sig_space,\n                                                                       [desc_sig_punctuation, '='],\n                                                                       desc_sig_space,\n                                                                       \"42\")]\n                                                    )],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"variable\",\n                domain=\"py\", objtype=\"variable\", noindex=False)\n    assert_node(doctree[1][0][1][2][2], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    assert 'var' in domain.objects\n    assert domain.objects['var'] == ('index', 'var', 'variable', False)\n", "def test_type_to_xref(app):\n    env = app.env\n    xref = type_to_xref(\"int\", env)\n    assert_node(xref, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    assert_node(xref[0], nodes.Text, \"int\")\n\n    xref = type_to_xref(\"None\", env)\n    assert_node(xref, pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n    assert_node(xref[0], nodes.Text, \"None\")\n\n    env.config.python_use_unqualified_type_names = True\n    xref = type_to_xref(\"module.Class\", env)\n    assert_node(xref, pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"module.Class\")\n    assert_node(xref[0], pending_xref_condition, condition=\"resolved\")\n    assert_node(xref[0][0], nodes.Text, \"Class\")\n    assert_node(xref[1], pending_xref_condition, condition=\"*\")\n    assert_node(xref[1][0], nodes.Text, \"module.Class\")\n", "def test_type_to_xref(app):\n    env = app.env\n    env.config.python_use_unqualified_type_names = True\n    env.ref_context['py:module'] = 'example'\n    env.ref_context['py:class'] = 'ExampleClass'\n\n    # Test for type 'None'\n    node = type_to_xref('None', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n    assert_node(node[0], pending_xref_condition, condition='resolved')\n    assert node[0].astext() == 'None'\n\n    # Test for regular type with unqualified names\n    node = type_to_xref('example.submodule.ClassName', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='example.submodule.ClassName')\n    assert_node(node[0], pending_xref_condition, condition='resolved')\n    assert node[0].astext() == 'ClassName'\n\n    # Test for regular type without unqualified names\n    env.config.python_use_unqualified_type_names = False\n    node = type_to_xref('example.submodule.ClassName', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='example.submodule.ClassName')\n    assert node[0].astext() == 'example.submodule.ClassName'\n", "def test_pymodule_with_options(app):\n    text = (\".. py:module:: example.module\\n\"\n            \"   :synopsis: This is an example module\\n\"\n            \"   :platform: Unix\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target, addnodes.index))\n    \n    # Check if module is noted correctly\n    assert 'example.module' in domain.modules\n    module_entry = domain.modules['example.module']\n    assert module_entry.docname == 'index'\n    assert module_entry.synopsis == 'This is an example module'\n    assert module_entry.platform == 'Unix'\n    assert module_entry.deprecated is True\n\n    # Check the index entries\n    assert_node(doctree[1], addnodes.index, entries=[\n        ('pair', 'module; example.module', 'module-example.module', '', None)\n    ])\n", "def test_pyclassmethod_with_decorator(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:classmethod:: meth\\n\"\n            \"   .. py:decoratormethod:: deco\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'meth() (Class class method)', 'Class.meth', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, (\"classmethod\", desc_sig_space)],\n                                                     [desc_name, \"meth\"],\n                                                     [desc_parameterlist, ()])],\n                                   [desc_content, ()]))\n    assert 'Class.meth' in domain.objects\n    assert domain.objects['Class.meth'] == ('index', 'Class.meth', 'method', False)\n\n    assert_node(doctree[1][1][2], addnodes.index,\n                entries=[('single', 'deco() (Class method)', 'Class.deco', '', None)])\n    assert_node(doctree[1][1][3], ([desc_signature, ([desc_addname, \"@\"],\n                                                     [desc_name, \"deco\"])],\n                                   [desc_content, ()]))\n    assert 'Class.deco' in domain.objects\n    assert domain.objects['Class.deco'] == ('index', 'Class.deco', 'method', False)\n"], "sample_967": ["def test_mathjax_inline_format(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<span class=\"math notranslate nohighlight\">\\\\(a^2 + b^2 = c^2\\\\)</span>')\n    assert re.search(html, content, re.S)\n", "def test_mathjax_async_option(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<script async=\"async\" integrity=\"sha384-9876543210\" '\n            'src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\">'\n            '</script>' in content)\n", "def test_mathjax_inline(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<span class=\"math notranslate nohighlight\">\\\\(a\\^2\\+b\\^2=c\\^2\\\\)</span>')\n    assert re.search(html, content, re.S)\n", "def test_mathjax_inline_delimiters(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<span class=\"math notranslate nohighlight\">\\$E=mc\\^2\\$</span>')\n    assert re.search(html, content, re.S)\n", "def test_mathjax_inline_config(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    inline_math = re.compile(r'<span class=\"math notranslate nohighlight\">\\[.*?\\]</span>')\n    assert inline_math.search(content)\n"], "sample_968": ["def test_parse_annotation_with_env():\n    \"\"\"Test _parse_annotation with BuildEnvironment object.\"\"\"\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    # Case 1: Basic annotation\n    annotation = \"int\"\n    doctree = _parse_annotation(annotation, env)\n    assert_node(doctree, ([pending_xref, \"int\"],))\n    assert_node(doctree[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n\n    # Case 2: Nested annotation\n    annotation = \"List[Tuple[int, str]]\"\n    doctree = _parse_annotation(annotation, env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    # Case 3: Multiple Nested annotation\n    annotation = \"Dict[str, List[Tuple[int, str]]]\"\n    doctree = _parse_annotation(annotation, env)\n    assert_node(doctree, ([pending_xref, \"Dict\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"Tuple\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"int\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [pending_xref, \"str\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"],\n                          [desc_sig_punctuation, \"]\"]))\n\n    # Case 4: Annotations with Literal\n    annotation = \"Literal['foo', 'bar', 1, True]\"\n    doctree = _parse_annotation(annotation, env)\n    assert_node(doctree, ([pending_xref, \"Literal\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [desc_sig_literal_string, \"'foo'\"],\n                          [desc_sig_punctuation, \",\"],\n                          desc_sig_space,\n                          [desc_sig_literal_string, \"'bar'\"],\n", "def test_pytypedfield_make_xref():\n    env = Mock()\n    inliner = Mock()\n    location = Mock()\n    py_typed_field = PyTypedField('param', label='Parameters', rolename='class', names=('param',), typenames=('type',))\n\n    refnode = py_typed_field.make_xref(\n        rolename='class', domain='py', target='int', innernode=nodes.strong,\n        contnode=nodes.Text('int'), env=env, inliner=inliner, location=location)\n\n    assert isinstance(refnode, pending_xref)\n    assert refnode['reftarget'] == 'int'\n    assert refnode['reftype'] == 'class'\n    assert refnode['refdomain'] == 'py'\n    assert_node(refnode[0], nodes.strong, 'int')\n\n    refnode = py_typed_field.make_xref(\n        rolename='class', domain='py', target='None', innernode=nodes.strong,\n        contnode=nodes.Text('None'), env=env, inliner=inliner, location=location)\n\n    assert isinstance(refnode, pending_xref)\n    assert refnode['reftarget'] == 'None'\n    assert refnode['reftype'] == 'obj'\n    assert refnode['refdomain'] == 'py'\n    assert_node(refnode[0], nodes.strong, 'None')\n", "def test_pytypedfield_signature(app):\n    text = (\".. py:class:: Foo\\n\"\n            \"\\n\"\n            \"   .. py:method:: bar(x: int, y: str = 'default') -> None\\n\"\n            \"   .. py:method:: baz(a: List[int], b: Tuple[str, ...]) -> Dict[str, int]\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"Foo\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n\n    # check `bar` method\n    assert_node(doctree[1][1][1], addnodes.index,\n                entries=[('single', 'bar() (Foo method)', 'Foo.bar', '', None)])\n    assert_node(doctree[1][1][2], ([desc_signature, ([desc_name, \"bar\"],\n                                                     [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"x\"],\n                                                                                             [desc_sig_punctuation, \":\"],\n                                                                                             desc_sig_space,\n                                                                                             [pending_xref, \"int\"])],\n                                                                                     [desc_parameter, ([desc_sig_name, \"y\"],\n                                                                                                       [desc_sig_punctuation, \":\"],\n                                                                                                       desc_sig_space,\n                                                                                                       [pending_xref, \"str\"],\n                                                                                                       desc_sig_space,\n                                                                                                       [desc_sig_operator, \"=\"],\n                                                                                                       desc_sig_space,\n                                                                                                       [nodes.inline, \"'default'\"])])],\n                                                     [desc_returns, pending_xref, \"None\"])],\n                                   [desc_content, ()]))\n    assert 'Foo.bar' in domain.objects\n    assert domain.objects['Foo.bar'] == ('index', 'Foo.bar', 'method', False)\n\n    # check `baz` method\n    assert_node(doctree[1][1][3], addnodes.index,\n                entries=[('single', 'baz() (Foo method)', 'Foo.baz', '', None)])\n    assert_node(doctree[1][1][4], ([desc_signature, ([desc_name, \"baz\"],\n                                                     [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"a\"],\n                                                                                             [desc_sig_punctuation, \":\"],\n                                                                                             desc_sig_space,\n                                                                                             [pending_xref", "def test_type_to_xref():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    env.ref_context = {'py:module': 'example.module', 'py:class': 'ExampleClass'}\n\n    # Test when target is 'None'\n    xref = type_to_xref('None', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='obj', reftarget='None')\n    assert_node(xref[0], pending_xref_condition, condition='resolved')\n    assert_node(xref[1], pending_xref_condition, condition='*')\n\n    # Test when target is a class with a dot prefix\n    xref = type_to_xref('.ExampleClass', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='ExampleClass', refspecific=True)\n    assert_node(xref[0], nodes.Text, 'ExampleClass')\n\n    # Test when target is a class with a tilde prefix\n    xref = type_to_xref('~example.module.ExampleClass', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='example.module.ExampleClass')\n    assert_node(xref[0], nodes.Text, 'ExampleClass')\n\n    # Test when target is a class without any prefix\n    xref = type_to_xref('example.module.ExampleClass', env)\n    assert_node(xref, pending_xref, refdomain='py', reftype='class', reftarget='example.module.ExampleClass')\n    assert_node(xref[0], nodes.Text, 'example.module.ExampleClass')\n\n    # Test when python_use_unqualified_type_names is True\n    env.config.python_use_unqualified_type_names = True\n    xref = type_to_xref('example.module.ExampleClass', env)\n    assert_node(xref[0], pending_xref_condition, condition='resolved')\n    assert_node(xref[1], pending_xref_condition, condition='*')\n    assert_node(xref[0][0], nodes.Text, 'ExampleClass')\n    assert_node(xref[1][0], nodes.Text, 'example.module.ExampleClass')\n", "def test_pyfunction_with_multiple_annotations(app):\n    text = (\".. py:function:: hello(name: str | None, age: int) -> str | None\\n\"\n            \"   :async:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"async\", desc_sig_space)],\n                                                    [desc_name, \"hello\"],\n                                                    [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                                                            [desc_sig_punctuation, \":\"],\n                                                                                            desc_sig_space,\n                                                                                            [desc_sig_name, ([pending_xref, \"str\"],\n                                                                                                             desc_sig_space,\n                                                                                                             [desc_sig_punctuation, \"|\"],\n                                                                                                             desc_sig_space,\n                                                                                                             [pending_xref, \"None\"])])],\n                                                                          [desc_parameter, ([desc_sig_name, \"age\"],\n                                                                                            [desc_sig_punctuation, \":\"],\n                                                                                            desc_sig_space,\n                                                                                            [pending_xref, \"int\"])])],\n                                                    [desc_returns, ([pending_xref, \"str\"],\n                                                                    desc_sig_space,\n                                                                    [desc_sig_punctuation, \"|\"],\n                                                                    desc_sig_space,\n                                                                    [pending_xref, \"None\"])])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1][0][1][3], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n    assert_node(doctree[1][0][1][0][1][5], pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n    assert_node(doctree[1][0][1][1][0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n    assert_node(doctree[1][0][1][1][2], pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n"], "sample_969": ["def test_get_type_hints():\n        return True\n\n    class SampleClass:\n        attribute: int\n\n            pass\n\n    assert get_type_hints(sample_function) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(SampleClass) == {'attribute': int}\n    assert get_type_hints(SampleClass.method) == {'x': float, 'return': None}\n", "def test_get_type_hints():\n    from sphinx.util.typing import get_type_hints\n\n        pass\n\n    class MyClass:\n        x: int\n        y: str\n\n    class BrokenClass:\n        __annotations__ = None\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': None}\n    assert get_type_hints(MyClass) == {'x': int, 'y': str}\n    assert get_type_hints(BrokenClass) == {}\n", "def test_get_type_hints():\n        pass\n\n    class SampleClass:\n        attr: int\n\n            return str(x)\n\n    globalns = {'int': int, 'str': str}\n    localns = {'float': float}\n\n    assert get_type_hints(sample_function) == {'a': int, 'b': str, 'return': None}\n    assert get_type_hints(SampleClass) == {'attr': int}\n    assert get_type_hints(SampleClass.method, globalns, localns) == {'self': SampleClass, 'x': float, 'return': str}\n    \n    # Test with an invalid object\n    assert get_type_hints(\"not_a_function\") == {}\n", "def test_get_type_hints():\n        return True\n\n    class MyClass:\n        x: int\n        y: str\n\n    assert get_type_hints(foo) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(MyClass) == {'x': int, 'y': str}\n\n    # Test with broken type hints\n    class BrokenClass:\n        __annotations__ = {'x': 'unknown'}\n\n    assert get_type_hints(BrokenClass) == {'x': 'unknown'}\n\n    # Test with ForwardRef\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef\n        class ForwardRefClass:\n            x: 'ForwardRef'\n\n        assert get_type_hints(ForwardRefClass) == {'x': ForwardRef('ForwardRef')}\n    else:\n        assert get_type_hints(\"x: 'ForwardRef'\") == {}\n", "def test_get_type_hints():\n        return True\n\n    class SampleClass:\n        attr: int\n\n            return str(x)\n\n    assert get_type_hints(func) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(SampleClass) == {'attr': int}\n    assert get_type_hints(SampleClass.method) == {'x': int, 'return': str}\n"], "sample_970": ["def test_isdescriptor_custom_descriptor():\n    class CustomDescriptor:\n            return 'value'\n\n            pass\n\n            pass\n\n    class Foo:\n        attr = CustomDescriptor()\n\n    assert inspect.isdescriptor(Foo.attr) is True\n    assert inspect.isdescriptor(CustomDescriptor) is False\n", "def test_getorigbases():\n    class Foo:\n        pass\n\n    class Bar(Foo):\n        __orig_bases__ = (Foo,)\n\n    class Baz(Bar):\n        __orig_bases__ = None\n\n    class Qux(Foo):\n        pass\n\n    assert inspect.getorigbases(Foo) is None\n    assert inspect.getorigbases(Bar) == (Foo,)\n    assert inspect.getorigbases(Baz) is None\n    assert inspect.getorigbases(Qux) is None\n\n    with pytest.raises(AttributeError):\n        class Quux:\n            __orig_bases__ = \"invalid\"\n        inspect.getorigbases(Quux)\n", "def test_getannotations():\n    class Foo:\n        __annotations__ = {'a': int, 'b': str}\n        \n    class Bar:\n        pass\n\n    assert inspect.getannotations(Foo) == {'a': int, 'b': str}\n    assert inspect.getannotations(Bar) == {}\n    assert inspect.getannotations(None) == {}\n", "def test_isNewType():\n    if sys.version_info >= (3, 10):\n        MyNewType = typing.NewType('MyNewType', int)\n        assert inspect.isNewType(MyNewType) is True\n        assert inspect.isNewType(int) is False\n    else:\n        class FakeNewType:\n            __module__ = 'typing'\n            __qualname__ = 'NewType.<locals>.new_type'\n        \n        assert inspect.isNewType(FakeNewType) is True\n        assert inspect.isNewType(int) is False\n", "def test_isNewType():\n    if sys.version_info >= (3, 10):\n        NewType = typing.NewType('NewType', int)\n        assert inspect.isNewType(NewType) is True\n    else:\n        NewType = typing.NewType('NewType', int)\n        assert inspect.isNewType(NewType) is True\n\n    assert inspect.isNewType(int) is False\n    assert inspect.isNewType(str) is False\n"], "sample_971": ["def test_memory_handler(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    mem_handler = logging.MemoryHandler()\n    logger.addHandler(mem_handler)\n\n    # Log some messages\n    logger.info('message1')\n    logger.warning('message2')\n    logger.error('message3')\n\n    # Ensure messages are in memory handler's buffer\n    assert any(record.getMessage() == 'message1' for record in mem_handler.buffer)\n    assert any(record.getMessage() == 'WARNING: message2' for record in mem_handler.buffer)\n    assert any(record.getMessage() == 'ERROR: message3' for record in mem_handler.buffer)\n\n    # Test flushTo method\n    mem_handler.flushTo(logger)\n    assert 'message1' in status.getvalue()\n    assert 'WARNING: message2' in warning.getvalue()\n    assert 'ERROR: message3' in warning.getvalue()\n\n    # Test clear method\n    mem_handler.clear()\n    assert not mem_handler.buffer\n", "def test_memory_handler(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    memhandler = logging.MemoryHandler()\n    logger.addHandler(memhandler)\n\n    logger.warning('message1')\n    logger.error('message2')\n\n    assert len(memhandler.buffer) == 2\n    assert memhandler.buffer[0].getMessage() == 'message1'\n    assert memhandler.buffer[1].getMessage() == 'message2'\n\n    logger.removeHandler(memhandler)\n    memhandler.flushTo(logger)\n\n    assert 'WARNING: message1' in warning.getvalue()\n    assert 'ERROR: message2' in warning.getvalue()\n", "def test_memory_handler_clear_and_flush(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    memhandler = logging.MemoryHandler()\n    logger.addHandler(memhandler)\n\n    # Log messages to memory handler\n    logger.info('message1')\n    logger.warning('message2')\n    assert len(memhandler.buffer) == 2\n\n    # Test clear method\n    memhandler.clear()\n    assert len(memhandler.buffer) == 0\n\n    # Log messages again\n    logger.info('message3')\n    logger.warning('message4')\n    assert len(memhandler.buffer) == 2\n\n    # Test flushTo method\n    flushed_logs = []\n    class TestLogger(logging.Logger):\n            flushed_logs.append(record.msg)\n    \n    test_logger = TestLogger('test_logger')\n    memhandler.flushTo(test_logger)\n    assert len(memhandler.buffer) == 0\n    assert 'message3' in flushed_logs\n    assert 'message4' in flushed_logs\n", "def test_memory_handler(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.pending_logging() as memhandler:\n        logger.info('info message')\n        logger.warning('warning message')\n\n    buffered_logs = memhandler.clear()\n    assert len(buffered_logs) == 2\n    assert buffered_logs[0].msg == 'info message'\n    assert buffered_logs[1].msg == 'warning message'\n", "def test_memory_handler(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    mem_handler = logging.MemoryHandler()\n    logger.addHandler(mem_handler)\n\n    logger.info('message1')\n    logger.warning('message2')\n\n    buffer = mem_handler.clear()\n    assert len(buffer) == 2\n    assert buffer[0].getMessage() == 'message1'\n    assert buffer[1].getMessage() == 'message2'\n"], "sample_972": ["def test_get_type_hints():\n    from sphinx.util.inspect import safe_getattr\n\n        return True\n\n    class DummyClass:\n        attribute: int\n\n    class DummyBrokenClass:\n        __annotations__ = None\n\n    # Check type hints for a function\n    result = get_type_hints(dummy_function)\n    assert result == {'param1': int, 'param2': str, 'return': bool}\n\n    # Check type hints for a class\n    result = get_type_hints(DummyClass)\n    assert result == {'attribute': int}\n\n    # Check fallback to __annotations__ attribute\n    result = get_type_hints(DummyBrokenClass)\n    assert result == {}\n\n    # Check safe getattr fallback\n    result = get_type_hints(\"not_a_function_or_class\")\n    assert result == {}\n\n    # Check safe getattr when __annotations__ is not present\n    obj_without_annotations = object()\n    result = get_type_hints(obj_without_annotations)\n    assert result == {}\n", "def test_get_type_hints():\n    from sphinx.util.typing import get_type_hints\n    from sphinx.util.inspect import safe_getattr\n\n    class SampleClass:\n            return str(param)\n\n    assert get_type_hints(SampleClass.method) == {\"param\": int, \"return\": str}\n\n    class ForwardRefClass:\n            return SampleClass()\n\n    assert get_type_hints(ForwardRefClass.method, globals(), locals()) == {\"param\": SampleClass, \"return\": SampleClass}\n\n    class InvalidClass:\n        __annotations__ = {\"param\": \"UnknownType\"}\n\n    assert get_type_hints(InvalidClass) == {\"param\": \"UnknownType\"}\n\n    class BrokenClass:\n        pass\n\n    BrokenClass.__annotations__ = None\n    assert get_type_hints(BrokenClass) == {}\n", "def test_restify_forward_ref_in_function():\n    from typing import ForwardRef\n\n        pass\n\n    assert restify(func.__annotations__['a']) == \":py:class:`str`\"\n", "def test_get_type_hints():\n        return True\n\n    class SampleClass:\n        a: int\n        b: str\n\n            return str(x)\n\n    assert get_type_hints(sample_function) == {'a': int, 'b': str, 'return': bool}\n    assert get_type_hints(SampleClass) == {'a': int, 'b': str}\n    assert get_type_hints(SampleClass.method) == {'x': float, 'return': str}\n\n    # test with broken type hints\n    class BrokenClass:\n        __annotations__ = {'a': 'undefined'}\n\n    assert get_type_hints(BrokenClass) == {'a': 'undefined'}\n\n    # test with ForwardRef\n    class ForwardRefClass:\n        a: 'ForwardRefType'\n\n    assert get_type_hints(ForwardRefClass) == {'a': 'ForwardRefType'}\n", "def test_stringify_TypeVar_with_bound():\n    T_bound = TypeVar('T_bound', bound=MyClass1)\n    assert stringify(T_bound) == \"tests.test_util_typing.T_bound\"\n    assert stringify(T_bound, \"smart\") == \"~tests.test_util_typing.T_bound\"\n\n    assert stringify(List[T_bound]) == \"List[tests.test_util_typing.T_bound]\"\n    assert stringify(List[T_bound], \"smart\") == \"~typing.List[~tests.test_util_typing.T_bound]\"\n\n    if sys.version_info >= (3, 10):\n        assert stringify(Union[T_bound, int]) == \"tests.test_util_typing.T_bound | int\"\n        assert stringify(Union[T_bound, int], \"smart\") == \"~tests.test_util_typing.T_bound | int\"\n    else:\n        assert stringify(Union[T_bound, int]) == \"Union[tests.test_util_typing.T_bound, int]\"\n        assert stringify(Union[T_bound, int], \"smart\") == \"~typing.Union[~tests.test_util_typing.T_bound, int]\"\n"], "sample_973": ["def test_getargspec():\n        \"\"\"Example function for testing.\"\"\"\n        pass\n\n    result = inspect.getargspec(example_function)\n\n    assert result.args == ['a', 'b']\n    assert result.varargs == 'args'\n    assert result.varkw == 'kwargs'\n    assert result.defaults == (2,)\n    assert result.kwonlyargs == ['c']\n    assert result.kwdefaults == {'c': 3}\n    assert result.annotations == {}\n", "def test_isNewType():\n    MyNewType = typing.NewType('MyNewType', int)\n    assert inspect.isNewType(MyNewType) is True  # NewType object\n    assert inspect.isNewType(int) is False       # Regular type\n    assert inspect.isNewType(\"string\") is False  # Non-type object\n", "def test_getall():\n    class MyModule:\n        __all__ = ['func1', 'func2']\n\n    class InvalidModule:\n        __all__ = ['func1', 123]\n\n    class NoAllModule:\n        pass\n\n    assert inspect.getall(MyModule) == ['func1', 'func2']\n    assert inspect.getall(NoAllModule) is None\n\n    with pytest.raises(ValueError):\n        inspect.getall(InvalidModule)\n", "def test_getargspec():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b', 'c']\n    assert argspec.varargs == 'args'\n    assert argspec.varkw == 'kwargs'\n    assert argspec.defaults == (1,)\n    assert argspec.kwonlyargs == []\n    assert argspec.kwdefaults is None\n    assert argspec.annotations == {}\n\n        pass\n\n    argspec = inspect.getargspec(annotated_func)\n    assert argspec.args == ['a', 'b', 'c']\n    assert argspec.varargs is None\n    assert argspec.varkw == 'kwargs'\n    assert argspec.defaults == (1.0,)\n    assert argspec.kwonlyargs == []\n    assert argspec.kwdefaults is None\n    assert argspec.annotations == {'a': int, 'b': str, 'c': float, 'return': None}\n", "def test_getall():\n    mod_with_all = types.ModuleType('mod_with_all')\n    mod_with_all.__all__ = ['a', 'b', 'c']\n\n    mod_without_all = types.ModuleType('mod_without_all')\n    \n    mod_with_invalid_all = types.ModuleType('mod_with_invalid_all')\n    mod_with_invalid_all.__all__ = [1, 2, 3]\n\n    assert inspect.getall(mod_with_all) == ['a', 'b', 'c']\n    assert inspect.getall(mod_without_all) is None\n\n    with pytest.raises(ValueError):\n        inspect.getall(mod_with_invalid_all)\n"], "sample_974": ["def test_ccode_MatrixElement():\n    A = MatrixSymbol('A', 3, 3)\n    assert ccode(A[1, 2]) == 'A[5]'\n    assert ccode(A[0, 0]) == 'A[0]'\n    assert ccode(A[2, 1]) == 'A[7]'\n    assert ccode(A[2, 2]) == 'A[8]'\n", "def test_ccode_Relational_with_Assignment():\n    from sympy import Eq, Le\n    expr1 = Eq(x, y)\n    assign1 = Assignment(z, expr1)\n    assert ccode(assign1) == 'z = x == y;'\n\n    expr2 = Le(x, y)\n    assign2 = Assignment(z, expr2)\n    assert ccode(assign2) == 'z = x <= y;'\n", "def test_ccode_custom_pow():\n    custom_pow = lambda base, exp: 'custom_pow(%s, %s)' % (base, exp)\n    custom_functions = {\"Pow\": custom_pow}\n    assert ccode(x**3, user_functions=custom_functions) == \"custom_pow(x, 3)\"\n    assert ccode(x**y, user_functions=custom_functions) == \"custom_pow(x, y)\"\n", "def test_ccode_MatrixElement():\n    M = MatrixSymbol('M', 3, 3)\n    expr = M[1, 2]\n    assert ccode(expr) == \"M[5]\"\n    expr = M[0, 0]\n    assert ccode(expr) == \"M[0]\"\n    expr = M[2, 1]\n    assert ccode(expr) == \"M[7]\"\n", "def test_ccode_custom_function():\n    from sympy import Function\n    custom_func = Function('custom_func')\n    custom_functions = {\"custom_func\": \"custom_c_func\"}\n    \n    expr = custom_func(x + y)\n    assert ccode(expr, user_functions=custom_functions) == \"custom_c_func(x + y)\"\n    \n    expr2 = custom_func(x * y + z)\n    assert ccode(expr2, user_functions=custom_functions) == \"custom_c_func(x*y + z)\"\n    \n    expr3 = custom_func(custom_func(x))\n    assert ccode(expr3, user_functions=custom_functions) == \"custom_c_func(custom_c_func(x))\"\n"], "sample_975": ["def test_unrad():\n    x, y = symbols('x y')\n    assert unrad(sqrt(x)*x**Rational(1, 3) + 2) == (x**5 - 64, [])\n    assert unrad(sqrt(x) + root(x + 1, 3)) == (x**3 - x**2 - 2*x - 1, [])\n    assert unrad(sqrt(x) + y, x) == (-y**2, [y, y**2 - x])\n    assert unrad(sqrt(x) + root(x, 3) - 2) == (_p**3 + _p**2 - 2, [_p, _p**6 - x])\n    assert unrad(sqrt(x) + sqrt(y) - 2, x) == (x**2 + y - 4, [])\n    assert unrad(sqrt(x) + sqrt(y) - 2, y) == (x + y**2 - 4, [])\n", "def test_checksol():\n    x, y = symbols('x y')\n    assert checksol(x**2 - 1, x, 1) == True\n    assert checksol(x**2 - 1, x, 0) == False\n    assert checksol(x**2 + y**2 - 25, {x: 3, y: 4}) == True\n    assert checksol(x**2 + x - x*(x + 1), {}) == True\n    assert checksol(sin(x)**2 + cos(x)**2 - 1, x, pi/4) == True\n    assert checksol(x**2 - 2*x + 1, x, 1) == True\n    assert checksol(exp(x) - 1, x, 0) == True\n    assert checksol(x + y - 2, {x: 1, y: 1}) == True\n    assert checksol(x + y - 2, {x: 1, y: 0}) == False\n", "def test_unrad():\n    from sympy import sqrt, root, Rational, simplify\n    x, y = symbols('x y')\n\n    eq = sqrt(x) + root(x + 1, 3) - 2\n    new_eq, cov = unrad(eq)\n    assert new_eq == _p**3 + _p**2 - 2\n    assert cov == [_p, _p**6 - x]\n\n    eq = sqrt(x) * x**Rational(1, 3) + 2\n    new_eq, cov = unrad(eq)\n    assert new_eq == x**5 - 64\n    assert cov == []\n\n    eq = sqrt(x) + root(x + y, 3) - 2\n    new_eq, cov = unrad(eq)\n    assert new_eq == _p**3 + _p**2 - 2\n    assert cov == [_p, _p**6 - x - y]\n\n    eq = root(x**2 + y, 4) + sqrt(x + 1) - 3\n    new_eq, cov = unrad(eq)\n    assert simplify(new_eq) == 0\n    assert cov == []\n\n    eq = sqrt(x) + sqrt(y) - sqrt(x + y)\n    new_eq, cov = unrad(eq)\n    assert new_eq == x + y - _p**2\n    assert cov == [_p, _p**2 - x - y]\n\n    eq = sqrt(x + sqrt(y)) + sqrt(x - sqrt(y)) - 1\n    new_eq, cov = unrad(eq)\n    assert new_eq == 4*x - 4*_p**2 - 1\n    assert cov == [_p, _p**2 - x + sqrt(y)]\n", "def test_issue_12099():\n    x, y = symbols('x y')\n    assert nsolve([x**2 - 2, y**2 - 3], [x, y], [1, 1]) == Matrix([[sqrt(2)], [sqrt(3)]])\n    assert nsolve([x**2 - 2, y**2 - 3], [x, y], [-1, -1]) == Matrix([[-sqrt(2)], [-sqrt(3)]])\n", "def test_unrad():\n    x, y = symbols('x y')\n    # Test unrad function with simple equation\n    eq = sqrt(x) - 3\n    assert unrad(eq) == (x - 9, [])\n    # Test unrad function with multiple radicals\n    eq = sqrt(x) + root(x + 1, 3)\n    assert unrad(eq) == (x**3 - x**2 - 2*x - 1, [])\n    # Test unrad function with higher-order radicals\n    eq = sqrt(x) + y**Rational(1, 3) - 2\n    assert unrad(eq, x) == (_p**3 + y - 8, [_p, _p**2 - x])\n    # Test unrad function with an equation that involves multiple variables\n    eq = sqrt(x) + sqrt(y) - 5\n    assert unrad(eq, x, y) == (x + y - 25, [])\n    # Test unrad function with integer power forcing\n    eq = x**2 - 4\n    assert unrad(eq, x, integer_power=True) == (x - 2, [])\n"], "sample_976": ["def test_Symbol_sanitize():\n    # Test commutative assumption\n    raises(ValueError, lambda: Symbol('x', commutative=None))\n    s = Symbol('x', commutative=True)\n    assert s.is_commutative is True\n    s = Symbol('x', commutative=False)\n    assert s.is_commutative is False\n\n    # Test other assumptions\n    s = Symbol('x', integer=1)\n    assert s.is_integer is True\n    s = Symbol('x', integer=0)\n    assert s.is_integer is False\n    s = Symbol('x', integer=None)\n    assert s.is_integer is None\n", "def test_wild_exclude():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    expr = 2*x + y\n\n    w = Wild('w', exclude=[x])\n    assert expr.match(w) is None\n\n    w = Wild('w', exclude=[y])\n    assert expr.match(w) is None\n\n    w = Wild('w', exclude=[z])\n    assert expr.match(w) == {w: 2*x + y}\n\n    w = Wild('w', exclude=[x, y])\n    assert expr.match(w) is None\n\n    w = Wild('w', exclude=[x, z])\n    assert expr.match(w) == {w: 2*x + y}\n", "def test_Symbol_sanitize():\n    assumptions = {'commutative': None}\n    raises(ValueError, lambda: Symbol._sanitize(assumptions))\n    \n    assumptions = {'commutative': True, 'bounded': 1, 'infinitesimal': 0}\n    Symbol._sanitize(assumptions)\n    assert assumptions == {'commutative': True, 'finite': True, 'zero': False}\n    \n    assumptions = {'commutative': False, 'positive': None, 'real': 1}\n    Symbol._sanitize(assumptions)\n    assert assumptions == {'commutative': False, 'real': True}\n", "def test_Symbol_sanitization():\n    # Test sanitization of assumptions\n    assumptions = {'commutative': None}\n    raises(ValueError, lambda: Symbol('a', **assumptions))\n    \n    assumptions = {'commutative': 1, 'bounded': 1, 'unbounded': 0, 'infinitesimal': 1}\n    a = Symbol('a', **assumptions)\n    assert a.is_commutative is True\n    assert a.is_finite is True\n    assert a.is_infinite is False\n    assert a.is_zero is True\n\n    assumptions = {'commutative': 0, 'bounded': 0, 'unbounded': 1, 'infinitesimal': 0}\n    b = Symbol('b', **assumptions)\n    assert b.is_commutative is False\n    assert b.is_finite is False\n    assert b.is_infinite is True\n    assert b.is_zero is False\n", "def test_Wild_exclude():\n    x = Symbol(\"x\")\n    y = Symbol(\"y\")\n    z = Symbol(\"z\")\n    w = Wild(\"w\", exclude=[x, y])\n\n    assert w.match(z) == {w: z}\n    assert w.match(x) is None\n    assert w.match(y) is None\n"], "sample_977": ["def test_custom_user_function():\n    custom_func = Function('custom_func')\n    settings = {'user_functions': {'custom_func': 'CustomFunc'}}\n    assert mcode(custom_func(x, y), **settings) == \"CustomFunc[x, y]\"\n", "def test_user_functions():\n    custom_func = Function('custom_func')\n    assert mcode(custom_func(x), user_functions={'custom_func': 'CustomFunc'}) == \"CustomFunc[x]\"\n    assert mcode(custom_func(x, y), user_functions={'custom_func': 'CustomFunc'}) == \"CustomFunc[x, y]\"\n    assert mcode(custom_func(x**2), user_functions={'custom_func': 'CustomFunc'}) == \"CustomFunc[x^2]\"\n", "def test_known_functions():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(tan(x)) == \"Tan[x]\"\n    assert mcode(cot(x)) == \"Cot[x]\"\n    assert mcode(asin(x)) == \"ArcSin[x]\"\n    assert mcode(acos(x)) == \"ArcCos[x]\"\n    assert mcode(atan(x)) == \"ArcTan[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(sech(x)) == \"Sech[x]\"\n    assert mcode(csch(x)) == \"Csch[x]\"\n    assert mcode(asinh(x)) == \"ArcSinh[x]\"\n    assert mcode(acosh(x)) == \"ArcCosh[x]\"\n    assert mcode(atanh(x)) == \"ArcTanh[x]\"\n    assert mcode(acoth(x)) == \"ArcCoth[x]\"\n    assert mcode(asech(x)) == \"ArcSech[x]\"\n    assert mcode(acsch(x)) == \"ArcCsch[x]\"\n", "def test_known_functions():\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(tan(x)) == \"Tan[x]\"\n    assert mcode(cot(x)) == \"Cot[x]\"\n    assert mcode(asin(x)) == \"ArcSin[x]\"\n    assert mcode(acos(x)) == \"ArcCos[x]\"\n    assert mcode(atan(x)) == \"ArcTan[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(log(x)) == \"Log[x]\"\n", "def test_user_defined_functions():\n    custom_functions = {\n        'foo': 'CustomFoo',\n        'bar': 'CustomBar'\n    }\n    assert mcode(Function('foo')(x), user_functions=custom_functions) == \"CustomFoo[x]\"\n    assert mcode(Function('bar')(x, y), user_functions=custom_functions) == \"CustomBar[x, y]\"\n"], "sample_978": ["def test_bspline_basis_invalid_degree():\n    d = -1\n    knots = range(5)\n    try:\n        bspline_basis_set(d, knots, x)\n    except ValueError as e:\n        assert str(e) == 'degree must be non-negative: -1'\n", "def test_invalid_degree():\n    d = -1  # Invalid degree\n    knots = range(5)\n    try:\n        bspline_basis_set(d, knots, x)\n    except ValueError as e:\n        assert str(e) == 'degree must be non-negative: -1'\n", "def test_invalid_degree():\n    d = -1\n    knots = range(5)\n    try:\n        bspline_basis_set(d, knots, x)\n    except ValueError as e:\n        assert str(e) == 'degree must be non-negative: -1'\n", "def test_invalid_degree_negative():\n    d = -1\n    knots = range(5)\n    try:\n        bspline_basis_set(d, knots, x)\n    except ValueError as e:\n        assert str(e) == 'degree must be non-negative: -1'\n    else:\n        assert False, \"Expected ValueError for negative degree\"\n", "def test_invalid_degree():\n    d = -1\n    knots = range(5)\n    try:\n        bspline_basis_set(d, knots, x)\n        assert False, \"Expected ValueError for negative degree\"\n    except ValueError as e:\n        assert str(e) == \"degree must be non-negative: -1\"\n"], "sample_979": ["def test_as_real_imag():\n    A = MatrixSymbol('A', n, m)\n    real, imag = A.as_real_imag()\n    assert isinstance(real, MatrixExpr)\n    assert isinstance(imag, MatrixExpr)\n    assert real.shape == A.shape\n    assert imag.shape == A.shape\n    assert real + imag * S.ImaginaryUnit == A\n", "def test_MatrixElement_invalid_indices():\n    A = MatrixSymbol('A', 3, 3)\n    raises(IndexError, lambda: A[3, 0])\n    raises(IndexError, lambda: A[0, 3])\n    raises(IndexError, lambda: A[-1, 0])\n    raises(IndexError, lambda: A[0, -1])\n    raises(IndexError, lambda: A[3, 3])\n    raises(IndexError, lambda: A['a', 1])\n    raises(IndexError, lambda: A[1, 'b'])\n    raises(IndexError, lambda: A['a', 'b'])\n", "def test_MatrixElement_derivative():\n    A = MatrixSymbol('A', 3, 3)\n    i, j, k = symbols('i j k', integer=True)\n    \n    # Derivative of matrix element with respect to itself should be KroneckerDelta\n    assert diff(A[i, j], A[i, j]) == KroneckerDelta(i, i) * KroneckerDelta(j, j)\n    \n    # Derivative of matrix element with respect to a different element should be zero\n    assert diff(A[i, j], A[i, k]) == S.Zero\n    \n    # Derivative of matrix element with respect to a scalar should be zero\n    scalar = Symbol('scalar')\n    assert diff(A[i, j], scalar) == S.Zero\n    \n    # Derivative of matrix element with respect to an expression involving another element\n    assert diff(A[i, j], A[j, i]) == S.Zero\n\n    # Testing higher order derivatives\n    assert diff(diff(A[i, j], A[i, j]), A[i, j]) == S.Zero\n", "def test_matrixelement_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    Z = ZeroMatrix(n, m)\n    i, j = symbols('i j')\n\n    element_A = A[i, j]\n    element_B = B[j, i]\n    element_Z = Z[i, j]\n\n    assert element_A.subs(A, B) == B[i, j]\n    assert element_B.subs(B, Z) == Z[j, i]\n    assert element_Z.subs(Z, A) == A[i, j]\n\n    C = Matrix([[1, 2], [3, 4]])\n    assert element_A.subs(A, C) == C[i, j]\n    assert element_Z.subs(Z, C) == C[i, j]\n\n    raises(IndexError, lambda: element_A.subs(A, MatrixSymbol('B', l, n)))\n", "def test_MatrixExpr_as_real_imag():\n    A = MatrixSymbol('A', n, m)\n    real, imag = A.as_real_imag()\n    assert isinstance(real, MatrixExpr)\n    assert isinstance(imag, MatrixExpr)\n    assert real.shape == A.shape\n    assert imag.shape == A.shape\n"], "sample_980": ["def test_af_invert():\n    assert _af_invert([1, 0, 2]) == [1, 0, 2]\n    assert _af_invert([2, 0, 1]) == [1, 2, 0]\n    assert _af_invert([3, 2, 0, 1]) == [2, 3, 1, 0]\n    assert _af_invert([4, 2, 3, 1, 0]) == [4, 3, 1, 2, 0]\n    assert _af_invert([2, 3, 1, 0]) == [3, 2, 0, 1]\n    assert _af_invert([1, 3, 4, 0, 2]) == [3, 0, 4, 1, 2]\n", "def test_af_functions():\n    # Test _af_rmul function\n    a = [1, 0, 2]\n    b = [2, 1, 0]\n    assert _af_rmul(a, b) == [2, 0, 1]\n\n    # Test _af_rmuln function\n    c = [0, 1, 2]\n    d = [2, 1, 0]\n    assert _af_rmuln(a, b, c) == [2, 0, 1]\n    assert _af_rmuln(a, b, c, d) == [2, 1, 0]\n\n    # Test _af_invert function\n    e = [1, 2, 0, 3]\n    assert _af_invert(e) == [2, 0, 1, 3]\n\n    # Test _af_pow function\n    f = [2, 0, 1]\n    assert _af_pow(f, 0) == [0, 1, 2]\n    assert _af_pow(f, 1) == [2, 0, 1]\n    assert _af_pow(f, 2) == [1, 2, 0]\n    assert _af_pow(f, 3) == [0, 1, 2]\n    assert _af_pow(f, -1) == [1, 2, 0]\n\n    # Test _af_parity function\n    g = [3, 1, 2, 0]\n    assert _af_parity(g) == 1\n    h = [0, 1, 2, 3]\n    assert _af_parity(h) == 0\n\n    # Test _af_commutes_with function\n    i = [1, 0, 2]\n    j = [0, 2, 1]\n    assert not _af_commutes_with(i, j)\n    k = [2, 1, 0]\n    l = [2, 0, 1]\n    assert _af_commutes_with(k, l)\n", "def test_af_invert():\n    # Testing _af_invert function\n    assert _af_invert([1, 2, 0, 3]) == [2, 0, 1, 3]\n    assert _af_invert([3, 2, 0, 1]) == [2, 3, 1, 0]\n    assert _af_invert([0, 1, 2, 3]) == [0, 1, 2, 3]\n    assert _af_invert([2, 3, 1, 0]) == [3, 2, 0, 1]\n", "def test_permutation_conversions():\n    # Test conversion from Cycle to Permutation and back\n    c = Cycle(1, 2, 3)\n    p = Permutation(c)\n    assert Permutation(Cycle(p)) == p\n\n    # Test conversion from list of transpositions to Permutation\n    t = [(0, 1), (1, 2), (2, 3)]\n    p = Permutation.rmul(*[Permutation(Cycle(*ti)) for ti in t])\n    assert p == Permutation([1, 2, 3, 0])\n\n    # Test conversion from inversion vector to Permutation and back\n    inv_vec = [2, 1, 0]\n    p = Permutation.from_inversion_vector(inv_vec)\n    assert p.inversion_vector() == inv_vec\n\n    # Test conversion from sequence to Permutation\n    seq = 'abcd'\n    p = Permutation.from_sequence(seq)\n    assert p == Permutation([0, 1, 2, 3])\n\n    # Test conversion from Permutation to cyclic form and back\n    p = Permutation([3, 0, 1, 2])\n    cyclic_form = p.cyclic_form\n    new_p = Permutation(cyclic_form)\n    assert new_p == p\n\n    # Test conversion from array form to Permutation and back\n    array_form = [3, 0, 1, 2]\n    p = Permutation(array_form)\n    assert p.array_form == array_form\n\n    # Test conversion from list of lists (cyclic form) to Permutation and back\n    cyclic_form = [[0, 3, 1, 2]]\n    p = Permutation(cyclic_form)\n    assert p.cyclic_form == cyclic_form\n", "def test_af_functions():\n    assert _af_rmul([1, 0, 2], [0, 2, 1]) == [1, 2, 0]\n    assert _af_rmul([2, 0, 1], [1, 0, 2]) == [0, 1, 2]\n\n    assert _af_rmuln([1, 0, 2], [0, 2, 1], [2, 1, 0]) == [0, 2, 1]\n    assert _af_rmuln([1, 0, 2], [2, 1, 0], [0, 2, 1]) == [1, 0, 2]\n\n    assert _af_parity([0, 1, 2]) == 0\n    assert _af_parity([1, 0, 2]) == 1\n    assert _af_parity([2, 1, 0]) == 1\n\n    assert _af_invert([1, 0, 2]) == [1, 0, 2]\n    assert _af_invert([2, 0, 1]) == [1, 2, 0]\n\n    assert _af_pow([1, 0, 2], 0) == [0, 1, 2]\n    assert _af_pow([1, 0, 2], 1) == [1, 0, 2]\n    assert _af_pow([1, 0, 2], 2) == [0, 1, 2]\n    assert _af_pow([1, 0, 2], -1) == [1, 0, 2]\n    assert _af_pow([1, 0, 2], -2) == [0, 1, 2]\n\n    assert _af_commutes_with([1, 0, 2], [0, 1, 2]) == True\n    assert _af_commutes_with([1, 0, 2], [2, 1, 0]) == False\n"], "sample_981": ["def test_af_functions():\n    # Testing _af_rmul function\n    a = [1, 0, 2]\n    b = [2, 0, 1]\n    assert _af_rmul(a, b) == [2, 1, 0]\n    assert _af_rmul(b, a) == [1, 2, 0]\n\n    # Testing _af_rmuln function with various lengths of input\n    assert _af_rmuln(a) == [1, 0, 2]\n    assert _af_rmuln(a, b) == [2, 1, 0]\n    assert _af_rmuln(a, b, [0, 1, 2]) == [2, 1, 0]\n    assert _af_rmuln(a, b, [2, 1, 0]) == [0, 1, 2]\n    assert _af_rmuln(a, b, [2, 1, 0], [0, 2, 1]) == [2, 0, 1]\n    assert _af_rmuln(a, b, [2, 1, 0], [0, 2, 1], [1, 0, 2]) == [1, 2, 0]\n    assert _af_rmuln(a, b, [2, 1, 0], [0, 2, 1], [1, 0, 2], [2, 1, 0]) == [0, 2, 1]\n    assert _af_rmuln(a, b, [2, 1, 0], [0, 2, 1], [1, 0, 2], [2, 1, 0], [1, 2, 0]) == [2, 0, 1]\n    assert _af_rmuln(a, b, [2, 1, 0], [0, 2, 1], [1, 0, 2], [2, 1, 0], [1, 2, 0], [0, 1, 2]) == [1, 0, 2]\n\n    # Testing _af_invert function\n    assert _af_invert([1, 0, 2]) == [1, 0,", "def test_af_rmul():\n    a = [3, 0, 2, 1]\n    b = [0, 2, 3, 1]\n    c = [1, 3, 0, 2]\n    \n    # Testing _af_rmul with different inputs\n    assert _af_rmul(a, b) == [3, 2, 1, 0]\n    assert _af_rmul(b, c) == [2, 1, 3, 0]\n    assert _af_rmul(a, c) == [0, 3, 2, 1]\n    \n    # Testing with the same permutation\n    assert _af_rmul(a, a) == [1, 3, 2, 0]\n    \n    # Check if it handles empty lists\n    assert _af_rmul([], []) == []\n    assert _af_rmul(a, []) == []\n    assert _af_rmul([], a) == []\n", "def test_af_rmul():\n    # Basic tests for _af_rmul\n    assert _af_rmul([1, 0, 2], [0, 2, 1]) == [1, 2, 0]\n    assert _af_rmul([3, 2, 1, 0], [0, 2, 3, 1]) == [3, 1, 0, 2]\n    assert _af_rmul([1, 2, 0], [2, 1, 0]) == [0, 2, 1]\n    assert _af_rmul([0, 1, 2], [2, 1, 0]) == [2, 1, 0]\n    assert _af_rmul([2, 0, 1], [1, 2, 0]) == [0, 1, 2]\n\n    # Edge case: Empty input\n    assert _af_rmul([], []) == []\n    # Edge case: Single element\n    assert _af_rmul([0], [0]) == [0]\n\n    # Larger input\n    a = list(range(10))\n    b = list(range(9, -1, -1))\n    assert _af_rmul(a, b) == list(range(9, -1, -1))\n    assert _af_rmul(b, a) == [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n", "def test_af_functions():\n    # Test _af_rmul\n    a = [1, 0, 2]\n    b = [0, 2, 1]\n    assert _af_rmul(a, b) == [1, 2, 0]\n    assert _af_rmul(b, a) == [0, 1, 2]\n\n    # Test _af_rmuln\n    c = [2, 1, 0]\n    assert _af_rmuln(a, b, c) == [1, 0, 2]\n    assert _af_rmuln(c, b, a) == [2, 1, 0]\n    assert _af_rmuln(a) == a\n    assert _af_rmuln() == []\n    assert _af_rmuln(a, b) == [1, 2, 0]\n\n    # Test _af_parity\n    assert _af_parity([0, 1, 2, 3]) == 0\n    assert _af_parity([3, 2, 0, 1]) == 1\n    assert _af_parity([1, 0, 2, 3]) == 1\n    assert _af_parity([0, 2, 1, 3]) == 1\n\n    # Test _af_invert\n    assert _af_invert([1, 2, 0, 3]) == [2, 0, 1, 3]\n    assert _af_invert([3, 2, 1, 0]) == [3, 2, 1, 0]\n    assert _af_invert([0, 1, 2, 3]) == [0, 1, 2, 3]\n\n    # Test _af_pow\n    assert _af_pow([2, 0, 3, 1], 4) == [0, 1, 2, 3]\n    assert _af_pow([1, 0, 2, 3], 2) == [0, 1, 2, 3]\n    assert _af_pow([3, 2, 1, 0], 3) == [0, 1, 2, 3]\n    assert _af_pow([0, 2, 1, 3], 0", "def test_af_invert():\n    assert _af_invert([1, 2, 0, 3]) == [2, 0, 1, 3]\n    assert _af_invert([0, 1, 2, 3]) == [0, 1, 2, 3]\n    assert _af_invert([3, 2, 1, 0]) == [3, 2, 1, 0]\n    assert _af_invert([2, 0, 3, 1]) == [1, 3, 0, 2]\n    raises(ValueError, lambda: _af_invert([]))\n"], "sample_982": ["def test_pollard_rho_and_pollard_pm1():\n    # Testing pollard_rho\n    assert pollard_rho(8051) == 97  # 8051 = 97 * 83\n    assert pollard_rho(10403, retries=3) == 101  # 10403 = 101 * 103\n    assert pollard_rho(10403, seed=42) == 101\n    assert pollard_rho(561, retries=5) is None  # 561 is Carmichael, so no factor found\n    assert pollard_rho(19) is None  # 19 is prime\n\n    # Testing pollard_pm1\n    assert pollard_pm1(8051, B=100) == 97  # 8051 = 97 * 83\n    assert pollard_pm1(10403, B=100, retries=3) == 101  # 10403 = 101 * 103\n    assert pollard_pm1(561, B=50) == 3  # 561 = 3 * 11 * 17\n    assert pollard_pm1(19, B=50) is None  # 19 is prime\n    assert pollard_pm1(2**61 - 1, B=10**6) == 2305843009213693951  # 2**61 - 1 is prime\n\n    # Testing edge cases\n    raises(ValueError, lambda: pollard_rho(4))  # pollard_rho should receive n > 4\n    raises(ValueError, lambda: pollard_pm1(3))  # pollard_pm1 should receive n > 3\n    raises(ValueError, lambda: pollard_pm1(10, B=2))  # B should be > 2\n\n    # Testing seed and retries for pollard_rho\n    assert pollard_rho(10403, seed=1234, retries=0) == 101\n    assert pollard_rho(10403, seed=1234, retries=1) == 101\n    assert pollard_rho(10403, seed=1234, retries=2) == 101\n\n    # Testing seed and retries for pollard_pm1\n    assert pollard_pm1(10403, B=100, seed=1234, retries=0) == 101\n   ", "def test_pollard_rho():\n    # Check Pollard's rho method with known composite number\n    assert pollard_rho(10403, seed=1234) in [101, 103]  # 10403 = 101 * 103\n    # Check with retries\n    assert pollard_rho(10403, seed=1234, retries=2) in [101, 103]\n    # Check Pollard's rho method with prime number\n    assert pollard_rho(104729, seed=1234) is None  # 104729 is a prime\n    # Check edge cases\n    raises(ValueError, lambda: pollard_rho(4))  # n must be greater than 4\n    raises(ValueError, lambda: pollard_rho(3))  # n must be greater than 4\n    # Check with a provided function F\n    F = lambda x: (x**2 + 1) % 10403\n    assert pollard_rho(10403, F=F, seed=1234) in [101, 103]\n", "def test_pollard_rho():\n    assert pollard_rho(8051, retries=3) == 97\n    assert pollard_rho(8051, retries=0) is None\n    assert pollard_rho(10403, seed=2) == 101\n    assert pollard_rho(99991) is None\n    assert pollard_rho(21477639576571) == 4410317\n    raises(ValueError, lambda: pollard_rho(4))\n", "def test_factorint_large_numbers():\n    # Test factorint with large numbers\n    assert factorint(2**10000019 + 1) == {long(2**10000019 + 1): 1}\n    assert factorint(10**200 + 1) == {long(10**200 + 1): 1}\n    assert factorint(2**128 * 3**64 * 5**32 + 1) == {long(2**128 * 3**64 * 5**32 + 1): 1}\n    assert factorint((2**10 * 3**5 * 5**3)**100003 + 1) == {long((2**10 * 3**5 * 5**3)**100003 + 1): 1}\n\n    # Check the behavior with verbose flag\n    assert 'Factoring' in capture(lambda: factorint(2**64 + 1, verbose=True))\n    assert 'Pollard' in capture(lambda: factorint(2**64 + 1, verbose=True))\n    assert 'with primes' in capture(lambda: factorint(2**64 + 1, verbose=True))\n", "def test_pollard_rho():\n    # Test Pollard's rho method with different parameters\n    assert pollard_rho(16843009, seed=1234) == 257\n    assert pollard_rho(16843009, s=1, seed=5678) == 257\n    assert pollard_rho(16843009, s=2, seed=5678) == 257\n    assert pollard_rho(16843009, s=2, a=1, retries=3, seed=1234) == 257\n\n    # Check that no factor is found for prime number\n    assert pollard_rho(19, seed=1) is None\n\n    # Test Pollard's rho with a custom function\n    F = lambda x: (x**2 + 1) % 16843009\n    assert pollard_rho(16843009, F=F, seed=1234) == 257\n\n    # Test Pollard's rho with max_steps\n    assert pollard_rho(16843009, max_steps=1000, seed=1234) == 257\n\n    # Test with edge cases\n    raises(ValueError, lambda: pollard_rho(4))\n    raises(ValueError, lambda: pollard_rho(3))\n\n    # Test with different seeds and retries\n    assert pollard_rho(16843009, retries=0, seed=1) == 257\n    assert pollard_rho(16843009, retries=1, seed=1) == 257\n    assert pollard_rho(16843009, retries=2, seed=1) == 257\n"], "sample_983": ["def test_sparse_solve_exceptions():\n    A = SparseMatrix([[1, 2], [3, 4]])\n    B = SparseMatrix([[1, 2], [3, 4], [5, 6]])\n\n    # Under-determined system\n    raises(ValueError, lambda: A.solve_least_squares(B))\n    \n    # Over-determined system\n    raises(ValueError, lambda: B.solve(A[:, 0]))\n\n    # Invalid method for inverse\n    raises(NotImplementedError, lambda: A.inv(method='INVALID'))\n", "def test_sparse_matrix_creation_with_callable():\n        return i + j\n\n    sm = SparseMatrix(3, 3, op)\n    assert sm == SparseMatrix([\n        [0, 1, 2],\n        [1, 2, 3],\n        [2, 3, 4]\n    ])\n\n        return 0\n\n    sm_zero = SparseMatrix(3, 3, op_zero)\n    assert sm_zero == SparseMatrix.zeros(3, 3)\n\n        return i if i == j else 0\n\n    sm_conditional = SparseMatrix(3, 3, op_conditional)\n    assert sm_conditional == SparseMatrix.eye(3)\n", "def test_extract():\n    a = SparseMatrix(((1, 2, 3), (4, 5, 6), (7, 8, 9)))\n    assert a.extract([0, 2], [1, 2]) == SparseMatrix([[2, 3], [8, 9]])\n    assert a.extract([1], [0, 2]) == SparseMatrix([[4, 6]])\n    assert a.extract([0, 1, 2], [0, 1, 2]) == a\n    assert a.extract([0, 0], [1, 1]) == SparseMatrix([[2], [2]])\n    assert a.extract([0, 1, 1], [0, 1, 1, 2]) == SparseMatrix([\n        [1, 2, 2, 3],\n        [4, 5, 5, 6],\n        [4, 5, 5, 6]])\n", "def test_LDL_solve():\n    A = SparseMatrix([[4, 2, 0],\n                      [2, 4, 2],\n                      [0, 2, 4]])\n    b = SparseMatrix(3, 1, [1, 2, 3])\n    x = A._LDL_solve(b)\n    assert A * x == b\n\n    # Check for matrix with a different size\n    A = SparseMatrix([[10, 5, 0, 0],\n                      [5, 10, 5, 0],\n                      [0, 5, 10, 5],\n                      [0, 0, 5, 10]])\n    b = SparseMatrix(4, 1, [1, 2, 3, 4])\n    x = A._LDL_solve(b)\n    assert A * x == b\n\n    # Check for a matrix that cannot be solved\n    raises(ValueError, lambda: SparseMatrix([[4, 2], [2, 4]])._LDL_solve(SparseMatrix([1, 2])))\n", "def test_slicing_with_expressions():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    m = SparseMatrix(3, 3, lambda i, j: x*i + y*j)\n    assert m[:2, :2] == SparseMatrix(2, 2, lambda i, j: x*i + y*j)\n    assert m[1:, 1:] == SparseMatrix(2, 2, lambda i, j: x*(i + 1) + y*(j + 1))\n\n    n = SparseMatrix(4, 4, lambda i, j: i + j)\n    assert n[1:3, 1:3] == SparseMatrix(2, 2, lambda i, j: (i + 1) + (j + 1))\n    assert n[-3:, -3:] == SparseMatrix(3, 3, lambda i, j: (i + 1) + (j + 1))\n\n    p = SparseMatrix([[x, y, x*y], [y, x, y*x], [x**2, y**2, x*y]])\n    assert p[:2, :2] == SparseMatrix([[x, y], [y, x]])\n    assert p[1:, 1:] == SparseMatrix([[x, y*x], [y**2, x*y]])\n\n    q = SparseMatrix(2, 2, [x, y, y, x])\n    q[1, 1] = x**2 + y**2\n    assert q == SparseMatrix([[x, y], [y, x**2 + y**2]])\n"], "sample_984": ["def test_strrepr_mixed():\n    assert sstrrepr([1, 'a', x + y, {'key': 'value'}]) == \"[1, 'a', x + y, {'key': 'value'}]\"\n    assert sstrrepr({'a': 'apple', 'b': 2, 'c': x + y}) == \"{'a': 'apple', 'b': 2, 'c': x + y}\"\n    assert sstrrepr((1, 2, 'a', x)) == \"(1, 2, 'a', x)\"\n    assert sstrrepr({1, 2, 'a', x}) == \"{1, 2, 'a', x}\"\n", "def test_EmptyPrinter():\n    # Test for string input\n    str_printer = StrPrinter()\n    assert str_printer.emptyPrinter(\"test_string\") == \"test_string\"\n    \n    # Test for Basic instance\n    class TestBasic(Basic):\n            return \"TestBasic()\"\n\n    test_basic = TestBasic()\n    assert str_printer.emptyPrinter(test_basic) == \"TestBasic()\"\n\n    # Test for integer input\n    assert str_printer.emptyPrinter(42) == \"42\"\n\n    # Test for other type (e.g. float)\n    assert str_printer.emptyPrinter(3.14) == \"3.14\"\n\n    # Test for unknown Basic instance without args attribute\n    class UnknownBasic(Basic):\n        pass\n\n    unknown_basic = UnknownBasic()\n    try:\n        str_printer.emptyPrinter(unknown_basic)\n    except:\n        pass  # Expected to raise an exception\n", "def test_BlockMatrix():\n    from sympy import Matrix, BlockMatrix\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    C = Matrix([[9, 10], [11, 12]])\n    D = Matrix([[13, 14], [15, 16]])\n    BM = BlockMatrix([[A, B], [C, D]])\n    assert str(BM) == \"Matrix([\\n[1,  2,  5,  6],\\n[3,  4,  7,  8],\\n[9, 10, 13, 14],\\n[11, 12, 15, 16]])\"\n", "def test_Quantity():\n    from sympy.physics.units import Quantity\n    length = Quantity('length')\n    assert str(length) == \"length\"\n    assert sstr(length) == \"length\"\n", "def test_Exp1():\n    assert str(exp(1)) == \"E\"\n"], "sample_985": ["def test_minmax_functions():\n    from sympy.abc import a, b, c\n\n    # Test Max function\n    assert Max(2, 3) == 3\n    assert Max(-5, 3, 2) == 3\n    assert Max(a, b).subs({a: 1, b: 2}) == 2\n    assert Max(a, b, c).subs({a: 1, b: -3, c: 0}) == 1\n    assert Max(a, 0).subs(a, -1) == 0\n    assert Max(a, 0).subs(a, 1) == 1\n    assert Max(1, 2, 3, 4, 5) == 5\n    assert Max(-1, -2, -3) == -1\n\n    # Test Min function\n    assert Min(2, 3) == 2\n    assert Min(-5, 3, 2) == -5\n    assert Min(a, b).subs({a: 1, b: 2}) == 1\n    assert Min(a, b, c).subs({a: 1, b: -3, c: 0}) == -3\n    assert Min(a, 0).subs(a, -1) == -1\n    assert Min(a, 0).subs(a, 1) == 0\n    assert Min(1, 2, 3, 4, 5) == 1\n    assert Min(-1, -2, -3) == -3\n\n    # Test Min/Max with symbolic and numeric mix\n    x, y, z = symbols('x y z')\n    assert Max(3, x).subs(x, 2) == 3\n    assert Min(3, x).subs(x, 2) == 2\n    assert Max(3, x).subs(x, 4) == 4\n    assert Min(3, x).subs(x, 4) == 3\n\n    # Test Min/Max with assumptions\n    p = symbols('p', positive=True)\n    assert Max(p, -1) == p\n    assert Min(p, -1) == -1\n\n    n = symbols('n', negative=True)\n    assert Max(n, 1) == 1\n    assert Min(n,", "def test_sqrt():\n    from sympy import Symbol, sqrt, Eq, powdenest\n    x = Symbol('x')\n    y = Symbol('y', positive=True)\n    \n    assert sqrt(x) == Pow(x, S.Half)\n    assert sqrt(x)**2 == x\n    assert sqrt(x**2) == Pow(x**2, S.Half)\n    assert sqrt(x**2).subs(x, -1) != -1  # should not simplify to x when x is negative\n    assert sqrt(y**2) == y\n    assert powdenest(sqrt(x**2), force=True) == x\n", "def test_sqrt_cbrt_root_real_root():\n    from sympy import sqrt, cbrt, root, real_root, Symbol, Eq, Rational\n    x = Symbol('x')\n    y = Symbol('y', positive=True)\n    n = Symbol('n')\n\n    # Testing sqrt function\n    assert sqrt(x) == Pow(x, S.Half)\n    assert sqrt(x)**2 == x\n    assert sqrt(x**2) == Pow(x**2, S.Half)\n    assert Eq(sqrt(x**2), x).subs(x, -1) == False\n    assert sqrt(y**2) == y\n\n    # Testing cbrt function\n    assert cbrt(x) == Pow(x, Rational(1, 3))\n    assert cbrt(x)**3 == x\n    assert cbrt(x**3) == Pow(x**3, Rational(1, 3))\n    assert Eq(cbrt(x**3), x).subs(x, -1) == False\n    assert cbrt(y**3) == y\n\n    # Testing root function\n    assert root(x, 2) == sqrt(x)\n    assert root(x, 3) == cbrt(x)\n    assert root(x, n) == Pow(x, 1/n)\n    assert root(x, -Rational(2, 3)) == Pow(x, -3/2)\n    assert root(-2, 3, 2) == -(-1)**(Rational(2, 3))*2**Rational(1, 3)\n\n    # Testing real_root function\n    assert real_root(-8, 3) == -2\n    assert real_root(root(-8, 3)) == -2\n    assert real_root(root(-8, 3, 2)) == -2*Pow(-1, Rational(2, 3))\n\n    # Additional test for various expressions in real_root\n    assert real_root(x**Rational(1, 3)) == x**Rational(1, 3)\n    assert real_root(-32, 5) == -2\n    assert real_root(root(-32, 5, 5//2)) == -2\n", "def test_sqrt():\n    from sympy import Symbol, sqrt, Eq, powdenest\n    x = Symbol('x')\n    y = Symbol('y', positive=True)\n\n    assert sqrt(x) == Pow(x, S.Half)\n    assert sqrt(x)**2 == x\n    assert sqrt(x**2) == sqrt(x**2)\n    assert Eq(sqrt(x**2), x).subs(x, -1) is False\n    assert sqrt(y**2) == y\n    assert powdenest(sqrt(x**2), force=True) == x\n\n    from sympy import rootof\n    assert [rootof(x**2 - 3, i) for i in (0, 1)] == [-sqrt(3), sqrt(3)]\n", "def test_MinMaxBase():\n    # Test basic functionality of Min and Max classes\n    x, y, z = symbols('x y z')\n    p = symbols('p', positive=True)\n    n = symbols('n', negative=True)\n    \n    assert Max(x, -2).subs(x, 3) == 3\n    assert Max(p, -2) == p\n    assert Max(x, y) == Max(x, y)\n    assert Max(x, y) == Max(y, x)\n    assert Max(x, Max(y, z)) == Max(x, y, z)\n    assert Max(n, 8, p, 7, -S.Infinity) == Max(8, p)\n    assert Max(1, x, S.Infinity) == S.Infinity\n    \n    assert Min(x, -2).subs(x, 3) == -2\n    assert Min(p, -3) == -3\n    assert Min(x, y) == Min(x, y)\n    assert Min(n, 8, p, -7, p, S.Infinity) == Min(n, -7)\n    \n    # Test _eval_simplify method for Min and Max\n    assert Max(x, y, Max(x, z)).simplify() == Max(x, y, z)\n    assert Min(x, y, Min(x, z)).simplify() == Min(x, y, z)\n    \n    # Test _eval_derivative method for Min and Max\n    assert Max(x, y).diff(x) == Heaviside(x - y)\n    assert Min(x, y).diff(x) == Heaviside(y - x)\n    \n    # Test _is_connected method for Min and Max\n    assert Max._is_connected(x, y) is None\n    assert Max._is_connected(x, x) is True\n    assert Min._is_connected(x, y) is None\n    assert Min._is_connected(x, x) is True\n    \n    # Test _eval_rewrite_as_Heaviside method for Min and Max\n    assert Max(x, y)._eval_rewrite_as_Heaviside(x, y) == x * Heaviside(x - y) + y * Heaviside(y - x)\n    assert Min(x, y)._eval_rewrite_as_Heaviside(x, y) == x * Heaviside(y - x) + y * Heaviside(x - y)\n    \n   "], "sample_986": ["def test_evalf_precision_exhausted():\n    expr = Sum(1/n**2, (n, 1, oo))\n    assert expr.evalf().evalf(20)._prec == 20 * 3.3219280948873626  # log2(10) = 3.3219...\n    raises(PrecisionExhausted, lambda: expr.evalf(strict=True, maxn=5))\n", "def test_evalf_special_functions():\n    # Test for Bernoulli numbers\n    from sympy import bernoulli\n    assert NS(bernoulli(4), 15) == '0.0333333333333333'\n    assert NS(bernoulli(10), 15) == '-0.075757575757576'\n    # Ensure the evaluation of higher Bernoulli numbers\n    assert NS(bernoulli(50), 10) == '-1.291285997e+16'\n    assert NS(bernoulli(50), 20) == '-1.2912859970626635400e+16'\n\n    # Test for Piecewise function\n    from sympy import Piecewise\n    expr = Piecewise((x**2, x < 1), (x, x >= 1))\n    assert NS(expr, 15, subs={x: 0.5}) == '0.250000000000000'\n    assert NS(expr, 15, subs={x: 1.5}) == '1.50000000000000'\n\n    # Test for integrals\n    assert NS(Integral(sin(x), (x, 0, pi)), 15) == '2.00000000000000'\n    assert NS(Integral(exp(-x**2), (x, -oo, oo)), 15) == '1.77245385090552'\n\n    # Test for limits with infinity\n    assert NS(Integral(exp(-x**2), (x, -oo, oo)), 15) == '1.77245385090552'\n    assert NS(Integral(1/(x**2 + 1), (x, -oo, oo)), 15) == '3.14159265358979'\n    assert NS(Sum(1/x**2, (x, 1, oo)), 15) == '1.64493406684823'\n", "def test_evalf_negative_powers():\n    assert NS('(-2)**2', 15) == '4.00000000000000'\n    assert NS('(-2)**3', 15) == '-8.00000000000000'\n    assert NS('(-2)**-1', 15) == '-0.500000000000000'\n    assert NS('(-2)**-2', 15) == '0.250000000000000'\n    assert NS('(-2)**-3', 15) == '-0.125000000000000'\n", "def test_evalf_logarithms_with_subs():\n    expr = log(x, 10) + log(y)\n    result = expr.evalf(subs={x: 100, y: E})\n    assert result == 2 + 1\n    result = expr.evalf(subs={x: 1000, y: E**2})\n    assert result == 3 + 2\n\n    expr = log(x, y)\n    result = expr.evalf(subs={x: 100, y: 10})\n    assert result == 2\n    result = expr.evalf(subs={x: 1000, y: 10})\n    assert result == 3\n", "def test_evalf_special_functions():\n    from sympy import bernoulli\n    assert NS(bernoulli(10), 20) == '0.075757575757575757576'\n    assert NS(bernoulli(20), 20) == '-0.0000000000000000005296'\n    assert NS(bernoulli(30), 20) == '0.00000000000000000000000000000629'\n    assert NS(bernoulli(2), 20) == '0.16666666666666666667'\n"], "sample_987": ["def test_evalf_hypergeometric_sum():\n    from sympy import Function, symbols, Sum, factorial\n    a, b = symbols('a b')\n    \n    f = Function('f')\n    f = lambda n: 1/factorial(n)\n    sum_expr = Sum(f(n), (n, 0, oo))\n    \n    # Ensure the sum evaluates correctly\n    assert NS(sum_expr, 15) == '2.71828182845905'  # Expected value is e\n\n    # Ensure the sum evaluates correctly with different precision\n    assert NS(sum_expr, 20) == '2.71828182845904523536'  # Higher precision\n", "def test_evalf_trig_special_cases():\n    assert NS('sin(0)', 15) == '0.000000000000000'\n    assert NS('cos(0)', 15) == '1.00000000000000'\n    assert NS('tan(0)', 15) == '0.000000000000000'\n    assert NS('sin(pi)', 15) == '0.000000000000000'\n    assert NS('cos(pi)', 15) == '-1.00000000000000'\n    assert NS('tan(pi)', 15) == '0.000000000000000'\n    assert NS('sin(pi/2)', 15) == '1.00000000000000'\n    assert NS('cos(pi/2)', 15) == '0.000000000000000'\n    assert NS('tan(pi/2)', 15, maxn=1000) == '16331239353195370.0'  # large value due to tan(pi/2) approaching infinity\n", "def test_evalf_abs():\n    assert NS(abs(-pi), 15) == '3.14159265358979'\n    assert NS(abs(pi + 1*I), 15) == '3.14159265358979 + 1.00000000000000*I'\n    assert NS(abs(3 + 4*I), 15) == '5.00000000000000'\n    assert NS(abs(I), 15) == '1.00000000000000'\n", "def test_evalf_pure_complex():\n    from sympy import sqrt, I\n    from sympy.core.evalf import pure_complex\n\n    assert pure_complex(2 + 3*I) == (2, 3)\n    assert pure_complex(2*I) == (0, 2)\n    assert pure_complex(2 + 0*I) is None\n    assert pure_complex(sqrt(2) + sqrt(3)*I) == (sqrt(2), sqrt(3))\n    assert pure_complex(2, or_real=True) == (2, 0)\n    assert pure_complex(sqrt(2)) is None\n", "def test_evalf_piecewise():\n    from sympy import Piecewise\n    p = Piecewise((x**2, x < 1), (2*x + 1, x >= 1))\n    assert NS(p, subs={x: 0.5}) == '0.250000000000000'\n    assert NS(p, subs={x: 1}) == '3.00000000000000'\n    assert NS(p, subs={x: 2}) == '5.00000000000000'\n    assert NS(p, subs={x: -2}) == '4.00000000000000'\n    raises(NotImplementedError, lambda: p.evalf())\n"], "sample_988": ["def test_comp():\n    from sympy import Float, Rational\n\n    assert comp(1, 1) is True\n    assert comp(1, 2) is False\n    assert comp(1.0, 1) is True\n    assert comp(1.0, 1.0) is True\n    assert comp(1.0, 2.0) is False\n    assert comp(Rational(1, 2), 0.5) is True\n    assert comp(Rational(1, 2), 0.6) is False\n    assert comp(Float(1.0), 1) is True\n    assert comp(Float(1.0), 2) is False\n    assert comp(1, '1.0') is False\n    assert comp(Rational(1, 2), '0.5') is False\n    assert comp(Float(1.0), '1.0') is False\n\n    # testing tolerance\n    assert comp(1, 1.01, 0.02) is True\n    assert comp(1, 1.01, 0.005) is False\n    assert comp(1.0, 1.01, 0.02) is True\n    assert comp(1.0, 1.01, 0.005) is False\n    assert comp(Float(1.0), Float(1.01), 0.02) is True\n    assert comp(Float(1.0), Float(1.01), 0.005) is False\n    assert comp(Rational(1, 2), 0.51, 0.02) is True\n    assert comp(Rational(1, 2), 0.51, 0.005) is False\n", "def test_comp():\n    assert comp(1, 1) is True\n    assert comp(1, 2) is False\n    assert comp(1.00001, 1.00002, tol=0.0001) is True\n    assert comp(1.00001, 1.00002, tol=0.000001) is False\n    assert comp('1.00001', '1.00002', tol=0.0001) is False  # comparing strings\n    raises(ValueError, lambda: comp('1.00001', 1))\n    raises(ValueError, lambda: comp(1, '1.00001'))\n", "def test_mpf_norm():\n    from mpmath.libmp.libmpf import mpf\n    # Check normalization of zero mantissa with different precision values\n    assert mpf_norm((0, 0, 0, 0), 53) == _mpf_zero\n    assert mpf_norm((1, 0, 0, 1), 53) == (1, 0, 0, 1)\n    assert mpf_norm((0, 0, 0, 1), 53) == (0, 0, 0, 1)\n    assert mpf_norm((1, 123456, -5, 53), 53) == mpf((1, 123456, -5, 53), 53)\n    assert mpf_norm((0, 0, 0, 0), 100) == _mpf_zero\n    assert mpf_norm((1, 0, 0, 1), 100) == (1, 0, 0, 1)\n    assert mpf_norm((0, 0, 0, 1), 100) == (0, 0, 0, 1)\n    assert mpf_norm((1, 123456, -5, 100), 100) == mpf((1, 123456, -5, 100), 100)\n", "def test_mpf_norm():\n    from mpmath.libmp.libmpf import from_float\n\n    # Test with finite mantissa and precision\n    finite_tuple = (0, from_float(1.5), 0, 53)  # equivalent to 1.5\n    assert mpf_norm(finite_tuple, 53) == finite_tuple\n\n    # Test with zero mantissa and finite exponent\n    zero_tuple = (0, 0, 0, 0)\n    assert mpf_norm(zero_tuple, 53) == _mpf_zero\n\n    # Test with zero mantissa but infinite exponent\n    inf_tuple = (0, 0, 0, 1)\n    assert mpf_norm(inf_tuple, 53) == inf_tuple\n", "def test_seterr():\n    from sympy.core.numbers import _errdict, seterr, Integer, S\n    \n    # Initial state should be False\n    assert _errdict[\"divide\"] == False\n    \n    # Change to True\n    seterr(divide=True)\n    assert _errdict[\"divide\"] == True\n    \n    # Division by zero should raise ValueError now\n    raises(ValueError, lambda: Integer(1) / S.Zero)\n    \n    # Change back to False\n    seterr(divide=False)\n    assert _errdict[\"divide\"] == False\n    \n    # Division by zero should now return NaN\n    assert (Integer(1) / S.Zero) is S.NaN\n"], "sample_989": ["def test_mpf_norm_zero():\n    # Test to ensure mpf_norm handles the zero mantissa correctly\n    from mpmath.libmp.libmpf import fnan, finf, fninf, fzero\n    assert mpf_norm((0, 0, 0, 0), 53) == fzero\n    assert mpf_norm((0, 0, -123, 0), 53) == fzero\n    assert mpf_norm((1, 0, -456, 0), 53) == fzero\n    assert mpf_norm((1, 0, -456, 1), 53) == (1, 0, -456, 1)\n    assert mpf_norm(fnan, 53) == fnan\n    assert mpf_norm(finf, 53) == finf\n    assert mpf_norm(fninf, 53) == fninf\n", "def test_mpf_norm():\n    from sympy.core.numbers import mpf_norm\n    assert mpf_norm((0, 0, 1, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((0, 1, 0, 1), 10) == (0, 1, 0, 1)\n    assert mpf_norm((1, 1, 0, 1), 10) == (1, 1, 0, 1)\n    assert mpf_norm((0, 1, 1, 1), 10) == (0, 1, 1, 1)\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((0, 10**10, -10**10, 10**10), 10)._mpf_ == (0, 10**10, -10**10, 10**10)\n", "def test_mpf_norm_inf_nan():\n    # Test normalization of mpf tuples representing inf and nan\n    inf_mpf = (0, long(0), 0, 0)\n    assert mpf_norm(_mpf_inf, 10) == inf_mpf\n    assert mpf_norm(_mpf_ninf, 10) == (1, long(0), 0, 0)\n    assert mpf_norm(_mpf_nan, 10) == (0, long(0), 0, 0)\n", "def test_mpf_norm():\n    from sympy.core.numbers import mpf_norm\n    from mpmath.libmp.libmpf import fnan, finf, fninf, fzero, mpf\n    from sympy import Float\n    \n    # Normalizing zero\n    assert mpf_norm(fzero, 10) == fzero\n    assert mpf_norm((0, 0, 0, 0), 10) == fzero\n    \n    # Normalizing infinity\n    assert mpf_norm(finf, 10) == finf\n    assert mpf_norm((0, 0, 1, 0), 10) == finf\n    \n    # Normalizing negative infinity\n    assert mpf_norm(fninf, 10) == fninf\n    assert mpf_norm((1, 0, 1, 0), 10) == fninf\n    \n    # Normalizing NaN\n    assert mpf_norm(fnan, 10) == fnan\n    assert mpf_norm((0, 0, 0, -1), 10) == fnan\n    \n    # Check normalization of large and small numbers\n    mpf_val = mpf('1.23456789e10')\n    normalized = mpf_norm(mpf_val, 10)\n    assert Float._new(normalized, 10)._mpf_ == mpf_val\n    \n    mpf_val = mpf('1.23456789e-10')\n    normalized = mpf_norm(mpf_val, 10)\n    assert Float._new(normalized, 10)._mpf_ == mpf_val\n", "def test_mpf_norm_zero_cases():\n    # Ensure mpf_norm correctly handles special zero cases\n    assert mpf_norm((1, 0, -123, 0), 10) == _mpf_zero\n    assert mpf_norm((0, 0, -456, 0), 10) == _mpf_zero\n    assert mpf_norm((1, 0, -789, 0), 10) == _mpf_zero\n    assert mpf_norm((1, 0, 0, 0), 10) == _mpf_zero\n"], "sample_990": ["def test_sinh_conjugate():\n    x = Symbol('x', real=True)\n    assert sinh(x + I).conjugate() == sinh(x - I)\n    ", "def test_sech_rewrite_expansion():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test expansion and rewriting of sech\n    assert sech(x + y).expand(trig=True) == 1 / (cosh(x) * cosh(y) + sinh(x) * sinh(y))\n    assert sech(2*x).expand(trig=True) == 1 / (cosh(x)**2 + sinh(x)**2)\n    assert sech(3*x).expand(trig=True).expand() == 1 / (4*cosh(x)**3 - 3*cosh(x))\n", "def test_hyperbolic_function_fdiff():\n    x = Symbol('x')\n    y = Symbol('y', integer=True)\n    \n    # Test fdiff method of sinh\n    s = sinh(x)\n    assert s.fdiff() == cosh(x)\n    raises(ArgumentIndexError, lambda: s.fdiff(2))\n    \n    # Test fdiff method of cosh\n    c = cosh(x)\n    assert c.fdiff() == sinh(x)\n    raises(ArgumentIndexError, lambda: c.fdiff(2))\n    \n    # Test fdiff method of tanh\n    t = tanh(x)\n    assert t.fdiff() == 1 - tanh(x)**2\n    raises(ArgumentIndexError, lambda: t.fdiff(2))\n    \n    # Test fdiff method of coth\n    ct = coth(x)\n    assert ct.fdiff() == -1 / sinh(x)**2\n    raises(ArgumentIndexError, lambda: ct.fdiff(2))\n    \n    # Test fdiff method of csch\n    cs = csch(x)\n    assert cs.fdiff() == -coth(x) * csch(x)\n    raises(ArgumentIndexError, lambda: cs.fdiff(2))\n    \n    # Test fdiff method of sech\n    se = sech(x)\n    assert se.fdiff() == -tanh(x) * sech(x)\n    raises(ArgumentIndexError, lambda: se.fdiff(2))\n    \n    # Test fdiff method of asinh\n    a_sinh = asinh(x)\n    assert a_sinh.fdiff() == 1 / sqrt(x**2 + 1)\n    raises(ArgumentIndexError, lambda: a_sinh.fdiff(2))\n    \n    # Test fdiff method of acosh\n    a_cosh = acosh(x)\n    assert a_cosh.fdiff() == 1 / sqrt(x**2 - 1)\n    raises(ArgumentIndexError, lambda: a_cosh.fdiff(2))\n    \n    # Test fdiff method of atanh\n    a_tanh = atanh(x)\n    assert a_tanh.fdiff() == 1 / (1 - x**2)\n    raises(ArgumentIndexError, lambda: a_tanh.fdiff(2))\n    \n    # Test fdiff method of acoth\n    a_coth = acoth(x)\n    assert a_coth.fdiff() == 1 / (1 -", "def test_sech_rewrite():\n    x = Symbol('x')\n\n    assert sech(x).rewrite(exp) == 1 / (exp(x)/2 + exp(-x)/2)\n    assert sech(x).rewrite(sinh) == I/sinh(x + I*pi/2)\n    assert sech(x).rewrite(cosh) == 2 / (exp(x) + exp(-x))\n    assert sech(x).rewrite(tanh) == (1 - tanh(S.Half * x)**2) / (1 + tanh(S.Half * x)**2)\n    assert sech(x).rewrite(coth) == (coth(S.Half * x)**2 - 1) / (coth(S.Half * x)**2 + 1)\n", "def test_sech_csch_simplifications():\n    x = Symbol('x')\n    assert sech(asinh(x)) == 1/sqrt(1 + x**2)\n    assert sech(acosh(x)) == 1/x\n    assert sech(atanh(x)) == sqrt(1 - x**2)\n    assert sech(acoth(x)) == sqrt(x - 1) * sqrt(x + 1)/x\n\n    assert csch(asinh(x)) == 1/x\n    assert csch(acosh(x)) == 1/(sqrt(x - 1) * sqrt(x + 1))\n    assert csch(atanh(x)) == sqrt(1 - x**2)/x\n    assert csch(acoth(x)) == sqrt(x - 1) * sqrt(x + 1)\n"], "sample_991": ["def test_eval_conjugate():\n    # Test the conjugate evaluation of a product\n    x, y, i, n = symbols('x y i n', complex=True)\n    p = Product(x*y, (i, 1, n)).conjugate()\n    assert p == Product(conjugate(x*y), (i, 1, n))\n    assert p.doit() == conjugate(x*y)**n\n", "def test_eval_is_zero():\n    i = Symbol(\"i\", integer=True)\n    assert Product(0, (i, 1, 5))._eval_is_zero() is True\n    assert Product(1, (i, 1, 5))._eval_is_zero() is False\n    assert Product(i, (i, 1, 5))._eval_is_zero() is False\n", "def test_eval_rewrite_as_Sum():\n    # Test the _eval_rewrite_as_Sum method\n    expr = Product(2**k, (k, 1, n))\n    rewritten_expr = expr.rewrite(Sum)\n    expected_expr = exp(Sum(log(2**k), (k, 1, n)))\n    assert rewritten_expr == expected_expr\n\n    expr = Product(k, (k, 1, 5))\n    rewritten_expr = expr.rewrite(Sum)\n    expected_expr = exp(Sum(log(k), (k, 1, 5)))\n    assert rewritten_expr == expected_expr\n\n    expr = Product(x**k, (k, 1, n))\n    rewritten_expr = expr.rewrite(Sum)\n    expected_expr = exp(Sum(log(x**k), (k, 1, n)))\n    assert rewritten_expr == expected_expr\n", "def test_issue_14123():\n    n = Symbol('n', integer=True)\n    assert Product(2*n + 1, (n, 1, 5)).doit() == 2027025\n    assert Product(n/2, (n, 2, 5)).doit() == Rational(5, 8)\n    assert Product(n**2, (n, 1, 4)).doit() == 576\n    assert Product(3**n, (n, 1, 3)).doit() == 162\n", "def test_issue_9999():\n    # Test for product with upper limit being inclusive and expression with multiple factors\n    i = symbols('i', integer=True)\n    assert product(i * (i + 1), (i, 1, 4)).doit() == 24 * 20  # 1*2 * 2*3 * 3*4 * 4*5 = 2*3*4*5*6 = 24*20\n    assert product((i + 1) * (i + 2), (i, 1, 3)).doit() == 2 * 3 * 3 * 4 * 4 * 5  # = 6 * 12 * 20 = 1440\n\n    # Test for product with mixed polynomial and exponential terms\n    a, b = symbols('a b', integer=True)\n    assert product(a**i * exp(b*i), (i, 1, 3)).doit() == (a**6) * exp(6*b)  # a^1 * e^(b*1) * a^2 * e^(b*2) * a^3 * e^(b*3) = (a^6) * e^(6*b)\n"], "sample_992": ["def test_MpmathPrinter():\n    p = MpmathPrinter()\n    expr = acos(x)\n    assert 'mpmath' not in p.module_imports\n    assert p.doprint(expr) == 'mpmath.acos(x)'\n    assert 'mpmath' in p.module_imports\n    assert p.doprint(pi) == 'mpmath.pi'\n    assert p.module_imports == {'mpmath': {'acos', 'pi'}}\n", "def test_MpmathPrinter():\n    p = MpmathPrinter()\n    expr = acos(x)\n    assert 'mpmath' not in p.module_imports\n    assert p.doprint(expr) == 'mpmath.acos(x)'\n    assert 'mpmath' in p.module_imports\n    assert p.doprint(Assignment(x, 2.5)) == 'x = mpmath.mpf((0, 2, 12500000000000000, -1))'\n    assert p.doprint(Symbol('pi')) == 'mpmath.pi'\n    assert 'mpmath' in p.module_imports\n    assert p.doprint(Symbol('log2', real=True)(x)) == 'mpmath.log(x)/mpmath.log(2)'\n", "def test_MpmathPrinter():\n    p = MpmathPrinter()\n    expr = acos(x)\n    assert 'mpmath' not in p.module_imports\n    assert p.doprint(expr) == 'mpmath.acos(x)'\n    assert 'mpmath' in p.module_imports\n    assert p.doprint(pi) == 'mpmath.pi'\n    assert 'mpmath' in p.module_imports\n    from sympy import uppergamma, lowergamma, log1p, log2\n    assert p.doprint(uppergamma(x, y)) == 'mpmath.gammainc(x, y, mpmath.inf)'\n    assert p.doprint(lowergamma(x, y)) == 'mpmath.gammainc(x, 0, y)'\n    assert p.doprint(log1p(x)) == 'mpmath.log(x + 1)'\n    assert p.doprint(log2(x)) == 'mpmath.log(x)/mpmath.log(2)'\n\n", "def test_MpmathPrinter():\n    p = MpmathPrinter()\n    expr = acos(x)\n    assert 'mpmath' not in p.module_imports\n    assert p.doprint(expr) == 'mpmath.acos(x)'\n    assert 'mpmath' in p.module_imports\n\n    # Test for Float printing\n    from sympy import Float\n    expr = Float(1.23)\n    assert p.doprint(expr) == 'mpmath.mpf((1, 12300000000000001, -52, 53))'\n\n    # Test for uppergamma function\n    from sympy import uppergamma\n    expr = uppergamma(x, y)\n    assert p.doprint(expr) == 'mpmath.gammainc(x, y, mpmath.inf)'\n\n    # Test for lowergamma function\n    from sympy import lowergamma\n    expr = lowergamma(x, y)\n    assert p.doprint(expr) == 'mpmath.gammainc(x, 0, y)'\n\n    # Test for log2 function\n    from sympy import log2\n    expr = log2(x)\n    assert p.doprint(expr) == 'mpmath.log(x)/mpmath.log(2)'\n\n    # Test for log1p function\n    from sympy import log1p\n    expr = log1p(x)\n    assert p.doprint(expr) == 'mpmath.log(x+1)'\n", "def test_MpmathPrinter():\n    p = MpmathPrinter()\n    expr = acos(x)\n    assert 'mpmath' not in p.module_imports\n    assert p.doprint(expr) == 'mpmath.acos(x)'\n    assert 'mpmath' in p.module_imports\n    assert p.doprint(pi) == 'mpmath.pi'\n    assert p.module_imports == {'mpmath': {'acos', 'pi'}}\n    float_expr = p.doprint(sympify('1.23'))\n    assert float_expr.startswith('mpmath.mpf(')\n    assert p.doprint(sympify('log2(x)')) == 'mpmath.log(x)/mpmath.log(2)'\n    assert p.doprint(sympify('log1p(x)')) == 'mpmath.log(x+1)'\n    assert p.doprint(sympify('uppergamma(x, y)')) == 'mpmath.gammainc(x, y, mpmath.inf)'\n    assert p.doprint(sympify('lowergamma(x, y)')) == 'mpmath.gammainc(x, 0, y)'\n"], "sample_993": ["def test_xfree_group():\n    F, (a, b, c) = xfree_group(\"a, b, c\")\n    assert len(F.generators) == 3\n    assert a**2*b**-1 in F\n    assert type(a**2*b**-1) == FreeGroup(\"a, b, c\").dtype\n    assert (a*b).inverse() == b**-1*a**-1\n    assert (a*b)**2 == a*b*a*b\n", "def test_FreeGroupElm_cyclic_conjugates():\n    w = x*y*x*y*x\n    assert w.cyclic_conjugates() == {x*y*x**2*y, x**2*y*x*y, y*x*y*x**2, y*x**2*y*x, x*y*x*y*x}\n    s = x*y*x**2*y*x\n    assert s.cyclic_conjugates() == {x**2*y*x**2*y, y*x**2*y*x**2, x*y*x**2*y*x}\n", "def test_FreeGroupElm_cyclic_operations():\n    w = x*y*x*y*x\n    assert w.cyclic_subword(1, 6) == y*x*y*x*y\n    assert w.cyclic_conjugates() == {x*y*x**2*y, x**2*y*x*y, y*x*y*x**2, y*x**2*y*x, x*y*x*y*x}\n    assert (x**2*y**2*x**-1).is_cyclically_reduced() == False\n    assert (y*x**2*y**2).is_cyclically_reduced() == True\n    assert (x**2*y**2*x**-1).identity_cyclic_reduction() == x*y**2\n    assert (x**-3*y**-1*x**5).identity_cyclic_reduction() == x**2*y**-1\n    assert (x**-3*y**-1*x**5).cyclic_reduction() == y**-1*x**2\n    assert (x**-3*y**-1*x**5).cyclic_reduction(removed=True) == (y**-1*x**2, x**-3)\n    assert ((x*y)**2).power_of(x*y) == True\n    assert (x**-3*y**-2*x**3).power_of(x**-3*y*x**3) == True\n", "def test_FreeGroup_clone():\n    G, a, b, c = free_group(\"a, b, c\")\n    H = G.clone()\n    assert G == H\n    assert G.generators == H.generators\n    assert G.symbols == H.symbols\n    assert len(G.generators) == len(H.generators)\n    assert str(G) == str(H)\n", "def test_FreeGroupElm_cyclic_operations():\n    w = x*y**2*x*y**2\n    assert w.cyclic_conjugates() == {x*y**2*x*y**2, y**2*x*y**2*x, y*x*y**2*x*y, x*y*x*y**2*x}\n    assert (x*y*x).is_cyclic_conjugate(y*x*y) == False\n    assert (x*y**2).is_cyclic_conjugate(y**2*x) == False\n    assert (x*y*x**2*y).is_cyclic_conjugate(y*x**2*y*x) == True\n    assert (x**2*y**-1*x).cyclic_subword(2, 5) == y**-1*x*x**2\n    assert (x*y*x**2).identity_cyclic_reduction() == x**2*y\n    assert (x*y*x**2*y).cyclic_reduction() == y*x**2\n"], "sample_994": ["def test_mpf_norm():\n    # Test zero handling\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)\n    \n    # Test non-zero handling\n    assert mpf_norm((1, 123456789, -10, 30), 53) == (1, 123456789, -10, 30)\n    \n    # Test precision handling\n    assert mpf_norm((1, 123456789, -10, 30), 20) == (1, 1234, -6, 20)\n\n    # Test handling of infinity and NaN\n    assert mpf_norm(_mpf_inf, 53) == _mpf_inf\n    assert mpf_norm(_mpf_ninf, 53) == _mpf_ninf\n    assert mpf_norm(_mpf_nan, 53) == _mpf_nan\n", "def test_mpf_norm_special_cases():\n    # Ensure correct normalization behavior for special cases\n    assert mpf_norm((1, 0, -123, 0), 10) == _mpf_zero  # Ensure normalization of zero\n    assert mpf_norm((0, 1, 0, 1), 10) == (0, 1, 0, 1)  # Normal case, no change\n    assert mpf_norm((1, 1, 0, 1), 10) == (1, 1, 0, 1)  # Normal case, no change\n", "def test_mpf_norm_special_cases():\n    # Test special cases in mpf_norm function\n    assert mpf_norm((1, 0, 0, 0), 10) == _mpf_zero  # Check for zero normalization\n    assert mpf_norm((0, 0, 0, 0), 10) == _mpf_zero  # Check for zero normalization\n    assert mpf_norm((1, 0, 1, 0), 10) == _mpf_zero  # Check for normalization to zero when mantissa is zero but bc is not zero\n    assert mpf_norm((0, 1, 0, 1), 10) == _mpf_nan   # Check for normalization to NaN when mantissa is non-zero\n    assert mpf_norm((0, 1, 0, 0), 10) == (0, 1, 0, 1)  # Check for mantissa normalization\n", "def test_mpf_norm_special_cases():\n    # Test special cases for mpf_norm function, specifically finf, fninf and fnan.\n    from mpmath.libmp.libmpf import finf, fninf, fnan\n    assert mpf_norm(finf, 53) == finf\n    assert mpf_norm(fninf, 53) == fninf\n    assert mpf_norm(fnan, 53) == fnan\n\n    assert Float._new(finf, 53) == Float('inf')\n    assert Float._new(fninf, 53) == Float('-inf')\n    assert Float._new(fnan, 53) == S.NaN\n", "def test_mpf_norm():\n    from sympy.core.numbers import mpf_norm\n    from mpmath.libmp.libmpf import _normalize, from_man_exp\n    import mpmath.libmp as mlib\n\n    rnd = mlib.round_nearest\n\n    # Normal case\n    mpf = (0, 12345, -10, 15)\n    assert mpf_norm(mpf, 15) == _normalize(mpf, 15, rnd)\n\n    # Zero mantissa with non-zero exponent\n    mpf = (0, 0, 1, 0)\n    assert mpf_norm(mpf, 10) == from_man_exp(0, 1)._mpf_\n\n    # Zero mantissa with zero exponent\n    mpf = (0, 0, 0, 0)\n    assert mpf_norm(mpf, 10) == from_man_exp(0, 0)._mpf_\n\n    # Positive infinity\n    mpf = mlib.finf\n    assert mpf_norm(mpf, 10) == mlib.finf\n\n    # Negative infinity\n    mpf = mlib.fninf\n    assert mpf_norm(mpf, 10) == mlib.fninf\n\n    # NaN\n    mpf = mlib.fnan\n    assert mpf_norm(mpf, 10) == mlib.fnan\n"], "sample_995": ["def test_mpf_norm_special_cases():\n    # Test the mpf_norm function with special mpf tuples\n    assert mpf_norm((1, 0, 1, 0), 10) == mpf('0')._mpf_  # Zero case\n    assert mpf_norm((0, 0, 0, 0), 10) == mpf('0')._mpf_  # Another zero case\n    assert mpf_norm((0, 1, 0, 1), 10) == mpf('1')._mpf_  # One case\n    assert mpf_norm((1, 1, 0, 1), 10) == mpf('-1')._mpf_  # Negative one case\n    assert mpf_norm((1, 0, 1000, 10), 10) == mpf('0')._mpf_  # Edge case for large exponent\n    assert mpf_norm((0, 1, -1000, 10), 10) == mpf('1e-1000')._mpf_  # Very small number\n", "def test_mpf_norm_special_cases():\n    # Special cases for mpf_norm function\n    assert mpf_norm((1, 0, 0, 0), 10) == (0, 0, 0, 0)  # normalize zero\n    assert mpf_norm((1, 0, 1, 1), 10) == (1, 0, 1, 1)  # mantissa is zero but bc is not\n    assert mpf_norm((0, 1, 1, 0), 10) == (0, 1, 1, 1)  # positive infinite\n    assert mpf_norm((1, 1, 1, 0), 10) == (1, 1, 1, 1)  # negative infinite\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)  # already normalized zero\n\n    # Test normalization for special mpf values (nan, inf, -inf, zero)\n    assert Float._new((0, 0, 0, 0), 10)._mpf_ == mpf('0')._mpf_  # zero\n    assert Float._new((0, 0, 1, 0), 10)._mpf_ == mpf('inf')._mpf_  # positive infinity\n    assert Float._new((1, 0, 1, 0), 10)._mpf_ == mpf('-inf')._mpf_  # negative infinity\n    assert Float._new((0, 0, 0, 0), 10)._mpf_ == mpf('nan')._mpf_  # nan case\n", "def test_issue_14157():\n    assert sqrt(Rational(4, 9)) == Rational(2, 3)\n    assert sqrt(Rational(4, 9)).evalf() == Float(2) / 3\n    assert sqrt(Rational(4, 9)).evalf().is_Rational is None\n    assert sqrt(Rational(4, 9)).is_Rational is True\n", "def test_mpf_norm_special_cases():\n    # Testing special cases in mpf_norm\n    assert mpf_norm(_mpf_zero, 53) == _mpf_zero\n    assert mpf_norm(_mpf_inf, 53) == _mpf_inf\n    assert mpf_norm(_mpf_ninf, 53) == _mpf_ninf\n    assert mpf_norm(_mpf_nan, 53) == _mpf_nan\n", "def test_mpf_norm():\n    # mpf_norm should handle zero mantissa correctly\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)  # should return mpf zero\n    assert mpf_norm((1, 0, 0, 0), 10) == _mpf_zero  # should return mpf zero\n    # Check non-zero mantissa handling\n    assert mpf_norm((0, 1, 1, 1), 10) == (0, 1, 1, 1)  # should normalize normally\n    # Check inf, -inf, and nan\n    assert mpf_norm(_mpf_inf, 10) == _mpf_inf\n    assert mpf_norm(_mpf_ninf, 10) == _mpf_ninf\n    assert mpf_norm(_mpf_nan, 10) == _mpf_nan\n"], "sample_996": ["def test_product_exponential():\n    f = Function('f')\n    i = Symbol(\"i\", integer=True)\n\n    # Test the product of an exponential function\n    assert product(exp(f(i)), (i, 1, n)) == exp(Sum(f(i), (i, 1, n)))\n    \n    # Test with specific exponential function\n    assert product(exp(i), (i, 1, 3)).doit() == exp(1) * exp(2) * exp(3)\n\n    # Test edge case of an exponential function with zero range\n    assert product(exp(i), (i, 1, 1)).doit() == exp(1)\n", "def test_product_with_polynomial_terms():\n    p = Product(k**2 + 2*k + 1, (k, 1, n)).doit()\n    expected = Product((k+1)**2, (k, 1, n)).doit()\n    assert p == expected\n", "def test_doit_deep_false():\n    assert Product(2**k, (k, 1, 4)).doit(deep=False) == 2**Product(k, (k, 1, 4))\n    assert Product(k + k**2, (k, 1, 3)).doit(deep=False) == (k + k**2) * (k + k**2) * (k + k**2)\n", "def test_product_doit():\n    # Test the `doit` method of the `Product` class with various examples\n    i = Symbol(\"i\", integer=True)\n\n    # Simple finite product\n    P1 = Product(i, (i, 1, 5))\n    assert P1.doit() == 120\n\n    # Product with a function\n    P2 = Product(2**i, (i, 1, 4))\n    assert P2.doit() == 2**10\n\n    # Product with symbolic limits\n    a, b = symbols('a b', integer=True)\n    P3 = Product(i, (i, a, b))\n    assert P3.doit() == factorial(b) / factorial(a - 1)\n\n    # Product with negative limits\n    P4 = Product(2**i, (i, 5, 1))\n    assert P4.doit() == 1 / (2**(1 + 2 + 3 + 4 + 5))\n\n    # Infinite product that does not converge\n    P5 = Product(1 + 1/i, (i, 1, oo))\n    assert P5.doit() == Product(1 + 1/i, (i, 1, oo))\n\n    # Infinite product that converges\n    P6 = Product(exp(-1/i**2), (i, 1, oo))\n    assert P6.doit() == exp(Sum(-1/i**2, (i, 1, oo)).doit())\n\n    # Product with a mix of symbolic and numeric limits\n    P7 = Product(i**2, (i, 2, n))\n    assert P7.doit() == factorial(n)**2 / factorial(n-2)**2\n\n    # Product with a function that simplifies\n    f = Function('f')\n    P8 = Product(f(i), (i, 1, 3))\n    assert P8.doit() == f(1) * f(2) * f(3)\n", "def test_issue_15000():\n    # Test for a product of an exponential function\n    assert product(exp(k), (k, 1, n)) == exp(Sum(k, (k, 1, n)).doit())\n    assert product(exp(k**2), (k, 1, n)) == exp(Sum(k**2, (k, 1, n)).doit())\n\n    # Test for a product with a polynomial\n    assert product(k**3 - k + 1, (k, 1, n)) == Product(k**3 - k + 1, (k, 1, n)).doit()\n\n    # Test for a product involving trigonometric functions\n    assert product(sin(k * pi / 2), (k, 1, n)).rewrite(Sum) == exp(Sum(log(sin(k * pi / 2)), (k, 1, n)))\n"], "sample_997": ["def test_exponentiation_of_functions():\n    transformations = standard_transformations + \\\n                      (function_exponentiation,)\n    x = Symbol('x')\n    assert parse_expr(\"sin**2(x)\", transformations=transformations) == sin(x)**2\n    assert parse_expr(\"exp**3(x)\", transformations=transformations) == exp(x)**3\n    assert parse_expr(\"cos**4(x+y)\", transformations=transformations, local_dict={'cos': sin}) == sin(x+y)**4\n", "def test_function_exponentiation():\n    transformations = standard_transformations + (split_symbols, implicit_multiplication_application, function_exponentiation)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"sin**2(x)\", transformations=transformations) == sin(x)**2\n    assert parse_expr(\"cos**2(y)\", transformations=transformations) == Function('cos')(y)**2\n    assert parse_expr(\"tan**3(x + y)\", transformations=transformations) == Function('tan')(x + y)**3\n", "def test_function_exponentiation():\n    transformations = standard_transformations + (function_exponentiation,)\n    x = Symbol('x')\n    sin = Function('sin')\n    assert parse_expr('sin**2(x)', transformations=transformations) == sin(x)**2\n    assert parse_expr('sin**2 x', transformations=transformations) == sin(x)**2\n    assert parse_expr('sin**2(x) + cos**2(x)', transformations=transformations) == sin(x)**2 + cos(x)**2\n    assert parse_expr('tan**2(x)', transformations=transformations) == tan(x)**2\n    assert parse_expr('tan**2 x', transformations=transformations) == tan(x)**2\n    assert parse_expr('exp**3(x)', transformations=transformations) == exp(x)**3\n    assert parse_expr('exp**3 x', transformations=transformations) == exp(x)**3\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"lambda x: x + 2\", transformations=transformations) == Function('Lambda')((x,), x + 2)\n    assert parse_expr(\"lambda x, y: x + y\", transformations=transformations) == Function('Lambda')((x, y), x + y)\n    raises(TokenError, lambda: parse_expr(\"lambda *args: sum(args)\", transformations=transformations))\n", "def test_lambda_notation():\n    inputs = {\n        'lambda x: x + 1': Function('Lambda')(Symbol('x'), Symbol('x') + 1),\n        'lambda x, y: x + y': Function('Lambda')((Symbol('x'), Symbol('y')), Symbol('x') + Symbol('y')),\n        'lambda x: x * 2': Function('Lambda')(Symbol('x'), Symbol('x') * 2),\n        'lambda x, y, z: x + y + z': Function('Lambda')((Symbol('x'), Symbol('y'), Symbol('z')), Symbol('x') + Symbol('y') + Symbol('z')),\n    }\n    transformations = standard_transformations + (lambda_notation,)\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n"], "sample_998": ["def test_latex_Divergence():\n    A = CoordSys3D('A')\n    assert latex(Divergence(A.i)) == r\"\\nabla\\cdot \\mathbf{\\hat{i}_{A}}\"\n    assert latex(Divergence(A.j * A.x)) == r\"\\nabla\\cdot \\left(\\mathbf{\\hat{j}_{A}} \\mathbf{{x}_{A}}\\right)\"\n    assert latex(Divergence(3 * A.x * A.j + A.i)) == r\"\\nabla\\cdot \\left(\\mathbf{\\hat{i}_{A}} + 3 \\mathbf{{x}_{A}} \\mathbf{\\hat{j}_{A}}\\right)\"\n    assert latex(Divergence(A.x * A.j) * A.y) == r\"\\left(\\nabla\\cdot \\left(\\mathbf{\\hat{j}_{A}} \\mathbf{{x}_{A}}\\right)\\right) \\mathbf{{y}_{A}}\"\n", "def test_latex_Limit():\n    expr = Limit(sin(x)/x, x, 0)\n    assert latex(expr) == r\"\\lim_{x \\to 0} \\frac{\\sin{\\left (x \\right )}}{x}\"\n    expr = Limit((1 + 1/x)**x, x, oo)\n    assert latex(expr) == r\"\\lim_{x \\to \\infty} \\left(1 + \\frac{1}{x}\\right)^{x}\"\n    expr = Limit(x**2, x, 1, '+')\n    assert latex(expr) == r\"\\lim_{x \\to 1^+} x^{2}\"\n    expr = Limit(x**2, x, 1, '-')\n    assert latex(expr) == r\"\\lim_{x \\to 1^-} x^{2}\"\n    expr = Limit(sin(x)/x, x, 0, '+')\n    assert latex(expr) == r\"\\lim_{x \\to 0^+} \\frac{\\sin{\\left (x \\right )}}{x}\"\n    expr = Limit(sin(x)/x, x, 0, '-')\n    assert latex(expr) == r\"\\lim_{x \\to 0^-} \\frac{\\sin{\\left (x \\right )}}{x}\"\n", "def test_latex_order_options():\n    expr = x**3 + x**2*y + 3*x*y**3 + y**4\n\n    # Test for 'lex' order\n    assert latex(expr, order='lex') == \"x^{3} + x^{2} y + 3 x y^{3} + y^{4}\"\n    \n    # Test for 'rev-lex' order\n    assert latex(expr, order='rev-lex') == \"y^{4} + 3 x y^{3} + x^{2} y + x^{3}\"\n    \n    # Test for 'none' order\n    assert latex(expr, order='none') == \"x^{3} + x^{2} y + 3 x y^{3} + y^{4}\"\n    \n    # Test for 'old' order\n    assert latex(expr, order='old') == \"x^{3} + x^{2} y + 3 x y^{3} + y^{4}\"\n", "def test_latex_modifiers_with_greek_letters():\n    assert latex(symbols(\"alphaHat\")) == r\"\\hat{\\alpha}\"\n    assert latex(symbols(\"betaCheck\")) == r\"\\check{\\beta}\"\n    assert latex(symbols(\"gammaBreve\")) == r\"\\breve{\\gamma}\"\n    assert latex(symbols(\"deltaAcute\")) == r\"\\acute{\\delta}\"\n    assert latex(symbols(\"epsilonGrave\")) == r\"\\grave{\\epsilon}\"\n    assert latex(symbols(\"zetaTilde\")) == r\"\\tilde{\\zeta}\"\n    assert latex(symbols(\"thetaPrime\")) == r\"{\\theta}'\"\n    assert latex(symbols(\"iotaDdDot\")) == r\"\\dddot{\\iota}\"\n    assert latex(symbols(\"kappaDDot\")) == r\"\\ddot{\\kappa}\"\n    assert latex(symbols(\"lambdaBold\")) == r\"\\boldsymbol{\\lambda}\"\n    assert latex(symbols(\"muNorm\")) == r\"\\left\\|{\\mu}\\right\\|\"\n    assert latex(symbols(\"nuHat\")) == r\"\\hat{\\nu}\"\n    assert latex(symbols(\"xiDot\")) == r\"\\dot{\\xi}\"\n    assert latex(symbols(\"omicronBar\")) == r\"\\bar{o}\"\n    assert latex(symbols(\"piVec\")) == r\"\\vec{\\pi}\"\n    assert latex(symbols(\"rhoAbs\")) == r\"\\left|{\\rho}\\right|\"\n    assert latex(symbols(\"sigmaMag\")) == r\"\\left|{\\sigma}\\right|\"\n    assert latex(symbols(\"tauPrM\")) == r\"{\\tau}'\"\n    assert latex(symbols(\"upsilonBM\")) == r\"\\boldsymbol{\\upsilon}\"\n", "def test_latex_beta_function():\n    assert latex(beta(x, y)) == r'\\operatorname{B}\\left(x, y\\right)'\n    assert latex(beta(x, y)**2) == r'\\operatorname{B}^{2}\\left(x, y\\right)'\n    assert latex(beta(x + 1, y - 1)) == r'\\operatorname{B}\\left(x + 1, y - 1\\right)'\n    assert latex(beta(x**2, y**3)) == r'\\operatorname{B}\\left(x^{2}, y^{3}\\right)'\n"], "sample_999": ["def test_latex_Integral():\n    expr = Integral(x**2, x)\n    assert latex(expr) == r\"\\int x^{2}\\, dx\"\n    assert latex(expr, mode='equation') == r\"\\begin{equation}\\int x^{2}\\, dx\\end{equation}\"\n    assert latex(expr, mode='equation*') == r\"\\begin{equation*}\\int x^{2}\\, dx\\end{equation*}\"\n    assert latex(expr, mode='inline') == r\"$\\int x^{2}\\, dx$\"\n    assert latex(expr, itex=True) == r\"$$\\int x^{2}\\, dx$$\"\n    assert latex(expr, mode='plain') == r\"\\int x^{2}\\, dx\"\n", "def test_latex_DiracDelta_with_Heaviside():\n    # Test for DiracDelta combined with Heaviside\n    expr = DiracDelta(x)*Heaviside(x)\n    assert latex(expr) == r\"\\delta\\left(x\\right) \\theta\\left(x\\right)\"\n    expr = DiracDelta(x, 1)*Heaviside(x - 1)\n    assert latex(expr) == r\"\\delta^{\\left( 1 \\right)}\\left( x \\right) \\theta\\left(x - 1\\right)\"\n", "def test_latex_Quaternion():\n    q = Quaternion(1, 0, 0, 0)\n    assert latex(q) == \"1\"\n    q = Quaternion(0, 1, 0, 0)\n    assert latex(q) == \"1 i\"\n    q = Quaternion(0, 0, 1, 0)\n    assert latex(q) == \"1 j\"\n    q = Quaternion(0, 0, 0, 1)\n    assert latex(q) == \"1 k\"\n    q = Quaternion(1, 1, 1, 1)\n    assert latex(q) == \"1 + 1 i + 1 j + 1 k\"\n    q = Quaternion(-1, -1, -1, -1)\n    assert latex(q) == \"-1 - 1 i - 1 j - 1 k\"\n", "def test_latex_printers():\n    expr = Mul(Symbol('x'), Add(Symbol('y'), 1, evaluate=False), evaluate=False)\n    assert latex(expr) == r'x \\left(y + 1\\right)'\n\n    expr = Pow(Add(Symbol('a'), Symbol('b'), evaluate=False), 2, evaluate=False)\n    assert latex(expr) == r'\\left(a + b\\right)^{2}'\n\n    expr = Add(Symbol('m'), Mul(Symbol('n'), Symbol('o'), evaluate=False), evaluate=False)\n    assert latex(expr) == r'm + n o'\n\n    expr = Mul(Add(Symbol('p'), Symbol('q'), evaluate=False), Symbol('r'), evaluate=False)\n    assert latex(expr) == r'\\left(p + q\\right) r'\n\n    expr = Add(Pow(Symbol('s'), 2, evaluate=False), Symbol('t'), evaluate=False)\n    assert latex(expr) == r's^{2} + t'\n", "def test_latex_modifiers_and_greek_combined():\n    # Test combination of modifiers and Greek letters\n    assert latex(Symbol('alphaHatDot')) == r\"\\dot{\\hat{\\alpha}}\"\n    assert latex(Symbol('GammaBreveBar')) == r\"\\bar{\\breve{\\Gamma}}\"\n    assert latex(Symbol('OmegaTildeVec')) == r\"\\vec{\\tilde{\\Omega}}\"\n    assert latex(Symbol('PsiMathringPrime')) == r\"{\\mathring{\\Psi}}'\"\n    assert latex(Symbol('thetaDDotBold')) == r\"\\boldsymbol{\\ddot{\\theta}}\"\n    assert latex(Symbol('phiCheckGrave')) == r\"\\grave{\\check{\\phi}}\"\n    assert latex(Symbol('chiAcuteAbs')) == r\"\\left|{\\acute{\\chi}}\\right|\"\n    assert latex(Symbol('kappaGraveNorm')) == r\"\\left\\|{\\grave{\\kappa}}\\right\\|\"\n    assert latex(Symbol('lambdaVecHat')) == r\"\\hat{\\vec{\\lambda}}\"\n"], "sample_1000": ["def test_user_defined_functions():\n    from sympy import Function, symbols\n\n    class MyFunction(Function):\n        pass\n\n    a, b = symbols('a b')\n    custom_functions = {\n        \"MyFunction\": \"my_custom_function\"\n    }\n    \n    expr = MyFunction(a + b)\n    assert mcode(expr, user_functions=custom_functions) == 'my_custom_function(a + b)'\n\n    custom_functions_with_conditions = {\n        \"MyFunction\": [(lambda x: x.is_Matrix, \"matrix_function\"), (lambda x: not x.is_Matrix, \"scalar_function\")]\n    }\n    \n    # Test with scalar arguments\n    assert mcode(MyFunction(a), user_functions=custom_functions_with_conditions) == 'scalar_function(a)'\n\n    # Test with matrix arguments\n    mat = Matrix([[1, 2], [3, 4]])\n    assert mcode(MyFunction(mat), user_functions=custom_functions_with_conditions) == 'matrix_function([1 2; 3 4])'\n", "def test_user_defined_functions():\n    from sympy import Function, Lambda\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_f\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_mat_g\"),\n              (lambda x: not x.is_Matrix, \"custom_g\")]\n    }\n    assert mcode(f(x) + g(x), user_functions=custom_functions) == \"custom_f(x) + custom_g(x)\"\n    mat = Matrix([[1, x]])\n    assert mcode(g(mat), user_functions=custom_functions) == \"custom_mat_g([1 x])\"\n", "def test_trigonometric_functions():\n    assert mcode(sin(x) + cos(x)) == \"cos(x) + sin(x)\"\n    assert mcode(tan(x) + cot(x)) == \"cot(x) + tan(x)\"\n    assert mcode(sec(x) + csc(x)) == \"csc(x) + sec(x)\"\n    assert mcode(asin(x) + acos(x)) == \"acos(x) + asin(x)\"\n    assert mcode(atan(x) + acot(x)) == \"acot(x) + atan(x)\"\n    assert mcode(asec(x) + acsc(x)) == \"acsc(x) + asec(x)\"\n    assert mcode(sinh(x) + cosh(x)) == \"cosh(x) + sinh(x)\"\n    assert mcode(tanh(x) + coth(x)) == \"coth(x) + tanh(x)\"\n    assert mcode(csch(x) + sech(x)) == \"csch(x) + sech(x)\"\n    assert mcode(asinh(x) + acosh(x)) == \"acosh(x) + asinh(x)\"\n    assert mcode(atanh(x) + acoth(x)) == \"acoth(x) + atanh(x)\"\n    assert mcode(asech(x) + acsch(x)) == \"acsch(x) + asech(x)\"\n", "def test_user_defined_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"my_f_function\",\n        \"g\": [lambda x: x.is_Matrix, \"my_g_matrix_function\"]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(mat), user_functions=custom_functions) == \"my_f_function(x) + my_g_matrix_function([1 x])\"\n    assert mcode(f(x)*g(x), user_functions=custom_functions) == \"my_f_function(x)*g(x)\"\n", "def test_Abs_function():\n    assert mcode(abs(x)) == \"abs(x)\"\n    assert mcode(abs(x + y)) == \"abs(x + y)\"\n    assert mcode(abs(x - y)) == \"abs(x - y)\"\n    assert mcode(abs(x * y)) == \"abs(x.*y)\"\n"], "sample_1001": ["def test_latex_Divergence_Gradient():\n    from sympy.vector import CoordSys3D, Divergence, Gradient\n    N = CoordSys3D('N')\n    \n    assert latex(Divergence(N.i + N.j + N.k)) == r\"\\nabla\\cdot \\left(\\mathbf{\\hat{i}_{N}} + \\mathbf{\\hat{j}_{N}} + \\mathbf{\\hat{k}_{N}}\\right)\"\n    assert latex(Divergence(N.i*N.x + N.j*N.y + N.k*N.z)) == r\"\\nabla\\cdot \\left(\\mathbf{{x}_{N}} \\mathbf{\\hat{i}_{N}} + \\mathbf{{y}_{N}} \\mathbf{\\hat{j}_{N}} + \\mathbf{{z}_{N}} \\mathbf{\\hat{k}_{N}}\\right)\"\n    \n    assert latex(Gradient(N.x*N.y*N.z)) == r\"\\nabla \\left(\\mathbf{{x}_{N}} \\mathbf{{y}_{N}} \\mathbf{{z}_{N}}\\right)\"\n    assert latex(Gradient(N.x + N.y + N.z)) == r\"\\nabla \\left(\\mathbf{{x}_{N}} + \\mathbf{{y}_{N}} + \\mathbf{{z}_{N}}\\right)\"\n", "def test_latex_LatexPrinter():\n    printer = LatexPrinter()\n    \n    # Test with a simple Add expression\n    expr = Add(x, y)\n    assert printer._print_Add(expr) == \"x + y\"\n\n    # Test with a simple Mul expression\n    expr = Mul(x, y)\n    assert printer._print_Mul(expr) == \"x y\"\n\n    # Test with a Pow expression\n    expr = Pow(x, 2)\n    assert printer._print_Pow(expr) == \"x^{2}\"\n\n    # Test with a complex expression\n    expr = Mul(Add(x, y), Pow(z, 2))\n    assert printer._print_Mul(expr) == r\"\\left(x + y\\right) z^{2}\"\n\n    # Test with integrals\n    expr = Integral(x**2, (x, 0, 1))\n    assert printer._print_Integral(expr) == r\"\\int_{0}^{1} x^{2}\\, dx\"\n\n    # Test with sums\n    expr = Sum(x**2, (x, 1, 10))\n    assert printer._print_Sum(expr) == r\"\\sum_{x=1}^{10} x^{2}\"\n\n    # Test with derivatives\n    expr = diff(sin(x), x)\n    assert printer._print_Derivative(expr) == r\"\\frac{d}{d x} \\sin{\\left (x \\right )}\"\n\n    # Test with products\n    expr = Product(x, (x, 1, 10))\n    assert printer._print_Product(expr) == r\"\\prod_{x=1}^{10} x\"\n\n    # Test with factorial\n    expr = factorial(x)\n    assert printer._print_factorial(expr) == \"x!\"\n\n    # Test with logic expressions\n    expr = And(x, y)\n    assert printer._print_And(expr) == \"x \\\\wedge y\"\n\n    # Test with matrices\n    M = Matrix([[x, y], [z, t]])\n    assert printer._print_MatrixBase(M) == r'\\left[\\begin{matrix}x & y\\\\z & t\\end{matrix}\\right]'\n", "def test_latex_RisingFactorial():\n    assert latex(RisingFactorial(x, y)) == r\"{x}^{\\left(y\\right)}\"\n    assert latex(RisingFactorial(x + 1, y)) == r\"{x + 1}^{\\left(y\\right)}\"\n    assert latex(RisingFactorial(x, y + 1)) == r\"{x}^{\\left(y + 1\\right)}\"\n    assert latex(RisingFactorial(x, y)**2) == r\"\\left({x}^{\\left(y\\right)}\\right)^{2}\"\n", "def test_latex_tuples():\n    # Test tuples with different types of elements\n    assert latex((x, y, z)) == r\"\\left ( x, \\quad y, \\quad z\\right )\"\n    assert latex((1, x**2, sin(x))) == r\"\\left ( 1, \\quad x^{2}, \\quad \\sin{\\left (x \\right )}\\right )\"\n    assert latex((x, y, (a, b))) == r\"\\left ( x, \\quad y, \\quad \\left ( a, \\quad b\\right )\\right )\"\n    assert latex(((x, y), (a, b))) == r\"\\left ( \\left ( x, \\quad y\\right ), \\quad \\left ( a, \\quad b\\right )\\right )\"\n", "def test_latex_modifiers():\n    # Testing combination of multiple modifiers on greek letters\n    assert latex(Symbol('lambdaDot')) == r'\\dot{\\lambda}'\n    assert latex(Symbol('sigmaHat')) == r'\\hat{\\sigma}'\n    assert latex(Symbol('omegaCheck')) == r'\\check{\\omega}'\n    assert latex(Symbol('alphaTilde')) == r'\\tilde{\\alpha}'\n    assert latex(Symbol('betaBreve')) == r'\\breve{\\beta}'\n    assert latex(Symbol('thetaAcute')) == r'\\acute{\\theta}'\n    assert latex(Symbol('phiGrave')) == r'\\grave{\\phi}'\n    assert latex(Symbol('gammaBar')) == r'\\bar{\\gamma}'\n    assert latex(Symbol('psiVec')) == r'\\vec{\\psi}'\n\n    # Testing combinations with subscript and superscript\n    assert latex(Symbol('rhoHat_1')) == r'\\hat{\\rho}_{1}'\n    assert latex(Symbol('chiTilde^2')) == r'\\tilde{\\chi}^{2}'\n    assert latex(Symbol('etaBar_3^4')) == r'\\bar{\\eta}_{3}^{4}'\n\n    # Testing nested modifiers\n    assert latex(Symbol('kappaDotHat')) == r'\\hat{\\dot{\\kappa}}'\n    assert latex(Symbol('xiBreveVec')) == r'\\vec{\\breve{\\xi}}'\n    assert latex(Symbol('zetaCheckBar')) == r'\\bar{\\check{\\zeta}}'\n"], "sample_1002": ["def test_mpf_norm():\n    from sympy.core.numbers import mpf_norm\n    from mpmath.libmp import mpf\n\n    # Check if zero is correctly identified and normalized\n    assert mpf_norm((0, 0, 0, 0), 53) == mpf('0')._mpf_\n    \n    # Check normalization of positive mpf value\n    mpf_val = mpf('1.23456789')._mpf_\n    normalized_val = mpf_norm(mpf_val, 53)\n    assert mpf(normalized_val) == mpf('1.23456789')\n    \n    # Check normalization of negative mpf value\n    mpf_val = mpf('-1.23456789')._mpf_\n    normalized_val = mpf_norm(mpf_val, 53)\n    assert mpf(normalized_val) == mpf('-1.23456789')\n    \n    # Check normalization of infinity\n    assert mpf_norm((0, 0, 0, 0), 53) == mpf('inf')._mpf_\n    assert mpf_norm((1, 0, 0, 0), 53) == mpf('-inf')._mpf_\n    \n    # Check normalization of NaN\n    assert mpf_norm((0, 0, 0, 0), 53) == mpf('nan')._mpf_\n", "def test_mpf_norm_with_special_cases():\n    # testing mpf_norm for cases where mantissa is 0 but bc is non-zero\n    assert mpf_norm((0, 0, 0, 1), 10) == (0, 0, 0, 1)  # +inf\n    assert mpf_norm((1, 0, 0, 1), 10) == (1, 0, 0, 1)  # -inf\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)  # zero\n    assert mpf_norm((0, 0, 0, 0), 10) == _mpf_zero     # zero using _mpf_zero\n    assert mpf_norm((0, 0, 0, 2), 10) == (0, 0, 0, 2)  # arbitrary non-zero bc\n", "def test_issue_12345():\n    # Ensure igcdex correctly handles large inputs without overflow\n    a = 12345678901234567890\n    b = 9876543210987654321\n    x, y, g = igcdex(a, b)\n    assert a*x + b*y == g\n    # Check the returned gcd is correct\n    assert g == igcd(a, b)\n\n    # Ensure mod_inverse handles large inputs\n    large_prime = 104729\n    assert mod_inverse(2, large_prime) == 52365\n    # Test negative values\n    assert mod_inverse(-3, large_prime) == large_prime - 34843\n    # Test non-prime modulus\n    assert mod_inverse(3, 10) == 7\n    raises(ValueError, lambda: mod_inverse(3, 6))\n", "def test_mpf_norm_special_cases():\n    # Test special cases with mpf_norm to ensure proper handling of edge cases\n    assert mpf_norm((1, 0, 0, 0), 10) == (0, 0, 0, 0)  # Testing with the mpf tuple of zero\n    assert mpf_norm((0, 1, 0, 1), 10) == (0, 1, 0, 1)  # Testing with mantissa and exponent both zero\n    assert mpf_norm((0, 1, 1, 1), 10) == (0, 1, 1, 1)  # Testing a normal case with a mantissa of 1\n    assert mpf_norm((0, 1, -1, 1), 10) == (0, 1, -1, 1)  # Testing a case with negative exponent\n    assert mpf_norm((1, 1, 1000, 1001), 10) == (1, 1, 1000, 1001)  # Large exponent\n    assert mpf_norm((1, 1, -1000, 1001), 10) == (1, 1, -1000, 1001)  # Large negative exponent\n\n    # Testing with a tuple that should not be normalized\n    assert mpf_norm((0, 0, -123456789, -123456789), 10) == (0, 0, -123456789, -123456789)\n", "def test_AlgebraicNumber():\n    from sympy.polys.polytools import minimal_polynomial\n    from sympy import sqrt, I\n\n    # Test basic properties and methods of AlgebraicNumber\n    a = AlgebraicNumber(sqrt(2))\n    assert a.root == sqrt(2)\n    assert a.minpoly == minimal_polynomial(sqrt(2))\n    assert a.coeffs() == [1, 0]\n    assert a.as_expr() == sqrt(2)\n    assert a.to_algebraic_integer().root == sqrt(2)*sqrt(2)\n    \n    b = AlgebraicNumber((x**2 + 1, I))\n    assert b.root == I\n    assert b.minpoly == x**2 + 1\n    assert b.coeffs() == [1, 0]\n    assert b.as_expr() == I\n\n    c = AlgebraicNumber((x**2 - 2, sqrt(2)), [1, sqrt(2)])\n    assert c.root == sqrt(2)\n    assert c.coeffs() == [1, sqrt(2)]\n    assert c.as_expr() == sqrt(2) + sqrt(2)*sqrt(2)\n    assert c.to_algebraic_integer().root == sqrt(2)\n    \n    # Test simplification of algebraic numbers\n    expr = sqrt(2) + sqrt(3)\n    assert simplify(AlgebraicNumber(expr)).as_expr() == expr\n"], "sample_1003": ["def test_Options_init():\n    opt = Options((x, y), {'domain': 'ZZ', 'expand': True, 'wrt': 'x'})\n\n    assert opt.gens == (x, y)\n    assert opt.domain == ZZ\n    assert opt.expand is True\n    assert opt.wrt == ['x']\n\n    raises(OptionError, lambda: Options((x, y), {'invalid_option': True}))\n\n    raises(OptionError, lambda: Options((x, y), {'domain': 'ZZ', 'field': True}))\n\n    raises(OptionError, lambda: Options((x, y), {'wrt': 0}))\n", "def test_Gen_preprocess():\n    assert Gen.preprocess(x) == x\n    assert Gen.preprocess(0) == 0\n\n    raises(OptionError, lambda: Gen.preprocess('x'))\n    raises(OptionError, lambda: Gen.preprocess(1.5))\n", "def test_Composite_preprocess():\n    assert Composite.preprocess(False) is False\n    assert Composite.preprocess(True) is True\n\n    assert Composite.preprocess(0) is False\n    assert Composite.preprocess(1) is True\n\n    raises(OptionError, lambda: Composite.preprocess(x))\n\n", "def test_Options_init():\n    opt = Options((x, y), {'domain': 'ZZ', 'field': True})\n\n    assert opt.gens == (x, y)\n    assert opt.domain == ZZ\n    assert opt.field is True\n    assert 'field' in opt\n    assert 'domain' in opt\n\n    raises(OptionError, lambda: Options((x, y), {'gens': (x, y)}))\n    raises(OptionError, lambda: Options((x, y), {'domain': 'ZZ', 'field': True, 'greedy': True}))\n", "def test_Options_init_with_flags():\n    opt = Options((x, y, z), {'domain': 'ZZ', 'field': True}, flags=['field'])\n\n    assert opt.gens == (x, y, z)\n    assert opt.domain == ZZ\n    assert opt.field is True\n\n    raises(OptionError, lambda: Options((x, y, z), {'domain': 'ZZ', 'field': True}, flags=['another_flag']))\n"], "sample_1004": ["def test_combined_conditions():\n    assert ConditionSet(x, And(x > 0, x < 2), Interval(0, 3)) == Interval(0, 2)\n    assert ConditionSet(x, And(x > 1, x < 3), Interval(0, 2)) == Interval(1, 2)\n    assert ConditionSet(x, And(x > 1, x < 3), Interval(0, 1)) == EmptySet()\n    assert ConditionSet(x, Or(x < 1, x > 2), Interval(0, 3)) == Union(Interval(0, 1), Interval(2, 3))\n    assert ConditionSet(x, Or(x < 1, x > 3), Interval(0, 4)) == Union(Interval(0, 1), Interval(3, 4))\n", "def test_condition_simplification():\n    assert ConditionSet(x, True, Interval(0, 10)) == Interval(0, 10)\n    assert ConditionSet(x, False, Interval(0, 10)) == EmptySet()\n    assert ConditionSet(x, x < 10, Interval(0, 10)) == Interval(0, 10)\n    assert ConditionSet(x, x > 0, Interval(0, 10)) == Interval(0, 10)\n    assert ConditionSet(x, x > 5, Interval(0, 10)) == Interval(5, 10)\n    assert ConditionSet(x, x < 5, Interval(0, 10)) == Interval(0, 5)\n    assert ConditionSet(x, x < 5, FiniteSet(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) == FiniteSet(1, 2, 3, 4)\n    assert ConditionSet(x, x > 5, FiniteSet(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) == FiniteSet(6, 7, 8, 9, 10)\n", "def test_non_symbol_dummy():\n    # Test case where non-symbol dummy is used and is recognized in condition\n    raises(ValueError, lambda: ConditionSet(x + 1, x + 1 < 1, S.Integers))\n\n    # Test case where non-symbol dummy is used and is not recognized in condition\n    raises(ValueError, lambda: ConditionSet(x + 1, x < 1, S.Integers))\n\n    # Test with a symbol containing an assumption\n    n = Symbol('n', negative=True)\n    cond = (n > 0)\n    assert cond is False\n    assert ConditionSet(n, cond, S.Integers) == S.EmptySet\n\n    # Test case where the base set is another ConditionSet\n    c = ConditionSet(x, x < y, ConditionSet(y, x + y < 2, S.Integers))\n    assert c == ConditionSet(Symbol('lambda'), (Symbol('lambda') < y) & (Symbol('lambda') + x < 2), S.Integers)\n", "def test_invalid_conditionset():\n    raises(TypeError, lambda: ConditionSet(x, x < 1, None))\n    raises(ValueError, lambda: ConditionSet(x, x < 1, Interval(1, 7)).subs(x, 1.5))\n    raises(ValueError, lambda: ConditionSet(x, x < 1, Interval(1, 7)).subs(x, -1))\n    raises(ValueError, lambda: ConditionSet(x, x < 1, Interval(1, 7)).subs(x, Symbol('nc', commutative=False)))\n    raises(TypeError, lambda: ConditionSet(Symbol('nc', commutative=False), x < 1, Interval(1, 7)))\n    raises(ValueError, lambda: ConditionSet(Dummy('lambda'), x < 1, Interval(1, 7)).subs(Dummy('lambda'), x))\n", "def test_CondSet_with_non_symbol_dummy():\n    # Ensure ValueError is raised when dummy symbol is non-symbol\n    raises(ValueError, lambda: ConditionSet(x + 1, x + 1 < 1, S.Integers))\n    raises(ValueError, lambda: ConditionSet(x + 1, x < 1, S.Integers))\n    # Test substitution ignoring non-symbol dummy\n    assert ConditionSet(x + 1, x + 1 < 1, FiniteSet(x, y)).subs(x, 1) == ConditionSet(x + 1, x + 1 < 1, FiniteSet(1, y))\n    assert ConditionSet(x + 1, x < 1, FiniteSet(x, y)).subs(x, 1) == ConditionSet(x + 1, x < 1, FiniteSet(1, y))\n"], "sample_1005": ["def test_latex_Gradient_Mul_Cross_Curl():\n    from sympy.vector import CoordSys3D, Gradient, Divergence, Curl, Cross\n    C = CoordSys3D('C')\n    assert latex(C.gradient(C.x)) == r'\\nabla \\left( \\mathbf{\\hat{i}_{C}} \\cdot \\mathbf{{x}_{C}} \\right)'\n    assert latex(C.divergence(C.i + C.j)) == r'\\nabla \\cdot \\left( \\mathbf{\\hat{i}_{C}} + \\mathbf{\\hat{j}_{C}} \\right)'\n    assert latex(C.curl(C.i + C.j)) == r'\\nabla \\times \\left( \\mathbf{\\hat{i}_{C}} + \\mathbf{\\hat{j}_{C}} \\right)'\n    assert latex(C.i.cross(C.j)) == r'\\mathbf{\\hat{i}_{C}} \\times \\mathbf{\\hat{j}_{C}}'\n    assert latex(2 * C.i.cross(C.j)) == r'2 \\left( \\mathbf{\\hat{i}_{C}} \\times \\mathbf{\\hat{j}_{C}} \\right)'\n    assert latex(C.i.cross(2 * C.j)) == r'\\mathbf{\\hat{i}_{C}} \\times \\left( 2 \\mathbf{\\hat{j}_{C}} \\right)'\n    assert latex(C.i.cross(C.j + C.k)) == r'\\mathbf{\\hat{i}_{C}} \\times \\left( \\mathbf{\\hat{j}_{C}} + \\mathbf{\\hat{k}_{C}} \\right)'\n", "def test_latex_printer_settings():\n    expr = x * y\n    printer = LatexPrinter()\n    assert printer._settings == {\n        \"order\": None,\n        \"mode\": \"plain\",\n        \"itex\": False,\n        \"fold_frac_powers\": False,\n        \"fold_func_brackets\": False,\n        \"fold_short_frac\": None,\n        \"long_frac_ratio\": None,\n        \"mul_symbol\": None,\n        \"inv_trig_style\": \"abbreviated\",\n        \"mat_str\": None,\n        \"mat_delim\": \"[\",\n        \"symbol_names\": {},\n        \"ln_notation\": False,\n    }\n\n    custom_printer = LatexPrinter(settings={\"mode\": \"equation\", \"mul_symbol\": \"dot\"})\n    assert custom_printer._settings[\"mode\"] == \"equation\"\n    assert custom_printer._settings[\"mul_symbol\"] == \"dot\"\n\n    with raises(ValueError):\n        LatexPrinter(settings={\"mode\": \"invalid_mode\"})\n", "def test_latex_precedence():\n    expr = (x + y) * (x - y)\n    assert latex(expr) == r\"\\left(x + y\\right) \\left(x - y\\right)\"\n    \n    expr = (x + y) ** (x - y)\n    assert latex(expr) == r\"\\left(x + y\\right)^{x - y}\"\n    \n    expr = x ** (y ** z)\n    assert latex(expr) == r\"x^{y^{z}}\"\n    \n    expr = (x ** y) ** z\n    assert latex(expr) == r\"\\left(x^{y}\\right)^{z}\"\n    \n    expr = (x + y) / (x - y)\n    assert latex(expr) == r\"\\frac{x + y}{x - y}\"\n    \n    expr = x / (y * z)\n    assert latex(expr) == r\"\\frac{x}{y z}\"\n    \n    expr = 1 / (2 / x)\n    assert latex(expr) == r\"\\frac{1}{\\frac{2}{x}}\"\n    \n    expr = (x + y) ** (z + t)\n    assert latex(expr) == r\"\\left(x + y\\right)^{z + t}\"\n    \n    expr = x * (y + z)\n    assert latex(expr) == r\"x \\left(y + z\\right)\"\n    \n    expr = x + (y * z)\n    assert latex(expr) == r\"x + y z\"\n", "def test_latex_Mul_with_negative_exponent():\n    expr = Mul(x, y**-1, evaluate=False)\n    assert latex(expr) == r\"x \\frac{1}{y}\"\n    expr = Mul(-1, x, y**-1, evaluate=False)\n    assert latex(expr) == r\"- x \\frac{1}{y}\"\n", "def test_latex_modifiers_combined():\n    assert latex(symbols('xHatDotBar')) == r\"\\bar{\\dot{\\hat{x}}}\"\n    assert latex(symbols('yVecTilde')) == r\"\\tilde{\\vec{y}}\"\n    assert latex(symbols('zPrimeCheck')) == r\"\\check{z}'\"\n    assert latex(symbols('aBoldBreve')) == r\"\\breve{\\boldsymbol{a}}\"\n    assert latex(symbols('bAcuteGrave')) == r\"\\grave{\\acute{b}}\"\n    assert latex(symbols('cNormAbs')) == r\"\\left\\|{\\left|{c}\\right|}\\right\\|\"\n"], "sample_1006": ["def test_multifactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True, nonnegative=True)\n    m = Symbol('m', integer=True, nonnegative=True)\n\n    class multifactorial(MultiFactorial):\n        @classmethod\n            n, k = map(sympify, (n, k))\n            if k.is_Number and k.is_integer:\n                if k.is_positive:\n                    if n.is_nonnegative:\n                        return factorial(n) / factorial(n - k)\n                    if n.is_negative:\n                        raise ValueError(\"n must be nonnegative for positive k\")\n                if k.is_zero:\n                    return S.One\n                if k.is_negative:\n                    raise ValueError(\"k must be nonnegative\")\n\n    assert multifactorial(6, 1) == factorial(6)\n    assert multifactorial(6, 2) == factorial(6) / factorial(4)\n    assert multifactorial(6, 3) == factorial(6) / factorial(3)\n    assert multifactorial(6, 6) == factorial(6) / factorial(0)\n    assert multifactorial(6, 0) == 1\n\n    raises(ValueError, lambda: multifactorial(6, -1))\n    raises(ValueError, lambda: multifactorial(-1, 1))\n\n    assert multifactorial(x, n).is_integer is None\n    assert multifactorial(n, m).is_integer\n    assert multifactorial(x, x).is_integer is None\n    assert multifactorial(n, n).is_integer\n", "def test_subfactorial_rewrite():\n    from sympy import uppergamma, factorial\n    x = Symbol('x')\n    assert subfactorial(x).rewrite(factorial) == factorial(x) - factorial(x)/E\n    assert subfactorial(x).rewrite(uppergamma) == uppergamma(x + 1, -1)/E\n", "def test_multifactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, positive=True)\n    m = Symbol('m', integer=True, nonnegative=True)\n\n    # MultiFactorial class is essentially a placeholder\n    # Ensure it can be instantiated and its properties are set correctly\n    mf = MultiFactorial(n)\n    assert isinstance(mf, MultiFactorial)\n    assert mf.args == (n,)\n    \n    # MultiFactorial should behave correctly with different types of inputs\n    assert MultiFactorial(5) == MultiFactorial(5)\n    assert MultiFactorial(oo) == MultiFactorial(oo)\n    assert MultiFactorial(S.NaN) == S.NaN\n    assert MultiFactorial(x).args == (x,)\n    assert MultiFactorial(x, evaluate=False).args == (x,)\n\n    # Check if it's recognized as an integer when the argument is integer\n    assert MultiFactorial(n).is_integer == True\n    assert MultiFactorial(x).is_integer is None\n    assert MultiFactorial(k).is_integer == True\n    assert MultiFactorial(m).is_integer == True\n    assert MultiFactorial(S.Half).is_integer is False\n\n    # Test rewrite methods\n    assert MultiFactorial(n).rewrite(factorial) == MultiFactorial(n)\n    assert MultiFactorial(n).rewrite(gamma) == MultiFactorial(n)\n    assert MultiFactorial(n).rewrite(ff) == MultiFactorial(n)\n    assert MultiFactorial(n).rewrite(rf) == MultiFactorial(n)\n    assert MultiFactorial(n).rewrite(binomial) == MultiFactorial(n)\n\n    # Ensure it works with substitution and evaluation\n    assert MultiFactorial(5).subs(n, 3) == MultiFactorial(3)\n    assert MultiFactorial(5).evalf() == MultiFactorial(5)\n\n    # Test differentiation\n    assert MultiFactorial(n).diff(n) == Derivative(MultiFactorial(n), n)\n    assert MultiFactorial(x).diff(x) == Derivative(MultiFactorial(x), x)\n", "def test_multifactorial():\n    n, k = symbols('n k', integer=True)\n    assert MultiFactorial(n) == MultiFactorial(n)\n    assert MultiFactorial(n).rewrite(factorial) == MultiFactorial(n).rewrite(factorial)\n    assert MultiFactorial(n).rewrite(gamma) == MultiFactorial(n).rewrite(gamma)\n    assert MultiFactorial(3).is_integer\n    assert MultiFactorial(3).is_positive\n    assert MultiFactorial(0) == 1\n    assert MultiFactorial(1) == 1\n    assert MultiFactorial(2) == 2\n    assert MultiFactorial(3) == 6\n    assert MultiFactorial(4) == 24\n    assert MultiFactorial(5) == 120\n    assert MultiFactorial(n).is_integer is None\n    assert MultiFactorial(k).is_integer is None\n", "def test_factorial_mod():\n    n = Symbol('n', integer=True)\n    q = Symbol('q', integer=True)\n\n    assert factorial(5)._eval_Mod(3) == 0  # 5! = 120, 120 % 3 = 0\n    assert factorial(6)._eval_Mod(7) == 0  # 6! = 720, 720 % 7 = 0 (since 7 is prime and 6 = 7-1)\n    assert factorial(7)._eval_Mod(7) == -1 % 7  # 7! = 5040, 5040 % 7 = 0, but Wilson's theorem applies\n    assert factorial(10)._eval_Mod(4) == 0  # 10! = 3628800, 3628800 % 4 = 0\n    assert factorial(10)._eval_Mod(5) == 0  # 10! = 3628800, 3628800 % 5 = 0\n    assert factorial(n)._eval_Mod(2) == 0  # for any integer n >= 2, n! % 2 = 0\n    assert factorial(1)._eval_Mod(1) == 0  # 1! = 1, 1 % 1 = 0\n    assert factorial(0)._eval_Mod(1) == 0  # 0! = 1, 1 % 1 = 0\n    assert factorial(n)._eval_Mod(n + 1).subs(n, 4) == 0  # 4! = 24, 24 % 5 = 0\n    assert factorial(n)._eval_Mod(n).subs(n, 4) == 0  # 4! = 24, 24 % 4 = 0\n    assert factorial(n)._eval_Mod(q).subs({n: 6, q: 3}) == 0  # 6! = 720, 720 % 3 = 0\n"], "sample_1007": ["def test_multifactorial():\n    from sympy import factorial, factorial2\n\n    class DummyMultifactorial(MultiFactorial):\n        @classmethod\n            if n.is_Number:\n                if n is S.NaN:\n                    return S.NaN\n                elif n.is_Integer and n.is_nonnegative:\n                    return factorial(n) * 2\n                elif n.is_Integer and n.is_negative:\n                    return factorial2(n) * 3\n                elif n is S.Infinity:\n                    return S.Infinity\n\n    assert DummyMultifactorial(nan) == nan\n    assert DummyMultifactorial(S.Infinity) == oo\n    assert DummyMultifactorial(0) == 2\n    assert DummyMultifactorial(3) == 12  # 3! * 2\n    assert DummyMultifactorial(-3) == -2  # (-3)!! * 3\n    assert DummyMultifactorial(Symbol('x')).func == DummyMultifactorial\n\n    assert DummyMultifactorial(5).is_integer\n    assert DummyMultifactorial(-7).is_integer\n    assert DummyMultifactorial(5).is_positive is True\n    assert DummyMultifactorial(-7).is_positive is None\n    assert DummyMultifactorial(5).is_even is None\n    assert DummyMultifactorial(-7).is_even is None\n    assert DummyMultifactorial(5).is_real is True\n    assert DummyMultifactorial(-7).is_real is True\n", "def test_multifactorial():\n    from sympy import Symbol\n    n = Symbol('n', integer=True)\n\n    # Placeholder test since MultiFactorial is a placeholder class\n    assert MultiFactorial(n).func == MultiFactorial\n    assert MultiFactorial(n).args == (n,)\n", "def test_factorial_mod():\n    from sympy import prime\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n\n    assert factorial(5) % 3 == 2\n    assert factorial(10) % 7 == 6\n    assert factorial(0) % 5 == 1\n    assert factorial(3) % 2 == 0\n    assert factorial(4) % 3 == 2\n    assert factorial(7) % prime(2) == 0\n    assert factorial(11) % prime(5) == 0\n\n    assert factorial(13).rewrite(Mod, 2) == 0\n    assert factorial(9).rewrite(Mod, 3) == 0\n    assert factorial(n).rewrite(Mod, n) == 0\n\n    assert factorial(n).rewrite(Mod, n + 1) == n % (n + 1)\n    assert factorial(n).rewrite(Mod, prime(n)) == 0 if n > 1 else 1\n", "def test_multifactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, positive=True)\n\n    # Since MultiFactorial is just a placeholder and doesn't have any method, \n    # we can only test its instantiation and basic properties\n    mf = MultiFactorial(x)\n    assert isinstance(mf, MultiFactorial)\n    assert mf.args == (x,)\n\n    assert mf.is_integer is None\n    assert mf.is_nonnegative is None\n\n    # Check if it raises errors for invalid attributes\n    raises(AttributeError, lambda: mf.eval())\n    raises(AttributeError, lambda: mf._eval_rewrite_as_gamma(x))\n\n    # Test an instance with an integer\n    mf = MultiFactorial(n)\n    assert isinstance(mf, MultiFactorial)\n    assert mf.args == (n,)\n    assert mf.is_integer is None\n    assert mf.is_nonnegative is None\n", "def test_multifactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n\n    assert isinstance(MultiFactorial(n), MultiFactorial)\n    assert MultiFactorial(n).func == MultiFactorial\n\n    # check the base class\n    assert issubclass(MultiFactorial, CombinatorialFunction)\n\n    # test properties\n    assert MultiFactorial(n).is_integer is None\n    assert MultiFactorial(k).is_integer is None\n\n    assert MultiFactorial(n).is_positive is None\n    assert MultiFactorial(k).is_positive is None\n"], "sample_1008": ["def test_angular_acceleration():\n    q1, q2, q3 = dynamicsymbols('q1 q2 q3')\n    q1d, q2d, q3d = dynamicsymbols('q1 q2 q3', 1)\n    q1dd, q2dd, q3dd = dynamicsymbols('q1 q2 q3', 2)\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n    C = B.orientnew('C', 'Axis', [q3, B.y])\n\n    # Define angular velocities\n    A.set_ang_vel(N, q1d * A.z)\n    B.set_ang_vel(A, q2d * A.x)\n    C.set_ang_vel(B, q3d * B.y)\n\n    # Define angular accelerations\n    A.set_ang_acc(N, q1dd * A.z)\n    B.set_ang_acc(A, q2dd * A.x)\n    C.set_ang_acc(B, q3dd * B.y)\n\n    assert A.ang_acc_in(N) == q1dd * A.z\n    assert B.ang_acc_in(N) == q2dd * A.x + q1dd * A.z\n    assert C.ang_acc_in(N) == q3dd * C.y + q2dd * A.x + q1dd * A.z\n\n    assert A.ang_acc_in(A) == 0\n    assert B.ang_acc_in(A) == q2dd * A.x\n    assert C.ang_acc_in(A) == q3dd * C.y + q2dd * A.x\n\n    assert B.ang_acc_in(B) == 0\n    assert C.ang_acc_in(B) == q3dd * B.y\n    assert C.ang_acc_in(C) == 0\n", "def test_dcm_cache():\n    \"\"\"Test the DCM cache for efficiency and correctness.\"\"\"\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n\n    # Check initial DCM cache state\n    assert N in A._dcm_cache\n    assert A in N._dcm_cache\n    assert B in A._dcm_cache\n    assert A in B._dcm_cache\n\n    # Perform DCM calculation and check if cache is used\n    dcm_ab = A.dcm(B)\n    dcm_bn = B.dcm(N)\n    assert A._dcm_cache[B] == dcm_ab\n    assert B._dcm_cache[N] == dcm_bn\n\n    # Re-orient frame A and ensure cache invalidation\n    A.orient(N, 'Axis', [q1 + q2, N.z])\n    assert B not in A._dcm_cache\n    assert A not in B._dcm_cache\n    assert A not in N._dcm_cache\n    assert N not in A._dcm_cache\n\n    # Ensure cache is updated with new orientation\n    new_dcm_ab = A.dcm(B)\n    new_dcm_bn = B.dcm(N)\n    assert A._dcm_cache[B] == new_dcm_ab\n    assert B._dcm_cache[N] == new_dcm_bn\n", "def test_frame_initialization():\n    # Test default initialization\n    A = ReferenceFrame('A')\n    assert A.name == 'A'\n    assert A.indices == ['x', 'y', 'z']\n    assert A.str_vecs == ['A.x', 'A.y', 'A.z']\n    assert A.pretty_vecs == ['a_x', 'a_y', 'a_z']\n    assert A.latex_vecs == [r\"\\mathbf{\\hat{a}_x}\", r\"\\mathbf{\\hat{a}_y}\", r\"\\mathbf{\\hat{a}_z}\"]\n\n    # Test custom indices\n    B = ReferenceFrame('B', indices=['i', 'j', 'k'])\n    assert B.indices == ['i', 'j', 'k']\n    assert B.str_vecs == [\"B['i']\", \"B['j']\", \"B['k']\"]\n    assert B.pretty_vecs == ['b_i', 'b_j', 'b_k']\n    assert B.latex_vecs == [r\"\\mathbf{\\hat{b}_{i}}\", r\"\\mathbf{\\hat{b}_{j}}\", r\"\\mathbf{\\hat{b}_{k}}\"]\n\n    # Test custom latex vectors\n    C = ReferenceFrame('C', latexs=['C1', 'C2', 'C3'])\n    assert C.latex_vecs == ['C1', 'C2', 'C3']\n\n    # Test invalid name\n    try:\n        ReferenceFrame(123)\n    except TypeError:\n        pass\n    else:\n        assert False, \"Expected TypeError for invalid name\"\n\n    # Test invalid indices type\n    try:\n        ReferenceFrame('D', indices='xyz')\n    except TypeError:\n        pass\n    else:\n        assert False, \"Expected TypeError for invalid indices type\"\n\n    # Test invalid indices length\n    try:\n        ReferenceFrame('E', indices=['x', 'y'])\n    except ValueError:\n        pass\n    else:\n        assert False, \"Expected ValueError for invalid indices length\"\n\n    # Test invalid latex vectors type\n    try:\n        ReferenceFrame('F', latexs='C1,C2,C3')\n    except TypeError:\n        pass\n    else:\n        assert False, \"Expected TypeError for invalid latex vectors type\"\n\n    # Test invalid latex vectors length\n    try:\n        ReferenceFrame('G', latexs=['C1', 'C2'])\n    except ValueError:\n        pass\n    else", "def test_variable_map_with_custom_dcm():\n    from sympy import symbols\n    q = symbols('q')\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    dcm_matrix = Matrix([\n        [cos(q), -sin(q), 0],\n        [sin(q), cos(q), 0],\n        [0, 0, 1]\n    ])\n    A.orient(N, 'DCM', dcm_matrix)\n    assert A.variable_map(N) == {\n        A[0]: N[0]*cos(q) + N[1]*sin(q),\n        A[1]: -N[0]*sin(q) + N[1]*cos(q),\n        A[2]: N[2]\n    }\n    assert N.variable_map(A) == {\n        N[0]: A[0]*cos(q) - A[1]*sin(q),\n        N[1]: A[0]*sin(q) + A[1]*cos(q),\n        N[2]: A[2]\n    }\n", "def test_set_ang_acc():\n    \"\"\"Tests the set_ang_acc method of the ReferenceFrame class\"\"\"\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [pi/4, N.z])\n    B = A.orientnew('B', 'Axis', [pi/6, A.x])\n    V = 5 * N.x + 3 * A.y + 2 * B.z\n    \n    # Setting angular acceleration of B in frame N\n    B.set_ang_acc(N, V)\n    \n    # Checking angular acceleration in different frames\n    assert B.ang_acc_in(N) == V\n    assert B.ang_acc_in(A) == V.express(A)\n    assert B.ang_acc_in(B) == V.express(B)\n    \n    # Check if the negative angular acceleration is set correctly in the other frame\n    assert N.ang_acc_in(B) == -V\n    assert A.ang_acc_in(B) == -V.express(A)\n    assert B.ang_acc_in(B) == 0\n\n    # Change angular acceleration and verify the change\n    V_new = 2 * N.y + 4 * A.z\n    B.set_ang_acc(N, V_new)\n    assert B.ang_acc_in(N) == V_new\n    assert N.ang_acc_in(B) == -V_new\n    assert B.ang_acc_in(A) == V_new.express(A)\n    assert A.ang_acc_in(B) == -V_new.express(A)\n"], "sample_1009": ["def test_vector_operations():\n    N = ReferenceFrame('N')\n    u1, u2, u3 = dynamicsymbols('u1 u2 u3')\n    v = u1 * N.x + u2 * N.y + u3 * N.z\n\n    # Test magnitude\n    magnitude = v.magnitude()\n    assert magnitude == sqrt(u1**2 + u2**2 + u3**2)\n\n    # Test normalize\n    normalized_v = v.normalize()\n    assert normalized_v == v / magnitude\n\n    # Test scalar multiplication\n    scalar = 2\n    scaled_v = v * scalar\n    assert scaled_v == 2*u1*N.x + 2*u2*N.y + 2*u3*N.z\n\n    # Test division by scalar\n    divided_v = v / scalar\n    assert divided_v == u1/2*N.x + u2/2*N.y + u3/2*N.z\n\n    # Test negation\n    neg_v = -v\n    assert neg_v == -u1*N.x + -u2*N.y + -u3*N.z\n\n    # Test applyfunc\n    squared_v = v.applyfunc(lambda x: x**2)\n    assert squared_v == u1**2*N.x + u2**2*N.y + u3**2*N.z\n\n    # Test subs\n    subs_v = v.subs({u1: 1, u2: 2, u3: 3})\n    assert subs_v == N.x + 2*N.y + 3*N.z\n", "def test_Vector_operations():\n    N = ReferenceFrame('N')\n    u = dynamicsymbols('u')\n    v = dynamicsymbols('v')\n    w = dynamicsymbols('w')\n\n    # Test Vector addition\n    vec1 = u * N.x\n    vec2 = v * N.y\n    vec3 = w * N.z\n    vec_sum = vec1 + vec2 + vec3\n    assert vec_sum == u * N.x + v * N.y + w * N.z\n\n    # Test Vector subtraction\n    vec_diff = vec_sum - vec1\n    assert vec_diff == v * N.y + w * N.z\n\n    # Test Vector negation\n    vec_neg = -vec1\n    assert vec_neg == -u * N.x\n\n    # Test Vector dot product\n    assert vec1 & vec2 == 0\n    assert vec1 & vec1 == u**2\n\n    # Test Vector cross product\n    vec_cross = vec1 ^ vec2\n    assert vec_cross == u * v * N.z\n\n    # Test Vector magnitude\n    assert vec1.magnitude() == sqrt(u**2)\n\n    # Test Vector normalization\n    norm_vec = vec1.normalize()\n    assert norm_vec == vec1 / sqrt(u**2)\n", "def test_vector_operations():\n    # Test the __mul__ method\n    scalar = 3\n    v1 = x*A.x + y*A.y + z*A.z\n    v2 = scalar * v1\n    assert dot(v2, A.x) == scalar * x\n    assert dot(v2, A.y) == scalar * y\n    assert dot(v2, A.z) == scalar * z\n\n    # Test the __neg__ method\n    v3 = -v1\n    assert dot(v3, A.x) == -x\n    assert dot(v3, A.y) == -y\n    assert dot(v3, A.z) == -z\n\n    # Test the __truediv__ method\n    v4 = v1 / scalar\n    assert dot(v4, A.x) == x / scalar\n    assert dot(v4, A.y) == y / scalar\n    assert dot(v4, A.z) == z / scalar\n\n    # Test the __xor__ method (cross product)\n    v5 = A.x ^ A.y\n    assert v5 == A.z\n    v6 = A.y ^ A.x\n    assert v6 == -A.z\n\n    # Test the applyfunc method\n    v7 = v1.applyfunc(lambda x: x**2)\n    assert dot(v7, A.x) == x**2\n    assert dot(v7, A.y) == y**2\n    assert dot(v7, A.z) == z**2\n\n    # Test the magnitude method\n    assert v1.magnitude() == sqrt(x**2 + y**2 + z**2)\n\n    # Test the normalize method\n    v8 = v1.normalize()\n    assert dot(v8, A.x) == x / sqrt(x**2 + y**2 + z**2)\n    assert dot(v8, A.y) == y / sqrt(x**2 + y**2 + z**2)\n    assert dot(v8, A.z) == z / sqrt(x**2 + y**2 + z**2)\n", "def test_vector_operations():\n    # Test vector multiplication\n    v1 = x * A.x + y * A.y + z * A.z\n    v2 = 2 * x * A.x + 3 * y * A.y + 4 * z * A.z\n    v3 = v1 * 2\n\n    assert isinstance(v3, Vector)\n    assert dot(v3, A.x) == 2 * x\n    assert dot(v3, A.y) == 2 * y\n    assert dot(v3, A.z) == 2 * z\n\n    # Test vector division\n    v4 = v2 / 2\n    assert isinstance(v4, Vector)\n    assert dot(v4, A.x) == x\n    assert dot(v4, A.y) == 1.5 * y\n    assert dot(v4, A.z) == 2 * z\n\n    # Test vector negation\n    v5 = -v1\n    assert isinstance(v5, Vector)\n    assert dot(v5, A.x) == -x\n    assert dot(v5, A.y) == -y\n    assert dot(v5, A.z) == -z\n\n    # Test vector equality and inequality\n    assert v1 == v1\n    assert v1 != v2\n\n    # Test vector magnitude\n    assert v1.magnitude() == sqrt(x**2 + y**2 + z**2)\n    assert v2.magnitude() == sqrt((2*x)**2 + (3*y)**2 + (4*z)**2)\n\n    # Test vector normalization\n    v6 = v1.normalize()\n    assert v6.magnitude() == 1\n", "def test_vector_magnitude_normalize():\n    x, y, z = symbols('x y z')\n    N = ReferenceFrame('N')\n    v = x*N.x + y*N.y + z*N.z\n\n    # Test magnitude\n    mag = v.magnitude()\n    assert mag == sqrt(x**2 + y**2 + z**2)\n\n    # Test normalize\n    norm = v.normalize()\n    assert norm == (x/sqrt(x**2 + y**2 + z**2))*N.x + (y/sqrt(x**2 + y**2 + z**2))*N.y + (z/sqrt(x**2 + y**2 + z**2))*N.z\n\n    # Test magnitude of normalized vector should be 1\n    norm_mag = norm.magnitude()\n    assert norm_mag == 1\n"], "sample_1010": ["def test_latex_modifiers_and_greek():\n    # Testing combinations of greek letters with various modifiers\n    assert latex(symbols('alpha_tildeVec')) == r\"\\vec{\\tilde{\\alpha}}\"\n    assert latex(symbols('Beta_HatAbs')) == r\"\\left|{\\hat{B}}\\right|\"\n    assert latex(symbols('gamma_dotBM')) == r\"\\boldsymbol{\\dot{\\gamma}}\"\n    assert latex(symbols('Delta_primeBar')) == r\"\\bar{\\Delta}'\"\n    assert latex(symbols('epsilon_Check')) == r\"\\check{\\epsilon}\"\n    assert latex(symbols('theta_ddotNorm')) == r\"\\left\\|{\\ddot{\\theta}}\\right\\|\"\n    assert latex(symbols('rho_graveAVG')) == r\"\\left\\langle{\\grave{\\rho}}\\right\\rangle\"\n    assert latex(symbols('omega_mathringMag')) == r\"\\left|{\\mathring{\\omega}}\\right|\"\n    \n    # Testing combinations of greek letters with single modifiers\n    assert latex(symbols('Phi_prime')) == r\"{\\Phi}'\"\n    assert latex(symbols('Xi_ddot')) == r\"\\ddot{\\Xi}\"\n    assert latex(symbols('Omega_tilde')) == r\"\\tilde{\\Omega}\"\n", "def test_latex_modifiers_with_greek_letters():\n    assert latex(symbols(\"alphahatdot\")) == r\"\\dot{\\hat{\\alpha}}\"\n    assert latex(symbols(\"betacheckbreve\")) == r\"\\breve{\\check{\\beta}}\"\n    assert latex(symbols(\"gammadotacute\")) == r\"\\acute{\\dot{\\gamma}}\"\n    assert latex(symbols(\"deltatildegrave\")) == r\"\\grave{\\tilde{\\delta}}\"\n    assert latex(symbols(\"epsilonhatbar\")) == r\"\\bar{\\hat{\\epsilon}}\"\n    assert latex(symbols(\"zetavec\")) == r\"\\vec{\\zeta}\"\n    assert latex(symbols(\"etadotprime\")) == r\"{\\dot{\\eta}}'\"\n    assert latex(symbols(\"thetaddotvec\")) == r\"\\vec{\\ddot{\\theta}}\"\n    assert latex(symbols(\"iotaavg\")) == r\"\\left\\langle{\\iota}\\right\\rangle\"\n    assert latex(symbols(\"kappanorm\")) == r\"\\left\\|{\\kappa}\\right\\|\"\n    assert latex(symbols(\"lambdanormal\")) == r\"\\lambda\"\n    assert latex(symbols(\"munormaldot\")) == r\"\\dot{\\mu}\"\n    assert latex(symbols(\"nunormalcheck\")) == r\"\\check{\\nu}\"\n    assert latex(symbols(\"xinormalacute\")) == r\"\\acute{\\xi}\"\n    assert latex(symbols(\"omicronhat\")) == r\"\\hat{o}\"\n    assert latex(symbols(\"pinormalvec\")) == r\"\\vec{\\pi}\"\n    assert latex(symbols(\"rhonorm\")) == r\"\\rho\"\n    assert latex(symbols(\"sigmanormalprime\")) == r\"{\\sigma}'\"\n    assert latex(symbols(\"taunorm\")) == r\"\\tau\"\n    assert latex(symbols(\"upsilonmathring\")) == r\"\\mathring{\\upsilon}\"\n    assert latex(symbols(\"phinormaltilde\")) == r\"\\tilde{\\phi}\"\n    assert latex(symbols(\"chinorm\")) == r\"\\chi\"\n    assert latex(symbols(\"psinormalbreve\")) == r\"\\breve{\\psi}\"\n    assert latex(symbols(\"omeganorm\")) == r\"\\omega\"\n", "def test_custom_latex_printer():\n    class MyLatexPrinter(LatexPrinter):\n            return f\"CustomSymbol({expr.name})\"\n\n    assert MyLatexPrinter().doprint(Symbol('x')) == \"CustomSymbol(x)\"\n    assert MyLatexPrinter().doprint(Symbol('alpha')) == \"CustomSymbol(alpha)\"\n    assert MyLatexPrinter().doprint(Symbol('beta')) == \"CustomSymbol(beta)\"\n    assert MyLatexPrinter().doprint(x + y) == \"CustomSymbol(x) + CustomSymbol(y)\"\n    assert MyLatexPrinter().doprint(exp(x)) == \"e^{CustomSymbol(x)}\"\n    assert MyLatexPrinter().doprint(sin(x)) == r\"\\sin{\\left (CustomSymbol(x) \\right )}\"\n    assert MyLatexPrinter().doprint(Matrix([[x, y], [y, x]])) == r'\\left[\\begin{matrix}CustomSymbol(x) & CustomSymbol(y)\\\\CustomSymbol(y) & CustomSymbol(x)\\end{matrix}\\right]'\n", "def test_latex_fractions():\n    assert latex(Rational(1, 2)) == r\"\\frac{1}{2}\"\n    assert latex(Rational(-1, 2)) == r\"- \\frac{1}{2}\"\n    assert latex(Rational(1, -2)) == r\"- \\frac{1}{2}\"\n    assert latex(Rational(-1, -2)) == r\"\\frac{1}{2}\"\n    assert latex(x + Rational(1, 2)) == r\"x + \\frac{1}{2}\"\n    assert latex(x - Rational(1, 2)) == r\"x - \\frac{1}{2}\"\n    assert latex(Rational(1, 2) - x) == r\"\\frac{1}{2} - x\"\n    assert latex(Rational(-1, 2) - x) == r\"- \\frac{1}{2} - x\"\n    assert latex((x + 1) / 2) == r\"\\frac{x + 1}{2}\"\n    assert latex((x - 1) / 2) == r\"\\frac{x - 1}{2}\"\n    assert latex((x + 1) / (y + 2)) == r\"\\frac{x + 1}{y + 2}\"\n    assert latex((x - 1) / (y - 2)) == r\"\\frac{x - 1}{y - 2}\"\n    assert latex(1 / (x + 2)) == r\"\\frac{1}{x + 2}\"\n    assert latex(1 / (x - 2)) == r\"\\frac{1}{x - 2}\"\n    assert latex(-1 / (x + 2)) == r\"- \\frac{1}{x + 2}\"\n    assert latex(-1 / (x - 2)) == r\"- \\frac{1}{x - 2}\"\n    assert latex(1 / (x + y + z + 2)) == r\"\\frac{1}{x + y + z + 2}\"\n    assert latex(1 / (x - y - z - 2)) == r\"\\frac{1}{x - y - z - 2}\"\n    assert latex(1 / (x + y - z + 2)) == r\"\\frac{1}{x + y - z + 2}\"\n    assert latex(1 / (x - y + z - 2)) == r\"\\", "def test_latex_subscript_superscript():\n    # Test latex rendering with both subscripts and superscripts\n    subscript_symbol = Symbol('x_i')\n    superscript_symbol = Symbol('x^j')\n    combined_symbol = Symbol('x_i^j')\n    \n    assert latex(subscript_symbol) == r'x_{i}'\n    assert latex(superscript_symbol) == r'x^{j}'\n    assert latex(combined_symbol) == r'x^{j}_{i}'\n    \n    # Test with more complex expressions\n    expr1 = subscript_symbol + superscript_symbol\n    expr2 = combined_symbol**2\n    \n    assert latex(expr1) == r'x_{i} + x^{j}'\n    assert latex(expr2) == r'\\left(x^{j}_{i}\\right)^{2}'\n    \n    # Test with functions\n    f = Function('f')\n    expr3 = f(combined_symbol)\n    \n    assert latex(expr3) == r'f{\\left(x^{j}_{i} \\right)}'\n"], "sample_1011": ["def test_Assignment():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    expr = Assignment(A, B + Identity(3))\n    assert mcode(expr) == \"A = B + eye(3);\"\n\n    expr = Assignment(A, 3*pi*B)\n    assert mcode(expr) == \"A = 3*pi*B;\"\n\n    expr = Assignment(x, sin(x) + cos(x))\n    assert mcode(expr) == \"x = sin(x) + cos(x);\"\n\n    expr = Assignment(x, Piecewise((y, x < 1), (z, True)))\n    assert mcode(expr, inline=False) == (\n        \"if (x < 1)\\n\"\n        \"  x = y;\\n\"\n        \"else\\n\"\n        \"  x = z;\\n\"\n        \"end\")\n", "def test_Assignment():\n    A = Matrix([[x, 2, x*y]])\n    B = MatrixSymbol('B', 1, 3)\n    assert mcode(Assignment(x, y)) == \"x = y;\"\n    assert mcode(Assignment(A[0, 0], x**2)) == \"A(1, 1) = x.^2;\"\n    assert mcode(Assignment(B[0, 1], sin(x))) == \"B(1, 2) = sin(x);\"\n    assert mcode(Assignment(B, A)) == \"B = [x 2 x.*y];\"\n    pw = Piecewise((x, x < 1), (x**2, True))\n    assert mcode(Assignment(y, pw), inline=False) == (\n        \"if (x < 1)\\n\"\n        \"  y = x;\\n\"\n        \"else\\n\"\n        \"  y = x.^2;\\n\"\n        \"end\")\n", "def test_Lambda():\n    f = Lambda(x, x**2 + 1)\n    assert mcode(f) == \"lambda x: x.^2 + 1\"\n    g = Lambda((x, y), x + y)\n    assert mcode(g) == \"lambda x, y: x + y\"\n    h = Lambda((x, y, z), x*y + z)\n    assert mcode(h) == \"lambda x, y, z: x.*y + z\"\n", "def test_octave_code_with_user_functions():\n    from sympy import Function\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_matrix_fcn\"),\n              (lambda x: not x.is_Matrix, \"custom_fcn\")]\n    }\n    assert mcode(f(x), user_functions=custom_functions) == \"custom_fcn(x)\"\n    assert mcode(g(x), user_functions=custom_functions) == \"custom_fcn(x)\"\n    mat = Matrix([[1, x]])\n    assert mcode(g(mat), user_functions=custom_functions) == \"custom_matrix_fcn([1 x])\"\n", "def test_Assignment():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = Assignment(A, B)\n    assert mcode(expr) == \"A = B;\"\n    expr = Assignment(A[0, 1], B[1, 0])\n    assert mcode(expr) == \"A(1, 2) = B(2, 1);\"\n    expr = Assignment(A[0, 0], 2*x + 3)\n    assert mcode(expr) == \"A(1, 1) = 2*x + 3;\"\n    expr = Assignment(A[1, 1], Piecewise((x**2, x < 1), (x**3, True)))\n    assert mcode(expr, inline=False) == (\n        \"if (x < 1)\\n\"\n        \"  A(2, 2) = x.^2;\\n\"\n        \"else\\n\"\n        \"  A(2, 2) = x.^3;\\n\"\n        \"end\")\n    expr = Assignment(A[1, 1], Piecewise((x**2, x < 1), (x**3, True)))\n    assert mcode(expr) == \"A(2, 2) = ((x < 1).*(x.^2) + (~(x < 1)).*(x.^3));\"\n"], "sample_1012": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' not in p.module_imports\n    smat = SparseMatrix(2, 5, {(0, 1): 3})\n    assert p.doprint(smat) == 'sympy.Matrix([[0, 3, 0, 0, 0], [0, 0, 0, 0, 0]])'\n    assert 'sympy' not in p.module_imports\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x > 6))) == 'sympy.Piecewise((1, x == 0), (2, x > 6))'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(Mod(x, 2)) == 'sympy.Mod(x, 2)'\n    assert p.doprint(And(x, y)) == 'sympy.And(x, y)'\n    assert p.doprint(Or(x, y)) == 'sympy.Or(x, y)'\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.doprint(zoo) == 'sympy.zoo'\n    assert p.doprint(-oo) == 'sympy.oo'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert p.doprint(sign(x)) == '(0.0 if x == 0 else sympy.copysign(1, x))'\n    assert p.doprint(And(x, y)) == 'sympy.And(x, y)'\n    assert p.doprint(Or(x, y)) == 'sympy.Or(x, y)'\n    assert p.doprint(Mod(x, 2)) == 'sympy.Mod(x, 2)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6))) == 'sympy.Piecewise((1, Eq(x, 0)), (2, x > 6))'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' not in p.module_imports  # SymPy functions should not add to module_imports\n    assert p.doprint(x + y) == 'x + y'  # Testing a simple expression\n    assert p.doprint(pi) == 'sympy.pi'\n    assert 'sympy' not in p.module_imports  # Constants should not add to module_imports\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert not p.module_imports  # SymPyPrinter should not import any modules by default\n    assert p.doprint(pi) == 'sympy.pi'\n    assert 'sympy' in p.module_imports  # Check if sympy module is correctly imported\n    smat = SparseMatrix(2, 5, {(0, 1): 3})\n    assert p.doprint(smat) == 'sympy.SparseMatrix([[0, 3, 0, 0, 0], [0, 0, 0, 0, 0]])'\n"], "sample_1013": ["def test_tensorflow_complex_operations():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n    \n    from sympy import Abs, re, im\n\n    expr = Abs(x + y*I) + re(x + y*I) + im(x + y*I)\n    func = lambdify((x, y), expr, modules=\"tensorflow\")\n    \n    a = tensorflow.placeholder(dtype=tensorflow.float32)\n    b = tensorflow.placeholder(dtype=tensorflow.float32)\n    \n    s = tensorflow.Session()\n    feed_dict = {a: 3, b: 4}\n    assert s.run(func(a, b), feed_dict=feed_dict) == 3 + 4 + 5\n", "def test_custom_lambda_printer():\n    # Testing custom lambda printer with a custom function and complex expression\n    class CustomPrinter(LambdaPrinter):\n            return \"add(\" + \", \".join(map(self._print, expr.args)) + \")\"\n\n        return CustomPrinter().doprint(expr)\n\n    expr = x + y + z\n    custom_add = implemented_function('add', lambda *args: sum(args))\n    func = lambdify((x, y, z), custom_add(x, y, z), printer=custom_lambdarepr)\n    assert func(1, 2, 3) == 6\n\n    expr = x**2 + y**2 + z**2\n    func = lambdify((x, y, z), expr, printer=custom_lambdarepr)\n    assert func(1, 2, 3) == 1 + 4 + 9\n", "def test_implemented_function_with_lambda():\n    # Test that an implemented function using a lambda expression works as expected\n    f = implemented_function('f', lambda x, y: x * y + 1)\n    lam = lambdify((x, y), f(x, y))\n    assert lam(3, 4) == 13\n    assert lam(0, 5) == 1\n    assert lam(-2, 3) == -5\n", "def test_import_module_reload():\n    from sympy.utilities.lambdify import _import\n\n    # Test reloading of modules\n    modules_to_test = [\"math\", \"mpmath\", \"numpy\", \"sympy\", \"tensorflow\"]\n    for mod in modules_to_test:\n        # Ensure importing the module works without reloading\n        _import(mod, reload=False)\n        initial_import = globals().get(mod.upper(), None)\n        assert initial_import is not None\n        # Reload the module and check if the import is the same\n        _import(mod, reload=True)\n        reloaded_import = globals().get(mod.upper(), None)\n        assert reloaded_import == initial_import\n", "def test_import_module():\n    if not numpy:\n        skip(\"numpy not installed.\")\n    if not mpmath:\n        skip(\"mpmath not installed.\")\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n\n    # Test importing math module\n    _import(\"math\")\n    assert \"sin\" in MATH\n\n    # Test importing numpy module\n    _import(\"numpy\")\n    assert \"sin\" in NUMPY\n\n    # Test importing mpmath module\n    _import(\"mpmath\")\n    assert \"sin\" in MPMATH\n\n    # Test importing tensorflow module\n    _import(\"tensorflow\")\n    assert \"sin\" in TENSORFLOW\n\n    # Test importing sympy module\n    _import(\"sympy\")\n    assert \"sin\" in SYMPY\n"], "sample_1014": ["def test_setitem_mutable():\n    mutable_array = MutableDenseNDimArray([0, 0, 0, 0], (2, 2))\n    assert mutable_array[0, 0] == 0\n    mutable_array[0, 0] = 5\n    assert mutable_array[0, 0] == 5\n    mutable_array[1, 1] = 10\n    assert mutable_array[1, 1] == 10\n    raises(TypeError, lambda: ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))[0, 0] = 5)\n", "def test_mutable_dense_ndim_array():\n    arr = MutableDenseNDimArray([1, 2, 3], (3,))\n    assert len(arr) == 3\n    assert arr.shape == (3,)\n    assert arr.rank() == 1\n\n    arr[0] = 10\n    assert arr[0] == 10\n    assert arr.tolist() == [10, 2, 3]\n\n    arr[1:3] = [20, 30]\n    assert arr.tolist() == [10, 20, 30]\n\n    arr = arr.reshape(1, 3)\n    assert arr.shape == (1, 3)\n    assert arr.rank() == 2\n    assert arr.tolist() == [[10, 20, 30]]\n\n    arr[0, 1] = 200\n    assert arr[0, 1] == 200\n    assert arr.tolist() == [[10, 200, 30]]\n\n    immut_arr = arr.as_immutable()\n    assert isinstance(immut_arr, ImmutableDenseNDimArray)\n    assert immut_arr.tolist() == [[10, 200, 30]]\n\n    raises(TypeError, lambda: immut_arr.__setitem__((0, 1), 2))\n", "def test_as_mutable():\n    imm_arr = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    mut_arr = imm_arr.as_mutable()\n    assert isinstance(mut_arr, MutableDenseNDimArray)\n    assert mut_arr.tolist() == [[1, 2], [3, 4]]\n    mut_arr[0, 0] = 5\n    assert mut_arr[0, 0] == 5\n    assert imm_arr[0, 0] == 1  # Ensure immutability of original array\n", "def test_mutable_array():\n    array = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert array.shape == (2, 2)\n    assert array.rank() == 2\n    assert array[0, 0] == 1\n    assert array[1, 1] == 4\n\n    # Test setting item\n    array[0, 0] = 10\n    assert array[0, 0] == 10\n    assert array.tolist() == [[10, 2], [3, 4]]\n\n    # Test zeros\n    zero_array = MutableDenseNDimArray.zeros(2, 2)\n    assert zero_array.shape == (2, 2)\n    assert zero_array.rank() == 2\n    assert zero_array.tolist() == [[0, 0], [0, 0]]\n\n    # Test reshaping\n    reshaped_array = array.reshape(4)\n    assert reshaped_array.shape == (4,)\n    assert reshaped_array.rank() == 1\n    assert reshaped_array.tolist() == [10, 2, 3, 4]\n\n    # Test converting to immutable\n    immutable_array = array.as_immutable()\n    assert isinstance(immutable_array, ImmutableDenseNDimArray)\n    assert immutable_array.tolist() == [[10, 2], [3, 4]]\n\n    # Test free symbols\n    x = Symbol('x')\n    symbolic_array = MutableDenseNDimArray([x, 1], (2,))\n    assert symbolic_array.free_symbols == {x}\n", "def test_mutable_ndim_array():\n    arr = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert len(arr) == 4\n    assert arr.shape == (2, 2)\n    assert arr.rank() == 2\n    assert arr.tolist() == [[1, 2], [3, 4]]\n\n    arr[0, 0] = 10\n    assert arr[0, 0] == 10\n    assert arr.tolist() == [[10, 2], [3, 4]]\n\n    arr[1, 1] = 20\n    assert arr[1, 1] == 20\n    assert arr.tolist() == [[10, 2], [3, 20]]\n\n    raises(ValueError, lambda: arr.reshape(3, 3))\n\n    arr = arr.reshape(4)\n    assert arr.shape == (4,)\n    assert arr.rank() == 1\n    assert arr.tolist() == [10, 2, 3, 20]\n\n    arr = arr.as_immutable()\n    raises(TypeError, lambda: arr.__setitem__((0,), 1))\n"], "sample_1015": ["def test_ccode_struct():\n    from sympy.codegen.ast import Struct\n    a, b, c = symbols('a b c')\n    struct_A = Struct('A', [Declaration(Variable(a, real)), Declaration(Variable(b, real)), Declaration(Variable(c, real))])\n    expected = (\n        'struct A {\\n'\n        '   double a;\\n'\n        '   double b;\\n'\n        '   double c;\\n'\n        '}'\n    )\n    assert ccode(struct_A) == expected\n", "def test_ccode_math_macros_suffixes():\n    # Test if math macros are correctly suffixed based on type alias settings\n    assert ccode(pi, type_aliases={real: float80}) == \"M_PIl\"\n    assert ccode(S.Pi / 2, type_aliases={real: float80}) == \"M_PI_2l\"\n    assert ccode(exp(1), type_aliases={real: float80}) == \"M_El\"\n    assert ccode(log2(S.Exp1), type_aliases={real: float80}) == \"M_LOG2El\"\n    assert ccode(sqrt(2), type_aliases={real: float80}) == \"M_SQRT2l\"\n    assert ccode(2 / sqrt(S.Pi), type_aliases={real: float80}) == \"M_2_SQRTPIl\"\n    assert ccode(1 / sqrt(2), type_aliases={real: float80}) == \"M_SQRT1_2l\"\n\n    assert ccode(pi, type_aliases={real: float32}) == \"M_PI\"\n    assert ccode(S.Pi / 2, type_aliases={real: float32}) == \"M_PI_2\"\n    assert ccode(exp(1), type_aliases={real: float32}) == \"M_E\"\n    assert ccode(log2(S.Exp1), type_aliases={real: float32}) == \"M_LOG2E\"\n    assert ccode(sqrt(2), type_aliases={real: float32}) == \"M_SQRT2\"\n    assert ccode(2 / sqrt(S.Pi), type_aliases={real: float32}) == \"M_2_SQRTPI\"\n    assert ccode(1 / sqrt(2), type_aliases={real: float32}) == \"M_SQRT1_2\"\n", "def test_ccode_math_functions():\n    assert ccode(sin(x), standard='c99') == 'sin(x)'\n    assert ccode(cos(x), standard='c99') == 'cos(x)'\n    assert ccode(tan(x), standard='c99') == 'tan(x)'\n    assert ccode(asin(x), standard='c99') == 'asin(x)'\n    assert ccode(acos(x), standard='c99') == 'acos(x)'\n    assert ccode(atan(x), standard='c99') == 'atan(x)'\n    assert ccode(sinh(x), standard='c99') == 'sinh(x)'\n    assert ccode(cosh(x), standard='c99') == 'cosh(x)'\n    assert ccode(tanh(x), standard='c99') == 'tanh(x)'\n    assert ccode(asinh(x), standard='c99') == 'asinh(x)'\n    assert ccode(acosh(x), standard='c99') == 'acosh(x)'\n    assert ccode(atanh(x), standard='c99') == 'atanh(x)'\n    assert ccode(erf(x), standard='c99') == 'erf(x)'\n    assert ccode(erfc(x), standard='c99') == 'erfc(x)'\n    assert ccode(gamma(x), standard='c99') == 'tgamma(x)'\n    assert ccode(loggamma(x), standard='c99') == 'lgamma(x)'\n    assert ccode(ceiling(x), standard='c99') == 'ceil(x)'\n    assert ccode(floor(x), standard='c99') == 'floor(x)'\n    assert ccode(sqrt(x), standard='c99') == 'sqrt(x)'\n    assert ccode(exp(x), standard='c99') == 'exp(x)'\n    assert ccode(log(x), standard='c99') == 'log(x)'\n    assert ccode(log10(x), standard='c99') == 'log10(x)'\n    assert ccode(log2(x), standard='c99') == 'log2(x)'\n    assert ccode(hypot(x, y), standard='c99') == 'hypot(x, y)'\n    assert ccode(fma(x, y, z), standard='c99') == 'fma(x, y, z)'\n    assert ccode(exp2(x), standard='c99') == 'exp2(x)'\n    assert ccode(expm", "def test_ccode_FunctionDefinition_nested():\n    # Nested function definition\n    inner_func = FunctionDefinition(real, 'inner', [Declaration(Variable(x, type=real))], [Return(x + 1)])\n    outer_func = FunctionDefinition(real, 'outer', [Declaration(Variable(y, type=real))], [\n        Assignment(x, y * 2),\n        inner_func,\n        Return(FunctionCall('inner', [x]))\n    ])\n    expected = (\n        'double outer(double y){\\n'\n        '   x = 2*y;\\n'\n        '   double inner(double x){\\n'\n        '      return x + 1;\\n'\n        '   }\\n'\n        '   return inner(x);\\n'\n        '}'\n    )\n    assert ccode(outer_func) == expected\n", "def test_ccode_infinity():\n    assert ccode(oo, standard='C99') == \"INFINITY\"\n    assert ccode(-oo, standard='C99') == \"-INFINITY\"\n    assert ccode(oo, standard='C89') == \"HUGE_VAL\"\n    assert ccode(-oo, standard='C89') == \"-HUGE_VAL\"\n    assert ccode(oo) == \"INFINITY\"\n    assert ccode(-oo) == \"-INFINITY\"\n"], "sample_1016": ["def test_complex_numbers():\n    assert mcode(x + 1j) == \"x + 1i\"\n    assert mcode(x - 1j) == \"x - 1i\"\n    assert mcode(1 + 2j) == \"1 + 2i\"\n    assert mcode(1 - 2j) == \"1 - 2i\"\n    assert mcode((1 + 2j) * x) == \"(1 + 2i)*x\"\n    assert mcode((1 - 2j) * x) == \"(1 - 2i)*x\"\n    assert mcode(x / (1 + 2j)) == \"x/(1 + 2i)\"\n    assert mcode(x / (1 - 2j)) == \"x/(1 - 2i)\"\n", "def test_custom_functions():\n    # Custom function mapping\n    from sympy import Function\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"custom_scalar_fcn\")]\n    }\n\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n           \"custom_fcn(x) + custom_scalar_fcn(x) + custom_mat_fcn([1 x])\"\n", "def test_Assignment():\n    # Single assignment\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    expr = Assignment(A, B)\n    assert mcode(expr) == \"A = B;\"\n    \n    # Multiple assignments in Piecewise\n    x, y = symbols('x y')\n    expr = Piecewise((Assignment(A, B), x > 0), (Assignment(A, 2*B), True))\n    assert mcode(expr, inline=False) == (\n        \"if (x > 0)\\n\"\n        \"  A = B;\\n\"\n        \"else\\n\"\n        \"  A = 2*B;\\n\"\n        \"end\"\n    )\n    \n    # Assignment with indexing\n    C = MatrixSymbol('C', 3, 3)\n    D = MatrixSymbol('D', 3, 3)\n    expr = Assignment(C[1, 2], D[1, 2])\n    assert mcode(expr) == \"C(2, 3) = D(2, 3);\"\n", "def test_custom_user_functions():\n    from sympy import Function, Lambda\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_matrix_fcn\"),\n              (lambda x: not x.is_Matrix, \"custom_scalar_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == 'custom_fcn(x) + custom_scalar_fcn(x) + custom_matrix_fcn([1 x])'\n", "def test_user_defined_functions():\n    from sympy import Function, Lambda\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"custom_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n        'custom_fcn(x) + custom_fcn(x) + custom_mat_fcn([1 x])'\n    assert mcode(f(x*y), user_functions={\"f\": \"custom_fcn\"}) == 'custom_fcn(x.*y)'\n"], "sample_1017": ["def test_BooleanAtom_noop():\n    b = BooleanAtom()\n    for op in ['+', '-', '*', '/', '**', '//', '%']:\n        raises(TypeError, lambda: eval(f'b {op} 1'))\n        raises(TypeError, lambda: eval(f'1 {op} b'))\n", "def test_Xor_simplification():\n    # Additional cases for simplifying Xor expressions\n    assert simplify_logic(Xor(A, A, B, B)) is false\n    assert simplify_logic(Xor(A, A, B, B, C)) == C\n    assert simplify_logic(Xor(A, Not(A), B)) == Not(B)\n    assert simplify_logic(Xor(A, Not(A), B, Not(B))) is true\n    assert simplify_logic(Xor(A, B, Not(A), Not(B))) is false\n", "def test_issue_9999():\n    # Additional coverage for the eval method in Not class\n    assert Not(Equality(x, y)).eval() == Ne(x, y)\n    assert Not(Ne(x, y)).eval() == Eq(x, y)\n    assert Not(GreaterThan(x, y)).eval() == StrictLessThan(x, y)\n    assert Not(LessThan(x, y)).eval() == StrictGreaterThan(x, y)\n    assert Not(StrictGreaterThan(x, y)).eval() == LessThan(x, y)\n    assert Not(StrictLessThan(x, y)).eval() == GreaterThan(x, y)\n    assert Not(S.true).eval() == S.false\n    assert Not(S.false).eval() == S.true\n", "def test_as_set():\n    assert And(x > 0, x < 1).as_set() == Interval.open(0, 1)\n    assert Or(x < 0, x > 1).as_set() == Union(Interval.open(-oo, 0), Interval.open(1, oo))\n    assert Not(x > 0).as_set() == Interval(-oo, 0)\n    assert And(x > 0, y > 0).as_set() == Intersection(Interval.open(0, oo), Interval.open(0, oo))\n    assert Or(x < 0, y < 0).as_set() == Union(Interval.open(-oo, 0), Interval.open(-oo, 0))\n    assert ITE(x > 0, y > 0, y < 0).as_set() == Union(Intersection(Interval.open(0, oo), Interval.open(0, oo)), Intersection(Interval.open(-oo, 0), Interval.open(-oo, 0)))\n    assert Xor(x > 0, y > 0).as_set() == Union(Intersection(Interval.open(0, oo), Interval.open(-oo, 0)), Intersection(Interval.open(-oo, 0), Interval.open(0, oo)))\n    assert And(x > 0, Or(y > 0, z > 0)).as_set() == Intersection(Interval.open(0, oo), Union(Interval.open(0, oo), Interval.open(0, oo)))\n    assert And(Or(x > 0, y > 0), z > 0).as_set() == Intersection(Union(Interval.open(0, oo), Interval.open(0, oo)), Interval.open(0, oo))\n    assert Or(And(x > 0, y > 0), z > 0).as_set() == Union(Intersection(Interval.open(0, oo), Interval.open(0, oo)), Interval.open(0, oo))\n", "def test_as_Boolean_edge_cases():\n    from sympy.core.symbol import Symbol\n    assert as_Boolean(Symbol('t', real=True)) == Symbol('t', real=True)\n    assert as_Boolean(Symbol('f', real=True)) == Symbol('f', real=True)\n    raises(TypeError, lambda: as_Boolean(Symbol('a', integer=True)))\n    raises(TypeError, lambda: as_Boolean(Symbol('b', positive=True)))\n"], "sample_1018": ["def test_fcode_AugmentedAssignment():\n    x, y, z = symbols('x y z')\n    # Test addition assignment\n    expr = aug_assign(x, '+', y)\n    assert fcode(expr, source_format='free') == 'x = x + y'\n    # Test subtraction assignment\n    expr = aug_assign(x, '-', y)\n    assert fcode(expr, source_format='free') == 'x = x - y'\n    # Test multiplication assignment\n    expr = aug_assign(x, '*', y)\n    assert fcode(expr, source_format='free') == 'x = x*y'\n    # Test division assignment\n    expr = aug_assign(x, '/', y)\n    assert fcode(expr, source_format='free') == 'x = x/y'\n    # Test complex expression assignment\n    expr = aug_assign(z, '+', x * y + 2)\n    assert fcode(expr, source_format='free') == 'z = z + 2 + x*y'\n", "def test_fcode_Mod():\n    x, y = symbols('x y')\n    \n    # Test cases for Fortran 90 and later standards\n    for standard in [90, 95, 2003, 2008]:\n        assert fcode(Mod(x, y), standard=standard) == \"      modulo(x, y)\"\n        assert fcode(x % y, standard=standard) == \"      modulo(x, y)\"\n\n    # Test cases for Fortran 66 and 77 standards (should raise NotImplementedError)\n    raises(NotImplementedError, lambda: fcode(Mod(x, y), standard=66))\n    raises(NotImplementedError, lambda: fcode(x % y, standard=66))\n    raises(NotImplementedError, lambda: fcode(Mod(x, y), standard=77))\n    raises(NotImplementedError, lambda: fcode(x % y, standard=77))\n", "def test_fcode_GoTo():\n    from sympy.codegen.ast import GoTo\n    x = symbols('x')\n    goto = GoTo('10')\n    assert fcode(goto, source_format='free') == 'go to 10'\n    computed_goto = GoTo(expr=x, labels=[1, 2, 3])\n    assert fcode(computed_goto, source_format='free') == 'go to (1, 2, 3), x'\n", "def test_fcode_Mod():\n    x, y = symbols('x y')\n    assert fcode(Mod(x, y), standard=95, source_format='fixed') == \"      modulo(x, y)\"\n    assert fcode(Mod(x, y), standard=2003, source_format='free') == \"modulo(x, y)\"\n    raises(NotImplementedError, lambda: fcode(Mod(x, y), standard=66))\n    raises(NotImplementedError, lambda: fcode(Mod(x, y), standard=77))\n", "def test_fcode_known_functions():\n    x, y = symbols('x y')\n    for sympy_func, fortran_func in known_functions.items():\n        if sympy_func == \"Abs\":\n            expr = abs(x)\n        elif sympy_func == \"conjugate\":\n            expr = conjugate(x)\n        elif sympy_func == \"Max\":\n            expr = Function('Max')(x, y)\n        elif sympy_func == \"Min\":\n            expr = Function('Min')(x, y)\n        else:\n            expr = Function(sympy_func)(x)\n        assert fcode(expr) == f\"      {fortran_func}(x)\"\n    assert fcode(Max(x, y)) == \"      max(x, y)\"\n    assert fcode(Min(x, y)) == \"      min(x, y)\"\n"], "sample_1019": ["def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(2*y + 3/z)) == (x**(y + 3/(2*z)), 2)\n", "def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(2.5*y)) == (x**(2.5*y), 1)\n", "def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(2*y/3.5)) == (x**(2*y/3.5), 1)\n    assert decompose_power_rat(x**sqrt(2)) == (x**sqrt(2), 1)\n", "def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(2*y + 3)) == (x**(2*y + 3), 1)\n    assert decompose_power_rat(x**(2*y/3 + 1/3)) == (x**(y/3 + 1/6), 2)\n", "def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**sqrt(2)) == (x**sqrt(2), 1)\n"], "sample_1020": ["def test_user_defined_function():\n    user_func = Function('user_func')\n    user_settings = {'user_functions': {'user_func': 'UserDefinedFunction'}}\n    assert mcode(user_func(x, y), **user_settings) == \"UserDefinedFunction[x, y]\"\n", "def test_user_defined_functions():\n    custom_func = Function('custom_func')\n    settings = {'user_functions': {'custom_func': 'CustomFunction'}}\n    assert mcode(custom_func(x, y), **settings) == \"CustomFunction[x, y]\"\n    settings = {'user_functions': {'custom_func': lambda *args: \"CustomFunction\"}}\n    assert mcode(custom_func(x, y), **settings) == \"CustomFunction[x, y]\"\n", "def test_user_defined_functions():\n    settings = {'user_functions': {'custom_func': 'CustomFunc'}}\n    from sympy import Function\n    custom_func = Function('custom_func')\n    assert mcode(custom_func(x, y, z), **settings) == \"CustomFunc[x, y, z]\"\n", "def test_user_defined_function():\n    user_funcs = {'myfunc': 'MyFunc'}\n    expr = Function('myfunc')(x, y)\n    assert mcode(expr, user_functions=user_funcs) == \"MyFunc[x, y]\"\n", "def test_custom_user_functions():\n    custom_functions = {'custom_func': 'CustomFunc'}\n    assert mcode(Function('custom_func')(x, y), user_functions=custom_functions) == \"CustomFunc[x, y]\"\n    custom_functions = {'custom_func': [(lambda *args: len(args) == 2, 'CustomFuncTwo'), (lambda *args: len(args) == 3, 'CustomFuncThree')]}\n    assert mcode(Function('custom_func')(x, y), user_functions=custom_functions) == \"CustomFuncTwo[x, y]\"\n    assert mcode(Function('custom_func')(x, y, z), user_functions=custom_functions) == \"CustomFuncThree[x, y, z]\"\n"], "sample_1021": ["def test_quaternion_subtraction():\n    q1 = Quaternion(5, 6, 7, 8)\n    q2 = Quaternion(1, 2, 3, 4)\n    q3 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    q4 = Quaternion(2, 3, 4, 5)\n\n    assert q1 - q2 == Quaternion(4, 4, 4, 4)\n    assert q2 - q1 == Quaternion(-4, -4, -4, -4)\n    assert q3 - (1 + 2*I) == Quaternion(2 + 2*I, 2 + 5*I, 0, 7 + 8*I)\n    assert q4 - 3 == Quaternion(-1, 3, 4, 5)\n", "def test_quaternion_static_methods():\n    assert Quaternion.__copysign(5, 3) == 5\n    assert Quaternion.__copysign(5, -3) == -5\n    assert Quaternion.__copysign(-5, 3) == 5\n    assert Quaternion.__copysign(-5, -3) == -5\n    assert Quaternion.__copysign(0, 3) == 0\n    assert Quaternion.__copysign(5, 0) == 0\n", "def test_quaternion_multiplication():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    q3 = Quaternion(0, 0, 0, 0)\n    q4 = Quaternion(1, 0, 0, 0)\n\n    assert q1 * q2 == Quaternion(-60, 12, 30, 24)\n    assert q1 * 2 == Quaternion(2, 4, 6, 8)\n    assert q1 * 0 == q3\n    assert q1 * q4 == q1\n    assert q4 * q1 == q1\n\n    q5 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    assert q5 * (2 + 3*I) == Quaternion((2 + 3*I)*(3 + 4*I), (2 + 3*I)*(2 + 5*I), 0, (2 + 3*I)*(7 + 8*I))\n    assert q5 * q2 == Quaternion(-64 - 84*I, 6 + 51*I, 58 + 21*I, 71 + 40*I)\n", "def test_quaternion_exceptions():\n    # Test if the constructor raises ValueError for non-commutative symbols\n    c = symbols(\"c\", commutative=False)\n    raises(ValueError, lambda: Quaternion(c, c, c, c))\n\n    # Test if the inverse function raises ValueError for quaternion with zero norm\n    q0 = Quaternion(0, 0, 0, 0)\n    raises(ValueError, lambda: q0.inverse())\n", "def test_quaternion_addition():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    \n    assert q1.add(q2) == Quaternion(6, 8, 10, 12)\n    assert q1 + q2 == Quaternion(6, 8, 10, 12)\n    assert q1.add(5) == Quaternion(6, 2, 3, 4)\n    \n    # Testing addition with real and complex fields\n    q3 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    q4 = Quaternion(1, 2, 3, 4)\n    \n    assert q3.add(2 + 3*I) == Quaternion(5 + 7*I, 2 + 5*I, 0, 7 + 8*I)\n    assert q4.add(2 + 3*I) == Quaternion(3, 5, 3, 4)\n    assert q3 + (2 + 3*I) == Quaternion(5 + 7*I, 2 + 5*I, 0, 7 + 8*I)\n    assert q4 + (2 + 3*I) == Quaternion(3, 5, 3, 4)\n"], "sample_1022": ["def test_repeated_decimals():\n    transformations = standard_transformations + (repeated_decimals,)\n    cases = {\n        '0.1[3]': 'Rational(1, 9)',\n        '0.[6]': 'Rational(2, 3)',\n        '0.[123]': 'Rational(123, 999)',\n        '1.2[34]': 'Integer(1) + Rational(2, 10) + Rational(34, 9900)',\n        '0.[012]': 'Rational(12, 999)',\n    }\n    for case, expected in cases.items():\n        implicit = parse_expr(case, transformations=transformations)\n        normal = parse_expr(expected, transformations=standard_transformations)\n        assert(implicit == normal), f\"Failed for {case} -> {implicit} != {normal}\"\n", "def test_convert_equals_signs():\n    cases = {\n        'x=2': 'Eq(x, 2)',\n        'x + y = 2': 'Eq(x + y, 2)',\n        'x=y=z': 'Eq(Eq(x, y), z)',\n        '(x + 1) = (y + 2)': 'Eq(x + 1, y + 2)',\n        '(x=2)=False': 'Eq(Eq(x, 2), False)',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (convert_equals_signs,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n\n    invalid_cases = ['1=2=3', 'x=(y=2)', 'sin(x)=cos(y)']\n    for case in invalid_cases:\n        raises(SyntaxError,\n               lambda: parse_expr(case, transformations=transformations2))\n", "def test_parse_expr():\n    local_dict = {'x': sympy.Symbol('x'), 'y': sympy.Symbol('y')}\n    global_dict = {'sin': sympy.sin, 'cos': sympy.cos}\n\n    # Test basic parsing\n    assert parse_expr('1 + 1') == sympy.Integer(2)\n    assert parse_expr('x + y', local_dict=local_dict) == sympy.Symbol('x') + sympy.Symbol('y')\n\n    # Test with global dictionary\n    assert parse_expr('sin(pi/2)', global_dict=global_dict) == sympy.Integer(1)\n\n    # Test with transformations\n    transformations = standard_transformations + (implicit_multiplication,)\n    assert parse_expr('2x', transformations=transformations) == sympy.Integer(2) * sympy.Symbol('x')\n    assert parse_expr('x y', transformations=transformations) == sympy.Symbol('x') * sympy.Symbol('y')\n\n    # Test evaluate=False\n    assert parse_expr('2**3', evaluate=False) == sympy.Pow(2, 3, evaluate=False)\n\n    # Test complex numbers\n    assert parse_expr('1 + 2j') == sympy.Integer(1) + sympy.Integer(2) * sympy.I\n\n    # Test repeated decimals\n    assert parse_expr('0.2[1]') == sympy.Rational(19, 90)\n\n    # Test factorial notation\n    assert parse_expr('5!') == sympy.factorial(5)\n\n    # Test convert_xor\n    transformations = standard_transformations + (convert_xor,)\n    assert parse_expr('2 ^ 3', transformations=transformations) == sympy.Pow(2, 3)\n", "def test_factorial_notation():\n    cases = {\n        '5!': 'factorial(5)',\n        'x!': 'factorial(x)',\n        'x!!': 'factorial2(x)',\n        'x! + y!': 'factorial(x) + factorial(y)',\n        '3! * 2!': 'factorial(3) * factorial(2)',\n        '(x + 1)!': 'factorial(x + 1)',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n\n    raises(TokenError, lambda: parse_expr('x!!!', transformations=transformations))\n", "def test_factorial_notation():\n    cases = {\n        '5!': 'factorial(5)',\n        '5!!': 'factorial2(5)',\n        'x!': 'factorial(x)',\n        'x!!': 'factorial2(x)',\n        '2*x! + 3*y!!': '2*factorial(x) + 3*factorial2(y)',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (factorial_notation,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal), (implicit, normal)\n\n    incorrect_cases = ['5!!!', 'x!!!']\n    for case in incorrect_cases:\n        raises(TokenError, lambda: parse_expr(case, transformations=transformations2))\n"], "sample_1023": ["def test_sieve_totientrange():\n    from sympy.ntheory.generate import sieve\n    sieve._reset()\n    assert list(sieve.totientrange(7, 18)) == [6, 4, 6, 4, 10, 4, 12, 6, 8, 8, 16]\n    assert list(sieve.totientrange(10, 20)) == [4, 12, 6, 8, 8, 16, 6, 18, 8, 12]\n    sieve._reset(totient=True)\n    assert list(sieve.totientrange(1, 1)) == []\n    assert list(sieve.totientrange(1, 2)) == [1]\n    assert list(sieve.totientrange(2, 3)) == [1, 2]\n", "def test_totient():\n    from sympy.ntheory.generate import sieve\n    sieve._reset()\n    assert list(sieve.totientrange(1, 10)) == [1, 1, 2, 2, 4, 2, 6, 4, 6]\n    assert list(sieve.totientrange(10, 20)) == [4, 10, 4, 12, 6, 8, 8, 16, 6, 18]\n    assert list(sieve.totientrange(20, 30)) == [8, 12, 10, 22, 8, 20, 12, 18, 12, 28]\n    sieve.extend_to_no(20)\n    assert list(sieve.totientrange(1, 30)) == [1, 1, 2, 2, 4, 2, 6, 4, 6, 4, 10, 4, 12, 6, 8, 8, 16, 6, 18, 8, 12, 10, 22, 8, 20, 12, 18, 12, 28]\n", "def test_sieve_extend_to_no():\n    sieve._reset()\n    sieve.extend_to_no(20)\n    expected_primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n    assert list(sieve[:20]) == expected_primes\n", "def test_nextprime_and_prevprime_exceptions():\n    from sympy.ntheory.generate import sieve\n    sieve._reset()\n    raises(ValueError, lambda: nextprime(2, ith=-1))\n    raises(ValueError, lambda: nextprime(2, ith=0))\n    raises(ValueError, lambda: nextprime(2.5))\n    raises(ValueError, lambda: prevprime(2))\n    raises(ValueError, lambda: prevprime(3.5))\n    raises(ValueError, lambda: prevprime(-5))\n", "def test_totient_and_mobius():\n    from sympy.ntheory.generate import sieve\n    sieve._reset()\n    assert list(sieve.totientrange(2, 10)) == [1, 2, 2, 4, 2, 6, 4, 6]\n    assert list(sieve.totientrange(15, 25)) == [8, 16, 6, 18, 8, 12, 10, 20, 12, 22]\n    assert list(sieve.mobiusrange(2, 10)) == [-1, -1, 1, 0, -1, 1, 0, 0]\n    assert list(sieve.mobiusrange(15, 25)) == [0, 1, -1, 0, 1, -1, 0, 1, 0, 0]\n\n    sieve._reset(totient=True)\n    assert list(sieve.totientrange(1, 5)) == [1, 1, 2, 2]\n    assert list(sieve.totientrange(10, 15)) == [4, 6, 4, 12, 6]\n\n    sieve._reset(mobius=True)\n    assert list(sieve.mobiusrange(1, 5)) == [1, -1, -1, 0]\n    assert list(sieve.mobiusrange(10, 15)) == [0, -1, 1, 0, 1]\n"], "sample_1024": ["def test_mpf_norm_nonzero_mantissa():\n    # Test case for mpf_norm function with non-zero mantissa\n    mpf_tuple = (1, 123456789, 10, 27)  # Arbitrary non-zero mantissa\n    prec = 20\n    normalized_tuple = mpf_norm(mpf_tuple, prec)\n    assert normalized_tuple == mpf_tuple, \"mpf_norm should return the same tuple when mantissa is non-zero\"\n\n    # Test with zero mantissa but non-zero bc\n    mpf_tuple_zero_man = (1, 0, 10, 27)\n    normalized_tuple_zero_man = mpf_norm(mpf_tuple_zero_man, prec)\n    assert normalized_tuple_zero_man == mpf_tuple_zero_man, \"mpf_norm should return the same tuple when mantissa is zero but bc is non-zero\"\n", "def test_mpf_norm_zeros():\n    assert mpf_norm((1, 0, 10, 0), 10) == (0, 0, 0, 0)  # zero mantissa, non-zero exponent\n    assert mpf_norm((0, 0, -10, 0), 10) == (0, 0, 0, 0)  # zero mantissa, non-zero exponent\n    assert mpf_norm((1, 0, 0, 0), 10) == (0, 0, 0, 0)  # zero mantissa, zero exponent\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)  # zero mantissa, zero exponent\n", "def test_ilcm_multiple_args():\n    assert ilcm(3, 5, 7) == 105\n    assert ilcm(2, 3, 4, 5) == 60\n    assert ilcm(6, 8, 9, 21) == 504\n    assert ilcm(1, 1, 1) == 1\n    raises(TypeError, lambda: ilcm(3))\n    raises(TypeError, lambda: ilcm())\n", "def test_issue_5107():\n    from sympy import Pow\n    assert Pow(-1, Rational(1, 3), evaluate=False) == (-1)**(Rational(1, 3))\n    assert Pow(-4, Rational(1, 2), evaluate=False) == I*2\n    assert Pow(-8, Rational(1, 3), evaluate=False) == -2\n    assert Pow(-8, Rational(1, 3), evaluate=True) == 2*(-1)**(Rational(1, 3))\n    assert Pow(-27, Rational(1, 3), evaluate=False) == -3\n    assert Pow(-27, Rational(1, 3), evaluate=True) == 3*(-1)**(Rational(1, 3))\n", "def test_mpf_norm():\n    # Check normalization with non-zero mantissa\n    assert mpf_norm((1, 1, 2, 2), 10) == (1, 1, 2, 2)\n    # Check normalization with zero exponent\n    assert mpf_norm((1, 1, 0, 2), 10) == (1, 1, 0, 2)\n    # Check normalization with negative exponent\n    assert mpf_norm((1, 1, -2, 2), 10) == (1, 1, -2, 2)\n    # Check normalization with zero mantissa and non-zero exponent\n    assert mpf_norm((1, 0, 2, 0), 10) == (0, 0, 0, 0)\n    # Check normalization with zero mantissa and zero exponent\n    assert mpf_norm((1, 0, 0, 0), 10) == (0, 0, 0, 0)\n    # Check normalization with zero mantissa and negative exponent\n    assert mpf_norm((1, 0, -2, 0), 10) == (0, 0, 0, 0)\n    # Check normalization with positive infinity\n    assert mpf_norm((0, 0, 0, -2), 10) == (0, 0, 0, 0)\n    # Check normalization with negative infinity\n    assert mpf_norm((1, 0, 0, -2), 10) == (1, 0, 0, -2)\n"], "sample_1025": ["def test_custom_print_functions():\n    class CustomFunc(Expr):\n            self.name = name\n            self.args = args\n\n            return f\"{self.name}({', '.join(map(str, self.args))})\"\n\n        return f\"custom.{expr.name}({', '.join(map(lambda arg: self._print(arg), expr.args))})\"\n\n    PythonCodePrinter._print_CustomFunc = custom_print_function\n\n    prntr = PythonCodePrinter()\n    expr = CustomFunc(\"myfunc\", x, y, z)\n    assert prntr.doprint(expr) == \"custom.myfunc(x, y, z)\"\n", "def test_Operators():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(x & y) == 'x and y'\n    assert prntr.doprint(x | y) == 'x or y'\n    assert prntr.doprint(~x) == 'not x'\n    assert prntr.doprint(x > y) == '(x > y)'\n    assert prntr.doprint(x >= y) == '(x >= y)'\n    assert prntr.doprint(x < y) == '(x < y)'\n    assert prntr.doprint(x <= y) == '(x <= y)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(Mod(x, 2)) == 'x % 2'\n    assert p.doprint(Assignment(x, 2)) == 'x = 2'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x > 6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert p.doprint(Piecewise((2, Le(x, 0)), (3, Gt(x, 0)), evaluate=False)) == '((2) if (x <= 0) else (3) if (x > 0) else None)'\n    assert p.doprint(And(x, y)) == 'x and y'\n    assert p.doprint(Or(x, y)) == 'x or y'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    assert not any(m.startswith('numpy') for m in p.module_imports)\n    assert not any(m.startswith('scipy') for m in p.module_imports)\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(And(x, y)) == 'sympy.And(x, y)'\n    assert p.doprint(Or(x, y)) == 'sympy.Or(x, y)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Rational(1, 2)) == 'sympy.Rational(1, 2)'\n    assert p.doprint(Assignment(x, 2)) == 'x = 2'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x > 6))) == 'sympy.Piecewise((1, sympy.Eq(x, 0)), (2, x > 6))'\n    assert p.doprint(MatrixSymbol('A', 2, 2)) == 'sympy.MatrixSymbol(A, 2, 2)'\n    assert p.doprint(Mod(x, 2)) == 'sympy.Mod(x, 2)'\n"], "sample_1026": ["def test_invalid_module():\n    # Test for invalid module name passed to _import function\n    raises(NameError, lambda: _import(\"invalid_module\"))\n", "def test_lambdastr():\n    # Test lambdastr with simple expressions\n    assert lambdastr(x, x**2) == 'lambda x: (x**2)'\n    assert lambdastr((x, y), x + y) == 'lambda x,y: (x + y)'\n    \n    # Test lambdastr with nested arguments\n    assert lambdastr((x, (y, z)), x + y) == 'lambda _0,_1: (lambda x,y,z: (x + y))(_0,_1[0],_1[1])'\n\n    # Test lambdastr with custom printer\n        return \"CUSTOM(%s)\" % expr\n    assert lambdastr(x, x**2, custom_printer) == 'lambda x: CUSTOM((x**2))'\n\n    # Test lambdastr with dummify\n    alpha = symbols('alpha')\n    assert lambdastr(alpha, alpha**2, dummify=True).startswith(\"lambda _\")\n\n    # Test lambdastr with DeferredVector\n    dvec = DeferredVector('vec')\n    assert lambdastr(dvec[0], dvec[0]**2) == 'lambda vec: (vec[0]**2)'\n\n    # Test lambdastr with iterable args\n    assert lambdastr([x, y], x + y) == 'lambda x,y: (x + y)'\n    assert lambdastr((x, y), x + y) == 'lambda x,y: (x + y)'\n\n    # Test lambdastr with strings\n    assert lambdastr(\"x, y\", \"x + y\") == 'lambda x, y: (x + y)'\n\n    # Test lambdastr with multiple expressions\n    assert lambdastr((x, y), [x, y, x + y]) == 'lambda x,y: ([x, y, x + y])'\n    assert lambdastr((x, y), (x, y, x + y)) == 'lambda x,y: ((x, y, x + y))'\n    assert lambdastr((x, y), {x: y, y: x}) == 'lambda x,y: ({x: y, y: x})'\n", "def test_lambdify_nested_function_call():\n    # Test nested function calls to ensure lambdify handles them correctly\n    f_inner = implemented_function('f_inner', lambda x: x + 1)\n    f_outer = implemented_function('f_outer', lambda x: 2 * x)\n    expr = f_outer(f_inner(x))\n    f = lambdify(x, expr)\n    assert f(3) == 8\n    assert f(0) == 2\n", "def test_lambdify_nested_expressions():\n    # Testing lambdify with nested expressions including trigonometric and piecewise functions\n    expr = Piecewise((cos(x) + sin(x), x < 1), (x**2, x >= 1))\n    f = lambdify(x, expr, modules=[\"numpy\"])\n    \n    # Test with numpy array\n    if numpy:\n        result = f(numpy.array([-1, 0, 0.5, 1, 2]))\n        expected = numpy.array([0.5403023058681398, 1.0, 1.479425538604203, 1.0, 4.0])\n        numpy.testing.assert_array_almost_equal(result, expected)\n    \n    # Test with single values\n    assert f(-1) == 0.5403023058681398\n    assert f(0) == 1.0\n    assert f(0.5) == 1.479425538604203\n    assert f(1) == 1.0\n    assert f(2) == 4.0\n", "def test_implemented_function():\n    from sympy import Function\n    from sympy.utilities.lambdify import implemented_function\n\n    # Define a new implemented function\n    MyFunc = implemented_function(Function('MyFunc'), lambda x, y: x + y + 1)\n\n    # Check if the function is implemented correctly\n    f = lambdify((x, y), MyFunc(x, y))\n    assert f(2, 3) == 6\n\n    # Check if we can use this implemented function within another expression\n    expr = MyFunc(x, y) + 2\n    g = lambdify((x, y), expr)\n    assert g(2, 3) == 8\n\n    # Check if we can nest implemented functions\n    NestedFunc = implemented_function(Function('NestedFunc'), lambda x: x * 2)\n    expr = NestedFunc(MyFunc(x, y))\n    h = lambdify((x, y), expr)\n    assert h(2, 3) == 12\n"], "sample_1027": ["def test_Poly_eval_subs():\n    # Test _eval_subs for Poly class\n    p = Poly(x**2 + y*x + 1, x, y)\n    \n    # Substitute x with 1\n    assert p._eval_subs(x, 1) == Poly(1 + y + 1, y)\n    # Substitute y with 2\n    assert p._eval_subs(y, 2) == Poly(x**2 + 2*x + 1, x)\n    # Substitute x with 1 and y with 2\n    assert p._eval_subs(x, 1)._eval_subs(y, 2) == 4\n    # Substitute y with 2 and x with 1\n    assert p._eval_subs(y, 2)._eval_subs(x, 1) == 4\n\n    # Substitute y with 0\n    assert p._eval_subs(y, 0) == Poly(x**2 + 1, x)\n    # Substitute x with y\n    assert p._eval_subs(x, y) == Poly(y**2 + y*y + 1, y)\n\n    # Test for expression substitution\n    q = Poly(x**2 + sin(x)*x + 1, x)\n    assert q._eval_subs(x, y) == Poly(y**2 + y*sin(y) + 1, y)\n    # Substitute x with sin(x)\n    assert q._eval_subs(x, sin(x)) == sin(x)**2 + x*sin(x) + 1\n\n    # Test for polynomial within polynomial\n    r = Poly(Poly(x**2 + y*x + 1, x), y)\n    assert r._eval_subs(x, y) == Poly(Poly(y**2 + y*y + 1, y), y)\n", "def test_as_dict():\n    # Testing as_dict() method\n    assert Poly(x**2 + 2*x*y + y**2, x, y).as_dict() == {(2, 0): 1, (1, 1): 2, (0, 2): 1}\n    assert Poly(3*x**2*y + 2*x*y**2 + 1, x, y).as_dict() == {(2, 1): 3, (1, 2): 2, (0, 0): 1}\n    assert Poly(4*x**2 + 3*x + 2, x).as_dict() == {(2,): 4, (1,): 3, (0,): 2}\n    assert Poly(5*x**2 + 6*y**2 + 7, x, y).as_dict() == {(2, 0): 5, (0, 2): 6, (0, 0): 7}\n    \n    # Testing the case when domain is 'EX'\n    assert Poly(x**2 + 2*x*sqrt(3) + 3, x, domain='EX').as_dict() == {(2,): 1, (1,): 2*sqrt(3), (0,): 3}\n    # Testing with non-trivial coefficients\n    assert Poly(x**2*y + 2*x*y**2 + y**3, x, y).as_dict() == {(2, 1): 1, (1, 2): 2, (0, 3): 1}\n", "def test_Poly_evalf():\n    assert Poly(x**2 + 2*x + 1, x).evalf() == Poly(x**2 + 2*x + 1, x, domain='RR')\n    assert Poly(1.0*x**2 + 2.0*x + 1.0, x).evalf() == Poly(1.0*x**2 + 2.0*x + 1.0, x, domain='RR')\n    assert Poly(sin(x) + cos(x), x).evalf() == Poly(sin(x) + cos(x), x, domain='RR')\n    assert Poly(sin(x) + 1, x).evalf() == Poly(sin(x) + 1, x, domain='EX')\n\n    p = Poly(x**2 + 2*x + 1)\n    assert p.evalf() == Poly(x**2 + 2*x + 1, x, domain='RR')\n    p = Poly(sin(x) + 1)\n    assert p.evalf() == Poly(sin(x) + 1, x, domain='EX')\n", "def test_Poly__eval_subs():\n    f = Poly(x**2 + 2*x + 1, x)\n\n    # Test substitution of the main generator\n    assert f._eval_subs(x, 1) == 4\n    assert f._eval_subs(x, y) == Poly(y**2 + 2*y + 1, y)\n    \n    # Test substitution of a non-generator symbol\n    f = Poly(x**2 + y*x + 1, x, y)\n    assert f._eval_subs(y, 2) == Poly(x**2 + 2*x + 1, x)\n\n    # Test substitution involving expressions\n    f = Poly(x**2 + y*x + 1, x, y)\n    assert f._eval_subs(x, y + 1) == Poly((y + 1)**2 + y*(y + 1) + 1, y)\n\n    # Test substitution with a number\n    f = Poly(x**2 + y*x + 1, x, y)\n    assert f._eval_subs(y, 3) == Poly(x**2 + 3*x + 1, x)\n\n    # Test for non-polynomial expressions\n    f = Poly(x**2 + sin(y*x), x, y, domain='EX')\n    assert f._eval_subs(y, 2) == Poly(x**2 + sin(2*x), x, domain='EX')\n", "def test_Poly_eval_subs():\n    p = Poly(x**2 + y, x, y)\n    \n    # Evaluating x with a number\n    assert p.eval(2) == Poly(4 + y, y)\n    assert p.eval({x: 2}) == Poly(4 + y, y)\n\n    # Evaluating y with a number\n    assert p.eval({y: 2}) == Poly(x**2 + 2, x)\n\n    # Evaluating both x and y\n    assert p.eval({x: 2, y: 3}) == 7\n\n    # Substituting x with y and vice versa\n    assert p.subs(x, y) == Poly(y**2 + y, y)\n    assert p.subs(y, x) == Poly(x**2 + x, x)\n\n    # Substituting y with a new symbol z\n    assert p.subs(y, z) == Poly(x**2 + z, x, z)\n\n    # Substituting x with an expression\n    assert p.subs(x, x + 1) == Poly((x + 1)**2 + y, x, y)\n    assert p.subs(x, 2*y) == Poly(4*y**2 + y, y)\n\n    # Test substitution where an error is expected\n    raises(DomainError, lambda: p.subs(x, 1/x))\n    raises(PolynomialError, lambda: p.subs(x, x/y))\n\n    # Test substitution with free symbols\n    q = Poly(x**2 + y*z, x, y, z)\n    assert q.subs(x, z) == Poly(z**2 + y*z, y, z)\n    assert q.subs(y, x) == Poly(x*z + x**2, x, z)\n    assert q.subs(z, y) == Poly(x**2 + y**2, x, y)\n"], "sample_1028": ["def test_Mod_eval():\n    assert Mod.eval(10, 3) == 1\n    assert Mod.eval(10, -3) == -2\n    assert Mod.eval(-10, 3) == 2\n    assert Mod.eval(-10, -3) == -1\n    assert Mod.eval(10, 5) == 0\n    assert Mod.eval(0, 5) == 0\n    assert Mod.eval(5, 0) is None  # Should raise ZeroDivisionError\n    assert Mod.eval(2, 2) == 0\n    assert Mod.eval(4, 2) == 0\n    assert Mod.eval(3, 2) == 1\n    assert Mod.eval(5, 2) == 1\n    assert Mod.eval(6, 2) == 0\n    assert Mod.eval(-2, 2) == 0\n    assert Mod.eval(-3, 2) == 1\n    assert Mod.eval(-4, 2) == 0\n    assert Mod.eval(3, 1) == 0\n    assert Mod.eval(-3, 1) == 0\n    assert Mod.eval(5, 3) == 2\n    assert Mod.eval(8, 3) == 2\n    assert Mod.eval(11, 3) == 2\n    assert Mod.eval(14, 3) == 2\n    assert Mod.eval(-5, 3) == 1\n    assert Mod.eval(-8, 3) == 1\n    assert Mod.eval(-11, 3) == 1\n    assert Mod.eval(-14, 3) == 1\n", "def test_Mod_eval():\n    # Check evaluation when both arguments are integers\n    assert Mod.eval(Integer(10), Integer(3)) == Integer(1)\n    assert Mod.eval(Integer(10), Integer(-3)) == Integer(-2)\n    assert Mod.eval(Integer(-10), Integer(3)) == Integer(2)\n    assert Mod.eval(Integer(-10), Integer(-3)) == Integer(-1)\n\n    # Check evaluation when divisor is zero\n    raises(ZeroDivisionError, lambda: Mod.eval(Integer(10), Integer(0)))\n\n    # Check evaluation when arguments are infinite\n    assert Mod.eval(S.Infinity, Integer(3)) == nan\n    assert Mod.eval(Integer(3), S.Infinity) == nan\n    assert Mod.eval(S.Infinity, S.Infinity) == nan\n    assert Mod.eval(S.NegativeInfinity, Integer(3)) == nan\n\n    # Check evaluation when arguments are NaN\n    assert Mod.eval(nan, Integer(3)) == nan\n    assert Mod.eval(Integer(3), nan) == nan\n\n    # Check evaluation when dividend is zero\n    assert Mod.eval(Integer(0), Integer(3)) == S.Zero\n\n    # Check evaluation for specific rational values\n    assert Mod.eval(Integer(5), Integer(2)) == Integer(1)\n    assert Mod.eval(Integer(4), Integer(2)) == S.Zero\n    assert Mod.eval(Integer(7), Integer(3)) == Integer(1)\n\n    # Check evaluation when dividend and divisor have the same value\n    assert Mod.eval(Integer(3), Integer(3)) == S.Zero\n    assert Mod.eval(Integer(-3), Integer(-3)) == S.Zero\n\n    # Check evaluation for expressions\n    assert Mod.eval(x + y, y) == Mod(x, y)\n    assert Mod.eval(x*y, y) == S.Zero\n    assert Mod.eval(x*y, x) == S.Zero\n", "def test_Mod_edge_cases():\n    # Testing edge cases for Mod function\n\n    # Modulus by one should always return zero\n    assert Mod(0, 1) == 0\n    assert Mod(1, 1) == 0\n    assert Mod(-1, 1) == 0\n    assert Mod(123456789, 1) == 0\n\n    # Testing with large numbers\n    large_num1 = 10**20\n    large_num2 = 10**15\n    assert Mod(large_num1, large_num2) == large_num1 % large_num2\n    assert Mod(large_num1, -large_num2) == large_num1 % -large_num2\n    assert Mod(-large_num1, large_num2) == -large_num1 % large_num2\n    assert Mod(-large_num1, -large_num2) == -large_num1 % -large_num2\n\n    # Modulus with infinity and negative infinity\n    assert Mod(oo, 1) == nan\n    assert Mod(-oo, 1) == nan\n    assert Mod(1, oo) == 1\n    assert Mod(1, -oo) == 1\n    assert Mod(oo, x) == nan\n    assert Mod(-oo, x) == nan\n\n    # Modulus with symbolic expressions\n    expr1 = x**2 + y\n    expr2 = x + 2\n    assert Mod(expr1, expr2) == Mod(x**2 + y, x + 2)\n\n    # Modulus with zero divisor should raise ZeroDivisionError\n    with raises(ZeroDivisionError):\n        Mod(1, 0)\n\n    # Modulus with complex numbers\n    assert Mod(1 + I, 2 + 3*I) == (1 + I) % (2 + 3*I)\n    assert Mod(3 + 4*I, 1 - I) == (3 + 4*I) % (1 - I)\n    assert Mod(-3 + 4*I, 1 - I) == (-3 + 4*I) % (1 - I)\n", "def test_Mod_special_cases():\n    # Test modulo with known zero dividend\n    assert Mod(0, x) == 0\n    assert Mod(0, 5) == 0\n    assert Mod(0, -5) == 0\n\n    # Test modulo with known divisor of 1\n    assert Mod(x, 1) == 0\n    assert Mod(-x, 1) == 0\n    assert Mod(5, 1) == 0\n    assert Mod(-5, 1) == 0\n\n    # Test modulo with both arguments being the same\n    assert Mod(x, x) == 0\n    assert Mod(-x, x) == 0\n    assert Mod(x, -x) == 0\n    assert Mod(-x, -x) == 0\n    assert Mod(5, 5) == 0\n    assert Mod(-5, 5) == 0\n    assert Mod(5, -5) == 0\n    assert Mod(-5, -5) == 0\n\n    # Test modulo with infinity\n    assert Mod(oo, 5) == nan\n    assert Mod(5, oo) == 5\n    assert Mod(-oo, 5) == nan\n    assert Mod(5, -oo) == 5\n    assert Mod(oo, oo) == nan\n    assert Mod(-oo, oo) == nan\n    assert Mod(oo, -oo) == nan\n    assert Mod(-oo, -oo) == nan\n\n    # Test modulo with zero divisor (should raise ZeroDivisionError)\n    raises(ZeroDivisionError, lambda: Mod(x, 0))\n    raises(ZeroDivisionError, lambda: Mod(5, 0))\n    raises(ZeroDivisionError, lambda: Mod(0, 0))\n", "def test_Mod_eval():\n    assert Mod.eval(5, 3) == 2\n    assert Mod.eval(-5, 3) == 1\n    assert Mod.eval(5, -3) == -1\n    assert Mod.eval(-5, -3) == -2\n    assert Mod.eval(6, 2) == 0\n    assert Mod.eval(6, 4) == 2\n    assert Mod.eval(0, 1) == 0\n    assert Mod.eval(5, 0) is None  # should raise ZeroDivisionError in actual use\n    assert Mod.eval(nan, 1) == nan\n    assert Mod.eval(1, nan) == nan\n    assert Mod.eval(nan, nan) == nan\n"], "sample_1029": ["def test_Cycle():\n    from sympy.combinatorics.permutations import Cycle\n    sT(Cycle(1, 2, 3), \"Cycle(1, 2, 3)\")\n", "def test_Cycle():\n    from sympy.combinatorics.permutations import Cycle\n    c = Cycle(1, 2, 3)\n    sT(c, \"Cycle(1, 2, 3)\")\n", "def test_emptyPrinter():\n    class CustomExpr:\n            self.args = args\n\n    class CustomExprWithModuleName:\n        __module__ = 'custom_module'\n        __name__ = 'CustomExpr'\n\n    sT(CustomExpr([x, y]), \"CustomExpr(Symbol('x'), Symbol('y'))\")\n    sT(CustomExprWithModuleName(), \"<'custom_module.CustomExpr'>\")\n    sT(\"string\", \"string\")\n", "def test_Cycle():\n    from sympy.combinatorics.permutations import Cycle\n    sT(Cycle()(1, 2, 3), \"Cycle()(1, 2, 3)\")\n", "def test_Cycle():\n    from sympy.combinatorics.permutations import Cycle\n    c = Cycle(0, 1, 2, 3)\n    sT(c, repr(c))\n"], "sample_1030": ["def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n    # Test coplanarity of points\n    a = Point3D(1, 2, 3)\n    b = Point3D(4, 5, 6)\n    c = Point3D(7, 8, 9)\n    d = Point3D(1, 4, 2)\n    assert are_coplanar(a, b, c) == False\n    assert are_coplanar(a, b, d) == True\n\n    # Test coplanarity of lines\n    line1 = Line3D(a, b)\n    line2 = Line3D(c, d)\n    line3 = Line3D(Point3D(2, 3, 4), Point3D(5, 6, 7))\n    assert are_coplanar(line1, line2) == True\n    assert are_coplanar(line1, line3) == False\n\n    # Test coplanarity of a mix of points and lines\n    assert are_coplanar(a, line1, b, line2) == True\n    assert are_coplanar(a, line1, c, line3) == False\n\n    # Test coplanarity with a plane\n    plane = Plane(Point3D(0, 0, 0), normal_vector=(1, 1, 1))\n    assert are_coplanar(plane, a, b) == True\n    assert are_coplanar(plane, c) == False\n\n    # Test with fewer than 3 points\n    assert are_coplanar(a, b) == False\n", "def test_are_coplanar():\n    from sympy.geometry import Line3D, Point3D, Plane\n    assert not are_coplanar(\n        Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1)),\n        Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1)),\n        Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    )\n    assert are_coplanar(\n        Line3D(Point3D(1, 1, 1), Point3D(2, 2, 2)),\n        Line3D(Point3D(2, 2, 2), Point3D(3, 3, 3)),\n        Line3D(Point3D(3, 3, 3), Point3D(4, 4, 4))\n    )\n    assert are_coplanar(\n        Point3D(1, 2, 3),\n        Point3D(2, 3, 4),\n        Point3D(3, 4, 5)\n    )\n    assert not are_coplanar(\n        Point3D(1, 1, 1),\n        Point3D(2, 2, 2),\n        Point3D(3, 3, 4)\n    )\n    p = Plane(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 0))\n    assert are_coplanar(p, Point3D(2, 2, 0), Point3D(3, 3, 0))\n    assert not are_coplanar(p, Point3D(2, 2, 2))\n", "def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n    # Coplanar points test\n    a = Point3D(0, 0, 0)\n    b = Point3D(1, 1, 1)\n    c = Point3D(2, 2, 2)\n    d = Point3D(3, 3, 3)\n    assert are_coplanar(a, b, c, d) is True\n\n    # Non-coplanar points test\n    e = Point3D(1, 0, 0)\n    f = Point3D(0, 1, 0)\n    g = Point3D(0, 0, 1)\n    assert are_coplanar(a, e, f, g) is False\n\n    # Coplanar lines test\n    line1 = Line3D(a, b)\n    line2 = Line3D(c, d)\n    assert are_coplanar(line1, line2) is True\n\n    # Non-coplanar lines test\n    line3 = Line3D(e, f)\n    assert are_coplanar(line1, line3) is False\n\n    # Plane with points test\n    plane = Plane(a, e, f)\n    assert are_coplanar(plane, a, e, f) is True\n    assert are_coplanar(plane, g) is False\n", "def test_are_coplanar():\n    from sympy.geometry import Point3D, Line3D, Plane\n    \n    # Coplanar points\n    a = Point3D(0, 0, 0)\n    b = Point3D(1, 1, 0)\n    c = Point3D(2, 2, 0)\n    assert are_coplanar(a, b, c) == True\n    \n    # Non-coplanar points\n    d = Point3D(1, 1, 1)\n    assert are_coplanar(a, b, d) == False\n    \n    # Coplanar lines\n    l1 = Line3D(a, b)\n    l2 = Line3D(c, d)\n    assert are_coplanar(l1, l2, Plane(a, b, c)) == True\n    \n    # Non-coplanar lines\n    e = Point3D(0, 0, 1)\n    l3 = Line3D(a, e)\n    assert are_coplanar(l1, l3) == False\n    \n    # Mixed geometrical entities\n    assert are_coplanar(a, l1, Plane(a, b, c)) == True\n    assert are_coplanar(a, l1, Plane(a, b, d)) == False\n\n    # 2D entities treated as 3D with z=0\n    p2d_1 = Point(1, 1)\n    p2d_2 = Point(2, 2)\n    p2d_3 = Point(3, 3)\n    assert are_coplanar(p2d_1, p2d_2, p2d_3) == True\n", "def test_are_coplanar():\n    from sympy.geometry import Point3D, Line3D, Plane\n    # 3 points defining a plane\n    p1, p2, p3 = Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 0)\n    assert are_coplanar(p1, p2, p3) is True\n\n    # points and lines that are not coplanar\n    l1 = Line3D(Point3D(1, 1, 1), Point3D(2, 2, 2))\n    l2 = Line3D(Point3D(1, 0, 1), Point3D(2, 1, 3))\n    assert are_coplanar(l1, l2) is False\n\n    # coplanar points and lines\n    l3 = Line3D(Point3D(0, 1, 0), Point3D(1, 1, 0))\n    assert are_coplanar(p1, p2, p3, l3) is True\n\n    # a mix of 2D and 3D points\n    p4 = Point3D(2, 2, 0)\n    p2d = Point(2, 2)\n    assert are_coplanar(p1, p2, p3, p2d) is True\n\n    # plane defined by points\n    pl = Plane(p1, p2, p3)\n    assert are_coplanar(pl, p4) is True\n    assert are_coplanar(pl, l1) is False\n"], "sample_1031": ["def test_quantity_properties():\n    # Test if the properties of Quantity instances are correctly set\n    newton = Quantity(\"newton\", abbrev=\"N\")\n    newton.set_dimension(force)\n    newton.set_scale_factor(kilogram * meter / second**2)\n\n    assert newton.name == \"newton\"\n    assert newton.abbrev == \"N\"\n    assert newton.dimension == force\n    assert newton.scale_factor == kilogram * meter / second**2\n\n    pascal = Quantity(\"pascal\", abbrev=\"Pa\")\n    pascal.set_dimension(pressure)\n    pascal.set_scale_factor(newton / meter**2)\n\n    assert pascal.name == \"pascal\"\n    assert pascal.abbrev == \"Pa\"\n    assert pascal.dimension == pressure\n    assert pascal.scale_factor == newton / meter**2\n", "def test_scale_factors():\n    # Test if scale factors of quantities are correctly set and derived units have correct scale factors\n    assert percent.scale_factor == Rational(1, 100)\n    assert permille.scale_factor == Rational(1, 1000)\n    assert radian.scale_factor == One\n    assert degree.scale_factor == pi/180\n    assert steradian.scale_factor == One\n    assert angular_mil.scale_factor == 2*pi/6400\n\n    assert meter.scale_factor == One\n    assert kilogram.scale_factor == One\n    assert second.scale_factor == One\n    assert ampere.scale_factor == One\n    assert kelvin.scale_factor == One\n    assert mole.scale_factor == One\n    assert candela.scale_factor == One\n\n    assert gram.scale_factor == kilogram/kilo\n    assert milligram.scale_factor == milli*gram\n    assert microgram.scale_factor == micro*gram\n\n    assert newton.scale_factor == kilogram*meter/second**2\n    assert joule.scale_factor == newton*meter\n    assert watt.scale_factor == joule/second\n    assert pascal.scale_factor == newton/meter**2\n    assert hertz.scale_factor == One\n\n    assert coulomb.scale_factor == One\n    assert volt.scale_factor == joule/coulomb\n    assert ohm.scale_factor == volt/ampere\n    assert siemens.scale_factor == ampere/volt\n    assert farad.scale_factor == coulomb/volt\n    assert henry.scale_factor == volt*second/ampere\n    assert tesla.scale_factor == volt*second/meter**2\n    assert weber.scale_factor == joule/ampere\n\n    assert dioptre.scale_factor == 1/meter\n    assert lux.scale_factor == steradian*candela/meter**2\n    assert katal.scale_factor == mol/second\n    assert gray.scale_factor == meter**2/second**2\n    assert becquerel.scale_factor == 1/second\n\n    assert kilometer.scale_factor == kilo*meter\n    assert decimeter.scale_factor == deci*meter\n    assert centimeter.scale_factor == centi*meter\n    assert millimeter.scale_factor == milli*meter\n    assert micrometer.scale_factor == micro*meter\n    assert nanometer.scale_factor == nano*meter\n    assert picometer.scale_factor == pico*meter\n\n    assert foot.scale_factor == Rational(3048, 10000)*meter\n    assert", "def test_quantity_dimensions():\n    # Test for various quantities and their dimensions\n    assert meter.dimension == length\n    assert kilogram.dimension == mass\n    assert second.dimension == time\n    assert ampere.dimension == current\n    assert kelvin.dimension == temperature\n\n    # Test for derived units\n    assert newton.dimension == force\n    assert joule.dimension == energy\n    assert watt.dimension == power\n    assert pascal.dimension == pressure\n    assert hertz.dimension == frequency\n\n    # Test for special units and constants\n    assert gravitational_constant.dimension == length**3*mass**-1*time**-2\n    assert speed_of_light.dimension == velocity\n    assert hbar.dimension == action\n    assert electronvolt.dimension == energy\n    assert boltzmann_constant.dimension == energy/temperature\n", "def test_quantity_definitions():\n    assert percent.scale_factor == Rational(1, 100)\n    assert percent.dimension == One\n\n    assert permille.scale_factor == Rational(1, 1000)\n    assert permille.dimension == One\n\n    assert radian.scale_factor == One\n    assert radian.dimension == One\n\n    assert degree.scale_factor == pi / 180\n    assert degree.dimension == One\n\n    assert steradian.scale_factor == One\n    assert steradian.dimension == One\n\n    assert angular_mil.scale_factor == 2 * pi / 6400\n    assert angular_mil.dimension == One\n\n    assert meter.scale_factor == One\n    assert meter.dimension == length\n\n    assert kilogram.scale_factor == One\n    assert kilogram.dimension == mass\n\n    assert second.scale_factor == One\n    assert second.dimension == time\n\n    assert ampere.scale_factor == One\n    assert ampere.dimension == current\n\n    assert kelvin.scale_factor == One\n    assert kelvin.dimension == temperature\n\n    assert mole.scale_factor == One\n    assert mole.dimension == amount_of_substance\n\n    assert candela.scale_factor == One\n    assert candela.dimension == luminous_intensity\n\n    assert gram.scale_factor == kilogram / kilo\n    assert gram.dimension == mass\n\n    assert milligram.scale_factor == milli * gram\n    assert milligram.dimension == mass\n\n    assert microgram.scale_factor == micro * gram\n    assert microgram.dimension == mass\n", "def test_quantity_properties():\n    meter = Quantity(\"meter\")\n    meter.set_dimension(length)\n    meter.set_scale_factor(One)\n\n    kilogram = Quantity(\"kilogram\")\n    kilogram.set_dimension(mass)\n    kilogram.set_scale_factor(One)\n\n    second = Quantity(\"second\")\n    second.set_dimension(time)\n    second.set_scale_factor(One)\n\n    assert meter.dimension == length\n    assert meter.scale_factor == One\n    assert kilogram.dimension == mass\n    assert kilogram.scale_factor == One\n    assert second.dimension == time\n    assert second.scale_factor == One\n\n    newton = Quantity(\"newton\")\n    newton.set_dimension(force)\n    newton.set_scale_factor(kilogram * meter / second ** 2)\n\n    assert newton.dimension == force\n    assert newton.scale_factor == kilogram * meter / second ** 2\n"], "sample_1032": ["def test_identity_function():\n    from sympy import Id, Symbol\n    x = Symbol('x')\n    \n    # Check that Id(x) returns x\n    assert Id(x) == x\n    \n    # Check that Id(2) returns 2\n    assert Id(2) == 2\n    \n    # Check that Id works within expressions\n    assert Id(x + 1) == x + 1\n    assert Id(x * 2) == x * 2\n    \n    # Check that Id works with more complex expressions\n    assert Id(x**2 + 2*x + 1) == x**2 + 2*x + 1\n    \n    # Check that Id works with functions\n    from sympy import sin, cos\n    assert Id(sin(x)) == sin(x)\n    assert Id(cos(x)) == cos(x)\n    \n    # Check that Id works with special values\n    assert Id(oo) == oo\n    assert Id(-oo) == -oo\n    assert Id(S.NaN) == S.NaN\n    assert Id(S.ComplexInfinity) == S.ComplexInfinity\n    \n    # Check that Id can be differentiated\n    assert Id(x).diff(x) == 1\n    assert Id(x**2).diff(x) == 2*x\n    \n    # Check that Id can be evaluated\n    assert Id(x).subs(x, 5) == 5\n    assert Id(x).evalf(subs={x: 3.14}) == 3.14\n", "def test_identity_function():\n    from sympy import Symbol, Id\n    x = Symbol('x')\n    assert Id(x) == x\n    assert Id(x + 1) == x + 1\n    assert Id(x**2) == x**2\n    assert Id(Id(x)) == x\n    assert Id(Id(x + 1)) == x + 1\n    assert Id(x).subs(x, 2) == 2\n    assert Id(x).diff(x) == 1\n    assert Id(0) == 0\n    assert Id(S(1)) == 1\n    assert Id(S.NegativeInfinity) == S.NegativeInfinity\n    assert Id(S.ComplexInfinity) == S.ComplexInfinity\n", "def test_special_cases():\n    # Test special cases and edge cases for Min and Max functions\n    from sympy.abc import x, y\n\n    # Min with zero and negative zero\n    assert Min(0, -0) == 0\n    assert Min(-0, 0) == 0\n\n    # Max with zero and negative zero\n    assert Max(0, -0) == 0\n    assert Max(-0, 0) == 0\n\n    # Min and Max with complex arguments\n    raises(ValueError, lambda: Min(3 + 4*I, 5 + 6*I))\n    raises(ValueError, lambda: Max(3 + 4*I, 5 + 6*I))\n\n    # Min and Max with a mix of real and complex numbers\n    raises(ValueError, lambda: Min(3, 5 + 6*I))\n    raises(ValueError, lambda: Max(3, 5 + 6*I))\n\n    # Check Max with infinities\n    assert Max(-oo, oo) == oo\n    assert Max(oo, -oo) == oo\n    assert Max(-oo, 0, oo) == oo\n\n    # Check Min with infinities\n    assert Min(-oo, oo) == -oo\n    assert Min(oo, -oo) == -oo\n    assert Min(-oo, 0, oo) == -oo\n\n    # Min and Max with symbolic infinities\n    from sympy import Symbol\n    a = Symbol('a', real=True)\n    assert Max(a, oo) == oo\n    assert Max(a, -oo) == a\n    assert Min(a, oo) == a\n    assert Min(a, -oo) == -oo\n\n    # Check behavior with NaN\n    from sympy import nan\n    raises(ValueError, lambda: Min(nan, 1))\n    raises(ValueError, lambda: Max(nan, 1))\n\n    # Min and Max with expressions involving multiple operations\n    from sympy import exp, log\n    assert Min(exp(1), log(1)) == log(1)\n    assert Max(exp(1), log(1)) == exp(1)\n\n    # Commutativity and associativity\n    assert Min(x, y) == Min(y, x)\n    assert Max(x, y) == Max(y, x)\n    assert Min(x, Min(y", "def test_issue_15000():\n    from sympy.abc import x, y, z\n\n    # Test for Min/Max with complex numbers and invalid comparisons\n    raises(ValueError, lambda: Min(3 + 4*I, 5))\n    raises(ValueError, lambda: Max(3 + 4*I, 5))\n    raises(ValueError, lambda: Min(3 + 4*I, 5, x))\n    raises(ValueError, lambda: Max(3 + 4*I, 5, x))\n    raises(ValueError, lambda: Min(x, 3 + 4*I))\n    raises(ValueError, lambda: Max(x, 3 + 4*I))\n\n    # Test for Min/Max with incompatible types\n    raises(ValueError, lambda: Min(x, \"string\"))\n    raises(ValueError, lambda: Max(x, \"string\"))\n    raises(ValueError, lambda: Min([1, 2, 3], x))\n    raises(ValueError, lambda: Max([1, 2, 3], x))\n\n    # Test Min/Max with nested expressions and assumptions\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n    assert Min(p, n, x) == Min(n, x)\n    assert Max(p, n, x) == Max(p, x)\n    assert Min(p, -5, x, y) == Min(-5, x, y)\n    assert Max(p, -5, x, y) == Max(p, x, y)\n", "def test_issue_14123():\n    from sympy.abc import a, b, c, d\n    # test multiple nested Min and Max combinations\n    assert Min(Max(a, Min(b, c)), d) == Min(d, Max(a, Min(b, c)))\n    assert Max(Min(a, Max(b, c)), d) == Max(d, Min(a, Max(b, c)))\n    assert Min(Max(a, b), Max(c, d)) == Max(Min(a, c), Min(a, d), Min(b, c), Min(b, d))\n    assert Max(Min(a, b), Min(c, d)) == Min(Max(a, c), Max(a, d), Max(b, c), Max(b, d))\n    assert Min(a, Max(b, Min(c, d)), Max(a, b)) == Min(a, b)\n    assert Max(Min(a, Max(b, c)), Min(d, Max(a, b))) == Max(Min(a, b), Min(a, d), Min(b, c), Min(d, c))\n"], "sample_1033": ["def test_Add_as_coefficients_dict():\n    from sympy.abc import a, b, x, y\n\n    # Test for simple Add expression\n    expr = 3*x + 2*y + a\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == {x: 3, y: 2, a: 1}\n\n    # Test for Add expression with constants\n    expr = 3*x + 2*y + a + 5\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == {x: 3, y: 2, a: 1, S.One: 5}\n\n    # Test for Add expression with repeated terms\n    expr = 3*x + 2*x + 4*y - 2*y + 3\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == {x: 5, y: 2, S.One: 3}\n\n    # Test for Add expression with no coefficients\n    expr = x + y\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == {x: 1, y: 1}\n\n    # Test for Add expression with negative coefficients\n    expr = -3*x + 2*y - 5 + a\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == {x: -3, y: 2, S.One: -5, a: 1}\n\n    # Test for non-Add expression\n    expr = 3*x\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == {x: 3}\n", "def test_unevaluated_Add():\n    from sympy.abc import x, y\n    from sympy import Add, S\n\n    # Test basic functionality\n    a = _unevaluated_Add(*[S(1.0), x, S(2)])\n    assert a.args[0] == S(3.0)\n    assert a.args[1] == x\n\n    # Test with nested Add and different order options\n    opts = (Add(x, y, evaluate=False), Add(y, x, evaluate=False))\n    a = _unevaluated_Add(x, y)\n    assert a in opts and a == _unevaluated_Add(x, y)\n\n    # Test with addition of expressions\n    assert _unevaluated_Add(x + 1, x + 2) == x + x + 3\n\n    # Test with nested unevaluated Add\n    nested_add = _unevaluated_Add(_unevaluated_Add(x, y), S(2))\n    assert nested_add.args[0] == S(2)\n    assert nested_add.args[1] == x + y\n\n    # Test with multiple numbers\n    b = _unevaluated_Add(*[S(1), S(2), x])\n    assert b.args[0] == S(3)\n    assert b.args[1] == x\n", "def test_as_coefficients_dict():\n    expr = 3*x + 5*y + 7\n    expected_dict = {S.One: 7, x: 3, y: 5}\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == expected_dict\n\n    expr = 3*a*x + 4*a*x + 5*y\n    expected_dict = {y: 5, a*x: 7}\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == expected_dict\n\n    expr = 3*a*x + 4*a*x + 5*y + 7\n    expected_dict = {S.One: 7, y: 5, a*x: 7}\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == expected_dict\n\n    expr = a*x + b*x + a*y + b*y\n    expected_dict = {a*x: 1, b*x: 1, a*y: 1, b*y: 1}\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == expected_dict\n\n    expr = 3*x**2 + 2*x + 1\n    expected_dict = {S.One: 1, x**2: 3, x: 2}\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == expected_dict\n\n    expr = 3*x**2 + 2*x*y + y\n    expected_dict = {x**2: 3, x*y: 2, y: 1}\n    coeff_dict = expr.as_coefficients_dict()\n    assert coeff_dict == expected_dict\n", "def test_unevaluated_add():\n    from sympy.core.add import _unevaluated_Add as uAdd\n    from sympy import S\n    a, b, c = symbols('a b c')\n\n    # Test basic functionality\n    assert uAdd(a, b).args == (a, b)\n    assert uAdd(a, b, S(1)).args == (1, a, b)\n    assert uAdd(S(2), S(3), a, b).args == (5, a, b)\n    \n    # Test nested Add objects\n    assert uAdd(uAdd(a, b), c).args == (a, b, c)\n    assert uAdd(a, uAdd(b, c)).args == (a, b, c)\n    \n    # Test with numbers\n    assert uAdd(S(1.5), S(2), a).args == (3.5, a)\n    assert uAdd(S(2), S(3.5), a, b).args == (5.5, a, b)\n    \n    # Test with zero and negative numbers\n    assert uAdd(S(0), a, b).args == (a, b)\n    assert uAdd(S(-1), S(1), a).args == (0, a)\n    \n    # Test noncommutative symbols\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    assert uAdd(A, B).args == (A, B)\n    assert uAdd(A, B, S(1)).args == (1, A, B)\n    assert uAdd(S(2), A, B).args == (2, A, B)\n\n    # Test order preservation\n    assert uAdd(c, b, a).args == (a, b, c)\n    assert uAdd(c, S(1), b, a).args == (1, a, b, c)\n", "def test_unevaluated_Add():\n    from sympy.core.add import _unevaluated_Add\n    from sympy import S, Add\n    from sympy.abc import x, y, z\n\n    # Check addition of numbers and symbols\n    a = _unevaluated_Add(*[S(1.0), x, S(2)])\n    assert a.args[0] == 3.0\n    assert a.args[1] == x\n\n    # Check order of arguments with symbols\n    opts = (Add(x, y, evaluated=False), Add(y, x, evaluated=False))\n    a = _unevaluated_Add(x, y)\n    assert a in opts and a == _unevaluated_Add(x, y)\n\n    # Check nested addition\n    a = _unevaluated_Add(x + 1, x + 2)\n    assert a == Add(x, x, 3, evaluated=False)\n\n    # Check addition involving zero\n    a = _unevaluated_Add(x, S(0))\n    assert a == x\n\n    # Check addition with an unevaluated Add as an argument\n    a = _unevaluated_Add(_unevaluated_Add(x, y), z)\n    assert a == Add(x, y, z, evaluated=False)\n\n    # Check addition with an Add object\n    a = _unevaluated_Add(Add(x, y, evaluate=False), z)\n    assert a == Add(x, y, z, evaluated=False)\n\n    # Check addition with repeated symbols\n    a = _unevaluated_Add(x, x)\n    assert a == Add(x, x, evaluated=False)\n\n    # Check handling of negative numbers\n    a = _unevaluated_Add(x, S(-2))\n    assert a == Add(x, -2, evaluated=False)\n"], "sample_1034": ["def test_invalid_OracleGate_args():\n    try:\n        OracleGate(2)\n    except QuantumError as e:\n        assert str(e) == 'Insufficient/excessive arguments to Oracle.  Please supply the number of qubits and an unknown function.'\n\n    try:\n        OracleGate(2, \"not_callable\")\n    except TypeError as e:\n        assert str(e) == \"Callable expected, got: 'not_callable'\"\n\n    try:\n        OracleGate(\"two\", return_one_on_two)\n    except TypeError as e:\n        assert str(e) == \"Integer expected, got: 'two'\"\n", "def test_invalid_oracle_gate():\n    try:\n        OracleGate(2, \"not_callable\")\n    except TypeError as e:\n        assert str(e) == \"Callable expected, got: 'not_callable'\"\n\n    try:\n        OracleGate(\"invalid_integer\", return_one_on_two)\n    except TypeError as e:\n        assert str(e) == \"Integer expected, got: 'invalid_integer'\"\n\n    try:\n        OracleGate(2)\n    except QuantumError as e:\n        assert str(e) == \"Insufficient/excessive arguments to Oracle.  Please supply the number of qubits and an unknown function.\"\n", "def test_apply_grover_with_zero_qubits():\n    try:\n        apply_grover(return_one_on_one, 0)\n    except QuantumError as e:\n        assert str(e) == \"Grover's algorithm needs nqubits > 0, received 0 qubits\"\n", "def test_invalid_OracleGate_initialization():\n    try:\n        OracleGate(2, \"not a function\")\n    except TypeError as e:\n        assert str(e) == \"Callable expected, got: 'not a function'\"\n    else:\n        assert False, \"TypeError not raised for non-callable oracle\"\n\n    try:\n        OracleGate(\"not an integer\", return_one_on_two)\n    except TypeError as e:\n        assert str(e) == \"Integer expected, got: 'not an integer'\"\n    else:\n        assert False, \"TypeError not raised for non-integer qubits\"\n\n    try:\n        OracleGate(2)\n    except QuantumError as e:\n        assert str(e) == \"Insufficient/excessive arguments to Oracle.  Please supply the number of qubits and an unknown function.\"\n    else:\n        assert False, \"QuantumError not raised for insufficient arguments\"\n", "def test_apply_grover_with_iterations():\n    nqubits = 3\n    iterations = 2  # Specify the number of iterations\n    result_state = apply_grover(return_one_on_one, nqubits, iterations)\n    \n    # Expected result after 2 iterations\n    expected_state = (-1/sqrt(8)) * superposition_basis(nqubits) + (3/sqrt(8)) * IntQubit(1, nqubits=nqubits)\n    \n    assert result_state == qapply(expected_state)\n"], "sample_1035": ["def test_qubit_flip():\n    q = Qubit(0, 1, 0, 1)\n    flipped_q = q.flip(0, 2)\n    assert flipped_q == Qubit(1, 1, 1, 1)\n    flipped_q = q.flip(1, 3)\n    assert flipped_q == Qubit(0, 0, 0, 0)\n    flipped_q = q.flip(2)\n    assert flipped_q == Qubit(0, 1, 1, 1)\n", "def test_qubit_flip():\n    q = IntQubit(5, nqubits=3)  # Corresponds to |101>\n    flipped_q = q.flip(0)  # Flip the least significant qubit (rightmost)\n    assert flipped_q == IntQubit(4, nqubits=3)  # Corresponds to |100>\n\n    flipped_q = q.flip(1)  # Flip the middle qubit\n    assert flipped_q == IntQubit(7, nqubits=3)  # Corresponds to |111>\n\n    flipped_q = q.flip(2)  # Flip the most significant qubit (leftmost)\n    assert flipped_q == IntQubit(1, nqubits=3)  # Corresponds to |001>\n\n    # Flip multiple qubits\n    flipped_q = q.flip(0, 1)  # Flip the least significant and middle qubits\n    assert flipped_q == IntQubit(6, nqubits=3)  # Corresponds to |110>\n", "def test_qubit_creation():\n    q1 = Qubit(0, 1, 0)\n    assert q1.nqubits == 3\n    assert q1.qubit_values == (0, 1, 0)\n\n    q2 = Qubit('011')\n    assert q2.nqubits == 3\n    assert q2.qubit_values == (0, 1, 1)\n\n    q3 = Qubit(1, 0, 1, 1)\n    assert q3.nqubits == 4\n    assert q3.qubit_values == (1, 0, 1, 1)\n\n", "def test_qubit_properties():\n    q = Qubit(1, 0, 1)\n    assert q.dimension == 3\n    assert q.nqubits == 3\n    assert q.qubit_values == (1, 0, 1)\n    assert len(q) == 3\n    assert q[0] == 1\n    assert q[1] == 0\n    assert q[2] == 1\n    assert q.flip(1) == Qubit(1, 1, 1)\n    assert q.flip(0, 2) == Qubit(0, 0, 0)\n", "def test_qubit_flip():\n    qubit = IntQubit(5, nqubits=3)  # |101>\n    flipped_qubit = qubit.flip(1)  # Flip the second qubit from the right (index 1)\n    assert flipped_qubit == IntQubit(7, nqubits=3)  # |111>\n    flipped_qubit = qubit.flip(0, 2)  # Flip the first and third qubits from the right (indices 0 and 2)\n    assert flipped_qubit == IntQubit(2, nqubits=3)  # |010>\n"], "sample_1036": ["def test_unevaluated_Mul():\n    from sympy import S, sqrt, Mul\n    from sympy.abc import x\n\n    # Check the collection of Numbers and their multiplication in unevaluated Mul\n    a = Mul(*[S(3.0), x, S(2)], evaluate=False)\n    assert a.args[0] == S(6.0)\n    assert a.args[1] == x\n\n    # Ensure unevaluated Muls with same arguments compare equal\n    m = Mul(sqrt(2), sqrt(3), evaluate=False)\n    assert m == Mul(sqrt(3), sqrt(2), evaluate=False)\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m == Mul(u, evaluate=False)\n    assert m != Mul(*m.args)\n\n    # Check behavior with nested unevaluated Muls\n    nested_mul = Mul(Mul(2, 3, evaluate=False), 4, evaluate=False)\n    assert nested_mul.args == (6, 4)\n\n    # Check behavior with Number in the middle\n    mid_num_mul = Mul(x, 2, y, evaluate=False)\n    assert mid_num_mul.args[1] == 2\n\n    # Ensure non-commutative arguments are not sorted\n    from sympy import symbols\n    A, B = symbols('A B', commutative=False)\n    non_comm_mul = Mul(A, B, A, evaluate=False)\n    assert non_comm_mul.args == (A, B, A)\n", "def test_unevaluated_Mul():\n    from sympy.core.mul import _unevaluated_Mul as uMul\n    from sympy import S, sqrt, Mul\n    from sympy.abc import x, y\n\n    a = uMul(*[S(3.0), x, S(2)])\n    assert a.args[0] == S(6.0)\n    assert a.args[1] == x\n\n    m = uMul(sqrt(2), sqrt(3))\n    assert m == uMul(sqrt(3), sqrt(2))\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m == uMul(u)\n    assert m != Mul(*m.args)\n\n    # Check combining of numbers\n    b = uMul(*[S(2), S(3.0), x])\n    assert b.args[0] == S(6.0)\n    assert b.args[1] == x\n\n    # Check noncommutative multiplication\n    c = uMul(*[S(2), x, y, S(3)])\n    assert c.args[0] == S(6)\n    assert c.args[1] == x\n    assert c.args[2] == y\n", "def test_unevaluated_Mul():\n    from sympy.core.mul import _unevaluated_Mul as uMul\n    from sympy import S, sqrt\n    from sympy.abc import x, y\n\n    # Test numeric multiplication with unevaluated Mul\n    m = uMul(S(3.0), S(2), x)\n    assert m.args == (6, x)\n\n    # Test symbolic multiplication with unevaluated Mul\n    n = uMul(x, y, S(2))\n    assert n.args == (2, x, y)\n\n    # Test equivalency of unevaluated Muls\n    p = uMul(sqrt(2), sqrt(3))\n    q = uMul(sqrt(3), sqrt(2))\n    assert p == q\n\n    # Test unevaluated Mul with nested unevaluated Muls\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    v = uMul(u)\n    assert p == v\n    assert p != Mul(*p.args)\n\n    # Test unevaluated Mul with a single argument\n    single_arg = uMul(x)\n    assert single_arg.args == (x,)\n\n    # Test unevaluated Mul with no arguments\n    no_args = uMul()\n    assert no_args.args == ()\n", "def test_unevaluated_mul():\n    from sympy import sqrt, S\n    from sympy.abc import x\n\n    # test number collection and multiplication\n    a = Mul._from_args([S(3.0), x, S(2)])\n    assert a.args[0] == 6.0\n    assert a.args[1] == x\n\n    # test unevaluated Muls with same arguments compare equal\n    m1 = Mul._from_args([sqrt(2), sqrt(3)])\n    m2 = Mul._from_args([sqrt(3), sqrt(2)])\n    assert m1 == m2\n\n    # test unevaluated Mul with an evaluated Mul\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m1 == Mul._from_args([u])\n\n    # test unevaluated Mul does not compare equal to evaluated Mul\n    assert m1 != Mul(*m1.args)\n", "def test_as_real_imag():\n    from sympy import re, im\n    c, d = symbols('c d', real=True)\n    assert (2*I*c*d).as_real_imag() == (0, 2*c*d)\n    assert (2*I*c*d).as_real_imag(deep=False) == (0, 2*c*d)\n    assert (2*I*c*d).as_real_imag(deep=True) == (0, 2*c*d)\n    assert (2*I*c*d).as_real_imag(hints={'ignore': 2*I*c*d}) == None\n    assert re(2*I*c*d) == 0\n    assert im(2*I*c*d) == 2*c*d\n"], "sample_1037": ["def test_assumptions_refine():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = MatMul(A, A.T)\n    with assuming(Q.orthogonal(A)):\n        assert refine(expr) == Identity(2)\n    expr = MatMul(A, A.H)\n    with assuming(Q.unitary(A)):\n        assert refine(expr) == Identity(2)\n    expr = MatMul(A, B.T)\n    with assuming(Q.orthogonal(A), Q.orthogonal(B)):\n        assert refine(expr) == MatMul(A, B.T)\n", "def test_validate_shapes():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    C = MatrixSymbol('C', 4, 2)\n    D = MatrixSymbol('D', 3, 4)\n    \n    # Should pass without raising ShapeError\n    validate(A, B)\n    validate(B, D)\n    \n    # Should raise ShapeError\n    with raises(ShapeError):\n        validate(A, C)\n    with raises(ShapeError):\n        validate(B, C)\n    with raises(ShapeError):\n        validate(C, D)\n", "def test_MatMul_doit():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, k)\n    D = MatrixSymbol('D', k, p)\n\n    expr = MatMul(A, B, C, D)\n    assert expr.shape == (n, p)\n    assert expr.doit().shape == (n, p)\n\n    scalar = 2\n    expr = MatMul(scalar, A, B)\n    assert expr.doit() == scalar * (A * B)\n\n    I = Identity(n)\n    Z = ZeroMatrix(m, l)\n    expr = MatMul(A, Z, I)\n    assert expr.doit() == ZeroMatrix(n, l)\n\n    expr = MatMul(A, I, B)\n    assert expr.doit() == A * B\n\n    expr = MatMul(A, B, I)\n    assert expr.doit() == A * B\n\n    expr = MatMul(I, A, I)\n    assert expr.doit() == A\n\n    expr = MatMul(I, I, I)\n    assert expr.doit() == I\n\n    expr = MatMul(0, A, B)\n    assert expr.doit() == ZeroMatrix(n, l)\n\n    expr = MatMul(1, A, B)\n    assert expr.doit() == A * B\n\n    expr = MatMul(A, B, C)\n    assert expr.doit() == A * B * C\n\n    expr = MatMul(scalar, A, B, C)\n    assert expr.doit() == scalar * (A * B * C)\n\n    expr = MatMul(scalar, 0, A, B)\n    assert expr.doit() == ZeroMatrix(n, l)\n", "def test_matmul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    X = MatrixSymbol('X', 2, 2)\n    I = Identity(2)\n\n    assert MatMul(A, B).doit() == A*B\n    assert MatMul(A, I).doit() == A\n    assert MatMul(I, B).doit() == B\n    assert MatMul(A, I, B).doit() == A*B\n    assert MatMul(A, 2).doit() == 2*A\n    assert MatMul(2, A).doit() == 2*A\n    assert MatMul(A, 0).doit() == ZeroMatrix(2, 2)\n    assert MatMul(0, A).doit() == ZeroMatrix(2, 2)\n    assert MatMul(A, X.T).doit() == A*X.T\n    assert MatMul(X.T, A).doit() == X.T*A\n\n    C = MatrixSymbol('C', 2, 3)\n    D = MatrixSymbol('D', 3, 2)\n    assert MatMul(C, D).doit() == C*D\n    assert MatMul(D, C).doit() == D*C\n    assert MatMul(C, I).doit() == C\n    assert MatMul(I, D).doit() == D\n\n    # Test scalar multiplication with matrices\n    assert MatMul(2, C).doit() == 2*C\n    assert MatMul(C, 2).doit() == 2*C\n    assert MatMul(2, C, D).doit() == 2*(C*D)\n    assert MatMul(C, D, 2).doit() == 2*(C*D)\n    assert MatMul(0, C, D).doit() == ZeroMatrix(2, 2)\n    assert MatMul(C, D, 0).doit() == ZeroMatrix(2, 2)\n", "def test_combine_powers():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n\n    expr = A**2 * A**3 * B * C**2 * C**-1\n    simplified_expr = canonicalize(expr)\n    \n    assert simplified_expr == A**5 * B * C\n"], "sample_1038": ["def test_matrixexpr_properties():\n    mat_expr = MatrixExpr()\n    assert mat_expr.is_Matrix\n    assert mat_expr.is_MatrixExpr\n    assert mat_expr.is_Identity is None\n    assert not mat_expr.is_Inverse\n    assert not mat_expr.is_Transpose\n    assert not mat_expr.is_ZeroMatrix\n    assert not mat_expr.is_MatAdd\n    assert not mat_expr.is_MatMul\n    assert not mat_expr.is_commutative\n    assert not mat_expr.is_number\n    assert not mat_expr.is_symbol\n", "def test_MatrixExpr_neg_abs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n\n    neg_expr = -A\n    assert isinstance(neg_expr, MatMul)\n    assert neg_expr.args == (S.NegativeOne, A)\n\n    with raises(NotImplementedError):\n        abs(A)\n", "def test_matrix_symbols():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = A + B\n    assert matrix_symbols(C) == [A, B]\n    D = A * B\n    assert matrix_symbols(D) == [A, B]\n    E = C * D\n    assert matrix_symbols(E) == [A, B]\n    F = A[0, 0] + B[1, 1]\n    assert matrix_symbols(F) == [A, B]\n    G = MatrixSymbol('G', 3, 3)\n    H = G + A * B\n    assert matrix_symbols(H) == [G, A, B]\n", "def test_MatrixElement_equality():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert A[0, 0] == A[0, 0]\n    assert A[0, 0] != A[1, 0]\n    assert A[0, 0] != B[0, 0]\n", "def test_as_explicit():\n    A = MatrixSymbol('A', 2, 2)\n    M = A.as_explicit()\n    assert M.shape == (2, 2)\n    assert isinstance(M, ImmutableMatrix)\n    assert M[0, 0] == A[0, 0]\n    assert M[0, 1] == A[0, 1]\n    assert M[1, 0] == A[1, 0]\n    assert M[1, 1] == A[1, 1]\n\n    B = MatrixSymbol('B', 3, 2)\n    N = B.as_explicit()\n    assert N.shape == (3, 2)\n    assert isinstance(N, ImmutableMatrix)\n    assert N[0, 0] == B[0, 0]\n    assert N[0, 1] == B[0, 1]\n    assert N[1, 0] == B[1, 0]\n    assert N[1, 1] == B[1, 1]\n    assert N[2, 0] == B[2, 0]\n    assert N[2, 1] == B[2, 1]\n"], "sample_1039": ["def test_apply_restore_patch():\n    # Create a MathMLContentPrinter instance\n    printer = MathMLContentPrinter()\n\n    # Create a simple MathML expression\n    expr = printer._print(1 + x)\n\n    # Check initial state of top-level element\n    assert expr.nodeName == 'apply'\n    assert expr.childNodes[0].nodeName == 'plus'\n\n    # Apply patch and check\n    printer.apply_patch()\n    pretty_xml = expr.toprettyxml()\n    assert '<apply>' in pretty_xml\n    assert '<plus/>' in pretty_xml\n\n    # Restore patch and check\n    printer.restore_patch()\n    pretty_xml_restored = expr.toprettyxml()\n    assert pretty_xml == pretty_xml_restored\n\n    # Ensure that applying and restoring patch does not change output\n    assert mathml(1 + x) == printer.doprint(1 + x)\n", "def test_apply_patch_restore_patch():\n    m = MathMLContentPrinter()\n    m.apply_patch()\n    from xml.dom.minidom import Element, Text\n    assert Element.writexml.__code__ == m._Element_writexml_old.__code__\n    assert Text.writexml.__code__ == m._Text_writexml_old.__code__\n\n    m.restore_patch()\n    assert Element.writexml.__code__ != m._Element_writexml_old.__code__\n    assert Text.writexml.__code__ != m._Text_writexml_old.__code__\n", "def test_mathml_apply_patch_and_restore_patch():\n    # Test the apply_patch method\n    mpp.apply_patch()\n    \n    # Check if the patch was applied by comparing the patched method\n    from xml.dom.minidom import Element, Text\n    assert Element.writexml.__code__ == mpp._Element_writexml_old.__code__\n    assert Text.writexml.__code__ == mpp._Text_writexml_old.__code__\n\n    # Test the restore_patch method\n    mpp.restore_patch()\n    \n    # Check if the patch was restored by comparing the restored method\n    assert Element.writexml.__code__ != mpp._Element_writexml_old.__code__\n    assert Text.writexml.__code__ != mpp._Text_writexml_old.__code__\n", "def test_mathml_function_inverse_trig():\n    mml_asin = mp._print(asin(x))\n    assert mml_asin.childNodes[0].nodeName == 'arcsin'\n    assert mml_asin.childNodes[1].nodeName == 'ci'\n    assert mml_asin.childNodes[1].childNodes[0].nodeValue == 'x'\n    \n    mml_acos = mp._print(acos(x))\n    assert mml_acos.childNodes[0].nodeName == 'arccos'\n    assert mml_acos.childNodes[1].nodeName == 'ci'\n    assert mml_acos.childNodes[1].childNodes[0].nodeValue == 'x'\n    \n    mml_atan = mp._print(atan(x))\n    assert mml_atan.childNodes[0].nodeName == 'arctan'\n    assert mml_atan.childNodes[1].nodeName == 'ci'\n    assert mml_atan.childNodes[1].childNodes[0].nodeValue == 'x'\n    \n    mml_acot = mp._print(Symbol('acot')(x))\n    assert mml_acot.childNodes[0].nodeName == 'arccot'\n    assert mml_acot.childNodes[1].nodeName == 'ci'\n    assert mml_acot.childNodes[1].childNodes[0].nodeValue == 'x'\n    \n    mml_asinh = mp._print(asinh(x))\n    assert mml_asinh.childNodes[0].nodeName == 'arcsinh'\n    assert mml_asinh.childNodes[1].nodeName == 'ci'\n    assert mml_asinh.childNodes[1].childNodes[0].nodeValue == 'x'\n    \n    mml_acosh = mp._print(acosh(x))\n    assert mml_acosh.childNodes[0].nodeName == 'arccosh'\n    assert mml_acosh.childNodes[1].nodeName == 'ci'\n    assert mml_acosh.childNodes[1].childNodes[0].nodeValue == 'x'\n    \n    mml_atanh = mp._print(atanh(x))\n    assert mml_atanh.childNodes[0].nodeName == 'arctanh'\n    assert mml_atanh.childNodes[1].nodeName == 'ci'\n    assert mml_atanh.childNodes[1].childNodes[0", "def test_content_mathml_subs():\n    expr = x - y\n    mp_with_subs = MathMLContentPrinter({'symbol_names': {x: 'X', y: 'Y'}})\n    mml = mp_with_subs._print(expr)\n    assert mml.childNodes[0].nodeName == 'minus'\n    assert mml.childNodes[1].childNodes[0].nodeValue == 'X'\n    assert mml.childNodes[2].childNodes[0].nodeValue == 'Y'\n"], "sample_1040": ["def test_mathml_apply_patch_restore_patch():\n    # Verify that apply_patch and restore_patch methods work correctly\n\n    # Create a new MathMLContentPrinter instance\n    printer = MathMLContentPrinter()\n\n    # Apply the patch\n    printer.apply_patch()\n\n    # Test the patched xml.dom.minidom behavior\n    doc1 = printer.dom.createElement(\"test\")\n    doc1.appendChild(printer.dom.createTextNode(\"Test Content\"))\n    pretty_xml = doc1.toprettyxml()\n    assert pretty_xml == '<test>\\nTest Content\\n</test>\\n'\n\n    # Restore the original behavior\n    printer.restore_patch()\n\n    # Verify that the original behavior is restored\n    import xml.dom.minidom\n    doc2 = xml.dom.minidom.Document()\n    element = doc2.createElement(\"test\")\n    element.appendChild(doc2.createTextNode(\"Test Content\"))\n    doc2.appendChild(element)\n    pretty_xml_original = doc2.toprettyxml()\n    assert pretty_xml_original == '<test>Test Content</test>\\n'\n", "def test_presentation_mathml_negative_infinity():\n    mml = mpp._print(-oo)\n    assert mml.nodeName == 'mrow'\n    assert mml.childNodes[0].nodeName == 'mo'\n    assert mml.childNodes[0].childNodes[0].nodeValue == '-'\n    assert mml.childNodes[1].nodeName == 'mi'\n    assert mml.childNodes[1].childNodes[0].nodeValue == '&#x221E;'\n", "def test_content_mathml_exponential():\n    mml = mp._print(E**x)\n    assert mml.nodeName == 'apply'\n    assert mml.childNodes[0].nodeName == 'power'\n    assert mml.childNodes[1].nodeName == 'exponentiale'\n    assert mml.childNodes[2].nodeName == 'ci'\n    assert mml.childNodes[2].childNodes[0].nodeValue == 'x'\n\n    mml = mp._print(E**(x+y))\n    assert mml.nodeName == 'apply'\n    assert mml.childNodes[0].nodeName == 'power'\n    assert mml.childNodes[1].nodeName == 'exponentiale'\n    assert mml.childNodes[2].nodeName == 'apply'\n    assert mml.childNodes[2].childNodes[0].nodeName == 'plus'\n    assert mml.childNodes[2].childNodes[1].nodeName == 'ci'\n    assert mml.childNodes[2].childNodes[1].childNodes[0].nodeValue == 'x'\n    assert mml.childNodes[2].childNodes[2].nodeName == 'ci'\n    assert mml.childNodes[2].childNodes[2].childNodes[0].nodeValue == 'y'\n", "def test_presentation_mathml_subscript_superscript():\n    # Testing subscript and superscript in presentation MathML\n    mml = mpp._print(Symbol(\"x_2^3\"))\n    assert mml.nodeName == 'msubsup'\n    assert mml.childNodes[0].nodeName == 'mi'\n    assert mml.childNodes[0].childNodes[0].nodeValue == 'x'\n    assert mml.childNodes[1].nodeName == 'mn'\n    assert mml.childNodes[1].childNodes[0].nodeValue == '2'\n    assert mml.childNodes[2].nodeName == 'mn'\n    assert mml.childNodes[2].childNodes[0].nodeValue == '3'\n\n    mml = mpp._print(Symbol(\"x__2^3\"))\n    assert mml.nodeName == 'msup'\n    assert mml.childNodes[0].nodeName == 'msub'\n    assert mml.childNodes[0].childNodes[0].nodeName == 'mi'\n    assert mml.childNodes[0].childNodes[0].childNodes[0].nodeValue == 'x'\n    assert mml.childNodes[0].childNodes[1].nodeName == 'mn'\n    assert mml.childNodes[0].childNodes[1].childNodes[0].nodeValue == '2'\n    assert mml.childNodes[1].nodeName == 'mn'\n    assert mml.childNodes[1].childNodes[0].nodeValue == '3'\n", "def test_print_unevaluated_integral():\n    integrand = x\n    unevaluated_integral = Integral(integrand, (x, 0, 1), evaluate=False)\n    mml = mp._print(unevaluated_integral)\n    assert mml.nodeName == 'apply'\n    assert mml.childNodes[0].nodeName == 'int'\n    assert mml.childNodes[1].nodeName == 'bvar'\n    assert mml.childNodes[2].nodeName == 'lowlimit'\n    assert mml.childNodes[3].nodeName == 'uplimit'\n    assert mml.childNodes[4].toxml() == mp._print(integrand).toxml()\n\n    mml_presentation = mpp._print(unevaluated_integral)\n    assert mml_presentation.nodeName == 'mrow'\n    assert mml_presentation.childNodes[0].nodeName == 'msubsup'\n    assert mml_presentation.childNodes[0].childNodes[0].childNodes[0].nodeValue == '&int;'\n    assert mml_presentation.childNodes[0].childNodes[1].childNodes[0].nodeValue == '0'\n    assert mml_presentation.childNodes[0].childNodes[2].childNodes[0].nodeValue == '1'\n    assert mml_presentation.childNodes[1].childNodes[0].nodeValue == 'x'\n"], "sample_1041": ["def test_MatrixExpr_as_explicit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n\n    expr = A + B * C\n    explicit_expr = expr.as_explicit()\n\n    assert explicit_expr.shape == (2, 2)\n    assert explicit_expr[0, 0] == A[0, 0] + B[0, 0] * C[0, 0] + B[0, 1] * C[1, 0]\n    assert explicit_expr[0, 1] == A[0, 1] + B[0, 0] * C[0, 1] + B[0, 1] * C[1, 1]\n    assert explicit_expr[1, 0] == A[1, 0] + B[1, 0] * C[0, 0] + B[1, 1] * C[1, 0]\n    assert explicit_expr[1, 1] == A[1, 1] + B[1, 0] * C[0, 1] + B[1, 1] * C[1, 1]\n", "def test_matrixexpr_neg():\n    A = MatrixSymbol('A', n, m)\n    neg_A = -A\n    assert isinstance(neg_A, MatMul)\n    assert neg_A.args[0] == S.NegativeOne\n    assert neg_A.args[1] == A\n\n    B = MatrixSymbol('B', m, l)\n    neg_AB = -(A * B)\n    assert isinstance(neg_AB, MatMul)\n    assert neg_AB.args[0] == S.NegativeOne\n    assert neg_AB.args[1] == A * B\n", "def test_as_explicit():\n    A = MatrixSymbol('A', 2, 2)\n    explicit_A = A.as_explicit()\n    \n    assert explicit_A[0, 0] == A[0, 0]\n    assert explicit_A[0, 1] == A[0, 1]\n    assert explicit_A[1, 0] == A[1, 0]\n    assert explicit_A[1, 1] == A[1, 1]\n    \n    B = MatrixSymbol('B', 3, 3)\n    explicit_B = B.as_explicit()\n    \n    assert explicit_B[0, 0] == B[0, 0]\n    assert explicit_B[0, 1] == B[0, 1]\n    assert explicit_B[0, 2] == B[0, 2]\n    assert explicit_B[1, 0] == B[1, 0]\n    assert explicit_B[1, 1] == B[1, 1]\n    assert explicit_B[1, 2] == B[1, 2]\n    assert explicit_B[2, 0] == B[2, 0]\n    assert explicit_B[2, 1] == B[2, 1]\n    assert explicit_B[2, 2] == B[2, 2]\n", "def test_matrix_symbols():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 3, 3)\n    expr1 = A + B[0, 0] * Identity(2)\n    assert matrix_symbols(expr1) == [A, B]\n    \n    expr2 = A * B[1, 1]\n    assert matrix_symbols(expr2) == [A, B]\n    \n    expr3 = A * 2 + B[2, 2] * Identity(2)\n    assert matrix_symbols(expr3) == [A, B]\n    \n    expr4 = 3 * A\n    assert matrix_symbols(expr4) == [A]\n", "def test_matrix_expr_properties():\n    # Testing the properties and some methods of MatrixExpr class\n    X = MatrixSymbol('X', 2, 3)\n    \n    # Test properties\n    assert X.is_Matrix == True\n    assert X.is_MatrixExpr == True\n    assert X.is_Identity == None\n    assert X.is_Inverse == False\n    assert X.is_Transpose == False\n    assert X.is_ZeroMatrix == False\n    assert X.is_MatAdd == False\n    assert X.is_MatMul == False\n    assert X.is_commutative == False\n    assert X.is_number == False\n    assert X.is_symbol == False\n    \n    # Test transpose method\n    assert isinstance(X.transpose(), Transpose)\n    assert X.T == X.transpose()\n    \n    # Test adjoint method\n    assert X.adjoint() == adjoint(X)\n    \n    # Test inverse method\n    assert X.inverse() == Inverse(X)\n    \n    # Test as_explicit method\n    assert X.as_explicit() == ImmutableMatrix([[X[0, 0], X[0, 1], X[0, 2]], [X[1, 0], X[1, 1], X[1, 2]]])\n    \n    # Test as_mutable method\n    assert X.as_mutable() == Matrix([[X[0, 0], X[0, 1], X[0, 2]], [X[1, 0], X[1, 1], X[1, 2]]])\n\n    # Test as_coeff_Mul method\n    assert X.as_coeff_Mul() == (S.One, X)\n    \n    # Test valid_index method\n    assert X.valid_index(1, 2) == True\n    assert X.valid_index(3, 0) == False\n    assert X.valid_index(1, n) == True\n\n    # Test _eval_derivative with respect to MatrixElement\n    Y = MatrixSymbol('Y', 2, 2)\n    assert Y[0, 0].diff(Y[0, 0]) == 1\n    assert Y[0, 0].diff(Y[0, 1]) == 0\n"], "sample_1042": ["def test_IndexedBase_strides():\n    i, j = symbols('i j', integer=True)\n    m, n, o = symbols('m n o', integer=True)\n    A = IndexedBase('A', strides=(m, n, o))\n    assert A.strides == Tuple(m, n, o)\n    assert A.strides[0] == m\n    assert A.strides[1] == n\n    assert A.strides[2] == o\n\n    # Test with different strides\n    B = IndexedBase('B', strides=(2*m, 3*n, 4*o))\n    assert B.strides == Tuple(2*m, 3*n, 4*o)\n\n    # Test error on rank mismatch\n    raises(IndexException, lambda: A[i])\n    raises(IndexException, lambda: A[i, j])\n    raises(IndexException, lambda: B[i, j])\n    \n    # Test with predefined strides 'C' and 'F'\n    C = IndexedBase('C', strides='C')\n    assert C.strides == 'C'\n    D = IndexedBase('D', strides='F')\n    assert D.strides == 'F'\n", "def test_IndexedBase_free_symbols():\n    i, j = symbols('i j', integer=True)\n    a, b, c = symbols('a b c')\n    A = IndexedBase(a)\n    B = IndexedBase(b)\n    C = IndexedBase(c)\n    \n    assert A[i].free_symbols == {a, i}\n    assert B[j].free_symbols == {b, j}\n    assert C[i, j].free_symbols == {c, i, j}\n    assert A[i].free_symbols == {a, i}\n    assert B[j].subs(b, a).free_symbols == {a, j}\n    assert C[i, j].subs(c, a).free_symbols == {a, i, j}\n    assert A[i].subs(i, j).free_symbols == {a, j}\n    assert C[i, j].subs({i: j, j: i}).free_symbols == {c, i, j}\n", "def test_IndexedBase_offset_and_strides():\n    i, j, k = symbols('i j k', integer=True)\n    l, m, n, o = symbols('l m n o', integer=True)\n\n    A = IndexedBase('A', strides=(l, m, n), offset=o)\n    assert A.strides == (l, m, n)\n    assert A.offset == o\n\n    expr = A[i, j, k]\n    assert expr.base.strides == (l, m, n)\n    assert expr.base.offset == o\n    assert expr._sympystr(lambda x: str(x)) == 'A[i, j, k]'\n\n    expr = expr.subs(i, 2)\n    assert expr == A[2, j, k]\n    assert expr._sympystr(lambda x: str(x)) == 'A[2, j, k]'\n    \n    expr = expr.subs(j, 1)\n    assert expr == A[2, 1, k]\n    assert expr._sympystr(lambda x: str(x)) == 'A[2, 1, k]'\n    \n    expr = expr.subs(k, 0)\n    assert expr == A[2, 1, 0]\n    assert expr._sympystr(lambda x: str(x)) == 'A[2, 1, 0]'\n", "def test_IndexedBase_offset_and_strides():\n    i, j, k = symbols('i j k', integer=True)\n    a, b = symbols('a b')\n    \n    # Test offset\n    A = IndexedBase(a, offset=10)\n    assert A.offset == 10\n    assert A[i, j].offset == 10\n    \n    B = IndexedBase(b, offset=15)\n    assert B.offset == 15\n    assert B[i, j].offset == 15\n    \n    # Test strides\n    C = IndexedBase(a, strides=(2, 3))\n    assert C.strides == (2, 3)\n    assert C[i, j].strides == (2, 3)\n    \n    D = IndexedBase(b, strides=(4, 5))\n    assert D.strides == (4, 5)\n    assert D[i, j].strides == (4, 5)\n    \n    # Test strides with C and F orders\n    E = IndexedBase(a, strides='C')\n    assert E.strides == 'C'\n    assert E[i, j].strides == 'C'\n    \n    F = IndexedBase(b, strides='F')\n    assert F.strides == 'F'\n    assert F[i, j].strides == 'F'\n    \n    # Test error cases for offset and strides\n    raises(TypeError, lambda: IndexedBase(a, strides=1.5))\n    raises(TypeError, lambda: IndexedBase(a, offset='invalid'))\n", "def test_IndexedBase_strides_and_offset():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a', strides=(2, 3), offset=1)\n    assert a.strides == (2, 3)\n    assert a.offset == 1\n    assert a.shape is None\n\n    b = IndexedBase('b', shape=(4, 5), strides=(2, 3), offset=1)\n    assert b.shape == Tuple(4, 5)\n    assert b.strides == (2, 3)\n    assert b.offset == 1\n\n    c = IndexedBase('c', strides='C', offset=0)\n    assert c.strides == 'C'\n    assert c.offset == 0\n\n    d = IndexedBase('d', strides='F', offset=0)\n    assert d.strides == 'F'\n    assert d.offset == 0\n\n    raises(TypeError, lambda: IndexedBase('e', strides=(2.5, 3)))\n    raises(TypeError, lambda: IndexedBase('f', offset=1.5))\n\n    A = b[i, j]\n    assert A.base.strides == (2, 3)\n    assert A.base.offset == 1\n"], "sample_1043": ["def test_custom_functions():\n    custom_functions = {'myfunc': 'MyCustomFunc'}\n    assert mcode(Function('myfunc')(x, y), user_functions=custom_functions) == \"MyCustomFunc[x, y]\"\n    custom_functions = {'myfunc': [(lambda x: True, 'MyCustomFunc')]}\n    assert mcode(Function('myfunc')(x), user_functions=custom_functions) == \"MyCustomFunc[x]\"\n", "def test_user_defined_function():\n    user_functions = {'custom_func': 'CustomFunc'}\n    expr = Function('custom_func')(x, y)\n    assert mcode(expr, user_functions=user_functions) == \"CustomFunc[x, y]\"\n", "def test_known_functions():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(tan(x)) == \"Tan[x]\"\n    assert mcode(cot(x)) == \"Cot[x]\"\n    assert mcode(asin(x)) == \"ArcSin[x]\"\n    assert mcode(acos(x)) == \"ArcCos[x]\"\n    assert mcode(atan(x)) == \"ArcTan[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(sech(x)) == \"Sech[x]\"\n    assert mcode(csch(x)) == \"Csch[x]\"\n    assert mcode(asinh(x)) == \"ArcSinh[x]\"\n    assert mcode(acosh(x)) == \"ArcCosh[x]\"\n    assert mcode(atanh(x)) == \"ArcTanh[x]\"\n    assert mcode(acoth(x)) == \"ArcCoth[x]\"\n    assert mcode(asech(x)) == \"ArcSech[x]\"\n    assert mcode(acsch(x)) == \"ArcCsch[x]\"\n    assert mcode(conjugate(x)) == \"Conjugate[x]\"\n    assert mcode(Max(x, y, z)) == \"Max[x, y, z]\"\n    assert mcode(Min(x, y, z)) == \"Min[x, y, z]\"\n", "def test_user_defined_function():\n    custom_functions = {\"custom_func\": \"CustomFunc\"}\n    assert mcode(Function('custom_func')(x, y, z), user_functions=custom_functions) == \"CustomFunc[x, y, z]\"\n    assert mcode(Function('custom_func')(sin(x), cos(y)), user_functions=custom_functions) == \"CustomFunc[Sin[x], Cos[y]]\"\n", "def test_known_functions():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(tan(x)) == \"Tan[x]\"\n    assert mcode(cot(x)) == \"Cot[x]\"\n    assert mcode(asin(x)) == \"ArcSin[x]\"\n    assert mcode(acos(x)) == \"ArcCos[x]\"\n    assert mcode(atan(x)) == \"ArcTan[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(sech(x)) == \"Sech[x]\"\n    assert mcode(csch(x)) == \"Csch[x]\"\n    assert mcode(asinh(x)) == \"ArcSinh[x]\"\n    assert mcode(acosh(x)) == \"ArcCosh[x]\"\n    assert mcode(atanh(x)) == \"ArcTanh[x]\"\n    assert mcode(acoth(x)) == \"ArcCoth[x]\"\n    assert mcode(asech(x)) == \"ArcSech[x]\"\n    assert mcode(acsch(x)) == \"ArcCsch[x]\"\n"], "sample_1044": ["def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(15) == 3\n    assert isqrt(17984395633462800708566937239551) == 134052434444\n    assert isqrt(17984395633462800708566937239552) == 134052434445\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(10) == 3\n    assert isqrt(17984395633462800708566937239551) == int(_sqrt(17984395633462800708566937239551))\n    assert isqrt(17984395633462800708566937239552) == 134217728\n", "def test_isqrt():\n    assert isqrt(16) == 4\n    assert isqrt(15) == 3\n    assert isqrt(17984395633462800708566937239551) == int(17984395633462800708566937239551**0.5)\n    assert isqrt(17984395633462800708566937239552) == integer_nthroot(17984395633462800708566937239552, 2)[0]\n    raises(ValueError, lambda: isqrt(-1))\n", "def test_Pow_is_finite():\n    assert Pow(S(2), S.Infinity).is_finite is False\n    assert Pow(S(2), -S.Infinity).is_finite is True\n    assert Pow(S(0), S(-2)).is_finite is False\n    assert Pow(S(2), S(2)).is_finite is True\n    assert Pow(S(2), S(-2)).is_finite is True\n    assert Pow(S(0), S(0)).is_finite is True\n    assert Pow(S(1), S.Infinity).is_finite is False\n    assert Pow(S(1), S(-Infinity)).is_finite is False\n", "def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(0, 5) == (0, True)\n    assert integer_nthroot(1, 1) == (1, True)\n    assert integer_nthroot(1, 2) == (1, True)\n    assert integer_nthroot(1, 3) == (1, True)\n    assert integer_nthroot(8, 3) == (2, True)\n    assert integer_nthroot(9, 2) == (3, True)\n    assert integer_nthroot(10, 2) == (3, False)\n    assert integer_nthroot(1024, 10) == (2, False)\n    raises(ValueError, lambda: integer_nthroot(-1, 2))\n    raises(ValueError, lambda: integer_nthroot(1, 0))\n"], "sample_1045": ["def test_mpf_norm_special_cases():\n    # Test special cases for mpf_norm function\n    assert mpf_norm((1, 0, 1, 0), 10) == _mpf_zero  # zero case\n    assert mpf_norm((0, 1, 0, 0), 10) == (0, 1, 0, 1)  # normalization check\n\n    # Check for normalizing infinities and NaNs\n    assert mpf_norm(_mpf_inf, 10) == _mpf_inf\n    assert mpf_norm(_mpf_ninf, 10) == _mpf_ninf\n    assert mpf_norm(_mpf_nan, 10) == _mpf_nan\n\n    # Test normalization with precision changes\n    assert mpf_norm((0, 123456789, -30, 27), 20) == (0, 123456789, -30, 27)\n    assert mpf_norm((0, 123456789, -30, 27), 30) == (0, 123456789, -30, 27)\n\n    # Check for a case where mantissa is not zero and needs adjustment\n    assert mpf_norm((1, 1000000, -3, 20), 10) == (1, 125000, -1, 20)\n", "def test_mpf_norm_zero_handling():\n    # Test handling of zero mantissa in mpf_norm function\n    assert mpf_norm((1, 0, 0, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((1, 0, 0, 1), 10) == (0, 0, 0, 0)\n    assert mpf_norm((1, 0, 0, 100), 10) == (0, 0, 0, 0)\n", "def test_mpf_norm_special_cases():\n    # Test for special cases in mpf_norm function\n    # +inf, -inf, nan, and zero are special cases handled in the code\n    from mpmath.libmp.libmpf import finf, fninf, fnan, fzero\n\n    assert mpf_norm(finf, 10) == finf\n    assert mpf_norm(fninf, 10) == fninf\n    assert mpf_norm(fnan, 10) == fnan\n    assert mpf_norm(fzero, 10) == fzero\n", "def test_issue_9999():\n    assert comp(sqrt(2).n(5), 1.4142) is False\n    assert comp(sqrt(2).n(5), 1.4143) is False\n    assert comp(sqrt(2).n(5), 1.41421)\n    assert comp(sqrt(2).n(5), 1.41422)\n    assert comp(sqrt(2).n(5), 1.41423) is False\n    assert comp(sqrt(2).n(5), 1.41421, tol=0)\n    assert comp(sqrt(2).n(5), 1.41423, tol=0.0001)\n    assert comp(sqrt(2).n(5), 1.41423, tol=0.00001) is False\n    assert comp(sqrt(2).n(2), '1.4') is False\n    assert comp(sqrt(2).n(3), '1.41') is False\n    assert comp(sqrt(2).n(4), '1.414') is False\n    assert comp(sqrt(2).n(5), '1.4142') is False\n    assert comp(sqrt(2).n(5), '1.41421') is False\n    assert comp(sqrt(2).n(5), '1.41422')\n    assert comp(sqrt(2).n(5), '1.41423') is False\n    assert comp(sqrt(2).n(5), Float('1.41421', 6), '') is False\n    raises(ValueError, lambda: comp(sqrt(2).n(5), 1.41421, ''))\n    assert comp(sqrt(2).n(5), Float('1.41422', 6), '')\n    assert comp(sqrt(2).n(5), Float('1.41422', 5), '') is False\n    assert comp(sqrt(2).n(5), Float('1.41422', 6), tol='') is False\n    assert comp(sqrt(2).n(5), Float('1.41421', 6), tol='') is False\n    assert comp(sqrt(2).n(5), Float('1.41422', 5), tol='') is False\n", "def test_mpf_norm():\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)  # check zero normalization\n    assert mpf_norm((0, 1, -1, 1), 10) == (0, 1, -1, 1)  # check normal non-zero\n    assert mpf_norm((1, 0, 0, 0), 10) == _mpf_zero  # check negative zero to zero\n    assert mpf_norm((0, -1, 1, 1), 10) == (0, -1, 1, 1)  # check normal negative number\n    assert mpf_norm((0, 1, 100, 1), 10) == (0, 1, 100, 1)  # check large exponent\n    assert mpf_norm((0, 1, -100, 1), 10) == (0, 1, -100, 1)  # check small exponent\n    assert mpf_norm((1, 123456789, -123456789, 10), 10) == (1, 123456789, -123456789, 10)  # check large mantissa\n    assert mpf_norm((1, 123456789, 123456789, 10), 10) == (1, 123456789, 123456789, 10)  # check positive large exponent\n    assert mpf_norm((0, 123456789, -123456789, 10), 10) == (0, 123456789, -123456789, 10)  # check negative large exponent\n    assert mpf_norm(_mpf_nan, 10) == _mpf_nan  # check NaN normalization\n    assert mpf_norm(_mpf_inf, 10) == _mpf_inf  # check positive infinity normalization\n    assert mpf_norm(_mpf_ninf, 10) == _mpf_ninf  # check negative infinity normalization\n"], "sample_1046": ["def test_tensor_data_lazy_evaluator():\n    from sympy.tensor.array import Array\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    a, b, c, d = tensor_indices('a b c d', Lorentz)\n    p = tensorhead('p', [Lorentz], [[1]])\n    q = tensorhead('q', [Lorentz], [[1]])\n    g = Lorentz.metric\n\n    _tensor_data_substitution_dict[p(a)] = [1, 2, 3, 4]\n    _tensor_data_substitution_dict[q(a)] = [1, 2, 3, 4]\n    _tensor_data_substitution_dict[g(a, b)] = [[1, 0, 0, 0],\n                                               [0, -1, 0, 0],\n                                               [0, 0, -1, 0],\n                                               [0, 0, 0, -1]]\n    _tensor_data_substitution_dict[g(-a, -b)] = [[1, 0, 0, 0],\n                                                 [0, -1, 0, 0],\n                                                 [0, 0, -1, 0],\n                                                 [0, 0, 0, -1]]\n\n    expr = p(a) * q(-a)\n    assert expr.data == 1*1 + 2*2 + 3*3 + 4*4\n\n    expr = p(a) * p(-a)\n    assert expr.data == 1*1 + 2*2 + 3*3 + 4*4\n\n    expr = g(a, b) * p(-b)\n    assert expr.data.tolist() == [1, -2, -3, -4]\n\n    expr = g(a, -b) * p(b)\n    assert expr.data.tolist() == [1, 2, 3, 4]\n\n    expr = g(a, b) * g(-a, -b)\n    assert expr.data == 4\n\n    # cleanup\n    del _tensor_data_substitution_dict[p(a)]\n    del _tensor_data_substitution_dict[q(a)]\n    del _tensor_data_substitution_dict[g(a, b)]\n    del _tensor_data_substitution_dict[g(-a, -b)]\n", "def test_tensor_data_lazy_evaluator():\n    from sympy import Symbol, Matrix\n    from sympy.tensor.tensor import TensorIndexType, tensor_indices, tensorhead, _TensorDataLazyEvaluator\n\n    L = TensorIndexType('L', dim=2)\n    i, j, k = tensor_indices('i, j, k', L)\n\n    # Create tensor objects\n    A = tensorhead('A', [L], [[1]])\n    B = tensorhead('B', [L], [[1]])\n\n    # Assign data to the tensors\n    A.data = [1, 2]\n    B.data = [3, 4]\n\n    # Access the data using _TensorDataLazyEvaluator\n    evaluator = _TensorDataLazyEvaluator()\n    assert evaluator[A(i)] == A.data\n    assert evaluator[B(i)] == B.data\n\n    # Ensure that data is correctly accessed and transformed\n    expr = A(i) * B(-i)\n    assert expr.data == 1*4 + 2*3\n\n    # Test for metric tensor data assignment and retrieval\n    metric = Matrix([[1, 0], [0, -1]])\n    L.data = metric\n    assert L.metric.data == metric\n    assert evaluator[L.metric(i, -j)] == metric\n\n    # Test for metric tensor inverse and transpose\n    inverse_transpose = _TensorDataLazyEvaluator.inverse_transpose_matrix(metric)\n    assert inverse_transpose == Matrix([[1, 0], [0, -1]])\n    assert evaluator[L.metric(-i, j)] == inverse_transpose\n\n    # Test for exception handling when mixing tensors with and without data\n    C = tensorhead('C', [L], [[1]])\n    expr = A(i) + C(i)\n    raises(ValueError, lambda: expr.data)\n\n    # Test for metric handling in _TensorDataLazyEvaluator\n    g = L.metric\n    expr = A(i) * g(i, j)\n    assert expr.data == [1, -2]\n", "def test_replace_dummy_indices():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    a, b, c, d = tensor_indices('a,b,c,d', Lorentz)\n    A = tensorhead('A', [Lorentz]*2, [[1]*2])\n    B = tensorhead('B', [Lorentz]*2, [[1]*2])\n    p = tensorhead('p', [Lorentz], [[1]])\n\n    expr = A(a, b)*B(-b, c)\n    # Replacing dummy index\n    expr_replaced = expr.substitute_indices((b, d))\n    assert expr_replaced == A(a, d)*B(-d, c)\n\n    expr = A(a, b)*B(-b, c) + p(a)\n    expr_replaced = expr.substitute_indices((b, d))\n    assert expr_replaced == A(a, d)*B(-d, c) + p(a)\n    \n    expr = A(a, b)*B(-b, c)*p(-a)\n    expr_replaced = expr.substitute_indices((a, c), (b, d))\n    assert expr_replaced == A(c, d)*B(-d, a)*p(-c)\n\n    # Test replacement of indices in TensAdd:\n    expr = A(a, b)*B(-b, c) + A(c, d)*B(-d, a)\n    expr_replaced = expr.substitute_indices((a, c), (b, d))\n    assert expr_replaced == A(c, d)*B(-d, c) + A(c, d)*B(-d, c)\n\n    # Test replacement of indices in Tensor:\n    expr = A(a, b)\n    expr_replaced = expr.substitute_indices((a, d))\n    assert expr_replaced == A(d, b)\n\n    # Test replacement of indices in TensMul:\n    expr = A(a, b) * B(-b, c)\n    expr_replaced = expr.substitute_indices((a, d))\n    assert expr_replaced == A(d, b) * B(-b, c)\n", "def test_tensor_element():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    i, j, k, l = tensor_indices('i j k l', Lorentz)\n    A, B, C = tensorhead('A B C', [Lorentz]*2, [[1, 1]])\n\n    A.data = [[1, 2], [3, 4]]\n    B.data = [[5, 6], [7, 8]]\n\n    # Creating a TensorElement and checking its properties\n    elem = TensorElement(A(i, j), {i: 0})\n    assert elem.get_free_indices() == [j]\n    assert elem.data == [1, 2]\n\n    # TensorElement in a larger expression\n    expr = elem * B(j, k)\n    expected = [5, 6, 10, 12]\n    assert expr.replace_with_arrays({B(j, k): [[5, 6], [7, 8]], Lorentz: eye(2)}, [j, k]).tolist() == expected\n\n    # Testing replacement with a different index map\n    elem2 = TensorElement(A(i, j), {i: 1})\n    assert elem2.get_free_indices() == [j]\n    assert elem2.data == [3, 4]\n\n    # Summation with TensorElement\n    sum_expr = elem + elem2\n    assert sum_expr.data == [4, 6]\n\n    # TensorElement with multiple index replacements\n    elem3 = TensorElement(A(i, j) * B(j, k), {i: 0})\n    expected_mult = [[5, 6], [14, 16]]\n    assert elem3.replace_with_arrays({B(j, k): [[5, 6], [7, 8]], Lorentz: eye(2)}, [j, k]).tolist() == expected_mult\n\n    # Ensure TensorElement can handle nested replacements\n    elem_nested = TensorElement(TensorElement(A(i, j), {i: 1}) * B(j, k), {j: 0})\n    assert elem_nested.get_free_indices() == [k]\n    assert elem_nested.data == [15, 21]\n", "def test_index_structure():\n    # Test creation of _IndexStructure\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    a, b, c, d = tensor_indices('a,b,c,d', Lorentz)\n    \n    # Test creation from indices\n    index_struct = _IndexStructure.from_indices(a, -b, c, -d)\n    assert index_struct.free == [(a, 0), (c, 2)]\n    assert index_struct.dum == [(1, 3)]\n    assert index_struct.index_types == [Lorentz, Lorentz, Lorentz, Lorentz]\n    assert index_struct.get_indices() == [a, -b, c, -d]\n\n    # Test creation from components, free, and dum\n    components = [tensorhead('A', [Lorentz]*2, [[1]*2])]\n    free = [(a, 0)]\n    dum = [(1, 2)]\n    index_struct = _IndexStructure.from_components_free_dum(components, free, dum)\n    assert index_struct.free == [(a, 0)]\n    assert index_struct.dum == [(1, 2)]\n    assert index_struct.index_types == [Lorentz, Lorentz]\n    assert index_struct.get_indices() == [a, -a, a, -a]\n\n    # Test sorting and permutation functions\n    sorted_free = index_struct._get_sorted_free_indices_for_canon()\n    assert sorted_free == [(a, 0)]\n    sorted_dum = index_struct._get_sorted_dum_indices_for_canon()\n    assert sorted_dum == [(1, 2)]\n    sorted_types = index_struct._get_lexicographically_sorted_index_types()\n    assert sorted_types == [Lorentz, Lorentz, Lorentz, Lorentz]\n\n    # Test perm2tensor method\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    A = tensorhead('A', [Lorentz, Lorentz], [[1]*2])\n    tensor = A(a, -b)\n    index_struct = tensor._index_structure\n    g = (1, 0)\n    perm_tensor = index_struct.perm2tensor(g, is_canon_bp=True)\n    assert perm_tensor.free == [(a, 1)]\n    assert perm_tensor.dum == [(0, 1)]\n\n    # Test indices_canon_args method\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L"], "sample_1047": ["def test_issue_sqrt2():\n    z = sqrt(2)\n    assert z.is_commutative is True\n    assert z.is_integer is False\n    assert z.is_rational is False\n    assert z.is_algebraic is True\n    assert z.is_transcendental is False\n    assert z.is_real is True\n    assert z.is_complex is True\n    assert z.is_noninteger is True\n    assert z.is_irrational is True\n    assert z.is_imaginary is False\n    assert z.is_positive is True\n    assert z.is_negative is False\n    assert z.is_nonpositive is False\n    assert z.is_nonnegative is True\n    assert z.is_even is False\n    assert z.is_odd is False\n    assert z.is_finite is True\n    assert z.is_infinite is False\n    assert z.is_comparable is True\n    assert z.is_prime is False\n    assert z.is_composite is False\n", "def test_polar_assumption():\n    x = Symbol('x', polar=True)\n    assert x.is_polar is True\n    assert x.is_real is None\n    assert x.is_complex is None\n    assert x.is_positive is None\n    assert x.is_negative is None\n    assert x.is_zero is None\n    assert x.is_finite is None\n", "def test_symbol_integer():\n    x = Symbol('x', integer=True)\n    assert x.is_integer is True\n    assert x.is_rational is True\n    assert x.is_algebraic is True\n    assert x.is_real is True\n    assert x.is_complex is True\n    assert x.is_noninteger is False\n    assert x.is_irrational is False\n    assert x.is_imaginary is False\n    assert x.is_positive is None\n    assert x.is_negative is None\n    assert x.is_nonpositive is None\n    assert x.is_nonnegative is None\n    assert x.is_even is None\n    assert x.is_odd is None\n    assert x.is_finite is True\n    assert x.is_infinite is False\n    assert x.is_comparable is None\n    assert x.is_prime is None\n    assert x.is_composite is None\n    assert x.is_number is False\n", "def test_issue_zero_odd_even():\n    # Testing edge cases for zero with odd and even assumptions\n    z = Integer(0)\n    assert z.is_odd is False\n    assert z.is_even is True\n", "def test_assumptions_with_symbols():\n    x = Symbol('x', negative=True, irrational=True)\n    assert x.is_negative is True\n    assert x.is_irrational is True\n    assert x.is_positive is False\n    assert x.is_rational is False\n\n    y = Symbol('y', integer=True, even=True, prime=True)\n    assert y.is_integer is True\n    assert y.is_even is True\n    assert y.is_prime is False  # Contradiction: even integer cannot be prime\n    assert y.is_odd is False\n\n    z = Symbol('z', transcendental=True, irrational=True)\n    assert z.is_transcendental is True\n    assert z.is_irrational is True\n    assert z.is_algebraic is False\n    assert z.is_rational is False\n"], "sample_1048": ["def test_parabola_axis_of_symmetry():\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n\n    pa1 = Parabola(p1, d1)\n    pa2 = Parabola(p2, d2)\n    pa3 = Parabola(Point(5, 5), Line(Point(2, 5), Point(8, 5)))\n\n    # Test vertical directrix\n    assert pa1.axis_of_symmetry == Line(Point(0, 0), Point(0, 1))\n    # Test horizontal directrix\n    assert pa2.axis_of_symmetry == Line(Point(3, 7), Point(4, 7))\n    # Test another horizontal directrix\n    assert pa3.axis_of_symmetry == Line(Point(5, 5), Point(5, 6))\n", "def test_parabola_axis_of_symmetry():\n    l1 = Line(Point(4, 0), Point(4, 9))\n    l2 = Line(Point(7, 6), Point(3, 6))\n    \n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    \n    parabola1 = Parabola(p1, l1)\n    parabola2 = Parabola(p2, l2)\n    \n    assert parabola1.axis_of_symmetry == Line(Point(0, 0), Point(0, 1))\n    assert parabola2.axis_of_symmetry == Line(Point(3, 7), Point(4, 7))\n\n    # Additional check to test if axis_of_symmetry matches with expected line\n    assert parabola1.axis_of_symmetry.equation() == Line(Point(0, 0), Point(0, 1)).equation()\n    assert parabola2.axis_of_symmetry.equation() == Line(Point(3, 7), Point(4, 7)).equation()\n", "def test_parabola_axis_of_symmetry():\n    # Testing vertical directrix\n    focus = Point(0, 0)\n    directrix = Line(Point(5, 8), Point(7, 8))\n    parabola = Parabola(focus, directrix)\n    assert parabola.axis_of_symmetry == Line(Point(0, 0), Point(0, 1))\n\n    # Testing horizontal directrix\n    focus = Point(0, 0)\n    directrix = Line(Point(4, 0), Point(4, 9))\n    parabola = Parabola(focus, directrix)\n    assert parabola.axis_of_symmetry == Line(Point(0, 0), Point(1, 0))\n\n    # Testing a different focus\n    focus = Point(2, 3)\n    directrix = Line(Point(4, 0), Point(4, 9))\n    parabola = Parabola(focus, directrix)\n    assert parabola.axis_of_symmetry == Line(Point(2, 3), Point(3, 3))\n", "def test_parabola_properties():\n    f1 = Point(0, 0)\n    f2 = Point(3, 4)\n    d1 = Line(Point(0, 3), Point(3, 3))\n    d2 = Line(Point(4, 0), Point(4, 3))\n    \n    p1 = Parabola(f1, d1)\n    p2 = Parabola(f2, d2)\n    \n    # Test ambient dimension\n    assert p1.ambient_dimension == 2\n    assert p2.ambient_dimension == 2\n\n    # Test axis of symmetry\n    assert p1.axis_of_symmetry == Line(Point2D(0, 0), Point2D(0, 1))\n    assert p2.axis_of_symmetry == Line(Point2D(3, 4), Point2D(3, 5))\n\n    # Test equation\n    assert p1.equation() == -x**2 - 16*y + 64\n    assert p2.equation() == 4*x - y**2 - 14*y + 48\n\n    # Test focal length\n    assert p1.focal_length == 1\n    assert p2.focal_length == 1\n\n    # Test focus\n    assert p1.focus == Point2D(0, 0)\n    assert p2.focus == Point2D(3, 4)\n\n    # Test p_parameter\n    assert p1.p_parameter == -1\n    assert p2.p_parameter == 1\n\n    # Test vertex\n    assert p1.vertex == Point2D(0, 1)\n    assert p2.vertex == Point2D(4, 4)\n\n    # Test eccentricity\n    assert p1.eccentricity == 1\n    assert p2.eccentricity == 1\n", "def test_parabola_axis_of_symmetry():\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    p3 = Point(0, 4)\n    p4 = Point(6, 0)\n\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n\n    pa1 = Parabola(p1, d2)\n    pa2 = Parabola(p2, d1)\n    pa3 = Parabola(p3, d2)\n    pa4 = Parabola(p4, d1)\n\n    assert pa1.axis_of_symmetry == Line(Point(0, 0), Point(1, 0))  # Horizontal directrix\n    assert pa2.axis_of_symmetry == Line(Point(3, 7), Point(3, 8))  # Vertical directrix\n    assert pa3.axis_of_symmetry == Line(Point(0, 4), Point(0, 5))  # Horizontal directrix\n    assert pa4.axis_of_symmetry == Line(Point(6, 0), Point(7, 0))  # Vertical directrix\n"], "sample_1049": ["def test_are_concurrent():\n    p1 = Plane(Point3D(1, 0, 0), normal_vector=(1, -1, 1))\n    p2 = Plane(Point3D(0, -2, 0), normal_vector=(3, 1, 1))\n    p3 = Plane(Point3D(0, -1, 0), normal_vector=(5, -1, 9))\n    p4 = Plane(Point3D(1, 1, 1), normal_vector=(0, 1, -1))\n\n    # Test with two concurrent planes\n    assert Plane.are_concurrent(p1, p2) is True\n\n    # Test with three non-concurrent planes\n    assert Plane.are_concurrent(p1, p2, p3) is False\n\n    # Test with planes that are all parallel\n    assert Plane.are_concurrent(p1, p4) is False\n\n    # Test with a single plane\n    assert Plane.are_concurrent(p1) is False\n\n    # Test with no planes\n    assert Plane.are_concurrent() is False\n\n    # Raise error for non-plane input\n    raises(ValueError, lambda: Plane.are_concurrent(p1, Point3D(1, 0, 0)))\n", "def test_are_concurrent():\n    # Define three planes that are concurrent (intersect along a common line)\n    p1 = Plane(Point3D(1, 2, 3), normal_vector=(1, -1, 1))\n    p2 = Plane(Point3D(2, 3, 4), normal_vector=(2, -2, 2))\n    p3 = Plane(Point3D(3, 4, 5), normal_vector=(3, -3, 3))\n    assert Plane.are_concurrent(p1, p2) is True\n    assert Plane.are_concurrent(p1, p2, p3) is True\n\n    # Define three planes that are not concurrent (do not intersect along a common line)\n    p4 = Plane(Point3D(0, 0, 0), normal_vector=(1, 1, 1))\n    p5 = Plane(Point3D(1, 1, 1), normal_vector=(1, 2, 3))\n    p6 = Plane(Point3D(2, 2, 2), normal_vector=(2, 1, 3))\n    assert Plane.are_concurrent(p4, p5, p6) is False\n\n    # Edge case: less than two planes\n    assert Plane.are_concurrent(p1) is False\n\n    # Invalid input: objects that are not planes\n    raises(ValueError, lambda: Plane.are_concurrent(p1, Point3D(0, 0, 0)))\n    raises(ValueError, lambda: Plane.are_concurrent(p1, Line3D(Point3D(0, 0, 0), Point3D(1, 1, 1))))\n", "def test_plane_contains():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n    p3 = Point3D(1, 2, 3)\n    pl = Plane(p1, p2, p3)\n\n    # Test with points\n    assert (0, 0, 0) in pl\n    assert Point3D(0.5, 1, 1.5) in pl\n    assert Point3D(1, 2, 3) in pl\n    assert (1, 1, 2) not in pl\n\n    # Test with lines\n    l1 = Line3D(Point3D(0, 0, 0), Point3D(1, 1, 1))\n    l2 = Line3D(Point3D(1, 1, 1), Point3D(2, 3, 4))\n    assert l1 in pl\n    assert l2 not in pl\n\n    # Test with rays\n    r1 = Ray3D(Point3D(0, 0, 0), Point3D(1, 1, 1))\n    r2 = Ray3D(Point3D(1, 1, 1), Point3D(2, 3, 4))\n    assert r1 in pl\n    assert r2 not in pl\n\n    # Test with segments\n    s1 = Segment3D(Point3D(0, 0, 0), Point3D(1, 1, 1))\n    s2 = Segment3D(Point3D(1, 1, 1), Point3D(2, 3, 4))\n    assert s1 in pl\n    assert s2 not in pl\n", "def test_plane_are_concurrent():\n    p1 = Plane(Point3D(0, 0, 0), normal_vector=(1, 0, 0))\n    p2 = Plane(Point3D(0, 0, 0), normal_vector=(0, 1, 0))\n    p3 = Plane(Point3D(0, 0, 0), normal_vector=(0, 0, 1))\n    p4 = Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))\n    assert Plane.are_concurrent(p1, p2) is True\n    assert Plane.are_concurrent(p1, p2, p3) is True\n    assert Plane.are_concurrent(p1, p2, p4) is False\n    assert Plane.are_concurrent(p1, p1) is False\n    raises(ValueError, lambda: Plane.are_concurrent(p1, p2, Point3D(1, 1, 1)))\n", "def test_plane_creation():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 0)\n    p3 = Point3D(1, 0, 1)\n    \n    # Test creating a plane with three non-collinear points\n    plane = Plane(p1, p2, p3)\n    assert plane.p1 == p1\n    assert plane.normal_vector == (1, 1, -1)\n\n    # Test creating a plane with a point and a normal vector\n    plane = Plane(p1, normal_vector=(1, 1, 1))\n    assert plane.p1 == p1\n    assert plane.normal_vector == (1, 1, 1)\n    \n    # Test exception for collinear points\n    raises(ValueError, lambda: Plane(p1, p1, p1))\n    raises(ValueError, lambda: Plane(p1, p2, Point3D(2, 2, 0)))\n\n    # Test exception for invalid normal vector\n    raises(ValueError, lambda: Plane(p1, normal_vector=(0, 0, 0)))\n    raises(ValueError, lambda: Plane(p1, normal_vector=(1,)))\n    \n    # Test creation with mixed input types\n    plane = Plane((0, 0, 0), (1, 1, 0), (1, 0, 1))\n    assert plane.p1 == p1\n    assert plane.normal_vector == (1, 1, -1)\n    plane = Plane([0, 0, 0], [1, 1, 0], [1, 0, 1])\n    assert plane.p1 == p1\n    assert plane.normal_vector == (1, 1, -1)\n"], "sample_1050": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(pi) == 'sympy.pi'\n    expr = acos(x) + sign(y) + pi\n    assert p.doprint(expr) == 'sympy.acos(x) + sympy.sign(y) + sympy.pi'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    expr = pi\n    assert p.doprint(expr) == 'sympy.pi'\n    expr = sign(x)\n    assert p.doprint(expr) == 'sympy.sign(x)'\n    expr = Mod(x, 2)\n    assert p.doprint(expr) == 'x % 2'\n    assert p.doprint(And(x, y)) == 'x and y'\n    assert p.doprint(Or(x, y)) == 'x or y'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x>6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert p.doprint(Piecewise((2, Le(x, 0)), (3, Gt(x, 0)), evaluate=False)) == '((2) if (x <= 0) else (3) if (x > 0) else None)'\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(A**(-1)) == \"sympy.Pow(A, -1)\"\n    assert p.doprint(A**5) == \"sympy.Pow(A, 5)\"\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)),\n                               (2, x > 6))) == 'sympy.Piecewise((1, Eq(x, 0)), (2, x > 6))'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    assert not any(m.startswith('scipy') for m in p.module_imports)\n    assert not any(m.startswith('numpy') for m in p.module_imports)\n    expr2 = sign(x)\n    assert p.doprint(expr2) == 'sympy.sign(x)'\n    assert 'sympy.sign' in p.module_imports\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x > 6))) == 'sympy.Piecewise((1, sympy.Eq(x, 0)), (2, x > 6))'\n"], "sample_1051": ["def test_purestr_with_noncommutative():\n    x, y = symbols('x y', commutative=False)\n    assert purestr(x*y) == \"Mul(Symbol('x'), Symbol('y'))\"\n    assert purestr(y*x) == \"Mul(Symbol('y'), Symbol('x'))\"\n", "def test_custom_styles():\n    custom_styles = [(Basic, {'color': 'red', 'shape': 'box'}),\n                     (Expr,  {'color': 'green'})]\n    text = dotprint(x+2, styles=custom_styles)\n    assert '\"color\"=\"green\"' in text\n    assert '\"shape\"=\"box\"' in text\n", "def test_dotnode_with_pos_and_repeat():\n    assert dotnode(x + 1, pos=(1, 2), repeat=True) == \\\n        '\"Add(Integer(1), Symbol(\\'x\\'))_(1, 2)\" [\"color\"=\"black\", \"label\"=\"Add\", \"shape\"=\"ellipse\"];'\n    assert dotnode(x * x**2, pos=(0,), repeat=True) == \\\n        '\"Mul(Symbol(\\'x\\'), Pow(Symbol(\\'x\\'), Integer(2)))_(0,)\" [\"color\"=\"black\", \"label\"=\"Mul\", \"shape\"=\"ellipse\"];'\n", "def test_custom_styles():\n    custom_styles = [(Basic, {'color': 'green', 'shape': 'rectangle'}),\n                     (Expr,  {'color': 'red'})]\n    assert styleof(Basic(1), custom_styles) == {'color': 'green', 'shape': 'rectangle'}\n    assert styleof(x + 1, custom_styles) == {'color': 'red', 'shape': 'rectangle'}\n\n    assert dotnode(x, styles=custom_styles, repeat=False) == \\\n           '\"Symbol(\\'x\\')\" [\"color\"=\"red\", \"label\"=\"x\", \"shape\"=\"rectangle\"];'\n    assert dotnode(x + 2, styles=custom_styles, repeat=False) == \\\n           '\"Add(Integer(2), Symbol(\\'x\\'))\" [\"color\"=\"red\", \"label\"=\"Add\", \"shape\"=\"rectangle\"];'\n\n    assert sorted(dotedges(x + 2, styles=custom_styles, repeat=False)) == [\n        '\"Add(Integer(2), Symbol(\\'x\\'))\" -> \"Integer(2)\";',\n        '\"Add(Integer(2), Symbol(\\'x\\'))\" -> \"Symbol(\\'x\\')\";'\n    ]\n\n    text = dotprint(x + 2, styles=custom_styles, repeat=False)\n    assert all(e in text for e in dotedges(x + 2, styles=custom_styles, repeat=False))\n    assert all(n in text for n in [dotnode(expr, styles=custom_styles, repeat=False) for expr in (x, Integer(2), x + 2)])\n    assert 'digraph' in text\n", "def test_attrprint_empty():\n    assert attrprint({}) == ''\n"], "sample_1052": ["def test_rust_codegen():\n    x, y, z = symbols('x, y, z')\n    expr = (x + y) * z\n    routine = make_routine(\"test\", expr)\n    code_gen = RustCodeGen()\n    source = get_string(code_gen.dump_rs, [routine])\n    expected = (\n        \"fn test(x: f64, y: f64, z: f64) -> f64 {\\n\"\n        \"   let test_result = z * (x + y);\\n\"\n        \"   test_result\\n\"\n        \"}\\n\"\n    )\n    assert source == expected\n", "def test_rust_codegen():\n    x, y, z = symbols('x, y, z')\n    expr = (x + y) * z\n    routine = make_routine(\"test\", expr)\n    code_gen = RustCodeGen()\n    source = get_string(code_gen.dump_rs, [routine], prefix=\"test\")\n    expected = (\n        \"/*\\n\"\n        \" *                    Code generated with sympy 0.7.2-git                    *\\n\"\n        \" *                                                                            *\\n\"\n        \" *              See http://www.sympy.org/ for more information.               *\\n\"\n        \" *                                                                            *\\n\"\n        \" *                       This file is part of 'project'                       *\\n\"\n        \" */\\n\"\n        \"fn test(x: f64, y: f64, z: f64) -> f64 {\\n\"\n        \"   let test_result = z * (x + y);\\n\"\n        \"   test_result\\n\"\n        \"}\\n\"\n    )\n    assert source == expected\n", "def test_rust_codegen():\n    x, y, z = symbols('x,y,z')\n    expr = (x + y) * z\n    routine = make_routine(\"test\", expr)\n    code_gen = RustCodeGen()\n    source = get_string(code_gen.dump_rs, [routine])\n    expected = (\n        \"fn test(x: f64, y: f64, z: f64) -> f64 {\\n\"\n        \"   let test_result = z * (x + y);\\n\"\n        \"   test_result\\n\"\n        \"}\\n\"\n    )\n    assert source == expected\n", "def test_fcode_with_matrix_expressions():\n    from sympy import Matrix\n    x, y = symbols('x y')\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = A * B + Matrix([[x, 0], [0, y]])\n    routine = make_routine(\"matrix_expr_test\", expr)\n    code_gen = FCodeGen()\n    source = get_string(code_gen.dump_f95, [routine])\n    expected = (\n        \"subroutine matrix_expr_test(A, B, x, y, out_%(hash)s)\\n\"\n        \"implicit none\\n\"\n        \"REAL*8, intent(in), dimension(1:2, 1:2) :: A\\n\"\n        \"REAL*8, intent(in), dimension(1:2, 1:2) :: B\\n\"\n        \"REAL*8, intent(in) :: x\\n\"\n        \"REAL*8, intent(in) :: y\\n\"\n        \"REAL*8, intent(out), dimension(1:2, 1:2) :: out_%(hash)s\\n\"\n        \"out_%(hash)s(1, 1) = A(1, 1)*B(1, 1) + A(1, 2)*B(2, 1) + x\\n\"\n        \"out_%(hash)s(1, 2) = A(1, 1)*B(1, 2) + A(1, 2)*B(2, 2)\\n\"\n        \"out_%(hash)s(2, 1) = A(2, 1)*B(1, 1) + A(2, 2)*B(2, 1)\\n\"\n        \"out_%(hash)s(2, 2) = A(2, 1)*B(1, 2) + A(2, 2)*B(2, 2) + y\\n\"\n        \"end subroutine\\n\"\n    )\n    # look for the magic number\n    a = source.splitlines()[5]\n    b = a.split('_')\n    out = b[1]\n    expected = expected % {'hash': out}\n    assert source == expected\n", "def test_rust_codegen():\n    x, y, z = symbols('x, y, z')\n    expr = (x + y)*z\n    routine = make_routine(\"test\", expr)\n    code_gen = RustCodeGen()\n    source = get_string(code_gen.dump_rs, [routine])\n    expected = (\n        'fn test(x: f64, y: f64, z: f64) -> f64 {\\n'\n        '   let test_result = z*(x + y);\\n'\n        '   test_result\\n'\n        '}\\n'\n    )\n    assert source == expected\n"], "sample_1053": ["def test_mpf_norm_cases():\n    from mpmath.libmp.libmpf import _normalize, fnan, finf, fninf, fzero, mpf\n    import mpmath.libmp as mlib\n    rnd = mlib.round_nearest\n\n    # Test normalization cases\n    assert mpf_norm((1, 0, 1, 0), 10) == fzero  # should normalize to zero\n    assert mpf_norm((0, 1, 1, 1), 10) == (0, 1, 1, 1)  # no change needed\n    assert mpf_norm((1, 1, 1, 1), 10) == (1, 1, 1, 1)  # no change needed\n\n    # Test normalization of special values\n    assert mpf_norm(fnan, 53) == fnan\n    assert mpf_norm(finf, 53) == finf\n    assert mpf_norm(fninf, 53) == fninf\n\n    # Create an mpf directly and check normalization\n    mpf_val = mpf((1, 123456789, -27, 53))\n    assert _normalize(mpf_val._mpf_, 53) == mpf_val._mpf_\n", "def test_number_symbol_simplify():\n    assert simplify(pi + E) == pi + E\n    assert simplify(GoldenRatio + EulerGamma) == GoldenRatio + EulerGamma\n    assert simplify(GoldenRatio**2) == GoldenRatio**2\n    assert simplify(EulerGamma + Catalan) == EulerGamma + Catalan\n    assert simplify(TribonacciConstant + GoldenRatio) == TribonacciConstant + GoldenRatio\n", "def test_mpf_norm_edge_cases():\n    # Case when mantissa is zero and bc is also zero\n    mpf_tuple = (1, 0, 0, 0)\n    assert mpf_norm(mpf_tuple, 10) == _mpf_zero\n\n    # Case when mantissa is zero but bc is not zero\n    mpf_tuple = (1, 0, 1, 1)\n    assert mpf_norm(mpf_tuple, 10) == mpf_tuple\n\n    # Case when mantissa is non-zero and bc is also non-zero\n    mpf_tuple = (1, 1234, 1, 4)\n    assert mpf_norm(mpf_tuple, 10) == mpf_tuple\n\n    # Case when mantissa is non-zero and bc is zero\n    mpf_tuple = (1, 1234, 1, 0)\n    assert mpf_norm(mpf_tuple, 10) == _mpf_zero\n", "def test_issue_11013():\n    assert divmod(Float('inf'), 0.2) == (oo, nan)\n    assert divmod(Float('-inf'), 0.2) == (-oo, nan)\n    assert divmod(0.2, Float('inf')) == (0, 0.2)\n    assert divmod(0.2, Float('-inf')) == (0, 0.2)\n", "def test_mpf_norm_zero_check():\n    assert mpf_norm((1, 0, 0, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((1, 1, 0, 1), 10) == (1, 1, 0, 1)\n"], "sample_1054": ["def test_Range_raises_ValueError():\n    raises(ValueError, lambda: Range(3, 10, 0))  # step cannot be 0\n    raises(ValueError, lambda: Range(1, 4, S.Half))  # step must be integer\n    raises(ValueError, lambda: Range(S(1)/2, 4, 1))  # start must be integer\n    raises(ValueError, lambda: Range(1, 4, S(1)/2))  # step must be literal integer\n    raises(ValueError, lambda: Range(0, oo, S(2)/3))  # step must be literal integer\n    raises(ValueError, lambda: Range(S.NegativeInfinity, S.Infinity))  # both start and end cannot be infinite\n    raises(ValueError, lambda: Range(S.Zero, S.Infinity, S.Zero))  # step cannot be zero\n", "def test_range_invalid_arguments():\n    raises(ValueError, lambda: Range(0, 10, 0))  # step cannot be 0\n    raises(ValueError, lambda: Range(0, 10, 2.5))  # step must be an integer\n    raises(ValueError, lambda: Range(0, 10, S.Pi))  # step must be a literal integer\n    raises(ValueError, lambda: Range(S.Pi))  # start and stop must be finite integers\n    raises(ValueError, lambda: Range(0, S.Pi, 1))  # start and stop must be finite integers\n    raises(ValueError, lambda: Range(-oo, oo))  # both start and stop cannot be infinite\n", "def test_complexregion_contains():\n    # Test ComplexRegion contains for both rectangular and polar forms\n    # Rectangular form\n    a = Interval(-3, 3)\n    b = Interval(-2, 2)\n    c_rect = ComplexRegion(a * b)\n    assert 1 + 1*I in c_rect\n    assert -3 + 2*I in c_rect\n    assert 3 - 2*I in c_rect\n    assert -3 - 2*I in c_rect\n    assert 3 + 3*I not in c_rect\n    assert -4 + 1*I not in c_rect\n\n    # Polar form\n    r = Interval(0, 2)\n    theta = Interval(0, S.Pi)\n    c_polar = ComplexRegion(r * theta, polar=True)\n    assert 2*I in c_polar  # 2*exp(I*pi/2)\n    assert -2 in c_polar  # 2*exp(I*pi)\n    assert 2*exp(I*3*S.Pi/4) in c_polar  # Check an angle in the first quadrant\n    assert 3*exp(I*S.Pi/2) not in c_polar  # r=3 is out of bounds\n    assert 2*exp(I*3*S.Pi/2) not in c_polar  # theta=3*pi/2 is out of bounds\n", "def test_Range_slice():\n    # Test slicing with positive step\n    r = Range(0, 10)\n    assert r[::2] == Range(0, 10, 2)\n    assert r[1::2] == Range(1, 10, 2)\n    assert r[:5:2] == Range(0, 5, 2)\n    assert r[1:5:2] == Range(1, 5, 2)\n    assert r[1:8:3] == Range(1, 8, 3)\n    assert r[2:9:3] == Range(2, 9, 3)\n    assert r[2:8:3] == Range(2, 8, 3)\n    assert r[:0] == Range(0, 0, 1)\n    assert r[:0:2] == Range(0, 0, 2)\n\n    # Test slicing with negative step\n    assert r[::-1] == Range(9, -1, -1)\n    assert r[::-2] == Range(9, -1, -2)\n    assert r[5::-1] == Range(5, -1, -1)\n    assert r[5::-2] == Range(5, -1, -2)\n    assert r[5:1:-1] == Range(5, 1, -1)\n    assert r[8:2:-2] == Range(8, 2, -2)\n    assert r[8:1:-2] == Range(8, 1, -2)\n    assert r[0::-1] == Range(0, -1, -1)\n    assert r[0::-2] == Range(0, -1, -2)\n    \n    # Test slicing with invalid step\n    raises(ValueError, lambda: r[::0])\n    raises(ValueError, lambda: r[5::-0])\n    raises(ValueError, lambda: r[:1:0])\n", "def test_range_boundary_cases():\n    # Test Range with step=1\n    r1 = Range(0, 10, 1)\n    assert r1.start == 0\n    assert r1.stop == 10\n    assert r1.step == 1\n    assert len(r1) == 10\n    assert list(r1) == list(range(10))\n\n    # Test Range with step=-1\n    r2 = Range(10, 0, -1)\n    assert r2.start == 10\n    assert r2.stop == 0\n    assert r2.step == -1\n    assert len(r2) == 10\n    assert list(r2) == list(range(10, 0, -1))\n\n    # Test Range with larger step\n    r3 = Range(0, 10, 2)\n    assert r3.start == 0\n    assert r3.stop == 10\n    assert r3.step == 2\n    assert len(r3) == 5\n    assert list(r3) == list(range(0, 10, 2))\n\n    # Test negative Range with negative step\n    r4 = Range(-10, -20, -2)\n    assert r4.start == -10\n    assert r4.stop == -20\n    assert r4.step == -2\n    assert len(r4) == 5\n    assert list(r4) == list(range(-10, -20, -2))\n\n    # Test empty Range due to step direction\n    r5 = Range(0, 10, -1)\n    assert len(r5) == 0\n    assert list(r5) == []\n\n    # Test Range starting from infinity\n    r6 = Range(oo, 0, -2)\n    assert r6.stop == 0\n    raises(ValueError, lambda: next(iter(r6)))\n\n    # Test Range ending at infinity\n    r7 = Range(0, oo, 2)\n    assert r7.start == 0\n    assert r7.stop == oo\n    assert r7.step == 2\n    assert r7[5] == 10\n    raises(ValueError, lambda: len(r7))\n\n    # Test Range with both start and end as negative infinity\n    raises(ValueError, lambda: Range(-oo, -oo, -1))\n\n    # Test slicing on infinite Range\n    assert list(Range("], "sample_1055": ["def test_elgamal_public_key():\n    pri = elgamal_private_key()\n    p, r, d = pri\n    pub = elgamal_public_key(pri)\n    assert pub[0] == p\n    assert pub[1] == r\n    assert pub[2] == pow(r, d, p)\n", "def test_encipher_decipher_elgamal():\n    pri = elgamal_private_key(10)\n    pub = elgamal_public_key(pri)\n    message = 123\n    enc = encipher_elgamal(message, pub)\n    dec = decipher_elgamal(enc, pri)\n    assert dec == message\n\n    pri = elgamal_private_key(20)\n    pub = elgamal_public_key(pri)\n    message = 4567\n    enc = encipher_elgamal(message, pub)\n    dec = decipher_elgamal(enc, pri)\n    assert dec == message\n\n    pri = elgamal_private_key(30)\n    pub = elgamal_public_key(pri)\n    message = 891011\n    enc = encipher_elgamal(message, pub)\n    dec = decipher_elgamal(enc, pri)\n    assert dec == message\n", "def test_encipher_decipher_atbash():\n    msg = \"HELLO\"\n    enciphered = encipher_atbash(msg)\n    deciphered = decipher_atbash(enciphered)\n    assert deciphered == msg\n", "def test_bifid10_encipher_decipher():\n    key = \"SYMPY\"\n    symbols = bifid10  # using the bifid10 which is printable characters\n    msg = \"Hello, World!123\"\n    enciphered = encipher_bifid(msg, key, symbols)\n    deciphered = decipher_bifid(enciphered, key, symbols)\n    assert deciphered == msg\n", "def test_encipher_substitution_with_mapping():\n    assert encipher_substitution(\"ABC\", {\"A\": \"X\", \"B\": \"Y\", \"C\": \"Z\"}) == \"XYZ\"\n    assert encipher_substitution(\"HELLO\", {\"H\": \"M\", \"E\": \"N\", \"L\": \"O\", \"O\": \"P\"}) == \"MNOOP\"\n"], "sample_1056": ["def test_boolean_ops():\n    from sympy import And, Or, Not\n\n    expr_and = And(x > 1, y < 2)\n    expr_or = Or(x > 1, y < 2)\n    expr_not = Not(x > 1)\n\n    assert lambdarepr(expr_and) == \"(((x > 1)) and ((y < 2)))\"\n    assert lambdarepr(expr_or) == \"(((x > 1)) or ((y < 2)))\"\n    assert lambdarepr(expr_not) == \"(not ((x > 1)))\"\n", "def test_logical_expressions():\n    from sympy import And, Or, Not, true, false\n    \n    assert lambdarepr(And(x > 1, y < 2)) == \"((x > 1) and (y < 2))\"\n    assert lambdarepr(Or(x > 1, y < 2)) == \"((x > 1) or (y < 2))\"\n    assert lambdarepr(Not(x > 1)) == \"(not (x > 1))\"\n    assert lambdarepr(true) == \"True\"\n    assert lambdarepr(false) == \"False\"\n", "def test_boolean_operations():\n    from sympy import And, Or, Not\n\n    expr = And(x > 1, y < 2)\n    assert lambdarepr(expr) == \"(((x > 1)) and ((y < 2)))\"\n\n    expr = Or(x > 1, y < 2)\n    assert lambdarepr(expr) == \"(((x > 1)) or ((y < 2)))\"\n\n    expr = Not(x > 1)\n    assert lambdarepr(expr) == \"(not ((x > 1)))\"\n\n    expr = And(Or(x > 1, y < 2), Not(z == 3))\n    assert lambdarepr(expr) == \"((((x > 1)) or ((y < 2))) and (not ((z == 3))))\"\n\n    expr = Or(And(x > 1, y < 2), Not(z == 3))\n    assert lambdarepr(expr) == \"((((x > 1)) and ((y < 2))) or (not ((z == 3))))\"\n", "def test_boolean_expressions():\n    # Test for boolean expressions\n    from sympy import And, Or, Not, true, false\n\n    expr_and = And(x > 1, y < 2)\n    l = lambdarepr(expr_and)\n    eval(\"lambda x, y: \" + l)\n    assert l == \"(((x > 1)) and ((y < 2)))\"\n\n    expr_or = Or(x > 1, y < 2)\n    l = lambdarepr(expr_or)\n    eval(\"lambda x, y: \" + l)\n    assert l == \"(((x > 1)) or ((y < 2)))\"\n\n    expr_not = Not(x > 1)\n    l = lambdarepr(expr_not)\n    eval(\"lambda x: \" + l)\n    assert l == \"(not ((x > 1)))\"\n\n    expr_true = true\n    l = lambdarepr(expr_true)\n    eval(\"lambda: \" + l)\n    assert l == \"True\"\n\n    expr_false = false\n    l = lambdarepr(expr_false)\n    eval(\"lambda: \" + l)\n    assert l == \"False\"\n", "def test_boolean_expressions():\n    # Testing BooleanTrue and BooleanFalse\n    assert lambdarepr(True) == \"True\"\n    assert lambdarepr(False) == \"False\"\n\n    from sympy import And, Or, Not\n\n    # Testing And\n    expr = And(x > 1, y < 2)\n    l = lambdarepr(expr)\n    eval(\"lambda x, y: \" + l)\n    assert l == \"(((x > 1)) and ((y < 2)))\"\n\n    # Testing Or\n    expr = Or(x > 1, y < 2)\n    l = lambdarepr(expr)\n    eval(\"lambda x, y: \" + l)\n    assert l == \"(((x > 1)) or ((y < 2)))\"\n\n    # Testing Not\n    expr = Not(x > 1)\n    l = lambdarepr(expr)\n    eval(\"lambda x: \" + l)\n    assert l == \"(not ((x > 1)))\"\n"], "sample_1057": ["def test_fully_qualified_modules():\n    ast = Print('x y'.split(), \"coordinate: %12.5g %12.5g\")\n    printer = PythonCodePrinter({'standard': 'python3', 'fully_qualified_modules': True})\n    pystr = printer.doprint(ast)\n    module_imports_str = '\\n'.join('import %s' % k for k in printer.module_imports)\n    \n    assert render_as_module(ast, standard='python3') == \\\n        module_imports_str + '\\n\\n' + pystr\n", "def test_fully_qualified_modules():\n    ast = Print('x y'.split(), \"coordinate: %12.5g %12.5g\")\n    printer = PythonCodePrinter({'standard': 'python3', 'fully_qualified_modules': True})\n    pystr = printer.doprint(ast)\n    if printer._settings['fully_qualified_modules']:\n        module_imports_str = '\\n'.join('import %s' % k for k in printer.module_imports)\n    else:\n        module_imports_str = '\\n'.join(['from %s import %s' % (k, ', '.join(v)) for\n                                        k, v in printer.module_imports.items()])\n    expected_output = module_imports_str + '\\n\\n' + pystr\n    assert render_as_module(ast, standard='python3') == expected_output\n", "def test_fully_qualified_modules():\n    from sympy import Symbol\n    x, y = Symbol('x'), Symbol('y')\n    expr = x + y\n    printer_settings = {'fully_qualified_modules': True}\n    printer = PythonCodePrinter(printer_settings)\n    pystr = printer.doprint(expr)\n    module_imports_str = 'import sympy.core.symbol'\n    expected_output = module_imports_str + '\\n\\n' + pystr\n    assert render_as_module(expr, standard='python3') == expected_output\n", "def test_module_imports():\n    from sympy import Symbol\n    from sympy.codegen.ast import Assignment\n\n    x = Symbol('x')\n    y = Symbol('y')\n    ast = Assignment(x, y)\n    \n    result = render_as_module(ast, standard='python3')\n    expected_imports = 'from sympy import Symbol\\nfrom sympy.codegen.ast import Assignment'\n    expected_code = '\\n\\nx = y'\n    \n    assert result == expected_imports + expected_code\n", "def test_module_imports():\n    from sympy import sin, cos\n    from sympy.codegen.ast import FunctionDefinition, Variable\n    from sympy.codegen.pyutils import render_as_module\n\n    # Create a simple function definition AST\n    func_def = FunctionDefinition(Variable('f', dtype='float'), [Variable('x', dtype='float')],\n                                  [sin(Variable('x')), cos(Variable('x'))])\n\n    result = render_as_module(func_def, standard='python3')\n\n    expected_imports = 'from sympy import cos, sin'\n    expected_code = '\\ndef f(x):\\n    return (sin(x), cos(x))'\n\n    assert result == expected_imports + '\\n\\n' + expected_code\n"], "sample_1058": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    \n    # Test known functions and constants\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(pi) == 'sympy.pi'\n    \n    # Test matrix printing\n    from sympy import Matrix\n    mat = Matrix([[1, 2], [3, 4]])\n    assert p.doprint(mat) == 'sympy.Matrix([[1, 2], [3, 4]])'\n    \n    # Test custom function printing\n    class CustomFunction(Expr):\n        pass\n    \n    custom_expr = CustomFunction()\n    assert p.doprint(custom_expr) == 'sympy.CustomFunction()'\n    \n    # Test power printing\n    assert p.doprint(x**2) == 'x**2'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    \n    # Test relational printing\n    from sympy import Eq\n    assert p.doprint(Eq(x, y)) == '(x == y)'\n    \n    # Test function with module\n    from sympy import sin\n    assert p.doprint(sin(x)) == 'sympy.sin(x)'\n    \n    # Ensure module imports are correct\n    assert 'sympy' in p.module_imports\n", "def test_Indexed():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(p[2, 3]) == 'p[2, 3]'\n    assert prntr.doprint(p[x, y]) == 'p[x, y]'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(p[2, 3]) == 'p[2, 3]'\n    assert prntr.doprint(p[x, y]) == 'p[x, y]'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(p[2, 3]) == 'p[2, 3]'\n    assert prntr.doprint(p[x, y]) == 'p[x, y]'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(Assignment(x, 2)) == 'x = 2'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x > 6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert p.doprint(Piecewise((2, Le(x, 0)), (3, Gt(x, 0)), evaluate=False)) == '((2) if (x <= 0) else (3) if (x > 0) else None)'\n    assert p.doprint(sign(x)) == '(0.0 if x == 0 else sympy.copysign(1, x))'\n    assert p.doprint(p[0, 1]) == 'p[0, 1]'\n    assert not p.module_imports  # Ensure no external module imports are used\n", "def test_PythonCodePrinter_print_functions():\n    prntr = PythonCodePrinter()\n\n    # Testing known functions\n    assert prntr.doprint(abs(x)) == 'abs(x)'\n    assert prntr.doprint(exp(x)) == 'math.exp(x)'\n    assert prntr.doprint(log(x)) == 'math.log(x)'\n    assert prntr.doprint(sin(x)) == 'math.sin(x)'\n    assert prntr.doprint(cos(x)) == 'math.cos(x)'\n    assert prntr.doprint(tan(x)) == 'math.tan(x)'\n    assert prntr.doprint(sinh(x)) == 'math.sinh(x)'\n    assert prntr.doprint(cosh(x)) == 'math.cosh(x)'\n    assert prntr.doprint(tanh(x)) == 'math.tanh(x)'\n    assert prntr.doprint(asin(x)) == 'math.asin(x)'\n    assert prntr.doprint(acos(x)) == 'math.acos(x)'\n    assert prntr.doprint(atan(x)) == 'math.atan(x)'\n    assert prntr.doprint(atan2(x, y)) == 'math.atan2(x, y)'\n\n    # Testing known constants\n    assert prntr.doprint(S.Pi) == 'math.pi'\n    assert prntr.doprint(S.Exp1) == 'math.e'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x > 0))) == 'sympy.Piecewise((1, sympy.Eq(x, 0)), (2, x > 0))'\n    assert p.doprint(Mod(x, 2)) == 'sympy.Mod(x, 2)'\n\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(x**Rational(1, 2)) == 'x**(1/2)'\n\n    assert p.doprint(Assignment(x, 2)) == 'x = 2'\n    assert p.doprint(p[0, 1]) == 'p[0, 1]'\n    assert p.doprint(Rational(1, 2)) == '1/2'\n    assert p.doprint(pi) == 'sympy.pi'\n"], "sample_1059": ["def test_jacobi_normalized():\n    n = Symbol(\"n\", integer=True)\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    assert jacobi_normalized(0, a, b, x) == 1 / sqrt(2**(a + b + 1) * gamma(a + 1) * gamma(b + 1) / (factorial(0) * gamma(a + b + 1)))\n    assert jacobi_normalized(1, a, b, x) == jacobi(1, a, b, x) / sqrt(2**(a + b + 1) * gamma(a + 2) * gamma(b + 2) / (3 * gamma(a + b + 2)))\n    assert jacobi_normalized(2, a, b, x) == jacobi(2, a, b, x) / sqrt(2**(a + b + 1) * gamma(a + 3) * gamma(b + 3) / (5 * gamma(a + b + 3)))\n\n    # Check orthogonality\n    f1 = jacobi_normalized(3, a, b, x)\n    f2 = jacobi_normalized(3, a, b, x)\n    w = (1 - x)**a * (1 + x)**b\n    assert integrate(f1*f2*w, (x, -1, 1)) == 1\n\n    f1 = jacobi_normalized(2, a, b, x)\n    f2 = jacobi_normalized(3, a, b, x)\n    assert integrate(f1*f2*w, (x, -1, 1)) == 0\n\n    raises(ValueError, lambda: jacobi_normalized(-2.1, a, b, x))\n", "def test_jacobi_normalized():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    # Normalized Jacobi polynomial test cases\n    assert jacobi_normalized(0, a, b, x) == jacobi(0, a, b, x)\n    assert jacobi_normalized(1, a, b, x) == jacobi(1, a, b, x) / sqrt(2**(a + b + 1)*gamma(a + 2)*gamma(b + 2)/(3*factorial(1)*gamma(a + b + 2)))\n\n    # Check orthogonality on [-1, 1] with respect to the weight (1-x)^a * (1+x)^b\n    from sympy import integrate\n    from sympy.abc import y\n\n    f = jacobi_normalized(n, a, b, y) * jacobi_normalized(n + 1, a, b, y) * (1 - y)**a * (1 + y)**b\n    assert integrate(f, (y, -1, 1)).simplify() == 0\n\n    # Check normalization\n    f = jacobi_normalized(n, a, b, y)**2 * (1 - y)**a * (1 + y)**b\n    assert integrate(f, (y, -1, 1)).simplify() == 1\n\n    # Check that normalized Jacobi polynomial at specific points\n    assert jacobi_normalized(n, a, b, 1) == jacobi(n, a, b, 1) / sqrt(2**(a + b + 1) * gamma(a + n + 1) * gamma(b + n + 1) / ((a + b + 2*n + 1) * factorial(n) * gamma(a + b + n + 1)))\n    assert jacobi_normalized(n, a, b, -1) == jacobi(n, a, b, -1) / sqrt(2**(a + b + 1) * gamma(a + n + 1) * gamma(b + n + 1) / ((a + b + 2*n + 1) * factorial(n) * gamma(a + b + n + 1)))\n", "def test_jacobi_normalized():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    # Ensure jacobi_normalized returns the correct normalized polynomial\n    expr = jacobi_normalized(n, a, b, x)\n    expected = jacobi(n, a, b, x) / sqrt(2**(a + b + 1) * gamma(a + n + 1) * gamma(b + n + 1) / ((a + b + 2 * n + 1) * factorial(n) * gamma(a + b + n + 1)))\n    assert expr == expected\n\n    # Test some specific values\n    assert jacobi_normalized(0, a, b, x) == 1 / sqrt(2**(a + b + 1) / (a + b + 1))\n    assert jacobi_normalized(1, 0, 0, x) == sqrt(3/2) * x\n    assert jacobi_normalized(2, 1, 1, x) == sqrt(15/2) * (3*x**2 - 1)/2\n\n    # Ensure conjugation maintains normalization\n    assert conjugate(jacobi_normalized(n, a, b, x)) == jacobi_normalized(n, conjugate(a), conjugate(b), conjugate(x))\n\n    # Ensure differentiation works correctly\n    _k = Dummy('k')\n    assert diff(jacobi_normalized(n, a, b, x), x) == diff(jacobi(n, a, b, x), x) / sqrt(2**(a + b + 1) * gamma(a + n + 1) * gamma(b + n + 1) / ((a + b + 2 * n + 1) * factorial(n) * gamma(a + b + n + 1)))\n\n    # Ensure errors are raised for invalid inputs\n    raises(ValueError, lambda: jacobi_normalized(-1, a, b, x))\n    raises(ValueError, lambda: jacobi_normalized(n, a, b, oo))\n", "def test_jacobi_normalized():\n    n = Symbol(\"n\", integer=True, positive=True)\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    # Check normalization property\n    jn = jacobi_normalized(n, a, b, x)\n    integral = Sum(jn.subs(x, cos(pi * k / n))**2 * (1 - cos(pi * k / n))**a * (1 + cos(pi * k / n))**b, (k, 1, n)).doit()\n    assert integral == 1\n\n    # Check special cases\n    assert jacobi_normalized(0, a, b, x) == 1\n    assert jacobi_normalized(1, a, b, x) == jacobi(1, a, b, x) / sqrt(2**(a + b + 1) * gamma(a + 2) * gamma(b + 2) / ((a + b + 3) * factorial(1) * gamma(a + b + 2)))\n\n    # Check conjugate property\n    assert conjugate(jacobi_normalized(n, a, b, x)) == jacobi_normalized(n, conjugate(a), conjugate(b), conjugate(x))\n\n    # Check differentiation properties\n    assert diff(jacobi_normalized(n, a, b, x), x) == jacobi_normalized(n - 1, a + 1, b + 1, x) * (a / 2 + b / 2 + n / 2 + S(1) / 2) / sqrt(2**(a + b + 1) * gamma(a + n) * gamma(b + n) / ((a + b + 2 * n + 1) * factorial(n) * gamma(a + b + n)))\n\n    # Check fdiff\n    raises(ArgumentIndexError, lambda: jacobi_normalized(n, a, b, x).fdiff(1))\n    raises(ArgumentIndexError, lambda: jacobi_normalized(n, a, b, x).fdiff(2))\n    raises(ArgumentIndexError, lambda: jacobi_normalized(n, a, b, x).fdiff(3))\n    raises(ArgumentIndexError, lambda: jacobi_normalized(n, a, b, x).fdiff(5))\n", "def test_jacobi_normalized():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    # Testing specific symbolic result\n    assert jacobi_normalized(n, a, b, x) == jacobi(n, a, b, x) / sqrt(2**(a + b + 1) * gamma(a + n + 1) * gamma(b + n + 1) / ((a + b + 2*n + 1) * factorial(n) * gamma(a + b + n + 1)))\n\n    # Testing evaluation of normalized Jacobi polynomial at specific points\n    assert jacobi_normalized(0, a, b, x) == 1\n    assert jacobi_normalized(1, 0, 0, x) == jacobi(1, 0, 0, x) / sqrt(1)\n    assert jacobi_normalized(1, 1, 1, x) == jacobi(1, 1, 1, x) / sqrt(2**(1 + 1 + 1) * gamma(1 + 1 + 1) * gamma(1 + 1 + 1) / ((1 + 1 + 2*1 + 1) * factorial(1) * gamma(1 + 1 + 1 + 1)))\n    assert jacobi_normalized(2, Rational(1, 2), Rational(1, 2), x) == jacobi(2, Rational(1, 2), Rational(1, 2), x) / sqrt(2**(Rational(1, 2) + Rational(1, 2) + 1) * gamma(Rational(1, 2) + 2 + 1) * gamma(Rational(1, 2) + 2 + 1) / ((Rational(1, 2) + Rational(1, 2) + 2*2 + 1) * factorial(2) * gamma(Rational(1, 2) + Rational(1, 2) + 2 + 1)))\n\n    # Testing conjugate of normalized Jacobi polynomial\n    assert conjugate(jacobi_normalized(n, a, b, x)) == jacobi_normalized(n, conjugate(a), conjugate(b), conjugate(x))\n\n    # Testing diff of normalized Jacobi polynomial"], "sample_1060": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x>6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert p.doprint(pi) == 'sympy.pi'\n", "def test_MatrixBase_printers():\n    prntr = PythonCodePrinter()\n    matrix_expr = SparseMatrix([[1, 2], [3, 4]])\n    assert prntr.doprint(matrix_expr) == 'Matrix([[1, 2], [3, 4]])'\n\n    np_prntr = NumPyPrinter()\n    assert np_prntr.doprint(matrix_expr) == 'numpy.array([[1, 2], [3, 4]])'\n\n    sp_prntr = SciPyPrinter()\n    assert sp_prntr.doprint(matrix_expr) == 'scipy.sparse.coo_matrix([1, 3, 2, 4], ([0, 1, 0, 1], [0, 0, 1, 1]), shape=(2, 2))'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    assert not any(m.startswith('numpy') or m.startswith('scipy') for m in p.module_imports)\n    mat = SparseMatrix(2, 2, {(0, 1): 3})\n    assert p.doprint(mat) == 'sympy.Matrix([[0, 3], [0, 0]])'\n    assert 'sympy' in p.module_imports\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x > 6))) == \\\n        'sympy.Piecewise((1, Eq(x, 0)), (2, x > 6))'\n    assert p.doprint(Identity(3)) == 'sympy.eye(3)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    expr = x + y\n    assert p.doprint(expr) == 'sympy.Add(x, y)'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(pi) == 'sympy.pi'\n"], "sample_1061": ["def test_integer_nthroot_additional_cases():\n    assert integer_nthroot(27, 3) == (3, True)  # Perfect cube\n    assert integer_nthroot(64, 4) == (2, True)  # Perfect fourth root\n    assert integer_nthroot(81, 2) == (9, True)  # Perfect square\n    assert integer_nthroot(100, 2) == (10, True)  # Another perfect square\n    assert integer_nthroot(100, 3) == (4, False)  # Non-perfect cube\n    assert integer_nthroot(1000000, 6) == (10, True)  # Perfect sixth root\n    assert integer_nthroot(123456789, 10) == (2, False)  # Large non-perfect root\n    assert integer_nthroot(1024, 2) == (32, True)  # Perfect power of 2\n    assert integer_nthroot(729, 3) == (9, True)  # Another perfect cube\n    assert integer_nthroot(2**60, 6) == (64, True)  # Large perfect sixth root\n", "def test_isqrt_edge_cases():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(2) == 1\n    assert isqrt(3) == 1\n    assert isqrt(4) == 2\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(17) == 4\n    assert isqrt(24) == 4\n    assert isqrt(25) == 5\n    assert isqrt(26) == 5\n    assert isqrt(99) == 9\n    assert isqrt(100) == 10\n    assert isqrt(101) == 10\n    assert isqrt(4503599761588223) == 67108863\n    assert isqrt(4503599761588224) == 67108864\n    assert isqrt(4503599761588225) == 67108864\n", "def test_isqrt_edge_cases():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(9) == 3\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(17) == 4\n    assert isqrt(24) == 4\n    assert isqrt(25) == 5\n    assert isqrt(26) == 5\n    assert isqrt(1e10) == int(1e5)\n    assert isqrt(1e18) == int(1e9)\n    assert isqrt(1e20) == int(1e10)\n    raises(ValueError, lambda: isqrt(-1))\n    raises(ValueError, lambda: isqrt(-100))\n", "def test_isqrt_edge_cases():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(2) == 1\n    assert isqrt(3) == 1\n    assert isqrt(4) == 2\n    assert isqrt(10) == 3\n    assert isqrt(100) == 10\n    assert isqrt(101) == 10\n    assert isqrt(1024) == 32\n    assert isqrt(1 << 2000) == 1 << 1000  # testing very large perfect square\n\n    raises(ValueError, lambda: isqrt(-1))  # testing negative input\n", "def test_Pow():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert Pow(x, y).as_base_exp() == (x, y)\n    assert Pow(4, 2).as_base_exp() == (4, 2)\n    assert Pow(x*y, 2).as_base_exp() == (x*y, 2)\n    assert Pow(x**2, y).as_base_exp() == (x**2, y)\n    assert Pow(x, -2).as_base_exp() == (x, -2)\n    assert Pow(x**-2, y).as_base_exp() == (x**-2, y)\n\n    assert Pow(2, 3).evalf() == 8\n    assert Pow(2, -3).evalf() == 0.125\n    assert Pow(-2, 3).evalf() == -8\n    assert Pow(-2, -3).evalf() == -0.125\n    assert Pow(2, 3, evaluate=False) == 2**3\n\n    assert Pow(2, Rational(1, 2)).evalf() == sqrt(2)\n    assert Pow(2, Rational(-1, 2)).evalf() == 1/sqrt(2)\n    assert Pow(-2, Rational(1, 3)).evalf().is_real is False\n    assert Pow(4, Rational(1, 2)).evalf() == 2\n\n    p = Pow(2, 3)\n    assert p.is_even is False\n    assert p.is_odd is True\n    assert p.is_integer is True\n\n    p = Pow(2, S.Half)\n    assert p.is_even is False\n    assert p.is_odd is False\n    assert p.is_integer is False\n\n    p = Pow(-2, 3)\n    assert p.is_even is False\n    assert p.is_odd is True\n    assert p.is_integer is True\n\n    p = Pow(-2, -3)\n    assert p.is_even is False\n    assert p.is_odd is True\n    assert p.is_integer is False\n    assert p.is_real is False\n\n    p = Pow(2, 0)\n    assert p.is_even is False\n    assert p.is_odd is False\n    assert p.is_integer is True\n\n    p = Pow(0,"], "sample_1062": ["def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(1 - tan(x)**2) == 1 - tan(x)**2\n    assert TR22(1 - cot(x)**2) == 1 - cot(x)**2\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n    assert TR22(cot(x)**4) == (csc(x)**2 - 1)**2\n", "def test_sincos_to_sum():\n    assert sincos_to_sum(sin(x)**2) == S(1)/2 - cos(2*x)/2\n    assert sincos_to_sum(cos(x)**3 * sin(x)**2) == (3*cos(x)/4 + cos(3*x)/4)*(S(1)/2 - cos(2*x)/2)\n    assert sincos_to_sum(sin(x)**4 * cos(x)**4) == (3/8 - 3*cos(2*x)/8 + cos(4*x)/16)*(3/8 - 3*cos(2*x)/8 + cos(4*x)/16)\n    assert sincos_to_sum(cos(2*x)**2 * sin(3*x)**3) == (cos(4*x)/2 + S(1)/2) * (3*sin(3*x)/4 - sin(9*x)/4)\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n    assert TR22(cot(x)**6) == (csc(x)**2 - 1)**3\n    assert TR22(1/tan(x)**2 + 1) == csc(x)**2 - 1 + 1\n", "def test_sincos_to_sum():\n    assert sincos_to_sum(sin(x)**6) == -15*cos(2*x)/32 + 3*cos(4*x)/16 - cos(6*x)/32 + S(5)/16\n    assert sincos_to_sum(cos(x)**3*sin(2*x)**2) == (3*cos(x)/4 + cos(3*x)/4)*(1 - cos(4*x)/2)\n    assert sincos_to_sum(sin(x)*cos(x)) == sin(2*x)/2\n    assert sincos_to_sum(sin(x)**2 * cos(x)**2) == -cos(4*x)/8 + S(1)/8\n    assert sincos_to_sum(sin(x)**4 * cos(x)**2) == 3*cos(2*x)/16 - cos(6*x)/32 + S(5)/32\n    assert sincos_to_sum(cos(x)**5 * sin(x)**3) == (-5*sin(2*x)/32 + sin(8*x)/64 + S(5)/64)*(3*cos(x)/4 + cos(3*x)/4)\n", "def test_sincos_to_sum():\n    assert sincos_to_sum(sin(x)**4) == 3*cos(2*x)/8 - cos(4*x)/8 + 3/8\n    assert sincos_to_sum(cos(x)**4) == 3*cos(4*x)/8 + cos(2*x)/2 + 3/8\n    assert sincos_to_sum(sin(x)**2*cos(x)**2) == -cos(4*x)/8 + 1/8\n    assert sincos_to_sum(sin(x)*cos(x)**3) == -cos(3*x)/4 + cos(x)/4\n    assert sincos_to_sum(sin(x)**3*cos(x)) == -cos(3*x)/4 + cos(x)/4\n    assert sincos_to_sum(cos(x)**6) == (5*cos(6*x) + 15*cos(4*x) + 15*cos(2*x) + 21)/32\n"], "sample_1063": ["def test_lambdify_with_custom_printer():\n    class CustomPrinter(LambdaPrinter):\n            if expr.exp == 2:\n                return f\"{self._print(expr.base)}**2\"\n            return super()._print_Pow(expr)\n\n    expr = x**2 + y\n    custom_printer = CustomPrinter()\n    func = lambdify([x, y], expr, printer=custom_printer)\n\n    assert func(2, 3) == 7  # 2**2 + 3 = 4 + 3 = 7\n    assert func(3, 4) == 13  # 3**2 + 4 = 9 + 4 = 13\n\n    # Ensure the custom printer's output is used in the function source\n    assert \"x**2\" in inspect.getsource(func)\n", "def test_lambdify_with_multiple_modules():\n    if not numpy:\n        skip(\"numpy not installed.\")\n    if not scipy:\n        skip(\"scipy not installed.\")\n\n    f = lambdify(x, sin(x) + cos(x), modules=[\"numpy\", \"scipy\", \"math\"])\n    assert abs(f(0.5) - (numpy.sin(0.5) + numpy.cos(0.5))) < 1e-10\n\n    g = lambdify((x, y), sin(x) * gamma(y), modules=[\"math\", \"scipy\"])\n    assert abs(g(0.5, 2) - (math.sin(0.5) * scipy.special.gamma(2))) < 1e-10\n\n    h = lambdify(x, Abs(x), modules=[\"math\", \"numpy\", \"mpmath\"])\n    assert h(-2) == 2\n    assert h(2) == 2\n", "def test_lambdastr():\n    # Test that lambdastr returns a correct string representation of a lambda function\n    expr = x**2 + y\n    expected_str = 'lambda x,y: (x**2 + y)'\n    assert lambdastr((x, y), expr) == expected_str\n\n    expr_nested = x + y + z\n    expected_str_nested = 'lambda x,y,z: (x + y + z)'\n    assert lambdastr((x, y, z), expr_nested) == expected_str_nested\n\n    # Test with nested tuples\n    expr_nested_tuples = x + y[0] + y[1]\n    expected_str_nested_tuples = 'lambda x,_0: (lambda x,y_0,y_1: (x + y_0 + y_1))(x,_0[0],_0[1])'\n    assert lambdastr((x, (y[0], y[1])), expr_nested_tuples) == expected_str_nested_tuples\n", "def test_custom_function_translation():\n        return math.log(x, 2)\n\n    custom_module = {'log': custom_log}\n    expr = log(x, 2)\n    f = lambdify(x, expr, modules=custom_module)\n    assert f(8) == 3\n    assert f(16) == 4\n", "def test_custom_functions():\n    # Test lambdify with custom functions that are not part of default modules\n        return x**3 + 2*x + 1\n\n        return 2**y + 3\n\n    custom_namespace = {\n        'custom_func1': custom_func1,\n        'custom_func2': custom_func2\n    }\n\n    f = lambdify([x, y], custom_func1(x) + custom_func2(y), modules=custom_namespace)\n    assert f(2, 3) == custom_func1(2) + custom_func2(3)  # Expected: 2**3 + 2*2 + 1 + 2**3 + 3\n"], "sample_1064": ["def test_tensorflow_piecewise():\n    from sympy import Piecewise\n    if not tf:\n        skip(\"TensorFlow not installed\")\n    \n    expr = Piecewise((x, x < 1), (x**2, x > 1))\n    assert tensorflow_code(expr, tensorflow_version='1.0') == \\\n        \"tensorflow.where(tensorflow.math.less(x, 1), x, tensorflow.where(tensorflow.math.greater(x, 1), tensorflow.math.pow(x, 2), 0))\"\n    \n    expr = Piecewise((x, x < 1), (x**2, x > 1), (0, True))\n    assert tensorflow_code(expr, tensorflow_version='1.0') == \\\n        \"tensorflow.where(tensorflow.math.less(x, 1), x, tensorflow.where(tensorflow.math.greater(x, 1), tensorflow.math.pow(x, 2), 0))\"\n\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.uniform(-2, 2))\n\n    expr = Piecewise((x + y, x < y), (x - y, x >= y))\n    assert tensorflow_code(expr, tensorflow_version='1.0') == \\\n        \"tensorflow.where(tensorflow.math.less(x, y), tensorflow.math.add(x, y), tensorflow.where(tensorflow.math.greater_equal(x, y), tensorflow.math.subtract(x, y), 0))\"\n\n    _compare_tensorflow_scalar((x, y), expr, rng=lambda: random.uniform(-2, 2))\n", "def test_tensorflow_Piecewise():\n    from sympy import Piecewise\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Piecewise((x + 1, x > 0), (x - 1, True))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        \"tensorflow.where(tensorflow.math.greater(x, 0), tensorflow.math.add(x, 1), tensorflow.math.subtract(x, 1))\"\n    _compare_tensorflow_scalar((x,), expr)\n\n    expr = Piecewise((x + 1, x > 0), (x - 1, x < 0), (0, True))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        \"tensorflow.where(tensorflow.math.greater(x, 0), tensorflow.math.add(x, 1), tensorflow.where(tensorflow.math.less(x, 0), tensorflow.math.subtract(x, 1), 0))\"\n    _compare_tensorflow_scalar((x,), expr)\n\n    expr = Piecewise((x**2, x > 1), (x, x <= 1))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        \"tensorflow.where(tensorflow.math.greater(x, 1), tensorflow.math.pow(x, 2), x)\"\n    _compare_tensorflow_scalar((x,), expr)\n", "def test_tensorflow_piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    from sympy import Piecewise\n\n    expr = Piecewise((x**2, x < 0), (x, True))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        'tensorflow.where(tensorflow.math.less(x, 0), tensorflow.math.pow(x, 2), x)'\n\n    expr = Piecewise((x**2, x < 0), (x, x >= 0))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        'tensorflow.where(tensorflow.math.less(x, 0), tensorflow.math.pow(x, 2), tensorflow.where(tensorflow.math.greater_equal(x, 0), x, 0))'\n\n    expr = Piecewise((x**2, x < 0), (x, True))\n    assert tensorflow_code(expr, tensorflow_version='0.12') == \\\n        'tensorflow.select(tensorflow.math.less(x, 0), tensorflow.math.pow(x, 2), x)'\n\n    _compare_tensorflow_scalar((x,), Piecewise((x**2, x < 0), (x, True)), rng=lambda: random.uniform(-1, 1))\n    _compare_tensorflow_scalar((x,), Piecewise((x**2, x < 0), (x, x >= 0)), rng=lambda: random.uniform(-1, 1))\n", "def test_tensorflow_codegen_piecewise():\n    from sympy import Piecewise\n    if not tf:\n        skip(\"TensorFlow not installed\")\n        \n    expr = Piecewise((x**2, x > 0), (0, True))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        'tensorflow.where(tensorflow.math.greater(x, 0), tensorflow.math.pow(x, 2), 0)'\n\n    expr = Piecewise((x**2, x > 0), (x, x < 0), (0, True))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        'tensorflow.where(tensorflow.math.greater(x, 0), tensorflow.math.pow(x, 2), tensorflow.where(tensorflow.math.less(x, 0), x, 0))'\n\n    expr = Piecewise((x**2, x > 0), (0, True))\n    assert tensorflow_code(expr, tensorflow_version='0.12') == \\\n        'tensorflow.select(tensorflow.math.greater(x, 0), tensorflow.math.pow(x, 2), 0)'\n        \n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.uniform(-1, 1))\n", "def test_tensorflow_Piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    from sympy import Piecewise\n\n    expr = Piecewise((x**2, x < 1), (x, True))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        'tensorflow.where(tensorflow.math.less(x, 1), tensorflow.math.pow(x, 2), x)'\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.uniform(-2, 2))\n\n    expr = Piecewise((x**2, x < 1), (x**3, x < 2), (x, True))\n    assert tensorflow_code(expr, tensorflow_version='1.14') == \\\n        'tensorflow.where(tensorflow.math.less(x, 1), tensorflow.math.pow(x, 2), ' \\\n        'tensorflow.where(tensorflow.math.less(x, 2), tensorflow.math.pow(x, 3), x))'\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.uniform(-2, 3))\n"], "sample_1065": ["def test_factorial2_diff():\n    n = Symbol('n', integer=True)\n\n    assert factorial2(n).diff(n) == \\\n        Piecewise((0, Eq(Mod(n, 2), 0)), (factorial2(n - 1)*(1 - log(n) - polygamma(0, (n + 1)/2)), Eq(Mod(n, 2), 1)))\n    raises(ArgumentIndexError, lambda: factorial2(n).fdiff(2))\n", "def test_factorial_properties():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    x = Symbol('x')\n\n    assert factorial(n).rewrite(Product) == Product(Dummy('i'), (Dummy('i'), 1, n))\n    assert factorial(k).rewrite(Product) == Product(Dummy('i'), (Dummy('i'), 1, k))\n\n    assert factorial(n + 1).expand(func=True) == (n + 1) * factorial(n)\n    assert factorial(n + 2).expand(func=True) == (n + 2) * (n + 1) * factorial(n)\n    assert factorial(n - 1).expand(func=True) == n * factorial(n - 1) / n\n\n    assert factorial(n).rewrite(gamma) == gamma(n + 1)\n\n    # Evaluate for large inputs\n    assert factorial(50) == 30414093201713378043612608166064768844377641568960512000000000000\n\n    # Evaluate for symbolic expressions\n    assert factorial(n).rewrite(ff) == ff(n, n)\n\n    # Check consistency with other combinatorial functions\n    assert factorial(n).rewrite(RisingFactorial) == RisingFactorial(n, n)\n    assert factorial(n).rewrite(FallingFactorial) == FallingFactorial(n, n)\n\n    # Ensure properties are maintained\n    assert factorial(n).is_integer is None\n    assert factorial(k).is_integer\n    assert factorial(x).is_integer is None\n\n    # Check nonnegative property\n    assert factorial(n).is_nonnegative is None\n    assert factorial(k).is_nonnegative is True\n\n    # Check even/odd property\n    assert factorial(n).is_even is None\n    assert factorial(n).is_odd is None\n    assert factorial(k).is_even is None\n    assert factorial(k).is_odd is None\n", "def test_subfactorial_properties():\n    n = Symbol('n', integer=True)\n    m = Symbol('m', integer=True, nonnegative=True)\n    \n    # Test subfactorial rewrite as uppergamma\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n\n    # Test subfactorial with specific values\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n    assert subfactorial(10) == 133496\n\n    # Test subfactorial with symbolic expressions\n    assert subfactorial(n + 1) == subfactorial(n + 1)\n    assert subfactorial(2*n) == subfactorial(2*n)\n\n    # Test properties of subfactorial\n    assert subfactorial(n).is_integer == (n.is_integer and n.is_nonnegative)\n    assert subfactorial(m).is_integer is True\n    assert subfactorial(n).is_nonnegative == (n.is_integer and n.is_nonnegative)\n    assert subfactorial(m).is_nonnegative is True\n\n    # Test subfactorial is even/odd\n    assert subfactorial(n).is_even == (n.is_integer and n.is_nonnegative and n % 2 == 1)\n    assert subfactorial(n).is_odd == (n.is_integer and n.is_nonnegative and n % 2 == 0)\n    assert subfactorial(m).is_even == (m % 2 == 1)\n    assert subfactorial(m).is_odd == (m % 2 == 0)\n", "def test_multifactorial():\n    from sympy import factorial2, factorial, Symbol\n    n = Symbol('n', integer=True)\n\n    assert MultiFactorial(n).func == MultiFactorial\n    assert MultiFactorial(n).rewrite(factorial) == MultiFactorial(n)\n    assert MultiFactorial(n).rewrite(factorial2) == MultiFactorial(n)\n\n    # Since MultiFactorial is not implemented, these are just basic structure tests\n    assert MultiFactorial(0).is_integer is None\n    assert MultiFactorial(1).is_integer is None\n    assert MultiFactorial(-1).is_integer is None\n    assert MultiFactorial(2).is_integer is None\n    assert MultiFactorial(n).is_integer is None\n\n    assert MultiFactorial(3).is_positive is None\n    assert MultiFactorial(n).is_positive is None\n\n    assert MultiFactorial(4).is_even is None\n    assert MultiFactorial(n).is_even is None\n\n    assert MultiFactorial(5).is_odd is None\n    assert MultiFactorial(n).is_odd is None\n\n    assert MultiFactorial(n)._eval_is_integer() is None\n    assert MultiFactorial(n)._eval_is_positive() is None\n    assert MultiFactorial(n)._eval_is_even() is None\n    assert MultiFactorial(n)._eval_is_odd() is None\n\n    assert MultiFactorial(n).rewrite(factorial) == MultiFactorial(n)\n    assert MultiFactorial(n).rewrite(factorial2) == MultiFactorial(n)\n", "def test_multifactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n\n    assert isinstance(MultiFactorial(), CombinatorialFunction)\n\n    # Check if MultiFactorial can be instantiated without errors\n    m_fact = MultiFactorial()\n    assert m_fact\n\n    # Ensure there are no existing implemented methods by default\n    assert not hasattr(m_fact, 'eval')\n    assert not hasattr(m_fact, '_eval_is_even')\n    assert not hasattr(m_fact, '_eval_is_integer')\n    assert not hasattr(m_fact, '_eval_is_odd')\n    assert not hasattr(m_fact, '_eval_is_positive')\n\n    # Check if it raises NotImplementedError when methods are not implemented\n    raises(NotImplementedError, lambda: m_fact.eval(x))\n    raises(NotImplementedError, lambda: m_fact._eval_is_even())\n    raises(NotImplementedError, lambda: m_fact._eval_is_integer())\n    raises(NotImplementedError, lambda: m_fact._eval_is_odd())\n    raises(NotImplementedError, lambda: m_fact._eval_is_positive())\n"], "sample_1066": ["def test_print_basic_special_symbols():\n    expr = Basic(Symbol(\"theta\"), Symbol(\"phi\"))\n    assert mpp.doprint(expr) == '<mrow><mi>basic</mi><mfenced><mi>&#952;</mi><mi>&#966;</mi></mfenced></mrow>'\n    assert mp.doprint(expr) == '<basic><ci>&#952;</ci><ci>&#966;</ci></basic>'\n", "def test_print_MatMul():\n    from sympy.matrices.expressions import MatMul\n\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n\n    assert mathml(MatMul(X, Y), printer='presentation') == \\\n        '<mrow><mi>X</mi><mo>&InvisibleTimes;</mo><mi>Y</mi></mrow>'\n    assert mathml(MatMul(A, B, evaluate=False), printer='presentation') == \\\n        '<mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi></mrow>'\n    assert mathml(MatMul(A, B, X, Y, evaluate=False), printer='presentation') == \\\n        '<mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi><mo>&InvisibleTimes;</mo><mi>X</mi><mo>&InvisibleTimes;</mo><mi>Y</mi></mrow>'\n", "def test_print_hadamard_and_matrix_functions():\n    from sympy.matrices.expressions import HadamardProduct, Transpose, Adjoint, Inverse\n    from sympy.matrices import Identity\n\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n\n    assert mathml(HadamardProduct(X, Y), printer=\"content\") == '<apply><times/><ci>X</ci><ci>Y</ci></apply>'\n    assert mathml(Transpose(HadamardProduct(X, Y)), printer=\"content\") == \\\n        '<apply><transpose/><apply><times/><ci>X</ci><ci>Y</ci></apply></apply>'\n    assert mathml(Adjoint(HadamardProduct(X, Y)), printer=\"content\") == \\\n        '<apply><adjoint/><apply><times/><ci>X</ci><ci>Y</ci></apply></apply>'\n    assert mathml(Inverse(HadamardProduct(X, Y)), printer=\"content\") == \\\n        '<apply><inverse/><apply><times/><ci>X</ci><ci>Y</ci></apply></apply>'\n\n    assert mathml(Identity(3), printer='content') == \\\n        '<apply><identitymatrix/><ci>3</ci></apply>'\n", "def test_presentation_mathml_exponents():\n    expr = x**y\n    assert mpp.doprint(expr) == '<msup><mi>x</mi><mi>y</mi></msup>'\n    expr = 2**x\n    assert mpp.doprint(expr) == '<msup><mn>2</mn><mi>x</mi></msup>'\n    expr = (x + y)**z\n    assert mpp.doprint(expr) == '<msup><mfenced><mrow><mi>x</mi><mo>+</mo><mi>y</mi></mrow></mfenced><mi>z</mi></msup>'\n    expr = (x * y)**z\n    assert mpp.doprint(expr) == '<msup><mfenced><mrow><mi>x</mi><mo>&InvisibleTimes;</mo><mi>y</mi></mrow></mfenced><mi>z</mi></msup>'\n    expr = (2*x)**3\n    assert mpp.doprint(expr) == '<msup><mfenced><mrow><mn>2</mn><mo>&InvisibleTimes;</mo><mi>x</mi></mrow></mfenced><mn>3</mn></msup>'\n    expr = 2**(x + 1)\n    assert mpp.doprint(expr) == '<msup><mn>2</mn><mfenced><mrow><mi>x</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msup>'\n    expr = (x - y)**2\n    assert mpp.doprint(expr) == '<msup><mfenced><mrow><mi>x</mi><mo>-</mo><mi>y</mi></mrow></mfenced><mn>2</mn></msup>'\n", "def test_mathml_apply_patch_restore_patch():\n    # Testing if apply_patch and restore_patch methods work correctly\n    import xml.dom.minidom\n\n    # Backup original methods\n    original_Element_writexml = xml.dom.minidom.Element.writexml\n    original_Text_writexml = xml.dom.minidom.Text.writexml\n\n    mp.apply_patch()\n    \n    # Check if the patch has been applied\n    assert xml.dom.minidom.Element.writexml != original_Element_writexml\n    assert xml.dom.minidom.Text.writexml != original_Text_writexml\n\n    mp.restore_patch()\n    \n    # Check if the patch has been restored to original\n    assert xml.dom.minidom.Element.writexml == original_Element_writexml\n    assert xml.dom.minidom.Text.writexml == original_Text_writexml\n"], "sample_1067": ["def test_issue_unevaluated_Mul():\n    x, y = symbols('x y')\n    a, b = symbols('a b', cls=Wild)\n\n    e = _unevaluated_Mul(x, y)\n    assert e.match(a * b) == {a: x, b: y}\n    assert e == Mul(x, y, evaluate=False)\n\n    e = _unevaluated_Mul(2, x, 3)\n    assert e.match(a * b) == {a: 6, b: x}\n    assert e == Mul(6, x, evaluate=False)\n\n    e = _unevaluated_Mul(S(3.0), x, S(2))\n    assert e.match(a * b) == {a: 6.0, b: x}\n    assert e == Mul(6.0, x, evaluate=False)\n\n    e = _unevaluated_Mul(x, S(2), S(3.0))\n    assert e.match(a * b) == {a: 6.0, b: x}\n    assert e == Mul(6.0, x, evaluate=False)\n\n    e = _unevaluated_Mul(S(2), S(3.0))\n    assert e == Mul(6.0, evaluate=False)\n", "def test_unevaluated_Mul():\n    from sympy import S, sqrt, Mul\n    from sympy.abc import x\n\n    a = _unevaluated_Mul(S(3.0), x, S(2))\n    assert a.args[0] == 6.0\n    assert a.args[1] == x\n\n    m = _unevaluated_Mul(sqrt(2), sqrt(3))\n    assert m == _unevaluated_Mul(sqrt(3), sqrt(2))\n\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m == _unevaluated_Mul(u)\n    assert m != Mul(*m.args)\n", "def test_unevaluated_Mul():\n    x, y = symbols('x y')\n    \n    # Test with numbers and symbols\n    um = _unevaluated_Mul(3.0, x, 2)\n    assert um.args[0] == 6.0\n    assert um.args[1] == x\n    \n    # Test with commutative arguments\n    um = _unevaluated_Mul(x, y, 3)\n    assert um.args[0] == 3\n    assert um.args[1] == x\n    assert um.args[2] == y\n    \n    # Test with non-commutative arguments\n    A, B = symbols('A B', commutative=False)\n    um = _unevaluated_Mul(2, A, B)\n    assert um.args[0] == 2\n    assert um.args[1] == A * B\n    \n    # Test with nested unevaluated Mul\n    sqrt2, sqrt3 = sqrt(2), sqrt(3)\n    m = _unevaluated_Mul(sqrt2, sqrt3)\n    assert m == _unevaluated_Mul(sqrt3, sqrt2)\n    assert m == _unevaluated_Mul(Mul(sqrt3, sqrt2, evaluate=False))\n    assert m != Mul(sqrt3, sqrt2)\n", "def test_unevaluated_Mul():\n    from sympy.abc import x, y\n    from sympy import S, sqrt, Mul\n\n    a = _unevaluated_Mul(*[S(3.0), x, S(2)])\n    assert a.args[0] == 6.00000000000000\n    assert a.args[1] == x\n\n    m = _unevaluated_Mul(sqrt(2), sqrt(3))\n    assert m == _unevaluated_Mul(sqrt(3), sqrt(2))\n\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m == _unevaluated_Mul(u)\n    assert m != Mul(*m.args)\n", "def test_unevaluated_Mul():\n    from sympy.abc import x, y, z\n    from sympy import S, sqrt\n\n    # Testing the basic functionality of _unevaluated_Mul\n    a = _unevaluated_Mul(*[S(3.0), x, S(2)])\n    assert a.args[0] == S(6.0)\n    assert a.args[1] == x\n\n    # Testing equality of unevaluated Muls with the same arguments\n    m = _unevaluated_Mul(sqrt(2), sqrt(3))\n    assert m == _unevaluated_Mul(sqrt(3), sqrt(2))\n\n    # Testing equality of unevaluated Mul with evaluated Mul\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m == _unevaluated_Mul(u)\n    assert m != Mul(*m.args)\n\n    # Testing nested unevaluated Muls\n    n = _unevaluated_Mul(_unevaluated_Mul(x, y), z)\n    assert n == _unevaluated_Mul(x, y, z)\n"], "sample_1068": ["def test_user_functions():\n    f = Function('f')\n    g = Function('g')\n    user_functions = {\n        \"f\": \"custom_f\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_g_matrix\"),\n              (lambda x: not x.is_Matrix, \"custom_g\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=user_functions) == \\\n        'custom_f(x) + custom_g(x) + custom_g_matrix([1 x])'\n", "def test_user_defined_functions():\n    from sympy import Function\n    from sympy.utilities.lambdify import implemented_function\n\n    f = Function('f')\n    g = implemented_function('g', Lambda(x, x**2 + 1))\n\n    # Custom function not defined in known functions\n    assert mcode(f(x)) == \"% Not supported in Octave:\\n% f\\nf(x)\"\n    \n    # Custom user defined function passed via user_functions\n    custom_functions = {\n        \"f\": \"user_defined_function\"\n    }\n    assert mcode(f(x), user_functions=custom_functions) == \"user_defined_function(x)\"\n\n    # Custom implemented function should be printed as expected\n    assert mcode(g(x)) == \"x.^2 + 1\"\n    \n    # Custom user defined function with condition\n    custom_functions_with_condition = {\n        \"h\": [(lambda x: x.is_Matrix, \"matrix_function\"),\n              (lambda x: not x.is_Matrix, \"scalar_function\")]\n    }\n    h = Function('h')\n    A = Matrix([[1, 2], [3, 4]])\n    assert mcode(h(A), user_functions=custom_functions_with_condition) == \"matrix_function([1 2; 3 4])\"\n    assert mcode(h(x), user_functions=custom_functions_with_condition) == \"scalar_function(x)\"\n", "def test_Assignment():\n    assert mcode(Assignment(x, y + 1)) == \"x = y + 1;\"\n    assert mcode(Assignment(x, y * sin(z))) == \"x = y.*sin(z);\"\n    assert mcode(Assignment(x, y / (z + 1))) == \"x = y./(z + 1);\"\n    assert mcode(Assignment(x, x**2 + y**2)) == \"x = x.^2 + y.^2;\"\n    assert mcode(Assignment(x, Piecewise((x + 1, x < 1), (x**2, True)), inline=False)) == (\n        \"if (x < 1)\\n\"\n        \"  x = x + 1;\\n\"\n        \"else\\n\"\n        \"  x = x.^2;\\n\"\n        \"end\"\n    )\n", "def test_piecewise_undefined_condition():\n    expr = Piecewise((x, x < 1), (x**2, x > 1), (sin(x), x > 0), (log(x), True))\n    assert mcode(expr) == (\n        \"((x < 1).*(x) + (~(x < 1)).*( ...\\n\"\n        \"(x > 1).*(x.^2) + (~(x > 1)).*( ...\\n\"\n        \"(x > 0).*(sin(x)) + (~(x > 0)).*(log(x)))))\"\n    )\n    assert mcode(expr, assign_to=\"r\") == (\n        \"r = ((x < 1).*(x) + (~(x < 1)).*( ...\\n\"\n        \"(x > 1).*(x.^2) + (~(x > 1)).*( ...\\n\"\n        \"(x > 0).*(sin(x)) + (~(x > 0)).*(log(x)))));\"\n    )\n    assert mcode(expr, assign_to=\"r\", inline=False) == (\n        \"if (x < 1)\\n\"\n        \"  r = x;\\n\"\n        \"elseif (x > 1)\\n\"\n        \"  r = x.^2;\\n\"\n        \"elseif (x > 0)\\n\"\n        \"  r = sin(x);\\n\"\n        \"else\\n\"\n        \"  r = log(x);\\n\"\n        \"end\"\n    )\n", "def test_unsupported_functions():\n    from sympy.functions.special.hyper import hyper\n    from sympy.functions.special.mathieu import MathieuC\n    expr1 = hyper([], [], x)\n    expr2 = MathieuC(x, y)\n    assert mcode(expr1) == (\n        \"% Not supported in Octave:\\n\"\n        \"% hyper\\n\"\n        \"hyper([], [], x)\"\n    )\n    assert mcode(expr2) == (\n        \"% Not supported in Octave:\\n\"\n        \"% MathieuC\\n\"\n        \"MathieuC(x, y)\"\n    )\n"], "sample_1069": ["def test_GLSLPrinter_basic_operations():\n    from sympy import Add, Mul\n    from sympy.printing.glsl import glsl_code\n\n    assert glsl_code(Add(x, y)) == \"x + y\"\n    assert glsl_code(Mul(x, y)) == \"x * y\"\n    assert glsl_code(Mul(2, x, y)) == \"2 * x * y\"\n    assert glsl_code(Add(x, -y)) == \"x - y\"\n    assert glsl_code(Mul(-1, x)) == \"-x\"\n    assert glsl_code(Mul(x, Add(y, z))) == \"x * (y + z)\"\n", "def test_glsl_code():\n    # Test basic integer and float\n    assert glsl_code(Integer(67)) == \"67.0\"\n    assert glsl_code(Integer(-1)) == \"-1.0\"\n    assert glsl_code(3.7) == \"3.7\"\n\n    # Test Rational\n    assert glsl_code(Rational(3, 7)) == \"3.0/7.0\"\n    assert glsl_code(Rational(18, 9)) == \"2.0\"\n    assert glsl_code(Rational(3, -7)) == \"-3.0/7.0\"\n    assert glsl_code(Rational(-3, -7)) == \"3.0/7.0\"\n\n    # Test Relational\n    assert glsl_code(Eq(x, y)) == \"x == y\"\n    assert glsl_code(Ne(x, y)) == \"x != y\"\n    assert glsl_code(Le(x, y)) == \"x <= y\"\n    assert glsl_code(Lt(x, y)) == \"x < y\"\n    assert glsl_code(Gt(x, y)) == \"x > y\"\n    assert glsl_code(Ge(x, y)) == \"x >= y\"\n\n    # Test Functions\n    assert glsl_code(sin(x)) == \"sin(x)\"\n    assert glsl_code(cos(x)) == \"cos(x)\"\n    assert glsl_code(tan(x)) == \"tan(x)\"\n    assert glsl_code(log(x)) == \"log(x)\"\n    assert glsl_code(exp(x)) == \"exp(x)\"\n    assert glsl_code(ceiling(x)) == \"ceil(x)\"\n    assert glsl_code(floor(x)) == \"floor(x)\"\n    assert glsl_code(sign(x)) == \"sign(x)\"\n\n    # Test Power function\n    assert glsl_code(x**2) == \"pow(x, 2.0)\"\n    assert glsl_code(x**0.5) == \"sqrt(x)\"\n    assert glsl_code(1/x) == \"1.0/x\"\n\n    # Test Piecewise\n    expr = Piecewise((x, x < 1), (x**2, True))\n    assert glsl_code(expr) == (\n        \"if (x < 1) {\\n\"\n        \"   x\\n\"\n        \"}\\n\"\n        \"else {\\n\"\n        \"   pow(x, 2.0)\\n", "def test_GLSL_Integer():\n    assert glsl_code(Integer(42)) == \"float 42.0;\"\n    assert glsl_code(Integer(-7)) == \"float -7.0;\"\n", "def test_glsl_code_Integer():\n    assert glsl_code(Integer(67)) == \"67.0\"\n    assert glsl_code(Integer(-1)) == \"-1.0\"\n", "def test_GLSL_basic_ops():\n    assert glsl_code(x*y) == \"x*y\"\n    assert glsl_code(x + y) == \"x + y\"\n    assert glsl_code(x - y) == \"x - y\"\n    assert glsl_code(-x) == \"-x\"\n"], "sample_1070": ["def test_log_as_real_imag():\n    assert log(2).as_real_imag() == (log(2), 0)\n    assert log(-2).as_real_imag() == (log(2), pi*I)\n    assert log(1 + I).as_real_imag() == (log(sqrt(2)), pi/4)\n    assert log(-1 + I).as_real_imag() == (log(sqrt(2)), 3*pi/4)\n    assert log(1 - I).as_real_imag() == (log(sqrt(2)), -pi/4)\n    assert log(-1 - I).as_real_imag() == (log(sqrt(2)), -3*pi/4)\n", "def test_exp_as_real_imag():\n    assert exp(I).as_real_imag() == (cos(1), sin(1))\n    assert exp(1).as_real_imag() == (E, 0)\n    assert exp(1 + I).as_real_imag() == (E*cos(1), E*sin(1))\n    assert exp(2 + 3*I).as_real_imag() == (exp(2)*cos(3), exp(2)*sin(3))\n    assert exp(I*pi/2).as_real_imag() == (cos(pi/2), sin(pi/2))\n", "def test_exp_eval():\n    assert exp(S.NaN).evalf() == S.NaN\n    assert exp(S.Zero).evalf() == S.One\n    assert exp(S.One).evalf() == S.Exp1\n    assert exp(S.Infinity).evalf() == S.Infinity\n    assert exp(S.NegativeInfinity).evalf() == S.Zero\n    assert exp(log(S.Zero)).evalf() == S.Zero\n    assert exp(log(S.One)).evalf() == S.One\n    assert exp(log(S.Infinity)).evalf() == S.Infinity\n    assert exp(log(S.NegativeInfinity)).evalf() == S.Zero\n    assert exp(log(S.NaN)).evalf() == S.NaN\n    assert exp(log(S.ComplexInfinity)).evalf() == S.NaN\n", "def test_exp_properties():\n    assert exp(x).is_finite is None\n    assert exp(-x).is_finite is None\n    assert exp(1).is_finite is False\n    assert exp(oo).is_finite is False\n    assert exp(-oo).is_finite is True\n\n    assert exp(x).is_zero is False\n    assert exp(0).is_zero is False\n    assert exp(-oo).is_zero is True\n\n    assert exp(x).is_rational is None\n    assert exp(0).is_rational is True\n    assert exp(1).is_rational is False\n\n    assert exp(x).is_extended_real is None\n    assert exp(1).is_extended_real is True\n    assert exp(I*pi).is_extended_real is True\n    assert exp(2 + I*pi).is_extended_real is False\n", "def test_LambertW_values():\n    # Basic known values and their corresponding results\n    assert LambertW(0) == 0\n    assert LambertW(1).evalf() == LambertW(1).evalf()\n    assert LambertW(0, 1) == -oo\n    assert LambertW(-1/E) == -1\n    assert LambertW(-1/E, -1) == -1\n    assert LambertW(-2*exp(-2), -1) == -2\n    assert LambertW(E) == 1\n    assert LambertW(-pi/2, -1) == -I*pi/2\n\n    # Check if LambertW of a known positive value returns a real value\n    assert LambertW(1).is_real is True\n\n    # Test for branches\n    assert LambertW(1, k=-1).is_real is False\n    assert LambertW(-1/E, k=0) == -1\n    assert LambertW(-1/E, k=-1) == -1\n\n    # Ensuring the function handles infinite input correctly\n    assert LambertW(oo) == oo\n    assert LambertW(-oo) == -oo\n\n    # Check handling of special cases with complex numbers\n    assert LambertW(-1) == LambertW(-1, 0)\n    assert LambertW(-1 + I) == LambertW(-1 + I, 0)\n    assert LambertW(-1 - I) == LambertW(-1 - I, 0)\n\n    # Check differentiation\n    assert LambertW(x).diff(x) == LambertW(x) / (x * (1 + LambertW(x)))\n    assert LambertW(x, k).diff(x) == LambertW(x, k) / (x * (1 + LambertW(x, k)))\n\n    # Check if the algebraic check works correctly\n    assert LambertW(1).is_algebraic is False\n    assert LambertW(0).is_algebraic is True\n    assert LambertW(-1/E).is_algebraic is True\n\n    # Check various properties\n    assert LambertW(2).is_real\n    p = Symbol('p', positive=True)\n    assert LambertW(p).is_real\n    assert LambertW(p - 1).is_real is None\n    assert LambertW(-p - 2/S.Exp1).is_real is False\n    assert LambertW(S.Half, -1"], "sample_1071": ["def test_get_conversion_matrix_for_expr():\n    from sympy.physics.units import joule, meter, kilogram, second, watt\n\n    expr = joule\n    target_units = [meter, kilogram, second]\n    expected = [1, 1, -2]\n\n    matrix = _get_conversion_matrix_for_expr(expr, target_units)\n    assert matrix == expected\n\n    expr = watt\n    target_units = [meter, kilogram, second]\n    expected = [2, 1, -3]\n\n    matrix = _get_conversion_matrix_for_expr(expr, target_units)\n    assert matrix == expected\n", "def test_get_conversion_matrix_for_expr():\n    from sympy import Matrix\n    from sympy.physics.units import meter, second, kilogram, joule\n    from sympy.physics.units.util import _get_conversion_matrix_for_expr\n\n    # Test conversion matrix for single unit\n    expr = joule\n    target_units = [meter, second, kilogram]\n    expected_matrix = Matrix([[-2], [2], [1]])\n    result_matrix = _get_conversion_matrix_for_expr(expr, target_units)\n    assert result_matrix == expected_matrix\n\n    # Test conversion matrix for incompatible units\n    expr = meter\n    target_units = [second, kilogram]\n    result_matrix = _get_conversion_matrix_for_expr(expr, target_units)\n    assert result_matrix is None\n\n    # Test conversion matrix for compatible units\n    expr = meter * second\n    target_units = [meter, second]\n    expected_matrix = Matrix([[1], [1]])\n    result_matrix = _get_conversion_matrix_for_expr(expr, target_units)\n    assert result_matrix == expected_matrix\n\n    # Test conversion matrix with dimensionless unit\n    expr = meter / second\n    target_units = [meter, second]\n    expected_matrix = Matrix([[1], [-1]])\n    result_matrix = _get_conversion_matrix_for_expr(expr, target_units)\n    assert result_matrix == expected_matrix\n", "def test_get_conversion_matrix_for_expr():\n    from sympy import Matrix\n    from sympy.physics.units import meter, second, kilogram\n\n    expr = 3 * meter * second**2 / kilogram\n    target_units = [meter, second, kilogram]\n    conversion_matrix = _get_conversion_matrix_for_expr(expr, target_units)\n\n    # Construct expected matrix manually for comparison\n    expected_matrix = Matrix([1, 2, -1])\n    assert conversion_matrix == expected_matrix\n\n    expr = meter / second\n    target_units = [meter, second]\n    conversion_matrix = _get_conversion_matrix_for_expr(expr, target_units)\n\n    expected_matrix = Matrix([1, -1])\n    assert conversion_matrix == expected_matrix\n\n    expr = meter**2 / kilogram\n    target_units = [meter, kilogram]\n    conversion_matrix = _get_conversion_matrix_for_expr(expr, target_units)\n\n    expected_matrix = Matrix([2, -1])\n    assert conversion_matrix == expected_matrix\n\n    # Test with incompatible dimensions\n    expr = meter * second\n    target_units = [meter, kilogram]\n    conversion_matrix = _get_conversion_matrix_for_expr(expr, target_units)\n    assert conversion_matrix is None\n", "def test_convert_to_incompatible_dimensions():\n    assert convert_to(joule, meter) == joule  # Incompatible dimensions, should return the original expression\n    assert convert_to(newton, second) == newton  # Incompatible dimensions, should return the original expression\n    assert convert_to(3*joule, kilogram) == 3*joule  # Incompatible dimensions, should return the original expression\n    assert convert_to(5*meter/second, kilogram) == 5*meter/second  # Incompatible dimensions, should return the original expression\n", "def test_get_conversion_matrix_for_expr():\n    from sympy import Matrix\n    from sympy.physics.units import meter, second\n\n    expr = 3 * meter / second\n    target_units = [meter, second]\n    res = _get_conversion_matrix_for_expr(expr, target_units)\n    expected = Matrix([1, -1])\n    assert res == expected\n\n    expr = meter**2\n    target_units = [meter]\n    res = _get_conversion_matrix_for_expr(expr, target_units)\n    expected = Matrix([2])\n    assert res == expected\n\n    expr = meter * second\n    target_units = [meter, second]\n    res = _get_conversion_matrix_for_expr(expr, target_units)\n    expected = Matrix([1, 1])\n    assert res == expected\n\n    expr = meter**3\n    target_units = [meter]\n    res = _get_conversion_matrix_for_expr(expr, target_units)\n    expected = Matrix([3])\n    assert res == expected\n\n    expr = meter**3 / second**2\n    target_units = [meter, second]\n    res = _get_conversion_matrix_for_expr(expr, target_units)\n    expected = Matrix([3, -2])\n    assert res == expected\n\n    # Test with incompatible units\n    expr = meter + second\n    target_units = [meter]\n    res = _get_conversion_matrix_for_expr(expr, target_units)\n    assert res is None\n"], "sample_1072": ["def test_floor_ceiling_frac_complex():\n\n    assert floor(2 + 3*I) == 2 + 3*I\n    assert ceiling(2 + 3*I) == 3 + 3*I\n    assert frac(2 + 3*I) == 0\n\n    assert floor(-2 + 3*I) == -2 + 3*I\n    assert ceiling(-2 + 3*I) == -1 + 3*I\n    assert frac(-2 + 3*I) == 0\n\n    assert floor(2 - 3*I) == 2 - 3*I\n    assert ceiling(2 - 3*I) == 3 - 3*I\n    assert frac(2 - 3*I) == 0\n\n    assert floor(-2 - 3*I) == -2 - 3*I\n    assert ceiling(-2 - 3*I) == -1 - 3*I\n    assert frac(-2 - 3*I) == 0\n\n    assert floor(2.5 + 3.7*I) == 2 + 3*I\n    assert ceiling(2.5 + 3.7*I) == 3 + 4*I\n    assert frac(2.5 + 3.7*I) == 0.5 + 0.7*I\n\n    assert floor(-2.5 + 3.7*I) == -3 + 3*I\n    assert ceiling(-2.5 + 3.7*I) == -2 + 4*I\n    assert frac(-2.5 + 3.7*I) == 0.5 + 0.7*I\n\n    assert floor(2.5 - 3.7*I) == 2 - 4*I\n    assert ceiling(2.5 - 3.7*I) == 3 - 3*I\n    assert frac(2.5 - 3.7*I) == 0.5 + 0.3*I\n\n    assert floor(-2.5 - 3.7*I) == -3 - 4*I\n    assert ceiling(-2.5 - 3.7*I) == -2 - 3*I\n    assert frac(-2", "def test_floor_ceiling_frac_zero():\n    # Testing floor, ceiling, and frac for zero and edge cases\n    assert floor(0) == 0\n    assert ceiling(0) == 0\n    assert frac(0) == 0\n\n    assert floor(-0.0) == 0\n    assert ceiling(-0.0) == 0\n    assert frac(-0.0) == 0\n\n    assert floor(Float('0.0')) == 0\n    assert ceiling(Float('0.0')) == 0\n    assert frac(Float('0.0')) == 0\n", "def test_floor_ceiling_properties():\n    # Test properties involving floor and ceiling operations on complex arguments\n    assert floor(3 + 2*I) == 3 + 2*I\n    assert ceiling(3 + 2*I) == 4 + 2*I\n    assert floor(-3 + 2*I) == -4 + 2*I\n    assert ceiling(-3 + 2*I) == -3 + 2*I\n\n    assert floor(3 - 2*I) == 3 - 2*I\n    assert ceiling(3 - 2*I) == 4 - 2*I\n    assert floor(-3 - 2*I) == -4 - 2*I\n    assert ceiling(-3 - 2*I) == -3 - 2*I\n\n    # Test floor and ceiling with symbols involving imaginary parts\n    assert floor(i + 1) == 1 + i\n    assert ceiling(i + 1) == 2 + i\n    assert floor(i - 1) == -1 + i\n    assert ceiling(i - 1) == i\n\n    # Ensure no simplification if symbol is involved\n    assert floor(i + x) == floor(i + x)\n    assert ceiling(i + x) == ceiling(i + x)\n\n    # Test nested floor and ceiling with real and imaginary parts\n    assert floor(ceiling(3.5 + 2.5*I)) == 4 + 3*I\n    assert ceiling(floor(3.5 + 2.5*I)) == 4 + 3*I\n    assert floor(ceiling(-3.5 - 2.5*I)) == -4 - 3*I\n    assert ceiling(floor(-3.5 - 2.5*I)) == -4 - 3*I\n\n    # Test exact zero and one boundary cases\n    assert floor(0 + I) == I\n    assert ceiling(0 + I) == I\n    assert floor(1 + I) == 1 + I\n    assert ceiling(1 + I) == 1 + I\n\n    # Test with more complex expressions\n    assert floor(exp(I*pi/4) + 1) == floor(exp(I*pi/4)) + 1\n    assert ceiling(exp(I*pi/4) + 1) == ceiling", "def test_round_function_eval():\n    assert floor(17) == 17\n    assert ceiling(17) == 17\n\n    assert floor(Rational(23, 10)) == 2\n    assert ceiling(Rational(23, 10)) == 3\n\n    assert floor(2 * E) == 5\n    assert ceiling(2 * E) == 6\n\n    assert floor(-Float(0.567)) == -1\n    assert ceiling(-Float(0.567)) == 0\n\n    assert floor(-I / 2) == -I\n    assert ceiling(I / 2) == I\n\n    assert floor(S(5) / 2 + 5 * I / 2) == 2 + 2 * I\n    assert ceiling(S(5) / 2 + 5 * I / 2) == 3 + 3 * I\n\n    assert floor(S(0)) == 0\n    assert ceiling(S(0)) == 0\n\n    assert floor(S.Infinity) == S.Infinity\n    assert ceiling(S.Infinity) == S.Infinity\n\n    assert floor(S.NegativeInfinity) == S.NegativeInfinity\n    assert ceiling(S.NegativeInfinity) == S.NegativeInfinity\n\n    assert floor(zoo) == zoo\n    assert ceiling(zoo) == zoo\n\n    assert floor(nan) == nan\n    assert ceiling(nan) == nan\n\n    assert floor(pi + I) == 3 + I\n    assert ceiling(pi + I) == 4 + I\n\n    assert floor(x + y).func == floor\n    assert ceiling(x + y).func == ceiling\n\n    assert floor(x * I).func == floor\n    assert ceiling(x * I).func == ceiling\n\n    assert floor(ceiling(pi)) == 4\n    assert ceiling(floor(pi)) == 3\n\n    assert floor(ceiling(-pi)) == -3\n    assert ceiling(floor(-pi)) == -4\n", "def test_round_functions_finite_real():\n    real_symbol = Symbol('real_symbol', real=True, finite=True)\n\n    assert floor(real_symbol).is_finite is True\n    assert ceiling(real_symbol).is_finite is True\n    assert frac(real_symbol).is_finite is True\n\n    assert floor(real_symbol).is_real is True\n    assert ceiling(real_symbol).is_real is True\n    assert frac(real_symbol).is_real is True\n"], "sample_1073": ["def test_is_sqrt():\n    assert is_sqrt(sqrt(2)) == True\n    assert is_sqrt(2**0.5) == False\n    assert is_sqrt(sqrt(2) + sqrt(3)) == False\n    assert is_sqrt(sqrt(2)*sqrt(3)) == False\n    assert is_sqrt(2) == False\n", "def test_is_sqrt():\n    assert is_sqrt(sqrt(2)) is True\n    assert is_sqrt(2**(S(1)/2)) is True\n    assert is_sqrt(2) is False\n    assert is_sqrt(2**2) is False\n    assert is_sqrt(2**(S(3)/2)) is False\n", "def test_is_sqrt():\n    assert is_sqrt(sqrt(2)) is True\n    assert is_sqrt(sqrt(2) * sqrt(3)) is False\n    assert is_sqrt(sqrt(5 + 2 * sqrt(6))) is True\n    assert is_sqrt((2**(S(1)/3))) is False\n", "def test_sqrt_depth():\n    from sympy import Symbol\n    x = Symbol('x')\n    assert sqrt_depth(1 + sqrt(2)*(1 + sqrt(3))) == 1\n    assert sqrt_depth(1 + sqrt(2)*sqrt(1 + sqrt(3))) == 2\n    assert sqrt_depth(sqrt(5 + sqrt(2 + sqrt(3 + x)))) == 3\n    assert sqrt_depth(5) == 0\n    assert sqrt_depth(sqrt(3)) == 1\n    assert sqrt_depth(x) == 0\n    assert sqrt_depth(1 + sqrt(x)) == 1\n", "def test_sqrtdenest_special_cases():\n    # Test the case with nested square roots that should simplify\n    z = sqrt(sqrt(2 + sqrt(3 + sqrt(5))) + sqrt(7))\n    expected = sqrt(7) + sqrt(2 + sqrt(5)/2 + sqrt(15)/2)\n    assert sqrtdenest(z) == expected\n\n    # Test deeply nested square roots\n    z = sqrt(sqrt(sqrt(2 + sqrt(3 + sqrt(5))) + sqrt(7)))\n    assert sqrtdenest(z) == z\n\n    # Test a case where no simplification is possible\n    z = sqrt(5 + sqrt(11 + sqrt(23)))\n    assert sqrtdenest(z) == z\n\n    # Test a case with multiple nested and non-nested terms\n    z = sqrt(1 + sqrt(2) + sqrt(3 + sqrt(5)))\n    assert sqrtdenest(z) == z\n"], "sample_1074": ["def test_subgroup_search_extra():\n    # Test with a specific property\n    S = SymmetricGroup(4)\n        return x.order() % 3 == 0\n    subgroup = S.subgroup_search(prop_div_3)\n    elements = list(subgroup.generate_dimino())\n    for element in elements:\n        assert prop_div_3(element) is True\n    assert subgroup.is_subgroup(S)\n\n    # Test with another specific property\n        return x(0) == 0 and x(1) == 1\n    subgroup = S.subgroup_search(prop_fix_0_and_1)\n    elements = list(subgroup.generate_dimino())\n    for element in elements:\n        assert prop_fix_0_and_1(element) is True\n    assert subgroup.is_subgroup(S)\n\n    # Test the trivial group\n        return x.is_Identity\n    trivial_subgroup = S.subgroup_search(prop_trivial)\n    assert trivial_subgroup.is_trivial\n\n    # Test the whole group\n        return True\n    whole_group = S.subgroup_search(prop_true)\n    assert whole_group == S\n", "def test_coset_representative():\n    G = SymmetricGroup(4)\n    H = G.subgroup([Permutation([0, 1, 2, 3]), Permutation([0, 1, 3, 2])])\n    g = Permutation([2, 3, 0, 1])\n    rep = G._coset_representative(g, H)\n    assert rep in G\n    assert rep*H.generators[0] == g*H.generators[0]\n    assert rep*H.generators[1] == g*H.generators[1]\n\n    G = PermutationGroup([Permutation(0, 1), Permutation(1, 2)])\n    H = PermutationGroup([Permutation(0, 1)])\n    g = Permutation([2, 0, 1])\n    rep = G._coset_representative(g, H)\n    assert rep in G\n    assert rep*H.generators[0] == g*H.generators[0]\n", "def test_centeralizer_element():\n    # Testing centralizer of individual elements in a group\n    S = SymmetricGroup(3)\n    element = Permutation([1, 2, 0])\n    centralizer = S.centralizer(element)\n    assert _verify_centralizer(S, element)\n    assert centralizer.is_subgroup(S)\n    assert centralizer.order() == 3\n\n    A = AlternatingGroup(5)\n    element = Permutation([0, 2, 1, 3, 4])\n    centralizer = A.centralizer(element)\n    assert _verify_centralizer(A, element)\n    assert centralizer.is_subgroup(A)\n    assert centralizer.order() == 3\n\n    # Testing centralizer of a more complex element in a larger group\n    element = Permutation([1, 2, 0, 4, 5, 3])\n    centralizer = S.centralizer(element)\n    assert _verify_centralizer(S, element)\n    assert centralizer.is_subgroup(S)\n    assert centralizer.order() == 1\n\n    # Testing centralizer of a trivial element in a group\n    triv = Permutation([0, 1, 2])\n    centralizer = S.centralizer(triv)\n    assert _verify_centralizer(S, triv)\n    assert centralizer.is_subgroup(S)\n    assert centralizer.order() == S.order()\n", "def test_polycyclic_group():\n    a = Permutation([0, 1, 2])\n    b = Permutation([2, 1, 0])\n    G = PermutationGroup([a, b])\n    assert G.is_polycyclic == True\n    polycyclic_group = G.polycyclic_group()\n    assert polycyclic_group.relative_order == [2, 3]\n    assert polycyclic_group.generators == [a, b]\n\n    a = Permutation([1, 2, 3, 4, 0])\n    b = Permutation([1, 0, 2, 3, 4])\n    G = PermutationGroup([a, b])\n    raises(ValueError, lambda: G.polycyclic_group())\n", "def test_coset_representative():\n    # Setup a permutation group G and a subgroup H\n    G = SymmetricGroup(5)\n    H = G.stabilizer(0)\n\n    # Test if the coset representative is correct\n    for g in G:\n        rep = G._coset_representative(g, H)\n        if rep is not None:\n            assert H.contains(rmul(rep, g**-1))\n        else:\n            assert not H.contains(g)\n"], "sample_1075": ["def test_beta_evalf():\n    from sympy import I, pi\n    assert beta(pi, pi).evalf(40) == 0.02671848900111377452242355235388489324562\n    assert beta(1 + I, 1 + I).evalf(20) == -0.2112723729365330143 - 0.7655283165378005676*I\n", "def test_beta_eval():\n    from sympy import S\n    # Evaluation at specific values\n    assert beta(S(1), S(1)) == 1\n    assert beta(S(2), S(3)) == S(1)/6\n    assert beta(S(0.5), S(0.5)) == pi\n\n    # Check for complex values\n    assert beta(1 + I, 1 + I).evalf() == beta(S(1) + I, S(1) + I).evalf()\n    \n    # Check evaluation at zero\n    assert beta(0, y) == oo\n    assert beta(x, 0) == oo\n", "def test_beta_eval():\n    from sympy import S\n\n    # Test known values\n    assert beta(1, 1) == 1\n    assert beta(2, 1) == 1/1\n    assert beta(1, 2) == 1/1\n    assert beta(2, 2) == 1/2\n    assert beta(3, 3) == 1/6\n\n    # Test numerical evaluation\n    assert abs(beta(S.Pi, S.Pi).evalf() - 0.0267184890011) < 1e-12\n    assert abs(beta(1+1j, 1+1j).evalf() - (-0.2112723729365330143 - 0.7655283165378005676j)) < 1e-12\n\n    # Test properties\n    assert beta(x, 1) == 1/x\n    assert beta(1, y) == 1/y\n    assert beta(x, y) == beta(y, x)\n", "def test_beta_evalf():\n    from sympy import pi, I\n\n    assert beta(pi, pi).evalf(40).epsilon_eq(0.02671848900111377452242355235388489324562, 1e-40)\n    assert beta(1 + I, 1 + I).evalf(20).epsilon_eq(-0.2112723729365330143 - 0.7655283165378005676*I, 1e-20)\n", "def test_beta_evalf():\n    from sympy import pi, I\n\n    assert beta(pi, pi).evalf(40) == 0.02671848900111377452242355235388489324562\n    assert beta(1 + I, 1 + I).evalf(20) == -0.2112723729365330143 - 0.7655283165378005676*I\n"], "sample_1076": ["def test_print_Stream():\n    import sys\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(sys.stdout) == 'sys.stdout'\n    assert prntr.doprint(sys.stderr) == 'sys.stderr'\n    assert prntr.module_imports == {'sys': {'stdout', 'stderr'}}\n", "def test_issue_12345():\n    from sympy import sin, cos, tan, log, exp\n\n    expr1 = sin(x)\n    expr2 = cos(x)\n    expr3 = tan(x)\n    expr4 = log(x)\n    expr5 = exp(x)\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.sin(x)'\n    assert prntr.doprint(expr2) == 'math.cos(x)'\n    assert prntr.doprint(expr3) == 'math.tan(x)'\n    assert prntr.doprint(expr4) == 'math.log(x)'\n    assert prntr.doprint(expr5) == 'math.exp(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.sin(x)'\n    assert prntr.doprint(expr2) == 'numpy.cos(x)'\n    assert prntr.doprint(expr3) == 'numpy.tan(x)'\n    assert prntr.doprint(expr4) == 'numpy.log(x)'\n    assert prntr.doprint(expr5) == 'numpy.exp(x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.sin(x)'\n    assert prntr.doprint(expr2) == 'numpy.cos(x)'\n    assert prntr.doprint(expr3) == 'numpy.tan(x)'\n    assert prntr.doprint(expr4) == 'numpy.log(x)'\n    assert prntr.doprint(expr5) == 'numpy.exp(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.sin(x)'\n    assert prntr.doprint(expr2) == 'mpmath.cos(x)'\n    assert prntr.doprint(expr3) == 'mpmath.tan(x)'\n    assert prntr.doprint(expr4) == 'mpmath.log(x)'\n    assert prntr.doprint(expr5) == 'mpmath.exp(x)'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr1) == 'sympy.sin(x)'\n    assert prntr.doprint(expr2) == 'sympy.cos(x)'\n    assert prntr.doprint(expr3) == 'sympy.tan", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Rational(1, 2)) == '1/2'\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(A**(-1)) == \"sympy.Inverse(A)\"\n    assert p.doprint(A**5) == \"A**5\"\n\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    \n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(1/sqrt(x)) == '1/sympy.sqrt(x)'\n\n    expr1 = MatrixSymbol('A', 2, 2)\n    expr2 = MatrixSymbol('B', 2, 2)\n    expr3 = expr1 * expr2\n    assert p.doprint(expr3) == 'A*B'\n", "def test_NaN_Infinity():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(float('nan')) == \"float('nan')\"\n    assert prntr.doprint(float('inf')) == \"float('inf')\"\n    assert prntr.doprint(float('-inf')) == \"float('-inf')\"\n\n    from sympy import I\n    assert prntr.doprint(I) == '1j'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(float('nan')) == \"numpy.nan\"\n    assert prntr.doprint(float('inf')) == \"numpy.inf\"\n    assert prntr.doprint(float('-inf')) == \"numpy.NINF\"\n    assert prntr.doprint(I) == '1j'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(float('nan')) == \"mpmath.nan\"\n    assert prntr.doprint(float('inf')) == \"mpmath.inf\"\n    assert prntr.doprint(float('-inf')) == \"mpmath.ninf\"\n    assert prntr.doprint(I) == '1j'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(float('nan')) == \"numpy.nan\"\n    assert prntr.doprint(float('inf')) == \"numpy.inf\"\n    assert prntr.doprint(float('-inf')) == \"numpy.NINF\"\n    assert prntr.doprint(I) == '1j'\n", "def test_ComplexInfinity():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(zoo) == \"float('nan')\"\n    assert prntr.doprint(oo) == \"float('inf')\"\n    assert prntr.doprint(-oo) == \"float('-inf')\"\n\n    assert prntr.doprint(ComplexInfinity) == \"float('nan')\"\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(zoo) == \"numpy.nan\"\n    assert prntr.doprint(oo) == \"numpy.inf\"\n    assert prntr.doprint(-oo) == \"numpy.NINF\"\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(zoo) == \"mpmath.nan\"\n    assert prntr.doprint(oo) == \"mpmath.inf\"\n    assert prntr.doprint(-oo) == \"mpmath.ninf\"\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(zoo) == \"numpy.nan\"\n    assert prntr.doprint(oo) == \"numpy.inf\"\n    assert prntr.doprint(-oo) == \"numpy.NINF\"\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(zoo) == \"sympy.nan\"\n    assert prntr.doprint(oo) == \"sympy.oo\"\n    assert prntr.doprint(-oo) == \"sympy.oo\"\n    assert prntr.doprint(ComplexInfinity) == \"sympy.zoo\"\n"], "sample_1077": ["def test_complexregion_with_symbolic_intervals():\n    r, theta = symbols('r theta', real=True)\n    a = Interval(r, r + 1)\n    b = Interval(theta, theta + pi/2)\n    c = Interval(r + 1, r + 2)\n    \n    cr1 = ComplexRegion(a * b, polar=True)\n    cr2 = ComplexRegion(c * b, polar=True)\n    \n    assert cr1.intersect(cr2) == ComplexRegion(Interval(r + 1, r + 1) * Interval(theta, theta + pi/2), polar=True)\n    assert cr1.union(cr2) == ComplexRegion(Union(a * b, c * b), polar=True)\n    assert cr1.contains(r + (r + 0.5)*I) == True\n    assert cr1.contains(r + (r + 1.5)*I) == False\n", "def test_Range_contains():\n    # Test for containment in finite ranges\n    assert 0 in Range(0, 10)\n    assert 10 not in Range(0, 10)\n    assert -1 not in Range(0, 10)\n    assert 5 in Range(0, 10)\n    assert 5.5 not in Range(0, 10)\n\n    # Test for containment in infinite ranges\n    assert 0 in Range(0, oo)\n    assert 10 in Range(0, oo)\n    assert -1 not in Range(0, oo)\n    assert 10 in Range(-oo, 10)\n    assert 11 not in Range(-oo, 10)\n    assert 5 in Range(-oo, oo)\n\n    # Test for symbolic containment\n    x = Symbol('x')\n    assert x not in Range(0, 10)\n    assert (x + 1) not in Range(0, 10)\n\n    # Test for step size\n    assert 5 in Range(0, 10, 2)\n    assert 4 not in Range(0, 10, 2)\n    assert 6 in Range(0, 10, 2)\n    assert 7 not in Range(0, 10, 2)\n\n    # Test for negative step sizes\n    assert -5 in Range(-10, 0, 2)\n    assert -4 not in Range(-10, 0, 2)\n    assert -6 in Range(-10, 0, 2)\n    assert -7 not in Range(-10, 0, 2)\n    assert -5 in Range(-5, -10, -2)\n    assert -4 not in Range(-5, -10, -2)\n    assert -6 in Range(-5, -10, -2)\n    assert -7 not in Range(-5, -10, -2)\n", "def test_rationals_contains():\n    R = S.Rationals\n    assert R._contains(S.One)\n    assert not R._contains(S.ImaginaryUnit)\n    assert not R._contains(S.Pi)\n    assert R._contains(S(3)/4)\n    assert not R._contains(sin(x))\n    assert R._contains(Symbol('r', rational=True))\n    assert not R._contains(Symbol('i', imaginary=True))\n", "def test_range_contains():\n    assert Range(1, 10, 2)._contains(3) is S.false\n    assert Range(1, 10, 2)._contains(4) is S.false\n    assert Range(1, 10, 2)._contains(5) is S.false\n    assert Range(1, 10, 2)._contains(7) is S.true\n    assert Range(1, 10, 2)._contains(8) is S.false\n    assert Range(1, 10, 2)._contains(9) is S.true\n    assert Range(1, 10, 2)._contains(10) is S.false\n    assert Range(1, 10, 2)._contains(11) is S.false\n", "def test_Range_symbolic_contains():\n    n = Symbol('n', integer=True)\n    r = Range(n, n + 10)\n    assert (n + 5) in r\n    assert (n + 10) not in r\n    assert (n - 1) not in r\n\n    m = Symbol('m', integer=True)\n    r = Range(n, n + m)\n    assert (n + m - 1) in r\n    assert (n + m) not in r\n    assert (n - 1) not in r\n\n    # Test Range with symbolic step\n    s = Symbol('s', positive=True, integer=True)\n    r = Range(n, n + 10, s)\n    assert (n + s) in r\n    assert (n + 10) not in r\n    assert (n + s*5) in r\n    assert (n + s*5 + 1) not in r\n"], "sample_1078": ["def test_IndexedBase_with_non_integer_shape():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a', shape=(sin(i), cos(j)))\n    assert a.shape == Tuple(sin(i), cos(j))\n    A = a[i, j]\n    assert A.shape == Tuple(sin(i), cos(j))\n    assert A.indices == (i, j)\n    raises(IndexException, lambda: a[i])\n    raises(IndexException, lambda: a[i, j, j])\n", "def test_Idx_exception_messages():\n    i, x = symbols('i x', integer=True)\n    from sympy.utilities.misc import filldedent\n\n    raises_message = lambda exc, msg, func: raises(exc, func) and str(exc) in str(msg)\n\n    raises_message(IndexException, \"Indexed needs at least one index.\", lambda: Indexed(\"A\"))\n    raises_message(TypeError, filldedent(\"\"\"\n        The base can only be replaced with a string, Symbol,\n        IndexedBase or an object with a method for getting\n        items (i.e. an object with a `__getitem__` method).\n        \"\"\"), lambda: Indexed(5, i))\n\n    raises_message(IndexException, \"Range is not defined for all indices in:\", lambda: Indexed(\"A\", Idx(i, x)))\n    raises_message(IndexException, \"Shape cannot be inferred from Idx with undefined range:\", lambda: Indexed(\"A\", Idx(i, 1/x)))\n\n    A = IndexedBase(\"A\")\n    raises_message(IndexException, \"Rank mismatch.\", lambda: A[i, i, i])\n    raises_message(IndexException, \"Rank mismatch.\", lambda: A[i])\n", "def test_IndexedBase_strides():\n    i, j, m, n, o, p = symbols('i j m n o p', integer=True)\n    a = IndexedBase('a', shape=(m, n), strides=(o, p))\n    assert a.strides == Tuple(o, p)\n    A = IndexedBase('A', shape=(m, n), strides='C')\n    assert A.strides == 'C'\n    B = IndexedBase('B', shape=(m, n), strides='F')\n    assert B.strides == 'F'\n    C = IndexedBase('C', shape=(m, n))\n    assert C.strides is None\n    assert A[i, j].strides == 'C'\n    assert B[i, j].strides == 'F'\n    assert C[i, j].strides is None\n", "def test_IndexedBase_getitem():\n    i, j, k = symbols('i j k', integer=True)\n    A = IndexedBase('A')\n    assert A[i, j] == Indexed(A, i, j)\n    assert A[(i, j)] == Indexed(A, i, j)\n    assert A[[i, j]] == Indexed(A, i, j)\n    assert A[Tuple(i, j)] == Indexed(A, i, j)\n    raises(IndexException, lambda: A[()])\n    raises(IndexException, lambda: A[1, 2, 3, 4])\n", "def test_IndexedBase_offset_and_strides():\n    i, j = symbols('i j', integer=True)\n    o, p = symbols('o p', integer=True)\n    n, m = symbols('n m', integer=True)\n    a = IndexedBase('a', shape=(o, p), offset=1, strides=(2, 3))\n    assert a.shape == Tuple(o, p)\n    assert a.offset == 1\n    assert a.strides == (2, 3)\n    assert Indexed(a, Idx(i, m), Idx(j, n)).shape == Tuple(o, p)\n    assert Indexed(a, Idx(i, m), Idx(j, n)).ranges == [Tuple(0, m - 1), Tuple(0, n - 1)]\n    assert Indexed(a, Idx(i, m), Idx(j)).shape == Tuple(o, p)\n    assert Indexed(a, Idx(i, m), Idx(j)).ranges == [Tuple(0, m - 1), Tuple(None, None)]\n"], "sample_1079": ["def test_orthogonal_direction():\n    assert Point(1, 0).orthogonal_direction == Point(0, 1)\n    assert Point(0, 1).orthogonal_direction == Point(1, 0)\n    assert Point(2, 3).orthogonal_direction == Point(-3, 2)\n    assert Point(2, 0, 0).orthogonal_direction == Point(1, 0, 0)\n    assert Point(0, 2, 0).orthogonal_direction == Point(0, 1, 0)\n    assert Point(0, 0, 2).orthogonal_direction == Point(0, 1, 0)\n", "def test_project():\n    p1 = Point(1, 2)\n    p2 = Point(2, 4)\n    p3 = Point3D(1, 2, 3)\n    p4 = Point3D(2, 4, 6)\n\n    # 2D projections\n    assert Point.project(p1, p2) == Point(1, 2)\n    assert Point.project(p2, p1) == Point(2, 4)\n    assert Point.project(p1, Point(2, 0)) == Point(1, 0)\n\n    # Check for ValueError for zero vector\n    raises(ValueError, lambda: Point.project(p1, Point(0, 0)))\n\n    # 3D projections\n    assert Point.project(p3, p4) == Point3D(1, 2, 3)\n    assert Point.project(p4, p3) == Point3D(2, 4, 6)\n    assert Point.project(p3, Point3D(0, 0, 2)) == Point3D(0, 0, 3)\n\n    # Check for ValueError for zero vector in 3D\n    raises(ValueError, lambda: Point.project(p3, Point3D(0, 0, 0)))\n", "def test_project():\n    p1 = Point(3, 4)\n    p2 = Point(1, 2)\n    assert Point.project(p1, p2) == Point(2.2, 4.4)\n    p3 = Point3D(1, 2, 3)\n    p4 = Point3D(4, 5, 6)\n    assert Point.project(p3, p4) == Point3D(14/7, 35/7, 56/7)\n    raises(ValueError, lambda: Point.project(p1, Point(0, 0)))\n\n    p1 = Point(3, 4)\n    p2 = Point(-1, -2)\n    assert Point.project(p1, p2) == Point(-0.2, -0.4)\n    p3 = Point3D(1, 2, 3)\n    p4 = Point3D(-4, -5, -6)\n    assert Point.project(p3, p4) == Point3D(-14/7, -35/7, -56/7)\n    raises(ValueError, lambda: Point.project(p3, Point3D(0, 0, 0)))\n", "def test_canberra_distance():\n    p1 = Point(1, 2)\n    p2 = Point(4, 6)\n    p3 = Point(0, 0)\n    p4 = Point(2, 1)\n\n    assert p1.canberra_distance(p2) == Rational(1, 5) + Rational(1, 4)\n    assert p3.canberra_distance(p2) == Rational(1, 4) + Rational(1, 6)\n    assert p1.canberra_distance(p3) == 3\n    assert p1.canberra_distance(p4) == Rational(1, 3) + Rational(1, 1)\n\n    raises(ValueError, lambda: p3.canberra_distance(p3))\n", "def test_canberra_distance():\n    p1 = Point2D(1, 2)\n    p2 = Point2D(4, 6)\n    p3 = Point2D(0, 0)\n    p4 = Point2D(1, 0)\n    \n    assert p1.canberra_distance(p2) == Rational(1, 5) + Rational(1, 4)\n    assert p1.canberra_distance(p3) == 1 + Rational(1, 2)\n    assert p3.canberra_distance(p4) == 1\n    raises(ValueError, lambda: p3.canberra_distance(p3))\n    \n    p5 = Point3D(1, 2, 3)\n    p6 = Point3D(4, 6, 9)\n    assert p5.canberra_distance(p6) == Rational(1, 5) + Rational(1, 4) + Rational(1, 3)\n    raises(ValueError, lambda: Point3D(0, 0, 0).canberra_distance(Point3D(0, 0, 0)))\n"], "sample_1080": ["def test_abs_complex():\n    x = Symbol('x', complex=True)\n    assert refine(Abs(x), Q.real(x)) == Abs(x)\n    assert refine(Abs(x), Q.positive(x)) == x\n    assert refine(Abs(x), Q.negative(x)) == -x\n    assert refine(Abs(x), Q.imaginary(x)) == Abs(x)\n    assert refine(Abs(x*y), Q.positive(x)) == x * Abs(y)\n    assert refine(Abs(x*y), Q.negative(x)) == -x * Abs(y)\n    assert refine(Abs(x*y), Q.real(x) & Q.real(y)) == Abs(x*y)\n", "def test_refine_abs():\n    assert refine(Abs(x * y), Q.positive(x) & Q.positive(y)) == x * y\n    assert refine(Abs(x * y), Q.positive(x) & Q.negative(y)) == -x * y\n    assert refine(Abs(x * y), Q.negative(x) & Q.positive(y)) == -x * y\n    assert refine(Abs(x * y), Q.negative(x) & Q.negative(y)) == x * y\n    assert refine(Abs(x * y), Q.positive(x) & Q.zero(y)) == S.Zero\n    assert refine(Abs(x * y), Q.negative(x) & Q.zero(y)) == S.Zero\n    assert refine(Abs(x * y), Q.zero(x) & Q.positive(y)) == S.Zero\n    assert refine(Abs(x * y), Q.zero(x) & Q.negative(y)) == S.Zero\n    assert refine(Abs(x * y), Q.zero(x) & Q.zero(y)) == S.Zero\n", "def test_refine_abs_with_mul():\n    from sympy import Mul\n    # Test refinement of Abs with Mul arguments\n    assert refine(Abs(x * y), Q.positive(x) & Q.real(y)) == x * Abs(y)\n    assert refine(Abs(x * y), Q.positive(y) & Q.real(x)) == y * Abs(x)\n    assert refine(Abs(x * y), Q.negative(x) & Q.real(y)) == -x * Abs(y)\n    assert refine(Abs(x * y), Q.negative(y) & Q.real(x)) == -y * Abs(x)\n    assert refine(Abs(x * y), Q.negative(x) & Q.negative(y)) == x * y\n\n    # Test nested Abs with Mul arguments\n    expr = Mul(Abs(x), Abs(y))\n    assert refine(expr, Q.positive(x) & Q.positive(y)) == x * y\n    assert refine(expr, Q.negative(x) & Q.positive(y)) == -x * y\n    assert refine(expr, Q.positive(x) & Q.negative(y)) == x * -y\n    assert refine(expr, Q.negative(x) & Q.negative(y)) == -x * -y\n", "def test_refine_sign():\n    # Test the refinement of `sign` with combined assumptions\n    assert refine(sign(x), Q.positive(x) & Q.nonzero(x)) == 1\n    assert refine(sign(x), Q.negative(x) & Q.nonzero(x)) == -1\n    assert refine(sign(x), Q.positive(x) & Q.negative(y)) == 1\n    assert refine(sign(x), Q.negative(x) & Q.positive(y)) == -1\n\n    # Test the refinement of `sign` with complex conditions\n    y = Symbol('y', real=True)\n    assert refine(sign(x + I*y), Q.positive(x) & Q.positive(y)) == sign(x + I*y)\n    assert refine(sign(x + I*y), Q.negative(x) & Q.negative(y)) == sign(x + I*y)\n    assert refine(sign(x + I*y), Q.zero(x) & Q.positive(y)) == S.ImaginaryUnit\n    assert refine(sign(x + I*y), Q.zero(x) & Q.negative(y)) == -S.ImaginaryUnit\n", "def test_refine_abs():\n    assert refine(Abs(x*y), Q.real(x) & Q.positive(y)) == Abs(x) * y\n    assert refine(Abs(x*y), Q.positive(x) & Q.positive(y)) == x * y\n    assert refine(Abs(x*y), Q.negative(x) & Q.negative(y)) == -x * -y\n    assert refine(Abs(x*y), Q.real(x) & Q.real(y) & Q.positive(x) & Q.negative(y)) == x * -y\n    assert refine(Abs(x), Q.positive(x) & ~Q.negative(x)) == x\n    assert refine(Abs(x), Q.negative(x) & ~Q.positive(x)) == -x\n"], "sample_1081": ["def test_pollard_rho():\n    # Testing Pollard's rho algorithm for factorization\n    assert pollard_rho(8051) == 97  # 8051 = 97 * 83\n    assert pollard_rho(10403) == 101  # 10403 = 101 * 103\n    assert pollard_rho(327509) == 509  # 327509 = 509 * 643\n    assert pollard_rho(391939, seed=2) == 397  # 391939 = 397 * 987\n    # Checking a non-prime factorization\n    n = 2**31 - 1\n    factors = factorint(n)\n    assert pollard_rho(n) in factors or pollard_rho(n, seed=2) in factors\n    # When the number is prime\n    assert pollard_rho(19) is None\n", "def test_trailing():\n    assert trailing(0) == 0\n    assert trailing(128) == 7\n    assert trailing(63) == 0\n    assert trailing(64) == 6\n    assert trailing(2**30) == 30\n    assert trailing(2**30 * 3) == 30\n    assert trailing(3 * 2**8) == 8\n    assert trailing(2**8 * 7) == 8\n    assert trailing(2**16 * 5 * 7) == 16\n    assert trailing(1 << 273956) == 273956\n    assert trailing(-1 << 273956) == 273956\n    assert trailing(2**300 * 3) == 300\n    assert trailing(2**300 * 3**2) == 300\n    assert trailing(-2**300 * 3**2) == 300\n", "def test_pollard_rho():\n    # Test Pollard's rho algorithm for factorization\n    assert pollard_rho(10403) in [101, 103]  # product of two primes\n    assert pollard_rho(8051) in [53, 151]  # product of two primes\n    assert pollard_rho(10403, s=3) in [101, 103]  # test with a different start value\n    assert pollard_rho(15) is None  # 15 = 3 * 5, small number test\n    assert pollard_rho(19) is None  # 19 is prime\n\n    # Test for robustness with large composite numbers\n    assert pollard_rho(1234567891011) is not None\n\n    # Test for seed variation\n    assert pollard_rho(10403, seed=42) in [101, 103]\n    assert pollard_rho(8051, seed=42) in [53, 151]\n\n    # Test case with retries\n    assert pollard_rho(10403, retries=3) in [101, 103]\n    assert pollard_rho(8051, retries=3) in [53, 151]\n\n    # Test failure case with high retries\n    assert pollard_rho(19, retries=10) is None\n\n    # Test max_steps parameter\n    assert pollard_rho(10403, max_steps=1000) in [101, 103]\n    assert pollard_rho(8051, max_steps=1000) in [53, 151]\n", "def test_perfect_power_big():\n    assert perfect_power(2**10007) == (2, 10007)\n    assert perfect_power(2**10007 + 1) is False\n    assert perfect_power(2**10007 - 1) is False\n    assert perfect_power((9**99 + 1)**60) == (9**99 + 1, 60)\n    assert perfect_power((9**99 + 1)**60 + 1) is False\n    assert perfect_power((9**99 + 1)**60 - 1) is False\n    assert perfect_power((10**40000)**2, big=False) == (10**40000, 2)\n    assert perfect_power(10**100000) == (10, 100000)\n    assert perfect_power(10**100001) == (10, 100001)\n", "def test_pollard_rho():\n    n = 10403  # 101 * 103\n    assert pollard_rho(n) in [101, 103]\n    n = 15485863 * 15485867\n    assert pollard_rho(n) in [15485863, 15485867]\n    # test with custom function F\n    F = lambda x: (x**2 + 1) % n\n    assert pollard_rho(n, F=F) in [15485863, 15485867]\n    # edge case with retries\n    assert pollard_rho(n, retries=0) is None\n    # large number\n    p = nextprime(10**9)\n    q = nextprime(p + 10**6)\n    n = p * q\n    assert pollard_rho(n) in [p, q]\n"], "sample_1082": ["def test_sech_hyperbolic_inverses():\n    x = Symbol('x')\n    assert sech(asinh(x)) == 1/sqrt(1 + x**2)\n    assert sech(acosh(x)) == 1/x\n    assert sech(atanh(x)) == sqrt(1 - x**2)\n    assert sech(acoth(x)) == sqrt(x - 1) * sqrt(x + 1)/x\n\n    assert sech(asinh(-x)) == 1/sqrt(1 + x**2)\n    assert sech(acosh(-x)) == 1/(-x)\n    assert sech(atanh(-x)) == sqrt(1 - x**2)\n    assert sech(acoth(-x)) == sqrt(x - 1) * sqrt(x + 1)/(-x)\n\n    assert sech(asinh(0)) == 1\n    assert sech(acosh(1)) == 1\n    assert sech(atanh(0)) == 1\n    assert sech(acoth(2)) == sqrt(3)/2\n", "def test_inverse_hyperbolic_functions():\n    x = Symbol('x')\n    \n    # Test for asinh\n    assert asinh(1).rewrite(log) == log(1 + sqrt(2))\n    assert asinh(0).rewrite(log) == 0\n    assert asinh(-1).rewrite(log) == log(1 + sqrt(2)) - log(sqrt(2) + 1)\n    assert asinh(x).rewrite(log) == log(x + sqrt(x**2 + 1))\n\n    # Test for acosh\n    assert acosh(1).rewrite(log) == 0\n    assert acosh(2).rewrite(log) == log(2 + sqrt(3))\n    assert acosh(3).rewrite(log) == log(3 + sqrt(8))\n    assert acosh(x).rewrite(log) == log(x + sqrt(x + 1) * sqrt(x - 1))\n\n    # Test for atanh\n    assert atanh(0).rewrite(log) == 0\n    assert atanh(Rational(1, 2)).rewrite(log) == (log(3) - log(1))/2\n    assert atanh(-Rational(1, 2)).rewrite(log) == (log(1) - log(3))/2\n    assert atanh(x).rewrite(log) == (log(1 + x) - log(1 - x)) / 2\n\n    # Test for acoth\n    assert acoth(2).rewrite(log) == (log(3/2) - log(1/2))/2\n    assert acoth(-2).rewrite(log) == (log(1/2) - log(3/2))/2\n    assert acoth(x).rewrite(log) == (log(1 + 1/x) - log(1 - 1/x)) / 2\n\n    # Test for asech\n    assert asech(Rational(1, 2)).rewrite(log) == log(2 + sqrt(3))\n    assert asech(-Rational(1, 2)).rewrite(log) == log(2 + sqrt(3)) + I*pi\n    assert asech(x).rewrite(log) == log(1/x + sqrt(1/x - 1) * sqrt(1/x + 1))\n\n    # Test for acsch\n    assert acsch(2).rewrite(log) == log(1/sqrt(3", "def test_sinh_inverses():\n    x = Symbol('x')\n\n    assert sinh(asinh(x)) == x\n    assert cosh(acosh(x)) == x\n    assert tanh(atanh(x)) == x\n    assert coth(acoth(x)) == x\n", "def test_peeloff_ipi():\n    from sympy import I, pi\n    x, y = symbols('x y')\n\n    # Test when ARG has a multiple of I*pi/2\n    assert _peeloff_ipi(x + I*pi/2) == (x, I*pi/2)\n    assert _peeloff_ipi(x + I*2*pi/3 + I*pi*y) == (x + I*pi*y + I*pi/6, I*pi/2)\n\n    # Test when ARG does not have a multiple of I*pi/2\n    assert _peeloff_ipi(x) == (x, S.Zero)\n    assert _peeloff_ipi(x + y) == (x + y, S.Zero)\n    assert _peeloff_ipi(x + y + I*pi/3) == (x + y + I*pi/3, S.Zero)\n\n    # Test with a complex expression\n    expr = x + I*3*pi/4 + I*pi*y + I*pi/3\n    assert _peeloff_ipi(expr) == (x + I*pi*y + I*pi/12, I*pi/2)\n", "def test_issue_12345():\n    x, y = symbols('x, y')\n\n    # Testing edge cases and specific values for hyperbolic functions\n    assert sinh(pi*I/7) == sin(pi/7)*I\n    assert sinh(pi*I*Rational(13, 7)) == sin(pi*Rational(13, 7))*I\n    assert sinh(pi*I*Rational(29, 7)) == sin(pi*Rational(29, 7))*I\n\n    assert cosh(pi*I/7) == cos(pi/7)\n    assert cosh(pi*I*Rational(13, 7)) == cos(pi*Rational(13, 7))\n    assert cosh(pi*I*Rational(29, 7)) == cos(pi*Rational(29, 7))\n\n    assert tanh(pi*I/7) == tan(pi/7)*I\n    assert tanh(pi*I*Rational(13, 7)) == tan(pi*Rational(13, 7))*I\n    assert tanh(pi*I*Rational(29, 7)) == tan(pi*Rational(29, 7))*I\n\n    assert coth(pi*I/7) == -cot(pi/7)*I\n    assert coth(pi*I*Rational(13, 7)) == -cot(pi*Rational(13, 7))*I\n    assert coth(pi*I*Rational(29, 7)) == -cot(pi*Rational(29, 7))*I\n\n    assert csch(pi*I/7) == -I/sin(pi/7)\n    assert csch(pi*I*Rational(13, 7)) == -I/sin(pi*Rational(13, 7))\n    assert csch(pi*I*Rational(29, 7)) == -I/sin(pi*Rational(29, 7))\n\n    assert sech(pi*I/7) == 1/cos(pi/7)\n    assert sech(pi*I*Rational(13, 7)) == 1/cos(pi*Rational(13, 7))\n    assert sech(pi*I*Rational(29, 7)) == 1/cos(pi*Rational(29, 7))\n"], "sample_1083": ["def test_sech_properties():\n    x = Symbol('x', extended_real=True)\n    n = Symbol('n', integer=True)\n\n    # Test if sech is even\n    assert sech(-x) == sech(x)\n\n    # Test if sech is periodic with period 2*pi*I\n    assert sech(x + 2*pi*I) == sech(x)\n\n    # Test fundamental values\n    assert sech(0) == 1\n    assert sech(I*pi/3) == 2\n    assert sech(I*pi/2) is zoo\n\n    # Test sech for imaginary values\n    assert sech(I*pi) == -1\n    assert sech(I*pi/4) == sqrt(2)\n    assert sech(I*2*pi) == 1\n\n    # Test sech for real values\n    assert sech(x).is_positive is True\n    assert sech(-x).is_positive is True\n\n    # Test sech periodicity with n\n    assert sech(x + 2*n*pi*I) == sech(x)\n", "def test_rewrite_hyperbolics_as_exp():\n    from sympy import sinh, cosh, tanh, coth, symbols, I, exp\n    x = symbols('x')\n    \n    expr1 = sinh(x) + cosh(x)\n    expr2 = tanh(x) * coth(x)\n    expr3 = sinh(x + I*pi/2)\n    \n    assert _rewrite_hyperbolics_as_exp(expr1) == (exp(x) - exp(-x)) / 2 + (exp(x) + exp(-x)) / 2\n    assert _rewrite_hyperbolics_as_exp(expr2) == ((exp(x) - exp(-x)) / (exp(x) + exp(-x))) * ((exp(x) + exp(-x)) / (exp(x) - exp(-x)))\n    assert _rewrite_hyperbolics_as_exp(expr3) == (exp(x + I*pi/2) - exp(-(x + I*pi/2))) / 2\n", "def test_taylor_terms():\n    x = Symbol('x')\n    for func, terms in [\n        (sinh, [0, x, 0, x**3/6, 0, x**5/120]),\n        (cosh, [1, 0, x**2/2, 0, x**4/24, 0]),\n        (tanh, [0, x, 0, -x**3/3, 0, 2*x**5/15]),\n        (coth, [x**(-1), 0, x/3, 0, -x**3/45, 0]),\n        (csch, [x**(-1), 0, -x/6, 0, 7*x**3/360, 0]),\n        (sech, [1, 0, -x**2/2, 0, 5*x**4/24, 0])\n    ]:\n        for n, term in enumerate(terms):\n            assert func.taylor_term(n, x) == term\n", "def test_acoth_expansion():\n    x, y = symbols('x,y')\n    assert acoth(x+y).expand(trig=True) == acoth(x)*acoth(y)\n    assert acoth(2*x).expand(trig=True) == 2*acoth(x)\n    assert acoth(3*x).expand(trig=True).expand() == 3*acoth(x)\n", "def test_cosh_rewrite_as_sinh():\n    x = Symbol('x')\n    assert cosh(x).rewrite(sinh) == -I*sinh(x + I*pi/2)\n"], "sample_1084": ["def test_intersection_image_sets():\n    from sympy.abc import n, m\n    S1 = ImageSet(Lambda(n, 2*n), S.Integers)\n    S2 = ImageSet(Lambda(m, 3*m), S.Integers)\n    S3 = ImageSet(Lambda(n, n**2), S.Integers)\n    S4 = ImageSet(Lambda(n, n), S.Integers)\n    S5 = ImageSet(Lambda(n, 2*n + 1), S.Integers)\n\n    assert S1.intersect(S2) == ImageSet(Lambda(n, 6*n), S.Integers)\n    assert S1.intersect(S3) == ImageSet(Lambda(n, 4*n**2), S.Integers)\n    assert S4.intersect(S2) == ImageSet(Lambda(n, 3*n), S.Integers)\n    assert S4.intersect(S5) == S.EmptySet\n    assert S3.intersect(S5) == S.EmptySet\n", "def test_intersection_sets():\n    N = S.Naturals\n    Z = S.Integers\n    R = S.Reals\n    C = ComplexRegion(Interval(0, 1) * Interval(0, 2*pi), polar=True)\n    interval = Interval(1, 4)\n    finite_set = FiniteSet(1, 2, 3, 4, 5)\n    empty_set = S.EmptySet\n\n    assert intersection_sets(N, Z) == N\n    assert intersection_sets(N, N) == N\n    assert intersection_sets(interval, N) == Intersection(interval, N)\n    assert intersection_sets(C, R) == ComplexRegion(Interval(0, 1) * Interval(0, pi), polar=True)\n    assert intersection_sets(Z, R) == Z\n    assert intersection_sets(interval, interval) == interval\n    assert intersection_sets(finite_set, finite_set) == FiniteSet(1, 2, 3, 4, 5)\n    assert intersection_sets(finite_set, interval) == FiniteSet(1, 2, 3, 4)\n    assert intersection_sets(empty_set, finite_set) == S.EmptySet\n    assert intersection_sets(UniversalSet(), finite_set) == finite_set\n\n    range1 = Range(0, 10, 2)\n    range2 = Range(1, 10, 2)\n    assert intersection_sets(range1, range2) == S.EmptySet\n\n    complex_region_1 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=True)\n    complex_region_2 = ComplexRegion(Interval(0, 1)*Interval(pi/2, 3*pi/2), polar=True)\n    assert intersection_sets(complex_region_1, complex_region_2) == ComplexRegion(Interval(0, 1) * Interval(pi/2, pi), polar=True)\n", "def test_intersection_sets():\n    N = S.Naturals\n    Z = S.Integers\n    Q = S.Rationals\n    R = S.Reals\n    C = S.Complexes\n\n    # Test intersection between Naturals and Integers\n    assert intersection_sets(N, Z) == N\n\n    # Test intersection between Naturals and Rationals\n    assert intersection_sets(N, Q) == N\n\n    # Test intersection between Integers and Rationals\n    assert intersection_sets(Z, Q) == Z\n\n    # Test intersection between Rationals and Reals\n    assert intersection_sets(Q, R) == Q\n\n    # Test intersection between ComplexRegion in rectangular form\n    assert intersection_sets(ComplexRegion(Interval(0, 1) * Interval(0, 2*S.Pi), polar=False), S.Reals) == Interval(0, 1)\n\n    # Test intersection between ComplexRegion in polar form\n    assert intersection_sets(ComplexRegion(Interval(0, 1) * Interval(0, 2*S.Pi), polar=True), S.Reals) == Interval(0, 1)\n\n    # Test intersection between Range and Integers\n    assert intersection_sets(Range(1, 10), Z) == Range(1, 10)\n\n    # Test intersection between Range and Naturals\n    assert intersection_sets(Range(0, 10), N) == Range(1, 10)\n\n    # Test intersection between Interval and Naturals\n    assert intersection_sets(Interval(0, 10), N) == Range(1, 11)\n\n    # Test intersection between Interval and Integers\n    assert intersection_sets(Interval(0, 10), Z) == Range(0, 11)\n\n    # Test intersection between FiniteSet and Set\n    assert intersection_sets(FiniteSet(1, 2, 3), Interval(2, 4)) == FiniteSet(2, 3)\n\n    # Test intersection between ProductSets\n    assert intersection_sets(ProductSet(Interval(0, 1), Interval(1, 2)), ProductSet(Interval(0, 1), Interval(1, 2))) == ProductSet(Interval(0, 1), Interval(1, 2))\n", "def test_intersection_different_sets():\n    from sympy.sets.sets import UniversalSet, EmptySet\n\n    # Intersection of UniversalSet with various sets\n    assert intersection_sets(UniversalSet(), S.Reals) == S.Reals\n    assert intersection_sets(UniversalSet(), S.Integers) == S.Integers\n    assert intersection_sets(UniversalSet(), FiniteSet(1, 2, 3)) == FiniteSet(1, 2, 3)\n    assert intersection_sets(UniversalSet(), Interval(1, 5)) == Interval(1, 5)\n    assert intersection_sets(UniversalSet(), EmptySet()) == EmptySet()\n\n    # Intersection of EmptySet with various sets\n    assert intersection_sets(EmptySet(), S.Reals) == EmptySet()\n    assert intersection_sets(EmptySet(), S.Integers) == EmptySet()\n    assert intersection_sets(EmptySet(), FiniteSet(1, 2, 3)) == EmptySet()\n    assert intersection_sets(EmptySet(), Interval(1, 5)) == EmptySet()\n    assert intersection_sets(EmptySet(), UniversalSet()) == EmptySet()\n\n    # Intersection of FiniteSet with different sets\n    assert intersection_sets(FiniteSet(1, 2, 3), FiniteSet(2, 3, 4)) == FiniteSet(2, 3)\n    assert intersection_sets(FiniteSet(1, 2, 3), Interval(1, 2)) == FiniteSet(1, 2)\n    assert intersection_sets(FiniteSet(1, 2, 3), S.Integers) == FiniteSet(1, 2, 3)\n    assert intersection_sets(FiniteSet(1, 2, 3), S.Reals) == FiniteSet(1, 2, 3)\n    assert intersection_sets(FiniteSet(1, 2, 3), UniversalSet()) == FiniteSet(1, 2, 3)\n\n    # Intersection of Interval with different sets\n    assert intersection_sets(Interval(1, 5), Interval(3, 7)) == Interval(3, 5)\n    assert intersection_sets(Interval(1, 5), S.Integers) == FiniteSet(1, 2, 3, 4, 5)\n    assert intersection_sets(Interval(1, 5), FiniteSet", "def test_ComplexRegion_intersect_finite_set():\n    # Test intersections involving ComplexRegion and FiniteSet\n    c1 = ComplexRegion(Interval(1, 3)*Interval(2, 4))\n    f1 = FiniteSet(2 + 3*I, 4 + 5*I, 1 + 2*I)\n    f2 = FiniteSet(1 + 2*I, 5 + 6*I)\n\n    assert c1.intersect(f1) == FiniteSet(2 + 3*I, 1 + 2*I)\n    assert c1.intersect(f2) == FiniteSet(1 + 2*I)\n    assert c1.intersect(FiniteSet(5 + 6*I)) is S.EmptySet\n"], "sample_1085": ["def test_comp_function():\n    # Test against various number formats and tolerances\n    assert comp(3.142, 3.142)\n    assert not comp(3.142, 3.141)\n    assert not comp(3.142, 3.143)\n    assert comp(sqrt(2), 1.414213562, tol=1e-9)\n    assert not comp(sqrt(2), 1.414213562, tol=1e-10)\n    assert comp(S.Half, 0.5)\n    assert not comp(S.Half, 0.500001, tol=1e-6)\n    assert comp(1/pi, 0.31831, tol=1e-5)\n    assert not comp(1/pi, 0.31831, tol=1e-6)\n    assert comp(sqrt(2) - sqrt(2), 0)\n    assert comp(sqrt(2)*I, sqrt(2)*I)\n    assert not comp(sqrt(2)*I, sqrt(3)*I)\n    assert comp(pi.evalf(), 3.14159, tol=1e-5)\n    assert not comp(pi.evalf(), 3.14159, tol=1e-6)\n    raises(ValueError, lambda: comp(pi, '3.14159'))\n    raises(ValueError, lambda: comp(sqrt(2), '1.414'))\n    raises(ValueError, lambda: comp(1/pi, '0.318'))\n", "def test_mpf_norm():\n    assert mpf_norm((0, 1, 1, 2), 2) == (0, 1, 1, 2)\n    assert mpf_norm((0, 1, 1, 0), 2) == (0, 1, 1, 2)\n    assert mpf_norm((0, 0, 1, 0), 2) == (0, 0, 0, 0)\n    assert mpf_norm((1, 0, 1, 0), 2) == (1, 0, 0, 0)\n    assert mpf_norm((0, 2, 3, 4), 5) == (0, 2, 3, 4)\n    assert mpf_norm((1, 5, 0, 4), 3) == (1, 5, 0, 4)\n    assert mpf_norm((0, 1, 2, 2), 2) == (0, 1, 2, 2)\n    assert mpf_norm((0, 1, 1, 1), 2) == (0, 1, 1, 1)\n", "def test_comp():\n    # Test comp for both exact and inexact comparisons\n    assert comp(1.2345, 1.2345) is True\n    assert comp(1.2345, 1.234) is False\n    assert comp(1.2345, 1.234, tol=0.001) is True\n    assert comp(1.2345, '1.2345') is True\n    assert comp(1.2345, '1.2345', tol='') is True\n    assert comp(1.2345, '1.2346') is False\n    assert comp(1.2345, '1.2345', tol=None) is True\n    assert comp(1.2345, '1.2346', tol=None) is False\n    assert comp(1.2345, 1.2345, tol='') is True\n    assert comp(1.2345, 1.2346, tol='') is False\n    assert comp(1.2345, 1.2345, tol=0.0001) is True\n    assert comp(1.2345, 1.2346, tol=0.0001) is False\n\n    # Test for complex numbers\n    assert comp(1+1j, 1+1j) is True\n    assert comp(1+1j, 1+1.0001j, tol=0.0002) is True\n    assert comp(1+1j, 1+1.0001j, tol=0.00005) is False\n\n    # Test for sympy objects\n    assert comp(pi.evalf(4), 3.142) is True\n    assert comp(pi.evalf(4), 3.141) is False\n    assert comp(pi.evalf(4), 3.142, tol=0.001) is True\n    assert comp(pi.evalf(4), 3.141, tol=0.001) is False\n    assert comp(sqrt(2).evalf(4), 1.414) is True\n    assert comp(sqrt(2).evalf(4), 1.413) is False\n\n    # Edge cases\n    assert comp(0, 0) is True\n    assert comp(0, 0.0) is True\n    assert", "def test_floordiv_infinity():\n    assert oo // 1 == oo\n    assert -oo // 1 == -oo\n    assert oo // 2 == oo\n    assert -oo // 2 == -oo\n    assert oo // -1 == -oo\n    assert -oo // -1 == oo\n    assert oo // -2 == -oo\n    assert -oo // -2 == oo\n    raises(ZeroDivisionError, lambda: oo // 0)\n    raises(ZeroDivisionError, lambda: -oo // 0)\n", "def test_mpf_norm():\n    assert mpf_norm((1, 0, 1, 0), 10) == mpf('0')._mpf_\n    assert mpf_norm((1, 0, 1, 0), 15) == mpf('0')._mpf_\n    assert mpf_norm((1, 123456789, 10, 30), 15) == (1, 123456789, 10, 30)\n    assert mpf_norm((0, 123456789, 10, 30), 15) == (0, 123456789, 10, 30)\n    assert mpf_norm((0, 0, 0, 0), 15) == (0, 0, 0, 0)\n    assert mpf_norm((0, 123456789, 0, 0), 15) == (0, 123456789, 0, 0)\n    assert mpf_norm((1, 0, -123, -1), 10) == mpf('0')._mpf_\n    assert mpf_norm((1, 0, -456, -2), 10) == mpf('0')._mpf_\n    assert mpf_norm((1, 0, -789, -3), 10) == mpf('0')._mpf_\n    assert mpf_norm((1, 123456789, 10, 53), 53) == (1, 123456789, 10, 53)\n    assert mpf_norm((0, 123456789, 10, 53), 53) == (0, 123456789, 10, 53)\n"], "sample_1086": ["def test_MatAdd():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = MatrixSymbol(\"C\", 2, 2)\n    assert str(A + B + C) == \"A + B + C\"\n    assert str(A - B + C) == \"A - B + C\"\n    assert str(A + (B - C)) == \"A + B - C\"\n    assert str((A - B) + C) == \"A - B + C\"\n", "def test_EmptySequence():\n    from sympy import EmptySequence\n    assert str(EmptySequence) == 'EmptySequence'\n", "def test_ElementwiseApplyFunction():\n    from sympy.matrices.expressions import MatrixSymbol\n    f = Function('f')\n    X = MatrixSymbol('X', 2, 2)\n    expr = f(X)\n    assert str(expr) == 'f(X...)'\n    expr = ElementwiseApplyFunction(f, X)\n    assert str(expr) == 'f(X...)'\n", "def test_AlgebraicNumber():\n    from sympy import AlgebraicNumber\n    alpha = AlgebraicNumber(sqrt(2), [3, 0, 1])\n    assert str(alpha) == \"3 + sqrt(2)\"\n    beta = AlgebraicNumber(sqrt(2), [1, 1])\n    assert str(beta) == \"sqrt(2) + 1\"\n    gamma = AlgebraicNumber(sqrt(2), [1, 0, 1])\n    assert str(gamma) == \"sqrt(2)**2 + 1\"\n", "def test_Mul_negative_coefficients():\n    assert str(-2 * x * y) == \"-2*x*y\"\n    assert str(-2 * x * y / 3) == \"-2*x*y/3\"\n    assert str(-2 * x * y / z) == \"-2*x*y/z\"\n    assert str(-2 * x / y / z) == \"-2*x/(y*z)\"\n    assert str(-2 * x * y * z**-1) == \"-2*x*y*z**(-1)\"\n"], "sample_1087": ["def test_fateman_polys():\n    # Test _f_0 to _f_6\n    f0, f1, f2, f3, f4, f5, f6 = f_polys()\n    assert f0 == Poly(x**2 * y * z**2 + 2 * x**2 * y * z + 3 * x**2 * y + 2 * x**2 + 3 * x + 4 * y**2 * z**2 + 5 * y**2 * z + 6 * y**2 + y * z**2 + 2 * y * z + y + 1, x, y, z)\n    assert f1 == Poly(x**3 * y * z + x**2 * y**2 * z**2 + x**2 * y**2 + 20 * x**2 * y * z + 30 * x**2 * y + x**2 * z**2 + 10 * x**2 * z + x * y**3 * z + 30 * x * y**2 * z + 20 * x * y**2 + x * y * z**3 + 10 * x * y * z**2 + x * y * z + 610 * x * y + 20 * x * z**2 + 230 * x * z + 300 * x + y**2 * z**2 + 10 * y**2 * z + 30 * y * z**2 + 320 * y * z + 200 * y + 600 * z + 6000, x, y, z)\n    assert f2 == Poly(x**5 * y**3 + x**5 * y**2 * z + x**5 * y * z**2 + x**5 * z**3 + x**3 * y**2 + x**3 * y * z + 90 * x**3 * y + 90 * x**3 * z + x**2 * y**2 * z - 11 * x**2 * y**2 + x**2 * z**3 - 11 * x**2 * z**2 + y * z - 11 * y + 90 * z - 990, x, y, z)\n    assert f3 == Poly(x**5 * y**2 + x**4 * z", "def test_wangs_polynomials():\n    from sympy.polys.specialpolys import f_polys, w_polys\n    R, x, y, z = ring(\"x,y,z\", ZZ)\n\n    # Wang's polynomials tests\n    expected_f_polys = [\n        x**2*y*z**2 + 2*x**2*y*z + 3*x**2*y + 2*x**2 + 3*x + 4*y**2*z**2 + 5*y**2*z + 6*y**2 + y*z**2 + 2*y*z + y + 1,\n        x**3*y*z + x**2*y**2*z**2 + x**2*y**2 + 20*x**2*y*z + 30*x**2*y + x**2*z**2 + 10*x**2*z + x*y**3*z + 30*x*y**2*z + 20*x*y**2 + x*y*z**3 + 10*x*y*z**2 + x*y*z + 610*x*y + 20*x*z**2 + 230*x*z + 300*x + y**2*z**2 + 10*y**2*z + 30*y*z**2 + 320*y*z + 200*y + 600*z + 6000,\n        x**5*y**3 + x**5*y**2*z + x**5*y*z**2 + x**5*z**3 + x**3*y**2 + x**3*y*z + 90*x**3*y + 90*x**3*z + x**2*y**2*z - 11*x**2*y**2 + x**2*z**3 - 11*x**2*z**2 + y*z - 11*y + 90*z - 990,\n        x**5*y**2 + x**4*z**4 + x**4 + x**3*y**3*z + x**3*z + x**2*y**4 + x**2*y**3*z**3 + x**2*y*z**5 + x**2*y*z + x*y**2*z**4 + x*y**2 + x*y*z**7 + x*y*z**3 + x*y*z**2 + y**2*z + y*z**4,\n        -x**9*y**8*z - x**8*y**5*z**3 - x", "def test_wang_polys():\n    from sympy.polys.specialpolys import f_polys, w_polys\n\n    f0, f1, f2, f3, f4, f5, f6 = f_polys()\n    w1, w2 = w_polys()\n\n    assert f0 == _f_0()\n    assert f1 == _f_1()\n    assert f2 == _f_2()\n    assert f3 == _f_3()\n    assert f4 == _f_4()\n    assert f5 == _f_5()\n    assert f6 == _f_6()\n\n    assert w1 == _w_1()\n    assert w2 == _w_2()\n", "def test_wang_polys():\n    from sympy.polys.specialpolys import _f_0, _f_1, _f_2, _f_3, _f_4, _f_5, _f_6, _w_1, _w_2, f_polys, w_polys\n    \n    f0, f1, f2, f3, f4, f5, f6 = f_polys()\n    w1, w2 = w_polys()\n\n    assert f0 == _f_0()\n    assert f1 == _f_1()\n    assert f2 == _f_2()\n    assert f3 == _f_3()\n    assert f4 == _f_4()\n    assert f5 == _f_5()\n    assert f6 == _f_6()\n\n    assert w1 == _w_1()\n    assert w2 == _w_2()\n", "def test_wang_polys():\n    from sympy.polys.specialpolys import _f_0, _f_1, _f_2, _f_3, _f_4, _f_5, _f_6, _w_1, _w_2\n\n    f0 = _f_0()\n    f1 = _f_1()\n    f2 = _f_2()\n    f3 = _f_3()\n    f4 = _f_4()\n    f5 = _f_5()\n    f6 = _f_6()\n    w1 = _w_1()\n    w2 = _w_2()\n\n    # Validate that the polynomials are instances of the correct class\n    assert isinstance(f0, f0.__class__)\n    assert isinstance(f1, f1.__class__)\n    assert isinstance(f2, f2.__class__)\n    assert isinstance(f3, f3.__class__)\n    assert isinstance(f4, f4.__class__)\n    assert isinstance(f5, f5.__class__)\n    assert isinstance(f6, f6.__class__)\n    assert isinstance(w1, w1.__class__)\n    assert isinstance(w2, w2.__class__)\n\n    # Validate the degrees of some polynomials\n    assert f0.degree() == 4\n    assert f1.degree() == 3\n    assert f2.degree() == 5\n    assert f3.degree() == 7\n    assert f4.degree() == 9\n    assert f5.degree() == 3\n    assert f6.degree() == 4\n    assert w1.degree() == 6\n    assert w2.degree() == 8\n"], "sample_1088": ["def test_viete_multivariate_error():\n    r1, r2 = symbols('r1, r2')\n    raises(MultivariatePolynomialError, lambda: viete(x*y + x, [r1, r2], x, y))\n", "def test_symmetrize_edge_cases():\n    # Test with no variables\n    assert symmetrize(x**2 + y**2, formal=True, symbols=()) == (S.Zero, S.Zero, [])\n    # Test with only one variable\n    assert symmetrize(x**2, x) == (x**2, 0)\n    # Test with formal=True and symbols provided\n    s = symbols('s')\n    assert symmetrize(x**2 + y**2, formal=True, symbols=[s]) == (s**2 - 2*x*y, 0, [(s, x + y)])\n    # Test with non-symmetric polynomial\n    assert symmetrize(x**3 + 2*y**2, x, y) == (x**3 + 3*x*y**2, -y**2)\n    # Test with variables not in polynomial\n    assert symmetrize(x**2, y, z) == (x**2, 0)\n", "def test_rational_interpolate_symbolic():\n    x, y = symbols('x,y')\n    data = [(1, x + y), (2, x*y), (3, x**2 + y**2)]\n    result = rational_interpolate(data, 2)\n    expected_result = (x**2*y - 2*x*y + x**2 + y**2 - y)/(3*x - 2)\n    assert result == expected_result\n", "def test_horner_multivariate():\n    assert horner(4*x**2*y**2 + 2*x**2*y + 2*x*y**2 + x*y) == (4*x**2 + 2*x)*y**2 + (2*x + 1)*y\n    assert horner(4*x**2*y**2 + 2*x**2*y + 2*x*y**2 + x*y + z, wrt=z) == z + y*(x*(2*y*(2*x + 1) + x*(4*y + 2)))\n    assert horner(a*x*y + b*y + c*x + d, wrt=x) == a*x*y + b*y + c*x + d\n    assert horner(a*x*y + b*y + c*x + d, wrt=y) == y*(a*x + b) + c*x + d\n", "def test_viete_multivariate_error():\n    r1, r2 = symbols('r1, r2')\n    raises(MultivariatePolynomialError, lambda: viete(a*x*y**2 + b*x + c, [r1, r2], x, y))\n"], "sample_1089": ["def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(y*Rational(2, 3))) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(2.5*y)) == (x**(2.5*y), 1)\n    assert decompose_power_rat(x**(S.Half)) == (x**(S.Half), 1)\n", "def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x, 2*y)\n    assert decompose_power_rat(x**(2*y/3)) == (x, 2*y/3)\n    assert decompose_power_rat(x**(y*Rational(2, 3))) == (x, 2*y/3)\n    assert decompose_power_rat(x**sqrt(2)) == (x, sqrt(2))\n", "def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(y*Rational(2, 3))) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(2.5)) == (x, 2.5)\n", "def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(y/3)) == (x**(y/3), 1)\n    assert decompose_power_rat(x**(y*Rational(2, 3))) == (x**(y/3), 2)\n    assert decompose_power_rat(2**(3*x + 1)) == (2, 3*x + 1)\n", "def test_decompose_power_rat():\n    assert decompose_power_rat(x) == (x, 1)\n    assert decompose_power_rat(x**2) == (x, 2)\n    assert decompose_power_rat(x**(2*y)) == (x**y, 2)\n    assert decompose_power_rat(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power_rat(x**(y*Rational(2, 3))) == (x**(y/3), 2)\n    assert decompose_power_rat(x**sqrt(2)) == (x**sqrt(2), 1)\n    assert decompose_power_rat((x*y)**(2/3)) == ((x*y)**Rational(1, 3), 2)\n"], "sample_1090": ["def test_operations_with_nan():\n    with evaluate(False):\n        # Test addition with nan\n        assert nan + 1 == nan\n        assert 1 + nan == nan\n        assert nan + nan == nan\n\n        # Test subtraction with nan\n        assert nan - 1 == nan\n        assert 1 - nan == nan\n        assert nan - nan == nan\n\n        # Test multiplication with nan\n        assert nan * 1 == nan\n        assert 1 * nan == nan\n        assert nan * nan == nan\n\n        # Test division with nan\n        assert nan / 1 == nan\n        assert 1 / nan == nan\n        assert nan / nan == nan\n\n        # Test power with nan\n        assert nan ** 1 == nan\n        assert 1 ** nan == nan\n        assert nan ** nan == nan\n\n    with evaluate(True):\n        assert nan + 1 == nan\n        assert 1 + nan == nan\n        assert nan + nan == nan\n\n        assert nan - 1 == nan\n        assert 1 - nan == nan\n        assert nan - nan == nan\n\n        assert nan * 1 == nan\n        assert 1 * nan == nan\n        assert nan * nan == nan\n\n        assert nan / 1 == nan\n        assert 1 / nan == nan\n        assert nan / nan == nan\n\n        assert nan ** 1 == nan\n        assert 1 ** nan == nan\n        assert nan ** nan == nan\n", "def test_comp():\n    from sympy.core.numbers import comp, pi\n    pi4 = pi.n(4)\n    \n    # Test tolerance-based comparison\n    assert comp(pi4, 3.142) is True\n    assert comp(pi4, 3.141) is False\n    assert comp(pi4, 3.143) is False\n    \n    # Test comparison with string tolerance\n    assert comp(pi4, 3.1415) is True\n    assert comp(pi4, 3.1415, '') is False\n    \n    # Test comparison with non-zero tolerance\n    assert comp(pi4, 3.14, 0.001) is True\n    assert comp(pi4, 3.14, 0.0005) is False\n    \n    # Test comparison with small values\n    assert comp(1/pi4, 0.3183, 1e-5) is True\n    \n    # Test absolute error comparison\n    assert comp(pi4 - 3.14, 0, .002) is True\n    assert comp(pi4 - 3.14, 0, .001) is False\n    \n    # Test comparison with zero\n    assert comp(0, 0) is True\n    assert comp(0, 0, 0.001) is True\n", "def test_sub():\n    with evaluate(False):\n        p = oo - 5\n        assert isinstance(p, Add) and p.args == (oo, -5)\n        p = 5 - oo\n        assert isinstance(p, Add) and p.args == (-oo, 5)\n        p = -oo - 5\n        assert isinstance(p, Add) and p.args == (-oo, -5)\n        p = 5 - (-oo)\n        assert isinstance(p, Add) and p.args == (5, oo)\n        p = (-oo) - 5\n        assert isinstance(p, Add) and p.args == (-oo, -5)\n\n    with evaluate(False):\n        expr = x - x\n        assert isinstance(expr, Add)\n        assert expr.args == (x, -x)\n\n        with evaluate(True):\n            assert (x - x).args == (0,)\n\n        assert (x - x).args == (x, -x)\n\n    assert isinstance(x - x, Add)\n\n    with evaluate(False):\n        assert S.One - 1 == Add(1, -1)\n        assert 1 - S.One == Add(1, -1)\n\n        assert S(4) - 3 == Add(4, -3)\n        assert -3 - S(4) == Add(-3, -4)\n\n        assert S(2) * 4 == Mul(2, 4)\n        assert 4 * S(2) == Mul(2, 4)\n\n        assert S(6) / 3 == Mul(6, S.One / 3)\n        assert S.One / 3 * 6 == Mul(S.One / 3, 6)\n\n        assert 9 ** S(2) == Pow(9, 2)\n        assert S(2) ** 9 == Pow(2, 9)\n\n        assert S(2) / 2 == Mul(2, S.One / 2)\n        assert S.One / 2 * 2 == Mul(S.One / 2, 2)\n\n        assert S(2) / 3 - 1 == Add(S(2) / 3, -1)\n        assert 1 - S(2) / 3 == Add(1, -S(2) / 3)\n\n        assert S(4) / 7 - 3 == Add(S(4) / 7, -3)\n        assert -3 -", "def test_comp():\n    assert comp(3.142, 3.142) == True\n    assert comp(3.142, 3.141) == False\n    assert comp(3.142, 3.143) == False\n    assert comp(3.142, 3.1415) == True\n    assert comp(3.142, 3.1415, '') == False\n    assert comp(3.142, 3.14, 0.001) == True\n    assert comp(3.142, 3.14, 0.0005) == False\n    assert comp(1/3.142, 0.3183, 1e-5) == True\n    assert comp(3.142 - 3.14, 0, 0.002) == True\n    assert comp(3.142 - 3.14, 0, 0.001) == False\n\n    with evaluate(False):\n        assert comp(\"3.142\", \"3.142\") == True\n        assert comp(\"3.142\", \"3.141\") == False\n\n    try:\n        comp(3.142, 'abc')\n    except ValueError:\n        pass\n    else:\n        assert False, \"Expected ValueError when comparing to a non-numeric string.\"\n", "def test_comp():\n    assert comp(3.142, 3.142) == True\n    assert comp(3.142, 3.141) == False\n    assert comp(3.142, 3.143) == False\n    assert comp(3.142, '3.142') == True\n    assert comp(3.142, '3.141', '') == False\n    assert comp(3.142, 3.14, 0.001) == True\n    assert comp(3.142, 3.14, 0.0005) == False\n    assert comp(1/3.142, 0.3183, 1e-5) == True\n    assert comp(3.142 - 3.14, 0, 0.002) == True\n    assert comp(3.142 - 3.14, 0, 0.001) == False\n    try:\n        comp('not a number', 'string')\n    except ValueError as e:\n        assert str(e) == 'when z2 is a str z1 must be a Number'\n    try:\n        comp(3.142, 'string')\n    except ValueError as e:\n        assert str(e) == 'expecting 2 numbers'\n"], "sample_1091": ["def test_relational_canonical_real():\n    a, b = symbols('a b', real=True)\n    expr1 = (a + b < a - b)\n    expr2 = (a - b > a + b)\n    expr3 = (a + b <= a - b)\n    expr4 = (a - b >= a + b)\n\n    assert expr1.canonical == expr2.canonical\n    assert expr3.canonical == expr4.canonical\n\n    expr5 = (a**2 + b**2 < 1)\n    expr6 = (1 > a**2 + b**2)\n    expr7 = (-a**2 - b**2 > -1)\n    expr8 = (-1 < -a**2 - b**2)\n\n    assert expr5.canonical == expr6.canonical\n    assert expr7.canonical == expr8.canonical\n\n    expr9 = Eq(a + 2*b, a - 2*b)\n    expr10 = Eq(a - 2*b, a + 2*b)\n    expr11 = Ne(a + 2*b, a - 2*b)\n    expr12 = Ne(a - 2*b, a + 2*b)\n\n    assert expr9.canonical == expr10.canonical\n    assert expr11.canonical == expr12.canonical\n", "def test_equality_simplification():\n    # Test simplification of Equality with different types of expressions\n    assert Eq(x + 1 - 1, y).simplify() == Eq(x, y)\n    assert Eq(x - x, 0).simplify() == S.true\n    assert Eq(x**2 - 2*x + 1, 0).simplify() == Eq((x - 1)**2, 0)\n    assert Eq(sin(x)**2 + cos(x)**2, 1).simplify() == S.true\n\n    # Test simplification of Equality with symbolic expressions\n    a, b = symbols('a b')\n    assert Eq(a + b, b + a).simplify() == S.true\n    assert Eq(a*b, b*a).simplify() == S.true\n\n    # Test simplification of Equality with complex expressions\n    assert Eq(exp(I*x), cos(x) + I*sin(x)).simplify() == S.true\n\n    # Test simplification of Equality with trigonometric identities\n    assert Eq(sin(2*x), 2*sin(x)*cos(x)).simplify() == S.true\n", "def test_relational_subclass_eval():\n    # Ensure that custom subclass of Relational evaluates correctly\n    class CustomRelational(Relational):\n        __slots__ = ()\n        rel_op = 'custom_op'\n\n        @classmethod\n            if lhs + rhs > 5:\n                return S.true\n            return S.false\n\n    assert CustomRelational(3, 3) is S.true\n    assert CustomRelational(1, 2) is S.false\n    assert CustomRelational(4, 1) is S.false\n    assert CustomRelational(2, 4) is S.true\n", "def test_relational_properties():\n    r1 = Ge(x, y)\n    r2 = Le(x, y)\n    r3 = Gt(x, y)\n    r4 = Lt(x, y)\n    \n    assert r1.is_Relational == True\n    assert r2.is_Relational == True\n    assert r3.is_Relational == True\n    assert r4.is_Relational == True\n    \n    assert r1.reversed == Le(y, x)\n    assert r2.reversed == Ge(y, x)\n    assert r3.reversed == Lt(y, x)\n    assert r4.reversed == Gt(y, x)\n    \n    assert r1.reversedsign == Le(-x, -y)\n    assert r2.reversedsign == Ge(-x, -y)\n    assert r3.reversedsign == Lt(-x, -y)\n    assert r4.reversedsign == Gt(-x, -y)\n    \n    assert r1.negated == Lt(x, y)\n    assert r2.negated == Gt(x, y)\n    assert r3.negated == Le(x, y)\n    assert r4.negated == Ge(x, y)\n", "def test_relational_as_set_multivariate():\n    x, y = symbols('x y')\n    assert (x*y > 1).as_set() == ConditionSet((x, y), x*y > 1, Reals*Reals)\n    assert (x*y < 1).as_set() == ConditionSet((x, y), x*y < 1, Reals*Reals)\n    assert (x**2 + y**2 <= 1).as_set() == ConditionSet((x, y), x**2 + y**2 <= 1, Reals*Reals)\n    assert (x + y > 2).as_set() == ConditionSet((x, y), x + y > 2, Reals*Reals)\n    assert (x**2 - y**2 == 0).as_set() == ConditionSet((x, y), x**2 - y**2 == 0, Reals*Reals)\n"], "sample_1092": ["def test_funcargtracker():\n    funcs = [Add(x, y), Mul(x, y), Pow(x, 2)]\n    tracker = cse_main.FuncArgTracker(funcs)\n    \n    assert tracker.get_args_in_value_order({0, 1}) == [x, y]\n    \n    arg_num_x = tracker.get_or_add_value_number(x)\n    arg_num_y = tracker.get_or_add_value_number(y)\n    assert arg_num_x == 0\n    assert arg_num_y == 1\n\n    assert tracker.func_to_argset == [\n        OrderedSet([0, 1]), \n        OrderedSet([0, 1]), \n        OrderedSet([0, 2])\n    ]\n\n    common_candidates = tracker.get_common_arg_candidates({0, 1})\n    assert common_candidates == {1: 2}\n    \n    subset_candidates = tracker.get_subset_candidates({0, 1})\n    assert subset_candidates == {0, 1}\n    \n    tracker.update_func_argset(1, OrderedSet([0, 2]))\n    assert tracker.func_to_argset[1] == OrderedSet([0, 2])\n    \n    tracker.stop_arg_tracking(1)\n    assert tracker.arg_to_funcset[0] == OrderedSet([0, 2])\n    assert tracker.arg_to_funcset[1] == OrderedSet([0])\n", "def test_func_arg_tracker():\n    from sympy.core.compatibility import ordered\n\n    a, b, c, d = symbols('a b c d')\n    func1 = Add(a, b)\n    func2 = Mul(b, c)\n    func3 = Add(a, c)\n    func4 = Add(b, d)\n    \n    funcs = [func1, func2, func3, func4]\n    tracker = cse_main.FuncArgTracker(funcs)\n\n    # Test get_or_add_value_number\n    assert tracker.get_or_add_value_number(a) == 0\n    assert tracker.get_or_add_value_number(b) == 1\n    assert tracker.get_or_add_value_number(c) == 2\n\n    # Test get_args_in_value_order\n    assert tracker.get_args_in_value_order({1, 0}) == [a, b]\n\n    # Test get_common_arg_candidates\n    assert tracker.get_common_arg_candidates({0, 1}) == {2: 1, 3: 1}\n    assert tracker.get_common_arg_candidates({1, 3}) == {0: 1}\n\n    # Test get_subset_candidates\n    assert tracker.get_subset_candidates({0, 1}) == {0, 3}\n    assert tracker.get_subset_candidates({1, 3}) == {3}\n\n    # Test update_func_argset\n    tracker.update_func_argset(0, {0, 2})\n    assert tracker.func_to_argset[0] == {0, 2}\n    assert tracker.arg_to_funcset[1] == OrderedSet([1, 3])\n\n    # Test stop_arg_tracking\n    tracker.stop_arg_tracking(1)\n    assert tracker.arg_to_funcset[1] == OrderedSet([3])\n", "def test_funcargtracker():\n    from sympy.core.containers import OrderedSet\n\n    f1, f2, f3 = symbols('f1 f2 f3', cls=Function)\n    x, y, z = symbols('x y z')\n\n    funcs = [f1(x, y), f2(x, z), f3(y, z)]\n    tracker = cse_main.FuncArgTracker(funcs)\n\n    # Test value number assignment\n    assert tracker.value_numbers == {x: 0, y: 1, z: 2}\n\n    # Test argument to function set mapping\n    assert tracker.arg_to_funcset == [\n        OrderedSet([0, 1]),  # x appears in f1 and f2\n        OrderedSet([0, 2]),  # y appears in f1 and f3\n        OrderedSet([1, 2])   # z appears in f2 and f3\n    ]\n\n    # Test function to argument set mapping\n    assert tracker.func_to_argset == [\n        OrderedSet([0, 1]),  # f1 has args x, y\n        OrderedSet([0, 2]),  # f2 has args x, z\n        OrderedSet([1, 2])   # f3 has args y, z\n    ]\n\n    # Test common argument candidates\n    common_arg_candidates = tracker.get_common_arg_candidates(OrderedSet([0, 1]))\n    assert common_arg_candidates == {2: 1}  # f3 has 1 argument in common\n\n    # Test subset candidates\n    subset_candidates = tracker.get_subset_candidates(OrderedSet([0, 1]))\n    assert subset_candidates == OrderedSet([0])  # Only f1 has args x, y\n\n    # Test updating function argument set\n    tracker.update_func_argset(0, OrderedSet([0, 2]))\n    assert tracker.func_to_argset[0] == OrderedSet([0, 2])\n    assert tracker.arg_to_funcset[0] == OrderedSet([1])\n    assert tracker.arg_to_funcset[2] == OrderedSet([0, 1, 2])\n    assert tracker.arg_to_funcset[1] == OrderedSet([2])\n\n    # Test stopping argument tracking\n    tracker.stop_arg_tracking(1)\n    assert tracker.arg_to_funcset[0] == OrderedSet([1])\n    assert tracker.arg_to_funcset[2] == OrderedSet([", "def test_func_arg_tracker():\n    func1 = Add(x, y)\n    func2 = Add(y, z)\n    func3 = Mul(x, y)\n    func4 = Mul(y, z)\n\n    tracker = cse_main.FuncArgTracker([func1, func2, func3, func4])\n\n    assert tracker.get_or_add_value_number(x) == 0\n    assert tracker.get_or_add_value_number(y) == 1\n    assert tracker.get_or_add_value_number(z) == 2\n\n    common_args = tracker.get_common_arg_candidates(tracker.func_to_argset[0], min_func_i=1)\n    assert common_args == {1: 1}\n\n    subset_candidates = tracker.get_subset_candidates(tracker.func_to_argset[0])\n    assert subset_candidates == OrderedSet([0, 1])\n\n    tracker.update_func_argset(0, OrderedSet([0, 1, 2]))\n    assert tracker.func_to_argset[0] == OrderedSet([0, 1, 2])\n\n    tracker.stop_arg_tracking(0)\n    assert tracker.arg_to_funcset[0] == OrderedSet()\n    assert tracker.arg_to_funcset[1] == OrderedSet([1, 2, 3])\n    assert tracker.arg_to_funcset[2] == OrderedSet([1, 3])\n", "def test_func_arg_tracker():\n    funcs = [Mul(x, y), Mul(y, z), Add(x, y, z), Add(x, y)]\n    tracker = cse_main.FuncArgTracker(funcs)\n    # Checking initial setup\n    assert len(tracker.value_numbers) == 4\n    assert tracker.get_or_add_value_number(x) == 0\n    assert tracker.get_or_add_value_number(y) == 1\n    assert tracker.get_or_add_value_number(z) == 2\n    assert tracker.get_or_add_value_number(Add(x, y)) == 4\n    assert tracker.get_args_in_value_order([0, 1, 2]) == [x, y, z]\n    # Checking common argument candidates\n    assert tracker.get_common_arg_candidates({0, 1}, min_func_i=1) == {2: 2}\n    # Checking subset candidates\n    assert tracker.get_subset_candidates({0, 1}) == {0, 2, 3}\n    # Updating function argument set and checking\n    tracker.update_func_argset(0, {1, 2})\n    assert tracker.func_to_argset[0] == {1, 2}\n    assert tracker.arg_to_funcset[0] == OrderedSet()\n    assert tracker.arg_to_funcset[1] == {0, 2, 3}\n    # Stopping argument tracking and checking\n    tracker.stop_arg_tracking(0)\n    assert tracker.arg_to_funcset[1] == {2, 3}\n    assert tracker.arg_to_funcset[2] == OrderedSet()\n"], "sample_1093": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(Rational(1, 2)) == '1/2'\n\n    assert p.doprint(S.Exp1) == 'sympy.E'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    assert p.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n    assert p.doprint(S.EulerGamma) == 'sympy.EulerGamma'\n    assert p.doprint(S.NaN) == 'sympy.nan'\n    assert p.doprint(S.Infinity) == 'sympy.oo'\n    assert p.doprint(S.NegativeInfinity) == 'sympy.oo'\n\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(A**(-1)) == \"A**(-1)\"\n    assert p.doprint(A**5) == \"A**5\"\n    assert p.doprint(Identity(3)) == \"sympy.eye(3)\"\n\n    assert p.doprint(MatrixSolve(A, x)) == 'A.solve(x)'\n    assert p.doprint(MatrixSolve(A, x) + y) == 'A.solve(x) + y'\n\n    assert p.doprint(ZeroMatrix(2, 3)) == \"sympy.zeros(2, 3)\"\n    assert p.doprint(OneMatrix(2, 3)) == \"sympy.ones(2, 3)\"\n    assert p.doprint(FunctionMatrix(4, 5, Lambda((a, b), a + b))) == \\\n        \"sympy.Lambda((a, b), a + b)(*sympy.indices(4, 5))\"\n\n    # Workaround for sympy negative integer power errors\n    assert p.doprint(x**-1) == 'x**-1'\n    assert p.doprint(x**-2) == 'x**-2'\n", "def test_print_FunctionDefinition():\n    from sympy.codegen.ast import FunctionDefinition\n    from sympy import symbols\n    i, j = symbols('i j')\n    body = [Assignment(i, i + 1), Assignment(j, j - 1)]\n    fd = FunctionDefinition('test_func', [i, j], body)\n    \n    prntr = PythonCodePrinter()\n    expected_output = \"def test_func(i, j):\\n    i = i + 1\\n    j = j - 1\"\n    assert prntr.doprint(fd) == expected_output\n\n    prntr = NumPyPrinter()\n    expected_output_numpy = \"def test_func(i, j):\\n    i = i + 1\\n    j = j - 1\"\n    assert prntr.doprint(fd) == expected_output_numpy\n\n    prntr = SciPyPrinter()\n    expected_output_scipy = \"def test_func(i, j):\\n    i = i + 1\\n    j = j - 1\"\n    assert prntr.doprint(fd) == expected_output_scipy\n\n    prntr = MpmathPrinter()\n    expected_output_mpmath = \"def test_func(i, j):\\n    i = i + 1\\n    j = j - 1\"\n    assert prntr.doprint(fd) == expected_output_mpmath\n\n    prntr = SymPyPrinter()\n    expected_output_sympy = \"def test_func(i, j):\\n    i = i + 1\\n    j = j - 1\"\n    assert prntr.doprint(fd) == expected_output_sympy\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n\n    assert p.doprint(S.Exp1) == 'sympy.E'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n\n    # Test for printing Function\n    from sympy import sin\n    assert p.doprint(sin(x)) == 'sympy.sin(x)'\n\n    # Test for printing Pow\n    assert p.doprint(x**2) == 'x**2'\n    assert p.doprint(x**(1/2)) == 'sympy.sqrt(x)'\n\n    # Test for Function with multiple arguments\n    from sympy import log\n    assert p.doprint(log(x, 2)) == 'sympy.log(x, 2)'\n\n    # Test for relational operators\n    from sympy import Eq, Lt, Gt\n    assert p.doprint(Eq(x, y)) == 'sympy.Eq(x, y)'\n    assert p.doprint(Lt(x, y)) == 'sympy.Lt(x, y)'\n    assert p.doprint(Gt(x, y)) == 'sympy.Gt(x, y)'\n\n    # Test for printing constants\n    assert p.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n    assert p.doprint(S.EulerGamma) == 'sympy.EulerGamma'\n    assert p.doprint(S.Catalan) == 'sympy.Catalan'\n\n    # Test for unsupported functions\n    from sympy import fresnelc, fresnels\n    assert p.doprint(fresnelc(x)) == 'sympy.fresnelc(x)'\n    assert p.doprint(fresnels(x)) == 'sympy.fresnels(x)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(x**2) == 'x**2'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(MatrixSymbol(\"A\", 2, 2)**-1) == 'sympy.Inverse(A)'\n    assert p.doprint(MatrixSymbol(\"A\", 2, 2)**2) == 'A**2'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    assert p.doprint(S.Exp1) == 'sympy.exp(1)'\n    assert p.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n    assert p.doprint(S.EulerGamma) == 'sympy.EulerGamma'\n    assert p.doprint(S.NaN) == 'sympy.nan'\n    assert p.doprint(S.Infinity) == 'sympy.oo'\n    assert p.doprint(S.NegativeInfinity) == 'sympy.oo * -1'\n    assert p.doprint(Mod(x, 2)) == 'Mod(x, 2)'\n    assert p.doprint(And(x, y)) == 'And(x, y)'\n    assert p.doprint(Or(x, y)) == 'Or(x, y)'\n    assert p.doprint(Identity(3)) == \"Identity(3)\"\n    assert p.doprint(Piecewise((1, Eq(x, 0)), (2, x>6))) == 'Piecewise((1, Eq(x, 0)), (2, x > 6))'\n    assert p.doprint(Piecewise((2, Le(x, 0)), (3, Gt(x, 0)), evaluate=False)) == 'Piecewise((2, Le(x, 0)), (3, Gt(x, 0)))'\n    assert p.doprint(p[0, 1]) == 'p[0, 1]'\n", "def test_print_ComplexInfinity():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"float('nan')\"\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"float('nan')\"\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"float('nan')\"\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"mpmath.nan\"\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"float('nan')\"\n"], "sample_1094": ["def test_copy():\n    assert b21.copy() == Basic(b2, b1)\n    assert b21.copy() is not b21\n    x, y = symbols('x y')\n    expr = Basic(x, y)\n    assert expr.copy() == Basic(x, y)\n    assert expr.copy() is not expr\n", "def test_fromiter():\n    x, y, z = symbols('x y z')\n    assert Basic.fromiter([x, y, z]).args == (x, y, z)\n    assert Basic.fromiter((x, y, z)).args == (x, y, z)\n    assert Basic.fromiter({x, y, z}).args == tuple({x, y, z})\n    assert Basic.fromiter(range(3)).args == (0, 1, 2)\n    assert Basic.fromiter([]).args == ()\n    assert Basic.fromiter((), commutative=False).args == ()\n    assert Basic.fromiter(set(), commutative=False).args == ()\n", "def test_compare():\n    x, y, z = symbols('x y z')\n    assert Basic(x).compare(Basic(x)) == 0\n    assert Basic(x).compare(Basic(y)) == -1\n    assert Basic(y).compare(Basic(x)) == 1\n    assert Basic(x, y).compare(Basic(x, z)) == -1\n    assert Basic(x, y).compare(Basic(x, y, z)) == -1\n    assert Basic(x, y).compare(Basic(y, x)) == -1\n    assert Basic(y, x).compare(Basic(x, y)) == 1\n    assert Basic(x, y, z).compare(Basic(x, y, y)) == 1\n", "def test_compare():\n    assert Basic().compare(Basic()) == 0\n    assert Basic(1).compare(Basic(1)) == 0\n    assert Basic(1).compare(Basic(2)) == -1\n    assert Basic(2).compare(Basic(1)) == 1\n\n    assert Basic(1, 2).compare(Basic(1, 2)) == 0\n    assert Basic(1, 2).compare(Basic(1, 3)) == -1\n    assert Basic(1, 3).compare(Basic(1, 2)) == 1\n\n    assert Basic(1, Basic(2, 3)).compare(Basic(1, Basic(2, 3))) == 0\n    assert Basic(1, Basic(2, 3)).compare(Basic(1, Basic(2, 4))) == -1\n    assert Basic(1, Basic(2, 4)).compare(Basic(1, Basic(2, 3))) == 1\n\n    class BasicSubclass(Basic):\n        pass\n\n    assert Basic().compare(BasicSubclass()) == -1\n    assert BasicSubclass().compare(Basic()) == 1\n\n    assert Basic(1).compare(BasicSubclass(1)) == -1\n    assert BasicSubclass(1).compare(Basic(1)) == 1\n\n    assert BasicSubclass(1, 2).compare(BasicSubclass(1, 2)) == 0\n    assert BasicSubclass(1, 2).compare(BasicSubclass(1, 3)) == -1\n    assert BasicSubclass(1, 3).compare(BasicSubclass(1, 2)) == 1\n", "def test_compare():\n    x, y, z = symbols('x y z')\n    assert Basic(x).compare(Basic(y)) == -1\n    assert Basic(y).compare(Basic(x)) == 1\n    assert Basic(x).compare(Basic(x)) == 0\n    assert Basic(x, y).compare(Basic(y, x)) == -1\n    assert Basic(x, y).compare(Basic(x, y, z)) == -1\n    assert Basic(x, y, z).compare(Basic(x, y)) == 1\n    assert Basic(x, y).compare(Basic(x, y)) == 0\n"], "sample_1095": ["def test_af_invert():\n    a = [1, 2, 0, 3]\n    inv_a = _af_invert(a)\n    assert inv_a == [2, 0, 1, 3]\n    assert _af_rmul(inv_a, a) == list(range(len(a)))\n    b = [3, 0, 2, 1]\n    inv_b = _af_invert(b)\n    assert inv_b == [1, 3, 2, 0]\n    assert _af_rmul(inv_b, b) == list(range(len(b)))\n    c = []\n    inv_c = _af_invert(c)\n    assert inv_c == []\n", "def test_af_invert():\n    a = [1, 0, 3, 2]\n    inv_a = _af_invert(a)\n    assert inv_a == [1, 0, 3, 2]\n    assert _af_rmul(inv_a, a) == list(range(4))\n    assert _af_rmul(a, inv_a) == list(range(4))\n\n    b = [2, 3, 1, 0]\n    inv_b = _af_invert(b)\n    assert inv_b == [3, 2, 0, 1]\n    assert _af_rmul(inv_b, b) == list(range(4))\n    assert _af_rmul(b, inv_b) == list(range(4))\n", "def test_af_functions():\n    a = [1, 0, 2]\n    b = [0, 2, 1]\n    c = [2, 1, 0]\n\n    # Testing _af_rmul\n    assert _af_rmul(a, b) == [1, 2, 0]\n    assert _af_rmul(b, c) == [1, 2, 0]\n    assert _af_rmul(c, a) == [0, 1, 2]\n\n    # Testing _af_rmuln\n    assert _af_rmuln(a, b, c) == [2, 1, 0]\n    assert _af_rmuln(c, b, a) == [1, 0, 2]\n    assert _af_rmuln(a, c, b) == [0, 2, 1]\n\n    # Testing _af_parity\n    assert _af_parity(a) == 1\n    assert _af_parity(b) == 1\n    assert _af_parity(c) == 1\n    assert _af_parity([0, 1, 2, 3]) == 0\n    assert _af_parity([3, 2, 0, 1]) == 1\n\n    # Testing _af_invert\n    assert _af_invert(a) == [1, 0, 2]\n    assert _af_invert(b) == [0, 2, 1]\n    assert _af_invert(c) == [2, 1, 0]\n\n    # Testing _af_pow\n    assert _af_pow(a, 2) == [0, 1, 2]\n    assert _af_pow(b, 3) == [2, 0, 1]\n    assert _af_pow(c, 4) == [0, 1, 2]\n    assert _af_pow(c, 0) == [0, 1, 2]\n    assert _af_pow(c, -1) == [0, 1, 2]\n\n    # Testing _af_commutes_with\n    assert _af_commutes_with(a, b) is False\n    assert _af_commutes_with(b, c) is False\n    assert _af_commutes_with(a, a) is True\n    assert _af_commutes_with(b, b) is True\n", "def test_af_functions():\n    # Test _af_rmul\n    a, b = [1, 0, 2], [0, 2, 1]\n    assert _af_rmul(a, b) == [1, 2, 0]\n    assert _af_rmul([2, 0, 1], [1, 2, 0]) == [0, 2, 1]\n    assert _af_rmul([0, 1, 2], [2, 1, 0]) == [2, 1, 0]\n\n    # Test _af_rmuln\n    a, b, c = [1, 0, 2], [0, 2, 1], [2, 1, 0]\n    assert _af_rmuln(a, b, c) == [2, 0, 1]\n    assert _af_rmuln([1, 2, 3], [3, 2, 1], [2, 1, 0]) == [3, 2, 1]\n    assert _af_rmuln([0, 1, 2], [2, 0, 1], [1, 2, 0]) == [1, 0, 2]\n\n    # Test _af_parity\n    assert _af_parity([0, 1, 2, 3]) == 0\n    assert _af_parity([3, 2, 0, 1]) == 1\n    assert _af_parity([1, 0, 3, 2]) == 1\n    assert _af_parity([2, 3, 1, 0]) == 0\n\n    # Test _af_invert\n    assert _af_invert([1, 2, 0, 3]) == [2, 0, 1, 3]\n    assert _af_invert([3, 2, 1, 0]) == [3, 2, 1, 0]\n    assert _af_invert([0, 1, 2, 3]) == [0, 1, 2, 3]\n\n    # Test _af_pow\n    assert _af_pow([2, 0, 3, 1], 4) == [0, 1,", "def test_af_commutes_with():\n    a = [1, 0, 2, 3]\n    b = [0, 1, 3, 2]\n    assert _af_commutes_with(a, b) == False\n    c = [1, 0, 2, 3]\n    assert _af_commutes_with(a, c) == True\n    d = [0, 1, 2, 3]\n    assert _af_commutes_with(d, d) == True\n"], "sample_1096": ["def test_IndexedBase_with_strides():\n    i, j = symbols('i j', integer=True)\n    m, n = symbols('m n', integer=True)\n    a = IndexedBase('a', shape=(m, n), strides=(2, 3))\n    assert a.shape == Tuple(m, n)\n    assert a.strides == (2, 3)\n    assert Indexed(a, Idx(i, m), Idx(j, n)).ranges == [Tuple(0, m - 1), Tuple(0, n - 1)]\n    assert Indexed(a, Idx(i, m), Idx(j, n)).shape == Tuple(m, n)\n    assert Indexed(a, Idx(i, m), Idx(j, n)).base.strides == (2, 3)\n", "def test_Idx_invalid():\n    i = symbols('i', integer=True)\n    raises(TypeError, lambda: Idx(i, (1, 2, 3)))\n    raises(TypeError, lambda: Idx(i, (S('a'), 5)))\n    raises(TypeError, lambda: Idx(i, (1, S('b'))))\n    raises(TypeError, lambda: Idx(i, (S('a'), S('b'))))\n    raises(TypeError, lambda: Idx(i, (1.5, 5)))\n    raises(TypeError, lambda: Idx(i, (1, 5.5)))\n    raises(TypeError, lambda: Idx(i, (1.5, 5.5)))\n    raises(TypeError, lambda: Idx(i, ('a', 'b')))\n", "def test_IndexedBase_with_shape_and_strides():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A', shape=(2, 3), strides=(1, 2))\n    assert A.shape == Tuple(2, 3)\n    assert A.strides == Tuple(1, 2)\n    assert A.offset == S.Zero\n    assert A[i, j] == Indexed(A, i, j)\n    raises(IndexException, lambda: A[i])\n    raises(IndexException, lambda: A[i, j, i])\n", "def test_IndexedBase_offset_and_strides():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a', shape=(4, 4), offset=2, strides=(1, 4))\n\n    assert a.offset == 2\n    assert a.strides == (1, 4)\n    assert a.shape == Tuple(4, 4)\n\n    idx_expr = a[i, j]\n    assert idx_expr.base == a\n    assert idx_expr.indices == (i, j)\n    assert idx_expr.shape == Tuple(4, 4)\n\n    # Test for unrolling into 1D form with given offset and strides\n    assert a[i, j]._sympystr(lambda x: x) == \"a[1*i + 4*j + 2]\"\n\n    # Test substitution\n    assert a[i, j].subs(i, 2) == a[2, j]\n    assert a[i, j].subs(j, 3) == a[i, 3]\n    assert a[i, j].subs(i, 2).subs(j, 3) == a[2, 3]\n\n    # Ensure that the offset and strides are correctly propagated through substitution\n    b = IndexedBase('b', shape=(4, 4), offset=1, strides=(2, 8))\n    assert b[i, j].subs(i, 2)._sympystr(lambda x: x) == \"b[2*2 + 8*j + 1]\"\n    assert b[i, j].subs(j, 3)._sympystr(lambda x: x) == \"b[2*i + 8*3 + 1]\"\n    assert b[i, j].subs(i, 2).subs(j, 3)._sympystr(lambda x: x) == \"b[2*2 + 8*3 + 1]\"\n", "def test_IndexedBase_offset_strides():\n    i, j = symbols('i j', integer=True)\n    m, n = symbols('m n', integer=True)\n    o, p = symbols('o p', integer=True)\n    a = IndexedBase('a', shape=(m, n), offset=o, strides=(p, 2*p))\n    assert a.shape == Tuple(m, n)\n    assert a.offset == o\n    assert a.strides == Tuple(p, 2*p)\n    assert a[i, j]._eval_derivative(a[i, j]) == 1\n    raises(IndexException, lambda: IndexedBase('a', strides=(p, 2*p, 3*p)))\n    raises(IndexException, lambda: IndexedBase('a', shape=(m, n), strides=(p, 2*p, 3*p)))\n"], "sample_1097": ["def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    X = BlockMatrix([[A, B], [C, D]])\n    real_X, imag_X = X.as_real_imag()\n\n    assert real_X == BlockMatrix([[re(A), re(B)], [re(C), re(D)]])\n    assert imag_X == BlockMatrix([[im(A), im(B)], [im(C), im(D)]])\n", "def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    X = BlockMatrix([[A, B], [C, D]])\n    real, imag = X.as_real_imag()\n    \n    assert real == BlockMatrix([[re(A), re(B)], [re(C), re(D)]])\n    assert imag == BlockMatrix([[im(A), im(B)], [im(C), im(D)]])\n", "def test_BlockMatrix_as_real_imag():\n    from sympy import I\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    real_part = Matrix([[1, 2], [3, 4]])\n    imag_part = Matrix([[5, 6], [7, 8]])\n    complex_matrix = BlockMatrix([[real_part + I*imag_part, Z],\n                                  [X, real_part - I*imag_part]])\n    \n    real_result, imag_result = complex_matrix.as_real_imag()\n    \n    expected_real = BlockMatrix([[real_part, Matrix([[0, 0], [0, 0]])],\n                                 [Matrix([[0, 0], [0, 0]]), real_part]])\n    \n    expected_imag = BlockMatrix([[imag_part, Matrix([[0, 0], [0, 0]])],\n                                 [Matrix([[0, 0], [0, 0]]), -imag_part]])\n    \n    assert real_result == expected_real\n    assert imag_result == expected_imag\n", "def test_BlockMatrix_as_real_imag():\n    from sympy import I\n    A = Matrix([[1 + 2*I, 3 - I], [4 + 5*I, 6 - 2*I]])\n    B = Matrix([[7 - I, 8 + 2*I], [9 - 3*I, 10 + 4*I]])\n    bm = BlockMatrix([[A, B], [B, A]])\n    real_part, imag_part = bm.as_real_imag()\n    \n    assert real_part == BlockMatrix([[A.as_real_imag()[0], B.as_real_imag()[0]], [B.as_real_imag()[0], A.as_real_imag()[0]]])\n    assert imag_part == BlockMatrix([[A.as_real_imag()[1], B.as_real_imag()[1]], [B.as_real_imag()[1], A.as_real_imag()[1]]])\n", "def test_BlockMatrix_structurally_equal():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    E = MatrixSymbol('E', m, m)\n    F = MatrixSymbol('F', m, m)\n\n    BM1 = BlockMatrix([[A, B], [C, D]])\n    BM2 = BlockMatrix([[A, B], [C, D]])\n    BM3 = BlockMatrix([[A, B], [C, E]])\n\n    assert BM1.structurally_equal(BM2) == True\n    assert BM1.structurally_equal(BM3) == False\n"], "sample_1098": ["def test_TupleArg_limit():\n    from sympy import Symbol\n    x = Symbol('x')\n    t = TupleArg(x**2, 1/x)\n    assert t.limit(x, 0) == TupleArg(0, oo)\n    assert t.limit(x, 1) == TupleArg(1, 1)\n    assert t.limit(x, oo) == TupleArg(oo, 0)\n", "def test_meijerg_get_period():\n    from sympy.abc import a, b\n    assert meijerg([1], [2], [3], [4], z).get_period() == oo\n    assert meijerg([a], [b], [], [], z).get_period() == 2*pi*ilcm(a.q, b.q)\n    assert meijerg([1], [], [], [], z).get_period() == 2*pi\n    assert meijerg([1, 1], [2], [1, S(1)/2, S(1)/3], [1], z).get_period() == 12*pi\n", "def test_hyper_properties():\n    # Test properties of the hyper function\n    h = hyper((1, 2), (3, 4), z)\n    assert h.argument == z\n    assert h.ap == Tuple(1, 2)\n    assert h.bq == Tuple(3, 4)\n    assert h.eta == -4\n    assert h.radius_of_convergence == 1\n    \n    # Test the convergence statement property\n    assert hyper((1, 1), (2,), z).convergence_statement == Or(And(re(-2) < 0, abs(z) <= 1), And(0 <= re(-2), re(-2) < 1, abs(z) <= 1, Ne(z, 1)), And(re(-2) >= 1, abs(z) < 1))\n    assert hyper((1, 1), (3,), z).convergence_statement == True\n    \n    # Test the _eval_expand_func method\n    assert hyper((1, 1), (2,), 1)._eval_expand_func() == gamma(2)*gamma(-1)/gamma(1)/gamma(1)\n    assert hyper((1, 1), (1,), 1)._eval_expand_func() == hyper((1, 1), (1,), 1)\n", "def test_prep_tuple():\n    from sympy import polar_lift, I, pi, exp_polar, unpolarify\n    assert _prep_tuple([1, 2, 3]) == Tuple(1, 2, 3)\n    assert _prep_tuple((4, 5)) == Tuple(4, 5)\n    assert _prep_tuple((7, 8, 9)) == Tuple(7, 8, 9)\n    assert _prep_tuple([polar_lift(1), polar_lift(2), polar_lift(3)]) == Tuple(1, 2, 3)\n    assert _prep_tuple((exp_polar(2*pi*I), exp_polar(4*pi*I))) == Tuple(1, 1)\n", "def test_tuplearg_limit():\n    from sympy import limit\n    f1 = hyper((1, 2), (3, 4, 5), z)\n    f2 = meijerg((1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), z)\n    ta = TupleArg(f1, f2)\n    assert ta.limit(z, 0) == TupleArg(f1.limit(z, 0), f2.limit(z, 0))\n    assert ta.limit(z, oo) == TupleArg(f1.limit(z, oo), f2.limit(z, oo))\n"], "sample_1099": ["def test_partial_derivative_doit_method():\n    # Test the doit method to see if it performs the partial derivatives correctly\n    tau, alpha = symbols(\"tau alpha\")\n\n    expr1 = PartialDerivative(tau**alpha, tau)\n    assert expr1.doit() == alpha * tau**(alpha - 1)\n\n    expr2 = PartialDerivative(2*tau + 3*tau**4, tau)\n    assert expr2.doit() == 2 + 12 * tau**3\n\n    expr3 = PartialDerivative(A(i), A(j))\n    assert expr3.doit() == expr3._perform_derivative()\n\n    expr4 = PartialDerivative(A(i) * H(-i, j), A(j))\n    assert expr4.doit().get_indices() == [i, -i]\n\n    expr5 = PartialDerivative((A(i) + B(i)), tau)\n    assert expr5.doit() == PartialDerivative(A(i), tau) + PartialDerivative(B(i), tau)\n", "def test_replace_indices_partial_derivative():\n    x, y = symbols(\"x y\")\n    \n    expr = PartialDerivative(A(i), A(j))\n    repl = {i: x, j: y}\n    new_expr = expr._replace_indices(repl)\n    assert new_expr.expr == A(x)\n    assert new_expr.variables == (A(y),)\n    assert new_expr.get_indices() == [x, -y]\n    \n    expr = PartialDerivative(A(-i), A(j))\n    repl = {-i: x, j: y}\n    new_expr = expr._replace_indices(repl)\n    assert new_expr.expr == A(x)\n    assert new_expr.variables == (A(y),)\n    assert new_expr.get_indices() == [x, -y]\n    \n    expr = PartialDerivative(A(i)*B(j), A(k))\n    repl = {i: x, j: y, k: x}\n    new_expr = expr._replace_indices(repl)\n    assert new_expr.expr == A(x) * B(y)\n    assert new_expr.variables == (A(x),)\n    assert new_expr.get_indices() == [x, y, -x]\n    \n    expr = PartialDerivative(H(i, -j), A(k))\n    repl = {i: x, -j: y, k: x}\n    new_expr = expr._replace_indices(repl)\n    assert new_expr.expr == H(x, y)\n    assert new_expr.variables == (A(x),)\n    assert new_expr.get_indices() == [x, y, -x]\n", "def test_invalid_partial_derivative_symbols():\n    x, y = symbols(\"x y\")\n\n    raises(ValueError, lambda: PartialDerivative(C(j), x))\n    raises(ValueError, lambda: PartialDerivative(x, D(j)))\n    raises(ValueError, lambda: PartialDerivative(x, y))\n", "def test_partial_derivative_nested_expr():\n    expr1 = PartialDerivative(A(i)*B(j), A(k), B(m))\n    assert expr1._expand_partial_derivative() == \\\n        PartialDerivative(A(i), A(k))*PartialDerivative(B(j), B(m)) + \\\n        PartialDerivative(A(i), A(k))*B(j)*PartialDerivative(B(j), B(m)) + \\\n        A(i)*PartialDerivative(B(j), A(k))*PartialDerivative(B(j), B(m))\n\n    expr2 = PartialDerivative(A(i)*B(j) + C(k)*D(m), A(k), B(m))\n    assert expr2._expand_partial_derivative() == \\\n        PartialDerivative(A(i), A(k))*PartialDerivative(B(j), B(m)) + \\\n        PartialDerivative(A(i), A(k))*B(j)*PartialDerivative(B(j), B(m)) + \\\n        A(i)*PartialDerivative(B(j), A(k))*PartialDerivative(B(j), B(m)) + \\\n        PartialDerivative(C(k), A(k))*PartialDerivative(D(m), B(m)) + \\\n        PartialDerivative(C(k), A(k))*D(m)*PartialDerivative(D(m), B(m)) + \\\n        C(k)*PartialDerivative(D(m), A(k))*PartialDerivative(D(m), B(m))\n", "def test_partial_derivative_doit():\n    expr1 = PartialDerivative(A(i), A(j))\n    assert expr1.doit() == PartialDerivative(A(i), A(j))\n\n    expr2 = PartialDerivative(A(i) + B(i), A(j))\n    assert expr2.doit() == PartialDerivative(A(i), A(j)) + PartialDerivative(B(i), A(j))\n\n    expr3 = PartialDerivative(A(i)*B(j), A(k))\n    assert expr3.doit() == PartialDerivative(A(i), A(k))*B(j) + A(i)*PartialDerivative(B(j), A(k))\n\n    expr4 = PartialDerivative(A(i) + B(i) + C(i), A(j))\n    assert expr4.doit() == PartialDerivative(A(i), A(j)) + PartialDerivative(B(i), A(j)) + PartialDerivative(C(i), A(j))\n\n    expr5 = PartialDerivative(2*A(i) + 3*B(i), A(j))\n    assert expr5.doit() == 2*PartialDerivative(A(i), A(j)) + 3*PartialDerivative(B(i), A(j))\n\n    expr6 = PartialDerivative(H(i, j), H(k, m))\n    assert expr6.doit() == PartialDerivative(H(i, j), H(k, m))\n\n    expr7 = PartialDerivative(A(i) * H(j, k), A(m))\n    assert expr7.doit() == PartialDerivative(A(i), A(m)) * H(j, k) + A(i) * PartialDerivative(H(j, k), A(m))\n"], "sample_1100": ["def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(9) == 3\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(24) == 4\n    assert isqrt(25) == 5\n    raises(ValueError, lambda: isqrt(-1))\n    raises(ValueError, lambda: isqrt(-100))\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(9) == 3\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(17) == 4\n    assert isqrt(24) == 4\n    assert isqrt(25) == 5\n    assert isqrt(26) == 5\n    raises(ValueError, lambda: isqrt(-1))\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(9) == 3\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(24) == 4\n    assert isqrt(25) == 5\n    assert isqrt(4503599761588223) == int(4503599761588223 ** 0.5)\n    raises(ValueError, lambda: isqrt(-1))\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(9) == 3\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(4503599761588223) == 67108863  # n < 4503599761588224\n    assert isqrt(4503599761588225) == 67108864  # n >= 4503599761588224\n    raises(ValueError, lambda: isqrt(-1))\n    raises(ValueError, lambda: isqrt(-100))\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(9) == 3\n    assert isqrt(15) == 3  # sqrt(15) is about 3.872, so the largest integer less than or equal to sqrt(15) is 3\n    assert isqrt(16) == 4\n    assert isqrt(4503599761588223) == 67108863  # sqrt(4503599761588223) is about 67108863.99999999\n    raises(ValueError, lambda: isqrt(-1))\n    raises(TypeError, lambda: isqrt(\"string\"))\n"], "sample_1101": ["def test_schur_partition_known_values():\n    # Adding tests for known values to improve coverage and check correctness\n    assert schur_partition(1) == [[1]]\n    assert schur_partition(3) == [[1, 2, 3]]\n    assert schur_partition(5) == [[1, 4], [2, 3]]\n    assert schur_partition(7) == [[1, 4, 7], [2, 3, 6]]\n    assert schur_partition(8) == [[1, 4, 7], [2, 3, 6], [5, 8]]\n", "def test_schur_partition_edge_cases():\n    # Test for edge case where n = 3\n    assert schur_partition(3) == [[1, 2, 3]]\n\n    # Test for higher known Schur number edge case where n = 44\n    partition_44 = schur_partition(44)\n    assert len(partition_44) == 4\n    for subset in partition_44:\n        _sum_free_test(subset)\n\n    # Test for large number beyond known Schur numbers\n    partition_50 = schur_partition(50)\n    for subset in partition_50:\n        _sum_free_test(subset)\n        assert all(1 <= elem <= 50 for elem in subset)\n", "def test_schur_number_exceptions():\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(3.5))\n    raises(ValueError, lambda: SchurNumber(\"string\"))\n", "def test_lower_bound():\n    # Test lower_bound method for known Schur numbers\n    assert SchurNumber(1).lower_bound() == 1\n    assert SchurNumber(2).lower_bound() == 4\n    assert SchurNumber(3).lower_bound() == 13\n    assert SchurNumber(4).lower_bound() == 44\n\n    # Test lower_bound method for larger k values\n    assert SchurNumber(5).lower_bound() == 121\n    assert SchurNumber(10).lower_bound() == 88573\n\n    # Ensure that lower_bound does not raise errors for symbolic inputs\n    n = symbols(\"n\")\n    assert SchurNumber(n).lower_bound() == (3**n - 1) / 2\n", "def test_schur_number_edge_cases():\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(-100))\n    raises(ValueError, lambda: SchurNumber(S(-1)))\n    raises(ValueError, lambda: SchurNumber('string'))\n    raises(ValueError, lambda: SchurNumber(None))\n    assert SchurNumber(5).lower_bound() == 121\n    assert SchurNumber(10).lower_bound() == 88573\n"], "sample_1102": ["def test_Poly_exclude():\n    f = Poly(x*y*z, x, y, z)\n    g = f.exclude()\n    assert g == Poly(x*y*z, x, y, z)  # no unnecessary generators removed\n    assert g == Poly(x*y*z, x, y, z)\n    \n    f = Poly(x*y, x, y, z)\n    g = f.exclude()\n    assert g == Poly(x*y, x, y)  # z removed as unnecessary generator\n    assert g == Poly(x*y, x, y)\n    \n    f = Poly(1, x, y, z)\n    g = f.exclude()\n    assert g == Poly(1, x, y, z)  # no unnecessary generators removed\n    assert g == Poly(1, x, y, z)\n    \n    # Testing with more than one unnecessary generator\n    f = Poly(x, x, y, z, t)\n    g = f.exclude()\n    assert g == Poly(x, x)  # y, z, t removed as unnecessary generators\n    assert g == Poly(x, x)\n", "def test_Poly_operations_with_constants():\n    p = Poly(2*x**2 + 3*x + 5)\n    q = Poly(2*x**2 + 3*x + 5, x)\n\n    # Test addition with a constant\n    assert p + 5 == Poly(2*x**2 + 3*x + 10, x)\n    assert 5 + p == Poly(2*x**2 + 3*x + 10, x)\n\n    # Test subtraction with a constant\n    assert p - 5 == Poly(2*x**2 + 3*x, x)\n    assert 5 - p == Poly(-2*x**2 - 3*x, x)\n\n    # Test multiplication with a constant\n    assert p * 5 == Poly(10*x**2 + 15*x + 25, x)\n    assert 5 * p == Poly(10*x**2 + 15*x + 25, x)\n\n    # Test division with a constant\n    assert p / 5 == Poly(Rational(2, 5)*x**2 + Rational(3, 5)*x + 1, x)\n\n    # Test power with an integer\n    assert p**2 == Poly(4*x**4 + 12*x**3 + 31*x**2 + 30*x + 25, x)\n\n    # Test negation\n    assert -p == Poly(-2*x**2 - 3*x - 5, x)\n\n    # Test equality with a constant\n    assert (p == 5) is False\n    assert (p == 2*x**2 + 3*x + 5) is True\n\n    # Test non-equality with a constant\n    assert (p != 5) is True\n    assert (p != 2*x**2 + 3*x + 5) is False\n\n    # Test addition with a different Poly instance\n    assert p + q == Poly(4*x**2 + 6*x + 10, x)\n\n    # Test subtraction with a different Poly instance\n    assert p - q == Poly(0, x)\n\n    # Test multiplication with a different Poly instance\n    assert p * q == Poly(4*x**4 + 12*x**3 + 31*x**2 + 30*x + 25, x)\n", "def test_Poly_high_degree():\n    # Test for high degree polynomial handling\n    f = x**100 - 1\n    assert Poly(f).degree() == 100\n    assert Poly(f).all_coeffs() == [1] + [0]*99 + [-1]\n    \n    g = (x - 1)**100\n    assert Poly(g).degree() == 100\n    assert Poly(g).factor_list() == (1, [(x - 1, 100)])\n\n    h = x**100 + x**99 + x**98 + ... + x + 1\n    assert Poly(h).degree() == 100\n    assert Poly(h).is_irreducible is True\n", "def test_Poly_unify_with_different_domains():\n    F = Poly(x**2 + 2*x + 1, x, domain='QQ')\n    G = Poly(x**2 + 2*x + 1, x, domain='ZZ')\n    H = Poly(x**2 + 2*x + 1, x, domain='QQ')\n\n    assert F._unify(G)[2:] == (F.rep, H.rep)\n    assert G._unify(F)[2:] == (H.rep, F.rep)\n\n    F = Poly(x**2 + sqrt(2)*x + 1, x, domain='EX')\n    G = Poly(x**2 + sqrt(2)*x + 1, x, domain='QQ<sqrt(2)>')\n    H = Poly(x**2 + sqrt(2)*x + 1, x, domain='EX')\n\n    assert F._unify(G)[2:] == (F.rep, H.rep)\n    assert G._unify(F)[2:] == (H.rep, F.rep)\n\n    raises(UnificationFailed, lambda: Poly(x**2 + 2, x, domain='ZZ')._unify(Poly(x**2 + sqrt(2), x, domain='EX')))\n    raises(UnificationFailed, lambda: Poly(x**2 + sqrt(2), x, domain='EX')._unify(Poly(x**2 + 2, x, domain='ZZ')))\n\n    assert Poly(x, x, domain='ZZ')._unify(Poly(2*x, x, domain='ZZ'))[2:] == (Poly(x, x, domain='ZZ').rep, Poly(2*x, x, domain='ZZ').rep)\n    assert Poly(x, x, domain='QQ')._unify(Poly(2*x, x, domain='QQ'))[2:] == (Poly(x, x, domain='QQ').rep, Poly(2*x, x, domain='QQ').rep)\n\n    F = Poly(x**2 + y, x, y, domain='ZZ')\n    G = Poly(x**2 + y, x, y, domain='QQ')\n    H = Poly(x**2 + y, x, y, domain='QQ')\n\n    assert F._unify(G)[2:] == (H.rep, H.rep)\n    assert G._unify(F)[2:] == (H.rep, H.rep)\n\n    raises(UnificationFailed,", "def test_Poly_unify_with_symbols():\n    F, A, B = field(\"a,b\", ZZ)\n\n    # Test for correct domain unification with symbols\n    assert Poly(a*x, x, domain='ZZ[a]').unify(Poly(a*b*x, x, domain='ZZ(a,b)'))[1:] == \\\n        (Poly(a*x, x, domain='ZZ(a,b)'), Poly(a*b*x, x, domain='ZZ(a,b)'))\n\n    # Test for unification when the domain already includes the symbols\n    assert Poly(a*x, x, domain='ZZ(a)').unify(Poly(a*b*x, x, domain='ZZ(a,b)'))[1:] == \\\n        (Poly(a*x, x, domain='ZZ(a,b)'), Poly(a*b*x, x, domain='ZZ(a,b)'))\n\n    # Test coercion failure in unification\n    raises(CoercionFailed, lambda: Poly(Poly(x**2 + x**2*z, y, field=True), domain='ZZ(x)'))\n"], "sample_1103": ["def test_integer_nthroot():\n    # Test case for integer_nthroot\n    assert integer_nthroot(16, 2) == (4, True)  # perfect square\n    assert integer_nthroot(26, 2) == (5, False)  # not a perfect square\n    assert integer_nthroot(27, 3) == (3, True)  # perfect cube\n    assert integer_nthroot(28, 3) == (3, False)  # not a perfect cube\n    assert integer_nthroot(1, 1) == (1, True)  # nth root of 1\n    assert integer_nthroot(0, 1) == (0, True)  # nth root of 0\n    assert integer_nthroot(1, 0) == (1, True)  # anything raised to the power of 0 is 1\n    assert integer_nthroot(0, 0) == (0, True)  # 0 raised to the power of 0 is typically treated as 1 but in this case 0\n    assert integer_nthroot(2**63, 2) == (2**31, True)  # large perfect square\n    assert integer_nthroot(2**63 - 1, 2) == (2**31 - 1, False)  # large non-perfect square\n\n    # Testing cases where y < 0 or n < 1 to trigger ValueError\n    with raises(ValueError):\n        integer_nthroot(-1, 2)\n    with raises(ValueError):\n        integer_nthroot(1, -2)\n    with raises(ValueError):\n        integer_nthroot(-1, -2)\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(9) == 3\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(24) == 4\n    assert isqrt(25) == 5\n    assert isqrt(4503599761588223) == 67108863\n    assert isqrt(4503599761588224) == 67108864\n    assert isqrt(4503599761588225) == 67108864\n    raises(ValueError, lambda: isqrt(-1))\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(10) == 3\n    assert isqrt(4503599761588223) == 67108863\n    assert isqrt(4503599761588224) == 67108864\n    assert isqrt(2**100) == 1267650600228229401496703205376\n    raises(ValueError, lambda: isqrt(-1))\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(17) == 4\n    assert isqrt(24) == 4\n    assert isqrt(25) == 5\n    raises(ValueError, lambda: isqrt(-1))\n    assert isqrt(4503599761588223) == int(_sqrt(4503599761588223))\n    assert isqrt(4503599761588224) == int(_sqrt(4503599761588224))\n    assert isqrt(4503599761588225) == integer_nthroot(4503599761588225, 2)[0]\n    assert isqrt(9999999999999999) == int(_sqrt(9999999999999999))\n    assert isqrt(10000000000000000) == integer_nthroot(10000000000000000, 2)[0]\n", "def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(4) == 2\n    assert isqrt(16) == 4\n    assert isqrt(25) == 5\n    assert isqrt(26) == 5\n    assert isqrt(4503599761588223) == int(_sqrt(4503599761588223))\n    raises(ValueError, lambda: isqrt(-1))\n"], "sample_1104": ["def test_Exp1():\n    assert str(exp(1)) == \"E\"\n    assert str(exp(0)) == \"1\"\n    assert str(exp(x)) == \"exp(x)\"\n    assert str(exp(x + y)) == \"exp(x + y)\"\n    assert str(exp(x**2 + 2*x*y + y**2)) == \"exp(x**2 + 2*x*y + y**2)\"\n    assert str(exp(-x)) == \"exp(-x)\"\n    assert str(exp(-x*y)) == \"exp(-x*y)\"\n", "def test_StrPrinter_custom_class():\n    class CustomClass:\n            self.name = name\n        \n            return f\"CustomClass({self.name})\"\n    \n    obj = CustomClass('test')\n    printer = StrPrinter()\n    assert printer.emptyPrinter(obj) == \"CustomClass(test)\"\n", "def test_Mul_with_negative_and_fractional():\n    assert str(Mul(-2, Rational(1, 3), evaluate=False)) == '-2/3'\n    assert str(Mul(-2, Rational(2, 3), evaluate=False)) == '-4/3'\n    assert str(Mul(Rational(2, 5), Rational(3, 7), evaluate=False)) == '6/35'\n    assert str(Mul(Rational(2, 5), -Rational(3, 7), evaluate=False)) == '-6/35'\n    assert str(Mul(-Rational(2, 5), -Rational(3, 7), evaluate=False)) == '6/35'\n", "def test_EmptySet():\n    assert str(S.EmptySet) == \"EmptySet\"\n", "def test_Float_boundary():\n    assert str(Float(0.1, 1)) == '0.1'\n    assert str(Float(0.1, 2)) == '0.1'\n    assert str(Float(0.1, 3)) == '0.1'\n    assert str(Float(0.123456789, 3)) == '0.123'\n    assert str(Float(0.123456789, 4)) == '0.1235'\n    assert str(Float(0.123456789, 5)) == '0.12346'\n"], "sample_1105": ["def test_validate():\n    F = MatrixSymbol('F', 2, 3)\n    G = MatrixSymbol('G', 3, 4)\n    H = MatrixSymbol('H', 4, 5)\n\n    # These should pass without raising an error\n    try:\n        validate(F, G)\n        validate(G, H)\n        validate(F, G, H)\n    except ShapeError:\n        assert False, \"Unexpected ShapeError\"\n\n    # This should raise a ShapeError\n    I = MatrixSymbol('I', 2, 2)\n    J = MatrixSymbol('J', 3, 3)\n    try:\n        validate(I, J)\n        assert False, \"Expected ShapeError\"\n    except ShapeError:\n        pass\n", "def test_validate():\n    M = MatrixSymbol('M', 2, 2)\n    N = MatrixSymbol('N', 2, 3)\n    P = MatrixSymbol('P', 3, 2)\n    \n    # Valid case\n    try:\n        validate(M, P)\n    except ShapeError:\n        assert False, \"validate raised ShapeError unexpectedly!\"\n    \n    # Invalid case\n    try:\n        validate(M, N)\n        assert False, \"validate did not raise ShapeError as expected!\"\n    except ShapeError:\n        pass\n", "def test_eval_inverse():\n    assert MatMul(A, B)._eval_inverse() == Inverse(B) * Inverse(A)\n    assert MatMul(C, D)._eval_inverse() == Inverse(D) * Inverse(C)\n    assert MatMul(A, B, C)._eval_inverse() == Inverse(C) * Inverse(B) * Inverse(A)\n\n    # Test with scalar multiplication\n    assert MatMul(2, C)._eval_inverse() == Inverse(C) / 2\n    assert MatMul(C, 2)._eval_inverse() == Inverse(C) / 2\n\n    # Test with invalid shapes\n    E = MatrixSymbol('E', 3, 2)\n    F = MatrixSymbol('F', 2, 3)\n    try:\n        MatMul(E, F)._eval_inverse()\n    except ShapeError:\n        pass  # expected\n    else:\n        assert False, \"Expected a ShapeError\"\n", "def test_as_coeff_matrices():\n    factor, matrices = MatMul(2, C, D, evaluate=False).as_coeff_matrices()\n    assert factor == 2\n    assert matrices == [C, D]\n\n    factor, matrices = MatMul(C, 2, D, 3, evaluate=False).as_coeff_matrices()\n    assert factor == 6\n    assert matrices == [C, D]\n\n    with pytest.raises(NotImplementedError):\n        MatMul(2, C, D, 2*I, evaluate=False).as_coeff_matrices()\n", "def test_eval_inverse():\n    inv_expr = MatMul(A, B)._eval_inverse()\n    assert inv_expr == Inverse(B) * Inverse(A)\n    inv_expr = MatMul(2, A, B)._eval_inverse()\n    assert inv_expr == (1/2) * Inverse(B) * Inverse(A)\n    with pytest.raises(ShapeError):\n        MatMul(A, C)._eval_inverse()\n"], "sample_1106": ["def test_matadd_shape():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n    D = MatrixSymbol('D', 2, 2)\n    \n    assert MatAdd(A, B, C).shape == (3, 3)\n\n    try:\n        MatAdd(A, B, D)\n    except ShapeError as e:\n        assert str(e) == \"Matrices A and D are not aligned\"\n", "def test_matadd_basic_operations():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    \n    # Test MatAdd creation and basic properties\n    expr = MatAdd(A, B, C)\n    assert expr.shape == (2, 2)\n    assert expr.args == (A, B, C)\n\n    # Test identity element\n    assert MatAdd() == GenericZeroMatrix()\n\n    # Test addition with zero matrix\n    Z = ZeroMatrix(2, 2)\n    assert MatAdd(A, Z) == A\n\n    # Test entry evaluation\n    M1 = Matrix([[1, 2], [3, 4]])\n    M2 = Matrix([[5, 6], [7, 8]])\n    M3 = Matrix([[9, 10], [11, 12]])\n    expr = MatAdd(M1, M2, M3)\n    assert expr._entry(0, 0) == 15\n    assert expr._entry(0, 1) == 18\n    assert expr._entry(1, 0) == 21\n    assert expr._entry(1, 1) == 24\n\n    # Test transpose\n    assert expr._eval_transpose() == MatAdd(transpose(M1), transpose(M2), transpose(M3)).doit()\n\n    # Test adjoint\n    assert expr._eval_adjoint() == MatAdd(adjoint(M1), adjoint(M2), adjoint(M3)).doit()\n\n    # Test trace\n    from sympy.matrices.expressions.trace import trace\n    assert expr._eval_trace() == Add(trace(M1), trace(M2), trace(M3)).doit()\n", "def test_matadd_basic_operations():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    assert MatAdd(X, Y) == X + Y\n    assert MatAdd(X, Y, Z).shape == (2, 2)\n    assert MatAdd(X, ZeroMatrix(2, 2)) == X\n    assert MatAdd(X, -X) == ZeroMatrix(2, 2)\n    assert MatAdd(X, Z, -Z).doit() == X\n", "def test_matadd_basic_operations():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    \n    # Basic addition\n    assert MatAdd(X, Y, Z) == X + Y + Z\n    \n    # Check identity property\n    assert MatAdd() == ZeroMatrix(0, 0)\n    assert MatAdd(X, ZeroMatrix(2, 2)) == X\n    \n    # Evaluate property\n    expr = MatAdd(X, Y, evaluate=True)\n    assert expr == X + Y\n    assert expr.doit() == X + Y\n\n    # Ensure MatAdd returns Add when all arguments are not matrices\n    assert isinstance(MatAdd(1, 2, 3), Add)\n", "def test_matadd_basic_operations():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    \n    # Test basic addition\n    assert MatAdd(A, B, C) == MatAdd(A, B, C)\n    assert MatAdd(A, B, ZeroMatrix(2, 2)) == MatAdd(A, B)\n    \n    # Test identity matrix\n    assert MatAdd() == GenericZeroMatrix()\n    \n    # Test shape\n    assert MatAdd(A, B).shape == (2, 2)\n    \n    # Test transpose\n    assert MatAdd(A, B).transpose() == MatAdd(A.T, B.T).doit()\n    \n    # Test adjoint\n    assert MatAdd(A, B).adjoint() == MatAdd(A.adjoint(), B.adjoint()).doit()\n    \n    # Test trace\n    from sympy.matrices.expressions.trace import trace\n    assert MatAdd(A, B)._eval_trace() == (trace(A) + trace(B)).doit()\n    \n    # Test doit\n    assert MatAdd(A, B).doit() == MatAdd(A, B)\n    \n    # Test validation\n    with pytest.raises(ShapeError):\n        MatAdd(A, MatrixSymbol('D', 3, 3), check=True)\n"], "sample_1107": ["def test_signed_permutations():\n    assert list(signed_permutations((0, 1, 2))) == [\n        (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2),\n        (0, 2, 1), (0, -2, 1), (0, 2, -1), (0, -2, -1),\n        (1, 0, 2), (-1, 0, 2), (1, 0, -2), (-1, 0, -2),\n        (1, 2, 0), (-1, 2, 0), (1, -2, 0), (-1, -2, 0),\n        (2, 0, 1), (-2, 0, 1), (2, 0, -1), (-2, 0, -1),\n        (2, 1, 0), (-2, 1, 0), (2, -1, 0), (-2, -1, 0)\n    ]\n    assert list(signed_permutations((1, 2))) == [\n        (1, 2), (-1, 2), (1, -2), (-1, -2),\n        (2, 1), (-2, 1), (2, -1), (-2, -1)\n    ]\n    assert list(signed_permutations((0, 0))) == [(0, 0)]\n", "def test_interactive_traversal():\n    from sympy import Eq, pprint\n    expr = Eq(x, y + z)\n    result = []\n\n        # Automatically select the next subexpression each time\n        options = {\n            \"Your choice [0-1,f,l,r,d,?]: \": '0',\n            \"Your choice [0-1,f,l,r,d,?]: \": '1',\n            \"Your choice [0-1,f,l,r,d,?]: \": 'd'\n        }\n        return options.get(prompt, 'd')\n\n        traversal = interactive_traversal(expr)\n        result.append(traversal)\n\n    import builtins\n    original_input = builtins.input\n    builtins.input = mock_input\n\n    try:\n        output = capture(capture_traversal)\n        assert result[0] == y\n    finally:\n        builtins.input = original_input\n", "def test_postorder_traversal_with_custom_keys():\n    expr = w + x + y + z\n    assert list(postorder_traversal(expr, keys=default_sort_key)) == [w, x, y, z, x + y, x + y + z, w + x + y + z]\n\n    expr = (w + x) * (y + z)\n    assert list(postorder_traversal(expr, keys=default_sort_key)) == [w, x, w + x, y, z, y + z, (w + x) * (y + z)]\n\n    expr = (w * x) + (y * z)\n    assert list(postorder_traversal(expr, keys=default_sort_key)) == [w, x, w * x, y, z, y * z, (w * x) + (y * z)]\n\n    expr = Piecewise((x + y, x < 1), (x * y, True))\n    expected = [\n        x, y, x + y, 1, x, x < 1, ExprCondPair(x + y, x < 1),\n        x, y, x * y, true, ExprCondPair(x * y, True),\n        Piecewise((x + y, x < 1), (x * y, True))\n    ]\n    assert list(postorder_traversal(expr, keys=default_sort_key)) == expected\n", "def test_reshape_exceptions():\n    # Test raising ValueError for invalid reshape inputs\n    raises(ValueError, lambda: reshape([0, 1], [-1]))\n    raises(ValueError, lambda: reshape([0, 1], [3]))\n    raises(ValueError, lambda: reshape([0, 1, 2], [2, [2]]))  # number of elements not divisible\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [\n        (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((3, 0, 1))) == [\n        (3, 0, 1), (-3, 0, 1), (3, 0, -1), (-3, 0, -1)]\n    assert list(permute_signs([0, 0, 1])) == [\n        (0, 0, 1), (0, 0, -1)]\n    assert list(permute_signs([1, -2, 3])) == [\n        (1, -2, 3), (-1, -2, 3), (1, 2, 3), (-1, 2, 3),\n        (1, -2, -3), (-1, -2, -3), (1, 2, -3), (-1, 2, -3)]\n    assert list(permute_signs((0, 0, 0))) == [(0, 0, 0)]\n"], "sample_1108": ["def test_rotate_left():\n    assert rotate_left([1, 2, 3, 4, 5], 2) == [3, 4, 5, 1, 2]\n    assert rotate_left([1, 2, 3, 4, 5], 5) == [1, 2, 3, 4, 5]\n    assert rotate_left([1, 2, 3, 4, 5], 0) == [1, 2, 3, 4, 5]\n    assert rotate_left([], 3) == []\n    assert rotate_left([1], 1) == [1]\n", "def test_iproduct_with_non_integer():\n    assert list(iproduct('ab', ['x', 'y'])) == [\n        ('a', 'x'), ('a', 'y'), ('b', 'x'), ('b', 'y')]\n    assert list(iproduct([1, 2], ['a', 'b'])) == [\n        (1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\n    assert list(iproduct([True, False], [None])) == [\n        (True, None), (False, None)]\n    assert list(iproduct('xy', 'ab', repeat=2)) == [\n        ('x', 'a', 'x', 'a'), ('x', 'a', 'x', 'b'), ('x', 'a', 'y', 'a'), ('x', 'a', 'y', 'b'), \n        ('x', 'b', 'x', 'a'), ('x', 'b', 'x', 'b'), ('x', 'b', 'y', 'a'), ('x', 'b', 'y', 'b'), \n        ('y', 'a', 'x', 'a'), ('y', 'a', 'x', 'b'), ('y', 'a', 'y', 'a'), ('y', 'a', 'y', 'b'), \n        ('y', 'b', 'x', 'a'), ('y', 'b', 'x', 'b'), ('y', 'b', 'y', 'a'), ('y', 'b', 'y', 'b')]\n", "def test_is_palindromic_edge_cases():\n    assert is_palindromic([1, 2, 3, 2, 1])  # Palindromic list\n    assert not is_palindromic([1, 2, 3, 4, 5])  # Non-palindromic list\n    assert is_palindromic('abba')  # Palindromic string\n    assert not is_palindromic('abcd')  # Non-palindromic string\n    assert is_palindromic('a')  # Single character string\n    assert is_palindromic('')  # Empty string\n\n    # Test with Python slicing\n    assert is_palindromic([1, 2, 3, 2, 1], 1, 4)  # Palindromic in the slice\n    assert not is_palindromic([1, 2, 3, 2, 1], 1, 5)  # Non-palindromic in the slice\n    assert is_palindromic('abccba', 1, 5)  # Palindromic in the slice\n    assert not is_palindromic('abcdcb', 1, 5)  # Non-palindromic in the slice\n", "def test_interactive_traversal():\n    from sympy import cos, sin\n    from sympy.abc import a, b, c\n    expr = cos(a + sin(b + c))\n    \n        if \"choice\" in prompt:\n            return 'd'  # done\n        return '0'  # default choice\n\n    import builtins\n    original_input = builtins.input\n    builtins.input = mock_input\n    try:\n        result = interactive_traversal(expr)\n    finally:\n        builtins.input = original_input\n\n    assert result == expr\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((1, -1, 0))) == [(1, -1, 0), (-1, -1, 0), (1, 1, 0), (-1, 1, 0)]\n    assert list(permute_signs((0, 0, 0))) == [(0, 0, 0)]\n    assert list(permute_signs((1, 0, 2))) == [(1, 0, 2), (-1, 0, 2), (1, 0, -2), (-1, 0, -2)]\n"], "sample_1109": ["def test_eval_methods():\n    # Test _eval_is_finite method\n    assert floor(E)._eval_is_finite() == True\n    assert floor(oo)._eval_is_finite() == False\n    assert ceiling(-E)._eval_is_finite() == True\n    assert ceiling(zoo)._eval_is_finite() == False\n\n    # Test _eval_is_real method\n    assert floor(pi)._eval_is_real() == True\n    assert floor(I)._eval_is_real() == False\n    assert ceiling(-pi)._eval_is_real() == True\n    assert ceiling(I)._eval_is_real() == False\n\n    # Test _eval_is_integer method\n    assert floor(2)._eval_is_integer() == True\n    assert floor(2.5)._eval_is_integer() == True\n    assert ceiling(3)._eval_is_integer() == True\n    assert ceiling(3.5)._eval_is_integer() == True\n", "def test_floor_ceiling_frac_combined():\n    assert floor(frac(x)) == 0\n    assert ceiling(frac(x)) == 1\n    assert frac(floor(x)) == 0\n    assert frac(ceiling(x)) == 0\n\n    assert floor(frac(E)) == 0\n    assert ceiling(frac(E)) == 1\n    assert frac(floor(E)) == 0\n    assert frac(ceiling(E)) == 0\n\n    assert floor(frac(pi)) == 0\n    assert ceiling(frac(pi)) == 1\n    assert frac(floor(pi)) == 0\n    assert frac(ceiling(pi)) == 0\n\n    assert floor(frac(I)) == 0\n    assert ceiling(frac(I)) == 1\n    assert frac(floor(I)) == 0\n    assert frac(ceiling(I)) == 0\n\n    assert floor(frac(2*E + 3)) == 0\n    assert ceiling(frac(2*E + 3)) == 1\n    assert frac(floor(2*E + 3)) == 0\n    assert frac(ceiling(2*E + 3)) == 0\n", "def test_eval_methods():\n    assert floor._eval_number(S.Half) == 0\n    assert floor._eval_number(Rational(-1, 2)) == -1\n    assert ceiling._eval_number(S.Half) == 1\n    assert ceiling._eval_number(Rational(-1, 2)) == 0\n\n    assert floor._eval_number(3 + pi*I + y*I) is None\n    assert ceiling._eval_number(3 + pi*I + y*I) is None\n\n    assert frac._eval_rewrite_as_floor(S.Half) == S.Half - floor(S.Half)\n    assert frac._eval_rewrite_as_ceiling(S.Half) == S.Half + ceiling(-S.Half)\n    assert frac._eval_rewrite_as_floor(-Rational(4, 3)) == -Rational(4, 3) - floor(-Rational(4, 3))\n    assert frac._eval_rewrite_as_ceiling(-Rational(4, 3)) == -Rational(4, 3) + ceiling(Rational(4, 3))\n\n    assert floor(floor(x) + y).rewrite(frac) == floor(x) + floor(y)\n    assert ceiling(ceiling(x) + y).rewrite(frac) == ceiling(x) + ceiling(y)\n    assert frac(frac(x) + y).rewrite(frac) == frac(x + y)\n", "def test_issue_20000():\n    assert floor(x + I) == floor(x) + I\n    assert ceiling(x + I) == ceiling(x) + I\n    assert floor(x - I) == floor(x) - I\n    assert ceiling(x - I) == ceiling(x) - I\n\n    complex_expr = E + pi*I\n    assert floor(complex_expr) == floor(E) + floor(pi)*I\n    assert ceiling(complex_expr) == ceiling(E) + ceiling(pi)*I\n    assert frac(complex_expr) == frac(E) + frac(pi)*I\n\n    expr_with_frac = frac(x) + frac(y*I)\n    assert expr_with_frac.rewrite(floor) == x - floor(x) + I*(y - floor(y))\n    assert expr_with_frac.rewrite(ceiling) == x + ceiling(-x) + I*(y + ceiling(-y))\n\n    expr_nested = floor(ceiling(x + y*I) + frac(E))\n    assert expr_nested == floor(ceiling(x) + ceiling(y)*I + frac(E))\n\n    assert floor(exp(I*pi/3)*oo) == exp(I*pi/3)*oo\n    assert ceiling(exp(I*pi/3)*oo) == exp(I*pi/3)*oo\n", "def test_issue_complex_rounding():\n    assert floor(2.5 + 3.5*I) == 2 + 3*I\n    assert ceiling(2.5 + 3.5*I) == 3 + 4*I\n    assert floor(-2.5 - 3.5*I) == -3 - 4*I\n    assert ceiling(-2.5 - 3.5*I) == -2 - 3*I\n    assert frac(2.5 + 3.5*I) == 0.5 + 0.5*I\n    assert frac(-2.5 - 3.5*I) == 0.5 + 0.5*I\n"], "sample_1110": ["def test_SymPyPrinter():\n    prntr = SymPyPrinter()\n    assert prntr.doprint(sign(x)) == 'sympy.sign(x)'\n    assert prntr.doprint(Rational(1, 2)) == '1/2'\n    assert prntr.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert prntr.doprint(sqrt(x)**2) == 'x'\n    assert prntr.doprint(Mod(x, 2)) == 'Mod(x, 2)'\n    assert prntr.doprint(acos(x)) == 'sympy.acos(x)'\n    assert prntr.doprint(Assignment(x, 2)) == 'x = 2'\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)), (2, x>6))) == 'Piecewise((1, Eq(x, 0)), (2, x > 6))'\n    assert prntr.doprint(Piecewise((2, Le(x, 0)), (3, Gt(x, 0)), evaluate=False)) == 'Piecewise((2, Le(x, 0)), (3, Gt(x, 0)))'\n    assert prntr.doprint(p[0, 1]) == 'p[0, 1]'\n    assert prntr.doprint(KroneckerDelta(x, y)) == 'KroneckerDelta(x, y)'\n    assert prntr.doprint(And(x, y)) == 'And(x, y)'\n    assert prntr.doprint(Or(x, y)) == 'Or(x, y)'\n    assert prntr.doprint(Identity(3)) == 'Identity(3)'\n    assert prntr.doprint(zoo) == 'zoo'\n    assert prntr.doprint(-oo) == '-oo'\n    assert prntr.doprint(pi) == 'pi'\n    assert prntr.doprint(S.Exp1) == 'E'\n    assert prntr.doprint(S.Pi) == 'pi'\n    assert prntr.doprint(S.GoldenRatio) == 'GoldenRatio'\n    assert prntr.doprint(S.EulerGamma) == 'EulerGamma'\n    assert prntr.doprint(S.NaN) == 'nan'\n    assert prntr.d", "def test_function_definition():\n    from sympy.codegen.ast import FunctionDefinition, Variable\n    prntr = PythonCodePrinter()\n    body = [Assignment(x, 2), Assignment(y, 3), Assignment(z, x + y)]\n    func_def = FunctionDefinition(name='my_func', parameters=[Variable(x), Variable(y)], body=body)\n    expected_code = (\n        \"def my_func(x, y):\\n\"\n        \"    x = 2\\n\"\n        \"    y = 3\\n\"\n        \"    z = x + y\"\n    )\n    assert prntr.doprint(func_def) == expected_code\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    \n    assert p.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    assert p.doprint(S.Exp1) == 'sympy.exp(1)'\n\n    expr = x**2 + y**2\n    assert p.doprint(expr) == 'x**2 + y**2'\n\n    expr = sqrt(x**2 + y**2)\n    assert p.doprint(expr) == 'sympy.sqrt(x**2 + y**2)'\n\n    expr = acos(x) + acos(y)\n    assert p.doprint(expr) == 'sympy.acos(x) + sympy.acos(y)'\n\n    expr = x + 1\n    assert p.doprint(expr) == 'x + 1'\n", "def test_abstract_printer_methods():\n    prntr = AbstractPythonCodePrinter()\n\n    # Test _declare_number_const\n    assert prntr._declare_number_const('CONST', 42) == 'CONST = 42'\n\n    # Test _module_format\n    assert prntr._module_format('math.sin') == 'math.sin'\n    assert prntr.module_imports == {'math': {'sin'}}\n    assert prntr._module_format('math.sin', register=False) == 'math.sin'\n    assert prntr._module_format('math.sin', register=True) == 'math.sin'\n\n    # Test _expand_fold_binary_op\n    expr = prntr._expand_fold_binary_op('add', [x, y, z])\n    assert expr == 'add(add(x, y), z)'\n\n    # Test _expand_reduce_binary_op\n    expr = prntr._expand_reduce_binary_op('add', [x, y, z, 1])\n    assert expr == 'add(add(x, y), add(z, 1))'\n\n    # Test _get_einsum_string\n    contraction_string, letters_free, letters_dum = prntr._get_einsum_string([2, 2], [(0, 2)])\n    assert contraction_string == 'aa,bb'\n    assert letters_free == ['a', 'b']\n    assert letters_dum == ['a', 'b']\n\n    # Test method return strings\n    assert prntr._print_NaN(None) == \"float('nan')\"\n    assert prntr._print_Infinity(None) == \"float('inf')\"\n    assert prntr._print_NegativeInfinity(None) == \"float('-inf')\"\n    assert prntr._print_ComplexInfinity(None) == \"float('nan')\"\n\n    # Test _print_Mod\n    assert prntr._print_Mod(Mod(x, y)) == 'x % y'\n\n    # Test _print_ImaginaryUnit\n    assert prntr._print_ImaginaryUnit(None) == '1j'\n\n    # Test _print_Stream\n    assert prntr._print_Stream('stdout') == 'sys.stdout'\n    assert prntr._print_Stream('stderr') == 'sys.stderr'\n    assert prntr._print_Stream('other') == 'other'\n\n    # Test _print_NoneToken\n    assert prntr._print_NoneToken(None) == '", "def test_PythonCodePrinter_functions():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(abs(x)) == 'abs(x)'\n    assert prntr.doprint(acos(x)) == 'math.acos(x)'\n    assert prntr.doprint(sign(x)) == '(0.0 if x == 0 else math.copysign(1, x))'\n    assert prntr.doprint(sqrt(x)) == 'math.sqrt(x)'\n    assert prntr.doprint(acos(x) + sign(x)) == 'math.acos(x) + (0.0 if x == 0 else math.copysign(1, x))'\n    assert prntr.module_imports == {'math': {'acos', 'copysign', 'sqrt'}}\n"], "sample_1111": ["def test_constant():\n    x = Symbol('x')\n    lines = [\n        '      1 |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '    0.5 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(S.Half, -1, 1)) == lines\n", "def test_constant_function():\n    x = Symbol('x')\n    lines = [\n        '      2 |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      1 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(1, -1, 1))\n", "def test_constant_function():\n    x = Symbol('x')\n    lines = [\n        '      1 |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '     -1 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(1, -1, 1)) == lines\n", "def test_constant_function():\n    x = Symbol('x')\n    lines = [\n        '      1 |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(0, -1, 1)) == lines\n", "def test_custom_expr():\n    x = Symbol('x')\n    lines = [\n        '      2 |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                     .                                 ',\n        '        |                    . .                                ',\n        '        |                   .   .                               ',\n        '        |                  .     .                              ',\n        '      1 |-------------------------------------------------------',\n        '        |                .        .                            ',\n        '        |               .          .                           ',\n        '        |              .            .                          ',\n        '        |             .              .                         ',\n        '        |            .                .                        ',\n        '        |           .                  .                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |_______________________________________________________',\n        '         -2                        0                           2'\n    ]\n    assert list(textplot_str(x**2, -2, 2)) == lines\n"], "sample_1112": ["def test_edge_cases_and_large_numbers():\n    assert digits(0) == [10, 0]\n    assert digits(0, 2) == [2, 0]\n    assert digits(-1) == [-10, 1]\n    assert digits(-1, 2) == [-2, 1]\n    assert digits(1, 2, 5) == [2, 0, 0, 0, 0, 1]\n    assert digits(-1, 2, 5) == [-2, 0, 0, 0, 0, 1]\n    assert digits(123456789, 16) == [16, 7, 5, 0, 9, 6, 2, 1, 5]\n    assert count_digits(0xDEADBEEF, 16) == {13: 2, 14: 3, 10: 1, 11: 1, 15: 1}\n    assert is_palindromic(0b1010101, 2)\n    assert is_palindromic(0b1001, 2)\n    assert not is_palindromic(0b1010, 2)\n    assert is_palindromic(0xA11A, 16)\n    assert not is_palindromic(0xA11B, 16)\n", "def test_digits_exceptions():\n    raises(ValueError, lambda: digits(123, 1))\n    raises(ValueError, lambda: digits(123, -10))\n    raises(ValueError, lambda: digits(123, 10, 1))\n    raises(ValueError, lambda: digits(123, 10, 2))\n    raises(ValueError, lambda: digits(123, 10, 3))\n    raises(ValueError, lambda: digits(1234567890, 10, 5))\n", "def test_invalid_base():\n    raises(ValueError, lambda: digits(123, 1))\n    raises(ValueError, lambda: digits(123, -5))\n    raises(ValueError, lambda: count_digits(123, 1))\n    raises(ValueError, lambda: count_digits(123, -5))\n    raises(ValueError, lambda: is_palindromic(123, 1))\n    raises(ValueError, lambda: is_palindromic(123, -5))\n", "def test_invalid_bases():\n    raises(ValueError, lambda: digits(10, 1))  # Base less than 2 is invalid\n    raises(ValueError, lambda: digits(10, -10))  # Negative base is invalid\n    raises(ValueError, lambda: digits(10, 0))  # Zero base is invalid\n", "def test_invalid_base():\n    raises(ValueError, lambda: digits(123, 1))\n    raises(ValueError, lambda: digits(123, 0))\n    raises(ValueError, lambda: digits(123, -10))\n"], "sample_1113": ["def test_BlockDiagMatrix():\n    I = Identity(3)\n    Z = ZeroMatrix(3, 3)\n    BDM = BlockDiagMatrix(I, Z)\n\n    assert BDM.shape == (6, 6)\n    assert BDM.blockshape == (2, 2)\n    assert BDM.rowblocksizes == [3, 3]\n    assert BDM.colblocksizes == [3, 3]\n    assert BDM.blocks[0, 0] == I\n    assert BDM.blocks[1, 1] == Z\n    assert BDM.blocks[0, 1] == ZeroMatrix(3, 3)\n    assert BDM.blocks[1, 0] == ZeroMatrix(3, 3)\n\n    # Check block multiplication\n    BDM2 = BlockDiagMatrix(Z, I)\n    assert BDM._blockmul(BDM2) == BlockDiagMatrix(I*Z, Z*I)\n\n    # Check block addition\n    BDM3 = BlockDiagMatrix(I, I)\n    assert BDM._blockadd(BDM3) == BlockDiagMatrix(I + I, Z + I)\n\n    # Check transpose\n    assert BDM._eval_transpose() == BlockDiagMatrix(I.T, Z.T)\n\n    # Check inverse\n    BDM4 = BlockDiagMatrix(I, Identity(3))\n    assert BDM4._eval_inverse() == BlockDiagMatrix(I.inverse(), Identity(3).inverse())\n", "def test_blockmatrix_structurally_equal():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = ZeroMatrix(2, 2)\n    B1 = BlockMatrix([[X, Z], [Z, Y]])\n    B2 = BlockMatrix([[X, Z], [Z, Y]])\n    B3 = BlockMatrix([[Y, Z], [Z, X]])\n    assert B1.structurally_equal(B2) == True\n    assert B1.structurally_equal(B3) == False\n", "def test_block_matrix_properties():\n    I = Identity(3)\n    Z = ZeroMatrix(3, 3)\n    B = BlockMatrix([[I, Z], [Z, I]])\n\n    assert B.shape == (6, 6)\n    assert B.blockshape == (2, 2)\n    assert B.rowblocksizes == [3, 3]\n    assert B.colblocksizes == [3, 3]\n\n    assert B.structurally_equal(B)\n    assert B.structurally_equal(BlockMatrix([[I, Z], [Z, I]]))\n    assert not B.structurally_equal(BlockMatrix([[I, I], [Z, Z]]))\n    assert not B.structurally_equal(BlockMatrix([[I]]))\n\n    assert B.is_structurally_symmetric\n    assert not BlockMatrix([[I, I], [Z, I]]).is_structurally_symmetric\n\n    assert B.is_Identity\n    assert not BlockMatrix([[I, I], [I, I]]).is_Identity\n\n    assert B.transpose().equals(BlockMatrix([[I.T, Z.T], [Z.T, I.T]]))\n    assert B.transpose().transpose().equals(B)\n\n    real, imag = B.as_real_imag()\n    assert real.equals(B)\n    assert imag.equals(BlockMatrix([[Z, Z], [Z, Z]]))\n\n    assert B.equals(BlockMatrix([[I, Z], [Z, I]]))\n    assert not B.equals(BlockMatrix([[I, I], [Z, I]]))\n", "def test_blockmatrix_multiplication():\n    I = Identity(2)\n    Z = ZeroMatrix(2, 2)\n    B1 = BlockMatrix([[I, Z], [Z, I]])\n    B2 = BlockMatrix([[I, I], [I, I]])\n    B3 = block_collapse(B1 * B2)\n    assert B3 == BlockMatrix([[I, I], [I, I]])\n    assert B3.shape == (4, 4)\n    assert B3.blockshape == (2, 2)\n    assert B3.rowblocksizes == [2, 2]\n    assert B3.colblocksizes == [2, 2]\n", "def test_blockmatrix_determinant():\n    # Test determinant of a regular block matrix\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n\n    BM = BlockMatrix([[A, B], [C, D]])\n\n    # Case when A is invertible\n    from sympy import Q, ask\n    assert BM._eval_determinant() == Determinant(BM)\n    assert ask(Q.invertible(A))\n    assert BM._eval_determinant().simplify() == det(A) * det(D - C * A.I * B)\n\n    # Case when D is invertible\n    assert ask(Q.invertible(D))\n    assert BM._eval_determinant().simplify() == det(D) * det(A - B * D.I * C)\n\n    # Test determinant of a BlockDiagMatrix\n    X = MatrixSymbol('X', 3, 3)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 1, 1)\n    BDM = BlockDiagMatrix(X, Y, Z)\n\n    assert BDM._eval_determinant().simplify() == det(X) * det(Y) * det(Z)\n"], "sample_1114": ["def test_Rationals_properties():\n    assert S.Rationals.is_iterable\n    assert not S.Rationals.is_empty\n    assert not S.Rationals.is_finite_set\n    assert S.Rationals.boundary == S.Reals\n    assert S.Rationals.inf == S.NegativeInfinity\n    assert S.Rationals.sup == S.Infinity\n\n    assert S.Rationals.contains(Rational(1, 3)) == True\n    assert S.Rationals.contains(Integer(2)) == True\n    assert S.Rationals.contains(pi) == False\n    assert S.Rationals.contains(I) == False\n    assert S.Rationals.contains(Symbol('x', rational=True)) == Contains(Symbol('x', rational=True), S.Rationals, evaluate=False)\n    assert S.Rationals.contains(Symbol('x', integer=True)) == True\n", "def test_Rationals_contains():\n    assert S.Rationals._contains(S.Half) == True\n    assert S.Rationals._contains(Rational(3, 2)) == True\n    assert S.Rationals._contains(Rational(-3, 2)) == True\n    assert S.Rationals._contains(S.One) == True\n    assert S.Rationals._contains(S.Zero) == True\n    assert S.Rationals._contains(S.Pi) == False\n    assert S.Rationals._contains(S.Infinity) == False\n    assert S.Rationals._contains(I) == False\n    assert S.Rationals._contains(Symbol('x', real=True)) is None\n    assert S.Rationals._contains(Symbol('x', integer=True)) is None\n    assert S.Rationals._contains(Symbol('x', positive=True)) is None\n    assert S.Rationals._contains(Symbol('x', negative=True)) is None\n    assert S.Rationals._contains(Symbol('x', rational=True)) is None\n    assert S.Rationals._contains(Basic()) == False\n", "def test_Rationals_properties():\n    R = S.Rationals\n    assert R.is_iterable is True\n    assert R.is_empty is False\n    assert R.is_finite_set is False\n    assert R._inf == -oo\n    assert R._sup == oo\n    assert R.boundary == S.Reals\n    assert R.as_relational(x) == And(Eq(x, floor(x)), x >= R._inf, x <= R._sup)\n", "def test_rationals_contains():\n    R = S.Rationals\n    assert 0 in R\n    assert 1 in R\n    assert -1 in R\n    assert S.Half in R\n    assert Rational(-1, 2) in R\n    assert Rational(2, 3) in R\n    assert 1.5 not in R\n    assert sqrt(2) not in R\n    assert pi not in R\n    assert I not in R\n    assert 0.5 not in R  # float not considered Rational\n    assert Basic() not in R\n", "def test_rationals_contains():\n    R = S.Rationals\n    assert Rational(1, 3) in R\n    assert Rational(-1, 7) in R\n    assert Rational(22, 7) in R\n    assert (1, 2) not in R\n    assert pi not in R\n    assert sqrt(2) not in R\n    assert Rational(1, 3) in iter(R)\n    assert Rational(-1, 2) in iter(R)\n    assert 0 in R\n    assert 1 in R\n    assert -1 in R\n    assert 1.1 not in R\n    assert 3.0 not in R\n"], "sample_1115": ["def test_tensor_index_structure():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    a, b, c, d = tensor_indices('a,b,c,d', Lorentz)\n    A = TensorHead('A', [Lorentz]*2, TensorSymmetry.fully_symmetric(2))\n\n    # Test creation from indices\n    index_structure = _IndexStructure.from_indices(a, -b, c, -d)\n    assert index_structure.free == [(a, 0), (c, 2)]\n    assert index_structure.dum == [(1, 3)]\n    assert index_structure.index_types == [Lorentz, Lorentz, Lorentz, Lorentz]\n    assert index_structure.get_indices() == [a, TensorIndex('L_0', Lorentz, False), c, TensorIndex('L_0', Lorentz, True)]\n\n    # Test creation from components, free and dum\n    index_structure_2 = _IndexStructure.from_components_free_dum([A], [(a, 0)], [(1, 2)])\n    assert index_structure_2.free == [(a, 0)]\n    assert index_structure_2.dum == [(1, 2)]\n    assert index_structure_2.index_types == [Lorentz, Lorentz]\n    assert index_structure_2.get_indices() == [a, TensorIndex('L_0', Lorentz, False)]\n\n    # Test permutation to tensor\n    permuted_index_structure = index_structure.perm2tensor(Permutation([2, 0, 1, 3]))\n    assert permuted_index_structure.free == [(c, 0), (a, 1)]\n    assert permuted_index_structure.dum == [(2, 3)]\n    assert permuted_index_structure.index_types == [Lorentz, Lorentz, Lorentz, Lorentz]\n\n    # Test generation of indices from free, dum, and index types\n    generated_indices = _IndexStructure.generate_indices_from_free_dum_index_types([(a, 0)], [(1, 2)], [Lorentz, Lorentz, Lorentz])\n    assert generated_indices == [a, TensorIndex('L_0', Lorentz, False), TensorIndex('L_0', Lorentz, True)]\n\n    # Test sorting of free and dummy indices for canonicalization\n    sorted_free = index_structure._get_sorted_free_indices_for_canon()\n    assert sorted_free == [(a, 0), (c, 2", "def test_index_structure_from_components_free_dum():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    a, b, c, d = tensor_indices('a,b,c,d', Lorentz)\n    A, B = tensor_heads('A,B', [Lorentz]*2, TensorSymmetry.fully_symmetric(2))\n\n    components = [A, B]\n    free = [(a, 0), (b, 1)]\n    dum = [(2, 3)]\n\n    index_structure = _IndexStructure.from_components_free_dum(components, free, dum)\n\n    expected_indices = [a, b, c, -c]\n    assert index_structure.get_indices() == expected_indices\n    assert index_structure.free == [(a, 0), (b, 1)]\n    assert index_structure.dum == [(2, 3)]\n    assert index_structure.index_types == [Lorentz, Lorentz, Lorentz, Lorentz]\n", "def test_TensorElement():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    a, b, c, d = tensor_indices('a, b, c, d', Lorentz)\n    A = TensorHead('A', [Lorentz]*2)\n    expr = A(a, b)\n\n    # Test TensorElement with single free index\n    te = TensorElement(expr, {a: 0})\n    assert te.get_free_indices() == [b]\n    assert te.expr == expr\n    assert te.index_map == {a: 0}\n\n    # Ensure TensorElement returns original tensor when no index_map is provided\n    assert TensorElement(expr, {}) == expr\n\n    # Ensure TensorElement correctly reduces rank\n    te = TensorElement(expr, {a: 0, b: 1})\n    assert te.get_free_indices() == []\n\n    # Ensure TensorElement raises ValueError for invalid indices\n    raises(ValueError, lambda: TensorElement(expr, {Symbol('invalid'): 0}))\n\n    # Test contraction with TensorElement\n    B = TensorHead('B', [Lorentz]*2)\n    expr = A(a, b) * B(-a, c)\n    te = TensorElement(expr, {a: 0})\n    assert str(te) == 'A(0, b)*B(0, c)'\n\n    # Verify the contraction simplifies with TensorElement\n    expr = A(a, -a)\n    te = TensorElement(expr, {a: 0})\n    assert te == A(Lorentz.dummy_name + '_0', -Lorentz.dummy_name + '_0')\n", "def test_tensor_data_lazy_evaluator():\n    # Test `_TensorDataLazyEvaluator` functionality\n    from sympy.tensor.tensor import _TensorDataLazyEvaluator, TensorHead, TensorIndexType, tensor_indices\n\n    # Define index type and indices\n    Lorentz = TensorIndexType('Lorentz', dim=4)\n    i0, i1, i2, i3 = tensor_indices('i0:4', Lorentz)\n\n    # Define tensor head and assign data\n    A = TensorHead('A', [Lorentz])\n    data = [1, 2, 3, 4]\n    A.data = data\n\n    evaluator = _TensorDataLazyEvaluator()\n\n    # Test getting data from the evaluator\n    assert evaluator[A] == data\n\n    # Test setting data through the evaluator\n    new_data = [5, 6, 7, 8]\n    evaluator[A] = new_data\n    assert evaluator[A] == new_data\n\n    # Test deleting data through the evaluator\n    del evaluator[A]\n    assert evaluator[A] is None\n\n    # Test data contract dum\n    from sympy.tensor.array import MutableDenseNDimArray\n    array1 = MutableDenseNDimArray([1, 2, 3, 4])\n    array2 = MutableDenseNDimArray([5, 6, 7, 8])\n    dum = [(0, 1)]\n    contracted_data = _TensorDataLazyEvaluator.data_contract_dum([array1, array2], dum, 2)\n    assert contracted_data == 70\n\n    # Test metric data assignment and retrieval\n    metric_data = [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, -1]]\n    evaluator.add_metric_data(Lorentz.metric, metric_data)\n    assert evaluator._substitutions_dict_tensmul[Lorentz.metric, True, True] == metric_data\n    assert evaluator._substitutions_dict_tensmul[Lorentz.metric, False, False] == [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, -1]]\n\n    # Test invalid assignment\n    with raises(ValueError):\n        evaluator[A] =", "def test_TensorDataLazyEvaluator():\n    from sympy.tensor.array import Array, MutableDenseNDimArray\n\n    # Create an instance of the _TensorDataLazyEvaluator\n    evaluator = _TensorDataLazyEvaluator()\n\n    # Define Lorentz index type and metric\n    Lorentz = TensorIndexType('Lorentz', dim=4)\n    metric = Lorentz.metric\n\n    # Define tensor indices\n    i, j, k = tensor_indices('i,j,k', Lorentz)\n\n    # Define tensors and their data\n    A = TensorHead('A', [Lorentz])\n    B = TensorHead('B', [Lorentz])\n    C = TensorHead('C', [Lorentz]*2, TensorSymmetry.fully_symmetric(2))\n\n    A.data = [1, 2, 3, 4]\n    B.data = [4, 3, 2, 1]\n    C.data = [[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7]]\n\n    # Add data to evaluator\n    evaluator[A] = A.data\n    evaluator[B] = B.data\n    evaluator[C] = C.data\n\n    # Test data retrieval\n    assert evaluator[A] == MutableDenseNDimArray([1, 2, 3, 4])\n    assert evaluator[B] == MutableDenseNDimArray([4, 3, 2, 1])\n    assert evaluator[C] == MutableDenseNDimArray([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7]])\n\n    # Test metric data assignment\n    metric_data = Array([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, -1]])\n    evaluator.add_metric_data(metric, metric_data)\n\n    # Test metric data retrieval\n    assert evaluator._substitutions_dict_tensmul[(metric, True, True)] == metric_data\n    assert evaluator._substitutions_dict_tensmul[(metric, False, False)] == metric_data.inv()\n    assert evaluator._sub"], "sample_1116": ["def test_refine_unitary_singular():\n    from sympy import assuming\n\n    with assuming(Q.unitary(C)):\n        assert refine(Inverse(C), Q.unitary(C)) == C.conjugate()\n    \n    with assuming(Q.singular(C)):\n        raises(ValueError, lambda: refine(Inverse(C), Q.singular(C)))\n", "def test_inverse_determinant():\n    from sympy.matrices.expressions.determinant import det\n    A = MatrixSymbol('A', 3, 3)\n    inv_A = Inverse(A)\n    assert inv_A._eval_determinant() == 1 / det(A)\n", "def test_refine_unitary():\n    U = MatrixSymbol('U', 2, 2)\n    assert refine(U.I, Q.unitary(U)) == U.conjugate()\n", "def test_refine_unitary():\n    U = MatrixSymbol('U', n, n)\n    assert refine(U.I, Q.unitary(U)) == U.conjugate()\n", "def test_refine_unitary():\n    U = MatrixSymbol('U', n, n)\n    assert refine(U.I, Q.unitary(U)) == U.conjugate()\n"], "sample_1117": ["def test_ask_diagonal_elements():\n    assert ask(Q.diagonal(X + Z.T), Q.diagonal(X) & Q.diagonal(Z))\n    assert ask(Q.diagonal(X * Z.T), Q.diagonal(X) & Q.diagonal(Z))\n    assert ask(Q.diagonal(A1x1 * B1x1), Q.diagonal(A1x1) & Q.diagonal(B1x1))\n    assert ask(Q.diagonal(A1x1 + B1x1), Q.diagonal(A1x1) & Q.diagonal(B1x1))\n    assert ask(Q.diagonal(V1.T * V1), Q.diagonal(V1.T) & Q.diagonal(V1))\n    assert ask(Q.diagonal(V1.T * (V1 + V2)), Q.diagonal(V1.T) & Q.diagonal(V1 + V2))\n    assert ask(Q.diagonal(V1.T * (V1 + V2) + A1x1), Q.diagonal(V1.T) & Q.diagonal(V1 + V2) & Q.diagonal(A1x1))\n", "def test_MatAdd():\n    assert ask(Q.upper_triangular(X + Z), Q.upper_triangular(X) & Q.upper_triangular(Z))\n    assert ask(Q.lower_triangular(X + Z), Q.lower_triangular(X) & Q.lower_triangular(Z))\n    assert ask(Q.unitary(X + Z), Q.unitary(X) & Q.unitary(Z)) is None\n    assert ask(Q.orthogonal(X + Z), Q.orthogonal(X) & Q.orthogonal(Z)) is None\n    assert ask(Q.real_elements(X + Z), Q.real_elements(X) & Q.real_elements(Z)) is None\n    assert ask(Q.complex_elements(X + Z), Q.complex_elements(X) & Q.complex_elements(Z)) is None\n    assert ask(Q.integer_elements(X + Z), Q.integer_elements(X) & Q.integer_elements(Z)) is None\n    assert ask(Q.diagonal(X + Z), Q.diagonal(X) & Q.diagonal(Z)) is None\n", "def test_orthogonal_unitary_matrixslice():\n    X = MatrixSymbol('X', 4, 4)\n    B = MatrixSlice(X, (1, 3), (1, 3))\n    C = MatrixSlice(X, (0, 3), (1, 3))\n    \n    assert ask(Q.unitary(B), Q.unitary(X))\n    assert not ask(Q.unitary(C), Q.unitary(X))\n    assert ask(Q.orthogonal(B), Q.orthogonal(X))\n    assert not ask(Q.orthogonal(C), Q.orthogonal(X))\n", "def test_fullrank_with_singular():\n    assert ask(Q.fullrank(X), Q.singular(X)) is False\n    assert ask(Q.fullrank(X), ~Q.singular(X)) is True\n    assert ask(Q.fullrank(X*Z), Q.fullrank(X) & Q.singular(Z)) is False\n    assert ask(Q.fullrank(Identity(3)), Q.singular(Identity(3))) is False\n    assert ask(Q.fullrank(ZeroMatrix(3, 3)), Q.singular(ZeroMatrix(3, 3))) is False\n    assert ask(Q.fullrank(OneMatrix(1, 1)), Q.singular(OneMatrix(1, 1))) is False\n    assert ask(Q.fullrank(OneMatrix(3, 3)), Q.singular(OneMatrix(3, 3))) is False\n    assert ask(Q.fullrank(X), Q.singular(X) & Q.square(X)) is False\n", "def test_ask_zero_matrix():\n    assert ask(Q.zero_matrix(ZeroMatrix(3, 3)))\n    assert not ask(Q.zero_matrix(Identity(3)))\n    assert not ask(Q.zero_matrix(OneMatrix(3, 3)))\n    assert ask(Q.zero_matrix(ZeroMatrix(1, 1))) is True\n    assert ask(Q.zero_matrix(MatrixSlice(ZeroMatrix(3, 3), (0, 1), (0, 1)))) is True\n    assert not ask(Q.zero_matrix(MatrixSlice(Identity(3), (0, 1), (0, 1))))\n"], "sample_1118": ["def test_matpow():\n    P = MatrixSymbol('P', n, n)\n\n    # Test power of Identity matrix\n    assert MatPow(Identity(n), 2).doit() == Identity(n)\n    assert MatPow(Identity(n), 0).doit() == Identity(n)\n\n    # Test power of ZeroMatrix\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n\n    # Test power of non-square matrix\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n\n    # Test power of a permutation matrix\n    perm_matrix = PermutationMatrix([1, 0])\n    assert MatPow(perm_matrix, 2).doit() == Identity(2)\n\n    # Test power of a MatrixBase instance\n    from sympy.matrices import Matrix\n    mat = Matrix([[1, 2], [3, 4]])\n    assert MatPow(mat, 2).doit() == mat**2\n\n    # Test negative power of a square matrix\n    assert MatPow(C, -1).doit() == Inverse(C).doit()\n    assert MatPow(C, -2).doit() == MatPow(Inverse(C).doit(), 2).doit()\n\n    # Test canonicalization\n    assert MatPow(MatPow(C, 2), 3).doit() == MatPow(C, 6).doit()\n    assert MatPow(MatPow(C, -2), 3).doit() == MatPow(C, -6).doit()\n", "def test_matpow_properties():\n    # Test MatPow properties: base, exp, shape\n    P = MatPow(C, 2)\n    assert P.base == C\n    assert P.exp == 2\n    assert P.shape == (n, n)\n\n    Q = MatPow(A, 3)\n    assert Q.base == A\n    assert Q.exp == 3\n    assert Q.shape == (n, m)\n\n    R = MatPow(D, 0)\n    assert R.base == D\n    assert R.exp == 0\n    assert R.shape == (n, n)\n", "def test_matpow():\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', n, n)\n    Z = ZeroMatrix(n, n)\n    I = Identity(n)\n    \n    # Test exponent is zero\n    assert MatPow(X, 0).doit() == I\n\n    # Test exponent is one\n    assert MatPow(X, 1).doit() == X\n\n    # Test exponent is negative one\n    assert MatPow(X, -1).doit() == Inverse(X).doit()\n\n    # Test zero matrix with positive exponent\n    assert MatPow(Z, 3).doit() == Z\n\n    # Test identity matrix with positive exponent\n    assert MatPow(I, 3).doit() == I\n\n    # Test non-square matrix error\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n\n    # Test non-invertible matrix error\n    raises(NonInvertibleMatrixError, lambda: MatPow(Z, -1).doit())\n\n    # Test multiplication of matrix powers\n    assert MatPow(MatPow(X, 2), 3).doit() == MatPow(X, 6).doit()\n\n    # Test evaluation with evaluate=True\n    assert MatPow(X, 2, evaluate=True).doit() == X**2\n", "def test_matpow():\n    from sympy import symbols\n    n, m = symbols('n m', integer=True)\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n\n    # Test MatPow properties\n    assert MatPow(A, 2).base == A\n    assert MatPow(A, 2).exp == 2\n    assert MatPow(A, 2).shape == (n, n)\n\n    # Test evaluation of MatPow\n    assert MatPow(A, 0).doit() == Identity(n)\n    assert MatPow(A, 1).doit() == A\n    raises(NonSquareMatrixError, lambda: MatPow(MatrixSymbol('B', n, m), 2).doit())\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n\n    # Test MatPow with permutation matrices\n    P = PermutationMatrix(3, [2, 0, 1])\n    assert MatPow(P, 2).doit() == P * P\n    assert MatPow(P, -1).doit() == Inverse(P).doit()\n    \n    # Test MatPow transpose\n    assert MatPow(A, 2).T == MatPow(A.T, 2)\n    assert MatPow(A, -1).T == MatPow(A.T, -1)\n\n    # Test MatPow derivative\n    x = symbols('x')\n    expr = MatPow(A, 2)\n    assert expr.diff(x) == MatPow(A, 2)._eval_derivative(x)\n", "def test_matpow_doit():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    \n    # Testing MatPow with Identity matrix\n    assert MatPow(Identity(n), 2).doit() == Identity(n)\n    assert MatPow(Identity(n), 0).doit() == Identity(n)\n    assert MatPow(Identity(n), -1).doit() == Identity(n)\n    \n    # Testing MatPow with ZeroMatrix\n    assert MatPow(ZeroMatrix(n, n), 3).doit() == ZeroMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n    \n    # Testing MatPow with other matrices\n    assert MatPow(A, 1).doit() == A\n    assert MatPow(A, 0).doit() == Identity(n)\n    assert MatPow(A, -1).doit() == Inverse(A).doit()\n    \n    # Testing MatPow with symbolic exponent\n    assert MatPow(A, B).doit() == MatPow(A, B)\n    assert MatPow(MatPow(A, 2), 3).doit() == MatPow(A, 6).doit()\n    \n    # Ensure deep evaluation\n    assert MatPow(MatPow(A, 2), 3).doit(deep=True) == MatPow(A, 6).doit(deep=True)\n    \n    # Ensure non-deep evaluation\n    assert MatPow(MatPow(A, 2), 3).doit(deep=False) == MatPow(MatPow(A, 2), 3).doit(deep=False)\n"], "sample_1119": ["def test_deferred_vector():\n    X = DeferredVector('X')\n    assert X[0] == Symbol('X[0]')\n    assert X[1] == Symbol('X[1]')\n    assert X[0] != Symbol('X[1]')\n    assert str(X) == 'X'\n    assert repr(X) == \"DeferredVector('X')\"\n    raises(IndexError, lambda: X[-1])\n    raises(IndexError, lambda: X[-10])\n", "def test_matrix_determinant_methods():\n    from sympy import Matrix\n    M = Matrix([[1, 2], [3, 4]])\n    \n    # Test det method\n    assert M.det() == -2\n    \n    # Test adjugate method\n    assert M.adjugate() == Matrix([[4, -2], [-3, 1]])\n    \n    # Test charpoly method\n    cp = M.charpoly()\n    assert cp.all_coeffs() == [1, -5, -2]\n    \n    # Test cofactor method\n    assert M.cofactor(0, 0) == 4\n    assert M.cofactor(0, 1) == -3\n    assert M.cofactor(1, 0) == -2\n    assert M.cofactor(1, 1) == 1\n    \n    # Test cofactor_matrix method\n    assert M.cofactor_matrix() == Matrix([[4, -3], [-2, 1]])\n    \n    # Test minor method\n    assert M.minor(0, 0) == 4\n    assert M.minor(0, 1) == 3\n    assert M.minor(1, 0) == 2\n    assert M.minor(1, 1) == 1\n    \n    # Test minor_submatrix method\n    assert M.minor_submatrix(0, 0) == Matrix([[4]])\n    assert M.minor_submatrix(0, 1) == Matrix([[3]])\n    assert M.minor_submatrix(1, 0) == Matrix([[2]])\n    assert M.minor_submatrix(1, 1) == Matrix([[1]])\n", "def test_deferred_vector():\n    from sympy import lambdify\n\n    X = DeferredVector('X')\n    assert str(X) == 'X'\n    assert repr(X) == \"DeferredVector('X')\"\n\n    expr = (X[0] + 2, X[2] + 3)\n    func = lambdify(X, expr)\n    assert func([1, 2, 3]) == (3, 6)\n\n    # Testing index out of range\n    raises(IndexError, lambda: X[-1])\n    raises(IndexError, lambda: X[2**31])\n", "def test_matrix_determinant_methods():\n    from sympy import Matrix\n    mat = Matrix([[1, 2], [3, 4]])\n\n    # Test _eval_det_bareiss\n    assert mat._eval_det_bareiss() == -2\n\n    # Test _eval_det_berkowitz\n    assert mat._eval_det_berkowitz() == -2\n\n    # Test _eval_det_lu\n    assert mat._eval_det_lu() == -2\n\n    # Test adjugate\n    assert mat.adjugate() == Matrix([[-4, 2], [3, -1]])\n\n    # Test charpoly\n    poly = mat.charpoly()\n    assert poly.coeffs() == [S.One, -5, -2]\n\n    # Test cofactor\n    assert mat.cofactor(0, 0) == -4\n    assert mat.cofactor(0, 1) == 3\n\n    # Test cofactor_matrix\n    assert mat.cofactor_matrix() == Matrix([[-4, 3], [2, -1]])\n\n    # Test det\n    assert mat.det() == -2\n\n    # Test minor\n    assert mat.minor(0, 0) == 4\n\n    # Test minor_submatrix\n    assert mat.minor_submatrix(0, 0) == Matrix([[4]])\n", "def test_Dirac_conjugate():\n    from sympy import Matrix, I, eye\n    m = Matrix((0, 1 + I, 2, 3))\n    assert m.D == Matrix([[0, 1 - I, -2, -3]])\n    m = eye(4) + I*eye(4)\n    m[0, 3] = 2\n    assert m.D == Matrix([\n        [1 - I,     0,      0,      0],\n        [    0, 1 - I,      0,      0],\n        [    0,     0, -1 + I,      0],\n        [    2,     0,      0, -1 + I]])\n    raises(AttributeError, lambda: Matrix(eye(2)).D)\n"], "sample_1120": ["def test_matrix_element():\n    A = MatrixSymbol('A', 3, 3)\n    i, j = symbols('i j')\n    element = A[i, j]\n\n    # Test that MatrixElement properties work as expected\n    assert element.parent == A\n    assert element.i == i\n    assert element.j == j\n\n    # Test MatrixElement evaluation\n    explicit_matrix = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert element.subs(A, explicit_matrix).doit() == explicit_matrix[i, j]\n\n    # Test differentiation with respect to another MatrixElement\n    B = MatrixSymbol('B', 3, 3)\n    diff_element = element.diff(B[i, j])\n    assert diff_element == KroneckerDelta(i, i, (0, 2)) * KroneckerDelta(j, j, (0, 2))\n\n    # Test invalid index\n    with raises(IndexError):\n        A[3, 3]\n    with raises(IndexError):\n        A[-1, 0]\n\n    # Test valid index\n    assert A[0, 0] == MatrixElement(A, 0, 0)\n", "def test_matrixexpr_properties():\n    expr = MatrixExpr()\n    \n    assert expr.is_Matrix\n    assert expr.is_MatrixExpr\n    assert expr.is_Identity is None\n    assert not expr.is_Inverse\n    assert not expr.is_Transpose\n    assert not expr.is_ZeroMatrix\n    assert not expr.is_MatAdd\n    assert not expr.is_MatMul\n\n    assert not expr.is_commutative\n    assert not expr.is_number\n    assert not expr.is_symbol\n    assert not expr.is_scalar\n\n    with raises(NotImplementedError):\n        abs(expr)\n\n    # Test _eval_simplify method\n    simplified_expr = expr._eval_simplify()\n    assert simplified_expr == expr\n", "def test_MatrixExpr_methods():\n    assert A.adjoint() == adjoint(A)\n    assert A.transpose() == transpose(A)\n    assert A.T == transpose(A)\n    assert A.conjugate() == conjugate(A)\n    assert A.inv() == A.inverse()\n\n    # Testing as_explicit and as_mutable\n    explicit_A = A.as_explicit()\n    assert explicit_A == ImmutableMatrix([[A[i, j] for j in range(m)] for i in range(n)])\n    mutable_A = A.as_mutable()\n    assert mutable_A == Matrix([[A[i, j] for j in range(m)] for i in range(n)])\n\n    # Testing element-wise equality\n    explicit_B = B.as_explicit()\n    assert A.equals(explicit_B) == explicit_A.equals(explicit_B)\n\n    # Testing the applyfunc method\n    func = lambda x: x**2\n    applied_func_matrix = A.applyfunc(func)\n    assert isinstance(applied_func_matrix, MatrixExpr)\n    assert applied_func_matrix[0, 0] == func(A[0, 0])\n\n    # Testing _eval_derivative_array and _eval_derivative\n    deriv_matrix = A._eval_derivative_array(w)\n    assert isinstance(deriv_matrix, MatrixExpr)\n    assert deriv_matrix.shape == (n, 1)\n    \n    deriv_matrix_scalar = A._visit_eval_derivative_scalar(w)\n    assert isinstance(deriv_matrix_scalar, MatrixExpr)\n    assert deriv_matrix_scalar.shape == (n, m)\n\n    deriv_matrix_array = A._visit_eval_derivative_array(w)\n    assert isinstance(deriv_matrix_array, MatrixExpr)\n    assert deriv_matrix_array.shape == (n, m)\n", "def test_matrix_operations():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n    \n    # Test addition\n    expr = A + B + C\n    assert expr == MatAdd(A, B, C).doit()\n    \n    # Test subtraction\n    expr = A - B - C\n    assert expr == MatAdd(A, -B, -C).doit()\n    \n    # Test multiplication\n    expr = A * B * C\n    assert expr == MatMul(A, B, C).doit()\n    \n    # Test power\n    expr = A**2\n    assert expr == MatPow(A, 2).doit()\n    \n    # Test inverse\n    expr = A.I\n    assert expr == Inverse(A).doit()\n    \n    # Test transpose\n    expr = A.T\n    assert expr == Transpose(A).doit()\n    \n    # Test adjoint\n    expr = A.adjoint()\n    assert expr == adjoint(A).doit()\n    \n    # Test matrix element\n    expr = A[0, 0]\n    assert isinstance(expr, MatrixElement)\n    assert expr.parent == A\n    assert expr.i == 0\n    assert expr.j == 0\n\n    # Test as_explicit and as_mutable\n    explicit_A = A.as_explicit()\n    assert explicit_A == ImmutableMatrix([[A[0, 0], A[0, 1]], [A[1, 0], A[1, 1]]])\n    mutable_A = A.as_mutable()\n    assert mutable_A == Matrix([[A[0, 0], A[0, 1]], [A[1, 0], A[1, 1]]])\n\n    # Test matrix operations involving ZeroMatrix and Identity\n    Z = ZeroMatrix(2, 2)\n    I = Identity(2)\n    assert A + Z == A\n    assert A * I == A\n    assert I * A == A\n    assert Z * A == ZeroMatrix(2, 2)\n\n    # Test equals method\n    assert A.equals(A)\n    assert not A.equals(B)\n    assert A.as_explicit().equals(ImmutableMatrix(2, 2, lambda i,", "def test_matrixexpr_addition_subtraction():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', n, m)\n    D = MatrixSymbol('D', n, n)\n\n    # Test addition of MatrixExpr objects\n    expr = A + B + C\n    assert isinstance(expr, MatAdd)\n    assert expr.shape == (n, m)\n    \n    # Test subtraction of MatrixExpr objects\n    expr = A - B - C\n    assert isinstance(expr, MatAdd)\n    assert expr.shape == (n, m)\n    \n    # Test addition with Identity matrix\n    expr = D + Identity(n)\n    assert isinstance(expr, MatAdd)\n    assert expr.shape == (n, n)\n    \n    # Test subtraction with Identity matrix\n    expr = D - Identity(n)\n    assert isinstance(expr, MatAdd)\n    assert expr.shape == (n, n)\n    \n    # Test addition with ZeroMatrix\n    Z = ZeroMatrix(n, m)\n    expr = A + Z\n    assert expr == A\n    \n    # Test subtraction with ZeroMatrix\n    expr = A - Z\n    assert expr == A\n    \n    # Test complex combination of additions and subtractions\n    expr = A + B - C + Z - A + Identity(n)\n    assert isinstance(expr, MatAdd)\n    assert expr.shape == (n, m)\n"], "sample_1121": ["def test_issue_18510():\n    # Ensure _unevaluated_Mul handles edge cases correctly\n    from sympy import sqrt, Mul, S\n    assert _unevaluated_Mul(S(2), S(3)) == 6\n    assert _unevaluated_Mul(S(3.0), S(2)) == 6.0\n    assert _unevaluated_Mul(S(2), S(3), sqrt(2)) == 6*sqrt(2)\n    assert _unevaluated_Mul(S(2), S(3), S(4)) == 24\n    assert _unevaluated_Mul(S(2.0), S(3), S(4)) == 24.0\n    assert _unevaluated_Mul(S(2), S(3), S(4.0)) == 24.0\n    assert _unevaluated_Mul(Mul(S(2), S(3)), S(4)) == 24\n    assert _unevaluated_Mul(S(2), Mul(S(3), S(4))) == 24\n    assert _unevaluated_Mul(S(2), S(3), Mul(S(4), S(5))) == 120\n", "def test_unevaluated_Mul():\n    from sympy.abc import a, b, c\n    from sympy import Mul, S\n    # Test that _unevaluated_Mul properly constructs an unevaluated Mul\n    expr = _unevaluated_Mul(a, b, c)\n    assert expr == Mul(a, b, c, evaluate=False)\n    assert expr.args == (a, b, c)\n\n    # Test that numbers are collected and put in slot 0\n    expr = _unevaluated_Mul(2, a, 3, b, 4)\n    assert expr == Mul(24, a, b, evaluate=False)\n    assert expr.args[0] == 24\n\n    # Test that nested Muls are flattened\n    expr = _unevaluated_Mul(a, Mul(b, c), 2)\n    assert expr == Mul(2, a, b, c, evaluate=False)\n    assert expr.args == (2, a, b, c)\n\n    # Test that args are sorted\n    expr = _unevaluated_Mul(c, b, a)\n    assert expr == Mul(a, b, c, evaluate=False)\n    assert expr.args == (a, b, c)\n\n    # Test that unevaluated Muls compare equal\n    expr1 = _unevaluated_Mul(2, a)\n    expr2 = _unevaluated_Mul(a, 2)\n    assert expr1 == expr2\n\n    # Test with non-commutative arguments\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    expr = _unevaluated_Mul(A, B, 3, a)\n    assert expr == Mul(3, a, A, B, evaluate=False)\n    assert expr.args == (3, a, A, B)\n", "def test_issue_12345_mul__unevaluated_Mul():\n    from sympy.abc import a, b, c, d\n    # _unevaluated_Mul should collect and combine numbers, flatten Muls, sort args\n    expr = _unevaluated_Mul(2, 3, a, b, c, d, 4)\n    assert expr == Mul(24, a, b, c, d)\n    expr = _unevaluated_Mul(a*b, c*d, b*a)\n    assert expr == Mul(a, a, b, b, c, d)\n    # check if returned unevaluated Mul is sorted\n    expr = _unevaluated_Mul(3, 2, c, b, a)\n    assert expr == Mul(6, a, b, c)\n    # test with non-commutative elements\n    A, B, C = symbols('A B C', commutative=False)\n    expr = _unevaluated_Mul(A*B, C)\n    assert expr == Mul(A, B, C, evaluate=False)\n    expr = _unevaluated_Mul(2, 3, A, B)\n    assert expr == Mul(6, A, B, evaluate=False)\n", "def test_unevaluated_Mul():\n    from sympy import Rational, Mul, S, sqrt\n    x, y = symbols('x y')\n\n    # Test collecting and flattening of Numbers\n    a = _unevaluated_Mul(*[S(3.0), x, S(2)])\n    assert a.args[0] == 6.0\n    assert a.args[1] == x\n\n    # Test comparison of unevaluated Muls\n    m1 = _unevaluated_Mul(sqrt(2), sqrt(3))\n    m2 = _unevaluated_Mul(sqrt(3), sqrt(2))\n    assert m1 == m2\n\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m1 == _unevaluated_Mul(u)\n    assert m1 != Mul(*m1.args)\n\n    # Test combination of rational and noncommutative parts\n    nc = Symbol('nc', commutative=False)\n    a = _unevaluated_Mul(2, 3, nc, nc)\n    assert a.args[0] == 6\n    assert a.args[1] == nc*nc\n\n    # Test proper handling of commutative and noncommutative parts\n    a = _unevaluated_Mul(2, x, Rational(1, 3))\n    assert a.args[0] == Rational(2, 3)\n    assert a.args[1] == x\n\n    b = _unevaluated_Mul(x, nc, Rational(2, 3))\n    assert b.args[0] == Rational(2, 3)\n    assert b.args[1] == x\n    assert b.args[2] == nc\n", "def test_unevaluated_Mul():\n    from sympy.core.mul import _unevaluated_Mul as uMul\n    a = uMul(2, 3, x)\n    assert a.args[0] == 6  # Numbers should be combined\n    assert a.args[1] == x\n\n    # Test with nested Muls\n    b = uMul(uMul(2, x), uMul(3, y))\n    assert b.args[0] == 6  # Numbers should be combined\n    assert b.args[1] == x\n    assert b.args[2] == y\n\n    # Test with non-commutative arguments\n    A = Symbol('A', commutative=False)\n    c = uMul(2, x, A, A, 3)\n    assert c.args[0] == 6\n    assert c.args[1] == x\n    assert c.args[2] == A**2\n"], "sample_1122": ["def test_abs_rewrite_as_conjugate():\n    x = Symbol('x', real=True)\n    y = Symbol('y', complex=True)\n    assert Abs(x + I*y).rewrite(conjugate) == sqrt((x + I*y) * (x - I*y))\n    assert Abs(2 + 3*I).rewrite(conjugate) == sqrt((2 + 3*I) * (2 - 3*I))\n    assert Abs(exp(I*pi/4)).rewrite(conjugate) == sqrt(exp(I*pi/4) * exp(-I*pi/4))\n    assert Abs(1 + sqrt(2)*I).rewrite(conjugate) == sqrt((1 + sqrt(2)*I) * (1 - sqrt(2)*I))\n    assert Abs(a * exp(I*x)).rewrite(conjugate) == sqrt((a * exp(I*x)) * (a * exp(-I*x)))\n\n    # Ensure the rewrite with conjugate works for matrices\n    A = Matrix([[1 + I, 2 - 3*I], [4 + 5*I, 6 - I]])\n    assert Abs(A).rewrite(conjugate) == sqrt(A * A.H)\n", "def test_polar_lift():\n    from sympy.functions.elementary.complexes import polar_lift\n    x = Symbol('x', real=True)\n    y = Symbol('y', complex=True)\n\n    # Test basic properties of polar_lift\n    assert polar_lift(x) == x * exp_polar(0)\n    assert polar_lift(2) == 2 * exp_polar(0)\n    assert polar_lift(I) == exp_polar(I * pi / 2)\n    assert polar_lift(-1) == exp_polar(I * pi)\n\n    # Test multiplication\n    assert polar_lift(x * y) == polar_lift(x) * polar_lift(y)\n    assert polar_lift(2 * I) == 2 * exp_polar(I * pi / 2)\n    assert polar_lift(-2 * I) == 2 * exp_polar(-I * pi / 2)\n\n    # Test nested polar_lift\n    assert polar_lift(polar_lift(x)) == polar_lift(x)\n    assert polar_lift(polar_lift(-1)) == exp_polar(I * pi)\n\n    # Test non-lifted expressions\n    assert polar_lift(1 + x) == polar_lift(1 + x)\n    assert polar_lift(x**2 + y) == polar_lift(x**2 + y)\n", "def test_periodic_argument_edge_cases():\n    from sympy import periodic_argument, polar_lift, pi\n\n    # Test boundary conditions\n    p = polar_lift(1 + I)\n    assert periodic_argument(p, 2*pi) == pi/4\n    assert periodic_argument(p, 4*pi) == pi/4\n\n    # Test large values\n    assert periodic_argument(polar_lift(1 + I), 1000*pi) == pi/4\n\n    # Test negative period\n    assert periodic_argument(polar_lift(1 + I), -2*pi).is_Float\n\n    # Test zero period\n    assert periodic_argument(polar_lift(1 + I), 0) == pi/4\n\n    # Test when arg is zero\n    assert periodic_argument(0, 2*pi) == 0\n", "def test_rewrite_as_re_im():\n    x, y = symbols('x y', real=True)\n    \n    expr = re(x) + I*im(y)\n    assert expr.rewrite(re) == x\n    assert expr.rewrite(im) == I*y\n    \n    expr = im(x) - re(y)\n    assert expr.rewrite(re) == -y\n    assert expr.rewrite(im) == -I*x\n    \n    expr = re(x + I*y)\n    assert expr.rewrite(re) == x\n    assert expr.rewrite(im) == y\n    \n    expr = im(x - I*y)\n    assert expr.rewrite(re) == -y\n    assert expr.rewrite(im) == -x\n    \n    expr = re(x + y*I)\n    assert expr.rewrite(re) == x\n    assert expr.rewrite(im) == y\n    \n    expr = im(x + y*I)\n    assert expr.rewrite(re) == y\n    assert expr.rewrite(im) == x\n", "def test_polarify_lift():\n    from sympy import polar_lift, sin, cos, sqrt\n\n    # test lifting of simple expressions\n    x = Symbol('x')\n    assert polar_lift(x + 1) == polar_lift(x + 1)\n    assert polar_lift(x + I) == polar_lift(x + I)\n    assert polar_lift(sin(x) + I*cos(x)) == polar_lift(sin(x) + I*cos(x))\n\n    # test lifting of more complex expressions\n    assert polar_lift((1 + I)**2) == polar_lift((1 + I)**2)\n    assert polar_lift(sqrt(2) + I*sqrt(2)) == polar_lift(sqrt(2) + I*sqrt(2))\n\n    # test lifting of expressions with nested functions\n    assert polar_lift(sin(x) + cos(x)) == polar_lift(sin(x) + cos(x))\n    assert polar_lift(sqrt(sin(x)**2 + cos(x)**2)) == polar_lift(sqrt(sin(x)**2 + cos(x)**2))\n\n    # test lifting with symbols having assumptions\n    r = Symbol('r', real=True)\n    assert polar_lift(r + I) == polar_lift(r + I)\n    p = Symbol('p', positive=True)\n    assert polar_lift(p + I) == polar_lift(p + I)\n    n = Symbol('n', negative=True)\n    assert polar_lift(n + I) == polar_lift(n + I)\n"], "sample_1123": ["def test_ConditionSet_with_unsolved_eqns():\n    eqns = (Eq(x + y, 1), Eq(x - y, 1))\n    cs = ConditionSet((x, y), FiniteSet(*eqns), S.Reals)\n    assert cs.sym == Tuple(x, y)\n    assert cs.condition == And(Eq(x + y, 1), Eq(x - y, 1))\n    assert cs.base_set == S.Reals\n    assert cs.dummy_eq(ConditionSet((x, y), And(Eq(x + y, 1), Eq(x - y, 1)), S.Reals))\n    assert cs != ConditionSet((x, y), And(Eq(x + y, 1), Eq(x - y, 1)), S.Complexes)\n", "def test_CondSet_with_non_symbol_dummy():\n    # Test non-symbol dummy being used in ConditionSet\n    expr = x + 1\n    condition = x < 1\n    base_set = S.Integers\n\n    # This should raise ValueError because x+1 is not recognized in the condition\n    raises(ValueError, lambda: ConditionSet(expr, condition, base_set))\n\n    # A valid usage where the condition recognizes the non-symbol dummy\n    assert ConditionSet(expr, expr < 1, base_set) == ConditionSet(expr, expr < 1, base_set)\n\n    # Ensuring the logic works correctly with non-symbol dummy\n    assert 0 in ConditionSet(expr, expr > 0, FiniteSet(-2, 0, 2))\n    assert -2 not in ConditionSet(expr, expr > 0, FiniteSet(-2, 0, 2))\n", "def test_condset_with_different_sym_types():\n    # Test ConditionSet with non-symbol dummy variables in conditions\n    raises(ValueError, lambda: ConditionSet(x + 1, x + 1 < 1, S.Integers))\n    raises(ValueError, lambda: ConditionSet(x + 1, x < 1, S.Integers))\n    raises(ValueError, lambda: ConditionSet(y + 2, y + 2 < 3, S.Integers))\n    \n    # Ensure the correct handling of non-symbol sym arguments\n    assert ConditionSet(x + 1, x + 1 < 1, S.Integers).dummy_eq(\n        ConditionSet(x + 1, x + 1 < 1, S.Integers)) is True\n    assert ConditionSet(x + 1, x + 1 < 1, S.Integers).dummy_eq(\n        ConditionSet(y + 2, y + 2 < 3, S.Integers)) is False\n\n    # Test with Lambda in condition\n    lam = Lambda(x, x**2 < 4)\n    assert ConditionSet(x, lam(x), Interval(-3, 3)) == \\\n        ConditionSet(x, x**2 < 4, Interval(-3, 3))\n", "def test_CondSet_subs_base_set():\n    s = FiniteSet(x, y)\n    c = ConditionSet(x, x < 2, s)\n    \n    # Substituting in the base set\n    new_base_set = FiniteSet(z, y)\n    assert c.subs(s, new_base_set) == ConditionSet(x, x < 2, new_base_set)\n    \n    # Substituting an element in the base set\n    assert c.subs(x, z) == ConditionSet(z, z < 2, FiniteSet(z, y))\n    assert c.subs(y, w) == ConditionSet(x, x < 2, FiniteSet(x, w))\n    \n    # Substituting a symbol not in the base set\n    assert c.subs(w, z) == c\n", "def test_eval_subs():\n    s = FiniteSet(x, y, z)\n    c = ConditionSet(x, x < 2, s)\n    assert c._eval_subs(x, 1) == c\n    assert c._eval_subs(x, y) == ConditionSet(y, y < 2, s)\n    assert c._eval_subs(x, w) == ConditionSet(w, w < 2, s)\n    assert ConditionSet(x, x < y, s)._eval_subs(y, w) == ConditionSet(x, x < w, s.subs(y, w))\n    \n    n = Symbol('n', negative=True)\n    assert ConditionSet(n, 0 < n, S.Integers)._eval_subs(x, y) == S.EmptySet\n    \n    p = Symbol('p', positive=True)\n    assert ConditionSet(n, n < y, S.Integers)._eval_subs(n, x) == ConditionSet(x, x < y, S.Integers)\n\n    nc = Symbol('nc', commutative=False)\n    raises(ValueError, lambda: ConditionSet(x, x < p, S.Integers)._eval_subs(x, nc))\n    raises(ValueError, lambda: ConditionSet(x, x < p, S.Integers)._eval_subs(x, n))\n    \n    assert ConditionSet(n, n < x, Interval(0, oo))._eval_subs(x, p) == Interval(0, oo)\n    assert ConditionSet(n, n < x, Interval(-oo, 0))._eval_subs(x, p) == Interval(-oo, 0)\n    \n    assert ConditionSet(f(x), f(x) < 1, {w, z})._eval_subs(f(x), y) == ConditionSet(y, y < 1, {w, z})\n\n    k = Symbol('k')\n    img1 = imageset(Lambda(k, 2*k*pi + asin(y)), S.Integers)\n    img2 = imageset(Lambda(k, 2*k*pi + asin(S.One/3)), S.Integers)\n    assert ConditionSet(x, Contains(y, Interval(-1,1)), img1)._eval_subs(y, S.One/3).dummy_eq(img2)\n"], "sample_1124": ["def test_FracElement_set_field():\n    F1, x1, y1 = field(\"x1,y1\", ZZ)\n    F2, x2, y2 = field(\"x2,y2\", QQ)\n\n    f = (x1**2 + 3*y1)/x1\n    f_new_field = f.set_field(F2)\n\n    assert f_new_field.field == F2\n    assert f_new_field.numer == F2.ring.ground_new(f.numer)\n    assert f_new_field.denom == F2.ring.ground_new(f.denom)\n\n    # Ensure the same field returns the original element\n    same_field = f.set_field(F1)\n    assert same_field is f\n", "def test_FracElement_set_field():\n    F1, x, y, z = field(\"x,y,z\", ZZ)\n    F2, a, b, c = field(\"a,b,c\", ZZ)\n    \n    f = (x**2 + y*z)/z\n    g = f.set_field(F2)\n    \n    assert g == (a**2 + b*c)/c\n    assert g.field == F2\n\n    F1, x, y = field(\"x,y\", ZZ)\n    F2, a, b = field(\"a,b\", QQ)\n\n    f = (x**2 + y)/x\n    g = f.set_field(F2)\n    \n    assert g == (a**2 + b)/a\n    assert g.field == F2\n", "def test_FracElement_set_field():\n    F1, x, y = field(\"x,y\", ZZ)\n    F2, u, v = field(\"u,v\", ZZ)\n\n    f = x/y\n\n    # Ensure the field change is correctly applied\n    f_new = f.set_field(F2)\n    assert f_new.numer == F2.ring(x)\n    assert f_new.denom == F2.ring(y)\n    \n    # Check that changing to the same field returns the same object\n    f_same = f.set_field(F1)\n    assert f_same is f\n\n    # Ensure the field change to a field with different generators fails\n    F3, a, b = field(\"a,b\", ZZ)\n    raises(NotImplementedError, lambda: f.set_field(F3))\n", "def test_FracElement_set_field():\n    F1, x, y = field(\"x,y\", ZZ)\n    F2, u, v = field(\"u,v\", QQ)\n\n    f = (x**2 + 3*y)/7\n\n    g = f.set_field(F2)\n    assert isinstance(g, F2.dtype)\n    assert g.numer == F2.ring.ground_new(x**2 + 3*y)\n    assert g.denom == F2.ring.ground_new(7)\n\n    raises(NotImplementedError, lambda: f.set_field(F1))\n", "def test_FracField_to_domain():\n    F, x, y, z = field(\"x,y,z\", QQ)\n    domain = F.to_domain()\n    assert isinstance(domain, FractionField)\n    assert domain.field == F\n"], "sample_1125": ["def test_operator_operations():\n    A = Operator('A')\n    B = Operator('B')\n    C = 2*A*A + I*B\n\n    # Testing attributes and properties\n    assert A.label == ('A',)\n    assert B.label == ('B',)\n    assert A.is_commutative is False\n    assert B.is_commutative is False\n\n    # Testing arithmetic operations\n    assert C == 2*A**2 + I*B\n\n    # Testing commutation properties\n    assert A*B != B*A\n\n    # Testing polynomial expansion\n    e = (A+B)**3\n    expanded_e = A*B*A + A*B**2 + A**2*B + A**3 + B*A*B + B*A**2 + B**2*A + B**3\n    assert e.expand() == expanded_e\n\n    # Testing inverse\n    assert A.inv() == A**(-1)\n    assert A*A.inv() == 1\n", "def test_operator():\n    A = Operator('A')\n    B = Operator('B')\n    \n    # Check if operators are correctly instantiated\n    assert A.label == ('A',)\n    assert B.label == ('B',)\n    \n    # Check operator arithmetic\n    expr = 2*A*A + I*B\n    assert expr == 2*A**2 + I*B\n    \n    # Check commutation properties\n    assert A.is_commutative is False\n    assert B.is_commutative is False\n    assert (A * B) != (B * A)\n    \n    # Check polynomial expansion of operators\n    poly_expr = (A + B)**3\n    expanded_poly_expr = poly_expr.expand()\n    expected_expr = A*B*A + A*B**2 + A**2*B + A**3 + B*A*B + B*A**2 + B**2*A + B**3\n    assert expanded_poly_expr == expected_expr\n\n    # Check operator inverse\n    assert A.inv() == A**(-1)\n    assert A * A.inv() == 1\n\n    # Check IdentityOperator multiplication\n    I = IdentityOperator()\n    assert A * I == A\n    assert I * A == A\n", "def test_operator_arithmetic():\n    A = Operator('A')\n    B = Operator('B')\n    \n    # Test commutativity\n    assert A*B != B*A\n    \n    # Test polynomial expansion\n    expr = (A + B)**3\n    expanded = expr.expand()\n    expected_expansion = A*B*A + A*B**2 + A**2*B + A**3 + B*A*B + B*A**2 + B**2*A + B**3\n    assert expanded == expected_expansion\n\n    # Test inverse\n    assert A.inv() == A**(-1)\n    assert A*A.inv() == 1\n    assert A.inv()*A == 1\n\n    # Test identity operator multiplication\n    I = IdentityOperator()\n    assert A*I == A\n    assert I*A == A\n", "def test_identity_operator():\n    I = IdentityOperator()\n    O = Operator('O')\n    \n    assert I * O == O\n    assert O * I == O\n    assert I * I == I\n    assert I.inv() == I\n    assert I._eval_commutator(O) == Integer(0)\n    assert I._eval_anticommutator(O) == 2 * O\n    assert I._apply_operator(O) == O\n    assert I._eval_power(2) == I\n", "def test_operator_commutation():\n    A = Operator('A')\n    B = Operator('B')\n    assert A.is_commutative is False\n    assert B.is_commutative is False\n    assert A*B != B*A\n"], "sample_1126": ["def test_dagger_of_sum():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n", "def test_dagger_sum():\n    O1 = Operator('O1')\n    O2 = Operator('O2')\n    assert Dagger(O1 + O2) == Dagger(O1) + Dagger(O2)\n\n    m1 = Matrix([[1, I], [2, 3]])\n    m2 = Matrix([[I, 0], [4, -I]])\n    assert Dagger(m1 + m2) == Dagger(m1) + Dagger(m2)\n", "def test_dagger_sum():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n", "def test_dagger_addition():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n    assert Dagger(A - B) == Dagger(A) - Dagger(B)\n    ", "def test_dagger_expression():\n    x, y = symbols('x y', commutative=False)\n    expr = x + y*I\n    dagger_expr = Dagger(expr)\n    assert dagger_expr == Dagger(x) + Dagger(y*I)\n    assert dagger_expr != expr\n\n    expr = x * y\n    dagger_expr = Dagger(expr)\n    assert dagger_expr == Dagger(y) * Dagger(x)\n"], "sample_1127": ["def test_is_alternating():\n    a = Permutation(0, 1, 2)\n    b = Permutation(0, 1, size=3)\n    assert PermutationGroup(a, b).is_alternating == False\n\n    a = Permutation(0, 1, 2, size=4)\n    b = Permutation(1, 2, size=4)\n    assert PermutationGroup(a, b).is_alternating == False\n\n    a = Permutation(0, 1, 2, 3, 4)\n    b = Permutation(0, 3, 1, 2)\n    assert PermutationGroup(a, b).is_alternating == False\n\n    a = Permutation(0, 1, 2, 3, 4, 5)\n    b = Permutation(1, 2, 3, 4)\n    assert PermutationGroup(a, b).is_alternating == False\n\n    a = Permutation(0, 1, 2, 3, 4, 5)\n    b = Permutation(0, 1, 2)(3, 4, 5)\n    assert PermutationGroup(a, b).is_alternating == True\n\n    a = Permutation(0, 1, 2, 3, 4, 5, 6)\n    b = Permutation(1, 2, 3, 4, 5, 6)\n    assert PermutationGroup(a, b).is_alternating == False\n", "def test_orbit_transversal_with_slp():\n    gens = [Permutation([1, 2, 0, 4, 3]), Permutation([0, 1, 3, 2, 4])]\n    G = PermutationGroup(gens)\n    alpha = 0\n    transversal, slps = _orbit_transversal(G.degree, G.generators, alpha, pairs=True, slp=True)\n    for beta, perm in transversal:\n        slp = slps[beta]\n        reconstructed_perm = G.identity\n        for idx in slp:\n            reconstructed_perm = rmul(G.generators[idx], reconstructed_perm)\n        assert reconstructed_perm == perm\n        assert perm(alpha) == beta\n", "def test_is_elementary():\n    # Test for abelian group which is also elementary\n    a = Permutation([1, 0])\n    G = PermutationGroup([a])\n    assert G.is_elementary(2) == True\n\n    # Test for abelian group which is not elementary\n    a = Permutation([1, 2, 0])\n    b = Permutation([2, 0, 1])\n    G = PermutationGroup([a, b])\n    assert G.is_elementary(3) == False\n\n    # Test for non-abelian group which is not elementary\n    a = Permutation([0, 1, 2, 3, 4])\n    b = Permutation([0, 2, 1, 3, 4])\n    G = PermutationGroup([a, b])\n    assert G.is_elementary(2) == False\n", "def test_commutator_subgroup():\n    a = Permutation([1, 2, 0, 4, 3])\n    b = Permutation([0, 1, 3, 2, 4])\n    G = PermutationGroup([a, b])\n    comm_subgroup = G.derived_subgroup()\n    assert comm_subgroup.is_subgroup(G)\n    assert comm_subgroup.is_normal(G)\n    assert comm_subgroup.order() == 3\n    assert comm_subgroup.generators == [Permutation([0, 1, 2, 4, 3])]\n    a = Permutation([1, 2, 3, 4, 0])\n    b = Permutation([1, 0, 2, 3, 4])\n    G = PermutationGroup([a, b])\n    comm_subgroup = G.derived_subgroup()\n    assert comm_subgroup.is_subgroup(G)\n    assert comm_subgroup.is_normal(G)\n    assert comm_subgroup.order() == 12\n    assert comm_subgroup.generators == [Permutation([0, 1, 3, 2, 4]), Permutation([0, 4, 2, 1, 3])]\n", "def test_schreier_sims_naive():\n    a = Permutation([2, 0, 1])\n    b = Permutation([2, 1, 0])\n    G = PermutationGroup([a, b])\n    G.schreier_sims()\n    assert G._base == [0, 1]\n    assert G._degree == 3\n    assert G._order == 6\n    assert G._transversals == [\n        {0: Permutation([0, 1, 2]), 2: Permutation([2, 0, 1]), 1: Permutation([1, 2, 0])},\n        {0: Permutation([0, 1, 2]), 2: Permutation([1, 2, 0])}\n    ]\n\n    gens = [Permutation([0, 2, 1, 3, 4]), Permutation([0, 1, 3, 4, 2])]\n    H = PermutationGroup(gens)\n    H.schreier_sims()\n    assert H._base == [0, 1, 2]\n    assert H._degree == 5\n    assert H._order == 60\n    assert len(H._transversals) == 3\n    assert len(H._transversals[0]) == 5\n    assert len(H._transversals[1]) == 4\n    assert len(H._transversals[2]) == 3\n"], "sample_1128": ["def test_point_set_pos_exceptions():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    \n    # Test TypeError for invalid name in locatenew\n    raises(TypeError, lambda: p1.locatenew(123, 10 * N.x))\n    \n    # Test TypeError for invalid value in set_pos\n    raises(TypeError, lambda: p1.set_pos(p2, \"invalid value\"))\n    \n    # Test TypeError for invalid point in set_pos\n    raises(TypeError, lambda: p1.set_pos(\"invalid point\", 10 * N.x))\n    \n    # Test proper error handling for invalid vector in locatenew\n    raises(TypeError, lambda: p1.locatenew('p3', \"invalid vector\"))\n    \n    # Test correct behavior with valid arguments\n    p1.set_pos(p2, 10 * N.x)\n    assert p1.pos_from(p2) == 10 * N.x\n", "def test_point_acc():\n    q1, q2 = dynamicsymbols('q1 q2')\n    qd1, qd2 = dynamicsymbols('q1 q2', 1)\n    qdd1, qdd2 = dynamicsymbols('q1 q2', 2)\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    P.set_acc(N, qdd1 * N.x + qdd2 * N.y)\n    assert P.acc(N) == qdd1 * N.x + qdd2 * N.y\n    O.set_vel(N, qd1 * N.z)\n    assert O.acc(N) == qd1.diff(dynamicsymbols._t) * N.z\n    O.set_acc(N, 0)\n    assert O.acc(N) == 0\n    raises(KeyError, lambda: P.acc(B))  # B frame not defined\n", "def test_point_set_pos():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    O = Point('O')\n    P = O.locatenew('P', 5 * N.x)\n    P.set_pos(O, 10 * B.x)\n    assert P.pos_from(O) == 10 * B.x\n    assert O.pos_from(P) == -10 * B.x\n    raises(TypeError, lambda: P.set_pos(O, 10))\n    raises(TypeError, lambda: P.set_pos(\"O\", 10 * B.x))\n", "def test_point_set_pos_exceptions():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    raises(TypeError, lambda: p1.set_pos(p2, \"not_a_vector\"))  # Should raise TypeError for invalid vector\n    raises(TypeError, lambda: p1.set_pos(\"not_a_point\", 10 * N.x))  # Should raise TypeError for invalid point\n", "def test_point_set_get_pos():\n    N = ReferenceFrame('N')\n    P = Point('P')\n    Q = Point('Q')\n    P.set_pos(Q, 10 * N.x)\n    assert P.pos_from(Q) == 10 * N.x\n    assert Q.pos_from(P) == -10 * N.x\n    P.set_pos(Q, 5 * N.y)\n    assert P.pos_from(Q) == 5 * N.y\n    assert Q.pos_from(P) == -5 * N.y\n"], "sample_1129": ["def test_expand_fold_reduce_binary_ops():\n    from sympy import Add, Mul\n\n    prntr = PythonCodePrinter()\n    expr_fold = Add(x, y, z, evaluate=False)\n    expr_reduce = Mul(x, y, z, evaluate=False)\n    \n    # Folding binary operations example\n    result_fold = prntr._expand_fold_binary_op('add', expr_fold.args)\n    assert result_fold == 'add(add(x, y), z)'\n\n    # Reducing binary operations example\n    result_reduce = prntr._expand_reduce_binary_op('mul', expr_reduce.args)\n    assert result_reduce == 'mul(mul(x, y), z)'\n\n", "def test_issue_12345():\n    from sympy import atan, atan2, exp, log, log10, log2, log1p\n\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(atan(x)) == 'math.atan(x)'\n    assert prntr.doprint(atan2(x, y)) == 'math.atan2(x, y)'\n    assert prntr.doprint(exp(x)) == 'math.exp(x)'\n    assert prntr.doprint(log(x)) == 'math.log(x)'\n    assert prntr.doprint(log10(x)) == 'math.log10(x)'\n    assert prntr.doprint(log2(x)) == 'math.log2(x)'\n    assert prntr.doprint(log1p(x)) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n\n    assert prntr.doprint(atan(x)) == 'mpmath.atan(x)'\n    assert prntr.doprint(atan2(x, y)) == 'mpmath.atan2(x, y)'\n    assert prntr.doprint(exp(x)) == 'mpmath.exp(x)'\n    assert prntr.doprint(log(x)) == 'mpmath.log(x)'\n    assert prntr.doprint(log10(x)) == 'mpmath.log10(x)'\n    assert prntr.doprint(log2(x)) == 'mpmath.log2(x)'\n    assert prntr.doprint(log1p(x)) == 'mpmath.log1p(x)'\n\n    prntr = NumPyPrinter()\n\n    assert prntr.doprint(atan(x)) == 'numpy.arctan(x)'\n    assert prntr.doprint(atan2(x, y)) == 'numpy.arctan2(x, y)'\n    assert prntr.doprint(exp(x)) == 'numpy.exp(x)'\n    assert prntr.doprint(log(x)) == 'numpy.log(x)'\n    assert prntr.doprint(log10(x)) == 'numpy.log10(x)'\n    assert prntr.doprint(log2(x)) == 'numpy.log2(x)'\n    assert prntr.doprint(log1p(x)) == 'numpy.log1p(x)'\n\n    prntr = SciPyPrinter()\n\n    assert prntr.doprint(atan(x)) == 'numpy.arctan(x)'\n    assert", "def test_pycodes_with_settings():\n    from sympy import sin, cos, log, factorial, tan\n    settings = {'fully_qualified_modules': False}\n\n    prntr = PythonCodePrinter(settings)\n    assert prntr.doprint(sin(x)) == 'sin(x)'\n    assert prntr.doprint(cos(x)) == 'cos(x)'\n    assert prntr.doprint(log(x)) == 'log(x)'\n    assert prntr.doprint(factorial(x)) == 'factorial(x)'\n    assert prntr.doprint(tan(x)) == 'tan(x)'\n    assert prntr.module_imports == set()\n    \n    settings['fully_qualified_modules'] = True\n    prntr = PythonCodePrinter(settings)\n    assert prntr.doprint(sin(x)) == 'math.sin(x)'\n    assert prntr.doprint(cos(x)) == 'math.cos(x)'\n    assert prntr.doprint(log(x)) == 'math.log(x)'\n    assert prntr.doprint(factorial(x)) == 'math.factorial(x)'\n    assert prntr.doprint(tan(x)) == 'math.tan(x)'\n    assert prntr.module_imports == {'math': {'sin', 'cos', 'log', 'factorial', 'tan'}}\n", "def test_CodegenArrayHandling():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct, CodegenArrayContraction, CodegenArrayDiagonal, CodegenArrayPermuteDims, CodegenArrayElementwiseAdd\n    from sympy import MatrixSymbol, symbols\n\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = MatrixSymbol(\"C\", 2, 2)\n    x, y, z = symbols('x y z')\n\n    # Test CodegenArrayTensorProduct\n    expr1 = CodegenArrayTensorProduct(A, B)\n    p = NumPyPrinter()\n    assert p.doprint(expr1) == \"numpy.einsum(A, [0, 1], B, [2, 3])\"\n\n    # Test CodegenArrayContraction\n    expr2 = CodegenArrayContraction(expr1, (0, 2))\n    assert p.doprint(expr2) == \"numpy.einsum(A, [0, 1], B, [2, 3], [0, 2])\"\n\n    # Test CodegenArrayDiagonal\n    expr3 = CodegenArrayDiagonal(A, (0, 1))\n    assert p.doprint(expr3) == \"numpy.diagonal(A, 0, axis1=0, axis2=1)\"\n\n    # Test CodegenArrayPermuteDims\n    expr4 = CodegenArrayPermuteDims(A, [1, 0])\n    assert p.doprint(expr4) == \"numpy.transpose(A, [1, 0])\"\n\n    # Test CodegenArrayElementwiseAdd\n    expr5 = CodegenArrayElementwiseAdd(A, B, C)\n    assert p.doprint(expr5) == \"numpy.add(numpy.add(A, B), C)\"\n", "def test_KroneckerProduct():\n    from sympy import KroneckerProduct\n\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 3, 3)\n    expr = KroneckerProduct(A, B)\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == \"numpy.kron(A, B)\"\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == \"numpy.kron(A, B)\"\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == \"numpy.kron(A, B)\"\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == \"  # Not supported in Python with mpmath:\\n  # KroneckerProduct\\nKroneckerProduct(A, B)\"\n"], "sample_1130": ["def test_point_set_pos_invalid_input():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    raises(TypeError, lambda: p1.set_pos(p2, \"invalid_vector\"))\n    raises(TypeError, lambda: Point(123))\n", "def test_point_set_pos_type_error():\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    raises(TypeError, lambda: P.set_pos(O, \"not a vector\"))  # Test should raise TypeError for invalid vector\n    raises(TypeError, lambda: Point(123))  # Test should raise TypeError for invalid name\n", "def test_set_pos_exceptions():\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    raises(TypeError, lambda: P.set_pos(O, 10))  # Value should be a Vector\n    raises(TypeError, lambda: P.set_pos(\"O\", 10 * N.x))  # Other point should be a Point instance\n", "def test_point_set_pos():\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    Q = Point('Q')\n    R = Point('R')\n    P.set_pos(O, 10 * N.x)\n    assert P.pos_from(O) == 10 * N.x\n    Q.set_pos(P, 5 * N.y)\n    assert Q.pos_from(O) == 10 * N.x + 5 * N.y\n    R.set_pos(Q, 2 * N.z)\n    assert R.pos_from(O) == 10 * N.x + 5 * N.y + 2 * N.z\n    raises(TypeError, lambda: P.set_pos(O, \"invalid\"))  # invalid position type\n    raises(TypeError, lambda: P.set_pos(\"invalid\", 10 * N.x))  # invalid point type\n", "def test_point_set_pos():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    P1 = Point('P1')\n    P2 = Point('P2')\n    P3 = Point('P3')\n    P4 = Point('P4')\n    P1.set_pos(P2, q * N.x)\n    P2.set_pos(P3, q * N.y)\n    P3.set_pos(P4, q * N.z)\n    assert P1.pos_from(P2) == q * N.x\n    assert P2.pos_from(P3) == q * N.y\n    assert P3.pos_from(P4) == q * N.z\n    assert P1.pos_from(P3) == q * N.x + q * N.y\n    assert P1.pos_from(P4) == q * N.x + q * N.y + q * N.z\n    assert P4.pos_from(P1) == -q * N.x - q * N.y - q * N.z\n"], "sample_1131": ["def test_custom_function_definition():\n    from sympy.codegen.ast import FunctionDefinition, Variable, Return, Symbol\n    x = Symbol('x')\n    y = Symbol('y')\n    f = FunctionDefinition(Symbol('f'), [Variable(x), Variable(y)], [Return(x + y)])\n\n    prntr = PythonCodePrinter()\n    result = prntr.doprint(f)\n    expected = 'def f(x, y):\\n    return x + y'\n    assert result == expected\n", "def test_MatrixBase_Printer():\n    from sympy import Matrix, SparseMatrix\n    dense_matrix = Matrix([[1, 2], [3, 4]])\n    sparse_matrix = SparseMatrix(2, 2, {(0, 0): 1, (1, 1): 4})\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(dense_matrix) == 'Matrix([[1, 2], [3, 4]])'\n    assert prntr.doprint(sparse_matrix) == 'Matrix([[1, 0], [0, 4]])'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(dense_matrix) == 'numpy.array([[1, 2], [3, 4]])'\n    assert prntr.doprint(sparse_matrix) == 'numpy.array([[1, 0], [0, 4]])'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(dense_matrix) == 'mpmath.matrix([[1, 2], [3, 4]])'\n    assert prntr.doprint(sparse_matrix) == 'mpmath.matrix([[1, 0], [0, 4]])'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(dense_matrix) == 'numpy.array([[1, 2], [3, 4]])'\n    assert prntr.doprint(sparse_matrix) == 'scipy.sparse.coo_matrix(([1, 4], ([0, 1], [0, 1])), shape=(2, 2))'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(dense_matrix) == 'sympy.Matrix([[1, 2], [3, 4]])'\n    assert prntr.doprint(sparse_matrix) == 'sympy.Matrix([[1, 0], [0, 4]])'\n", "def test_custom_functions_and_constants():\n    # Add custom user-defined functions and constants\n    custom_functions = {'myfunc': 'my_module.myfunc'}\n    custom_constants = {'MYCONST': 'my_module.MYCONST'}\n    prntr = PythonCodePrinter({'user_functions': custom_functions, 'user_constants': custom_constants})\n    \n    # Test custom function\n    class MyFunc(Expr):\n        pass\n\n    expr = MyFunc(x)\n    assert prntr.doprint(expr) == 'my_module.myfunc(x)'\n\n    # Test custom constant\n    class MyConst(Expr):\n        pass\n\n    expr = MyConst()\n    assert prntr.doprint(expr) == 'my_module.MYCONST'\n\n    # Ensure that custom functions and constants are registered correctly\n    assert prntr.known_functions['MyFunc'] == 'my_module.myfunc'\n    assert prntr.known_constants['MyConst'] == 'my_module.MYCONST'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'math' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n\n    expr = x**y\n    assert p.doprint(expr) == 'x**y'\n\n    expr = log1p(x)\n    assert p.doprint(expr) == 'sympy.log1p(x)'\n\n    expr = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(expr**(-1)) == \"A**(-1)\"\n    assert p.doprint(expr**5) == \"A**5\"\n    \n    assert p.doprint(S.Exp1) == 'sympy.E'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    \n    smat = SparseMatrix(2, 5, {(0, 1): 3})\n    assert p.doprint(smat) == 'SparseMatrix(2, 5, {(0, 1): 3})'\n", "def test_print_ITE():\n    from sympy import ITE\n\n    expr = ITE(x > y, x, y)\n    \n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '((x) if (x > y) else (y) if (True) else None)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.where(x > y, x, y)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.where(x > y, x, y)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.where(x > y, x, y)'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == 'sympy.ITE(x > y, x, y)'\n"], "sample_1132": ["def test_least_rotation():\n    assert least_rotation([3, 1, 5, 1, 2]) == 3\n    assert least_rotation('bbaacca') == 3\n    assert least_rotation([1, 2, 3]) == 0\n    assert least_rotation([2, 3, 1]) == 2\n    assert least_rotation([4, 4, 1, 4, 4]) == 2\n    assert rotate_left([3, 1, 5, 1, 2], least_rotation([3, 1, 5, 1, 2])) == [1, 2, 3, 1, 5]\n", "def test_interactive_traversal():\n    from sympy import sin, cos\n    expr = x * (y + z) + w\n    result = capture(lambda: interactive_traversal(expr))\n    assert \"Current expression (stage  0)\" in result\n    assert str(expr) in result\n\n    expr = sin(x) + cos(y)\n    result = capture(lambda: interactive_traversal(expr))\n    assert \"Current expression (stage  0)\" in result\n    assert str(expr) in result\n\n    expr = x**2 + 2*x + 1\n    result = capture(lambda: interactive_traversal(expr))\n    assert \"Current expression (stage  0)\" in result\n    assert str(expr) in result\n", "def test_unflatten():\n    r = list(range(10))\n    assert unflatten(r) == list(zip(r[::2], r[1::2]))\n    assert unflatten(r, 5) == [tuple(r[:5]), tuple(r[5:])]\n    raises(ValueError, lambda: unflatten(list(range(10)), 3))\n    raises(ValueError, lambda: unflatten(list(range(10)), -2))\n", "def test_reshape_exceptions():\n    raises(ValueError, lambda: reshape([1, 2, 3], [2, -1]))\n    raises(ValueError, lambda: reshape([1, 2, 3], [2, 2]))\n", "def test_interactive_traversal():\n    expr = x + (y + z)*w\n    traversal_output = capture(lambda: interactive_traversal(expr))\n    assert \"Current expression\" in traversal_output\n    assert \"Your choice\" in traversal_output\n\n    expr = Piecewise((x, x < 1), (x**2, True))\n    traversal_output = capture(lambda: interactive_traversal(expr))\n    assert \"Current expression\" in traversal_output\n    assert \"Your choice\" in traversal_output\n\n    # Test with nested expressions\n    expr = x + (y + (z + w)*(x + y))*w\n    traversal_output = capture(lambda: interactive_traversal(expr))\n    assert \"Current expression\" in traversal_output\n    assert \"Your choice\" in traversal_output\n"], "sample_1133": ["def test_critical_angle_exceptions():\n    m1 = Medium('m1', n=1)\n    m2 = Medium('m2', n=1.33)\n    raises(ValueError, lambda: critical_angle(m1, m2))  # n1 < n2, TIR impossible\n    raises(ValueError, lambda: critical_angle(1, 1.33))\n", "def test_transverse_magnification_negative_values():\n    si, so = symbols('si, so')\n    assert transverse_magnification(-30, -15) == -2\n    assert transverse_magnification(-30, 15) == 2\n    assert transverse_magnification(30, -15) == -2\n", "def test_refraction_angle_with_invalid_angle():\n    raises(ValueError, lambda: refraction_angle(-0.1, 1, 1))  # angle < 0\n    raises(ValueError, lambda: refraction_angle(pi / 2, 1, 1))  # angle >= pi/2\n    raises(ValueError, lambda: refraction_angle(0.5, 1, 1, normal=Matrix([0, 0, 1])))  # normal provided\n    raises(ValueError, lambda: refraction_angle(0.5, 1, 1, plane=Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])))  # plane provided\n", "def test_critical_angle_exceptions():\n    m1 = Medium('m1', n=1)\n    m2 = Medium('m2', n=1.33)\n    raises(ValueError, lambda: critical_angle(m1, m2))\n    raises(ValueError, lambda: critical_angle(1, 1.33))\n", "def test_critical_angle_invalid():\n    m1 = Medium('m1', n=1)\n    m2 = Medium('m2', n=1.5)\n    raises(ValueError, lambda: critical_angle(m1, m2))\n    raises(ValueError, lambda: critical_angle(1, 1.5))\n"], "sample_1134": ["def test_latex_printers():\n    from sympy import symbols, Function, Basic\n\n    class CustomClass(Basic):\n            return \"custom_latex\"\n\n    x = symbols('x')\n    f = Function('f')\n\n    # Custom class with _latex method\n    assert latex(CustomClass()) == 'custom_latex'\n\n    # Default Printer\n    class CustomPrinter(LatexPrinter):\n            return \"custom_function(%s)\" % self._print(expr.args[0])\n\n    # Using custom printer for a function\n    assert CustomPrinter().doprint(f(x)) == 'custom_function(x)'\n    \n    # Using default printer for the same function\n    assert latex(f(x)) == r'f{\\left(x \\right)}'\n", "def test_latex_Determinant():\n    from sympy import det, Matrix\n    M = Matrix([[a, b], [c, d]])\n    expr = det(M)\n    assert latex(expr) == r\"\\left|{\\begin{matrix}a & b\\\\c & d\\end{matrix}}\\right|\"\n    assert latex(expr**2) == r\"\\left(\\left|{\\begin{matrix}a & b\\\\c & d\\end{matrix}}\\right|\\right)^{2}\"\n", "def test_latex_Eq():\n    # Coverage test for the Eq class and handling of custom symbol names\n    eq = Eq(x + 1, y**2)\n    assert latex(eq) == \"x + 1 = y^{2}\"\n    eq = Eq(x + 1, y**2, evaluate=False)\n    assert latex(eq) == \"x + 1 = y^{2}\"\n    eq = Eq(x + 1, y**2, evaluate=True)\n    assert latex(eq) == \"x + 1 = y^{2}\"\n    eq = Eq(x**2 + y**2, 1)\n    assert latex(eq) == \"x^{2} + y^{2} = 1\"\n    eq = Eq(x**2 + y**2, 1, evaluate=False)\n    assert latex(eq) == \"x^{2} + y^{2} = 1\"\n    eq = Eq(x**2 + y**2, 1, evaluate=True)\n    assert latex(eq) == \"x^{2} + y^{2} = 1\"\n    eq = Eq(x, y, evaluate=False)\n    assert latex(eq) == \"x = y\"\n    assert latex(Eq(x, y, evaluate=False), symbol_names={x: \"x_i\", y: \"y_j\"}) == \"x_i = y_j\"\n    assert latex(Eq(x + y, 0, evaluate=False), symbol_names={x: \"x_i\", y: \"y_j\"}) == \"x_i + y_j = 0\"\n", "def test_latex_special_functions():\n    from sympy.functions.special.tensor_functions import KroneckerDelta\n    from sympy.functions.special.gamma_functions import gamma, lowergamma\n    from sympy.functions.special.beta_functions import beta\n    from sympy.functions.special.delta_functions import DiracDelta\n    from sympy.functions.special.error_functions import Chi\n\n    assert latex(KroneckerDelta(x, y)) == r'\\delta_{x y}'\n    assert latex(gamma(x)) == r'\\Gamma{\\left(x \\right)}'\n    assert latex(lowergamma(x, y)) == r'\\gamma{\\left(x, y\\right)}'\n    assert latex(beta(x, y)) == r'\\operatorname{B}\\left(x, y\\right)'\n    assert latex(DiracDelta(x)) == r'\\delta{\\left(x\\right)}'\n    assert latex(Chi(x)) == r'\\operatorname{Chi}\\left(x\\right)'\n    assert latex(Chi(x)**2) == r'\\operatorname{Chi}^{2}\\left(x\\right)'\n\n    # Check special functions with exponents\n    assert latex(gamma(x)**2) == r'\\Gamma^{2}{\\left(x \\right)}'\n    assert latex(lowergamma(x, y)**2) == r'\\gamma^{2}{\\left(x, y\\right)}'\n    assert latex(beta(x, y)**2) == r'\\operatorname{B}^{2}\\left(x, y\\right)'\n    assert latex(DiracDelta(x)**2) == r'\\delta^{2}{\\left(x\\right)}'\n    assert latex(KroneckerDelta(x, y)**2) == r'\\left(\\delta_{x y}\\right)^{2}'\n    assert latex(Chi(x + y)**2) == r'\\operatorname{Chi}^{2}\\left(x + y\\right)'\n", "def test_latex_delimiters():\n    assert latex(x + y, mode='plain', mul_symbol='dot') == \"x + y\"\n    assert latex(x + y, mode='inline', mul_symbol='dot') == \"$x + y$\"\n    assert latex(x + y, mode='equation', mul_symbol='dot') == \"\\\\begin{equation}x + y\\\\end{equation}\"\n    assert latex(x + y, mode='equation*', mul_symbol='dot') == \"\\\\begin{equation*}x + y\\\\end{equation*}\"\n    assert latex(x + y, itex=True, mode='inline', mul_symbol='dot') == \"$$x + y$$\"\n    assert latex(x*y, mode='plain', mul_symbol='times') == \"x \\\\times y\"\n    assert latex(x*y, mode='inline', mul_symbol='times') == \"$x \\\\times y$\"\n    assert latex(x*y, mode='equation', mul_symbol='times') == \"\\\\begin{equation}x \\\\times y\\\\end{equation}\"\n    assert latex(x*y, mode='equation*', mul_symbol='times') == \"\\\\begin{equation*}x \\\\times y\\\\end{equation*}\"\n    assert latex(x*y, itex=True, mode='inline', mul_symbol='times') == \"$$x \\\\times y$$\"\n"], "sample_1135": ["def test__unevaluated_Mul():\n    # Verify the behavior of _unevaluated_Mul with various inputs\n    from sympy import sqrt\n    from sympy.abc import a, b, c\n\n    # Test combining numbers\n    assert _unevaluated_Mul(2, 3, 4) == 24\n    assert _unevaluated_Mul(2, 3, sqrt(2)) == 6 * sqrt(2)\n    assert _unevaluated_Mul(2, 3, a) == 6 * a\n\n    # Test with symbolic expressions\n    assert _unevaluated_Mul(a, b, c) == a * b * c\n    assert _unevaluated_Mul(a, 2, b) == 2 * a * b\n    assert _unevaluated_Mul(2, a, b, sqrt(2)) == 2 * a * b * sqrt(2)\n\n    # Test with nested Muls\n    assert _unevaluated_Mul(a * b, c) == a * b * c\n    assert _unevaluated_Mul(2 * a, 3 * b) == 6 * a * b\n\n    # Test with one element being a Mul\n    assert _unevaluated_Mul(Mul(a, b), c) == a * b * c\n    assert _unevaluated_Mul(2, Mul(a, b)) == 2 * a * b\n\n    # Test with non-commutative elements\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    assert _unevaluated_Mul(A, B) == Mul(A, B)\n    assert _unevaluated_Mul(A, b, B) == Mul(A, b, B)\n    assert _unevaluated_Mul(b, B, A) == Mul(b, B, A)\n", "def test_unevaluated_mul():\n    from sympy.abc import a, b, c\n    from sympy import Mul\n\n    expr = _unevaluated_Mul(a, b, c)\n    assert expr == a*b*c\n\n    # Check that numbers are collected and placed in the first slot\n    expr = _unevaluated_Mul(2, 3, a)\n    assert expr == 6*a\n\n    # Check that arguments which are Muls are flattened\n    expr = _unevaluated_Mul(a, Mul(b, c))\n    assert expr == a*b*c\n\n    # Check that args are sorted\n    expr = _unevaluated_Mul(b, a, c)\n    assert expr == a*b*c\n\n    # Check that non-commutative parts are kept separate and not sorted\n    from sympy import symbols\n    A, B = symbols('A B', commutative=False)\n    expr = _unevaluated_Mul(A, B, a, b)\n    assert expr == a*b*A*B\n\n    # Check that numbers are collected and non-commutative parts are sorted\n    expr = _unevaluated_Mul(2, A, B, 3, a, b)\n    assert expr == 6*a*b*A*B\n", "def test_unevaluated_Mul():\n    from sympy.abc import a, b, c\n    from sympy import Mul, sqrt\n\n    # Test combination of numbers and symbols\n    expr = _unevaluated_Mul(3, a, 2)\n    assert expr == 6*a\n    assert expr.args == (6, a)\n    \n    # Test sorting of arguments\n    expr = _unevaluated_Mul(a, b, c)\n    assert expr.args == (a, b, c)\n\n    # Test flattening of Muls\n    expr = _unevaluated_Mul(2, a, Mul(3, b), 4)\n    assert expr.args == (24, a, b)\n    \n    # Test handling of non-commutative arguments\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    expr = _unevaluated_Mul(2, A, 3, B)\n    assert expr.args == (6, Mul(A, B))\n    \n    # Test handling of nested unevaluated Muls\n    expr = _unevaluated_Mul(2, Mul(3, a, evaluate=False), 4)\n    assert expr.args == (24, a)\n    \n    # Test sorting with _args_sortkey\n    expr = _unevaluated_Mul(c, a, b)\n    assert expr.args == (a, b, c)\n    \n    # Test combining sqrt values\n    expr = _unevaluated_Mul(sqrt(2), sqrt(3))\n    assert expr == sqrt(6)\n    assert expr.args == (sqrt(6),)\n    \n    # Test commutative and non-commutative mixing\n    expr = _unevaluated_Mul(3, A, 2, B, a)\n    assert expr.args == (6, a, Mul(A, B))\n", "def test_unevaluated_Mul():\n    from sympy.abc import a, b, c\n\n    # Test multiplication with numbers\n    assert _unevaluated_Mul(S(3), S(4)) == Mul(12)\n    assert _unevaluated_Mul(S(3.0), S(2)) == Mul(6.0)\n\n    # Test multiplication with symbols\n    assert _unevaluated_Mul(a, b) == Mul(a, b)\n    assert _unevaluated_Mul(a, b, c) == Mul(a, b, c)\n    assert _unevaluated_Mul(a, a) == Mul(a**2)\n    assert _unevaluated_Mul(a, a, a) == Mul(a**3)\n\n    # Test multiplication with mixed types\n    assert _unevaluated_Mul(S(3), a) == Mul(3, a)\n    assert _unevaluated_Mul(S(3), a, b) == Mul(3, a, b)\n    assert _unevaluated_Mul(a, S(3), b) == Mul(a, 3, b)\n\n    # Test multiplication with nested Muls\n    assert _unevaluated_Mul(a, Mul(b, c)) == Mul(a, b, c)\n    assert _unevaluated_Mul(Mul(a, b), Mul(b, c)) == Mul(a, b**2, c)\n\n    # Test properties\n    assert _unevaluated_Mul(S(-1), a).args[0] == -1\n\n    # Test commutativity\n    assert _unevaluated_Mul(a, b).args == (a, b)\n    assert _unevaluated_Mul(b, a).args == (a, b)\n\n    # Test with rational numbers\n    assert _unevaluated_Mul(Rational(2, 3), Rational(3, 4)) == Mul(Rational(1, 2))\n    assert _unevaluated_Mul(Rational(2, 3), a) == Mul(Rational(2, 3), a)\n\n    # Test with non-commutative symbols\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    assert _unevaluated_Mul(A, B).args == (A, B)\n    assert _unevaluated_Mul(B, A).args", "def test_unevaluated_Mul():\n    from sympy.abc import a, b, c\n    expr = _unevaluated_Mul(a, b, c)\n    assert isinstance(expr, Mul)\n    assert expr.args == (a, b, c)\n    assert _unevaluated_Mul(3, 2, a, b).args == (6, a, b)\n    assert _unevaluated_Mul(3, 2, Mul(a, b)).args == (6, a, b)\n    assert _unevaluated_Mul(3, 2, Mul(a, 2)).args == (12, a)\n    assert _unevaluated_Mul(3, 2, Mul(3, 2)).args == (36,)\n    assert _unevaluated_Mul(3, 2, Mul(3, c)).args == (18, c)\n    assert _unevaluated_Mul(3, 2, Mul(a, 3), Mul(b, 2)).args == (36, a, b)\n    assert _unevaluated_Mul(3, 2, Mul(a, 3), Mul(b, 2), Mul(c, 2)).args == (72, a, b, c)\n"], "sample_1136": ["def test_ExpressionDomain_operations():\n    ex1 = ExpressionDomain.Expression('x + 1')\n    ex2 = ExpressionDomain.Expression('x - 1')\n\n    # Addition\n    ex_add = ex1 + ex2\n    assert str(ex_add) == 'EX(2*x)'\n\n    # Subtraction\n    ex_sub = ex1 - ex2\n    assert str(ex_sub) == 'EX(2)'\n\n    # Multiplication\n    ex_mul = ex1 * ex2\n    assert str(ex_mul) == 'EX(x**2 - 1)'\n\n    # Division\n    ex_div = ex1 / ex2\n    assert str(ex_div) == 'EX((x + 1)/(x - 1))'\n\n    # Power\n    ex_pow = ex1 ** 2\n    assert str(ex_pow) == 'EX((x + 1)**2)'\n\n    # Unary operations\n    assert str(-ex1) == 'EX(-x - 1)'\n    assert str(abs(ex2)) == 'EX(abs(x - 1))'\n\n    # Simplify\n    assert str(ex1.simplify(ex1.ex)) == 'EX(x + 1)'\n\n    # GCD and LCM\n    ex3 = ExpressionDomain.Expression('x**2 - 1')\n    gcd_expr = ex3.gcd(ex1)\n    lcm_expr = ex3.lcm(ex2)\n    assert str(gcd_expr) == 'EX(x - 1)'\n    assert str(lcm_expr) == 'EX((x - 1)*(x + 1)**2)'\n", "def test_ExpressionDomain_basic_operations():\n    EX = ExpressionDomain()\n    a = EX.from_sympy(x + 1)\n    b = EX.from_sympy(x - 1)\n    c = EX.from_sympy(2)\n    \n    # Test addition\n    assert (a + b).as_expr() == 2 * x\n    assert (a + c).as_expr() == x + 3\n    assert (b + c).as_expr() == x + 1\n    \n    # Test subtraction\n    assert (a - b).as_expr() == 2\n    assert (a - c).as_expr() == x - 1\n    assert (b - c).as_expr() == x - 3\n    \n    # Test multiplication\n    assert (a * b).as_expr() == x**2 - 1\n    assert (a * c).as_expr() == 2 * x + 2\n    assert (b * c).as_expr() == 2 * x - 2\n    \n    # Test division\n    assert (a / b).as_expr() == (x + 1) / (x - 1)\n    assert (a / c).as_expr() == (x + 1) / 2\n    assert (b / c).as_expr() == (x - 1) / 2\n    \n    # Test power\n    assert (a ** 2).as_expr() == (x + 1)**2\n    assert (b ** 2).as_expr() == (x - 1)**2\n    \n    # Test simplification\n    assert EX.dtype(x**2 + 2*x + 1).simplify(x**2 + 2*x + 1).as_expr() == x**2 + 2*x + 1\n    \n    # Test as_expr\n    assert a.as_expr() == x + 1\n    assert b.as_expr() == x - 1\n    assert c.as_expr() == 2\n\n    # Test equality\n    assert (a == EX.from_sympy(x + 1)) is True\n    assert (a == b) is False\n    assert (b == c) is False\n    assert (c == EX.from_sympy(2)) is True\n", "def test_ExpressionDomain_operations():\n    from sympy.polys.domains import EX\n    expr1 = EX.dtype(\"x + y\")\n    expr2 = EX.dtype(\"x - y\")\n\n    # Test addition\n    assert (expr1 + expr2) == EX.dtype(\"2*x\")\n    assert (expr1 + 3) == EX.dtype(\"x + y + 3\")\n    assert (4 + expr2) == EX.dtype(\"x - y + 4\")\n    \n    # Test subtraction\n    assert (expr1 - expr2) == EX.dtype(\"2*y\")\n    assert (expr1 - 3) == EX.dtype(\"x + y - 3\")\n    assert (4 - expr2) == EX.dtype(\"4 - x + y\")\n\n    # Test multiplication\n    assert (expr1 * expr2) == EX.dtype(\"x**2 - y**2\")\n    assert (expr1 * 3) == EX.dtype(\"3*x + 3*y\")\n    assert (4 * expr2) == EX.dtype(\"4*x - 4*y\")\n\n    # Test division\n    assert (expr1 / expr2) == EX.dtype(\"(x + y)/(x - y)\")\n    assert (expr1 / 2) == EX.dtype(\"(x + y)/2\")\n    assert (4 / expr2) == EX.dtype(\"4/(x - y)\")\n\n    # Test power\n    assert (expr1**2) == EX.dtype(\"(x + y)**2\")\n    assert (expr2**3) == EX.dtype(\"(x - y)**3\")\n\n    # Test negation and absolute\n    assert (-expr1) == EX.dtype(\"-x - y\")\n    assert abs(expr2) == EX.dtype(\"Abs(x - y)\")\n\n    # Test GCD and LCM\n    assert expr1.gcd(expr2) == EX.dtype(\"1\")\n    assert expr1.lcm(expr2) == EX.dtype(\"x**2 - y**2\")\n", "def test_ExpressionDomain_operations():\n    ex1 = EX(2*x + 3)\n    ex2 = EX(x - 1)\n    ex3 = EX(3*y + 2)\n\n    assert ex1 + ex2 == EX(3*x + 2)\n    assert ex1 - ex2 == EX(x + 4)\n    assert ex1 * ex2 == EX(2*x**2 + x - 3)\n    assert ex1 / ex2 == EX((2*x + 3)/(x - 1))\n    assert ex1 + 3 == EX(2*x + 6)\n    assert ex1 - 3 == EX(2*x)\n    assert ex1 * 3 == EX(6*x + 9)\n    assert ex1 / 3 == EX(2*x/3 + 1)\n    assert ex1 + ex3 == EX(2*x + 3 + 3*y + 2)\n    assert ex1 - ex3 == EX(2*x + 3 - 3*y - 2)\n    assert ex1 * ex3 == EX((2*x + 3)*(3*y + 2))\n    assert ex1 / ex3 == EX((2*x + 3)/(3*y + 2))\n\n    assert ex1.as_expr() == 2*x + 3\n    assert ex1.numer() == EX(2*x + 3)\n    assert ex1.denom() == EX(1)\n    assert ex1.simplify(2*x + 3) == EX(2*x + 3)\n    assert abs(ex1) == EX(Abs(2*x + 3))\n    assert -ex1 == EX(-2*x - 3)\n    assert ex1 == EX(2*x + 3)\n    assert ex1 != EX(2*x + 4)\n    assert bool(ex1) is True\n    assert bool(EX(0)) is False\n\n    assert ex1.gcd(EX(4*x + 6)) == EX(2*x + 3)\n    assert ex1.lcm(EX(4*x + 6)) == EX(4*x + 6)\n", "def test_ExpressionDomain_operations():\n    EX = ExpressionDomain()\n    a = EX.from_sympy(x + 1)\n    b = EX.from_sympy(x - 1)\n    c = EX.from_sympy(x**2 - 1)\n    \n    assert a + b == EX.from_sympy(2*x)\n    assert a - b == EX.from_sympy(2)\n    assert a * b == EX.from_sympy(x**2 - 1)\n    assert a / b == EX.from_sympy((x + 1)/(x - 1))\n    assert a**2 == EX.from_sympy((x + 1)**2)\n    \n    assert a.numer() == a\n    assert a.denom() == EX.one\n    \n    assert a.gcd(b) == EX.from_sympy(1)\n    assert a.lcm(b) == EX.from_sympy((x + 1)*(x - 1))\n    \n    # Checking simplification\n    assert EX.dtype.simplify(EX.dtype(x**2 - 1), EX.dtype(x**2 - 1).as_expr()) == EX.from_sympy(x**2 - 1)\n\n    # Boolean checks\n    assert EX.is_positive(EX.from_sympy(1))\n    assert not EX.is_positive(EX.from_sympy(-1))\n    assert EX.is_nonnegative(EX.from_sympy(0))\n    assert not EX.is_nonnegative(EX.from_sympy(-1))\n    assert EX.is_nonpositive(EX.from_sympy(0))\n    assert not EX.is_nonpositive(EX.from_sympy(1))\n    assert EX.is_negative(EX.from_sympy(-1))\n    assert not EX.is_negative(EX.from_sympy(1))\n"], "sample_1137": ["def test_quantity_simplify():\n    from sympy.physics.units import kilo\n    from sympy.physics.units.definitions import foot, inch\n    \n    expr = kilo * foot * inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 250 * foot**2 / 3\n\n    expr = foot - 6 * inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == foot / 2\n\n    # Test with more complex expressions\n    expr = (kilo * foot * inch) / (2 * foot)\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 125 * foot / 3\n\n    expr = (kilo * foot**2) / (2 * inch)\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 500 * foot / 3\n", "def test_quantity_simplify():\n    from sympy.physics.units.prefixes import kilo\n    from sympy.physics.units import foot, inch\n\n    # Test conversion of prefixes to numerical values\n    expr = kilo*foot*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 250*foot**2/3\n\n    # Test simplification of units of the same dimension\n    expr = foot - 6*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == foot/2\n\n    # Test with no prefixes or quantities\n    expr = 5\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 5\n\n    # Test with only quantities\n    from sympy.physics.units import meter\n    expr = 2*meter + 3*meter\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 5*meter\n\n    # Test with mixed quantities and prefixes\n    expr = kilo*meter + 500*meter\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 1500*meter\n", "def test_quantity_simplify():\n    from sympy.physics.units import kilo, foot, inch\n    from sympy.physics.units.util import quantity_simplify\n\n    expr = kilo * foot * inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 250 * foot**2 / 3\n\n    expr = foot - 6 * inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == foot / 2\n\n    expr = (kilo * foot * inch) / (foot * inch)\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 1000\n", "def test_quantity_simplify():\n    from sympy.physics.units.prefixes import kilo, milli\n    from sympy.physics.units import foot, inch, meter, gram\n\n    expr = kilo * foot * inch\n    simplified = quantity_simplify(expr)\n    assert simplified == 250 * foot**2 / 3\n\n    expr = foot - 6 * inch\n    simplified = quantity_simplify(expr)\n    assert simplified == foot / 2\n\n    expr = milli * gram\n    simplified = quantity_simplify(expr)\n    assert simplified == gram / 1000\n\n    expr = 2 * kilo * meter + 500 * meter\n    simplified = quantity_simplify(expr)\n    assert simplified == 2500 * meter\n", "def test_quantity_simplify():\n    from sympy.physics.units.util import quantity_simplify\n    from sympy.physics.units import joule, newton, meter, second, kilo\n    from sympy import symbols\n    \n    q = kilo*meter*meter/newton\n    assert quantity_simplify(q) == 1000*meter**2/newton\n\n    x, y = symbols('x y')\n    expr = kilo*x*meter**2 / (y*newton)\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 1000*x*meter**2 / (y*newton)\n"], "sample_1138": ["def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(1 + tan(x)**4) == (sec(x)**2 - 1)**2 + 2*sec(x)**2\n    assert TR22(1 + cot(x)**4) == (csc(x)**2 - 1)**2 + 2*csc(x)**2\n    assert TR22(tan(x)**2 + 1) == sec(x)**2\n    assert TR22(cot(x)**2 + 1) == csc(x)**2\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n    assert TR22(cot(x)**4) == (csc(x)**2 - 1)**2\n    assert TR22(tan(x)**2 + tan(x)**2) == 2*(sec(x)**2 - 1)\n    assert TR22(cot(x)**2 + cot(x)**2) == 2*(csc(x)**2 - 1)\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(1 + tan(x)**4) == (sec(x)**2 - 1)**2 + 2*sec(x)**2\n    assert TR22(1 + cot(x)**4) == (csc(x)**2 - 1)**2 + 2*csc(x)**2\n", "def test_TR4():\n    from sympy.abc import x\n    assert TR4(cos(0)) == 1\n    assert TR4(cos(pi/6)) == sqrt(3)/2\n    assert TR4(cos(pi/4)) == sqrt(2)/2\n    assert TR4(cos(pi/3)) == S.Half\n    assert TR4(cos(pi/2)) == 0\n    assert TR4(sin(0)) == 0\n    assert TR4(sin(pi/6)) == S.Half\n    assert TR4(sin(pi/4)) == sqrt(2)/2\n    assert TR4(sin(pi/3)) == sqrt(3)/2\n    assert TR4(sin(pi/2)) == 1\n    assert TR4(tan(0)) == 0\n    assert TR4(tan(pi/6)) == sqrt(3)/3\n    assert TR4(tan(pi/4)) == 1\n    assert TR4(tan(pi/3)) == sqrt(3)\n    assert TR4(tan(pi/2)) == S.ComplexInfinity\n", "def test_TR4():\n    assert TR4(cos(0)) == 1\n    assert TR4(cos(pi/6)) == sqrt(3)/2\n    assert TR4(cos(pi/4)) == sqrt(2)/2\n    assert TR4(cos(pi/3)) == S.Half\n    assert TR4(cos(pi/2)) == 0\n    assert TR4(sin(0)) == 0\n    assert TR4(sin(pi/6)) == S.Half\n    assert TR4(sin(pi/4)) == sqrt(2)/2\n    assert TR4(sin(pi/3)) == sqrt(3)/2\n    assert TR4(sin(pi/2)) == 1\n    assert TR4(tan(0)) == 0\n    assert TR4(tan(pi/6)) == sqrt(3)/3\n    assert TR4(tan(pi/4)) == 1\n    assert TR4(tan(pi/3)) == sqrt(3)\n    assert TR4(tan(pi/2)) == S.ComplexInfinity\n"], "sample_1139": ["def test_ComplexRegion_repr():\n    r = Interval(0, 1)\n    theta = Interval(0, 2*S.Pi)\n    cr_polar = ComplexRegion(r*theta, polar=True)\n    cr_rect = ComplexRegion(r*r)\n    assert repr(cr_polar) == \"PolarComplexRegion(ProductSet(Interval(0, 1), Interval.Ropen(0, 2*pi)))\"\n    assert repr(cr_rect) == \"CartesianComplexRegion(ProductSet(Interval(0, 1), Interval(0, 1)))\"\n", "def test_FiniteSet_Rationals():\n    F = FiniteSet(S.Half, 1, -1, S(3)/2)\n    assert all(f in S.Rationals for f in F)  # all elements should be in Rationals\n    assert 0.5 not in S.Rationals\n    assert F.intersect(S.Rationals) == F\n    assert F.union(S.Rationals) == S.Rationals\n", "def test_rationals():\n    R = S.Rationals\n    assert 1/3 in R\n    assert -1/3 in R\n    assert 3.14 not in R\n    assert Rational(22, 7) in R\n    assert 22/7 not in R\n    assert 0 in R\n    assert S.One in R\n    assert S.NegativeOne in R\n    assert R._contains(S.Half) == True\n    assert R._contains(2.5) == False\n    assert R._contains(x) == x.is_rational\n\n    ri = iter(R)\n    a, b, c, d, e, f = next(ri), next(ri), next(ri), next(ri), next(ri), next(ri)\n    assert (a, b, c, d, e, f) == (0, 1, -1, S.Half, 2, Rational(-1, 2))\n\n    assert R.boundary == S.Reals\n    assert R.is_open == False\n    assert R.is_closed == False\n", "def test_Rationals_boundary_closure():\n    assert S.Rationals.boundary == S.Reals\n    assert S.Rationals.closure == S.Reals\n    assert S.Rationals.is_open == False\n    assert S.Rationals.is_closed == False\n", "def test_Rationals_contains():\n    R = S.Rationals\n    assert 1 in R\n    assert 0 in R\n    assert -1 in R\n    assert S.Half in R\n    assert Rational(-1, 2) in R\n    assert 1.5 not in R\n    assert pi not in R\n    assert sqrt(2) not in R\n    assert 1 + I not in R\n    assert 2 + 3 * I not in R\n    assert S.Reals.is_subset(R) is False\n    assert R.contains(S(3.5)) == Contains(S(3.5), R, evaluate=False)\n"], "sample_1140": ["def test_pretty_IdentityMorphism():\n    from sympy.categories import Object, IdentityMorphism, Category\n\n    A = Object(\"A\")\n    id_A = IdentityMorphism(A)\n    K = Category(\"K\")\n\n    assert pretty(id_A) == \"id:A-->A\"\n    assert upretty(id_A) == \"id:A\u2014\u2014\u25b6A\"\n\n    # Ensure that the identity morphism behaves correctly within a category\n    assert K.identity_morphism(A) == id_A\n    assert pretty(K.identity_morphism(A)) == \"id:A-->A\"\n    assert upretty(K.identity_morphism(A)) == \"id:A\u2014\u2014\u25b6A\"\n", "def test_pretty_Mul_unevaluated():\n    expr = Mul(2, 3, evaluate=False)\n    ascii_str = \"2*3\"\n    ucode_str = \"2\u22c53\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(2, -3, evaluate=False)\n    ascii_str = \"2*(-3)\"\n    ucode_str = \"2\u22c5(-3)\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(-2, -3, evaluate=False)\n    ascii_str = \"(-2)*(-3)\"\n    ucode_str = \"(-2)\u22c5(-3)\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(2, 3, 4, evaluate=False)\n    ascii_str = \"2*3*4\"\n    ucode_str = \"2\u22c53\u22c54\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(2, 3, -4, evaluate=False)\n    ascii_str = \"2*3*(-4)\"\n    ucode_str = \"2\u22c53\u22c5(-4)\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(2, -3, -4, evaluate=False)\n    ascii_str = \"2*(-3)*(-4)\"\n    ucode_str = \"2\u22c5(-3)\u22c5(-4)\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(-2, -3, -4, evaluate=False)\n    ascii_str = \"(-2)*(-3)*(-4)\"\n    ucode_str = \"(-2)\u22c5(-3)\u22c5(-4)\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(2, Rational(1, 3), evaluate=False)\n    ascii_str = \"2*1/3\"\n    ucode_str = \"2\u22c51/3\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(2,", "def test_pretty_Mul_unevaluated():\n    expr = Mul(2, x + 1, evaluate=False)\n    ascii_str = \"2*(x + 1)\"\n    ucode_str = \"2\u22c5(x + 1)\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(3, x - y, evaluate=False)\n    ascii_str = \"3*(x - y)\"\n    ucode_str = \"3\u22c5(x - y)\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(-4, x + y, evaluate=False)\n    ascii_str = \"-4*(x + y)\"\n    ucode_str = \"-4\u22c5(x + y)\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(5, 1/(x + y), evaluate=False)\n    ascii_str = \"5/(x + y)\"\n    ucode_str = \"5/(x + y)\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mul(1/(x + y), z, evaluate=False)\n    ascii_str = \"z/(x + y)\"\n    ucode_str = \"z/(x + y)\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n", "def test_pretty_PrimitiveRoot():\n    from sympy.ntheory import primitive_root\n    assert pretty(primitive_root(7)) == '3'\n    assert upretty(primitive_root(7)) == '3'\n    assert pretty(primitive_root(13)) == '2'\n    assert upretty(primitive_root(13)) == '2'\n", "def test_prettyprinter_imaginary_unit():\n    # Test the PrettyPrinter instantiation with different imaginary_unit settings\n    expr = PrettyPrinter({\"imaginary_unit\": \"i\"})\n    assert expr._settings['imaginary_unit'] == \"i\"\n\n    expr = PrettyPrinter({\"imaginary_unit\": \"j\"})\n    assert expr._settings['imaginary_unit'] == \"j\"\n\n    raises(TypeError, lambda: PrettyPrinter({\"imaginary_unit\": 1}))\n    raises(ValueError, lambda: PrettyPrinter({\"imaginary_unit\": \"k\"}))\n"], "sample_1141": ["def test_expr_sort_key():\n    e1 = Expr()\n    assert e1.sort_key() == (e1.class_key(), (0, ()), (S.One.sort_key(),), S.One)\n    \n    class MyExpr(Expr):\n            self._args = args\n    \n            return self._args\n    \n    e2 = MyExpr(1, 2, 3)\n    assert e2.sort_key() == (e2.class_key(), (3, ((1,), (2,), (3,))), (S.One.sort_key(),), S.One)\n", "def test_expr_subs():\n    expr = x + 2*x - 3*x + 4\n    assert expr.subs(x, 1) == 4\n    assert expr.subs(x, 2) == 4\n    assert expr.subs(x, 0) == 4\n\n    expr = x * y + y * z\n    assert expr.subs(y, 1) == x + z\n    assert expr.subs(z, 2) == x*y + 2*y\n    assert expr.subs(x, 3) == 3*y + y*z\n\n    expr = sin(x) + cos(x)\n    assert expr.subs(x, 0) == 1\n    assert expr.subs(x, pi/2) == 0\n    assert expr.subs(x, pi) == -1\n\n    expr = Expr()\n    assert expr.subs(x, 1) == expr\n    assert expr.subs(x, 2) == expr\n", "def test_as_leading_term():\n    expr = (x + 1)**2 * (x**2 + x + 1)\n    leading_term = expr.as_leading_term(x)\n    assert leading_term == x**4\n    assert expr.as_leading_term(y) == expr  # y is not in expr, so it should return expr itself\n\n    expr = 1/x + x\n    leading_term = expr.as_leading_term(x)\n    assert leading_term == 1/x\n\n    expr = exp(x)\n    leading_term = expr.as_leading_term(x)\n    assert leading_term == 1\n", "def test_as_coeff_mul():\n    x, y, z = symbols('x y z')\n    expr = x*y*z\n    assert expr.as_coeff_Mul() == (S.One, x*y*z)\n    assert (2*expr).as_coeff_Mul() == (2, x*y*z)\n    assert (x**2*expr).as_coeff_Mul() == (S.One, x**2*y*z)\n    assert (2*x**2*expr).as_coeff_Mul() == (2, x**2*y*z)\n    expr = 2*x**2*y*z\n    assert expr.as_coeff_Mul() == (2, x**2*y*z)\n", "def test_expr_diff_wrt():\n    class MyScalar(Expr):\n        _diff_wrt = True\n\n    class MySymbol(Expr):\n        _diff_wrt = True\n        is_scalar = False\n\n    e = Expr()\n    assert e._diff_wrt == False\n\n    s = MyScalar()\n    assert s._diff_wrt == True\n    assert s.diff(s) == S.One\n\n    sym = MySymbol()\n    assert sym._diff_wrt == True\n    assert sym.diff(sym) == sym.diff(sym)\n"], "sample_1142": ["def test_invalid_indexing():\n    A = MatrixSymbol('A', 3, 3)\n    raises(IndexError, lambda: A[3, 3])  # Out of bounds\n    raises(IndexError, lambda: A[-1, 0])  # Negative index\n    raises(IndexError, lambda: A[0, 3])  # Out of bounds column\n    raises(IndexError, lambda: A[0, -1])  # Negative index\n    raises(IndexError, lambda: A['a', 1])  # Invalid type\n    raises(IndexError, lambda: A[1, 'b'])  # Invalid type\n    raises(IndexError, lambda: A[1, 1, 1])  # Too many indices\n    raises(IndexError, lambda: A[:3])  # Invalid slice\n    raises(IndexError, lambda: A[:, 3])  # Out of bounds slice\n    raises(IndexError, lambda: A[3, :])  # Out of bounds slice\n", "def test_matrixexpr_methods():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n    expr = A + B * C\n\n    # Test as_real_imag method\n    real, imag = expr.as_real_imag()\n    assert real == S.Half * (expr + expr._eval_conjugate())\n    assert imag == (expr - expr._eval_conjugate()) / (2 * I)\n\n    # Test adjoint method\n    adj = expr.adjoint()\n    assert adj == adjoint(expr)\n\n    # Test conjugate method\n    conj = expr.conjugate()\n    assert conj == conjugate(expr)\n\n    # Test transpose method\n    trans = expr.transpose()\n    assert trans == expr.T\n    assert trans == expr._eval_transpose()\n\n    # Test canonicalize method\n    canon = expr.canonicalize()\n    assert canon == expr\n\n    # Test valid_index method\n    assert expr.valid_index(0, 0) == True\n    assert expr.valid_index(3, 3) == False  # out of range\n\n    # Test _entry method\n    raises(NotImplementedError, lambda: expr._entry(0, 0))\n\n    # Test inverse method on non-square matrix\n    non_square_expr = MatrixSymbol('D', 2, 3)\n    raises(NonSquareMatrixError, lambda: non_square_expr.inverse())\n", "def test_matrixexpr_properties():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    M = MatrixSymbol('M', 3, 3)\n    \n    # Test is_square property\n    assert A.is_square\n    assert B.is_square\n    assert M.is_square\n    \n    # Test as_real_imag method\n    real, im = A.as_real_imag()\n    assert real == S.Half * (A + adjoint(transpose(A)))\n    assert im == (A - adjoint(transpose(A))) / (2 * S.ImaginaryUnit)\n    \n    # Test valid_index method\n    assert A.valid_index(0, 0) is True\n    assert A.valid_index(2, 2) is True\n    assert A.valid_index(3, 3) is False  # out of bounds\n    assert A.valid_index(-1, -1) is False  # negative index\n    \n    # Test _sympifyit decorator\n    assert A + 1 == NotImplemented\n    assert 1 + A == NotImplemented\n    assert A - 1 == NotImplemented\n    assert 1 - A == NotImplemented\n    assert A * 1 == MatMul(A, 1).doit()\n    assert 1 * A == MatMul(1, A).doit()\n    assert A ** 1 == MatPow(A, 1).doit()\n    assert 1 ** A == NotImplemented\n\n    # Test as_mutable method\n    assert A.as_mutable() == A.as_explicit().as_mutable()\n    \n    # Test as_explicit method with valid matrix\n    M = MatrixSymbol('M', 2, 2)\n    M_explicit = M.as_explicit()\n    assert M_explicit == ImmutableMatrix([[M[0, 0], M[0, 1]], [M[1, 0], M[1, 1]]])\n    \n    # Test as_explicit method with invalid (symbolic) matrix\n    A = MatrixSymbol('A', n, m)\n    raises(ValueError, lambda: A.as_explicit())\n", "def test_applyfunc():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert A.applyfunc(sin).shape == A.shape\n    assert A.applyfunc(sin) != B.applyfunc(sin)\n    assert A.applyfunc(sin)[0, 1] == sin(A[0, 1])\n    assert A.applyfunc(lambda x: x**2)[1, 0] == (A[1, 0]**2)\n    \n    # Testing with a non-lambda function\n        return x**2\n    \n    assert A.applyfunc(square)[1, 1] == (A[1, 1]**2)\n    raises(NotImplementedError, lambda: A.applyfunc(None))\n", "def test_MatrixExpr_equality():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', n, m)\n    assert (A + B).equals(A + B) == True\n    assert (A + B).equals(A + C) == False\n    assert (A * B).equals(A * B) == True\n    assert (A * B).equals(B * A) == False\n    assert (A * Identity(m)).equals(A) == True\n    assert (Identity(n) * A).equals(A) == True\n    assert (A * ZeroMatrix(m, l)).equals(ZeroMatrix(n, l)) == True\n    assert (ZeroMatrix(n, m) * A).equals(ZeroMatrix(n, m)) == True\n"], "sample_1143": ["def test_mpf_norm_special_cases():\n    # Check that mpf_norm handles special cases correctly\n    from mpmath.libmp.libmpf import fnan, finf, fninf, fzero\n    \n    # Zero mantissa and zero exponent should result in fzero\n    assert mpf_norm((0, 0, 0, 0), 53) == fzero\n\n    # Mantissa zero but exponent non-zero should result in original mpf tuple\n    assert mpf_norm((1, 0, 1, 0), 53) == (1, 0, 1, 0)\n\n    # Special cases for +inf, -inf and nan\n    assert mpf_norm(finf, 53) == finf\n    assert mpf_norm(fninf, 53) == fninf\n    assert mpf_norm(fnan, 53) == fnan\n    \n    # Check that Float._new handles these correctly\n    assert Float._new((0, 0, 0, 0), 53) == S.Zero\n    assert Float._new(finf, 53) == S.Infinity\n    assert Float._new(fninf, 53) == S.NegativeInfinity\n    assert Float._new(fnan, 53) == S.NaN\n", "def test_mpf_norm_nonzero_cases():\n    assert mpf_norm((0, 1, 1, 1), 53) == (0, 1, 1, 1)\n    assert mpf_norm((1, 1, 1, 1), 53) == (1, 1, 1, 1)\n    assert mpf_norm((0, 0, 1, 1), 53) == (0, 0, 1, 1)\n    assert mpf_norm((1, 0, 1, 1), 53) == (1, 0, 1, 1)\n", "def test_issue_9999():\n    assert S(2).invert(Rational(5, 2)) == S.Half\n    assert S(2).invert(5.0) == 0.5\n    assert S(0.5).invert(4) == 2\n    assert S(3).invert(7) == 5\n    assert S(6).invert(11) == 2\n    assert S(1).invert(3.0) == 1.0/3\n    assert S(-3).invert(7) == 5\n    assert S(-2).invert(9) == 5\n    assert mod_inverse(3, 7) == 5\n    assert mod_inverse(4, 9) == 7\n    assert mod_inverse(6, 11) == 2\n    raises(ValueError, lambda: mod_inverse(2, 4))  # Inverse does not exist\n", "def test_fuzzy_not():\n    assert fuzzy_not(True) is False\n    assert fuzzy_not(False) is True\n    assert fuzzy_not(None) is None\n    assert fuzzy_not(S(1)) is False\n    assert fuzzy_not(S(0)) is True\n    assert fuzzy_not(S(2)) is False\n    assert fuzzy_not(S(-1)) is False\n    assert fuzzy_not(S.Half) is False\n    assert fuzzy_not(S.NaN) is None\n    assert fuzzy_not(oo) is False\n    assert fuzzy_not(-oo) is False\n    assert fuzzy_not(zoo) is None\n", "def test_numbers_inequality():\n    assert Rational(1, 3) < Rational(2, 3)\n    assert Rational(2, 3) > Rational(1, 3)\n    assert Rational(1, 3) <= Rational(1, 3)\n    assert Rational(2, 3) >= Rational(1, 3)\n    assert Float(0.1) < Float(0.2)\n    assert Float(0.2) > Float(0.1)\n    assert Float(0.1) <= Float(0.1)\n    assert Float(0.2) >= Float(0.1)\n    assert Float(0.1) < Rational(2, 3)\n    assert Rational(2, 3) > Float(0.1)\n    assert Float(0.1) <= Rational(1, 10)\n    assert Rational(1, 10) >= Float(0.1)\n"], "sample_1144": ["def test_split_super_sub_edge_cases():\n    assert split_super_sub(\"_a_b\") == (\"\", [], [\"a\", \"b\"])\n    assert split_super_sub(\"x__^\") == (\"x\", [\"\"], [])\n    assert split_super_sub(\"x^^\") == (\"x\", [\"\"], [])\n    assert split_super_sub(\"x__\") == (\"x\", [\"\"], [])\n    assert split_super_sub(\"^a\") == (\"\", [\"a\"], [])\n    assert split_super_sub(\"_^a\") == (\"\", [\"a\"], [\"\"])\n    assert split_super_sub(\"__a\") == (\"\", [\"a\"], [])\n    assert split_super_sub(\"a__1__\") == (\"a\", [\"\"], [\"1\"])\n", "def test_split_super_sub_edge_cases():\n    assert split_super_sub(\"a__1\") == (\"a\", [\"1\"], [])\n    assert split_super_sub(\"a^_1\") == (\"a\", [\"\"], [\"1\"])\n    assert split_super_sub(\"a__^1\") == (\"a\", [\"^1\"], [])\n    assert split_super_sub(\"a_^_1^\") == (\"a\", [\"\"], [\"\", \"1\"])\n    assert split_super_sub(\"^a_b_c\") == (\"\", [\"a\"], [\"b\", \"c\"])\n    assert split_super_sub(\"__a_b_c\") == (\"\", [\"a\"], [\"b\", \"c\"])\n    assert split_super_sub(\"a___b\") == (\"a\", [\"_b\"], [])\n    assert split_super_sub(\"a_b__c\") == (\"a\", [\"c\"], [\"b\"])\n    assert split_super_sub(\"a_b___c\") == (\"a\", [\"_c\"], [\"b\"])\n", "def test_split_super_sub_edge_cases():\n    # Test with no superscripts or subscripts\n    assert split_super_sub(\"simple\") == (\"simple\", [], [])\n\n    # Test with complex combination of superscripts and subscripts\n    assert split_super_sub(\"x_1^a_b__c^d__e\") == (\"x\", [\"a\", \"d\", \"e\"], [\"1\", \"b\", \"c\"])\n\n    # Test with leading and trailing underscores and carets\n    assert split_super_sub(\"_x^_^_^_\") == (\"\", [\"_\", \"_\"], [\"x\", \"_\"])\n\n    # Test with symbols containing digits and underscores\n    assert split_super_sub(\"var_1__sup2_3^4\") == (\"var\", [\"sup2\", \"4\"], [\"1\", \"3\"])\n\n    # Test with only digits\n    assert split_super_sub(\"123_45^6\") == (\"\", [\"6\"], [\"123\", \"45\"])\n\n    # Test with mixture of Unicode characters\n    assert split_super_sub(\"\u03b1_\u03b2^\u03b3__\u03b4_\u03b5\") == (\"\u03b1\", [\"\u03b3\", \"\u03b4\"], [\"\u03b2\", \"\u03b5\"])\n", "def test_split_super_sub_with_mixed_content():\n    assert split_super_sub(\"x_1_2^a_b\") == (\"x\", [\"a\"], [\"1\", \"2\", \"b\"])\n    assert split_super_sub(\"a1_b2__c^d\") == (\"a\", [\"c\", \"d\"], [\"1\", \"b2\"])\n    assert split_super_sub(\"var_1^2__3__4_5\") == (\"var\", [\"2\", \"3\", \"4\"], [\"1\", \"5\"])\n    assert split_super_sub(\"x_a^b__c_d^e_f\") == (\"x\", [\"b\", \"c\", \"e\"], [\"a\", \"d\", \"f\"])\n    assert split_super_sub(\"variable_10^20__30_40^50\") == (\"variable\", [\"20\", \"30\", \"50\"], [\"10\", \"40\"])\n", "def test_split_super_sub_edge_cases():\n    # Single character names with superscripts and subscripts\n    assert split_super_sub(\"a^1_b\") == (\"a\", [\"1\"], [\"b\"])\n    assert split_super_sub(\"a__1_b\") == (\"a\", [\"1\"], [\"b\"])\n    assert split_super_sub(\"a^1^2_b_b\") == (\"a\", [\"1\", \"2\"], [\"b\", \"b\"])\n    assert split_super_sub(\"a__1__2_b_b\") == (\"a\", [\"1\", \"2\"], [\"b\", \"b\"])\n\n    # Special characters in name\n    assert split_super_sub(\"alpha_\u03b2^\u03b3\") == (\"alpha\", [\"\u03b3\"], [\"\u03b2\"])\n    assert split_super_sub(\"alpha_\u03b2__\u03b3\") == (\"alpha\", [\"\u03b3\"], [\"\u03b2\"])\n\n    # Digits in the name and subscript/superscript\n    assert split_super_sub(\"var_123__456\") == (\"var\", [\"456\"], [\"123\"])\n    assert split_super_sub(\"var_123^456\") == (\"var\", [\"456\"], [\"123\"])\n\n    # Edge cases with mixed characters\n    assert split_super_sub(\"var_abc123^def456\") == (\"var\", [\"def456\"], [\"abc123\"])\n    assert split_super_sub(\"var_abc123__def456\") == (\"var\", [\"def456\"], [\"abc123\"])\n\n    # Empty and special cases\n    assert split_super_sub(\"_\") == (\"\", [], [])\n    assert split_super_sub(\"^\") == (\"\", [], [])\n    assert split_super_sub(\"__\") == (\"\", [], [])\n    assert split_super_sub(\"^^\") == (\"\", [], [])\n"], "sample_1145": ["def test_refine_multivariable_abs():\n    a = Symbol('a', real=True)\n    b = Symbol('b', real=True)\n    c = Symbol('c', real=True)\n    assert refine(Abs(a*b*c), Q.positive(a) & Q.negative(b)) == a * Abs(b) * c\n    assert refine(Abs(a*b*c), Q.positive(a) & Q.negative(b) & Q.negative(c)) == a * Abs(b) * Abs(c)\n    assert refine(Abs(a*b*c), Q.negative(a) & Q.positive(b) & Q.positive(c)) == Abs(a) * b * c\n    assert refine(Abs(a*b*c), Q.negative(a) & Q.negative(b) & Q.positive(c)) == Abs(a) * Abs(b) * c\n    assert refine(Abs(a*b*c), Q.negative(a) & Q.negative(b) & Q.negative(c)) == Abs(a) * Abs(b) * Abs(c)\n", "def test_refine_abs_mul():\n    assert refine(Abs(x * y), Q.real(x) & Q.positive(x)) == x * Abs(y)\n    assert refine(Abs(x * y), Q.real(x) & Q.negative(x)) == -x * Abs(y)\n    assert refine(Abs(x * y * z), Q.real(x) & Q.positive(x) & Q.negative(z)) == x * Abs(y) * -z\n    assert refine(Abs(x * y * z), Q.real(x) & Q.negative(x) & Q.positive(z)) == -x * Abs(y) * z\n    assert refine(Abs(x * y * z), Q.real(x) & Q.positive(x) & Q.positive(z)) == x * Abs(y) * z\n", "def test_refine_integer():\n    # Test refining expressions with integer assumptions\n    n = Symbol('n', integer=True)\n    assert refine(n > 0, Q.positive(n)) is True\n    assert refine(n > 0, ~Q.positive(n)) is not True\n    assert refine(n < 0, Q.negative(n)) is True\n    assert refine(n < 0, ~Q.negative(n)) is not True\n    assert refine(n >= 0, Q.nonnegative(n)) is True\n    assert refine(n >= 0, ~Q.nonnegative(n)) is not True\n    assert refine(n <= 0, Q.nonpositive(n)) is True\n    assert refine(n <= 0, ~Q.nonpositive(n)) is not True\n", "def test_refine_abs_mul():\n    assert refine(Abs(x * y), Q.real(x) & Q.positive(x)) == x * Abs(y)\n    assert refine(Abs(x * y), Q.real(x) & Q.negative(x)) == -x * Abs(y)\n    assert refine(Abs(x * y), Q.real(x) & Q.nonnegative(x)) == x * Abs(y)\n    assert refine(Abs(x * y * z), Q.real(x) & Q.positive(x)) == x * Abs(y * z)\n    assert refine(Abs(x * y * z), Q.real(x) & Q.negative(x)) == -x * Abs(y * z)\n    assert refine(Abs(x * y * z), Q.real(x) & Q.nonnegative(x)) == x * Abs(y * z)\n", "def test_multiple_assumptions():\n    assert refine(Abs(x), Q.positive(x) & Q.real(x)) == x\n    assert refine(Abs(x), Q.negative(x) & Q.real(x)) == -x\n    assert refine(Abs(x), Q.positive(x) & Q.negative(y)) == x\n    assert refine(Abs(x), Q.positive(y) & Q.negative(x)) == -x\n    \n    assert refine(sqrt(x**2), Q.positive(x) & Q.real(x)) == x\n    assert refine(sqrt(x**2), Q.positive(x) & Q.positive(y)) == x\n    assert refine(sqrt(x**2), Q.negative(x) & Q.real(x)) == Abs(x)\n    assert refine(sqrt(x**2), Q.negative(x) & Q.negative(y)) == Abs(x)\n    \n    assert refine(Abs(x*y), Q.positive(x) & Q.positive(y)) == x*y\n    assert refine(Abs(x*y), Q.positive(x) & Q.negative(y)) == -x*y\n    assert refine(Abs(x*y), Q.negative(x) & Q.positive(y)) == -x*y\n    assert refine(Abs(x*y), Q.negative(x) & Q.negative(y)) == x*y\n    \n    assert refine((-1)**(x + y), Q.even(x) & Q.odd(y)) == -1\n    assert refine((-1)**(x + y), Q.odd(x) & Q.even(y)) == -1\n    assert refine((-1)**(x + y), Q.even(x) & Q.even(y)) == 1\n    assert refine((-1)**(x + y), Q.odd(x) & Q.odd(y)) == 1\n"], "sample_1146": ["def test_issue_12345():\n    # Test case for latex representation of functions with complex and nested arguments.\n    x_star = Symbol('x^*')\n    f = Function('f')\n    g = Function('g')\n    h = Function('h')\n    assert latex(f(x_star, g(x, h(x**2 + 1)))) == r\"f{\\left(x^{*}, g{\\left(x, h{\\left(x^{2} + 1 \\right)} \\right)} \\right)}\"\n    assert latex(Derivative(f(x_star, g(x, h(x**2 + 1))), x)) == r\"\\frac{\\partial}{\\partial x} f{\\left(x^{*}, g{\\left(x, h{\\left(x^{2} + 1 \\right)} \\right)} \\right)}\"\n    assert latex(Derivative(f(x_star, g(x, h(x**2 + 1))), x, 2)) == r\"\\frac{\\partial^{2}}{\\partial x^{2}} f{\\left(x^{*}, g{\\left(x, h{\\left(x^{2} + 1 \\right)} \\right)} \\right)}\"\n", "def test_latex_Quaternion_operations():\n    q1 = Quaternion(x, y, z, t)\n    q2 = Quaternion(a, b, c, d)\n    \n    assert latex(q1 + q2) == r\"x + y i + z j + t k + a + b i + c j + d k\"\n    assert latex(q1 - q2) == r\"x + y i + z j + t k - a - b i - c j - d k\"\n    assert latex(q1 * q2) == r\"\\left(x + y i + z j + t k\\right) \\left(a + b i + c j + d k\\right)\"\n    assert latex(q1 / q2) == r\"\\frac{x + y i + z j + t k}{a + b i + c j + d k}\"\n    assert latex(q1.conjugate()) == r\"x - y i - z j - t k\"\n", "def test_latex_printer_expression_modes():\n    expr = x**2 + y**2\n\n    # Test 'plain' mode\n    assert latex(expr, mode='plain') == 'x^{2} + y^{2}'\n\n    # Test 'inline' mode\n    assert latex(expr, mode='inline') == '$x^{2} + y^{2}$'\n\n    # Test 'equation' mode\n    assert latex(expr, mode='equation') == r'\\begin{equation}x^{2} + y^{2}\\end{equation}'\n\n    # Test 'equation*' mode\n    assert latex(expr, mode='equation*') == r'\\begin{equation*}x^{2} + y^{2}\\end{equation*}'\n\n    # Test unknown mode, should raise ValueError\n    raises(ValueError, lambda: latex(expr, mode='unknown_mode'))\n", "def test_latex_Subs_invalid():\n    # Test Subs with invalid input types\n    raises(TypeError, lambda: latex(Subs(x*y, (x, y), (1, \"invalid\"))))\n    raises(ValueError, lambda: latex(Subs(x*y, (x,), (1,))))\n    raises(ValueError, lambda: latex(Subs(x*y, (), (1, 2))))\n", "def test_latex_symbol_names():\n    theta, phi = symbols('theta phi')\n    expr = sin(theta) + cos(phi)\n    assert latex(expr, symbol_names={theta: r'\\vartheta', phi: r'\\varphi'}) == r'\\sin{\\left(\\vartheta \\right)} + \\cos{\\left(\\varphi \\right)}'\n    expr = exp(theta) + exp(phi)\n    assert latex(expr, symbol_names={theta: r'\\vartheta', phi: r'\\varphi'}) == r'e^{\\vartheta} + e^{\\varphi}'\n    expr = theta * phi\n    assert latex(expr, symbol_names={theta: r'\\vartheta', phi: r'\\varphi'}) == r'\\vartheta \\varphi'\n    expr = theta**2 + phi**2\n    assert latex(expr, symbol_names={theta: r'\\vartheta', phi: r'\\varphi'}) == r'\\vartheta^{2} + \\varphi^{2}'\n    expr = Function('f')(theta, phi)\n    assert latex(expr, symbol_names={theta: r'\\vartheta', phi: r'\\varphi'}) == r'f{\\left(\\vartheta, \\varphi \\right)}'\n"], "sample_1147": ["def test_latex_nested_functions():\n    f = Function('f')\n    g = Function('g')\n    h = Function('h')\n\n    expr = f(g(h(x)))\n    assert latex(expr) == r'f{\\left(g{\\left(h{\\left(x \\right)} \\right)} \\right)}'\n    assert latex(Derivative(f(g(h(x))), x)) == r\"\\frac{d}{d x} f{\\left(g{\\left(h{\\left(x \\right)} \\right)} \\right)}\"\n    assert latex(Integral(f(g(h(x))), (x, 0, 1))) == r\"\\int\\limits_{0}^{1} f{\\left(g{\\left(h{\\left(x \\right)} \\right)} \\right)}\\, dx\"\n", "def test_latex_imaginary_unit_settings():\n    assert latex(I, imaginary_unit='j') == r'j'\n    assert latex(1 + I, imaginary_unit='j') == r'1 + j'\n    assert latex(1 + I, imaginary_unit='i') == r'1 + i'\n    assert latex(1 + I, imaginary_unit='ri') == r'1 + \\mathrm{i}'\n    assert latex(1 + I, imaginary_unit='ti') == r'1 + \\text{i}'\n    assert latex(I**2, imaginary_unit='j') == r'j^{2}'\n    assert latex(I**2, imaginary_unit='i') == r'i^{2}'\n    assert latex(I**2, imaginary_unit='ri') == r'\\mathrm{i}^{2}'\n    assert latex(I**2, imaginary_unit='ti') == r'\\text{i}^{2}'\n", "def test_multiline_latex_with_functions():\n    from sympy import factorial, gamma\n    expr = factorial(x) + gamma(y) - exp(z)\n    expected = r'\\begin{align*}' + '\\n'\\\n               r'f = &\\ \\Gamma{\\left(y \\right)} + \\operatorname{factorial}\\left(x\\right) \\\\'+ '\\n'\\\n               r'& - e^{z} ' + '\\n'\\\n               r'\\end{align*}'\n\n    assert multiline_latex(f, expr, 2) == expected\n", "def test_issue_17544():\n    a = Symbol('a')\n    b = Symbol('b')\n    p1 = Piecewise((a, a > b), (a + b, True))\n    assert latex(p1) == r'\\begin{cases} a & \\text{for}\\: a > b \\\\ a + b & \\text{otherwise} \\end{cases}'\n    p2 = Piecewise((a, a > b), (a**2, a > 2*b), (a + b, True))\n    assert latex(p2) == r'\\begin{cases} a & \\text{for}\\: a > b \\\\ a^{2} & \\text{for}\\: a > 2 b \\\\ a + b & \\text{otherwise} \\end{cases}'\n", "def test_latex_Basic_with_custom_latex():\n    class MyBasic(Basic):\n            return r\"\\text{MyBasic}\"\n\n    expr = MyBasic()\n    assert latex(expr) == r\"\\text{MyBasic}\"\n    assert latex(expr + x) == r\"\\text{MyBasic} + x\"\n    assert latex(x * MyBasic()) == r\"x \\text{MyBasic}\"\n    assert latex(MyBasic()**2) == r\"\\left(\\text{MyBasic}\\right)^{2}\"\n"], "sample_1148": ["def test_matrixexpr_basic_operations():\n    a = MatrixSymbol('a', 2, 2)\n    b = MatrixSymbol('b', 2, 2)\n    c = MatrixSymbol('c', 2, 2)\n    d = MatrixSymbol('d', 2, 2)\n\n    # Test addition\n    expr = a + b\n    assert isinstance(expr, MatAdd)\n    assert expr.shape == (2, 2)\n\n    # Test multiplication\n    expr = a * b\n    assert isinstance(expr, MatMul)\n    assert expr.shape == (2, 2)\n\n    # Test subtraction\n    expr = a - b\n    assert isinstance(expr, MatAdd)\n    assert expr.shape == (2, 2)\n\n    # Test negation\n    expr = -a\n    assert isinstance(expr, MatMul)\n    assert expr.shape == (2, 2)\n\n    # Test scalar multiplication\n    expr = 3 * a\n    assert isinstance(expr, MatMul)\n    assert expr.shape == (2, 2)\n\n    # Test scalar division\n    expr = a / 2\n    assert isinstance(expr, MatMul)\n    assert expr.shape == (2, 2)\n\n    # Test invalid operations\n    raises(ShapeError, lambda: a + c)\n    raises(ShapeError, lambda: a * c)\n", "def test_matrixexpr_methods():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    I = Identity(2)\n\n    # Testing transpose\n    assert A.T == Transpose(A)\n    assert (A + B).T == A.T + B.T\n    assert (A * B).T == B.T * A.T\n    assert (A.T).T == A\n\n    # Testing adjoint\n    assert A.adjoint() == Adjoint(A)\n    assert (A + B).adjoint() == A.adjoint() + B.adjoint()\n    assert (A * B).adjoint() == B.adjoint() * A.adjoint()\n    assert (A.adjoint()).adjoint() == A\n\n    # Testing inverse\n    assert A.inv() == Inverse(A)\n    assert (A * B).inv() == B.inv() * A.inv()\n    assert (A.inv()).inv() == A\n    assert I.inv() == I\n\n    # Testing as_real_imag\n    real, imag = A.as_real_imag()\n    assert real == S.Half * (A + A._eval_conjugate())\n    assert imag == (A - A._eval_conjugate()) / (2 * S.ImaginaryUnit)\n\n    # Testing as_mutable and as_explicit\n    assert A.as_mutable() == A.as_explicit().as_mutable()\n\n    # Testing canonicalize\n    assert A.canonicalize() == A\n\n    # Testing equals method\n    assert A.equals(A) == True\n    assert A.equals(B) == False\n    assert A.equals(Identity(2)) == False\n\n    # Testing valid_index\n    assert A.valid_index(0, 0) == True\n    assert A.valid_index(2, 0) == False\n    assert A.valid_index(0, 2) == False\n    assert A.valid_index(-1, 0) == False\n    assert A.valid_index(0, -1) == False\n\n    # Testing _eval_simplify\n    assert A._eval_simplify() == A\n    assert (A + B)._eval_simplify() == A + B\n", "def test_matrix_symbol_eq():\n    A1 = MatrixSymbol('A1', 2, 2)\n    A2 = MatrixSymbol('A2', 2, 2)\n    B1 = MatrixSymbol('B1', 2, 3)\n    B2 = MatrixSymbol('B2', 3, 2)\n\n    assert (A1 == A2) == False\n    assert (A1 != A2) == True\n    assert (A1 == B1) == False\n    assert (A1 != B1) == True\n    assert (A1 == B2) == False\n    assert (A1 != B2) == True\n", "def test_matrixexpr_properties():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, n)\n\n    assert A.is_Matrix is True\n    assert A.is_MatrixExpr is True\n    assert A.is_Identity is None\n    assert A.is_Inverse is False\n    assert A.is_Transpose is False\n    assert A.is_ZeroMatrix is False\n    assert A.is_MatAdd is False\n    assert A.is_MatMul is False\n    assert A.is_commutative is False\n    assert A.is_number is False\n    assert A.is_symbol is False\n    assert A.is_scalar is False\n    assert A.rows == n\n    assert A.cols == m\n    assert B.is_square == (m == n)\n", "def test_matrixexpr_addition_and_multiplication():\n    F = MatrixSymbol('F', 2, 2)\n    G = MatrixSymbol('G', 2, 2)\n    H = MatrixSymbol('H', 2, 2)\n\n    # Test addition of matrix expressions\n    expr_add = F + G + H\n    assert isinstance(expr_add, MatAdd)\n    assert expr_add.shape == F.shape\n    assert expr_add.args == (F, G, H)\n\n    # Test multiplication of matrix expressions\n    expr_mul = F * G * H\n    assert isinstance(expr_mul, MatMul)\n    assert expr_mul.shape == F.shape\n    assert expr_mul.args == (F, G, H)\n\n    # Test commutativity of addition\n    expr_add_comm = G + F + H\n    assert expr_add_comm == expr_add\n    assert expr_add_comm.args == (F, G, H)\n\n    # Test non-commutativity of multiplication\n    expr_mul_diff = G * F * H\n    assert expr_mul_diff != expr_mul\n\n    # Test zero multiplication and addition\n    Z = ZeroMatrix(2, 2)\n    assert F + Z == F\n    assert F * Z == Z\n    assert Z * F == Z\n"], "sample_1149": ["def test_singleton_sympify_call():\n    from sympy import Rational\n\n    # Test that S(1) is correctly converted to Integer(1)\n    assert S(1) == S.One\n    assert S(0) == S.Zero\n\n    # Test that S(\"1/2\") is correctly converted to Rational(1, 2)\n    assert S(\"1/2\") == Rational(1, 2)\n    assert S(\"1/3\") == Rational(1, 3)\n\n    # Test that S(\"x**2\") is correctly converted to Symbol(\"x\")**2\n    x_squared = S(\"x**2\")\n    assert str(x_squared) == \"x**2\"\n    assert x_squared.args[1] == 2\n\n    # Test that S calls sympify on various types\n    assert S(True) == S.One\n    assert S(False) == S.Zero\n", "def test_singleton_sympify():\n    from sympy.core.sympify import sympify\n\n    class AnotherSingleton(Basic, metaclass=Singleton):\n        pass\n\n    singleton_instance = AnotherSingleton()\n    assert singleton_instance is sympify(singleton_instance)\n    assert singleton_instance is S.AnotherSingleton\n\n    # Test that S() works as a shortcut for sympify\n    assert S(singleton_instance) is singleton_instance\n    assert S(\"AnotherSingleton()\") == singleton_instance\n", "def test_singleton_sympify():\n    # Test that S(1) returns Integer(1) and S(\"x**2\") returns Symbol(\"x\")**2\n    from sympy.core.numbers import Integer\n    from sympy.core.symbol import Symbol\n\n    assert S(1) == Integer(1)\n    assert S(\"x**2\") == Symbol(\"x\")**2\n", "def test_singleton_sympify_call():\n    from sympy.core.sympify import sympify\n\n    # Ensure that S behaves as a shortcut for sympify\n    assert S(1) == sympify(1)\n    assert S(\"x**2\") == sympify(\"x**2\")\n    assert S(\"1/2\") == sympify(\"1/2\")\n\n    # Ensure that S can be used to create Rational numbers\n    from sympy import Symbol\n    x = Symbol('x')\n    assert (x + S(1)/2) == (x + sympify(\"1/2\"))\n", "def test_sympify_call():\n    from sympy.core.sympify import SympifyError\n\n    assert S(1) == sympify(1)\n    assert S(\"x**2\") == sympify(\"x**2\")\n\n    try:\n        S(object())\n    except SympifyError:\n        pass\n    else:\n        raise AssertionError(\"SympifyError was not raised for non-sympifiable object\")\n"], "sample_1150": ["def test_rationals_contains():\n    R = S.Rationals\n\n    # Test containment of different types of numbers\n    assert Rational(3, 4) in R\n    assert Rational(-5, 7) in R\n    assert Rational(0) in R\n    assert 2 in R\n    assert -3 in R\n    assert 0.25 not in R\n    assert sqrt(2) not in R\n    assert I not in R\n    assert pi not in R\n\n    # Test containment of symbolic expressions\n    half = Symbol('half', rational=True)\n    assert half in R\n    x = Symbol('x', real=True, positive=True)\n    assert x not in R\n\n    # Test containment of invalid types\n    assert (1, 2) not in R\n    assert '1/2' not in R\n\n    # Test containment of infinity\n    assert oo not in R\n    assert -oo not in R\n\n    # Test containment of finite sets\n    assert FiniteSet(Rational(1, 2), Rational(3, 4)).is_subset(R)\n    assert FiniteSet(1, 2, 3).is_subset(R)\n    assert not FiniteSet(1, 2.5, 3).is_subset(R)\n    assert not FiniteSet(sqrt(2), pi).is_subset(R)\n", "def test_rationals_contains():\n    R = S.Rationals\n    assert Rational(1, 3) in R\n    assert Rational(-7, 5) in R\n    assert Rational(0) in R\n    assert Rational(8, 2) in R\n    assert 0.25 not in R\n    assert 3.14 not in R\n    assert sqrt(2) not in R\n    assert I not in R\n    assert (1 + I) not in R\n", "def test_rationals_contains():\n    assert S.Rationals._contains(Rational(1, 3)) == True\n    assert S.Rationals._contains(1.5) == False\n    assert S.Rationals._contains(oo) == False\n    assert S.Rationals._contains(I) == False\n    assert S.Rationals._contains(Symbol('x')) == Contains(Symbol('x'), S.Rationals, evaluate=False)\n", "def test_rationals_contains():\n    assert S.Half in S.Rationals\n    assert Rational(2, 3) in S.Rationals\n    assert Rational(-5, 7) in S.Rationals\n    assert 3 not in S.Rationals  # because 3 is an integer, not rational\n    assert 0.5 not in S.Rationals  # floating-point numbers are not rational\n    assert S.Rationals.contains(Rational(3, 4)) == True\n    assert S.Rationals.contains(Rational(-3, 4)) == True\n    assert S.Rationals.contains(S.Half) == True\n    assert S.Rationals.contains(oo) == False\n    assert S.Rationals.contains(-oo) == False\n    assert S.Rationals.contains(x) == Contains(x, S.Rationals, evaluate=False)\n", "def test_Rationals_boundary():\n    assert S.Rationals.boundary == S.Reals\n    assert S.Rationals.is_open == False\n    assert S.Rationals.is_closed == False\n    assert S.Rationals.closure == S.Reals\n"], "sample_1151": ["def test_Mod_negative_dividend():\n    assert Mod(-10, 3) == 2\n    assert Mod(-10, 4) == 2\n    assert Mod(-10, 5) == 0\n    assert Mod(-10, 6) == 2\n", "def test_Mod_special_cases():\n    # Test modulo with negative integers\n    assert Mod(-10, 3) == 2\n    assert Mod(-10, -3) == -1\n    assert Mod(10, -3) == -2\n\n    # Test modulo with floating-point numbers\n    assert Mod(7.5, 2.5) == 0\n    assert Mod(-7.5, 2.5) == 2.5\n    assert Mod(7.5, -2.5) == -0.0\n    assert Mod(-7.5, -2.5) == -2.5\n\n    # Test modulo with symbolic expressions involving powers and sums\n    assert Mod(x**3 + 3*x**2 + 3*x + 1, x + 1) == Mod(1, x + 1)\n    assert Mod(x**2 + 2*x + 1, x + 1) == 0\n\n    # Test nested modulo operations with symbolic expressions\n    assert Mod(Mod(x + 3, 4), 2) == Mod(x + 3, 2)\n    assert Mod(Mod(x**2 + 5, x + 1), x) == Mod(5, x)\n\n    # Test modulo with zero results\n    assert Mod(0, x) == 0\n    assert Mod(x, 1) == 0\n    assert Mod(0, 1) == 0\n\n    # Test modulo with infinity\n    assert Mod(oo, 3) == nan\n    assert Mod(-oo, 3) == nan\n    assert Mod(oo, x) == nan\n    assert Mod(x, oo) == x\n", "def test_Mod_eval():\n    from sympy.abc import a, b\n    # Modulo by zero should raise ZeroDivisionError\n    raises(ZeroDivisionError, lambda: Mod.eval(1, 0))\n    \n    # Modulo with NaN should return NaN\n    assert Mod.eval(nan, 1) is nan\n    assert Mod.eval(1, nan) is nan\n    assert Mod.eval(nan, nan) is nan\n    \n    # Simple cases\n    assert Mod.eval(5, 3) == 2\n    assert Mod.eval(-5, 3) == 1\n    assert Mod.eval(5, -3) == -1\n    assert Mod.eval(-5, -3) == -2\n    \n    # Zero dividend\n    assert Mod.eval(0, 3) == 0\n    \n    # Same as divisor\n    assert Mod.eval(3, 3) == 0\n    assert Mod.eval(-3, 3) == 0\n    assert Mod.eval(3, -3) == 0\n    assert Mod.eval(-3, -3) == 0\n    \n    # Integer property\n    assert Mod.eval(a, 1) == 0 if a.is_integer else None\n    \n    # Handling even and odd properties\n    assert Mod.eval(2, 2) == 0\n    assert Mod.eval(3, 2) == 1\n    assert Mod.eval(4, 2) == 0\n    \n    # Symbolic modulo with known sign properties\n    p = Symbol('p', positive=True)\n    assert Mod.eval(p + 5, p + 3) == 2\n    assert Mod.eval(-p - 5, p + 3) == p + 1\n    assert Mod.eval(p + 5, -p - 3) == -p - 1\n    assert Mod.eval(-p - 5, -p - 3) == -2\n    assert Mod.eval(p + 1, p - 1) == None\n\n    # Test denesting\n    assert Mod.eval(Mod(7, 5), 5) == 2\n    assert Mod.eval(Mod(8, 4), 2) == 0\n    assert Mod.eval(Mod(10, 3), 3) == 1\n", "def test_mod_denesting_cases():\n    # Test cases to improve coverage for denesting in Mod\n    assert Mod(Mod(x, 3), 3) == Mod(x, 3)\n    assert Mod(Mod(x, 4), 2) == Mod(x, 2)\n    assert Mod(Mod(x, -5), 2) == Mod(x, 2)\n    assert Mod(Mod(x, 6), -3) == Mod(x, 3)\n    assert Mod(-Mod(x, 7), 7) == Mod(-x, 7)\n    assert Mod(Mod(x, y), y) == Mod(x, y)\n    assert Mod(-Mod(x, y), y) == Mod(-x, y)\n    assert Mod(Mod(x + 1, 2) + 1, 2) == Mod(x, 2)\n    assert Mod(Mod(x + 2, 4) * (x + 4), 4) == Mod(x * (x + 2), 4)\n    assert Mod(Mod(x + 2, 4) * 4, 4) == 0\n", "def test_Mod_properties():\n    # Test that Mod respects properties of its arguments\n    assert Mod(-x, 2).is_integer\n    assert Mod(x**2, 1) == 0\n    assert Mod(x, x + 1) == x\n    assert Mod(3*x, 3) == 0\n    assert Mod(x + y, x) == Mod(y, x)\n    assert Mod(0, y) == 0\n    assert Mod(2*oo, 3) is nan\n    assert Mod(x, 2).is_nonnegative\n    assert Mod(x, -2).is_nonpositive\n    assert Mod(-3, 7) == 4\n    assert Mod(-3, -7) == -3\n\n    # Edge case for large powers\n    assert Mod(x**1000, x**999) == 0\n    assert Mod(x**1000 + 1, x**999) == x**999 + 1\n    \n    # Check behavior with floats and negatives\n    assert Mod(5.5, 2) == 1.5\n    assert Mod(-5.5, 2) == 0.5\n    assert Mod(-5.5, -2) == -1.5\n    assert Mod(5.5, -2) == -0.5\n\n    # Check combinations involving Mod in expressions\n    assert (Mod(x, 2) + Mod(y, 3)).expand() == Mod(x, 2) + Mod(y, 3)\n    assert (Mod(x, 2)*Mod(y, 3)).expand() == Mod(x, 2)*Mod(y, 3)\n    assert (Mod(x + y, x*y)).expand() == Mod(x + y, x*y)\n"], "sample_1152": ["def test_powsimp_symbols_with_assumptions():\n    p, q, r = symbols('p q r', positive=True)\n    m, n, o = symbols('m n o', nonnegative=True)\n    u, v, w = symbols('u v w', nonzero=True)\n\n    assert powsimp(p**x * q**x * r**x) == (p*q*r)**x\n    assert powsimp(p**x * q**x) == (p*q)**x\n    assert powsimp((p*q)**x * r**x) == (p*q*r)**x\n    assert powsimp((p**x * q**x)**y) == (p*q)**(x*y)\n    assert powsimp(m**x * n**x * o**x) == (m*n*o)**x\n    assert powsimp(u**x * v**x * w**x) == (u*v*w)**x\n    assert powsimp(p**x * m**x) == (p*m)**x\n    assert powsimp(q**x * u**x) == (q*u)**x\n    assert powsimp((p*q)**x * (u*v)**x) == (p*q*u*v)**x\n    assert powsimp((m*n)**x * (o*p)**x) == (m*n*o*p)**x\n", "def test_powsimp_with_complex():\n    from sympy import I\n    a, b, c, d = symbols('a b c d')\n    # Test complex exponentiation and simplification\n    expr1 = (I**2)**a\n    assert powsimp(expr1) == (-1)**a\n    expr2 = (I**3)**b\n    assert powsimp(expr2) == (-I)**b\n    expr3 = (I**4)**c\n    assert powsimp(expr3) == 1**c\n    expr4 = (I**a)*(I**b)\n    assert powsimp(expr4) == I**(a + b)\n    expr5 = (I**a)**b\n    assert powsimp(expr5) == I**(a*b)\n", "def test_powsimp_with_rational_powers():\n    r, s = symbols('r s', rational=True)\n    assert powsimp((2**r * 2**s)) == 2**(r + s)\n    assert powsimp((3**r * 3**s), combine='exp') == 3**(r + s)\n    assert powsimp((3**r * 3**s), combine='base') == 3**r * 3**s\n    assert powsimp((x**r * x**s)) == x**(r + s)\n    assert powsimp((x**r * y**r)) == (x*y)**r\n    assert powsimp((x**(2*r) * x**(3*r))) == x**(5*r)\n    assert powsimp((x**(r/2) * x**(r/3))) == x**(5*r/6)\n", "def test_powsimp_with_nested_exponents():\n    x, y, z, n = symbols('x,y,z,n')\n    assert powsimp(x**(y**z)*x**(y**(z + 1))) == x**(y**z + y**(z + 1))\n    assert powsimp(x**(2*y)*x**(3*y)) == x**(5*y)\n    assert powsimp(x**(x**y)*x**(x**(y + 1))) == x**(x**y + x**(y + 1))\n    assert powsimp(x**(2*y*z)*x**(3*y*z), deep=True) == x**(5*y*z)\n    assert powsimp((x**y)**z*(x**y)**(z + 1)) == (x**y)**(2*z + 1)\n", "def test_powsimp_with_forced_combination():\n    x, y, z, n = symbols('x,y,z,n')\n    assert powsimp(x**2 * x**(-2), force=True) == 1\n    assert powsimp(x**2 * x**(-2), combine='base', force=True) == 1\n    assert powsimp(x**2 * y**2 * x**(-2) * y**(-2), force=True) == 1\n    assert powsimp(x**2 * y**2 * x**(-2) * y**(-2), combine='base', force=True) == 1\n    assert powsimp(exp(x) * exp(-x), force=True) == 1\n    assert powsimp(exp(x) * exp(-x), combine='exp', force=True) == 1\n    assert powsimp(exp(x) * exp(y) * exp(-x) * exp(-y), force=True) == 1\n    assert powsimp(exp(x) * exp(y) * exp(-x) * exp(-y), combine='exp', force=True) == 1\n"], "sample_1153": ["def test_abs_of_function():\n    x = Symbol('x', real=True)\n    f = Function('f', real=True)\n    g = Function('g')\n\n    # Test Abs of a real function\n    assert Abs(f(x)).diff(x) == sign(f(x)) * Derivative(f(x), x)\n\n    # Test Abs of a generic function\n    assert Abs(g(x)).diff(x) == (conjugate(g(x)) * Derivative(g(x), x) + g(x) * Derivative(conjugate(g(x)), x)) / Abs(g(x))\n\n    # Test Abs of a real function with specific value\n    assert Abs(sin(x)).rewrite(Heaviside).simplify() == sin(x) * Heaviside(sin(x)) - sin(x) * Heaviside(-sin(x))\n    assert Abs(cos(x)).diff(x) == -sin(x) * sign(cos(x))\n\n    # Test Abs of an imaginary function\n    y = Symbol('y', imaginary=True)\n    assert Abs(g(y)).diff(y) == -I * g(y) / Abs(g(y)) * Derivative(g(y), y)\n\n    # Test nested function\n    h = Function('h')\n    assert Abs(h(f(x))).diff(x).doit() == (conjugate(h(f(x))) * h(f(x)).diff(f(x)) * f(x).diff(x) + h(f(x)) * conjugate(h(f(x)).diff(f(x))) * f(x).diff(x)) / Abs(h(f(x)))\n\n    # Test Abs with constants\n    a = Symbol('a', real=True, positive=True)\n    b = Symbol('b', real=True, negative=True)\n    c = Symbol('c', imaginary=True)\n    assert Abs(a * f(x)).diff(x) == a * sign(f(x)) * Derivative(f(x), x)\n    assert Abs(b * f(x)).diff(x) == -b * sign(f(x)) * Derivative(f(x), x)\n    assert Abs(c * g(x)).diff(x) == c * (conjugate(g(x)) * Derivative(g(x), x) + g(x) * Derivative(conjugate(g(x)), x)) / Abs(c * g(x))\n", "def test_derivatives_sign():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    assert Derivative(sign(x), x).doit() == 2*DiracDelta(x)\n    assert Derivative(sign(y), y).doit() == 2*DiracDelta(-I*y)\n    assert Derivative(sign(x**2 + 1), x).doit() == 2*(x/(x**2 + 1))*DiracDelta(x**2 + 1)\n", "def test_issue_16614():\n    # issue 16614: conjugate of Abs(x) should be Abs(x)\n    x = Symbol('x')\n    assert conjugate(Abs(x)) == Abs(x)\n    assert conjugate(Abs(x)).rewrite(Abs) == Abs(x)\n    assert conjugate(Abs(x)).rewrite(conjugate) == Abs(x)\n", "def test_polar_lift():\n    x, y = symbols('x y')\n    assert polar_lift(1) == exp_polar(0)\n    assert polar_lift(-1) == exp_polar(I * pi)\n    assert polar_lift(I) == exp_polar(-I * pi/2)\n    assert polar_lift(-I) == exp_polar(I * pi/2)\n    assert polar_lift(1 + I) == polar_lift(1 + I)\n    assert polar_lift(x*y) == polar_lift(x) * polar_lift(y)\n    assert polar_lift(x + I) != polar_lift(x) + polar_lift(I)\n    assert polar_lift(2*x) == 2 * polar_lift(x)\n    assert polar_lift(2*I) == 2 * exp_polar(-I * pi/2)\n    \n    a = Symbol('a', real=True, positive=True)\n    assert polar_lift(a) == a * exp_polar(0)\n    b = Symbol('b', real=True, negative=True)\n    assert polar_lift(b) == Abs(b) * exp_polar(I * pi)\n\n    assert polar_lift(2*pi*I) == 2*pi*exp_polar(-I * pi/2)\n    \n    assert polar_lift(3 + 4*I) == polar_lift(3 + 4*I)\n    assert polar_lift(0) == 0\n    \n    # Check with matrix\n    A = Matrix([[1 + I, 2], [3, 4 - I]])\n    assert polar_lift(A) == Matrix([\n        [polar_lift(1 + I), polar_lift(2)],\n        [polar_lift(3), polar_lift(4 - I)]\n    ])\n", "def test_periodic_argument_edge_cases():\n    from sympy import (periodic_argument, polar_lift, pi, exp_polar)\n    p = Symbol('p', positive=True)\n    \n    # Test periodic argument with non-polar numbers and edge cases\n    assert periodic_argument(0, 2*pi) == 0\n    assert periodic_argument(1, 2*pi) == 0\n    assert periodic_argument(-1, 2*pi) == pi\n    assert periodic_argument(I, 2*pi) == pi/2\n    assert periodic_argument(-I, 2*pi) == -pi/2\n    assert periodic_argument(exp_polar(I*pi), 2*pi) == pi\n    assert periodic_argument(polar_lift(0), 2*pi) == 0\n    assert periodic_argument(polar_lift(-1), 2*pi) == pi\n    \n    # Ensure that polar_lift of 1 is handled properly\n    assert periodic_argument(polar_lift(1), 2*pi) == 0\n    \n    # Test periodic argument with an extended period\n    assert periodic_argument(exp_polar(4*pi*I), 6*pi) == 4*pi\n    assert periodic_argument(exp_polar(-7*pi*I), 6*pi) == -pi\n    \n    # Test with arbitrary expression\n    expr = polar_lift(3 + 4*I)\n    assert periodic_argument(expr, pi).rewrite(atan2) == atan2(4, 3)\n\n    # Checking the unbranched argument with periodic argument\n    assert unbranched_argument(expr) == periodic_argument(expr, oo)\n"], "sample_1154": ["def test__linsolve_symbolic_coefficients():\n    a, b, c = S('a b c').as_real_imag()[0]  # Using symbolic coefficients\n    eqs = [\n        a*x + b*y - c,\n        b*x + a*y - c\n    ]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: c*(a**2 - b**2)/(a**3 + a*b**2), y: c*b/(a**2 + b**2)}\n\n    # Test with one equation being zero\n    eqs = [\n        a*x + b*y - c,\n        0\n    ]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: (c - b*y)/a, y: y}\n", "def test__linsolve_symbolic():\n    a, b, c = S('a b c'.split())\n    \n    # Unique solution with symbolic coefficients:\n    eqs = [a*x + b*y - c, b*x - a*y - c]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: (a*c + b*c)/(a**2 + b**2), y: (a*c - b*c)/(a**2 + b**2)}\n    \n    # Underdetermined system with symbolic coefficients:\n    eqs = [a*x + b*y]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: -b*y/a, y: y}\n    \n    # No solution with symbolic coefficients:\n    eqs = [a*x + b*y - c, a*x + b*y - 2*c]\n    assert _linsolve(eqs, [x, y]) is None\n    \n    # Homogeneous system with symbolic coefficients:\n    eqs = [a*x + b*y, b*x - a*y]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: x, y: -b*x/a}\n", "def test__linsolve_inconsistent_system():\n    # Test for inconsistent system which should return None\n    eqs = [\n        Eq(x + y, 1),\n        Eq(x + y, 2)\n    ]\n    assert _linsolve(eqs, [x, y]) is None\n\n    # Test for inconsistent system with three variables\n    eqs = [\n        Eq(x + y + z, 1),\n        Eq(x + y + z, 2)\n    ]\n    assert _linsolve(eqs, [x, y, z]) is None\n\n    # Test for inconsistent system with specific values\n    eqs = [\n        Eq(2*x + 3*y, 1),\n        Eq(4*x + 6*y, 3)\n    ]\n    assert _linsolve(eqs, [x, y]) is None\n", "def test__linsolve_underdetermined():\n    eqs = [Eq(x + y, 1)]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: 1 - y, y: y}\n\n    eqs = [Eq(x + y + z, 1), Eq(x - y, 2)]\n    sol = _linsolve(eqs, [x, y, z])\n    assert sol == {x: 1 + y, y: y, z: -2*y}\n", "def test__linsolve_underdetermined():\n    eqs = [\n        Eq(x + 2*y, 3),\n        Eq(2*x + 4*y, 6)\n    ]\n    sol = _linsolve(eqs, [x, y])\n    expected_sol = {x: 3 - 2*y, y: y}\n    assert sol == expected_sol\n\n    eqs = [\n        Eq(x + y + z, 1),\n        Eq(2*x + 2*y + 2*z, 2)\n    ]\n    sol = _linsolve(eqs, [x, y, z])\n    expected_sol = {x: 1 - y - z, y: y, z: z}\n    assert sol == expected_sol\n"], "sample_1155": ["def test_construct_domain_with_mixed_types():\n    # Testing domain construction with mixed integer, rational, and complex numbers\n    assert construct_domain([1, 2, Rational(3, 4), 5 + I]) == (QQ_I, [QQ_I(1, 0), QQ_I(2, 0), QQ_I(Rational(3, 4), 0), QQ_I(5, 1)])\n    \n    # Testing domain construction with mixed algebraic and transcendental numbers\n    assert construct_domain([sqrt(2), pi]) == (EX, [EX(sqrt(2)), EX(pi)])\n    \n    # Testing domain construction with nested expressions containing symbols\n    expr1 = x * (1 + y)\n    expr2 = y * (1 + x)\n    assert construct_domain([expr1, expr2]) == (ZZ[x, y], [ZZ[x, y].convert(expr1), ZZ[x, y].convert(expr2)])\n    \n    # Testing domain construction with floats of different precisions\n    f1 = Float(\"1.123456789123456789\")\n    f2 = Float(\"2.987654321987654321\", precision=100)\n    result = construct_domain([f1, f2])\n    assert isinstance(result[0], RealField)\n    assert result[1][0] == RR(f1)\n    assert result[1][1] == RR(f2)\n    \n    # Testing domain construction with complex expressions involving floats\n    f3 = Float(\"1.5\")\n    result = construct_domain([f3 + I, 2.0 + 3.0 * I])\n    assert isinstance(result[0], ComplexField)\n    assert result[1] == [CC(1.5, 1.0), CC(2.0, 3.0)]\n", "def test_dict_input():\n    # Testing with dictionary input\n    domain, elements = construct_domain({(0,): 1, (1,): 2*x})\n    assert domain == ZZ[x]\n    assert elements == {(0,): ZZ[x].convert(1), (1,): ZZ[x].convert(2*x)}\n\n    domain, elements = construct_domain({(0, 0): 1, (1, 1): 2*x*y})\n    assert domain == ZZ[x, y]\n    assert elements == {(0, 0): ZZ[x, y].convert(1), (1, 1): ZZ[x, y].convert(2*x*y)}\n\n    domain, elements = construct_domain({(0,): S.Half, (1,): x/2})\n    assert domain == QQ[x]\n    assert elements == {(0,): QQ[x].convert(S.Half), (1,): QQ[x].convert(x/2)}\n\n    domain, elements = construct_domain({(0,): sqrt(2), (1,): sqrt(3)}, extension=True)\n    alg = QQ.algebraic_field(sqrt(2) + sqrt(3))\n    assert domain == alg\n    assert elements == {(0,): alg.convert(sqrt(2)), (1,): alg.convert(sqrt(3))}\n", "def test_mixed_numeric_types():\n    assert construct_domain([1, Rational(1, 2), 3.14]) == (EX, [EX(1), EX(Rational(1, 2)), EX(3.14)])\n    result = construct_domain([1, Rational(1, 2), 3.14, I])\n    assert isinstance(result[0], ComplexField)\n    assert result[1] == [CC(1), CC(0.5), CC(3.14), CC(1j)]\n", "def test_construct_domain_expressions():\n    expr1 = x**2 + 2*x + 1\n    expr2 = y**2 - 1\n    expr3 = x*y + y**2\n\n    dom, elems = construct_domain([expr1, expr2, expr3])\n    assert dom == ZZ[x, y]\n    assert elems == [dom.convert(expr1), dom.convert(expr2), dom.convert(expr3)]\n\n    complex_expr1 = sin(x) + I*cos(y)\n    complex_expr2 = exp(I*x) + exp(-I*y)\n\n    dom, elems = construct_domain([complex_expr1, complex_expr2])\n    assert dom == EX\n    assert elems == [dom.convert(complex_expr1), dom.convert(complex_expr2)]\n\n    mixed_expr1 = sqrt(2) + x\n    mixed_expr2 = 1 + y\n\n    dom, elems = construct_domain([mixed_expr1, mixed_expr2], extension=True)\n    assert dom == QQ.algebraic_field(sqrt(2))\n    assert elems == [dom.convert(mixed_expr1), dom.convert(mixed_expr2)]\n", "def test_mixed_expressions():\n    # Test with a mix of integers, rationals, floats, and algebraic numbers\n    result = construct_domain([2, S.Half, 3.14, sqrt(2)], extension=True)\n    assert result[0] == EX\n    assert result[1] == [EX(2), EX(S.Half), EX(3.14), EX(sqrt(2))]\n\n    # Test with a mix of symbols, integers, and algebraic numbers\n    result = construct_domain([x, 2, sqrt(3)], extension=True)\n    assert result[0] == EX\n    assert result[1] == [EX(x), EX(2), EX(sqrt(3))]\n\n    # Test with a mix of rational functions and algebraic numbers\n    result = construct_domain([x/y, sqrt(2)], extension=True)\n    assert result[0] == EX\n    assert result[1] == [EX(x/y), EX(sqrt(2))]\n\n    # Test with a mix of transcendental and algebraic numbers\n    result = construct_domain([exp(1), sqrt(2)], extension=True)\n    assert result[0] == EX\n    assert result[1] == [EX(exp(1)), EX(sqrt(2))]\n"], "sample_1156": ["def test_sinh_finite():\n    x = Symbol('x')\n    assert sinh(x).is_finite is None  # x is symbolic, so it cannot be determined if sinh(x) is finite\n    assert sinh(0).is_finite is True  # sinh(0) = 0, which is finite\n    assert sinh(oo).is_finite is False  # sinh(oo) = oo, which is not finite\n    assert sinh(-oo).is_finite is False  # sinh(-oo) = -oo, which is not finite\n    assert sinh(I*pi).is_finite is True  # sinh(I*pi) = 0, which is finite\n", "def test_sinh_eval():\n    x, y = symbols('x, y')\n\n    assert sinh(0) == 0\n    assert sinh(pi) == sinh(pi)\n    assert sinh(-pi) == -sinh(pi)\n    assert sinh(oo) == oo\n    assert sinh(-oo) == -oo\n    assert sinh(I*pi) == 0\n    assert sinh(-I*pi) == 0\n    assert sinh(I*pi/2) == I\n    assert sinh(-I*pi/2) == -I\n    assert sinh(pi*I/3) == S.Half*sqrt(3)*I\n    assert sinh(-pi*I/3) == -S.Half*sqrt(3)*I\n    assert sinh(pi*I/6) == S.Half*I\n    assert sinh(-pi*I/6) == -S.Half*I\n    assert sinh(pi*I/4) == S.Half*sqrt(2)*I\n    assert sinh(-pi*I/4) == -S.Half*sqrt(2)*I\n    assert sinh(pi*I*Rational(5, 2)) == I\n    assert sinh(pi*I*Rational(7, 2)) == -I\n    assert sinh(pi*I*Rational(17, 4)) == S.Half*sqrt(2)*I\n    assert sinh(pi*I*Rational(7, 6)) == Rational(-1, 2)*I\n", "def test_asinh_symmetry():\n    x = Symbol('x', real=True)\n\n    assert asinh(-x) == -asinh(x)\n    assert asinh(I*x).is_real is False\n    assert asinh(-I*x) == -asinh(I*x)\n\n    # test for imaginary argument\n    assert asinh(I).is_real is False\n    assert asinh(-I) == -asinh(I)\n\n    # specific values\n    assert asinh(-2) == -asinh(2)\n    assert asinh(I*2).is_real is False\n    assert asinh(-I*2) == -asinh(I*2)\n\n    # check inverse composition for negative and imaginary values\n    assert asinh(sinh(-3, evaluate=False)) == -3\n    assert asinh(sinh(I*3, evaluate=False)) == I*3\n    assert asinh(sinh(-I*3, evaluate=False)) == -I*3\n", "def test_tanh_is_finite():\n    x, y = symbols('x y')\n\n    assert tanh(nan).is_finite is False\n    assert tanh(zoo).is_finite is False\n    assert tanh(oo).is_finite is True\n    assert tanh(-oo).is_finite is True\n    assert tanh(0).is_finite is True\n\n    assert tanh(x).is_finite is None\n    assert tanh(y).is_finite is None\n\n    assert tanh(I*pi/3).is_finite is True\n    assert tanh(-I*pi/3).is_finite is True\n", "def test_hyperbolic_function_peeloff_ipi():\n    x, y = symbols('x y')\n    assert _peeloff_ipi(x + I*pi/2) == (x, I*pi/2)\n    assert _peeloff_ipi(x + I*2*pi/3 + I*pi*y) == (x + I*pi*y + I*pi/6, I*pi/2)\n    assert _peeloff_ipi(x + I*3*pi) == (x, I*3*pi)\n    assert _peeloff_ipi(x + y*I*pi/2) == (x + y*I*pi/2, S.Zero)\n    assert _peeloff_ipi(x + y*I*pi) == (x, y*I*pi)\n    assert _peeloff_ipi(x + y*I*pi + I*pi/2) == (x + y*I*pi, I*pi/2)\n"], "sample_1157": ["def test_apply_functions_with_parentheses():\n    local_dict = {'f': Function('f')}\n    global_dict = {'g': Function('g')}\n    inputs = {\n        'f(g(x))': local_dict['f'](global_dict['g'](Symbol('x'))),\n        'f((x + y))': local_dict['f'](Symbol('x') + Symbol('y')),\n        'f((g(x)))': local_dict['f'](global_dict['g'](Symbol('x'))),\n        'f(g((x)))': local_dict['f'](global_dict['g'](Symbol('x')))\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, local_dict=local_dict, global_dict=global_dict) == result\n", "def test_auto_symbol_transformation():\n    # Test auto_symbol transformation with undefined variables\n    inputs = {\n        'a': Symbol('a'),\n        'b + c': Symbol('b') + Symbol('c'),\n        'd * e': Symbol('d') * Symbol('e'),\n        'f / g': Symbol('f') / Symbol('g'),\n        'h - i': Symbol('h') - Symbol('i'),\n        'j**k': Symbol('j')**Symbol('k'),\n        'l(m)': Function('l')(Symbol('m')),\n        'n(o, p)': Function('n')(Symbol('o'), Symbol('p')),\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text) == result\n\n    # Test auto_symbol transformation with defined variables in local_dict\n    local_dict = {\n        'a': Symbol('a'),\n        'b': Symbol('b'),\n        'my_func': Function('my_func')\n    }\n    inputs = {\n        'a': Symbol('a'),\n        'b + c': Symbol('b') + Symbol('c'),\n        'my_func(d)': Function('my_func')(Symbol('d')),\n        'my_func(e + f)': Function('my_func')(Symbol('e') + Symbol('f')),\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, local_dict=local_dict) == result\n\n    # Test auto_symbol transformation with redefinition in local_dict\n    local_dict = {\n        'a': Function('a'),\n        'b': None\n    }\n    inputs = {\n        'a(2)': Function('a')(Integer(2)),\n        'b + c': Symbol('b') + Symbol('c'),\n        'a + b': Function('a') + Symbol('b')\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, local_dict=local_dict) == result\n", "def test_implicit_multiplication_edge_cases():\n    transformations = standard_transformations + (implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    f = Function('f')\n    \n    inputs = {\n        'sin(x)cos(x)': sin(x) * cos(x),\n        '(x+2)(x+3)': (x+2) * (x+3),\n        '2(x+1)': 2 * (x+1),\n        '2 sin(x)': 2 * sin(x),\n        'sin(x) 2': sin(x) * 2,\n        '(x+1)(x+2)(x+3)': (x+1) * (x+2) * (x+3),\n        'f(x) y': f(x) * y,\n        'x sin(x)': x * sin(x),\n        'sin x (cos x + 1)': sin(x) * (cos(x) + 1),\n        '(x+1) sin x': (x+1) * sin(x)\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n", "def test_explicit_multiplication():\n    transformations = standard_transformations + (implicit_multiplication_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"2*x*y\", transformations=transformations) == 2 * x * y\n    assert parse_expr(\"3*(x + y)\", transformations=transformations) == 3 * (x + y)\n    assert parse_expr(\"(x + y)*(x - y)\", transformations=transformations) == (x + y) * (x - y)\n    assert parse_expr(\"2*3*x\", transformations=transformations) == 6 * x\n    assert parse_expr(\"4*x*y*z\", transformations=transformations, local_dict={'z': 5}) == 20 * x * y\n", "def test_function_exponent_with_symbols():\n    transformations = standard_transformations + (function_exponentiation,)\n    f = Function('f')\n    g = Function('g')\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert parse_expr('f**g(x)', transformations=transformations) == (f(x))**g(x)\n    assert parse_expr('f**2(x)', transformations=transformations) == (f(x))**2\n    assert parse_expr('f**y(x)', transformations=transformations) == (f(x))**y\n    assert parse_expr('g**f**2(x)', transformations=transformations) == (g(f(x)))**2\n"], "sample_1158": ["def test_sympify_numpy_matrix():\n    if not numpy:\n        skip('numpy not installed. Abort numpy tests.')\n    np = numpy\n\n    # Test numpy matrix\n    mat = np.matrix([[1, 2], [3, 4]])\n    sym_mat = sympify(mat)\n    assert isinstance(sym_mat, ImmutableDenseNDimArray)\n    assert sym_mat.shape == (2, 2)\n    assert sym_mat[0, 0] == 1\n    assert sym_mat[0, 1] == 2\n    assert sym_mat[1, 0] == 3\n    assert sym_mat[1, 1] == 4\n\n    # Test numpy matrix with complex numbers\n    mat_complex = np.matrix([[1 + 2j, 3 + 4j], [5 + 6j, 7 + 8j]])\n    sym_mat_complex = sympify(mat_complex)\n    assert isinstance(sym_mat_complex, ImmutableDenseNDimArray)\n    assert sym_mat_complex.shape == (2, 2)\n    assert sym_mat_complex[0, 0] == 1 + 2*I\n    assert sym_mat_complex[0, 1] == 3 + 4*I\n    assert sym_mat_complex[1, 0] == 5 + 6*I\n    assert sym_mat_complex[1, 1] == 7 + 8*I\n", "def test_sympify_CantSympify():\n    class MyClass(CantSympify):\n            self.value = value\n\n    obj = MyClass(42)\n    raises(SympifyError, lambda: sympify(obj))\n\n    class MyOtherClass:\n            self.value = value\n\n    obj = MyOtherClass(42)\n    assert sympify(obj) == obj  # Should not raise error as it doesn't inherit CantSympify\n", "def test_sympify_numpy_dtype():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    a = numpy.array([1, 2, 3], dtype=numpy.int32)\n    assert sympify(a).dtype == numpy.int32\n\n    b = numpy.array([1.1, 2.2, 3.3], dtype=numpy.float64)\n    assert sympify(b).dtype == numpy.float64\n\n    c = numpy.array([1+1j, 2+2j, 3+3j], dtype=numpy.complex128)\n    assert sympify(c).dtype == numpy.complex128\n\n    d = numpy.array([True, False, True], dtype=numpy.bool_)\n    assert sympify(d).dtype == numpy.bool_\n\n    e = numpy.array(['a', 'b', 'c'], dtype=numpy.str_)\n    assert sympify(e).dtype == numpy.object_\n", "def test_sympify_numpy_array_handling():\n    if not numpy:\n        skip(\"numpy not installed.\")\n    np = numpy\n\n    # Test conversion of different types of numpy arrays\n    arr_int = np.array([1, 2, 3])\n    arr_float = np.array([1.1, 2.2, 3.3])\n    arr_complex = np.array([1+2j, 3+4j])\n    arr_bool = np.array([True, False, True])\n\n    assert sympify(arr_int) == ImmutableDenseNDimArray([1, 2, 3])\n    assert sympify(arr_float) == ImmutableDenseNDimArray([1.1, 2.2, 3.3])\n    assert sympify(arr_complex) == ImmutableDenseNDimArray([1+2*I, 3+4*I])\n    assert sympify(arr_bool) == ImmutableDenseNDimArray([true, false, true])\n\n    # Test conversion of multidimensional numpy arrays\n    arr_2d = np.array([[1, 2], [3, 4]])\n    arr_3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n    assert sympify(arr_2d) == ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    assert sympify(arr_3d) == ImmutableDenseNDimArray([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n", "def test_sympify_numpy_types():\n    if not numpy:\n        skip(\"numpy not installed.\")\n    \n    import numpy as np\n    \n    # Test for various numpy data types\n    a_float = np.float64(1.23)\n    a_complex = np.complex128(1 + 2j)\n    a_int = np.int32(42)\n    a_array = np.array([1, 2, 3])\n    \n    assert sympify(a_float) == Float(1.23)\n    assert sympify(a_complex) == 1.0 + 2.0*I\n    assert sympify(a_int) == Integer(42)\n    assert sympify(a_array) == ImmutableDenseNDimArray([1, 2, 3])\n\n    # Test for numpy scalars in strict mode\n    raises(SympifyError, lambda: sympify(a_float, strict=True))\n    raises(SympifyError, lambda: sympify(a_complex, strict=True))\n    raises(SympifyError, lambda: sympify(a_int, strict=True))\n    raises(SympifyError, lambda: sympify(a_array, strict=True))\n\n    # Test for numpy scalars in non-strict mode\n    assert sympify(a_float, strict=False) == Float(1.23)\n    assert sympify(a_complex, strict=False) == 1.0 + 2.0*I\n    assert sympify(a_int, strict=False) == Integer(42)\n    assert sympify(a_array, strict=False) == ImmutableDenseNDimArray([1, 2, 3])\n"], "sample_1159": ["def test_assumptions_with_Mul():\n    x = Symbol('x', positive=True)\n    y = Symbol('y', negative=True)\n    assert (x * y).is_positive is False\n    assert (x * y).is_negative is True\n    assert (x * x).is_positive is True\n    assert (x * x).is_negative is False\n    assert (y * y).is_positive is True\n    assert (y * y).is_negative is False\n    assert (x * y * y).is_positive is True\n    assert (x * y * y).is_negative is False\n", "def test_assumptions_function():\n    x = Symbol('x', real=True, integer=False)\n    y = Symbol('y', integer=True, positive=True)\n    expr = x + y\n    assert assumptions(expr) == {\n        'commutative': True,\n        'complex': True,\n        'extended_real': True,\n        'finite': True,\n        'imaginary': False,\n        'integer': None,\n        'irrational': None,\n        'noninteger': None,\n        'nonnegative': None,\n        'nonpositive': None,\n        'nonzero': None,\n        'negative': None,\n        'positive': None,\n        'rational': None,\n        'real': True,\n        'transcendental': None,\n        'zero': None,\n    }\n    expr = y - S.Pi\n    assert assumptions(expr) == {\n        'algebraic': None,\n        'commutative': True,\n        'complex': True,\n        'extended_real': True,\n        'finite': True,\n        'imaginary': False,\n        'integer': None,\n        'irrational': None,\n        'noninteger': None,\n        'nonnegative': None,\n        'nonpositive': None,\n        'nonzero': None,\n        'negative': None,\n        'positive': None,\n        'rational': None,\n        'real': True,\n        'transcendental': None,\n        'zero': None,\n    }\n", "def test_symbol_extended_real_true():\n    x = Symbol('x', extended_real=True)\n    assert x.is_extended_real is True\n    assert x.is_real is None\n    assert x.is_integer is None\n    assert x.is_complex is None\n    assert x.is_finite is None\n    assert x.is_infinite is None\n    assert x.is_zero is None\n    assert x.is_nonzero is None\n    assert x.is_positive is None\n    assert x.is_nonpositive is None\n    assert x.is_negative is None\n    assert x.is_nonnegative is None\n    assert x.is_even is None\n    assert x.is_odd is None\n    assert x.is_comparable is None\n    assert x.is_prime is None\n    assert x.is_composite is None\n    assert x.is_number is False\n", "def test_symbol_extended_nonpositive():\n    x = Symbol('x', extended_nonpositive=True)\n    assert x.is_extended_nonpositive is True\n    assert x.is_extended_nonnegative is None\n    assert x.is_extended_positive is False\n    assert x.is_extended_negative is None\n    assert x.is_nonpositive is None\n    assert x.is_nonnegative is None\n    assert x.is_positive is None\n    assert x.is_negative is None\n    assert x.is_zero is None\n    assert x.is_nonzero is None\n    assert x.is_finite is None\n    assert x.is_infinite is None\n", "def test_issue_17986():\n    # Test whether a finite, nonzero symbol is treated correctly.\n    a = Symbol('a', finite=True, zero=False)\n    assert a.is_finite is True\n    assert a.is_zero is False\n    assert a.is_nonzero is True\n    assert a.is_infinite is False\n    assert a.is_extended_real is None\n    assert a.is_extended_nonzero is None\n"], "sample_1160": ["def test_intersection_sets():\n    # Test ConditionSet intersection\n    x = symbols('x')\n    a = ConditionSet(x, x > 0, S.Integers)\n    b = ConditionSet(x, x < 5, S.Integers)\n    assert intersection_sets(a, b) == ConditionSet(x, And(x > 0, x < 5), S.Integers)\n    \n    # Test Naturals and Integers intersection\n    assert intersection_sets(S.Naturals, S.Integers) == S.Naturals\n\n    # Test Naturals and Naturals intersection\n    assert intersection_sets(S.Naturals, S.Naturals) == S.Naturals\n\n    # Test Interval and Naturals intersection\n    assert intersection_sets(Interval(0, 10), S.Naturals) == Range(1, 11)\n\n    # Test ComplexRegion intersection in rectangular form\n    rect1 = ComplexRegion(Interval(-1, 1) * Interval(-1, 1))\n    rect2 = ComplexRegion(Interval(0, 2) * Interval(0, 2))\n    assert intersection_sets(rect1, rect2) == ComplexRegion(Interval(0, 1) * Interval(0, 1))\n\n    # Test ComplexRegion intersection in polar form\n    polar1 = ComplexRegion(Interval(0, 1) * Interval(0, 2*pi), polar=True)\n    polar2 = ComplexRegion(Interval(0, 0.5) * Interval(pi/2, pi), polar=True)\n    assert intersection_sets(polar1, polar2) == ComplexRegion(Interval(0, 0.5) * Interval(pi/2, pi), polar=True)\n\n    # Test Integers and Reals intersection\n    assert intersection_sets(S.Integers, S.Reals) == S.Integers\n\n    # Test Range and Interval intersection\n    assert intersection_sets(Range(0, 10), Interval(2, 8)) == Range(2, 9)\n\n    # Test Range and Naturals intersection\n    assert intersection_sets(Range(0, 10), S.Naturals) == Range(1, 10)\n\n    # Test Range and Range intersection\n    assert intersection_sets(Range(0, 10, 2), Range(1, 10, 2)) == FiniteSet()\n\n    # Test Integers and Rationals intersection\n    assert intersection", "def test_intersection_sets():\n    # Test various intersections to cover `intersection_sets`\n    assert intersection_sets(S.Naturals, S.Integers) == S.Naturals\n    assert intersection_sets(S.Naturals, S.Naturals) == S.Naturals\n    assert intersection_sets(Interval(1, 5), S.Naturals) == Range(1, 6)\n    assert intersection_sets(Interval(1, 5, True, True), S.Naturals) == Range(2, 5)\n    assert intersection_sets(ComplexRegion(Interval(0, 1) * Interval(0, 2 * pi), polar=True), S.Reals) == Interval(0, 1)\n    assert intersection_sets(S.Integers, S.Rationals) == S.Integers\n    assert intersection_sets(S.Naturals, S.Rationals) == S.Naturals\n    assert intersection_sets(S.Rationals, S.Reals) == S.Rationals\n    assert intersection_sets(FiniteSet(1, 2, 3), FiniteSet(2, 3, 4)) == FiniteSet(2, 3)\n    assert intersection_sets(FiniteSet(1, 2, 3), Interval(1.5, 2.5)) == FiniteSet(2)\n    assert intersection_sets(ProductSet(Interval(1, 2), Interval(3, 4)), ProductSet(Interval(1.5, 2), Interval(3.5, 4))) == ProductSet(Interval(1.5, 2), Interval(3.5, 4))\n\n    # Test edge cases\n    assert intersection_sets(S.EmptySet, S.Integers) == S.EmptySet\n    assert intersection_sets(S.EmptySet, S.Naturals) == S.EmptySet\n    assert intersection_sets(UniversalSet(), S.Integers) == S.Integers\n    assert intersection_sets(UniversalSet(), S.Naturals) == S.Naturals\n", "def test_intersection_sets():\n    # Test intersection of ConditionSet with ConditionSet\n    x = symbols('x')\n    cs1 = ConditionSet(x, x > 0, S.Integers)\n    cs2 = ConditionSet(x, x < 5, S.Integers)\n    assert intersection_sets(cs1, cs2) == ConditionSet(x, And(x > 0, x < 5), S.Integers)\n\n    # Test intersection of ConditionSet with Interval\n    interval = Interval(1, 10)\n    assert intersection_sets(cs1, interval) == ConditionSet(x, x > 0, Intersection(S.Integers, interval))\n\n    # Test intersection of ConditionSet with EmptySet\n    assert intersection_sets(cs1, S.EmptySet) == S.EmptySet\n\n    # Test intersection of Naturals with Integers\n    assert intersection_sets(S.Naturals, S.Integers) == S.Naturals\n\n    # Test intersection of Interval with Naturals\n    interval_naturals = Interval(3, 15)\n    assert intersection_sets(interval_naturals, S.Naturals) == Intersection(S.Naturals, interval_naturals)\n\n    # Test intersection of ComplexRegion with ComplexRegion\n    complex_reg1 = ComplexRegion(Interval(0, 1) * Interval(0, 2 * pi), polar=True)\n    complex_reg2 = ComplexRegion(Interval(0, 0.5) * Interval(0, pi), polar=True)\n    assert intersection_sets(complex_reg1, complex_reg2) == ComplexRegion(Intersection(Interval(0, 1), Interval(0, 0.5)) * Intersection(Interval(0, 2 * pi), Interval(0, pi)), polar=True)\n\n    # Test intersection of ProductSet with ProductSet\n    ps1 = ProductSet(Interval(0, 1), Interval(0, 1))\n    ps2 = ProductSet(Interval(0.5, 1.5), Interval(0.5, 1.5))\n    assert intersection_sets(ps1, ps2) == ProductSet(Interval(0.5, 1), Interval(0.5, 1))\n\n    # Test intersection of FiniteSet with FiniteSet\n    fs1 = FiniteSet(1, 2, 3)\n    fs2 = FiniteSet(3, 4, 5)\n    assert intersection_sets", "def test_issue_12345():\n    # Testing intersection of ComplexRegion and Interval in polar form\n    r1 = Interval(0, 3)\n    theta1 = Interval(0, S.Pi)\n    c1 = ComplexRegion(r1*theta1, polar=True)\n    assert c1.intersect(Interval(1, 2)) == Interval(1, 2)\n    assert c1.intersect(Interval(3, 4)) == FiniteSet(3)\n    assert c1.intersect(Interval(4, 5)) is S.EmptySet\n\n    # Testing intersection of ComplexRegion and Interval in rectangular form\n    x1 = Interval(-2, 2)\n    y1 = Interval(-2, 2)\n    c2 = ComplexRegion(x1*y1)\n    assert c2.intersect(Interval(-1, 1)) == Interval(-1, 1)\n    assert c2.intersect(Interval(2, 3)) == FiniteSet(2)\n    assert c2.intersect(Interval(3, 4)) is S.EmptySet\n\n    # Testing ComplexRegion intersection with Set\n    s = FiniteSet(1 + I, 2 + 2*I, 3 + 3*I)\n    c3 = ComplexRegion(Interval(0, 4)*Interval(0, 4))\n    assert c3.intersect(s) == FiniteSet(1 + I, 2 + 2*I, 3 + 3*I)\n    assert c3.intersect(FiniteSet(5 + 5*I)) is S.EmptySet\n\n    # Testing ComplexRegion intersection with another ComplexRegion in polar form\n    r2 = Interval(1, 4)\n    theta2 = Interval(S.Pi/2, S.Pi)\n    c4 = ComplexRegion(r2*theta2, polar=True)\n    assert c1.intersect(c4) == ComplexRegion(Interval(1, 3)*Interval(S.Pi/2, S.Pi), polar=True)\n", "def test_intersection_sets():\n    x, y, z = symbols('x y z')\n\n    # Test intersection of ConditionSet and Set\n    condition_set = ConditionSet(x, x**2 - 1, S.Reals)\n    regular_set = Interval(-1, 1)\n    result = intersection_sets(condition_set, regular_set)\n    assert result == ConditionSet(x, x**2 - 1, Interval(-1, 1))\n\n    # Test intersection of Naturals and Integers\n    naturals = S.Naturals\n    integers = S.Integers\n    result = intersection_sets(naturals, integers)\n    assert result == S.Naturals\n\n    # Test intersection of ComplexRegion in rectangular form\n    complex_region1 = ComplexRegion(Interval(0, 1) * Interval(0, 2*S.Pi), polar=False)\n    complex_region2 = ComplexRegion(Interval(0, 2) * Interval(0, S.Pi), polar=False)\n    result = intersection_sets(complex_region1, complex_region2)\n    assert result == ComplexRegion(Interval(0, 1) * Interval(0, S.Pi))\n\n    # Test intersection of Range and Interval\n    range_set = Range(0, 10, 2)\n    interval_set = Interval(3, 8)\n    result = intersection_sets(range_set, interval_set)\n    assert result == Range(4, 9, 2)\n\n    # Test intersection of ProductSet\n    product_set1 = ProductSet(Interval(0, 1), Interval(2, 3))\n    product_set2 = ProductSet(Interval(0, 2), Interval(1, 3))\n    result = intersection_sets(product_set1, product_set2)\n    assert result == ProductSet(Interval(0, 1), Interval(2, 3))\n"], "sample_1161": ["def test_ExprCondPair():\n    from sympy import ExprCondPair\n    a, b, c = symbols('a b c')\n    expr_cond_pair = ExprCondPair(a + b, c > 0)\n    assert str(expr_cond_pair) == \"(a + b, c > 0)\"\n    assert sstr(expr_cond_pair) == \"(a + b, c > 0)\"\n", "def test_issue_12345():\n    class CustomExpr(Expr):\n            return \"Custom(%s)\" % printer._print(self.args[0])\n\n    a = Symbol('a')\n    b = Symbol('b')\n    expr = CustomExpr(a + b)\n    assert sstr(expr) == \"Custom(a + b)\"\n", "def test_ConditionSet():\n    from sympy import ConditionSet, Symbol, Eq, S\n    from sympy.sets.sets import UniversalSet\n\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    cs1 = ConditionSet(x, x > 0, UniversalSet)\n    assert str(cs1) == 'ConditionSet(x, x > 0)'\n\n    cs2 = ConditionSet((x, y), Eq(x**2 + y**2, 1), S.Reals)\n    assert str(cs2) == 'ConditionSet((x, y), Eq(x**2 + y**2, 1), Reals)'\n\n    cs3 = ConditionSet(x, Eq(x**2 - 2, 0))\n    assert str(cs3) == 'ConditionSet(x, Eq(x**2 - 2, 0))'\n\n    cs4 = ConditionSet((x, y, z), Eq(x*y + z, 1), S.Reals)\n    assert str(cs4) == 'ConditionSet((x, y, z), Eq(x*y + z, 1), Reals)'\n", "def test_WildFunction():\n    w = WildFunction('w')\n    f = Function('f')\n    expr = w(x) + f(x)\n    assert str(expr) == \"w_(x) + f(x)\"\n    assert str(w(x, y)) == \"w_(x, y)\"\n    expr2 = w(x) * f(y)\n    assert str(expr2) == \"w_(x)*f(y)\"\n    expr3 = w(x)**2 + w(x, y)\n    assert str(expr3) == \"w_(x)**2 + w_(x, y)\"\n    expr4 = w(x) / f(x)\n    assert str(expr4) == \"w_(x)/f(x)\"\n", "def test__print_ConditionSet():\n    from sympy import ConditionSet, Eq, Interval\n    A = Symbol('A')\n    B = Symbol('B')\n    C = Symbol('C')\n    cond = Eq(A**2, B + C)\n    base_set = Interval(-1, 1)\n    cs = ConditionSet(A, cond, base_set)\n    assert str(cs) == 'ConditionSet(A, Eq(A**2, B + C), Interval(-1, 1))'\n    cs_universal = ConditionSet(A, cond, S.UniversalSet)\n    assert str(cs_universal) == 'ConditionSet(A, Eq(A**2, B + C))'\n"], "sample_1162": ["def test_Function_kind():\n    from sympy import Function\n    f = Function('f')\n    assert f.kind is UndefinedKind\n    assert f(comm_x).kind is UndefinedKind\n    assert f(noncomm_x).kind is UndefinedKind\n\n    # Test defined function kinds\n    from sympy import sin\n    assert sin.kind is UndefinedKind\n    assert sin(comm_x).kind is NumberKind\n    assert sin(noncomm_x).kind is UndefinedKind\n", "def test_Lambda_kind():\n    assert Lambda(comm_x, comm_x + 1).kind is NumberKind\n    assert Lambda(noncomm_x, noncomm_x + 1).kind is UndefinedKind\n    assert Lambda((comm_x, noncomm_x), comm_x + noncomm_x).kind is UndefinedKind\n", "def test_FunctionClass_nargs():\n    class MyFunctionClass(FunctionClass):\n        @classmethod\n            return x**2\n\n    assert MyFunctionClass.nargs == FiniteSet(1)\n    assert MyFunctionClass(\"x\").nargs == FiniteSet(1)\n\n    # Test with multiple arguments\n    class MyFunctionClassMulti(FunctionClass):\n        @classmethod\n            return x * y\n\n    MyFunctionClassMulti._nargs = (1, 2)\n    assert MyFunctionClassMulti.nargs == FiniteSet(1, 2)\n    assert MyFunctionClassMulti(\"x\", \"y\").nargs == FiniteSet(1, 2)\n\n    # Test with invalid nargs\n    try:\n        class InvalidFunctionClass(FunctionClass):\n            @classmethod\n                return x\n        InvalidFunctionClass._nargs = ()\n    except ValueError as e:\n        assert \"Incorrectly specified nargs\" in str(e)\n", "def test_Function_kind():\n    from sympy.core.function import Function\n    f = Function('f')\n    assert f.kind is UndefinedKind\n    assert f(comm_x).kind is NumberKind\n    assert f(noncomm_x).kind is UndefinedKind\n", "def test_Function_kind():\n    from sympy import Function\n    f = Function('f')\n    g = Function('g', commutative=False)\n    assert f.kind is UndefinedKind\n    assert g.kind is UndefinedKind\n    assert f(comm_x).kind is NumberKind\n    assert g(noncomm_x).kind is UndefinedKind\n"], "sample_1163": ["def test_polar_lift():\n    from sympy import polar_lift, I, pi, exp_polar\n\n    # Test basic functionality\n    assert polar_lift(1) == 1 * exp_polar(0)\n    assert polar_lift(-1) == 1 * exp_polar(I * pi)\n    assert polar_lift(I) == exp_polar(I * pi / 2)\n    assert polar_lift(-I) == exp_polar(-I * pi / 2)\n\n    # Test lifting of expressions\n    x = Symbol('x')\n    assert polar_lift(4 * x) == 4 * polar_lift(x)\n    assert polar_lift(4 * exp_polar(I * pi)) == 4 * exp_polar(I * pi)\n\n    # Test commutativity with other functions\n    assert polar_lift(4 * x + I) == polar_lift(4 * x + I)\n    assert polar_lift(exp(x + I * pi)) == exp(x + I * pi)\n\n    # Test nested polar lift\n    assert polar_lift(polar_lift(4 * x)) == polar_lift(4 * x)\n\n    # Test non-number inputs\n    assert polar_lift(Symbol('a')) == polar_lift(Symbol('a'))\n\n    # Ensure no lifting for positive terms\n    p = Symbol('p', positive=True)\n    assert polar_lift(4 * p) == 4 * p\n\n    # Ensure correct lifting for negative and non-positive terms\n    n = Symbol('n', negative=True)\n    assert polar_lift(4 * n) == 4 * polar_lift(n)\n    np = Symbol('np', nonpositive=True)\n    assert polar_lift(4 * np) == 4 * polar_lift(np)\n\n    # Test with complex expressions\n    expr = 2 + 3 * I\n    assert polar_lift(expr) == (2 + 3 * I) * exp_polar(0)\n\n    # Check combinations with other complex functions\n    assert polar_lift(2 * x + I * x) == polar_lift(2 * x + I * x)\n\n    # Nested expressions involving polar lift\n    assert polar_lift(x * polar_lift(1 + I)) == x * polar_lift(1 + I)\n", "def test_conjugate_properties():\n    x = Symbol('x', imaginary=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', complex=True)\n\n    # Test conjugate for various kinds of symbols\n    assert conjugate(x) == -x\n    assert conjugate(y) == y\n    assert conjugate(z) == conjugate(z)\n\n    # Test conjugate for combinations of symbols\n    assert conjugate(x + y) == -x + y\n    assert conjugate(x * y) == -x * y\n    assert conjugate(x * z) == -x * conjugate(z)\n    assert conjugate(y * z) == y * conjugate(z)\n    assert conjugate(x + z) == -x + conjugate(z)\n\n    # Test for conjugate of functions\n    f = Function('f')\n    assert conjugate(f(x)) == conjugate(f(x))\n    assert conjugate(f(y)) == f(y)\n    assert conjugate(f(z)) == conjugate(f(z))\n", "def test_issue_24678():\n    from sympy import polar_lift, principal_branch, periodic_argument, oo\n\n    x = Symbol('x')\n    p = Symbol('p', positive=True)\n\n    # test periodic_argument with principal_branch and polar_lift\n    assert periodic_argument(principal_branch(x, p), p) == periodic_argument(x, p)\n    assert periodic_argument(polar_lift(x), p) == periodic_argument(x, p)\n    assert periodic_argument(principal_branch(polar_lift(x), p), p) == periodic_argument(polar_lift(x), p)\n\n    # test principal_branch with polar_lift\n    assert principal_branch(polar_lift(2 + I), 2*pi) == 2 + I\n    assert principal_branch(polar_lift(2 + I), 3*pi) == 2 + I\n\n    # test edge cases for periodic_argument\n    assert periodic_argument(exp_polar(2*I*pi), 2*pi) == 0\n    assert periodic_argument(exp_polar(-3*I*pi), 2*pi) == -pi\n    assert periodic_argument(exp_polar(4*I*pi), 2*pi) == 0\n    assert periodic_argument(exp_polar(0), 2*pi) == 0\n\n    # test principal_branch with periodic_argument\n    assert principal_branch(periodic_argument(x, p), p) == x\n", "def test_re_and_im_with_complex_expression():\n    x, y = symbols('x y', real=True)\n    z = x + y*I\n\n    # Test re and im with more complex expressions involving algebraic operations\n    expr1 = (2 + I) * (3 + 4*I)\n    expr2 = (x + I*y) * (2 + 3*I)\n    \n    # Test re and im functions on these expressions\n    assert re(expr1) == re((2 + I) * (3 + 4*I))\n    assert im(expr1) == im((2 + I) * (3 + 4*I))\n    \n    assert re(expr2).expand(complex=True) == (2*x - 3*y)\n    assert im(expr2).expand(complex=True) == (2*y + 3*x)\n\n    # Check that re and im maintain functionality after complex expansions\n    expr3 = (x**2 + y**2*I).expand(complex=True)\n    assert re(expr3) == re(x**2) + re(y**2*I)\n    assert im(expr3) == im(x**2) + im(y**2*I)\n", "def test_conjugate_matrix():\n    n, m = symbols('n m')\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    assert conjugate(A) == adjoint(A)\n    assert conjugate(A + B) == conjugate(A) + conjugate(B)\n    assert conjugate(A * B.T) == conjugate(A) * conjugate(B.T)\n    \n    C = Matrix([[1 + I, 2 - 3*I], [4, -I]])\n    assert conjugate(C) == Matrix([[1 - I, 2 + 3*I], [4, I]])\n    \n    D = ImmutableMatrix([[I, -I], [1 + I, 2 - I]])\n    assert conjugate(D) == ImmutableMatrix([[-I, I], [1 - I, 2 + I]])\n    \n    E = SparseMatrix([[0, I], [-I, 1]])\n    assert conjugate(E) == SparseMatrix([[0, -I], [I, 1]])\n    \n    F = FunctionMatrix(3, 3, Lambda((n, m), I*n + m))\n    assert conjugate(F) == FunctionMatrix(3, 3, Lambda((n, m), -I*n + m))\n"], "sample_1164": ["def test_wigner6j():\n    w6j = Wigner6j(2, 3, 4, 5, 6, 7)\n    assert str(w6j) == 'Wigner6j(2, 3, 4, 5, 6, 7)'\n    ascii_str = \\", "def test_wigner_symbols_doit():\n    # Testing the `doit` method for Wigner symbols with numerical arguments\n    w3j = Wigner3j(1, 1, 1, -1, 1, 0)\n    assert w3j.doit() == -sqrt(2)/3\n\n    w6j = Wigner6j(1, 1, 1, 1, 1, 1)\n    assert w6j.doit() == Rational(1, 3)\n\n    w9j = Wigner9j(1, 1, 1, 1, 1, 1, 1, 1, 1)\n    assert w9j.doit() == Rational(1, 18)\n\n    # Testing the `doit` method for symbolic arguments raises ValueError\n    ws = Wigner3j(Symbol('j1'), Symbol('m1'), Symbol('j2'), Symbol('m2'), Symbol('j3'), Symbol('m3'))\n    try:\n        ws.doit()\n    except ValueError as e:\n        assert str(e) == 'Coefficients must be numerical'\n    else:\n        assert False, \"Expected ValueError\"\n", "def test_wigner6j():\n    wigner6j = Wigner6j(2, 1, 3, 2, 1, 3)\n    wigner6j_symbolic = Wigner6j(Symbol('j1'), Symbol('j2'), Symbol('j12'), Symbol('j3'), Symbol('j'), Symbol('j23'))\n    \n    assert str(wigner6j) == 'Wigner6j(2, 1, 3, 2, 1, 3)'\n    ascii_str = \\", "def test_wigner_symbols_evaluation():\n    # Test numerical evaluation for Wigner symbols\n    w3j = Wigner3j(1, 0, 1, 0, 2, 0)\n    assert w3j.doit() == sqrt(5)/5\n\n    w6j = Wigner6j(1, 1, 1, 1, 1, 1)\n    assert w6j.doit() == -1/6\n\n    w9j = Wigner9j(1, 1, 1, 1, 1, 1, 1, 1, 1)\n    assert w9j.doit() == Rational(-1, 18)\n\n    cg = CG(1, 1, 1, -1, 1, 0)\n    assert cg.doit() == 1/sqrt(2)\n\n    # Test symbolic instances\n    j1, j2, j3 = symbols('j1 j2 j3')\n    w3j_sym = Wigner3j(j1, 0, j2, 0, j3, 0)\n    try:\n        w3j_sym.doit()\n    except ValueError as e:\n        assert str(e) == \"Coefficients must be numerical\"\n\n    cg_sym = CG(j1, j1, j2, -j2, j3, 0)\n    try:\n        cg_sym.doit()\n    except ValueError as e:\n        assert str(e) == \"Coefficients must be numerical\"\n", "def test_cg_doit():\n    # Testing the `doit` method for CG and Wigner symbols\n    cg = CG(3/2, 1/2, 1/2, -1/2, 1, 0)\n    wigner3j = Wigner3j(2, 1, 1, -1, 1, 0)\n    wigner6j = Wigner6j(2, 3, 4, 2, 3, 1)\n    wigner9j = Wigner9j(1, 1, 1, 1, 1, 1, 1, 1, 1)\n\n    assert cg.doit() == sqrt(2)/2\n    assert wigner3j.doit() == sqrt(6)/6\n    assert wigner6j.doit() == -Rational(1, 18)\n    assert wigner9j.doit() == Rational(1, 6)\n"], "sample_1165": ["def test_quaternion_subtraction():\n    q1 = Quaternion(5, 6, 7, 8)\n    q2 = Quaternion(1, 2, 3, 4)\n    q3 = Quaternion(3, 2, 1, 0)\n    \n    assert q1 - q2 == Quaternion(4, 4, 4, 4)\n    assert q2 - q1 == Quaternion(-4, -4, -4, -4)\n    assert q1 - 2 == Quaternion(3, 6, 7, 8)\n    assert 2 - q1 == Quaternion(-3, -6, -7, -8)\n    assert q1 - q3 == Quaternion(2, 4, 6, 8)\n    assert q3 - q1 == Quaternion(-2, -4, -6, -8)\n", "def test_quaternion_properties():\n    # Test property getters\n    q = Quaternion(1, 2, 3, 4)\n    assert q.a == 1\n    assert q.b == 2\n    assert q.c == 3\n    assert q.d == 4\n    assert q.real_field is True\n\n    # Test with real_field set to False\n    q2 = Quaternion(1, 2, 3, 4, real_field=False)\n    assert q2.real_field is False\n\n    # Ensure proper exception raising for non-commutative symbols\n    nc = Symbol('nc', commutative=False)\n    raises(ValueError, lambda: Quaternion(1, 2, nc, 4))\n", "def test_quaternion_static_methods():\n    # Testing Quaternion.from_axis_angle\n    axis = (1, 0, 0)\n    angle = pi / 2\n    q = Quaternion.from_axis_angle(axis, angle)\n    assert q == Quaternion(cos(pi/4), sin(pi/4), 0, 0)\n    \n    axis = (0, 1, 0)\n    q = Quaternion.from_axis_angle(axis, angle)\n    assert q == Quaternion(cos(pi/4), 0, sin(pi/4), 0)\n\n    axis = (0, 0, 1)\n    q = Quaternion.from_axis_angle(axis, angle)\n    assert q == Quaternion(cos(pi/4), 0, 0, sin(pi/4))\n    \n    # Testing Quaternion.from_rotation_matrix\n    M = Matrix([[1, 0, 0], [0, 0, -1], [0, 1, 0]])\n    q = Quaternion.from_rotation_matrix(M)\n    assert q == Quaternion(sqrt(2)/2, sqrt(2)/2, 0, 0)\n    \n    M = Matrix([[0, 0, 1], [0, 1, 0], [-1, 0, 0]])\n    q = Quaternion.from_rotation_matrix(M)\n    assert q == Quaternion(sqrt(2)/2, 0, sqrt(2)/2, 0)\n    \n    M = Matrix([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n    q = Quaternion.from_rotation_matrix(M)\n    assert q == Quaternion(sqrt(2)/2, 0, 0, sqrt(2)/2)\n", "def test_quaternion_exceptions():\n    # Test for exception when norm is zero for inverse\n    q_zero_norm = Quaternion(0, 0, 0, 0)\n    raises(ValueError, lambda: q_zero_norm.inverse())\n\n    # Test for exception when a non-commutative symbol is used in multiplication\n    q = Quaternion(1, 2, 3, 4)\n    nc_symbol = symbols('nc', commutative=False)\n    raises(ValueError, lambda: q * nc_symbol)\n    raises(ValueError, lambda: nc_symbol * q)\n\n    # Test for exception when a non-commutative symbol is used in addition\n    raises(ValueError, lambda: q + nc_symbol)\n\n    # Test for exception when arguments are not commutative in Quaternion instantiation\n    raises(ValueError, lambda: Quaternion(1, nc_symbol, 3, 4))\n", "def test_quaternion_inverse():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(0, 0, 0, 0)\n    q3 = Quaternion(5, -2, 7, -6)\n\n    # Test inverse calculation for a non-zero quaternion\n    assert q1.inverse() == Quaternion(1, -2, -3, -4) / 30\n    assert q3.inverse() == Quaternion(5, 2, -7, 6) / 114\n\n    # Ensure ValueError is raised for quaternion with zero norm\n    raises(ValueError, lambda: q2.inverse())\n"], "sample_1166": ["def test_monomial_deg():\n    assert monomial_deg((1, 2, 3)) == 6\n    assert monomial_deg((0, 0, 0)) == 0\n    assert monomial_deg((4, 5, 6)) == 15\n    assert monomial_deg((2, 3, 0)) == 5\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (3, 4, 1)) == (0, 0, 0)\n    assert monomial_ldiv((3, 4, 1), (4, 5, 2)) == (-1, -1, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (3, 4, 1)) == (0, 0, 0)\n    assert monomial_ldiv((3, 4, 1), (4, 2, 1)) == (-1, 2, 0)\n", "def test_MonomialOps():\n    ops = MonomialOps(3)\n\n    mul = ops.mul()\n    assert mul((1, 2, 3), (4, 5, 6)) == (5, 7, 9)\n\n    pow_op = ops.pow()\n    assert pow_op((1, 2, 3), 2) == (2, 4, 6)\n\n    mulpow = ops.mulpow()\n    assert mulpow((1, 2, 3), (4, 5, 6), 2) == (9, 12, 15)\n\n    ldiv = ops.ldiv()\n    assert ldiv((4, 5, 6), (1, 2, 3)) == (3, 3, 3)\n\n    div = ops.div()\n    assert div((4, 5, 6), (1, 2, 3)) == (3, 3, 3)\n    assert div((1, 2, 3), (4, 5, 6)) is None\n\n    lcm = ops.lcm()\n    assert lcm((1, 2, 3), (4, 1, 5)) == (4, 2, 5)\n\n    gcd = ops.gcd()\n    assert gcd((4, 5, 6), (1, 5, 3)) == (1, 5, 3)\n"], "sample_1167": ["def test_issue_19587():\n    # Test for correct handling of fractions and powers\n    assert latex((x**2 + 1)/(x + 1)) == r\"\\frac{x^{2} + 1}{x + 1}\"\n    assert latex(1/(x*(x + 1))) == r\"\\frac{1}{x \\left(x + 1\\right)}\"\n    assert latex((x + 1)**2/(x**2 + x + 1)) == r\"\\frac{\\left(x + 1\\right)^{2}}{x^{2} + x + 1}\"\n    assert latex((x + 1)**(S(3)/2)/(x**2 + x + 1)) == r\"\\frac{\\left(x + 1\\right)^{\\frac{3}{2}}}{x^{2} + x + 1}\"\n    assert latex(1/(x**2 + x + 1)) == r\"\\frac{1}{x^{2} + x + 1}\"\n    assert latex(1/(x**(S(3)/2) + x + 1)) == r\"\\frac{1}{x^{\\frac{3}{2}} + x + 1}\"\n", "def test_latex_Integral_with_assumptions():\n    # Integral with assumptions\n    from sympy.assumptions.assume import Q\n    integral = Integral(x, (x, 0, 1))\n    assert latex(integral, symbol_names={Q.real(x): r\"\\text{real}\\ x\"}) == \\\n        r\"\\int\\limits_{0}^{1} \\text{real}\\ x\\, d\\left(\\text{real}\\ x\\right)\"\n", "def test_latex_ImmutableDenseNDimArray():\n    # Tests for ImmutableDenseNDimArray\n    from sympy.tensor.array import ImmutableDenseNDimArray\n    assert latex(ImmutableDenseNDimArray([1, 2, 3])) == r'\\left[\\begin{matrix}1 & 2 & 3\\end{matrix}\\right]'\n    assert latex(ImmutableDenseNDimArray([[1, 2], [3, 4]])) == r'\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]'\n    assert latex(ImmutableDenseNDimArray([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])) == r'\\left[\\begin{matrix}\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right] & \\left[\\begin{matrix}5 & 6\\\\7 & 8\\end{matrix}\\right]\\end{matrix}\\right]'\n", "def test_latex_derivatives_with_symbols():\n    f = Function('f')\n    g = Function('g')\n    u = symbols('u')\n    v = symbols('v')\n    w = symbols('w')\n    x_star = Symbol('x^*')\n\n    assert latex(diff(f(x_star), x_star, 2)) == r'\\frac{d^{2}}{d \\left(x^{*}\\right)^{2}} f{\\left(x^{*} \\right)}'\n    assert latex(diff(f(u*v), u)) == r'\\frac{\\partial}{\\partial u} f{\\left(u v \\right)}'\n    assert latex(diff(f(u*v*w), u)) == r'\\frac{\\partial}{\\partial u} f{\\left(u v w \\right)}'\n    assert latex(diff(g(u*v*w), v)) == r'\\frac{\\partial}{\\partial v} g{\\left(u v w \\right)}'\n    assert latex(diff(f(u*v*w), u, v)) == r'\\frac{\\partial^{2}}{\\partial v \\partial u} f{\\left(u v w \\right)}'\n    assert latex(diff(f(u*v*w), u, v, w)) == r'\\frac{\\partial^{3}}{\\partial w \\partial v \\partial u} f{\\left(u v w \\right)}'\n", "def test_latex_MatAdd():\n    # Create MatrixSymbols for testing\n    X = MatrixSymbol('X', 3, 3)\n    Y = MatrixSymbol('Y', 3, 3)\n    Z = MatrixSymbol('Z', 3, 3)\n\n    # Test simple addition\n    assert latex(X + Y) == r'X + Y'\n    \n    # Test multiple additions\n    assert latex(X + Y + Z) == r'X + Y + Z'\n\n    # Test addition with negative sign\n    assert latex(X - Y) == r'X - Y'\n\n    # Test addition with scalar multiplication\n    assert latex(2*X + Y) == r'2 X + Y'\n\n    # Test addition within an expression\n    assert latex(X + 2*Y - Z) == r'X + 2 Y - Z'\n\n    # Test addition in a nested expression\n    expr = (X + Y)*(Z - X) + Y*(Z + X)\n    assert latex(expr) == r'\\left(X + Y\\right) \\left(Z - X\\right) + Y \\left(Z + X\\right)'\n\n    # Test addition with Transpose\n    assert latex(X + Transpose(Y)) == r'X + Y^{T}'\n    assert latex(X - Transpose(Y)) == r'X - Y^{T}'\n\n    # Test addition with Adjoint\n    assert latex(X + Adjoint(Y)) == r'X + Y^{\\dagger}'\n    assert latex(X - Adjoint(Y)) == r'X - Y^{\\dagger}'\n\n    # Test addition with inverse\n    assert latex(X + Inverse(Y)) == r'X + Y^{-1}'\n    assert latex(X - Inverse(Y)) == r'X - Y^{-1}'\n"], "sample_1168": ["def test_interactive_traversal():\n    from sympy import sin\n    expr = sin(x + y)\n        inputs = iter(inputs)\n        return lambda prompt: next(inputs)\n    interactive_traversal.__globals__['input'] = simulate_user_input('0', 'd')\n    result = interactive_traversal(expr)\n    assert result == y\n    interactive_traversal.__globals__['input'] = input  # Restore original input\n", "def test_filter_symbols():\n    iterator = iter(symbols('x0:10'))\n    exclude = symbols('x1 x3 x5')\n    filtered = list(filter_symbols(iterator, exclude))\n    assert filtered == list(symbols('x0 x2 x4 x6 x7 x8 x9'))\n", "def test_interactive_traversal():\n    expr = Piecewise((x, x < 1), (x**2, True))\n        inputs = iter(['0', '1', 'f', '0', 'l', '0', 'r', 'd'])\n        return next(inputs)\n    \n    from sympy.utilities.iterables import interactive_traversal\n    import builtins\n    original_input = builtins.input\n    builtins.input = mock_input\n\n    result = interactive_traversal(expr)\n    assert result == expr.args[0][0]\n\n    result = interactive_traversal(expr.args[0][1])\n    assert result == expr.args[0][1]\n\n    builtins.input = original_input\n", "def test_interactive_traversal():\n    from sympy import Add, Mul, symbols\n    from sympy.abc import x, y, z\n\n    expr = x + y * z\n        inputs = iter(['1', '0', 'd'])\n        return lambda: next(inputs)\n    \n    # Monkey patch input function\n    import builtins\n    old_input = builtins.input\n    builtins.input = user_input()\n\n    try:\n        result = interactive_traversal(expr)\n        assert result == y\n    finally:\n        # Restore the original input function\n        builtins.input = old_input\n", "def test_interactive_traversal():\n    from sympy import Add, Mul\n    from sympy.core.compatibility import StringIO\n    import sys\n\n    expr = Add(Mul(x, y), z)\n    \n        responses = {\n            \"Your choice [0-1,f,l,r,d,?]: \": \"0\",\n            \"Your choice [0-1,f,l,r,d,?]: \": \"d\"\n        }\n        return responses[prompt]\n\n    old_stdin = sys.stdin\n    sys.stdin = StringIO()\n    sys.stdin.read = simulate_input\n\n    result = interactive_traversal(expr)\n    assert result == x*y\n\n    sys.stdin = old_stdin\n"], "sample_1169": ["def test_boson_apply_operator():\n    n, m = symbols(\"n,m\")\n    ket = BKet([n, m])\n    annihilate_op = B(0)\n    create_op = Bd(1)\n    \n    # Test annihilation operator\n    assert annihilate_op.apply_operator(ket) == sqrt(n) * BKet([n - 1, m])\n    \n    # Test creation operator\n    assert create_op.apply_operator(ket) == sqrt(m + 1) * BKet([n, m + 1])\n    \n    # Test symbolic state\n    symbolic_ket = BKet([n])\n    assert annihilate_op.apply_operator(symbolic_ket) == annihilate_op * symbolic_ket\n    assert create_op.apply_operator(symbolic_ket) == create_op * symbolic_ket\n", "def test_fermionic_operator_properties():\n    i, j, n, m = symbols('i,j,n,m')\n    op = AnnihilateFermion(i)\n    assert op.is_restricted == -1\n    assert op.is_above_fermi == False\n    assert op.is_below_fermi == True\n    assert op.is_only_below_fermi == True\n    assert op.is_only_above_fermi == False\n    assert op.is_q_creator == -1\n    assert op.is_q_annihilator == 0\n    assert op.is_only_q_creator == True\n    assert op.is_only_q_annihilator == False\n\n    op = CreateFermion(i)\n    assert op.is_restricted == -1\n    assert op.is_above_fermi == False\n    assert op.is_below_fermi == True\n    assert op.is_only_below_fermi == True\n    assert op.is_only_above_fermi == False\n    assert op.is_q_creator == 0\n    assert op.is_q_annihilator == -1\n    assert op.is_only_q_creator == False\n    assert op.is_only_q_annihilator == True\n\n    op = AnnihilateFermion(n)\n    assert op.is_restricted == 1\n    assert op.is_above_fermi == True\n    assert op.is_below_fermi == False\n    assert op.is_only_below_fermi == False\n    assert op.is_only_above_fermi == True\n    assert op.is_q_creator == 0\n    assert op.is_q_annihilator == 1\n    assert op.is_only_q_creator == False\n    assert op.is_only_q_annihilator == True\n\n    op = CreateFermion(n)\n    assert op.is_restricted == 1\n    assert op.is_above_fermi == True\n    assert op.is_below_fermi == False\n    assert op.is_only_below_fermi == False\n    assert op.is_only_above_fermi == True\n    assert op.is_q_creator == 1\n    assert op.is_q_annihilator == 0\n    assert op.is_only_q_creator == True\n    assert op.is_only_q_annihilator == False\n", "def test_FermionicOperator_restricted():\n    a = Symbol('a', above_fermi=True)\n    i = Symbol('i', below_fermi=True)\n    p = Symbol('p')\n\n    assert F(a).is_restricted == 1\n    assert Fd(a).is_restricted == 1\n    assert F(i).is_restricted == -1\n    assert Fd(i).is_restricted == -1\n    assert F(p).is_restricted == 0\n    assert Fd(p).is_restricted == 0\n\n    assert F(a).is_above_fermi is True\n    assert F(i).is_above_fermi is False\n    assert F(p).is_above_fermi is True\n\n    assert F(a).is_below_fermi is False\n    assert F(i).is_below_fermi is True\n    assert F(p).is_below_fermi is True\n\n    assert F(a).is_only_below_fermi is False\n    assert F(i).is_only_below_fermi is True\n    assert F(p).is_only_below_fermi is False\n\n    assert F(a).is_only_above_fermi is True\n    assert F(i).is_only_above_fermi is False\n    assert F(p).is_only_above_fermi is False\n", "def test_FockStateFermion():\n    # Test creation of FockStateFermionKet and FockStateFermionBra\n    a, i = symbols('a i', above_fermi=True)\n    fock_ket = FockStateFermionKet([a, i])\n    fock_bra = FockStateFermionBra([a, i])\n    assert str(fock_ket) == \"|[a, -i]>\"\n    assert str(fock_bra) == \"<[a, -i]|\"\n    assert repr(fock_ket) == \"FockStateKet((a, i))\"\n    assert repr(fock_bra) == \"FockStateBra((a, i))\"\n    assert fock_ket.up(a) == FockStateFermionKet([a, a, i])\n    assert fock_ket.down(i) == FockStateFermionKet([a])\n    assert fock_bra.up(a) == FockStateFermionBra([a, a, i])\n    assert fock_bra.down(i) == FockStateFermionBra([a])\n", "def test_secondquantizationerror():\n    with raises(SecondQuantizationError):\n        raise SecondQuantizationError(\"This is a general second quantization error\")\n\n    with raises(AppliesOnlyToSymbolicIndex):\n        raise AppliesOnlyToSymbolicIndex(\"This applies only to symbolic index\")\n\n    with raises(ContractionAppliesOnlyToFermions):\n        raise ContractionAppliesOnlyToFermions(\"This contraction applies only to fermions\")\n\n    with raises(ViolationOfPauliPrinciple):\n        raise ViolationOfPauliPrinciple(\"This is a violation of Pauli principle\")\n\n    with raises(SubstitutionOfAmbigousOperatorFailed):\n        raise SubstitutionOfAmbigousOperatorFailed(\"Substitution of ambiguous operator failed\")\n\n    with raises(WicksTheoremDoesNotApply):\n        raise WicksTheoremDoesNotApply(\"Wick's theorem does not apply\")\n"], "sample_1170": ["def test_Inverse():\n    from sympy import Matrix\n    M = Matrix([[1, 2], [3, 4]])\n    assert str(M.inv()) == \"Matrix([[-2, 1], [3/2, -1/2]])\"\n    assert str(1 / M) == \"Matrix([[-2, 1], [3/2, -1/2]])\"\n    assert str(M**(-1)) == \"Matrix([[-2, 1], [3/2, -1/2]])\"\n    A = MatrixSymbol('A', 2, 2)\n    assert str(Inverse(A)) == \"A**(-1)\"\n", "def test_ImaginaryUnit_custom_printer():\n    class CustomPrinter(StrPrinter):\n            return \"j\"\n    assert CustomPrinter().doprint(I) == \"j\"\n", "def test_Exp1():\n    assert str(E) == \"E\"\n    assert str(exp(1)) == \"E\"\n    assert str(exp(x)) == \"exp(x)\"\n    assert str(E**x) == \"E**x\"\n", "def test_ComplexRootOf():\n    from sympy import CRootOf, Poly\n    p = Poly(x**5 + 2*x - 1)\n    assert str(CRootOf(p, 0)) == \"CRootOf(x**5 + 2*x - 1, 0)\"\n    assert str(CRootOf(p, 1)) == \"CRootOf(x**5 + 2*x - 1, 1)\"\n    assert str(CRootOf(p, 2)) == \"CRootOf(x**5 + 2*x - 1, 2)\"\n    assert str(CRootOf(p, 3)) == \"CRootOf(x**5 + 2*x - 1, 3)\"\n    assert str(CRootOf(p, 4)) == \"CRootOf(x**5 + 2*x - 1, 4)\"\n", "def test_Quaternion_applyfunc():\n    q = Quaternion(x, y, z, t)\n    q_sin = q.applyfunc(sin)\n    assert str(q_sin) == \"sin(x) + sin(y)*i + sin(z)*j + sin(t)*k\"\n    q_exp = q.applyfunc(exp)\n    assert str(q_exp) == \"exp(x) + exp(y)*i + exp(z)*j + exp(t)*k\"\n"], "sample_1171": ["def test_Rationals_properties():\n    R = S.Rationals\n    assert R.is_iterable == True\n    assert R.is_empty == False\n    assert R.is_finite_set == False\n    assert R._inf == S.NegativeInfinity\n    assert R._sup == S.Infinity\n\n    # Boundary property\n    assert R._boundary == S.Reals\n\n    # Test for some common rational numbers\n    assert S.Half in R\n    assert Rational(2, 3) in R\n    assert Rational(-7, 8) in R\n\n    # Test non-rational numbers\n    assert pi not in R\n    assert I not in R\n\n    # Ensure that non-Expr instances are not in Rationals\n    assert Basic() not in R\n\n    # Check the iterative generation of rational numbers\n    gen = iter(R)\n    expected_rationals = [0, 1, -1, 1/2, 2, -1/2, -2, 1/3, 3, -1/3, -3, 2/3]\n    for expected in expected_rationals:\n        assert next(gen) == expected\n", "def test_rationals_contains():\n    R = S.Rationals\n    assert R.contains(2) == True\n    assert R.contains(S.Half) == True\n    assert R.contains(0.5) == Contains(0.5, R, evaluate=False)\n    assert R.contains(sin(x)) == Contains(sin(x), R)\n    assert R.contains(oo) == False\n    assert R.contains(-oo) == False\n    assert R.contains(I) == False\n    assert R.contains(Rational(1, 3)) == True\n    assert R.contains(Rational(-1, 2)) == True\n    assert R.contains(1.1) == Contains(1.1, R, evaluate=False)\n    assert R.contains(log(2)) == Contains(log(2), R)\n", "def test_rationals_contains():\n    R = S.Rationals\n    assert R._contains(1) == True\n    assert R._contains(S.Half) == True\n    assert R._contains(S.Pi) == False\n    assert R._contains(I) == False\n    assert R._contains(oo) == False\n    assert R._contains(-oo) == False\n    assert R._contains(\"string\") == False\n", "def test_Rationals_boundary():\n    R = S.Rationals\n    assert R.boundary == S.Reals\n    assert R.closure == S.Reals\n    assert R.is_open == False\n    assert R.is_closed == False\n", "def test_Rationals_as_relational():\n    r = Symbol('r')\n    assert S.Rationals.as_relational(r) == And(Eq(floor(r), r), r.is_rational)\n"], "sample_1172": ["def test_solve_generic():\n    from sympy.polys import Options\n\n    f_1 = Poly(x**2 + y**2 - 1, x, y)\n    f_2 = Poly(x - y, x, y)\n    opt = Options((x, y), {'domain': 'ZZ'})\n    \n    assert solve_generic([f_1, f_2], opt) == [(Rational(-1, 2), Rational(-1, 2)), (Rational(1, 2), Rational(1, 2))]\n    \n    f_3 = Poly(x**2 + y**2 - 4, x, y)\n    f_4 = Poly(x + y - 1, x, y)\n    opt = Options((x, y), {'domain': 'ZZ'})\n\n    assert solve_generic([f_3, f_4], opt) == [(-3, 4), (1, 0)]\n\n    f_5 = Poly(x**3 + y**2 - 4, x, y)\n    f_6 = Poly(x + y - 1, x, y)\n    opt = Options((x, y), {'domain': 'ZZ'})\n\n    assert solve_generic([f_5, f_6], opt) == [(-1, 2), (1, 0)]\n\n    # Testing failure with non-zero-dimensional systems\n    f_7 = Poly(x**2 + y**2, x, y)\n    f_8 = Poly(x - 2*y, x, y)\n    opt = Options((x, y), {'domain': 'ZZ'})\n    raises(NotImplementedError, lambda: solve_generic([f_7, f_8], opt))\n", "def test_solve_generic():\n    from sympy.polys.polytools import Options\n\n    # Examples from the solve_generic function docstring\n    NewOption = Options((x, y), {'domain': 'ZZ'})\n\n    a = Poly(x - y + 5, x, y, domain='ZZ')\n    b = Poly(x + y - 3, x, y, domain='ZZ')\n    assert solve_generic([a, b], NewOption) == [(-1, 4)]\n\n    a = Poly(x - 2*y + 5, x, y, domain='ZZ')\n    b = Poly(2*x - y - 3, x, y, domain='ZZ')\n    assert solve_generic([a, b], NewOption) == [(11/3, 13/3)]\n\n    a = Poly(x**2 + y, x, y, domain='ZZ')\n    b = Poly(x + y*4, x, y, domain='ZZ')\n    assert solve_generic([a, b], NewOption) == [(0, 0), (1/4, -1/16)]\n\n    # Additional tests to improve coverage\n    a = Poly(x**3 + y**2, x, y, domain='ZZ')\n    b = Poly(x + y, x, y, domain='ZZ')\n    raises(NotImplementedError, lambda: solve_generic([a, b], NewOption))\n\n    a = Poly(x**2 + y**2 + z, x, y, z, domain='ZZ')\n    b = Poly(x + y + z, x, y, z, domain='ZZ')\n    c = Poly(x - y, x, y, z, domain='ZZ')\n    NewOption = Options((x, y, z), {'domain': 'ZZ'})\n    result = solve_generic([a, b, c], NewOption)\n    assert result == [(-S(1)/2, -S(1)/2, 1)]\n\n    a = Poly(x**2 + y**2 - 1, x, y, domain='ZZ')\n    b = Poly(x*y - 1/2, x, y, domain='QQ')\n    NewOption = Options((x, y), {'domain': 'QQ'})\n    result = solve_generic([a, b], NewOption)\n    assert result == [(-sqrt(2)/2, -sqrt(2)), (sqrt(2)/2, sqrt", "def test_solve_generic():\n    from sympy.polys.polytools import Options\n\n    f_1 = x**2 + y\n    f_2 = x + y**2\n\n    opt = Options((x, y), {'domain': 'ZZ'})\n    assert solve_generic([Poly(f_1), Poly(f_2)], opt) == [(0, 0), (1/4, -1/16)]\n\n    f_1 = x**2 - y**2 - 1\n    f_2 = x - y\n\n    assert solve_generic([Poly(f_1), Poly(f_2)], opt) == [(1/sqrt(2), 1/sqrt(2)), (-1/sqrt(2), -1/sqrt(2))]\n\n    f_1 = x**2 + y**2 - 1\n    f_2 = x**2 - y**2\n\n    assert solve_generic([Poly(f_1), Poly(f_2)], opt) == [(1, 0), (-1, 0), (0, 1), (0, -1)]\n    \n    f_1 = x**2 + y - 1\n    f_2 = x**2 + y**2 - 1\n    \n    assert solve_generic([Poly(f_1), Poly(f_2)], opt) == [(0, 1)]\n", "def test_solve_triangulated_complex():\n    f_1 = x**2 + y + z - I\n    f_2 = x + y**2 + z - I\n    f_3 = x + y + z**2 - I\n\n    a, b = sqrt(2*I) - I, -sqrt(2*I) - I\n\n    assert solve_triangulated([f_1, f_2, f_3], x, y, z) == \\\n        [(0, 0, I), (0, I, 0), (I, 0, 0)]\n\n    dom = QQ.algebraic_field(sqrt(2*I))\n\n    assert solve_triangulated([f_1, f_2, f_3], x, y, z, domain=dom) == \\\n        [(0, 0, I), (0, I, 0), (I, 0, 0), (a, a, a), (b, b, b)]\n", "def test_solve_generic():\n    from sympy.polys import Options\n\n    NewOption = Options((x, y), {'domain': 'ZZ'})\n\n    # Simple linear system\n    assert solve_generic([Poly(x - 2, x, y), Poly(y - 3, x, y)], NewOption) == [(2, 3)]\n\n    # System with no real solutions\n    assert solve_generic([Poly(x**2 + 1, x, y), Poly(y - 3, x, y)], NewOption) == []\n\n    # Quadratic system with two variables\n    assert solve_generic([Poly(x**2 - y, x, y), Poly(x - y, x, y)], NewOption) == [(0, 0), (1, 1)]\n\n    # System with dependent equations\n    assert solve_generic([Poly(x + y - 1, x, y), Poly(2*x + 2*y - 2, x, y)], NewOption) == [(1 - t, t) for t in [0, 1]]\n\n    # System with more equations than variables\n    raises(NotImplementedError, lambda: solve_generic([Poly(x + y - 1, x, y), Poly(2*x + 2*y - 2, x, y), Poly(x - 1, x, y)], NewOption))\n\n    # Non-zero-dimensional system\n    raises(NotImplementedError, lambda: solve_generic([Poly(x + y, x, y)], NewOption))\n"], "sample_1173": ["def test_auto_symbol_unexpected_keyword():\n    # Test that unexpected keywords are not converted to symbols or functions\n    inputs = {\n        'True': True,\n        'False': False,\n        'None': None,\n        'if': 'if',\n        'else': 'else'\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text) == result\n", "def test_implicit_multiplication_application():\n    transformations = standard_transformations + (implicit_multiplication_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    assert parse_expr(\"10sin**2 x**2 + 3xyz + tan theta\", transformations=transformations) == 10*sin(x**2)**2 + 3*x*y*z + Function('tan')(Symbol('theta'))\n    assert parse_expr(\"2xy\", transformations=transformations) == 2*x*y\n    assert parse_expr(\"3x + 4y\", transformations=transformations) == 3*x + 4*y\n    assert parse_expr(\"x(x + y)\", transformations=transformations) == x*(x + y)\n    assert parse_expr(\"(x + y)z\", transformations=transformations) == (x + y)*z\n    assert parse_expr(\"sin x cos y\", transformations=transformations) == sin(x)*cos(y)\n", "def test_auto_number():\n    transformations = standard_transformations\n    inputs = {\n        '123': Integer(123),\n        '123.456': Float(123.456),\n        '1.23e4': Float(1.23e4),\n        '3+4j': 3 + 4*I,\n        '0.12345678901234567890': Float('0.12345678901234567890')\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    assert parse_expr('lambda x: x + 1', transformations=transformations) == Function('Lambda')((x,), x + 1)\n    assert parse_expr('lambda x, y: x + y', transformations=transformations) == Function('Lambda')((x, Symbol('y')), x + Symbol('y'))\n    raises(TokenError, lambda: parse_expr('lambda *x: x', transformations=transformations))  # Starred arguments not supported\n", "def test_lambda_notation():\n    local_dict = {}\n    transformations = standard_transformations + (lambda_notation,)\n    assert parse_expr('lambda x: x + 2', transformations=transformations) == Function('Lambda')((Symbol('x'),), Symbol('x') + 2)\n    assert parse_expr('lambda x, y: x + y', transformations=transformations) == Function('Lambda')((Symbol('x'), Symbol('y')), Symbol('x') + Symbol('y'))\n    raises(TokenError, lambda: parse_expr('lambda *args: args', transformations=transformations))\n"], "sample_1174": ["def test_polar_lift():\n    assert polar_lift(1) == 1 * exp_polar(0)\n    assert polar_lift(-1) == 1 * exp_polar(I * pi)\n    assert polar_lift(I) == exp_polar(I * pi / 2)\n    assert polar_lift(-I) == exp_polar(-I * pi / 2)\n    assert polar_lift(2 + 3*I) == 2 + 3*I\n    assert polar_lift(0) == 0\n    assert polar_lift(4*I) == 4 * exp_polar(I * pi / 2)\n    assert polar_lift(-4*I) == 4 * exp_polar(-I * pi / 2)\n    assert polar_lift(4 * exp_polar(I * pi)) == 4 * exp_polar(I * pi)\n    assert polar_lift(4 * polar_lift(I)) == 4 * exp_polar(I * pi / 2)\n\n    x = Symbol('x')\n    assert polar_lift(x).is_polar\n    assert polar_lift(4*x) == 4 * polar_lift(x)\n    assert polar_lift(4*polar_lift(x)) == 4 * polar_lift(x)\n    \n    # Test evaluation with expressions containing multiple terms\n    assert polar_lift(x + I) == polar_lift(x + I)\n    assert polar_lift(x * (1 + I)) == polar_lift(x) * polar_lift(1 + I)\n    assert polar_lift(1 + I*x) == polar_lift(1 + I*x)\n", "def test_principal_branch_simplify():\n    from sympy import principal_branch, simplify\n    p = Symbol('p', positive=True)\n    neg = Symbol('neg', negative=True)\n\n    assert simplify(principal_branch(polar_lift(neg), 2*pi)) == polar_lift(neg)\n    assert simplify(principal_branch(exp_polar(-I*pi/2)*polar_lift(p), 2*pi)) == exp_polar(-I*pi/2)*polar_lift(p)\n    assert simplify(principal_branch(exp_polar(I*pi/2)/polar_lift(p), 2*pi)) == exp_polar(I*pi/2)/polar_lift(p)\n    assert simplify(principal_branch(exp_polar(3*I*pi/2)*polar_lift(neg), 2*pi)) == exp_polar(-I*pi/2)*polar_lift(neg)\n", "def test_conjugate_derivative():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    f = Function('f')\n\n    # Test conjugate of derivative for real variable\n    assert conjugate(Derivative(f(x), x)) == Derivative(conjugate(f(x)), x)\n    \n    # Test conjugate of derivative for imaginary variable\n    assert conjugate(Derivative(f(y), y)) == -Derivative(conjugate(f(y)), y)\n    \n    # Ensure conjugation and differentiation commute\n    assert (conjugate(f(x)).diff(x)) == (conjugate(f(x).diff(x)))\n    assert (conjugate(f(y)).diff(y)) == -(conjugate(f(y).diff(y)))\n", "def test_unpolarify_edge_cases():\n    from sympy import sqrt\n    x = Symbol('x')\n    \n    # Test unpolarify with sqrt of polar numbers\n    assert unpolarify(sqrt(polar_lift(4))) == sqrt(4)\n    assert unpolarify(sqrt(polar_lift(2 + 3*I))) == sqrt(2 + 3*I)\n\n    # Test nested unpolarify with multiple polar_lift\n    expr = polar_lift(polar_lift(2 + I))\n    assert unpolarify(expr) == 2 + I\n\n    # Test unpolarify with polar lifted expressions within functions\n    expr = sin(polar_lift(2*I))\n    assert unpolarify(expr) == sin(2*I)\n\n    # Test unpolarify with exponential functions\n    expr = exp(polar_lift(2 + I))\n    assert unpolarify(expr) == exp(2 + I)\n\n    # Test unpolarify with polar lifted constants\n    expr = polar_lift(pi)\n    assert unpolarify(expr) == pi\n\n    # Test unpolarify with complex conjugate\n    expr = conjugate(polar_lift(2 + 3*I))\n    assert unpolarify(expr) == conjugate(2 + 3*I)\n\n    # Edge case where argument is zero\n    expr = polar_lift(0)\n    assert unpolarify(expr) == 0\n", "def test_issue_2868():\n    x, y = symbols('x y')\n    assert re(x**y).expand(complex=True) == re(x**re(y) * exp(-im(y) * log(x)) * exp(S.ImaginaryUnit * im(y) * log(x)))\n    assert im(x**y).expand(complex=True) == im(x**re(y) * exp(-im(y) * log(x)) * exp(S.ImaginaryUnit * im(y) * log(x)))\n"], "sample_1175": ["def test_issue_20271():\n    from sympy import Sum, Function\n    f = Function('f')\n    n = Symbol('n')\n    assert pretty(Sum(f(n), (n, 1, 10))) == '  10   \\n  ___  \\n  \\\\  ` \\n   \\\\   \\n   )  f(n)\\n   /    \\n  /___, \\n n = 1 '\n    assert upretty(Sum(f(n), (n, 1, 10))) == '  10   \\n  ___  \\n  \u2572    \\n   \u2572   \\n   \u2571  f(n)\\n  \u2571    \\n  \u203e\u203e\u203e\u203e  \\n n = 1 '\n", "def test_pretty_Modulo_with_Negative_Numbers():\n    from sympy.core import Mod\n    expr = Mod(x, -7)\n    ascii_str = \"x mod -7\"\n    ucode_str = \"x mod -7\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mod(-x, 7)\n    ascii_str = \"-x mod 7\"\n    ucode_str = \"-x mod 7\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mod(-x, -7)\n    ascii_str = \"-x mod -7\"\n    ucode_str = \"-x mod -7\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = Mod(x - 1, -7)\n    ascii_str = \"(x - 1) mod -7\"\n    ucode_str = \"(x - 1) mod -7\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n", "def test_pretty_Integral_with_condition():\n    from sympy import Piecewise\n\n    expr = Integral(Piecewise((sin(x), x < 1), (cos(x), x >= 1)), (x, 0, pi))\n    ascii_str = \\", "def test_pretty_Prime():\n    from sympy.core.numbers import Prime\n    p = Prime(5)\n    assert pretty(p) == '5th prime'\n    assert upretty(p) == '5th prime'\n\n    p = Prime(10)\n    assert pretty(p) == '10th prime'\n    assert upretty(p) == '10th prime'\n\n    p = Prime(100)\n    assert pretty(p) == '100th prime'\n    assert upretty(p) == '100th prime'\n", "def test_pretty_noncommutative_expressions():\n    A, B, C = symbols('A B C', commutative=False)\n\n    expr = A*B + B*A\n    ascii_str = \"A*B + B*A\"\n    ucode_str = \"A\u22c5B + B\u22c5A\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = A*B - B*A\n    ascii_str = \"A*B - B*A\"\n    ucode_str = \"A\u22c5B - B\u22c5A\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = A*B*C - C*B*A\n    ascii_str = \"A*B*C - C*B*A\"\n    ucode_str = \"A\u22c5B\u22c5C - C\u22c5B\u22c5A\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = A*B*C + C*B*A\n    ascii_str = \"A*B*C + C*B*A\"\n    ucode_str = \"A\u22c5B\u22c5C + C\u22c5B\u22c5A\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = A*(B + C)\n    ascii_str = \"A*(B + C)\"\n    ucode_str = \"A\u22c5(B + C)\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n\n    expr = (A + B)*C\n    ascii_str = \"(A + B)*C\"\n    ucode_str = \"(A + B)\u22c5C\"\n    assert pretty(expr) == ascii_str\n    assert upretty(expr) == ucode_str\n"], "sample_1176": ["def test_complex_nan():\n    # Test operations involving NaN and complex numbers\n    assert (S.NaN + I) is S.NaN\n    assert (I + S.NaN) is S.NaN\n    assert (S.NaN * I) is S.NaN\n    assert (I * S.NaN) is S.NaN\n    assert (S.NaN / I) is S.NaN\n    assert (I / S.NaN) is S.NaN\n    assert (S.NaN - I) is S.NaN\n    assert (I - S.NaN) is S.NaN\n    assert (S.NaN**I) is S.NaN\n    assert (I**S.NaN) is S.NaN\n", "def test_mpf_norm_special():\n    # Testing special mpf values that should not be altered\n    from mpmath.libmp.libmpf import finf, fninf, fnan\n\n    # These values should be returned as is\n    assert mpf_norm(finf, 53) == finf\n    assert mpf_norm(fninf, 53) == fninf\n    assert mpf_norm(fnan, 53) == fnan\n", "def test_mpf_norm_edge_cases():\n    # Cases where mpf_norm should return exactly zero\n    assert mpf_norm((0, 0, 0, 0), 53) == (0, 0, 0, 0)\n    assert mpf_norm((1, 0, 1, 0), 53) == (0, 0, 0, 0)\n    \n    # Cases where mpf_norm should return normalized values\n    assert mpf_norm((0, 1, 0, 1), 53) == (0, 1, 0, 1)\n    assert mpf_norm((1, 1, 0, 1), 53) == (1, 1, 0, 1)\n\n    # Cases with infinities and NaN\n    assert mpf_norm((0, 0, 0, 0), 53) == (0, 0, 0, 0)  # should be zero\n    assert mpf_norm((0, 0, 1, 0), 53) == (0, 0, 0, 0)  # should be zero\n    assert mpf_norm((0, 0, -123, -1), 53) == (0, 0, 0, 0)  # should be zero\n    assert mpf_norm((0, 0, -456, -2), 53) == (0, 0, 0, 0)  # should be zero\n    assert mpf_norm((1, 0, -789, -3), 53) == (0, 0, 0, 0)  # should be zero\n\n    # Normalizing actual values\n    assert mpf_norm((0, 123456, 0, 17), 53) == (0, 123456, 0, 17)\n    assert mpf_norm((1, 123456, 0, 17), 53) == (1, 123456, 0, 17)\n\n    # Cases of normalization with precision adjustment\n    assert mpf_norm((0, 123456, 0, 17), 10) == (0, 123456, 0, 17)\n    assert mpf_norm((1, 123456, 0, 17),", "def test_mpf_norm_special_cases():\n    from mpmath.libmp.libmpf import finf as mpf_inf, fninf as mpf_ninf, fnan as mpf_nan, fzero, _normalize\n    from sympy.core.numbers import mpf_norm\n\n    # Test normalization of special mpf values\n    assert mpf_norm(mpf_inf, 53) == mpf_inf\n    assert mpf_norm(mpf_ninf, 53) == mpf_ninf\n    assert mpf_norm(mpf_nan, 53) == mpf_nan\n\n    # Test normalization of zero with zero mantissa\n    assert mpf_norm((0, 0, 0, 0), 53) == fzero\n    assert mpf_norm((0, 0, 1, 0), 53) == fzero\n\n    # Ensure that normalization does not alter already normalized values\n    normalized_mpf = (0, 1, -3, 4)  # An example normalized mpf value\n    assert mpf_norm(normalized_mpf, 53) == normalized_mpf\n", "def test_mpf_norm():\n    from sympy.mpmath.libmp.libmpf import _normalize\n    import sympy.mpmath.libmp as mlib\n    rnd = mlib.round_nearest\n\n    # Test normalization of zero\n    assert _normalize((0, 0, 0, 0), 53, rnd) == (0, 0, 0, 0)\n\n    # Test normalization of a positive number\n    assert _normalize((0, 8, -2, 3), 53, rnd) == (0, 2, 0, 2)\n\n    # Test normalization of a negative number\n    assert _normalize((1, 8, -2, 3), 53, rnd) == (1, 2, 0, 2)\n\n    # Test normalization of a large positive number\n    assert _normalize((0, 1234567890123456, 10, 53), 53, rnd) == (0, 1234567890123456, 10, 53)\n\n    # Test normalization of a large negative number\n    assert _normalize((1, 1234567890123456, 10, 53), 53, rnd) == (1, 1234567890123456, 10, 53)\n"], "sample_1177": ["def test_polar_lift():\n    a = Symbol('a', real=True)\n    b = Symbol('b', positive=True)\n    c = Symbol('c', negative=True)\n\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n\n    assert polar_lift(4*a) == 4*polar_lift(a)\n    assert polar_lift(4*b) == 4*b\n    assert polar_lift(4*c) == 4*polar_lift(c)\n\n    x = Symbol('x')\n    assert polar_lift(x).evalf() == polar_lift(x)\n    assert polar_lift(x*y).evalf() == polar_lift(x*y)\n    assert polar_lift(a + I*b).evalf() == polar_lift(a + I*b)\n    assert polar_lift(c + I*b).evalf() == polar_lift(c + I*b)\n\n    assert polar_lift(exp(I*pi/4)) == exp_polar(I*pi/4)\n    assert polar_lift(exp(I*pi/3)).expand() == exp_polar(I*pi/3)\n", "def test_polar_lift():\n    assert polar_lift(1) == 1 * exp_polar(0)\n    assert polar_lift(-1) == 1 * exp_polar(I * pi)\n    assert polar_lift(I) == exp_polar(I * pi / 2)\n    assert polar_lift(-I) == exp_polar(-I * pi / 2)\n    assert polar_lift(1 + I) == polar_lift(1 + I)\n    assert polar_lift(0) == 0\n    assert polar_lift(2 * exp_polar(I * pi / 3)) == 2 * exp_polar(I * pi / 3)\n\n    x = Symbol('x')\n    assert polar_lift(x).is_polar\n    p = Symbol('p', polar=True)\n    assert polar_lift(p) == p\n    assert polar_lift(4 * p) == 4 * p\n    assert polar_lift(4 * x) == 4 * polar_lift(x)\n    assert polar_lift(x * y).func == Mul\n    assert polar_lift(x * y).args == (x, y)\n    assert polar_lift(exp_polar(I * pi)) == exp_polar(I * pi)\n", "def test_sign_with_nested_expressions():\n    x, y = symbols('x y', real=True)\n    expr1 = sign(x + I*y)\n    assert expr1.doit() == sign(x + I*y)\n    assert expr1.rewrite(Piecewise) == Piecewise(\n        (0, Eq(x + I*y, 0)),\n        ((x + I*y)/Abs(x + I*y), True)\n    )\n\n    expr2 = sign(x**2 - y**2 + I*x*y)\n    assert expr2.doit() == sign((x - y)*(x + y) + I*x*y)\n    assert expr2.rewrite(Piecewise) == Piecewise(\n        (0, Eq((x - y)*(x + y) + I*x*y, 0)),\n        (((x - y)*(x + y) + I*x*y)/Abs((x - y)*(x + y) + I*x*y), True)\n    )\n\n    # Test nested sign expressions\n    nested_expr = sign(sign(x + y*I))\n    assert nested_expr.doit() == sign(sign(x + y*I))\n    assert nested_expr.rewrite(Piecewise) == Piecewise(\n        (0, Eq(sign(x + y*I), 0)),\n        ((sign(x + y*I))/Abs(sign(x + y*I)), True)\n    )\n", "def test_polar_lift():\n    from sympy.functions.elementary.complexes import polar_lift\n    x, y = symbols('x y')\n\n    assert polar_lift(2) == 2 * exp_polar(0)\n    assert polar_lift(-2) == 2 * exp_polar(I * pi)\n    assert polar_lift(I) == exp_polar(I * pi / 2)\n    assert polar_lift(-I) == exp_polar(-I * pi / 2)\n    assert polar_lift(2 + I) == polar_lift(2 + I)\n    assert polar_lift(2 * x) == 2 * polar_lift(x)\n    assert polar_lift(x * y) == polar_lift(x) * polar_lift(y)\n    \n    m = Matrix([[1, 2], [3, 4]])\n    assert polar_lift(m) == Matrix([[polar_lift(1), polar_lift(2)], [polar_lift(3), polar_lift(4)]])\n\n    expr = polar_lift(x) + polar_lift(y)\n    subs_dict = {x: 3, y: 4}\n    assert expr.subs(subs_dict) == polar_lift(3) + polar_lift(4)\n\n    assert polar_lift(0) == 0\n    assert polar_lift(0) == polar_lift(0)\n", "def test_polar_lift():\n    from sympy.functions.elementary.complexes import polar_lift\n    \n    x = Symbol('x')\n    y = Symbol('y', positive=True)\n    p = polar_lift(2 + I)\n    \n    assert polar_lift(2) == 2*exp_polar(0)\n    assert polar_lift(-2) == 2*exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(2*I) == 2*exp_polar(I*pi/2)\n    assert polar_lift(2 + I) == p\n    assert polar_lift(2*y) == 2*y*exp_polar(0)\n    assert polar_lift(2*y + I*y) == y*polar_lift(2 + I)\n    assert polar_lift(x).is_polar\n    assert polar_lift(1 + x).is_polar is None  # because x is not real/imaginary\n    assert polar_lift(exp_polar(I*pi)) == exp_polar(I*pi)\n    assert polar_lift(exp_polar(2*I*pi)) == exp_polar(2*I*pi)\n    assert polar_lift(4*x).subs(x, 2 + I) == 4*p\n"], "sample_1178": ["def test_BreakToken_and_ContinueToken():\n    assert break_ == BreakToken()\n    assert continue_ == ContinueToken()\n    assert break_ != continue_\n    assert break_.func(*break_.args) == break_\n    assert continue_.func(*continue_.args) == continue_\n\n    # Testing code generation for break and continue tokens\n    from sympy import ccode, fcode\n    assert ccode(break_) == 'break'\n    assert ccode(continue_) == 'continue'\n    assert fcode(break_, source_format='free') == 'exit'\n    assert fcode(continue_, source_format='free') == 'cycle'\n", "def test_BreakToken_and_ContinueToken():\n    from sympy.codegen.ast import break_, continue_\n    assert repr(break_) == 'BreakToken()'\n    assert repr(continue_) == 'ContinueToken()'\n\n    # Ensure that multiple instances are identical (since they are singletons)\n    assert break_ is BreakToken()\n    assert continue_ is ContinueToken()\n\n    # Ensure these tokens can be used in a code block without errors\n    c = CodeBlock(break_, continue_)\n    assert repr(c) == 'CodeBlock(\\n' + \\\n                      '    BreakToken(),\\n' + \\\n                      '    ContinueToken()\\n' + \\\n                      ')'\n", "def test_BreakToken_ContinueToken():\n    from sympy.codegen.ast import break_, continue_\n    from sympy.codegen.ast import BreakToken, ContinueToken\n\n    assert isinstance(break_, BreakToken)\n    assert isinstance(continue_, ContinueToken)\n    assert break_ == BreakToken()\n    assert continue_ == ContinueToken()\n    \n    # Check string representations\n    assert str(break_) == \"BreakToken()\"\n    assert str(continue_) == \"ContinueToken()\"\n    \n    # Check that repr returns the correct string\n    assert repr(break_) == \"BreakToken()\"\n    assert repr(continue_) == \"ContinueToken()\"\n    \n    # Check that equality works\n    assert break_ == break_\n    assert continue_ == continue_\n    assert break_ != continue_\n    \n    # Check that kwargs and func work correctly\n    assert break_.kwargs() == {}\n    assert continue_.kwargs() == {}\n    assert break_.func(*break_.args) == break_\n    assert continue_.func(*continue_.args) == continue_\n", "def test_Stream():\n    out = Stream('out')\n    err = Stream('err')\n    assert out.name == String('out')\n    assert err.name == String('err')\n    assert out != err\n    assert out.func(*out.args) == out\n    assert out != stdout\n    assert err != stderr\n    assert stdout.name == String('stdout')\n    assert stderr.name == String('stderr')\n", "def test_BreakToken():\n    brk = BreakToken()\n    assert brk.func(*brk.args) == brk\n    assert brk == break_\n    assert str(brk) == 'BreakToken()'\n    assert repr(brk) == 'BreakToken()'\n    "], "sample_1179": ["def test_ImaginaryUnit_with_symbols():\n    # Test _print_ImaginaryUnit with more complex expressions\n    expr = I * (x + y)\n    assert str(expr) == \"I*(x + y)\"\n    expr = I * (x - y)\n    assert str(expr) == \"I*(x - y)\"\n    expr = I * (x * y)\n    assert str(expr) == \"I*x*y\"\n", "def test_Boolean():\n    assert str(true) == \"True\"\n    assert str(false) == \"False\"\n    assert str(true & false) == \"False\"\n    assert str(true | false) == \"True\"\n    assert str(~true) == \"False\"\n    assert str(~false) == \"True\"\n    assert str(true & true) == \"True\"\n    assert str(false | false) == \"False\"\n", "def test_UnevaluatedExpr_with_function():\n    f = Function(\"f\")\n    expr = UnevaluatedExpr(f(x + y))\n    assert str(expr) == \"f(x + y)\"\n    expr = UnevaluatedExpr(f(x + y) + z)\n    assert str(expr) == \"f(x + y) + z\"\n    expr = UnevaluatedExpr(f(x + y) * z)\n    assert str(expr) == \"f(x + y)*z\"\n", "def test_Interval_open():\n    assert str(Interval(0, 1, False, False)) == 'Interval(0, 1)'\n    assert str(Interval(0, 1, True, False)) == 'Interval.open(0, 1)'\n    assert str(Interval(0, 1, False, True)) == 'Interval.Ropen(0, 1)'\n    assert str(Interval(0, 1, True, True)) == 'Interval.Lopen(0, 1)'\n", "def test_Interval_edge_cases():\n    # Test interval with both endpoints being infinity\n    assert str(Interval(S.NegativeInfinity, S.Infinity)) == 'Interval(-oo, oo)'\n    # Test interval with one endpoint as infinity\n    assert str(Interval(S.NegativeInfinity, 0)) == 'Interval(-oo, 0]'\n    assert str(Interval(0, S.Infinity)) == 'Interval(0, oo)'\n    # Test open interval\n    assert str(Interval(0, 1, False, False)) == 'Interval.open(0, 1)'\n    # Test half-open intervals\n    assert str(Interval(0, 1, True, False)) == 'Interval.Lopen(0, 1)'\n    assert str(Interval(0, 1, False, True)) == 'Interval.Ropen(0, 1)'\n    # Test interval with equal endpoints\n    assert str(Interval(1, 1)) == 'Interval(1, 1)'\n"], "sample_1180": ["def test_midpoint():\n    # Test midpoint between two Point2D instances\n    p1 = Point2D(1, 2)\n    p2 = Point2D(3, 4)\n    assert p1.midpoint(p2) == Point2D(2, 3)\n\n    # Test midpoint between two Point3D instances\n    p3 = Point3D(1, 2, 3)\n    p4 = Point3D(3, 4, 5)\n    assert p3.midpoint(p4) == Point3D(2, 3, 4)\n\n    # Test midpoint between Point2D and tuple\n    p5 = (4, 6)\n    assert p1.midpoint(p5) == Point2D(2.5, 4)\n\n    # Test midpoint between Point3D and list\n    p6 = [4, 6, 8]\n    assert p3.midpoint(p6) == Point3D(2.5, 4, 5.5)\n\n    # Ensure midpoint raises error for different dimensions\n    with warns(UserWarning):\n        raises(ValueError, lambda: p1.midpoint(Point3D(1, 2, 3)))\n", "def test_point_operators():\n    # Ensure Point operators work correctly\n    p1 = Point(1, 2)\n    p2 = Point(3, 4)\n    assert p1 + p2 == Point(4, 6)\n    assert p1 - p2 == Point(-2, -2)\n    assert p1 * 2 == Point(2, 4)\n    assert p1 / 2 == Point(0.5, 1)\n    assert -p1 == Point(-1, -2)\n\n    # Ensure Point2D operators work correctly\n    p3 = Point2D(1, 2)\n    p4 = Point2D(3, 4)\n    assert p3 + p4 == Point2D(4, 6)\n    assert p3 - p4 == Point2D(-2, -2)\n    assert p3 * 2 == Point2D(2, 4)\n    assert p3 / 2 == Point2D(0.5, 1)\n    assert -p3 == Point2D(-1, -2)\n\n    # Ensure Point3D operators work correctly\n    p5 = Point3D(1, 2, 3)\n    p6 = Point3D(4, 5, 6)\n    assert p5 + p6 == Point3D(5, 7, 9)\n    assert p5 - p6 == Point3D(-3, -3, -3)\n    assert p5 * 2 == Point3D(2, 4, 6)\n    assert p5 / 2 == Point3D(0.5, 1, 1.5)\n    assert -p5 == Point3D(-1, -2, -3)\n", "def test_point_properties():\n    p = Point(3, 4)\n    assert p.is_Point\n    assert p.length == 0\n    assert abs(p) == 5\n    assert p.unit == Point(Rational(3, 5), Rational(4, 5))\n    assert p.midpoint(Point(0, 0)) == Point(3/2, 2)\n    assert p.orthogonal_direction == Point(-4, 3)\n    assert p.coordinates == (3, 4)\n    assert p.origin == Point(0, 0)\n\n    p3 = Point(1, 2, 3)\n    assert p3.ambient_dimension == 3\n    assert p3.midpoint(Point(4, 5, 6)) == Point(5/2, 7/2, 9/2)\n    assert p3.orthogonal_direction == Point(-2, 1, 0)\n    assert p3.coordinates == (1, 2, 3)\n    assert p3.origin == Point(0, 0, 0)\n\n    assert Point(1, 1).is_zero is False\n    assert Point(0, 0).is_zero is True\n    assert Point(1, 1).is_nonzero is True\n    assert Point(0, 0).is_nonzero is False\n", "def test_point3D_direction_ratio():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 6, 8)\n    assert p1.direction_ratio(p2) == [3, 4, 5]\n    assert p2.direction_ratio(p1) == [-3, -4, -5]\n\n    p3 = Point3D(0, 0, 0)\n    p4 = Point3D(1, 1, 1)\n    assert p3.direction_ratio(p4) == [1, 1, 1]\n    assert p4.direction_ratio(p3) == [-1, -1, -1]\n\n    p5 = Point3D(-1, -1, -1)\n    p6 = Point3D(1, 1, 1)\n    assert p5.direction_ratio(p6) == [2, 2, 2]\n    assert p6.direction_ratio(p5) == [-2, -2, -2]\n\n    p7 = Point3D(0, 1, 2)\n    p8 = Point3D(3, 4, 5)\n    assert p7.direction_ratio(p8) == [3, 3, 3]\n    assert p8.direction_ratio(p7) == [-3, -3, -3]\n\n    # Test symbolic direction ratios\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    p9 = Point3D(x, y, z)\n    p10 = Point3D(x + 1, y + 1, z + 1)\n    assert p9.direction_ratio(p10) == [1, 1, 1]\n    assert p10.direction_ratio(p9) == [-1, -1, -1]\n", "def test_canberra_distance():\n    p1 = Point2D(1, 2)\n    p2 = Point2D(3, 4)\n    p3 = Point2D(1, 1)\n    p4 = Point2D(0, 0)\n    assert p1.canberra_distance(p2) == 2 / 3\n    assert p1.canberra_distance(p3) == 1 / 3\n    assert p2.canberra_distance(p3) == 2 / 3\n    raises(ValueError, lambda: p4.canberra_distance(p4))\n"], "sample_1181": ["def test_HadamardProduct():\n    if not np:\n        skip(\"NumPy not installed\")\n    \n    from sympy import HadamardProduct\n    M = MatrixSymbol(\"M\", 2, 2)\n    N = MatrixSymbol(\"N\", 2, 2)\n    \n    expr = HadamardProduct(M, N)\n    printer = NumPyPrinter()\n    assert printer.doprint(expr) == 'numpy.multiply(M, N)'\n    \n    f = lambdify((M, N), expr, 'numpy')\n    ma = np.array([[1, 2], [3, 4]])\n    mb = np.array([[5, 6], [7, 8]])\n    assert np.array_equal(f(ma, mb), np.multiply(ma, mb))\n", "def test_array_diagonal():\n    if not np:\n        skip(\"NumPy not installed\")\n    \n    M = MatrixSymbol(\"M\", 2, 2)\n    N = MatrixSymbol(\"N\", 2, 2)\n    \n    cg = ArrayDiagonal(ArrayTensorProduct(M, N), (1, 2))\n    f = lambdify((M, N), cg, 'numpy')\n    \n    ma = np.array([[1, 2], [3, 4]])\n    mb = np.array([[5, 6], [7, 8]])\n    \n    result = f(ma, mb)\n    expected = np.diagonal(np.einsum('ij,kl->ikjl', ma, mb), axis1=1, axis2=2)\n    assert np.array_equal(result, expected)\n", "def test_cupy_printer():\n    cupy = import_module('cupy')\n    if not cupy:\n        skip(\"CuPy not installed\")\n\n    # Testing if CuPyPrinter can handle basic functions and constants\n    printer = CuPyPrinter()\n    assert printer.doprint(logaddexp(a, b)) == 'cupy.logaddexp(a, b)'\n    assert printer.doprint(logaddexp2(a, b)) == 'cupy.logaddexp2(a, b)'\n    assert printer.doprint(sqrt(a)) == 'cupy.sqrt(a)'\n    assert printer.doprint(Piecewise((1, x < 0), (0, True))) == \\\n        'cupy.select([cupy.less(x, 0),True], [1,0], default=cupy.nan)'\n    assert printer.doprint(Mod(a, b)) == 'cupy.mod(a, b)'\n\n    # Testing matrix operations\n    M = MatrixSymbol(\"M\", 2, 2)\n    N = MatrixSymbol(\"N\", 2, 2)\n    expr = M * N\n    f = lambdify((M, N), expr, 'cupy')\n\n    ma = cupy.array([[1, 2], [3, 4]])\n    mb = cupy.array([[1, -2], [-1, 3]])\n    assert (f(ma, mb) == ma @ mb).all()\n\n    # Testing logical operations\n    e = Equality(x, 1)\n    f = lambdify((x,), e, 'cupy')\n    x_ = cupy.array([0, 1, 2])\n    assert cupy.array_equal(f(x_), cupy.array([False, True, False]))\n\n    # Testing array operations\n    assert printer.doprint(Array(((1, 2), (3, 5)))) == 'cupy.array([[1, 2], [3, 5]])'\n    assert printer.doprint(Array((1, 2))) == 'cupy.array((1, 2))'\n\n    # Testing known functions and constants\n    assert hasattr(printer, '_print_acos')\n    assert hasattr(printer, '_print_log')\n    assert hasattr(printer, '_print_Pi')\n    assert hasattr(printer, '_print_EulerGamma')\n", "def test_numpy_logical_operations():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    # Test logical and\n    e = (x > 1) & (x < 3)\n    f = lambdify(x, e, 'numpy')\n    x_ = np.array([0, 1, 2, 3])\n    assert np.array_equal(f(x_), [False, False, True, False])\n\n    # Test logical or\n    e = (x < 1) | (x > 2)\n    f = lambdify(x, e, 'numpy')\n    assert np.array_equal(f(x_), [True, False, False, True])\n\n    # Test logical not\n    e = ~(x < 2)\n    f = lambdify(x, e, 'numpy')\n    assert np.array_equal(f(x_), [False, False, True, True])\n", "def test_cupy_print_methods():\n    cupy = import_module('cupy')\n    if not cupy:\n        skip(\"CuPy not installed\")\n    prntr = CuPyPrinter()\n    assert hasattr(prntr, '_print_acos')\n    assert hasattr(prntr, '_print_log')\n    assert hasattr(prntr, '_print_Exp1')\n    assert hasattr(prntr, '_print_Pi')\n"], "sample_1182": ["def test_print_Integral_with_constants():\n    from sympy.integrals.integrals import Integral\n    from sympy.abc import a, b\n    \n    expr = Integral(a * x, (x, 0, b))\n    \n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == \"Not supported\"\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == \"Not supported\"\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == \"Not supported\"\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == \"Not supported\"\n", "def test_known_constants():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(S.Exp1) == 'math.e'\n    assert prntr.doprint(S.Pi) == 'math.pi'\n    assert prntr.doprint(S.E) == 'math.e'\n    assert prntr.doprint(S.Infinity) == 'float(\"inf\")'\n    assert prntr.doprint(S.NaN) == 'float(\"nan\")'\n    assert prntr.doprint(S.ComplexInfinity) == 'float(\"nan\")'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(S.Exp1) == 'mpmath.e'\n    assert prntr.doprint(S.Pi) == 'mpmath.pi'\n    assert prntr.doprint(S.GoldenRatio) == 'mpmath.phi'\n    assert prntr.doprint(S.EulerGamma) == 'mpmath.euler'\n    assert prntr.doprint(S.NaN) == 'mpmath.nan'\n    assert prntr.doprint(S.Infinity) == 'mpmath.inf'\n    assert prntr.doprint(S.NegativeInfinity) == 'mpmath.ninf'\n    assert prntr.doprint(S.ComplexInfinity) == 'mpmath.nan'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(S.Exp1) == 'numpy.e'\n    assert prntr.doprint(S.Pi) == 'numpy.pi'\n    assert prntr.doprint(S.GoldenRatio) == 'numpy.golden_ratio'\n    assert prntr.doprint(S.EulerGamma) == 'numpy.euler_gamma'\n    assert prntr.doprint(S.NaN) == 'numpy.nan'\n    assert prntr.doprint(S.Infinity) == 'numpy.PINF'\n    assert prntr.doprint(S.NegativeInfinity) == 'numpy.NINF'\n    assert prntr.doprint(S.ComplexInfinity) == 'numpy.nan'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(S.GoldenRatio) == 'scipy.constants.golden_ratio'\n    assert prntr.doprint(S.Pi) == 'scipy.constants.pi'\n    assert prntr.doprint(S.Exp1) == 'numpy.e'\n    assert prn", "def test_undeclared_reserved_word_suffix():\n    prntr = PythonCodePrinter({'reserved_word_suffix': '_reserved'})\n    assert prntr.doprint(symbols('del')) == 'del_reserved'\n    assert prntr.doprint(symbols('for')) == 'for_reserved'\n    assert prntr.doprint(symbols('class')) == 'class_reserved'\n\n    prntr = PythonCodePrinter({'reserved_word_suffix': '_custom'})\n    assert prntr.doprint(symbols('if')) == 'if_custom'\n    assert prntr.doprint(symbols('else')) == 'else_custom'\n    assert prntr.doprint(symbols('while')) == 'while_custom'\n", "def test_ComplexInfinity():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"float('nan')\"\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"mpmath.nan\"\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"numpy.nan\"\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"numpy.nan\"\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(S.ComplexInfinity) == \"sympy.nan\"\n", "def test_known_constants_math():\n    prntr = PythonCodePrinter()\n    \n    assert prntr.doprint(S.Exp1) == 'math.e'\n    assert prntr.doprint(S.Pi) == 'math.pi'\n    assert prntr.doprint(S.E) == 'math.e'\n    assert prntr.doprint(S.Infinity) == 'float(\"inf\")'\n    assert prntr.doprint(S.NaN) == 'float(\"nan\")'\n    assert prntr.doprint(S.ComplexInfinity) == 'float(\"nan\")'\n    \n    prntr = MpmathPrinter()\n    assert prntr.doprint(S.Exp1) == 'mpmath.e'\n    assert prntr.doprint(S.Pi) == 'mpmath.pi'\n    assert prntr.doprint(S.GoldenRatio) == 'mpmath.phi'\n    assert prntr.doprint(S.EulerGamma) == 'mpmath.euler'\n    assert prntr.doprint(S.NaN) == 'mpmath.nan'\n    assert prntr.doprint(S.Infinity) == 'mpmath.inf'\n    assert prntr.doprint(S.NegativeInfinity) == 'mpmath.ninf'\n"], "sample_1183": ["def test_FractionField_basic_operations():\n    F, x, y = field(\"x,y\", ZZ)\n    K, a, b = F, x, y\n    \n    # Test addition\n    f1 = a / (b + 1)\n    f2 = b / (a + 1)\n    assert f1 + f2 == (a*(a + 1) + b*(b + 1)) / ((b + 1)*(a + 1))\n    \n    # Test subtraction\n    assert f1 - f2 == (a*(a + 1) - b*(b + 1)) / ((b + 1)*(a + 1))\n    \n    # Test multiplication\n    assert f1 * f2 == (a * b) / ((b + 1)*(a + 1))\n    \n    # Test division\n    assert f1 / f2 == (a * (a + 1)) / (b * (b + 1))\n    \n    # Test power\n    assert (f1 ** 2) == (a ** 2) / ((b + 1) ** 2)\n    assert (f2 ** 3) == (b ** 3) / ((a + 1) ** 3)\n    \n    # Test negation\n    assert -f1 == -a / (b + 1)\n    assert +f2 == f2\n    \n    # Test equality\n    assert f1 == (a / (b + 1))\n    assert f2 != (a / (b + 1))\n    \n    # Test from_expr\n    expr = (x**2 + y**2) / (x + 1)\n    f = K.from_expr(expr)\n    assert f == (x**2 + y**2) / (x + 1)\n    \n    # Test as_expr\n    assert f.as_expr() == expr\n    \n    # Test diff\n    f_diff = f.diff(x)\n    assert f_diff == ((2*x*(x + 1) - (x**2 + y**2)) / ((x + 1)**2))\n    \n    # Test subs\n    f_subs = f.subs(x, 1)\n    assert f_subs == (1 + y**2) / 2\n", "def test_sfield():\n    from sympy import symbols, log, exp\n\n    x, y = symbols('x y')\n\n    K, f = sfield(x**2 / y + log(x)*exp(y))\n    assert str(K) == 'Rational function field in x, y, exp(y), log(x) over ZZ with lex order'\n    assert str(f) == 'x**2/y + exp(y)*log(x)'\n\n    K, f = sfield((x + log(x))*exp(1/y))\n    assert str(K) == 'Rational function field in x, exp(1/y), log(x) over ZZ with lex order'\n    assert str(f) == '(x + log(x))*exp(1/y)'\n\n    K, f = sfield((x + y)*exp(x + y))\n    assert str(K) == 'Rational function field in x, y, exp(x + y) over ZZ with lex order'\n    assert str(f) == '(x + y)*exp(x + y)'\n\n    K, f = sfield(x**2 + y**2)\n    assert str(K) == 'Rational function field in x, y over ZZ with lex order'\n    assert str(f) == 'x**2 + y**2'\n", "def test_FracField_operations():\n    R, x, y, z = field(\"x, y, z\", ZZ)\n    f1 = x / y\n    f2 = y / z\n    f3 = x / z\n\n    assert f1 + f2 == (x*z + y**2) / (y*z)\n    assert f1 - f2 == (x*z - y**2) / (y*z)\n    assert f1 * f2 == x / z\n    assert f1 / f2 == x / y**2\n\n    # Test differentiation\n    f4 = (x**2 + y**2) / z\n    assert f4.diff(x) == 2*x / z\n    assert f4.diff(y) == 2*y / z\n    assert f4.diff(z) == -(x**2 + y**2) / z**2\n\n    # Test evaluation\n    assert f4.evaluate([(x, 2), (y, 3), (z, 1)]) == 13\n    assert f4.evaluate(x, 2) == (4 + y**2) / z\n\n    # Test substitution\n    assert f4.subs(x, 2) == (4 + y**2) / z\n    assert f4.subs([(x, 2), (y, 3)]) == 13 / z\n\n    # Test power\n    assert (f1 ** 2) == x**2 / y**2\n    assert (f1 ** -1) == y / x\n\n    # Test complex expressions\n    f5 = (x + y) / (y - z)\n    assert f5.diff(x) == 1 / (y - z)\n    assert f5.diff(y) == (y - z - (x + y)) / (y - z)**2\n    assert f5.diff(z) == (x + y) / (y - z)**2\n", "def test_FractionField_operations():\n    # Create a fraction field with generators x, y over the integers\n    F, x, y = field(\"x, y\", ZZ)\n    \n    # Test addition\n    f1 = (x**2 + y**2) / (x + 1)\n    f2 = (x + y) / (x - 1)\n    result = f1 + f2\n    expected = (x**2 + y**2 + (x + y)*(x + 1)) / ((x + 1)*(x - 1))\n    assert result == expected\n\n    # Test subtraction\n    result = f1 - f2\n    expected = (x**2 + y**2 - (x + y)*(x + 1)) / ((x + 1)*(x - 1))\n    assert result == expected\n\n    # Test multiplication\n    result = f1 * f2\n    expected = (x**2 + y**2)*(x + y) / ((x + 1)*(x - 1))\n    assert result == expected\n\n    # Test division\n    result = f1 / f2\n    expected = (x**2 + y**2)*(x - 1) / ((x + 1)*(x + y))\n    assert result == expected\n\n    # Test raising to a power\n    result = f1**2\n    expected = (x**2 + y**2)**2 / (x + 1)**2\n    assert result == expected\n\n    # Test raising to a negative power (inversion)\n    result = f1**(-1)\n    expected = (x + 1) / (x**2 + y**2)\n    assert result == expected\n\n    # Test partial derivative\n    result = f1.diff(x)\n    expected = (2*x*(x + 1) - (x**2 + y**2)) / (x + 1)**2\n    assert result == expected\n\n    # Test evaluation of function with substitutions\n    result = f1(1, 2)\n    expected = 5 / 2\n    assert result == expected\n\n    # Test substitution of generators\n    result = f1.subs(x, y)\n    expected = (y**2 + y**2) / (y + 1)\n    assert result == expected\n", "def test_FracField_operations():\n    R, x, y = ring(\"x,y\", ZZ)\n    F, X, Y = field(\"x,y\", ZZ)\n\n    f = (x**2 + y**2)/(x + 1)\n    g = (x**2 + y**2)/4\n\n    assert (F(f) + F(g)).as_expr() == ((x**2 + y**2)/(x + 1) + (x**2 + y**2)/4).as_expr()\n    assert (F(f) - F(g)).as_expr() == ((x**2 + y**2)/(x + 1) - (x**2 + y**2)/4).as_expr()\n    assert (F(f) * F(g)).as_expr() == ((x**2 + y**2)/(x + 1) * (x**2 + y**2)/4).as_expr()\n    assert (F(f) / F(g)).as_expr() == ((x**2 + y**2)/(x + 1) / (x**2 + y**2)/4).as_expr()\n    assert F(f).subs(x, y).as_expr() == ((y**2 + y**2)/(y + 1)).as_expr()\n\n    h = x**2 + y**2\n    assert (F(f) + F(h)).as_expr() == ((x**2 + y**2)/(x + 1) + (x**2 + y**2)).as_expr()\n    assert (F(f) - F(h)).as_expr() == ((x**2 + y**2)/(x + 1) - (x**2 + y**2)).as_expr()\n    assert (F(f) * F(h)).as_expr() == ((x**2 + y**2)/(x + 1) * (x**2 + y**2)).as_expr()\n    assert (F(f) / F(h)).as_expr() == ((x**2 + y**2)/(x + 1) / (x**2 + y**2)).as_expr()\n    assert F(f).subs(x, y).as_expr() == ((y**2 + y**2)/(y + 1)).as_expr()\n\n    assert F(f)**2 == F((x**2 + y**2)**2 / (x + 1)**2)\n    assert F(f).diff(x"], "sample_1184": ["def test_ray_transfer_matrix_multiplication():\n    from sympy import symbols\n\n    # Create symbols for testing\n    A1, B1, C1, D1 = symbols('A1 B1 C1 D1')\n    A2, B2, C2, D2 = symbols('A2 B2 C2 D2')\n    h, angle, wavelen = symbols('h angle wavelen')\n    \n    # Initialize Ray Transfer Matrices\n    mat1 = RayTransferMatrix(A1, B1, C1, D1)\n    mat2 = RayTransferMatrix(A2, B2, C2, D2)\n    \n    # Test RayTransferMatrix * RayTransferMatrix\n    product_matrix = mat1 * mat2\n    expected_matrix = RayTransferMatrix(A1*A2 + B1*C2, A1*B2 + B1*D2, C1*A2 + D1*C2, C1*B2 + D1*D2)\n    assert product_matrix == expected_matrix\n\n    # Initialize a GeometricRay\n    gRay = GeometricRay(h, angle)\n    \n    # Test RayTransferMatrix * GeometricRay\n    transformed_ray = mat1 * gRay\n    expected_ray = GeometricRay(A1*h + B1*angle, C1*h + D1*angle)\n    assert transformed_ray == expected_ray\n    \n    # Initialize a BeamParameter\n    bp = BeamParameter(wavelen, 1, w=1e-3)\n    \n    # Test RayTransferMatrix * BeamParameter\n    transformed_bp = mat1 * bp\n    q_transformed = (A1*bp.q + B1) / (C1*bp.q + D1)\n    expected_bp = BeamParameter(bp.wavelen, together(re(q_transformed)), z_r=together(im(q_transformed)))\n    assert transformed_bp.q == expected_bp.q\n    assert transformed_bp.wavelen == expected_bp.wavelen\n    assert transformed_bp.z_r == expected_bp.z_r\n", "def test_free_space():\n    d = symbols('d')\n    fs = FreeSpace(d)\n    assert fs == Matrix([[1, d], [0, 1]])\n    assert fs.A == 1\n    assert fs.B == d\n    assert fs.C == 0\n    assert fs.D == 1\n", "def test_ray_transfer_matrix_mul():\n    mat1 = RayTransferMatrix(1, 2, 3, 4)\n    mat2 = RayTransferMatrix(2, 0, 1, 2)\n    result_mat = mat1 * mat2\n    assert result_mat == RayTransferMatrix(Matrix([[4, 4], [10, 8]]))\n\n    h, angle = symbols('h angle')\n    geom_ray = GeometricRay(h, angle)\n    result_ray = mat1 * geom_ray\n    assert result_ray == GeometricRay(Matrix([[2*h + 2*angle], [3*h + 4*angle]]))\n\n    wavelen, z, z_r = symbols('wavelen z z_r')\n    beam_param = BeamParameter(wavelen, z, z_r=z_r)\n    result_beam = mat1 * beam_param\n    expected_beam = BeamParameter(wavelen, (z + 2)/(3*z + 4), z_r=z_r/(3*z + 4))\n    assert result_beam.wavelen == expected_beam.wavelen\n    assert result_beam.z == expected_beam.z\n    assert result_beam.z_r == expected_beam.z_r\n    assert result_beam.n == expected_beam.n\n", "def test_ray_transfer_matrix_multiplication():\n    from sympy import symbols\n\n    # Define symbols\n    A1, B1, C1, D1 = symbols('A1 B1 C1 D1')\n    A2, B2, C2, D2 = symbols('A2 B2 C2 D2')\n    h, angle = symbols('h angle')\n    wavelen, z, w = symbols('wavelen z w')\n\n    # Create RayTransferMatrix instances\n    mat1 = RayTransferMatrix(A1, B1, C1, D1)\n    mat2 = RayTransferMatrix(A2, B2, C2, D2)\n\n    # Test RayTransferMatrix multiplication\n    result = mat1 * mat2\n    expected = RayTransferMatrix(\n        A1 * A2 + B1 * C2, A1 * B2 + B1 * D2, C1 * A2 + D1 * C2, C1 * B2 + D1 * D2)\n    assert result == expected\n\n    # Create GeometricRay instance\n    geom_ray = GeometricRay(h, angle)\n\n    # Test RayTransferMatrix * GeometricRay multiplication\n    result = mat1 * geom_ray\n    expected = GeometricRay(A1 * h + B1 * angle, C1 * h + D1 * angle)\n    assert result == expected\n\n    # Create BeamParameter instance\n    beam_param = BeamParameter(wavelen, z, w=w)\n\n    # Test RayTransferMatrix * BeamParameter multiplication\n    result = mat1 * beam_param\n    q = (A1 * beam_param.q + B1) / (C1 * beam_param.q + D1)\n    expected = BeamParameter(wavelen, together(re(q)), z_r=together(im(q)))\n    assert result == expected\n\n    # Validate properties\n    assert result.wavelen == expected.wavelen\n    assert result.z == expected.z\n    assert result.z_r == expected.z_r\n    assert result.n == expected.n\n", "def test_ray_transfer_matrix_mul():\n    # Test multiplication of RayTransferMatrix with another RayTransferMatrix\n    mat1 = RayTransferMatrix(1, 2, 3, 4)\n    mat2 = RayTransferMatrix(2, 0, 1, 2)\n    result = mat1 * mat2\n    expected = RayTransferMatrix(4, 4, 10, 8)\n    assert result == expected\n\n    # Test multiplication of RayTransferMatrix with GeometricRay\n    h, angle = symbols('h angle')\n    ray = GeometricRay(h, angle)\n    result = mat1 * ray\n    expected = GeometricRay(2 * h + 2 * angle, 3 * h + 4 * angle)\n    assert result == expected\n\n    # Test multiplication of RayTransferMatrix with BeamParameter\n    wavelen = 530e-9\n    beam_param = BeamParameter(wavelen, 1, w=1e-3)\n    result = mat1 * beam_param\n    q = (2 * beam_param.q + 2) / (3 * beam_param.q + 4)\n    expected = BeamParameter(wavelen, together(re(q)), z_r=together(im(q)))\n    assert result.q == expected.q\n\n    # Test invalid multiplication\n    try:\n        mat1 * Matrix([1, 2, 3])\n        assert False, \"Expected an exception but didn't get one\"\n    except ValueError:\n        pass\n"], "sample_1185": ["def test_decompogen_nested_functions():\n    assert decompogen(sin(exp(cos(x))), x) == [sin(x), exp(x), cos(x)]\n    assert decompogen(log(sin(cos(exp(x)))), x) == [log(x), sin(x), cos(x), exp(x)]\n    assert decompogen(tan(sqrt(log(x**2 + 1))), x) == [tan(x), sqrt(x), log(x), x**2 + 1]\n", "def test_compogen_single_function():\n    assert compogen([sin(x)], x) == sin(x)\n    assert compogen([x**2 + x + 1], x) == x**2 + x + 1\n    assert compogen([sqrt(x)], x) == sqrt(x)\n    assert compogen([cos(x)], x) == cos(x)\n    assert compogen([Abs(x)], x) == Abs(x)\n", "def test_compogen_single_function():\n    assert compogen([sin(x)], x) == sin(x)\n    assert compogen([x**2 + x + 1], x) == x**2 + x + 1\n    assert compogen([sqrt(x)], x) == sqrt(x)\n", "def test_compogen_edge_cases():\n    assert compogen([x], x) == x\n    assert compogen([1], x) == 1\n    assert compogen([Max(3, x)], x) == Max(3, x)\n    raises(TypeError, lambda: compogen([x < 5], x))\n    assert compogen([Max(sqrt(x), x**2), 2*x + 3], x) == Max(sqrt(2*x + 3), (2*x + 3)**2)\n    assert compogen([Max(x, x**2, y), 2*x + 3], x) == Max(2*x + 3, (2*x + 3)**2, y)\n    assert compogen([Max(sin(x), x**2 + 3*x + 2)], x) == Max(sin(x), x**2 + 3*x + 2)\n", "def test_decompogen_edge_cases():\n    assert decompogen(exp(sin(x)), x) == [exp(x), sin(x)]\n    assert decompogen(sin(x)**3, x) == [x**3, sin(x)]\n    assert decompogen(exp(x**2), x) == [exp(x), x**2]\n    assert decompogen(Max(sin(x), cos(x)), x) == [Max(sin(x), cos(x))]\n    assert decompogen(Max(sin(x), x**2), x) == [Max(sin(x), x**2)]\n"], "sample_1186": ["def test_ndimarray_arithmetic_operations():\n    for ArrayType in array_types:\n        a = ArrayType([1, 2, 3, 4], (2, 2))\n        b = ArrayType([5, 6, 7, 8], (2, 2))\n        \n        # Test addition\n        c = a + b\n        assert c.tolist() == [[6, 8], [10, 12]]\n        \n        # Test subtraction\n        d = a - b\n        assert d.tolist() == [[-4, -4], [-4, -4]]\n        \n        # Test scalar multiplication\n        e = a * 2\n        assert e.tolist() == [[2, 4], [6, 8]]\n        \n        # Test scalar division\n        f = b / 2\n        assert f.tolist() == [[2.5, 3], [3.5, 4]]\n        \n        # Test negation\n        g = -a\n        assert g.tolist() == [[-1, -2], [-3, -4]]\n        \n        # Test equality\n        h = ArrayType([1, 2, 3, 4], (2, 2))\n        assert a == h\n        assert a != b\n        \n        # Test transpose for 2D arrays\n        if ArrayType in [MutableDenseNDimArray, ImmutableDenseNDimArray]:\n            i = ArrayType([[1, 2], [3, 4]])\n            assert i.transpose().tolist() == [[1, 3], [2, 4]]\n        \n        # Test conjugate\n        j = ArrayType([1 + 1j, 2 + 2j], (1, 2))\n        k = j.conjugate()\n        assert k.tolist() == [[1 - 1j, 2 - 2j]]\n        \n        # Test adjoint\n        if ArrayType in [MutableDenseNDimArray, ImmutableDenseNDimArray]:\n            l = ArrayType([[1 + 1j, 2 + 2j], [3 + 3j, 4 + 4j]])\n            assert l.adjoint().tolist() == [[1 - 1j, 3 - 3j], [2 - 2j, 4 - 4j]]\n", "def test_array_creation_from_iterable():\n    for ArrayType in array_types:\n        test_array = ArrayType([1, 2, 3, 4], (2, 2))\n        assert test_array.shape == (2, 2)\n        assert test_array.tolist() == [[1, 2], [3, 4]]\n\n        test_array = ArrayType([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n        assert test_array.shape == (2, 2, 2)\n        assert test_array.tolist() == [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n", "def test_ndim_array_addition():\n    for array_type in array_types:\n        A = array_type([1, 2, 3], (3,))\n        B = array_type([4, 5, 6], (3,))\n        C = A + B\n        assert isinstance(C, array_type)\n        assert list(C) == [5, 7, 9]\n\n        A = array_type([[1, 2], [3, 4]], (2, 2))\n        B = array_type([[5, 6], [7, 8]], (2, 2))\n        C = A + B\n        assert isinstance(C, array_type)\n        assert list(C) == [6, 8, 10, 12]\n\n        raises(ValueError, lambda: A + array_type([1, 2, 3], (3,)))\n", "def test_array_arithmetic_operations():\n    for ArrayType in array_types:\n        A = ArrayType([1, 2, 3, 4], (2, 2))\n        B = ArrayType([5, 6, 7, 8], (2, 2))\n        C = ArrayType([1, 2, 3, 4, 5, 6], (2, 3))\n\n        # Test addition\n        result = A + B\n        expected = ArrayType([6, 8, 10, 12], (2, 2))\n        assert result == expected\n\n        # Test subtraction\n        result = B - A\n        expected = ArrayType([4, 4, 4, 4], (2, 2))\n        assert result == expected\n\n        # Test scalar multiplication\n        result = A * 2\n        expected = ArrayType([2, 4, 6, 8], (2, 2))\n        assert result == expected\n\n        # Test scalar division\n        result = B / 2\n        expected = ArrayType([2.5, 3, 3.5, 4], (2, 2))\n        assert result == expected\n\n        # Test negation\n        result = -A\n        expected = ArrayType([-1, -2, -3, -4], (2, 2))\n        assert result == expected\n\n        # Test invalid operations\n        raises(ValueError, lambda: A * B)\n        raises(ValueError, lambda: A / B)\n        raises(ValueError, lambda: A * C)\n        raises(ValueError, lambda: A + C)\n        raises(ValueError, lambda: A - C)\n", "def test_array_arithmetic_operations():\n    for ArrayType in array_types:\n        A = ArrayType([1, 2, 3, 4], (2, 2))\n        B = ArrayType([5, 6, 7, 8], (2, 2))\n\n        # Test addition\n        C = A + B\n        assert C.tolist() == [[6, 8], [10, 12]]\n\n        # Test subtraction\n        D = B - A\n        assert D.tolist() == [[4, 4], [4, 4]]\n\n        # Test element-wise multiplication\n        E = A * 2\n        assert E.tolist() == [[2, 4], [6, 8]]\n\n        # Test element-wise division\n        F = B / 2\n        assert F.tolist() == [[2.5, 3], [3.5, 4]]\n\n        # Test negation\n        G = -A\n        assert G.tolist() == [[-1, -2], [-3, -4]]\n\n        # Test equality and inequality\n        H = ArrayType([1, 2, 3, 4], (2, 2))\n        assert A == H\n        assert A != B\n"], "sample_1187": ["def test_left_integral2D():\n    from sympy import Point\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    facets = triangle.sides\n    m = 3\n    index = 0\n    x0 = facets[0].points[0]\n    expr = 1\n    gens = (x, y)\n    assert left_integral2D(m, index, facets, x0, expr, gens) == 5\n    expr = x + y\n    assert left_integral2D(m, index, facets, x0, expr, gens) == 5 * x0.x + 5 * x0.y\n", "def test_gradient_terms():\n    assert gradient_terms(2) == [\n        [1, 0, 0, 0], [y, 0, 1, 0], [y**2, 0, 2, 0], [x, 1, 0, 0],\n        [x*y, 1, 1, 0], [x**2, 2, 0, 0]\n    ]\n    assert gradient_terms(1, 3) == [\n        [[[1, 0, 0, 0, 0, 0, 0, 0]]], [[[y, 0, 1, 0, 1, 0, 0, 0], [z, 0, 0, 1, 1, 0, 1, 0]], [[x, 1, 0, 0, 1, 1, 0, 0]]]\n    ]\n    assert gradient_terms(2, 3) == [\n        [[[1, 0, 0, 0, 0, 0, 0, 0]], [[y, 0, 1, 0, 1, 0, 0, 0], [z, 0, 0, 1, 1, 0, 1, 0]], [[x, 1, 0, 0, 1, 1, 0, 0]]],\n        [[[y**2, 0, 2, 0, 2, 0, 0, 0], [y*z, 0, 1, 1, 2, 0, 1, 0], [z**2, 0, 0, 2, 2, 0, 2, 0]], [[x*y, 1, 1, 0, 2, 1, 0, 0], [x*z, 1, 0, 1, 2, 1, 1, 0]], [[x**2, 2, 0, 0, 2, 2, 0, 0]]]\n    ]\n", "def test_gradient_terms():\n    # Test for 2D case\n    assert gradient_terms(2) == [[1, 0, 0, 0], [y, 0, 1, 0], [y**2, 0, 2, 0], [x, 1, 0, 0], [x*y, 1, 1, 0], [x**2, 2, 0, 0]]\n    \n    # Test for 3D case\n    assert gradient_terms(2, 3) == [\n        [[[1, 0, 0, 0, 0, 0, 0, 0]]], \n        [[[y, 0, 1, 0, 1, 0, 0, 0], [z, 0, 0, 1, 1, 0, 1, 0]], [[x, 1, 0, 0, 1, 1, 0, 0]]],\n        [[[y**2, 0, 2, 0, 2, 0, 0, 0], [y*z, 0, 1, 1, 2, 0, 1, 0], [z**2, 0, 0, 2, 2, 0, 2, 0]], \n        [[x*y, 1, 1, 0, 2, 1, 0, 0], [x*z, 1, 0, 1, 2, 1, 1, 0]], \n        [[x**2, 2, 0, 0, 2, 2, 0, 0]]]\n    ]\n", "def test_gradient_terms():\n    assert gradient_terms(2) == [\n        [1, 0, 0, 0],\n        [y, 0, 1, 0],\n        [y**2, 0, 2, 0],\n        [x, 1, 0, 0],\n        [x*y, 1, 1, 0],\n        [x**2, 2, 0, 0]\n    ]\n    assert gradient_terms(2, 3) == [\n        [[[1, 0, 0, 0, 0, 0, 0, 0]]],\n        [[[y, 0, 1, 0, 1, 0, 0, 0], [z, 0, 0, 1, 1, 0, 1, 0]], [[x, 1, 0, 0, 1, 1, 0, 0]]],\n        [[[y**2, 0, 2, 0, 2, 0, 0, 0], [y*z, 0, 1, 1, 2, 0, 1, 0], [z**2, 0, 0, 2, 2, 0, 2, 0]],\n         [[x*y, 1, 1, 0, 2, 1, 0, 0], [x*z, 1, 0, 1, 2, 1, 1, 0]], [[x**2, 2, 0, 0, 2, 2, 0, 0]]]]\n    ]\n", "def test_gradient_terms():\n    # Testing for 2D case\n    assert gradient_terms(0) == [[1, 0, 0, 0]]\n    assert gradient_terms(1) == [[1, 0, 0, 0], [y, 0, 1, 0], [x, 1, 0, 0]]\n    assert gradient_terms(2) == [\n        [1, 0, 0, 0], [y, 0, 1, 0], [y**2, 0, 2, 0],\n        [x, 1, 0, 0], [x*y, 1, 1, 0], [x**2, 2, 0, 0]\n    ]\n\n    # Testing for 3D case\n    assert gradient_terms(0, 3) == [[[[1, 0, 0, 0, 0, 0, 0, 0]]]]\n    assert gradient_terms(1, 3) == [\n        [[[1, 0, 0, 0, 0, 0, 0, 0]]], \n        [[\n            [y, 0, 1, 0, 1, 0, 0, 0], [z, 0, 0, 1, 1, 0, 1, 0]\n        ], [\n            [x, 1, 0, 0, 1, 1, 0, 0]\n        ]]\n    ]\n    assert gradient_terms(2, 3) == [\n        [[[1, 0, 0, 0, 0, 0, 0, 0]]], \n        [[\n            [y, 0, 1, 0, 1, 0, 0, 0], [z, 0, 0, 1, 1, 0, 1, 0]\n        ], [\n            [x, 1, 0, 0, 1, 1, 0, 0]\n        ]],\n        [[\n            [y**2, 0, 2, 0, 2, 0, 0, 0], [y*z, 0, 1, 1, 2, 0, "], "sample_1188": ["def test_pretty_printer_settings():\n    from sympy import Symbol\n    expr = Symbol(\"x\")\n    \n    # Test imaginary_unit setting\n    pp = PrettyPrinter(settings={\"imaginary_unit\": \"i\"})\n    assert pp.doprint(1j * expr) == 'i*x'\n    \n    pp = PrettyPrinter(settings={\"imaginary_unit\": \"j\"})\n    assert pp.doprint(1j * expr) == 'j*x'\n    \n    # Test use_unicode setting\n    pp = PrettyPrinter(settings={\"use_unicode\": True})\n    assert pp.doprint(Integral(expr)) == \"\u2320\\n\u23ae x dx\\n\u2321\"\n    \n    pp = PrettyPrinter(settings={\"use_unicode\": False})\n    assert pp.doprint(Integral(expr)) == \"/\\n| x dx\\n/\"\n    \n    # Test wrap_line setting\n    long_expr = sum(Symbol(f\"x{i}\") for i in range(100))\n    pp = PrettyPrinter(settings={\"wrap_line\": True, \"num_columns\": 20})\n    assert '\\n' in pp.doprint(long_expr)\n    \n    pp = PrettyPrinter(settings={\"wrap_line\": False})\n    assert '\\n' not in pp.doprint(long_expr)\n    \n    # Test full_prec setting\n    from sympy import Float\n    expr = Float(\"1.12345678901234567890\")\n    pp = PrettyPrinter(settings={\"full_prec\": True})\n    assert pp.doprint(expr) == \"1.12345678901235\"\n    \n    pp = PrettyPrinter(settings={\"full_prec\": False})\n    assert pp.doprint(expr) == \"1.12\"\n\n    # Test root_notation setting\n    from sympy import sqrt\n    expr = sqrt(2)\n    pp = PrettyPrinter(settings={\"root_notation\": True})\n    assert pp.doprint(expr) == \"\u221a2\"\n    \n    pp = PrettyPrinter(settings={\"root_notation\": False})\n    assert pp.doprint(expr) == \"2**(1/2)\"\n", "def test_prettyprinter_settings():\n    from sympy import Symbol\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test imaginary_unit setting\n    pp_i = PrettyPrinter({'imaginary_unit': 'i'})\n    pp_j = PrettyPrinter({'imaginary_unit': 'j'})\n    assert pp_i._settings['imaginary_unit'] == 'i'\n    assert pp_j._settings['imaginary_unit'] == 'j'\n    with pytest.raises(ValueError):\n        PrettyPrinter({'imaginary_unit': 'k'})\n\n    # Test root_notation setting\n    pp_root_true = PrettyPrinter({'root_notation': True})\n    pp_root_false = PrettyPrinter({'root_notation': False})\n    assert pp_root_true._settings['root_notation']\n    assert not pp_root_false._settings['root_notation']\n\n    # Test wrap_line setting\n    pp_wrap_true = PrettyPrinter({'wrap_line': True})\n    pp_wrap_false = PrettyPrinter({'wrap_line': False})\n    assert pp_wrap_true._settings['wrap_line']\n    assert not pp_wrap_false._settings['wrap_line']\n    \n    # Test matrix symbol style\n    pp_bold = PrettyPrinter({'mat_symbol_style': 'bold'})\n    pp_plain = PrettyPrinter({'mat_symbol_style': 'plain'})\n    assert pp_bold._settings['mat_symbol_style'] == 'bold'\n    assert pp_plain._settings['mat_symbol_style'] == 'plain'\n", "def test_pretty_printer_settings():\n    # Test PrettyPrinter initialization with custom settings\n    from sympy.core.symbol import symbols\n    from sympy.printing.pretty.pretty import PrettyPrinter\n\n    expr = symbols('x') + symbols('y')\n\n    # Test default settings\n    pp = PrettyPrinter()\n    assert pp._settings['order'] is None\n    assert pp._settings['full_prec'] == \"auto\"\n    assert pp._settings['use_unicode'] is None\n    assert pp._settings['wrap_line'] is True\n    assert pp._settings['num_columns'] is None\n    assert pp._settings['use_unicode_sqrt_char'] is True\n    assert pp._settings['root_notation'] is True\n    assert pp._settings['mat_symbol_style'] == \"plain\"\n    assert pp._settings['imaginary_unit'] == \"i\"\n    assert pp._settings['perm_cyclic'] is True\n\n    # Test custom settings\n    custom_settings = {\n        \"order\": \"lex\",\n        \"full_prec\": True,\n        \"use_unicode\": True,\n        \"wrap_line\": False,\n        \"num_columns\": 80,\n        \"use_unicode_sqrt_char\": False,\n        \"root_notation\": False,\n        \"mat_symbol_style\": \"bold\",\n        \"imaginary_unit\": \"j\",\n        \"perm_cyclic\": False,\n    }\n    pp = PrettyPrinter(custom_settings)\n    for key, value in custom_settings.items():\n        assert pp._settings[key] == value\n\n    # Test imaginary_unit validation\n    with pytest.raises(TypeError):\n        PrettyPrinter({\"imaginary_unit\": 1})\n    with pytest.raises(ValueError):\n        PrettyPrinter({\"imaginary_unit\": \"k\"})\n\n    # Test doprint method\n    pp = PrettyPrinter({\"use_unicode\": False})\n    assert pp.doprint(expr) == 'x + y'\n    pp = PrettyPrinter({\"use_unicode\": True})\n    assert pp.doprint(expr) == 'x + y'\n", "def test_prettyprinter_initialization():\n    # Test PrettyPrinter initialization with default settings\n    pp = PrettyPrinter()\n    assert pp._settings[\"order\"] is None\n    assert pp._settings[\"full_prec\"] == \"auto\"\n    assert pp._settings[\"use_unicode\"] is None\n    assert pp._settings[\"wrap_line\"]\n    assert pp._settings[\"num_columns\"] is None\n    assert pp._settings[\"use_unicode_sqrt_char\"]\n    assert pp._settings[\"root_notation\"]\n    assert pp._settings[\"mat_symbol_style\"] == \"plain\"\n    assert pp._settings[\"imaginary_unit\"] == \"i\"\n    assert pp._settings[\"perm_cyclic\"]\n\n    # Test PrettyPrinter initialization with custom settings\n    settings = {\n        \"order\": \"old\",\n        \"full_prec\": True,\n        \"use_unicode\": True,\n        \"wrap_line\": False,\n        \"num_columns\": 80,\n        \"use_unicode_sqrt_char\": False,\n        \"root_notation\": False,\n        \"mat_symbol_style\": \"bold\",\n        \"imaginary_unit\": \"j\",\n        \"perm_cyclic\": False\n    }\n    pp_custom = PrettyPrinter(settings=settings)\n    assert pp_custom._settings[\"order\"] == \"old\"\n    assert pp_custom._settings[\"full_prec\"] is True\n    assert pp_custom._settings[\"use_unicode\"] is True\n    assert not pp_custom._settings[\"wrap_line\"]\n    assert pp_custom._settings[\"num_columns\"] == 80\n    assert not pp_custom._settings[\"use_unicode_sqrt_char\"]\n    assert not pp_custom._settings[\"root_notation\"]\n    assert pp_custom._settings[\"mat_symbol_style\"] == \"bold\"\n    assert pp_custom._settings[\"imaginary_unit\"] == \"j\"\n    assert not pp_custom._settings[\"perm_cyclic\"]\n", "def test_prettyprinter_initialization():\n    from sympy.printing.pretty.pretty import PrettyPrinter\n\n    settings = {\n        \"order\": None,\n        \"full_prec\": \"auto\",\n        \"use_unicode\": None,\n        \"wrap_line\": True,\n        \"num_columns\": None,\n        \"use_unicode_sqrt_char\": True,\n        \"root_notation\": True,\n        \"mat_symbol_style\": \"plain\",\n        \"imaginary_unit\": \"i\",\n        \"perm_cyclic\": True\n    }\n\n    pp = PrettyPrinter(settings)\n    assert pp._settings == settings\n\n    invalid_settings = {\n        \"imaginary_unit\": 5  # not a string\n    }\n    try:\n        PrettyPrinter(invalid_settings)\n    except TypeError as e:\n        assert str(e) == \"'imaginary_unit' must a string, not 5\"\n\n    invalid_settings = {\n        \"imaginary_unit\": \"k\"  # not 'i' or 'j'\n    }\n    try:\n        PrettyPrinter(invalid_settings)\n    except ValueError as e:\n        assert str(e) == \"'imaginary_unit' must be either 'i' or 'j', not 'k'\"\n"], "sample_1189": ["def test_lambdify_numpy_cupy_interchangeability():\n    if not numpy:\n        skip(\"numpy not installed.\")\n    if not cupy:\n        skip(\"CuPy not installed.\")\n\n    # Define the expression\n    expr = x ** 2 + y\n\n    # Create lambdified functions for both numpy and cupy\n    f_numpy = lambdify((x, y), expr, modules='numpy')\n    f_cupy = lambdify((x, y), expr, modules='cupy')\n\n    # Test with numpy arrays\n    numpy_x = numpy.array(2.0)\n    numpy_y = numpy.array(3.0)\n    numpy_result = f_numpy(numpy_x, numpy_y)\n    assert numpy_result == 7.0\n\n    # Test with cupy arrays\n    cupy_x = cupy.array(2.0)\n    cupy_y = cupy.array(3.0)\n    cupy_result = f_cupy(cupy_x, cupy_y)\n    assert cupy_result == 7.0\n\n    # Ensure numpy function works with cupy arrays\n    numpy_with_cupy = f_numpy(cupy_x, cupy_y)\n    assert numpy_with_cupy == 7.0\n\n    # Ensure cupy function works with numpy arrays\n    cupy_with_numpy = f_cupy(numpy_x, numpy_y)\n    assert cupy_with_numpy == 7.0\n", "def test_custom_function_namespace():\n    # Create a custom function to be used in the namespace\n        return a + b\n    \n    # Use lambdify with the custom namespace\n    f = lambdify((x, y), custom_add(x, y), modules={\"custom_add\": custom_add})\n    \n    # Test the lambdified function with various inputs\n    assert f(2, 3) == 5\n    assert f(-1, 1) == 0\n    assert f(0, 0) == 0\n    assert f(100, -50) == 50\n    \n    # Ensure that the function raises an error when called with the wrong number of arguments\n    raises(TypeError, lambda: f(1))\n    raises(TypeError, lambda: f(1, 2, 3))\n\n    # Ensure that the custom function is actually used and not overwritten by default modules\n    f_default = lambdify((x, y), x + y)\n    assert f_default(2, 3) == 5\n    assert f(2, 3) == 5  # Should still use custom_add and not default addition\n", "def test_custom_translations():\n    # Test that custom translations work properly\n    custom_translations = {\n        'cos': lambda x: x + 1,\n        'sin': lambda x: x + 2,\n    }\n\n    expr = cos(x) + sin(y)\n    f = lambdify((x, y), expr, modules=custom_translations)\n    assert f(1, 1) == 1 + 1 + 1 + 2  # cos(1) -> 1+1, sin(1) -> 1+2\n\n    expr = cos(x)\n    f = lambdify(x, expr, modules=custom_translations)\n    assert f(3) == 4  # cos(3) -> 3+1\n\n    expr = sin(x)\n    f = lambdify(x, expr, modules=custom_translations)\n    assert f(4) == 6  # sin(4) -> 4+2\n", "def test_lambda_generated_source():\n    f = lambdify(x, x**3 + x**2 + x + 1)\n    source_code = inspect.getsource(f)\n    assert \"def _lambdifygenerated(x):\" in source_code\n    assert \"return ((x**3) + (x**2)) + (x + 1)\" in source_code\n", "def test_invalid_module():\n    # Test for invalid module name\n    with raises(NameError, match=\"'invalid_module' module cannot be used for lambdification\"):\n        lambdify(x, x + 1, modules='invalid_module')\n"], "sample_1190": ["def test_unit_system():\n    length_unit = Quantity(\"length_unit\")\n    time_unit = Quantity(\"time_unit\")\n    speed_unit = Quantity(\"speed_unit\")\n\n    length_unit.set_global_relative_scale_factor(1, meter)\n    time_unit.set_global_relative_scale_factor(1, second)\n    speed_unit.set_global_relative_scale_factor(1, meter/second)\n\n    units = [length_unit, time_unit]\n    derived_units = {length/time: speed_unit}\n\n    unit_system = UnitSystem(base_units=units, derived_units=derived_units, name=\"TestSystem\")\n\n    assert unit_system.name == \"TestSystem\"\n    assert unit_system.dim == 2\n    assert unit_system.is_consistent\n    assert unit_system.derived_units == derived_units\n\n    extended_unit_system = unit_system.extend([speed_unit], name=\"ExtendedTestSystem\")\n    assert extended_unit_system.name == \"ExtendedTestSystem\"\n    assert extended_unit_system.dim == 3\n    assert extended_unit_system.is_consistent\n    assert speed_unit in extended_unit_system._base_units\n\n    assert UnitSystem.get_unit_system(\"TestSystem\") == unit_system\n    assert UnitSystem.get_unit_system(\"ExtendedTestSystem\") == extended_unit_system\n\n    with raises(ValueError):\n        UnitSystem.get_unit_system(\"NonExistentSystem\")\n", "def test_get_unit_system():\n    base_units = [meter, kilogram, second]\n    unit_system = UnitSystem(base_units, name=\"test_system\")\n    assert UnitSystem.get_unit_system(\"test_system\") == unit_system\n    assert UnitSystem.get_unit_system(unit_system) == unit_system\n    raises(ValueError, lambda: UnitSystem.get_unit_system(\"non_existent_system\"))\n", "def test_unit_system_initialization():\n    # Create dummy units and dimension systems for testing\n    base_unit1 = Quantity(\"base_unit1\")\n    base_unit2 = Quantity(\"base_unit2\")\n    unit1 = Quantity(\"unit1\")\n    unit2 = Quantity(\"unit2\")\n    dimension_system = SI.get_dimension_system()\n    derived_units = {length: base_unit1, time: base_unit2}\n\n    # Initialize a UnitSystem\n    us = UnitSystem([base_unit1, base_unit2], [unit1, unit2], \"TestSystem\", \"Test Description\", dimension_system, derived_units)\n\n    # Assert properties\n    assert us.name == \"TestSystem\"\n    assert us.descr == \"Test Description\"\n    assert us._base_units == (base_unit1, base_unit2)\n    assert us._units == (base_unit1, base_unit2, unit1, unit2)\n    assert us._dimension_system == dimension_system\n    assert us._derived_units == derived_units\n\n    # Test __str__ method\n    assert str(us) == \"TestSystem\"\n\n    # Test __repr__ method\n    assert repr(us) == '<UnitSystem: (base_unit1, base_unit2)>'\n", "def test_get_units_non_prefixed():\n    u1 = Quantity(\"u1\", abbrev=\"m\")\n    u1.set_global_relative_scale_factor(S.One, meter)\n    u2 = Quantity(\"u2\", abbrev=\"cm\")\n    u2.set_global_relative_scale_factor(S.One, centimeter)\n    u3 = Quantity(\"u3\", abbrev=\"km\")\n    u3.set_global_relative_scale_factor(S.One, kilometer)\n    u4 = Quantity(\"u4\", abbrev=\"N\")\n    u4.set_global_relative_scale_factor(S.One, joule)  # Not an actual constant, just for testing\n    u5 = Quantity(\"u5\", abbrev=\"m\")\n    u5.set_global_relative_scale_factor(S.One, meter)\n\n    units = [u1, u2, u3, u4, u5]\n    base_units = [u1]\n    unit_system = UnitSystem(base_units, units)\n\n    non_prefixed_units = unit_system.get_units_non_prefixed()\n    assert u1 in non_prefixed_units\n    assert u2 not in non_prefixed_units\n    assert u3 not in non_prefixed_units\n    assert u4 in non_prefixed_units\n    assert u5 in non_prefixed_units\n", "def test_unit_system():\n    base_units = (meter, kilogram, second)\n    units = (joule, volt, ohm)\n    name = \"TestSystem\"\n    descr = \"A test unit system\"\n    dimension_system = SI.get_dimension_system()\n    derived_units = {energy: joule, pressure: pascal}\n\n    test_system = UnitSystem(base_units, units, name, descr, dimension_system, derived_units)\n\n    assert test_system.name == \"TestSystem\"\n    assert test_system.descr == \"A test unit system\"\n    assert test_system.dim == 3\n    assert test_system.is_consistent\n\n    extended_system = test_system.extend((kelvin,), (watt,), \"ExtendedSystem\", \"An extended test unit system\")\n    assert extended_system.name == \"ExtendedSystem\"\n    assert extended_system.descr == \"An extended test unit system\"\n    assert extended_system.dim == 4\n    assert extended_system.is_consistent\n\n    unit_system_by_name = UnitSystem.get_unit_system(\"TestSystem\")\n    assert unit_system_by_name == test_system\n\n    with raises(ValueError):\n        UnitSystem.get_unit_system(\"NonExistentSystem\")\n\n    assert UnitSystem.get_default_unit_system() == SI\n\n    assert test_system.derived_units[energy] == joule\n\n    expr = 3 * joule + 2 * joule\n    assert test_system.get_dimensional_expr(expr) == energy.name\n\n    factor, dimension = test_system._collect_factor_and_dimension(expr)\n    assert factor == 5\n    assert dimension == energy\n"], "sample_1191": ["def test_add_columns():\n    m = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    add_columns(m, 0, 1, 1, 2, 3, 4)\n    assert m == [\n        [5, 11, 3],\n        [14, 31, 6],\n        [23, 51, 9]\n    ]\n\n    m = [\n        [1, 2],\n        [3, 4]\n    ]\n    add_columns(m, 0, 1, 0, 1, 1, 0)\n    assert m == [\n        [2, 1],\n        [4, 3]\n    ]\n\n    # Test with negative and zero coefficients\n    m = [\n        [1, 2],\n        [3, 4]\n    ]\n    add_columns(m, 0, 1, -1, 0, 0, 1)\n    assert m == [\n        [-1, 2],\n        [-3, 4]\n    ]\n", "def test_invariant_factors():\n    # Test for zero matrix\n    m = DM([[0, 0], [0, 0]], ZZ)\n    assert invariant_factors(m) == (0, 0)\n    \n    # Test for identity matrix\n    m = DM([[1, 0], [0, 1]], ZZ)\n    assert invariant_factors(m) == (1, 1)\n    \n    # Test for a singular matrix\n    m = DM([[2, 4], [6, 12]], ZZ)\n    assert invariant_factors(m) == (2, 0)\n    \n    # Test for a non-square matrix\n    m = DM([[2, 4, 8], [6, 12, 18]], ZZ)\n    assert invariant_factors(m) == (2, 6)\n    \n    # Test for matrix with different domains\n    raises(ValueError, lambda: invariant_factors(DM([[1, 2], [3, 4]], QQ)))\n    \n    # Test for matrix with different shape\n    m = DM([[0]], ZZ)\n    assert invariant_factors(m) == (0,)\n    \n    m = DM([[2], [3]], ZZ)\n    assert invariant_factors(m) == (1, 6)\n", "def test_invariant_factors():\n    m = DM([[4, 6], [10, 12]], ZZ)\n    assert invariant_factors(m) == (2, 14)\n\n    m = DM([[1, 2, 3], [4, 5, 6]], ZZ)\n    assert invariant_factors(m) == (1, 1)\n\n    m = DM([[0, 0], [0, 0]], ZZ)\n    assert invariant_factors(m) == ()\n\n    m = DM([[ZZ(0)]*5]*5, ZZ)\n    assert invariant_factors(m) == ()\n\n    raises(ValueError, lambda: invariant_factors(DM([[1]], QQ)))\n", "def test_add_columns():\n    m = [[1, 2], [3, 4]]\n    add_columns(m, 0, 1, 1, 2, 3, 4)\n    assert m == [[7, 10], [15, 22]]\n\n    m = [[0, 0], [0, 0]]\n    add_columns(m, 0, 1, 1, 2, 3, 4)\n    assert m == [[0, 0], [0, 0]]\n\n    m = [[1, -1], [-1, 1]]\n    add_columns(m, 0, 1, 1, -1, -1, 1)\n    assert m == [[2, -2], [2, -2]]\n", "def test_hermite_normal_zero_matrix():\n    m = DM([[0, 0, 0], [0, 0, 0], [0, 0, 0]], ZZ)\n    hnf = DM([[0, 0, 0], [0, 0, 0], [0, 0, 0]], ZZ)\n    assert hermite_normal_form(m) == hnf\n\n    m = DM([[0, 0], [0, 0], [0, 0]], ZZ)\n    hnf = DM([[0, 0], [0, 0], [0, 0]], ZZ)\n    assert hermite_normal_form(m) == hnf\n"], "sample_1192": ["def test_Wild_exclude():\n    x, y, z = symbols('x y z')\n    w1 = Wild('w1', exclude=[x])\n    w2 = Wild('w2', exclude=[x, y])\n    w3 = Wild('w3', exclude=[x, y, z])\n\n    expr1 = y + z\n    expr2 = x + y\n    expr3 = x + y + z\n\n    assert expr1.match(w1 + z) == {w1: y}\n    assert expr1.match(w2 + z) == {w2: y}\n    assert expr1.match(w3 + z) is None\n\n    assert expr2.match(w1 + y) is None\n    assert expr2.match(w2 + y) is None\n    assert expr2.match(w3 + y) is None\n\n    assert expr3.match(w1 + y + z) is None\n    assert expr3.match(w2 + y + z) is None\n    assert expr3.match(w3 + y + z) is None\n\n    # Check matching with a function\n    f = Function('f')\n    wf = Wild('wf', exclude=[f(x)])\n    assert f(y).match(wf) == {wf: f(y)}\n    assert f(x).match(wf) is None\n\n    # Check matching with a constant\n    c = 5\n    wc = Wild('wc', exclude=[c])\n    assert Symbol('k').match(wc) == {wc: Symbol('k')}\n    assert c.match(wc) is None\n", "def test_uniquely_named_symbol_with_modify():\n        return name + '_custom'\n\n    x = Symbol('x')\n    x_custom = Symbol('x_custom')\n\n    assert uniquely_named_symbol('x', modify=custom_modify) == x_custom\n    assert uniquely_named_symbol('x', (x, x_custom), modify=custom_modify).name == 'x_custom_custom'\n\n    y = Symbol('y')\n    assert uniquely_named_symbol('x', (x, y), modify=custom_modify).name == 'x_custom'\n", "def test_Wild_exclude():\n    x, y, z = symbols('x y z')\n    a = Wild('a', exclude=[x])\n    b = Wild('b', exclude=[y])\n    c = Wild('c', exclude=[x, y])\n\n    # 'a' should not match any expression containing 'x'\n    assert (2*y).match(a*y) == {a: 2}\n    assert (2*x).match(a*x) is None\n    assert (x*y).match(a*y) is None\n\n    # 'b' should not match any expression containing 'y'\n    assert (2*x).match(b*x) == {b: 2}\n    assert (2*y).match(b*y) is None\n    assert (x*y).match(b*y) is None\n\n    # 'c' should not match any expression containing 'x' or 'y'\n    assert (2*z).match(c*z) == {c: 2}\n    assert (2*x).match(c*x) is None\n    assert (2*y).match(c*y) is None\n    assert (x*y).match(c*x*y) is None\n", "def test_Wild_exclude():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    a = Symbol('a')\n    b = Symbol('b')\n\n    w = Wild('w', exclude=[x, y])\n\n    assert w.matches(z) == {w: z}\n    assert w.matches(a) == {w: a}\n    assert w.matches(b) == {w: b}\n    assert w.matches(x) is None\n    assert w.matches(y) is None\n\n    w2 = Wild('w2', exclude=[z, a])\n    \n    assert w2.matches(x) == {w2: x}\n    assert w2.matches(y) == {w2: y}\n    assert w2.matches(b) == {w2: b}\n    assert w2.matches(z) is None\n    assert w2.matches(a) is None\n", "def test_Wild_exclude():\n    a, b, c, x = symbols('a b c x')\n    \n    # Wild that excludes 'a'\n    w = Wild('w', exclude=[a])\n    \n    # Should not match 'a'\n    assert a.match(w) is None\n\n    # Should match other symbols\n    assert x.match(w) == {w: x}\n    assert b.match(w) == {w: b}\n    assert c.match(w) == {w: c}\n"], "sample_1193": ["def test_are_similar():\n    from sympy import Circle, Triangle\n\n    # Similar Triangles\n    t1 = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n    t2 = Triangle(Point(0, 0), Point(2, 0), Point(0, 2))\n    t3 = Triangle(Point(0, 0), Point(3, 0), Point(0, 1))\n    assert are_similar(t1, t2) == True\n    assert are_similar(t1, t3) == False\n\n    # Similar Circles\n    c1 = Circle(Point(0, 0), 4)\n    c2 = Circle(Point(1, 4), 3)\n    c3 = Circle(Point(0, 0), 4)\n    assert are_similar(c1, c2) == True\n    assert are_similar(c1, c3) == True\n\n    # Circle and Triangle are not comparable\n    raises(GeometryError, lambda: are_similar(c1, t1))\n", "def test_are_similar():\n    from sympy.geometry import Circle, Triangle\n    \n    c1, c2 = Circle(Point(0, 0), 4), Circle(Point(1, 4), 3)\n    t1 = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n    t2 = Triangle(Point(0, 0), Point(2, 0), Point(0, 2))\n    t3 = Triangle(Point(0, 0), Point(3, 0), Point(0, 1))\n    \n    assert are_similar(c1, c2) == True\n    assert are_similar(t1, t2) == True\n    assert are_similar(t1, t3) == False\n    raises(GeometryError, lambda: are_similar(c1, t1))\n", "def test_find():\n    from sympy.abc import x, y\n    from sympy import Dummy\n\n    assert find('x', x) == x\n    assert find('x', Dummy('x')).name == 'x'\n    raises(ValueError, lambda: find(x, Dummy('x')))\n    raises(ValueError, lambda: find('y', x))\n    assert find('y', y + x) == y\n    raises(ValueError, lambda: find('z', y + x))\n", "def test_are_similar():\n    from sympy import Circle, Triangle\n\n    # Create geometrical entities\n    c1 = Circle(Point(0, 0), 4)\n    c2 = Circle(Point(1, 4), 4)\n    c3 = Circle(Point(2, 2), 2)\n    t1 = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n    t2 = Triangle(Point(0, 0), Point(2, 0), Point(0, 2))\n    t3 = Triangle(Point(0, 0), Point(3, 0), Point(0, 1))\n\n    # Test for similarity\n    assert are_similar(c1, c2) == True  # Same radius, different center\n    assert are_similar(c1, c3) == False  # Different radius\n    assert are_similar(t1, t2) == True  # Uniformly scaled triangles\n    assert are_similar(t1, t3) == False  # Non-uniform scaling\n\n    # Edge cases\n    raises(GeometryError, lambda: are_similar(c1, t1))  # Different types\n    raises(GeometryError, lambda: are_similar(Point(0, 0), c1))  # Point and Circle\n", "def test_are_similar():\n    from sympy.geometry import Circle, Triangle\n\n    c1 = Circle(Point(0, 0), 4)\n    c2 = Circle(Point(1, 4), 3)\n    t1 = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n    t2 = Triangle(Point(0, 0), Point(2, 0), Point(0, 2))\n    t3 = Triangle(Point(0, 0), Point(3, 0), Point(0, 1))\n    assert are_similar(c1, c2) == True\n    assert are_similar(c1, t1) == False\n    assert are_similar(t1, t2) == True\n    assert are_similar(t1, t3) == False\n    raises(GeometryError, lambda: are_similar(c1, \"string\"))\n"], "sample_1194": ["def test_special_constants():\n    assert julia_code(S.Pi) == \"pi\"\n    assert julia_code(S.Exp1) == \"e\"\n    assert julia_code(S.EulerGamma) == \"eulergamma\"\n    assert julia_code(S.GoldenRatio) == \"golden\"\n    assert julia_code(S.Catalan) == \"catalan\"\n", "def test_user_defined_functions():\n    g = implemented_function('g', Lambda(x, 2*x))\n    custom_functions = {\n        \"g\": \"custom_g\"\n    }\n    assert julia_code(g(x), user_functions=custom_functions) == \"custom_g(x)\"\n    assert julia_code(g(x) + sin(x), user_functions=custom_functions) == \"custom_g(x) + sin(x)\"\n    assert julia_code(g(x) * cos(x), user_functions=custom_functions) == \"custom_g(x) .* cos(x)\"\n", "def test_user_defined_functions():\n    g = Function('g')\n    custom_functions = {\"g\": \"custom_g\"}\n    assert julia_code(g(x), user_functions=custom_functions) == \"custom_g(x)\"\n    assert julia_code(g(x*y), user_functions=custom_functions) == \"custom_g(x .* y)\"\n    custom_functions_with_conditions = {\n        \"g\": [(lambda x: x.is_Matrix, \"custom_g_matrix\"),\n              (lambda x: not x.is_Matrix, \"custom_g_scalar\")]\n    }\n    assert julia_code(g(Matrix([[1, 2], [3, 4]])), user_functions=custom_functions_with_conditions) == \"custom_g_matrix([1 2;\\n3 4])\"\n    assert julia_code(g(x), user_functions=custom_functions_with_conditions) == \"custom_g_scalar(x)\"\n", "def test_custom_user_functions():\n    myfunc = Function('myfunc')\n    custom_func = {myfunc: \"my_custom_func\"}\n    assert julia_code(myfunc(x), user_functions=custom_func) == \"my_custom_func(x)\"\n    # Test with a function that has different names for different arguments\n    g = Function('g')\n    custom_func_multi = {g: [(lambda x: x.is_Matrix, \"matrix_func\"), (lambda x: not x.is_Matrix, \"scalar_func\")]}\n    assert julia_code(g(x), user_functions=custom_func_multi) == \"scalar_func(x)\"\n    assert julia_code(g(Matrix([[x, y]])), user_functions=custom_func_multi) == \"matrix_func([x y])\"\n", "def test_Indexed():\n    from sympy.tensor.indexed import IndexedBase, Idx\n    i, j = symbols('i j')\n    A = IndexedBase('A')\n    expr = A[i, j] + A[i+1, j]\n    assert julia_code(expr) == \"A[i, j] + A[i + 1, j]\"\n    expr = A[i, j] * A[i, j+1]\n    assert julia_code(expr) == \"A[i, j] .* A[i, j + 1]\"\n    expr = A[i, j] / A[i+1, j]\n    assert julia_code(expr) == \"A[i, j] ./ A[i + 1, j]\"\n    expr = A[i, j] - A[i, j+1]\n    assert julia_code(expr) == \"A[i, j] - A[i, j + 1]\"\n"], "sample_1195": ["def test_simplify_gpgp():\n    mu, nu, rho, sigma = tensor_indices(\"mu, nu, rho, sigma\", LorentzIndex)\n    a1, a2, a3, a4 = tensor_indices(\"a1:5\", LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n\n    # test case 1: simplify product of gamma matrices with momenta\n    t = G(mu) * p(-mu) * G(nu) * p(-nu)\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, p(mu) * p(-mu))\n\n    # test case 2: more complex product\n    t = G(mu) * p(-mu) * G(nu) * q(-nu) * G(rho) * p(-rho)\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, G(rho) * p(mu) * p(-mu) * q(-rho))\n\n    # test case 3: product with a mix of gamma matrices and momenta\n    t = G(a1) * p(-a1) * G(a2) * q(-a2) * G(a3) * p(-a3) * G(a4) * q(-a4)\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, p(a3) * p(-a3) * q(a1) * q(-a1))\n\n    # test case 4: already simplified expression\n    t = p(mu) * p(-mu)\n    r = simplify_gpgp(t)\n    assert _is_tensor_eq(r, p(mu) * p(-mu))\n", "def test_extract_type_tens():\n    i0, i1, i2 = tensor_indices('i0:3', LorentzIndex)\n    t = G(i0) * G(i1) * G(i2)\n    component = GammaMatrix\n    \n    # Test extraction of GammaMatrix tensors\n    extracted, residual = extract_type_tens(t, component)\n    assert _is_tensor_eq(extracted, t)\n    assert _is_tensor_eq(residual, S.One)\n    \n    A = TensorHead('A', [LorentzIndex])\n    t = G(i0) * A(i1) * G(i2)\n    \n    extracted, residual = extract_type_tens(t, component)\n    assert _is_tensor_eq(extracted, G(i0) * G(i2))\n    assert _is_tensor_eq(residual, A(i1))\n    \n    # Test with no GammaMatrix tensors\n    t = A(i0) * A(i1)\n    extracted, residual = extract_type_tens(t, component)\n    assert _is_tensor_eq(extracted, S.One)\n    assert _is_tensor_eq(residual, t)\n    \n    # Test with single GammaMatrix tensor\n    t = G(i0)\n    extracted, residual = extract_type_tens(t, component)\n    assert _is_tensor_eq(extracted, t)\n    assert _is_tensor_eq(residual, S.One)\n", "def test_simplify_gpgp():\n    from sympy.tensor.tensor import tensor_heads\n\n    # Setup tensor indices and heads\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    i0, i1, i2, i3, i4, i5 = tensor_indices('i0:6', LorentzIndex)\n\n    # Test example from docstring\n    ps = p(i0) * G(-i0)\n    qs = q(i0) * G(-i0)\n    expr = ps * qs * qs\n    simplified_expr = simplify_gpgp(expr)\n    expected_result = G(-i0) * p(i0) * q(i1) * q(-i1)\n    assert _is_tensor_eq(simplified_expr, expected_result)\n\n    # Additional test cases\n    expr = G(i0) * p(-i0) * G(i1) * p(-i1)\n    simplified_expr = simplify_gpgp(expr)\n    expected_result = p(i0) * p(-i0) * G(i1)\n    assert _is_tensor_eq(simplified_expr, expected_result)\n\n    expr = G(i0) * G(i1) * p(-i0) * p(-i1)\n    simplified_expr = simplify_gpgp(expr)\n    expected_result = G(i0) * G(i1) * p(-i0) * p(-i1)\n    assert _is_tensor_eq(simplified_expr, expected_result)\n\n    expr = G(i0) * p(-i0) * G(i1) * p(-i1) * G(i2) * p(-i2)\n    simplified_expr = simplify_gpgp(expr)\n    expected_result = p(i0) * p(-i0) * G(i1) * p(-i1) * G(i2) * p(-i2)\n    assert _is_tensor_eq(simplified_expr, expected_result)\n", "def test_extract_type_tens():\n    i, j = tensor_indices('i,j', LorentzIndex)\n    A = TensorHead('A', [LorentzIndex])\n    \n    t = A(i) * G(j)\n    extracted, residual = extract_type_tens(t, GammaMatrix)\n    assert _is_tensor_eq(extracted, G(j))\n    assert _is_tensor_eq(residual, A(i))\n\n    t = A(i) * A(j) * G(i) * G(j)\n    extracted, residual = extract_type_tens(t, GammaMatrix)\n    assert _is_tensor_eq(extracted, G(i) * G(j))\n    assert _is_tensor_eq(residual, A(i) * A(j))\n\n    t = G(i) * G(j)\n    extracted, residual = extract_type_tens(t, GammaMatrix)\n    assert _is_tensor_eq(extracted, G(i) * G(j))\n    assert _is_tensor_eq(residual, 1)\n    \n    t = A(i) * A(j)\n    extracted, residual = extract_type_tens(t, GammaMatrix)\n    assert _is_tensor_eq(extracted, 1)\n    assert _is_tensor_eq(residual, A(i) * A(j))\n", "def test_simplify_gpgp():\n    i0, i1, i2, i3, i4, i5 = tensor_indices('i0:6', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    \n    # Test 1: Simplifying a single gamma matrix product\n    t = G(i0)*p(-i0)\n    r = simplify_gpgp(t)\n    assert r.equals(t)  # should return the same as input since there's nothing to simplify\n    \n    # Test 2: Simplifying a product of gamma matrices and momenta\n    t = G(i0)*p(-i0)*G(i1)*p(-i1)\n    r = simplify_gpgp(t)\n    assert r.equals(p(i0)*p(-i0))  # should simplify to p(i0)*p(-i0)\n    \n    # Test 3: More complex product with repeated terms\n    t = G(i0)*p(-i0)*G(i1)*p(-i1)*G(i2)*p(-i2)\n    r = simplify_gpgp(t)\n    assert r.equals(p(i0)*p(-i0)*G(i2)*p(-i2))  # should simplify partially\n    \n    # Test 4: Mixed gamma matrices and different momenta\n    t = p(i0)*G(-i0)*q(i1)*G(-i1)*p(i2)*G(-i2)\n    r = simplify_gpgp(t)\n    assert r.equals(p(i0)*q(i1)*p(i2)*G(-i2)*G(-i1)*G(-i0))  # should not simplify completely\n    \n    # Test 5: Handling already simplified input\n    t = p(i0)*p(-i0)\n    r = simplify_gpgp(t)\n    assert r.equals(t)  # should return the same as input\n    \n    # Test 6: Nested gamma matrices and momenta\n    t = G(i0)*p(-i0)*G(i1)*p(-i1)*G(i2)*p(-i2)*G(i3)*p(-i3)\n    r = simplify_gpgp(t)\n    assert r.equals(p(i0)*p(-i0)*p(i2)*p(-i2))  # expected simplified form\n"], "sample_1196": ["def test_contains_with_non_boolean_return():\n    # Create a mock set class to return a non-boolean, non-Contains, non-Set value\n    class MockSet(Set):\n            return \"non-boolean\"\n\n    x = Symbol('x')\n    mock_set = MockSet()\n\n    # Ensure Contains correctly handles this case\n    assert Contains(x, mock_set) is None\n", "def test_contains_eval():\n    x = Symbol('x')\n    s = Interval(0, 10)\n    # Test when the return value is not an instance of Contains\n    assert Contains.eval(x, s) == s.contains(x)\n    assert Contains.eval(5, s) is S.true\n    assert Contains.eval(-1, s) is S.false\n    \n    # Test when s is not a Set\n    raises(TypeError, lambda: Contains.eval(x, 42))\n", "def test_contains_with_non_boolean_return():\n    class CustomSet(Set):\n            return 'non_boolean_value'\n\n    custom_set = CustomSet()\n    x = Symbol('x')\n    assert Contains(x, custom_set) is None\n", "def test_contains_with_custom_set():\n    class CustomSet(Set):\n            if other == 3:\n                return S.true\n            return S.false\n\n    custom_set = CustomSet()\n\n    assert Contains(3, custom_set) is S.true\n    assert Contains(4, custom_set) is S.false\n", "def test_contains_with_custom_set():\n    class CustomSet(Set):\n            return S.true if other == 42 else S.false\n\n    assert Contains(42, CustomSet()) is S.true\n    assert Contains(43, CustomSet()) is S.false\n\n    raises(TypeError, lambda: Contains(42, \"NotASet\"))\n"], "sample_1197": ["def test_get_units_non_prefixed():\n    non_prefixed_units = SI.get_units_non_prefixed()\n    expected_units = {\n        meter, gram, second, ampere, kelvin, mole, candela,\n        radian, steradian, hertz, newton, pascal, joule, watt,\n        coulomb, volt, farad, ohm, siemens, weber, tesla, henry,\n        lumen, lux, becquerel, gray, sievert, katal\n    }\n    assert non_prefixed_units == expected_units\n", "def test_extend_unit_system():\n    u1 = Quantity(\"u1\")\n    u2 = Quantity(\"u2\")\n    u3 = Quantity(\"u3\")\n\n    u1.set_global_relative_scale_factor(1, meter)\n    u2.set_global_relative_scale_factor(1, second)\n    u3.set_global_relative_scale_factor(1, meter/second)\n    \n    base_units = (u1, u2)\n    units = (u3, )\n    dimension_system = SI.get_dimension_system()\n    derived_units = {length/time: u3}\n\n    us = UnitSystem(base_units, units, \"Custom\", \"Custom unit system\", dimension_system, derived_units)\n    new_us = us.extend((u3,), (u2,), \"Extended\", \"Extended unit system\")\n\n    assert new_us.name == \"Extended\"\n    assert new_us.descr == \"Extended unit system\"\n    assert u1 in new_us._base_units\n    assert u2 in new_us._units\n    assert u3 in new_us._base_units\n    assert new_us.derived_units[length/time] == u3\n", "def test_get_units_non_prefixed():\n    u1 = Quantity(\"u1\")\n    u2 = Quantity(\"u2\")\n    u3 = Quantity(\"u3\")\n    u4 = Quantity(\"u4\")\n    u5 = Quantity(\"u5\")\n    u1.set_global_relative_scale_factor(S(1), meter)\n    u2.set_global_relative_scale_factor(S(1), kilometer)\n    u3.set_global_relative_scale_factor(S(1), kilogram)\n    u4.set_global_relative_scale_factor(S(1), joule)\n    u5.set_global_relative_scale_factor(S(1), kilometer)\n    us = UnitSystem((u1, u4), (u2, u3, u5))\n\n    non_prefixed_units = us.get_units_non_prefixed()\n\n    assert u1 in non_prefixed_units\n    assert u4 in non_prefixed_units\n    assert u2 not in non_prefixed_units\n    assert u3 not in non_prefixed_units\n    assert u5 not in non_prefixed_units\n", "def test_extend_unit_system():\n    # Create base units and a UnitSystem instance\n    base_units = [meter, second]\n    derived_units = {length: kilometer, time: minute}\n    us = UnitSystem(base_units, name=\"TestSystem\", derived_units=derived_units)\n\n    # Extend the UnitSystem\n    extended_units = [joule, coulomb]\n    new_derived_units = {energy: joule, charge: coulomb}\n    extended_us = us.extend(base=extended_units, derived_units=new_derived_units)\n\n    # Check properties of the extended UnitSystem\n    assert isinstance(extended_us, UnitSystem)\n    assert extended_us.name == \"\"\n    assert set(extended_us._base_units) == set(base_units + extended_units)\n    assert extended_us._derived_units[energy] == joule\n    assert extended_us._derived_units[charge] == coulomb\n\n    # Check that the original UnitSystem remains unchanged\n    assert us.name == \"TestSystem\"\n    assert set(us._base_units) == set(base_units)\n    assert us._derived_units[length] == kilometer\n    assert us._derived_units[time] == minute\n", "def test_get_units_non_prefixed():\n    unit_system = UnitSystem(base_units=(meter, kilogram, second), units=(meter, kilogram, second, kilometer, gram, joule, volt, ohm), name=\"TestSystem\")\n\n    non_prefixed_units = unit_system.get_units_non_prefixed()\n    expected_units = {meter, kilogram, second, joule, volt, ohm}\n\n    assert non_prefixed_units == expected_units\n"], "sample_1198": ["def test_mathematica_parser_custom_translations():\n    # Custom translations\n    custom_translations = {\n        'MySin[x]': 'sin(x)',\n        'MyCos[x]': 'cos(x)',\n        'MyLog[x,y]': 'log(y, x)',\n        'MyExp[x]': 'exp(x)'\n    }\n\n    parser = MathematicaParser(additional_translations=custom_translations)\n\n    assert parser._convert_function('MySin[x]') == 'sin(x)'\n    assert parser._convert_function('MyCos[y]') == 'cos(y)'\n    assert parser._convert_function('MyLog[a, b]') == 'log(b, a)'\n    assert parser._convert_function('MyExp[z]') == 'exp(z)'\n\n    # Test parsing Mathematica expression with custom translations\n    assert parse_mathematica('MySin[x] + MyCos[y]') == sympify('sin(x) + cos(y)')\n    assert parse_mathematica('MyLog[2, 8] * MyExp[3]') == sympify('log(8, 2) * exp(3)')\n", "def test_parser_mathematica_edge_cases():\n    d = {\n        'Sin[Exp[Log[4]]]^2': 'sin(exp(log(4)))**2',\n        'Cos[ArcTan[x, y]]': 'cos(atan2(y, x))',\n        'Sin[x+Cos[y+Tan[z]]]': 'sin(x+cos(y+tan(z)))',\n        'Sin[Exp[Cos[Pi]]]': 'sin(exp(cos(pi)))',\n        'Exp[Sin[Cos[Pi]]]': 'exp(sin(cos(pi)))',\n        'Log[Sin[Exp[Cos[Pi]]]]': 'log(sin(exp(cos(pi))))',\n        'Cos[Sin[Exp[Log[4]]]]': 'cos(sin(exp(log(4))))',\n        'Tan[Cos[Sin[Exp[Log[4]]]]]': 'tan(cos(sin(exp(log(4)))))',\n        'ArcSin[ArcCos[ArcTan[x]]]': 'asin(acos(atan(x)))',\n        'Cos[ArcSin[Sin[Cos[Pi]]]]': 'cos(asin(sin(cos(pi))))',\n        'Sin[Exp[Log2[16]]]': 'sin(exp(log(16, 2)))',\n        'Exp[Log[Pi]]': 'exp(log(pi))',\n        'Sin[Exp[Log2[Pi]]]': 'sin(exp(log(pi, 2)))',\n        'Sin[Exp[Log10[Pi]]]': 'sin(exp(log(pi, 10)))',\n        'Sin[ExpIntegralEi[Pi]]': 'sin(Ei(pi))',\n        'Sin[CosIntegral[Pi]]': 'sin(Ci(pi))',\n        'Sin[SinIntegral[Pi]]': 'sin(Si(pi))',\n        'AiryAi[Exp[Log[4]]]': 'airyai(exp(log(4)))',\n        'AiryBi[AiryAiPrime[5]]': 'airybi(airyaiprime(5))',\n        'PrimeQ[AiryBiPrime[7]]': 'isprime(airybiprime(7))'\n    }\n\n    for e in d:\n        assert parse_mathematica(e) == sympify(d[e])\n\n    # Edge cases involving Optional and Pattern functions\n    assert parse_mathematica('f[x_, y_.] := x + y') == sympify('SetDelayed", "def test_parser_mathematica_with_complex_expressions():\n    # Test various complex expressions to ensure correct parsing\n    complex_expressions = {\n        '((x + y) * (a + b)) / (c + d)': '(x + y)*(a + b)/(c + d)',\n        'Sin[x]^2 + Cos[y]^2 == 1': 'Eq(sin(x)**2 + cos(y)**2, 1)',\n        'Exp[I * Pi * x] + Exp[-I * Pi * x]': 'exp(I*pi*x) + exp(-I*pi*x)',\n        'Log[2, x*y]': 'log(x*y, 2)',\n        'Sqrt[x^2 + y^2]': 'sqrt(x**2 + y**2)',\n        'Max[Sin[x], Cos[y]]': 'Max(sin(x), cos(y))',\n        'Min[Exp[x], Log[y]]': 'Min(exp(x), log(y))',\n        'PrimeQ[Factorial[5]]': 'isprime(factorial(5))',\n        'Sum[x^n, {n, 0, Infinity}]': 'Sum(x**n, (n, 0, oo))',\n        'Integrate[Sin[x], {x, 0, Pi}]': 'Integral(sin(x), (x, 0, pi))'\n    }\n\n    for expr, expected in complex_expressions.items():\n        assert parse_mathematica(expr) == sympify(expected)\n", "def test_parser_mathematica_custom_translations():\n    custom_translations = {\n        'CustomFunc[x]': 'sin(x)**2 + cos(x)**2',\n        'MultiplyByPi[x]': 'pi*x',\n        'SquarePlusOne[x]': 'x**2 + 1'\n    }\n\n    parser = MathematicaParser(additional_translations=custom_translations)\n\n    assert parser._convert_function('CustomFunc[y]') == 'sin(y)**2 + cos(y)**2'\n    assert parser._convert_function('MultiplyByPi[z]') == 'pi*z'\n    assert parser._convert_function('SquarePlusOne[a]') == 'a**2 + 1'\n    assert parse_mathematica('CustomFunc[x]', additional_translations=custom_translations) == sympify('sin(x)**2 + cos(x)**2')\n    assert parse_mathematica('MultiplyByPi[w]', additional_translations=custom_translations) == sympify('pi*w')\n    assert parse_mathematica('SquarePlusOne[n]', additional_translations=custom_translations) == sympify('n**2 + 1')\n\n    # Test with function that is in both predefined and custom translations\n    custom_translations_update = {\n        'Sqrt[x]': 'x**0.5'\n    }\n    parser_update = MathematicaParser(additional_translations=custom_translations_update)\n    assert parser_update._convert_function('Sqrt[4]') == '4**0.5'\n    assert parse_mathematica('Sqrt[9]', additional_translations=custom_translations_update) == sympify('9**0.5')\n", "def test_parser_mathematica_advanced_cases():\n    # Testing more complex expressions\n    assert parse_mathematica(\"Derivative[2][Cos][x]\") == sympify(\"diff(cos(x), x, 2)\")\n    assert parse_mathematica(\"Sum[x^i, {i, 1, 10}]\") == sympify(\"Sum(x**i, (i, 1, 10))\")\n    assert parse_mathematica(\"Limit[Sin[x]/x, x -> 0]\") == sympify(\"limit(sin(x)/x, x, 0)\")\n    assert parse_mathematica(\"Product[x^i, {i, 1, 10}]\") == sympify(\"Product(x**i, (i, 1, 10))\")\n    assert parse_mathematica(\"Series[Exp[x], {x, 0, 5}]\") == sympify(\"series(exp(x), x, 0, 6)\")\n\n    # Testing complex numbers\n    assert parse_mathematica(\"Complex[1, 2]\") == sympify(\"1 + 2*I\")\n    assert parse_mathematica(\"Conjugate[1 + 2 I]\") == sympify(\"1 - 2*I\")\n\n    # Testing piecewise functions\n    assert parse_mathematica(\"Piecewise[{{x^2, x < 1}, {x, x >= 1}}]\") == sympify(\"Piecewise((x**2, x < 1), (x, x >= 1))\")\n\n    # Testing differentials\n    assert parse_mathematica(\"D[Sin[x^2], x]\") == sympify(\"diff(sin(x**2), x)\")\n    assert parse_mathematica(\"Integrate[Sin[x], x]\") == sympify(\"integrate(sin(x), x)\")\n\n    # Testing symbolic matrix\n    assert parse_mathematica(\"{{a, b}, {c, d}}\") == sympify(\"Matrix([[a, b], [c, d]])\")\n\n    # Test invalid Mathematica syntax which should raise appropriate errors\n    with raises(SyntaxError):\n        parse_mathematica(\"a + b +\")\n    with raises(SyntaxError):\n        parse_mathematica(\"a * [b, c]\")\n    with raises(SyntaxError):\n        parse_mathematica(\"a[b"], "sample_1199": ["def test_combined_tensor_printing():\n    # Test the combined tensor printing functionality\n    from sympy.physics.quantum.tensorproduct import combined_tensor_printing\n    \n    combined_tensor_printing(True)\n    ket1 = Ket('a')\n    ket2 = Ket('b')\n    assert str(TensorProduct(ket1, ket2)) == '|a,b>'\n    \n    combined_tensor_printing(False)\n    assert str(TensorProduct(ket1, ket2)) == '|a>x|b>'\n    \n    combined_tensor_printing(True)\n    bra1 = Bra('a')\n    bra2 = Bra('b')\n    assert str(TensorProduct(bra1, bra2)) == '<a,b|'\n    \n    combined_tensor_printing(False)\n    assert str(TensorProduct(bra1, bra2)) == '<a|x<b>'\n", "def test_combined_tensor_printing():\n    from sympy.physics.quantum.tensorproduct import combined_tensor_printing, _combined_printing\n    from sympy.physics.quantum.state import Ket, Bra\n\n    k1 = Ket('0')\n    k2 = Ket('1')\n    b1 = Bra('0')\n    b2 = Bra('1')\n\n    # Test combined bra/ket printing\n    combined_tensor_printing(True)\n    assert _combined_printing is True\n    assert str(TensorProduct(k1, k2)) == '|0, 1>'\n    assert str(TensorProduct(b1, b2)) == '<0, 1|'\n\n    # Test explicit tensor product notation\n    combined_tensor_printing(False)\n    assert _combined_printing is False\n    assert str(TensorProduct(k1, k2)) == '|0>x|1>'\n    assert str(TensorProduct(b1, b2)) == '<0|x<1|'\n", "def test_tensor_product_latex():\n    from sympy.printing.latex import latex\n\n    # Test latex printing of TensorProduct with combined printing\n    from sympy.physics.quantum.tensorproduct import combined_tensor_printing\n    combined_tensor_printing(True)\n\n    ket1 = Qubit(0)\n    ket2 = Qubit(1)\n    tp_ket = TensorProduct(ket1, ket2)\n    assert latex(tp_ket) == r'{\\left|{0, 1}\\right\\rangle}'\n\n    bra1 = QubitBra(0)\n    bra2 = QubitBra(1)\n    tp_bra = TensorProduct(bra1, bra2)\n    assert latex(tp_bra) == r'{\\left\\langle{0, 1}\\right|}'\n\n    combined_tensor_printing(False)\n\n    # Test latex printing of TensorProduct without combined printing\n    assert latex(tp_ket) == r'{\\left|{0}\\right\\rangle} \\otimes {\\left|{1}\\right\\rangle}'\n    assert latex(tp_bra) == r'{\\left\\langle{0}\\right|} \\otimes {\\left\\langle{1}\\right|}'\n", "def test_tensor_product_latex():\n    from sympy.printing.latex import latex\n    assert latex(TensorProduct(A, B)) == r'{A} \\otimes {B}'\n    assert latex(TensorProduct(A + B, C)) == r'{\\left(A + B\\right)} \\otimes {C}'\n    assert latex(TensorProduct(A, B + C)) == r'{A} \\otimes {\\left(B + C\\right)}'\n    assert latex(TensorProduct(A*B, C)) == r'{A B} \\otimes {C}'\n", "def test_tensor_product_matrix_input():\n    m1 = Matrix([[1, 2], [3, 4]])\n    m2 = Matrix([[0, 5], [6, 7]])\n    result = TensorProduct(m1, m2)\n    expected = Matrix([\n        [0, 5, 0, 10],\n        [6, 7, 12, 14],\n        [0, 15, 0, 20],\n        [18, 21, 24, 28]\n    ])\n    assert result == expected\n\n    spm1 = SparseMatrix([[1, 0], [0, 1]])\n    spm2 = SparseMatrix([[0, 1], [1, 0]])\n    result_spm = TensorProduct(spm1, spm2)\n    expected_spm = SparseMatrix([\n        [0, 1, 0, 0],\n        [1, 0, 0, 0],\n        [0, 0, 0, 1],\n        [0, 0, 1, 0]\n    ])\n    assert result_spm == expected_spm\n"], "sample_1200": ["def test_extend():\n    from sympy.physics.units.systems.si import dimsys_SI\n\n    base_units = (meter, second)\n    units = (kilometer, hour)\n    derived_units = {length: kilometer, time: hour}\n    us = UnitSystem(base_units, units, \"custom\", \"A custom unit system\", dimsys_SI, derived_units)\n\n    extended_us = us.extend((kilogram,), (gram,), name=\"extended_custom\", description=\"An extended custom unit system\")\n\n    assert extended_us.name == \"extended_custom\"\n    assert extended_us.descr == \"An extended custom unit system\"\n    assert set(extended_us._base_units) == {meter, second, kilogram}\n    assert set(extended_us._units) == {meter, second, kilometer, hour, kilogram, gram}\n    assert extended_us._derived_units[length] == kilometer\n    assert extended_us._derived_units[time] == hour\n\n    # Check if the original unit system is unaffected\n    assert us.name == \"custom\"\n    assert us.descr == \"A custom unit system\"\n    assert set(us._base_units) == {meter, second}\n    assert set(us._units) == {meter, second, kilometer, hour}\n    assert us._derived_units[length] == kilometer\n    assert us._derived_units[time] == hour\n", "def test_get_units_non_prefixed():\n    from sympy.physics.units import gram, kilogram, meter, centimeter, second, minute\n\n    base_units = [meter, kilogram, second]\n    units = [gram, centimeter, minute]\n    name = \"TestSystem\"\n    descr = \"A test unit system.\"\n\n    test_system = UnitSystem(base_units, units, name, descr)\n\n    non_prefixed_units = test_system.get_units_non_prefixed()\n\n    assert meter in non_prefixed_units\n    assert kilogram in non_prefixed_units\n    assert second in non_prefixed_units\n    assert gram not in non_prefixed_units\n    assert centimeter not in non_prefixed_units\n    assert minute not in non_prefixed_units\n", "def test_get_units_non_prefixed():\n    # Create some units with and without prefixes\n    u1 = Quantity(\"meter\")\n    u2 = Quantity(\"centimeter\")\n    u3 = Quantity(\"kilometer\")\n    u4 = Quantity(\"second\")\n    u5 = Quantity(\"millisecond\")\n\n    # Create a UnitSystem including these units\n    us = UnitSystem(base_units=[u1, u4], units=[u2, u3, u5])\n\n    # Check get_units_non_prefixed\n    non_prefixed_units = us.get_units_non_prefixed()\n    \n    assert u1 in non_prefixed_units\n    assert u4 in non_prefixed_units\n    assert u2 not in non_prefixed_units\n    assert u3 not in non_prefixed_units\n    assert u5 not in non_prefixed_units\n", "def test_get_units_non_prefixed():\n    # Set up some quantities\n    meter_unprefixed = Quantity(\"meter_unprefixed\")\n    kilometer_prefixed = Quantity(\"kilometer_prefixed\")\n    kilogram_prefixed = Quantity(\"kilogram_prefixed\")\n    joule_unprefixed = Quantity(\"joule_unprefixed\")\n    physical_constant = Quantity(\"physical_constant\", abbrev=\"phys_const\")\n    \n    # Set global relative scale factors\n    meter_unprefixed.set_global_relative_scale_factor(1, meter)\n    kilometer_prefixed.set_global_relative_scale_factor(kilo, meter)\n    kilogram_prefixed.set_global_relative_scale_factor(kilo, gram)\n    joule_unprefixed.set_global_relative_scale_factor(1, joule)\n    physical_constant.set_global_relative_scale_factor(1, joule)  # Assume some physical constant with joule dimension\n\n    # Set is_physical_constant property\n    physical_constant.is_physical_constant = True\n\n    # Initialize the UnitSystem\n    us = UnitSystem([meter_unprefixed, joule_unprefixed], [kilometer_prefixed, kilogram_prefixed, physical_constant])\n\n    # Get non-prefixed units\n    non_prefixed_units = us.get_units_non_prefixed()\n\n    # Check that only unprefixed, non-constant units are in the set\n    assert meter_unprefixed in non_prefixed_units\n    assert joule_unprefixed in non_prefixed_units\n    assert kilometer_prefixed not in non_prefixed_units\n    assert kilogram_prefixed not in non_prefixed_units\n    assert physical_constant not in non_prefixed_units\n", "def test_get_units_non_prefixed():\n    meter_unit = Quantity(\"meter\")\n    kilometer_unit = Quantity(\"kilometer\")\n    kilometer_unit.set_global_relative_scale_factor(kilo, meter_unit)\n\n    unit_system = UnitSystem([meter_unit], [kilometer_unit])\n\n    non_prefixed_units = unit_system.get_units_non_prefixed()\n\n    assert meter_unit in non_prefixed_units\n    assert kilometer_unit not in non_prefixed_units\n"], "sample_1201": ["def test_cgs_gauss_other_units():\n\n    assert convert_to(erg, joule, cgs_gauss) == joule/10**7\n    assert convert_to(joule, erg, cgs_gauss) == 10**7*erg\n\n    assert convert_to(dyne, newton, cgs_gauss) == newton/10**5\n    assert convert_to(newton, dyne, cgs_gauss) == 10**5*dyne\n\n    assert convert_to(statampere, ampere, cgs_gauss) == 10*speed_of_light*statcoulomb/second\n    assert convert_to(ampere, statampere, cgs_gauss) == statcoulomb/second / (10*speed_of_light)\n\n    assert convert_to(statvolt, volt, cgs_gauss) == 10**6*statvolt/speed_of_light\n    assert convert_to(volt, statvolt, cgs_gauss) == speed_of_light / (10**6*statvolt)\n\n    assert convert_to(maxwell, weber, cgs_gauss) == 10**8*maxwell\n    assert convert_to(weber, maxwell, cgs_gauss) == maxwell / (10**8)\n\n    assert convert_to(gauss, tesla, cgs_gauss) == 10**4*gauss\n    assert convert_to(tesla, gauss, cgs_gauss) == gauss / (10**4)\n", "def test_cgs_gauss_convert_additional_units():\n    assert convert_to(ampere, statampere, cgs_gauss) == 10 * speed_of_light * statampere / second\n    assert convert_to(statampere, ampere, cgs_gauss) == ampere / (10 * speed_of_light)\n    assert convert_to(weber, maxwell, cgs_gauss) == 10**8 * maxwell\n    assert convert_to(maxwell, weber, cgs_gauss) == weber / 10**8\n    assert convert_to(tesla, gauss, cgs_gauss) == 10**4 * gauss\n    assert convert_to(gauss, tesla, cgs_gauss) == tesla / 10**4\n    assert convert_to(debye, statcoulomb * centimeter, cgs_gauss) == statcoulomb * centimeter / 10**18\n    assert convert_to(oersted, sqrt(gram / centimeter) / second, cgs_gauss) == sqrt(gram / centimeter) / second\n    assert convert_to(farad, centimeter, cgs_gauss) == speed_of_light**2 * centimeter / 10**5\n    assert convert_to(henry, second**2 / centimeter, cgs_gauss) == 10**5 / speed_of_light**2 / centimeter * second**2\n", "def test_cgs_gauss_additional_conversions():\n    # Testing additional unit conversions in CGS Gaussian system\n\n    # Magnetic field density (gauss to tesla)\n    assert convert_to(gauss, tesla, cgs_gauss) == tesla/10**4\n    assert convert_to(tesla, gauss, cgs_gauss) == 10**4*gauss\n\n    # Magnetic flux (maxwell to weber)\n    assert convert_to(maxwell, weber, cgs_gauss) == weber/10**8\n    assert convert_to(weber, maxwell, cgs_gauss) == 10**8*maxwell\n\n    # Current (statampere to ampere)\n    assert convert_to(statampere, ampere, cgs_gauss) == ampere/2997924580\n    assert convert_to(ampere, statampere, cgs_gauss) == 2997924580*statampere\n\n    # Capacitance (farad to statfarad)\n    assert convert_to(farad, centimeter, cgs_gauss) == 299792458**2*centimeter/10**5\n    assert convert_to(centimeter, farad, cgs_gauss) == 10**5*farad/299792458**2\n\n    # Inductance (henry to stathenry)\n    assert convert_to(henry, second**2/centimeter, cgs_gauss) == 25000*second**2/(22468879468420441*centimeter)\n    assert convert_to(second**2/centimeter, henry, cgs_gauss) == 22468879468420441*henry/(25000*second**2)\n", "def test_cgs_gauss_additional_conversions():\n    assert convert_to(statvolt, volt, cgs_gauss) == 299792458*volt/10**6\n    assert convert_to(volt, statvolt, cgs_gauss) == 10**6*statvolt/299792458\n    assert convert_to(gauss, tesla, cgs_gauss) == tesla/10**4\n    assert convert_to(tesla, gauss, cgs_gauss) == 10**4*gauss\n    assert convert_to(weber, maxwell, cgs_gauss) == 10**8*maxwell\n    assert convert_to(maxwell, weber, cgs_gauss) == weber/10**8\n    assert convert_to(debye, statcoulomb*centimeter, cgs_gauss) == 10**18*debye\n    assert convert_to(statcoulomb*centimeter, debye, cgs_gauss) == debye/10**18\n    assert convert_to(oersted, sqrt(gram/centimeter)/second, cgs_gauss) == oersted\n    assert convert_to(ohm, second/centimeter, cgs_gauss) == 10**5*second/(centimeter*299792458**2)\n    assert convert_to(farad, centimeter, cgs_gauss) == 299792458**2*centimeter/10**5\n    assert convert_to(henry, second**2/centimeter, cgs_gauss) == 10**5*second**2/(centimeter*299792458**2)\n", "def test_cgs_gauss_additional_conversions():\n    # Test conversion of magnetic units\n    assert convert_to(statvolt, volt, cgs_gauss) == 299792458*volt/10**6\n    assert convert_to(gauss, tesla, cgs_gauss) == tesla/10000\n    assert convert_to(maxwell, weber, cgs_gauss) == weber/10**8\n    assert convert_to(oersted, sqrt(gram/centimeter)/second, cgs_gauss) == sqrt(gram/centimeter)/second\n    assert convert_to(debye, statcoulomb*centimeter, cgs_gauss) == 10**18*statcoulomb*centimeter\n\n    # Test conversion of mechanical units\n    assert convert_to(dyne*centimeter, joule, cgs_gauss) == joule/10**7\n    assert convert_to(erg, joule, cgs_gauss) == joule/10**7\n    assert convert_to(joule, erg, cgs_gauss) == 10**7*erg\n\n    # Test conversions involving time and mass\n    assert convert_to(gram, kilogram, cgs_gauss) == kilogram/1000\n    assert convert_to(second, minute, cgs_gauss) == minute/60\n    assert convert_to(minute, second, cgs_gauss) == 60*second\n    assert convert_to(hour, second, cgs_gauss) == 3600*second\n\n    # Test conversion of derived units\n    assert convert_to(volt/ampere, ohm, cgs_gauss) == 10**5*second/(29979245800*centimeter)\n    assert convert_to(ampere/volt, farad, cgs_gauss) == 299792458**2*centimeter/10**5\n    assert convert_to(volt*second/ampere, henry, cgs_gauss) == 29979245800**2*second**2/(10**5*centimeter)\n\n    # Test conversion of mixed units\n    assert convert_to(statcoulomb/gram, coulomb/kilogram, cgs_gauss) == 2997924580*coulomb/kilogram\n    assert convert_to(joule/gram, erg/gram, cgs_gauss) == 10**7*erg/gram\n"], "sample_1202": ["def test_mpf_norm_special_cases():\n    from mpmath.libmp.libmpf import fnan, finf, fninf, fzero\n    assert mpf_norm(fnan, 53) == fzero\n    assert mpf_norm(finf, 53) == finf\n    assert mpf_norm(fninf, 53) == fninf\n", "def test_modular_inverse_edge_cases():\n    assert mod_inverse(1, 2) == 1\n    assert mod_inverse(1, -2) == -1\n    assert mod_inverse(-1, 2) == 1\n    assert mod_inverse(-1, -2) == -1\n    assert mod_inverse(0, 1) == 0\n\n    with raises(ValueError):\n        mod_inverse(2, 4)\n    \n    with raises(ValueError):\n        mod_inverse(0, 0)\n    \n    with raises(ValueError):\n        mod_inverse(1, 0)\n    \n    with raises(ValueError):\n        mod_inverse(0, 2)\n", "def test_mpf_norm_special_cases():\n    # Testing special cases for mpf_norm function\n    assert mpf_norm((1, 0, 1, 1), 10) == (0, 0, 0, 0)  # Special case for zero\n    assert mpf_norm((0, 1, 0, 53), 53) == (0, 1, 0, 53)  # Already normalized\n    assert mpf_norm((1, 1, 1, 1), 10) == (1, 1, 1, 1)  # Negative number\n", "def test_comp_function():\n    assert comp(3.142, 3.142) is True\n    assert comp(3.142, 3.141) is False\n    assert comp(3.142, 3.143) is False\n    assert comp(3.1415, 3.1415) is True\n    assert comp(3.1415, 3.1415, '') is True\n    assert comp(3.1415, 3.141, '') is False\n    assert comp(1.41421356237, 1.41421356237, 1e-10) is True\n    assert comp(1.41421356237, 1.41421356236, 1e-10) is True\n    assert comp(1.41421356237, 1.41421356235, 1e-10) is False\n    assert comp(1.0 / 3, 0.3333333333333333, 1e-15) is True\n    assert comp(1.0 / 3, 0.3333333333333333, 1e-16) is False\n    assert comp(1.0 / 3 - 0.1, 0.2333333333333333, 1e-15) is True\n    assert comp(1.0 / 3 - 0.1, 0.2333333333333333, 1e-16) is False\n    assert comp(3 + 4*I, 3.0 + 4.0*I, 1e-15) is True\n    assert comp(3 + 4*I, 3.0 + 4.1*I, 1e-1) is True\n    assert comp(3 + 4*I, 3.0 + 4.1*I, 1e-2) is False\n    assert comp(1.0, \"1.0\") is False\n    assert comp(S.Half, \"0.5\") is False\n    raises(ValueError, lambda: comp(\"1.0\", 1.0))\n    raises(ValueError, lambda: comp(\"1.0\", \"1.0\"))\n    raises(ValueError, lambda: comp(S.Half, \"0.5\"))\n    raises(ValueError, lambda:", "def test_issue_XXXXX():\n    from sympy import to_number_field\n\n    # Test for converting elements of algebraic number fields\n    # and check if minpoly and coefficients are correct.\n    a1 = AlgebraicNumber(sqrt(2))\n    assert a1.minpoly_of_element().as_expr(Symbol('x')) == Symbol('x')**2 - 2\n    assert a1.coeffs() == [1, 0]\n\n    a2 = AlgebraicNumber(a1, [3, -5])\n    assert a2.as_expr() == -5 + 3*sqrt(2)\n\n    a3 = AlgebraicNumber(a2, [2, -1])\n    assert a3.as_expr() == -11 + 6*sqrt(2)\n\n    a4 = to_number_field(sqrt(2), sqrt(2) + sqrt(3))\n    assert a4.minpoly_of_element().as_expr(Symbol('x')) == Symbol('x')**2 - 2\n    assert a4.to_root() == sqrt(2)\n    assert a4.primitive_element() == sqrt(2) + sqrt(3)\n    assert a4.coeffs() == [Rational(1, 2), 0, Rational(-9, 2), 0]\n\n    a5 = AlgebraicNumber(CRootOf(cyclotomic_poly(5), -1),\n                         [-1, -1, 0, 0], alias=Symbol('zeta'))\n    assert a5.as_poly().as_expr() == -Symbol('zeta')**3 - Symbol('zeta')**2\n    assert a5.evalf() == GoldenRatio.evalf()\n\n    a6 = AlgebraicNumber(a5.to_root(), coeffs=[2, 0], alias=Symbol('phi'))\n    assert a6.as_poly().as_expr() == 2*Symbol('phi')\n    assert a6.primitive_element().evalf() == GoldenRatio.evalf()\n"], "sample_1203": ["def test_orbit_homomorphism():\n    a = Permutation(1, 2, 3)\n    b = Permutation(0, 1, 2, 3)\n    G = PermutationGroup([a, b])\n    omega = [0, 1, 2, 3]\n    H = orbit_homomorphism(G, omega)\n    c = Permutation(0, 1, 2, 3)\n    d = Permutation(1, 2, 3)\n    assert H(a) == d\n    assert H(b) == c\n    assert H.is_isomorphism() == False\n", "def test_group_homomorphism_properties():\n    from sympy.combinatorics import Permutation\n    from sympy.combinatorics.named_groups import SymmetricGroup\n\n    # SymmetricGroup -> SymmetricGroup (non-trivial homomorphism)\n    G = SymmetricGroup(3)\n    H = SymmetricGroup(3)\n\n    # Identity permutation\n    id_perm = Permutation([0, 1, 2])\n\n    # Non-trivial permutations\n    perm1 = Permutation([1, 2, 0])\n    perm2 = Permutation([2, 0, 1])\n\n    # Define homomorphism from G to H\n    T = homomorphism(G, H, [G.generators[0], G.generators[1]], [perm1, perm2])\n\n    assert T.is_surjective()\n    assert not T.is_trivial()\n    assert T.is_injective()\n\n    # Test kernel\n    kernel = T.kernel()\n    assert kernel.order() == 1\n    assert kernel.is_subgroup(G)\n\n    # Test composition (identity composition in this case)\n    T_comp = T.compose(T)\n    assert T_comp.domain == G\n    assert T_comp.codomain == H\n    assert T_comp.is_trivial()\n\n    # Test restriction to a subgroup\n    subG = G.subgroup([G.generators[0]])\n    T_restrict = T.restrict_to(subG)\n    assert T_restrict.domain == subG\n    assert T_restrict.codomain == H\n    assert T_restrict(subG.generators[0]) == perm1\n\n    # Test invert_subgroup\n    inv_subgroup = T.invert_subgroup(H.subgroup([perm1]))\n    assert inv_subgroup.is_subgroup(G)\n    assert inv_subgroup.order() == 3\n", "def test_orbit_homomorphism():\n    # Testing orbit homomorphism for a permutation group acting on a set\n    G = PermutationGroup([Permutation(1, 2, 3, 4), Permutation(1, 2)])\n    omega = [1, 2, 3, 4]\n    hom = orbit_homomorphism(G, omega)\n    assert hom.codomain.order() == 24  # SymmetricGroup(4) has order 4!\n    assert hom(Permutation(1, 2)) == Permutation(0, 1)\n    assert hom(Permutation(1, 3, 2, 4)) == Permutation(0, 2, 1, 3)\n    assert hom.kernel().order() == 1\n", "def test_orbit_homomorphism():\n    from sympy.combinatorics.named_groups import SymmetricGroup\n\n    # Symmetric group S3 acting on {0, 1, 2}\n    S3 = SymmetricGroup(3)\n    omega = {0, 1, 2}\n    hom = orbit_homomorphism(S3, omega)\n\n    assert hom.domain == S3\n    assert hom.codomain.order() == S3.order()\n    assert hom.is_surjective()\n    assert hom.is_injective()\n\n    # Check specific permutations\n    p = Permutation(0, 1, 2)\n    assert hom(p) == p\n    assert hom.invert(p) == p\n", "def test_kernel():\n    # Test kernel computation for FpGroup -> PermutationGroup\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    c = Permutation(3)(0, 1, 2)\n    d = Permutation(3)(1, 2, 3)\n    A = AlternatingGroup(4)\n    T = homomorphism(G, A, [a, b], [c, d])\n    kernel = T.kernel()\n    assert kernel.order() == 1\n    assert kernel.generators == [G.identity]\n\n    # Test kernel computation for PermutationGroup -> PermutationGroup\n    D = DihedralGroup(8)\n    p = Permutation(0, 1, 2, 3, 4, 5, 6, 7)\n    T = homomorphism(D, D, D.generators, D.generators)\n    kernel = T.kernel()\n    assert kernel.order() == 1\n    assert kernel.generators == [D.identity]\n\n    # Test kernel computation for FpGroup with non-trivial kernel\n    E, e = free_group(\"e\")\n    G = FpGroup(E, [e**8])\n    P = PermutationGroup([Permutation(0, 1, 2, 3), Permutation(0, 2)])\n    T = homomorphism(G, P, [e], [Permutation(0, 1, 2, 3)])\n    kernel = T.kernel()\n    assert kernel.order() == 2  # e^4 generates the kernel\n    assert kernel.generators[0] == e**4\n"], "sample_1204": ["def test_PermutationGroup_equality():\n    a = Permutation([1, 2, 0])\n    b = Permutation([2, 0, 1])\n    G1 = PermutationGroup([a, b])\n    G2 = PermutationGroup([b, a])\n    assert G1 == G2\n\n    c = Permutation([0, 2, 1])\n    d = Permutation([1, 0, 2])\n    G3 = PermutationGroup([c, d])\n    G4 = PermutationGroup([c, a])\n    assert G3 != G4\n\n    G5 = PermutationGroup([a])\n    G6 = PermutationGroup([a, a])\n    assert G5 == G6\n", "def test_permutation_group_properties():\n    a = Permutation([1, 2, 0])\n    b = Permutation([2, 0, 1])\n    G = PermutationGroup([a, b])\n    assert G.is_group\n    assert G.degree == 3\n    assert G.is_abelian is False\n    assert G.is_trivial is False\n\n    H = PermutationGroup()\n    assert H.is_group\n    assert H.degree == 1\n    assert H.is_abelian is True\n    assert H.is_trivial is True\n\n    G = PermutationGroup(Permutation(0, 1))\n    assert G.is_group\n    assert G.degree == 2\n    assert G.is_abelian is True\n    assert G.is_trivial is False\n", "def test_schreier_sims_properties():\n    S = SymmetricGroup(4)\n    S.schreier_sims()\n    assert S.base == [0, 1, 2]\n    assert S.strong_gens == [Permutation(0, 1, 2, 3), Permutation(0, 1), Permutation(2, 3)]\n    assert S.basic_orbits == [[0, 1, 2, 3], [1, 2, 3], [2, 3]]\n    assert S.basic_stabilizers == [\n        PermutationGroup([Permutation(0, 1, 2, 3), Permutation(0, 1), Permutation(2, 3)]),\n        PermutationGroup([Permutation(1, 2, 3), Permutation(2, 3)]),\n        PermutationGroup([Permutation(2, 3)])\n    ]\n    assert S.basic_transversals == [\n        {0: Permutation(3), 1: Permutation(0, 1), 2: Permutation(0, 2, 1), 3: Permutation(0, 3, 1, 2)},\n        {1: Permutation(3), 2: Permutation(1, 2), 3: Permutation(1, 3, 2)},\n        {2: Permutation(3), 3: Permutation(2, 3)}\n    ]\n", "def test_base_properties():\n    # Test for base property\n    S = SymmetricGroup(3)\n    base = S.base\n    assert base == [0, 1]\n\n    # Test for basic_orbits property\n    assert S.basic_orbits == [[0, 1, 2], [1, 2], [2]]\n\n    # Test for basic_stabilizers property\n    stabilizers = S.basic_stabilizers\n    assert len(stabilizers) == 3\n    assert stabilizers[0].generators == S.generators\n\n    # Test for basic_transversals property\n    transversals = S.basic_transversals\n    assert len(transversals) == 3\n    assert transversals[0] == {0: Permutation([0, 1, 2]), 1: Permutation([0, 2, 1]), 2: Permutation([0, 1, 2])}\n\n    # Test for degree property\n    assert S.degree == 3\n\n    # Test for elements property\n    assert set(S.elements) == set(S._elements)\n\n    # Test for identity property\n    assert S.identity == Permutation([0, 1, 2])\n\n    # Test for max_div property\n    assert S.max_div == 3\n", "def test_permutationgroup_equality():\n    a = Permutation([1, 2, 0, 3, 4])\n    b = Permutation([1, 0, 2, 3, 4])\n    c = Permutation([1, 2, 3, 0, 4])\n    G1 = PermutationGroup([a, b])\n    G2 = PermutationGroup([b, a])\n    G3 = PermutationGroup([a, c])\n    assert G1.equals(G2) is True\n    assert G1.equals(G3) is False\n    assert G1 == G2\n    assert G1 != G3\n"], "sample_1205": ["def test_PolyElement_content_primitive():\n    R, x, y, z = ring(\"x,y,z\", ZZ)\n    f = 3*x**2*y - 6*x*y*z + 9*z**3 + 12\n\n    # Test content function\n    assert f.content() == 3\n\n    # Test primitive function\n    assert f.primitive() == (3, x**2*y - 2*x*y*z + 3*z**3 + 4)\n\n    g = 2*x**2*y - 4*x*y*z + 6*z**3 + 8\n\n    # Test content function\n    assert g.content() == 2\n\n    # Test primitive function\n    assert g.primitive() == (2, x**2*y - 2*x*y*z + 3*z**3 + 4)\n\n    h = x**2*y - 2*x*y*z + 3*z**3 + 4\n\n    # Test content function\n    assert h.content() == 1\n\n    # Test primitive function\n    assert h.primitive() == (1, h)\n\n    R, = ring(\"\", ZZ)\n    assert R(12).content() == 12\n    assert R(12).primitive() == (12, R(1))\n", "def test_PolyElement_div_and_rem():\n    R, x, y = ring(\"x,y\", ZZ, lex)\n\n    # Testing division and remainder for univariate polynomials\n    f = x**3 + 2*x**2 + 3*x + 4\n    g = x + 1\n    q, r = divmod(f, g)\n    assert q == x**2 + x + 2\n    assert r == 2\n\n    # Testing division and remainder for multivariate polynomials\n    f = x**3 * y + 2*x**2 * y + 3*x * y + 4 * y\n    g = x * y + 1\n    q, r = divmod(f, g)\n    assert q == x**2 * y + x * y + 2 * y\n    assert r == 2 * y\n\n    f = x**3 + 3*x**2 + 3*x + 1\n    g = x + 2\n    q, r = divmod(f, g)\n    assert q == x**2 + x + 1\n    assert r == -1\n\n    f = x**2 + 2*x + 1\n    g = x + 1\n    q, r = divmod(f, g)\n    assert q == x + 1\n    assert r == 0\n\n    # Testing division and remainder with zero polynomial\n    f = x**2 + 2*x + 1\n    g = R(0)\n    raises(ZeroDivisionError, lambda: divmod(f, g))\n", "def test_PolyRing_to_field():\n    R, x, y, z = ring(\"x,y,z\", ZZ)\n    F = R.to_field()\n    assert isinstance(F, FracField)\n    assert F.symbols == R.symbols\n    assert F.domain == R.domain\n    assert F.order == R.order\n", "def test_PolyElement___neg__():\n    R, x, y, z = ring(\"x,y,z\", QQ)\n\n    assert -R(0) == R(0)\n    assert -R(1) == R(-1)\n    assert -(-R(1)) == R(1)\n\n    f = x**2 - 3*x*y + z\n    g = -x**2 + 3*x*y - z\n\n    assert -f == g\n    assert -(-f) == f\n\n    f = x*y**2 - 2*x*y + 1\n    g = -x*y**2 + 2*x*y - 1\n\n    assert -f == g\n    assert -(-f) == f\n", "def test_PolyElement_str():\n    R, x, y = ring(\"x,y\", ZZ)\n\n    f = 3*x**2*y + 2*x*y**2 - x + 1\n    assert str(f) == \"3*x**2*y + 2*x*y**2 - x + 1\"\n\n    g = -x**3 + 4*x**2*y - 5*x*y**2 + y**3\n    assert str(g) == \"-x**3 + 4*x**2*y - 5*x*y**2 + y**3\"\n\n    h = 2*x + 3\n    assert str(h) == \"2*x + 3\"\n\n    R, t = ring(\"t\", QQ)\n    f = 3*t**2 - 1\n    assert str(f) == \"3*t**2 - 1\"\n\n    R, t = ring(\"t\", QQ.algebraic_field(sqrt(2)))\n    f = t**2 + sqrt(2)*t + 1\n    assert str(f) == \"t**2 + sqrt(2)*t + 1\"\n"], "sample_1206": ["def test_comp_function():\n    assert comp(1, 1) is True\n    assert comp(1, 1.0000001) is True\n    assert comp(1, 1.1, 0.2) is True\n    assert comp(1, 1.1, 0.05) is False\n    assert comp(1.0000001, 1) is True\n    assert comp(1.1, 1, 0.2) is True\n    assert comp(1.1, 1, 0.05) is False\n    assert comp(0, 0) is True\n    assert comp(0, 0.1, 0.2) is True\n    assert comp(0, 0.3, 0.2) is False\n    assert comp(0.1, 0.2, 0.11) is True\n    assert comp(0.1, 0.2, 0.09) is False\n    assert comp(1e-10, 0) is True\n    assert comp(1e-10, 0, 1e-11) is False\n    assert comp(0, 1e-10) is True\n    assert comp(0, 1e-10, 1e-11) is False\n    assert comp(Float('1.23456789123456789'), '1.23456789123456789') is True\n    assert comp(Float('1.23456789123456789'), '1.2345678912345678') is False\n    assert comp(S('pi'), pi.n(15)) is True\n    assert comp(S('pi'), '3.14159265358979') is True\n    assert comp(S('pi'), '3.1415926535898') is False\n    assert comp(S('1/3'), Rational(1, 3)) is True\n    assert comp(S('1/3'), '0.3333333333333') is True\n    assert comp(S('1/3'), '0.333333333333') is False\n    assert comp(pi.n(20), '3.1415926535897932385', 1e-19) is True\n    assert comp(pi.n(20), '3.1415926535897932385', 1e-20)", "def test_issue_000():\n    # testing comp function with different types of inputs\n    # comparing complex numbers\n    assert comp(2 + 3*I, 2 + 3*I)\n    assert not comp(2 + 3*I, 2 + 4*I)\n    assert comp(2 + 3*I, 2 + 3*I, tol=1e-10)\n    # comparing rational numbers\n    assert comp(Rational(1, 3), Rational(1, 3))\n    assert not comp(Rational(1, 3), Rational(1, 2))\n    assert comp(Rational(1, 3), 1/3, tol=1e-15)\n    # comparing irrational numbers\n    assert comp(sqrt(2).n(15), sqrt(2).n(15))\n    assert not comp(sqrt(2).n(15), sqrt(3).n(15))\n    assert comp(sqrt(2).n(15), sqrt(2).n(15), tol=1e-15)\n    # comparing with tolerance\n    assert comp(1.000000001, 1.0, tol=1e-8)\n    assert not comp(1.0000001, 1.0, tol=1e-8)\n    # test with string\n    assert comp(pi.n(4), '3.1416', '')\n    assert not comp(pi.n(4), '3.1415', '')\n    # test with zero\n    assert comp(0, 0.0)\n    assert comp(0, 0, tol=1e-10)\n    # test with negative numbers\n    assert comp(-1.5, -1.5)\n    assert not comp(-1.5, -1.4)\n    assert comp(-1.5, -1.5, tol=1e-10)\n    # test with mixed types\n    assert comp(Rational(1, 3), 1/3)\n    assert not comp(Rational(1, 3), 0.33333)\n    assert comp(Rational(1, 3), 0.3333333333333333, tol=1e-10)\n    # test with large numbers\n    assert comp(1e100, 1e100)\n    assert not comp(1e100, 1e99)\n    assert comp", "def test_comp_edge_cases():\n    # comp should return False for different types\n    assert not comp(1, Symbol('x'))\n    assert not comp(pi, 3.14)\n    assert not comp(E, exp(1).evalf())\n\n    # comp with tolerance checking for symbolic expressions\n    assert comp(sqrt(2) + sqrt(3), sqrt(2) + sqrt(3), 1e-12)\n    assert not comp(sqrt(2) + sqrt(3), sqrt(2) + sqrt(4), 1e-12)\n\n    # comp with symbolic expressions and large tolerance\n    assert comp(sqrt(2) + sqrt(3), sqrt(2) + sqrt(4), 1)\n    assert not comp(sqrt(2) + sqrt(3), sqrt(2) + sqrt(5), 1)\n\n    # comp with edge cases involving zero\n    assert comp(0, 0.0)\n    assert not comp(0, 1e-10)\n    assert comp(0.0, 0)\n    assert not comp(1e-10, 0)\n\n    # comp with edge cases involving infinity\n    assert not comp(oo, 1e10)\n    assert not comp(oo, -oo)\n    assert comp(oo, oo)\n    assert not comp(-oo, oo)\n    assert comp(-oo, -oo)\n    assert not comp(oo, zoo)\n    assert not comp(zoo, zoo)\n\n    # comp with negative numbers and tolerance\n    assert comp(-1, -1.0)\n    assert not comp(-1, -1.1, 0.05)\n    assert comp(-1, -1.05, 0.1)\n", "def test_mpf_norm_exceptions():\n    # Check that invalid inputs to mpf_norm raise appropriate exceptions\n    from mpmath.libmp.backend import MPZ\n    mpf = (0, MPZ(0), -123, -1)\n    raises(ValueError, lambda: mpf_norm(mpf, 'invalid_prec'))  # invalid precision type\n    mpf = (1, MPZ(123), -123, -2)\n    raises(ValueError, lambda: mpf_norm(mpf, 10))  # invalid exponent\n    mpf = (0, MPZ(0), 0, 0)\n    raises(ValueError, lambda: mpf_norm(mpf, 10))  # zero mantissa with non-zero exponent\n", "def test_mpf_norm_special_cases():\n    mpf_special_cases = [\n        ((1, 0, 1, 0), 10, (0, 0, 0, 0)),  # Zero\n        ((0, 0, 0, 0), 10, (0, 0, 0, 0)),  # Zero\n        ((_mpf_inf), 10, (_mpf_inf)),      # Positive infinity\n        ((_mpf_ninf), 10, (_mpf_ninf)),    # Negative infinity\n        ((_mpf_nan), 10, (_mpf_nan)),      # NaN\n    ]\n    for mpf, prec, expected in mpf_special_cases:\n        assert mpf_norm(mpf, prec) == expected\n\n    assert mpf_norm((1, 5, 0, 3), 10) == (1, 5, 0, 3)\n    assert mpf_norm((0, 5, -3, 3), 10) == (0, 5, -3, 3)\n"], "sample_1207": ["def test_auto_symbol_and_number():\n    # Test to ensure that auto_symbol and auto_number transformations are correctly applied\n    inputs = {\n        'x': Symbol('x'),\n        'y + 1': Symbol('y') + 1,\n        'z * 5.0': Symbol('z') * Float(5.0),\n        '3 + 2.0': 3 + 2.0,\n    }\n    transformations = (auto_symbol, auto_number)\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n\n    # Test to ensure transformations are correctly applied in combination\n    inputs = {\n        '2*x + y': 2 * Symbol('x') + Symbol('y'),\n        '3.5 * z': Float(3.5) * Symbol('z'),\n        'a + b': Symbol('a') + Symbol('b'),\n        '4.0 / c': Float(4.0) / Symbol('c'),\n    }\n    transformations = standard_transformations + (auto_number,)\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n", "def test_token_splittable():\n    assert _token_splittable(\"alpha\") is False\n    assert _token_splittable(\"beta\") is False\n    assert _token_splittable(\"theta\") is False\n    assert _token_splittable(\"xyz\") is True\n    assert _token_splittable(\"x_y_z\") is False\n    assert _token_splittable(\"gamma\") is False\n    assert _token_splittable(\"delta\") is False\n    assert _token_splittable(\"abcd\") is True\n", "def test_issue_factorial_edge_cases():\n    # Testing edge cases for factorial notation\n    transformations = standard_transformations + (factorial_notation,)\n    inputs = {\n        'factorial(x + 1)': factorial(x + 1),  # standard factorial notation\n        'factorial2(x + 1)': factorial2(x + 1),  # double factorial notation\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n\n    # Ensuring multiple factorials raise an error\n    inputs = ['x!!!', 'x!!!!']\n    for text in inputs:\n        raises(TokenError, lambda: parse_expr(text, transformations=transformations))\n", "def test_custom_split_symbols():\n        return symbol not in ('custom', 'split')\n\n    custom_transformation = split_symbols_custom(custom_split)\n    transformations = standard_transformations + (custom_transformation, implicit_multiplication)\n\n    custom = Symbol('custom')\n    split = Symbol('split')\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert parse_expr(\"customsplit\", transformations=transformations) == custom * split\n    assert parse_expr(\"customxsplit\", transformations=transformations) == custom * x * split\n    assert parse_expr(\"customsplit\", transformations=standard_transformations) == Symbol('customsplit')\n", "def test_auto_symbol_with_function():\n    local_dict = {'my_var': Symbol('my_var')}\n    assert parse_expr('my_var + 1') == Symbol('my_var') + 1\n\n    # Test an undefined variable is automatically converted to a Symbol\n    assert parse_expr('undefined_var + 1', local_dict) == Symbol('undefined_var') + 1\n\n    # Test an undefined function is automatically converted to a Function\n    assert parse_expr('my_function(2)', local_dict) == Function('my_function')(2)\n\n    # Test predefined function in local_dict is used\n    local_dict['my_function'] = lambda x: x + 2\n    assert parse_expr('my_function(2)', local_dict) == 4\n\n    # Ensure predefined variable in local_dict is used as Symbol\n    assert parse_expr('my_var') == Symbol('my_var')\n\n    # Ensure predefined function in local_dict is used as Function\n    assert parse_expr('my_function(2)', {'my_function': Function('my_function')}) == Function('my_function')(2)\n"], "sample_1208": ["def test_MatrixPSpace_sample():\n    M = MatrixGammaDistribution(1, 2, [[2, 1], [1, 2]])\n    MP = MatrixPSpace('M', M, 2, 2)\n    scipy = import_module('scipy')\n    if not scipy:\n        skip('Scipy not installed. Abort tests for sample.')\n    else:\n        samples = MP.sample(size=5)\n        for sample_matrix in samples[MP.value]:\n            assert Matrix(sample_matrix) in M.set\n", "def test_MatrixGamma_check():\n    raises(ValueError, lambda: MatrixGammaDistribution.check(-1, 2, MatrixSymbol('A', 2, 2)))\n    raises(ValueError, lambda: MatrixGammaDistribution.check(1, -2, MatrixSymbol('A', 2, 2)))\n    raises(ValueError, lambda: MatrixGammaDistribution.check(1, 2, Matrix([[1, 2], [2, 1]])))\n    raises(ValueError, lambda: MatrixGammaDistribution.check(1, 2, MatrixSymbol('A', 2, 3)))\n    assert MatrixGammaDistribution.check(1, 2, MatrixSymbol('A', 2, 2)) is None\n", "def test_MatrixPSpace_sample():\n    M = MatrixGammaDistribution(1, 2, [[2, 1], [1, 2]])\n    MP = MatrixPSpace('M', M, 2, 2)\n    scipy = import_module('scipy')\n    if not scipy:\n        skip('Scipy not installed. Abort tests for MatrixPSpace.sample.')\n    else:\n        samples = MP.sample(size=5, library='scipy')\n        assert MP.value in samples\n        for sample_matrix in samples[MP.value]:\n            assert Matrix(sample_matrix) in MP.distribution.set\n        raises(NotImplementedError, lambda: MP.sample(size=5, library='numpy'))\n", "def test_MatrixPSpace_properties():\n    M = MatrixGammaDistribution(1, 2, [[2, 1], [1, 2]])\n    MP = MatrixPSpace('M', M, 2, 2)\n    assert MP.domain == MatrixDomain(MP.symbol, M.set)\n    assert MP.value == RandomMatrixSymbol('M', 2, 2, MP)\n    assert MP.values == {RandomMatrixSymbol('M', 2, 2, MP)}\n    raises(NotImplementedError, lambda: MP.compute_density(MP.value + 1))\n", "def test_MatrixGammaDistribution_check():\n    from sympy.stats.rv import RandomMatrixSymbol\n    from sympy.stats.matrix_distributions import MatrixGammaDistribution\n\n    # Valid parameters\n    alpha = symbols('alpha', positive=True)\n    beta = symbols('beta', positive=True)\n    scale_matrix = MatrixSymbol('scale_matrix', 2, 2)\n    dist = MatrixGammaDistribution(alpha, beta, scale_matrix)\n    \n    # Invalid parameters\n    invalid_alpha = symbols('invalid_alpha', positive=False)\n    invalid_beta = symbols('invalid_beta', positive=False)\n    non_square_matrix = MatrixSymbol('non_square_matrix', 2, 3)\n    non_pos_def_matrix = MatrixSymbol('non_pos_def_matrix', 2, 2)\n\n    raises(ValueError, lambda: MatrixGammaDistribution(invalid_alpha, beta, scale_matrix))\n    raises(ValueError, lambda: MatrixGammaDistribution(alpha, invalid_beta, scale_matrix))\n    raises(ValueError, lambda: MatrixGammaDistribution(alpha, beta, non_square_matrix))\n    raises(ValueError, lambda: MatrixGammaDistribution(alpha, beta, non_pos_def_matrix))\n"], "sample_1209": ["def test_properties():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n\n    assert m.name == 'milli'\n    assert m.abbrev == 'm'\n    assert m.scale_factor == 10**-3\n    assert m._latex(None) == r'\\text{m}'\n\n    assert k.name == 'kilo'\n    assert k.abbrev == 'k'\n    assert k.scale_factor == 10**3\n    assert k._latex(None) == r'\\text{k}'\n\n    dodeca = Prefix('dodeca', 'dd', 1, base=12, latex_repr=r'\\text{dd}')\n    assert dodeca.name == 'dodeca'\n    assert dodeca.abbrev == 'dd'\n    assert dodeca.scale_factor == 12\n    assert dodeca._latex(None) == r'\\text{dd}'\n    assert str(dodeca) == 'dd'\n    assert repr(dodeca) == \"Prefix('dodeca', 'dd', 1, 12)\"\n", "def test_prefix_properties():\n    name = \"kilo\"\n    abbrev = \"k\"\n    exponent = 3\n    base = 10\n    latex_repr = r\"\\text{k}\"\n\n    k = Prefix(name, abbrev, exponent, base, latex_repr)\n\n    assert k.name == name\n    assert k.abbrev == abbrev\n    assert k.scale_factor == base ** exponent\n    assert k._latex(None) == latex_repr\n    assert k.base == base\n    assert str(k) == abbrev\n    assert repr(k) == f\"Prefix('{name}', '{abbrev}', {exponent})\"\n", "def test_prefix_properties():\n    p = Prefix('test', 't', 5, base=2)\n\n    assert p.name == 'test'\n    assert p.abbrev == 't'\n    assert p.scale_factor == 2**5\n    assert p.base == 2\n    assert p._latex(None) == r'\\text{t}'\n\n    p_latex = Prefix('latex_test', 'lt', 2, base=10, latex_repr=r'\\mathrm{lt}')\n    assert p_latex._latex(None) == r'\\mathrm{lt}'\n", "def test_prefix_properties():\n    micro_prefix = PREFIXES['mu']\n    assert micro_prefix.name == 'micro'\n    assert micro_prefix.abbrev == 'mu'\n    assert micro_prefix.scale_factor == 10**-6\n    assert micro_prefix._latex(None) == r\"\\mu\"\n\n    giga_prefix = PREFIXES['G']\n    assert giga_prefix.name == 'giga'\n    assert giga_prefix.abbrev == 'G'\n    assert giga_prefix.scale_factor == 10**9\n    assert giga_prefix._latex(None) == r'\\text{G}'\n\n    # Test for binary prefix properties\n    mebi_prefix = BIN_PREFIXES['Mi']\n    assert mebi_prefix.name == 'mebi'\n    assert mebi_prefix.abbrev == 'Y'\n    assert mebi_prefix.scale_factor == 2**20\n    assert mebi_prefix.base == 2\n    assert mebi_prefix._latex(None) == r'\\text{Y}'\n", "def test_latex_representation():\n    micro_with_latex = PREFIXES['mu']\n    kilo_without_latex = PREFIXES['k']\n    \n    assert micro_with_latex._latex(None) == r\"\\mu\"\n    assert kilo_without_latex._latex(None) == r\"\\text{k}\"\n"], "sample_4": ["    def test_read_html_table_format_error(self, read, tmp_path):\n        \"\"\"Test if format is incorrect.\"\"\"\n        fp = tmp_path / \"test_read_html_table_format_error.html\"\n\n        # Create a dummy HTML file\n        with open(fp, \"w\") as f:\n            f.write(\"<html><body><table><tr><td>Dummy</td></tr></table></body></html>\")\n\n        # Test with incorrect format\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html'\"):\n            read(fp, format=\"wrong_format\")\n", "    def test_html_identify(self):\n        \"\"\"Test the html_identify function.\"\"\"\n        assert html_identify(None, \"file.html\", None) is True\n        assert html_identify(None, \"file.txt\", None) is False\n        assert html_identify(None, 12345, None) is False\n", "    def test_html_identify_function(self, tmp_path):\n        \"\"\"Test the html_identify function.\"\"\"\n        valid_html_path = tmp_path / \"test_valid.html\"\n        invalid_html_path = tmp_path / \"test_invalid.txt\"\n        \n        assert html_identify(None, str(valid_html_path), None)\n        assert not html_identify(None, str(invalid_html_path), None)\n        assert not html_identify(None, None, None)\n", "    def test_write_html_table_invalid_format(self, write, tmp_path):\n        \"\"\"Test writing with an invalid format.\"\"\"\n        fp = tmp_path / \"test_write_html_table_invalid_format.html\"\n\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html'\"):\n            write(fp, format=\"csv\")\n", "    def test_html_identify(self):\n        \"\"\"Test the html_identify function.\"\"\"\n        assert html_identify(None, \"test.html\", None) is True\n        assert html_identify(None, \"test.htm\", None) is False\n        assert html_identify(None, \"test.txt\", None) is False\n        assert html_identify(None, 123, None) is False\n"], "sample_11": ["def test_sanitize_slices_single_int():\n    slices = 5\n    ndim = 3\n    expected = [5, slice(None), slice(None)]\n    result = sanitize_slices(slices, ndim)\n    assert result == expected\n", "def test_world_axis_object_classes_spectral_coord():\n    wcs = WCS_SPECTRAL_CUBE\n\n    # Injecting a spectral coordinate\n    wcs.world_axis_object_classes['spectral'] = (SpectralCoord, (), {'unit': u.Hz})\n\n    sub = SlicedLowLevelWCS(wcs, np.s_[0, 0, :])\n\n    dwd = sub.dropped_world_dimensions\n    wao_classes = dwd.pop(\"world_axis_object_classes\")\n    wao_components = dwd.pop(\"world_axis_object_components\")\n\n    validate_info_dict(dwd, {\n        \"value\": [12.86995801, 20.49217541, 20],\n        \"world_axis_physical_types\": [\"pos.galactic.lat\", \"pos.galactic.lon\", \"em.freq\"],\n        \"world_axis_names\": [\"Latitude\", \"Longitude\", \"Frequency\"],\n        \"world_axis_units\": [\"deg\", \"deg\", \"Hz\"],\n        \"serialized_classes\": False,\n    })\n\n    assert wao_classes['celestial'][0] is SkyCoord\n    assert wao_classes['celestial'][1] == ()\n    assert isinstance(wao_classes['celestial'][2]['frame'], Galactic)\n    assert wao_classes['celestial'][2]['unit'] is u.deg\n\n    assert wao_classes['spectral'][0] is SpectralCoord\n    assert wao_classes['spectral'][1] == ()\n    assert wao_classes['spectral'][2]['unit'] is u.Hz\n\n    assert wao_components[0] == ('celestial', 1, 'spherical.lat.degree')\n    assert wao_components[1] == ('celestial', 0, 'spherical.lon.degree')\n    assert wao_components[2][0:2] == ('spectral', 0)\n", "def test_invalid_pixel_bounds():\n    wcs = WCS_SPECTRAL_CUBE\n    wcs.pixel_bounds = None\n    sliced_wcs = SlicedLowLevelWCS(wcs, Ellipsis)\n    assert sliced_wcs.pixel_bounds is None\n", "def test_sanitize_slices_with_incorrect_ellipsis():\n    with pytest.raises(IndexError, match=\"an index can only have a single ellipsis\"):\n        sanitize_slices([Ellipsis, 1, Ellipsis], 4)\n", "def test_sanitize_slices_with_integer():\n    slices = 2\n    ndim = 3\n    expected = [2, slice(None), slice(None)]\n    result = sanitize_slices(slices, ndim)\n    assert result == expected\n"], "sample_15": ["    def test_to_value(self):\n        q = 3.0 * u.m\n        assert q.to_value(u.cm) == 300.0\n        assert q.to_value(u.km) == 0.003\n        assert q.to_value() == 3.0  # Default unit\n        assert q.to_value(u.dimensionless_unscaled) == 3.0  # Dimensionless conversion\n", "def test_additional_testcases(self, tc):\n    return test_testcase(tc)\n", "    def test_quantity_initialization(self):\n        q = u.Quantity(10, u.m)\n        assert q.value == 10\n        assert q.unit == u.m\n", "    def test_quantity_creation_from_scalar(self):\n        q = u.Quantity(5, unit=u.meter)\n        assert q.value == 5\n        assert q.unit == u.meter\n", "def test_quantity_decompose(self):\n    q = 5.0 * u.N\n    decomposed = q.decompose()\n    assert decomposed.unit == u.kg * u.m / u.s**2\n    assert decomposed.value == 5.0\n\n    q = np.array([1.0, 2.0, 3.0]) * u.J\n    decomposed = q.decompose()\n    assert decomposed.unit == u.kg * u.m**2 / u.s**2\n    assert np.all(decomposed.value == np.array([1.0, 2.0, 3.0]))\n\n    # Test with bases\n    decomposed = q.decompose(bases=[u.cm, u.g, u.s])\n    assert decomposed.unit == u.g * u.cm**2 / u.s**2\n    assert np.all(decomposed.value == np.array([1.0, 2.0, 3.0]) * 1e7)\n"], "sample_27": ["def test_different_column_attributes():\n    ca = Column(\"A\", format=\"L\", array=[True, False])\n    cb = Column(\"B\", format=\"X\", array=[[0], [1]], unit=\"m\")\n    cc = Column(\"C\", format=\"4I\", dim=\"(2, 2)\", array=[[0, 1, 2, 3], [4, 5, 6, 7]])\n    \n    cd = Column(\"A\", format=\"L\", array=[True, False])\n    ce = Column(\"B\", format=\"X\", array=[[0], [1]], unit=\"cm\")\n    cf = Column(\"C\", format=\"4I\", dim=\"(2, 2)\", array=[[0, 1, 2, 3], [4, 5, 6, 7]])\n    \n    ta = BinTableHDU.from_columns([ca, cb, cc])\n    tb = BinTableHDU.from_columns([cd, ce, cf])\n\n    diff = TableDataDiff(ta.data, tb.data)\n    assert not diff.identical\n    assert diff.diff_column_attributes == [((\"B\", \"unit\"), (\"m\", \"cm\"))]\n    \n    report = diff.report()\n    assert \"Column B has different units:\" in report\n    assert \"a> m\" in report\n    assert \"b> cm\" in report\n", "def test_different_table_field_attributes(self):\n    \"\"\"\n    Test tables with the same column names but different secondary attributes.\n    \"\"\"\n    ca = Column(\"A\", format=\"L\", array=[True, False], unit=\"m/s\")\n    cb = Column(\"A\", format=\"L\", array=[True, False], unit=\"km/s\")\n\n    ta = BinTableHDU.from_columns([ca])\n    tb = BinTableHDU.from_columns([cb])\n\n    diff = TableDataDiff(ta.data, tb.data)\n\n    assert not diff.identical\n    assert len(diff.common_columns) == 1\n    assert diff.common_column_names == {\"a\"}\n    assert diff.diff_column_attributes == [(('A', 'unit'), ('m/s', 'km/s'))]\n\n    report = diff.report()\n    assert \"Column A has different units:\" in report\n    assert \"a> m/s\\n    b> km/s\" in report\n", "    def test_report_diff_keyword_attr(self):\n        ha = Header([(\"A\", 1, \"Comment A1\"), (\"B\", 2, \"Comment B1\")])\n        hb = Header([(\"A\", 1, \"Comment A2\"), (\"B\", 3, \"Comment B2\")])\n        diff = HeaderDiff(ha, hb)\n\n        with io.StringIO() as buf:\n            report_diff_keyword_attr(buf, \"values\", diff.diff_keyword_values, \"B\", ind=1)\n            report_diff_keyword_attr(buf, \"comments\", diff.diff_keyword_comments, \"A\", ind=1)\n            output = buf.getvalue()\n\n        assert \"Keyword B        has different values\" in output\n        assert \"Keyword A        has different comments\" in output\n        assert \"  a> 2\\n  b> 3\" in output\n        assert \"  a> Comment A1\\n  b> Comment A2\" in output\n", "def test_table_data_diff_with_extra_attributes():\n    \"\"\"\n    Test diffing table data with columns that have additional attributes like units,\n    bscale, and bzero.\n    \"\"\"\n    c1 = Column(\"A\", format=\"L\", array=[True, False])\n    c2 = Column(\"B\", format=\"X\", array=[[0], [1]])\n    c3 = Column(\"C\", format=\"4I\", dim=\"(2, 2)\", array=[[0, 1, 2, 3], [4, 5, 6, 7]], unit=\"cm\")\n    c4 = Column(\"D\", format=\"J\", bscale=2.0, array=[0, 1])\n    c5 = Column(\"E\", format=\"A3\", array=[\"abc\", \"def\"])\n    c6 = Column(\"F\", format=\"E\", unit=\"m\", array=[0.0, 1.0], bzero=0.1)\n    c7 = Column(\"G\", format=\"D\", bzero=-0.1, array=[0.0, 1.0])\n    c8 = Column(\"H\", format=\"C\", array=[0.0 + 1.0j, 2.0 + 3.0j])\n    c9 = Column(\"I\", format=\"M\", array=[4.0 + 5.0j, 6.0 + 7.0j])\n    c10 = Column(\"J\", format=\"PI(2)\", array=[[0, 1], [2, 3]])\n    c11 = Column(\"K\", format=\"QJ(2)\", array=[[0, 1], [2, 3]])\n\n    columns_a = [c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11]\n    columns_b = [col.copy() for col in columns_a]\n\n    columns_b[2].unit = \"m\"  # Change unit\n    columns_b[5].bzero = 0.2  # Change bzero\n\n    ta = BinTableHDU.from_columns(columns_a)\n    tb = BinTableHDU.from_columns(columns_b)\n\n    diff = TableDataDiff(ta.data, tb.data)\n    assert not diff.identical\n\n    # Check diff_column_attributes\n    assert len(diff.diff_column_attributes) == 2\n   ", "def test_ignore_blanks_in_data():\n    \"\"\"Test to ensure that the diffing process can ignore trailing blanks in data values if specified.\"\"\"\n    a = np.array([\"abc   \", \"def   \", \"ghi   \"])\n    b = np.array([\"abc\", \"def\", \"ghi\"])\n    \n    # By default, trailing blanks are ignored\n    diff = ImageDataDiff(a, b)\n    assert diff.identical\n\n    # Do not ignore trailing blanks\n    diff = ImageDataDiff(a, b, ignore_blanks=False)\n    assert not diff.identical\n    assert diff.diff_total == 3\n    assert diff.diff_pixels == [((0,), (\"abc   \", \"abc\")), ((1,), (\"def   \", \"def\")), ((2,), (\"ghi   \", \"ghi\"))]\n\n    report = diff.report()\n    assert \"Data differs at [1]:\" in report\n    assert \"a> 'abc   '\\n    b> 'abc'\" in report\n    assert \"3 different pixels found (100.00% different)\" in report\n"], "sample_68": ["    def test_is_active_debug_false(self):\n        \"\"\"\n        Test is_active when DEBUG is False.\n        \"\"\"\n        with self.settings(DEBUG=False):\n            request = self.rf.get('/')\n            filter = SafeExceptionReporterFilter()\n            self.assertTrue(filter.is_active(request))\n", "    def test_cleansed_multivaluedict(self):\n        filter = SafeExceptionReporterFilter()\n        request = RequestFactory().post('/test_view/', data={\n            'username': 'user',\n            'password': 'secret',\n            'token': '12345',\n        })\n        request.sensitive_post_parameters = ['password', 'token']\n        cleansed_data = filter.get_cleansed_multivaluedict(request, request.POST)\n        self.assertEqual(cleansed_data['username'], 'user')\n        self.assertEqual(cleansed_data['password'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_data['token'], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_special_types(self):\n        \"\"\"\n        Test the cleansing of special types like MultiValueDict in\n        SafeExceptionReporterFilter.\n        \"\"\"\n        filter = SafeExceptionReporterFilter()\n        multi_value_dict = MultiValueDict({\n            'key1': ['value1', 'value2'],\n            'key2': ['sensitive_value'],\n        })\n        request = self.rf.post('/test_view/', multi_value_dict)\n        request.sensitive_post_parameters = ['key2']\n\n        cleansed = filter.cleanse_special_types(request, multi_value_dict)\n        self.assertEqual(cleansed['key1'], ['value1', 'value2'])\n        self.assertEqual(cleansed['key2'], CLEANSED_SUBSTITUTE)\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_is_active(self):\n        \"\"\"\n        Test the behavior of the is_active method in SafeExceptionReporterFilter.\n        \"\"\"\n        filter = SafeExceptionReporterFilter()\n        request = mock.Mock()\n        with self.settings(DEBUG=True):\n            self.assertFalse(filter.is_active(request))\n        with self.settings(DEBUG=False):\n            self.assertTrue(filter.is_active(request))\n"], "sample_80": ["def test_aggregate_annotation(self):\n    query = Query(Author)\n    query.add_annotation(Count('num'), 'num_count')\n    where = query.build_where(Q(num_count__gt=2))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.source_expressions[0].source.name, 'num')\n", "    def test_annotation_select(self):\n        query = Query(Author)\n        query.add_annotation(Count('num'), alias='num_count', is_summary=True)\n        where = query.build_where(Q(num_count__gt=2))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, GreaterThan)\n        self.assertEqual(lookup.rhs, 2)\n        self.assertIsInstance(lookup.lhs, SimpleCol)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n", "def test_raw_query_get_columns(self):\n    query = RawQuery(\"SELECT id, name FROM test_table\", 'default')\n    connection = connections['default']\n    \n    # Mock cursor description to simulate database response.\n    mock_cursor_description = [('id',), ('name',)]\n    query.cursor = connection.cursor()\n    query.cursor.description = mock_cursor_description\n    \n    columns = query.get_columns()\n    self.assertEqual(columns, ['id', 'name'])\n", "def test_empty_result_set(self):\n        query = Query(Author)\n        query.set_empty()\n        self.assertTrue(query.is_empty())\n", "    def test_rawquery_initialization(self):\n        sql = \"SELECT * FROM author WHERE num > %s\"\n        params = (2,)\n        using = 'default'\n        raw_query = RawQuery(sql, using, params)\n\n        self.assertEqual(raw_query.sql, sql)\n        self.assertEqual(raw_query.params, params)\n        self.assertEqual(raw_query.using, using)\n        self.assertIsNone(raw_query.cursor)\n        self.assertEqual(raw_query.low_mark, 0)\n        self.assertIsNone(raw_query.high_mark)\n        self.assertEqual(raw_query.extra_select, {})\n        self.assertEqual(raw_query.annotation_select, {})\n"], "sample_86": ["    def test_lazy_hash(self):\n        \"\"\"\n        Ensure that the lazy object's hash is the same as the hash of the resolved object.\n        \"\"\"\n        original_object = 'Lazy hash test'\n        lazy_obj = lazy(lambda: original_object, str)\n        self.assertEqual(hash(original_object), hash(lazy_obj()))\n\n", "    def test_lazy_mod_operator(self):\n        \"\"\"Test lazy evaluation with the modulus operator.\"\"\"\n        lazy_string = lazy(lambda: \"Hello %s\", str)\n        self.assertEqual(lazy_string() % \"World\", \"Hello World\")\n        \n        lazy_integer = lazy(lambda: 10, int)\n        self.assertEqual(lazy_integer() % 3, 1)\n", "    def test_lazy_deepcopy(self):\n        \"\"\"Test that deep copying a lazy object works correctly.\"\"\"\n        class Klazz:\n                self.value = value\n\n                if isinstance(other, Klazz):\n                    return self.value == other.value\n                return False\n\n        original = lazy(lambda: Klazz(42), Klazz)()\n        copied = copy.deepcopy(original)\n\n        self.assertEqual(original, copied)\n        self.assertIsNot(original, copied)\n        self.assertEqual(original.value, copied.value)\n", "    def test_lazy_object_method_call(self):\n        \"\"\"\n        Ensure that lazy objects correctly call methods on the wrapped object.\n        \"\"\"\n        class Klazz:\n                return x * 2\n\n        t = lazy(lambda: Klazz(), Klazz)()\n        self.assertEqual(t.method(5), 10)\n", "    def test_lazy_text_cast(self):\n        \"\"\"Test the lazy evaluation and text casting behavior.\"\"\"\n        lazy_text = lazy(lambda: \"Lazy Evaluation\", str)\n        self.assertEqual(str(lazy_text()), \"Lazy Evaluation\")\n"], "sample_95": ["    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, no_store=True, max_age=3600, custom_key=\"custom_value\")\n        expected = {'no-store', 'max-age=3600', 'custom_key=custom_value'}\n        self.assertEqual(set(response['Cache-Control'].split(', ')), expected)\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=60, public=True, stale_while_revalidate=30)\n        self.assertEqual(\n            set(response['Cache-Control'].split(', ')),\n            {'max-age=60', 'public', 'stale-while-revalidate=30'}\n        )\n", "    def test_patch_cache_control(self):\n        \"\"\"\n        Test patch_cache_control function.\n        \"\"\"\n        response = HttpResponse()\n        patch_cache_control(response, no_cache=True, max_age=60, custom_option=\"value\")\n        self.assertIn('Cache-Control', response)\n        self.assertEqual(\n            set(response['Cache-Control'].split(', ')),\n            {'no-cache', 'max-age=60', 'custom-option=value'}\n        )\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, no_cache=True, max_age=3600, public=True)\n        self.assertEqual(\n            set(response['Cache-Control'].split(', ')),\n            {'no-cache', 'max-age=3600', 'public'}\n        )\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600, public=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600, public')\n\n        # Test with existing Cache-Control header\n        response['Cache-Control'] = 'max-age=1800'\n        patch_cache_control(response, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'max-age=1800')\n"], "sample_97": ["    def test_ensure_echo_on_enabled(self, mocked_isatty, mocked_termios):\n        mocked_termios.ECHO = 8\n        attrs = [0, 0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attrs\n        autoreload.ensure_echo_on()\n        self.assertTrue(attrs[3] & mocked_termios.ECHO)\n        mocked_termios.tcsetattr.assert_called_once_with(sys.stdin, mocked_termios.TCSANOW, attrs)\n", "    def test_ensure_echo_on_enabled(self, mock_stdin, mock_termios):\n        mock_stdin.isatty.return_value = True\n        attrs = [0, 0, 0, 0]\n        mock_termios.tcgetattr.return_value = attrs\n        attrs[3] = 0\n        autoreload.ensure_echo_on()\n        self.assertTrue(attrs[3] & mock_termios.ECHO)\n        self.assertTrue(mock_termios.tcsetattr.called)\n", "    def test_ensure_echo_on(self, mock_tcsetattr, mock_tcgetattr, mock_isatty, mock_termios):\n        attr_list = [0, 0, 0, 0]  # Dummy list for termios attributes\n        mock_tcgetattr.return_value = attr_list\n        autoreload.ensure_echo_on()\n        self.assertIn(termios.ECHO, attr_list[3])\n        mock_tcsetattr.assert_called_once_with(sys.stdin, termios.TCSANOW, attr_list)\n", "    def test_termios_not_installed(self):\n        with mock.patch('django.utils.autoreload.termios', None):\n            # Ensure no exception is raised\n            autoreload.ensure_echo_on()\n", "    def setUp(self):\n        self.termios_patch = mock.patch('django.utils.autoreload.termios')\n        self.signal_patch = mock.patch('django.utils.autoreload.signal')\n        self.sys_patch = mock.patch('django.utils.autoreload.sys.stdin.isatty', return_value=True)\n        self.termios = self.termios_patch.start()\n        self.signal = self.signal_patch.start()\n        self.sys = self.sys_patch.start()\n        self.addCleanup(self.termios_patch.stop)\n        self.addCleanup(self.signal_patch.stop)\n        self.addCleanup(self.sys_patch.stop)\n"], "sample_108": ["    def test_route_pattern_converter(self):\n        route_pattern = RoutePattern('<int:year>/<str:month>/<slug:day>/', is_endpoint=True)\n        match = route_pattern.match('2023/October/10th/')\n        self.assertIsNotNone(match)\n        new_path, args, kwargs = match\n        self.assertEqual(new_path, '')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'year': 2023, 'month': 'October', 'day': '10th'})\n", "    def test_resolvermatch_repr(self):\n        func = lambda x: x\n        match = ResolverMatch(func, args=('arg1', 'arg2'), kwargs={'kwarg1': 'value1'}, url_name='test-url', app_names=['app1'], namespaces=['ns1'], route='test/route/')\n        expected_repr = (\n            \"ResolverMatch(func=<lambda>, args=('arg1', 'arg2'), kwargs={'kwarg1': 'value1'}, \"\n            \"url_name=test-url, app_names=['app1'], namespaces=['ns1'], route=test/route/)\"\n        )\n        self.assertEqual(repr(match), expected_repr)\n", "    def test_resolve_with_unexpected_keyword_arguments(self):\n        class FakeView:\n                pass\n\n        pattern = path('fake/<int:year>/', FakeView(), name='fake-view')\n        match = pattern.resolve('fake/2023/')\n        self.assertEqual(match.kwargs, {'year': 2023, 'unexpected_kwarg': None})\n        self.assertEqual(match.url_name, 'fake-view')\n        self.assertEqual(match.route, 'fake/<int:year>/')\n", "    def test_regex_compilation_for_locale(self):\n        pattern = RegexPattern(r'^example/(?P<param>\\w+)/$')\n        # Simulating different languages by manipulating _regex_dict\n        pattern._regex_dict = {\n            'en': re.compile(r'^example/(?P<param>\\w+)/$'),\n            'fr': re.compile(r'^exemple/(?P<param>\\w+)/$')\n        }\n        # Testing for English language code\n        with self.settings(LANGUAGE_CODE='en'):\n            regex = pattern.regex\n            self.assertEqual(regex.pattern, r'^example/(?P<param>\\w+)/$')\n        # Testing for French language code\n        with self.settings(LANGUAGE_CODE='fr'):\n            regex = pattern.regex\n            self.assertEqual(regex.pattern, r'^exemple/(?P<param>\\w+)/$')\n", "    def test_regex_pattern_compile_error(self):\n        with self.assertRaises(ImproperlyConfigured):\n            RegexPattern('[')  # Invalid regex pattern\n"], "sample_117": ["    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context(name='password', value=None, attrs={})\n        self.assertIn('summary', context)\n        self.assertEqual(context['summary'], [{'label': gettext(\"No password set.\")}])\n", "    def test_to_python_normalizes_unicode(self):\n        field = UsernameField()\n        # U+2126 OHM SIGN should be normalized to U+03A9 GREEK CAPITAL LETTER OMEGA\n        data = 'test\u2126'\n        self.assertEqual(field.to_python(data), 'test\u03a9')\n    ", "    def test_has_changed(self):\n        user = User.objects.get(username='testclient')\n        initial_password = user.password\n        data = {'password': 'new_password'}\n\n        form = UserChangeForm(instance=user, data=data)\n        password_field = form.fields['password']\n\n        # Test that the password field has not changed.\n        self.assertFalse(password_field.has_changed(initial_password, data['password']))\n", "    def test_has_changed(self):\n        field = ReadOnlyPasswordHashField()\n        # Test that has_changed method always returns False\n        self.assertFalse(field.has_changed('initial_value', 'data_value'))\n        self.assertFalse(field.has_changed(None, 'data_value'))\n        self.assertFalse(field.has_changed('initial_value', None))\n        self.assertFalse(field.has_changed(None, None))\n", "    def test_reset_email_context(self):\n        \"\"\"\n        Ensure that the context passed to the email template contains the correct values.\n        \"\"\"\n        (user, username, email) = self.create_dummy_user()\n        form = PasswordResetForm({\"email\": email})\n        self.assertTrue(form.is_valid())\n        with mock.patch.object(PasswordResetForm, 'send_mail') as mock_send_mail:\n            form.save()\n            context = mock_send_mail.call_args[0][2]\n            self.assertEqual(context['email'], email)\n            self.assertEqual(context['user'], user)\n            self.assertIn('token', context)\n            self.assertIn('uid', context)\n            self.assertEqual(context['protocol'], 'http')\n"], "sample_133": ["    def setUp(self):\n        self.factory = RequestFactory()\n", "    def test_jsi18n_for_specific_package(self):\n        \"\"\"\n        Test that the JavaScriptCatalog view correctly returns translations\n        for a specific package.\n        \"\"\"\n        with override('fr'):\n            response = self.client.get('/jsi18n_specific_package/')\n            self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n            self.assertContains(response, 'il faut traduire cette cha\u00eene de caract\u00e8res de specific_package')\n            self.assertNotContains(response, 'this string should not be translated')\n", "    def test_setlang_with_nonexistent_language_code(self):\n        \"\"\"\n        The set_language view should not change the session language if an\n        invalid language code is provided.\n        \"\"\"\n        current_language = get_language()\n        post_data = {'language': 'nonexistent_language_code', 'next': '/'}\n        response = self.client.post('/i18n/setlang/', post_data)\n        self.assertRedirects(response, '/')\n        # The language should not be changed in the session.\n        with ignore_warnings(category=RemovedInDjango40Warning):\n            self.assertEqual(self.client.session.get(LANGUAGE_SESSION_KEY), current_language)\n        # The language should not be set in a cookie.\n        language_cookie = self.client.cookies.get(settings.LANGUAGE_COOKIE_NAME)\n        self.assertIsNone(language_cookie)\n", "    def test_get_paths_valid_packages(self):\n        view = JavaScriptCatalog()\n        valid_packages = ['view_tests.app1', 'view_tests.app2']\n        paths = view.get_paths(valid_packages)\n        expected_paths = [\n            path.join(settings.BASE_DIR, 'view_tests', 'app1', 'locale'),\n            path.join(settings.BASE_DIR, 'view_tests', 'app2', 'locale'),\n        ]\n        self.assertEqual(paths, expected_paths)\n", "    def test_jsi18n_invalid_domain(self):\n        \"\"\"\n        Test that an invalid domain raises the appropriate error.\n        \"\"\"\n        response = self.client.get('/jsi18n/', {'domain': 'invalid_domain'})\n        self.assertEqual(response.status_code, 500)\n        self.assertIn('Invalid domain', response.content.decode())\n"], "sample_138": ["    def test_manifest_file_recreate(self):\n        \"\"\"\n        Test that the manifest file is recreated when it's deleted.\n        \"\"\"\n        manifest_path = storage.staticfiles_storage.path(storage.staticfiles_storage.manifest_name)\n        \n        # Ensure the manifest file exists initially\n        self.assertTrue(os.path.exists(manifest_path))\n        \n        # Delete the manifest file and ensure it's gone\n        os.remove(manifest_path)\n        self.assertFalse(os.path.exists(manifest_path))\n        \n        # Run post_process to trigger manifest recreation\n        storage.staticfiles_storage.post_process({})\n        \n        # Ensure the manifest file is recreated\n        self.assertTrue(os.path.exists(manifest_path))\n        self.assertPostCondition()\n", "    def test_hashed_name_with_directory(self):\n        \"\"\"\n        Test if hashed_name correctly handles directory paths and returns\n        the original name for directories instead of attempting to hash them.\n        \"\"\"\n        directory_name = 'cached/'\n        self.assertTrue(storage.staticfiles_storage.exists(directory_name))\n        hashed_name = storage.staticfiles_storage.hashed_name(directory_name)\n        self.assertEqual(hashed_name, directory_name)\n        self.assertPostCondition()\n", "    def setUp(self):\n        self.manifest_files_mixin = ManifestFilesMixin()\n", "    def setUp(self):\n        self.storage = storage.HashedFilesMixin()\n", "    def setUp(self):\n        self.storage = HashedFilesMixin()\n"], "sample_651": ["def test_warns_pop_best_match() -> None:\n    with pytest.warns(Warning) as record:\n        warnings.warn(\"test warning 1\", UserWarning)\n        warnings.warn(\"test warning 2\", DeprecationWarning)\n        warnings.warn(\"test warning 3\", RuntimeWarning)\n\n    assert len(record) == 3\n    warn = record.pop(Warning)\n    assert warn.category == UserWarning\n    assert str(warn.message) == \"test warning 1\"\n    assert len(record) == 2\n", "def test_recwarn_no_warnings(recwarn: WarningsRecorder) -> None:\n    \"\"\"\n    Test that the recorder remains empty if no warnings are emitted.\n    \"\"\"\n    assert len(recwarn) == 0\n    assert recwarn.list == []\n", "def test_warnings_recorder_reenter() -> None:\n    \"\"\"Ensure WarningsRecorder can be re-entered after being exited.\"\"\"\n    rec = WarningsRecorder(_ispytest=True)\n    with rec:\n        warnings.warn(\"first warning\", UserWarning)\n        assert len(rec.list) == 1\n    with rec:\n        warnings.warn(\"second warning\", UserWarning)\n        assert len(rec.list) == 2\n    assert str(rec.list[0].message) == \"first warning\"\n    assert str(rec.list[1].message) == \"second warning\"\n", "def test_warns_context_manager_preserves_filters_and_registry() -> None:\n    \"\"\"Ensure the context manager preserves the warning filters and registry.\"\"\"\n    original_filters = warnings.filters[:]\n    original_registry = warnings.onceregistry.copy()\n\n    with pytest.warns(UserWarning):\n        warnings.warn(\"user warning\", UserWarning)\n\n    assert warnings.filters == original_filters\n    assert warnings.onceregistry == original_registry\n", "def test_warns_invalid_warning_type() -> None:\n    \"\"\"Test that an invalid warning type raises a TypeError.\"\"\"\n    with pytest.raises(TypeError, match=\"exceptions must be derived from Warning\"):\n        with pytest.warns(int):\n            warnings.warn(\"this should not be allowed\", UserWarning)\n"], "sample_653": ["def test_custom_log_formatter(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import logging\n\n        class CustomFormatter(logging.Formatter):\n                return \"CUSTOM: \" + super().format(record)\n\n            plugin = config.pluginmanager.getplugin('logging-plugin')\n            plugin.formatter = CustomFormatter('%(levelname)s %(message)s')\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info(\"test with custom formatter\")\n            assert True\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*CUSTOM: INFO test with custom formatter*\",\n        ]\n    )\n", "def test_set_level_and_at_level(caplog):\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Test set_level\n    caplog.set_level(logging.INFO)\n    logger.debug(\"This debug message should not be captured\")\n    logger.info(\"This info message should be captured\")\n    assert (\"test_set_level_and_at_level\", logging.INFO, \"This info message should be captured\") in caplog.record_tuples\n    assert (\"test_set_level_and_at_level\", logging.DEBUG, \"This debug message should not be captured\") not in caplog.record_tuples\n\n    # Test at_level\n    with caplog.at_level(logging.WARNING):\n        logger.info(\"This info message should not be captured in context manager\")\n        logger.warning(\"This warning message should be captured in context manager\")\n\n    assert (\"test_set_level_and_at_level\", logging.WARNING, \"This warning message should be captured in context manager\") in caplog.record_tuples\n    assert (\"test_set_level_and_at_level\", logging.INFO, \"This info message should not be captured in context manager\") not in caplog.record_tuples\n", "def test_log_capture_fixture_at_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n            with caplog.at_level(logging.WARNING):\n                logger.debug('debug message')\n                logger.info('info message')\n                logger.warning('warning message')\n                logger.error('error message')\n                assert len(caplog.records) == 2  # only warning and error should be captured\n                assert caplog.records[0].message == 'warning message'\n                assert caplog.records[1].message == 'error message'\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n", "def test_colored_level_formatter(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        from _pytest.logging import ColoredLevelFormatter\n        from _pytest.config import create_terminal_writer\n\n            terminal_writer = create_terminal_writer(None)\n            formatter = ColoredLevelFormatter(terminal_writer, fmt=\"%(levelname)s: %(message)s\")\n\n            logger = logging.getLogger(\"test_logger\")\n            handler = logging.StreamHandler()\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n            logger.setLevel(logging.DEBUG)\n\n            logger.debug(\"debug message\")\n            logger.info(\"info message\")\n            logger.warning(\"warning message\")\n            logger.error(\"error message\")\n            logger.critical(\"critical message\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*DEBUG* debug message\",\n            \"*INFO* info message\",\n            \"*WARNING* warning message\",\n            \"*ERROR* error message\",\n            \"*CRITICAL* critical message\",\n        ]\n    )\n", "def test_log_capture_handler_reset(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        from _pytest.logging import LogCaptureHandler\n        \n            handler = LogCaptureHandler()\n            logger = logging.getLogger(__name__)\n            logger.addHandler(handler)\n            logger.warning('This is a warning')\n            assert len(handler.records) == 1\n            handler.reset()\n            assert len(handler.records) == 0\n            assert handler.stream.getvalue() == ''\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n"], "sample_654": ["def test_fixture_with_dynamic_scope(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        # Fixture with dynamic scope based on parameter\n        @pytest.fixture\n            if request.param == \"session\":\n                request.scope = \"session\"\n            elif request.param == \"module\":\n                request.scope = \"module\"\n            else:\n                request.scope = \"function\"\n            return request.scope\n\n        @pytest.mark.parametrize(\"dynamic_fixture\", [\"session\", \"module\", \"function\"], indirect=True)\n            assert dynamic_fixture in [\"session\", \"module\", \"function\"]\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-v\")\n    result.assert_outcomes(passed=3)\n    result.stdout.fnmatch_lines([\n        \"*::test_dynamic_scope[session]*\",\n        \"*::test_dynamic_scope[module]*\",\n        \"*::test_dynamic_scope[function]*\",\n    ])\n", "def test_fixture_request_getfixturevalue_adds_finalizer(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        teardown_calls = []\n\n        @pytest.fixture\n            request.addfinalizer(lambda: teardown_calls.append(\"resource finalizer\"))\n            return \"resource\"\n\n        @pytest.fixture\n            resource = request.getfixturevalue(\"resource\")\n            return resource\n\n            assert dependent == \"resource\"\n        \n            assert teardown_calls == [\"resource finalizer\"]\n        \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=2)\n", "def test_fixture_with_finalizer_and_parametrize(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        values = []\n\n        @pytest.fixture(params=[1, 2])\n            values.append(f\"setup-{request.param}\")\n            yield request.param\n            values.append(f\"teardown-{request.param}\")\n\n            values.append(f\"test1-{param_fixture}\")\n\n            values.append(f\"test2-{param_fixture}\")\n\n            assert values == [\n                \"setup-1\", \"test1-1\", \"teardown-1\",\n                \"setup-2\", \"test1-2\", \"teardown-2\",\n                \"setup-1\", \"test2-1\", \"teardown-1\",\n                \"setup-2\", \"test2-2\", \"teardown-2\",\n            ]\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=3)\n    result.stdout.fnmatch_lines([\n        \"*::test_fixture_param_1[1]*\",\n        \"*::test_fixture_param_1[2]*\",\n        \"*::test_fixture_param_2[1]*\",\n        \"*::test_fixture_param_2[2]*\",\n        \"*::test_check*\"\n    ])\n", "def test_fixture_with_exception_in_teardown(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            yield \"resource\"\n            raise RuntimeError(\"Error during teardown\")\n\n            assert resource == \"resource\"\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_using_resource PASSED*\",\n            \"*ERROR at teardown of test_using_resource*\",\n            \"*RuntimeError: Error during teardown*\",\n        ]\n    )\n    assert result.ret == 1\n", "def test_fixture_with_exception_handling(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            raise Exception(\"Something went wrong!\")\n\n            assert False, \"This should not be reached due to the exception in the fixture\"\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*ERROR at setup of test_broken_fixture*\",\n            \"*Exception: Something went wrong!*\",\n            \"*1 error*\"\n        ]\n    )\n"]}, "raw_scored_generations": {"sample_1": [false, false, false, false, false], "sample_4": [false, false, false, true, false], "sample_3": [false, false, true, true, true], "sample_0": [false, false, false, false, false], "sample_2": [true, false, false, true, false], "sample_5": [false, false, false, false, false], "sample_6": [false, false, false, false, true], "sample_7": [true, false, false, false, false], "sample_8": [false, false, false, true, false], "sample_9": [true, true, true, true, true], "sample_10": [false, false, true, false, true], "sample_11": [true, false, true, true, true], "sample_12": [false, false, false, false, false], "sample_13": [false, false, false, true, false], "sample_14": [false, false, false, false, false], "sample_15": [true, false, true, true, false], "sample_16": [false, false, false, false, false], "sample_17": [true, false, false, true, false], "sample_18": [false, false, false, true, false], "sample_19": [false, false, false, false, false], "sample_20": [true, false, true, false, false], "sample_21": [false, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_23": [false, true, false, false, true], "sample_24": [false, true, false, false, false], "sample_25": [false, false, false, false, false], "sample_26": [true, true, false, false, false], "sample_27": [true, false, true, true, false], "sample_28": [false, false, false, false, false], "sample_29": [false, true, false, false, false], "sample_30": [false, false, false, false, false], "sample_31": [false, false, false, false, false], "sample_32": [false, false, true, false, true], "sample_33": [true, false, true, true, true], "sample_34": [true, false, false, false, false], "sample_35": [false, true, false, false, true], "sample_36": [false, false, false, true, false], "sample_37": [false, false, false, false, false], "sample_38": [true, false, false, false, true], "sample_39": [true, false, false, false, false], "sample_40": [true, true, false, false, true], "sample_41": [false, true, false, true, false], "sample_42": [false, false, true, false, false], "sample_43": [false, false, false, false, true], "sample_44": [false, false, false, false, false], "sample_45": [false, false, false, false, false], "sample_46": [false, false, false, false, false], "sample_47": [false, false, false, false, false], "sample_48": [false, false, false, false, false], "sample_49": [false, true, true, false, false], "sample_50": [true, true, true, true, true], "sample_51": [false, false, false, true, false], "sample_52": [true, false, false, true, false], "sample_54": [true, true, true, true, true], "sample_53": [true, false, false, false, true], "sample_55": [false, false, false, false, false], "sample_58": [false, true, false, true, false], "sample_56": [true, true, false, true, true], "sample_57": [true, true, true, true, true], "sample_59": [true, true, true, false, true], "sample_60": [false, false, false, false, false], "sample_61": [true, true, true, false, true], "sample_62": [false, true, false, true, false], "sample_63": [false, false, true, true, false], "sample_64": [false, false, false, false, false], "sample_65": [true, false, true, true, true], "sample_67": [false, false, false, false, false], "sample_66": [false, false, false, false, false], "sample_68": [false, false, false, false, false], "sample_69": [false, false, false, false, false], "sample_70": [false, false, false, false, false], "sample_71": [false, false, false, false, false], "sample_72": [false, true, false, false, false], "sample_73": [false, false, true, false, false], "sample_75": [false, false, false, false, false], "sample_74": [true, true, true, true, true], "sample_76": [true, true, true, true, true], "sample_77": [false, false, false, false, false], "sample_78": [false, true, false, true, false], "sample_79": [false, false, false, false, false], "sample_80": [true, false, true, true, false], "sample_82": [false, false, false, false, false], "sample_81": [false, false, false, false, false], "sample_83": [false, false, false, false, false], "sample_85": [false, true, true, false, false], "sample_84": [false, false, false, false, false], "sample_86": [true, true, false, false, true], "sample_88": [true, false, false, false, false], "sample_87": [false, false, false, false, false], "sample_89": [false, false, false, false, false], "sample_90": [true, false, false, false, false], "sample_91": [true, false, false, false, true], "sample_92": [false, false, false, false, false], "sample_93": [true, false, true, false, true], "sample_94": [false, false, false, false, false], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, false], "sample_96": [true, true, true, true, true], "sample_99": [false, false, false, false, true], "sample_97": [false, false, false, false, false], "sample_100": [false, false, false, false, false], "sample_102": [false, false, false, false, false], "sample_101": [false, false, false, true, false], "sample_103": [false, false, false, true, false], "sample_104": [true, false, false, false, true], "sample_107": [false, false, false, false, false], "sample_106": [true, true, true, false, true], "sample_105": [false, false, false, false, false], "sample_108": [false, false, false, false, false], "sample_109": [true, false, false, false, false], "sample_111": [true, true, false, true, true], "sample_110": [false, true, false, false, false], "sample_112": [false, false, false, false, false], "sample_113": [true, false, false, true, true], "sample_114": [false, true, false, true, true], "sample_115": [false, false, false, false, false], "sample_116": [true, false, true, true, false], "sample_117": [false, false, true, true, false], "sample_118": [true, true, true, true, true], "sample_119": [false, true, true, true, false], "sample_120": [false, false, true, false, true], "sample_121": [false, true, false, false, false], "sample_122": [false, false, true, true, true], "sample_123": [false, false, false, false, false], "sample_124": [false, false, false, false, false], "sample_125": [false, true, true, true, true], "sample_126": [false, true, false, false, false], "sample_127": [false, true, false, true, false], "sample_128": [true, false, true, true, false], "sample_129": [false, true, false, false, false], "sample_130": [false, true, false, true, false], "sample_131": [false, false, true, true, true], "sample_132": [true, false, false, false, false], "sample_133": [false, true, true, true, true], "sample_135": [true, true, false, true, true], "sample_134": [false, false, false, false, false], "sample_136": [false, false, false, false, false], "sample_139": [true, true, true, true, true], "sample_137": [false, false, false, false, false], "sample_138": [false, false, false, false, false], "sample_140": [false, false, false, false, false], "sample_141": [false, false, false, false, false], "sample_142": [true, false, false, false, true], "sample_143": [true, false, true, true, true], "sample_144": [false, true, false, true, false], "sample_145": [true, true, true, true, true], "sample_146": [true, true, true, true, true], "sample_147": [false, true, true, true, false], "sample_148": [false, false, false, false, false], "sample_151": [true, true, false, true, true], "sample_149": [true, true, true, true, false], "sample_152": [false, false, false, false, false], "sample_150": [false, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [false, false, false, false, false], "sample_155": [false, true, true, true, true], "sample_156": [true, false, false, false, true], "sample_157": [false, false, false, false, false], "sample_158": [true, true, true, true, false], "sample_159": [true, false, true, true, true], "sample_160": [false, false, false, false, false], "sample_161": [true, true, false, false, false], "sample_162": [false, false, false, false, true], "sample_163": [false, false, false, false, false], "sample_164": [false, false, false, false, false], "sample_165": [false, false, false, false, false], "sample_166": [true, true, true, true, true], "sample_168": [true, true, false, false, false], "sample_167": [false, false, false, false, false], "sample_169": [false, true, false, false, true], "sample_171": [false, false, false, false, false], "sample_170": [false, false, false, false, false], "sample_172": [true, false, false, true, false], "sample_173": [false, false, false, false, false], "sample_174": [false, false, false, true, false], "sample_175": [false, false, false, false, false], "sample_176": [false, true, true, false, true], "sample_177": [false, false, false, true, true], "sample_178": [false, false, false, false, false], "sample_180": [false, false, false, false, true], "sample_179": [false, true, false, true, false], "sample_182": [true, false, false, false, true], "sample_181": [true, true, false, true, false], "sample_183": [false, false, false, false, false], "sample_184": [false, true, true, false, false], "sample_185": [true, false, false, false, true], "sample_186": [true, true, true, true, false], "sample_187": [true, true, false, true, true], "sample_188": [false, false, false, false, false], "sample_189": [false, false, false, false, true], "sample_190": [true, true, false, false, false], "sample_191": [false, false, false, false, false], "sample_192": [false, false, false, false, false], "sample_193": [false, false, false, true, false], "sample_194": [true, false, true, true, true], "sample_195": [false, false, false, false, false], "sample_196": [false, false, false, true, false], "sample_198": [false, false, false, false, false], "sample_197": [true, false, true, true, true], "sample_199": [false, true, true, true, false], "sample_200": [true, true, false, false, true], "sample_201": [true, true, true, true, false], "sample_202": [true, true, false, true, true], "sample_203": [true, true, true, false, false], "sample_204": [false, false, false, false, false], "sample_205": [true, true, false, true, true], "sample_206": [false, true, false, false, false], "sample_207": [false, false, false, false, false], "sample_208": [false, true, false, false, false], "sample_209": [false, false, false, true, false], "sample_210": [true, false, false, true, false], "sample_211": [false, false, false, false, false], "sample_213": [false, false, false, false, false], "sample_214": [false, false, false, false, true], "sample_212": [false, true, false, true, true], "sample_215": [false, true, false, false, false], "sample_216": [true, true, false, true, false], "sample_217": [false, false, false, false, true], "sample_218": [false, false, false, false, false], "sample_219": [false, true, true, false, false], "sample_220": [true, true, false, true, true], "sample_221": [false, false, false, true, false], "sample_222": [true, true, true, true, false], "sample_223": [true, false, false, true, false], "sample_224": [true, true, false, false, false], "sample_225": [false, false, false, false, false], "sample_226": [false, false, true, false, false], "sample_227": [false, false, false, false, false], "sample_228": [true, false, true, false, false], "sample_229": [false, false, false, false, true], "sample_230": [true, false, false, true, false], "sample_231": [true, false, false, false, false], "sample_232": [false, false, false, true, false], "sample_233": [false, true, true, true, true], "sample_234": [true, true, false, true, true], "sample_235": [true, false, false, true, true], "sample_236": [true, false, false, true, false], "sample_237": [true, false, false, true, true], "sample_238": [false, false, false, false, false], "sample_239": [true, false, false, false, false], "sample_240": [true, true, true, true, true], "sample_241": [false, false, false, false, false], "sample_242": [false, false, false, false, false], "sample_243": [true, true, true, true, true], "sample_244": [true, true, false, false, false], "sample_245": [true, true, true, false, false], "sample_246": [false, false, true, false, false], "sample_247": [true, true, false, false, true], "sample_248": [false, false, false, false, false], "sample_249": [false, true, false, false, true], "sample_250": [false, true, true, true, true], "sample_251": [true, true, true, true, true], "sample_252": [true, false, false, true, false], "sample_253": [false, false, false, false, false], "sample_254": [true, false, true, true, true], "sample_256": [false, false, false, false, false], "sample_255": [false, false, false, false, false], "sample_257": [false, false, false, false, false], "sample_258": [false, true, false, false, false], "sample_259": [true, true, true, true, true], "sample_260": [true, false, false, false, false], "sample_261": [true, false, false, false, false], "sample_262": [false, false, true, false, false], "sample_263": [true, true, false, true, true], "sample_264": [true, false, false, true, false], "sample_265": [false, false, false, false, false], "sample_266": [false, false, false, false, true], "sample_267": [false, true, true, false, false], "sample_268": [false, false, false, false, false], "sample_269": [true, true, true, true, true], "sample_270": [false, false, false, false, true], "sample_271": [false, false, false, false, false], "sample_272": [false, false, false, false, false], "sample_273": [false, false, false, false, false], "sample_274": [false, false, false, true, false], "sample_275": [false, false, false, false, false], "sample_276": [false, false, false, false, false], "sample_277": [false, false, false, false, true], "sample_278": [false, false, false, false, false], "sample_279": [true, true, false, true, true], "sample_280": [false, false, true, false, false], "sample_281": [true, true, true, true, true], "sample_282": [true, false, true, false, true], "sample_283": [true, true, true, true, true], "sample_284": [true, false, false, false, false], "sample_285": [false, false, true, false, false], "sample_286": [false, false, false, true, false], "sample_287": [true, true, true, false, true], "sample_288": [false, false, false, false, true], "sample_289": [true, true, true, false, true], "sample_290": [false, false, false, false, false], "sample_291": [false, false, false, false, false], "sample_292": [false, false, false, false, false], "sample_293": [false, false, false, true, false], "sample_294": [false, false, false, false, false], "sample_295": [false, false, false, true, false], "sample_296": [true, false, false, true, true], "sample_297": [false, false, false, false, false], "sample_298": [true, true, true, true, true], "sample_299": [true, true, true, true, true], "sample_300": [false, true, false, false, true], "sample_301": [false, false, false, false, false], "sample_302": [true, false, false, false, true], "sample_303": [false, false, false, false, false], "sample_304": [true, true, true, true, false], "sample_305": [false, false, false, false, false], "sample_306": [false, false, false, false, false], "sample_307": [true, true, true, true, true], "sample_308": [false, true, true, true, false], "sample_309": [false, false, false, false, false], "sample_310": [false, false, false, false, false], "sample_312": [true, true, true, false, true], "sample_311": [false, false, false, false, false], "sample_313": [false, false, false, false, false], "sample_314": [false, false, false, false, false], "sample_315": [true, true, false, true, true], "sample_316": [false, true, false, false, true], "sample_317": [false, true, false, false, true], "sample_318": [true, false, false, false, false], "sample_319": [false, false, false, true, false], "sample_320": [false, false, true, false, false], "sample_321": [true, true, false, true, true], "sample_322": [false, false, false, false, false], "sample_323": [false, false, false, false, false], "sample_324": [false, false, false, false, false], "sample_325": [false, true, true, false, false], "sample_326": [false, false, false, false, false], "sample_327": [true, true, true, true, true], "sample_328": [true, false, true, true, false], "sample_329": [false, false, false, false, false], "sample_330": [false, false, false, false, false], "sample_331": [false, false, false, false, false], "sample_332": [true, false, false, false, false], "sample_333": [true, false, false, true, true], "sample_334": [true, false, false, true, false], "sample_335": [true, false, true, true, true], "sample_336": [false, false, true, true, false], "sample_337": [false, false, false, false, false], "sample_338": [false, false, false, false, false], "sample_339": [false, false, true, false, false], "sample_340": [false, false, false, false, false], "sample_341": [false, false, false, true, false], "sample_342": [true, true, true, true, true], "sample_343": [false, false, true, false, false], "sample_344": [true, true, false, true, true], "sample_345": [false, false, false, false, false], "sample_346": [false, false, false, false, false], "sample_347": [true, false, true, true, false], "sample_348": [false, false, false, false, false], "sample_349": [false, false, false, false, true], "sample_350": [false, true, false, true, false], "sample_351": [false, false, true, true, false], "sample_352": [false, false, true, true, false], "sample_353": [false, false, false, false, false], "sample_354": [false, true, false, true, false], "sample_355": [false, true, true, true, true], "sample_356": [false, true, false, false, true], "sample_357": [false, false, false, false, false], "sample_358": [false, false, false, false, false], "sample_359": [false, false, false, false, false], "sample_360": [true, false, false, false, false], "sample_361": [false, false, false, false, false], "sample_362": [false, false, false, false, false], "sample_363": [true, true, true, true, true], "sample_364": [false, false, true, true, false], "sample_365": [false, false, false, false, false], "sample_366": [false, false, false, false, false], "sample_367": [false, false, false, false, false], "sample_368": [false, false, false, false, false], "sample_369": [false, false, false, true, false], "sample_370": [false, true, false, false, true], "sample_371": [false, false, false, false, false], "sample_372": [true, false, true, false, true], "sample_373": [false, false, false, false, false], "sample_374": [false, false, false, false, false], "sample_375": [true, true, false, false, false], "sample_376": [false, true, false, true, true], "sample_377": [false, false, false, false, false], "sample_378": [true, true, true, false, false], "sample_379": [true, true, true, true, true], "sample_380": [true, false, true, false, false], "sample_381": [true, true, true, true, false], "sample_382": [false, false, false, false, false], "sample_383": [false, false, false, false, false], "sample_384": [false, false, false, false, true], "sample_385": [true, true, false, false, true], "sample_386": [true, true, true, true, true], "sample_387": [false, true, true, true, true], "sample_388": [false, true, false, false, false], "sample_389": [true, false, false, false, true], "sample_390": [false, false, false, false, false], "sample_391": [false, false, false, true, false], "sample_392": [false, false, false, false, false], "sample_393": [false, true, false, true, true], "sample_394": [false, true, false, false, false], "sample_395": [false, false, false, false, false], "sample_396": [false, false, true, false, false], "sample_397": [false, true, false, false, false], "sample_398": [false, false, false, false, false], "sample_399": [false, false, false, true, false], "sample_400": [false, false, true, false, false], "sample_401": [true, true, false, false, true], "sample_402": [true, true, false, true, false], "sample_403": [false, false, false, false, false], "sample_404": [true, true, false, true, false], "sample_405": [false, true, false, true, false], "sample_406": [true, false, true, false, true], "sample_407": [true, true, false, false, true], "sample_408": [true, false, false, false, false], "sample_409": [false, false, false, false, true], "sample_410": [false, false, false, true, false], "sample_411": [false, true, true, false, true], "sample_412": [false, false, false, false, false], "sample_413": [true, true, false, true, true], "sample_414": [false, false, true, true, false], "sample_415": [true, false, true, false, false], "sample_416": [true, true, true, true, true], "sample_417": [false, true, false, false, false], "sample_418": [false, false, false, false, false], "sample_419": [false, false, false, false, true], "sample_420": [false, false, true, false, true], "sample_421": [false, false, false, false, false], "sample_422": [false, false, false, false, false], "sample_423": [true, false, false, true, false], "sample_424": [false, false, false, true, false], "sample_425": [false, false, true, false, true], "sample_426": [true, false, true, false, true], "sample_427": [false, false, false, false, true], "sample_428": [true, false, false, false, false], "sample_429": [true, true, false, true, true], "sample_430": [true, false, false, false, false], "sample_431": [true, false, true, false, false], "sample_432": [true, true, false, false, false], "sample_433": [false, true, true, false, true], "sample_434": [false, false, false, false, false], "sample_435": [true, true, false, true, false], "sample_436": [false, false, false, false, false], "sample_437": [false, true, false, false, false], "sample_438": [false, true, false, false, false], "sample_439": [true, true, true, true, true], "sample_440": [true, true, false, false, true], "sample_441": [false, true, false, false, false], "sample_442": [false, false, true, false, true], "sample_443": [false, true, false, true, false], "sample_444": [false, false, false, false, false], "sample_445": [true, true, true, true, true], "sample_446": [false, true, false, false, true], "sample_447": [true, true, false, false, false], "sample_448": [true, true, true, false, false], "sample_449": [false, false, false, false, false], "sample_450": [false, false, false, false, false], "sample_451": [false, true, false, false, false], "sample_453": [false, false, false, false, false], "sample_452": [false, true, false, false, false], "sample_454": [false, false, false, false, false], "sample_455": [true, false, true, true, true], "sample_456": [true, false, true, true, true], "sample_457": [true, true, false, true, true], "sample_458": [false, false, true, false, true], "sample_459": [false, false, false, false, false], "sample_460": [false, false, false, false, false], "sample_461": [false, false, false, false, false], "sample_462": [true, true, true, true, false], "sample_463": [true, false, false, false, false], "sample_464": [false, true, true, true, true], "sample_465": [true, false, false, false, false], "sample_466": [true, false, true, false, false], "sample_467": [false, false, false, false, false], "sample_469": [false, false, false, true, true], "sample_468": [false, false, true, false, false], "sample_470": [false, false, false, false, false], "sample_471": [true, true, true, true, true], "sample_472": [true, false, true, false, false], "sample_473": [false, false, false, false, false], "sample_474": [false, true, false, true, false], "sample_475": [true, true, true, true, true], "sample_476": [false, false, false, false, false], "sample_477": [false, false, false, false, false], "sample_478": [true, true, true, true, true], "sample_479": [false, true, false, false, false], "sample_480": [true, false, false, false, false], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [true, false, true, false, true], "sample_484": [false, false, false, false, false], "sample_485": [false, true, false, false, false], "sample_486": [false, false, false, true, true], "sample_487": [true, true, true, false, true], "sample_488": [true, false, false, true, false], "sample_489": [true, false, false, false, true], "sample_490": [false, false, true, false, true], "sample_491": [true, true, true, true, false], "sample_492": [true, true, false, false, true], "sample_493": [false, false, false, false, false], "sample_494": [false, false, false, false, false], "sample_495": [true, false, true, true, false], "sample_496": [false, false, false, false, false], "sample_497": [false, false, false, true, true], "sample_498": [false, false, false, false, false], "sample_499": [false, false, false, false, false], "sample_500": [false, true, true, true, false], "sample_501": [false, false, false, false, false], "sample_502": [false, true, true, true, false], "sample_503": [false, false, false, true, false], "sample_504": [true, false, true, true, false], "sample_505": [true, false, false, false, true], "sample_506": [true, false, false, false, false], "sample_507": [false, true, false, true, true], "sample_508": [true, false, false, false, false], "sample_509": [true, true, false, false, false], "sample_510": [false, false, true, true, false], "sample_511": [false, false, false, false, false], "sample_512": [false, true, true, true, false], "sample_513": [false, false, false, false, false], "sample_514": [true, true, true, true, true], "sample_515": [false, true, false, false, false], "sample_516": [false, false, false, false, false], "sample_517": [false, true, false, true, true], "sample_518": [true, false, false, true, true], "sample_519": [true, false, false, true, false], "sample_520": [false, true, true, true, true], "sample_521": [true, false, true, true, false], "sample_522": [true, false, true, true, false], "sample_523": [false, false, true, false, true], "sample_524": [true, false, false, false, true], "sample_525": [false, true, true, false, true], "sample_526": [false, false, true, false, false], "sample_527": [false, true, true, false, false], "sample_528": [false, false, true, true, false], "sample_529": [false, false, false, false, false], "sample_530": [false, false, true, false, false], "sample_531": [true, false, false, false, false], "sample_532": [false, false, false, false, false], "sample_533": [false, true, false, false, false], "sample_534": [false, false, false, false, false], "sample_535": [true, false, true, true, true], "sample_536": [true, true, false, true, false], "sample_537": [true, false, false, false, true], "sample_538": [true, false, false, false, true], "sample_539": [true, false, false, false, false], "sample_540": [true, false, false, false, false], "sample_541": [false, true, false, false, false], "sample_542": [false, true, false, true, false], "sample_543": [false, false, false, false, true], "sample_544": [true, false, false, true, true], "sample_545": [true, true, true, false, false], "sample_546": [false, true, false, false, false], "sample_547": [true, false, true, false, false], "sample_548": [false, false, false, false, false], "sample_549": [true, false, false, false, true], "sample_550": [false, false, false, false, true], "sample_551": [true, true, true, true, false], "sample_552": [true, false, false, false, false], "sample_553": [false, false, false, false, true], "sample_554": [true, false, true, false, true], "sample_555": [true, false, false, false, false], "sample_556": [false, false, false, false, true], "sample_557": [false, true, false, true, true], "sample_558": [true, true, true, false, true], "sample_559": [false, false, true, true, false], "sample_560": [true, true, false, false, false], "sample_561": [true, false, false, false, true], "sample_562": [false, false, false, true, true], "sample_563": [false, false, false, false, false], "sample_564": [false, false, true, false, false], "sample_565": [false, false, false, false, false], "sample_566": [false, true, false, true, true], "sample_567": [true, false, true, false, true], "sample_568": [true, true, true, true, true], "sample_569": [false, false, false, true, false], "sample_570": [false, false, false, false, false], "sample_571": [true, false, false, false, false], "sample_572": [true, false, false, false, false], "sample_573": [false, false, false, false, false], "sample_574": [false, false, false, false, true], "sample_575": [false, false, true, false, false], "sample_576": [false, true, false, true, false], "sample_577": [false, false, false, false, false], "sample_578": [false, true, false, true, false], "sample_579": [true, false, false, false, true], "sample_580": [true, true, false, false, false], "sample_581": [false, false, false, false, false], "sample_582": [false, false, false, false, false], "sample_583": [true, false, false, true, true], "sample_584": [false, false, true, false, true], "sample_585": [false, false, false, false, false], "sample_586": [false, false, false, false, false], "sample_587": [true, false, false, false, false], "sample_588": [false, true, true, false, false], "sample_589": [false, true, true, false, true], "sample_590": [false, false, true, true, true], "sample_591": [false, false, false, false, false], "sample_592": [true, true, false, false, true], "sample_593": [false, true, true, false, false], "sample_594": [true, false, false, true, false], "sample_595": [false, true, true, false, false], "sample_596": [false, false, false, false, false], "sample_597": [true, false, true, false, false], "sample_598": [true, true, true, false, false], "sample_599": [true, false, true, true, false], "sample_600": [true, true, false, false, true], "sample_601": [false, false, false, false, false], "sample_602": [false, false, false, false, false], "sample_603": [false, true, false, true, true], "sample_604": [false, true, true, true, true], "sample_605": [false, false, true, true, false], "sample_606": [false, false, false, false, false], "sample_607": [false, false, false, false, true], "sample_608": [false, true, true, true, true], "sample_609": [false, false, false, false, false], "sample_610": [true, false, false, false, false], "sample_611": [false, false, false, false, false], "sample_612": [false, false, true, false, false], "sample_613": [false, false, false, false, false], "sample_614": [false, false, false, false, true], "sample_615": [false, false, false, false, false], "sample_616": [false, true, false, false, false], "sample_617": [true, false, false, false, false], "sample_618": [false, false, false, false, false], "sample_619": [false, true, false, false, false], "sample_620": [false, false, false, false, true], "sample_621": [false, false, false, false, false], "sample_622": [false, false, true, false, false], "sample_623": [true, true, false, false, false], "sample_624": [true, true, false, true, false], "sample_625": [false, false, false, false, false], "sample_626": [false, false, false, false, false], "sample_627": [false, false, false, false, true], "sample_628": [false, true, false, false, false], "sample_629": [false, true, false, false, false], "sample_630": [false, false, true, false, false], "sample_631": [false, false, false, false, false], "sample_632": [true, true, false, false, false], "sample_633": [false, true, true, false, false], "sample_634": [false, true, true, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, false, false], "sample_638": [true, false, false, false, false], "sample_639": [false, false, false, false, false], "sample_640": [false, false, false, false, false], "sample_641": [false, false, false, false, false], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [false, true, false, false, false], "sample_646": [false, false, false, false, true], "sample_647": [true, true, true, true, false], "sample_648": [false, false, false, false, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, true, false, true, true], "sample_652": [false, false, false, false, false], "sample_653": [false, false, false, false, false], "sample_654": [false, false, false, false, false], "sample_655": [false, true, false, true, false], "sample_656": [false, true, true, true, false], "sample_657": [false, false, false, false, false], "sample_658": [true, false, true, true, true], "sample_659": [false, false, false, false, false], "sample_660": [false, false, true, false, true], "sample_661": [false, false, true, true, false], "sample_662": [false, false, false, false, false], "sample_663": [false, false, false, true, false], "sample_664": [false, false, false, false, false], "sample_665": [false, false, false, false, false], "sample_666": [false, false, false, false, false], "sample_667": [true, true, true, false, false], "sample_668": [false, false, false, false, false], "sample_669": [true, true, true, false, false], "sample_670": [false, true, true, true, false], "sample_671": [false, false, true, true, false], "sample_672": [false, false, false, false, false], "sample_673": [false, false, true, false, true], "sample_674": [false, false, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, false, false, false], "sample_677": [true, false, false, false, false], "sample_678": [false, false, false, false, false], "sample_679": [false, false, false, false, false], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, false], "sample_682": [false, false, false, false, false], "sample_683": [false, false, false, true, true], "sample_684": [false, false, false, false, false], "sample_685": [false, false, false, false, false], "sample_686": [false, false, false, false, false], "sample_687": [false, false, false, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, true, false, false], "sample_691": [false, false, false, true, false], "sample_692": [true, true, false, true, true], "sample_693": [false, false, false, false, false], "sample_694": [false, false, false, false, false], "sample_695": [false, false, true, false, false], "sample_696": [false, false, false, false, false], "sample_697": [true, true, true, true, true], "sample_698": [false, false, false, false, false], "sample_699": [false, false, false, true, false], "sample_700": [false, true, false, true, false], "sample_701": [false, false, false, false, false], "sample_702": [false, false, false, false, false], "sample_703": [true, true, true, true, true], "sample_704": [false, false, false, false, false], "sample_705": [false, false, false, true, false], "sample_706": [true, false, true, true, true], "sample_707": [false, false, false, false, false], "sample_708": [false, false, false, true, false], "sample_709": [false, false, false, false, false], "sample_710": [false, false, true, false, false], "sample_711": [false, false, false, false, false], "sample_712": [false, false, false, false, false], "sample_713": [true, true, false, false, false], "sample_714": [false, false, false, false, false], "sample_715": [false, true, false, false, false], "sample_716": [true, false, false, false, false], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [false, false, false, false, false], "sample_720": [true, true, true, true, true], "sample_721": [false, false, false, false, false], "sample_722": [false, false, false, false, false], "sample_723": [true, false, true, true, true], "sample_724": [false, true, true, true, true], "sample_725": [false, false, false, true, false], "sample_726": [false, false, true, false, true], "sample_727": [true, true, true, true, true], "sample_728": [false, false, false, false, false], "sample_729": [false, false, false, false, true], "sample_730": [true, true, true, true, false], "sample_731": [true, true, true, true, true], "sample_732": [true, true, true, true, true], "sample_733": [false, false, false, false, true], "sample_734": [false, false, true, false, false], "sample_735": [false, true, true, true, false], "sample_736": [true, false, false, true, false], "sample_737": [false, true, false, false, false], "sample_738": [true, false, false, false, false], "sample_739": [true, true, true, false, true], "sample_740": [false, false, false, false, false], "sample_741": [true, true, true, false, true], "sample_742": [false, true, false, false, false], "sample_743": [false, false, false, false, false], "sample_744": [true, false, false, false, false], "sample_745": [true, true, false, true, false], "sample_746": [false, false, false, false, false], "sample_747": [true, false, true, false, true], "sample_748": [true, false, false, false, true], "sample_749": [false, false, true, false, false], "sample_750": [true, true, true, false, true], "sample_751": [false, false, false, false, false], "sample_752": [true, false, false, false, false], "sample_753": [false, true, false, true, true], "sample_754": [false, false, false, false, false], "sample_755": [true, true, false, false, false], "sample_756": [true, false, false, false, false], "sample_757": [false, false, false, true, true], "sample_758": [false, false, false, true, true], "sample_759": [false, true, true, false, false], "sample_760": [false, true, false, true, true], "sample_761": [false, true, false, false, false], "sample_762": [false, false, false, false, false], "sample_763": [false, false, true, false, true], "sample_764": [false, false, true, false, false], "sample_765": [false, false, false, true, false], "sample_766": [false, false, true, false, true], "sample_767": [false, false, false, false, true], "sample_768": [false, true, true, true, true], "sample_769": [false, false, false, false, false], "sample_770": [false, false, false, false, false], "sample_771": [true, true, true, true, false], "sample_772": [true, true, true, true, true], "sample_773": [false, true, false, false, false], "sample_774": [true, false, false, false, true], "sample_775": [false, false, false, false, false], "sample_776": [true, false, false, false, false], "sample_777": [true, false, false, false, false], "sample_778": [false, true, true, false, false], "sample_779": [false, false, false, false, false], "sample_780": [true, true, true, true, true], "sample_781": [true, false, false, false, false], "sample_782": [false, false, true, false, true], "sample_783": [false, true, false, false, true], "sample_784": [false, false, false, false, false], "sample_785": [false, true, false, true, true], "sample_786": [true, false, true, false, true], "sample_787": [false, false, false, false, false], "sample_788": [false, true, false, false, false], "sample_789": [true, true, true, false, true], "sample_790": [false, false, false, false, false], "sample_791": [false, false, false, false, false], "sample_792": [true, false, false, true, true], "sample_793": [true, true, true, false, false], "sample_794": [true, false, false, false, false], "sample_795": [false, false, false, false, false], "sample_796": [true, false, true, true, true], "sample_797": [true, false, true, true, true], "sample_798": [false, true, false, true, false], "sample_799": [false, false, false, true, true], "sample_800": [false, false, false, false, false], "sample_801": [false, false, false, false, false], "sample_802": [true, false, false, false, false], "sample_803": [false, false, false, false, false], "sample_804": [false, false, false, false, false], "sample_805": [true, true, true, true, true], "sample_806": [false, false, false, false, false], "sample_807": [false, false, true, true, true], "sample_808": [false, true, false, true, false], "sample_809": [false, false, false, false, false], "sample_810": [true, true, true, false, false], "sample_811": [false, false, false, true, false], "sample_812": [false, false, false, false, false], "sample_813": [false, true, true, false, false], "sample_814": [false, false, false, false, false], "sample_815": [false, false, false, true, true], "sample_816": [false, false, false, false, false], "sample_817": [true, false, true, false, true], "sample_818": [true, false, true, true, true], "sample_819": [true, true, true, true, true], "sample_820": [true, false, true, true, true], "sample_821": [false, true, true, true, true], "sample_822": [false, false, false, false, true], "sample_823": [true, true, false, false, true], "sample_824": [false, false, false, false, false], "sample_825": [true, true, true, true, false], "sample_826": [true, false, false, false, false], "sample_827": [false, false, false, false, false], "sample_828": [false, false, true, false, true], "sample_829": [true, false, true, true, false], "sample_830": [true, false, false, false, true], "sample_831": [true, true, true, false, false], "sample_832": [false, false, true, false, true], "sample_833": [false, false, true, true, false], "sample_834": [false, true, true, false, true], "sample_835": [true, true, false, true, false], "sample_836": [false, false, true, false, true], "sample_837": [true, false, false, false, false], "sample_838": [false, false, false, false, false], "sample_839": [false, false, false, true, false], "sample_840": [false, true, true, true, true], "sample_841": [false, false, false, true, false], "sample_842": [false, false, false, false, false], "sample_843": [false, false, false, false, false], "sample_844": [true, false, false, true, false], "sample_845": [false, false, true, false, false], "sample_846": [false, true, false, false, true], "sample_847": [true, true, false, true, true], "sample_848": [true, false, true, true, false], "sample_849": [false, true, true, true, true], "sample_850": [false, false, true, true, false], "sample_851": [true, false, true, false, false], "sample_852": [false, false, false, false, false], "sample_853": [false, true, false, false, false], "sample_854": [true, false, false, false, false], "sample_855": [true, false, true, false, true], "sample_856": [false, true, false, false, false], "sample_857": [true, true, true, true, true], "sample_858": [false, true, true, true, true], "sample_859": [false, true, false, false, true], "sample_860": [true, false, false, false, true], "sample_861": [true, true, false, false, false], "sample_862": [false, false, false, false, true], "sample_863": [false, false, true, true, false], "sample_864": [false, false, true, false, false], "sample_865": [false, true, true, true, true], "sample_866": [true, true, true, true, false], "sample_867": [true, false, false, true, true], "sample_868": [false, false, false, false, false], "sample_869": [false, false, true, false, false], "sample_870": [false, false, true, false, false], "sample_871": [false, true, true, true, true], "sample_872": [false, false, true, false, false], "sample_873": [false, true, false, true, true], "sample_874": [true, false, false, true, false], "sample_875": [false, false, false, false, false], "sample_876": [false, false, true, true, false], "sample_877": [false, false, false, false, false], "sample_878": [false, false, false, false, false], "sample_879": [false, false, false, false, false], "sample_880": [false, false, false, false, false], "sample_881": [false, false, false, false, false], "sample_882": [true, false, true, false, false], "sample_883": [true, true, false, false, true], "sample_884": [true, true, true, true, true], "sample_885": [true, true, false, false, false], "sample_886": [false, false, false, false, false], "sample_887": [true, true, true, false, false], "sample_888": [true, true, false, true, true], "sample_889": [false, false, false, false, false], "sample_890": [false, false, true, false, false], "sample_891": [false, false, false, false, false], "sample_892": [false, false, true, true, false], "sample_893": [false, false, false, false, false], "sample_894": [false, true, false, false, false], "sample_895": [false, false, true, true, false], "sample_896": [true, false, true, false, false], "sample_897": [false, true, true, false, false], "sample_898": [false, false, false, false, false], "sample_899": [false, false, false, false, false], "sample_900": [false, false, false, true, false], "sample_901": [true, true, true, false, false], "sample_902": [false, true, true, true, false], "sample_903": [false, true, false, false, true], "sample_904": [false, false, false, false, false], "sample_905": [false, false, true, true, false], "sample_906": [false, false, false, false, false], "sample_907": [false, false, false, false, false], "sample_908": [true, true, true, false, true], "sample_909": [false, true, false, true, false], "sample_910": [false, false, false, false, false], "sample_911": [false, false, false, false, false], "sample_912": [false, false, false, false, true], "sample_913": [true, false, false, false, false], "sample_914": [true, false, false, false, true], "sample_915": [false, false, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [false, true, false, true, false], "sample_919": [false, false, false, false, false], "sample_920": [false, false, true, false, false], "sample_921": [false, false, false, false, false], "sample_922": [true, false, false, false, false], "sample_923": [false, false, false, false, false], "sample_924": [false, false, false, true, false], "sample_925": [true, false, false, true, false], "sample_926": [false, false, false, false, false], "sample_927": [false, false, false, false, false], "sample_928": [false, false, false, false, false], "sample_929": [false, true, false, false, false], "sample_930": [false, false, false, false, false], "sample_931": [false, true, false, false, false], "sample_932": [false, false, false, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, false, false, false, false], "sample_935": [false, false, false, false, false], "sample_936": [false, false, false, true, false], "sample_937": [true, false, false, false, true], "sample_938": [false, false, false, false, false], "sample_939": [false, false, false, false, false], "sample_940": [false, false, false, false, false], "sample_941": [false, false, false, false, true], "sample_942": [false, true, true, false, false], "sample_943": [true, false, false, false, false], "sample_944": [false, false, false, false, false], "sample_945": [false, false, true, false, false], "sample_946": [false, false, false, false, false], "sample_947": [false, false, false, false, false], "sample_948": [false, false, true, false, false], "sample_949": [false, false, false, false, false], "sample_950": [true, false, true, true, false], "sample_951": [false, false, true, false, true], "sample_952": [true, false, false, true, true], "sample_953": [false, false, false, false, false], "sample_954": [false, false, false, false, false], "sample_955": [false, false, false, false, false], "sample_956": [false, true, true, false, false], "sample_957": [false, false, true, false, true], "sample_958": [false, false, false, false, false], "sample_959": [false, false, false, false, false], "sample_960": [true, true, false, false, true], "sample_961": [false, false, true, false, false], "sample_962": [false, false, false, false, false], "sample_963": [true, true, false, false, false], "sample_964": [false, false, false, false, false], "sample_965": [false, false, false, false, true], "sample_966": [false, false, false, true, true], "sample_967": [false, false, false, false, false], "sample_968": [false, false, false, false, false], "sample_969": [false, false, false, false, false], "sample_970": [false, false, true, true, false], "sample_971": [false, false, false, false, false], "sample_972": [false, false, false, false, false], "sample_973": [false, false, true, false, false], "sample_974": [true, true, false, true, true], "sample_975": [false, false, false, false, false], "sample_976": [false, false, false, false, false], "sample_977": [true, true, false, false, false], "sample_978": [false, false, false, false, false], "sample_979": [false, false, false, false, true], "sample_980": [false, false, false, true, false], "sample_981": [false, false, true, false, false], "sample_982": [false, true, false, false, true], "sample_983": [true, false, false, false, true], "sample_984": [false, false, false, false, true], "sample_985": [false, false, false, false, false], "sample_986": [false, false, true, true, false], "sample_987": [false, false, false, false, false], "sample_988": [false, false, false, false, false], "sample_989": [true, false, false, false, false], "sample_990": [true, false, false, false, true], "sample_991": [false, false, true, false, false], "sample_992": [false, false, false, false, false], "sample_993": [false, true, false, true, false], "sample_994": [false, false, false, true, false], "sample_995": [false, false, false, false, false], "sample_996": [false, true, false, false, false], "sample_997": [false, false, false, false, false], "sample_998": [false, false, false, true, true], "sample_999": [false, true, false, false, true], "sample_1000": [true, true, false, false, true], "sample_1001": [false, false, false, true, false], "sample_1002": [false, false, false, false, false], "sample_1003": [true, true, false, false, false], "sample_1004": [false, false, false, false, false], "sample_1005": [false, false, false, false, false], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, false, false, false], "sample_1008": [false, false, false, true, false], "sample_1009": [false, false, false, false, false], "sample_1010": [false, false, false, false, false], "sample_1011": [false, false, false, true, false], "sample_1012": [false, false, false, false, false], "sample_1013": [true, false, true, false, true], "sample_1014": [false, false, false, false, false], "sample_1015": [false, true, false, false, true], "sample_1016": [false, true, false, true, true], "sample_1017": [false, false, false, false, false], "sample_1018": [false, true, false, true, false], "sample_1019": [false, false, false, false, false], "sample_1020": [true, false, true, true, false], "sample_1021": [true, false, false, true, true], "sample_1022": [false, false, false, false, false], "sample_1023": [false, true, false, false, false], "sample_1024": [false, true, true, false, false], "sample_1025": [false, false, false, false, false], "sample_1026": [true, false, true, false, true], "sample_1027": [false, true, false, false, false], "sample_1028": [false, false, false, false, false], "sample_1029": [false, false, false, false, false], "sample_1030": [false, false, false, false, false], "sample_1031": [false, false, false, false, false], "sample_1032": [true, true, false, false, false], "sample_1033": [true, false, true, false, false], "sample_1034": [false, false, false, false, false], "sample_1035": [false, true, false, false, false], "sample_1036": [false, true, false, false, false], "sample_1037": [false, false, false, false, false], "sample_1038": [true, true, false, true, true], "sample_1039": [true, false, false, false, false], "sample_1040": [false, false, false, false, false], "sample_1041": [true, false, true, false, false], "sample_1042": [false, false, false, false, false], "sample_1043": [false, true, false, false, false], "sample_1044": [false, false, false, false, false], "sample_1045": [false, false, true, false, false], "sample_1046": [false, false, false, false, false], "sample_1047": [true, true, false, true, false], "sample_1048": [false, false, true, false, false], "sample_1049": [false, false, false, false, false], "sample_1050": [false, false, false, false, false], "sample_1051": [false, true, false, false, true], "sample_1052": [false, false, false, false, false], "sample_1053": [false, true, false, false, true], "sample_1054": [true, true, true, true, false], "sample_1055": [true, false, true, true, true], "sample_1056": [false, false, false, false, false], "sample_1057": [false, false, false, false, false], "sample_1058": [false, true, false, false, false], "sample_1059": [false, false, false, false, false], "sample_1060": [false, false, false, false, false], "sample_1061": [false, false, true, true, false], "sample_1062": [false, false, false, false, false], "sample_1063": [false, true, false, false, false], "sample_1064": [true, true, true, true, true], "sample_1065": [false, false, false, false, false], "sample_1066": [true, true, false, false, true], "sample_1067": [false, false, false, false, false], "sample_1068": [true, true, false, true, false], "sample_1069": [false, false, false, false, false], "sample_1070": [false, true, false, false, false], "sample_1071": [false, false, false, true, false], "sample_1072": [false, true, false, false, true], "sample_1073": [false, false, false, false, false], "sample_1074": [false, false, false, false, false], "sample_1075": [false, false, false, false, false], "sample_1076": [false, false, false, false, false], "sample_1077": [false, false, true, false, false], "sample_1078": [true, false, false, false, true], "sample_1079": [false, true, false, false, false], "sample_1080": [true, false, true, false, true], "sample_1081": [false, true, false, true, false], "sample_1082": [false, false, true, false, true], "sample_1083": [false, false, false, false, true], "sample_1084": [false, false, false, false, true], "sample_1085": [false, false, false, false, false], "sample_1086": [true, true, false, false, false], "sample_1087": [false, false, false, true, false], "sample_1088": [true, false, false, false, true], "sample_1089": [false, false, false, false, false], "sample_1090": [false, false, false, false, false], "sample_1091": [true, false, false, true, false], "sample_1092": [false, false, false, false, false], "sample_1093": [false, false, false, false, false], "sample_1094": [true, false, true, true, true], "sample_1095": [false, false, false, false, false], "sample_1096": [true, false, true, false, false], "sample_1097": [false, false, false, false, false], "sample_1098": [false, false, false, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [false, false, false, false, false], "sample_1101": [false, false, false, false, false], "sample_1102": [true, false, false, false, false], "sample_1103": [false, false, false, false, false], "sample_1104": [true, false, false, true, false], "sample_1105": [false, false, false, false, false], "sample_1106": [false, false, false, false, false], "sample_1107": [false, false, false, true, false], "sample_1108": [true, false, true, false, false], "sample_1109": [false, false, false, false, true], "sample_1110": [false, false, false, false, true], "sample_1111": [false, false, false, false, false], "sample_1112": [false, false, true, true, true], "sample_1113": [false, false, false, false, false], "sample_1114": [false, false, false, true, true], "sample_1115": [false, false, false, false, false], "sample_1116": [false, true, true, true, true], "sample_1117": [true, false, true, false, false], "sample_1118": [false, true, false, false, true], "sample_1119": [false, true, false, false, true], "sample_1120": [false, true, false, false, false], "sample_1121": [false, false, false, false, false], "sample_1122": [false, false, false, false, true], "sample_1123": [false, false, false, false, false], "sample_1124": [false, false, false, false, false], "sample_1125": [false, false, true, true, true], "sample_1126": [true, true, true, true, true], "sample_1127": [false, false, false, false, false], "sample_1128": [true, false, true, true, true], "sample_1129": [false, false, false, false, false], "sample_1130": [false, false, true, true, true], "sample_1131": [false, false, false, false, false], "sample_1132": [false, false, true, false, false], "sample_1133": [true, false, false, true, true], "sample_1134": [false, false, true, false, false], "sample_1135": [false, false, false, false, false], "sample_1136": [false, false, false, false, false], "sample_1137": [false, false, true, false, true], "sample_1138": [false, false, false, false, false], "sample_1139": [true, false, false, true, false], "sample_1140": [false, false, false, true, false], "sample_1141": [false, false, false, false, false], "sample_1142": [false, false, false, false, false], "sample_1143": [false, true, false, false, false], "sample_1144": [false, false, false, true, false], "sample_1145": [false, true, false, true, false], "sample_1146": [false, false, true, false, false], "sample_1147": [true, false, false, false, false], "sample_1148": [false, false, true, false, true], "sample_1149": [false, false, true, true, false], "sample_1150": [false, false, false, false, true], "sample_1151": [false, false, false, false, false], "sample_1152": [false, false, false, false, true], "sample_1153": [false, false, false, false, false], "sample_1154": [false, false, true, false, true], "sample_1155": [true, true, false, false, true], "sample_1156": [true, true, false, false, false], "sample_1157": [false, false, false, true, false], "sample_1158": [true, false, true, true, true], "sample_1159": [true, false, false, false, false], "sample_1160": [false, false, false, true, false], "sample_1161": [false, false, false, false, true], "sample_1162": [false, false, false, false, false], "sample_1163": [false, false, false, true, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, true, true, true, true], "sample_1166": [false, false, false, false, false], "sample_1167": [true, false, true, false, false], "sample_1168": [false, true, false, false, false], "sample_1169": [false, false, true, false, false], "sample_1170": [false, false, false, true, false], "sample_1171": [false, true, false, true, false], "sample_1172": [false, false, false, false, false], "sample_1173": [false, false, true, false, false], "sample_1174": [false, false, false, false, false], "sample_1175": [false, false, false, false, true], "sample_1176": [true, true, false, false, false], "sample_1177": [false, false, false, false, false], "sample_1178": [false, false, true, false, false], "sample_1179": [true, true, false, false, false], "sample_1180": [false, true, true, true, false], "sample_1181": [true, true, true, true, true], "sample_1182": [false, false, true, false, false], "sample_1183": [false, false, false, false, false], "sample_1184": [false, true, false, false, false], "sample_1185": [false, true, true, false, true], "sample_1186": [false, true, false, true, true], "sample_1187": [false, false, false, false, false], "sample_1188": [false, false, false, false, true], "sample_1189": [true, false, true, false, false], "sample_1190": [false, false, false, false, false], "sample_1191": [false, false, false, false, false], "sample_1192": [false, false, false, true, true], "sample_1193": [false, false, false, false, false], "sample_1194": [true, true, true, false, false], "sample_1195": [false, false, false, false, false], "sample_1196": [false, false, false, false, false], "sample_1197": [false, false, false, false, false], "sample_1198": [false, false, false, false, false], "sample_1199": [false, false, false, false, false], "sample_1200": [false, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [false, false, false, false, false], "sample_1203": [false, false, false, false, false], "sample_1204": [false, false, false, false, false], "sample_1205": [true, false, true, true, false], "sample_1206": [false, false, false, false, false], "sample_1207": [false, false, false, false, true], "sample_1208": [true, false, true, false, false], "sample_1209": [false, false, false, false, true]}}